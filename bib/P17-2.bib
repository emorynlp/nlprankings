@inproceedings{P17-2001,
 abstract = {Temporal relation classification is becoming an active research field. Lots of
methods have been proposed, while most of them focus on extracting features
from external resources. Less attention has been paid to a significant advance
in a closely related task: relation extraction. In this work, we borrow a
state-of-the-art method in relation extraction by adopting bidirectional long
short-term memory (Bi-LSTM) along dependency paths (DP). We make a “common
root” assumption to extend DP representations of cross-sentence links. In the
final comparison to two state-of-the-art systems on TimeBank-Dense, our model
achieves comparable performance, without using external knowledge, as well as
manually annotated attributes of entities (class, tense, polarity, etc.).},
 address = {Vancouver, Canada},
 author = {Cheng, Fei  and  Miyao, Yusuke},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {1--6},
 publisher = {Association for Computational Linguistics},
 title = {Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths},
 year = {2017}
}

@inproceedings{P17-2002,
 abstract = {This paper addresses the task of AMR-to-text generation by leveraging
synchronous
node replacement grammar. During training, graph-to-string rules are learned
using a heuristic extraction algorithm. At test time, a graph transducer is
applied to collapse input AMRs and generate output sentences. Evaluated on a
standard benchmark, our method gives the state-of-the-art result.},
 address = {Vancouver, Canada},
 author = {Song, Linfeng  and  Peng, Xiaochang  and  Zhang, Yue  and  Wang, Zhiguo  and  Gildea, Daniel},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {7--13},
 publisher = {Association for Computational Linguistics},
 title = {AMR-to-text Generation with Synchronous Node Replacement Grammar},
 year = {2017}
}

@inproceedings{P17-2003,
 abstract = {Lexical features are a major source of information in state-of-the-art
coreference resolvers. Lexical features implicitly model some of the linguistic
phenomena at a fine granularity level. They are especially useful for
representing the context of mentions. In this paper we investigate a drawback
of using many lexical features in state-of-the-art coreference resolvers. We
show that if coreference resolvers mainly rely on lexical features, they can
hardly generalize to unseen domains. Furthermore, we show that the current
coreference resolution evaluation is clearly flawed by only evaluating on a
specific split of a specific dataset in which there is a notable overlap
between the training, development and test sets.},
 address = {Vancouver, Canada},
 author = {Moosavi, Nafise Sadat  and  Strube, Michael},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {14--19},
 publisher = {Association for Computational Linguistics},
 title = {Lexical Features in Coreference Resolution: To be Used With Caution},
 year = {2017}
}

@inproceedings{P17-2004,
 abstract = {MT evaluation metrics are tested for correlation with human judgments either at
the sentence- or the corpus-level. Trained metrics ignore corpus-level
judgments and are trained for high sentence-level correlation only. We show
that training only for one objective (sentence or corpus level), can not only
harm the performance on the other objective, but it can also be suboptimal for
the objective being optimized. To this end we present a metric trained for
corpus-level and show empirical comparison against a metric trained for
sentence-level exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize
both objectives simultaneously and show that it is far more stable than--and on
average outperforms--both models on both objectives.},
 address = {Vancouver, Canada},
 author = {Stanojevi\'{c}, Milo\v{s}  and  Sima'an, Khalil},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {20--25},
 publisher = {Association for Computational Linguistics},
 title = {Alternative Objective Functions for Training MT Evaluation Metrics},
 year = {2017}
}

@inproceedings{P17-2005,
 abstract = {We present a new framework for evaluating extractive summarizers, which is
based on a principled representation as optimization problem. We prove that
every extractive summarizer can be decomposed into an objective function  and
an optimization technique. We perform a comparative analysis and evaluation of
several objective functions embedded in well-known summarizers regarding their
correlation with human judgments. Our comparison of these correlations across
two datasets yields surprising insights into the role and performance of
objective functions in the different  summarizers.},
 address = {Vancouver, Canada},
 author = {Peyrard, Maxime  and  Eckle-Kohler, Judith},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {26--31},
 publisher = {Association for Computational Linguistics},
 title = {A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments},
 year = {2017}
}

@inproceedings{P17-2006,
 abstract = {A common test administered during neurological examination is the semantic
fluency test, in which the patient must list as many examples of a given
semantic category as possible under timed conditions. Poor performance
is associated with neurological conditions characterized
by impairments in executive function, such as dementia, schizophrenia, and
autism
spectrum disorder (ASD). Methods for analyzing semantic fluency responses at
the level of detail necessary to uncover these differences have typically
relied on subjective manual annotation.
In this paper, we explore automated approaches for scoring semantic fluency
responses that leverage ontological resources and distributional semantic
models to characterize the semantic fluency responses
produced by young children with and without ASD. Using these methods, we find
significant differences
in the semantic fluency responses of children with ASD, demonstrating
the utility of using objective methods for clinical language analysis.},
 address = {Vancouver, Canada},
 author = {Prud'hommeaux, Emily  and  van Santen, Jan  and  Gliner, Douglas},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {32--37},
 publisher = {Association for Computational Linguistics},
 title = {Vector space models for evaluating semantic fluency in autism},
 year = {2017}
}

@inproceedings{P17-2007,
 abstract = {In this paper, we address semantic parsing in a multilingual context. We train
one multilingual model that is capable of parsing natural language sentences
from multiple different languages into their corresponding formal semantic
representations. We extend an existing sequence-to-tree model to a multi-task
learning framework which shares the decoder for generating semantic
representations. We report evaluation results on the multilingual GeoQuery
corpus and introduce a new multilingual version of the ATIS corpus.},
 address = {Vancouver, Canada},
 author = {Susanto, Raymond Hendy  and  Lu, Wei},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {38--44},
 publisher = {Association for Computational Linguistics},
 title = {Neural Architectures for Multilingual Semantic Parsing},
 year = {2017}
}

@inproceedings{P17-2008,
 abstract = {There is a growing demand for automatic assessment of spoken English
proficiency. These systems need to handle large variations in input
data owing to the wide range of candidate skill levels and L1s, and
errors from ASR. Some candidates will be a poor match
to the training data set, undermining the validity of the predicted grade. For
high stakes tests it is essential for such systems not only to grade well, but
also to provide a measure
of their uncertainty in their predictions, enabling rejection to human
graders. Previous work examined Gaussian Process (GP) graders which, though
successful, do not scale well with large data sets. Deep Neural Network (DNN)
may also be used to provide uncertainty using Monte-Carlo Dropout (MCD). This
paper proposes a novel method to yield uncertainty and compares it to GPs and
DNNs with MCD. The proposed approach explicitly teaches a DNN to have low
uncertainty on training data and high uncertainty on generated artificial data.
On experiments conducted on data from the Business Language Testing Service
(BULATS), the proposed approach is found to outperform GPs and DNNs with MCD in
uncertainty-based rejection whilst achieving comparable grading performance.},
 address = {Vancouver, Canada},
 author = {Malinin, Andrey  and  Ragni, Anton  and  Knill, Kate  and  Gales, Mark},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {45--50},
 publisher = {Association for Computational Linguistics},
 title = {Incorporating Uncertainty into Deep Learning for Spoken Language Assessment},
 year = {2017}
}

@inproceedings{P17-2009,
 abstract = {Language identification (LID) is a critical first step for processing
multilingual text.  Yet most LID systems are not designed to handle the
linguistic diversity of global platforms like Twitter, where local dialects and
rampant code-switching lead language classifiers to systematically miss
minority dialect speakers and multilingual speakers.  We propose a new dataset
and a character-based sequence-to-sequence model for LID designed to support
dialectal and multilingual language varieties. Our model achieves
state-of-the-art performance on multiple LID benchmarks.  Furthermore, in a
case study using Twitter for health tracking,  our method substantially
increases the availability of texts written by underrepresented populations, enabling the development of "socially inclusive" NLP tools.},
 address = {Vancouver, Canada},
 author = {Jurgens, David  and  Tsvetkov, Yulia  and  Jurafsky, Dan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {51--57},
 publisher = {Association for Computational Linguistics},
 title = {Incorporating Dialectal Variability for Socially Equitable Language Identification},
 year = {2017}
}

@inproceedings{P17-2010,
 abstract = {Traditionally, compound splitters are evaluated intrinsically on gold-standard
data or extrinsically on the task of statistical machine translation. We
explore a novel way for the extrinsic evaluation of compound splitters, namely
recognizing textual entailment. Compound splitting has great potential for this
novel task that is both transparent and well-defined. Moreover, we show that it
addresses certain aspects that are either ignored in intrinsic evaluations or
compensated for by taskinternal mechanisms in statistical machine translation.
We show significant improvements using different compound splitting methods on
a German textual entailment dataset.},
 address = {Vancouver, Canada},
 author = {Jagfeld, Glorianna  and  Ziering, Patrick  and  van der Plas, Lonneke},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {58--63},
 publisher = {Association for Computational Linguistics},
 title = {Evaluating Compound Splitters Extrinsically with Textual Entailment},
 year = {2017}
}

@inproceedings{P17-2011,
 abstract = {A large amount of recent research has focused on
tasks that combine language and vision, resulting in
a proliferation of datasets and methods. One such task is
action recognition, whose applications
include image annotation, scene understanding and
image retrieval. In this survey, we categorize the
existing approaches based on how
they conceptualize this problem
and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and
disadvantages. We focus on recently developed
datasets which link visual information with
linguistic resources and provide a fine-grained syntactic and semantic
analysis of actions in images.},
 address = {Vancouver, Canada},
 author = {Gella, Spandana  and  Keller, Frank},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {64--71},
 publisher = {Association for Computational Linguistics},
 title = {An Analysis of Action Recognition Datasets for Language and Vision Tasks},
 year = {2017}
}

@inproceedings{P17-2012,
 abstract = {There has been relatively little attention to incorporating linguistic prior to
neural machine translation. Much of the previous work was further constrained
to considering linguistic prior on the source side. In this paper, we propose a
hybrid model, called NMT+RNNG, that learns to parse and translate by combining
the recurrent neural network grammar into the attention-based neural machine
translation. Our approach encourages the neural machine translation model to
incorporate linguistic prior during training, and lets it translate on its own
afterward. Extensive experiments with four language pairs show the
effectiveness of the proposed NMT+RNNG.},
 address = {Vancouver, Canada},
 author = {Eriguchi, Akiko  and  Tsuruoka, Yoshimasa  and  Cho, Kyunghyun},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {72--78},
 publisher = {Association for Computational Linguistics},
 title = {Learning to Parse and Translate Improves Neural Machine Translation},
 year = {2017}
}

@inproceedings{P17-2013,
 abstract = {Natural language processing has increasingly moved from modeling documents and
words toward studying the people behind the language. This move to working with
data at the user or community level has presented the field with different
characteristics of linguistic data. In this paper, we empirically characterize
various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level
(as with traditional NLP), they follow the central limit theorem to become much
more Log-normal or even Normal at the user- and county-levels. Finally, we
demonstrate that modeling lexical features for the correct level of analysis
leads to marked improvements in common social scientific prediction tasks.},
 address = {Vancouver, Canada},
 author = {Almodaresi, Fatemeh  and  Ungar, Lyle  and  Kulkarni, Vivek  and  Zakeri, Mohsen  and  Giorgi, Salvatore  and  Schwartz, H. Andrew},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {79--84},
 publisher = {Association for Computational Linguistics},
 title = {On the Distribution of Lexical Features at Multiple Levels of Analysis},
 year = {2017}
}

@inproceedings{P17-2014,
 abstract = {We present the first attempt at using sequence to sequence neural networks to
model text simplification (TS). Unlike the previously proposed automated TS
systems, our neural text simplification (NTS) systems are able to
simultaneously perform lexical simplification and content reduction. An
extensive human evaluation of the output has shown that NTS systems achieve
almost perfect                          grammaticality and meaning preservation of
output
sentences and
higher level of simplification than the state-of-the-art automated TS systems},
 address = {Vancouver, Canada},
 author = {Nisioi, Sergiu  and  \v{S}tajner, Sanja  and  Ponzetto, Simone Paolo  and  Dinu, Liviu P.},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {85--91},
 publisher = {Association for Computational Linguistics},
 title = {Exploring Neural Text Simplification Models},
 year = {2017}
}

@inproceedings{P17-2015,
 abstract = {This paper highlights challenges in industrial research related to translating
research in natural language processing into commercial products.
While the interest in natural language processing from industry is
significant, the transfer of research to commercial products is
non-trivial and its challenges are often unknown to or underestimated
by many researchers. I discuss current obstacles and provide
suggestions for increasing the chances for translating research to commercial
success based on my experience in industrial research.},
 address = {Vancouver, Canada},
 author = {Dahlmeier, Daniel},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {92--96},
 publisher = {Association for Computational Linguistics},
 title = {On the Challenges of Translating NLP Research into Commercial Products},
 year = {2017}
}

@inproceedings{P17-2016,
 abstract = {We provide several methods for sentence-alignment of texts with different
complexity levels. Using the best of them, we sentence-align the Newsela
corpora, thus providing large training materials for automatic text
simplification (ATS) systems. We show that using this dataset, even the
standard phrase-based statistical machine translation models for ATS can
outperform the state-of-the-art ATS systems.},
 address = {Vancouver, Canada},
 author = {\v{S}tajner, Sanja  and  Franco-Salvador, Marc  and  Ponzetto, Simone Paolo  and  Rosso, Paolo  and  Stuckenschmidt, Heiner},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {97--102},
 publisher = {Association for Computational Linguistics},
 title = {Sentence Alignment Methods for Improving Text Simplification Systems},
 year = {2017}
}

@inproceedings{P17-2017,
 abstract = {Linguistically diverse datasets are critical for training and evaluating robust
machine learning systems, but data collection is a costly process that often
requires experts. Crowdsourcing the process of paraphrase generation is an
effective means of expanding natural language datasets, but there has been
limited analysis of the trade-offs that arise when designing tasks. In this
paper, we present the first systematic study of the key factors in
crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for
correctness, grammaticality, and linguistic diversity. Our observations provide
new insight into the trade-offs between accuracy and diversity in crowd
responses that arise as a result of task design, providing guidance for future
paraphrase generation procedures.},
 address = {Vancouver, Canada},
 author = {Jiang, Youxuan  and  Kummerfeld, Jonathan K.  and  Lasecki, Walter S.},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {103--109},
 publisher = {Association for Computational Linguistics},
 title = {Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection},
 year = {2017}
}

@inproceedings{P17-2018,
 abstract = {Transition-based dependency parsers often need sequences of local shift and
reduce operations to produce certain attachments. Correct individual decisions
hence require global information about the sentence context and mistakes
cause error propagation. This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a
single transition. This allows the parser to leverage lexical information more
directly in transition decisions. Hence, arc-swift can achieve significantly
better performance with a very small beam size. Our parsers reduce error by
3.7--7.6% relative to those using existing transition systems on the Penn
Treebank dependency parsing task and English Universal Dependencies.},
 address = {Vancouver, Canada},
 author = {Qi, Peng  and  Manning, Christopher D.},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {110--117},
 publisher = {Association for Computational Linguistics},
 title = {Arc-swift: A Novel Transition System for Dependency Parsing},
 year = {2017}
}

@inproceedings{P17-2019,
 abstract = {Generative models defining joint distributions over parse trees and sentences
are useful for parsing and language modeling, but impose restrictions on the
scope of features and are often outperformed by discriminative models. We
propose a framework for parsing and language modeling which marries a
generative model with a discriminative recognition model in an encoder-decoder
setting. We provide interpretations of the framework based on expectation
maximization and variational inference, and show that it enables parsing and
language modeling within a single implementation. On the English Penn
Treen-bank, our framework obtains competitive performance on constituency
parsing while matching the state-of-the-art single- model language modeling
score.},
 address = {Vancouver, Canada},
 author = {Cheng, Jianpeng  and  Lopez, Adam  and  Lapata, Mirella},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {118--124},
 publisher = {Association for Computational Linguistics},
 title = {A Generative Parser with a Discriminative Recognition Algorithm},
 year = {2017}
}

@inproceedings{P17-2020,
 abstract = {Recently, the neural machine translation systems showed their promising
performance and surpassed the phrase-based systems for most translation tasks.
Retreating into conventional concepts machine translation while utilizing
effective neural models is vital for comprehending the leap accomplished by
neural machine translation over phrase-based methods. This work proposes a
direct HMM with neural network-based lexicon and alignment models, which are
trained jointly using the Baum-Welch algorithm. The direct HMM is applied to
rerank the n-best list created by a state-of-the-art phrase-based translation
system and it provides improvements by up to 1.0% Bleu scores on two different
translation tasks.},
 address = {Vancouver, Canada},
 author = {Wang, Weiyue  and  Alkhouli, Tamer  and  Zhu, Derui  and  Ney, Hermann},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {125--131},
 publisher = {Association for Computational Linguistics},
 title = {Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation},
 year = {2017}
}

@inproceedings{P17-2021,
 abstract = {We present a simple method to incorporate syntactic information about the
target language in a neural machine translation system by translating into
linearized, lexicalized constituency trees. An experiment on the WMT16
German-English news translation task resulted in an improved BLEU score when
compared to a syntax-agnostic NMT baseline trained on the same dataset. An
analysis of the translations from the syntax-aware system shows that it
performs more reordering during translation in comparison to the baseline. A
small-scale human evaluation also showed an advantage to the syntax-aware
system.},
 address = {Vancouver, Canada},
 author = {Aharoni, Roee  and  Goldberg, Yoav},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {132--140},
 publisher = {Association for Computational Linguistics},
 title = {Towards String-To-Tree Neural Machine Translation},
 year = {2017}
}

@inproceedings{P17-2022,
 abstract = {Informal first-person narratives are a unique resource for computational mod-
els of everyday events and people’s affective reactions to them. People
blogging about their day tend not to explicitly say I am happy. Instead they
describe situations from which other humans can readily infer their affective
reactions. However current sentiment dictionaries are missing much of the
information needed to make similar inferences. We build on recent work that
models affect in terms of lexical predicate functions and affect on the
predicate’s arguments. We present a method to learn proxies for these
functions from first- person narratives. We construct a novel fine-grained test
set, and show that the pat- terns we learn improve our ability to pre- dict
first-person affective reactions to everyday events, from a Stanford sentiment
baseline of .67F to .75F.},
 address = {Vancouver, Canada},
 author = {Reed, Lena  and  Wu, Jiaqi  and  Oraby, Shereen  and  Anand, Pranav  and  Walker, Marilyn},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {141--147},
 publisher = {Association for Computational Linguistics},
 title = {Learning Lexico-Functional Patterns for First-Person Affect},
 year = {2017}
}

@inproceedings{P17-2023,
 abstract = {This paper makes a focused contribution to supervised aspect extraction. It
shows that if the system has performed aspect extraction from many past domains
and retained their results as knowledge, Conditional Random Fields (CRF) can
leverage this knowledge in a lifelong learning manner to extract in a new
domain markedly better than the traditional CRF without using this prior
knowledge. The key innovation is that even after CRF training, the model can
still improve its extraction with experiences in its applications.},
 address = {Vancouver, Canada},
 author = {Shu, Lei  and  Xu, Hu  and  Liu, Bing},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {148--154},
 publisher = {Association for Computational Linguistics},
 title = {Lifelong Learning CRF for Supervised Aspect Extraction},
 year = {2017}
}

@inproceedings{P17-2024,
 abstract = {A fundamental advantage of neural models
for NLP is their ability to learn representations
from scratch. However, in
practice this often means ignoring existing
external linguistic resources, e.g., WordNet
or domain specific ontologies such
as the Unified Medical Language System
(UMLS). We propose a general, novel
method for exploiting such resources via
weight sharing. Prior work on weight
sharing in neural networks has considered
it largely as a means of model compression.
In contrast, we treat weight sharing
as a flexible mechanism for incorporating
prior knowledge into neural models.
We show that this approach consistently
yields improved performance on classification tasks compared to baseline
strategies that do not exploit weight sharing.},
 address = {Vancouver, Canada},
 author = {Zhang, Ye  and  Lease, Matthew  and  Wallace, Byron C.},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {155--160},
 publisher = {Association for Computational Linguistics},
 title = {Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization},
 year = {2017}
}

@inproceedings{P17-2025,
 abstract = {Recent work has proposed several generative neural models for constituency
parsing that achieve state-of-the-art results. Since direct search in these
generative models is difficult, they have primarily been used to rescore
candidate outputs from base parsers in which decoding is more straightforward.
We first present an algorithm for direct search in these generative models.  We
then demonstrate that the rescoring results are at least partly due to implicit
model combination rather than reranking effects.  Finally, we show that
explicit model combination can improve performance even further, resulting in
new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold
data and 94.66 F1 when using external data.},
 address = {Vancouver, Canada},
 author = {Fried, Daniel  and  Stern, Mitchell  and  Klein, Dan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {161--166},
 publisher = {Association for Computational Linguistics},
 title = {Improving Neural Parsing by Disentangling Model Combination and Reranking Effects},
 year = {2017}
}

@inproceedings{P17-2026,
 abstract = {In this paper we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution
and the product of their marginal distributions. Then, we show that word2vec's
skip-gram with negative sampling embedding algorithm finds the optimal
low-dimensional approximation of this JS dependency measure between the words
and their contexts. The gap between the optimal score and the low-dimensional
approximation is demonstrated on a standard text corpus.},
 address = {Vancouver, Canada},
 author = {Melamud, Oren  and  Goldberger, Jacob},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {167--171},
 publisher = {Association for Computational Linguistics},
 title = {Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function},
 year = {2017}
}

@inproceedings{P17-2027,
 abstract = {In this work, we propose a novel, implicitly-defined neural network
architecture
and describe a method to compute its components.
The proposed architecture forgoes the causality assumption
used to formulate recurrent neural networks
and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies.
Initial experiments demonstrate the new architecture outperforms both the
Stanford Parser
and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging
task
and a baseline bidirectional network on an additional artificial random biased
walk task.},
 address = {Vancouver, Canada},
 author = {Kazi, Michaeel  and  Thompson, Brian},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {172--177},
 publisher = {Association for Computational Linguistics},
 title = {Implicitly-Defined Neural Networks for Sequence Labeling},
 year = {2017}
}

@inproceedings{P17-2028,
 abstract = {This study explores the role of speech register and prosody for the task of
word segmentation. Since these two factors are thought to play an important
role in early language acquisition, we aim to quantify their contribution for
this task. We study a Japanese corpus containing both infant- and
adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries. The results showed that the
difference between registers is smaller than previously reported and that
prosodic boundary information helps more adult- than infant-directed speech.},
 address = {Vancouver, Canada},
 author = {Ludusan, Bogdan  and  Mazuka, Reiko  and  Bernard, Mathieu  and  Cristia, Alejandrina  and  Dupoux, Emmanuel},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {178--183},
 publisher = {Association for Computational Linguistics},
 title = {The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective},
 year = {2017}
}

@inproceedings{P17-2029,
 abstract = {Previous work introduced transition-based algorithms to form a unified
architecture
of parsing rhetorical structures (including span, nuclearity and relation), but
did not achieve satisfactory performance. In this paper, we propose that
transition-based model is more appropriate for parsing the naked discourse tree
(i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and
should be treated elaborately with consideration of three kinds of relations
including within-sentence, across-sentence and across-paragraph relations.
Thus, we design a pipelined two-stage parsing method for generating an RST tree
from text. Experimental results show that our method achieves state-of-the-art
performance, especially on span and nuclearity identification.},
 address = {Vancouver, Canada},
 author = {Wang, Yizhong  and  Li, Sujian  and  Wang, Houfeng},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {184--188},
 publisher = {Association for Computational Linguistics},
 title = {A Two-Stage Parsing Method for Text-Level Discourse Analysis},
 year = {2017}
}

@inproceedings{P17-2030,
 abstract = {We propose a new dependency parsing scheme which jointly parses a sentence and
repairs grammatical errors by extending the non-directional transition-based
formalism of Goldberg and Elhadad (2010) with three additional actions:
SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in
derivation, we also introduce simple constraints that ensure the parser
termination. We evaluate our model with respect to dependency accuracy and
grammaticality improvements for ungrammatical sentences, demonstrating the
robustness and applicability of our scheme.},
 address = {Vancouver, Canada},
 author = {Sakaguchi, Keisuke  and  Post, Matt  and  Van Durme, Benjamin},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {189--195},
 publisher = {Association for Computational Linguistics},
 title = {Error-repair Dependency Parsing for Ungrammatical Texts},
 year = {2017}
}

@inproceedings{P17-2031,
 abstract = {Modeling attention in neural multi-source sequence-to-sequence learning remains
a relatively unexplored area, despite its usefulness in tasks that incorporate
multiple source languages or modalities.
We propose two novel approaches to combine the outputs of attention mechanisms
over each source sequence, flat and hierarchical.
We compare the proposed methods with existing techniques and present results of
systematic evaluation of those methods on the WMT16 Multimodal Translation and
Automatic Post-editing tasks.
We show that the proposed methods achieve competitive results on both tasks.},
 address = {Vancouver, Canada},
 author = {Libovick\'{y}, Jind\v{r}ich  and  Helcl, Jind\v{r}ich},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {196--202},
 publisher = {Association for Computational Linguistics},
 title = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning},
 year = {2017}
}

@inproceedings{P17-2032,
 abstract = {We investigate the problem of sentence-level supporting argument detection from
relevant documents for user-specified claims. A dataset containing claims and
associated citation articles is collected from online debate website
idebate.org. We then manually label sentence-level supporting arguments from
the documents along with their types as study, factual, opinion, or reasoning.
We further characterize arguments of different types, and explore whether
leveraging type information can facilitate the supporting arguments detection
task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses
features informed by argument types yields better performance than the same
ranker trained without type information.},
 address = {Vancouver, Canada},
 author = {Hua, Xinyu  and  Wang, Lu},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {203--208},
 publisher = {Association for Computational Linguistics},
 title = {Understanding and Detecting Supporting Arguments of Diverse Types},
 year = {2017}
}

@inproceedings{P17-2033,
 abstract = {We propose a simple yet effective text-based user geolocation model based on a
neural network with one hidden layer, which achieves state of the art
performance over three Twitter benchmark geolocation datasets, in addition to
producing word                                and phrase embeddings in the hidden
layer
that we
show to
be
useful for detecting dialectal terms. As part of our analysis of dialectal
terms, we release DAREDS, a dataset for evaluating dialect term detection
methods.},
 address = {Vancouver, Canada},
 author = {Rahimi, Afshin  and  Cohn, Trevor  and  Baldwin, Timothy},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {209--216},
 publisher = {Association for Computational Linguistics},
 title = {A Neural Model for User Geolocation and Lexical Dialectology},
 year = {2017}
}

@inproceedings{P17-2034,
 abstract = {We present a new visual reasoning language dataset, containing 92,244 pairs of
examples of natural statements grounded in synthetic images with 3,962 unique
sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of
linguistic phenomena, requiring visual and set-theoretic reasoning. We
experiment with various models, and show the data presents a strong challenge
for future research.},
 address = {Vancouver, Canada},
 author = {Suhr, Alane  and  Lewis, Mike  and  Yeh, James  and  Artzi, Yoav},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {217--223},
 publisher = {Association for Computational Linguistics},
 title = {A Corpus of Natural Language for Visual Reasoning},
 year = {2017}
}

@inproceedings{P17-2035,
 abstract = {We present a neural architecture for containment relation identification
between medical events and/or temporal expressions. We experiment on a corpus
of de-identified clinical notes in English from the Mayo Clinic, namely the
THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best
result reported on this corpus to date.},
 address = {Vancouver, Canada},
 author = {Tourille, Julien  and  Ferret, Olivier  and  Neveol, Aurelie  and  Tannier, Xavier},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {224--230},
 publisher = {Association for Computational Linguistics},
 title = {Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers},
 year = {2017}
}

@inproceedings{P17-2036,
 abstract = {Generative conversational systems are attracting increasing attention in
natural language processing (NLP). Recently, researchers have noticed the
importance of context information in dialog processing, and built various
models to utilize context. However, there is no systematic comparison to
analyze how to use context effectively. In this paper, we conduct an empirical
study to compare various models and investigate the effect of context
information in dialog systems. We also propose a variant that explicitly
weights context vectors by context-query relevance, outperforming the other
baselines.},
 address = {Vancouver, Canada},
 author = {Tian, Zhiliang  and  Yan, Rui  and  Mou, Lili  and  Song, Yiping  and  Feng, Yansong  and  Zhao, Dongyan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {231--236},
 publisher = {Association for Computational Linguistics},
 title = {How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models},
 year = {2017}
}

@inproceedings{P17-2037,
 abstract = {Discourse segmentation is a crucial step in building end-to-end discourse
parsers. However, discourse segmenters only exist for a few languages and
domains. Typically they only  detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on
high-quality syntactic parses and rich heuristics that are not generally
available across languages and domains. In this paper, we propose statistical
discourse segmenters for five languages and three domains that do not rely on
gold pre-annotations.  We also consider the problem of learning discourse
segmenters when no labeled data is available for a language. Our fully
supervised system obtains 89.5% F1 for English newswire, with slight drops in
performance on other domains, and we report supervised and unsupervised
(cross-lingual) results for five languages in total.},
 address = {Vancouver, Canada},
 author = {Braud, Chlo\'{e}  and  Lacroix, Oph\'{e}lie  and  S{\o}gaard, Anders},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {237--243},
 publisher = {Association for Computational Linguistics},
 title = {Cross-lingual and cross-domain discourse segmentation of entire documents},
 year = {2017}
}

@inproceedings{P17-2038,
 abstract = {Automatic identification of good arguments on a controversial topic has
applications in civics and education, to name a few. While in the civics
context it might be acceptable to create separate models for  each topic, in
the context of              scoring of students' writing there is a preference for a
single
model that applies to all responses. Given that good arguments for one topic
are likely to be irrelevant for another, is a single model for detecting good
arguments a contradiction in terms? We investigate the extent to which it is
possible to close the performance gap between topic-specific and across-topics
models for identification of good arguments.},
 address = {Vancouver, Canada},
 author = {Beigman Klebanov, Beata  and  Gyawali, Binod  and  Song, Yi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {244--249},
 publisher = {Association for Computational Linguistics},
 title = {Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?},
 year = {2017}
}

@inproceedings{P17-2039,
 abstract = {Argumentation quality is viewed differently in argumentation theory and in
practical assessment approaches. This paper studies to what extent the views
match empirically. We find that most observations on quality phrased
spontaneously are in fact adequately represented by theory. Even more, relative
comparisons of arguments in practice correlate with absolute quality ratings
based on theory. Our results clarify how the two views can learn from each
other.},
 address = {Vancouver, Canada},
 author = {Wachsmuth, Henning  and  Naderi, Nona  and  Habernal, Ivan  and  Hou, Yufang  and  Hirst, Graeme  and  Gurevych, Iryna  and  Stein, Benno},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {250--255},
 publisher = {Association for Computational Linguistics},
 title = {Argumentation Quality Assessment: Theory vs. Practice},
 year = {2017}
}

@inproceedings{P17-2040,
 abstract = {We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.},
 address = {Vancouver, Canada},
 author = {R\"{o}nnqvist, Samuel  and  Schenk, Niko  and  Chiarcos, Christian},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {256--262},
 publisher = {Association for Computational Linguistics},
 title = {A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations},
 year = {2017}
}

@inproceedings{P17-2041,
 abstract = {The availability of the Rhetorical Structure Theory (RST) Discourse Treebank
has spurred substantial research into discourse analysis of written texts;
however, limited research has been conducted to date on RST annotation and
parsing of spoken language, in particular, non-native spontaneous speech.
Considering that the measurement of discourse coherence is typically a key
metric in human scoring rubrics for assessments of spoken language, we
initiated a research effort to obtain RST annotations of a large number of
non-native spoken responses from a standardized assessment of academic English
proficiency. The resulting inter-annotator kappa agreements on the three
different levels of Span, Nuclearity, and Relation are 0.848, 0.766, and 0.653, respectively. Furthermore, a set of features was explored to evaluate the
discourse structure of non-native spontaneous speech based on these
annotations; the highest performing feature resulted in a correlation of 0.612
with scores of discourse coherence provided by expert human raters.},
 address = {Vancouver, Canada},
 author = {Wang, Xinhao  and  Bruno, James  and  Molloy, Hillary  and  Evanini, Keelan  and  Zechner, Klaus},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {263--268},
 publisher = {Association for Computational Linguistics},
 title = {Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework},
 year = {2017}
}

@inproceedings{P17-2042,
 abstract = {We introduce a simple and effective method to learn discourse-specific word
embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit
discourse data, and capable of capturing discourse relationships between words.
On the PDTB data set, using DSWE as features achieves significant improvements
over baselines.},
 address = {Vancouver, Canada},
 author = {Wu, Changxing  and  Shi, Xiaodong  and  Chen, Yidong  and  Su, Jinsong  and  Wang, Boli},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {269--274},
 publisher = {Association for Computational Linguistics},
 title = {Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings},
 year = {2017}
}

@inproceedings{P17-2043,
 abstract = {This paper derives an Integer Linear Programming (ILP) formulation to obtain an
oracle summary of the compressive summarization paradigm in terms of ROUGE. The
oracle summary is essential to reveal the upper bound performance of the
paradigm. Experimental results on the DUC dataset showed that ROUGE scores of
compressive oracles are significantly higher than those of extractive oracles
and state-of-the-art summarization systems. These results reveal that
compressive summarization is a promising  paradigm and encourage us to continue
with the research to produce informative summaries.},
 address = {Vancouver, Canada},
 author = {Hirao, Tsutomu  and  Nishino, Masaaki  and  Nagata, Masaaki},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {275--280},
 publisher = {Association for Computational Linguistics},
 title = {Oracle Summaries of Compressive Summarization},
 year = {2017}
}

@inproceedings{P17-2044,
 abstract = {In English, high-quality sentence compression models by deleting words have
been trained on automatically created large training datasets. We work on
Japanese sentence compression by a similar approach. To create a large Japanese
training dataset, a method of creating English training dataset is modified
based on the characteristics of the Japanese language. The created dataset is
used to train Japanese sentence compression models based on the recurrent
neural network.},
 address = {Vancouver, Canada},
 author = {Hasegawa, Shun  and  Kikuchi, Yuta  and  Takamura, Hiroya  and  Okumura, Manabu},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {281--286},
 publisher = {Association for Computational Linguistics},
 title = {Japanese Sentence Compression with a Large Training Dataset},
 year = {2017}
}

@inproceedings{P17-2045,
 abstract = {We propose a model to automatically describe changes introduced in the source
code of a program using natural language. Our method receives as input a set of
code commits, which contains both the modifications and  message introduced by
an user. These two modalities are used to train  an encoder-decoder
architecture. We evaluated our approach on twelve real world open source
projects from four different programming languages. Quantitative and
qualitative results showed that the proposed approach can generate feasible and
semantically sound descriptions not only in standard in-project settings, but
also in a cross-project setting.},
 address = {Vancouver, Canada},
 author = {Loyola, Pablo  and  Marrese-Taylor, Edison  and  Matsuo, Yutaka},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {287--292},
 publisher = {Association for Computational Linguistics},
 title = {A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes},
 year = {2017}
}

@inproceedings{P17-2046,
 abstract = {We propose novel radical features from automatic translation for event
extraction.
Event detection is a complex language processing task for which it is expensive
to collect training data, making generalisation challenging.
We derive meaningful subword features from automatic translations into target
language.
Results suggest this method is particularly useful when using languages with
writing systems that facilitate easy decomposition into subword features, e.g., logograms and Cangjie.
The best result combines logogram features from Chinese and Japanese with
syllable features from Korean, providing an additional 3.0 points f-score when
added to state-of-the-art generalisation features on the TAC KBP 2015 Event
Nugget task.},
 address = {Vancouver, Canada},
 author = {Wei, Sam  and  Korostil, Igor  and  Nothman, Joel  and  Hachey, Ben},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {293--298},
 publisher = {Association for Computational Linguistics},
 title = {English Event Detection With Translated Language Features},
 year = {2017}
}

@inproceedings{P17-2047,
 abstract = {A critical task for question answering is the final answer selection stage, which has to combine multiple signals available about each answer candidate.
This paper proposes EviNets: a novel neural network architecture for factoid
question answering. EviNets scores candidate answer entities by combining the
available supporting evidence, e.g., structured knowledge bases and
unstructured text documents. EviNets represents each piece of evidence with a
dense embeddings vector, scores their relevance to the question, and aggregates
the support for each candidate to predict their final scores. Each of the
components is generic and allows plugging in a variety of models for semantic
similarity scoring and information aggregation. We demonstrate the
effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies
benchmarks, and on the new Yahoo! Answers dataset introduced in this paper.
EviNets can be extended to other information types and could facilitate future
work on combining evidence signals for joint reasoning in question answering.},
 address = {Vancouver, Canada},
 author = {Savenkov, Denis  and  Agichtein, Eugene},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {299--304},
 publisher = {Association for Computational Linguistics},
 title = {EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering},
 year = {2017}
}

@inproceedings{P17-2048,
 abstract = {Existing Knowledge Base Population methods extract relations from a closed
relational schema with limited coverage leading to sparse KBs. We propose
Pocket Knowledge Base Population (PKBP), the task of dynamically constructing a
KB of entities related to a query and finding the best characterization of
relationships between entities. We describe novel Open Information Extraction
methods which leverage the PKB to find informative trigger words. We evaluate
using existing KBP shared-task data as well anew annotations collected for this
work. Our methods produce high quality KB from just text with many more
entities and relationships than existing KBP systems.},
 address = {Vancouver, Canada},
 author = {Wolfe, Travis  and  Dredze, Mark  and  Van Durme, Benjamin},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {305--310},
 publisher = {Association for Computational Linguistics},
 title = {Pocket Knowledge Base Population},
 year = {2017}
}

@inproceedings{P17-2049,
 abstract = {While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a
large body of knowledge and inference techniques. Open Information Extraction
(Open IE) provides a way to generate semi-structured knowledge for QA, but to
date such knowledge has only been used to answer simple questions with
retrieval-based methods. We overcome this limitation by presenting a method for
reasoning with Open IE knowledge, allowing more complex questions to be
handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work
effectively with multiple short facts, noise, and the relational structure of
tuples. Our model significantly outperforms a state-of-the-art structured
solver on complex questions of varying difficulty, while also removing the
reliance on manually curated knowledge.},
 address = {Vancouver, Canada},
 author = {Khot, Tushar  and  Sabharwal, Ashish  and  Clark, Peter},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {311--316},
 publisher = {Association for Computational Linguistics},
 title = {Answering Complex Questions Using Open Information Extraction},
 year = {2017}
}

@inproceedings{P17-2050,
 abstract = {We design and release BONIE, the first open numerical relation extractor, for
extracting Open IE tuples where one of the arguments is a number or a
quantity-unit phrase. BONIE uses bootstrapping to learn the specific dependency
patterns that express numerical relations in a sentence. BONIE’s novelty lies
in task-specific customizations, such as inferring implicit relations, which
are clear due to context such as units (for e.g., ‘square kilometers’
suggests
area, even if the word ‘area’ is missing in the sentence). BONIE obtains
1.5x yield and 15 point precision gain on numerical facts over a
state-of-the-art Open IE system.},
 address = {Vancouver, Canada},
 author = {Saha, Swarnadeep  and  Pal, Harinder  and  Mausam},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {317--323},
 publisher = {Association for Computational Linguistics},
 title = {Bootstrapping for Numerical Open IE},
 year = {2017}
}

@inproceedings{P17-2051,
 abstract = {We propose jointly modelling Knowledge Bases and aligned text with Feature-Rich
Networks. Our models perform Knowledge Base Completion by learning to represent
and compose diverse feature types from partially aligned and noisy resources.
We perform experiments on Freebase utilizing additional entity type information
and syntactic textual relations. Our evaluation suggests that the proposed
models can better incorporate side information than previously proposed
combinations of bilinear models with convolutional neural networks, showing
large improvements when scoring the plausibility of unobserved facts with
associated textual mentions.},
 address = {Vancouver, Canada},
 author = {Komninos, Alexandros  and  Manandhar, Suresh},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {324--329},
 publisher = {Association for Computational Linguistics},
 title = {Feature-Rich Networks for Knowledge Base Completion},
 year = {2017}
}

@inproceedings{P17-2052,
 abstract = {As entity type systems become richer and more fine-grained, we expect the
number of types assigned to a given entity to increase. However, most
fine-grained typing work has focused on datasets that exhibit a low degree of
type multiplicity. In this paper, we consider the high-multiplicity regime
inherent in data sources such as Wikipedia that have semi-open type systems. We
introduce a set-prediction approach to this problem and show that our model
outperforms unstructured baselines on a new Wikipedia-based fine-grained typing
corpus.},
 address = {Vancouver, Canada},
 author = {Rabinovich, Maxim  and  Klein, Dan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {330--334},
 publisher = {Association for Computational Linguistics},
 title = {Fine-Grained Entity Typing with High-Multiplicity Assignments},
 year = {2017}
}

@inproceedings{P17-2053,
 abstract = {Question classification is an important task with wide applications.
However, traditional techniques treat questions as general sentences, ignoring
the corresponding answer data.
In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders
which refine question representation by utilizing group information in the
answer set.
We then propose novel group sparse CNNs
which naturally learn question representation with respect
to their answers by implanting group sparse autoencoders into traditional CNNs.
The proposed model significantly outperform strong baselines on four datasets.},
 address = {Vancouver, Canada},
 author = {Ma, Mingbo  and  Huang, Liang  and  Xiang, Bing  and  Zhou, Bowen},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {335--340},
 publisher = {Association for Computational Linguistics},
 title = {Group Sparse CNNs for Question Classification with Answer Sets},
 year = {2017}
}

@inproceedings{P17-2054,
 abstract = {Keyphrase boundary classification (KBC) is the task of detecting keyphrases in
scientific articles and labelling them with respect to predefined types.
Although important in practice, this task is so far underexplored, partly due
to the lack of labelled data.
To overcome this, we explore several auxiliary tasks, including semantic
super-sense tagging and identification of multi-word expressions, and cast the
task as a multi-task learning problem with deep recurrent neural networks. Our
multi-task models perform significantly better than previous state of the art
approaches on two scientific KBC datasets, particularly for long keyphrases.},
 address = {Vancouver, Canada},
 author = {Augenstein, Isabelle  and  S{\o}gaard, Anders},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {341--346},
 publisher = {Association for Computational Linguistics},
 title = {Multi-Task Learning of Keyphrase Boundary Classification},
 year = {2017}
}

@inproceedings{P17-2055,
 abstract = {Information extraction (IE) from text has largely focused on relations between
individual entities, such as who has won which award. However, some facts are
never fully mentioned, and no IE method has perfect recall. Thus, it is
beneficial to also tap contents about the cardinalities of these relations, for
example, how many awards someone has won. We introduce this novel problem of
extracting cardinalities and discusses the specific challenges that set it
apart from standard IE. We present a distant supervision method using
conditional random fields. A preliminary evaluation results in precision
between 3% and 55%, depending on the difficulty of relations.},
 address = {Vancouver, Canada},
 author = {Mirza, Paramita  and  Razniewski, Simon  and  Darari, Fariz  and  Weikum, Gerhard},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {347--351},
 publisher = {Association for Computational Linguistics},
 title = {Cardinal Virtues: Extracting Relation Cardinalities from Text},
 year = {2017}
}

@inproceedings{P17-2056,
 abstract = {Previous models for the assessment of commitment towards a predicate in a
sentence (also known as factuality prediction) were trained and tested against
a specific annotated dataset, subsequently limiting the generality of their
results. In this work we propose an intuitive method for mapping three
previously annotated corpora onto a single factuality scale, thereby enabling
models to be tested across these corpora. In addition, we design a novel model
for factuality prediction by first extending a previous rule-based factuality
prediction system and applying it over an abstraction of dependency trees, and
then using the output of this system in a supervised classifier. We show that
this model outperforms previous methods on all three datasets. We make both the
unified factuality corpus and our new model publicly available.},
 address = {Vancouver, Canada},
 author = {Stanovsky, Gabriel  and  Eckle-Kohler, Judith  and  Puzikov, Yevgeniy  and  Dagan, Ido  and  Gurevych, Iryna},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {352--357},
 publisher = {Association for Computational Linguistics},
 title = {Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets},
 year = {2017}
}

@inproceedings{P17-2057,
 abstract = {Existing question answering methods infer answers either from a knowledge base
or from raw text.
While knowledge base (KB) methods are good at answering compositional
questions, their performance is often affected by the incompleteness of the KB.
Au contraire, web text contains millions of facts that are absent in the KB, however in an
unstructured form. Universal schema can support reasoning on the union of
both structured KBs and unstructured text by aligning them in a common embedded
space. In this paper we extend universal schema to natural language question
answering, employing Memory networks to attend to the large body of
facts in the combination of text and KB.
Our models can be trained in an end-to-end fashion on question-answer pairs.
Evaluation results on Spades fill-in-the-blank question answering dataset show
that exploiting universal schema for question answering is better than using
either a KB or text alone.
This model also outperforms the current state-of-the-art by 8.5 F1 points.},
 address = {Vancouver, Canada},
 author = {Das, Rajarshi  and  Zaheer, Manzil  and  Reddy, Siva  and  McCallum, Andrew},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {358--365},
 publisher = {Association for Computational Linguistics},
 title = {Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks},
 year = {2017}
}

@inproceedings{P17-2058,
 abstract = {We demonstrate that a continuous relaxation of the argmax operation can be used
to create a differentiable approximation to greedy decoding in
sequence-to-sequence (seq2seq) models. By incorporating this approximation into
the scheduled sampling training procedure--a well-known technique for
correcting exposure bias--we introduce a new training objective that is
continuous and differentiable everywhere and can provide informative gradients
near points where previous decoding decisions change their value. By using a
related approximation, we also demonstrate  a similar approach to sampled-based
training. We show that our approach outperforms both standard cross-entropy
training and scheduled sampling procedures in two sequence prediction tasks:
named entity recognition and machine translation.},
 address = {Vancouver, Canada},
 author = {Goyal, Kartik  and  Dyer, Chris  and  Berg-Kirkpatrick, Taylor},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {366--371},
 publisher = {Association for Computational Linguistics},
 title = {Differentiable Scheduled Sampling for Credit Assignment},
 year = {2017}
}

@inproceedings{P17-2059,
 abstract = {While natural languages are compositional, how state-of-the-art neural models
achieve compositionality is still unclear. We propose a deep network, which
not only achieves competitive accuracy for text classification, but also
exhibits compositional behavior. That is, while creating hierarchical
representations of a piece of text, such as a sentence, the lower layers of the
network distribute their layer-specific attention weights to individual words.
In contrast, the higher layers compose meaningful phrases and clauses, whose
lengths increase as the networks get deeper until fully composing the
sentence.},
 address = {Vancouver, Canada},
 author = {Guo, Hongyu},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {372--377},
 publisher = {Association for Computational Linguistics},
 title = {A Deep Network with Visual Text Composition Behavior},
 year = {2017}
}

@inproceedings{P17-2060,
 abstract = {Neural machine translation (NMT) becomes a new approach to machine translation
and generates much more fluent results compared to statistical machine
translation (SMT). However, SMT is usually better than NMT in translation
adequacy. It is therefore a promising direction to combine the advantages of
both NMT and SMT. In this paper, we propose a neural system combination
framework leveraging multi-source NMT, which takes as input the outputs of NMT
and SMT systems and produces the final translation. Extensive experiments on
the Chinese-to-English translation task show that our model archives
significant improvement by 5.3 BLEU points over the best single system output
and 3.4 BLEU points over the state-of-the-art traditional system combination
methods.},
 address = {Vancouver, Canada},
 author = {Zhou, Long  and  Hu, Wenpeng  and  Zhang, Jiajun  and  Zong, Chengqing},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {378--384},
 publisher = {Association for Computational Linguistics},
 title = {Neural System Combination for Machine Translation},
 year = {2017}
}

@inproceedings{P17-2061,
 abstract = {In this paper, we propose a novel domain adaptation method named "mixed fine
tuning'' for neural machine translation (NMT). We combine two existing
approaches namely fine tuning and multi domain NMT. We first train an NMT model
on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus
which is a mix of the in-domain and out-of-domain corpora. All corpora are
augmented with artificial tags to indicate specific domains. We empirically
compare our proposed method against fine tuning and multi domain methods and
discuss its benefits and shortcomings.},
 address = {Vancouver, Canada},
 author = {Chu, Chenhui  and  Dabre, Raj  and  Kurohashi, Sadao},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {385--391},
 publisher = {Association for Computational Linguistics},
 title = {An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation},
 year = {2017}
}

@inproceedings{P17-2062,
 abstract = {We propose a new method for extracting pseudo-parallel sentences from a pair of
large monolingual corpora, without relying on any document-level information.
Our method first exploits word embeddings in order to efficiently evaluate
trillions of candidate sentence pairs and then a classifier to find the most
reliable ones. We report significant improvements in domain adaptation for
statistical machine translation when using a translation model trained on the
sentence pairs extracted from in-domain monolingual corpora.},
 address = {Vancouver, Canada},
 author = {Marie, Benjamin  and  Fujita, Atsushi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {392--398},
 publisher = {Association for Computational Linguistics},
 title = {Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings},
 year = {2017}
}

@inproceedings{P17-2063,
 abstract = {We evaluate feature hashing for language identification (LID), a method not
previously used for this task. Using a standard dataset, we first show that
while feature performance is high, LID data is highly dimensional and mostly
sparse (>99.5%) as it includes large vocabularies for many languages; memory
requirements grow as languages are added. Next we apply hashing using various
hash sizes, demonstrating that there is no performance loss with dimensionality
reductions of up to 86%. We also show that using an ensemble of low-dimension
hash-based classifiers further boosts performance. Feature hashing is highly
useful for LID and holds great promise for future work in this area.},
 address = {Vancouver, Canada},
 author = {Malmasi, Shervin  and  Dras, Mark},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {399--403},
 publisher = {Association for Computational Linguistics},
 title = {Feature Hashing for Language and Dialect Identification},
 year = {2017}
}

@inproceedings{P17-2064,
 abstract = {Selecting appropriate words to compose a sentence is one common problem faced
by non-native Chinese learners. In this paper, we propose (bidirectional) LSTM
sequence labeling models and explore various features to detect word usage
errors in Chinese sentences. By combining CWINDOW word embedding features and
POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and
MRR 0.6789 on the HSK dataset. For 80.79% of the test data, the model ranks the
ground-truth within the top two at position level.},
 address = {Vancouver, Canada},
 author = {Shiue, Yow-Ting  and  Huang, Hen-Hsen  and  Chen, Hsin-Hsi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {404--410},
 publisher = {Association for Computational Linguistics},
 title = {Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM},
 year = {2017}
}

@inproceedings{P17-2065,
 abstract = {Compositor attribution, the clustering of pages in a historical printed
document by the individual who set the type, is a bibliographic task that
relies on analysis of orthographic variation and inspection of visual details
of the printed page. In this paper, we introduce a novel unsupervised model
that jointly describes the textual and visual features needed to distinguish
compositors. Applied to images of Shakespeare's First Folio, our model predicts
attributions that agree with the manual judgements of bibliographers with an
accuracy of 87%, even on text that is the output of OCR.},
 address = {Vancouver, Canada},
 author = {Ryskina, Maria  and  Alpert-Abrams, Hannah  and  Garrette, Dan  and  Berg-Kirkpatrick, Taylor},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {411--416},
 publisher = {Association for Computational Linguistics},
 title = {Automatic Compositor Attribution in the First Folio of Shakespeare},
 year = {2017}
}

@inproceedings{P17-2066,
 abstract = {In recent years, automatic generation of image descriptions (captions), that
is, image captioning, has attracted a great deal of attention.
In this paper, we particularly consider generating Japanese captions for
images.
Since most available caption datasets have been constructed for English
language, there are few datasets for Japanese.
To tackle this problem, we construct a large-scale Japanese image caption
dataset based on images from MS-COCO, which is called STAIR Captions.
STAIR Captions consists of 820,310 Japanese captions for 164,062 images.
In the experiment, we show that a neural network trained using STAIR Captions
can generate more natural and better Japanese captions, compared to those
generated using English-Japanese machine translation after generating English
captions.},
 address = {Vancouver, Canada},
 author = {Yoshikawa, Yuya  and  Shigeto, Yutaro  and  Takeuchi, Akikazu},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {417--421},
 publisher = {Association for Computational Linguistics},
 title = {STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset},
 year = {2017}
}

@inproceedings{P17-2067,
 abstract = {Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by
the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.},
 address = {Vancouver, Canada},
 author = {Wang, William Yang},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {422--426},
 publisher = {Association for Computational Linguistics},
 title = {"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection},
 year = {2017}
}

@inproceedings{P17-2068,
 abstract = {Because syntactic structures and spans of multiword expressions (MWEs) are
independently annotated in many English syntactic corpora, they are generally
inconsistent with respect to one another, which is harmful to the
implementation of an aggregate system. In this work, we construct a corpus that
ensures consistency between dependency structures and MWEs, including named
entities. Further, we explore models that predict both MWE-spans and an
MWE-aware dependency structure. Experimental results show that our joint model
using additional MWE-span features achieves an MWE recognition improvement of
1.35 points over a pipeline model.},
 address = {Vancouver, Canada},
 author = {Kato, Akihiko  and  Shindo, Hiroyuki  and  Matsumoto, Yuji},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {427--432},
 publisher = {Association for Computational Linguistics},
 title = {English Multiword Expression-aware Dependency Parsing Including Named Entities},
 year = {2017}
}

@inproceedings{P17-2069,
 abstract = {Count-based distributional semantic models suffer from sparsity due to
unobserved but plausible co-occurrences in any text collection. This problem is
amplified for models like Anchored Packed Trees (APTs), that take the
grammatical type of a co-occurrence into account. We therefore introduce a
novel form of distributional inference that exploits the rich type structure in
APTs and infers missing data by the same mechanism that is used for semantic
composition.},
 address = {Vancouver, Canada},
 author = {Kober, Thomas  and  Weeds, Julie  and  Reffin, Jeremy  and  Weir, David},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {433--440},
 publisher = {Association for Computational Linguistics},
 title = {Improving Semantic Composition with Offset Inference},
 year = {2017}
}

@inproceedings{P17-2070,
 abstract = {Distributed word representations are widely used for modeling words in NLP
tasks. Most of the existing models generate one representation per word and do
not consider different meanings of a word.
We present two approaches to learn multiple topic-sensitive representations per
word by using Hierarchical Dirichlet Process. We observe that by modeling
topics and integrating topic distributions for each document  we obtain
representations that are able to distinguish between different meanings of a
given word.
Our models yield statistically significant improvements for the lexical
substitution task
indicating that commonly used single word representations, even when combined
with contextual information, are insufficient for this task.},
 address = {Vancouver, Canada},
 author = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {441--447},
 publisher = {Association for Computational Linguistics},
 title = {Learning Topic-Sensitive Word Representations},
 year = {2017}
}

@inproceedings{P17-2071,
 abstract = {This paper introduces the concept of temporal word analogies: pairs of words
which occupy the same semantic space at different points in time. One
well-known property of word embeddings is that they are able to effectively
model traditional word analogies (“word w1 is to word w2 as word w3 is to
word w4”) through vector addition. Here, I show that temporal word analogies
(“word w1 at time tα is like word w2 at time tβ”) can effectively be
modeled with diachronic word embeddings, provided that the independent
embedding spaces from each time period are appropriately transformed into a
common vector space. When applied to a diachronic corpus of news articles, this
method is able to identify temporal word analogies such as “Ronald Reagan in
1987 is like Bill Clinton in 1997”, or “Walkman in 1987 is like iPod in
2007”.},
 address = {Vancouver, Canada},
 author = {Szymanski, Terrence},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {448--453},
 publisher = {Association for Computational Linguistics},
 title = {Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings},
 year = {2017}
}

@inproceedings{P17-2072,
 abstract = {Many unsupervised learning techniques have been proposed to obtain meaningful
representations of words from text. In this study, we evaluate these various
techniques when used to generate Arabic word embeddings. We first build a
benchmark for the Arabic language that can be utilized to perform intrinsic
evaluation of different word embeddings. We then perform additional extrinsic
evaluations of the embeddings based on two NLP tasks.},
 address = {Vancouver, Canada},
 author = {Elrazzaz, Mohammed  and  Elbassuoni, Shady  and  Shaban, Khaled  and  Helwe, Chadi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {454--458},
 publisher = {Association for Computational Linguistics},
 title = {Methodical Evaluation of Arabic Word Embeddings},
 year = {2017}
}

@inproceedings{P17-2073,
 abstract = {People around the globe respond to major real world events through social
media.              To study targeted public sentiments across many languages and
geographic locations, we introduce multilingual connotation frames: an
extension from English connotation frames of Rashkin et al. (2016) with 10
additional European languages, focusing on the implied sentiments among event
participants engaged in a frame. As a case study, we present large scale
analysis on targeted public sentiments toward salient events and entities using
1.2 million multilingual connotation frames extracted from Twitter.},
 address = {Vancouver, Canada},
 author = {Rashkin, Hannah  and  Bell, Eric  and  Choi, Yejin  and  Volkova, Svitlana},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {459--464},
 publisher = {Association for Computational Linguistics},
 title = {Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast},
 year = {2017}
}

@inproceedings{P17-2074,
 abstract = {Rating scales are a widely used method for data annotation; however, they
present several challenges, such as difficulty in maintaining inter- and
intra-annotator consistency. Best--worst scaling (BWS) is an alternative
method of annotation that is claimed to produce high-quality annotations while
keeping the required number of annotations similar to that of rating scales.
However, the veracity of this claim has never been systematically established.
Here for the first time, we set up an experiment that directly compares the
rating scale method with BWS. We show that with the same total number of
annotations, BWS produces significantly more reliable results than the rating
scale.},
 address = {Vancouver, Canada},
 author = {Kiritchenko, Svetlana  and  Mohammad, Saif},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {465--470},
 publisher = {Association for Computational Linguistics},
 title = {Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation},
 year = {2017}
}

@inproceedings{P17-2075,
 abstract = {In social media, demographic inference is a critical task in order to gain a
better understanding of a cohort and to facilitate interacting with one's
audience. Most previous work has made independence assumptions over
topological, textual and label information on social networks. In this work, we
employ recursive neural networks to break down these independence assumptions
to obtain inference about demographic characteristics on Twitter. We show that
our model performs better than existing models including the state-of-the-art.},
 address = {Vancouver, Canada},
 author = {Kim, Sunghwan Mac  and  Xu, Qiongkai  and  Qu, Lizhen  and  Wan, Stephen  and  Paris, Cecile},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {471--477},
 publisher = {Association for Computational Linguistics},
 title = {Demographic Inference on Twitter using Recursive Neural Networks},
 year = {2017}
}

@inproceedings{P17-2076,
 abstract = {Twitter should be an ideal place to get a fresh read on how different issues
are playing with the public, one that's potentially more reflective of
democracy in this new media age than traditional polls. Pollsters typically ask
people a fixed set of questions, while in social media people use their own
voices to speak about whatever is on their minds. However, the demographic
distribution of users on Twitter is not representative of the general
population. In this paper, we present a demographic classifier for gender, age, political orientation and location on Twitter. We collected and curated a
robust Twitter demographic dataset for this task. Our classifier uses a deep
multi-modal multi-task learning architecture to reach a state-of-the-art
performance, achieving an F1-score of 0.89, 0.82, 0.86, and 0.68 for gender, age, political orientation, and location respectively.},
 address = {Vancouver, Canada},
 author = {Vijayaraghavan, Prashanth  and  Vosoughi, Soroush  and  Roy, Deb},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {478--483},
 publisher = {Association for Computational Linguistics},
 title = {Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning},
 year = {2017}
}

@inproceedings{P17-2077,
 abstract = {This paper focuses on the task of noisy label aggregation in social media, where users with different social or culture backgrounds may annotate invalid
or malicious tags for documents. To aggregate noisy labels at a small cost, a
network framework is proposed by calculating the matching degree of a
document's topics and the annotators' meta-data. Unlike using the
back-propagation algorithm, a probabilistic inference approach is adopted to
estimate network parameters. Finally, a new simulation method is designed for
validating the effectiveness of the proposed framework in aggregating noisy
labels.},
 address = {Vancouver, Canada},
 author = {Zhan, Xueying  and  Wang, Yaowei  and  Rao, Yanghui  and  Xie, Haoran  and  Li, Qing  and  Wang, Fu Lee  and  Wong, Tak-Lam},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {484--490},
 publisher = {Association for Computational Linguistics},
 title = {A Network Framework for Noisy Label Aggregation in Social Media},
 year = {2017}
}

@inproceedings{P17-2078,
 abstract = {This work explores different approaches of using normalization for parser
adaptation.  Traditionally, normalization is used as separate pre-processing
step. We show that integrating the normalization model into the
parsing algorithm is more beneficial. This way, multiple normalization
candidates can be leveraged, which improves parsing performance on social
media.
We test this hypothesis by modifying the Berkeley parser; out-of-the-box it
achieves an F1 score of 66.52.                          Our integrated approach
reaches a
significant
improvement with an F1 score of 67.36, while using the best normalization
sequence results in an F1 score of only 66.94.},
 address = {Vancouver, Canada},
 author = {van der Goot, Rob  and  van Noord, Gertjan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {491--497},
 publisher = {Association for Computational Linguistics},
 title = {Parser Adaptation for Social Media by Integrating Normalization},
 year = {2017}
}

@inproceedings{P17-2079,
 abstract = {We propose AliMe Chat, an open-domain chatbot engine that integrates the joint
results of Information Retrieval (IR) and Sequence to Sequence (Seq2Seq) based
generation models. AliMe Chat uses an attentive Seq2Seq based rerank model to
optimize the joint results. Extensive experiments show our engine outperforms
both IR and generation based models. We launch AliMe Chat for a real-world
industrial application and observe better results than another public chatbot.},
 address = {Vancouver, Canada},
 author = {Qiu, Minghui  and  Li, Feng-Lin  and  Wang, Siyu  and  Gao, Xing  and  Chen, Yan  and  Zhao, Weipeng  and  Chen, Haiqing  and  Huang, Jun  and  Chu, Wei},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {498--503},
 publisher = {Association for Computational Linguistics},
 title = {AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine},
 year = {2017}
}

@inproceedings{P17-2080,
 abstract = {Deep latent variable models have been shown to facilitate the response
generation for open-domain dialog systems. However, these latent variables are
highly randomized, leading to uncontrollable generated responses. In this
paper, we propose a framework allowing conditional response generation based on
specific attributes. These attributes can be either manually assigned or
automatically detected. Moreover, the dialog states for both speakers are
modeled separately in order to reflect personal features. We validate this
framework on two different scenarios, where the attribute refers to genericness
and sentiment states respectively. The experiment result testified the
potential of our model, where meaningful responses can be generated in
accordance with the specified attributes.},
 address = {Vancouver, Canada},
 author = {Shen, Xiaoyu  and  Su, Hui  and  Li, Yanran  and  Li, Wenjie  and  Niu, Shuzi  and  Zhao, Yang  and  Aizawa, Akiko  and  Long, Guoping},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {504--509},
 publisher = {Association for Computational Linguistics},
 title = {A Conditional Variational Framework for Dialog Generation},
 year = {2017}
}

@inproceedings{P17-2081,
 abstract = {We show that the task of question answering (QA) can significantly benefit from
the transfer learning of models trained on a different large, fine-grained QA
dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique
from SQuAD. For WikiQA, our model outperforms the previous best model by more
than 8%. We demonstrate that finer supervision provides better guidance for
learning lexical and syntactic information than coarser supervision, through
quantitative results and visual analysis. We also show that a similar transfer
learning procedure  achieves  the state of the art on an entailment task.},
 address = {Vancouver, Canada},
 author = {Min, Sewon  and  Seo, Minjoon  and  Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {510--517},
 publisher = {Association for Computational Linguistics},
 title = {Question Answering through Transfer Learning from Large Fine-grained Supervision Data},
 year = {2017}
}

@inproceedings{P17-2082,
 abstract = {In this paper we introduce a self-training strategy for crowdsourcing.
The training examples are automatically selected to train the crowd workers.
Our experimental results show an impact of 5% Improvement in terms of F1 for
relation extraction task, compared to the method based on distant supervision.},
 address = {Vancouver, Canada},
 author = {Abad, Azad  and  Nabi, Moin  and  Moschitti, Alessandro},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {518--523},
 publisher = {Association for Computational Linguistics},
 title = {Self-Crowdsourcing Training for Relation Extraction},
 year = {2017}
}

@inproceedings{P17-2083,
 abstract = {We propose a novel generative neural network architecture for Dialogue Act
classification. Building upon the Recurrent Neural Network framework, our model
incorporates a novel attentional technique and a label to label connection for
sequence learning, akin to Hidden Markov Models. The experiments show that both
of these innovations lead  our model to outperform strong baselines for
dialogue act classification on MapTask and Switchboard corpora. We further
empirically analyse the effectiveness of each of the new innovations.},
 address = {Vancouver, Canada},
 author = {Tran, Quan Hung  and  Haffari, Gholamreza  and  Zukerman, Ingrid},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {524--529},
 publisher = {Association for Computational Linguistics},
 title = {A Generative Attentional Neural Network Model for Dialogue Act Classification},
 year = {2017}
}

@inproceedings{P17-2084,
 abstract = {Topical PageRank (TPR) uses latent topic distribution inferred by Latent
Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from
documents. The ranking procedure consists of running PageRank K times, where K
is the number of topics used in the LDA model. In this paper, we propose a
modification of TPR, called Salience Rank. Salience Rank only needs to run
PageRank once and extracts comparable or better keyphrases on benchmark
datasets. In addition to quality and efficiency benefit, our method has the
flexibility to extract keyphrases with varying tradeoffs between topic
specificity and corpus specificity.},
 address = {Vancouver, Canada},
 author = {Teneva, Nedelina  and  Cheng, Weiwei},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {530--535},
 publisher = {Association for Computational Linguistics},
 title = {Salience Rank: Efficient Keyphrase Extraction with Topic Modeling},
 year = {2017}
}

@inproceedings{P17-2085,
 abstract = {Traditional Entity Linking (EL) technologies rely on rich structures and
properties in the target knowledge base (KB). However, in many applications, the KB may be as simple and sparse as lists of names of the same type (e.g., lists of products). We call it as List-only Entity Linking problem.
Fortunately, some mentions may have more cues for linking, which can be used as
seed mentions to bridge other mentions and the uninformative entities.
In this work, we select most linkable mentions as seed mentions and
disambiguate other mentions by comparing them with the seed mentions rather
than directly with the entities.
Our experiments on linking mentions to seven automatically mined lists show
promising results and demonstrate the effectiveness of our approach.},
 address = {Vancouver, Canada},
 author = {Lin, Ying  and  Lin, Chin-Yew  and  Ji, Heng},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {536--541},
 publisher = {Association for Computational Linguistics},
 title = {List-only Entity Linking},
 year = {2017}
}

@inproceedings{P17-2086,
 abstract = {In this paper, we explore spelling errors as a source of information for
detecting the native language of a writer, a previously under-explored area.
We note that character n-grams from misspelled words are very indicative of
the native language of the author. In combination with other lexical features, spelling error features lead to 1.2% improvement in accuracy on classifying
texts in the TOEFL11 corpus by the author's native language, compared to
systems participating in the NLI shared task.},
 address = {Vancouver, Canada},
 author = {Chen, Lingzhen  and  Strapparava, Carlo  and  Nastase, Vivi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {542--546},
 publisher = {Association for Computational Linguistics},
 title = {Improving Native Language Identification by Using Spelling Errors},
 year = {2017}
}

@inproceedings{P17-2087,
 abstract = {This paper presents a model for disfluency detection in spontaneous speech
transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel
Model (NCM) to generate n-best candidate disfluency analyses and a Long
Short-Term Memory (LSTM) language model to score the underlying fluent
sentences of each analysis. The LSTM language model scores, along with other
features, are used in a MaxEnt reranker to identify the most plausible
analysis. We show that using an LSTM language model in the reranking process of
noisy channel disfluency model improves the state-of-the-art in disfluency
detection.},
 address = {Vancouver, Canada},
 author = {Jamshid Lou, Paria  and  Johnson, Mark},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {547--553},
 publisher = {Association for Computational Linguistics},
 title = {Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model},
 year = {2017}
}

@inproceedings{P17-2088,
 abstract = {We show the equivalence of two state-of-the-art models for
link prediction/knowledge graph completion:
Nickel et al's holographic embeddings and Trouillon et al.'s complex
embeddings.
We first consider a spectral version of the holographic embeddings, exploiting the frequency domain in the Fourier transform for efficient
computation.
The analysis of the resulting model reveals that it can be viewed as
an instance of the complex embeddings
with a certain constraint imposed on the initial vectors upon training.
Conversely, any set of complex embeddings can be converted to a set of
equivalent holographic embeddings.},
 address = {Vancouver, Canada},
 author = {Hayashi, Katsuhiko  and  Shimbo, Masashi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {554--559},
 publisher = {Association for Computational Linguistics},
 title = {On the Equivalence of Holographic and Complex Embeddings for Link Prediction},
 year = {2017}
}

@inproceedings{P17-2089,
 abstract = {Although new corpora are becoming increasingly available for machine
translation, only those that belong to the same or similar domains are
typically able to improve translation performance. Recently Neural Machine
Translation (NMT) has become prominent in the field. However, most of the
existing domain adaptation methods only focus on phrase-based machine
translation. In this paper, we exploit the NMT's internal embedding of the
source sentence and use the sentence embedding similarity to select the
sentences which are close to in-domain data. The empirical adaptation results
on the IWSLT English-French and NIST Chinese-English tasks show that the
proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU
points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU
points.},
 address = {Vancouver, Canada},
 author = {Wang, Rui  and  Finch, Andrew  and  Utiyama, Masao  and  Sumita, Eiichiro},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {560--566},
 publisher = {Association for Computational Linguistics},
 title = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
 year = {2017}
}

@inproceedings{P17-2090,
 abstract = {The quality of a Neural Machine Translation system depends substantially on the
availability of sizable parallel corpora.
For low-resource language pairs this is not the case, resulting in poor
translation quality.
Inspired by work in computer vision, we propose a novel data augmentation
approach that targets low-frequency words by generating new sentence pairs
containing rare words in new, synthetically created contexts.
Experimental results on simulated low-resource settings show that our method
improves translation quality by up to 2.9 BLEU points over the baseline and up
to 3.2 BLEU over back-translation.},
 address = {Vancouver, Canada},
 author = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {567--573},
 publisher = {Association for Computational Linguistics},
 title = {Data Augmentation for Low-Resource Neural Machine Translation},
 year = {2017}
}

@inproceedings{P17-2091,
 abstract = {We speed up Neural Machine Translation (NMT) decoding by shrinking run-time
target vocabulary. We experiment with two shrinking approaches: Locality
Sensitive Hashing (LSH) and word alignments. Using the latter method, we get a
2x overall speed-up over a highly-optimized GPU implementation, without hurting
BLEU. On certain low-resource language pairs, the same methods improve BLEU by
0.5 points. We also report a negative result for LSH on GPUs, due to relatively
large overhead, though it was successful on CPUs. Compared with Locality
Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs.},
 address = {Vancouver, Canada},
 author = {Shi, Xing  and  Knight, Kevin},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {574--579},
 publisher = {Association for Computational Linguistics},
 title = {Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary},
 year = {2017}
}

@inproceedings{P17-2092,
 abstract = {In typical neural machine translation~(NMT), the decoder generates a sentence
word by word, packing all linguistic granularities in the same time-scale of
RNN. In this paper, we propose a new type of decoder for NMT, which splits the
decode state into two parts and updates them in two different time-scales.
Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated.
In this way, the target sentence is translated hierarchically from chunks to
words, with information in different granularities being leveraged.
Experiments show that our proposed model significantly improves the translation
performance over the state-of-the-art NMT model.},
 address = {Vancouver, Canada},
 author = {Zhou, Hao  and  Tu, Zhaopeng  and  Huang, Shujian  and  Liu, Xiaohua  and  Li, Hang  and  Chen, Jiajun},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {580--586},
 publisher = {Association for Computational Linguistics},
 title = {Chunk-Based Bi-Scale Decoder for Neural Machine Translation},
 year = {2017}
}

@inproceedings{P17-2093,
 abstract = {Cross-lingual model transfer is a compelling and popular method for predicting
annotations in a low-resource language, whereby parallel corpora provide a
bridge to a high-resource language, and its associated annotated corpora.
However, parallel data is not readily available for many languages, limiting
the applicability of these approaches. We address these drawbacks in our
framework which takes advantage of cross-lingual word embeddings trained solely
on a high coverage dictionary. We propose a novel neural network model for
joint training from both sources of data based on cross-lingual word
embeddings, and show substantial empirical improvements over baseline
techniques. We also propose several active learning heuristics, which result in
improvements over competitive benchmark methods.},
 address = {Vancouver, Canada},
 author = {Fang, Meng  and  Cohn, Trevor},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {587--593},
 publisher = {Association for Computational Linguistics},
 title = {Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary},
 year = {2017}
}

@inproceedings{P17-2094,
 abstract = {Parallel corpora are widely used in a variety of Natural Language Processing
tasks, from Machine Translation to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate
high-quality sense annotations on a large scale. In this paper we present
EuroSense, a multilingual sense-annotated resource based on the joint
disambiguation of the Europarl parallel corpus, with almost 123 million sense
annotations for over 155 thousand distinct concepts and entities from a
language-independent unified sense inventory. We evaluate the quality of our
sense annotations intrinsically and extrinsically, showing their effectiveness
as training data for Word Sense Disambiguation.},
 address = {Vancouver, Canada},
 author = {Delli Bovi, Claudio  and  Camacho-Collados, Jose  and  Raganato, Alessandro  and  Navigli, Roberto},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {594--600},
 publisher = {Association for Computational Linguistics},
 title = {EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text},
 year = {2017}
}

@inproceedings{P17-2095,
 abstract = {Word segmentation plays a pivotal role in improving any Arabic NLP application.
Therefore, a lot of research has been spent in improving its accuracy.
Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect
dependent. We explore three language-independent alternatives to morphological
segmentation us- ing: i) data-driven sub-word units, ii) characters as a unit
of learning, and iii) word embeddings learned using a character CNN
(Convolution Neural Network). On the tasks of Machine Translation and POS
tagging, we found these methods to achieve close to, and occasionally surpass
state-of-the-art performance. In our analysis, we show that a neural machine
translation system is sensitive to the ratio of source and target tokens, and a
ratio close to 1 or greater, gives optimal performance.},
 address = {Vancouver, Canada},
 author = {Sajjad, Hassan  and  Dalvi, Fahim  and  Durrani, Nadir  and  Abdelali, Ahmed  and  Belinkov, Yonatan  and  Vogel, Stephan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {601--607},
 publisher = {Association for Computational Linguistics},
 title = {Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging},
 year = {2017}
}

@inproceedings{P17-2096,
 abstract = {Neural models with minimal feature engineering have achieved competitive
performance against traditional methods for the task of Chinese word
segmentation. However, both training and working procedures of the current
neural models are computationally inefficient. In this paper, we propose a
greedy neural word segmenter with balanced word and character embedding inputs
to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable
of performing segmentation much faster and even more accurate than
state-of-the-art neural models on Chinese benchmark datasets.},
 address = {Vancouver, Canada},
 author = {Cai, Deng  and  Zhao, Hai  and  Zhang, Zhisong  and  Xin, Yuan  and  Wu, Yongjian  and  Huang, Feiyue},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {608--615},
 publisher = {Association for Computational Linguistics},
 title = {Fast and Accurate Neural Word Segmentation for Chinese},
 year = {2017}
}

@inproceedings{P17-2097,
 abstract = {We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present
several findings. We develop a model that uses hierarchical recurrent networks
with attention to encode the sentences in the story and score candidate
endings. By discarding the large training set and only training on the
validation set, we achieve an accuracy of 74.7%. Even when we discard the story
plots (sentences before the ending) and only train to choose the better of two
endings, we can still reach 72.5%. We then analyze this “ending-only” task
setting. We estimate human accuracy to be 78% and find several types of clues
that lead to this high accuracy, including those related to sentiment, negation, and general ending likelihood regardless of the story context.},
 address = {Vancouver, Canada},
 author = {Cai, Zheng  and  Tu, Lifu  and  Gimpel, Kevin},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {616--622},
 publisher = {Association for Computational Linguistics},
 title = {Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task},
 year = {2017}
}

@inproceedings{P17-2098,
 abstract = {A fundamental challenge in developing semantic parsers is the paucity of strong
supervision in the form of language utterances annotated with logical form. In
this paper, we propose to exploit structural regularities in language in
different domains, and train semantic parsers over multiple knowledge-bases
(KBs), while sharing information across datasets. We find that we can
substantially improve parsing accuracy by training a single
sequence-to-sequence model over multiple KBs, when providing an encoding of the
domain at decoding time. Our model achieves state-of-the-art performance on the
Overnight dataset (containing eight domains), improves performance over a
single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the
number of model parameters.},
 address = {Vancouver, Canada},
 author = {Herzig, Jonathan  and  Berant, Jonathan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {623--628},
 publisher = {Association for Computational Linguistics},
 title = {Neural Semantic Parsing over Multiple Knowledge-bases},
 year = {2017}
}

@inproceedings{P17-2099,
 abstract = {Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent
semantics is beneficial to multiple downstream applications. We observe a
simple geometry of sentences -- the word representations of a given sentence
(on average 10.23 words in all SemEval datasets with a standard deviation 4.84)
roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this
observation, we represent a sentence by the low-rank subspace spanned by its
word vectors. Such an unsupervised representation is empirically validated via
semantic textual similarity tasks on 19 different datasets, where it
outperforms the sophisticated neural network models,  including skip-thought
vectors, by 15% on average.},
 address = {Vancouver, Canada},
 author = {Mu, Jiaqi  and  Bhat, Suma  and  Viswanath, Pramod},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {629--634},
 publisher = {Association for Computational Linguistics},
 title = {Representing Sentences as Low-Rank Subspaces},
 year = {2017}
}

@inproceedings{P17-2100,
 abstract = {Current Chinese social media text summarization models are based on an
encoder-decoder framework. Although its generated summaries are similar to
source texts literally, they have low semantic relevance. In this work, our
goal is to improve semantic relevance between source texts and summaries for
Chinese social media summarization. We introduce a Semantic Relevance Based
neural model to encourage high semantic similarity between texts and summaries.
In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the
similarity score between the representations is maximized during training. Our
experiments show that the proposed model outperforms baseline systems on a
social media corpus.},
 address = {Vancouver, Canada},
 author = {Ma, Shuming  and  Sun, Xu  and  Xu, Jingjing  and  Wang, Houfeng  and  Li, Wenjie  and  Su, Qi},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {635--640},
 publisher = {Association for Computational Linguistics},
 title = {Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization},
 year = {2017}
}

@inproceedings{P17-2101,
 abstract = {This paper describes an approach to determine whether people participate in the
events they tweet about. Specifically, we determine whether people are
participants in events with respect to the tweet timestamp. We target all
events expressed by verbs in tweets, including past, present and events that
may occur in the future. We present new annotations using 1,096 event mentions, and experimental results showing that the task is challenging.},
 address = {Vancouver, Canada},
 author = {Sanagavarapu, Krishna Chaitanya  and  Vempala, Alakananda  and  Blanco, Eduardo},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {641--646},
 publisher = {Association for Computational Linguistics},
 title = {Determining Whether and When People Participate in the Events They Tweet About},
 year = {2017}
}

@inproceedings{P17-2102,
 abstract = {Pew research polls report 62 percent of U.S. adults get news on social media
(Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults
said that “made-up news” has caused a “great deal of confusion” about
the facts of current events (Barthel et al., 2016). Fabricated stories in
social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global
stability.
In this work we build predictive models to classify 130 thousand news posts as
suspicious or verified, and predict four sub-types of suspicious news --
satire, hoaxes, clickbait and propaganda. We show that neural network models
trained on tweet content and social network interactions outperform lexical
models. Unlike previous work on deception detection, we find that adding syntax
and grammar features to our models does not improve performance. Incorporating
linguistic features improves classification results, however, social
interaction features are most in- formative for finer-grained separation be-
tween four types of suspicious news posts.},
 address = {Vancouver, Canada},
 author = {Volkova, Svitlana  and  Shaffer, Kyle  and  Jang, Jin Yea  and  Hodas, Nathan},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {647--653},
 publisher = {Association for Computational Linguistics},
 title = {Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter},
 year = {2017}
}

@inproceedings{P17-2103,
 abstract = {Counterfactual statements, describing events that did not occur and their
consequents, have been studied in areas including problem-solving, affect
management, and behavior regulation. People with more counterfactual thinking
tend to perceive life events as more personally meaningful. Nevertheless, counterfactuals have not been studied in computational linguistics. We create a
counterfactual tweet dataset and explore approaches for detecting
counterfactuals using rule-based and supervised statistical approaches. A
combined rule-based and statistical approach yielded the best results (F1 =
0.77) outperforming either approach used alone.},
 address = {Vancouver, Canada},
 author = {Son, Youngseo  and  Buffone, Anneke  and  Raso, Joe  and  Larche, Allegra  and  Janocko, Anthony  and  Zembroski, Kevin  and  Schwartz, H. Andrew  and  Ungar, Lyle},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {654--658},
 publisher = {Association for Computational Linguistics},
 title = {Recognizing Counterfactual Thinking in Social Media Texts},
 year = {2017}
}

@inproceedings{P17-2104,
 abstract = {Automatically estimating a user's socio-economic profile from their language
use in social media can significantly help social science research and various
downstream applications ranging from business to politics. The current paper
presents the first study where user cognitive structure is used to build a
predictive model of income. In particular, we first develop a classifier using
a weakly supervised learning framework to automatically time-tag tweets as
past, present, or future. We quantify a user's overall temporal orientation
based on their distribution of tweets, and use it to build a predictive model
of income. Our analysis uncovers a correlation between future temporal
orientation and income. Finally, we measure the predictive power of future
temporal orientation on income by performing regression.},
 address = {Vancouver, Canada},
 author = {Hasanuzzaman, Mohammed  and  Kamila, Sabyasachi  and  Kaur, Mandeep  and  Saha, Sriparna  and  Ekbal, Asif},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {659--665},
 publisher = {Association for Computational Linguistics},
 title = {Temporal Orientation of Tweets for Predicting Income of Users},
 year = {2017}
}

@inproceedings{P17-2105,
 abstract = {We develop a language-independent, deep learning-based approach to the task of
morphological disambiguation.
Guided by the intuition that the correct analysis should be ``most similar'' to
the context, we propose dense representations for morphological analyses and
surface context and a simple yet effective way of combining the two to perform
disambiguation.
Our approach improves on the language-dependent state of the art for two
agglutinative languages (Turkish and Kazakh) and can be potentially applied to
other morphologically complex languages.},
 address = {Vancouver, Canada},
 author = {Toleu, Alymzhan  and  Tolegen, Gulmira  and  Makazhanov, Aibek},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {666--671},
 publisher = {Association for Computational Linguistics},
 title = {Character-Aware Neural Morphological Disambiguation},
 year = {2017}
}

@inproceedings{P17-2106,
 abstract = {We present a transition-based dependency parser that uses a convolutional
neural network to compose word representations from characters. The character
composition model shows great improvement over the word-lookup model, especially
for parsing agglutinative languages. These improvements are even better than
using pre-trained word
embeddings from extra data. On the SPMRL data sets, our system outperforms the
previous best greedy parser (Ballesteros et. al, 2015) by a margin of 3% on
average.},
 address = {Vancouver, Canada},
 author = {Yu, Xiang  and  Vu, Ngoc Thang},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {672--678},
 publisher = {Association for Computational Linguistics},
 title = {Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages},
 year = {2017}
}

@inproceedings{P17-2107,
 abstract = {In dependency parsing, jackknifing taggers is indiscriminately used as a simple
adaptation strategy. Here, we empirically evaluate when and how (not) to use
jackknifing in parsing. On 26 languages, we reveal a preference that conflicts
with, and surpasses the ubiquitous ten-folding. We show no clear benefits of
tagging the training data in cross-lingual parsing.},
 address = {Vancouver, Canada},
 author = {Agi\'{c}, \v{Z}eljko  and  Schluter, Natalie},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 month = {July},
 pages = {679--684},
 publisher = {Association for Computational Linguistics},
 title = {How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers},
 year = {2017}
}

