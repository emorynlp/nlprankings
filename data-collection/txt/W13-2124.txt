










































GenNext: A Consolidated Domain Adaptable NLG System


Proceedings of the 14th European Workshop on Natural Language Generation, pages 178–182,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

GenNext: A Consolidated Domain Adaptable NLG System

Frank Schilder, Blake Howald and Ravi Kondadadi∗
Thomson Reuters, Research & Development

610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com

Abstract

We introduce GenNext, an NLG system
designed specifically to adapt quickly and
easily to different domains. Given a do-
main corpus of historical texts, GenNext
allows the user to generate a template bank
organized by semantic concept via derived
discourse representation structures in con-
junction with general and domain-specific
entity tags. Based on various features
collected from the training corpus, the
system statistically learns template rep-
resentations and document structure and
produces well–formed texts (as evaluated
by crowdsourced and expert evaluations).
In addition to domain adaptation, Gen-
Next’s hybrid approach significantly re-
duces complexity as compared to tradi-
tional NLG systems by relying on tem-
plates (consolidating micro-planning and
surface realization) and minimizing the
need for domain experts. In this descrip-
tion, we provide details of GenNext’s the-
oretical perspective, architecture and eval-
uations of output.

1 Introduction

NLG systems are typically tailored to very spe-
cific domains and tasks such as text summaries
from neonatal intensive care units (SUMTIME-
NEONATE (Portet et al., 2007)) or offshore oil
rig weather reports (SUMTIME-METEO (Reiter et
al., 2005)) and require significant investments in
development resources (e.g. people, time, etc.).
For example, for SUMTIME-METEO, 12 person
months were required for two of the system com-
ponents alone (Belz, 2007). Given the subject
matter of such systems, the investment is perfectly

∗Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.

reasonable. However, if the domains to be gener-
ated are comparatively more general, such as fi-
nancial reports or biographies, then the scaling of
development costs becomes a concern in NLG.

NLG in the editorial process for companies and
institutions where content can vary must be do-
main adaptable. Spending a year or more of devel-
opment time to produce high quality market sum-
maries, for example, is not a viable solution if it is
necessary to start from scratch to produce other re-
ports. GenNext, a hybrid system that statistically
learns document and sentence template represen-
tations from existing historical data, is developed
to be consolidated and domain adaptable. In par-
ticular, GenNext reduces complexity by avoiding
the necessity of having a separate document plan-
ner, surface realizer, etc., and extensive expert in-
volvement at the outset of system development.

Section 2 describes the theoretical background,
architecture and implementation of GenNext. Sec-
tion 3 discusses the results of a non–expert and ex-
pert crowdsourced sentence preference evaluation
task. Section 4 concludes with several future ex-
periments for system improvement.

2 Architecture of GenNext

In general, NLG systems follow a prototypical ar-
chitecture where some input data from a given do-
main is sent to a “document planner” which de-
cides content and structuring to create a document
plan. That document plan serves as an input to
a “micro planner” where the content is converted
into a syntactic expression (with associated con-
siderations of aggregation and referring expres-
sion generation) and a text specification is created.
The text specification then goes through the final
stage of “surface realization” where everything is
put together into an output text (McKeown, 1985;
Reiter and Dale, 2000; Bateman and Zock, 2003).

In contrast, the architecture of GenNext (sum-
marized in Figure 1) is driven by a domain-specific

178



Figure 1: GenNext System Architecture.

corpus text. There is often a structured database
underlying the domains of corpus text, the fields
of which are used for domain specific entity tag-
ging (in addition to domain general entity tagging
[e.g. DATE, LOCATION, etc.]). An overview of
the different stages, which are a combination of
statistical (e.g., Langkilde and Knight (1998)) and
template–based (e.g., van Deemter, et al. (2005))
approaches, follows in (A-E).1

A: Semantic Representation - We take a do-
main specific training corpus and reduce each
sentence to a Discourse Representation Structure
(DRS) - formal semantic representations of sen-
tences (and texts) from Discourse Representation
Theory (Kamp and Reyle, 1993; Basile and Bos,
2011). Each DRS is a combination of domain gen-
eral named entities, predicates (content words) and
relational elements (function words). In parallel,
domain specific named entity tags are identified
and are used to create templates that syntactically
represent some conceptual meaning; for example,
the short biography in (1):
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing

Director of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor’s in finance from USC and a MBA

from UCLA.

Conceptual Meaning
c. SERVING | MANAGING | DIRECTOR | PERSON | ...
d. HOLDS | BACHELOR | FINANCE | MBA | HOLD | ...

Once the semantic representations are created,
they are organized and identified by semantic con-
cept (“CuId”) (described in (B)). Our assumption
is that each cluster equates with a CuId repre-
sented by each individual sentence in the cluster
and is contrastive with other CuIds (for similar ap-

1For more detail see Howald, et al. (2013) - semantic
clustering and micro-planning and Kondadadi, et al. (2013) -
document planning.

proaches, see Barzilay and Lapata (2005), Angeli,
et al. (2010) and Lu and Ng (2011)).

B: Creating Conceptual Units - To create the
CuIds (a semi-automatic process), we cluster the
sentences using k-means clustering with k set ar-
bitrarily high to over-generate (Witten and Frank,
2005). This facilitates manual verification of the
generated clusters to merge (rather than split) them
if necessary. We assign a unique CuId to each
cluster and associate each template in the corpus to
a corresponding CuId. For example, in (2), using
the sentences in (1a-b), the identified named en-
tities are assigned to a clustered CuId (2a-b) and
then each sentence in the training corpus is re-
duced to a template (2c-d).
(2) Content Mapping
a. {CuId : 000} – Information: person: Mr. Mitsutaka

Kambe; title: Managing Director; company: 77 Bank,
Ltd.; date: June 27, 2008

b. {CuId : 001} – Information: person: he; degree:
Bachelor’s, MBA; subject: finance; institution: USC;
UCLA

Templates
c. {CuId : 000}: [person] has been serving as [title] of the

[company] since [date].
d. {CuId : 001}: [person] holds a [degree] in [subject]

from [institution] and a [degree] from [institution].

At this stage, we will have a set of CuIds with cor-
responding template collections which represent
the entire “micro-planning” aspect of our system.

C: Collecting Statistics - For the “document plan-
ning” stage, we collect a number of statistics for
each domain, for example:

• Frequency distribution of CuIds by position
• Frequency distribution of templates by position
• Frequency distribution of entity sequence
• Average number of entities by CuId and position

These statistics, in addition to entity tags and tem-
plates, are used in building different features used
by the ranking model (D).

D: Building a Ranking Model - The core compo-
nent of our system is a statistical model that ranks
a set of templates for a given position (e.g. sen-
tence 1, sentence 2, ..., sentence n) based on the
input data (see also Konstas and Lapata (2012).
The learning task is to find the rank for all the tem-
plates from all CuIds at each position. To gener-
ate the training data, we first exclude the templates
that have named entities not specified in the input
data (ensuring completeness). We then rank tem-
plates according to the edit distance (Levenshtein,

179



1966) from the template corresponding to the cur-
rent sentence in the training document. For each
template, we build a ranking model with features,
for example:

• Prior template and CuId
• Difference in number of words given position
• Most likely CuId given position and previous CuId
• Template 1-3grams given position and CuId

We use a linear kernel for a ranking SVM
(Joachims, 2002) to learn the weights associated
with each feature. Each domain has its own model
that is used when generating texts (E).

E: Generation: At generation time, our system
has a set of input data, a semantically organized
template bank and a model from training on a
given domain of texts. For each sentence, we first
exclude those templates that contain a named en-
tity not present in the input data. Then we cal-
culate the feature values times the model weight
for each of the remaining templates. The tem-
plate with the highest score is selected, filled
with matching entities from the input data and ap-
pended to the generated text. Example generations
for each domain are included in (3).
(3) Financial
a. First quarter profit per share for Brown-Forman

Corporation expected to be $0.91 per share by analysts.
b. Brown-Forman Corporation July first quarter profits will

be below that previously estimated by Wall Street with
a range between $0.89 and $0.93 per share and a projected
mean per share of $0.91 per share.

c. The consensus recommendation is Hold.

Biography
d. Mr. Satomi Mitsuzaki has been serving as Managing

Director of Mizuho Bank since June 27, 2008.
e. He was previously Director of Regional Compliance of

Kyoto Branch.
f. He is a former Managing Executive Officer and Chief

Executive Officer of new Industrial Finance Business
Group in Mitsubishi Corporation.

Weather
g. Complex low from southern Norway will drift slowly NNE

to the Lofoten Islands by early tomorrow.
h. A ridge will persist to the west of British Isles for Saturday

with a series of weak fronts moving east across
the North Sea.

i. A front will move ENE across the northern North Sea

Saturday.

3 Evaluation and Discussion

We have tested GenNext on three domains: Corpo-
rate Officer and Director Biographies (1150 texts
ranging from 3-10 period ended sentences), Fi-
nancial Texts (Mutual Fund Performances [162
texts, 2-4 sentences] and Broker Recommenda-
tions [905 texts, 8-20 sentences]), and Offshore

Oil Rig Weather Reports (1054 texts, 2-6 sen-
tences) from SUMTIME-METEO (Reiter et al.,
2005). The total number of templates for the finan-
cial domain is 1379 distributed across 38 different
semantic concepts; 2836 templates across 19 con-
cepts for biography; and 2749 templates across 9
concepts for weather texts.

We have conducted several evaluation experi-
ments comparing two versions of GenNext, one
applying the ranking model (rank) and one with
random selection of templates (non-rank) (both
systems use the same template bank, CuId as-
signment and filtering) and the original texts from
which the data was extracted (original).

We used a combination of automatic (e.g.
BLEU–4 (Papineni et al., 2002), METEOR
(Denkowski and Lavie, 2011)) and human metrics
(using crowdsourcing) to evaluate the output (see
generally, Belz and Reiter (2006). However, in the
interest of space, we will restrict the discussion to
a human judgment task on output preferences. We
found this evaluation task to be most informative
for system improvement. The task asks an evalu-
ator to provide a binary preference determination
(100 sentence pairs/domain): “Do you prefer Sen-
tence A (from original) or the corresponding Sen-
tence B (from rank or non-rank)”. This task was
performed for each domain.2 We also engaged 3
experts from the financial and 4 from the biogra-
phy domains to perform the same preference task
(average agreement was 76.22) as well as provide
targeted feedback.

For the preference results, summarized in Fig-
ure 2, we would like to see no statistically signifi-
cant difference between GenNext-rank and orig-
inal, but statistically significant differences be-
tween GenNext-rank and GenNext-non-rank, and
original and GenNext-non-rank. If this is the case,
then GenNext-rank is producing texts similar to
the original texts, and is providing an observ-
able improvement over not including the model at
all (GenNext-non-rank). This is exactly what we
see for all domains.3 However, in general, there

2Over 100 native English speakers contributed, each one
restricted to providing no more than 50 responses and only
after they successfully answered 4 initial gold data questions
correctly and continued to answer periodic gold data ques-
tions. The pair orderings were randomized to prevent click
bias. 8 judgments per sentence pair was collected (2400 judg-
ments) and average agreement was 75.87.

3Original vs. GenNext-rank : financial - χ2=.29, p≤.59;
biography - χ2=3.01, p≤.047; weather - χ2=.95, p≤.32.
Original vs. GenNext-non-rank : financial - χ2=16.71,
p≤.0001; biography - χ2=45.43, p≤.0001; weather -

180



Figure 2: Cross-Domain Non-Expert Preference Evaluations.

is a greater difference between the original and
GenNext-rank biographies compared to the finan-
cial and weather texts. We take it as a goal to ap-
proach, as close as possible, the preferences for
the original texts.

The original financial documents were machine
generated from a different existing system. As
such, it is not surprising to see similarity in perfor-
mance compared to GenNext-rank and potentially
explains why preferences for the originals is some-
what low (assuming a higher preference rating for
well-formed human texts). Further, the original
weather documents are highly technical and not
easily understood by the lay person, so, again, it is
not surprising to see similar performance. Biogra-
phies were human generated and easy to under-
stand for the average reader. Here, both GenNext-
rank and GenNext-non-rank have some ground to
make up. Insights from domain experts are poten-
tially helpful in this regard.

Expert evaluations provided similar results and
agreements compared to the non–expert crowd.
Most beneficial about the expert evaluations was
the discussion of integrating certain editorial stan-
dards into the system. For example, shorter texts
were preferred to longer texts in the financial do-
main, but not the biographies. Consequently, we
could adjust weights to favor shorter templates.
Also, in biographies, sentences with subordinated
elaborations were not preferred because these con-
tained subjective comments (e.g. a leader in in-
dustry, a well respected individual, etc.). Here,

χ2=24.27, p≤.0001. GenNext-rank vs. GenNext-non-rank
: financial - χ2=12.81, p≤.0003; biography - χ2=25.19,
p≤.0001; weather - χ2=16.19, p≤.0001.

we could manually curate or could automatically
detect templates with subordinated clauses and re-
move them. These types of comments are useful
to adjust the system accordingly to end user ex-
pectations.

4 Conclusion and Future Work

We have presented our system GenNext which is
domain adaptable, given adequate historical data,
and has a significantly reduced complexity com-
pared to other NLG systems (see generally, Robin
and McKeown (1996)). To the latter point, devel-
opment time for semantically processing the cor-
pus, applying domain general and specific tags,
and building a model is accomplished in days and
weeks as opposed to months and years.

Future experimentation will focus on being able
to automatically extract templates for different do-
mains to create preset banks of templates in the
absence of adequate historical data. We are also
looking into different ways to increase the vari-
ability of output texts from selecting templates
within a range of top scores (rather than just the
highest score) to providing additional generated
information from input data analytics.

Acknowledgments

This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragement, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.

181



References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A

simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502–512.

Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331–338.

Valerio Basile and Johan Bos. 2011. Towards generat-
ing text from discourse representation structures. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation (ENLG), pages 145–150.

John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284–304. Ox-
ford University Press, Oxford.

Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL’06), pages 313–320.

Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT’07), pages
164–171.

Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85–91.

Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in sta-
tistical NLG. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143–154. Association for Com-
putational Linguistics, March.

Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.

Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.

Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical NLG framework for aggregated
planning and realization. In Proceedings of the An-
nual Conference for the Association of Computa-
tional Linguistics (ACL 2013). Association for Com-
putational Linguistics.

Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369–
378.

Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’98),
pages 704–710.

Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707–710.

Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611–1622.

Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.

Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL’02), pages 311–318.

Franois Portet, Ehud Reiter, Jim Hunter, and Somaya-
julu Sripada. 2007. Automatic generation of tex-
tual summaries from neonatal intensive care data. In
In Proccedings of the 11th Conference on Artificial
Intelligence in Medicine (AIME 07). LNCS, pages
227–236.

Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.

Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.

Jacques Robin and Kathy McKeown. 1996. Empiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).

Kees van Deemter, Mariët Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15–24.

Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.

182


