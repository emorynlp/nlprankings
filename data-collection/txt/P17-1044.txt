



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 473–483
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1044

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 473–483
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1044

Deep Semantic Role Labeling: What Works and What’s Next
Luheng He†, Kenton Lee†, Mike Lewis‡, and Luke Zettlemoyer†∗

† Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA
{luheng, kentonl, lsz}@cs.washington.edu

‡ Facebook AI Research, Menlo Park, CA
mikelewis0@fb.com

∗Allen Institute for Artificial Intelligence, Seattle, WA
lukez@allenai.org

Abstract

We introduce a new deep learning model
for semantic role labeling (SRL) that sig-
nificantly improves the state of the art,
along with detailed analyses to reveal its
strengths and limitations. We use a deep
highway BiLSTM architecture with con-
strained decoding, while observing a num-
ber of recent best practices for initializa-
tion and regularization. Our 8-layer en-
semble model achieves 83.2 F1 on the
CoNLL 2005 test set and 83.4 F1 on
CoNLL 2012, roughly a 10% relative er-
ror reduction over the previous state of the
art. Extensive empirical analysis of these
gains show that (1) deep models excel at
recovering long-distance dependencies but
can still make surprisingly obvious errors,
and (2) that there is still room for syntactic
parsers to improve these results.

1 Introduction

Semantic role labeling (SRL) systems aim to re-
cover the predicate-argument structure of a sen-
tence, to determine essentially “who did what to
whom”, “when”, and “where.” Recently break-
throughs involving end-to-end deep models for
SRL without syntactic input (Zhou and Xu, 2015;
Marcheggiani et al., 2017) seem to overturn the
long-held belief that syntactic parsing is a pre-
requisite for this task (Punyakanok et al., 2008).
In this paper, we show that this result can be
pushed further using deep highway bidirectional
LSTMs with constrained decoding, again signifi-
cantly moving the state of the art (another 2 points
on CoNLL 2005). We also present a careful em-
pirical analysis to determine what works well and
what might be done to progress even further.

Our model combines a number of best prac-
tices in the recent deep learning literature. Fol-

lowing Zhou and Xu (2015), we treat SRL as a
BIO tagging problem and use deep bidirectional
LSTMs. However, we differ by (1) simplifying
the input and output layers, (2) introducing high-
way connections (Srivastava et al., 2015; Zhang
et al., 2016), (3) using recurrent dropout (Gal
and Ghahramani, 2016), (4) decoding with BIO-
constraints, and (5) ensembling with a product of
experts. Our model gives a 10% relative error re-
duction over previous state of the art on the test
sets of CoNLL 2005 and 2012. We also report per-
formance with predicted predicates to encourage
future exploration of end-to-end SRL systems.

We present detailed error analyses to better un-
derstand the performance gains, including (1) de-
sign choices on architecture, initialization, and
regularization that have a surprisingly large im-
pact on model performance; (2) different types of
prediction errors showing, e.g., that deep models
excel at predicting long-distance dependencies but
still struggle with known challenges such as PP-
attachment errors and adjunct-argument distinc-
tions; (3) the role of syntax, showing that there
is significant room for improvement given oracle
syntax but errors from existing automatic parsers
prevent effective use in SRL.

In summary, our main contributions incluede:

• A new state-of-the-art deep network for end-
to-end SRL, supported by publicly available
code and models.1

• An in-depth error analysis indicating where
the model works well and where it still strug-
gles, including discussion of structural con-
sistency and long-distance dependencies.

• Experiments that point toward directions for
future improvements, including a detailed
discussion of how and when syntactic parsers
could be used to improve these results.

1https://github.com/luheng/deep_srl

473

https://doi.org/10.18653/v1/P17-1044
https://doi.org/10.18653/v1/P17-1044


2 Model

Two major factors contribute to the success of our
deep SRL model: (1) applying recent advances
in training deep recurrent neural networks such as
highway connections (Srivastava et al., 2015) and
RNN-dropouts (Gal and Ghahramani, 2016),2 and
(2) using an A∗ decoding algorithm (Lewis and
Steedman, 2014; Lee et al., 2016) to enforce struc-
tural consistency at prediction time without adding
more complexity to the training process.

Formally, our task is to predict a sequence y
given a sentence-predicate pair (w, v) as input.
Each yi ∈ y belongs to a discrete set of BIO tags
T . Words outside argument spans have the tag O,
and words at the beginning and inside of argument
spans with role r have the tags Br and Ir respec-
tively. Let n = |w| = |y| be the length of the
sequence.

Predicting an SRL structure under our model
involves finding the highest-scoring tag sequence
over the space of all possibilities Y:

ŷ = argmax
y∈Y

f(w,y) (1)

We use a deep bidirectional LSTM (BiLSTM) to
learn a locally decomposed scoring function con-
ditioned on the input:

∑n
t=1 log p(yt | w).

To incorporate additional information (e.g.,
structural consistency, syntactic input), we aug-
ment the scoring function with penalization terms:

f(w,y) =
n∑

t=1

log p(yt | w)−
∑

c∈C
c(w, y1:t) (2)

Each constraint function c applies a non-negative
penalty given the input w and a length-t prefix
y1:t. These constraints can be hard or soft depend-
ing on whether the penalties are finite.

2.1 Deep BiLSTM Model
Our model computes the distribution over tags us-
ing stacked BiLSTMs, which we define as follows:

il,t = σ(W
l
i [hl,t+δl ,xl,t] + b

l
i) (3)

ol,t = σ(W
l
o[hl,t+δl ,xl,t] + b

l
o) (4)

fl,t = σ(W
l
f[hl,t+δl ,xl,t] + b

l
f + 1) (5)

c̃l,t = tanh(W
l
c[hl,t+δl ,xl,t] + b

l
c) (6)

cl,t = il,t ◦ c̃l,t + fl,t ◦ ct+δl (7)
hl,t = ol,t ◦ tanh(cl,t) (8)

2We thank Mingxuan Wang for suggesting highway con-
nections with simplified inputs and outputs. Part of our model
is extended from his unpublished implementation.

+

+

+

The 0

P (BARG0)

+

+

+

cats 0

P (IARG0)

+

+

+

love 1

P (BV)

+

+

+

hats 0

P (BARG1)

Softmax

Transform
Gates

LSTM

Word &
Predicate

Figure 1: Highway LSTM with four layers. The
curved connections represent highway connec-
tions, and the plus symbols represent transform
gates that control inter-layer information flow.

where xl,t is the input to the LSTM at layer l and
timestep t. δl is either 1 or −1, indicating the di-
rectionality of the LSTM at layer l.

To stack the LSTMs in an interleaving pattern,
as proposed by Zhou and Xu (2015), the layer-
specific inputs xl,t and directionality δl are ar-
ranged in the following manner:

xl,t =

{
[Wemb(wt),Wmask(t = v)] l = 1

hl−1,t l > 1
(9)

δl =

{
1 if l is even
−1 otherwise

(10)

The input vector x1,t is the concatenation of token
wt’s word embedding and an embedding of the bi-
nary feature (t = v) indicating whether wt word
is the given predicate.

Finally, the locally normalized distribution over
output tags is computed via a softmax layer:

p(yt | x) ∝ exp(WytaghL,t + btag) (11)

Highway Connections To alleviate the vanish-
ing gradient problem when training deep BiL-
STMs, we use gated highway connections (Zhang
et al., 2016; Srivastava et al., 2015). We include
transform gates rt to control the weight of lin-
ear and non-linear transformations between layers
(See Figure 1). The output hl,t is changed to:

rl,t = σ(W
l
r[hl,t−1,xt] + b

l
r) (12)

h′l,t = ol,t ◦ tanh(cl,t) (13)
hl,t = rl,t ◦ h′l,t + (1− rl,t) ◦Wlhxl,t (14)

474



Recurrent Dropout To reduce over-fitting, we
use dropout as described in Gal and Ghahramani
(2016). A shared dropout mask zl is applied to the
hidden state:

h̃l,t = rl,t ◦ h′l,t + (1− rl,t) ◦Wlhxl,t (15)
hl,t = zl ◦ h̃l,t (16)

zl is shared across timesteps at layer l to avoid am-
plifying the dropout noise along the sequence.

2.2 Constrained A∗ Decoding
The approach described so far does not model any
dependencies between the output tags. To incor-
porate constraints on the output structure at decod-
ing time, we use A∗ search over tag prefixes for
decoding. Starting with an empty sequence, the
tag sequence is built from left to right. The score
for a partial sequence with length t is defined as:

f(w, y1:t) =
t∑

i=1

log p(yi | w)−
∑

c∈C
c(w, y1:i)

(17)
An admissible A∗ heuristic can be computed effi-
ciently by summing over the best possible tags for
all timesteps after t:

g(w, y1:t) =
n∑

i=t+1

max
yi∈T

log p(yi | w) (18)

Exploration of the prefixes is determined by
an agenda A which is sorted by f(w, y1:t) +
g(w, y1:t). In the worst case, A∗ explores expo-
nentially many prefixes, but because the distribu-
tion p(yt | w) learned by the BiLSTM models is
very peaked, the algorithm is efficient in practice.
We list some example constraints as follows:

BIO Constraints These constraints reject any
sequence that does not produce valid BIO transi-
tions, such as BARG0 followed by IARG1.

SRL Constraints Punyakanok et al. (2008);
Täckström et al. (2015) described a list of SRL-
specific global constraints:
• Unique core roles (U): Each core role

(ARG0-ARG5, ARGA) should appear at
most once for each predicate.
• Continuation roles (C): A continuation role

C-X can exist only when its base role X is
realized before it.
• Reference roles (R): A reference role R-X

can exist only when its base role X is realized
(not necessarily before R-X).

We only enforce U and C constraints, since the R
constraints are more commonly violated in gold
data and enforcing them results in worse perfor-
mance (see discussions in Section 4.3).

Syntactic Constraints We can enforce consis-
tency with a given parse tree by rejecting or penal-
izing arguments that are not constituents. In Sec-
tion 4.4, we will discuss the motivation behind us-
ing syntactic constraints and experimental results
using both predicted and gold syntax.

2.3 Predicate Detection

While the CoNLL 2005 shared task assumes gold
predicates as input (Carreras and Màrquez, 2005),
this information is not available in many down-
stream applications. We propose a simple model
for end-to-end SRL, where the system first pre-
dicts a set of predicate words v from the input sen-
tence w. Then each predicate in v is used as an in-
put to argument prediction. We independently pre-
dict whether each word in the sentence is a predi-
cate, using a binary softmax over the outputs of a
bidirectional LSTM trained to maximize the like-
lihood of the gold labels.

3 Experiments

3.1 Datasets

We measure the performance of our SRL sys-
tem on two PropBank-style, span-based SRL
datasets: CoNLL-2005 (Carreras and Màrquez,
2005) and CoNLL-2012 (Pradhan et al., 2013)3.
Both datasets provide gold predicates (their in-
dex in the sentence) as part of the input. There-
fore, each provided predicate corresponds to one
training/test tag sequence. We follow the train-
development-test split for both datasets and use
the official evaluation script from the CoNLL 2005
shared task for evaluation on both datasets.

3.2 Model Setup

Our network consists of 8 BiLSTM layers (4 for-
ward LSTMs and 4 reversed LSTMs) with 300-
dimensional hidden units, and a softmax layer for
predicting the output distribution.

Initialization All the weight matrices in BiL-
STMs are initialized with random orthonormal
matrices as described in Saxe et al. (2013).

3We used the version of OntoNotes downloaded at:
http://cemantix.org/data/ontonotes.html.

475



Development WSJ Test Brown Test Combined

Method P R F1 Comp. P R F1 Comp. P R F1 Comp. F1

Ours (PoE) 83.1 82.4 82.7 64.1 85.0 84.3 84.6 66.5 74.9 72.4 73.6 46.5 83.2
Ours 81.6 81.6 81.6 62.3 83.1 83.0 83.1 64.3 72.9 71.4 72.1 44.8 81.6

Zhou 79.7 79.4 79.6 - 82.9 82.8 82.8 - 70.7 68.2 69.4 - 81.1
FitzGerald (Struct.,PoE) 81.2 76.7 78.9 55.1 82.5 78.2 80.3 57.3 74.5 70.0 72.2 41.3 -
Täckström (Struct.) 81.2 76.2 78.6 54.4 82.3 77.6 79.9 56.0 74.3 68.6 71.3 39.8 -
Toutanova (Ensemble) - - 78.6 58.7 81.9 78.8 80.3 60.1 - - 68.8 40.8 -
Punyakanok (Ensemble) 80.1 74.8 77.4 50.7 82.3 76.8 79.4 53.8 73.4 62.9 67.8 32.3 77.9

Table 1: Experimental results on CoNLL 2005, in terms of precision (P), recall (R), F1 and percentage of
completely correct predicates (Comp.). We report results of our best single and ensemble (PoE) model.
The comparison models are Zhou and Xu (2015), FitzGerald et al. (2015), Täckström et al. (2015),
Toutanova et al. (2008) and Punyakanok et al. (2008).

Development Test

Method P R F1 Comp. P R F1 Comp.

Ours (PoE) 83.5 83.2 83.4 67.5 83.5 83.3 83.4 68.5
Ours 81.8 81.4 81.5 64.6 81.7 81.6 81.7 66.0

Zhou - - 81.1 - - - 81.3 -
FitzGerald (Struct.,PoE) 81.0 78.5 79.7 60.9 81.2 79.0 80.1 62.6
Täckström (Struct.) 80.5 77.8 79.1 60.1 80.6 78.2 79.4 61.8
Pradhan (revised) - - - - 78.5 76.6 77.5 55.8

Table 2: Experimental results on CoNLL 2012 in the same metrics as above. We compare our best
single and ensemble (PoE) models against Zhou and Xu (2015), FitzGerald et al. (2015), Täckström
et al. (2015) and Pradhan et al. (2013).

All tokens are lower-cased and initialized with
100-dimensional GloVe embeddings pre-trained
on 6B tokens (Pennington et al., 2014) and up-
dated during training. Tokens that are not covered
by GloVe are replaced with a randomly initialized
UNK embedding.

Training We use Adadelta (Zeiler, 2012) with
� = 1e−6 and ρ = 0.95 and mini-batches of size
80. We set RNN-dropout probability to 0.1 and
clip gradients with norm larger than 1. All the
models are trained for 500 epochs with early stop-
ping based on development results. 4

Ensembling We use a product of experts (Hin-
ton, 2002) to combine predictions of 5 mod-
els, each trained on 80% of the training corpus
and validated on the remaining 20%. For the
CoNLL 2012 corpus, we split the training data
from each sub-genre into 5 folds, such that the
training data will have similar genre distributions.

Constrained Decoding We experimented with
different types of constraints on the CoNLL 2005

4Training the full model on CoNLL 2005 takes about 5
days on a single Titan X Pascal GPU.

and CoNLL 2012 development sets. Only the BIO
hard constraints significantly improve over the en-
semble model. Therefore, in our final results, we
only use BIO hard constraints during decoding. 5

3.3 Results

In Table 1 and 2, we compare our best single
and ensemble model with previous work. Our en-
semble (PoE) has an absolute improvement of 2.1
F1 on both CoNLL 2005 and CoNLL 2012 over
the previous state of the art. Our single model
also achieves more than a 0.4 improvement on
both datasets. In comparison with the best re-
ported results, our percentage of completely cor-
rect predicates improves by 5.9 points. While the
continuing trend of improving SRL without syn-
tax seems to suggest that neural end-to-end sys-
tems no longer needs parsers, our analysis in Sec-
tion 4.4 will show that accurate syntactic informa-
tion can improve these deep models.

5A∗ search in this setting finds the optimal sequence for
all sentences and is therefore equivalent to Viterbi decoding.

476



Predicate Detection End-to-end SRL (Single) End-to-end SRL (PoE)

Dataset P R F1 P R F1 P R F1 ∆ F1

CoNLL 2005 Dev. 97.4 97.4 97.4 80.3 80.4 80.3 81.8 81.2 81.5 -1.2
WSJ Test 94.5 98.5 96.4 80.2 82.3 81.2 82.0 83.4 82.7 -1.9
Brown Test 89.3 95.7 92.4 67.6 69.6 68.5 69.7 70.5 70.1 -3.5

CoNLL 2012 Dev. 88.7 90.6 89.7 74.9 76.2 75.5 76.5 77.8 77.2 -6.2
CoNLL 2012 Test 93.7 87.9 90.7 78.6 75.1 76.8 80.2 76.6 78.4 -5.0

Table 3: Predicate detection performance and end-to-end SRL results using predicted predicates. ∆ F1
shows the absolute performance drop compared to our best ensemble model with gold predicates.

100 200 300 400 500

65

70

75

80

Num. epochs

D
ev

.F
1

%

Our model
No highway connections
No dropout
No orthogonal initialization

Figure 2: Smoothed learning curve of various
ablations. The combination of highway layers,
orthonormal parameter initialization and recur-
rent dropout is crucial to achieving strong perfor-
mance. The numbers shown here are without con-
strained decoding.

3.4 Ablations

Figure 2 shows learning curves of our model ab-
lations on the CoNLL 2005 development set. We
ablate our full model by removing highway con-
nections, RNN-dropout, and orthonormal initial-
ization independently. Without dropout, the model
overfits at around 300 epochs at 78 F1. Or-
thonormal parameter initialization is surprisingly
important—without this, the model achieves only
65 F1 within the first 50 epochs. All 8 layer abla-
tions suffer a loss of more than 1.7 in absolute F1
compared to the full model.

3.5 End-to-end SRL

The network for predicate detection (Section 2.3)
contains 2 BiLSTM layers with 100-dimensional
hidden units, and is trained for 30 epochs. For
end-to-end evaluation, all arguments predicted for
the false positive predicates are counted as preci-
sion loss, and all arguments for the false negative
predicates are considered as recall loss.

Table 3 shows the predicate detection F1 as well
as end-to-end SRL results with predicted predi-

cates.6 On CoNLL 2005, the predicate detector
achieved over 96 F1, and the final SRL results only
drop 1.2-3.5 F1 compared to using the gold pred-
icates. However, on CoNLL 2012, the predicate
detector has only about 90 F1, and the final SRL
results decrease by up to 6.2 F1. This is at least
in part due to the fact that CoNLL 2012 contains
some nominal and copula predicates (Weischedel
et al., 2013), making predicate identification a
more challenging problem.

4 Analysis

To better understand our deep SRL model and its
relation to previous work, we address the follow-
ing questions with a suite of empirical analyses:
• What is the model good at and what kinds of

mistakes does it make?
• How well do LSTMs model global structural

consistency, despite conditionally indepen-
dent tagging decisions?
• Is our model implicitly learning syntax, and

could explicitly modeling syntax still help?
All the analysis in this section is done on the
CoNLL 2005 development set with gold predi-
cates, unless otherwise stated. We are also able
to compare to previous systems whose model pre-
dictions are available online (Punyakanok et al.,
2005; Pradhan et al., 2005).7

4.1 Error Types Breakdown

Inspired by Kummerfeld et al. (2012), we define a
set of oracle transformations that fix various pre-
diction errors sequentially and observe the relative
improvement after each operation (see Table 4).
Figure 3 shows how our work compares to the pre-

6The frame identification numbers reported in Pradhan
et al. (2013) are not comparable, due to errors in the original
release of the data, as mentioned in Täckström et al. (2015).

7Model predictions of CoNLL 2005 systems: http://
www.cs.upc.edu/˜srlconll/st05/st05.html

477



Orig. Fix
Labels

Move
Core
Arg.

Merge
Spans

Split
Spans

Fix
Span

Boundary

Drop
Arg.

Add
Arg.

75

80

85

90

95

100

F1
%

Ours
Pradhan

Punyakanok

Figure 3: Performance after doing each type of or-
acle transformation in sequence, compared to two
strong non-neural baselines. The gap is closed af-
ter the Add Arg. transformation, showing how our
approach is gaining from predicting more argu-
ments than traditional systems.

vious systems in terms of different types of mis-
takes. While our model makes a similar number
of labeling errors to traditional syntax-based sys-
tems, it has far fewer missing arguments (perhaps
due to parser errors making some arguments diffi-
cult to recover for syntax-based systems).

Label Confusion As shown in Table 4, our sys-
tem most commonly makes labeling errors, where
the predicted span is an argument but the role was
incorrectly labeled. Table 5 shows a confusion
matrix for the most frequent labels. The model of-
ten confuses ARG2 with AM-DIR, AM-LOC and
AM-MNR. These confusions can arise due to the
use of ARG2 in many verb frames to represent se-
mantic relations such as direction or location. For
example, ARG2 in the frame move.01 is defined as
Arg2-GOL: destination. 8 This type of argument-
adjunct distinction is known to be difficult (Kings-
bury et al., 2002), and it is not surprising that our
neural model has many such failure cases.

Attachment Mistakes A second common
source of error is reflected by the Merge Spans
transformation (10.6%) and the Split Spans trans-
formation (14.7%). These errors are closely tied
to prepositional phrase (PP) attachment errors,
which are also known to be some of the biggest
challenges for linguistic analysis (Kummerfeld
et al., 2012). Figure 4 shows the distribution of
syntactic span labels involved in an attachment
mistake, where 62% of the syntactic spans are
prepositional phrases. For example, in Sumitomo

8Source: Unified verb index: http://verbs.
colorado.edu.

Operation Description %

Fix Labels Correct the span label if its boundarymatches gold. 29.3

Move Arg. Move a unique core argument to its cor-rect position. 4.5

Merge
Spans

Combine two predicted spans into a gold
span if they are separated by at most one
word.

10.6

Split
Spans

Split a predicted span into two gold
spans that are separated by at most one
word.

14.7

Fix
Boundary

Correct the boundary of a span if its la-
bel matches an overlapping gold span. 18.0

Drop Arg. Drop a predicted argument that does notoverlap with any gold span. 7.4

Add Arg. Add a gold argument that does not over-lap with any predicted span. 11.0

Table 4: Oracle transformations paired with the
relative error reduction after each operation. All
the operations are permitted only if they do not
cause any overlapping arguments.

pred. \ gold A0 A1 A2 A3 ADV DIR LOC MNR PNC TMP
A0 - 55 11 13 4 0 0 0 0 0
A1 78 - 46 0 0 22 11 10 25 14
A2 11 23 - 48 15 56 33 41 25 0
A3 3 2 2 - 4 0 0 0 25 14

ADV 0 0 0 4 - 0 15 29 25 36
DIR 0 0 5 4 0 - 11 2 0 0

LOC 5 9 12 0 4 0 - 10 0 14
MNR 3 0 12 26 33 0 0 - 0 21
PNC 0 3 5 4 0 11 4 2 - 0
TMP 0 8 5 0 41 11 26 6 0 -

Table 5: Confusion matrix for labeling errors,
showing the percentage of predicted labels for
each gold label. We only count predicted argu-
ments that match gold span boundaries.

financed the acquisition from Sears, our model
mistakenly labels the prepositional phrase from
Sears as the ARG2 of financed, whereas it should
instead attach to acquisition.

4.2 Long-range Dependencies

To analyze the model’s ability to capture long-
range dependencies, we compute the F1 of our
model on arguments with various distances to the
predicate. Figure 5 shows that performance tends
to degrade, for all models, for arguments further
from the predicate. Interestingly, the gap between
shallow and deep models becomes much larger for
the long-distance predicate-argument structures.
The absolute gap between our 2 layer and 8 layer
models is 3-4 F1 for arguments that are within 2
words to the predicate, and 5-6 F1 for arguments
that are farther away from the predicate. Surpris-

478



PP VP NP SBAR ADVP Other

0

20

40

60

80

100

62

10 10 5 4 9

%
of

la
be

ls

Figure 4: For cases where our model either splits
a gold span into two (Z → XY ) or merges two
gold constituents (XY → Z), we show the distri-
bution of syntactic labels for the Y span. Results
show the major cause of these errors is inaccurate
prepositional phrase attachment.

0 1-2 3-6 7-max

55

60

65

70

75

80

85

Distance (num. words in between)

F1
% L8

L6
L4
L2
Punyakanok
Pradhan

Figure 5: F1 by surface distance between predi-
cates and arguments. Performance degrades least
rapidly on long-range arguments for the deeper
neural models.

ingly, the neural model performance deteriorates
less severely on long-range dependencies than tra-
ditional syntax-based models.

4.3 Structural Consistency

We can quantify two types of structural consis-
tencies: the BIO constraints and the SRL-specific
constraints. Via our ablation study, we show
that deeper BiLSTMs are better at enforcing these
structural consistencies, although not perfectly.

BIO Violations The BIO format requires argu-
ment spans to begin with a B tag. Any I tag di-
rectly following an O tag or a tag with different
label is considered a violation. Table 6 shows the
number of BIO violations per token for BiLSTMs
with different depths. The number of BIO viola-
tions decreases when we use a deeper model. The
gap is biggest between 2-layer and 4-layer models,
and diminishes after that.

It is surprising that although the deeper models
generate impressively accurate token-level predic-

Housing starts are expected to quicken a bit from August’s pace

ARG0

ARG1

ARG2ARG2

ARG1

V ARG2 ARG3

ARG2ARG0

V

V

Gold

Pred.

+SRL

Figure 6: Example where performance is hurt by
enforcing the constraint that core roles may only
occur once (+SRL).

tions, they still make enough BIO errors to signif-
icantly hurt performance—when these constraints
are simple enough to be enforced by trivial rules.
We compare the average entropy between tokens
involved in BIO violations with the averaged en-
tropy of all tokens. For the 8-layer model, the aver-
age entropy on these tokens is 30 times higher than
the averaged entropy on all tokens. This suggests
that BIO inconsistencies occur when there is some
ambiguity. For example, if the model is unsure
whether two consecutive words should belong to
an ARG0 or ARG1, it might generate inconsistent
BIO sequences such as BARG0, IARG1 when decod-
ing at the token level. Using BIO-constrained de-
coding can resolve this ambiguity and result in a
structurally consistent solution.

SRL Structure Violations The model predic-
tions can also violate the SRL-specific constraints
commonly used in prior work (Punyakanok et al.,
2008; Täckström et al., 2015). As shown in Ta-
ble 7, the model occasionally violates these SRL
constraints. With our constrained decoding algo-
rithm, it is straightforward to enforce the unique
core roles (U) and continuation roles (C) con-
straints during decoding. The constrained de-
coding results are shown with the model named
L8+PoE+SRL in Table 7.

Although the violations are eliminated, the per-
formance does not significantly improve. This is
mainly due to two factors: (1) the model often
already satisfies these constraints on its own, so
the number of violations to be fixed are relatively
small, and (2) the gold SRL structure sometimes
violates the constraints and enforcing hard con-
straints can hurt performance. Figure 6 shows
a sentence in the CoNLL 2005 development set.
Our original model produces two ARG2s for the
predicate quicken, and this violates the SRL con-
straints. When the A∗ decoder fixes this viola-
tion, it changes the first ARG1 into ARG2 because
ARG0,ARG1,ARG2 is a more frequent pattern in
the training data and has higher overall score.

479



Accuracy Violations Avg. Entropy

Model (no BIO) F1 Token BIO All BIO

L8+PoE 81.5 91.5 0.07 0.02 0.72
L8 80.5 90.9 0.07 0.02 0.73
L6 80.1 90.3 0.06 0.02 0.72
L4 79.1 90.2 0.08 0.02 0.70
L2 74.6 88.4 0.18 0.03 0.66

Table 6: Comparison of BiLSTM models without
BIO decoding. We compare F1 and token-level
accuracy (Token), averaged BIO violations per to-
ken (BIO), overall model entropy (All) model en-
tropy at tokens involved in BIO violations (BIO).
Increasing the depth of the model beyond 4 does
not produce more structurally consistent output,
emphasizing the need for constrained decoding.

4.4 Can Syntax Still Help SRL?

The Propbank-style SRL formalism is closely tied
to syntax (Bonial et al., 2010; Weischedel et al.,
2013). In Table 7, we show that 98.7% of the gold
SRL arguments match an unlabeled constituent in
the gold syntax tree. Similar to some recent work
(Zhou and Xu, 2015), our model achieves strong
performance without directly modeling syntax. A
natural question follows: are neural SRL mod-
els implicitly learning syntax? Table 7 shows the
trend of deeper models making predictions that are
more consistent with the gold syntax in terms of
span boundaries. With our best model (L8+PoE),
94.3% of the predicted arguments spans are part of
the gold parse tree. This consistency is on par with
previous CoNLL 2005 systems that directly model
constituency and use predicted parse trees as fea-
tures (Punyakanok, 95.3% and Pradhan, 93.0%).

Constrained Decoding with Syntax The above
analysis raises a further question: would improv-
ing consistency with syntax provide improvements
for SRL? Our constrained decoding algorithm de-
scribed in Section 2.2 enables us to inject syn-
tax as a decoding constraint without having to re-
train the model. Specifically, if the decoded se-
quence contains k arguments that do not match
any unlabeled syntactic constituent, it will receive
a penalty of kC, whereC is a single parameter dic-
tating how much the model should trust the pro-
vided syntax. In Figure 7, we compare the SRL
accuracy with syntactic constraints specified by
gold parse or automatic parses. When using gold
syntax, the predictions improve up to 2 F1 as the
penalty increases. A state-of-the-art parser (Choe

SRL-Violations

Model or Oracle F1 Syn % U C R

Gold 100.0 98.7 24 0 61

L8+PoE 82.7 94.3 37 3 68
L8 81.6 94.0 48 4 73
L6 81.4 93.7 39 3 85
L4 80.5 93.2 51 3 84
L2 77.2 91.3 96 5 72

L8+PoE+SRL 82.8 94.2 5 1 68
L8+PoE+AutoSyn 83.2 96.1 113 3 68
L8+PoE+GoldSyn 85.0 97.6 102 3 68

Punyakanok 77.4 95.3 0 0 0
Pradhan 78.3 93.0 84 3 58

Table 7: Comparison of models with different
depths and decoding constraints (in addition to
BIO) as well as two previous systems. We com-
pare F1, unlabeled agreement with gold con-
stituency (Syn%) and each type of SRL-constraint
violations (Unique core roles, Continuation roles
and Reference roles). Our best model produces a
similar number of constraint violations to the gold
annotation, explaining why deterministically en-
forcing these constraints is not helpful.

and Charniak, 2016) provides smaller gains, while
using the Charniak parser (Charniak, 2000) hurts
performance if the model places too much trust in
it. These results suggest that high-quality syntax
can still make a large impact on SRL.

A known challenge for syntactic parsers is ro-
bustness on out-of-domain data. Therefore we
provide experimental results in Table 8 for both
CoNLL 2005 and CoNLL 2012, which consists
of 8 different genres. The penalties are tuned on
the two development sets separately (C = 10000
on CoNLL 2005 and C = 20 on CoNLL 2012).
On the CoNLL 2005 development set, the pre-
dicted syntax gives a 0.5 F1 improvement over
our best model, while on in-domain test and out-
of-domain development sets, the improvement is
much smaller. As expected, on CoNLL 2012, syn-
tax improves most on the newswire (NW) domain.
These improvements suggest that while decoding
with hard constraints is beneficial, joint training or
multi-task learning could be even more effective
by leveraging full, labeled syntactic structures.

5 Related Work

Traditional approaches to semantic role labeling
have used syntactic parsers to identify constituents
and model long-range dependencies, and enforced

480



0 1 10 100 1000 10000 ∞

82

83

84

85

Penalty C

F1
%

Gold
Choe
Charniak

Figure 7: Performance of syntax-constrained de-
coding as the non-constituent penalty increases for
syntax from two parsers (from Choe and Charniak
(2016) and Charniak (2000)) and gold syntax. The
best existing parser gives a small improvement,
but the improvement from gold syntax shows that
there is still potential for syntax to help SRL.

CoNLL-05 CoNLL-2012 Dev.

Dev. Test BC BN NW MZ PT TC WB

L8+PoE 82.7 84.6 81.4 82.8 82.8 80.4 93.6 84.8 81.0
+AutoSyn 83.2 84.8 81.5 82.8 83.2 80.6 93.7 84.9 81.1

Table 8: F1 on CoNLL 2005, and the devel-
opment set of CoNLL 2012, broken down by
genres. Syntax-constrained decoding (+AutoSyn)
shows bigger improvement on in-domain data
(CoNLL 05 and CoNLL 2012 NW).

global consistency using integer linear program-
ming (Punyakanok et al., 2008) or dynamic pro-
grams (Täckström et al., 2015). More recently,
neural methods have been employed on top of syn-
tactic features (FitzGerald et al., 2015; Roth and
Lapata, 2016). Our experiments show that off-
the-shelf neural methods have a remarkable ability
to learn long-range dependencies, syntactic con-
stituency structure, and global constraints without
coding task-specific mechanisms for doing so.

An alternative line of work has attempted to re-
duce the dependency on syntactic input for seman-
tic role labeling models. Collobert et al. (2011)
first introduced an end-to-end neural-based ap-
proach with sequence-level training and uses a
convolutional neural network to model the con-
text window. However, their best system fell
short of traditional feature-based systems. Neu-
ral methods have also been used as classifiers in
transition-based SRL systems (Henderson et al.,
2013; Swayamdipta et al., 2016). Most re-
cently, several successful LSTM-based architec-

tures have achieved state-of-the-art results in En-
glish span-based SRL (Zhou and Xu, 2015), Chi-
nese SRL (Wang et al., 2015), and dependency-
based SRL (Marcheggiani et al., 2017) with little
to no syntactic input. Our techniques push results
to more than 3 F1 over the best syntax-based mod-
els. However, we also show that there is potential
for syntax to further improve performance.

6 Conclusion and Future Work

We presented a new deep learning model for span-
based semantic role labeling with a 10% rela-
tive error reduction over the previous state of the
art. Our ensemble of 8 layer BiLSTMs incorpo-
rated some of the recent best practices such as or-
thonormal initialization, RNN-dropout, and high-
way connections, and we have shown that they are
crucial for getting good results with deep models.

Extensive error analysis sheds light on the
strengths and limitations of our deep SRL model,
with detailed comparison against shallower mod-
els and two strong non-neural systems. While
our deep model is better at recovering long-
distance predicate-argument relations, we still ob-
serve structural inconsistencies, which can be al-
leviated by constrained decoding.

Finally, we posed the question of whether deep
SRL still needs syntactic supervision. Despite re-
cent success without syntactic input, we found that
our best neural model can still benefit from ac-
curate syntactic parser output via straightforward
constrained decoding. In our oracle experiment,
we observed a 3 F1 improvement by leveraging
gold syntax, showing the potential for high quality
parsers to further improve deep SRL models.

Acknowledgments

The research was supported in part by DARPA
under the DEFT program (FA8750-13-2-0019),
the ARO (W911NF-16-1-0121), the NSF (IIS-
1252835, IIS-1562364), gifts from Google and
Tencent, and an Allen Distinguished Investigator
Award. We are grateful to Mingxuan Wang for
sharing his highway LSTM implementation and
Sameer Pradhan for help with the CoNLL 2012
dataset. We thank Nicholas FitzGerald, Dan Gar-
rette, Julian Michael, Hao Peng, and Swabha
Swayamdipta for helpful comments, and the
anonymous reviewers for valuable feedback.

481



References

Claire Bonial, Olga Babko-Malaya, Jinho D Choi, Jena
Hwang, and Martha Palmer. 2010. Propbank anno-
tation guidelines. Center for Computational Lan-
guage and Education Research Institute of Cognitive
Science University of Colorado at Boulder .

Xavier Carreras and Lluı́s Màrquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning. Associ-
ation for Computational Linguistics, pages 152–164.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the First North American chap-
ter of the Association for Computational Linguis-
tics conference (NAACL). Association for Compu-
tational Linguistics, pages 132–139.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proc. of the 2016 Con-
ference of Empirical Methods in Natural Language
Processing (EMNLP).

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12:2493–2537.

Nicholas FitzGerald, Oscar Täckström, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic role
labeling with neural network factors. In Proc. of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pages 960–970.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems. pages 1019–1027.

James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multilingual joint pars-
ing of syntactic and semantic dependencies with a
latent variable model. Computational Linguistics
39(4):949–998.

Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation 14(8):1771–1800.

Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the penn tree-
bank. In Proceedings of the human language tech-
nology conference. pages 252–256.

Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of er-
ror types in parser output. In Proc. of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). pages 1048–1059.

Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016.
Global neural ccg parsing with optimality guaran-
tees. In Proc. of the 2016 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Mike Lewis and Mark Steedman. 2014. A* ccg pars-
ing with a supertag-factored model. In Proc. of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP). pages 990–1000.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
arXiv preprint arXiv:1701.02593 .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proc. of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). pages 1532–1543.

Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H Martin, and Daniel Jurafsky. 2005. Seman-
tic role chunking combining complementary syntac-
tic views. In Proc. of the 2005 Conference on Com-
putational Natural Language Learning (CoNLL).
pages 217–220.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using ontonotes. In Proc.
of the 2013 Conference on Computational Natural
Language Learning (CoNLL). pages 143–152.

Vasin Punyakanok, Peter Koomen, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proc.
of the 2005 Conference on Computational Natural
Language Learning (CoNLL).

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics
34(2):257–287.

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. In Proc. of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint arXiv:1312.6120 .

Rupesh K Srivastava, Klaus Greff, and Jürgen Schmid-
huber. 2015. Training very deep networks. In Ad-
vances in neural information processing systems.
pages 2377–2385.

Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer,
and Noah A Smith. 2016. Greedy, joint syntactic-
semantic parsing with stack lstms. In Proc. of the
2016 Conference on Computational Natural Lan-
guage Learning (CoNLL). page 187.

482



Oscar Täckström, Kuzman Ganchev, and Dipanjan
Das. 2015. Efficient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association for Computational Linguistics 3:29–41.

Kristina Toutanova, Aria Haghighi, and Christopher D
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics 34(2):161–
191.

Zhen Wang, Tingsong Jiang, Baobao Chang, and Zhi-
fang Sui. 2015. Chinese semantic role labeling with
bidirectional recurrent neural networks. In Proc. of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1626–
1631.

Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2013. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadel-
phia, PA .

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass. 2016. High-
way long short-term memory rnns for distant speech
recognition. In 2016 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP). pages 5755–5759.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

483


	Deep Semantic Role Labeling: What Works and What’s Next

