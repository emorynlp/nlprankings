










































Discourse Level Explanatory Relation Extraction from Product Reviews Using First-Order Logic


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Discourse Level Explanatory Relation Extraction from Product Reviews
Using First-order Logic

Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, Xuanjing Huang
School of Computer Science

Fudan University
Shanghai, P.R. China

{qz, 12110240030, 12210240054, 12210240059, xjhuang}@fudan.edu.cn

Abstract

Explanatory sentences are employed to clarify
reasons, details, facts, and so on. High quality
online product reviews usually include not
only positive or negative opinions, but also a
variety of explanations of why these opinions
were given. These explanations can help
readers get easily comprehensible informa-
tion of the discussed products and aspect-
s. Moreover, explanatory relations can also
benefit sentiment analysis applications. In
this work, we focus on the task of identi-
fying subjective text segments and extracting
their corresponding explanations from prod-
uct reviews in discourse level. We propose
a novel joint extraction method using first-
order logic to model rich linguistic features
and long distance constraints. Experimental
results demonstrate the effectiveness of the
proposed method.

1 Introduction

Through analyzing product reviews with high help-
fulness ratings assigned by readers, we find that a
large number of explanatory sentences are used to
clarify the causes, details, or consequences of opin-
ions. According to the statistic based on the dataset
we crawled from a popular product review website,
more than 56.1% opinion expressions are further
explained by other sentences. Since most consumers
are not experts, these explanations would bring lots
of helpful and easy comprehension information for
them. Suggestions about writing a product review
also advise authors to include not only whether they
like or dislike a product, but also why.1

1http://www.reviewpips.com/
http://www.amazon.com/gp/community-help/customer-

For example, let us consider the following snip-
pets extracted from online reviews:

Example 1: TVs with lower refresh rates may
suffer from motion blur. If you’re watching
a fast-paced football game, for example, you
may notice a bit of blurring as the players run
around the field.

Example 2: The LED screen is highly reflec-
tive. The reflection of my own face makes it very
hard to see the subject I am trying to shoot.

The first sentence of example 1 expresses negative
opinion about refresh rate, which is one of the most
important attributes of TV. The second sentence
describes the consequence of it through an example.
In example 2, detail descriptions are used to explain
the reflection problem of the camera screen.

Although, explanations provide valuable infor-
mation, to the best of our knowledge, there is no
existing work that deals with explanation extraction
for opinions in discourse level. We think that if
explanatory relations can be automatically identified
from reviews, sentiment analysis applications may
benefit from it. Existing opinion mining approaches
mainly focus on subjective text. They try to de-
termine the subjectivity and polarity of fragments
of documents (e.g. a paragraph, a sentence, a
phrase and a word) (Pang et al., 2002; Riloff et
al., 2003; Takamura et al., 2005; Mihalcea et al.,
2007; Dasgupta and Ng, ; Hassan and Radev, 2010;
Meng et al., 2012; Dragut et al., 2012). Fine-grained
methods were also introduced to extract opinion
holder, opinion expression, opinion target, and other
opinion elements (Kobayashi et al., 2007; Wu et al.,

reviews-guidelines

946



2011; Xu et al., 2013; Yang and Cardie, 2013). Ma-
jor research directions and challenges of sentiment
analysis can also be found in surveys (Pang and Lee,
2008; Liu, 2012).

In this work, we aim to identify subjective tex-
t segments and extract their corresponding expla-
nations from product reviews in discourse level.
We propose to use Markov Logic Networks (ML-
N) (Richardson and Domingos, 2006) to learn the
joint model for subjective classification and explana-
tory relation extraction. MLN has been applied in
several natural language processing tasks (Singla
and Domingos, 2006; Poon and Domingos, 2008;
Yoshikawa et al., 2009; Andrzejewski et al., 2011;
Song et al., 2012) and demonstrated its advantages.
It can easily incorporate rich linguistic features and
global constraints by designing various logic for-
mulas, which can also be viewed as templates or
rules. Logic formulas are combined in a proba-
bilistic framework to model soft constraints. Hence,
the proposed approach can benefit a lot from this
framework.

To evaluate the proposed method, we crawled a
large number of product reviews and constructed a
labeled corpus through Amazon’s Mechanical Turk.
Two tasks were deployed for labeling the corpus.
We compared the proposed method with state-of-
the-art methods on the dataset. Experimental results
demonstrate that the proposed approach can achieve
better performance than state-of-the-art methods.

The remaining part of this paper is organized as
follows: In Section 2, we define the problem and
give some examples to show the challenges of this
task. Section 3 describes the proposed MLN based
method. Dataset construction, experimental results
and analyses are given in Section 4. In Section 5, we
present the related work and Section 6 concludes the
paper.

2 Problem Statement

Motivated by the argument structure of discourse
relations used in Penn Discourse Treebank (Rash-
mi Prasad and Webber, 2008), in this work, we
adopt the clause unit-based definition. It means that
clauses are treated as the basic units of opinion ex-
pressions and explanations. Let d = {c1, c2, ...cn}
be the clauses of document d. Directed graph

G = (V, E) is used to represent the subjectivity
of clauses and explanatory relationships between
them. In the graph, vertices represent clauses,
whose categories are specified by the vertex at-
tributes. Directed edges describe the explanatory
relationships between them, of which the heads are
explanatory clauses. If clause ca describes a set of
facts which clarify the causes, context, situation, or
consequences of another clause cb, ca −→ cb is used
to indicate that clause ca explains cb.

Adopting clause unit-based definition is based
on the following reasons: 1) clause is normally
considered as the smallest grammatical unit which
can express a complete proposition (Kroeger, 2005);
2) from analyzing online reviews, we observe that
a clause can express a complete opinion about one
aspect in most of cases; 3) in Penn Discourse
Treebank, the basic unit of discourse relations (with
a few exceptions) is also taken to be a clause (Rash-
mi Prasad and Webber, 2008).

Figure 1(a) illustrates a sample document. Figure
1(b) is the corresponding output of the given docu-
ment. In the graph, vertices whose color are black
stand for subjective clauses. The other clauses are
represented by white vertices. Edges describe the
explanatory relationships between them, of which
the heads are explanatory clauses.

Although the explanatory relation extraction task
has been studied from the view of linguistic and
discourse representation by existing works (Carston,
1993; Lascarides and Asher, 1993), the automatic
extraction task is still an open question. Consider the
following examples extracting from online reviews:

Example 3: It takes great pictures. Color ren-
ditions, skin tones, exposure levels are all first rate.
From the example, we can observe that the second
sentence explains the first one. However, the second
sentence itself also expresses opinion on various
opinion targets. In other words, both subjective and
objective sentences can be used as explanations.

Example 4: When we called their service center
they made us wait for them the whole day and no
one turned up. This level of service is simply not
acceptable. The first sentence in example 4 explains
the second one. Hence, the feature of relative
location between two sentences does not always
work well in all cases.

Example 5: This backpack is great! its very big

947



(c1) I have both the Panasonic LX3 and the Canon

S90. (c2) Both cameras are quite different but truly

excellent. (c3) The S90 is a true pocket camera.

(c4) It is very compact. (c5) The build quality is

also top notch. (c6) It feels solid and it is easy to

grip. (c7) It is so small and convenient, (c8) you

will find that you will always carry it with you.

C5

C3

C2

C4

C6

C7

C1

(a) Example Review (b) Directed Graph Representation

C8

Figure 1: Directed graph representation of a sample document.

and fits more than enough stuff. Many sentences,
which express explanatory relation, do not contain
any connectives (e.g. “because”, “the reason is”,
and so on). Lin et al.(2009) generalized four chal-
lenges (include ambiguity, inference, context, and
world knowledge) to automated implicit discourse
relation recognition. In this task, we also need to
address those challenges.

From the these examples, we can observe that ex-
tracting explanatory relations from product reviews
is a challenging task. Both linguistic and global
constraints should be carefully studied.

3 The Proposed Approach

In this section, we present our method for jointly
classifying the subjectivity of text segments and
extracting explanatory relations. Firstly, we briefly
describe the framework of Markov Logic Networks.
Then, we introduce the clause extraction method
based on the definition described in the Section 2.
Finally, we present the first-order logic formulas
including local formulas and global formulas used
for joint modeling in this work.

3.1 Markov Logic Networks

A MLN consists of a set of logic formulas that
describe first-order knowledge base. Each formula
consists of a set of first-order predicates, logical
connectors and variables. Different with first-order
logic, these hard logic formulas are softened and
can be violated with some penalty (the weight of

formula) in MLN.
We use M to represent a MLN and {(ϕi, wi)}

to represent formula ϕi and its weight wi. These
weighted formulas define a probability distribution
over sets of possible worlds. Let y denote a possible
world, the p(y) is defined as follows (Richardson
and Domingos, 2006):

p(y) =
1

Z
exp

 ∑
(ϕi,wi)∈M

wi
∑

c∈Cnϕi

fϕic (y)

 ,
where each c is a binding of free variable in ϕi to
constraints; fϕic (y) is a binary feature function that
returns 1 if the true value is obtained in the ground
formula we get by replacing the free variables in
ϕi with the constants in c under the given possible
world y, and 0 otherwise; Cnϕi is all possible
bindings of variables to constants, and Z is a nor-
malization constant.

Many methods have been proposed to learn the
weights of MLN using both generative and dis-
criminative approaches (Richardson and Domingos,
2006; Singla and Domingos, 2006). There are
also several MLN learning packages available online
such as thebeast2, Tuffy3, PyMLNs4, Alchemy5, and
so on.

2http://code.google.com/p/thebeast
3http://hazy.cs.wisc.edu/hazy/tuffy/
4http://www9-old.in.tum.de/people/jain/mlns/
5http://alchemy.cs.washington.edu/

948



Describing the attributes of words
subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al.,

2010).

relationLexicon(w) The word w belongs to the lexicon of explanation relation
connectives (Pitler and Nenkova, 2009).

Describing the attributes of the clause ci
word(i, w) The clause ci has word w.

firstWord(i, w) The first word of clause ci is word w.

pos(i, w, t) The POS tag of word w is t in clause ci.

dep(i, h,m) Word m and h are governor and dependent of a dependency
relation in clause ci.

Describing the attributes of relations between clause ci and clause cj
clauseDistance(i, j, m) Distance between clause ci and clause cj in clauses is m.

sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n.

Table 1: Descriptions of observed predicates.

3.2 Clause Identification
We model the clause boundary identification prob-
lem through sequence labeling and use Conditional
Random Fields (CRFs) to identify clause bound-
aries. Words and part-of-speech (POS) tags are used
as feature sets. Since we do not allow embedded
segments, the performance of our method is promis-
ing, which achieves the F1 score of 92.8%. The
result is comparable with the best results obtained
during the CoNLL-2001 campaign (Tjong et al.,
2001).

3.3 Formulas
In this work, we propose to use predicate subj(i)
to indicate that the ith clause is subjective and
explain(i, j) to indicate that the jth clause explains
the ith clause. Both subj and explain are hidden
predicates and jointly modeled by MLN. We use
local and global formulas to model rich linguistic
features and long distance constraints.

3.3.1 Local Formulas
The local formulas relate one or more observed

predicates to exactly one hidden predicate. In this
work, we define a list of observed predicates to
describe the properties of individual clauses and
attributes of relations between two clauses. The
observed predicates and descriptions are shown in

Table 1. The observed predicates can be categorized
into 3 groups: words, clauses, and relations between
clauses. We use two lexicons to capture background
knowledge of words. Lexical, part-of-speech tag,
and dependency relation are used to describe a single
clause. We also propose two predicates to model
distance between clauses.

Table 2 lists the local formulas used in this work.
The “+” notation in the formulas indicates that each
constant of the logic variable should be weighted
separately. For subjective classification and relation
extraction, we construct a number of formulas re-
spectively.

For subjective classification, the first two formu-
las model the influence of lexical and POS tag. It
is similar as the bag-of-words model, which is a
simplifying representation and has been successfully
used for various natural language processing tasks.
Since words which provide positive or negative
opinions may provide important information for
subjectivity classification, we combine predicates of
words and lexicon of opinion words. Bigrams are
also proved to be useful for textual classification in
several NLP tasks. Hence, we also combine predi-
cates about individual word and POS tag to capture
this kind of information. Word-level relations are
explicitly presented at the dependency trees, we

949



Formulas for subjective classification
word(i,w+) ⇒ subj(i)
pos(i,w+,t+) ⇒ subj(i)
word(i,w+) ∧ subjLexicon(w) ⇒ subj(i)
pos(i,w+,t+) ∧ subjLexicon(w) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ⇒ subj(i)
pos(i,w1+,t+) ∧ pos(i,w2+,t+) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ∧ subjLexicon(w1) ⇒ subj(i)
word(i,w1+) ∧ word(i,w2+) ∧ subjLexicon(w2) ⇒ subj(i)
dep(i,w1+,w2+) ⇒ subj(i)
dep(i,w1+,w2+) ∧ subjLexicon(w1) ⇒ subj(i)
dep(i,w1+,w2+) ∧ subjLexicon(w2) ⇒ subj(i)

Formulas for explanatory relation extraction
word(i,w1+) ∧ word(j,w2+) ∧ j ̸=i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ j̸=i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+)∧ j̸=i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ clauseDistance(i,j,m+) ∧ j̸=i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ clauseDistance(i,j,m+) ∧ j̸=i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ clauseDistance(i,j,m+) ∧ j̸=i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ sentenceDistance(i,j,n+) ∧ j ̸=i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ sentenceDistance(i,j,n+) ∧ j ̸=i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ sentenceDistance(i,j,n+) ∧ j̸=i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ firstWord(j,w+) ∧ j̸=i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ firstWord(j,w+) ∧ j̸=i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ firstWord(j,w+) ∧ j̸=i ⇒ explain(i,j)
word(i,w1+) ∧ word(j,w2+) ∧ subjLexicon(w1) ∧ j̸=i ⇒ explain(i,j)
pos(i,w1+,t+) ∧ pos(j,w2+,t+) ∧ subjLexicon(w1) ∧ j̸=i ⇒ explain(i,j)
dep(i,h1+,m1+) ∧ dep(j,h2+,m2+) ∧ subjLexicon(m1) ∧ j̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ clauseDistance(i,j,m+) ∧ j̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ sentenceDistance(i,j,n+) ∧ j ̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ pos(j,w,t+) ∧ clauseDistance(i,j,m+) ∧ j ̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ pos(j,w,t+) ∧ sentenceDistance(i,j,n+) ∧ j̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(i,w1+) ∧ clauseDistance(i,j,m+) ∧ j ̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(i,w1+) ∧ sentenceDistance(i,j,n+) ∧ j̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(j,w1+) ∧ clauseDistance(i,j,m+) ∧ j ̸=i ⇒ explain(i,j)
firstWord(j,w+) ∧ relationLexicon(w) ∧ word(j,w1+) ∧ sentenceDistance(i,j,n+) ∧ j̸=i ⇒ explain(i,j)

Table 2: Descriptions of local formulas.

950



also construct local formulas based on predicates
extracted from dependency trees of clauses.

For explanatory relation extraction, we firstly use
formulas to capture lexical and syntactic information
from both of the clauses. Since distances between
clauses are helpful in determining the relation, we
incorporate two kinds of distance features with lex-
ical and syntactic predicates. Connective words
such as for example, since, explicitly signal the
presence of the explanation relation. Although some
connective words are ambiguous in terms of relation
they mark (Pitler and Nenkova, 2009), they may still
be useful for explanation relation extraction. Hence,
we construct local formulas with relation lexicon
and other predicates.

3.3.2 Global Formulas
Local formulas are designed to deal with sub-

jective classification of a single clause or relation
determination of a single pair of clauses. Global
formulas are designed to handle global constraints
of multiple clauses. From the definition of explana-
tory relation and corpus statistics, we observe the
following properties:

Property 1: One clause can only serve as the
explanation of one subjective clause.

Property 2: Explanatory clauses occur immedi-
ately before or after their corresponding subjective
clauses.

Property 3: The positions of explanatory clauses
are consecutive. In other words, if clause ck and
ck+2 explain clause cj , the clause ck+1 would also
be explanatory clause of cj .

For property 1, we use the following global for-
mula to make sure that one clause only explains at
most one another clause.

explain(i, j) ⇒ ¬explain(k, j) ∀k ̸= i, j (1)

Based on the property 2 and 3, explanatory claus-
es are consecutive and immediately before or after
their corresponding subjective clauses. We use the
following formulas to guarantee the property:

explain(i, i + k) ⇒ explain(i, i + m),
1 ≤ m ≤ k − 1 (2)

explain(i, i− k) ⇒ explain(i, i−m),
1 ≤ m ≤ k − 1 (3)

Since our aim is to extract explanatory for subjec-
tive clauses, we also use the following formulas to
make sure that the clauses which are explained are
subjective ones.

explain(i, j) ⇒ subj(i) (4)

4 Experiments

4.1 Data Set
We crawled a number of reviews about digital cam-
eras from Buzzillions6, which is a product review
site and contains more than 16 million reviews.
We randomly select 100 reviews whose usefulness
ratings are 5 on a 5-point scale. They contain 1137
sentences, which are composed by 1665 clauses.
Amazon’s Mechanical Turk is used to deploy two
tasks for labeling the corpus. 694 clauses are labeled
subjective and 478 clauses explain other ones. More
than 56.1% opinion expressions are explained by
their corresponding explanatory sentences.

The two projects we deployed on Amazon’s Me-
chanical Turk are: 1) Determine whether a clause
contains opinion expressions or not; 2) Determine
whether a clause clarifies causes, reasons, or conse-
quences of another given clause. In order to control
the labeling quality, we configured parameters of
the project to make sure that all the tasks should
be judged by at least 20 annotators. Most of the
annotators can complete a task within 25 seconds.
Figure 2 shows the screenshots of the two projects.

Over all, 127 workers participated in the project.
About 72% of them submitted more than 5 tasks.
Although we listed several examples on the project
descriptions, different people may have their own
understanding and criteria for those tasks. In order
to measure the quality of the labeling task, we use
perplexity to evaluate each task. If the perplexity
of a task is below 0.51, which means that more than
80% of the workers submitted the same decision, the
result of the task will be used as training or testing
data. From the statistic of the corpus, we observe
that only 6.2% of the clauses’ subjectiveness and
15.6% of explanation relations can not be certainly
decided. For the first project, we treated those claus-
es as objective one. And, those clause pairs in the
second project were not considered as explanation
relations.

6www.buzzillions.com

951



Task2: Help us check whether a sentence is an 
explanation of the opinion sentence. 

The opinion sentence (red one) is extracted from product reviews and 
express opinion towards some attributes/parts of a product. Please 
help us check whether the following blue sentences describe a set of 
facts which clarifies the causes, reason, and consequences of the 
opinion given in the opinion sentence. 
 
click "yes" if there is an explanation relation between them, "no" 
otherwise. 
 
     The battery life is something I come to expect from this line of camera. 
     I can leave the camera on for better than 8 hours shooting  
     ()YES 
     ()NO 
 
     The battery life is something I come to expect from this line of camera. 
     and I have the camera set to shut off the sensor after about 30 seconds   
     ()YES 
     ()NO 

Task1: Help us determine whether a sentence is 
subjective or objective. 

The following sentences are extracted from product reviews. Please 
help us check whether the following sentences  expressing opinion 
towards some attributes/parts of a product. 
 
     The battery life is something I come to expect from this line of camera. 
     ()Subjective 
     ()Objective 
 
     I have the camera set to shut off the sensor after about 30 seconds   
     ()Subjective 
     ()Objective 

Figure 2: Screenshots of the two tasks on Amazon
Mechanical Turk.

4.2 Experiments Configurations

Stanford parser (Klein and Manning, 2003) is used
for extracting features from dependency parse trees.
For resolving Markov logic network, we use the
toolkit thebeast 7. The detailed setting of thebeast
engine is as follows: The inference algorithm is
the MAP inference with a cutting plane approach.
For parameter learning, the weights for formulas are
updated by an online learning algorithm with MIRA
update rule. All the initial weights are set to zeros.
The number of iterations is set to 10 epochs.

Evaluation metrics used for subjectivity classifi-
cation and relation extraction throughout the experi-
ments include: Precision, Recall, and F1-score. We
randomly select 80% reviews as training set and the
others as testing set.

Since the dataset is newly created for this task, to
compare the performance of the proposed method to
other models, we also reimplemented several state-

7http://code.google.com/p/thebeast

of-the-art methods for comparison.

• CRF-Subj: We follow the method proposed by
Zhao et al. (2008), which regard the subjec-
tivity of all clauses throughout a paragraph as
a sequential flow of sentiments and use CRFs
to model it. The feature sets are similar as
the local formulas for MLN including words,
POS tags, dependency relations, and opinion
lexicon.

• RAE-Subj: Socher et al. (2011) proposed to
use recursive autoencoders for sentence-level
predication of sentiment label distributions. To
compare with it, we also reimplement their
method without any hand designed lexicon.

• PDTB-Rel: For discourse relation extraction,
we use “PDTB-Styled End-to-End Discourse
Parser” (Lin et al., 2010) to extract discourse
level relations as baseline. Since it is a gener-
al discourse relations identification algorithms,
“Cause”, “Pragmatic Cause”, “Instantiation”,
and “Restatement” relation types are treated as
explanatory relation in this work.

• SVM-Rel: We also use LibSVM (Chang and
Lin, 2011) to classify the relations between
clauses. Following the configurations reported
by Feng and Hirst (2012), we use linear kernel
and probability estimation to model it.

4.3 Results

Table 3 shows the comparisons of the proposed
method with the state-of-the-art systems on subjec-
tivity classification and explanatory relation extrac-
tion. From the results, we can observe that recur-
sive autoencoders based subjectivity classification
method achieves slightly better performance than
our method and conditional random fields based
method. The performances of the proposed method
are similar as CRFs’. We think that the main reason
is that only lexical features are used in MLN models
for subjective classification. However, conditional
random fields consider not only lexical information
but also inference of the contexts of sentences.
RAE method learns vector space representations for
multi-word phrases and uses compositional seman-
tics to understand sentiment.

952



Methods
Subjective Classification

P R F1
CRF-Subj 83.5% 76.9% 80.1%
RAE-Subj 85.3% 79.1% 82.1%
MLN 79.2% 80.6% 79.9%

Methods
Relation Extraction
P R F1

RAE-Subj + PDTB-Rel 28.5% 38.6% 32.8%
RAE-Subj + SVM-Rel 32.4% 89.7% 47.6%
MLN 56.2% 72.9% 63.5%

Table 3: Performance comparisons between the proposed
method and state-of-the-art methods. “MLN” represents
the method proposed in this work.

For evaluating the performance of relation extrac-
tion, we combine the results of RAE with PDTB-
Rel and SVM-Rel. For all the subjective clauses
identified by RAE, PDTB-Rel and SVM-Rel are
used to extract corresponding explanatory clauses.
The results are shown in the last three rows in
the Table 3. From the results, we can observe
that the proposed joint model achieves best F1
score and precision among all methods. Although
the proposed method achieve slightly worse result
in processing subjectivity classification. We think
that the error propagation is the main reason for
worse results of cascaded methods. The relative
improvement of MLN over SVM-Rel is more than
33.4%.

To show the effectiveness of different observed
predicates, we evaluate the performances of the
proposed method with different predicate sets. We
subtract one observed predicate and its correspond-
ing local formulas from the original sets at a time.
The results of both subjectivity classification and
relation extraction are shown in Table 4. The first
row shows the result of the MLN based method with
all observed predicates and local formulas. From the
results we can observe that the observed predicates
which are not used in the local formulas for sub-
jectivity classification also impact the performance
of subjectivity classification. We think that the per-
formance is effected by the global formulas, which
combine the procedure of subjectivity classification

and relation extraction. Among all predicates, we
observe that words and dependency relations play
the most important roles. Without word predicate,
the F1 score of subjectivity classification and re-
lation extraction significantly drop to 51.2% and
42.9% respectively. For subjectivity classification,
subjective lexicon contributes a lot for recall. For
relation extraction, the impacts of clause distance
and sentence distance are not as significant as the
other features.

5 Related Work

Our work relates to three research areas: sentiment
analysis/opinion mining, discourse-level relation ex-
traction, and Markov logic networks. Along with the
increasing requirement, subjectivity classification
has recently received considerable attention from
both the industry and researchers. A variety of
approaches and methods have been proposed for
this task from different aspects. Among them, a
number of approaches focus on classifying senti-
ments of text in different levels (e.g. words (Kim
and Hovy, 2004), phrases (Wilson et al., 2005),
sentences (Zhao et al., 2008), documents (Pang et
al., 2002) and so on.), and detecting the overall
polarity of them.

Another research direction tries to convert the
sentiment analysis task into entity identification and
relation extraction. Hu and Liu (2004) proposed
to use a set of methods to produce feature-based
summary of a large number of customer reviews.
Kobayashi et al. (2007) assumed that evaluative
opinions could be structured as a frame which is
composed by opinion holder, subject, aspect, and
evaluation. They converted the task to two kinds
of relation extraction tasks and proposed a machine
learning-based method which used both contextual
and statistical clues.

Analysis of some special types of sentences were
also introduced in recent years. Jindal and Li-
u (2006) studied the problem of identifying com-
parative sentences. They analyzed different types
of comparative sentences and proposed learning
approaches to identify them. Conditional sentences
were studied by Narayanan et al (2009). They
analyzed the conditional sentences in both linguistic
and computitional perspectives and used learning

953



Subjective Classification Relation Extraction
P R F1 P R F1

MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5%

−subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4%
−relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0%
−word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9%
−firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7%
−pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0%
−dep(i, h,m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8%
−clauseDistance(i, j, m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3%
−sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2%

Table 4: Performance comparisons of different observed predicates

method to do it. They followed the feature-based
sentiment analysis model (Hu and Liu, 2004), which
also use flat frames to represent evaluations.

Since the cross sentences relations are considered
in this work, the discourse-level relation extrac-
tion methods are also related to ours. Marcu and
Echihabi (2002) proposed to use an unsupervised
approach to recognizing discourse relations. Lin et
al.(2009) analyzed the impacts of features extracted
from contextual information, constituent parse trees,
dependency parse trees, and word pairs. Asher
et al.(2009) studied discourse segments containing
opinion expressions from the perspective of linguis-
tics. Chen et al. (2010) introduced a multi-label
model to detect emotion causes. They developed
two sets of linguistic features for this task base on
linguistic cues. Zirn et al. (2011) proposed to use
MLN framework to capture the context information
in analysing (sub-)sentences.

The most similar work to ours was proposed by
Somasundaran et al.(2009). They proposed to use it-
erative classification algorithm to capture discourse-
level associations. However different to us, they
focused on pairwise relationships between opinion
expressions. In this paper, we used MLN framework
to capture another different discourse-level relation,
which exists between subject clauses or subject
clause and objective clause.

Richardson and Domingos (2006) proposed
Markov Logic Networks, which combines first-
order logic and probabilistic graphical models. In

recent years, MLN has been adopted for several
natural language processing tasks and achieved
a certain level of success (Singla and Domingos,
2006; Riedel and Meza-Ruiz, 2008; Yoshikawa
et al., 2009; Andrzejewski et al., 2011; Jiang
et al., 2012; Huang et al., 2012). Singla and
Domingos (2006) modeled the entity resolution
problem with MLN. They demonstrated the
capability of MLN to seamlessly combine a number
of previous approaches. Poon and Domingos (2008)
proposed to use MLN for joint unsupervised
coreference resolution. Yoshikawa et al. (2009)
proposed to use Markov logic to incorporate both
local features and global constraints that hold
between temporal relations. Andrzejewski et
al. (2011) introduced a framework for incorporating
general domain knowledge, which is represented by
First-Order Logic (FOL) rules, into LDA inference
to produce topics shaped by both the data and the
rules.

6 Conclusions

In this paper, we propose to use Markov logic
networks to identify subjective text segments and ex-
tract their corresponding explanations in discourse
level. We use MLN to jointly model subjectivity
classification and explanatory relation extraction.
Rich linguistic features and global constraints are
incorporated by various logic formulas and global
formulas. To evaluate the proposed method, we
collected a large number of product reviews and

954



constructed a labeled corpus through Amazon’s Me-
chanical Turk. Experimental results demonstrate
that the proposed approach achieve better perfor-
mance than state-of-the-art methods.

7 Acknowledgement

The authors wish to thank the anonymous reviewers
for their helpful comments and Kang Han for
preparing the corpus. This work was partially
funded by National Natural Science Foundation
of China (61003092, 61073069), Key Projects
in the National Science & Technology Pillar
Program(2012BAH18B01), National Major
Science and Technology Special Project of China
(2014ZX03006005), Shanghai Municipal Science
and Technology Commission (12511504502) and
“Chen Guang” project supported by Shanghai
Municipal Education Commission and Shanghai
Education Development Foundation(11CG05).

References
David Andrzejewski, Xiaojin Zhu, Mark Craven, and

Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirichlet
allocation using first-order logic. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence - Volume Volume Two, IJCAI’11,
pages 1171–1177. AAAI Press.

Nicholas Asher, Farah Benamara, and Yannick Mathieu.
2009. Appraisal of opinion expressions in discourse.
Lingvisticæ Investigationes.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Language
Resources Association (ELRA).

R. Carston. 1993. Conjunction, explanation and
relevance. Lingua 90, pages 27–48.

Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27, May.

Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and
Chu-Ren Huang. 2010. Emotion cause detection
with linguistic constructions. In Proceedings ofColing
2010.

Sajib Dasgupta and Vincent Ng. Mine the easy, classify
the hard: A semi-supervised approach to automatic
sentiment classification. In Proceedings of ACL-
IJCNLP 2009.

Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency
checking for sentiment dictionaries. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 997–1005, Jeju Island, Korea, July. Association
for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic features.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 60–68, Jeju Island, Korea, July.
Association for Computational Linguistics.

Ahmed Hassan and Dragomir R. Radev. 2010.
Identifying text polarity using random walks. In
Proceedings of ACL 2010, Uppsala, Sweden, July.

Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
SIGKDD 2004.

Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence.

Shangpu Jiang, D. Lowd, and Dejing Dou. 2012. Learn-
ing to refine an automatically extracted knowledge
base using markov logic. In Data Mining (ICDM),
2012 IEEE 12th International Conference on, pages
912–917.

Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR
2006.

Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COLING
2004.

Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS 2003.

Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.

Paul Kroeger. 2005. Analyzing Grammar: An
Introduction. Cambridge.

Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437–
493.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP 2009.

955



Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR,
abs/1011.0835.

Bing Liu. 2012. Sentiment Analysis and Opinion
Mining. Morgan & Claypool Publishers.

Daniel Marcu and Abdessamad Echihabi. 2002.
An unsupervised approach to recognizing discourse
relations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 368–375.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-
lingual mixture model for sentiment classification.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 572–581, Jeju Island, Korea, July.
Association for Computational Linguistics.

Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL 2007.

Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP 2009.

Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends in
Information Retrieval, 2:1–135, January.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP 2002.

Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009.

Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’08, pages
650–659, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In
Bente Maegaard Joseph Mariani Jan Odjik Stelios
Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of L-
REC’08, Marrakech, Morocco, may. http://www.lrec-
conf.org/proceedings/lrec2008/.

Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107–136.

Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with markov logic. In
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, CoNLL ’08,

pages 193–197, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of HLT-NAACL 2003.

P. Singla and P. Domingos. 2006. Entity resolution with
markov logic. In Data Mining, 2006. ICDM ’06. Sixth
International Conference on, pages 572–582.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 151–161,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion graphs for
polarity and discourse classification. In Proceedings
of TextGraphs-4.

Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint learning for coreference
resolution with markov logic. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1245–1254, Jeju
Island, Korea, July. Association for Computational
Linguistics.

Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL 2005.

Erik F. Tjong, Kim Sang, and Hervé Déjean. 2001.
Introduction to the conll-2001 shared task: clause
identification. In Proceedings of the 2001 workshop
on Computational Natural Language Learning -
Volume 7, ConLL ’01, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP
2005.

Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2011. Structural opinion mining for graph-based
sentiment representation. In Proceedings of EMNLP
2011.

Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and
Jun Zhao. 2013. Walk and learn: a two-stage
approach for opinion words and opinion targets co-
extraction. In Proceedings of the 22nd international
conference on World Wide Web companion, WWW
’13 Companion, pages 95–96, Republic and Canton of
Geneva, Switzerland. International World Wide Web
Conferences Steering Committee.

956



Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings of
ACL 2013.

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly
identifying temporal relations with markov logic. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ’09,
pages 405–413, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proceedings of EMNLP 2008.

Cäcilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 336–344, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.

957


