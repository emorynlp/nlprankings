



































wn-rerank.dvi


Parse Ranking with Semantic Dependencies and WordNet

Xiaocheng Yin♠ Jungjae Kim♠ Zinaida Pozen♦♣ Francis Bond♠
♠ Nanyang Technological University, Singapore

⋄ University of Washington, Seattle
yinx0005@e.ntu.edu.sg,jungjae.kim@ntu.edu.sg,

zpozen@gmail.com,bond@ieee.org

Abstract

In this paper, we investigate which fea-
tures are useful for ranking semantic rep-
resentations of text. We show that two
methods of generalization improved re-
sults: extended grand-parenting and super-
types. The models are tested on a subset of
SemCor that has been annotated with both
Dependency Minimal Recursion Seman-
tic representations and WordNet senses.
Using both types of features gives a sig-
nificant improvement in whole sentence
parse selection accuracy over the baseline
model.

1 Introduction

In this paper we investigate various features to
improve the accuracy of semantic parse ranking.
There has been considerable successful work on
syntactic parse ranking and reranking (Toutanova
et al., 2005; Collins and Koo, 2006; McClosky
et al., 2006), but very little that uses pure semantic
representations. With recent work on building se-
mantic representations (from deep grammars such
as LFG (Butt et al., 1999) and HPSG (Sag et al.,
1999), directly through lambda calculus, or as in
intermediate step in machine translation) the ques-
tion of ranking them has become more important.

The closest related work is Fujita et al. (2010)
who ranked parses using semantic features from
Minimal Recursion Semantics (MRS) and syntac-
tic trees, using a Maximum Entropy Ranker. They
experimented with Japanese data, using the Hinoki
Treebank (Bond et al., 2008), using primarily ele-
mentary dependencies: single arcs between pred-

♣Currently at PointInside, Inc.

S

NP

N
I

VP

V
treat NP

N

N
dogs N

CONJ
and

N
cats

PP

with worms

Figure 1: Syntactic view of sentence “I treat dogs
and cats with worms”.

icates and their arguments. These can miss some
important connections between predicates.

An example parse tree for I treat dogs and cats
with worms is shown in Figure 1.1, for the interpre-
tation “I treat both dogs and cats that have worms”
(not “I treat, using worms, dogs and cats” or any
of the other possibilities)

The semantic representation we use is De-
pendency Minimal Recursion Semantics (DRMS:
Copestake, 2009). The Minimal Recursion Se-
mantics (MRS: Copestake et al., 2005) is a com-
putationally tractable flat semantics that under-
specifies quantifier scope. The Dependency MRS
is an MRS representation format that keeps all
the information from the MRS but is simpler to
manipulate. DMRSs differ from syntactic de-
pendency graphs in that the relations are defined
between slightly abstract predicates, not between

1Simplified by omission of non-branching nodes.



surface forms. Some semantically empty surface
tokens (such as infinitive to) are not included,
while some predicates are inserted that are not in
the original text (such as the null article).

A simplified MRS representation of our exam-
ple sentence and its DMRS equivalent are shown
in Figure 2.

In the DMRS, the basic links between the nodes
are present. However, potentially interesting rela-
tions such as that between the verb treat and its
conjoined arguments dogs and cats are not linked
directly. Similarly, the relation between dogs and
cats and worms is conveyed by the preposition
with, which links them through its external argu-
ment (ARG1: and) and internal argument (ARG2:
worms). There is no direct link. We investigate
new features that make these links more direct
(Section 3.2).

We also explore the significance of the effec-
tiveness of links between words that are connected
arbitrarily far away in the semantic graph (Sec-
tion 3.2.3).

Finally, we experimented with generalizing
over semantic classes. We used WordNet semantic
files as supertypes to reduce data sparseness (Sec-
tion 3.2.4). This will generalize the lexical seman-
tics of the predicates, resulting in a reduction of
feature size and ambiguity.

2 Previous Work

This paper follows up on the work of Fujita et al.
(2010) in ranking MRS semantic representations,
which was carried out for Japanese. We are con-
ducting a similar investigation for English, and
add new features and approaches. Fujita et al.
(2010) worked with the Japanese Hinoki Corpus
(Bond et al., 2008) data and used hypernym chains
from the Goi-Taikei Japanese ontology (Ikehara
et al., 1997) for variable-level semantic backoff.
This is in contrast to the uniform WordNet seman-
tic file backoff performed here. In addition, this
work only focuses on MRS ranking, whereas Fu-
jita et al. (2010) combined MRS features with syn-
tactic features to improve syntactic parse ranking
accuracy.

Our use of WordNet Semantic Files (SF) to re-
duce lexical feature sparseness is inspired by sev-
eral recent papers. Agirre et al. (2008, 2011) have
experimented with replacing open-class words
with their SFs. Agirre et al. (2008) have shown
an improvement in full parse and PP attachment

scores with statistical constituency parsers using
SFs. Agirre et al. (2011) have followed up on
those results and re-trained a dependency parser
on the data where words were replaced with their
SFs. This resulted in a very modest labeled at-
tachment score improvement, but with a signifi-
cantly reduced feature set. In a recent HPSG work,
MacKinlay et al. (2012) attempted to integrate lex-
ical semantic features, including SF backoff, into
a discriminative parse ranking model. However,
this was not shown to help, presumably because
the lexical semantic features were built from syn-
tactic constituents rather than MRS predicates.

The ancestor features found to be helpful here
are inspired by the use of grand-parenting in syn-
tactic parse ranking (Toutanova et al., 2005) and
chains in dependency parsing ranking (Le Roux
et al., 2012).

3 Resources and Methodology

In this section we introduce the corpus we work
on, and the features we extract from it.

3.1 Corpus: SemCor

To evaluate our ranking methods, we are using
the Redwoods Treebank (Oepen et al., 2004) of
manually disambiguated HPSG parses, storing full
signs for each analysis and supporting export into
a variety of formats, including the Dependency
MRS (DMRS) format used in this work.

The HPSG parses in Redwoods are based on
the English Resource Grammar (ERG; Flickinger,
2000) – a hand-crafted broad-coverage HPSG
grammar of English.

For our experiments, we used a subset of the
Redwoods Treebank, consisting of 2,590 sen-
tences drawn from SemCor (Landes et al., 1998).
In the SemCor corpus each of the sentences is
tagged with WordNet senses created at Princeton
University by the WordNet Project research team.
The average length of the Redwoods SemCor sen-
tences is 15.4 words, and the average number of
parses is 247.

From the treebank we can export the DMRS.
The choice of which words become predicates is
slightly different in the SemCor/WordNet and the
ERG. The ERG lexicon groups together all senses
that have the same syntactic properties, making
them underspecified for many sense differences.
Thus elementary predicate catn:1 could be any of
the WordNet senses catn:1 “feline mammal usu-



I treat dogs and cats with worms.











































































mrs
LTOP h1 h
INDEX e3 e

RELS

〈







pron〈0:1〉
LBL h4 h
ARG0 x5 x






,















treatv:1〈2:6〉
LBL h2 h
ARG0 e3
ARG1 x5
ARG2 x9 x















,







dogn:1〈7:11〉
LBL h17 h
ARG0 x15






,















andc〈12:15〉
LBL h22 h
ARG0 x9
L-INDEX x15
R-INDEX x19















,







catn:1〈16:20〉
LBL h23 h
ARG0 x19






,















withp〈21:25〉
LBL h22
ARG0 e24 e
ARG1 x9
ARG2 x25 x















,







wormn:1〈26:31〉
LBL h29 h
ARG0 x25







〉











































































pron treatv:1 dogn:1 andc catn:1 withp wormn:1

1
2

L R 2
1

with

2

2 1

1

Simplified by omission of quantifiers
Dashed lines show Preposition (P) features
Dotted lines show Conjunction (LR) features
Arc labels show the roles: 1 is ARG1, 2 is ARG2, . . . .

Figure 2: MRS and DMRS for I treat cats and dogs with worms.



Topsn actn
animaln artifactn
attributen bodyn
cognitionn communicationn
eventn feelingn
foodn groupn
locationn motiven
objectn personn
phenomenonn plantn
possessionn processn
quantityn relationn
shapen staten
substancen timen

Table 1: WordNet Noun Semantic Files.

ally having thick soft fur and no ability to roar”,
catn:2 “an informal term for a youth or man” and
six more.2 In some cases, DMRS decomposes a
single predicate into multiple predicates (e.g. here
into inp thisq placen). The ERG and WordNet also
often make different decisions about what consti-
tutes a multiword expression. For these reasons
the mapping between the two annotations is not
always straightforward. In this paper we use the
mapping between the DRMS and WordNet anno-
tations produced by Pozen (2013).

Using the mapping, we exploited the sense tag-
ging of the SemCor in several ways. We ex-
perimented both with replacing elementary pred-
icates with their synsets, their hypernyms at var-
ious levels and with their semantic files (Landes
et al., 1998), which generalize the meanings of
words that belong to the same broad semantic cat-
egories.3 These dozens of generalized semantic
tags help to address the issue of feature sparse-
ness, compared to thousands of synsets. We show
the semantic files for nouns and verbs in Tables 1
and 2. In this paper, we only report on the parse
selection accuracy using semantic files to reduce
ambiguity, as it gave the best results.

3.2 Semantic Dependency Features

In this section we introduce the baseline features
for parse ranking.

Table 3 shows example features extracted from
the DMRS depicted in Figure 2.Features 1–16 are

2Elementary predicates are shown in sans-serif font, Word-
Net senses in bold italic, WordNet semantic files
are shown in bold typewriter.

3Semantic Files are also sometimes referred to as Seman-
tic Fields, Lexical Fields or Supersenses.

bodyv changev
cognitionv communicationv
competitionv consumptionv
contactv creationv
emotionv motionv
perceptionv possessionv
socialv stativev
weatherv

Table 2: WordNet Verb Semantic Files.

the semantic dependency features (Baseline). 17–
18 are the conjunctive features (LR). 19–22 are the
preposition role features (PR).

# Sample Features
0 〈0 treatv:1 ARG1 pron ARG2 andc〉
1 〈0 andc L-IND dogn:1 R-IND catn:1〉
2 〈0 withp ARG1 andc ARG2 wormn:1〉
3 〈1 treatv:1 ARG1 pron〉
4 〈1 treatv:1 ARG2 andc〉
5 〈1 andc L-IND dogn:1〉
6 〈1 andc R-IND catn:1〉
7 〈1 withp ARG1 andc〉
8 〈1 withp ARG2 wormn:1〉
9 〈2 treatv:1 pron andc〉

10 〈2 withp andc wormn:1〉
11 〈3 treatv:1 pron〉
12 〈3 treatv:1 andc〉
13 〈3 andc dogn:1〉
14 〈3 andc catn:1〉
15 〈3 withp andc〉
16 〈3 withp wormn:1〉
17 〈1 treatv:1 ARG2 dogn:1〉
18 〈1 treatv:1 ARG2 catn:1〉
19 〈 0 andc L-IND dogn:1 R-IND catn:1

withp wormn:1 〉
20 〈1 andcwithp wormn:1〉
21 〈2 andc wormn:1〉
22 〈3 andc wormn:1〉

Table 3: Features for the DMRS in Fig 2.

Baseline features are those that directly reflect
the dependencies of the DMRS. In Table 3, fea-
ture type 〈0〉 (0–2) shows predicates with all their
arguments. Feature type 〈1〉 (3–8) shows each ar-
gument individually. Feature type 〈2〉 shows all ar-
guments without the argument types. Feature type
〈3〉 is the least specified, showing individual argu-
ments without the labels. These types are the same
as the MRS features of Toutanova et al. (2005) and



the SEM-DEP features of Fujita et al. (2010).

3.2.1 Conjunctive Features

We further create two more features, called
Left/Right Handle Features (LR), to link directly
the two arguments of conjunctive relations with
their parent, independently from the other ar-
gument. In Table 1, for example, the feature
〈treatv:1 ARG2 andc〉, although valid, does not con-
vey the meaning of the sentence. Instead, we add
the two LR features 〈treatv:1 ARG2 dogn:1〉 (fea-
ture 17) and 〈treatv:1 ARG2 catn:1〉 (feature 18),
which better model the conjunction relation.

3.2.2 Preposition Role Features

As shown in Figure 2, the node withp has two
links: to andc (ARG1) and to wormn:1 (ARG2). The
two relations together indicate a noun-preposition-
noun relationship. Instead of breaking the rela-
tionship into the two separate features, we intro-
duce it, as a whole, as a new type of feature, where
the two arguments of the preposition (e.g. andc,
wormn:1) will have a direct relation via the preposi-
tion (e.g. withp). We name these Preposition Role
features (PR), as they are similar in spirit to se-
mantic roles. Some sample PR features are given
in Table 3, features 19–22.

The new features explicitly convey, for exam-
ple, noun-preposition-noun relations. Parses con-
taining features like something at somewhere can
be further distinguished from parses containing at
somewhere and something at separately. When the
features become more representative, active parses
are more likely to be selected, though with the cost
of a larger feature set size.

As 4 types of features can be developed based
on one relationship, a Preposition Role link would
have 4 separate features. While the Conjunctive
features mentioned in previous section give 2 to 4
additional features, Baseline-PR features normally
give 4 more. Thus, the feature size of Baseline-
PR model is larger than that of the Baseline-LR
model.

3.2.3 Ancestor Features

While the semantic dependency features corre-
spond to direct dependencies, we introduce a new
type of features that represent indirect dependen-
cies between ancestors and their descendants in
the DMRS. For each predicate, we collect all its
descendants linked through more than one depen-
dency and create features to represent the indirect

# Sample Features
0 〈0 treatv:1 ARG1 pron ARG2 andc〉
1 〈0 andc L-IND dogn:1 R-IND catn:1〉
2 〈0 treatv:1 ARG2 dogn:1 ARG2 catn:1〉
3 〈1 treatv:1 ARG1 pron〉
4 〈1 treatv:1 ARG2 andc〉
5 〈1 treatv:1 ARG2 dogn:1〉
6 〈1 treatv:1 ARG2 catn:1〉
7 〈1 andc L-IND dogn:1〉
8 〈1 andc R-IND catn:1〉
9 〈2 treatv:1 pron andc〉

10 〈2 treatv:1 dogn:1 catn:1〉
11 〈3 treatv:1 pron〉
12 〈3 treatv:1 andc〉
13 〈3 treatv:1 dogn:1〉
14 〈3 treatv:1 catn:1〉

Table 4: Ancestor Features (AF).

dependencies between the predicate and the de-
scendants. We name these features Ancestor Fea-
tures (AF).

Table 4 has some sample AF features such as
that linking from treatv:1 to dogn:1 and catn:1 (i.e.
feature 2). This is a one-level ancestor, involving
two predicates, while multi-level ancestors deal
with more than two predicates linked in a se-
quence. Note that these are different from the LR
features (features 15, 16 in Table 1), in that AF
features include both arguments of a conjunction,
for example, connecting the predicate treatv:1 to its
grandchildren dogn:1 and catn:1 via the argument
role of andc in the predicate (feature 2 in Table 4).

When a sentence has n dependencies, our
method generates O(n(n−1)2 ) = O(n

2) AF fea-
tures. In the corpus we use, the dependency struc-
ture of a sentence typically has 4 levels. In prac-
tice the number of AF features is roughly triple
the number of Baseline features. In the evaluation
experiments, we investigated all the eight combi-
nations of the three types of LR, PR, and AF fea-
tures, where each combination is combined with
the baseline features.

3.2.4 Semantic File Features

In the features up until now, words have been rep-
resented as elementary predicate semantic depen-
dencies (SD). Because SemCor also has WordNet
senses, we experiment with replacing open class
words with their supertypes, in this case using
the WordNet semantic files (SF). If a word is not
matched to a WordNet synset we continue to use



# Sample Features
0 〈0 bodyv ARG1 pron ARG2 andc〉
1 〈0 andc L-IND animaln R-IND animaln〉
2 〈0 withp ARG1 andc ARG2 animaln〉
3 〈1 bodyv ARG1 pron〉
4 〈1 bodyv ARG2 andc〉
5 〈1 andc L-IND animaln〉
6 〈1 andc R-IND animaln〉
7 〈1 withc ARG1 animaln〉
8 〈1 withc ARG2 animaln〉
9 〈2 bodyv pron andc〉

10 〈2 withpandc animaln〉
11 〈3 bodyv pron〉
12 〈3 bodyv andc〉
13 〈3 andc animaln〉
14 〈3 andc animaln〉
15 〈3 withc andc〉
16 〈3 withc animaln〉

Table 5: Baseline features with Semantic Files
(SF).

the elementary predicate. This SF representation
is also applied to the eight combinations of feature
types. A sample of the features in the SF represen-
tations are given in Table 5.

Sometimes two features, such as 13 and 14 in
Table 3, are replaced with the same feature, like
9 in Table 5, because dogn:1 and catn:1 are both
replaced with animaln. There are about half as
many Semantic File features as there are SD fea-
tures.

4 Results

We set up the evaluation task as reranking of the
top 500 Redwoods analyses, previously selected
by the syntactic MaxEnt ranker. The subset of
SemCor introduced in Section 3.1 is trained and
tested with the features introduced in Section 3.2.
We grouped the feature sets into two according to
the two word representation of basic Semantic De-
pendencies (SD) and generalized Semantic Files
(SF). Sometime two or more different parses of a
sentence have the same set of features. That is, the
features failed to distinguish between two parses:
often because of spurious syntactic ambiguity that
had no effect on the semantics. In this case we
merged duplicate feature sets to reduce the ambi-
guity in machine learning. If an inactive parse has
the same set of features as that of the active one,
the resulting merged parse was treated as active.

Features Accuracy Features
(%) (×1,000)

SD-Baseline 25.4 454
SD+LR 25.3 469
SD+PR 25.8 563
SD+LR+PR 25.6 582
SD+AF 24.8 1,430
SD+AF+LR 27.1 1,497
SD+AF+PR 25.8 1,761
SD+AF+LR+PR 26.3 1,842

Table 6: Parse selection results with SD.

Features Accuracy Features
(%) (×1,000)

SF-Baseline 25.0 223
SF+LR 25.1 235
SF+PR 26.3 306
SF+LR+PR 26.3 321
SF+AF 28.2 1,051
SF+AF+LR 28.0 1,101
SF+AF+PR 28.1 1,310
SF+AF+LR+PR 27.7 1,375

Table 7: Parse selection results with SF.

We used TADM (Toolkit for Advanced Dis-
criminative Modeling; Malouf, 2002) for the train-
ing and testing of our machine learning model, fol-
lowing Fujita et al. (2010). We carried out 10-fold
cross-validation for evaluation. We measured the
parse selection accuracy at the sentence level. A
parse was considered correct only when all the de-
pendencies of the parse are correct.

The results of parse selection based on SD and
SF representations are shown in Tables 6 and 7.
The addition of the ancestor features (AF) gives
the most increase in the parse selection accuracy.
This result indicates that indirect dependencies as
well as direct dependencies in a successful parse
frequently appear in other active parses. Second,
the SF representation shows better results than the
SD representation in most cases. The semantic ab-
straction of the semantic files reduces the problem
of feature sparseness and is enough to effectively
rerank parses, whose syntactic properties are al-
ready to some extent validated during parsing.

Third, the addition of the PR features also usu-
ally increases the parse selection accuracy. We
plan to (semi-)automatically find more such multi-
dependency structures whose combination shows
better performance than the individual dependen-



cies. Fourth, the LR features do not improve the
accuracy significantly in most cases, though the
SD+AF+LR combination shows the best results
among the feature sets of the SD representation.
This is understandable since the number of the LR
features in our corpus is much smaller than those
of the other features of SD, PR and AF. We need
to test it with a bigger corpus.

5 Discussion

These results show the validity of our assumption
that long distance features and supertypes are both
useful for selecting the correct interpretation of a
sentence. Currently the SD+AF+LR model is the
best for using the elementary predicates. How-
ever the best overall results come from the SF+AF
model when we generalize to the semantic files.
In future work we will investigate on larger-sized
and more richly annotated corpora so that we can
discover more about the relation between feature
size and parse selection accuracy. In addition, we
expect that increasing the corpus size will lead di-
rectly to higher accuracy. Other avenues we would
like to explore is backing off not to the semantic
files, but rather to WordNet hypernyms at various
levels.

These results show that generalizing to seman-
tic supertypes allows us to build semantic ranking
models that are not only smaller, but more accu-
rate. In general, learning time was roughly pro-
portional to the number of features, so a smaller
model can be learned faster. We hypothesize that it
is the combination of dependencies and supertypes
that makes the difference: approaches that used se-
mantic features on phrase structure trees (such as
Bikel (2000) and MacKinlay et al. (2012)) have in
general failed to get much improvement.

Figure 3: Learning curve for SF+AF.

The overall accuracy is still quite low, due prin-
cipally to the lack of training data. We show
the learning curves for the SF+AF configuration
in Figure 3 (the other configurations are similar).
The curve is still clearly rising: the accuracy of
parse selection on our corpus is far from saturated.
This observation gives us confidence that with a
larger corpus the accuracy of parse selection will
improve considerably. The learning curve in Fujita
et al. (2010) showed similar results for the same
amount of data, and increased rapidly with more
(they had a larger corpus for Japanese).

As there are so far still very few corpora with
both structural and lexical semantic annotation,
we are currently investigating the use of automatic
word sense disambiguation to create the features,
in a similar way to Agirre et al. (2008). Finally, we
would like to investigate even more features, such
as the dependency chains of Le Roux et al. (2012).

One exciting possibility is projecting ranking
features across languages: wordnet semantic files
are the supertypes for all wordnets linked to
the Princeton Wordnet, of which there are many
(Bond and Foster, 2013). The predicates that are
not in the wordnets are generally either named
entities or from smallish closed sets of function
words such as conjunctions, prepositions and pro-
nouns. We are currently investigating mapping
these between Japanese and English using trans-
fer rules from an existing machine translation sys-
tem (Bond et al., 2011). In principal, a small set
of mappings for closed class words could allow us
to quickly boot-strap a semantic ranking model for
any language with a wordnet.

6 Conclusion

In summary, we showed some features that help
parse selection. In the SD group, LR features
together with AF features achieved a 1.75% im-
provement in accuracy over the basic Baseline
model (25.36% → 27.12%). However, LR feature
alone and AF feature alone both decrease the accu-
racy (25.36% → 25.28% and 25.36% → 24.84%).
PR features and combination of PR and AF fea-
tures both achieved small improvements (0.416%
Baseline → Baseline+PR, 0.410% Baseline →
Baseline-PR+AF). LR combined with PR features
did not improve the accuracy.

When features get generalized to supertypes, as
shown in the SF group, models with more fea-
tures achieved higher accuracies with the best be-



ing the model with ancestor features (AF) added.
This (SF+AF) achieved an improvement of 3.21%
absolute over the baseline model (24.97% →
28.18%). Adding more features to AF only de-
creases the accuracy. Generalizing to semantic su-
pertypes allows us to build dependency ranking
models that are not only smaller, but more accu-
rate.

Acknowledgments

The authors are grateful to Mathieu Morey and
other members of the Deep Linguistic Processing
with HPSG Initiative along with other members of
their research groups for many extremely helpful
discussions.

References
Eneko Agirre, Timothy Baldwin, and David Martinez. 2008.

Improving parsing and PP attachment performance with
sense information. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics
(ACL HLT 2008), pages 317–325. Columbus, USA.

Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing with
semantic classes. Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics, pages
699–703.

Daniel M. Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Corpora, pages
155–163. Hong Kong.

Francis Bond and Ryan Foster. 2013. Linking and extending
an open multilingual wordnet. In 51st Annual Meeting
of the Association for Computational Linguistics: ACL-
2013, pages 1352–1362. Sofia. URL http://aclweb.
org/anthology/P13-1133.

Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2008.
The Hinoki syntactic and semantic treebank of
Japanese. Language Resources and Evaluation,
42(2):243–251. URL http://dx.doi.org/
10.1007/s10579-008-9062-z, (Re-issue of DOI
10.1007/s10579-007-9036-6 as Springer lost the Japanese
text).

Francis Bond, Stephan Oepen, Eric Nichols, Dan Flickinger,
Erik Velldal, and Petter Haugereid. 2011. Deep
open source machine translation. Machine Transla-
tion, 25(2):87–105. URL http://dx.doi.org/10.
1007/s10590-011-9099-4, (Special Issue on Open
source Machine Translation).

Miriam Butt, Tracy Holloway King, Marı́a-Eugenia Niño,
and Frédérique Segond. 1999. A Grammar Writer’s Cook-
book. CSLI publications.

Michael Collins and Terry Koo. 2006. Discriminative rerank-
ing for natural language parsing. Computational Linguis-
tics, 31(1).

Ann Copestake. 2009. Slacker semantics: Why superficiality,
dependency and avoidance of commitment can be the right
way to go. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 1–9.
Athens.

Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl Pol-
lard. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281–
332.

Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6 (1):15–28.

Sanae Fujita, Francis Bond, Takaaki Tanaka, and Stephan
Oepen. 2010. Exploiting semantic information for HPSG
parse selection. Research on Language and Computation,
8(1):1–22. URL http://dx.doi.org/10.1007/
s11168-010-9069-7.

Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.

Shari Landes, Claudia Leacock, and Christiane Fellbaum.
1998. Building semantic concordances. In Christine Fell-
baum, editor, WordNet: An Electronic Lexical Database,
chapter 8, pages 199–216. MIT Press.

Joseph Le Roux, Benoit Favre, Alexis Nasr, and Seyed Abol-
ghasem Mirroshandel. 2012. Generative constituent pars-
ing and discriminative dependency reranking: Experi-
ments on english and french. In Proceedings of the ACL
2012 Joint Workshop on Statistical Parsing and Seman-
tic Processing of Morphologically Rich Languages, pages
89–99. Association for Computational Linguistics, Jeju,
Republic of Korea. URL http://www.aclweb.org/
anthology/W12-3412.

Andrew MacKinlay, Rebecca Dridan, Diana McCarthy, and
Timothy Baldwin. 2012. The effects of semantic an-
notations on precision parse ranking. In SEM 2012:
The First Joint Conference on Lexical and Computational
Semantics-, volume 2, pages 228–236. Association for
Computational Linguistics.

Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In CONLL-2002,
pages 49–55. Taipei, Taiwan.

David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation. In
44th Annual Meeting of the Association for Computational
Linguistics and 21st International Conference on Compu-
tational Linguistics: COLING/ACL-2006, pages 337–344.

Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2004. LinGO Redwoods: A rich
and dynamic treebank for HPSG. Research on Language
and Computation, 2(4):575–596.

Zinaida Pozen. 2013. Using Lexical and Composi-
tional Semantics to Improve HPSG Parse Selection.
Master’s thesis, University of Washington. URL
https://digital.lib.washington.edu/
researchworks/handle/1773/%23469.

Ivan A. Sag, Tom Wasow, and Emily Bender. 1999. Syntactic
Theory: A Formal Introduction. CSLI Publications, Stan-
ford, second edition.

Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the Redwoods corpus. Research on Lan-
guage and Computation, 3(1):83–105.


