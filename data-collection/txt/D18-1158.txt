















































Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1267â€“1276
Brussels, Belgium, October 31 - November 4, 2018. cÂ©2018 Association for Computational Linguistics

1267

Collective Event Detection via a Hierarchical and Bias Tagging Networks
with Gated Multi-level Attention Mechanisms

Yubo Chen1, Hang Yang1, Kang Liu1,2, Jun Zhao1,2 and Yantao Jia3
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

3 Huawei Technologies Co., Ltd, Beijing, 100085, China
{yubo.chen, hang.yang, kliu, jzhao}@nlpr.ia.ac.cn, jamaths.h@163.com

Abstract

Traditional approaches to the task of ACE
event detection primarily regard multiple
events in one sentence as independent ones
and recognize them separately by using
sentence-level information. However, events
in one sentence are usually interdependent
and sentence-level information is often insuf-
ficient to resolve ambiguities for some types
of events. This paper proposes a novel
framework dubbed as Hierarchical and Bias
Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to solve
the two problems simultaneously. Firstly, we
propose a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. Then, we devise a gated
multi-level attention to automatically extract
and dynamically fuse the sentence-level and
document-level information. The experimen-
tal results on the widely used ACE 2005
dataset show that our approach significantly
outperforms other state-of-the-art methods.

1 Introduction

Event detection (ED) is a crucial subtask of event
extraction, which aims to identify event triggers
and classify them into specific types from texts.
According to the task defined in Automatic Con-
text Extraction1 (ACE), given the following sen-
tence S1, a robust ED system should be able to rec-
ognize two events: a Die event triggered by died
and an Attack event triggered by fired.

S1: In Baghdad, a cameraman died when an
American tank fired on the Palestine Hotel.

To this end, most methods (Ahn, 2006; Hong
et al., 2011; Chen et al., 2015; Nguyen and Grish-
man, 2016; Liu et al., 2017) model ED as a multi-
classification task and predict every word in the

1http://projects.ldc.upenn.edu/ace/

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

Attack Die Transport Injure Meet

Figure 1: Top 5 event types that co-occur with Attack
event in the same sentence in ACE 2005.

sentence separately to determine whether it trig-
gers a specific type of event by using sentence-
level information. However, they face two prob-
lems: (1) Neglecting event interdependency by
separately predicting each event; (2) Sentence-
level information is usually insufficient to resolve
ambiguities for some types of events. In the fol-
lowing, we will use examples to illustrate these
two problems specifically.

S2: The project leader was fired for the
bankruptcy of the subsidiary company.

Event interdependency: In S1, fired triggers
an Attack event, while it triggers an End-Position
event in S2. Because of the ambiguity, a tradi-
tional approach may mislabel fired in S1 as a trig-
ger of End-Position event. However, if we know
died triggers a Die event in S1, which is easier to
disambiguate, we tend to predict that fired triggers
an Attack event. The reason is that the events men-
tioned in the same sentence tend to be semanti-
cally coherent and a Die event usually co-occurs
with an Attack event. The similar phenomenon
can be found in S2. We conduct a statistical anal-
ysis on ACE 2005 dataset, and find that nearly
30% sentences contain multiple events which is
a proportion we can not ignore. To give an in-
tuitive illustration, the top 5 event types that co-
occur with Attack event in the same sentence are
shown in Figure 1. We call such clues as event



1268

interdependency. Some works (Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b) rely
on a set of elaborately designed features and com-
plicated natural language processing (NLP) tools
to capture event interdependency. However, these
methods lack generalization, take a large amount
of human effort and are prone to error propaga-
tion problem. Though Nguyen et al. (2016) use a
Recurrent Neural Networks (RNN) based classifi-
cation model to capture the event interdependency
between current event candidate and the former
(left) predicted events, they miss the event interde-
pendency between current event candidate and the
later (right) predicted events, and the later events
can not change the type of current event. The rea-
son is that they classify the words of the sentence
from left to right one by one and only use the for-
mer events to predict the later event types. We
claim that both of the former and later predicted
events are important to predict the event type of
current trigger candidate. For example in S1, the
former predicted Die event can help us to predict
that fired triggers an Attack event, while in S2 the
later predicted bankruptcy event can help us to
predict that fired triggers an End-Position event.
Thus, how to use a neural-based model to capture
all event interdependencies (the interdependencies
between the current event candidate and its for-
mer/later predicted events) in the whole sentence
is a challenging problem.

Sentence-level and document-level informa-
tion: Besides event interdependency, knowing that
American tank is a weapon can also give us addi-
tional evidence to predict that fired triggers an At-
tack event in S1. Similarly in S2, knowing that
project leader is a job title can also help us to
predict that fired triggers an End-Position event.
We call such clues as sentence-level information.
However, sometimes it is difficult even for peo-
ple to classify event types from an isolated sen-
tence. We must resort to document-level informa-
tion. For example, considering the following sen-
tence with an ambiguous word left:

S3: He left the company.
It is hard to tell left triggers a Transport event

which means that he left the place, or an End-
Position event which means that he resigned from
the company. However, if we read the whole doc-
ument, a clue like â€œHe planned to go shopping
before he went home, because he got off work
early today.â€ would give us more confidence to

believe that left triggers a Transport event, while
a clue like â€œThey held a party for his retire-
ment.â€ would indicate the aforementioned event
is an End-Position event. We call such clues as
document-level information. Moreover, the confi-
dence of sentence-level and document-level infor-
mation should be taken into consideration when
using them together to construct a broader range
of contextual information. For example in S3,
document-level information will give us more ev-
idence, while in S1 sentence-level information
is enough to disambiguate the types of events.
There have been some feature-based studies (Ji
and Grishman, 2008; Liao and Grishman, 2010;
Huang and Riloff, 2012) that construct rules to
capture document-level information for improv-
ing sentence-level ED. However, they suffer from
two problems: (1) The features they used of-
ten need to be manually designed and may in-
volve error propagation from existing NLP tools;
(2) Sentence-level and document-level informa-
tion are integrated by a large number of fixed rules,
which is complicated to construct and it will be far
from complete. Thus, how to use a neural-based
model to automatically extract sentence-level and
document-level information and dynamically inte-
grate them is another challenging problem.

In this paper, we propose a Hierarchical and
Bias Tagging Networks with Gated Multi-level
Attention Mechanisms (HBTNGMA) to address
the two problems stated above simultaneously. To
capture event interdependency and collectively de-
tect multiple events in one sentence, we propose a
hierarchical and bias tagging networks for event
detection. In which, we exploit a hierarchical
RNN-based tagging layer to capture all event in-
terdependencies in the whole sentence and de-
vise a bias objective function to reinforce the in-
fluence of trigger tags on the model2. To use
a broader range of contextual information of the
event candidate, we propose a gated multi-level at-
tention, which can automatically extract sentence-
level and document-level information and inte-
grate them dynamically. In summary, the contri-
butions of this paper are as follows:

â€¢ We propose a novel framework for event
detection, which can automatically extract

2Compared with the task like Named Entities Recogni-
tion, the number of â€œOâ€ tags is much more than the number
of trigger tags in ED task, i.e. if we use the non-bias objec-
tive function and tag all words in one sentence as â€œOâ€, we will
gain a low loss. Thus we devise a bias objective function.



1269

and dynamically integrate sentence-level and
document-level information and collectively
detect multiple events in one sentence.

â€¢ To capture event interdependency, we ex-
ploit a hierarchical and bias tagging net-
works to detect multiple events in one sen-
tence collectively. To automatically extract
and dynamically integrate contextual infor-
mation, we devise a gated multi-level atten-
tion Mechanisms. To our knowledge, this
is the first work to jointly use event inter-
dependency, sentence-level information and
document-level information via a neural tag-
ging schema for event detection task.

â€¢ We conduct extensive experiments on a
widely used ACE 2005 dataset, and the ex-
perimental results show that our approach
significantly outperforms other state-of-the-
art methods 3.

2 Task Description

Event detection (ED) is a crucial subtask of event
extraction (EE). In this paper, we focus on ED
task defined in ACE evaluation, where an event
is defined as a specific occurrence involving one
or more participants. Firstly, we introduce some
ACE terminology to facilitate the understanding of
this task: Event trigger: the main word or phrase
that most clearly expresses the occurrence of an
event. Event arguments: the mentions that are in-
volved in an event (viz., participants). Event men-
tion: a phrase or sentence within which an event
is described, including a trigger and arguments.

Given an English text document, an ED system
should identify event triggers and categorize their
event types for each sentence. For instance, in the
sentence â€œHe died in the hospitalâ€, an ED system
is expected to detect a Die event along with the
trigger word â€œdiedâ€. The ACE 2005 evaluation
defines 8 event types and 33 subtypes, such as At-
tack or Die. Following previous works (Li et al.,
2013; Chen et al., 2015; Liu et al., 2017; Nguyen
and Grishman, 2016), we categorize triggers into
these 33 subtypes.

3 Methodology

In this paper, we formulate event detection as a
sequence labelling task. As shown in Figure 2,

3Our source code, including all hyper-parameter settings
and pre-trained word embeddings, is openly available at
https://github.com/yubochen/NBTNGMA4ED

we label all words in one sentence collectively via
a Hierarchical and Bias Tagging Networks with
Gated Multi-level Attention Mechanisms (HBT-
NGMA). We assign a tag for each word to indi-
cate whether it triggers a specific type of event.
We adopt the â€œBIOâ€ tags schema, where tag â€œOâ€
represent the â€œotherâ€ tag which means that the cor-
responding word does not trigger any event, tags
â€œB-EventTypeâ€ and â€œI-EventTypeâ€ represent the
â€œBegin-EventTypeâ€ and â€œInside-EventTypeâ€ tag
respectively. â€œEventTypeâ€ means that the word
triggers a specific type of event. â€œBâ€ and â€œIâ€ rep-
resent the position of the word in a trigger to solve
the problem that a trigger word contains multi-
ple words such as â€œtake overâ€, â€œgo offâ€ and so
on. Thus, the total number of tags is Nt = 2 â‡¤
|NeventType|+1, where |NeventType| is the size of
the predefined event types and |NeventType| = 33
in this paper as stated above.

Figure 2 describes the architecture of HBT-
NGMA, which primarily involves the following
four components: (i) embedding layer, which
transforms each word into a continuous vector; (ii)
BiLSTM layer, which uses a Bidirectional Long
Short Term Memory (BiLSTM) to encode the se-
mantics of each word considering the forward and
backward information; (iii) gated multi-level at-
tention, in which we propose a sentence-level and
document-level attention to automatically extract
sentence-level and document-level information re-
spectively, and we devise a fusion gate to dy-
namically integrate them as context information;
and (iv) Hierarchical tagging layer, in which we
propose two Tagging LSTM (TLSTM1 and TL-
STM2) and a tagging attention to automatically
capture the event interdependency and tag all the
words of the sequence collectively.

3.1 Embedding Layer

This paper uses the learned word embeddings as
the source of basic features. Specifically, we use
the Skip-gram model (Mikolov et al., 2013) to
learn word embeddings on the NYT corpus.

Given a document d = {s1, s2, ..., si, ..., sN
s

},
where Ns is the number of sentences in the doc-
ument. The i-th sentence si can be represented
as token sequence si = {w1, w2, ..., wt, ..., wN

w

},
where Nw is length of the sentence and wt is the
t-th token of the sentence. Assume that the word
embedding for token wt is et and we use it as the
input of the following layer.



1270

BiLSTM

S-Attention

TLSTM1

ğ‘‡21

â„2

ğ‘ â„2

Fusion Gate

ğ‘’2

TLSTM2

ğ‘‡22

T-Attention

xr2
ğ‘ğ‘Ÿ2

ğ‘’2

BiLSTM

S-Attention

TLSTM1

ğ‘‡11

â„1

ğ‘ â„1

Fusion Gate

ğ‘’1

TLSTM2

ğ‘‡12

T-Attention

xr1
ğ‘ğ‘Ÿ1

ğ‘’1

BiLSTM

S-Attention

TLSTM1

ğ‘‡31

â„3

ğ‘ â„3

Fusion Gate

ğ‘’3

TLSTM2

ğ‘‡32

T-Attention

xr3
ğ‘ğ‘Ÿ3

ğ‘’3

BiLSTM

S-Attention

TLSTM1

ğ‘‡41

â„4

ğ‘ â„4

Fusion Gate

ğ‘’4

TLSTM2

ğ‘‡42

T-Attention

xr4
ğ‘ğ‘Ÿ4

ğ‘’4

BiLSTM

S-Attention

TLSTM1

ğ‘‡51

â„5

ğ‘ â„5

Fusion Gate

ğ‘’5

TLSTM2

ğ‘‡52

T-Attention

xr5
ğ‘ğ‘Ÿ5

ğ‘’5

BiLSTM

S-Attention

TLSTM1

ğ‘‡61

â„6

ğ‘ â„6

Fusion Gate

ğ‘’6

TLSTM2

ğ‘‡62

T-Attention

xr6
ğ‘ğ‘Ÿ6

ğ‘’6

BiLSTM

S-Attention

TLSTM1

ğ‘‡71

â„7

ğ‘ â„7

Fusion Gate

ğ‘’7

TLSTM2

ğ‘‡72

T-Attention

xr7
ğ‘ğ‘Ÿ7

ğ‘’7

O B-Die O O O B-Attack

â€¦ cameraman                  died when                           an                       American                     tank               fired       â€¦       (Sentence i)

O

D-Attention

Document

BiLSTM Layer

Embedding Layer

Input 

Gated Multi-level
Attention

Hierarchical 
Tagging  Layer

1ST Tagging Layer

2nd  Tagging Layer

ğ‘‘â„ğ‘–

Figure 2: The architecture of our proposed hierarchical and bias tagging networks with gated multi-level attention.

3.2 BiLSTM Layer
In sequence labelling problems, the BiLSTM has
been proven effective to capture the semantic in-
formation of each word (Lample et al., 2016). In
this paper, we use the LSTM unit as described in
(Zaremba and Sutskever, 2014). For each word
wt, the forward LSTM encodes wt by consider-
ing the contextual information from word w1 to
wt, which is marked as

ï¿½!
h t. Similarly, the back-

ward LSTM encodes wt based on the contextual
information from wN

w

to wt, which is marked as ï¿½
h t. Finally, we concatenate

ï¿½!
h t and

 ï¿½
h t to rep-

resent the information of the word wt, denoted as
ht = [

ï¿½!
h t,
 ï¿½
h t], and we concatenate

ï¿½!
h N

w

and
 ï¿½
h 1

to represent the encoding information of the whole
sentence si, denoted as hs

i

= [
ï¿½!
h N

w

,
 ï¿½
h 1].

3.3 Gated Multi-level Attention
Gated multi-level attention primarily involves the
following three components: (i) sentence-level at-
tention layer, which automatically captures im-
portant sentence-level information by consider-
ing the current word; (ii) document-level atten-
tion layer, which automatically captures impor-
tant document-level information by considering
the current sentence; and (iii) fusion gate layer,
which use a fusion gate to dynamically integrate
sentence-level and document-level information.

Sentence-level Attention Layer
Sentence-level attention layer aims to capture the
important clues in sentence level. For each can-
didate word wt in the sentence, its sentence-level

semantic information sht is calculated as follows:

sh
t

=

X
N

w

k=1
â†µk
s

h
k

(1)

where â†µks is the weight of each word representa-
tion hk. In this paper, we define â†µks as following:

â†µk
s

=

exp(zk
s

)

P
N

w

j=1 exp(z
j

s

)

(2)

where zks is the relatedness between the t-th word
representation ht and the k-th word representation
hk, modeled by bilinear attention as:

zk
s

= tanh(h
t

W
sa

hT
k

+ b
sa

) (3)

where Wsa is the weight matrix and bsa is the
bias term. Following above sentence-level atten-
tion mechanism, we can get the sentence-level in-
formation for each word wt by considering the se-
mantic information of the word wt.

Document-level Attention Layer
Similar to sentence-level attention, document-
level attention captures the vital clues in the doc-
ument level. The document-level semantic infor-
mation dhi for i-th sentence calculated as follows:

dh
i

=

X
N

s

k=1
â†µk
d

h
s

k

â†µk
d

=

exp(zk
d

)

P
N

s

j=1 exp(z
j

d

)

zk
d

= tanh(h
s

i

W
da

hT
s

k

+ b
da

)

(4)

where â†µkd is the weight of each sentence repre-
sentation hs

k

, zkd is the relatedness between i-th



1271

sentence representation hs
i

and the k-th sentence
representation hs

k

, Wda is the weight matrix and
bda is the bias term. Compared with sentence-level
information, all words in the i-th sentence have the
same document-level information dhi.

Fusion Gate Layer
We devise a fusion gate to dynamically integrate
sentence-level information sht and document-
level information dhi for the t-th word wt in the
i-th sentence si, and calculate the contextual in-
formation representation crt as follows:

cr
t

= (G
t

ï¿½ sh
t

) + ((1ï¿½G
t

)ï¿½ dh
i

) (5)

where Gt is a fusion gate aims to model the con-
fidence of clues provided by sentence-level infor-
mation sht and document-level information dhi,
which is calculated as follows:

G = ï¿½(W
g

[sh
t

, dh
i

] + b
g

) (6)

where Wg is the weight matrix and bg is the bias
term, ï¿½ is a sigmoid function and ï¿½ denotes
element-wise multiplication. Finally, the contex-
tual information crt of word wt and its word em-
bedding et are concatenated into a single vector
xrt = [et, crt] as the feature representation of wt.

3.4 Hierarchical Tagging Layer
In hierarchical tagging layer, we propose two Tag-
ging LSTMs (TLSTM1 and TLSTM2) and a tag-
ging attention to automatically capture the event
interdependency and tag the sequence collectively.

The First Tagging Layer: TLSTM1
When detecting the tag of word wt in TLSTM1,
the inputs are: the feature representation xrt ob-
tained from embedding layer and gated multi-level
attention layer, former predicted tag vector T 1tï¿½1,
and former hidden vector h1tï¿½1 in TLSTM1. The
detail operations are defined as follows:

i1
t

= ï¿½(W 1
i

x

xr
t

+W 1
i

h

h1
tï¿½1 +W

1
i

T

T 1
tï¿½1)

f1
t

= ï¿½(W 1
f

x

xr
t

+W 1
f

h

h1
tï¿½1 +W

1
f

T

T 1
tï¿½1)

o1
t

= ï¿½(W 1
o

x

xr
t

+W 1
o

h

h1
tï¿½1 +W

1
o

T

T 1
tï¿½1)

u1
t

= '(W 1
u

x

xr
t

+W 1
u

h

h1
tï¿½1 +W

1
u

T

T 1
tï¿½1)

c1
t

= i1
t

ï¿½ u1
t

+ f1
t

ï¿½ c1
tï¿½1

h1
t

= o1
t

ï¿½ '(c1
t

)

T 1
t

= W 1
T

h1
t

+ b1
T

(7)

Where it is an input gate, ut is an input modulation
gate, ft is a forget gate, ot is an output gate, ct is a
memory cell and T 1t is a predicted tagging vector.

The Second Tagging Layer: TLSTM2
Though the TLSTM1 can capture the interdepen-
dency between current event candidate and the for-
mer predicted event tags, it can not capture the
interdependency between the current event candi-
date and the later predicted event tags. Thus, we
devise a second tagging layer (TLSTM2) upon the
LSTM1 to capture the interdependency between
the current event candidate and both of former and
later predicted event tags from TLSTM1. When
detecting the tag of word wt in TLSTM2, the in-
puts are: the feature representation xrt, former
predicted tag vector T 2tï¿½1 in TLSTM2, the pre-
liminary predicted information T at calculated from
TLSTM1, and former hidden vector h2tï¿½1 in TL-
STM2. The detail operations are defined as fol-
lows:

i2
t

= ï¿½(W 2
i

x

xr
t

+W 2
i

h

h2
tï¿½1 +W

2
i

T

T 2
tï¿½1 +W

2
i

a

T a
t

)

f2
t

= ï¿½(W 2
f

x

xr
t

+W 2
f

h

h2
tï¿½1 +W

2
f

T

T 2
tï¿½1 +W

2
f

a

T a
t

)

o2
t

= ï¿½(W 2
o

x

xr
t

+W 2
o

h

h2
tï¿½1 +W

2
o

T

T 2
tï¿½1 +W

2
o

a

T a
t

)

u2
t

= '(W 2
u

x

xr
t

+W 2
u

h

h2
tï¿½1 +W

2
u

T

T 2
tï¿½1 +W

2
u

a

T a
t

)

c2
t

= i2
t

ï¿½ u2
t

+ f2
t

ï¿½ c2
tï¿½1

h2
t

= o2
t

ï¿½ '(c2
t

)

T 2
t

= W 2
T

h2
t

+ b2
T

(8)

The unit structure of TLSTM2 is similar to the
unit of TLSTM1. The parts need to pay attention
are follows: (1) the initial hidden input h20 of the
TLSTM2 is the last hidden vector h1N

w

of the TL-
STM1. (2) the preliminary predicted information
T at is calculated from TLSTM1 by using a tagging
attention as follows.

Tagging Attention
Tagging attention aims to automatically encode
the preliminary predicted information T at for the
word wt and the details are as follows:

T a
t

=

X
N

w

k=1
â†µk
T

T 1
k

â†µk
T

=

exp(zk
T

)

P
N

w

j=1 exp(z
j

T

)

zk
T

= tanh(T 1
t

W
ta

(T 1
k

)

T

+ b
ta

)

(9)

where â†µkT is the weight of each preliminary pre-
dicted tag T 1k , z

k
T is the relatedness between t-th

preliminary predicted tag T 1t and the k-th prelim-
inary predicted tag T 1k , Wta is the weight matrix
and bta is the bias term.

The final normalized tag probability for word
wt is based on the predicted tag vector T 2t from



1272

TLSTM2 and computed as follows:

O
t

= W
y

T 2
t

+ b
y

p(Oi
t

|s
j

, âœ“) =
exp(Oi

t

)

P
N

t

k=1 exp(O
k

t

)

(10)

where p(Oit|sj , âœ“) is the probability that assigning
the i-th tag to word wt in sentence sj when param-
eters is âœ“, and Nt is the total number of tags.

3.5 Training with Bias Objective Function
In one sentence, the number of â€œOâ€ tags is much
more than the number of trigger tags. Thus, we
devise a bias objective function J(âœ“) to reinforce
the influence of trigger tags on the model, which
is defined as follows:

J(âœ“) = max
X

N

ts

j=1

X
N

w

t=1
(log p(Oyt

t

|s
j

, âœ“) Â· I(O)

+ â†µ log p(Oyt
t

|s
j

, âœ“) Â· (1ï¿½ I(O)))
(11)

where Nts is the number of training sentences, Nw
is the length of sentence sj , p(O

y
t

t |sj , âœ“) is the nor-
malized probabilities of tags defined in Formula
10 and yt is the golden tag of word wt in sentence
sj , â†µ is the bias weight and the larger â†µ will bring
the greater influence of trigger tags on the model.
Besides, I(O) is a switching function to distin-
guish the loss of tag â€œOâ€ and trigger tags, which
is defined as follows:

I(O) =

â‡¢
1, if tag = â€œOâ€
0, if tag 6= â€œOâ€ (12)

To compute the network parameter âœ“, we maxi-
mize the log likelihood J (âœ“) through stochastic
gradient descent over shuffled mini-batches with
the Adadelta (Zeiler, 2012) update rule.

4 Experiments

4.1 Experimental Setting
Dataset and Evaluation Metrics
We conduct experiments on the widely used ACE
2005 dataset. For comparison, as the same as pre-
vious works (Liao and Grishman, 2010; Li et al.,
2013; Chen et al., 2015; Nguyen et al., 2016; Liu
et al., 2017), we used the same test set with 40
documents and the same development set with 30
documents and the rest 529 documents are used
for training. Finally, we use Precision (P ), Recall
(R) and F measure (F1) as the evaluation metrics
as the same as previous work.

Hyper-parameter Setting
Hyper-parameters are tuned on the development
dataset by grid search. We train the word embed-
ding using Skip-gram algorithm 4 on the NYT cor-
pus 5. We set the dimension of word embeddings
as 100, the dimension of tag vector as 20, all the
size of LSTM in BiLSTM layer, TLSTM1 and TL-
STM2 layer as 100, the bias parameter â†µ in For-
mula 11 as 5, the batch size as 20, the learning rate
as 0.001, the dropout rate as 0.5.

4.2 Our Method vs. State-of-the-art Methods
We select the following state-of-the-art methods
for comparison, which can be classified as two
types: separate and collective methods:

Separate methods: 1) Liâ€™s MaxEnt: the
method that detects events in one sentence sepa-
rately by using human-designed features (Li et al.,
2013). 2) Liaoâ€™s CrossEvent : the method that
uses cross event information (Liao and Grish-
man, 2010). 3) Hongâ€™s CrossEntity: the method
that uses cross entity information (Hong et al.,
2011). 4) Chenâ€™s DMCNN: the dynamic multi-
pooling convolutional neural networks method
(Chen et al., 2015). 5) Chenâ€™s DMCNN+:
the DMCNN method argumented with automati-
cally labeled data (Chen et al., 2017). 6) Liuâ€™s
FrameNet : the method that leverages FrameNet
as extended training data to improve ED (Liu et al.,
2016a). 7) Liuâ€™s ANN-Aug: the method that use
the annotated argument information via a super-
vised attention to improve ED (Liu et al., 2017).

Collective methods: 1) Liâ€™s Structure: the
method that collectively detects events by us-
ing human-designed features (Li et al., 2013).
2) Yangâ€™s JointEE: the method that detects
events and entities in one sentence jointly based
on human-designed features (Yang and Mitchell,
2016). 3) Nguyenâ€™s JRNN: the method that ex-
ploits a RNN model to collectively detects events
by only using sentence-level information (Nguyen
et al., 2016). 4) Liuâ€™s PSL : the method that uses
a probabilistic soft logic to detect events by using
human-designed features (Liu et al., 2016b).

Experimental results are shown in Table 1.
From the table, we have the following observa-
tions: (1) Among all the methods, our HBT-
NGMA achieves the best performance. It can
improve the best collective methodâ€™s F1 by

4https://code.google.com/p/word2vec/
5https://catalog.ldc.upenn.edu/LDC2008T19



1273

Methods P R F1
Liâ€™s MaxEnt (2013) 74.5 59.1 65.9

Liaoâ€™s CrossEvent (2010) 68.7 68.9 68.8
Hongâ€™s CrossEntity (2011) 72.9 64.3 68.3

Chenâ€™s DMCNN (2015) 75.6 63.6 69.1
Chenâ€™s DMCNN+ â€  (2017) 75.7 66.0 70.5
Liuâ€™s FrameNet â€  (2016a) 77.6 65.2 70.7
Liuâ€™s ANN-Aug â€  (2017) 76.8 67.5 71.9

Liâ€™s Structure (2013) 73.7 62.3 67.5
Yangâ€™s JointEE (2016) 75.1 63.3 68.7
Nguyenâ€™s JRNN (2016) 66.0 73.0 69.3

Liuâ€™s PSL (2016b) 75.3 64.4 69.4
Ours HBTNGMA 77.9 69.1 73.3

Table 1: Overall performance on blind test data. The
upper table illustrates the performance of separate ED
systems and the lower illustrates collective ED systems.
â€  means the method that uses external resources.

3.9% (Liuâ€™s PSL) and improve the best sepa-
rate methodâ€™s F1 by 1.4% (Liuâ€™s ANN-Aug) al-
though Liuâ€™s ANN-Aug uses FrameNet as exter-
nal resources. We also perform a t-test (p 6
0.05), which indicates that our method signifi-
cantly outperforms all of the compared methods.
(2) Comparing our HBTNGMA to separate meth-
ods, it achieves a better performance. It proves
that collectively predicting multiple events in one
sentence is effective. (3) Our HBTNGMA out-
performs feature-based collective methods (Liâ€™s
Structure, Yangâ€™s JointEE and Liuâ€™s PSL), it
proves that our automatically learned features
can efficiently capture semantic information from
plain texts. (4) Compared with Nguyenâ€™s JRNN,
HBTNGMA gains a 4.0% improvement on F1
value. The reason is that Nguyenâ€™s JRNN only
uses sentence-level information while our model
exploits multi-level information, and our model
can capture the interdependencies between the
current event candidate and its former/later pre-
dicted events simultaneously.

4.3 Effect of The Hierarchical and Bias
Tagging Networks

In this subsection, we prove the effectiveness of
hierarchical and bias tagging networks for collec-
tive ED. We select following methods as base-
lines: 1) LSTM+Softmax: a simplified version
of our HBTNGMA, which directly use a soft-
max layer to separately detect events after we
get the feature representation xrt of each word
wt. 2) LSTM+CRF: the method is similar to
our HBTNGMA, which uses a CRF layer to tag
words instead of our Hierarchical TLSTM (HTL-
STM) tagging layer. 3) LSTM+TLSTM: the
method is similar to our HBTNGMA, which only

use a TLSTM1 and takes all tags have same in-
fluence in training loss (i.e. â†µ in is set as 1)
. 4) LSTM+HTLSTM: the method is similar
to our HBTNGMA, which use a HTLSTM (TL-
STM1+TLSTM2) and do not use bias objective
function. And LSTM+HTLSTM+Bias is our pro-
posed HBTNGMA. Moreover, we divide the test-
ing data into two parts according the event num-
ber in a sentence (single event and multiple events)
and perform evaluations separately.

Method 1/1 1/N all
LSTM+Softmax 74.7 44.6 66.8

LSTM+CRF 75.1 49.5 68.5
LSTM+TLSTM 76.8 51.2 70.2

LSTM+HTLSTM 77.9 57.3 72.4
LSTM+HTLSTM+Bias 78.4 59.5 73.3

Table 2: Performance of different ED systems. 1/1
means one sentence that only has one event and 1/N
means that one sentence has multiple events.

Table 2 shows the results. And we have
the following observations: 1) Compared
with LSTM+Softmax, LSTM-based collective
ED methods (LSTM+CRF, LSTM+TLSTM,
LSTM+HTLSTM, LSTM+HTLSTM+Bias) achie-
ves a better performance. Surprisingly, the
LSTM+HTLSTM+Bias yields a 14.9% improve-
ment on the sentence contains multiple events
over the LSTM+Softmax. It proves neural tagging
schema is effective for ED task especially for
the sentences contain multiple events. 2) The
LSTM+TLSTM achieve better performances than
LSTM+CRF. And the LSTM+HTLSTM achieve
better performances than LSTM+TLSTM. The
results prove the effectiveness of the TLSTM
layer and HTLSTM layer. 3) Compared with
LSTM+HTLSTM, the LSTM+HTLSTM+Bias
gains a 0.9% improvement on all sentence. It
demonstrates the effectiveness of our proposed
bias objective function.

4.4 Effect of The Gated Multi-level Attention
This subsection studies the effectiveness of our
gated multi-level attention. We adopt same archi-
tecture of our HBTNGMA as shown in Figure 2
with different level clues as baselines: 1) Word
Only is the method only uses word embedding et
to identify events. 2) Word+SA uses sentence-
level attention to capture important sentence-level
information as additional clues. 3) Word+DA
uses document-level attention to capture important
document-level information as additional clues. 4)
Word+Average MA uses both of sentence-level



1274

and document-level attention to capture multi-
level information and integrate them with a aver-
age gate (all the dimension of the fusion gate are
set as 0.5 ), which is a special case of our pro-
posed HBTNGMA. And Word+Gated MA is our
proposed HBTNGMA model.

Method P R F1
Word Only 70.1 63.4 66.6
Word+SA 75.6 68.2 71.7
Word+DA 73.1 65.8 69.3

Word+Average MA 76.5 68.7 72.4
Word+Gated MA 77.9 69.1 73.3

Table 3: Performance of gated multi-level attention.

Results are shown in Table 3. From the re-
sults, we have the following observations: 1)
Compared with word only, Word+SA achieves a
better performance. We can make the same ob-
servation when comparing Word+DA with word
only. It proves that both sentence-level and
document-level information are helpful for ED
task. 2) Compared with Word+DA, Word+SA
achieves a better performance. It proves that in
most of cases sentence-level information provides
more clues than document-level information. 3)
Word+Gated MA gains a 0.9% improvement than
Word+Average MA. It demonstrates that the effec-
tiveness of our fusion gate to dynamically inte-
grate clues from multiple levels.

4.5 Case Study
Interesting Cases: Our neural tagging schema not
only can model the interdependency between mul-
tiple events in one sentence as proved in Subsec-
tion 4.3, but also the â€œBIOâ€ tagging schema can
solve the multiple words trigger inherently. We
conduct a statistical analysis on the experimental
results, and find that nearly 50% cases with multi-
ple word trigger was solved by our model. Exam-
ple is shown in Figure 3.

Early Saturdayï¼Œmore units were waiting in Kuwait to    smash through    any Iraqi resistance. 
B-Attack I-AttackO O O O O O O O O O O O

Figure 3: The example of case solved by our model.

Attention Visualization: As limited of space,
we take one sentence with high sentence-level
gated weight (example 1) and one sentence with
high document-level gated weight (example 2) as
examples for attention visualization. As shown in
Figure 4, in example 1, sentence-level informa-
tion plays more important role in disambiguating
fired, and the words (tank, died and Baghdad) give

Sentence Document

Sentence Document

Example 1 (sentence-level attention) ï¼š

Example 2 (document-level attention) ï¼š

In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.

â€¦ this is when you were in the Senate --â€œless and less information was new, 
fewer and fewer arguments were fresh,and the repetitiveness of the old arguments 
became tiresome.â€ â€œI was becoming almost as cynical as my constituents. I knew 
it was time to leave." Isn't that a great argument for term limits? 

Figure 4: Attention visualization. The heat map indi-
cate the contextual attention. Blue for sentence-level
and orange for document-level. The pie chart indicate
the fusion gate weight.

us ample evidence to predict that fired triggers an
Attack event. While document-level information
plays a more important role in example 2. The
surrounding sentence â€œthis is ... tiresome.â€ gives
us more confidence to predict that leave triggers
an End-Position event.

5 Related Works

Event detection is an increasingly hot and chal-
lenging research topic in NLP. Generally, exist-
ing approaches could roughly be divided into two
groups: separate and collective methods.

Separate methods: These methods regard mul-
tiple events in one sentence as independent ones
and recognize them separately. These meth-
ods include feature-based methods which exploit
a diverse set of strategies to convert classifica-
tion clues into feature vectors (Ahn, 2006; Ji
and Grishman, 2008; Liao and Grishman, 2010;
Hong et al., 2011; Huang and Riloff, 2012), and
neural-based methods which use neural networks
to automatically capture clues from plain texts
(Chen et al., 2015; Nguyen and Grishman, 2015;
Feng et al., 2016; Nguyen and Grishman, 2016;
Chen et al., 2017; Duan et al., 2017; Liu et al.,
2017). Though effective these methods, they ne-
glect event interdependency by separately predict-
ing each event.

Collective methods: These methods try to
model the event interdependency and detect mul-
tiple events in one sentence collectively. How-
ever, nearly all of these methods are feature-based
methods (McClosky et al., 2011; Li et al., 2013;
Yang and Mitchell, 2016; Liu et al., 2016b), which
rely on elaborately designed features and suffer er-
ror propagation from existing NLP tools. Nguyen
et al. (2016) exploits a neural-based method to de-
tect multiple events collectively. However, they
only use the sentence-level information and ne-



1275

glect document-level clues, and can only capture
the interdependencies between the current event
candidate and its former predicted events. More-
over, there method can not handle the multiple
words trigger problem.

6 Conclusion

This paper proposes a novel framework for event
detection, which can automatically extract and dy-
namically integrate sentence-level and document-
level information and collectively detect multi-
ple events in one sentence. A hierarchical and
bias tagging networks is proposed to detect mul-
tiple events in one sentence collectively. A gated
multi-level attention is devised to automatically
extract and dynamically integrate contextual infor-
mation. The experimental results on the widely
used dataset prove the effectiveness of the pro-
posed method.

Acknowledgments

The research work is supported by the Natural
Science Foundation of China (No.61533018 and
No.61702512), and the independent research
project of National Laboratory of Pattern Recog-
nition. This work is also supported in part by
Huawei Technologies Co., Ltd.

References
David Ahn. 2006. The stages of event extraction. In

Proceedings of 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1â€“
8.

Yubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and
Jun Zhao. 2017. Automatically labeled data genera-
tion for large scale event extraction. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 409â€“419,
Vancouver, Canada. Association for Computational
Linguistics.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dynamic
multi-pooling convolutional neural networks. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistic (ACL), pages
167â€“176.

Shaoyang Duan, Ruifang He, and Wenli Zhao. 2017.
Exploiting document level information to improve
event detection via recurrent neural networks. In
Proceedings of IJNLP, volume 1, pages 352â€“361.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
Bing Qin, and Ting Liu. 2016. A language-
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume 2, pages 66â€“71.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1127â€“1136.

Ruihong Huang and Ellen Riloff. 2012. Modeling tex-
tual cohesion for event extraction. In Proceedings of
AAAI.

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
254â€“262.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL, pages 260â€“270.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 73â€“82.

Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 789â€“797.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and Jun
Zhao. 2016a. Leveraging framenet to improve auto-
matic event detection. In Proceedings of ACL, pages
2134â€“2143. Association for Computational Linguis-
tics.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1789â€“1798.

Shulin Liu, Kang Liu, Shizhu He, and Jun Zhao. 2016b.
A probabilistic soft logic based approach to exploit-
ing latent and global information in event classifica-
tion. In Proceedings of AAAI, pages 2993â€“2999.

David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1626â€“1635.



1276

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Huu Thien Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistic (ACL), pages 365â€“371.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-
ishman. 2016. Joint event extraction via recurrent
neural networks. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
300â€“309.

Thien Huu Nguyen and Ralph Grishman. 2016. Mod-
eling skip-grams for event detection with convolu-
tional neural networks. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 886â€“891.

Bishan Yang and Tom M. Mitchell. 2016. Joint extrac-
tion of events and entities within a document con-
text. In Proceedings of NAACL, pages 289â€“299, San
Diego, California. Association for Computational
Linguistics.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.

Matthew D Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.


