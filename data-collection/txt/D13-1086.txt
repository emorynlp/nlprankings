










































Automatic Domain Partitioning for Multi-Domain Learning


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 869–873,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Automatic Domain Partitioning for Multi-Domain Learning

Di Wang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
diwang@cs.cmu.edu

Chenyan Xiong
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA

cx@cs.cmu.edu

William Yang Wang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA

ww@cmu.edu

Abstract

Multi-Domain learning (MDL) assumes that
the domain labels in the dataset are known.
However, when there are multiple metadata at-
tributes available, it is not always straightfor-
ward to select a single best attribute for do-
main partition, and it is possible that combin-
ing more than one metadata attributes (includ-
ing continuous attributes) can lead to better
MDL performance. In this work, we propose
an automatic domain partitioning approach
that aims at providing better domain identi-
ties for MDL. We use a supervised clustering
approach that learns the domain distance be-
tween data instances , and then cluster the data
into better domains for MDL. Our experiment
on real multi-domain datasets shows that us-
ing our automatically generated domain parti-
tion improves over popular MDL methods.

1 Introduction

Instead of assuming data are i.i.d, Multi-domain
learning (MDL) methods assumes that data come
from several domains and make use of domain la-
bels to improve modeling performance (Daumé III,
2007). The motivation of using MDL is that datasets
from different domains could be different, in two
ways. First, the feature distribution p(x) could be
domain specific, meaning that the importance of
each feature is different across domains. Second,
the distribution of label Y given X , p(y|x), of dif-
ferent domains could be different. These differ-
ences could create problems for traditional machine
learning methods: models learned from one domain

might not be generalizable to other domains (Ben-
David et al., 2006; Ben-David et al., 2010).

One common assumption of MDL methods is that
the domain identities are pre-defined. For example,
in the multi-domain Amazon product review dataset
(Finkel and Manning, 2009), the product categories
are typically used as the domain identities. How-
ever, a question raised by Joshi et al. (2012) is that,
in real-world data sets, there could be many ways to
split data into domains, and it is hard to decide which
one to use. Consider the Amazon product reviews,
where we have multiple attributes attached to each
review: for example, product category, reviewer lo-
cation, price, and number of feedback. Which at-
tribute is the most informative domain label? Or we
should use all of these meta-data and partition the
data into many small domains?

In this paper, we investigate the problem of au-
tomatic domain partitioning. We propose an em-
pirical domain difference testing method to exam-
ine whether two groups of data are i.i.d, or gener-
ated from different distributions, and how different
they are. Using this approach, we generate data pairs
that belong to the same distribution, and data pairs
that should be partitioned into different domains.
These pairs are then used as training data for a super-
vised clustering algorithm, which automatically par-
titions the dataset into several domains. In the eval-
uation, we show that our automatically-partitioned
domains improve the performances of two popular
MDL methods on real sentiment analysis data sets.

Note that Joshi et al. (2013) proposed a Multi-
Attribute Multi-Domain learning (MAMD) method,
which also exploited multiple dimensions of meta-

869



data and provided extensions to two traditional MDL
methods. However, extensions to the MAMD set-
ting may not be trivial for every MDL algorithm,
while our method serves as a pre-processing step and
can be easily used for all MDL approaches. In ad-
dition to this, MAMD only works with categorical
metadata, and can not fully utilize information in the
form of continuous metadata values.

2 Automatic Domain Partitioning

In this section, we introduce the Automatic Domain
Partitioning (ADP) problem: given data X , meta-
data M and label Y , find a function g : M 7→ I such
that the common MDL methods perform better with
data X and domain identity I in the prediction of Y .
For example, on Amazon sentiment analysis data, X
is the feature matrix extracted from reviews, Y is the
positive or negative label vector, and M is the meta-
data matrix associated with reviews (e.g. product
price and category).

Our approach works as follows: in training, we
first use an empirical domain difference testing
method to detect whether two groups of data should
be considered as different domains; after that we ap-
ply supervised clustering to learn the distance met-
ric between two data points, i.e. how different thay
are in MDL view, from training data generated by
our domain difference test method; finally, based on
the distance metric learned, we cluster our data into
several clusters, and train MDL models with those
clusters as domain labels; in testing, we assign data
instance to its nearest cluster and use that cluster as
its domain identity, and then apply the trained MDL
models for prediction.

2.1 Empirical Domain Difference Test

The key motivation of MDL is that a model fits for
one domain may not fit well for other domains. Fol-
lowing the same motivation, we propose an empiri-
cal method for domain difference test called Domain
Model Loss (DML) that provides us the domain dif-
ference score d(G1, G2) between two groups of data
G1 = {X1, Y1} and G2 = {X2, Y2}.

Domain Model Loss If the mapping functions f1 :
X1 7→ Y1 and f2 : X2 7→ Y2 are different for
two data groups, we could directly use the disagree-
ment of f1 and f2 as domain difference score. More

specifically, if we train two classifiers f̂1 : X1 7→
Y1, f̂2 : X2 7→ Y2 individually on G1 and G2, we
could have the K-fold empirical loss:

l̂(f1, G1) =
1

K

∑
i

Error of f1 on i-th fold of G1,

l̂(f2, G2) =
1

K

∑
i

Error of f2 on i-th fold of G2.

And we could also apply the trained model f1 on
G2, and f2 on G1 to get:

l̂(f1, G2) = Error of f1 on G2,

l̂(f2, G1) = Error of f2 on G1.

Then, if G1 and G2 are actually the same with each
other, then both models will have same empirical
loss on either data set, but if they are not, we will
have a positive DML score:

DML(G1, G2) =
1

2
(L̂(f1, G2) + L̂(f2, G1)),

where:

L̂(f1, G2) =
l̂(f1, G2)− l̂(f1, G1)

l̂(f1, G1)
,

L̂(f2, G1) =
l̂(f2, G1)− l̂(f2, G2)

l̂(f2, G2)
.

2.2 Supervised Clustering for Domain
Partitioning

Our domain difference test method calculates the
distance between two partitioned data groups. How-
ever, to directly use it for domain partitioning, we
must go through all possible combinations of do-
main assignments in exponential time, which is in-
feasible. Our solution is to use a polynomial-time
supervised clustering method developed by Xing et
al. (2002) to learn a distance function that calculates
the distance between any two data points. Formally,
given a set of data pairs D, which belong to different
domains, and a set of data pairs S, which belong to
the same domain, it learns a distance metric A by:

max
A

g(A) =
∑

(i,j)∈D

√
(mi −mj)T A(mi −mj)

s.t.f(A) =
∑

(i,j)∈S

(mi −mj)T A(mi −mj) ≤ 1

A � 0,

870



where mi, mj are meta data of i and j.
The metadata M are preprocessed as follows: 1)

Each categorical attribute was converted to several
binary questions, one per category, and each bi-
nary question was considered as one metadata di-
mension in ADP method. For example, if categor-
ical attribute “Product Type” has two values “Mu-
sic” and “Electronics”, then there will be two dimen-
sions of metadata corresponding to “Product Type”
in ADP. Two metadata dimensions correspond to bi-
nary questions: “Is Product Type Music” and “Is
Product Type Electronics”. 2) Each continuous at-
tribute was normalized by scaling between 0 and 1.

The training data S, D for metric learning are gen-
erated as follows:

1. For each dimension Mk of M , split data at
value 0.5, sample two equally sized groups, ap-
ply our domain difference testing method and
find the difference between these data groups.

2. Assign distance to each pair of instances by the
average distance of all partitions that partitions
the pair into different groups.

3. Select top n similar pairs as S and top n differ-
ent pairs as D.

The learned distance metric A now conveys the
domain difference information obtained from our
domain distance test results: which meta attributes
are important for domain partitioning and which are
not as important. Following Xing et al. (2002), we
transfer the instance’s metadata feature M by MBT ,
where BT B = A. Then we use a clustering method
on MBT , and the output is our domain partitioning
result.

3 Experiment Methodology

Datasets To evaluate our methods, we used two
subsets of Amazon review corpus (Jindal and Liu,
2008), which originally contain 5.8 million reviews
with a variety of metadata about products and users.
The first subset (BOOK) contains 20,000 reviews on
books published by eleven most popular publishers,
while the second (PROD) is reviews about products
within seven most common product categories. We
randomly split each dataset into training and testing
sets with equal size. The task is to predict a positive
or negative label for each review. Case insensitive

unigrams excluding stop words are used as features,
and all features appear less than 500 times are re-
moved for efficient experiment processing. Reviews
of 4 or 5 stars are considered positive and 1 or 2 stars
are considered negative, while 3 stars reviews are ex-
cluded. Each review has multiple metadata such as
book’s publisher, product’s type, user’s state loca-
tion, product price, review year, and number of other
user feedback. Reviews with missing metadata are
filtered out.

MDL Methods Our first MDL algorithm is the
Frustratingly Easy Domain Adaptation (FEDA)
(Daumé III, 2007) which is easy to implement and
achieved competitive performance on many applica-
tions. It creates an augmented feature space as the
Cartesian product of the input features and the orig-
inal domains plus a shared domain. Then it uses a
SVM classifier over the augmented feature space to
obtain classification result. Specifically, our FEDA
methods use L2-regularized SVM with linear ker-
nel by LIBLINEAR package1. The parameters C =
0.01 was selected using five-fold cross-validation on
training set.

Our second MDL algorithm is Multi-Domain
Regularization (MDR) (Dredze and Crammer,
2008), which is a classifier combination ap-
proach based on Confidence-Weighted (CW) learn-
ing (Dredze et al., 2008). The CW learning is an on-
line update method that maintains probabilistic con-
fidence for each parameter by keeping track of its
variance. In our experiments, we use the CW im-
plementation provided by its authors and choose the
best performing configurations described in (Dredze
and Crammer, 2008).

Domain Partition Methods We evaluated the do-
main partition results provided by our ADP on the
two MDL methods (FEDA & MDR). For simplic-
ity and efficiency, we use Naive Bayes as our base
prediction model f1 and f2 to generate the domain
model loss score, described in section 2.1. In train-
ing data generation, we choose top 10% similar pairs
as S and top 10% different pairs as D. And given
the learnt distance metric A, we use K-means to do
the clustering. The number of clusters is selected by
five-fold cross-validation on training set.

1http://www.csie.ntu.edu.tw/∼cjlin/liblinear

871



We compare our domain partition quality with
three other methods: 1) 1-Best chooses best per-
forming categorical metadata on a validation set as
domain indicators, where the original training set
was splitted equally to train and validate the per-
formance of each categorical attribute; 2) Random
partition that assigns domain identities to instances
randomly with same number of domains as 1-Best.
We run each random partition ten times and took the
average; 3) MAMD proposed by Joshi et al. (2013).
However, the original version of MAMD does not
support continuous attribute such as price. So we
made an extension that sorts these values to ten bins
and then treats them as categorical values.

4 Results and Discussions

Partition + MDL PROD BOOK
ADP + FEDA 82.02 ∗ 86.22 ‡∗
MAMD + FEDA 81.04 86.08
1-Best + FEDA 82.00 85.85
Random + FEDA 79.36 84.72
ADP + MDR 82.10 ] ‡ ∗ 86.62 ] ‡ ∗
MAMD + MDR 80.17 84.37
1-Best + MDR 79.79 83.68
Random + MDR 74.65 81.16

Table 1: Overall accuracies on PROD and BOOK
datasets. ADP results that are statistically significantly
better than MAMD are marked with ], and better than 1-
Best and Random are indicated by ‡ and ∗ respectively,
using a paired t-test, with p < 0.05.

Table 1 shows the overall experimental results
of four domain partition methods with two MDL
methods on PROD and BOOK datasets. One could
see that when using MDR method, ADP could
significantly outperform all baselines on both data
sets, with relatively more than 2% gains. For
FEDA, on PROD data, ADP performs the same with
MAMD and 1-Best; on BOOK data, ADP outper-
forms 1-Best significantly, but is just slightly better
than MAMD. One possible reason is that the best
numbers of cluster selected by cross-validation are
around 150. With such large number of none-perfect
domains, FEDA will generate huge dimension of
features and perhaps require more training data to
provide better performances. Another possible rea-
son is that FEDA and the SVM underlying FEDA

are very robust against bad domain partition results.
This might be the reason of high FEDA baselines.
In general, our ADP method helps existing MDL
approaches achieve better performance, while bad
(Random) partitioning does hurt.

Figure 1(a) and 1(b) shows the performances of
applying FEDA on different domain partitioning
methods on PROD and BOOK, while Figure 1(c)
and 1(d) shows experiment results with MDR. The
x-axis is the size of the output domains (the K in
our K-means clustering), and y-axis is the accuracy
of models. With our domain partitioning approach,
MDR can perform consistently higher than all the
three baselines on both dataset when k > 50. As
we discussed for Table 1, FEDA might be less sen-
sitive to domain partition results, which causes high
baseline performance and high ADP+FEDA perfor-
mance with small K. Since the performance trends
to increase along with k until 50 in three figures
(1(b), 1(c) and 1(d)), we believe that the ground-
truth domain size is likely larger than 50. These
results clearly indicate ADP does provide more de-
sirable domain assignments for MDL. The domain
selected by 1-Best such as publishers has only 11
domains, which limits the ability of 1-Best to com-
pletely express domain information. And our gener-
ated domains integrate multiple metadata attributes,
lead to more detailed domain partitions, and enhance
the ability of MDL methods to capture the difference
between different groups of data. Although accu-
racies are growing with k in general, we also see
that there are fluctuations on curves especially when
curves are zoomed to a small range. To get smoother
results, we can sample more data to calculate do-
main similarity and repeat the K-means clustering
with more different initializations.

5 Conclusions

In this paper, we propose an Automatic Domain Par-
tition (ADP) method that provides better domain
identities for multi-domain learning methods. We
first propose a new approach to identify whether two
data groups should be considered as different do-
mains, by comparing the differences using Domain
Model Loss. We use a supervised clustering ap-
proach to train our model with labels generated by
domain difference tests, and cluster the re-weighted

872



metadata as our domain partition by K-means. Ex-
periments on real world multi-domain data show
that the domain identities generated by our method
can improve the performance of MDL models.

0 50 100 150 200
79

79.5

80

80.5

81

81.5

82

82.5

Number of domains (K)

A
c
c
u
ra

c
y
 %

 

 

ADP
1−Best
Random
MAMD

(a) FEDA results on PROD

0 50 100 150 200
84.5

85

85.5

86

86.5

87

Number of domains (K)

A
c
c
u
ra

c
y
 %

 

 

ADP
1−Best
Random
MAMD

(b) FEDA results on BOOK

0 50 100 150 200
72

74

76

78

80

82

84

Number of domains (K)

A
c
c
u
ra

c
y
 %

 

 

ADP
1−Best
Random
MAMD

(c) MDR results on PROD

0 50 100 150 200
81

82

83

84

85

86

87

Number of domains (K)

A
c
c
u
ra

c
y
 %

 

 

ADP
1−Best
Random
MAMD

(d) MDR results on BOOK

Figure 1: Accuracies over different size of the output do-
mains (K)

References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-

nando Pereira. 2006. Analysis of representations for
domain adaptation. In Advances in Neural Informa-
tion Processing Systems (NIPS), pages 137–144.

Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151–175.

Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 689–697.

Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
Machine Learning, Proceedings of the Twenty-Fifth
International Conference (ICML), pages 264–271.

Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of the 2009 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 602–610.

Hal Daumé III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL).

Nitin Jindal and Bing Liu. 2008. Opinion spam and anal-
ysis. In Proceedings of the International Conference
on Web Search and Web Data Mining (WSDM), pages
219–230.

Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn Penstein Rosé. 2012. Multi-domain learn-
ing: When do domains matter? In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, (EMNLP-CoNLL), pages 1302–
1312.

Mahesh Joshi, Mark Dredze, William W. Cohen, and
Carolyn P. Rosé. 2013. Whats in a domain? multi-
domain learning for multi-attribute data. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (NAACL-HLT),
pages 685–690, Atlanta, Georgia, June. Association
for Computational Linguistics.

Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stu-
art J. Russell. 2002. Distance metric learning with
application to clustering with side-information. In
Advances in Neural Information Processing Systems
(NIPS), pages 505–512.

873


