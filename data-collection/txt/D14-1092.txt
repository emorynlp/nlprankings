



















































A Joint Model for Unsupervised Chinese Word Segmentation


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854–863,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

A Joint Model for Unsupervised Chinese Word Segmentation

Miaohong Chen Baobao Chang Wenzhe Pei
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
Beijing, P.R.China, 100871

miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn

Abstract

In this paper, we propose a joint model for
unsupervised Chinese word segmentation
(CWS). Inspired by the “products of ex-
perts” idea, our joint model firstly com-
bines two generative models, which are
word-based hierarchical Dirichlet process
model and character-based hidden Markov
model, by simply multiplying their proba-
bilities together. Gibbs sampling is used
for model inference. In order to further
combine the strength of goodness-based
model, we then integrated nVBE into our
joint model by using it to initializing the
Gibbs sampler. We conduct our experi-
ments on PKU and MSRA datasets pro-
vided by the second SIGHAN bakeoff.
Test results on these two datasets show
that the joint model achieves much bet-
ter results than all of its component mod-
els. Statistical significance tests also show
that it is significantly better than state-
of-the-art systems, achieving the highest
F-scores. Finally, analysis indicates that
compared with nVBE and HDP, the joint
model has a stronger ability to solve both
combinational and overlapping ambigui-
ties in Chinese word segmentation.

1 Introduction

Unlike English and many other western languages,
there are no explicit word boundaries in Chinese
sentences. Therefore, word segmentation is a cru-
cial first step for many Chinese language process-
ing tasks such as syntactic parsing, information re-
trieval and machine translation. A great deal of su-
pervised methods have been proposed for Chinese
word segmentation. While successful, they re-
quire manually labeled resources and often suffer
from issues like poor domain adaptability. Thus,

unsupervised word segmentation methods are still
attractive to researchers due to its independence on
domain and manually labeled corpora.

Previous unsupervised approaches to word seg-
mentation can be roughly classified into two types.
The first type uses carefully designed goodness
measure to identify word candidates. Popular
goodness measures include description length gain
(DLG) (Kit and Wilks, 1999), accessor variety
(AV) (Feng et al., 2004), boundary entropy (BE)
(Jin and Tanaka-Ishii, 2006) and normalized vari-
ation of branching entropy (nVBE) (Magistry and
Sagot, 2012) etc. Goodness measure based model
is not segmentation model in a very strict mean-
ing and is actually strong in generating word list
without supervision. It inherently lacks capabil-
ity to deal with ambiguous string, which is one of
main sources of segmentation errors and has been
extensively explored in supervised Chinese word
segmentation.

The second type focuses on designing sophis-
ticated statistical model, usually nonparametric
Bayesian models, to find the segmentation with
highest posterior probability, given the observed
character sequences. Typical statistical mod-
els includes Hierarchical Dirichlet process (HDP)
model (Goldwater et al., 2009), Nested Pitman-
Yor process (NPY) model (Mochihashi et al.,
2009) etc, which are actually nonparametric lan-
guage models and therefor can be categorized as
word-based model. Word-based model makes de-
cision on wordhood of a candidate character se-
quence mainly based on information outside the
sequence, namely, the wordhood of character se-
quences being adjacent to the concerned sequence.

Inspired by the success of character-based
model in supervised word segmentation, we pro-
pose a Bayesian HMM model for unsupervised
Chinese word segmentation. With the Bayesian
HMM model, we formulate the unsupervised seg-
mentation tasks as procedure of tagging positional

854



tags to characters. Different from word-based
model, character-based model like HMM-based
model as we propose make decisions on word-
hood of a candidate character sequence based on
information inside the sequence, namely, ability of
characters to form words. Although the Bayesian
HMM model alone does not produce competi-
tive results, it contributes substantially to the joint
model as proposed in this paper.

Our joint model takes advantage from three dif-
ferent models: namely, a character-based model
(HMM-based), a word-based model (HDP-based)
and a goodness measure based model (nVBE
model). The combination of HDP-based model
and HMM-based model enables to utilize infor-
mation of both word-level and character-level. We
also show that using nVBE model as initialization
model could further improve the performance to
outperform the state-of-the-art systems and leads
to improvement in both wordhood judgment and
disambiguation ability.

Word segmentation systems are usually eval-
uated with metrics like precision, recall and F-
Score, regardless of supervised or unsupervised.
Following normal practice, we evaluate our model
and compare it with state-of-the-art systems us-
ing F-Score. However, we argue that the ability
to solve segmentation ambiguities is also impor-
tant when evaluating different types of unsuper-
vised word segmentation systems.

This paper is organized as follows. In Section
2, we will introduce several related systems for
unsupervised word segmentation. Then our joint
model is presented in Section 3. Section 4 shows
our experiment results on the benchmark datasets
and Section 5 concludes the paper.

2 Related Work

Unsupervised Chinese word segmentation has
been explored in a number of previous works and
by various methods. Most of these methods can
be divided into two categories: goodness measure
based methods and nonparametric Bayesian meth-
ods.

There have been a plenty of work that is based
on a specific goodness measure. Zhao and Kit
(2008) compared several popular unsupervised
models within a unified framework. They tried
various types of goodness measures, such as De-
scription Length Gain (DLG) proposed by Kit and
Wilks (1999), Accessor Variety (AV) proposed by

Feng et al. (2004) and Boundary Entropy (Jin and
Tanaka-Ishii, 2006). A notable goodness-based
method is ESA: “Evaluation, Selection, Adjust-
ment”, which is proposed by Wang et al. (2011)
for unsupervised Mandarin Chinese word segmen-
tation. ESA is an iterative model based on a new
goodness algorithm that adopts a local maximum
strategy and avoids threshold setting. One disad-
vantage of ESA is that it needs to iterate the pro-
cess several times on the corpus to get good perfor-
mance. Another disadvantage is the requirement
for a manually segmented training corpus to find
best value for parameters (they called it proper ex-
ponent). Another notable work is nVBE: Mag-
istry and Sagot (2012) proposed a model based
on the Variation of Branching Entropy. By adding
normalization and viterbi decoding, they improve
performance over Jin and Tanaka-Ishii (2006)
and remove most of the parameters and thresholds
from the model.

Nonparametric Bayesian models also achieved
state-of-the-art performance in unsupervised word
segmentation. Goldwater et al. (2009) introduced
a unigram and a bigram model for unsupervised
word segmentation, which are based on Dirichlet
process and hierarchical Dirichlet process (Teh et
al., 2006) respectively. The main drawback is that
it needs almost 20,000 iterations before the Gibbs
sampler converges. Mochihashi et al. (2009) ex-
tended this method by introducing a nested charac-
ter model and an efficient blocked Gibbs sampler.
Their method is based on what they called nested
Pitman-Yor language model.

One disadvantage of goodness measure based
methods is that they do not have any disambigua-
tion ability in theory in spite of their competitive
performances. This is because once the goodness
measure is given, the decoding algorithm will seg-
ment any ambiguous strings into the same word
sequences, no matter what their context is. In
contrast, nonparametric Bayesian language mod-
els aim to segment character string into a “reason-
able” sentence according to the posterior probabil-
ity. Thus, theoretically, this method should have
better ability to solve ambiguities over goodness
measure based methods.

3 Joint Model

In this section, we will discuss our joint model in
detail.

855



3.1 Combining HDP and HMM
In supervised Chinese word segmentation lit-
erature, word-based approaches and character-
based approaches often have complementary ad-
vantages (Wang et al., 2010).Since the two types
of model try to solve the problem from different
perspectives and by utilizing different levels of in-
formation (word level and character level). In un-
supervised Chinese word segmentation literature,
the HDP-base model can be viewed as a typi-
cal word-based method. And we can also build
a character-based unsupervised model by using a
hidden Markov model. We believe that the HDP-
based model and the HMM-based model are also
complementary with each other, and a combina-
tion of them will take advantage of both and thus
capture different levels of information.

Now the problem we are facing is how to com-
bine these two models. To keep the joint model
simple and involve as little extra parameters as
possible, we combine the two baseline models by
just multiplying their probabilities together and
then renormalizing it. Let C = c1c2 · · · c|C| be a
string of characters and W = w1w2 · · ·w|W | is the
corresponding segmented words sequence. Then
the conditional probability of the segmentation W
given the character string C in our joint model is
defined as:

PJ(W |C) = 1
Z(C)

PD(W |C)PM (W |C) (1)

where PD(W |C) is the probability from the HDP
model as given in Equation 6 and PM (W |C)
is the probability given by the Bayesian HMM
model as given in Equation 2. Z(C) is a nor-
malization term to make sure that PJ(W |C) is a
probability distribution. The combining method is
inspired by Hinton (1999), which proved that it is
possible to combine many individual expert mod-
els by multiplying the probabilities and then renor-
malizing it. They called it “product of experts”.
We can see that combining models in this way
does not involve any extra parameters and Gibbs
sampling can be easily used for model inference.

3.2 Bayesian HMM
The dominant method for supervised Chinese
word segmentation is character-based model
which was first proposed by Xue (2003). This
method treats word segmentation as a tagging
problem, each tag indicates the position of a char-
acter within a word. The most commonly used

tag set is {Single, Begin, Middle, End}. Specifi-
cally, S means the character forms a single word,
B/E means the character is the begining/ending
character of the word, and M means the charac-
ter is in the middle of the word. Existing models
are trained on manually annotated data in a super-
vised way based on discriminative models such as
Conditional Random Fields (Peng et al., 2004;
Tseng et al., 2005). Supervised character-based
methods make full use of character level informa-
tion and thus have been very successful in the last
decade. However, no unsupervised model has uti-
lized character level information in the way as su-
pervised method does.

We can also build a character-based model for
Chinese word segmentation using hidden Markov
model(HMM) as formulated in the following
equation:

PM (W |C) =
|C|∏
i=1

Pt(ti|ti−1)Pe(ci|ti) (2)

where C and W have the same meaning as be-
fore. Pt(ti|ti−1) is the transition probability of
tag ti given its former tag ti−1 and Pe(ci|ti) is the
emission probability of character ci given its tag ti.
This model can be easily trained with Maximum
Likelihood Estimation (MLE) on annotated data
or with Expectation Maximization (EM) on raw
texts. But using any of this methods will make it
difficult to combine it with the HDP-based model.
Instead, we propose a Bayesian HMM for unsu-
pervised word segmentation. The Bayesian HMM
model is defined as follows:

ti|ti−1 = t, pt ∼ Mult(pt)
ci|ti = t, et ∼ Mult(et)

pt|θ ∼ Dirichlet(θ)
et|σ ∼ Dirichlet(σ)

where pt and et are transition and emission dis-
tributions, θ and σ are the symmetric parameters
of Dirichlet distributions. Now suppose we have
observed tagged text h, then the conditional prob-
ability PM (wi|wi−1 = l, h) can be obtained:

PM (wi|wi−1 = l, h)

=
|wi|∏
j=1

Pt(tj |tj−1, h)Pe(cj |tj , h) (3)

where < wi−1, wi > is a word bigram, l is the in-
dex of word wi−1, cj is the jth character in word

856



wi and tj is the corresponding tag.Pt(tj |tj−1, h)
and Pe(cj |tj , h) are the posterior probabilities,
they are given as:

Pt(tj |tj−1, h) =
n<tj−1,tj> + θ
n<tj−1,∗> + Tθ

(4)

Pe(cj |tj , h) =
n<tj ,cj> + σ
n<tj ,∗> + V σ

(5)

where n<tj−1,tj> is the tag bigram count of <
tj−1, tj > in h, n<tj ,cj> denotes the number of oc-
currences of tag tj and character cj , and ∗ means
a sum operation. T and V are the size of character
tag set (we follow the commonly used {SBME}
tag set and thus T = 4 in this case) and character
vocabulary.

3.3 HDP Model
Goldwater et al. (2009) proposed a nonparametric
Bayesian model for unsupervised word segmenta-
tion which is based on HDP (Teh et al., 2006). In
this model, the conditional probability of the seg-
mentation W given the character string C is de-
fined as:

PD(W |C) =
|W |∏
i=0

PD(wi|wi−1) (6)

where wi is the ith word in W . This is actually
a nonparametric bigram language model. This bi-
gram model assumes that each different word has
a different distribution over words following it, but
all these different distributions are linked through
a HDP model:

wi|wi−1 = l ∼ Gl
Gl ∼ DP (α1, G0)
G0 ∼ DP (α, H)

where DP denotes a Dirichlet process.
Suppose we have observed segmentation re-

sult h, then we can get the posterior probability
PD(wi|wi−1 = l, h) by integrating out Gl:

PD(wi|wi−1 = l, h)

=
n<wi−1,wi> + α1PD(wi|h)

n<wi−1,∗> + α1
(7)

where n<wi−1,wi> denotes the total number of oc-
currences of the bigram < wi−1, wi > in the ob-
servation h. And PD(wi|h) can be got by integrat-
ing out G0:

PD(wi|h) = twi + αH(wi)
t + α

(8)

where twi denotes the number of tables associ-
ated with wi in the Chinese Restaurant Franchise
metaphor (Teh et al., 2006), t is the total number
of tables and H(wi) is the base measure of G0. In
fact, H(wi) is the prior distribution over words, so
prior knowledge can be injected in this distribution
to enhance the performance.

In Goldwater et al. (2009)’s work, the base
measure H(wi) are defined as a character unigram
model:

H(wi) = (1− ps)|wi|−1ps
∏
j

P (cij)

where, ps is the probability of generating a word
boundary. P (cij) is the probability of the jth char-
acter cij in word wi, this probability can be esti-
mated from the training data using maximum like-
lihood estimation.

3.4 Initializing with nVBE

Among various goodness measure based models,
we choose nVBE (Magistry and Sagot, 2012) to
initialize our Gibbs sampler with its segmentation
results. nVBE achieved a relatively high perfo-
mance over other goodness measure based meth-
ods. And it’s very simple as well as efficient.

Theoretically, the Gibbs sampler may be initial-
ized at random or using any other methods. Initial-
ization does not make a difference since the Gibbs
sampler will eventually converge to the posterior
distribution if it iterates as much as possible. This
is an essential attribute of Gibbs sampling. How-
ever, we believe that initializing the Gibbs sam-
pler with the result of nVBE will benefit us in
two ways. On one hand, in consideration of its
combination of nonparametric Bayesian method
and goodness-based method, it will improve the
overall performance as well as solve more seg-
mentation ambiguities with the help of HDP-based
model. On the other hand, it makes the conver-
gence of Gibbs sampling faster. In practice, ran-
dom initialization often leads to extremely slow
convergence.

3.5 Inference with Gibbs Sampling

In our proposed joint model, Gibbs sam-
pling (Casella and George, 1992) can be easily
used to identify the highest probability segmen-
tation from among all possibilities. Following
Goldwater et al. (2009), we can repeatedly sample
from potential word boundaries. Each boundary

857



variable can only take on two possible values, cor-
responding to a word boundary or not word bound-
ary.

For instance, suppose we have obtained a seg-
mentation result β|ci−2ci−1cici+1ci+2|γ, where β
and γ are the words sequences to the left and
right and ci−2ci−1cici+1ci+2 are characters be-
tween them. Now we are sampling at location i
to decide whether there is a word boundary be-
tween ci and ci+1. Denote h1 as the hypothesis
that it forms a word boundary (the correspond-
ing result is βw1w2γ where w1 = ci−2ci−1ci and
w2 = ci+1ci+2), and h2 as the opposite hypoth-
esis (then the corresponding result is βwγ where
w = ci−2ci−1cici+1ci+2). The posterior probabil-
ity for these two hypotheses would be:

P (h1|h−) ∝ PD(h1|h−)PM (h1|h−) (9)
P (h2|h−) ∝ PD(h2|h−)PM (h2|h−) (10)

where PD(h|h−) and PM (h|h−) are the pos-
terior probabilities in HDP-based model and in
HMM-based model, and h− denotes the current
segmentation results for all observed data except
ci−2ci−1cici+1ci+2. Note that the normalization
term Z(C) can be ignored during inference. The
posterior probabilities for these two hypotheses in
the HDP-based model is given as:

PD(h1|h−) = PD(w1|wl, h−)
× PD(w2|w1, h−)PD(wr|w2, h−) (11)

PD(h2|h−) = PD(w|wl, h−)
× PD(wr|w, h−) (12)

where wl(wr) is the first word to the left (right) of
w. And the posterior probabilities for the Bayesian
HMM model is given as:

PM (h1|h−)

∝
i+2∏

j=i−2
Pt(tj |tj−1, h−)Pe(cj |tj , h−) (13)

PM (h2|h−)

∝
i+2∏

j=i−2
Pt(tj |tj−1, h−)Pe(cj |tj , h−) (14)

where Pt(tj |tj−1, h−) and Pe(cj |tj , h−) are given
in Equation 4 and 5. The difference is that un-
der hypothesis h1, ci−2ci−1cici+1ci+2 are tagged
as “BMEBE” and under hypothesis h2 as “BM-
MME”.

Once the Gibbs sampler is converged, a natu-
ral way to is to treat the result of last iteration as
the final segmentation result, since each set of as-
signments to the boundary variables uniquely de-
termines a segmentation.

4 Experiments

In this section, we test our joint model on PKU
and MSRA datesets provided by the Second Seg-
mentation Bake-off (SIGHAN 2005) (Emerson,
2005). Most previous works reported their results
on these two datasets, this will make it convenient
to directly compare our joint model with theirs.

4.1 Setting

The second SIGHAN Bakeoff provides several
large-scale labeled data for evaluating the per-
formance of Chinese word segmentation systems.
Two of the four datasets are used in our exper-
iments. Both of the dataset contains only sim-
plified Chinese. Table 1 shows the statistics of
the two selected corpus. For development set, we
randomly select a small subset (about 10%) of
the training data. Specifically, 2000 sentences are
selected for PKU corpus and 8000 sentences for
MSRA corpus. The rest training data plus the test
set is then combined for segmentation but only test
data is used for evaluation. The development set is
used to tune parameters of the HDP-based model
and HMM-based model separately. Since our joint
model does not involve any additional parameters,
we reuse the parameters of the HDP-based model
and HMM-based model in the joint model. Specif-
ically, we set α1 = 1000.0, α = 10.0, ps = 0.5 for
the HDP-based model and set θ = 1.0, σ = 0.01
for the HMM-based model.

For evaluation, we use standard F-Score on
words for all following experiments. F-Score is
the harmonic mean of the word precision and re-
call. Precision is given as:

P =
#correct words in result
#total words in result

and recall is given as:

R =
#correct words in result

#total words in gold corpus

then F-Score is calculated as:

F =
2×R × F

R + F

858



Corpus TrainingSize (words) TestSize (words)
PKU 1.1M 104K

MSRA 2.37M 107K

Table 1: Statistics of training and testing data

Huang and Zhao (2007) provided an empirical
method to estimate the consistency between the
four different segmentation standards involved in
the Bakeoff-3. A lowest consistency rate 84.8%
is found among the four standards. Zhao and Kit
(2008) considered this figure as the upper bound
for any unsupervised Chinese word segmentation
systems. We also use it as the topline in our com-
parison.

4.2 Prior Knowledge Used

When it comes to the evaluation and compari-
son for unsupervised word segmentation systems,
an important issue is what kind of pre-processing
steps and prior knowledge are needed. To be fully
unsupervised, any prior knowledge such as punc-
tuation information, encoding scheme and word
length could not be used in principle. Neverthe-
less, information like punctuation can be easily in-
jected to most existing systems and significantly
enhance the performance. The problem we are
faced with is that we don’t know for sure what
kind of prior information are used in other sys-
tems. One may use a small punctuation set to
segment a long sentence into shorter ones, while
another may write simple regular expressions to
identify dates and numbers. Lot of work we com-
pare to don’t even mention this subject.

Fortunately, we notice that Wang et al. (2011)
provided four kinds of preprocessings (they call
settings). In their settings 1 and 2, punctuation
and other encoding information are not used. In
setting 3, punctuation is used to segment charac-
ter sequences into sentences, and both punctuation
and other encoding information are used in setting
4. Then the results reported in Magistry and Sagot
(2012) relied on setting 3 and setting 4. In order
to make the comparison as fair as possible, we use
setting 3 in our experiment, i.e., only a punctua-
tion set for simplified Chinese is used in all our
experiments. We will compare our experiment re-
sults to previous work on the same setting if they
are provided .

4.3 Experiment Results
Table 2 summarizes the F-Scores obtained by dif-
ferent models on PKU and MSRA corpus, as well
as several state-of-the-art systems. Detailed infor-
mation about the presented models are listed as
follows:

• nVBE: the model based on Variation of
Branching Entropy in Magistry and Sagot
(2012). We re-implement their model on set-
ting 31.

• HDP: the HDP-based model proposed by
Goldwater et al. (2009), initialized randomly.

• HDP+HMM: the model combining HDP-
based model and HMM-based model as pro-
posed in Section 3, initialized randomly.

• HDP+nVBE: the HDP-based model, initial-
ized with the results of nVBE model.

• Joint: the “HDP+HMM” model initialized
with nVBE model.

• ESA: the model proposed in Wang et al.
(2011), as mentioned above, the conducted
experiments on four different settings, we re-
port their results on setting 3.

• NPY(2): the 2-gram language model pre-
sented by Mochihashi et al. (2009).

• NPY(3): the 3-gram language model pre-
sented by Mochihashi et al. (2009).

For all of our Gibbs samplers, we run 5 times to
get the averaged F-Scores. We also give the vari-
ance of the F-Scores in Table 2. For each run, we
find that random initialization takes around 1,000
iterations to converge, while initialing with nVBE
only takes as few as 10 iterations. This makes

1The results we got with our implementation is slightly
lower than what was reported in Magistry and Sagot (2012).
According to Pei et al. (2013), they had contacted the authors
and confirmed that the higher results was due to a bug in code.
So we report the results with our bug free implementation as
Pei et al. (2013) did. Our reported results are identical to
those of Pei et al. (2013)

859



System PKU MSRAR P F R P F
nVBE 78.3 77.5 77.9 79.1 77.3 78.2
HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020)
HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013)
HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005)
Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005)
ESA N/A N/A 77.4 N/A N/A 78.4
NPY(2) N/A N/A N/A N/A N/A 80.2
NPY(3) N/A N/A N/A N/A N/A 80.7
Topline N/A N/A 84.8 N/A N/A 84.8

Table 2: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote
the variance the of F-Scores.

our joint model very efficient and possible to work
in practical applications as well. At last, a single
sample (the last one) is used for evaluation.

From Table 2, we can see that the joint
model (Joint) outperforms all the presented sys-
tems in F-Score on all testing corpora. Specifi-
cally, comparing “HDP+HMM” with “HDP”, the
former model increases the overall F-Score from
68.7% to 75.3% (+6.6%) in PKU corpora and
from 69.9% to 76.3% (+6.4%) in MSRA corpora,
which proves that the character information in
the HMM-based model can actually enhance the
performance of the HDP-based model. Compar-
ing “HDP+nVBE” with “HDP”, the former model
also increases the overall F-Score by 10.6%/9.6%
in PKU/MSRA corpora, which demonstrates that
initializing the HDP-based model with nVBE will
improve the performance by a large margin. Fi-
nally, the joint model “Joint” take advantage from
both from the character-based HMM model and
the nVBE model, it achieves a F-Score of 81.1%
on PKU and 81.7% on MSRA. This result outper-
forms all its component baselines such as “HDP”,
“HDP+HMM” and “HDP+nVBE”.

Our joint model also shows competitive advan-
tages over several state-of-the-art systems. Com-
pared with nVBE,the F-Score increases by 3.2%
on PKU corpora and by 3.5% on MSRA cor-
pora. Compared with ESA, the F-Score increases
by 3.7%/3.3% in PKU/MSRA corpora. Lastly,
compared to the nonparametric Bayesian models
(NPY(n)), our joint model still increases the F-
Score by 1.5% (NPY(2)) and 1.0% (NPY(3)) on
MSRA corpora. Moreover, compared with the
empirical topline figure 84.8%, our joint model
achieves a pretty close F-Score. The differences

are 3.7% on PKU corpora and 3.1% on MSRA
corpora.

An phenomenon we should pay attention to is
the poor performance of the HMM-based model.
With our implementation of the Bayesian HMM,
we achieves a 34.3% F-Score on PKU corpora and
a 34.9% F-Score on MSRA corpora, just slightly
better than random segmentation. The result show
that the hidden Markov Model alone is not suit-
able for character-based Chinese word segmenta-
tion problem. However, it still substantially con-
tributes to the joint model.

We find that the variance of the results are rather
small, this shows the stability of our Gibbs sam-
plers. From the segmentation results generated
by the joint model, we also found that quite a
large amount of errors it made are related to dates,
numbers (both Chinese and English) and English
words. This problem can be easily addressed dur-
ing preprocessing by considering encoding infor-
mation as previous work, and we believe this will
bring us much better performance.

4.4 Disambiguation Ability

Previous unsupervised work usually evaluated
their models using F-score, regardless of goodness
measure based model or nonparametric Bayesian
model. However, segmentation ambiguity is
a very important factor influencing accuracy of
Chinese word segmentation systems (Huang and
Zhao, 2007). We believe that the disambigua-
tion ability of the models should also be consid-
ered when evaluating different types of unsuper-
vised segmentation systems, since different type
of models shows different disambiguation ability.
We will compare the disambiguation ability of dif-

860



ferent systems in this section.
In general, there are mainly two kinds of ambi-

guity in Chinese word segmentation problem:

• Combinational Ambiguity: Given charac-
ter strings “A” and “B”, if “A”, “B”, “AB”
are all in the vocabulary, and “AB” or “A-B”
(here “-” denotes a space) occurred in the real
text,then “AB” can be called a combinational
ambiguous string.

• Overlapping Ambiguity: Given character
strings “A”, “J” and “B”, if “A”, “B”, “AJ”
and “JB” are all in the vocabulary, and “A-
JB” or “AJ-B” occurred in the real text, then
“AJB” can be called an overlapping ambigu-
ous string.

We count the total number of mistakes differ-
ent systems made at ambiguous strings (the vo-
cabulary is obtained from the gold standard an-
swer of testing set). As we have mentioned in
Section 2, goodness measure based methods such
as nVBE do not have any disambiguation ability
in theory. Our observation is identical to this ar-
gument. We find that nVBE always segments am-
biguous strings into the same result. Take a combi-
national string “k” as an example, “ (just)”,
“k (have)” and “k (only)” are all in the vo-
cabulary. In the PKU test set, this string occurs
14 times as “-k (just have)” and 18 times as
“k (only)”, 32 times in total. nVBE segments
all the 32 strings into “k (only)” (i.e. 18 of
them are correct), while the joint model segments
it 22 times as “k (only)” and 10 times as “-
k (just have)” according to its context, and 24 of
them are correct.

Table 3 and 4 show the statistics of combi-
national ambiguity and overlapping ambiguity re-
spectively. The numbers in parentheses denote the
total number of ambiguous strings. From these
tables, we can see that HDP+nVBE makes less
mistakes than nVBE in most circumstances, ex-
cept that it solves less combinational ambigui-
ties on MSRA corpora. But our proposed joint
model solves the most combinational and over-
lapping ambiguities, on both PKU and MSRA
corpora. Specifically, compared to nVBE, the
joint model correctly solves 171/871 more com-
binational ambiguities on PKU/MSRA corpora,
which is a 0.6%/13.8% relative error reduction.
It also solves 28/45 more overlapping ambiguities
on PKU/MSRA corpora, which is a 11.5%/23.4%

relative error reduction. This indicates that the
joint model has a stronger ability of disambigua-
tion over the compared systems.

System PKU(35371) MSRA(38506)
nVBE 8087 7236

HDP+nVBE 7970 7500
Joint 7916 6305

Table 3: Statistics of combinational ambiguity.
This table shows the total number of mistakes
made by different systems at combinational am-
biguous strings. The numbers in parentheses de-
note the total number of combinational ambiguous
strings.

System PKU(603) MSRA(467)
nVBE 244 192

HDP+nVBE 239 164
Joint 216 157

Table 4: Statistics of overlapping ambiguity. This
table shows the total number of mistakes made
by different systems at overlapping ambiguous
strings. The numbers in parentheses denote the to-
tal number of overlapping ambiguous strings.

4.5 Statistical Significance Test

The main results presented in Table 2 has shown
that our proposed joint model outperforms the
two baselines as well as state-of-the-art systems.
But it is also important to know if the improve-
ment is statistically significant over these sys-
tems. So we conduct statistical significance tests
of F-scores among these various models. Follow-
ing Wang et al. (2010), we use the bootstrapping
method (Zhang et al., 2004).

Here is how it works: suppose we have a testing
set T0 to test several word segmentation systems,
there are N testing examples (sentences or line of
characters) in T0. We create a new testing set T1
with N examples by sampling with replacement
from T0, then repeat these process M − 1 times.
And we will have a total M +1 testing sets. In our
test procedures, M is set to 2000.

Since we just implement our joint model and
its component models, we can not generate paired
samples for other models (i.e. ESA and NPY(n)).
Instead, we follow Wang et al. (2010)’s method
and first calculate the 95% confidence interval for

861



our proposed model. Then other systems can be
compared with the joint model in this way: if the
F-score of system B doesn’t fall into the 95% con-
fidence interval of system A, they are considered
as statistically significantly different from each
other.

For all significant tests, we measure the 95%
confidence interval for the difference between
two models. First, the test results show that
“HDP+nVBE” and “HDP+HMM” are both sig-
nificantly better than “HDP”. Second, the
“Joint” model significantly outperforms all its
component models, including “HDP”, “nVBE”,
“HDP+nVBE” and “HDP+HMM”. Finally, the
comparison also shows that the joint model signif-
icantly outperforms state-of-the-art systems like
ESA and NPY(n).

5 Conclusion

In this paper, we proposed a joint model for un-
supervised Chinese word segmentation. Our joint
model is a combination of the HDP-based model,
which is a word-based model, and HMM-based
model, which is a character-based model. The
way we combined these two component base-
lines makes it natural and simple to inference with
Gibbs sampling. Then the joint model take ad-
vantage of a goodness-based method (nVBE) by
using it to initialize the sampler. Experiment re-
sults conducted on PKU and MSRA datasets pro-
vided by the second SIGHAN Bakeoff show that
the proposed joint model not only outperforms the
baseline systems but also achieves better perfor-
mance (F-Score) over several state-of-the-art sys-
tems. Significance tests showed that the improve-
ment is statistically significant. Analysis also in-
dicates that the joint model has a stronger abil-
ity to solve ambiguities in Chinese word segmen-
tation. In summary, the joint model we pro-
posed combines the strengths of character-based
model, nonparametric Bayesian language model
and goodness-based model.

Acknowledgments

The contact author of this paper, according to
the meaning given to this role by Key Labora-
tory of Computational Linguistics, Ministry of Ed-
ucation, School of Electronics Engineering and
Computer Science, Peking University, is Baobao
Chang. And this work is supported by National
Natural Science Foundation of China under Grant

No. 61273318 and National Key Basic Research
Program of China 2014CB340504.

References
George Casella, Edward I. George. 1992. Explain-

ing the Gibbs sampler. The American Statistician,
46(3): 167-174.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, 133. MLA.

Haodi Feng, Kang Chen, Xiaotie Deng, et al. 2004.
Accessor variety criteria for Chinese word extraction
Computational Linguistics, 30(1): 75-93.

Sharon Goldwater, Thomas L. Griffiths, Mark Johnson.
2009. A Bayesian framework for word segmenta-
tion: Exploring the effects of context. Cognition
112(1): 21-54.

Geoffrey E. Hinton. 1999. Products of experts. Arti-
ficial Neural Networks. Ninth International Confer-
ence on Vol. 1.

Changning Huang, Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3): 8-20.

Zhihui Jin, Kumiko Tanaka-Ishii. 2006. Unsupervised
segmentation of Chinese text by use of branching
entropy. Proceedings of the COLING/ACL on Main
conference poster sessions, page 428-435.

Chunyu Kit, Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. Proceedings of the CoNLL99 ACL Workshop.
Bergen, Norway: Association for Computational
Linguis-tics, page 1-6.

Pierre Magistry, Benoit Sagot. 2012. Unsuper-
vized word segmentation: the case for mandarin
chinese. Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Short Papers-Volume 2. Association for Computa-
tional Linguistics, page 383-387.

Daichi Mochihashi, Takeshi Yamada, Naonori Ueda.
2009. Bayesian unsupervised word segmentation
with nested Pitman-Yor language modeling. Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1-Volume 1. Association for Com-
putational Linguistics, page 100-108.

Wenzhe Pei, Dongxu Han, Baobao Chang. 2013. A
Refined HDP-Based Model for Unsupervised Chi-
nese Word Segmentation. Chinese Computational
Linguistics and Natural Language Processing Based
on Naturally Annotated Big Data. Springer Berlin
Heidelberg, page 44-51.

862



Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, et
al. 2006. Sharing Clusters among Related Groups:
Hierarchical Dirichlet Processes. NIPS.

Fuchun Peng, Fangfang Feng, Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Proceedings
of COLING, page 562-568.

Huihsin Tseng, Pichuan Chang, Galen Andrew, et al.
2005. A conditional random field word segmenter
for sighan bakeoff 2005. Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, Vol. 171.

Kun Wang, Chengqing Zong, Keh-Yih Su. 2010. A
character-based joint model for Chinese word seg-
mentation. Proceedings of the 23rd International
Conference on Computational Linguistics. Associa-
tion for Computational Linguistics, page 1173-1181.

Hanshi Wang, Jian Zhu, Shiping Tang, et al. 2011. A
new unsupervised approach to word segmentation.
Computational Linguistics, 37(3): 421-454.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1): 29-48.

Hai Zhao, Chunyu Kit. 2008. An Empirical Compar-
ison of Goodness Measures for Unsupervised Chi-
nese Word Segmentation with a Unified Framework.
IJCNLP, page 6-16.

Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? LREC.

863


