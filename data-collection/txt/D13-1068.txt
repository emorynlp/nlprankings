










































Optimized Event Storyline Generation based on Mixture-Event-Aspect Model


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 726–735,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Optimized Event Storyline Generation based on Mixture-Event-Aspect
Model

Lifu Huang
Shenzhen Key Lab for Cloud Computing

Technology and Applications,
Peking University Shenzhen Graduate School,

Shenzhen, Guangdong 518055, P.R.China
warrior.fu@gmail.com

Lian’en Huang∗

Shenzhen Key Lab for Cloud Computing
Technology and Applications,

Peking University Shenzhen Graduate School,
Shenzhen, Guangdong 518055, P.R.China

hle@net.pku.edu.cn

Abstract

Recently, much research focuses on event s-
toryline generation, which aims to produce a
concise, global and temporal event summary
from a collection of articles. Generally, each
event contains multiple sub-events and the sto-
ryline should be composed by the componen-
t summaries of all the sub-events. However,
different sub-events have different part-whole
relationship with the major event, which is
important to correspond to users’ interests
but seldom considered in previous work. To
distinguish different types of sub-events, we
propose a mixture-event-aspect model which
models different sub-events into local and
global aspects. Combining these local/global
aspects with summarization requirements to-
gether, we utilize an optimization method to
generate the component summaries along the
timeline. We develop experimental systems on
6 distinctively different datasets. Evaluation
and comparison results indicate the effective-
ness of our proposed method.

1 Introduction

With the rapid growth of the World Wide Web, in-
formation explosion has become an important issue
to modern people. Those who search for informa-
tion from the Internet often get lost and confused
by the overwhelmingly large collection of web doc-
uments. So how to get a concise and global pic-
ture for a given event subject is an urgent prob-
lem to be solved. Although many document un-
derstanding systems have been proposed, such as

∗Corresponding author

multi-document summarization systems, to gener-
ate a compressed summary by extracting the ma-
jor information from the collection of documents,
they ignored the dynamic development information
of an event. Intuitively, each event is long-running
and contains multiple sub-events, including related
events. Users are likely to prefer a summary of all
occurrences of all the sub-events along the timeline
of the event. This motivates us to study the task of
generating event storyline from a collection of web
documents related to an event subject.

The research of event storyline summarization is
popular in recent years. Its task is to summarize a
collection of web documents by extracting represen-
tative information based on all the sub-events and
generate a global summary. Generally, generating
such a global storyline is quite interesting for the fol-
lowing main reasons: (1) It can help people catch the
whole incident based on an overall temporal struc-
tured summary for a given subject, and understand
the cause, climax, development process and result
of an event. (2) It can also make people know what
other events are related, or the effect of this incident
to subsequent events, which can present the evolu-
tion of an event along a timeline.

Though several methods of generating event sto-
ryline have been proposed recently, there are still
some problems unresolved. As event storyline sum-
marization is a process to generate component sum-
maries based on the multiple sub-events, which is d-
ifferent from traditional summarization focusing on
only one subject, so how to exactly extract all the
sub-events is the first challenge. Moreover, user-
s tend to bias to the sub-events which have global

726



consistency with the given event subject, so the sub-
events should not be considered equally when gen-
erating the component summaries. It is also a great
challenge to generate a qualified summary based on
the different types of sub-events. The componen-
t summaries should be correlative across differen-
t dates based on the global collection (Yan et al.,
2011a).

Mei and Zhai (Mei and Zhai, 2005) proposed to
use theme or topic to model different sub-events,
which is to some extent similar to our method. To
be different, in this paper we introduce “local/glob-
al” property to distinguish different part-whole rela-
tionship between the sub-events and the major even-
t, which have not been considered before in story-
line generation or summarization, to improve the
quality of the storyline. The local/global proper-
ty corresponds to the elements of an event, such
as the place, characters and other body informa-
tion. These information reflects the relationship be-
tween the sub-events and the major event. Some
sub-events have distinctive body information and lit-
tle relevance with each other. They generally occur
for a local period, which we name as “local-sub-
events”. While other sub-events often share com-
mon properties with each other and have close re-
lationship with the major event and we call them
as “global-sub-events”. Here we give some exam-
ples to illustrate the difference. For the event “Con-
necticut school shooting” which occurred on Dec.14
2012, its sub-events such as “Obama’s speech for
this massacre” or “Gun control Act” have little word
co-occurrences and distinctive event body informa-
tion to each other, while the process and result of this
tragedy can be regarded as global-sub-events which
have a lot of word co-occurrences and share com-
mon properties with the major event.

Inspired by these, to detect different types of sub-
events based on word co-occurrences between sub-
events and the major event, we propose a mixture-
event-aspect (MEA) model to formalize differen-
t types of sub-events into local/global aspects, which
are implicated with clusters of sentences. Then com-
bining the local/global aspects with summarization
requirements together, we utilized an optimization
approach to get the optimal component summaries
along the timeline. We evaluate our method on 6 dis-
tinctively different datasets. Performance compar-

isons among different system-generated storylines
demonstrate the necessity to distinguish differen-
t types of sub-events and also indicates the effective-
ness of the proposed mixture-event-aspect model.

The rest of the paper is organized as follows. We
briefly review the related work in section 2. In sec-
tion 3 we present the details of optimized event s-
toryline generation based on mixture-event-aspect
model. Experiments and results are discussed in
Section 4. Finally we draw a conclusion of this s-
tudy in Section 5.

2 Related Work

Our work is related to several lines of research in the
literature including multi-document summarization
(MDS), topic detection and tracking (TDT), tempo-
ral text mining (TXM) and temporal news summa-
rization (TNS).

Multi-document summarization is a process to
generate a summary by reducing documents in size
while retaining the main information. To date, dif-
ferent features and ranking strategies have been s-
tudied. Radev et al. (Radev et al., 2004) proposed
to implement MEAD as a centroid-based summa-
rizer by combining several predefined features to s-
core the sentence. LexPageRank (Erkan and Radev,
2004) is the representative work which is based on
PageRank (Page et al., 1999) algorithm. Some meth-
ods have been proposed to extend the conventional
graph-based models recently including multi-layer
graph incorporated with different relationship (Wan,
2008), ToPageRank based on the topic information
(Pei et al., 2012) and DivRank (Mei et al., 2010) bal-
ancing the prestige and diversity.

Topic detection and tracking (TDT) aims to group
news articles based on the topics discussed in them,
detect some novel and previously unreported events
and track future events related to the topics (Wang
et al., 2012). Kumaran and Allan (Kumaran and Al-
lan, 2004) showed how performance on new event
detection could be improved by the use of text clas-
sification techniques as well as by using named en-
tities in a new way. Makkonen et al. (Makkonen
et al., 2004) proposed a method that incorporated
simple semantics into TDT by splitting the term s-
pace into groups of terms. Krause et al. Wang et al.
(Wang et al., 2007) and Wang et al. (Wang et al.,

727



2009) worked on topic tracking from multiple news
streams. Their methods extracted meaningful top-
ics from multi-source news collections and tracked
different topics as they evolved from one to another
along the timeline.

Our work is also related to temporal text mining
and temporal news summarization. The task of tem-
poral news summarization is to generate news sum-
maries along the timeline from massive data. Chieu
et al. (Chieu and Lee, 2004) built a system that ex-
tracted events relevant to a query from a collection
of related documents and placed such events along
a timeline. Yan et al. (Yan et al., 2011b) designed
an evolutionary timeline summarization approach to
construct a timeline of a topic by optimizing the rel-
evance, coverage, coherence, and diversity. Lin et al.
(Lin et al., 2012) explored the problem of generating
storylines from microblogs for user input queries.
They first proposed a language model with dynamic
pseudo relevance feedback to obtain relevant tweet-
s and then generated storylines via graph optimiza-
tion.

3 Approach Details

In this section, we first propose a mixture-event-
aspect model to detect local/global sub-events based
on part-whole relationship with the major event and
then present a new method to estimate the bursty of
each aspect on a certain date. Afterwards we uti-
lize an optimization method based on local/global
aspects to extract the qualified summary.

3.1 Mixture-Event-Aspect Model

The key challenge to our storyline generation task
is to detect and distinguish different types of sub-
events contained in the article collection. In the col-
lection, each sentence is assigned with a certain date
and sentences that are assigned with the same date
are grouped into the same sub-collection. Consid-
ering the consistency of content between the sub-
events and the major event, we model different sub-
events into two types: local-sub-event and global-
sub-event, and introduce local/global aspects cor-
respondingly. Generally, local aspects which cor-
respond to local-sub-events have distinctive word-
s distribution from each other and sustain for a lo-
cal context while the global aspects corresponding

to global-sub-events have coincident words distri-
bution with the major event. To capture specific
words, Titov and McDonald (Titov and McDonald,
2008) proposed a multi-grain topic model, relying
on word co-occurrences within short paragraphs and
Li et al. (Li et al., 2010) proposed a entity-aspect
model based on word co-occurrences within single
sentences. Inspired by these ideas, we rely on word
co-occurrences within local period context to detec-
t mixed local and global aspects implicated in the
whole collection. We name this model as “Mixture-
Event-Aspect (MEA)” model which can simultane-
ously detect local/global aspects and cluster sen-
tences and words into different aspects.

3.1.1 Model Description
Our mixture-event-aspect (MEA) model can be

extended from both the Probabilistic Latent Seman-
tic Analysis (PLSA) (Hofmann, 1999) and Laten-
t Dirichlet Allocation (LDA) (Blei et al., 2003). We
model two distinct types of aspects: global aspect-
s and local aspects, based on their relationship with
the major event. The distribution of global aspects is
fixed for the collection while the distribution of local
aspects is fixed to a local period of sub-collections.
That means a sentence is sampled either from the
mixture of the global aspects or from the local as-
pects specific for the local context. Here we take
the event “Connecticut school shooting” as an exam-
ple. For the sentence “On Sunday, President Obama
came to Connecticut to give a lecture, expressing his
sorrow for ... and calling for an end to such inci-
dents”, the words such as “Obama”, “lecture”, “ex-
press” are only occurred for the local period of two
days and have no co-occurrence with other neigh-
boring period sentences, so we sample the sentence
as a local aspect sentence. But for the sentence “All
schools in Newtown, the northeastern U.S. state of
Connecticut were in lockdown after a shooting was
reported at a local elementary school”, the word-
s such as “Connecticut”, “shooting”, “elementary”
have high co-occurrence frequency in the whole col-
lection, so we sample the sentence as a global aspect
sentence.

To detect aspects, we first divide words into two
types: aspect words and background words. Back-
ground words are commonly used in the whole even-
t corpus while aspect words are clearly associat-

728



ed with the aspects of the sentences they occur in.
Stop words are removed using a standard stop word
list. In order to get the distribution of local aspect-
s, we implement a mechanism called “Time Win-
dow” which covers Sp sequential time-based sub-
collections. We associate each time window with
a distribution over local aspects and a distribution
defining preference of local aspects versus global as-
pects.

draw φB ∼ Dir(β), ψc(v) ∼ Dir(λ), π ∼ Dir(γ)
draw φgl ∼ Dir(β) for Agl times
draw φloc ∼ Dir(β) for Aloc times
choose a distribution of global aspects θgl ∼ Dir(αgl)

For each time window v in collection c

choose θlocc,v ∼ Dir(αloc)
choose ρc,v ∼ Beta(αmix)

For each sentence s in collection c

choose window νc,s ∼ ψc
choose ηc,s ∼ ρc,νc,s
if ηc,s = gl, zc,s ∼ θgl

if ηc,s = loc, zc,s ∼ θlocc,v
For each word w of sentence s in collection c

draw yc,s,n ∼Multi(π)
draw wc,s,n ∼Multi(φB) if yc,s,n = 1
draw wc,s,n ∼Multi(φzc,s) if yc,s,n = 2

Figure 1: The Collection Generation Process

Formally, let C = {Ct|t = 1, 2, 3, ..., T} be
T time based sub-collections related to the even-
t subject, Ct represents the collection of sentences
which are assigned with the date t. Let v be a time
window containing Sp sequential sub-collections,
v = {Ct|t = i, i + 1, ..., i + Sp − 1}. We draw
a background unigram language model which gen-
erates words for all sub-collections, and draw Agl
global aspect unigram language models for global
aspects andAloc word distributions for local aspects.
We assume these word distributions have a uniform
Dirichlet prior Dir(β). There is also a multinomi-
al distribution π that controls in each sentence how
often the word occurs as a background word or an
aspect word. π has a Dirichlet prior with parame-
ter γ. We assign each window v with an distribution
over local aspects and a distribution ρ defining pref-
erence for local aspects versus global aspects. ρ has

a Beta prior αmix. A sentence can be sampled using
any window which is chosen according to a categor-
ical distribution.

In Figure 2 the corresponding graphical model is
presented. This model allows for fast approximate
inference with collapsed Gibbs sampling.

Figure 2: Mixture-Event-Aspect Model

Let SC denotes the number of sentences in col-
lection C, Nc,s denotes the number of words in sen-
tence s of collection c, and wc,s,n denotes the nth

word in sentence s. There are two kinds of hidden
variables: zc,s for each sentence to indicate the as-
pect a sentence belongs to, and yc,s,n for each word
to indicate whether a word is generated from the
background model or the aspect model.

3.1.2 Inference via Gibbs Sampling

In order to estimate the hidden parameter-
s in the model, we try to maximize distribution
p(z,y|w;α, β, γ, λ), where z, y and w represent the
set of all z, y and w variables, respectively. Giv-
en a sentence s in the collection c, we apply Gibbs
Sampling to estimate the conditional probability for
local/global aspects using the following rules:

p(vc,s = vh, ηc,s = gl, zc,s = a|v
′
, z

′
, y, w) ∝

nc
h∗+λ

nc
(.)

+S∗pλ
·

n
c,vh
gl

+αmixgl
n

c,vh
(.)

+
∑

r
′∈gl,loc

αmix
r
′

· n
c
gl,a+α

gl

nc
gl

+Aglαgl
·

∏L
l=1

∏E(l)−1
i=0

(Ca
(l)

+i+β)

Π
E(.)−1
i=0 (C

a
(.)

+i+Lβ)

729



p(vc,s = vh, ηc,s = loc, zc,s = a|v
′
, z

′
, y, w) ∝

nc
h∗+λ

nc
(.)

+S∗pλ
· n

c,vh
loc

+αmixloc
n

c,vh
(.)

+
∑

r
′∈gl,loc

αmix
r
′

·
n

c,vh
loc,a

+αloc

n
c,vh
loc

+Alocαloc
·

∏L
l=1

∏E(l)−1
i=0

(Ca
(l)

+i+β)∏E(.)−1
i=0

(Ca
(.)

+i+Lβ)

where nch∗ =#{si|vc,si = vi−sp+h∗+1}, denotes
the number of times a sentence si in collection c is
assigned to its h∗th window and for each sentence
si, we have h = i− Sp + 1 + h∗. S∗p is the number
of windows that contain sentence s. nc(.) denotes the
number of sentences in collection c. Sp represents
the number of dates a window covered. nc,vhgl and
nc,vhloc are the number of sentences in window vh that
are assigned to global or local aspects. nc,vh(.) is the
number of sentences assigned to window vh. ncgl,a
is the number of sentences in all global aspects that
are assigned to aspect a and nc,vhloc,a is the number of
local aspect sentences in the window vh are assigned
to aspect a. Agl is the number of global aspects in
collection C while Aloc is the number of local as-
pects. E(l) represents the number of times of word
l occurs in the current sentence and is assigned to
be an aspect word, while E(.) is the total number of
words in the current sentence that are assigned to be
an aspect word.

Then we compute the assignments of the consid-
ered word. We sample the hidden variables yc,s,n for
each word with the following rules:

p(yc,s,n = 1|z, y
′
) ∝

Cπ(1) + γ

Cπ(·) + 2 · γ
·
CB(wc,s,n) + β

CB(·) + L · β

p(yc,s,n = 2|z, y
′
) ∝

Cπ(2) + γ

Cπ(·) + 2 · γ
·
Ca(wc,s,n) + β

Ca(·) + L · β

where Cπ(1) and C
π
(2) are the numbers of words as-

signed to be background words and aspect words.
Cπ(·) is the total number of words. C

B
(·) is the total

number of background words and Ca(·) is the number
of words assigned to aspect a.

By using the samples from Gibbs sampling, we
can effectively make the following estimations:

φBw =
CBw +β

CB· +L·β
, φaw =

Caw+β
Caw+L·β

, ψh∗ =
nc

h∗+λ

nc
(.)

+S∗pλ

πy =
Cπy +γ

Cπ· +2·γ
, ρc,vη =

nc,vη +α
mix
η

nc,v+
∑

r∈(gl,loc)
αmixr

θgla =
ncgl,a+α

gl

nc
gl

+Aglαgl
, θloca =

nc,v
loc,a

+αloc

nc,v
loc

+Alocαloc

The hyper-parameters like α, β, γ, λ can be estimat-
ed using standard methods introduced in (Minka,
2000).

3.2 Bursty Period Detection
We borrow the definition of “bursty” from (Lappas
et al., 2009) to measure the popularity of the event
on a certain date. Intuitively, each aspect have differ-
ent bursties on different dates. In this section, we try
to obtain the temporal aspect sequences of an event
based on the bursty periods of all the aspects. Dur-
ing its bursty period, one aspect should (1)be more
popular than other aspects (2) be continuously more
popular than other time. Following these intuitions,
we design a method to measure the bursty of each
aspect and get the bursty period.

Let Ak be the kth aspect obtained from the
mixture-event-aspect model, we estimate the bursty
of Ak at a certain date t as follows.

bursty(Ak,t) = p(t|Ak) =
p(Ak|t) · p(t)∑

t′
p(Ak|t′)p(t′)

where p(Ak|t) is measured by the number of sen-
tences assigned to aspect Ak in date t divided by the
total number of sentences in date t. p(t) is estimated
by the total number of sentences in aspect Ak divid-
ed by the overall number of sentences in the collec-
tion C.

After getting the bursty of aspect Ak at each date,
we can find the most popular date and expand on
both sides to obtain the whole burst period in which
the bursties are higher than the neighboring aspects
and continuous higher than other dates.

3.3 Optimization-based Storyline Generation
With the methods discussed in previous sections, we
can get the local/global aspect sequence. Each as-
pect contains numbers of sentences and we are aim-
ing to select the most representative ones to compose
the final storyline. Considering users’ bias and the
length requirement, different aspects should have d-
ifferent proportions in the last storyline. For glob-
al aspects which correspond more to users’ interest,
they should share a larger proportion in the final sto-
ryline than local aspects. Thus, we use an optimiza-
tion method to determine if a sentence is selected to
be an summary sentence or to be discarded based on
the multiple local/global aspects and finally get the
optimal storyline. We formalize this problem as se-
lecting a subset of sentences S from the aspect Ak

730



to minimize the information loss.

arg min
∑

Ak∈C,S∈Ak

∑
z∈Ak−S,s∈S

O(z, s)

where O(z, s) is the cost function which measures
the cost of representing sentence z with sentence s.
Generally, this is an NP-hard problem (Cheung et
al., 2009) but we can use POPSTAR, an implemen-
tation of an approximate solution proposed by Re-
sende and Werneck (Resende and Werneck, 2004).
To model different costs between global or local
aspects and determine the proportions of differen-
t aspects in the final storyline, we utilize a func-
tion ζ(s). When sentences z and s are local as-
pect sentences, ζ(s) = χ, or, ζ(s) = 1 − χ. For-
mally, we incorporate two kinds of decreasing/in-
creasing logistic functions, ℓ1(x) = 1/(1 + ex) and
ℓ2(x) = e

x/(1 + ex), to define the cost function as

O(z, s) = ζ(s) · ℓ1(S(s)) · ℓ2(S(z)) ·DKL(s, z)

where S(s) and S(z) are the ranking scores of sen-
tences s and z among the aspectAk with LexPageR-
ank algorithm. DKL(s, z) is used to measure the
similarity between sentence s and z with Kullback-
Leibler divergence here.

With this optimization method, we get the repre-
sentative sentences of each aspect for the given event
subject. Combining all the representative sentences
together based on the aspect sequence, we finally
generate the storyline.

4 Experiments and Evaluation

4.1 Datasets
To evaluate our framework for event storyline gen-
eration, we conduct our experiments on the dataset-
s amounting to 12418 articles for 6 event subjects
from 6 famous news websites, which provide date
edited by professional editors. Each article consists
of three parts, title, publish-time and news content.
Table 1 and Table 2 give the brief description of the
12418 articles. To generate reference summary, we
invite 12 undergraduate students with good English
ability to read the sentences, and for each event sub-
ject we ask two students to label human storylines.

4.2 Evaluation Metrics
We use the ROUGE1 (Lin and Hovy, 2003) (Recall
Oriented Understudy for Gisting Evaluation) toolkit

1http://www.isi/edu/licensed-sw/see/rouge/

Table 1: News sources of the 6 datasets
News Sources Number of Articles

CNN 2357
Fox News 1936

New York Times 2178
ABC 2113

Washington Post 1405
Xinhua 2429

Table 2: Event subjects of the datasets
Event Subjects Number of Articles

Connecticut school shooting 1792
The earthquake in Tokyo, Japan 2046
The U.S. presidential election 2573

Sandy hurricane attacked America 1827
American curiosity rover landed on Mars 1651

The 30th London Olympic Games 2529

to evaluate our framework, which has been widely
applied for summarization evaluation. It evaluates
the quality of a summary by counting the overlap-
ping units between the candidate summary and ref-
erence summaries. There are many kinds of ROUGE
metrics to measure the system-generated summa-
rization such as ROUGE-N, ROUGE-L, ROUGE-
W, and ROUGE-U, of which the most important one
is ROUGE-N with 3 sub-metrics: precision, recall,
and F-score.

ROUGE − N =

∑
S∈RS

∑
N−gram∈S

Countmatch(N − gram)∑
S∈RS

∑
N−gram∈S

Count(N − gram)

where RS represents the reference summaries. N-
gram∈RS in the metrics denotes the N-grams in ref-
erence summaries. Countmatch(N − gram) is the
maximum number of N-grams co-occurring in the
candidate summary and in the set of reference sum-
maries. Count(N − gram) is the number of N-
grams in the reference summaries.

The ROUGE toolkit can report separate scores for
1, 2, 3, and 4-gram. In the experimental results we
report three ROUGE F-measure scores: ROUGE-
1, ROUGE-2, ROUGE-W metrics. The higher the
ROUGE scores, the better the summary is.

4.3 Algorithms for Comparison
Given a collection of news articles, we first decom-
pose them into sentences, and then assign each sen-
tence with a certain date, afterwards stop-words re-
moving and words stemming are performed. We
choose the following algorithms as baseline system-
s. Specifically, baseline 2 and 3 are summarization

731



systems which are similar to our storyline genera-
tion system. Then we choose baseline 4, 5 to evalu-
ate the effectiveness of the proposed method. It must
be said that all the systems are required to generate
the same number of summary words with the hu-
man reference. We conduct the same preprocessing
for all algorithms for fairness.
• Random : The method selects sentences ran-

domly from the sentence collection.
• LexPageRank (LexRank): This method applies

the graph-based multi-document summarization al-
gorithm which first constructs a sentence connec-
tivity graph based on the cosine similarity and then
chooses top-ranked sentences with PageRank.
• Chieu : This method was proposed by Chieu

(Chieu and Lee, 2004), utilizing interest and bursti-
ness to rank sentences, and choosing the top-ranked
query related sentences to construct the timeline.
• LDA+LexPageRank (LDALR) : This method

first applies standard LDA to detect latent topics
from the collection and clusters sentences to mul-
tiple aspects, then utilizes PageRank to generate the
most representative component summaries from all
the aspects.
• MEA+LexPageRank (MEALR) : This method

applies the proposed mixture-event-aspect model to
cluster sentences into multiple aspects and then uti-
lizes PageRank to generate the most representative
component summaries from all the aspects.
• MEA+Optimization (MEAOp) : This method

extracts local/global aspects with the proposed
mixture-event-aspect model, and then utilizes the
optimization method to get the qualified summary.

Figure 3: Overall performance for comparison

Table 3: Results of different systems on 6 subjects
Subject1 Subject2

Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.234 0.037 0.188 0.242 0.039 0.192
LexRank 0.317 0.045 0.257 0.326 0.051 0.262

Chieu 0.332 0.056 0.277 0.351 0.055 0.283
LDALR 0.356 0.069 0.297 0.369 0.066 0.327
MEALR 0.369 0.072 0.313 0.381 0.076 0.348
MEAOp 0.381 0.075 0.331 0.398 0.081 0.364

Subject3 Subject4
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.258 0.042 0.191 0.234 0.036 0.179
LexRank 0.339 0.049 0.272 0.309 0.043 0.242

Chieu 0.364 0.059 0.296 0.329 0.053 0.264
LDALR 0.383 0.071 0.331 0.347 0.065 0.308
MEALR 0.396 0.076 0.3512 0.368 0.069 0.312
MEAOp 0.419 0.082 0.371 0.376 0.071 0.323

Subject5 Subject6
Systems R-1 R-2 R-W R-1 R-2 R-W
Random 0.222 0.034 0.166 0.264 0.045 0.195
LexRank 0.309 0.042 0.237 0.349 0.054 0.276

Chieu 0.319 0.049 0.258 0.371 0.062 0.293
LDALR 0.342 0.062 0.291 0.392 0.073 0.325
MEALR 0.372 0.068 0.299 0.406 0.079 0.349
MEAOp 0.384 0.070 0.309 0.427 0.087 0.368

4.4 Overall Performance Comparison

We experiment with all the baselines and our frame-
work on the 6 datasets. We take the average F-
score performance in terms of 3 ROUGE-F scores:
ROUGE-1, ROUGE-2 and ROUGE-SU4. The over-
all results are shown in Figure 3 and details are listed
in Tables 3.

Figure 3 and Table 3 show the performance of
these systems on the same datasets. The local/global
optimization balance parameter χ = 0.5. From Fig-
ure 3 and Table 3 we have following observations:
• Generally, the Random gets the worst perfor-

mance;
• The LexRank system outperforms Random al-

gorithm. This is due to the fact that LexRank ranks
all the sentences based on eigenvector centrality and
the global relationship between sentences, which
tends to select the most informative sentences as the
summary.
• The results of Chieu (Chieu and Lee, 2004) sys-

tem are better than those of LexRank. This may
be mainly for the reason that Chieu used the date
dimension to filter away uninteresting sentences by
paraphrasing and defined two different ranking mea-
sures: interest and burtiness, to select top-ranked in-
formative sentences.
• The LDALR system outperforms the Chieu sys-

732



tem. This may be for the fact that Chieu’s method is
actually based on flat clustering-based summariza-
tion, which is not as effective as LDA topic model
to extract latent sub-events.

Figure 4: Examine the performance of the balance pa-
rameter χ

• The MEALR system outperforms the LDAL-
R system. This may be mainly for the reason that
MEALR utilizes the mixture-event-aspect model to
detect the more salient sub-events based on the sub-
whole relationship, which seems to satisfy users’
bias to different sub-events.
• The MEAOp system which utilizes our method

outperforms all the baselines, indicating the effec-
tiveness of detecting different types of sub-events
with mixture-event-aspect model and the necessity
to distinguish different proportions of the compo-
nent summaries based on local/global aspects.

4.5 Parameter Tuning

Figure 5: Aspect sequence of event “Connecticut school
shooting” (X-axis is the number of days after Dec. 14,
2012. Y-axis is bursty(Ak,t))

In this section, we compare the performance of
the parameters. The hyper-parameters such as α, β,
γ, λ can be estimated using standard methods intro-
duced by Minka (Minka, 2000). So we mainly ex-
amine the local/global optimization balance parame-
ter χ. We try to evaluate the influence of this param-
eter on the three kinds of ROUGE measure results
respectively. Figure 4 shows the performance of the
balance parameters χ. It is obvious that when the
balance parameter χ is set to 0.7 this method per-
forms best.

4.6 Sample Output and Case Study

Figure 6: Bursties of sub-event “Gun control debate” (X-
axis is the number of days after Dec. 14, 2012. Y-axis is
bursty(Ak,t))

We take the event “Connecticut school shooting”
as an example to show the usefulness of our method.
Figure 5 shows the aspect sequence based on the
bursty periods of all aspects. We select a sub-
event “Gun control debate” and Figure 6 shows the
bursties of this sub-event on the whole timeline. Ta-
ble 4 shows part of the storylines for the event “Con-
necticut school shooting” generated by human and
our method. Through observation, we find that the
peak of the event “Connecticut school shooting” is
around the date when it occurred, and the sub-event
“Gun control debate” has two bursty periods around
the two peaks. Compared with the human summary,
our framework can extract the important sub-events
contained in the collection, and satisfy users’ inter-
est on different sub-events based on the part-whole
relationship with the event subject.

From the sample output and the human storylines,
we also get some observations. (1) The component
summary of global aspect tend to share larger pro-

733



Table 4: Selected part of storyline generated by MEAOp and human
Storyline Generated by human:
December 14, 2012 Global Aspect
Shooting massacre occurred in a primary school in Connecticut town, Sandy hoot Newton.
20-year-old Adam lanza broke into the primary school after shotting his mother, and in 10 minutes shot more than 100 times, killing twenty children
and eight adult, including himself.
The youngest death was a children in preschool students.
December 15, 2012 Global Aspect
Photos of the teachers and students shoot in the Connecticut massacre are released as well as the shooter’s.
The shooter was very smart but lonely.
December 16-17, 2012 Local Aspect
President Obama arrived in the locality of school shooting, mourned for the victims and made a speech.
December 18-20, 2012 Local Aspect
American gun control bill was put on the agenda again.

Storyline Generated by MEAOp:
December 14, 2012 Global Aspect
Children and adults gunned down in Connecticut school massacre.
20 children, six adults and the shooter are dead after shooting at Sandy Hoot Elementary School in Newtown, Connecticut.
Three law enforcement officials say Adam Lanza, 20, was the shooter, and that he died apparently by his own hand.
Suspect’s mother, Nancy Lanza, found dead in suspects home in Newtown, law enforcement source says.
Ryan Lanza, older brother of Adam Lanza, questioned by police but not labeled a suspect.
December 15-16, 2012 Global Aspect
Victims’ names released Saturday; all of the slain children were either 6 or 7 years old.
Understanding school lockdowns in regards to Connecticut shooting.
Connecticut gunman recalled as intelligent but remote.
December 17, 2012 Local Aspect
President obama leads interfaith preyer vigil in newtown connecticut.
A tearful Obama says “we’ve endured too many of these tragedies”.
December 18-20, 2012 Local Aspect
Moderate dems join gun control debate call for commission on us violence gains.
Gun debate gains traction as some lawmakers say its time to act.

portion in the final storyline. This is mainly for
the reason that when researching for an event sub-
ject, users bias more to the information about the
global-sub-events that have closely connection and
coincident properties with the major event based on
the part-whole relationship. So it is really neces-
sary to distinguish different sub-events with distinc-
tive properties. (2) Our system performs better for
the persistent event, such as “The U.S. presidential
election”. This may be for the fact that these events
are usually long running and have more global-sub-
events than local-sub-events.

5 Conclusion

In this work, we study the task of event storyline
generation and present a novel method. We inno-
vatively introduce the properties of different sub-
events based on word co-occurrences to determine
the part-whole relationship with the major event and
develop a mixture-event-aspect (MEA) model to for-
malize different types of sub-events into local/global
aspects. Based on these local/global aspects, we uti-
lize an optimization method to get the optimal com-

ponent summaries along the aspect sequence. We
conduct experiments with our method and various
baselines on real web datasets. Through our exper-
iments we notice that our method generates over-
all better storyline than other baselines. This indi-
cates the effectiveness to detect different types of
sub-events with the proposed mixture-event-aspect
model and the necessity to distinguish different pro-
portions of the component summaries based on lo-
cal/global aspects.

Acknowledgments.
We thank the anonymous reviewers for their

valuable and constructive comments. This work
is financially supported by NSFC under the grant
NO.61272340 and 60933004.

References

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Jackie Chi Kit Cheung, Giuseppe Carenini, and Ray-
mond T Ng. 2009. Optimization-based content se-

734



lection for opinion summarization. In Proceedings of
the 2009 Workshop on Language Generation and Sum-
marisation, pages 7–14. ACL.

Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 425–432. ACM.

G. Erkan and D.R. Radev. 2004. Lexpagerank: Prestige
in multi-document text summarization. In Proceed-
ings of EMNLP, volume 4.

Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of the Fifteenth conference
on Uncertainty in artificial intelligence, pages 289–
296. Morgan Kaufmann Publishers Inc.

Giridhar Kumaran and James Allan. 2004. Text clas-
sification and named entities for new event detection.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 297–304. ACM.

Theodoros Lappas, Benjamin Arai, Manolis Platakis,
Dimitrios Kotsakos, and Dimitrios Gunopulos. 2009.
On burstiness-aware search for document sequences.
In Proceedings of the 15th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 477–486. ACM.

Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th annual meeting of the Association for Com-
putational Linguistics, pages 640–649. ACL.

C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71–78. ACL.

Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang
Chen, and Tao Li. 2012. Generating event storylines
from microblogs. In Proceedings of the 21st ACM in-
ternational conference on Information and knowledge
management, pages 175–184. ACM.

Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3-4):347–
368.

Qiaozhu Mei and ChengXiang Zhai. 2005. Discover-
ing evolutionary theme patterns from text: an explo-
ration of temporal text mining. In Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 198–207.
ACM.

Q. Mei, J. Guo, and D. Radev. 2010. Divrank: the inter-
play of prestige and diversity in information networks.

In Proceedings of the 16th ACM SIGKDD internation-
al conference on Knowledge discovery and data min-
ing, pages 1009–1018. ACM.

Thomas Minka. 2000. Estimating a dirichlet distribu-
tion.

Lawrence Page, Sergey Brin, Rajeev Motwani, and Ter-
ry Winograd. 1999. The pagerank citation ranking:
bringing order to the web.

Yulong Pei, Wenpeng Yin, et al. 2012. Generic multi-
document summarization using topic-oriented infor-
mation. In PRICAI 2012: Trends in Artificial Intel-
ligence, pages 435–446. Springer.

D.R. Radev, H. Jing, M. Styś, and D. Tam. 2004.
Centroid-based summarization of multiple documents.
Information Processing & Management, 40(6):919–
938.

Mauricio GC Resende and Renato F Werneck. 2004. A
hybrid heuristic for the p-median problem. Journal of
heuristics, 10(1):59–88.

Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th international conference on World
Wide Web, pages 111–120. ACM.

X. Wan. 2008. Document-based hits model for multi-
document summarization. PRICAI 2008: Trends in
Artificial Intelligence, pages 454–465.

Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 784–
793. ACM.

Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, pages 192–201. ACM.

Dingding Wang, Tao Li, and Mitsunori Ogihara. 2012.
Generating pictorial storylines via minimum-weight
connected dominating set approximation in multi-view
graphs. Proceddings of AAAI 2012.

Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, X-
iaoming Li, and Yan Zhang. 2011a. Timeline gener-
ation through evolutionary trans-temporal summariza-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 433–
443. ACL.

Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceedings
of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 745–754. ACM.

735


