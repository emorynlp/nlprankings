



















































Active Learning with Amazon Mechanical Turk


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546–1556,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Active Learning with Amazon Mechanical Turk

Florian Laws Christian Scheible Hinrich Schütze
Institute for Natural Language Processing

Universität Stuttgart
{lawsfn, scheibcn}@ims.uni-stuttgart.de

Abstract

Supervised classification needs large amounts
of annotated training data that is expensive to
create. Two approaches that reduce the cost
of annotation are active learning and crowd-
sourcing. However, these two approaches
have not been combined successfully to date.
We evaluate the utility of active learning in
crowdsourcing on two tasks, named entity
recognition and sentiment detection, and show
that active learning outperforms random selec-
tion of annotation examples in a noisy crowd-
sourcing scenario.

1 Introduction

Supervised classification is the predominant tech-
nique for a large number of natural language pro-
cessing (NLP) tasks. The large amount of labeled
training data that supervised classification relies on
is time-consuming and expensive to create, espe-
cially when experts perform the data annotation.
Recently, crowdsourcing services like Amazon Me-
chanical Turk (MTurk) have become available as an
alternative that offers acquisition of non-expert an-
notations at low cost. MTurk is a software service
that outsources small annotation tasks – called HITs
– to a large group of freelance workers. The cost of
MTurk annotation is low, but a consequence of us-
ing non-expert annotators is much lower annotation
quality. This requires strategies for quality control
of the annotations.

Another promising approach to the data acqui-
sition bottleneck for supervised learning is active

learning (AL). AL reduces annotation effort by set-
ting up an annotation loop where, starting from a
small seed set, only the maximally informative ex-
amples are chosen for annotation. With these an-
notated examples, the classifier is then retrained to
again select more informative examples for further
annotation. In general, AL needs a lot fewer anno-
tations to achieve a desired performance level than
random sampling.

AL has been successfully applied to a number of
NLP tasks such as part-of-speech tagging (Ringger
et al., 2007), parsing (Osborne and Baldridge, 2004),
text classification (Tong and Koller, 2002), senti-
ment detection (Brew et al., 2010), and named entity
recognition (NER) (Tomanek et al., 2007). Until
recently, most AL studies focused on simulating the
annotation process by using already available gold
standard data. In reality, however, human annota-
tors make mistakes, leading to noise in the annota-
tions. For this reason, some authors have questioned
the applicability of AL to noisy annotation scenarios
such as MTurk (Baldridge and Palmer, 2009; Re-
hbein et al., 2010).

AL and crowdsourcing are complementary ap-
proaches: AL reduces the number of annotations
used while crowdsourcing reduces the cost per an-
notation. Combined, the two approaches could sub-
stantially lower the cost of creating training sets.

Our main contribution in this paper is that we
show for the first time that AL is significantly bet-
ter than randomly selected annotation examples in
a real crowdsourcing annotation scenario. Our
experiments directly address two tasks, named en-
tity recognition and sentiment detection, but our

1546



evidence suggests that AL is of general benefit in
crowdsourcing. We also show that the effectiveness
of MTurk annotation with AL can be further en-
hanced by using two techniques that increase label
quality: adaptive voting and fragment recovery.

2 Related Work

2.1 Crowdsourcing

Pioneered by Snow et al. (2008), Crowdsourcing,
especially using MTurk, has become a widely used
service in the NLP community. A number of stud-
ies have looked at crowdsourcing for NER. Voyer et
al. (2010) use a combination of expert and crowd-
sourced annotations. Finin et al. (2010) annotate
Twitter messages – short sequences of words – and
this is reflected in their vertically oriented user in-
terface. Lawson et al. (2010) choose an annotation
interface where annotators have to drag the mouse
to select entities. Carpenter and Poesio (2010) ar-
gue that dragging is less convenient for workers than
marking tokens.

These papers do not address AL in crowdsourc-
ing. Another important difference is that previous
studies on NER have used data sets for which no
“linguistic” gold annotation is available. In con-
trast, we reannotate the CoNLL-2003 English NER
dataset. This allows us to conduct a detailed com-
parison of MTurk AL to conventional expert anno-
tation.

2.2 Active Learning with Noisy Labels

Hachey et al. (2005) were among the first to in-
vestigate the effect of actively sampled instances
on agreement of labels and annotation time. They
demonstrate applicability of AL when annotators are
trained experts. This is an important result. How-
ever, AL depends on accurate assessments of uncer-
tainty and informativeness and such an accurate as-
sessment is made more difficult if labels are noisy
as is the case in crowdsourcing. For this reason, the
problem of AL performance with noisy labels has
become a topic of interest in the AL community. Re-
hbein et al. (2010) investigate AL with human expert
annotators for word sense disambiguation, but do
not find convincing evidence that AL reduces anno-
tation cost in a realistic (non-simulated) annotation
scenario. Brew et al. (2010) carried out experiments

on sentiment active learning through crowdsourcing.
However, they use a small set of volunteer labelers
instead of anonymous paid workers.

Donmez and Carbonell (2008) propose a method
to choose annotators from a set of noisy annotators.
However, in a crowdsourcing scenario, it is not pos-
sible to ask specific annotators for a label, as crowd-
sourcing workers join and leave the site. Further-
more, they only evaluate their approach in simula-
tions. We use the actual labels of human annotators
to avoid the risk of unrealistic assumptions when
modeling annotators.

We are not aware of any study that shows that AL
is significantly better than a simple baseline of hav-
ing annotators annotate randomly selected examples
in a highly noisy annotation setting like crowdsourc-
ing. While AL generally is superior to this base-
line in simulated experiments, it is not clear that
this result carries over to crowdsourcing annotation.
Crowdsourcing differs in a number of ways from
simulated experiments: the difficulty and annotation
consistency of examples drawn by AL differs from
that drawn by random sampling; crowdsourcing la-
bels are noisy; and because of the noisiness of labels
statistical classifiers behave differently in simulated
and real annotation experiments.

3 Annotation System

One fundamental design criterion for our annotation
system was the ability to select examples in real time
to support, e.g., the interactive annotation experi-
ments presented in this paper. Thus, we could not
use the standard MTurk workflow or services like
CrowdFlower.1

We therefore designed our own system for anno-
tation experiments. It consists of a two-tiered ap-
plication architecture. The frontend tier is a web
application that serves two purposes. First, the ad-
ministrator can manage annotation experiments us-
ing a web interface and publish annotation tasks as-
sociated with an experiment on MTurk. The front-
end also provides tools for efficient review of the
received answers. Second, the frontend web appli-
cation presents annotation tasks to MTurk workers.
Because we wanted to implement interactive anno-
tation experiments, we used the “external question”

1http://crowdflower.com/

1547



feature of MTurk. An external question contains
an URL to our frontend web application, which is
queried when a worker views an annotation task.
Our frontend then in turn queries our backend com-
ponent for an example to be annotated and renders it
in HTML.

The backend component is responsible for selec-
tion of an example to be annotated in response to a
worker’s request for an annotation task. The back-
end implements a diverse choice of random and ac-
tive selection strategies as well as the multilabel-
ing strategies described in section 3.2. The backend
component runs as a standalone server and is queried
by the frontend via REST-like HTTP calls.

For the NER task, we present one sentence per
HIT, segmented into tokens, with a select box under-
neath each token containing the classes. The defini-
tion of the classes is based on the CoNLL-2003 an-
notation guidelines (Tjong Kim Sang and De Meul-
der, 2003). Examples were given for every class.
Annotators are forced to make a selection for upper-
case tokens. Lowercase tokens are prelabeled with
“O” (no named entity), but annotators are encour-
aged to change this label if the token is in fact part
of an entity phrase.

For sentiment annotation, we found in prelim-
inary experiments that using simple radio button
selection for the choice of the document label
(positive or negative) leads to a very high
amount of spam submissions, taking the overall clas-
sification accuracy down to around 55%. We then
designed a template that forced annotators to type
the label as well as a randomly chosen word from
the text. Individual label accuracy was around 75%
in this scheme.

3.1 Concurrent example selection

AL works by setting up an interactive annotation
loop where at each iteration, the most informative
example is selected for annotation. We use a pool-
based AL setup where the most informative exam-
ple is selected from a pool of unlabeled examples.
Informativeness is calculated as uncertainty (Lewis
and Gale, 1994) using the margin metric (Schein
and Ungar, 2007). This metric chooses examples for
which the margin of probabilities from the classifier
between the two most probable classes is the small-
est:

Mn = |P̂ (c1|xn)− P̂ (c2|xn)|

Here, xn is the instance to be classified, c1 and c2
are the two most likely classes, and P̂ the classifier’s
estimate of probability.

For NER, the margins of the tokens are averaged
to get an uncertainty assessment of the sentence. For
sentiment, whole documents are classified, thus un-
certainties can be used directly.

After annotation, the selected example is removed
from the unlabeled pool and, together with its la-
bel(s), added to the set of labeled examples. The
classifier is then retrained on the labeled examples
and the informativeness of the remaining examples
in the pool is re-evaluated.

Depending on the classifier and the sizes of pool
and labeled set, retraining and reevaluation can take
some time. To minimize wait times, traditional AL
implementations select examples in batches of the
n most informative examples. However, batch se-
lection might not give the optimum selection (exam-
ples in a batch are likely to be redundant, see Brinker
(2003)) and wait times can still occur between one
batch and the next.

When performing annotation with MTurk, wait
times are unacceptable. Thus, we perform the re-
training and uncertainty rescoring concurrently with
the annotation user interface. The unlabeled pool is
stored in a priority queue that is ordered according to
the examples’ informativeness. The annotation user
interface takes the most informative example from
the pool and presents it to the annotator. The la-
beled example is then inserted into a second queue
that feeds and updates retraining and rescoring pro-
cesses. The pool queue then is resorted according to
the new informativeness. In this way, annotation and
example selection can run in parallel. This is similar
to Haertel et al. (2010).

3.2 Adaptive voting and fragment recovery

MTurk labels often have a high error rate. A com-
mon strategy for improving label quality is to ac-
quire multiple labels by different workers for each
example and then consolidate the annotations into
a single label of higher quality. To trade off num-
ber of annotated examples against quality of anno-
tations, we adopt adaptive voting. It uses majority

1548



NER Sentiment
Budget 5820 6931 1130 1756

#train F1 cost/sent w.-accuracy #train F1 #train Acc cost/doc w.-accuracy #train Acc
RS 1 S 5820 59.6 1.00 51.6 – – 1130 70.4 1 74.8 – –

2 3-v 1624 61.4† 3.58 70.1 – – – – – – – –
3 5/4-v 1488 63.0† 3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.2
4 5-v+f 1996 63.6† 2.91 71.8 2385 64.9† – – – – – –

AL 5 S 5820 67.0 1.00 66.5 – – 1130 74.8 1 76.0 – –
6 3-v 1808 70.0† 3.21 78.8 – – – – – – – –
7 5/4-v 1679 70.4† 3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.8
8 5-v+f 2165 70.5 2.68 79.3 2691 71.2 – – – – – –

Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F1 evaluated on
CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-
and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment
budget 1756 averaged over 2 runs.

voting and is adaptive in the number of repeated an-
notations. For NER, a sentence is first annotated by
two workers. Then majority voting is performed for
each token individually. If there is a majority for ev-
ery token that is greater than an agreement threshold
α, the sentence is accepted with each token labeled
with the majority label. Otherwise additional anno-
tations are requested. A sentence is discarded if the
number of repeated annotations exceeds a discard
threshold d (d-voting).2 We use the same scheme
for sentiment; note that there is just one decision per
HIT in this case, not several as in NER.

For NER, we also use fragment recovery: we sal-
vage tokens with agreeing labels from discarded sen-
tences. We cut the token sequence of a discarded
sentence into several fragments that have agreeing
tokens and discard only those parts that disagree. We
then include these recovered fragments in the train-
ing data just like complete sentences.

Software release. Our active learning framework
used can be downloaded at http://www.ims.
uni-stuttgart.de/˜lawsfn/active/.

4 Experiments, Results and Analysis

4.1 Experiments

In our NER experiments, we have workers reanno-
tate the English corpus of the CoNLL-2003 NER
shared task. We chose this corpus to be able to com-
pare crowdsourced annotations with gold standard

2It can take a while in this scheme for annotators to agree
on a final annotation for a sentence. We make tentative labels
of a sentence available to the classifier immediately and replace
them with the final labels once voting is completed.

annotations. A HIT is one sentence and is offered
for a base payment of $0.01. We filtered out answers
that contained unannotated tokens or were obvious
spam (e.g., all tokens labeled as MISC). For test-
ing NER performance, we used a system based on
conditional random fields with standard named en-
tity features including the token itself, orthographic
features like the occurrence of capitalization or spe-
cial characters and context information about the to-
kens to the left/right of the current token.

The sentiment detection task was modeled after a
well-known document analysis setup for sentiment
classification, introduced by Pang et al. (2002). We
use their corpus of 1000 positive and 1000 negative
movie reviews and the Stanford maximum entropy
classifier (Manning and Klein, 2003) to predict the
sentiment label of each document d from a unigram
representation of d. We randomly split this corpus
into a test set of 500 reviews and an active learn-
ing pool of 1500 reviews. Each HIT consists of one
document, valued at $0.01.

We compare random sampling (RS) and AL in
combination with the proposed voting and fragment
strategies with different parameters. We want to
avoid rerunning experiments on MTurk over and
over again, but on the other hand, we believe that us-
ing synthetic data for simulations is problematic be-
cause it is difficult to generate synthetic data with a
realistic model of annotator errors. Thus, we logged
a play-by-play record of the annotator interactions
and labels. With this recording, we can then rerun
strategies with different parameters.

We chose voting with at most d = 5 repetitions as
1549



our main reannotation strategy for both random and
active sampling for NER annotation. We use simple
majority voting (α = .5) for NER.

For sentiment, we set d = 4 and minimum agree-
ment α = .75 because the number of labels is
smaller (2 vs. 5) and so random agreement is more
likely for sentiment.

To get results for 3-voting NER, we take the
recording and discard 5-voting votes not needed in
3-voting. This will result in roughly the same num-
ber of annotated sentences, but at a lower cost. This
simulation of 3-voting is not exactly what would
have happened on MTurk (e.g., the final vote on a
sentence might be different, which then influences
AL example selection), but we will assume that dif-
ferences are rare and simulated and actual results
are similar. The same considerations apply to sin-
gle votes and to the sentiment experiments.

We always compare two strategies for the same
annotation budget. For example, the number of
training sentences in Table 1 differ in the two rel-
evant columns, but all strategies compared use ex-
actly the same annotation budget (5820, 6931, 1130,
and 1756, respectively).

For the single annotation strategy, each interac-
tion record contained only about 40% usable anno-
tations, the rest were repeats. A comparison with
the single annotation strategy over approx. 2000 sen-
tences or 450 documents would not have been mean-
ingful; therefore we chose to run an extra experiment
with the single annotation strategy to match this up
with the budgets of the voting strategies. The re-
sults are presented in two separate columns of Ta-
ble 1 (budgets 6931 and 1756).

4.2 Results

For sentiment detection, worker accuracy or label
quality – the percentage of correctly annotated doc-
uments – is 74.8. In contrast, for NER, worker accu-
racy – the percentage of non-O tokens annotated cor-
rectly – is only 51.6 (Table 1, line 1). This demon-
strates the challenge of using MTurk for NLP an-
notation tasks. When we use single annotations of
each sentence, NER performance is 59.6 F1 for ran-
dom sampling (line 1). When training with gold la-
bels on the same sentences, the performance is 80.0
(not shown). This means we lose more than 20%
due to poor worker accuracy. Adaptive voting and

fragment recovery manage to recover a small part of
the lost performance (lines 2–4); each of the three
F1 scores is significantly better than the one above
it as indicated by † (Approximate Randomization
Test (Noreen, 1989; Chinchor et al., 1993) as im-
plemented by Padó (2006)).

Using AL turns out to be quite successful for NER
performance. For single annotations, NER perfor-
mance is 67.0 (line 5), an improvement of 7.4%
compared to random sampling. Adaptive voting
and fragment recovery again increase worker accu-
racy (lines 6–8) although total improvement of 3.5%
(lines 8 vs. 5) is smaller than 4% for random (lines
4 vs. 1). The learning curves of AL vs. random in
Figure 1 (top left) confirm this good result for AL.
These learning curves are for tokens – not for sen-
tences – to show that the reason for AL’s better per-
formance is not that it selects slightly longer sen-
tences than random. In addition, the relative advan-
tage of AL vs random decreases over time, which is
typical of pool-based AL experiments.

We carried out two runs of the same experiment
for sentiment to validate our first positive result since
the difference between the two conditions is not as
large as in NER (Figure 1, top right). After about
300 documents, active learning consistently outper-
forms random sampling. The first AL run performs
better because of higher label quality in the begin-
ning. The overall advantage of AL over random
is lower than for NER because the set of labels is
smaller in sentiment, making the classification task
easier. Second, there is a large amount of simple lex-
ical clues for detecting sentiment (cf. Wilson et al.
(2005)). It is likely that some of them can be learned
well through random sampling at first; however, ac-
tive learning can gain accuracy over time because it
selects examples with more difficult clues.

In Figure 1 (bottom), we compare single annota-
tion with adaptive voting. The graphs show F1 as
a function of cost. Adaptive voting trades quantity
of sampled sentences for quality of labels and thus
incurs higher net costs per sentence. This results in
a smaller dataset for a given budget, but this dataset
is still more useful for classifier training. For NER
(Figure 1, bottom left), the single annotation strat-
egy has a faster start; so for small budgets, cover-
ing a somewhat larger portion of the sample space
is beneficial. For larger budgets, however, quality of

1550



0 5000 10000 15000 20000

0.
4

0.
5

0.
6

0.
7

0.
8

Tokens

F
−

S
co

re

active, 5−voting
random, 5−voting

0 200 400 600

0.
4

0.
5

0.
6

0.
7

0.
8

0.
9

Documents

A
cc

ur
ac

y

active 1
active 2
random 1
random 2

0 1000 2000 3000 4000 5000 6000

0.
4

0.
5

0.
6

0.
7

0.
8

Cost

F
−

S
co

re

single ann.
3−voting
5−voting
5−voting +frags

0 500 1000 1500 2000

0.
4

0.
5

0.
6

0.
7

0.
8

0.
9

Cost

A
cc

ur
ac

y

single ann.
4−voting

Figure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right). Bottom: Active
learning: adaptive voting vs. single annotation for NER (left) and sentiment (right).

the voted labels trumps quantity.
For sentiment (Figure 1, bottom right), results are

similar: voting has no benefit initially, but as find-
ing maximally informative examples to annotate be-
comes harder in later stages of learning, adaptive
voting gains an advantage over single annotations.

The main result of the experiment is that active
learning is better by about 7% F1 than random sam-
pling for NER and by 2.6% accuracy for sentiment
(averaged over two runs at budget 1756). Adaptive
voting further improves AL performance for both
NER and sentiment.

4.3 Annotation time per token

Most AL work assumes constant cost per annotation
unit. This assumption has been questioned because
AL often selects hard examples that take longer to
annotate (Hachey et al., 2005; Settles et al., 2008).

In annotation with MTurk, cost is not a function

of annotation time because workers are paid a fixed
amount per HIT. Nevertheless, annotation time plays
a part in whether workers are willing to work on a
given task for the offered reward. This is particularly
problematic for NER since workers have to examine
each token individually. We therefore investigate
for NER whether the time MTurk workers spend on
annotating sentences differs for random vs. AL.

We first compute median and mean annotation
times and number of tokens per sentence:

sec/sentence tokens/sentence
strategy median mean all required

random 17.2 33.1 15.0 3.4
AL 17.8 33.0 17.7 4.0

We see that most sentences are annotated in a very
short time; but the mean is much larger than the me-
dian because there are outliers of up to eight min-
utes. AL tends to select slightly longer sentences as

1551



0 500 1000 1500 2000

0.
4

0.
5

0.
6

0.
7

0.
8

0.
9

Sentences

F
−

S
co

re

gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels

0 200 400 600

0.
50

0.
60

0.
70

0.
80

Documents

A
cc

ur
ac

y

gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels

Figure 3: Performance on gold labels. Left: NER. Right: sentiment (run 1).

0 2 4 6 8 10 13 16 19 23 29

0
10

0
20

0
30

0
40

0

Number of uppercase tokens

A
nn

ot
at

io
n 

tim
e 

(s
ec

on
ds

)

random
active

Figure 2: Annotation time vs. # uppercase tokens

well as sentences with slightly more uppercase to-
kens that require annotation.

In a more detailed analysis, we attempt to distin-
guish between (i) the effect of more uppercase (“an-
notation required”) tokens vs. (ii) the effect of ex-
ample difficulty. We fit a linear regression model
to annotation time vs. the number of uppercase to-
kens. For the regression fit, we removed all annota-
tion times > 60 seconds. Such long times indicate
distraction of the worker and are not a reliable mea-
sure of difficulty.

Figure 2 shows the distribution of annotation
times for both cases combined and the fitted models
for each. The model estimated an annotation time of

2.3 secs for each required token for random vs. 2.7
secs for AL. We conclude that the difference in dif-
ficulty between sentences selected by random sam-
pling vs. AL is small, but noticeable.

4.4 Influence of noise on the selection process

While NER performance for AL is much higher than
for random sampling, it is still quite a bit lower than
what is possible on gold labels. In the case of AL,
there are two reasons why this happens: (i) The
noisy labels negatively affect the classifier’s ability
to learn a good model that is used for classifying the
test set. (ii) The noisy labels result in bad interme-
diate models that then select suboptimal examples
to be annotated next. The AL selection process is
“misled” by the noisy examples.

We conduct an experiment to determine the con-
tribution of factors (i) and (ii) to the performance
loss. First, we preserve the sequence of sentences
chosen by our AL experiments on MTurk, with 5-
voting for NER and 4-voting for sentiment but re-
place the noisy worker-provided labels by gold la-
bels. The performance of classifiers trained on this
sequence is the dashed line “MTurk selection, gold
labels” in Figure 3 for NER (left) and sentiment
(right).

Second, we compare with a traditional simulated
AL experiment with gold labels. Here, the selection
too is controlled by gold labels, so the selection has
a noiseless classifier available for scoring and can
perform optimal uncertainty selection. These are the

1552



1 5 10 50 100 500

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

Number of sentences

Q
ua

lit
y 

(%
 c

or
re

ct
 e

nt
ity

 to
ke

ns
)

1 2 5 10 20 50 100 200

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

Number of documents

Q
ua

lit
y 

(%
 c

or
re

ct
 d

oc
um

en
t l

ab
el

s)

Figure 4: Worker accuracy vs. number of HITs. Each point corresponds to one worker (◦ = active, +
=random sampling; black and grey for different runs). Left: NER. Right: Sentiment.

dotted lines “gold selection, gold labels” in Figure 3.
We used a batch-mode AL setup for this compari-

son experiment. For a fair comparison, we adjust the
batchsize to be equal to the average staleness of a se-
lected example in concurrent MTurk active learning.
The staleness of an example is defined as the num-
ber of annotations the system has received, but not
yet incorporated in the computation of an example’s
uncertainty score (Haertel et al., 2010).

For our concurrent NER system, the average stal-
eness of an example was about 12 (min: 1, max: 40),
for sentiment it was about 2. The figure for NER is
higher than the number cited by Haertel et al. (2010)
because there are more annotators accessing our sys-
tem at the same time via MTurk but not as high for
sentiment since documents are longer and retraining
the sentiment classifier is faster. The average stale-
ness of an example in a batch-mode system is half
the batch size. Thus, we set the batch size of our
comparison system to 25 for NER and to 4 for sen-
timent.

Returning to the two factors introduced above –
(i) final effect of noise on test set performance vs.
(ii) intermediate effect of noise on example selec-
tion – we see in Figure 3 that (i) has a large effect
on NER whereas (ii) has a noticeable, but small ef-
fect.3 For example, at 1966 sentences, F1 scores are

3Our comparison unit for NER is the sentence. We can-
not compare on cost here since we do not know what the per-
sentence cost of a “gold” expert annotation is.

70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9
(gold-gold). This means that a performance differ-
ence of 10 points F1 has to be attributed to noisy
labels resulting in a worse final classifier (effect i),
and another 3.5 points are lost due to sub-optimal
example selection (effect ii).

For sentiment, the results are different. There is
no clear difference between the three runs. We at-
tribute this to the fact that the quality of the labels
is higher in sentiment than in NER. Our initial ex-
periments on sentiment were all negative (showing
no improvement of AL compared to random) be-
cause label quality was too low. Only after we intro-
duced the template described in Section 3 and used
4-voting with α = .75 did we get positive results for
AL. This leads to an overall label quality of about
90% (over all runs) which is so high that the differ-
ence to using gold labels is small if present at all.

5 Worker Quality

So far we have assumed that all workers provide
annotations of the same quality. However, this is
not the case. Figure 4 shows plots of worker accu-
racy as a function of worker productivity (number
of annotated examples). Some workers submit only
one or two HITs just to try out the task. For NER,
the majority of workers submit between 5 and 10
sentences, with label qualities between 0.5 and 0.8.
The chance level for correctness is around 0.25 (four

1553



different named entity categories for uppercase to-
kens). For sentiment, most workers submit 1 to 5
documents, with label qualities between 0.5 and 1.
Chance level lies at around 0.5 (for two equally dis-
tributed labels).

While quality for highly productive workers is
mediocre in our experiments, other researchers have
found extremely bad quality for their most prolific
workers (Callison-Burch, 2009). Some of these
workers might be spammers who try to submit an-
swers with automatic scripts. We encountered some
spammers that our heuristics did not detect (shown
in the bottom-right areas of Figure 4, left), but the
voting mechanism was able to mitigate their nega-
tive influence.

Given the large variation in Figure 4, using worker
quality in crowdsourcing for improved training set
creation seems promising. We now test two such
strategies for NER in an oracle setup.

5.1 Blocking low-quality workers

A simple approach is to refuse annotations from
workers that have been determined to provide low
quality answers. We simulated this strategy on NER
data using oracle quality ratings. We chose NER be-
cause of its lower overall label quality. The re-
sults are presented in Figure 5 for random (a) and
AL (b). For random, quality filtering with low cut-
offs helps by removing bad annotations that likely
come from spammers. While the voting strategy
prevented a performance decrease with bad anno-
tations, it needed to expend many extra annotations
for correction. With filtering, these extra annotations
become unnecessary and the system can learn faster.
When low-quality workers are less active, as in the
AL dataset, we find no meaningful performance in-
crease for low cutoffs up to 0.4. For very high cut-
offs (0.7), the beginning of the performance curve
shows that further cost reductions can be achieved.
However, we did not have enough recorded human
annotations available to perform a simulation for the
full budget.

5.2 Trusting high-quality workers

The complementary approach is to take annotations
from highly rated workers at face value and imme-
diately accept them as the correct label, bypassing
the voting procedure. Bypassing saves the cost of

repeated annotation of the same sentence. Figure 5
shows learning curves for two bypass thresholds on
worker quality (measured as proportion of correct
non-O tokens) for random (c) and AL (d). Bypass-
ing performs surprisingly well. We find a steeper
rise of the learning curve, meaning less cost for the
same performance. Not only do we find substantial
cost reductions, but also higher overall performance.
We believe this is because high-quality annotations
can sometimes be voted down by other annotations.
If we can identify high-quality workers and directly
use their annotations, this can be avoided.

These experiments are oracle experiments using
gold data that is normally not available. In future
work, we would like to repeat the experiments using
methods for worker quality estimation (Ipeirotis et
al., 2010; Donmez et al., 2009). For AL, the choice
as to which labels are used (as a result of voting, by-
passing or other) also has an influence on the selec-
tion. However, we had to keep the sequence of the
selected sentences fixed in the simulations reported
above. While our method of sample selection for
AL proved to be quite robust even in the presence
of noise, higher quality labels do have an influence
on the sample selection (see section 4.4), so the im-
provement could be even better than indicated here.

5.3 Differences in quality between AL and
random

The essence of AL is to select examples that are dif-
ficult to classify. As observed in our experiments
on annotation time, this difficulty is reflected in the
amount of time a human needs to work on examples
selected through AL. Another effect to expect from
difficulty could be lower annotation accuracy. We
therefore examined the accuracies for each worker
who contributed to both the AL and the random ex-
periment. We found that in the NER task, the 20
workers in this group had a slightly higher (0.07) av-
erage quality for randomly selected examples. This
difference is low and does not suggest a significant
drop in accuracy for examples selected in AL.

6 Conclusion

We have investigated the use of AL in a real-life
annotation experiment with human annotators in-
stead of traditional simulations with gold labels for

1554



(a) (b) (c) (d)

0 1000 2000 3000 4000

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

Cost

F
−

S
co

re

baseline 5−voting
min. quality 0.1
min. quality 0.4
min. quality 0.7

0 1000 2000 3000 4000
0.

50
0.

55
0.

60
0.

65
0.

70
0.

75

Cost

F
−

S
co

re

baseline 5−voting
min. quality 0.1
min. quality 0.4
min. quality 0.7

0 1000 2000 3000 4000

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

Cost

F
−

S
co

re

baseline 5−voting
bypass 0.9
bypass 0.7

0 1000 2000 3000 4000

0.
50

0.
55

0.
60

0.
65

0.
70

0.
75

Cost

F
−

S
co

re

baseline 5−voting
bypass 0.9
bypass 0.7

Figure 5: Blocking low-quality workers: (a) random, (b) AL. Bypass voting: (c) random, (d) AL.

named entity recognition and sentiment classifica-
tion. The annotation was performed using MTurk in
an AL framework that features concurrent example
selection without wait times. We also evaluated two
strategies, adaptive voting and fragment recovery, to
improve label quality at low additional cost. We find
that even for the relatively high noise levels of anno-
tations gathered with MTurk, AL is successful, im-
proving performance by +6.9 points F1 compared to
random sampling for NER and by +2.6% accuracy
for sentiment. Furthermore, this performance level
is reached at a smaller MTurk cost compared to ran-
dom sampling. Thus AL not only reduces annotation
costs, but also offers an improvement in absolute
performance for these tasks. This is clear evidence
that active learning and crowdsourcing are comple-
mentary methods for lowering annotation cost and
should be used together in training set creation for
natural language processing tasks.

We have also conducted oracle experiments that
show that further performance gains and cost sav-
ings can be achieved by using information about
worker quality. We plan to confirm these results by
using estimates of quality in the future.

7 Acknowledgments

Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Christian Scheible is supported by the Deutsche
Forschungsgemeinschaft project Sonderforschungs-
bereich 732.

References

Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based eval-
uation of cost-reduction strategies for language docu-
mentation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 296–305.

Anthony Brew, Derek Greene, and Pádraig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. In Proceeding of the
2010 conference on ECAI 2010: 19th European Con-
ference on Artificial Intelligence, pages 145–150.

Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on Ma-
chine Learning (ICML 2003), pages 59–66.

Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon’s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286–295.

Bob Carpenter and Massimo Poesio. 2010. Models
of data annotation. Tutorial at the seventh interna-
tional conference on Language Resources and Eval-
uation (LREC 2010).

Nancy Chinchor, David D. Lewis, and Lynette
Hirschman. 1993. Evaluating message understanding
systems: an analysis of the third message understand-
ing conference (muc-3). Computational Linguistics,
19(3):409–449.

Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 619–628.

Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-
der. 2009. Efficiently learning the accuracy of la-

1555



beling sources for selective sampling. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 259–
268.

Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 80–88.

Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ’05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144–151.

Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin
Seppi. 2010. Parallel active learning: Eliminating
wait time with minimal staleness. In Proceedings of
the NAACL HLT 2010 Workshop on Active Learning
for Natural Language Processing, pages 33–41.

Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation (HCOMP ’10).

Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 71–79.

David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3–12.

Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, pages 8–8.

Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: an introduction. Wiley.

Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 89–
96.

Sebastian Padó, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the

2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79–86.

Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 949–957.

Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101–108.

Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235–265.

Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069–1078.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 254–263.

Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL (CoNLL 2003), pages 142–147.

Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486–495.

Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. The Journal of Machine Learning Re-
search, 2:45–66.

Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-
nah Copperman. 2010. A hybrid model for anno-
tating named entity training corpora. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
243–246.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347–354.

1556


