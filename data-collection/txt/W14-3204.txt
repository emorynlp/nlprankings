



















































Aided diagnosis of dementia type through computer-based analysis of spontaneous speech


Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 27–37,
Baltimore, Maryland USA, June 27, 2014. c©2014 Association for Computational Linguistics

Aided Diagnosis of Dementia Type through Computer-Based Analysis 
of Spontaneous Speech 

 
William Jarrold 

Nuance Communications1 
william.jarrold@gmail.com 

Bart Peintner  
Soshoma1 

bpeintner@gmail.com 

David Wilkins  
Language & Linguistic Consulting 
wilkinsdavidp@gmail.com 

Dimitra Vergryi and Colleen Richey 
SRI International 

dverg@speech.sri.com, 
colleen.richey@sri.com 

Maria Luisa Gorno-Tempini and Jennifer Ogar 
University of California, San Francisco 

{marilu|jogar}@memory.ucsf.edu 

 
 

Abstract 

This pilot study evaluates the ability of machined 
learned algorithms to assist with the differential 
diagnosis of dementia subtypes based on brief (< 
10 min) spontaneous speech samples.  We ana-
lyzed1recordings of a brief spontaneous speech 
sample from 48 participants from 5 different 
groups: 4 types of dementia plus healthy con-
trols.  Recordings were analyzed using a speech 
recognition system optimized for speaker-
independent spontaneous speech. Lexical and 
acoustic features were automatically extracted. 
The resulting feature profiles were used as input 
to a machine learning system that was trained to 
identify the diagnosis assigned to each research 
participant.  Between groups lexical and acoustic 
differences features were detected in accordance 
with expectations from prior research literature 
suggesting that classifications were based on fea-
tures consistent with human-observed sympto-
matology. Machine learning algorithms were 
able to identify participants' diagnostic group 
with accuracy comparable to existing diagnostic 
methods in use today. Results suggest this clini-
cal speech analytic approach offers promise as an 
additional, objective and easily obtained source 
of diagnostic information for clinicians. 

1 Introduction 
Accurately differentiating certain neurodegenera-
tive disorders such as Alzheimer’s Disease (AD) 
and variants of Fronto-temporal Lobar Degener-
ation (FTLD) is extremely difficult (Varma et 
al., 1999). Differential diagnosis is often left to 
tertiary care settings (e.g. Research I Universities 
with medical schools). While the most definitive 
diagnosis is made post-mortem using brain tissue 
                                                
1 Research conducted while at SRI International 

samples, the treatment and prognostic implica-
tions of living patients are often determined in 
large part on the basis of language assessment.  
 
Although language is clearly not the exclusive 
diagnostic factor for AD, existing literature sug-
gests it is an important one. Studies show sig-
nificant differences in the written language abili-
ties of AD patients and healthy older adults (Pes-
tell et al., 2008 and Platel et al., 1993). The 
speech of patients with AD is partly character-
ized by word-finding difficulties, smaller vocab-
ularies, and problems with semantic processing 
(Forbes at el., 2002). These symptoms appear 
early in the disease’s progression, however lan-
guage assessment of AD patients can fail to iden-
tify early symptoms that family members report 
to be present in their conversations (Crockford 
and Lesser, 1994).  
 
FTLD has a prevalence similar to AD in patients 
under the age of 65 years (Mendez at el., 1993). 
Misdiagnosis of FTLD is common Mendez at el., 
1993). Three variants are defined by the widely 
adopted Neary criteria (Neary at el., 1998); one 
with altered social conduct, the behavioral vari-
ant of frontotemporal dementia (bvFTD); the 
second characterized by a deterioration of con-
ceptual-semantic knowledge, semantic dementia 
(SD); and the third marked by a disorder of ex-
pressive language fluency, progressive non-
fluent aphasia (PNFA).  
 
Clinicians diagnose using a wide array of evi-
dence including patient history, imaging and 
neuropsychological assessment in which speech 
and language diagnostics feature prominently.  In 
AD, cognitive disturbance is a required diagnos-
tic feature and language impairment one several 
sufficient signs of such impairment.  In the case 

27



of SD and PNFA, changes in speech and lan-
guage are core diagnostic features, with changes 
in lexical content features being highly diagnos-
tic of SD, and changes in the acoustic properties 
of speech being highly diagnostic of PNFA. 
Even in bvFTD, where changes in social behav-
ior are the defining features, analysis of lan-
guage-based differences is important, because 
language is an essential mediator of social be-
havior. To be sure, the clinician does not diag-
nose exclusively on language features -- patient 
history, imaging, memory functioning and more 
play a role.  However, language does feature 
prominently in the differential diagnosis of AD, 
FTLD and its three subtypes.  For this reason, 
computerized analysis of speech may offer an 
important aid to the clinical diagnosis of these 
syndromes. 
 
Prior work in clinical speech analytics supports 
the possibility of computer-based diagnosis of 
dementia related syndromes. Singh (2001) de-
scribes a means of quantifying the degree of 
speech deficits derived from human transcrip-
tions of the speech of patient with AD. Machine 
Learning has already been applied to distinguish 
AD from controls using human transcribed spon-
taneous speech (Thomas at el., 2005). Abel et al. 
(2009) applied a connectionist net that models 
patient speech errors (naming and repetition dis-
orders) to the problem of diagnosis. Tur et al. 
(2010) have shown the ability to automatically 
score patient speech from a story recall and pic-
ture description task that is on par with human 
performance. Lehr et al. (2012) have developed a 
system that automatically transcribes and scores 
patient speech obtained during the story recall 
portion of the Wechsler Logical Memory test.  
The evaluation demonstrated it could distinguish 
mild cognition impairment from typical controls 
at performance level comparable to human scor-
ers.  
 
Our work builds upon these prior studies along a 
number of dimensions.  First, we distinguish be-

tween a wider array of dementia subtypes, i.e. 
not only AD vs controls, but also the three sub-
types of FTLD. Second, we use not just lexical 
features but also acoustic/prosodic related fea-
tures.  Third, in order to shed light on the opaque 
“black box” nature of many machine-learned 
classifiers, we identify relationships between 
model features and symptoms from the clinical 
literature.  Fourth, our approach can claim to be 
more ecologically valid because it analyzes spon-
taneous speech as input rather than recall of a 
remembered passage.  Fifth, we do not require 
human transcription - a labor-intensive step that 
hinders broader use in a clinical setting. Sixth we 
provide a comparison of our system performance 
against benchmarks obtained from practicing 
clinicians. Our paper is the first we know of to 
exhibit all of the above properties. 
 
In sum we used computational techniques to ana-
lyze acoustic and lexical features of the speech of 
patients with AD and FTLD variants, and we 
investigated whether models derived from these 
features via machine learning could accurately 
identify a patient’s diagnosis.  

2 METHOD 
2.1 Participant Recruitment and Diagnosis 
We obtained spontaneous speech data from 9 
controls, 9 AD patients and 30 FTLD patients—9 
with frontotemporal dementia (bvFTD), 13 with 
semantic dementia (SD), and 8 with progressive 
nonfluent aphasia (PNFA). Table 1 shows demo-
graphic information. 
 
Data were collected in an ongoing series of NIH-
funded studies being performed at the UCSF 
Memory and Aging Center. Patients were diag-
nosed by expert clinicians at the center by apply-
ing current clinical criteria. Patients underwent 
detailed standard speech and language, cognitive, 
emotional, genetic, pathological, and neuroimag-
ing evaluations. Age-matched healthy controls 
were community volunteers obtained by SRI In-

 bvFTD PNFA SD AD Controls 
Male/Female 5/4 1/7 6/7 5/4 3/6 

Age 63.00(8.25) 62.88(7.75) 65.23(6.61) 59.11(7.47) 61.7(6.0) 
Education * 17.33(1.73) 16.13(2.30) 16.45(2.54) 15.44(2.30) 17.27(2.1) 

MMSE 24.4(5.85) 22.0(9.34) 17.09(8.15) 18.67(7.53) Not Adminis-tered 

Table 1. Demographic information for participants 

28



ternational and were paid $10 for their participa-
tion.  

2.2 Speech Samples 
Speech samples were recordings of Part 1 of the 
Western Aphasia Battery (Kertesz, 1980). Partic-
ipants are administered a semi-structured inter-
view (e.g., questions such as “How are you?”) 
and asked to describe a drawn picture of a picnic 
scene. The resulting 3 to 5 minutes of speech was 
recorded via wireless lapel microphones. Con-
trols were recorded via digital audio recorder 
sampling at 48 kHz, 16 bit PCM, and later down-
sampled to 16 kHz for use with the speech rec-
ognizer. Digital audio was down-sampled at 16 
kHz, 16 bit PCM. Recordings were manually 
segmented in order to separate the interviewee’s 
voice from the interviewer’s.  Only patient 
speech segments were subject to analysis 

2.3 Procedure 
To tackle speech-based diagnosis of AD, bvFTD, 
SD and PNFA, we employ several types of com-
puter-based analyses (see Figure 1). Audio re-
cordings were processed via the Meeting Under-
standing system (Stolcke at el., 2007), which was 
custom-tailored to recognize speaker-
independent, multi-person speech. First, using 
this system we perform acoustic-level feature 

extraction (AFE), which obtains measures the 
duration of consonants, vowels, pauses, and oth-
er acoustic-phonetic categories. In parallel, we 
perform a lexical feature extraction (LFE) on 
transcripts of participant speech producing pro-
files of each speaker’s language use.  This profile 
characterizes frequencies of different types of 
words – e.g. frequency of nouns, verbs, function 
words, words about emotion, etc. – present in a 
language sample along ~100 dimensions.  
  
Next, The AFE and LFE profiles are combined 
to form one large vector of features that collec-
tively characterize the speaker. Feature selection 
is applied to select the most informative features. 
For feature selection, we performed a one-way 
ANOVA on each extracted feature to determine 
which features were significantly related to a 
diagnostic category using the Benjamini-
Hochberg adjustment for multiple comparisons. 
 
The vector of selected features for the speech 
samples in the training set is taken as input to 
machine learning.  Based on these data machine 
learning automatically induces a diagnostic mod-
el that should predict any speaker’s diagnosis 
based the AFE and LFE profiles of his or her 
speech sample.  
 

  

  

Microphone 

Subject 
audio 

recordings 
Automatic 

Transcription 
(AT) 

POS 
tagger 

POS feature 
profile 

LIWC LIWC feature profile 

Acoustic 
Feature 
Extractor   

Acoustic 
feature profile 

Machine 
Learning 

Disease Identifying 
Model 

Speech to Text  

Training Data Labels  
(Each participant’s 

Diagnosis) 

Human  
Transcriptionist 

Human 
Transcription 

(HT) 

Automatic 
Speech 
Analysis 
(ASA) 

Acoustic Feature Extraction (AFE) 

Lexical Feature Extraction (LFE) 

Figure 1. System Information Flow and Evaluation. Participant speech is subjected to automatic 
speech analysis of two kinds: Acoustic Feature Extraction (AFE) and Lexical Feature Extraction 

(LFE). Feature selection (not shown) is explained in Sects 2.3 and 2.6. Each machine learning algo-
rithm produces a classification model based on labeled training data.  All models used both acoustic 
and lexical features. Each such disease identifying model is evaluated against held-out training data 

(not shown).  To measure sensitivity to ASR error, half of these models were based on lexical features 
derived from automatic transcription (AT), the other half from human transcription (HT). 

29



The performance of a learned diagnostic model 
is measured in terms of ability to generalize to 
cases that it has not been trained on is measured 
by feeding test set cases – i.e. cases that have not 
been a part of the training set.  We compared the 
accuracy of the machine learning induced algo-
rithms with accuracy studies of traditional diag-
nostic methods in the literature. 
 
In addition to the above, as part of a desire to 
achieve insight into the way these models were 
functioning, we sought verification that a differ-
ences in feature profiles as a function of diagnos-
tic group correspond meaningfully to existing 
expectations derived from the literature. To do 
so, we formed and tested several predictions 
about specific feature differences based on the 
clinical literature (see Hypotheses below). 
 
Finally, we wanted to determine how sensitive 
the feature differences and classification models 
were to speech recognition error.  To do so we 
tested each hypothesis on both the human and 
automatic transcriptions.  In addition, we learned 
a set of models based on automatic transcriptions 
and a second set of models based on human tran-
scriptions and compared accuracies.  

2.4 Acoustic Feature Extraction   
We used the automatic speech recognition (ASR) 
system to extract a set of acoustic-level features 
corresponding to the overall rate, plus the mean 
and standard deviation of (a) pause lengths and 
(b) hypothesized phoneme durations. For each 
speech sample, the speech rate as well as the 
mean and standard deviation of the duration of 
pauses, vowels, and consonants were comput-
ed. The SRI speech processing system also fur-
ther identified consonant classes based on man-
ner features (e.g., fricative, stop, etc. …)   voic-
ing features (voiced, voiceless) and measured the 
mean and standard deviation of the duration of 
these classes. Our Automatic Speech Analysis 
system produced 41 different duration-based 
measures extracted from the speech stream.  

2.5 Lexical Feature Extraction (LFE) 
For each transcript we performed two types of 
computer-based lexical analysis. The first deter-
mined frequencies of 14 different parts of speech 
(e.g. nouns, verbs, pronouns etc.) using an auto-
matic part-of-speech (POS) tagger. The second 
involved Dr. Pennebaker’s Linguistic Inquiry 
and Word Count (LIWC) software (Pennebaker, 
et al 2001), which determines word frequencies 

organized into 81 categories, such as psychologi-
cal processes (e.g., emotional or cognitive) and 
linguistic dimensions (e.g. function words, verb 
tenses, negations).  
 
To measure sensitivity to speech to text error, 
each ANOVA was performed twice, once for the 
“ground truth” human transcriptions (HT) and 
once for the automatic transcriptions (AT). Dur-
ing hypothesis testing, statistical significance of 
each pair of AT versus HT based LFEs (i.e., 
“ground truth”) was compared.  Additionally 
different models were learned, half using HT the 
other half using AT. To test for lexical-level dif-
ferences between diagnostic categories, we per-
formed a one-way ANOVA for each of the 95 
LFE features (e.g. frequency of nouns) in which 
diagnosis was the independent variable and the 
given feature’s frequency was the dependent var-
iable.  

2.6 Machine Learning 
We assessed how well a variety of machine 
learning algorithms predicted a patient’s diagno-
sis, using his or her combined AFE and LFE pro-
file.  Evaluation was conducted using five-fold 
cross-validation over the set of patients, with 
each “fold” consisting of two phases: a training 
phase, where the feature profiles and diagnoses 
from 4/5ths of the subjects are used to select fea-
tures and then train the given learning algorithm, 
and a test phase where the trained learner is giv-
en just the feature profiles of the remaining pa-
tients, and attempts to predict their diagnoses. 
This procedure is executed five times, each time 
using different sets of subjects for the train and 
test phases, with overall accuracy being the aver-
age performance on the test subjects, across all 
five folds. We applied three learning methods, 
(1) logistic regression, a statistical learning tech-
nique for determining categorical outcomes, (2) 
Multi-Layered Perceptrons, an artificial intelli-
gence (AI) learning method that roughly mimics 
biological neural networks, and (3) decision 
trees, another AI technique which induces sets of 
rules used to predict outcomes. All three are 
commonly used machine learning techniques, 
and for this study we used implementations 
available in Weka, an off-the-shelf machine 
learning toolkit (Witten and Frank, 2005). 

2.7 Hypotheses 
Machine learned classification models can be 
difficult to understand and often used merely as 
black boxes. To address this issue, we tried to 

30



draw a meaningful link between certain features 
and diagnosis. In particular, we formed and test-
ed several hypotheses based on expectations de-
rived from clinical literature.  We used all the 
data (rather than one of the training folds) to test 
these hypotheses.  
 
The hypotheses about the lexical features are as 
follows.  First, based on (Forbes at el., 2002) we 
predicted that AD patients use more pronouns, 
verbs, and adjectives and fewer nouns than con-
trols (H1). 
 
In SD, one sees decreased lexical access to con-
crete concepts, so patients tend to use fewer 
nouns (H2). To compensate for such difficulties 
with word retrieval, they also use more pronouns 
(H3).  This gives the impression of empty or cir-
cumlocutory speech. For example, rather than 
saying “The boy is flying a kite,” a SD patient 
would be more prone to say “He is flying that.” 
(Grossman and Ash, 2004).  
 
In PNFA, one sees fewer verbs (H4) (Grossman 
and Ash, 2004).  In addition, PNFA patients of-
ten exhibit agrammatism. Such speech is simpli-
fied and ungrammatical and involves fewer func-
tion words, for example “give cupcake” or “wa-
ter now”.  Thus (H5) is that the speech of pa-

tients with PNFA will have fewer function words 
(H5) (Saffran at el., 1989). These hypotheses, 
along with whether each was supported by our 
analyses, are listed in Table 2 in Results. 
 
The first acoustic hypothesis about acoustic fea-
tures (H6) is related to the Neary criteria (Neary 
et al., 1998), which notes that PNFA is character-
ized by non-fluent spontaneous speech (among 
other required features). Additionally, patients in 
this group have significant apraxia of speech 
(Gorno-Tempini at el., 2004). Signs of this con-
dition difficulty include articulatory groping – 
i.e. where the mouth searches for the correct con-
figurations.  Such trial and error speech often 
sounds “robotic” and can involve sounds that 
may be held out longer. Thus, given the duration 
features that are generally associated with aprax-
ia of speech (Samuel at el., 1996; Edythe at el., 
1996; Ballard and Robin, 2002), we hypothesize 
that PNFA patients would exhibit significantly 
longer vowel and consonant durations than con-
trols (H6).  
 
The second acoustic feature hypothesis (H7) is 
based on the fact that in the Neary criteria (Neary 
at el., 1998) pressured speech is a supportive 
(but not a core) diagnostic feature of both SD 
and bvFTD. In pressured speech one sees rapid 

Hypothesis and source Supported in LFE of HT? 
Supported in LFE 
or AFE of AT? 

Figures (see 
Supplementary 
Materials) 

H1. AD patients use more 
pronouns, verbs, and adjectives 
and fewer nouns than controls 
(Forbes at el., 2002) 

Yes, but only 
significant for 
nouns  

Yes, significant for 
nouns, pronouns, 
and adjectives 

Figure 3  

H2. SD patients use fewer nouns 
(Grossman and Ash, 2004) Yes 

Yes, but not 
significant vs PNFA  Figure 3 

H3. SD patients use more 
pronouns (Grossman and Ash, 
2004)  

Yes Partial: SD sig. > CNTRL only  Figure 3 

H4. Lower verb frequency in 
PNFA (Grossman and Ash, 2004) 

Yes, but only 
significant vs. SD No Figure 3 

H5. Fewer function words in 
PNFA (Saffran at el., 1989) Yes 

Yes, but only 
significant vs SD Figure 3 

H6. PNFA patients would exhibit 
longer vowel and consonant 
durations 

N/A Yes Figure 2 

H7. SD and bvFTD patients have 
shorter pauses than controls. N/A Yes Figure 2 

Table 2. Hypotheses extracted from literature and whether our measures—based on human transcripts 
(HT) and automatic transcripts (AT)—support them [Hypotheses 1-5 relate to Lexical Feature Ex-

traction; Hypotheses 6-7 relate to Acoustic Level Analyses] 

31



“flight of ideas” speech. We would thus expect 
some patients in these conditions to exhibit press 
of speech, and so hypothesize that the mean du-
ration of pauses should be significantly less than 
controls (H7). 

3 RESULTS 
Results suggest that analyses at the lexical and 
acoustic levels are capable of detecting differ-
ences in accordance with expectations of prior 
research. Additionally, machine-learning algo-
rithms predict clinical diagnosis surprisingly 
well. 

3.1 Results: Acoustic-Level Hypotheses 
For each measure, we performed an ANOVA 
with respect to diagnosis and found that 25 out of 
41 measures were significant at the (Benjamini-
Hochberg multiple comparison adjusted) 0.05 
level.  Hypotheses 7 and 8 in Table 2 and Figure 
2 in Supplementary Materials deal specifically 
with AFE measures. These show that PNFA pa-

tients do exhibit significantly longer vowel and 
consonant durations, as the literature linking 
PNFA with apraxia of speech would predict. Fur-
thermore, SD and bvFTD patients have signifi-
cantly shorter pauses than controls, which is con-
sistent with the hypothesis that some patients 
with these diagnoses exhibit press of speech.  

3.2 Results: Lexical-Level Hypotheses  
There were several lexical-level differences be-
tween diagnostic groups. We checked for signifi-
cant differences (hereafter, “significant fea-
tures”) with respect to diagnosis while using the 
Benjamini-Hochberg test for multiple compari-
sons (Benjamini and Hochberg, 1995).  (We use 
this adjustment for all multiple comparisons). 
There were several more lexical level differences 
based on the HTs than one would predict by 
chance. For example, 11 of the 14 POS features 
were significant (p ≤ .05) including verbs, nouns, 
adjectives and adverbs. For LIWC features, 22 of 
81 features were statistically significant at the p 

  (A) (B) (C) (D) 

 
 FTLD vs 

AD vs 
Controls 

AD vs SD 
vs PNFA 
vs bvFTD 
vs Control 

FTLD vs AD AD vs Controls 

1. Random diagnosis 33% 20% 50% 50% 

2. Naïve learner (always picks largest class in training set) 63% 27% 77% 50% 

3. Our method 80% 61% 

88% 
Sens/Spec AD 

.58/0.77 
Sens/Spec FTLD 

.95/.89 

88% 
! = .64 /Spec AD .83/.90 

Sens/Spec Controls 
.92/.86 

4. Radiologists in Klöppel at el. (2008) using MRI data   
69% 

Sens/Spec AD 
.64/.71 

89% Sensi/Spec AD 
.88/.90 

5. 
Frontal Behavioral 
Inventory in Blair at el. 
(2007) 

  75%  

6. Neuropsychiatric inventory in Blair at el. (2007).   54%  

7. NINCDS-ARDA criteria in Lopez at el. (1990)    ! = .36 − .65 

8. DSM-III criteria in Kukull at el. (1990)    ! = .55 

9. NINCDS criteria in Kukull at el. (1990)    ! = .64 

10. ECRDC criteria in Kukull at el. (1990)    ! = .37 

Table 3. Accuracy, sensitivity and specificity for Layered Perceptron learned models for FTLD subtypes. 
(Accuracy of a random and naïve learner id 33% and 43% respectively) 

32



<= 0.05 level, with p ≤ 0.005 for 17 of them. As 
to the question of whether the profile differences 
correspond meaningfully to existing literature, 
Table 2 shows which literature-generated hy-
potheses were supported. See Figure 3 in Sup-
plementary Materials which show the means and 
standard error for each diagnostic class on a par-
ticular feature.  

3.3 Machine Learning Results 
Using cross-validation, we tested the ability of 
machine learning methods to produce algorithms 
that could synthesize lexical-level and acoustic-
level profiles and then identify the clinician di-
agnosis. 
 
We tried several different machine-learning algo-
rithms and found that performance was roughly 
the same.  See Table 3 for the performance of the 
Multi-layered Perceptron algorithm, which was 
slightly superior. Performance was measured 
across several different diagnostic problems 
(e.g., FTLD vs AD vs Controls (Column A), AD 
vs Controls (Column D), etc.). For purposes of 
rough comparison, Table 3 also provides diag-
nostic performance of other methods, including 
radiologists using MRI data.  
 
In evaluating machine learning results, we 
wished to compare model performance against 
various benchmarks. The two easiest such 
benchmark are random guessing (see Table 3 
Row 1: given N diagnostic alternatives, one has a 
1 / N chance of correctly guessing) and naïve 
learner guessing, (see Table 3 Row 2) which 
always chooses the most frequent (i.e., modal) 
diagnosis found in the training sample.  The row 
labeled “Our method” corresponds to the accura-
cy of models generated from lexical and acoustic 
features using AT. For this case, HT results dif-
fers from AT in accuracy by only 2-3% for all 
prediction problems. Note that our method is at 
least equal to the accuracies, sensitivities, speci-
ficities, and kappa’s of the other clinical bench-
marks in most cases. See Table 4, which shows 
the performance on distinguishing FTLD sub-
types.  For more detail on machine learning re-
sults see Peintner et al (2008). 

4 DISCUSSION 
The accuracy of the best machine learned diag-
nostic model was 88% in the binary classifica-
tions of AD versus FTLD, and AD versus Con-
trols (Table 3).  Acoustic and lexical level differ-

ences are detectable despite the present level of 
ASA inaccuracy. Although diagnosis should 
never be made on the basis of one source of in-
formation, our pilot data show that automatic 
computer-based analyses of spontaneous speech 
show promise as diagnostic aids by detecting the 
at times subtle differences that characterize these 
neurodegenerative disorders. 
 
Inferences drawn from these results are subject 
to a variety of assumptions and limitations.  Per-
haps the biggest limitation is the small number of 
research participants.  Larger samples will be 
needed in order to make valid generalizations to 
the population. Small samples increase the prob-
ability of Type I and II Errors and decrease pow-
er in testing for normality.  That said, many of 
our hypothesized linguistic differences based on 
prior research were confirmed.  Additionally, 
low N in each group entailed that test sets in each 
fold were small.  Though it is remarkable in our 
pilot study that we obtained classification accu-
racy on par with clinical judgment, a larger sam-
ple size is required to make a rigorously valid 
claim about on par accuracy.   
 
Statistically minded readers may question our 
use of parametric statistics (ANOVA) in feature 
selection because we have not tested the normali-
ty assumption.  There are too few observations in 
each group to test for normality of residuals with 
any power.  In future work with a larger sample 
we should perform such a test.  Alternatively, on 
the present data we could use the non-parametric 
Kruskal-Wallis test as a stand in for ANOVA. 
 
Additionally, such readers may question our use 
of the Benjamini-Hochberg (BH) adjustment 
which controls false discovery rate over a more 
stringent correction for familywise error rate 
such as Bonferoni or Holm.  Our rationale was 
that an occasional false positive (5% if we have a 
5% false positive rate) among our total set of 
positives isn’t a big concern.  As our focal aim 
was machine learning, scientific discovery, was a 
secondary concern.  Thus, we were less interest-
ed in the question “was there any difference be-
tween the groups".  We were more interested in 
which features showed a difference.  Better to 
have a small proportion of false positives than to 
miss true positives.  In addition, because the false 
negative rate criterion is less stringent about false 
positives, the BH procedure tends to have greater 
power than multiple comparison approaches that 
control the familywise error rate. 

33



 
The success of our methods is surprising given 
(1) we have performed no customization of “off 
the shelf” LFE and machine learning techniques; 
(2) models were trained on a relatively small 
number of subjects; (3) speech samples were 
short (3-5 minutes). Larger speech samples, larg-
er N and more tailored tools (e.g. language mod-
els) will enable lower word error rate, higher ac-
curacy and finer discrimination amongst and 
within diagnostic types. It also suggests that this 
can be accomplished without training the system 
to the voice of each subject.  
 
The results also draw significance because the 
overall approach may be applied to other neuro-
logical or psychological disorders.    Many such 
disorders have characteristic lexical or acoustic 
profiles.  For example, Jarrold (2011) and Stir-
man et al (2001) have shown that depression is 
associated with high frequencies of first person 
words (I, me, I’ve) and lower frequencies of so-
cial and second person words (us,we).  Sanchez 
et al (2011) and Keskinpala (2007) have shown 
acoustic prosodic features indicative of depres-
sion or suicide risk.  Our results suggest a very 
similar study design can be applied to detect the-
se kinds of depression related lexical and acous-
tic/prosodic profiles. 
   
Our results suggest we may be able train the 
models to assess specific highly diagnostic lan-
guage symptoms – such as fluency, circumlocu-
tion, and apraxia of speech.  This can be particu-
larly important where the inter-rater reliability of 
given symptoms is poor.  We believe that poor 
inter-rater reliability is mainly caused by the ina-
bility to precisely delineate the objective charac-
teristics of these symptoms.  Assuming we can 
get a range of values that characterize a given 
symptom, we can apply machine learning to 
identify symptoms in addition to diagnosis.  
 
We view the methods described as analogous to 
EKG.  The EKG trace affords a more quantita-

tive and objective picture of cardiac functioning 
which complements the stethoscope.  Analogous-
ly, if scaled-up studies can demonstrate adequate 
diagnostic accuracy results, then computationally 
extracted lexico-acoustic profiles may someday 
augment information provided by current speech 
and language diagnostic methods which are cur-
rently based substantially on subjective clinical 
judgment.  As modern EKG’s provide automatic 
interpretation, our analysis suggests that classifi-
cation of speech as AD-like or FTLD-like may 
be possible.  The competent physician never re-
lies only the automated diagnosis provided by 
EKG but also interprets a profile of measures in 
the context of clinical observation. Our assump-
tion is that the methods outlined above should be 
used in a way analogously to the EKG. 
 
The results of our hypothesis testing show that 
differences in feature profiles are generally con-
sistent with what we would expect from the clin-
ical literature.  This may be the first of several 
steps required to provide assurance to clinicians 
who would prefer to trust a model that had 
somewhat transparent features to the opaque 
“black box” models that are often learned.  Es-
tablishing trust of clinicians is required for wide 
scale adoption and future work should build on 
these results.  
 
Our pilot data suggest this approach provides 
diagnoses of comparable accuracy to other more 
time intensive or more invasive methods (e.g. 
neuropsychological testing or imaging). This is a 
fast, inexpensive, and non-invasive means of 
obtaining diagnostically useful information. Thus 
the tool may show most promise as a screening 
tool to decide which patients need deeper evalua-
tion.   Additionally, it may provide objective and 
quantifiable measures of speech and language 
symptomatology – a kind of symptomatology for 
which there are few objective, quantifiable 
measures.   

5 Conclusion 
Clinical speech analytics applied to spontaneous 
speech can detect distinguish between AD, 
bvFTD, SD PNFA and healthy control groups 
via lexico-acoustic profiles. Diagnostic accuracy 
is comparable to other clinical data sources de-
spite speech sample brevity.  Accuracy levels 
suggest the approach offers promise as an addi-
tional, objective and easily obtained source of 
diagnostic information for clinicians. 

Accuracy bvFTD (Sens/Specif) PNFA SD 

63% .51 / .58 .54 / .72 
.76 / 
.62 

Table 4. Accuracy, sensitivity and specificity for 
Lay-ered Perceptron learned models for FTLD 

subtypes. (Accuracy of a random and naïve 
learner id 33% and 43% respectively) 

34



Reference 
Varma A.R., Snowden J.S., Lloyd J.J., Talbot P.R., 

Mann D.M.A., Neary D. 1999. Evaluation of the 
NINCDS-ADRDA criteria in the differentiation of 
Alzheimer’s disease and frontotemporal dementia, 
Journal of Neurology, Neurosurgery and Psychia-
try, 66: 184-188. 

Klöppel, S., Stonnington, C.M., Barnes, J., Chen, F., 
Chu, C., Good, C.D., Mader, I., Mitchell, L.A., Pa-
tel, A.C., Roberts, C.C., Fox, N.C., Jack, R. Jr, 
Ashburner, J., Frackowiak , RS. 2008. Accuracy of 
dementia diagnosis: a direct comparison between 
radiologists and a computerized method, Brain, 
131(11): 2969–2974. 

S. Pestell, M. Shanks, J. Warrington, and A. Venneri. 
2000. Quality of spelling breakdown in Alzheimer's 
disease is independent of disease progression, 
Journal of Clinical and Experimental Neuropsy-
chology, volume 22, pages 599–612. 

H. Platel, J. Lambert, F. Eustache, B. Cadet, M. Dary, 
F. Viader, and B. Lechevalier. 1993. Character-
stics and evolution of writing impairment in Alz-
heimer's disease, Journal of Clinical and Experi-
mental Neuropsychology, volume 22, pages 599–
612. 

K. Forbes, A. Venneri, and M. Shanks. 2002. Distinct 
patterns of spontaneous speech deterioration: an 
early predictor of Alzheimer's disease, Brain and 
Cognition, volume 48(2-3): 356–61. 

 C. Crockford and R. Lesser. 1994. Assessing func-
tional communication in aphasia: Clinical utility 
and time demands of three methods, European 
Journal of Disorders of Communication, volume 
29: 165–182. 

Thomas, V., Keselj, N., Cercone, K., Rockwood, E. 
2005. Automatic detection and rating of dementia 
of Alzheimer type through lexical analysis of spon-
taneous speech, IEEE International Conference on 
Mechatronics and Automation. 

Mendez, M.F., Selwood, A., Mastri, A.R., Frey, W.H. 
1993. 2nd, Pick's disease versus Alzheimer's dis-
ease: A comparison of clinical characteristics, 
Neurology, 43(2): 289–92. 

Neary, D., Snowden, J.S., Gustafson, L., Passant, U., 
Stuss, D., Black, S., Freedman, M., Kertesz, A., 
Robert, H., Albert, M., Boone, K., Miller, B.L., 
Cummings, J., Benson, D.F. 1998. Frontotemporal 
lobar degeneration: A consensus on clinical diag-
nostic criteria, Neurology, 51(6): 1546–54. 

Davies, R.R., Hodges, J.R., Kril, J.J., et al. 2005. The 
pathological basis of semantic dementia. Brain, 
128(9): 1984–95.  

Josephs, K.A., Duffy, J.R., Strand, E.A., et al. 2006. 
Clinicopathological and imaging correlates of 

progressive aphasia and apraxia of speech. Brain, 
129(6): 1385–98. 

Bright, P., Moss, H. E., Stamatakis, E. A., & Tyler, L. 
K. 2008. Longitudinal studies of semantic demen-
tia: The relationship between structural and func-
tional changes over time, Neuropsychologia, 46: 
2177-2188. 

S. Singh, R. Bucks, and J. Cuerden. 2001. Evaluation 
of an objective technique for analysing temporal 
variables in DAT spontaneous speech, Aphasiolo-
gy, volume 15(6): 571–584.  

Stefanie Abel, Walter Huber, Gary S. Dell. 2009. 
Connectionist diagnosis of lexical disorders in 
aphasia, Aphasiology, volume 23. 

Dilek Hakkani-Tür, Dimitra Vergyri, Gökhan Tür. 
2010. Speech-based automated cognitive status as-
sessment. Interspeech 2010: pages 258-261. 

Maider Lehr, Emily T. Prud’hommeaux, Izhak 
Shafran and Brian Roark. 2012. Fully Automated 
Neuropsychological Assessment for Detecting Mild 
Cognitive Impairment. In Proceedings of Inter-
speech. 

Kertesz, A. 1980. Western Aphasia Battery, London, 
Ontario: University of Western Ontario Press. 

Stolcke, A., Boakye, K., Cetin, Ö., Janin, A., 
Magimai-Doss, M., Wooters, C., Zheng, J. 2007. 
The SRI-ICSI Spring 2007 meeting and lecture 
recognition system, Proc. NIST 2007 Rich Tran-
scription Workshop. 

Pennebaker, J.W., Francis, M.E., Booth, R.J. 2001. 
Linguistic Inquiry and Word Count (LIWC): 
LIWC2001, Mahwah, NJ: Erlbaum Publishers. 

Toutanova, K., Klein, D., Manning, C., Singer, Y. 
2003. Feature-rich part-of-speech tagging with a 
cyclic dependency network, in Proceedings of 
HLT-NAACL 2003, pages 252–259. 

Grossman, M., Ash, S. 2004. Primary Progressive 
Aphasia: A Review, Neurocase, 10(1): 3–18. 

Gorno-Tempini, M.L, Dronkers, N.F., Rankin, K.P., 
Ogar, J.M., La Phengrasamy, B.A., Rosen, H.J., 
Johnson, J.K., Weiner, M.W., Miller, B.L, Cogni-
tion and Anatomy in three variants of primary pro-
gressive aphasia, Annals of Neurology, 2004. 55: 
335–346. 

Samuel A. K. Seddoh, Donald A. Robin, Hyun-Sub 
Sim, Carlin Hageman, Jerald B. Moon, John W. 
Folkins. 1996. Speech Timing in Apraxia of Speech 
versus Conduction Aphasia, Journal of Speech and 
Hearing Research, 39: 590–603. 

Edythe A. Strand, E.A., McNeil, M.R. 1996. Effects 
of Length and Linguistic Complexity on Temporal 
Acoustic Measures in Apraxia of Speech, Journal 
of Speech and Hearing Research, 39: 1018–33. 

35



Kirrie J. Ballarrd, Ph.D., and Donald A. Robin. 2002. 
Assessment of AOS for Treatment Planning, Semi-
nars in Speech and Language, 23(4): 281–291. 

Witten, I.H., Frank, E. 2005. Data mining: Practical 
machine learning tools and techniques, San Fran-
cisco: Morgan Kaufmann. Second edition. 

Benjamini, Y., Hochberg Y. 1995. Controlling the 
False Discover Rate: A Practical and Powerful 
Approach to Multiple Testing, Journal of the Royal 
Statistical Society. Series B (Methodological), 
57(1): 289–300. 

Saffran, E.M., Berndt, R.S., Schwartz, M.F. 1989. The 
quantitative analysis of agrammatic production: 
procedure and data, Brain and Language, 37(3): 
440–79. 

Blair, M., Kertesz, A., Davis-Faroque, N., Hsiung, 
G.Y.R., Black, S.E., Bouchard, R.W., Gauthier, S., 
Guzman, D.A., Hogan, D.B., Rockwood, K., 
Feldman. H. 2007. Behavioural Measures in Fron-
totemporal Lobar Dementia and Other Dementias: 
The Utility of the Frontal Behavioural Inventory 
and the Neuropsychiatric Inventory in a National 
Cohort Study, Dementia and Geriatric Cognitive 
Disorder, 23: 406-15 

Lopez, O. L., Swihart, A. A., Becker, J. T., Reinmuth, 
O. M., Reynolds, C. F., Rezek, D. L., Daly, F. L. 
1990. Reliability of NINCDS-ADRDA clinical cri-
teria for the diagnosis of Alzheimer's disease, Neu-
rology, 40: 1517 

Kukull, W. A., Larson, E. B., Reifler, B. V., Lampe, 
T. H., Yerby, M., Hughes, J. 1990. Interrater reli-
ability of Alzheimer's disease diagnosis, Neurolo-
gy, 40(2): 257-60 

Peintner, B., Jarrold, W, Vergyri, D., Richey, C., 
Gorno Tempini, M., and Ogar, J. 2008. Learning 
Diagnostic Models Using Speech and Language 
Measures, 30th Annual International IEEE EMBS 
Conference, August 20-24, Vancouver, British Co-
lumbia, Canada. 

Jarrold, W., Javitz, H.S., Krasnow, R., Peintner, B., 
Yeh E., Swan, G.E. (2011) Depression and Self-
Focused Language in Structured Interviews with 
Older Adults Psychological Reports 
Oct;109(2):686-700. 

Stirman, S.W., & Pennebaker, J.W. (2001). Word use 
in the poetry of suicidal and non-suicidal poets. 
Psychosomatic Medicine 63, 517-522. 

Michelle Hewlett Sanchez, Dimitra Vergyri, Luciana 
Ferrer,,Colleen Richey, Pablo Garcia, Bruce 
Knoth, William Jarrold: Using Prosodic and Spec-
tral Features in Detecting Depression in Elderly-
Males. INTERSPEECH 2011: 3001-3004 

H. Kaymaz Keskinpala, T. Yingthawornsuk, D. 
Mitchell Wilkes, Richard G. Shiavi, R. M. Salo-
mon: Distinguishing high risk suicidal subjects 

among depressed subjects using mel-frequency 
cepstrum coefficients and cross validation tech-
nique. MAVEBA 2007: 157-160 

 

 

 

 

 

36



Supplementary Materials 
 
 

 

Figure 2. Vowel, consonant, and pause 
 

 

Figure 3. Verb, adjective, pronoun, noun and function word frequencies (H1, H2, H3, H4, H5) 

 

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

Vowel duration Consonant duration Pause/Spurt

SD AD PNFA FTD CTRL

0%

10%

20%

30%

40%

50%

60%

70%

80%

Verbs(HT) Verbs(AT) Adjectives(HT) Adjectives(AT) Pronouns(HT) Pronouns(AT) Nouns(HT) Nouns(AT) Function
Words(HT)

Function
Words(AT)

SD AD PNFA FTD CTRL

37


