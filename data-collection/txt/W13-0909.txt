










































Robust Extraction of Metaphor from Novel Data


Proceedings of the First Workshop on Metaphor in NLP, pages 67–76,
Atlanta, Georgia, 13 June 2013. c©2013 Association for Computational Linguistics

Robust Extraction of Metaphors from Novel Data  
 

Tomek Strzalkowski1, George Aaron Broadwell1, Sarah Taylor2, Laurie Feldman1, Boris 
Yamrom1, Samira Shaikh1, Ting Liu1, Kit Cho1, Umit Boz1, Ignacio Cases1 and Kyle El-

liott3 
1State University of New York 2Sarah M. Taylor Consulting LLC 3Plessas Experts 

University at Albany 121 South Oak St.  Network Inc. 
Albany NY USA 12222 Falls Church VA USA 22046 Herndon VA 20171 
tomek@albany.edu talymail59@gmail.com  kelliot@plessas.net 

 
 

 
 

Abstract 

This article describes our novel approach to 
the automated detection and analysis of meta-
phors in text. We employ robust, quantitative 
language processing to implement a system 
prototype combined with sound social science 
methods for validation. We show results in 4 
different languages and discuss how our 
methods are a significant step forward from 
previously established techniques of metaphor 
identification. We use Topical Structure and 
Tracking, an Imageability score, and innova-
tive methods to build an effective metaphor 
identification system that is fully automated 
and performs well over baseline.  

1 Introduction 

The goal of this research is to automatically identi-
fy metaphors in textual data.  We have developed a 
prototype system that can identify metaphors in 
naturally occurring text and analyze their seman-
tics, including the associated affect and force. Met-
aphors are mapping systems that allow the 
semantics of a familiar Source domain to be ap-
plied to a Target domain so that new frameworks 
of reasoning can emerge in the Target domain. 
Metaphors are pervasive in discourse, used to con-
vey meanings indirectly. Thus, they provide criti-
cal insights into the preconceptions, assumptions 
and motivations underlying discourse, especially 
valuable when studied across cultures. When met-
aphors are thoroughly understood within the con-
text of a culture, we gain substantial knowledge 
about cultural values. These insights can help bet-
ter shape cross-cultural understanding and facili-

tate discussions and negotiations among different 
communities.  

A longstanding challenge, however, is the large-
scale, automated identification of metaphor in vol-
umes of data, and especially the interpretation of 
their complex, underlying semantics.  

We propose a data-driven computational ap-
proach that can be summarized as follows: Given 
textual input, we first identify any sentence that 
contains references to Target concepts in a given 
Target Domain (Target concepts are elements that 
belong to a particular domain; for instance “gov-
ernment bureaucracy” is a Target concept in the 
“Governance” domain). We then extract a passage 
of length 2N+1, where N is the number of sentenc-
es preceding (or succeeding) the sentence with 
Target Concept. We employ dependency parsing to 
determine the syntactic structure of each input sen-
tence. Topical structure and imageability analysis 
are then combined with dependency parsing output 
to locate the candidate metaphorical expressions 
within a sentence. For this step, we identify nouns 
and verbs in the passage (of length 2N+1) and link 
their occurrences – including repetitions, pronomi-
nal references, synonyms and hyponyms. This 
linking uncovers the topical structure that holds the 
narrative together.  We then locate content words 
that are outside the topical structure and compute 
their imageability scores. Any nouns or adjectives 
outside the main topical structure that also have 
high imageability scores and are dependency-
linked in the parse structure to the Target Concept 
are identified as candidate source relations, i.e., 
expressions borrowed from a Source domain to 
describe the Target concept. In addition, any verbs 
that have a direct dependency on the Target Con-

67



cept are considered as candidate relations. These 
candidate relations are then used to compute and 
rank proto-sources. We search for their arguments 
in a balanced corpus, assumed to represent stand-
ard use of the language, and cluster the results. 
Proto-source clusters and their ranks are exploited 
to determine whether the candidate relations are 
metaphorical or literal. Finally, we compute the 
affect and force associated with the metaphor.    

Our approach is shown to work in four lan-
guages – American English, Mexican Spanish, 
Russian Russian and Iranian Farsi. We detail in 
this paper the application of our approach to detec-
tion of metaphors using specific examples from the 
“Governance” domain. However, our approach can 
be expanded to work on extracting metaphors in 
any domain, even unspecified ones. We shall brief-
ly explain this in Section 5; we defer the details of 
the expanded version of the algorithm to a separate 
larger publication. In addition, we shall primarily 
present examples in English to illustrate details of 
our algorithms. However, modules for all four lan-
guages have the same implementation in our sys-
tem.  

The rest of the paper is organized as follows: in 
Section 2, we discuss related research in this field. 
Section 3 presents our approach in detail; Section 4 
describes our evaluation and results. In Section 5 
we discuss our conclusions and future directions.  

2 Related Work 

Most current research on metaphor falls into three 
groups: (1) theoretical linguistic approaches (as 
defined by Lakoff & Johnson, 1980; and their fol-
lowers) that generally look at metaphors as abstract 
language constructs with complex semantic prop-
erties; (2) quantitative linguistic approaches (e.g., 
Charteris-Black, 2002; O’Halloran, 2007) that at-
tempt to correlate metaphor semantics with their 
usage in naturally occurring text but generally lack 
robust tools to do so; and (3) social science ap-
proaches, particularly in psychology and anthro-
pology that seek to explain how people deploy and 
understand metaphors in interaction, but which 
lack the necessary computational tools to work 
with anything other than relatively isolated exam-
ples. 
    Metaphor study in yet other disciplines has in-
cluded cognitive psychologists (e.g., Allbritton, 
McKoon & Gerrig, 1995) who have focused on the 

way metaphors may signify structures in human 
memory and human language processing. Cultural 
anthropologists, such as Malkki in her work on 
refugees (1992), see metaphor as a tool to help out-
siders interpret the feelings and mindsets of the 
groups they study, an approach also reflective of 
available metaphor case studies, often with a Polit-
ical Science underpinning (Musolff, 2008; Lakoff, 
2001).  
    In computational investigations of metaphor, 
knowledge-based approaches include MetaBank 
(Martin, 1994), a large knowledge base of meta-
phors empirically collected. Krishnakumaran and 
Zhu (2007) use WordNet (Felbaum, 1998) 
knowledge to differentiate between metaphors and 
literal usage. Such approaches entail the existence 
of lexical resources that may not always be present 
or satisfactorily robust in different languages. 
Gedigan et al (2006) identify a system that can 
recognize metaphor. However their approach is 
only shown to work in a narrow domain (Wall 
Street Journal, for example).  
   Computational approaches to metaphor (largely 
AI research) to date have yielded only limited 
scale, often hand designed systems (Wilks, 1975; 
Fass, 1991; Martin, 1994; Carbonell, 1980; Feld-
man & Narayan, 2004; Shutova & Teufel, 2010; 
inter alia, also Shutova, 2010b for an overview). 
Baumer et al (2010) used semantic role labels and 
typed dependency parsing in an attempt towards 
computational metaphor identification. However 
they self-report their work to be an initial explora-
tion and hence, inconclusive. Shutova et al (2010a) 
employ an unsupervised method of metaphor iden-
tification using nouns and verb clustering to auto-
matically impute metaphoricity in a large corpus 
using an annotated training corpus of metaphors as 
seeds. Their method relies on annotated training 
data, which is difficult to produce in large quanti-
ties and may not be easily generated in different 
languages.  

By contrast, we propose an approach that is fully 
automated and can be validated using empirical 
social science methods. Details of our algorithm 
follow next. 
 

3 Our Approach 

In this section, we walk through the steps of meta-
phor identification in detail. Our overall algorithm 

68



consists of five main steps from obtaining textual 
input to classification of input as metaphorical or 
literal.  

3.1 Passage Identification 

The input to our prototype system is a piece of 
text. This text may be taken from any genre – news 
articles, blogs, magazines, official announcements, 
broadcast transcripts etc.  

Given the text, we first identify sentences that 
contain Target concepts in the domain we are in-
terested in. Target concepts are certain keywords 
that occur within the given domain and represent 
concepts that may be targets of metaphor. For in-
stance, in the “Governance” domain, concepts such 
as “federal bureaucracy” and “state mandates” 
serve as Target concepts. We keep a list of Target 
concepts to search through when analyzing given 
input. This list can be automatically created by 
mining Target Concepts from resource such as 
Wikipedia, given the Target domain, or manually 
constructed. Space limits the discussion of how 
such lists may be automatically created; a separate 
larger publication addresses our approach to this 
task in greater detail.  

In Figure 1, we show a piece of text drawn from 
a 2008 news article. The sentence in italics con-
tains one of our Target concepts: “federal bureau-
cracy”. We extract the sentence containing Target 
concepts that match any of those in our list, includ-
ing N sentences before and N sentences after the 
sentence if they exist, to yield a passage of at most 
2N+1 sentences. For the example shown in Figure 
1, the Target concept is “federal bureaucracy”. In 
current system prototype, N=2. Hence, we extract 
two sentences prior to the sentence containing 
“federal bureaucracy” (in Figure 1 example, these 
are omitted for ease of presentation) and two sen-
tences following the given sentence.  
     Once this passage is extracted, we need to de-
termine whether a metaphor is present in the mid-
dle sentence. To accomplish that, we follow the 
steps as described in the next section.  
 
 

 
Figure 1. Excerpt from news article. Passage containing 

target concept highlighted in italics. The callouts 1, 2 
etc., indicate topic chains (see next section). 

     

3.2 Topical Structure and Imageability Anal-
ysis 

Our hypothesis is that metaphorically used terms 
are typically found outside the topical structure of 
the text. This is an entirely novel method of effec-
tively selecting candidate relations. It draws on  
Broadwell et al. (2012), who proposed a method to 
establish the topic chains in discourse as a means 
of modeling associated socio-linguistic phenomena 
such as topic control and discourse cohesiveness. 
We adapted this method to identify and exclude 
any words that serve to structure the core discus-
sion, since the metaphorical words, except in the 
cases of extended and highly elaborated meta-
phors, are not the main subject, and thus unlikely 
to be repeated or referenced in the context sur-
rounding the sentence.  

We link the occurrences of each noun and verb 
in the passage (5 sentence length). Repetitions via 
synonyms, hyponyms, lexical variants and pronoun 
references are linked together. These words, as 
elements of the several topic chains in a text, are 
then excluded from further consideration. WordNet 
(Felbaum, 1998) is used to look up synonyms and 
hyponyms of the remaining content words. We 

These qualities1 have helped him4 navigate the 
labyrinthine federal bureaucracy in his demand-
ing $191,300-a-year job as the top federal offi-
cial3 responsible for bolstering airline, border2, 
port and rail security against a second cata-
strophic terrorist attack.  
But those same personal qualities1 also explain 
why the 55-year-old Cabinet officer3 has alienat-
ed so many Texans along the U.S.-Mexico bor-
der2 with his4 relentless implementation of the 
Bush administration's hard-nosed approach to 
immigration enforcement - led by his unyielding 
push to construct 670 miles of border2 fencing by 
the end of the year.  
Some Texas officials are so exasperated that they 
say they'll just await the arrival of the next presi-
dent before revisiting border enforcement with 
the federal government. 
 

Copyright 2008. The Houston Chronicle Publishing Company. All 
Rights Reserved. 

 

69



illustrate this in Figure 1. We show the two sen-
tences that form the latter context in the example 
passage. We show four of the topic chains discov-
ered in this passage. These have been labeled via 
superscripts in Figure 1. 1 and 2 are the repetitions 
of word “qualities” and “border”. The 3 identifies 
repetition via lexical variants “officer” and “offi-
cial” and 4 identifies the pronoun co-references  
“him” and “his”. We shall exclude these words 
from consideration when searching for candidate 
metaphorical relations in the middle sentence of 
the passage.  

To further narrow the pool of candidate relations 
in this sentence, we compute the imageability 
scores of the remaining words. The hypothesis is 
metaphors use highly imageable words to convey 
their meaning. The use of imageability scores for 
the primary purpose of metaphor detection distin-
guishes our approach from other research on this 
problem. While Turney et al. (2011) explored the 
use of word concreteness (a concept related but not 
identical to imageability) in an attempt to disam-
biguate between abstract and concrete verb senses, 
their method was not specifically applied to detec-
tion of metaphors; rather it was used to classify 
verb senses for the purpose of resolving textual 
entailment. Broadwell et al. (2013) present a de-
tailed description of our approach and how we use 
imageability scores to detect metaphors. 

Our assertion is that any highly imageable word 
is more likely to be a metaphorical relation. We 
use the MRCPD (Coltheart 1981, Wilson 1988) 
expanded lexicon to look up the imageability 
scores of words not excluded via the topic chains. 
Although the MRCPD contains data for over 
150,000 words, a major limitation of the database 
for our purposes is that the MRCPD has imageabil-
ity ratings (i.e., how easily and quickly the word 
evokes a mental image) for only ~9,240  (6%) of 
the total words in its database. To fill this gap, we 
expanded the MRCPD database by adding imagery 
ratings for an further 59,989 words. This was done 
by taking the words for which the MRCPD data-
base has an imageability rating and using that word 
as an index to synsets determined using WordNet 
(Miller, 1995). The expansion and validation of the 
expanded MRCPD imageability rating is presented 
in a separate, future publication.  

Words that have an imageability rating lower 
than an experimentally determined threshold are 
further excluded from consideration. In the exam-

ple shown in Figure 1, words that have sufficiently 
high imageability scores are “labyrinthine”, “port”, 
“rail” and “airline”. We shall consider them as 
candidate relations, to be further investigated, as 
explained in the dependency parsing step described 
next.   

3.3 Relation Extraction 

Dependency parsing reveals the syntactic structure 
of the sentence with the Target concept. We use 
the Stanford parser (Klein and Manning, 2003) for 
English language data. We identify candidate met-
aphorical relations to be any verbs that have the 
Target concept in direct dependency path (other 
than auxiliary and modal verbs). We exclude verbs 
of attitude (“think”, “say”, “consider”), since these 
have been found to be more indicative of metony-
my than of metaphor. This list of attitude verbs is 
automatically derived from WordNet. 

From the example shown in Figure 1, one of the 
candidate relations extracted would be the verb 
“navigate”.  
    In addition, we have a list of candidate relations 
from Step 3.2, which are the highly imageable 
nouns and adjectives that remain after topical 
structure analysis. Since “port”, “rail” and “airline” 
do not have a direct dependency path to our Target 
concept of “federal bureaucracy”, we drop these 
from further consideration. The highly imageable 
word remaining in this list is “labyrinthine”.  
    Thus, two candidate relations are extracted from 
this passage – “navigate” and “labyrinthine”. We 
shall now show how we use these to discover pro-
to-sources for the potential metaphor.  

3.4 Discovery of Proto-sources 

Once candidate relations are identified, we exam-
ine whether the usage of these relations is meta-
phorical or literal. To determine this, we search for 
all uses of these relations in a balanced corpus and 
examine in which contexts the candidate relations 
occur. To demonstrate this via our example, we 
shall consider one of the candidate relations identi-
fied in Figure 1 – “navigate”; the search method is 
the same for all candidate relations identified. In 
the case of the verb “navigate” we search a bal-
anced corpus for the collocated words, that is, 
those that occur within a 4-word window following 
the verb, with high mutual information (>3) and 
occurring together in the corpus with a frequency 

70



at least 3. This search returns a list of words, most-
ly nouns in this case, that are the objects of the 
verb “navigate”, just as “federal bureaucracy” is 
the object in the given example. However, since 
the search occurs in a balanced corpus, given the 
parameters we search for, we discover words 
where the objects are literally navigated. Given 
these search parameters, the top results we get are 
generally literal uses of the word “navigate”. We 
cluster the resulting literal uses as semantically 
related words using WordNet and corpus statistics. 
Each such cluster is an emerging prototype source 
domain, or a proto-source, for the potential meta-
phor. 

In Figure 2, we show three of the clusters ob-
tained when searching for the literal usage of the 
verb “navigate”. We use elements of the clusters to 
give names or label the proto-source domains. 
WordNet hypernyms or synonyms are used in most 
cases. The clusters shown in Figure 2 represent 
three potential source domains for the given exam-
ple, the labels “MAZE”, “WAY” and “COURSE” 
are derived from WordNet. 
 

 
Figure 2. Three of several clusters obtained from bal-
anced corpus search for objects of verb “navigate”. 

 
     We rank the clusters according to the combined 
frequency of cluster elements in the balanced cor-
pus. In a similar fashion, clusters are obtained for 
the candidate relation “labyrinthine”; however here 
we search for the nouns modified by the adjective 
“labyrinthine”.       

3.5 Estimation of Linguistic Metaphor 

A ranked list of proto-sources from the previous 
step serves as evidence for the presence of a meta-
phor.   

If any Target domain elements are found in the 
top two ranked clusters, we consider the phrase 
being investigated to be literal. This eliminates 
examples where one of the most frequently en-
countered sources is within the target domain.  

If neither of the top two most frequent clusters 
contains any elements from the target domain, we 
then compute the average imageability scores for 
each cluster from the mean imageability score of 
the cluster elements. If no cluster has a sufficiently 
high imageability score (experimentally deter-
mined to be >.50 in the current prototype), we 
again consider the given input to be literal. This 
step reinforces the claim that metaphors use highly 
imageable language to convey their meaning. If a 
proto-source cluster is found to meet both criteria, 
we consider the given phrase to be metaphorical. 
For the example shown in Figure 1, our system 
finds “navigate the …federal bureaucracy” to be 
metaphorical. One of the top Source domains iden-
tified for this metaphor is “MAZE”. Hence the 
conceptual metaphor output for this example can 
be: 

“FEDERAL BUREAUCRACY IS A MAZE”. 
Our system can thus classify input sentences as 
metaphorical or literal by the series of steps out-
lined above. In addition, we have modules that can 
determine a more complex conceptual metaphor, 
based upon evidence of one or more metaphorical 
passages as identified above. We do not discuss 
those modules in this article. Once a metaphor is 
identified, we compute associated Mappings, Af-
fect and Force. 

3.6 Mappings 

In the current prototype system, we assign meta-
phors to one of three types of mappings. Propertive 
mappings – which state what the domain objects  

1. Proto-source Name: MAZE 
Proto-source Elements: [mazes, system, net-
works] 
IMG Score: 0.74 
2. Proto-source Name: WAY 
Proto-source Elements: [way, tools] 
IMG Score: 0.60 
3. Proto-source Name: COURSE 
Proto-source Elements: [course, streams] 
IMG: 0.55 

Table 1. Algorithm assigns affect of metaphor based upon mappings. 

Rel  < Negative 

Rel  = Neutral 

Rel ≥ Positive 

71



are and descriptive features; Agentive mappings – 
which describe what the domain elements do to 
other objects in the same or different domains; and 
Patientive mappings – which describe what is done 
to the objects in these domains. These are broad 
categories to which relations can, with some ex-
ceptions be assigned at the linguistic metaphor lev-
el by the parse tag of the relation. Relations that 
take Target concepts as objects are usually Pa-
tientive relations. Similarly, relations that are 
Agentive take Target concepts as subjects. Proper-
tive relations are usually determined by adjectival 
relations.  
   Once mappings are assigned, we can use them to 
group linguistic metaphors. A set of linguistic met-
aphors on the same or semantically equivalent 
Target concepts can be grouped together if the re-
lations are all agentive, patientive or propertive. 
The mapping assigned to set of examples in Figure 
3 is Patientive.  
    One immediate consequence of the proposed 
approach is the simplicity with which we can rep-
resent domains, their elements, and the metaphoric 
mappings between domains. Regardless of what 
specific relations may operate within a domain (be 
it Source or Target), they can be classified into just 
3 categories. We are further expanding this module 
to include semantically richer distinctions within 
the mappings. This includes the determination of 
the sub-dimensions of mappings i.e. assigning 
groups of relations to a semantic category.  

3.7 Affect and Force  

Affect of a metaphor may be positive, negative or 
neutral. Our affect estimation module computes an 
affect score taking into account the relation, Target 
concept and the subject or object of the relation 
based on the dependency between relation and 
Target concept. The algorithm is applied according 
to the categories shown in Table 1.  
    The expanded ANEW lexicon (Bradley and 
Lang, 2010) is used to look up affect scores of 
words. ANEW assigns scores from 0 (highly nega-
tive) to 9 (highly positive); 5 being neutral. We 
compute the affect of a metaphorical phrase within 
a sentence by summing the affect scores of the re-
lation and its object or subject.  If the relation is 
agentive, we then look at the object in source do-
main that the Target concept is acting upon. If the 
object (denoted in above table as X) has an affect 

score that is greater than neutral, and the relation 
itself has an affect score that is greater than neutral, 
then a POSITIVE affect is assigned to the meta-
phor. This is denoted by the cell at the intersection 
of the row labeled “Rel > Positive” and the 3rd col-
umn in Table 1. Similarly affect for the other map-
ping categories can be assigned.  
 

 
Figure 3. Four metaphors for the Target concept “feder-

al bureaucracy”.  
 
We also seek to determine the impact of metaphor 
on the reader. This is explored using the concept of 
Force in our system. The force of a metaphor is 
estimated currently by the commonness of the ex-
pression in the given Target domain. We compute 
the frequency of the relation co-occurring with 
Target concept in a corpus of documents in the 
given Target domain. This frequency represents 
the commonness of expression, which is the in-
verse of Force. The more common a metaphorical 
expression is, the lesser its force.  
   For the example shown below in Figure 4, the 
affect is computed to be positive (“navigate” and 
“veterans” are both found to have positive affect 
scores, the relation is patientive). The force of this 
expression is low, since its commonness is 742 
(commonness score > 100 is high commonness, 
determined experimentally).  
 

 
Figure 4. Example of metaphor with positive affect and 

low force. 

1. His attorney described him as a family man 
who was lied to by a friend and who got tangled 
in federal bureaucracy he knew nothing about. 
2. The chart, composed of 207 boxes illustrates 
the maze of federal bureaucracy that would have 
been created by then-President Bill Clinton's rela-
tion health reform plan in the early 1990s. 
3. "Helping my constituents navigate the federal 
bureaucracy is one of the most important things I 
can do," said Owens. 
4. A Virginia couple has donated $1 million to 
help start a center at Arkansas State University 
meant to help wounded veterans navigate the 
federal bureaucracy as they return to civilian life. 
 

A Virginia couple has donated $1 million to help 
start a center at Arkansas State University meant 
to help wounded veterans navigate the federal 
bureaucracy as they return to civilian life. 
 

72



   The focus of this article is the automatic identifi-
cation of metaphorical sentences in naturally oc-
curring text. Affect and force modules are utilized 
to understand metaphors in context and contrast 
them across cultures, if feasible. We defer more 
detailed discussion of affect and force and their 
implications to a future, larger article.  

4 Evaluation and Results 

In order to determine the efficacy of our system in 
classifying metaphors as well as to validate various 
system modules such as affect and force, we per-
formed a series of experiments to collect human 
validation of metaphors in a large set of examples.  

4.1 Experimental Setup  

We constructed validation tasks that aimed at per-
forming evaluation of linguistic metaphor extrac-
tion accuracy. The first task – Task 1, consists of a 
series of examples, typically 50, split more or less 
equally between those proposed by the system to 
be metaphorical and those proposed to be literal. 
This task was designed to elicit subject and expert 
judgments on several aspects related to the pres-
ence or absence of linguistic metaphors in text. 
Subjects are presented with brief passages where a 
Target concept and a relation are highlighted. They 
are asked to rank their responses on a 7-point scale 
for the following questions: 
 
Q1: To what degree does the above passage use 
metaphor to describe the highlighted concept? 
Q2: To what degree does this passage convey an 
idea that is either positive or negative?  
Q3: To what degree is it a common way to express 
this idea?  
   
 There are additional questions that ask subjects to 
judge the imageability and arousal of a given pas-
sage, which we do not discuss in this article. Q1 
deals with assessing the metaphoricity of the ex-
ample, Q2 deals with affect and Q3 deals with 
force. 
  
Each instance of Task 1 consists of a set of instruc-
tions, training examples, and a series of passages to 
be judged. Instructions provide training examples 
whose ratings fall at each end the rating continu-
um. Following the task, participants take a gram-

mar test to demonstrate native language proficien-
cy in the target language. All task instances are 
then posted on Amazon’s Mechanical Turk. The 
goal is to collect at least 30 valid judgments per 
task instance. We typically collect ~50 judgments 
from Mechanical Turkers, so that after filtering for 
invalid data which includes turkers selecting items 
at random, taking too little time to complete the 
task, grammar test failures, and other inconsistent 
data, we would still retain 30 valid judgments per 
passage. In addition to grammar test and time fil-
ter, we also inserted instance of known metaphors 
and known literal passages randomly within the 
Task. Any turker judgments that classify these 
known instance incorrectly more than 30% of the 
total known instance size are discarded. 
    The valid turker judgments are then converted to 
a binary judgment for the questions we presented. 
For example, for question Q1, the anchors to 7-
point scale are 0 (none at all i.e. literal) to 7 (highly 
i.e metaphorical). We take [0, 2] as a literal judg-
ment and [4, 6] as metaphorical and take a majority 
vote. If the majority vote is 3, we discard that pas-
sage from our test set, since it is undetermined 
whether the passage is literal or metaphorical.     
    We have collected human judgments on hun-
dreds of metaphors in all four languages of inter-
est. In Section 4.3, we explain our performance 
and compare our results to baseline where appro-
priate.  

4.2 Test Reliability 

The judgments collected from subjects are tested 
for reliability and validity. Reliability among the 
raters is computed by measuring intra-class corre-
lation (ICC) (McGraw & Wong, 1996; Shrout & 
Fleiss, 1979). A coefficient value above 0.7 indi-
cates strong reliability.  
Table 3 shows the current reliability coefficients 
established for the selected Task 1 questions in all 
4 languages. In general, our analyses have shown 
that with approximately 30 or more subjects we 
obtain a reliability coefficient of at least 0.7. We 
note that Russian and Farsi reliability scores are 
low in some categories, primarily due to lack of 
sufficient subject rating data. However, reliability 
of subject ratings for metaphor question (Q1) is 
sufficiently high in three of the four languages we 
are interested in. 
 

73



Dimension English Spanish Russian  Farsi 
Metaphor .908 .882 .838 .606 
Affect  .831 .776 .318 .798 
Commonness .744 .753 .753 .618 
Table 3. Intraclass correlations for linguistic metaphor 
assessment by Mechanical Turk subjects (Task 1) 

4.3 Results 

In Table 4, we show our performance at classifying 
metaphors across four different languages. The 
baseline in this table assigns all given examples in 
the test set to be metaphorical. We note that per-
formance of the system at the linguistic metaphor 
level when compared to human gold standard is 
significantly over baseline for all four languages. 
The system performances cited in Table 4 validate 
the system against test sets that contain the distri-
bution of metaphorical vs. literal examples as out-
lined in Table 5. 
 

 English Spanish Russian Farsi 
Baseline 45.8% 41.7% 56.4% 50% 
System 71.3% 80% 69.2% 78% 
Table 4. Performance accuracy of system when com-

pared to baseline for linguistic metaphor classification. 
 

 English Spanish Russian Farsi 
Metaphor 50 50 22 25 

Literal 59 70 17 25 
Total 109 120 39 50 

Table 5. Number of metaphorical and literal examples 
in test sets across all four languages. 

 
Table 6 shows the accuracy in classification by the 
Affect and Force modules. We note that the low 
performance of affect and force for languages oth-
er than English. Our focus has been on improving 
NLP tools for Spanish, Russian and Farsi, so that a 
similar robust performance for those language can 
be achieved as we can demonstrate in English. 
 
Accuracy English Spanish Russian Farsi 
Affect  72% 54% 51% 40% 
Force 67% 50% 33% 66% 

Table 6. Affect and force performance of system on 
linguistic metaphor level. 

 

5 Discussion and Future Work 

In this article, we described in detail our approach 
to detecting metaphors in text. We have developed 

an automated system that does not require the ex-
istence of annotated training data or a knowledge 
base of predefined metaphors. We have described 
the various steps for detecting metaphors from re-
ceiving an input, to selecting candidate relations, to 
the discovery of prototypical source domains, and 
leading to the identification of a metaphor as well 
as the discovery of the potential source domain 
being applied in the metaphor. We presented two 
novel concepts that have heretofore not been fully 
explored in computational metaphor identification 
systems. The first is the exclusion of words that 
form the thread of the discussion in the text, by the 
application of a Topic Tracking module. The se-
cond is the application of Imageability scores in 
the selection of salient candidate relations.  
Our evaluation consists first of validating the eval-
uation task itself. Once we ensure that sufficient 
reliability has been established on the various di-
mensions we seek to evaluate – metaphoricity, af-
fect and force – we compare our system 
performance to the human gold standard. The per-
formance of our system as compared to baseline is 
quite high, across all four languages of interest 
when measured against human assessed gold 
standard.  
In this article, we discuss examples of metaphors 
belonging to a specific Target domain – “Govern-
ance”. However, we can run our system through 
data in any domain perform the same kind of met-
aphor identification. In cases where the Target do-
main is unknown, we plan to use our Topic 
tracking module to recognize content words that 
may form part of a metaphorical phrase. This is 
essentially a process that is the reverse of that de-
scribed in Section 3.3. We will find the salient 
Target concepts where there are directly dependent 
relations with the imageable verbs or adjectives.  
In a separate larger publication, we plan to discuss 
in detail revisions to our Mapping module as well 
as the discovery and analyses of more complex 
conceptual metaphors. Such complex metaphors 
are based upon evidence from one or more instance 
of linguistic metaphors. Additional modules would 
recognize the manifold mappings, affect and force 
associated with the complex conceptual metaphors.   

Acknowledgments 
This research is supported by the Intelligence Advanced 
Research Projects Activity (IARPA) via Department of 

74



Defense US Army Research Laboratory contract num-
ber W911NF-12-C-0024. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Governmental purposes notwithstanding any copyright 
annotation thereon.  Disclaimer: The views and conclu-
sions contained herein are those of the authors and 
should not be interpreted as necessarily representing the 
official policies or endorsements, either expressed or 
implied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. 

References  
Allbritton, David W., Gail McKoon, and Richard J. 

Gerrig. 1995. Metaphor-Based Schemas and Text 
Representations: Making Connections Through Con-
ceptual Metaphors, Journal of Experimental Psy-
chology: Learning, Memory, and Cognition, Vol. 21, 
No. 3, pp. 612-625. 

Baumer, Erik. P.S., White, James., Tomlinson, Bill. 
2010. Comparing Semantic Role Labeling with 
Typed Dependency Parsing in Computational Meta-
phor Identification. Proceedings of the NAACL HLT 
2010 Second Workshop on Computational Ap-
proaches to Linguistic Creativity, pages 14–22, Los 
Angeles, California, June 2010.  

Bradley, M.M. & Lang, P.J. 2010. Affective Norms for 
English Words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-2. University of 
Florida, Gainesville, FL. 

Broadwell George A., Jennifer Stromer-Galley, Tomek 
Strzalkowski, Samira Shaikh, Sarah Taylor, Umit 
Boz, Alana Elia, Laura Jiao, Ting Liu and Nick 
Webb. 2012. Modeling Socio-Cultural Phenomena in 
Discourse. Journal of Natural Language Engineer-
ing, Cambridge Press. 

Broadwell, George A., Umit Boz, Ignacio Cases, Tomek 
Strzalkowski, Laurie Feldman, Sarah Taylor, Samira 
Shaikh, Ting Liu, Kit Cho, aand Nick Webb. 2013. 
Using imageability and topic chaining to locate met-
aphors in linguistic corpora. in Ariel M. Greenberg, 
William G. Kennedy, Nathan D. Bos and Stephen 
Marcus, eds. Proceedings of the 6th International 
Conference on Social Computing, Behavioral-
Cultural Modeling and Prediction SBP 2013. 

Carbonell, Jaime. 1980. Metaphor: a key to extensible 
semantic analysis. Proceedings of the 18th Annual 
Meeting on Association for Computational Linguis-
tics. 

Charteris-Black, Jonathan 2002 Second Language Fig-
urative Proficiency: A Comparative Study of Malay 
and English. Applied Linguistics 23/1: 104-133. 

Coltheart, M. 1981. The MRC Psycholinguistic Data-
base. Quarterly Journal of Experimental Psychology, 
33A, 497-505. 

Fass, Dan. 1991. met*: A Method for Discriminating 
Metonymy and Metaphor by Computer. Computa-
tional Linguistics, Vol 17:49-90 

Feldman, J. and S. Narayanan. 2004. Embodied mean-
ing in a neural theory of language. Brain and Lan-
guage, 89(2):385–392. 

Fellbaum, C. editor. 1998. WordNet: An Electronic 
Lexical Database (ISBN: 0-262-06197-X). MIT 
Press, first edition. 

Gedigian, M., Bryant, J., Narayanan, S., & Ciric, B. 
(2006). Catching Metaphors. Proceedings of the 
Third Workshop on Scalable Natural Language Un-
derstanding ScaNaLU 06 (pp. 41-48). Association 
for Computational Linguistics. 

Klein, Dan and Manning, Christoper D. 2003. Accurate 
Unlexicalized Parsing. Proceedings of the 41st Meeting 
of the Association for Computational Linguistics, pp. 
423-430. 

Krishnakumaran, S. and X. Zhu. 2007. Hunting elusive 
metaphors using lexical resources. In Proceedings of 
the Workshop on Computational Approaches to Fig-
urative Language, pages 13–20, Rochester, NY. 

Lakoff, George and Johnson, Mark. 1980. Metaphors 
We Live By. University Of Chicago Press. 

Lakoff, George. 2001. Moral Politics: what Conserva-
tives Know that Liberals Don’t. University of Chica-
go Press. 

Malkki, Liisa. 1992. National Geographic: The Rooting 
of People and the Territorialization of National Iden-
tity Among Scholars and Refugees. Society for Cul-
tural Anthropology 7(1):24-44 

Martin, James. 1988. A Computational Theory of Meta-
phor. PH.D. Dissertation 

McGraw, K. O., & Wong, S. P. 1996. Forming infer-
ences about some intraclass correlation coefficients. 
Psychological Methods, 1(1), 30-46. 

Musolff, Andreas. 2008. What can Critical Metaphor 
Analysis Add to the Understanding of Racist Ideolo-
gy? Recent Studies of Hitler’s Anti-Semitic Meta-
phors, Critical Approaches to Discourse Analysis 
across Disciplines, http://cadaad.org/ejournal, Vol. 
2(2): 1-10. 

O’Halloran, Kieran. 2007. Critical Discourse Analysis 
and the Corpus-informed Interpretation of Metaphor 
at the Register Level. Oxford University Press 

75



Shrout, P. E., & Fleiss, J. L. 1979. Intraclass correla-
tions: Uses in assessing rater reliability. Psychologi-
cal Bulletin, 86 (2), 420-428. 

Shutova, E. 2010. Models of Metaphors in NLP. In 
Proceedings of ACL 2010, Uppsala, Sweden. 

Shutova, E. and S. Teufel. 2010a. Metaphor corpus an-
notated for source - target domain mappings. In Pro-
ceedings of LREC 2010, Malta. 

Shutova, E., T. Van de Cruys and A. Korhonen. 
2012. Unsupervised Metaphor Paraphrasing Using a 
Vector Space Model, In Proceedings of COLING 
2012, Mumbai, India 

Turney, Peter., Yair Neuman, Dan Assaf, and Yohai 
Cohen. 2011. Literal and metaphorical sense identifi-
cation through concrete and abstract context. In Pro-
ceedings of EMNLP, pages 680–690, Edinburgh, UK 

Wilks, Yorick. 1975. Preference semantics. Formal 
Semantics of Natural Language, E. L. Keenan, Ed. 
Cambridge University Press, Cambridge, U.K., 329--
348. 

Wilson, M.D. (1988) The MRC Psycholinguistic Data-
base: Machine Readable Dictionary, Version 2. Be-
havioural Research Methods, Instruments and 
Computers, 20(1), 6-11. 

 
 
 
 

76


