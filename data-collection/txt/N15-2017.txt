



















































Speeding Document Annotation with Topic Models


Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 126–132,
Denver, Colorado, June 1, 2015. c©2015 Association for Computational Linguistics

Speeding Document Annotation with Topic Models

Forough Poursabzi-Sangdeh and Jordan Boyd-Graber
Computer Science

University of Colorado Boulder
{forough.poursabzisangdeh, Jordan.Boyd.Graber}@colorado.edu

Abstract

Document classification and topic models
are useful tools for managing and under-
standing large corpora. Topic models are
used to uncover underlying semantic and
structure of document collections. Catego-
rizing large collection of documents requires
hand-labeled training data, which is time
consuming and needs human expertise. We
believe engaging user in the process of docu-
ment labeling helps reduce annotation time
and address user needs. We present an in-
teractive tool for document labeling. We
use topic models to help users in this pro-
cedure. Our preliminary results show that
users can more effectively and efficiently ap-
ply labels to documents using topic model
information.

1 Introduction

Many fields depend on texts labeled by human
experts; computational linguistics uses such an-
notation to determine word senses and senti-
ment (Kelly and Stone, 1975; Kim and Hovy,
2004); social science uses “coding” to scale up
and systemetize content analysis (Budge, 2001;
Klingemann et al., 2006). In general text clas-
sification is a standard tool for managing large
document collections.

However, these labeled data have to come from
somewhere. The process for creating a broadly
applicable, consistent, and generalizable label set
and then applying them to the dataset is long
and difficult, requiring expensive annotators to
examine large swaths of the data.

We present a user interactive tool for document
labeling that uses topic models to help users as-
sign appropriate labels to documents (Section 2).
In Section 3, we describe our user interface and
experiments on Congressional Bills data set. We
also explain an evaluation metric to assess the
quality of assigned document labels. In prelimi-
nary results, we show that annotators can more
quickly label a document collection given a topic
modeling overview. While engaging user in the
process of content-analysis has been studied be-
fore(as we discuss in Section 4), in Section 4 we
describe how our new framework allows for more
flexibility and interactivity. Finally, in Section 5,
we discuss the limitation of our framework and
how we plan to extend it in future.

2 Interactive Document Labeling

We propose an alternative framework for assign-
ing labels to documents. We use topic models to
give an overview of the document contents to the
user. Users can create a label set incrementally,
see the content of documents, assign labels to
documents, and classify documents. They can
go back and forth in these steps and edit label
set or document labels and re-classify.

Having labeled documents is necessary for au-
tomatic text classification. With a large collec-
tion of unstructured documents, labeling can be
excruciating since it is essential to label enough
documents in different labels to obtain accept-
able accuracy. Topic models are a solution to
reduce this effort since they provide some infor-
mation about the underlying theme of corpus.
Given a fixed number of topics, topic models

126



Classification

Topic Models

Start with LDA

Document Labelling

label 1 label 2 label m

...

label 1 label 2 label m

...

SLDA/LDA

Figure 1: Interactive document labeling: Start with lda topic modeling, show users relevant documents for
each topic, get user labels, classify documents, and use slda to generate topics1. Repeat this until the user is
satisfied with labels.

output (i) a set of words for each topic (Topic
words) and (ii) a distribution over topics for each
document (Document’s Topic Distribution).

Topic words can be used to reveal the content
of a topic and thus content of documents with a
high probability of that topic. Therefore, assum-
ing the number of topics is chosen carefully, top
documents for each topic are similar in content
and can be labeled appropriately.

Thus, rather than showing an unstructured
collection of documents to the user, providing
the topic words and highly relevant documents to
that topic helps them in the process of document
labeling, both in the step of choosing appropriate
label names and choosing appropriate document
to assign a label to. Another way to think about
this is that if the topics are perfect (they are not
too general or too detailed), all labels associated
with the topic’s high relevant documents can be
viewed as subjects explaining the topic. Table 1
provides an example of how topic models can
help a user craft document labels.

Having a set of user labeled documents, classifi-
cation algorithms can be used to predict the label
of unseen documents. Next, classification results
are shown. Users can change document labels.
They can also edit/delete label set and re-run
the classifier. The explained procedure can be
repeated iteratively until satisfaction is achieved
with existing (document,label) pairs. Figure 1

shows the explained procedure.

3 Experiments with Interactive
Labeling Interface

Data: In our experiments, we need a labeled
corpus to be able to assess the quality of user-
generated labels. We chose US Congressional
Bills corpus (Adler and Wilkerson, 2006). Gov-
Track provides bill texts along with the discussed
congressional issues as labels. Example of labels
are “education”, “agriculture”, “health”, and
“defense”. There are total of 19 unique labels.
We use the 112th congress, which has 12274 doc-
uments. We remove bills with no assigned gold
label or that are short. We end with 6528 docu-
ments.

Topic Modeling: To generate topics, we use
Mallet (McCallum, 2002) to apply lda on the
data. A set of extra stop words are generated
based on tf-idf scores to avoid displaying non-
informative words to the user.

Features and Classification: A crucial step
for text classification is to extract useful features
to represent documents. Some common features
for text classification are n-grams, which makes
the dimensionality very high and classification
slower. Since response time is very important in
user interactive systems, instead of n-grams, we

1Currently, we are not using slda. We just use the
original topics generated by lda. The idea behind slda
is explained in Section 5.

127



Topic Words Document Title Document Labels
16 dod, sbir,

afghanistan, phase,
sttr, missile, com-
bat, capabilities,
command, elements

HR 4243 IH 112th CONGRESS 2d Session H. R. 4243
To strengthen the North Atlantic Treaty Organiza-
tion.

military

19 historic,conveyance, dated,
monument,
depicted, generally,
boundary, creek,
preservation,
recreation

HR 4334 IH 112th CONGRESS 2d Session H. R. 4334
To establish a monument in Dona Ana County, New
Mexico, and for other purposes.

wildlife

S 617 IS 112th CONGRESS 1st Session S. 617 To
require the Secretary of the Interior to convey certain
Federal land to Elko County, Nevada, and to take
land into trust for the Te-moak Tribe of Western
Shoshone Indians of Nevada, and for other purposes.

nature

Table 1: An example of topic words and the labels user has assigned to top documents for that topic.

use topic probabilities as features, which reduces
the dimensionality and classification time signifi-
cantly. User can choose 10, 15, 25, or 50 topics.
We want to show the label probabilities gener-
ated by classifier to users. We use Liblinear (Fan
et al., 2008) to run L2 regularized logistic regres-
sion for classifying documents and generating
label probabilities.

Interface: We start with the web-based in-
terface of Hu et al. (2014) for interactive topic
modeling. The existing interface starts with ask-
ing user information, corpus name, and number
of topics they want to explore. Then it displays
topic words and the most relevant documents for
each topic. Also, the user can see the content of
documents. Users can create new labels and/or
edit/delete an existing label.

When seeing a document, user has 3 options:

1. Create a new label and assign that label to
the document.

2. Choose an existing label for the document.
3. Skip the document.

At any point, the user can run the classifier.
After classification is finished, the predicted la-
bels along with the certainty is shown for each
document. User can edit/delete document la-
bels and re-run classifier as many times as they
desire. We Refer to this task as Topic Guided
Annotation(TGA).

Figure 2 shows a screenshot of the interface
when choosing a label for a document.

3.1 Evaluation

We introduce an interactive framework for docu-
ment labeling using topic models. In this section,
we evaluate our system.

Our goal is to measure whether showing users
a topic modeling overview of the corpus helps
them apply labels to documents more effectively
and efficiently. Thus, we compare user-generated
labels (considering labels assigned by user and
classifier altogether) with gold labels of US Con-
gressional Bills provided by GovTrack. Since
user labels can be more specific than gold labels,
we want each user label to be “pure” in gold
labels. Thus, we use the purity score (Zhao and
Karypis, 2001) to measure how many gold labels
are associated with each user label. Purity score
is

purity(U , G ) =
1
N

∑
k

max
j
|Uk ∩Gj |, (1)

where U = {U1, U2, ..., UK} is the user clustering
of documents, G = {G1, G2, ..., GJ} is gold clus-
tering of documents, and N is the total number
of documents. Moreover, we interpret Uk and
Gj as the set of documents in user cluster UK
or gold cluster Gj . Figure 3 shows an example
of purity calculation for a clustering, given gold
labels.

Purity is an external metric for cluster evalua-
tion. A very bad labeling has a purity score close
to 0 and a perfect labeling has purity score of 1.

128



Figure 2: A screenshot of interactive document labeling interface. The user sees topic words and the most
relevant documents for each topic. The user has created two labels: “Education” and “Health” and sees the
content of a documents. The user can create a new label and assign the new label to the document, or choose
one of the two existing labels to assign to the document, or skip the document and view the previous or next
document.

Figure 3: An example of computing purity: Clusters
correspond to user labels and different shapes corre-
spond to different associated gold labels. Majority
gold label numbers for three clusters are 4(U1), 3(U2),
and 5(U3). Purity is 117 × (4 + 3 + 5) ≈ 0.71.

The higher this score, the higher the quality of
user labels.

To evaluate TGA, We did a study on two
different users. For User 1, we chose 15 topics and
for User 2, we chose 25 topics. They were asked
to stop labeling whenever they were satisfied
with the predicted document labels.

We compare the user study results with a base-
line. Our baseline ignores topic modeling infor-

mation for choosing documents to labels. It con-
siders the scenario when users are given a large
document collection and are asked to categorize
the documents without any other information.
Thus, we show randomly chosen documents to
users and want them to apply label to them. All
users can go back and edit or delete document la-
bels, or refuse to label a document if they find it
confusing. After each single labeling, we use the
same features and classifier that we used for user
study with topic models to classify documents.
Then we calculate purity for user labels with
respect to gold labels. Figure 4 shows the purity
score over different number of labeled documents
for User 1, User 2, and baseline.

User 1 did the labeling in 6 rounds, whereas
User 2 did total of 7 rounds. User 1 ended with
116 labeled documents and user 2 had 42 labeled
documents in the end.

User 2 starts with a label set of size 9 and labels
11 documents. Two documents are labeled as
“wildlife”, other two are labeled as “tax”, and all
other documents have unique labels. This means
that even if there are very few instance per label,
baseline is outperformed. This is an evidence of

129



Number of labeled documents
0 20 40 60 80 100 120

Pu
rit

y

0.1

0.2

0.3

0.4

baseline
TGA 2
TGA 1

Figure 4: Purity score over number of labeled docu-
ments. TGA 1 and TGA 2 refer to results for User
1 and User 2.

User 1 Baseline User 2 Baseline
36 12 11 12
50 52 17 18
58 60 20 38
82 109 23 40
103 > 116 30 112
116 > 116 35 116

42 115

(a) (b)

Table 2: The number of required labeled documents
for baseline to get the same purity score as (a) User
1 (b) User 2, in each round

choosing informative documents to assign labels
with the help of topic models. On the other hand,
User 1 starts with a label set of size 7 and labels
36 documents and is outperformed by baseline
significantly. One reason for this is that assigning
too many documents relevant to a topic, with the
same label doesn’t provide any new information
to the classifier and thus the user could get the
same purity score with a lower number of labeled
documents, which would lead to outperforming
baseline. User 1 outperforms the baseline in the
second (8 labels and 50 labeled documents) and
third round (9 labels and 58 labeled documents)
slightly. In the fourth round, user creates more
labels. With total of 13 labels and 82 labeled
documents, the gap between user’s purity score
and baseline gets larger. Both users outperform
baseline in the final round.

To see how topic models help speed up labeling
process, we compare the number of user labeled
documents with the approximate number of re-
quired labeled documents to get the same purity
score in baseline. Table 2 shows the results for
User 1 and User 2.

User 1 starts with man labeled documents and
baseline can achieve the same performance with
one third of the labeled documents. As the user
keeps labeling more documents, the performance
improves and baseline needs more labeled docu-
ments to get the same level of purity. For User
2, baseline on average needs over two times as
many labeled documents to achieve the same
purity score as user labels. These tables indicate
that topic models help users choose documents
to assign labels to and achieve an acceptable
performance with fewer labeled documents.

4 Related Work

Topic Models such as Latent Dirichlet Allocation
(Blei et al., 2003, lda) are unsupervised learning
algorithms and are a useful tool for understand-
ing the content of large collection of documents.
The topics found by these models are the set of
words that are observed together in many doc-
uments and they introduce correlation among
words. Top words in each topic explain the se-
mantics of that topic. Moreover, each document
is considered a mixture of topics. Top topics
for each document explain the semantics of that
document.

When all documents are assigned a la-
bel, supervised topic models can be used.
slda (Mcauliffe and Blei, 2008) is a supervised
topic model that generates topics that give an
overview of both document contents and assigned
labels. Perotte et al. (2011) extend slda and in-
troduce hslda, which is a model for large-scale
multiply-labeled documents and takes advantage
of hierarchical structure in label space. hslda
is used for label prediction. In general, super-
vised topic models help users understand labeled
document collections.

Text classification predicts a label for docu-
ments and help manage document collections.
There are known classifiers as well as feature
extraction methods for this task. However, pro-
viding an initial set of labeled documents for both
text classification and supervised topic models
still requires lots of time and human effort.

Active learning (Settles, 2010), reduces the
amount of required labeled data by having a

130



learner which actively queries the label for spe-
cific documents and collects a labeled training
set. In a user interactive system, the active
learner queries document labels from users (Set-
tles, 2010). In other words, the learner suggests
some documents to the user and wants the user
to assign a label to those. Settles (2011) dis-
cusses that having interactive users in annotation
process along with active learning, reduces the
amount of annotation time while still achieving
acceptable performance. In more detail, they
presents an interactive learning framework to get
user annotations and produce accurate classifiers
in less time. The shortcoming of active learning
is that they don’t provide any overview infor-
mation of corpus, like topic model approaches
do.

Nevertheless, new methods in both analysis
and evaluation are needed. Classification algo-
rithms restrict document labels to a predefined
label set. Grimmer and Stewart (2013) show
that to be able to use the output of automatic
text analysis in political science, we need care-
ful validation methods. There has been some
work done on bringing user in this task for re-
fining and evaluating existing methods. Hu et
al. (2014) show that topic models are not perfect
from the user view and introduce a framework to
interactively get user feedback and refine topic
models. Chuang et al. (2013) present an inter-
active visualization for exploring documents by
topic models to address user needs.

We bring these tools together to speed up anno-
tation process. We believe having users engaged
in content analysis, not only reduces the amount
of annotation time, but also helps to achieve user
satisfaction. We propose an iterative and user
interactive procedure for document annotation.
We use topic models to provide some high-level
information about the corpus and guid users in
this task. We show top words and documents
for each topic to the user and have them start la-
beling documents. Users can create/edit/delete
labels. Then users can run a classifier to predict
the labels for the unlabeled documents. They
can change document labels and re-classify docu-
ments iteratively, until satisfaction is achieved.

5 Future Work

There are some obvious directions that will ex-
pand this ongoing research. First, we are plan-
ning to use active learning to better aid clas-
sification. We expect that active learning will
reduce the number of required labeled documents
while still getting a high purity score and user
satisfaction.

Second, we will use supervised topic mod-
els (Mcauliffe and Blei, 2008, slda) instead of
lda after the first round to update topics based
on document labels. slda uses labeled docu-
ments to find topics that explain both docu-
ment content and their associated labels. We
believe using slda instead of lda after the first
round will give users more information about the
overview of documents and help them further for
applying labels to documents.

Third, we want to allow the user to refine
and correct labels further. Our existing interface
allows the user to delete a label or edit a label.
We believe it is also important for users to merge
labels if they think the labels are too specific. In
addition, we believe a crucially important step is
to generate the label set. Giving the user some
information about the range of documents can
help them generate a better label set. One other
option is to suggest labels to users based on topic
models (Lau et al., 2010).

Fourth, we will explore other corpora such
as European Parliament corpus (Koehn, 2005).
To our knowledge, there are no true labels for
Europarl corpus and using our interactive tool
can help users find the categorized information
they need.

Finally, for evaluating our method, in addition
to using the correct labeling and purity score, we
will conduct a user experiment with more users
involved. Since the task of labeling congress
data set requires some political knowledge, we
will choose annotators who have some political
science background.

Acknowledgments

We thank the anonymous reviewers for their
insightful comments. We thank Dr. Niklas
Elmqvist for his advice for revising the user inter-

131



face. We also thank Alvin Grissom II for helping
us in the user study. This work was supported
by NSF Grant NCSE-1422492. Any opinions,
findings, results, or recommendations expressed
here are of the authors and do not necessarily
reflect the view of the sponsor.

References

E Scott Adler and John Wilkerson. 2006. Congres-
sional bills project. NSF, 880066:00880061.

David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3.

Ian Budge. 2001. Mapping policy preferences: esti-
mates for parties, electors, and governments, 1945-
1998, volume 1. Oxford University Press.

Jason Chuang, Yuening Hu, Ashley Jin, John D
Wilkerson, Daniel A McFarland, Christopher D
Manning, and Jeffrey Heer. 2013. Document ex-
ploration with topic modeling: Designing inter-
active visualizations to support effective analysis
workflows. In NIPS Workshop on Topic Models:
Computation, Application, and Evaluation.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal
of Machine Learning Research, 9:1871–1874.

Justin Grimmer and Brandon M Stewart. 2013. Text
as data: The promise and pitfalls of automatic con-
tent analysis methods for political texts. Political
Analysis, page mps028.

Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,
and Alison Smith. 2014. Interactive topic model-
ing. Machine learning, 95(3):423–469.

Edward F Kelly and Philip J Stone. 1975. Com-
puter recognition of English word senses, volume 13.
North-Holland.

Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of the
20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.

Hans-Dieter Klingemann, Andrea Volkens, Judith
Bara, Ian Budge, Michael D McDonald, et al. 2006.
Mapping policy preferences II: estimates for par-
ties, electors, and governments in Eastern Europe,
European Union, and OECD 1990-2003. Oxford
University Press Oxford.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT summit,
volume 5, pages 79–86.

Jey Han Lau, David Newman, Sarvnaz Karimi, and
Timothy Baldwin. 2010. Best topic word selection
for topic labelling. In Coling 2010: Posters, pages
605–613, Beijing, China, August.

Jon D Mcauliffe and David M Blei. 2008. Supervised
topic models. In Advances in neural information
processing systems, pages 121–128.

Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.

Adler J. Perotte, Frank Wood, Noemie Elhadad, and
Nicholas Bartlett. 2011. Hierarchically supervised
latent Dirichlet allocation. In Advances in Neural
Information Processing Systems 24, pages 2609–
2617.

Burr Settles. 2010. Active learning literature survey.
University of Wisconsin, Madison, 52(55-66):11.

Burr Settles. 2011. Closing the loop: Fast, interac-
tive semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467–1478. Association for Com-
putational Linguistics.

Ying Zhao and George Karypis. 2001. Criterion
functions for document clustering: Experiments
and analysis. Technical report, Citeseer.

132


