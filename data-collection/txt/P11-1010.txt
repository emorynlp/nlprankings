










































Semi-Supervised SimHash for Efficient Document Similarity Search


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93–101,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

Semi-Supervised SimHash for Efficient Document Similarity Search

Qixia Jiang and Maosong Sun
State Key Laboratory on Intelligent Technology and Systems

Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China

qixia.jiang@gmail.com, sms@tsinghua.edu.cn

Abstract

Searching documents that are similar to a
query document is an important component
in modern information retrieval. Some ex-
isting hashing methods can be used for effi-
cient document similarity search. However,
unsupervised hashing methods cannot incor-
porate prior knowledge for better hashing.
Although some supervised hashing methods
can derive effective hash functions from prior
knowledge, they are either computationally
expensive or poorly discriminative. This pa-
per proposes a novel (semi-)supervised hash-
ing method named Semi-Supervised SimHash
(S3H) for high-dimensional data similarity
search. The basic idea of S3H is to learn the
optimal feature weights from prior knowledge
to relocate the data such that similar data have
similar hash codes. We evaluate our method
with several state-of-the-art methods on two
large datasets. All the results show that our
method gets the best performance.

1 Introduction

Document Similarity Search (DSS) is to find sim-
ilar documents to a query doc in a text corpus or
on the web. It is an important component in mod-
ern information retrieval since DSS can improve the
traditional search engines and user experience (Wan
et al., 2008; Dean et al., 1999). Traditional search
engines accept several terms submitted by a user
as a query and return a set of docs that are rele-
vant to the query. However, for those users who
are not search experts, it is always difficult to ac-
curately specify some query terms to express their

search purposes. Unlike short-query based search,
DSS queries by a full (long) document, which allows
users to directly submit a page or a document to the
search engines as the description of their informa-
tion needs. Meanwhile, the explosion of information
has brought great challenges to traditional methods.
For example, Inverted List (IL) which is a primary
key-term access method would return a very large
set of docs for a query document, which leads to the
time-consuming post-processing. Therefore, a new
effective algorithm is required.

Hashing methods can perform highly efficient but
approximate similarity search, and have gained great
success in many applications such as Content-Based
Image Retrieval (CBIR) (Ke et al., 2004; Kulis et
al., 2009b), near-duplicate data detection (Ke et
al., 2004; Manku et al., 2007; Costa et al., 2010),
etc. Hashing methods project high-dimensional ob-
jects to compact binary codes called fingerprints and
make similar fingerprints for similar objects. The
similarity search in the Hamming space1 is much
more efficient than in the original attribute space
(Manku et al., 2007).

Recently, several hashing methods have been pro-
posed. Specifically, SimHash (SH) (Charikar M.S.,
2002) uses random projections to hash data. Al-
though it works well with long fingerprints, SH has
poor discrimination power for short fingerprints. A
kernelized variant of SH, called Kernelized Local-
ity Sensitive Hashing (KLSH) (Kulis et al., 2009a),
is proposed to handle non-linearly separable data.
These methods are unsupervised thus cannot incor-
porate prior knowledge for better hashing. Moti-

1Hamming space is a set of binary strings of length L.

93



vated by this, some supervised methods are pro-
posed to derive effective hash functions from prior
knowledge, i.e., Spectral Hashing (Weiss et al.,
2009) and Semi-Supervised Hashing (SSH) (Wang
et al., 2010a). Regardless of different objectives,
both methods derive hash functions via Principle
Component Analysis (PCA) (Jolliffe, 1986). How-
ever, PCA is computationally expensive, which lim-
its their usage for high-dimensional data.

This paper proposes a novel (semi-)supervised
hashing method, Semi-Supervised SimHash (S3H),
for high-dimensional data similarity search. Un-
like SSH that tries to find a sequence of hash func-
tions, S3H fixes the random projection directions
and seeks the optimal feature weights from prior
knowledge to relocate the objects such that simi-
lar objects have similar fingerprints. This is im-
plemented by maximizing the empirical accuracy
on the prior knowledge (labeled data) and the en-
tropy of hash functions (estimated over labeled and
unlabeled data). The proposed method avoids us-
ing PCA which is computationally expensive espe-
cially for high-dimensional data, and leads to an
efficient Quasi-Newton based solution. To evalu-
ate our method, we compare with several state-of-
the-art hashing methods on two large datasets, i.e.,
20 Newsgroups (20K points) and Open Directory
Project (ODP) (2.4 million points). All experiments
show that S3H gets the best search performance.

This paper is organized as follows: Section 2
briefly introduces the background and some related
works. In Section 3, we describe our proposed Semi-
Supervised SimHash (S3H). Section 4 provides ex-
perimental validation on two datasets. The conclu-
sions are given in Section 5.

2 Background and Related Works

Suppose we are given a set of N documents, X =
{xi | xi ∈ RM}Ni=1. For a given query doc q, DSS
tries to find its nearest neighbors in X or a subset
X ′ ⊂ X in which distance from the documents to
the query doc q is less than a give threshold. How-
ever, such two tasks are computationally infeasible
for large-scale data. Thus, it turns to the approxi-
mate similarity search problem (Indyk et al., 1998).
In this section, we briefly review some related ap-
proximate similarity search methods.

2.1 SimHash
SimHash (SH) is first proposed by Charikar
(Charikar M.S., 2002). SH uses random projections
as hash functions, i.e.,

h(x) = sign(wT x) =

{
+1, if wT x ≥ 0
−1, otherwise (1)

where w ∈ RM is a vector randomly generated. SH
specifies the distribution on a family of hash func-
tions H = {h} such that for two objects xi and xj ,

Pr
h∈H

{h(xi) = h(xj)} = 1−
θ(xi,xj)

π
(2)

where θ(xi,xj) is the angle between xi and xj . Ob-
viously, SH is an unsupervised hashing method.

2.2 Kernelized Locality Sensitive Hashing

A kernelized variant of SH, named Kernelized
Locality Sensitive Hashing (KLSH) (Kulis et al.,
2009a), is proposed for non-linearly separable data.
KLSH approximates the underling Gaussian distri-
bution in the implicit embedding space of data based
on central limit theory. To calculate the value of
hashing fuction h(·), KLSH projects points onto the
eigenvectors of the kernel matrix. In short, the com-
plete procedure of KLSH can be summarized as fol-
lows: 1) randomly select P (a small value) points
from X and form the kernel matrix, 2) for each hash
function h(ϕ(x)), calculate its weight ω ∈ RP just
as Kernel PCA (Schölkopf et al., 1997), and 3) the
hash function is defined as:

h(ϕ(x)) = sign(

P∑
i=1

ωi · κ(x,xi)) (3)

where κ(·, ·) can be any kernel function.
KLSH can improve hashing results via the kernel

trick. However, KLSH is unsupervised, thus design-
ing a data-specific kernel remains a big challenge.

2.3 Semi-Supervised Hashing

Semi-Supervised Hashing (SSH) (Wang et al.,
2010a) is recently proposed to incorporate prior
knowledge for better hashing. Besides X , prior
knowledge in the form of similar and dissimilar
object-pairs is also required in SSH. SSH tries to
find L optimal hash functions which have maximum

94



empirical accuracy on prior knowledge and maxi-
mum entropy by finding the top L eigenvectors of
an extended covariance matrix2 via PCA or SVD.

However, despite of the potential problems of nu-
merical stability, SVD requires massive computa-
tional space and O(M3) computational time where
M is feature dimension, which limits its usage for
high-dimensional data (Trefethen et al., 1997). Fur-
thermore, the variance of directions obtained by
PCA decreases with the decrease of the rank (Jol-
liffe, 1986). Thus, lower hash functions tend to have
smaller entropy and larger empirical errors.

2.4 Others

Some other related works should be mentioned. A
notable method is Locality Sensitive Hashing (LSH)
(Indyk et al., 1998). LSH performs a random
linear projection to map similar objects to similar
hash codes. However, LSH suffers from the effi-
ciency problem that it tends to generate long codes
(Salakhutdinov et al., 2007). LAMP (Mu et al.,
2009) considers each hash function as a binary par-
tition problem as in SVMs (Burges, 1998). Spec-
tral Hashing (Weiss et al., 2009) maintains similar-
ity between objects in the reduced Hamming space
by minimizing the averaged Hamming distance3 be-
tween similar neighbors in the original Euclidean
space. However, spectral hashing takes the assump-
tion that data should be distributed uniformly, which
is always violated in real-world applications.

3 Semi-Supervised SimHash

In this section, we present our hashing method,
named Semi-Supervised SimHash (S3H). Let XL =
{(x1, c1) . . . (xu, cu)} be the labeled data, c ∈
{1 . . . C}, x ∈ RM , and XU = {xu+1 . . .xN} the
unlabeled data. Let X = XL ∪ XU . Given the
labeled data XL, we construct two sets, attraction
set Θa and repulsion set Θr. Specifically, any pair
(xi,xj) ∈ Θa, i, j ≤ u, denotes that xi and xj
are in the same class, i.e., ci = cj , while any pair
(xi,xj) ∈ Θr, i, j ≤ u, denotes that ci ̸= cj . Unlike

2The extended covariance matrix is composed of two com-
ponents, one is an unsupervised covariance term and another is
a constraint matrix involving labeled information.

3Hamming distance is defined as the number of bits that are
different between two binary strings.

previews works that attempt to find L optimal hyper-
planes, the basic idea of S3H is to fix L random hy-
perplanes and to find an optimal feature-weight vec-
tor to relocate the objects such that similar objects
have similar codes.

3.1 Data Representation

Since L random hyperplanes are fixed, we can rep-
resent a object x ∈ X as its relative position to these
random hyperplanes, i.e.,

D = Λ ·V (4)

where the element Vml ∈ {+1,−1, 0} of V indi-
cates that the object x is above, below or just in the
l-th hyperplane with respect to the m-th feature, and
Λ = diag(|x1|, |x2|, . . . , |xM |) is a diagonal matrix
which, to some extent, reflects the distance from x
to these hyperplanes.

3.2 Formulation

Hashing maps the data set X to an L-dimensional
Hamming space for compact representations. If we
represent each object as Equation (4), the l-th hash
function is then defined as:

hl(x) = ~l(D) = sign(wTdl) (5)

where w ∈ RM is the feature weight to be deter-
mined and dl is the l-th column of the matrix D.

Intuitively, the ”contribution” of a specific feature
to different classes is different. Therefore, we hope
to incorporate this side information in S3H for better
hashing. Inspired by (Madani et al., 2009), we can
measure this contribution overXL as in Algorithm 1.
Clearly, if objects are represented as the occurrence
numbers of features, the output of Algorithm 1 is
just the conditional probability Pr(class|feature).
Finally, each object (x, c) ∈ XL can be represented
as an M × L matrix G:

G = diag(ν1,c, ν2,c, . . . , νM,c) ·D (6)

Note that, one pair (xi,xj) in Θa or Θr corresponds
to (Gi,Gj) while (Di,Dj) if we ignore features’
contribution to different classes.

Furthermore, we also hope to maximize the em-
pirical accuracy on the labeled data Θa and Θr and

95



Algorithm 1: Feature Contribution Calculation
for each (x, c) ∈ XL do

for each f ∈ x do
νf ← νf + xf ;
νf,c ← νf,c + xf ;

end
end
for each feature f and class c do

νf,c ←
νf,c
νf

;

end

maximize the entropy of hash functions. So, we de-
fine the following objective for ~(·)s:

J(w) =
1

Np

L∑
l=1

{ ∑
(xi,xj)∈Θa

~l(xi)~l(xj)

−
∑

(xi,xj)∈Θr

~l(xi)~l(xj)
}

+ λ1

L∑
l=1

H(~l)

(7)

where Np = |Θa| + |Θr| is the number of attrac-
tion and repulsion pairs and λ1 is a tradeoff between
two terms. Wang et al. have proven that hash func-
tions with maximum entropy must maximize the
variance of the hash values, and vice-versa (Wang
et al., 2010b). Thus, H(~(·)) can be estimated over
the labeled and unlabeled data, XL and XU .

Unfortunately, direct solution for above problem
is non-trivial since Equation (7) is not differentiable.
Thus, we relax the objective and add an additional
regularization term which could effectively avoid
overfitting. Finally, we obtain the total objective:

L(w) = 1
Np

L∑
l=1

{
∑

(Gi,Gj)∈Θa

ψ(wT gi,l)ψ(w
T gj,l)

−
∑

(Gi,Gj)∈Θr

ψ(wT gi,l)ψ(w
T gj,l)}

+
λ1
2N

L∑
l=1

{
u∑

i=1

ψ2(wT gi,l) +
N∑

i=u+1

ψ2(wT di,l)}

− λ2
2
∥w∥22

(8)

where gi,l and di,l denote the l-th column of Gi and
Di respectively, and ψ(t) is a piece-wise linear func-
tion defined as:

ψ(t) =

 Tg t > Tgt −Tg ≤ t ≤ Tg−Tg t < −Tg (9)
This relaxation has a good intuitive explanation.
That is, similar objects are desired to not only have

the similar fingerprints but also have sufficient large
projection magnitudes, while dissimilar objects are
desired to not only differ in their fingerprints but also
have large projection margin. However, we do not
hope that a small fraction of object-pairs with very
large projection magnitude or margin dominate the
complete model. Thus, a piece-wise linear function
ψ(·) is applied in S3H.

As a result, Equation (8) is a simply uncon-
strained optimization problem, which can be ef-
ficiently solved by a notable Quasi-Newton algo-
rithm, i.e., L-BFGS (Liu et al., 1989). For descrip-
tion simplicity, only attraction set Θa is considered
and the extension to repulsion set Θr is straightfor-
ward. Thus, the gradient of L(w) is as follows:

∂L(w)
∂w

=
1

Np

L∑
l=1

{ ∑
(Gi, Gj) ∈ Θa,
|wT gi,l| ≤ Tg

ψ(wT gj,l) · gi,l

+
∑

(Gi, Gj) ∈ Θa,
|wT gj,l| ≤ Tg

ψ(wT gi,l) · gj,l
}

(10)

+
λ1
N

L∑
l=1

{ u∑
i = 1,

|wT gi,l| ≤ Tg

ψ(wT gi,l) · gi,l

+

N∑
i = u + 1,

|wT di,l| ≤ Tg

ψ(wT di,l) · di,l
}
− λ2w

Note that ∂ψ(t)/∂t = 0 when |t| > Tg.

3.3 Fingerprint Generation
When we get the optimal weight w∗, we generate
fingerprints for given objects through Equation (5).
Then, it tunes to the problem how to efficiently ob-
tain the representation as in Figure 4 for a object.
After analysis, we find: 1) hyperplanes are randomly
generated and we only need to determine which
sides of these hyperplanes the given object lies on,
and 2) in real-world applications, objects such as
docs are always very sparse. Thus, we can avoid
heavy computational demands and efficiently gener-
ate fingerprints for objects.

In practice, given an object x, the procedure of
generating anL-bit fingerprint is as follows: it main-
tains an L-dimensional vector initialized to zero.
Each feature f ∈ x is firstly mapped to an L-bit
hash value by Jenkins Hashing Function4. Then,

4http://www.burtleburtle.net/bob/hash/doobs.html

96



Algorithm 2: Fast Fingerprint Generation
INPUT: x and w∗;
initialize α← 0,β ← 0, α,β ∈ RL;
for each f ∈ x do

randomly project f to hf ∈ {−1, +1}L;
α← α + xf · w∗f · hf ;

end
for l = 1 to L do

if αl > 0 then
βl ← 1;

end
end
RETURN β;

these L bits increment or decrement the L compo-
nents of the vector by the value xf × w∗f . After all
features processed, the signs of components deter-
mine the corresponding bits of the final fingerprint.
The complete algorithm is presented in Algorithm 2.

3.4 Algorithmic Analysis
This section briefly analyzes the relation between
S3H and some existing methods. For analysis sim-
plicity, we assume ψ(t) = t and ignore the regular-
ization terms. So, Equation (8) can be rewritten as
follows:

J(w)S3H =
1

2
wT [

L∑
l=1

Γl(Φ
+ −Φ−)ΓTl ]w (11)

where Φ+ij equals to 1 when (xi,xj) ∈ Θa otherwise
0, Φ−ij equals to 1 when (xi,xj) ∈ Θr otherwise
0, and Γl = [g1,l . . .gu,l,du+1,l . . .dN,l]. We de-
note

∑
l ΓlΦ

+ΓTl and
∑

l ΓlΦ
−ΓTl as S

+ and S−

respectively. Therefore, maximizing above function
is equivalent to maximizing the following:

J̃(w)S3H =
|wT S+w|
|wT S−w|

(12)

Clearly, Equation (12) is analogous to Linear Dis-
criminant Analysis (LDA) (Duda et al., 2000) ex-
cept for the difference: 1) measurement. S3H uses
similarity while LDA uses distance. As a result, the
objective function of S3H is just the reciprocal of
LDA’s. 2) embedding space. LDA seeks the best
separative direction in the original attribute space. In
contrast, S3H firstly maps data from RM to RM×L
through the following projection function

ϕ(x) = x · [diag(sign(r1)), . . . ,diag(sign(rL))] (13)

where rl ∈ RM , l = 1, . . . , L, are L random hyper-
planes. Then, in that space (RM×L), S3H seeks a
direction5 that can best separate the data.

From this point of view, it is obvious that the basic
SH is a special case of S3H when w is set to e =
[1, 1, . . . , 1]. That is, SH firstly maps the data via
ϕ(·) just as S3H. But then, SH directly separates the
data in that feature space at the direction e.

Analogously, we ignore the regularization terms
in SSH and rewrite the objective of SSH as:

J(W)SSH =
1

2
tr[WT X(Φ+ −Φ−)XT W] (14)

where W = [w1, . . . ,wL] ∈ RM×L are L hyper-
planes and X = [x1, . . . ,xN ]. Maximizing this ob-
jective is equivalent to maximizing the following:

J̃(W)SSH =
| tr[WT S′+W]|
| tr[WT S′−W]|

(15)

where S′+ = XΦ+XT and S′− = XΦ−XT . Equa-
tion (15) shows that SSH is analogous to Multiple
Discriminant Analysis (MDA) (Duda et al., 2000).
In fact, SSH uses top L best-separative hyperplanes
in the original attribute space found via PCA to hash
the data. Furthermore, we rewrite the projection
function ϕ(·) in S3H as:

ϕ(x) = x · [R1, . . . ,RL] (16)

where Rl = diag(sign(rl)). Each Rl is a mapping
from RM to RM and corresponds to one embedding
space. From this perspective, unlike SSH, S3H glob-
ally seeks a direction that can best separate the data
in L different embedding spaces simultaneously.

4 Experiments

We use two datasets 20 Newsgroups and Open Di-
rectory Project (ODP) in our experiments. Each doc-
ument is represented as a vector of occurrence num-
bers of the terms within it. The class information
of docs is considered as prior knowledge that two
docs within a same class should have more similar
fingerprints while two docs within different classes
should have dissimilar fingerprints. We will demon-
strate that our S3H can effectively incorporate this
prior knowledge to improve the DSS performance.

5The direction is determined by concatenating w L times.

97



24 32 40 48 56 64

0.1

0.2

0.3

0.4

0.5

M
ea

n 
A

ve
ra

ge
d 

Pr
ec

is
io

n 
(M

A
P)

Number of bits

 S3H
 S3Hf
 SSH
 SH
 KLSH

(a)

24 32 40 48 56 64

0.1

0.2

0.3

0.4

0.5

M
ea

n 
A

ve
ra

ge
d 

Pr
ec

is
io

n 
(M

A
P)

Number of bits

 S3H
 S3Hf
 SSH
 SH
 KLSH

(b)

Figure 1: Mean Averaged Precision (MAP) for different
number of bits for hash ranking on 20 Newsgroups. (a)
10K features. (b) 30K features.

We use Inverted List (IL) (Manning et al., 2002)
as the baseline. In fact, given a query doc, IL re-
turns all the docs that contain any term within it.
We also compare our method with three state-of-
the-art hashing methods, i.e., KLSH, SSH and SH.
In KLSH, we adopt the RBF kernel κ(xi,xj) =

exp(−∥xi−xj∥
2
2

δ2
), where the scaling factor δ2 takes

0.5 and the other two parameters p and t are set to
be 500 and 50 respectively. The parameter λ in SSH
is set to 1. For S3H, we simply set the parameters λ1
and λ2 in Equation (8) to 4 and 0.5 respectively. To
objectively reflect the performance of S3H, we eval-
uate our S3H with and without Feature Contribution
Calculation algorithm (FCC) (Algorithm 1). Specif-
ically, FCC-free S3H (denoted as S3Hf ) is just a
simplification when Gs in S3H are simply set to Ds.

For quantitative evaluation, as in literature (Wang
et al., 2010b; Mu et al., 2009), we calculate the pre-
cision under two scenarios: hash lookup and hash
ranking. For hash lookup, the proportion of good
neighbors (have the same class label as the query)
among the searched objects within a given Hamming
radius is calculated as precision. Similarly to (Wang
et al., 2010b; Weiss et al., 2009), for a query doc-
ument, if no neighbors within the given Hamming
radius can be found, it is considered as zero preci-
sion. Note that, the precision of IL is the propor-
tion of good neighbors among the whole searched
objects. For hash ranking, all the objects in X are
ranked in terms of their Hamming distance from the
query document, and the top K nearest neighbors
are returned as the result. Then, Mean Averaged Pre-
cision (MAP) (Manning et al., 2002) is calculated.
We also calculate the averaged intra- and inter- class
Hamming distance for various hashing methods. In-

24 32 40 48 56 64
0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

Pr
ec

is
io

n 
w

ith
in

 H
am

m
in

g 
ra

di
us

 3

Number of bits

 S3H
 S3Hf
 SSH
 SH
 KLSH
 IL

(a)

24 32 40 48 56 64
0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

Pr
ec

is
io

n 
w

ith
in

 H
am

m
in

g 
ra

di
us

 3

Number of bits

 S3H
 S3Hf
 SSH
 SH
 KLSH
 IL

(b)

Figure 2: Precision within Hamming radius 3 for hash
lookup on 20 Newsgroups. (a) 10K features. (b) 30K
features.

tuitively, a good hashing method should have small
intra-class distance while large inter-class distance.

We test all the methods on a PC with a 2.66 GHz
processor and 12GB RAM. All experiments repeate
10 times and the averaged results are reported.

4.1 20 Newsgroups

20 Newsgroups6 contains 20K messages, about 1K
messages from each of 20 different newsgroups.
The entire vocabulary includes 62,061 words. To
evaluate the performance for different feature di-
mensions, we use Chi-squared feature selection al-
gorithm (Forman, 2003) to select 10K and 30K fea-
tures. The averaged message length is 54.1 for 10K
features and 116.2 for 30K features. We randomly
select 4K massages as the test set and the remain
16K as the training set. To train SSH and S3H,
from the training set, we randomly generate 40K
message-pairs as Θa and 80K message-pairs as Θr.

For hash ranking, Figure 1 shows MAP for vari-
ous methods using different number of bits. It shows
that performance of SSH decreases with the grow-
ing of hash bits. This is mainly because the variance
of the directions obtained by PCA decreases with
the decrease of their ranks. Thus, lower bits have
larger empirical errors. For S3H, FCC (Algorithm 1)
can significantly improve the MAP just as discussed
in Section 3.2. Moreover, the MAP of FCC-free
S3H (S3Hf ) is affected by feature dimensions while
FCC-based (S3H) is relatively stable. This implies
FCC can also improve the satiability of S3H. As we
see, S3Hf ignores the contribution of features to dif-
ferent classes. However, besides the local descrip-
tion of data locality in the form of object-pairs, such

6http://www.cs.cmu.edu/afs/cs/project/theo-3/www/

98



 S3H
 S3Hf
 SSH
 SH
 KLSH
 IL

24 32 40 48 56 64
10-1

100

101

102

103

104

N
um

be
r o

f s
ea

rc
he

d 
da

ta

Number of bits

(a)

 S3H
 S3Hf
 SSH
 SH
 KLSH
 IL

24 32 40 48 56 64

10-1

100

101

102

103

104

N
um

be
r o

f s
ea

rc
he

d 
da

ta

Number of bits

(b)

Figure 3: Averaged searched sample numbers using 4K
query messages for hash lookup. (a) 10K features. (b)
30K features.

(global) information also provides a proper guidance
for hashing. So, for S3Hf , the reason why its re-
sults with 30K features are worse than the results
with 10K features is probably because S3Hf learns
to hash only according to the local description of
data locality and many not too relevant features lead
to relatively poor description. In contrast, S3H can
utilize global information to better understand the
similarity among objects. In short, S3H obtains the
best MAP for all bits and feature dimensions.

For hash lookup, Figure 2 presents the precision
within Hamming radius 3 for different number of
bits. It shows that IL even outperforms SH. This
is because few objects can be hashed by SH into one
hash bucket. Thus, for many queries, SH fails to
return any neighbor even in a large Hamming radius
of 3. Clearly, S3H outperforms all the other methods
for different number of hash bits and features.

The number of messages searched by different
methods are reported in Figure 3. We find that the
number of searched data of S3H (with/without FCC)
decreases much more slowly than KLSH, SH and
SSH with the growing of the number of hash bits. As
discussed in Section 3.4, this mainly benefits from
the design of S3H that S3H (globally) seeks a di-
rection that can best separate the data in L embed-
ding spaces simultaneously. We also find IL returns
a large number of neighbors of each query message
which leads to its poor efficiency.

The averaged intra- and inter- class Hamming dis-
tance of different methods are reported in Table 1.
As it shows, S3H has relatively larger margin (∆)
between intra- and inter-class Hamming distance.
This indicates that S3H is more effective to make
similar points have similar fingerprints while keep

intra-class inter-class ∆
S3H 13.1264 15.6342 2.5078

S3Hf 12.5754 13.3479 0.7725
SSH 6.4134 6.5262 0.1128
SH 15.3908 15.6339 0.2431

KLSH 10.2876 10.8713 0.5841

Table 1: Averaged intra- and inter- class Hamming dis-
tance of 20 Newsgroups for 32-bit fingerprint. ∆ is the
difference between the averaged inter- and intra- class
Hamming distance. Large ∆ implies good hashing.

10 20 30 40

100

101

102

103

Ti
m

e 
(s

ec
.)

Feature dimension (K)

 S3H
 SSH
 SH
 KLSH
 IL

(a)

10 20 30 40
101

102

103

104

Sp
ac

e 
(M

B
) 

Feature dimension (K)

 S3H
 SSH
 SH
 KLSH
 IL

(b)

Figure 4: Computational complexity of training for dif-
ferent feature dimensions for 32-bit fingerprint. (a) Train-
ing time (sec). (b) Training space cost (MB).

the dissimilar points away enough from each other.
Figure 4 shows the (training) computational com-

plexity of different methods. We find that the time
and space cost of SSH grows much faster than SH,
KLSH and S3H with the growing of feature dimen-
sion. This is mainly because SSH requires SVD to
find the optimal hashing functions which is compu-
tational expensive. Instead, S3H seeks the optimal
feature weights via L-BFGS, which is still efficient
even for very high-dimensional data.

4.2 Open Directory Project (ODP)

Open Directory Project (ODP)7 is a multilingual
open content directory of web links (docs) organized
by a hierarchical ontology scheme. In our exper-
iment, only English docs8 at level 3 of the cate-
gory tree are utilized to evaluate the performance.
In short, the dataset contains 2,483,388 docs within
6,008 classes. There are totally 862,050 distinct
words and each doc contains 14.13 terms on aver-
age. Since docs are too short, we do not conduct

7http://rdf.dmoz.org/
8The title together with the corresponding short description

of a page are considered as a document in our experiments.

99



1 10 100 1k 10k 100k
0.00

0.01

0.02

0.03

0.04

Pe
rc
en

ta
ge

Class size

(a)

0 20 40 60 80 100 120
0.00

0.02

0.04

0.06

0.08

0.10

Pe
rc
en

ta
ge

Document length

(b)

Figure 5: Overview of ODP data set. (a) Class distribu-
tion at level 3. (b) Distribution of document length.

intra-class inter-class ∆
S3H 14.0029 15.9508 1.9479

S3Hf 14.3801 15.5260 1.1459
SH 14.7725 15.6432 0.8707

KLSH 9.3382 10.5700 1.2328

Table 2: Averaged intra- and inter- class Hamming dis-
tance of ODP for 32-bit fingerprint (860K features). ∆
is the difference between averaged intra- and inter- class
Hamming distance.

feature selection9. An overview of ODP is shown in
Figure 5. We randomly sample 10% docs as the test
set and the remain as the training set. Furthermore,
from training set, we randomly generate 800K doc-
pairs as Θa, and 1 million doc-pairs as Θr. Note
that, since there are totally over 800K features, it
is extremely inefficient to train SSH. Therefore, we
only compare our S3H with IL, KLSH and SH.

The search performance is given in Figure 6. Fig-
ure 6(a) shows the MAP for various methods using
different number of bits. It shows KLSH outper-
forms SH, which mainly contributes to the kernel
trick. S3H and S3Hf have higher MAP than KLSH
and SH. Clearly, FCC algorithm can improve the
MAP of S3H for all bits. Figure 6(b) presents the
precision within Hamming radius 2 for hash lookup.
We find that IL outperforms SH since SH fails for
many queries. It also shows that S3H (with FCC)
can obtain the best precision for all bits.

Table 2 reports the averaged intra- and inter-class
Hamming distance for various methods. It shows
that S3H has the largest margin (∆). This demon-

9We have tested feature selection. However, if we select
40K features via Chi-squared feature selection method, docu-
ments are represented by 3.15 terms on average. About 44.9%
documents are represented by no more than 2 terms.

24 32 40 48 56 64

0.15

0.20

0.25

0.30

0.35

M
ea

n 
A

ve
ra

ge
d 

Pr
ec

is
io

n 
(M

A
P)

Number of bits

 S3H
 S3Hf
 SH
 KLSH

(a)

24 32 40 48 56 64

0.03

0.06

0.09

0.12

0.15

0.18

Pr
ec

is
io

n 
w

ith
in

 H
am

m
in

g 
ra

di
us

 2

Number of bits

 S3H
 S3Hf
 SH
 KLSH
 IL

(b)

Figure 6: Retrieval performance of different methods on
ODP. (a) Mean Averaged Precision (MAP) for different
number of bits for hash ranking. (b) Precision within
Hamming radius 2 for hash lookup.

strates S3H can measure the similarity among the
data better than KLSH and SH.

We should emphasize that KLSH needs 0.3ms
to return the results for a query document for hash
lookup, and S3H needs <0.1ms. In contrast, IL re-
quires about 75ms to finish searching. This is mainly
because IL always returns a large number of ob-
jects (dozens or hundreds times more than S3H and
KLSH) and requires much time for post-processing.

All the experiments show S3H is more effective,
efficient and stable than the baseline method and the
state-of-the-art hashing methods.

5 Conclusions

We have proposed a novel supervised hashing
method named Semi-Supervised Simhash (S3H) for
high-dimensional data similarity search. S3H learns
the optimal feature weights from prior knowledge
to relocate the data such that similar objects have
similar fingerprints. This is implemented by max-
imizing the empirical accuracy on labeled data to-
gether with the entropy of hash functions. The
proposed method leads to a simple Quasi-Newton
based solution which is efficient even for very high-
dimensional data. Experiments performed on two
large datasets have shown that S3H has better search
performance than several state-of-the-art methods.

6 Acknowledgements

We thank Fangtao Li for his insightful suggestions.
We would also like to thank the anonymous review-
ers for their helpful comments. This work is sup-
ported by the National Natural Science Foundation
of China under Grant No. 60873174.

100



References
Christopher J.C. Burges. 1998. A tutorial on support

vector machines for pattern recognition. Data Mining
and Knowledge Discovery, 2(2):121-167.

Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of the 34th annual ACM symposium on Theory of
computing, pages 380-388.

Gianni Costa, Giuseppe Manco and Riccardo Ortale.
2010. An incremental clustering scheme for data de-
duplication. Data Mining and Knowledge Discovery,
20(1):152-187.

Jeffrey Dean and Monika R. Henzinge. 1999. Finding
Related Pages in the World Wide Web. Computer
Networks, 31:1467-1479.

Richard O. Duda, Peter E. Hart and David G. Stork.
2000. Pattern classification, 2nd edition. Wiley-
Interscience.

George Forman 2003. An extensive empirical study of
feature selection metrics for text classification. The
Journal of Machine Learning Research, 3:1289-1305.

Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the 30th annual
ACM symposium on Theory of computing, pages
604-613.

Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag, New York.

Yan Ke, Rahul Sukthankar and Larry Huston. 2004.
Efficient near-duplicate detection and sub-image
retrieval. In Proceedings of the ACM International
Conference on Multimedia.

Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130-2137.

Brian Kulis, Prateek Jain and Kristen Grauman. 2009.
Fast similarity search for learned metrics. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, pages 2143-2157.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.

Mathematical programming, 45(1): 503-528.

Omid Madani, Michael Connor and Wiley Greiner.
2009. Learning when concepts abound. The Journal
of Machine Learning Research, 10:2571-2613.

Gurmeet Singh Manku, Arvind Jain and Anish Das
Sarma. 2007. Detecting near-duplicates for web
crawling. In Proceedings of the 16th international
conference on World Wide Web, pages 141-150.

Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schütze. 2002. An introduction to information
retrieval. Spring.

Yadong Mu, Jialie Shen and Shuicheng Yan. 2010.
Weakly-Supervised Hashing in Kernel Space. In Pro-
ceedings of International Conference on Computer
Vision and Pattern Recognition, pages 3344-3351.

Ruslan Salakhutdinov and Geoffrey Hintona. 2007.
Semantic hashing. In SIGIR workshop on Information
Retrieval and applications of Graphical Models.

Bernhard Schölkopf, Alexander Smola and Klaus-Robert
Müller. 1997. Kernel principal component analysis.
Advances in Kernel Methods - Support Vector Learn-
ing, pages 583-588. MIT.

Lloyd N. Trefethen and David Bau. 1997. Numerical
linear algebra. Society for Industrial Mathematics.

Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2008.
Towards a unified approach to document similarity
search using manifold-ranking of blocks. Information
Processing & Management, 44(3):1032-1048.

Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010a.
Semi-Supervised Hashing for Scalable Image Re-
trieval. In Proceedings of International Conference
on Computer Vision and Pattern Recognition, pages
3424-3431.

Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010b.
Sequential Projection Learning for Hashing with
Compact Codes. In Proceedings of International
Conference on Machine Learning.

Yair Weiss, Antonio Torralba and Rob Fergus. 2009.
Spectral hashing. In Proceedings of Advances in Neu-
ral Information Processing Systems.

101


