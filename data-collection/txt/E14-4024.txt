



















































Bayesian Word Alignment for Massively Parallel Texts


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 123–127,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Bayesian Word Alignment for Massively Parallel Texts

Robert Östling
Department of Linguistics

Stockholm University
robert@ling.su.se

Abstract
There has been a great amount of work
done in the field of bitext alignment, but
the problem of aligning words in mas-
sively parallel texts with hundreds or thou-
sands of languages is largely unexplored.
While the basic task is similar, there
are also important differences in purpose,
method and evaluation between the prob-
lems. In this work, I present a non-
parametric Bayesian model that can be
used for simultaneous word alignment in
massively parallel corpora. This method
is evaluated on a corpus containing 1144
translations of the New Testament.

1 Introduction

Bitext word alignment is the problem of finding
links between words given pairs of translated sen-
tences (Tiedemann, 2011). Initially, this was mo-
tivated by Statistical Machine Translation (SMT)
applications (Brown et al., 1993), but word-
aligned texts have also been used to transfer lin-
guistic annotation between languages (Yarowsky
et al., 2001; Täckström, 2013), for Word Sense
Disambiguation (WSD) (Diab and Resnik, 2002)
and lexicon extraction (Wu and Xia, 1994).

Massively parallel texts, in the sense used by
Cysouw and Wälchli (2007), are essentially the
same as bitexts, only with hundreds or thousands
of languages rather than just two. Parallel corpora
used in SMT, for instance the Europarl Corpus
(Koehn, 2005), tend to contain few (up to tens of)
languages, but many (up to billions of) words in
each language. Massively parallel corpora, on the
other hand, contain many (hundreds of) languages,
but usually fewer (less than a million) words in
each language.

Additionally, aligned massively parallel corpora
have different applications than traditional paral-
lel corpora with pairwise alignments. Whereas the

latter tend to be used for the various NLP tasks
mentioned above, massively parallel corpora have
mostly been used for investigations in linguistic
typology (Cysouw and Wälchli, 2007).

There has been surprisingly few studies on mul-
tilingual word alignment. Mayer and Cysouw
(2012) treat alignment as a clustering problem,
where the words in each sentence are clustered ac-
cording to some measure of co-occurrence. They
provide no evaluation, but alignment methods
based on co-occurrence statistics have been found
to have lower accuracy than even very simple gen-
erative models (Och and Ney, 2003), so this might
not be a promising direction as far as accuracy is
concerned.

A related line of research is due to Lardilleux
et al. (2011), who learn sets of multilingual trans-
lation equivalent phrases. Although later work
(Lardilleux et al., 2013) uses phrase pairs ex-
tracted with this method for (bitext) word align-
ment, their method solves a somewhat different
problem from what is considered here.

Some authors have studied how multilingual
parallel corpora can be used to improve bitext
alignment. Filali and Bilmes (2005) use (bitext)
alignments to addditional languages as features in
bitext alignment, while Kumar et al. (2007) in-
terpolate alignments through multiple bridge lan-
guages to produce a bitext alignment for another
language pair. Since the goal of this research is
not multilingual alignment, it will not be consid-
ered further here.

2 Multilingual Alignment

In bitext alignment, the goal is to find links be-
tween individual word tokens in parallel sentence
pairs. The IBM models (Brown et al., 1993) for-
malize this in a directional fashion where each
word j in a source language is linked to word i
in the target language through alignment variables
i = aj , thus specifying a 1-to-n mapping from

123



source language words to target language words.
An intuitively appealing way to formalize the

multilingual alignment problem is through a com-
mon representation (or interlingua) to which each
individual language is aligned. If the common rep-
resentation is isomorphic to one of the languages
in the corpus, this is equivalent to using that lan-
guage as a bridge. However, since all languages
(and all translations) have their own idiosyncrasies
that make linking to other translations difficult, it
seems better to learn a common representation that
corresponds to information in a sentence that is
present in as many of the translations as possible.

3 Method

Recently, it has been shown that Bayesian meth-
ods that use priors to bias towards linguistically
more plausible solutions can improve bitext word
alignment (Mermer and Saraçlar, 2011; Riley and
Gildea, 2012; Gal and Blunsom, 2013). Given
these promising results and the fact that massively
parallel texts tend to be rather short, which makes
the role of realistic priors more important, I have
decided to use a Bayesian alignment model for this
work.

3.1 Model
The model used in this work uses a common rep-
resentation of concepts generated by a Chinese
Restaurant Process (CRP), which is aligned to
each of the languages in a corpus using the model
of Mermer and Saraçlar (2011).

Table 1 introduces the variables (observed and
latent) as well as the hyperparameters of the
model. Basically, the model consists of a common
representation c (where token i of sentence s is de-
noted csi), which is aligned to one or more words
wlsj (from language l, sentence s, token j) through
a set of alignment variables alsj which contain the
index within cs that wlsj is linked to.

The probability of an assignment c is:

CRP(c;α) =
Γ (1 + α)
Γ (n+ α)

·α|Ec|−1 ·
∏
e∈Ec

(ne − 1)!

where ne is the number of occurrences of concept
type e in the assignment c, and n =

∑
e ne is the

(fixed) total number of tokens in the common rep-
resentation.

For the translation probabilities, I follow
Mermer and Saraçlar (2011) in assuming that
p(fl|e) ∼ Dir(tl; θl), and that the priors θl are

symmetric (i.e. all values in these vectors are
equal, θlef = β). By specifying a low value for
β (a sparse prior), we can encode our prior knowl-
edge that translation probability functions p(fl|e)
tend to have a low entropy, or in other words,
that each concept is typically only translated into
a very small number of words in each language.

The joint probability of the common represen-
tation and the alignments is given by:

p(c, a, w, t;α, θ) =
p(c;α) · p(w|c, a, t) · p(a|c) · p(t; θ) (1)

where p(c;α) = CRP(c;α) and the remaining
factors are the same as in Mermer and Saraçlar
(2011) with the common representation being the
“target language”, except that there is a product
across all languages l. Note that since word order
is not modeled, p(a|c) is constant.
3.2 Learning
The model is trained using a collapsed Gibbs sam-
pler. Due to space limitations, the full derivation
is omitted, but the sampling distribution turns out
to be as follows for the common representation:

p(csi = e′) ∝ 1
n− 1 + α ·

{
α if ne′ = 1
ne′ − 1 if ne′ > 1

·
∏
l

∏
f∈Alsi

∏mlsif
k=1

(
nle′f + θle′f − k

)
∏∑

f mlsif
k=1

(∑
f∈Fl nle′f + θle′f − k

)
(2)

where Alsi is the set of word types f in language l
which are aligned to csi, and mlsif is the number
of times each such f is aligned to csi. In order to
speed up calculations, the product in Equation 2
can be approximated by letting l run over a small
random subset of languages. The experiments car-
ried out in this work only use this approximation
when the full corpus of 1144 translations is used,
then a subset of 24 languages is randomly selected
when each csi is sampled. An empirical evalua-
tion of the effects of this approximation is left for
future work.

The alignment sampling distribution is:

p(alsj = i) ∝ nle
′f ′ + θle′f ′ − 1∑

f

(
nle′f + θle′f

)− 1 (3)
where e′ = csalsj is the concept type aligned to
word type f ′ = wlsj .

Rather than sampling directly from the distribu-
tions above, one can sample from p̂(csi = e′) ∝

124



Table 1: Variables used in the model.

Observed variables
Fl the set of word types in language l
wlsj ∈ Fl word j of sentence s in language l
Is ∈ N length of sentence s in the common representation
Jls ∈ N length of sentence s in language l

Latent variables
Ec the set of concepts in the assignment c
csi ∈ Ec concept i of sentence s in the common representation
alsj ∈ {1..Is} alignment of wlsj to csi; i = alsj
tlef ∈ R translation probability p(fl|e), where fl ∈ Fl and e ∈ Ec

Hyperparameters
α CRP hyperparameter, fixed to 1000 in the experiments
β symmetric Dirichlet prior for translation distributions θl,

fixed to 0.001 in the experiments

p(csi = e′)λ and p̂(alsj = i) ∝ p(alsj = i)λ.
The temperature parameter λ can be varied dur-
ing training to change the amount of randomness
while sampling.

3.3 Initialization

In order to obtain a reasonable initial state for the
Gibbs sampling, one can simply initialize the com-
mon representation to be identical to one of the
languages in the corpus. For this language one
then (trivially) has a perfect alignment, while the
remaining languages are initialized randomly and
their alignments are learned. Random initializa-
tion of the common representation is possible, but
turns out to perform poorly.

4 Experiments

The most basic question about the present model
is whether sampling the common representation is
helpful, compared to simply choosing a language
and aligning all other languages to that one.

In order to test this, I initialize the model as de-
scribed in section 3.3 and sample alignments (but
not the common representation) for 200 iterations
with λ linearly increasing from 0 to 2, followed by
two iterations with λ → ∞. This gives a strong
baseline, from which one can start learning the
joint model.

4.1 Data

I use a corpus containing verse-aligned transla-
tions of the New Testament into a great number of
languages. After some exclusions due to e.g. non-
standard formatting or improperly segmented text,

the version used in this work contains 1144 trans-
lations in 986 different languages. The mean num-
ber of tokens among the translations is 236 000,
and the mean number of types is 9 500.

4.2 Evaluation Measures

Previous authors have tended to avoid multilingual
evaluation altogether. Mayer and Cysouw (2012)
do not evaluate their method, while Lardilleux et
al. (2011) only use bilingual evaluation.

Cysouw et al. (2007) use the fact that some
translations of the Bible have been annotated with
Strong’s Numbers, which map most word tokens
to the lemma of its translation equivalent in the
original language, to perform bilingual evaluation
of Bible corpus alignments.

Strong’s Numbers can be used in a different
way to evaluate the type of multilingual alignment
produced by the method in this work. Both the
Strong’s Numbers and the common representation
can be interpreted as clusterings of the word to-
kens in each language. Ideally one would want
these two clusterings to be identical, as they would
be if the original language had been perfectly con-
structed. Standard clustering evaluation measures
can be used for this task, and in this work I use
normalized mutual information (also reinvented as
V-measure by Rosenberg and Hirschberg (2007)).
The evaluation is limited to words which are as-
signed exactly one Strong’s Number, in an attempt
to avoid some of the problems with scope dis-
cussed by Cysouw et al. (2007). Note that even a
perfect alignment from one language to itself does
not achieve the maximum score using this mea-

125



0 200 400 600 800 1000
Iterations

0.68

0.70

0.72

0.74

0.76

0.78

0.80
No

rm
al

iz
ed

 M
ut

ua
l I

nf
or

m
at

io
n

deu

eng
eng

fra

ind

ind

nld

por
rus

Figure 1: Alignment quality of Mandarin-
initialized model.

sure, only a successful reconstruction of the origi-
nal text (minus inflections) would.

In the Bible corpus used here, nine translations
in seven languages contain Strong’s Numbers an-
notations: English and Indonesian (two transla-
tions each), as well as German, French, Dutch,
Portuguese and Russian (one translation each).

4.3 Results

Figure 1 shows alignment quality during training
in a model initialized using a translation in Man-
darin, which is not related to any of the languages
in the evaluation sample and was chosen to avoid
initialization bias. After an initial drop when noise
is introduced during the Gibbs sampling process,
alignment quality quickly increases as the com-
mon representation moves towards the versions in
the evaluation sample. The final two iterations
(with λ→∞) remove the sampling noise and the
model rapidly converges to a local maximum, re-
sulting in a sharp increase in alignment quality at
the end. Further iterations only result in minor im-
provements.

Table 2 contains the baseline and joint model re-
sults for models initialized with either English or
Mandarin versions. The joint model outperforms
the baseline in all cases except when the initial-
ization language is the same as the evaluation lan-
guage (the two English translations in the left col-
umn), which is expected since it is easy to align a
text to itself or to a very similar version.

The two models described so far only use the
nine-translation evaluation sample to learn the
common representation, since using additional
languages would unfairly penalize the joint learn-

English Mandarin
A A+J A A+J

deu 0.817 0.824 0.708 0.788
eng 0.854 0.851 0.714 0.800
eng2 0.834 0.833 0.708 0.790
fra 0.807 0.816 0.712 0.783
ind 0.774 0.785 0.710 0.770
ind2 0.791 0.803 0.721 0.786
nld 0.839 0.850 0.724 0.809
por 0.807 0.813 0.709 0.782
rus 0.792 0.800 0.699 0.772

Table 2: Normalized mutual information with re-
spect to Strong’s Numbers, using alignment only
(A) or joint alignment + common representation
learning (A+J), for models initialized using En-
glish or Mandarin.

ing model. I have also tested the model on the
full corpus of 1144 translations with an English-
initialized model and the same training setup as
above (initialized from English). In this case,
alignment quality decreased somewhat for the lan-
guages most similar to English, which is to be ex-
pected since the majority of languages in the cor-
pus are unrelated to English and pull the common
representation away from the European languages
in the evaluation sample. Although it is not possi-
ble to directly evaluate alignment quality outside
the evaluation sample with Strong’s Numbers, the
log-probability of the entire data under the model
(Equation 1) increases as expected, by about 5%.

5 Conclusions and Future Work

As the number of translations in a parallel cor-
pus increases, the problem of aligning them be-
comes a rather different one from aligning trans-
lation pairs. I have presented a Bayesian method
that jointly learns a common structure along with
alignments to each language in the corpus. In
an empirical evaluation, the joint method outper-
forms the baseline where the common structure is
one of the languages.

Currently the underlying alignment model is
quite simplistic, and preliminary results indicate
that including the HMM word order model of Vo-
gel et al. (1996) further improves alignments.

Acknowledgments

Thanks to Jörg Tiedemann, Mats Wirén and the
anonymous reviewers for their comments.

126



References
Peter F. Brown, Vincent J. Della Pietra, Stephen

A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311, June.

Michael Cysouw and Bernhard Wälchli. 2007. Paral-
lel texts: Using translational equivalents in linguistic
typology. STUF - Language Typology and Univer-
sals, 60(2):95–99.

Michael Cysouw, Chris Biemann, and Matthias Ongy-
erth. 2007. Using Strong’s Numbers in the Bible to
test an automatic alignment of parallel texts. STUF -
Language Typology and Universals, 60(2):158–171.

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 255–262, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Karim Filali and Jeff Bilmes. 2005. Leveraging multi-
ple languages to improve statistical MT word align-
ments. In IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 92–97, San
Juan, November. IEEE.

Yarin Gal and Phil Blunsom. 2013. A systematic
bayesian treatment of the ibm alignment models. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.

Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 42–50,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Adrien Lardilleux, Yves Lepage, and Franois Yvon.
2011. The contribution of low frequencies to multi-
lingual sub-sentential alignment: a differential asso-
ciative approach. International Journal of Advanced
Intelligence, 3(2):189–217.

Adrien Lardilleux, Francois Yvon, and Yves Lepage.
2013. Hierarchical sub-sentential alignment with
Anymalign. In Proceedings of the 16th EAMT Con-
ference, pages 279–286, Trento, Italy, 28-30 May
2012.

Thomas Mayer and Michael Cysouw. 2012. Lan-
guage comparison through sparse multilingual word

alignment. In Proceedings of the EACL 2012 Joint
Workshop of LINGVIS & UNCLH, EACL 2012,
pages 54–62, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Coşkun Mermer and Murat Saraçlar. 2011. Bayesian
word alignment for statistical machine translation.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ’11, pages 182–187, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.

Darcey Riley and Daniel Gildea. 2012. Improving the
IBM alignment models using variational Bayes. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers
- Volume 2, ACL ’12, pages 306–310, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420, Prague, Czech Republic, June. Association for
Computational Linguistics.

Oscar Täckström. 2013. Predicting Linguistic Struc-
ture with Incomplete and Cross-Lingual Supervi-
sion. Ph.D. thesis, Uppsala University, Department
of Linguistics and Philology.

Jörg Tiedemann. 2011. Bitext Alignment. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In Proceedings of the 16th Conference
on Computational Linguistics - Volume 2, COLING
’96, pages 836–841, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Dekai Wu and Xuanyin Xia. 1994. Learning an
English-Chinese lexicon from a parallel corpus. In
Proceedings of the First Conference of the Associa-
tion for Machine Translation in the Americas, pages
206–213.

David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of the First International Con-
ference on Human Language Technology Research,
HLT ’01, pages 1–8, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

127


