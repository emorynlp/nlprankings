










































A Dependency Based Statistical Translation Model


Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 79–87,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c©2011 Association for Computational Linguistics

A Dependency Based Statistical Translation Model 

 

Giuseppe Attardi 

 

Università di Pisa 

Dipartimento di Informatica 

attardi@di.unipi.it 

Atanas Chanev 

 

Università di Pisa 

Dipartimento di Informatica 

chanev@di.unipi.it 

Antonio Valerio Miceli Barone 

 

Università di Pisa 

Dipartimento di Informatica 

miceli@di.unipi.it 

 

 

Abstract 

We present a translation model based on 

dependency trees. The model adopts a tree-

to-string approach and extends Phrase-

Based translation (PBT) by using the de-

pendency tree of the source sentence for 

selecting translation options and for reor-

dering them. Decoding is done by translat-

ing each node in the tree and combining its 

translations with those of its head in alter-

native orders with respect to its siblings. 

Reordering of the siblings exploits a heu-

ristic based on the syntactic information 

from the parse tree which is learned from 

the corpus. The decoder uses the same 

phrase tables produced by a PBT system 

for looking up translations of single words 

or of partial sub-trees. A mathematical 

model is presented and experimental re-

sults are discussed.  

1 Introduction 

Several efforts are being made to incorporate syn-

tactic analysis into phrase-base statistical transla-

tion (PBT) (Och 2002; Koehn et. al. 2003), which 

represents the state of the art in terms of robust-

ness in modeling local word reordering and effi-

ciency in decoding. Syntactic analysis is meant to 

improve some of the pitfalls of PBT: 

 Translation options selection: candidate phrases 
for translation are selected as consecutive n-

grams. This may miss to consider certain syn-

tactic phrases if their component words are far 

apart. 

 Phrase reordering: especially for languages 
with different word order, e.g. subject-verb-

object (SVO) and subject-object-verb (SVO) 

languages, long distance reordering is a prob-

lem. This has been addressed with a distance 

based distortion model (Och 2002; Koehn et al. 

2003), lexicalized phrase reordering (Tillmann, 

2004; Koehn, et.al., 2005; Al-Onaizan and Pa-

pineni, 2006), by hierarchical phrase reordering 

model (Galley and Manning, 2008) or by reor-

dering the nodes in a dependency tree (Xu et 

al., 2009) 

 Movement of translations of fertile words: a 
word with fertility higher than one can be trans-

lated into several words that do not occur con-

secutively. For example, the Italian sentence 

“Lui partirà domani” translates into German as 

“Er wird morgen abreisen”. The Italian word 

“partirà” (meaning “will leave”) translates into 

“wird gehen” in German, but the infinite “ab-

reisen” goes to the end of the sentence with a 

movement that might be quite long. 

Reordering of phrases is necessary because of dif-

ferent word order typologies of languages: consti-

tuent word order like SOV for Hindi vs. SVO for 

English; order of modifiers like noun–adjective for 

French, Italian vs. adjective-noun in English. Xu et 

al. (2009) tackle this issue by introducing a reor-

dering approach based on manual rules that are 

applied to the parse tree produced by a dependen-

cy parser. 

However the splitting phenomenon mentioned 

above requires more elaborate solutions than sim-

ple reordering grammatical rules. 

Several schemes have been proposed for im-

proving PBMT systems based on dependency 

trees.  Our approach extends basic PBT as de-

79



scribed in (Koehn et. al., 2003) with the following 

differences: 

 we perform tree-to-string translation. The de-
pendency tree of the source language sentence 

allows identifying syntactically meaningful 

phrases as translation options, instead of n-

grams. However these phrases are then still 

looked up in a Phrase Translation Table (PT) 

quite similarly to PBT. Thus we avoid the 

sparseness problem that other methods based 

on treelets suffer (Quirk et al., 2005). 

 reordering of phrases is carried out traversing 
the dependency tree and selecting as options 

phrases that are children of each head. Hence a 

far away but logically connected portion of a 

phrase can be included in the reordering. 

 phrase combination is performed by combining 
the translations of a node with those of its head. 

Hence only phrases that have a syntactic rela-

tion are connected. The Language Model (LM) 

is still consulted to ensure that the combination 

is proper, and the overall score of each transla-

tion is carried along.  

 when all the links in the parse tree have been 
reduced, the root node contains candidate trans-

lations for the whole sentences 

 alternative visit orderings of the tree may pro-
duce different translations so the final transla-

tion is the one with the highest score. 

Some of the benefits of our approach include: 

1) reordering is based on syntactic phrases rather 
than arbitrary chunks 

2) computing the future cost estimation can be 
avoided, since the risk of choosing an easier n-

gram is mitigated by the fact that phrases are 

chosen according to the dependency tree 

3) since we are translating from tree to string, we 
can directly exploit the standard phrase tables 

produced by PBT tools such as giza++ (Och 

and Ney, 2000) and Moses (Koehn, 2007) 

4) integration with the parser: decoding can be 
performed incrementally while a dependency 

Shift/Reduce parser builds the parse tree (At-

tardi, 2006). 

2 The  Dependency Based Decoder 

We describe in more detail the approach by pre-

senting a simple example. 

The translation of an input sentence is generated 

by reducing the dependency tree one link at a time, 

i.e. merging one node with its parent and combin-

ing their translations, until a single node remains. 

Links must be chosen in an order that preserves 

the connectivity of the dependency tree. Since 

there is a one-to-one correspondence between 

links and nodes (i.e. the link between a node and 

its head), we can use any ordering that corres-

ponds to a topological ordering of the nodes of the 

tree. 

A sentence is a sequence of words (w1, … , wn), 

so we can use their index to identify words and 

hence each ordering is a permutation of those in-

dexes. 

Consider for example the dependency tree for 

the Italian sentence: Il ragazzo alto (“The tall 

boy”). 

 
There are only two possible topological orderings 

for this tree: 1-3-2 and 3-1-2.  

In principle the decoding process should ex-

plore all possible topological orderings for gene-

rating translations, but their number is too big, 

being proportional to the factorial of the number of 

words, so we will introduce later a criterion for 

selecting a subset of these, which conform best 

with the rules of the languages. 

Given a permutation we obtain a translation by 

merging in that order each node with its parent. 

The initialization step of the decoder creates 

nodes corresponding to the parse tree and collects 

translations for each individual word from the PT. 

 

ragazzo 

boy 

 

alto 

tall 

high 

Il 

The 

Il   ragazzo   alto 

80



Case 1: Permutation 1-3-2 

The first merge step is applied to the nodes for w1 

and its head w2, performing the concatenation of 

the translations of nodes il (the) and ragazzo (boy), 

both in normal and reverse order. Hence expansion 

of this hypothesis reduces the tree to the follow-

ing, where we show also the partial translations 

associated to each node. Each translation has asso-

ciated weights (i.e. the LM weight, the translation 

model weight, etc.) and a cumulative score. The 

score is the dot product of the weights for the sen-

tence and the vector of tuning parameters for the 

model. The score is used to rank the sentences and 

also to limit how many of them are kept according 

to the beam size parameter of the algorithm. 

 

The second step merges the node for word w3 (“al-

to”) with that of its head w2 (“ragazzo”) producing 

a single node with four translations: “the boy tall”, 

“boy the tall”, “tall the boy” and “tall boy the”. 

 
Case 2: Permutation 3-1-2 

The first merge between w3 and w2 generates two 

translation fragments: “boy tall” and “tall boy”. 

The second one creates four translations: “the boy 

tall”, “boy tall the”, “the tall boy”, “tall boy the”. 

 

When the tree has been reduced to a single root 

node and the results of both permutations are col-

lected, the node will contain all eight alternative 

translations ranked according to the language 

model, so that the best one, possibly “the tall boy”, 

can be selected as overall sentence translation. 

3 Node Merge 

The operation of node merge consists of taking all 

possible translations for the two nodes and conca-

tenating them in either sequential or reverse order, 

adding them to the translation of the parent node 

and dropping the child. 

In certain cases though, for example idiomatic 

phrases, the best translation is not obtained by 

combining the individual translations of each 

word, but instead a proper translation might be 

found in the Phrase Translation Table (PT). Hence 

besides performing combination of translations, 

we also consider the sub-tree rooted at the head 

node hri of node ri. We consider the phrase corres-

ponding to the leaves of the sub-tree rooted at hri 

and all children already merged into it, including 

ri: if this phrase is present in the PT, then its trans-

lations are also added to the node. 

This is sometimes useful, since it allows the de-

coder to exploit phrases that only correspond to 

partial sub-trees that it will otherwise miss. 

4 Reordering Rules 

In order to restrict the number of permutations to 

consider, we introduce a reordering step based on 

rules that examine the dependency tree of the 

source sentence. 

The rules are dependent on the language pair 

and they can be learned automatically from the 

corpus. 

We report first a simple set of hand crafted rules 

devised for the pair Italian-English that we used as 

a baseline. 

The default ordering is to start numbering the 

left children of a node backwards, i.e. the node 

closer to the head comes first, then continuing 

with the right children in sequential order. 

Special rules handle these cases: 

1) The head is a verb: move an adverb child to 
first position.  This lets a sequence of VA VM 

V R be turned into VA VM R V, where VA is 

the POS for auxiliary verbs, VM for modals, 

V for main verb and R for adverbs. 

2) The head is a noun: move adjectives or prepo-
sitions immediately following the head to the 

beginning. 

Il ragazzo alto 

the boy tall 

boy the tall 

tall the boy 

tall boy the 

Il ragazzo 

the boy 

boy the 

alto 

tall 

high 

81



4.1 Learning Reordering Rules 

In order to learn the reordering rules we created a 

word-aligned parallel corpus from 1.3 million 

source sentences selected from the parallel corpus. 

The corpus is parsed and each parse tree is ana-

lyzed using the giza++ word alignments of its 

translation to figure out node movements. 

For each source-language word, we estimate a 

unique alignment to a target-language word. If the 

source word is aligned to more than one target 

word we select the first one appearing in the 

alignment file. If a source word is not aligned to 

any word, we choose the first alignment in its des-

cendants in the dependency tree. If no alignment 

can be found in the descendants, we assume that 

the word stays in its original position. 

We reorder the source sentence according to 

this alignment, putting it in target-language order. 

We produce a training event consisting of a pair 

(context, offset) for each non-root word. The con-

text of the event consists of a set of features (the 

POS tag of a word, its dependency tag and the 

POS of its head) extracted for the word and its 

children. The outcome of the event is the offset of 

the word relative to its parent (negative for words 

that appear on the left of their parent in target-

language order, positive otherwise). 

We calculate the relative frequency of each 

event conditioned on the context, deriving rules of 

the form: 

(context, offset, Pr[Offset = offset | Context = 

context]). 

During decoding, we compute a reordering posi-

tion for each source word by adding to the word 

position to the offset predicted by the most likely 

reordering rule matching the word context (or 0 if 

no matching context is found). 

The reordering position drives the children 

combination procedure in the decoder. 

Our reordering rules are similar to those pro-

posed by Xu at al. (2009), except that we derive 

them automatically from the training set, rather 

than being hand-coded. 

4.2 Beam Search 

Search through the space of hypotheses generated 

is performed using beam search that keeps in each 

node the list of the top best translations for the 

node. The score for the translation is computed 

using the weights of the individual phrases that 

make up the translation and the overall LM proba-

bility of the combination. 

The scores are computed querying the standard 

Moses Phrase Table and the LM for the target lan-

guage; other weights uses by moses such as the 

reordering weights or the future cost estimates are 

discarded or not computed. 

5 The Model 

A mathematical model of the dependency based 

translation process can be formulated as follows. 

Consider the parse of a sentence f of length n. 

Let R denote all topological ordering of the nodes 

according to the dependency tree. 

Let fr denote the parse tree along with a consis-

tent node ordering r. Each ordering gives rise to 

several different translations. Let Er denote the set 

of translations corresponding to fr. We assign to 

each translation er  Er a probability according to 

the formula below. The final translation is the best 

result obtained through combinations over all or-

derings. 
Error! Objects cannot be created from editing field 

codes. 
Where er denotes any of the translations of f ob-

tained when nodes are combined according to 

node ordering r.  

The probability of a translation er corresponding 

to a node ordering r for a phrase f, p(er | f ) is de-

fined as: 
Error! Objects cannot be created from editing field 

codes. 
where 

Error! Objects cannot be created from editing 

field codes. andError! Objects cannot be 

created from editing field codes.denote the leaf 

words from node ri and those of its head node hri,  

respectively. 

Error! Objects cannot be created from edit-

ing field codes.is either Error! Objects cannot 

82



be created from editing field codes.or Error! 

Objects cannot be created from editing field 

codes. 

p(f, e) = pPT(str(f), e) if str(f)  PT 

str(f) is the sentence at the leaves of node ri 

pLM is the Language Model probability 

pPT is the Phrase Table probability 

6 Related Work 

Yamada and Knight (2001) introduced a syntax-

based translation model that incorporated source-

language syntactic knowledge within statistical 

translation. Many similar approaches are based on 

constituent grammars, among which we mention 

(Chiang, 2005) who introduced hierarchical trans-

lation models. 

The earliest approach based on dependency 

grammars is the work by Ashlawi et al. (2000), 

who developed a tree-to-tree translation model, 

based on middle-out string transduction capable of 

phrase reordering. It translated transcribed spoken 

utterances from English to Spanish and from Eng-

lish to Japanese. Improvements were reported over 

a word-for-word baseline. 

Ambati (2008) presents a survey of other ap-

proaches based on dependency trees. 

Quirk et. al. (2005) explore a tree-to-tree ap-

proach, called treelet translation, that extracts tree-

lets, i.e. sub-trees, from both source and target 

language by means of a dependency parser. A 

word aligner is used to align the parallel corpus. 

The source dependency is projected onto the target 

language sentence in order to extract treelet trans-

lation pairs. Given a foreign input sentence, their 

system first generates its dependency tree made of 

treelets. These treelets are translated into treelets 

of the target language, according to the dependen-

cy treelet translation model. Translated treelets are 

then reordered according to a reorder model. 

The ordering model is trained on the parallel 

corpus. Treelet translation pairs are used for de-

coding. The reordering is done at the treelet level 

where all the child nodes of a node are allowed all 

possible orders. The results show marginal im-

provements in the BLEU score (40.66) in compar-

ison with Pharaoh and MSR-MT.  But the treelet 

translation algorithm is more than an order of 

magnitude slower. 

Shen et. al. (2008) present a hierarchical ma-

chine translation method from string to trees. The 

scheme uses the dependency structure of the target 

language to use transfer rules while generating a 

translation. The scheme uses well-formed depen-

dency structure which involves fixed and floating 

type structures. The floating structures allow the 

translation scheme to perform different concatena-

tion, adjoining and unification operations still be-

ing within the definition of well-formed structures. 

While decoding the scheme uses the probability of 

a word being the root, and also the left-side, right-

side generative probabilities. The number of rules 

used varies from 27 M (for a string to dependency 

system) to 140 M (baseline system). The perfor-

mance reached 37.25% for the system with 3-

grams, 39.47% for 5-grams. 

Marcu and Wong (2002) propose a joint- prob-

ability model. The model establishes a correspon-

dence between a source phrase and a target phrase 

through some concept. The reordering is inte-

grated into the joint probability model with the 

help of: 

3) Phrase translation probabilities Error! Ob-

jects cannot be created from editing field 

codes. denoting the probability that concept ci 
generates the translation Error! Objects can-

not be created from editing field codes. for 

the English and Error! Objects cannot be 

created from editing field codes. for the for-

eign language inputs. 

4) Distortion probabilities based on absolute po-
sitions of the phrases.  

Decoding uses a hill-climbing algorithm.  Perfor-

mance wise the approach records an average 

BLEU score of 23.25%, with about 2% of im-

provement over the baseline IBM system. 
Zhang et. al. (2007) present a reordering model 

that uses linguistic knowledge to guide both 

phrase reordering and translation between linguis-

tically correct phrases by means of rules. Rules are 

encoded in the form of weighted synchronous 

grammar and express transformations on the parse 

trees. They experiment also mixing constituency 

and dependency trees achieving some improve-

83



ments in BLEU score (27.37%) over a baseline 

system (26.16%). 

Cherry (2008) introduces a cohesion feature in-

to a traditional phrase based decoder. It is imple-

mented as a soft constraint which is based on the 

dependency syntax of the source language. He 

reports a BLEU score improvement on French-

English translation. 

The work by Xu et al. (2009) is the closest to 

our approach. They perform preprocessing of the 

foreign sentences by parsing them with a depen-

dency parser and applying a set of hand written 

rules to reorder the children of certain nodes. The 

preprocessing is applied to both the training cor-

pus and to the sentences to translate, hence after 

reordering a regular hierarchical system can be 

applied. Translation experiments between English 

and five non SVO Asian languages show signifi-

cant improvements in accuracy in 4 out of 5 lan-

guages. With respect to our approach the solution 

by Xu et al. does not require any intervention on 

the translation tools, since the sentences are rewrit-

ten before being passed to the processing chain: on 

the other hand the whole collection has to undergo 

full parsing with higher performance costs and 

higher dependency on the accuracy of the parser. 

Dyer and Resnik (2010) introduce a translation 

model based on a Synchronous Context Free 

Grammar (SCFG). In their model, translation 

examples are stored as a context-free forest. The 

process of translation comprise two steps: tree-

based reordering and phrase transduction. While 

reordering is modeled with the context-free forest, 

the reordered source is transduced into the target 

language by a Finite State Transducer (FST). The 

implemented model is trained on those portions of 

the data which it is able to generate. An increase 

of BLEU score is achieved for Chinese-English 

when compared to the phrase based baseline. 

Our approach is a true tree-to-string model and 

differs from (Xu et al., 2009), which uses trees 

only as an intermediate representation to rearrange 

the original sentences. We perform parsing and 

reordering only on the phrases to be translated. 

The training collection is kept in the original form, 

and this has two benefits: training is not subject to 

parsing errors and our system can share the same 

model of a regular hierarchical system. 

Another difference is in the selection of transla-

tion options: our method exploits the parse tree to 

select grammatical phrases as translation options. 

7 Implementation 

The prototype decoder consists of the following 

components: 

1) A specialized table lookup server, providing 
an XML-RPC interface for querying both the 

phrase table and the LM 

2) A parser engine based on DeSR (DeSR, 2009) 
3) A reordering algorithm that adds ordering 

numbers to the output produced by DeSR in 

CoNLL-X format. Before reordering, this step 

also performs a restructuring of the parse tree, 

converting from the conventions of the Italian 

Tanl Treebank to a structure that helps the 

analysis. In particular it converts conjunctions, 

which are represented as chains, where each 

conjunct connects to the previous, to a tree 

where they are all dependent of the same head 

word. Compound verbs are also revised: in the 

dependency tree each auxiliary of a verb is a 

direct child of the main verb. For example in 

“avrebbe potuto vedere”, both the auxiliary 

“avrebbe” and the modal “potuto” depend on 

the verb “vedere”.  This steps groups all aux-

iliaries of a verb under the first one, i.e. “potu-

to”. This helps so that the full auxiliary can be 

looked up separately from the verb in the 

phrase table. 

4) A decoder that uses the output produced by 
the reordering algorithm, queries the phrase 

table and performs a beam search on the hypo-

theses produced according to the suggested 

reordering. 

8 Experimental Setup and Results 

Moses (Koehn et al., 2007) is used as a baseline 

phrase-based SMT system. The following tools 

and data were used in our experiments:  

1) the IRSTLM toolkit (Marcello and Cettolo, 
2007) is used to train a 5-gram language mod-

84



el with Kneser-Ney smoothing on a set of 4.5 

million sentences from the Italian Wikipedia. 

2) the Europarl version 6 corpus, consisting of 
1,703,886 sentence pairs, is used for training. 

A tuning set of 2000 sentences from ACL 

WMT 2007 is used to tune the parameters.  

3) the model is trained with lexical reordering. 
4) the model is tuned with mert (Bertoldi, et al. ) 
5) the official test set from ACL WMT 2008 

(Callison-Burch et al., 2008), consisting of 

2000 sentences, is used as test set. 

6) the open-source parser DeSR (DeSR, 2009) is 
used to parse Italian sentences, trained on the 

Evalita 2009 corpus (Bosco et al., 2009). Pars-

er domain adaptation is obtained by adding to 

this corpus a set of 1200 sentences from the 

ACL WMT 2005 test set, parsed by DeSR and 

then corrected by hand. 

Both the training corpora and the test set had to be 

cleaned in order to normalize tokens: for example 

the English versions contained possessives split 

like this “Florence' s”. We applied the same toke-

nizer used by the parser which conforms to the 

PTB standard. 

DeSR achieved a Labeled Accuracy Score of 

88.67% at Evalita 2009, but for the purpose of 

translation, just the Unlabeled Accuracy is rele-

vant, which was 92.72%. 

The table below shows the results of our decod-

er (Desrt) in the translation from Italian to English, 

compared to a baseline Moses system trained on 

the same corpora and to the online version of 

Google translate. 

Desrt was run with a beam size of 10, since ex-

periments showed no improvements with a larger 

beam size. 

We show two versions of Desrt, one with parse 

trees as obtained by the parser and one (Desrt 

gold) where the trees were corrected by hand. The 

difference is minor and this confirms that the de-

coder is robust and not much affected by parsing 

errors. 

System BLEU NIST 

Moses 29.43 7.22 

Moses tree phrases 28.55 7.10 

Desrt gold 26.26 6.88 

Desrt 26.08 6.86 

Google Translate 24.96 6.86 

Desrt learned 24.37 6.76 
Table 1. Results of the experiments. 

Since we used the same phrase table produced by 

Moses also for Desrt, Moses has an advantage, 

because it can look up n-grams that do not corres-

pond to grammatical phrases, which Desrt never 

considers. In order to determine how this affects 

the results, we tested Moses restricting its choice 

to phrases corresponding to treelets form the parse 

tree. The result is shown in the row in the table 

labeled as “Moses tree phrases”. The score is low-

er, as expected, but this confirms that Desrt makes 

quite good use of the portion of the phrase table it 

uses. 

Since the version of the reordering algorithm we 

used produces a single reordering, the Desrt de-

coder has linear complexity on the length of the 

sentence. Indeed, despite being written in Python 

and having to query the PT as a network service, it 

is quite faster than Moses.  

9 Error Analysis 

Despite that fact that Desrt is driven by the parse 

tree, it is capable of selecting fairly good and even 

long sentences for look up in the phrase table. 

How close is the Desrt translation from those of 

the Moses baseline can be seen from this table: 

 1-gram 2-gram 3-gram 4-gram 5-gram 

NIST 7.28 3.05 1.0 0.27 0.09 

BLEU 84.73 67.69 56.94 48.59 41.78 

Sometimes Desrt fails to select a better translation 

for a verb, since it looks up prepositional phrases 

separately from the verb, while Moses often con-

nects the preposition to the verb. 

This could be improved by performing a check 

and scoring higher translations which include the 

translation of the preposition dependent on the 

verb. 

Another improvement could come from creating 

phrase tables limited to treelet phrases, i.e. phrases 

corresponding to treelets from the parser. 

85



10 Enhancements 

The current algorithm needs to be improved to 

fully deal with certain aspects of long distance 

dependencies. Consider for example the sentence 

“The grass around the house is wet”. The depen-

dency tree of the sentence contains the non-

contiguous phrases “The grass” and “wet”, whose 

Italian translation must obey a morphological 

gender agreement between the subject “grass” 

(“erba”, feminine), and the adjective “wet” (“bag-

nata”). 

However, the current combination algorithm 

does not exploit this dependence, because the last 

phases of node merge will occur when the tree has 

been reduced to this: 

The PT however could tell us that “erba bagnata” 

is more likely than “erba bagnato” and allow us to 

score the former higher. 

11 Conclusions 

We have described a decoding algorithm guided 

by the dependency tree of the source sentence. By 

exploiting the dependency tree and deterministic 

reordering rules among the children of a node, the 

decoder is fast and can be kept simple by avoiding 

to consider multiple reorderings, to use reordering 

weights and to estimate future costs. 

There is still potential for improving the algo-

rithm exploiting information implicit in the PT in 

terms of morphological constraints, while main-

taining a simple decoding algorithm that does not 

involve complex grammatical transformation 

rules. 

The experiments show encouraging results with 

respect to state of the art PBT systems. We plan to 

test the system on other language pairs to see how 

it generalizes to other situations where phrase 

reordering is relevant. 

Acknowledgments 

Zauhrul Islam helped setting up our baseline sys-

tem and Niladri Chatterjie participated in the early 

design of the model.  

References 

G. Attardi. 2006. Experiments with a Multilanguage 

Non-Projective Dependency Parser. Proc. of the 

Tenth Conference on Natural Language Learning, 

New York, (NY). 

H. Alshawi, S. Douglas and S. Bangalore. 2000. 

Learning Dependency Translation Models as 

Collections of Finite State Head Transducers. 

Computational Linguistics 26(1), 45–60. 

N. Bertoldi, B. Haddow, J-B. Fouet. 2009. Improved 

Minimum Error Rate Training in Moses. In Proc. of 

3rd MT Marathon, Prague, Czech Republic. 

V. Ambati. 2008. Dependency Structure Trees in Syn-

tax Based Machine Translation. Adv. MT Seminar 

Course Report. 

C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo, F. 

Dell’Orletta and A. Lenci. 2009. Evalita’09 Parsing 

Task: comparing dependency parsers and treebanks. 

Proc. of Evalita 2009. 

P. F. Brown, V. J. Della Pietra, S. A. and  R. L. Mercer. 

1993. The Mathematics of Statistical Machine Trans-

lation: Parameter Estimation. Computational Lin-

guistics, 19(2), 263–311. 

Callison-Burch et al. 2008. Further Meta-Evaluation of 

Machine Translation. Proc. of ACL WMT 2008. 

C. Cherry. 2008. Cohesive phrase-based decoding for 

statistical machine translation. Proc. of ACL 2008: 

HLT. 

D. Chiang. 2005. A hierarchical phrase-based model for 

statistical machine translation. In Proc. of ACL 2005.  

DeSR. Dependency Shift Reduce parser. 

http://sourceforge.net/projects/desr/ 

Y. Ding, and M. Palmer. 2005.  Machine Translation 

using Probabilistic Synchronous Dependency Inser-

tion Grammar.  Proc. of ACL’05, 541–548. 

C. Dyer and P. Resnik. 2010. Context-free reordering, 

finite-state translation. Proc. of HLT: The 2010 

Annual Conference of the North American Chapter 

of the ACL, 858–866. 

grass 

L’ erba intorno alla casa 

is 

è 

wet 

bagnata 

bagnato 

 

 

86



F. Marcello, M. Cettolo. 2007. Efficient Handling of N-

gram Language Models for Statistical Machine 

Translation. Workshop on Statistical Machine Trans-

lation 2007. 

M. Galley and C. D. Manning. 2008. A Simple and 

Effective Hierarchical Phrase Reordering Model. In 

Proc. of EMNLP 2008. 

R. Hwa, P. Resnik, A. Weinberg, C. Cabezas and O. 

Kolak, 2005.  Bootstrapping Parsers via Syntactic 

Projection across Parallel texts. Natural Language 

Engineering 11(3), 311-325. 

P. Koehn, F. J. Och and D. Marcu.  2003. Statistical 

Phrase-Based Translation. Proc. of Human Lan-

guage Technology and North American Association 

for Computational Linguistics Conference 

(HLT/NAACL), 127–133. 

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 

Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-

ran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and 

E. Herbst. 2007. Moses: Open source toolkit for sta-

tistical machine translation. In Proc. of the 45th An-

nual Meeting of the ACL, demonstration session, 

177–180, Prague, Czech Republic. 

P. Koehn. 2009. Statistical Machine Translation.  

Cambridge University Press. 

Y. Liu, Q. Liu and S. Lin. 2006. Tree-to-string Align-

ment Template for Statistical Machine Translation, 

In Proc. of COLING-ACL. 

D. Marcu and W. Wong. 2002. A Phrase-Based Joint 

Probability Model for Statistical Machine Transla-

tion. Proc. Empirical Methods in Natural Language 

Processing (EMNLP), 133–139. 

C. Quirk, A. Menzes and C. Cherry. 2005. Dependency 

Treelet Translation: Syntactically Informed Phrasal 

SMT. Proc. 43rd Annual Meeting of the ACL, 217–

279. 

S. Libin, J. Xu and R. Weischedel. 2008. A New String-

to-Dependency Machine Translation Algorithm with 

a Target Dependency Language Model. Proc. ACL-

08, 577–585. 

F. J. Och 2002. Statistical Machine Translation: From 

Single Word Models to Alignment Template. Ph.D. 

Thesis, RWTH Aachen, Germany. 

F.J. Och, H. Ney. 2000. Improved Statistical Alignment 

Models. Proc. of the 38th Annual Meeting of the 

ACL.  Hong Kong, China. 440-447. 

K. Yamada and K. Knight. 2001. A Syntax-Based Sta-

tistical Translation Model. Proc. 39th Annual Meet-

ing of ACL (ACL-01), 6–11. 

P. Xu, J. Kang, M. Ringgaard and F. Och. 2009. Using 

a Dependency Parser to Improve SMT for Subject-

Object-Verb Languages. Proc. of NAACL 2009, 245–

253, Boulder, Colorado. 

D. Zhang, Mu Li, Chi-Ho Li and M. Zhou.  2007. 

Phrase Reordering Model Integrating Syntactic 

Knowledge for SMT.  Proc. Joint Conference on 

Empirical Methods in Natural Language Processing 

and Computational  Natural Language Processing: 

533–540. 

 

 

87


