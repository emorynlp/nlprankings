



















































Name List Only? Target Entity Disambiguation in Short Texts


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 654‚Äì664,
Lisbon, Portugal, 17-21 September 2015. c¬©2015 Association for Computational Linguistics.

Name List Only? Target Entity Disambiguation in Short Texts

Yixin Cao1, Juanzi Li1, Xiaofei Guo1, Shuanhu Bai2, Heng Ji3, Jie Tang1
1 Tsinghua National Laboratory for Information Science and Technology

Dept. of Computer Science and Technology, Tsinghua University, China 100084
2 Sina Corporation, China 100084

3 Dept. of Computer Science, Rensselaer Polytechnic Institute, USA 12180
{caoyixin2011,lijuanzi2008,sophiaguo.thu,jery.tang}@gmail.com

shuanhu@staff.sina.com.cn, jih@rpi.edu

Abstract

Target entity disambiguation (TED), the
task of identifying target entities of the
same domain, has been recognized as a
critical step in various important applica-
tions. In this paper, we propose a graph-
based model called TremenRank to collec-
tively identify target entities in short texts
given a name list only. TremenRank prop-
agates trust within the graph, allowing for
an arbitrary number of target entities and
texts using inverted index technology. Fur-
thermore, we design a multi-layer direct-
ed graph to assign different trust levels to
short texts for better performance. The
experimental results demonstrate that our
model outperforms state-of-the-art meth-
ods with an average gain of 24.8% in accu-
racy and 15.2% in the F1-measure on three
datasets in different domains.

1 Introduction

Currently, a growing number of people prefer to
express their views and comments online. These
messages, which are updated at the rate of mil-
lions per day, become a potentially rich source of
information. From such a large number of texts,
entity disambiguation is a critical step when ex-
tracting text information from these messages, and
for various applications, such as natural language
processing and knowledge acquisition (Dredze et
al., 2010). Take the application of customer feed-
back analysis (CFA) as an example. An enterprise
is typically interested in public reviews of its own
products as well as those of its competitors‚Äô; the
identification of these entities is thus critical for
further analysis.

The product names comprise a list of entities
to be identified. We refer to these entities of the
same domain as target entities, and the identifica-

tion process is called target entity disambigua-
tion (Wang et al., 2012). All of target entities (e.g.,
car brands) share a common domain, which we re-
fer to as the target domain. The target domain is
the only constraint to target entities, which implies
that the entities are in a specific domain, rather
than being general things. In the case of entity
recognition from short texts, the disambiguation
can be performed on the document level. Given a
collection of short documents, our goal is to deter-
mine which documents contain the target entities.

Challenge and Related Work
In contrast to traditional entity disambiguation

tasks, TED in short texts can require as little in-
formation as a name list. There are three types of
information that are utilized in Named Entity Dis-
ambiguation related tasks: knowledge resources,
the context in which a target word occurs and sta-
tistical information (Navigli, 2009). However, the
lack of the first two types of information makes
this problem more challenging.

Knowledge Sparsity A large number of meth-
ods focus on using knowledge bases (KBs) like
Wikipedia or YAGO to enrich the named enti-
ties (e.g., in-links and out-links) (Hoffart et al.,
2011; Bunescu and Pasca, 2006; Milne and Wit-
ten, 2008; Kulkarni et al., 2009; Han et al., 2011;
Mihalcea and Csomai, 2007; Shen et al., 2012).
These methods compared the context of the enti-
ties and their reference pages in the KBs through a
similarity measurement. However, we tested 32 d-
ifferent names of General Motors (GM) car brand-
s, and only four of the brands exist in Wikipedi-
a. This circumstance is not unusual. For another
larger dataset that included 2468 stock names, we
found only 340 of them had their reference pages
in Wikipedia. Thus, the methods that rely heavily
on KBs might not be appropriate here.

Shortness of the Texts The context in which a

654



target entity occurs plays an important role in dis-
ambiguation. (Cassidy et al., 2012; Li et al., 2013)
applied the underlying topical coherence informa-
tion to a set of mentions for entity linking. In
(Wang et al., 2012), mentionRank leverages some
additional features, such as co-mention; however,
people prefer to share their comments in brief or
even informal words on social media platforms,
such as Twitter, which becomes increasingly im-
portant as an information source. We have count-
ed 350,498 microblogs, 37,379 tweets and 34,018
text snippets in various domains in Chinese and
English from Twitter, Sina Weibo and Google. Af-
ter preprocessing, their average length are 16, 5
and 11 words, respectively. To alleviate the short-
ness issue, additional information has been used
to expand the context, such as the user‚Äôs interest
(Shen et al., 2013) and related documents (Guo et
al., 2013). Such information is still sparse or un-
available in this case, and thus, the existing meth-
ods might not be suitable.

Large Scale Hundreds of millions of tweets are
posted daily on Twitter (Liu et al., 2013). When
scaling to a large document collection, the disam-
biguation task becomes increasingly important as
the ambiguity increases (Cucerzan, 2007). Men-
tionRank, the only state-of-the-art method for T-
ED, is a graph-based model that focuses on a s-
mall set of entities (e.g., 50 or 100 entities) and
conducts experiments on thousands of documents
(Wang et al., 2012). However, the graph has a
quadratic growth as the number of documents in-
creases. In our experiments, a dataset that includ-
ed 2,468 target entities and 350,498 microblogs
generated a directed graph with billions of edges,
which required more computer memory than was
available even when using a sparse matrix. There-
fore, the scalability remains a challenge.

Contributions
To address these challenges, we propose a

collective method called TremenRank to disam-
biguate the target entities simultaneously. The
main idea is to propagate trust within a graph
based on our observations that true mentions are
more similar in context than false mentions in a
specific domain. Specifically, inverted indexes is
used to construct the graph locally that allows for
an arbitrary number of target entities and docu-
ments. Furthermore, a multi-layer directed graph
is designed to assign different trust levels to doc-

uments, which significantly improves the perfor-
mance. The contributions of our work can be sum-
marized as follows:

‚Ä¢ We propose a novel graph-based method,
TremenRank, to collectively identify target
entities in short texts. This method construct-
s the graph locally to avoid storing the entire
graph in memory, which provides a linear s-
cale up with respect to the number of target
entities and documents.

‚Ä¢ We design a multi-layer directed (MLD)
graph for modeling short texts that pos-
sess different trust levels during propaga-
tion, which significantly improves the perfor-
mance.

‚Ä¢ We conduct a series of experiments on three
practical datasets from different domains.
The experimental results demonstrate that
TremenRank has a similar performance to
the state-of-the-art when addressing the TED
problem at a large scale, and the use of MLD
graph significantly improves our method.

2 Problem Definition

Example Suppose that GM wants to collect tweets
that talk about its cars. As shown in Figure 1, we
take (i) a list of car brands (target entities), and (i-
i) a collection of tweets (i.e., short texts) as inputs.
‚ÄúCar‚Äù is the target domain, and ‚ÄúSonic‚Äù appears
in documents d1 and d2, which can be character-
ized as two mentions and the latter is a true men-
tion. The goal is to find as many true mentions as
possible.

Our method models the collection of documents
as a directed graph and outputs a trust score for
each document via propagation. The trust score
indicates the likelihood of a document containing
a true mention. For flexibility, the trust scores
lie within the range of 0 to 1 and are globally
comparable; thus, we can obtain the top-k doc-
uments or choose an appropriate cut-off value to
balance the precision and recall for different ap-
plication requirements. For example, in the appli-
cation of CFA, a company expects a higher recall
to achieve a comprehensive understanding of its
product, whereas a recommendation system must
provide as many precise microblogs as possible.

Formally, the problem of TED in short texts is
defined as follows:

655



Entity

Sonic

Express

Spark

There is too much sonic

I drive Chevrolet Sonic today

Late night thoughts express life

I recently added Spark and
Chevrolet Express to inventory

Don‚Äôt drink and drive Spark

Mentions in short texts

Target Entity

A single spark can start a
prairie fire

d1 r1:0.1

Input Output

d2 r2:0.9

d3 r3:0.1

d4 r4:0.9

d5 r5:0.1

d6 r6:0.8

Figure 1: Illustration of TED. True mentions are
shown as solid arrows, and the underlined words
are their overlapping context.

Target Entity Disambiguation in Short Texts
Given a list of target entities E = {ei|i =
1, . . . ,m}, and a collection of text documents
D = {dj |j = 1, . . . , n}, Edj = {ej1, . . . , ejk} is
the set of target entities contained in dj . The goal
is to output the trust score rj ‚àà [0, 1] for each doc-
ument dj ‚àà D. All the target entities in Edj share
the same trust score rj .

All of the mentions in one document have the
same score because they share a common context.
In the graph, the context similarity between doc-
uments is computed and used as the edges nor-
mally, where the width of the context window that
surround the target entity in the document is typ-
ically chosen to be 10, 20 or 50 words. Howev-
er, the documents considered here are limited in
length, and a user seldom changes the topic in so
few words; thus, we regard the entire document as
the context of its target entities. When multiple
target entities occur in one document, all of them
are more likely to refer to the entities in the target
domain. Thus, the trust score for the document is
higher, as indicated by d4 (Figure 1).

The only constraint for the target entities is that
they are in the same target domain. This constraint
is reasonable in practice. The majority of applica-
tions identify a set of entities in one domain at a
time. For example, a computer company focuses
its attentions on the brands in the computer area,
whereas an investment company is mainly inter-
ested in stocks. Additionally, even if the target
domain is general and contains several small do-
mains, an intuitive solution is to split it into sev-
eral subproblems, where each subproblem focuses

on the target entities in one small domain. Then,
the task can be achieved by solving the subprob-
lems individually.

3 Our Approach

TremenRank is a graph-based method that identi-
fies target entities collectively. It propagates trust
scores on the graph where each vertex denotes one
document and an edge indicates the similarity be-
tween them. Considering the large scale of the
problem, we obtain the neighbors of one vertex
when propagating by searching two indexes in-
stead of storing the entire graph in memory. This
approach allows an arbitrary number of target en-
tities and documents to be processed. To further
improve the performance, a multi-layer directed
graph is designed to treat the documents at differ-
ent trust levels based on prior estimations.

3.1 Hypotheses
The documents within a domain share the char-
acteristic of unitary similarity. This characteristic
implies that all of the true mentions have a sim-
ilar context due to the target domain constraint
and that false mentions are distinct because their
meanings belong to diversified domains. We in-
vestigated the ambiguity of 2468 stock names (tar-
get entities in experiments), and manually labeled
301 of those names. As shown in Figure 2, there
are many different meanings for these names out-
side of the target domain, such as plant, bank, me-
dia and animal. The distribution of meanings are
long-tailed; thus, we gathered a group of meanings
together (the class ‚Äúothers‚Äù).

Based on the statistical results, we can make the
following hypotheses:

‚Ä¢ The context of true mentions are similar to
one another.
‚Ä¢ The context of a false mention is different

from any of the true mention.
‚Ä¢ False mentions have distinct contexts across

different entities.

For example, the true mentions in d2, d4 and d6
(Figure 1) describe car brands of GM and share
common pieces of text: ‚Äúdrive‚Äù or ‚ÄúChevrolet‚Äù.
However, ‚Äúsonic‚Äù, ‚Äúexpress‚Äù and ‚Äúspark‚Äù in d1,
d3 and d5 are all false mentions; in their contexts,
they refer to sound, giving opinions, and a small
amount of fire, respectively. These false mentions
are different in context from one another and from
any true mention.

656



The assumptions resemble the main insight of
MentionRank except the co-mention (multiple en-
tities occur in one document). Co-mention sel-
dom happens in short texts, and can be treated
as the same because they share a common con-
text. In conclusion, the assumptions suggest that
a collective method could perform better than a
method that disambiguates entities separately, be-
cause more comprehensive information on the en-
tities from multiple ‚Äúcollaborators‚Äù (i.e., the men-
tions have similar contexts) has been used (Chen
and Ji, 2011; Kulkarni et al., 2009; Pennacchiotti
and Pantel, 2009; Will et al., 2010; Fernandez et
al., 2010; Cucerzan, 2011; Guo et al., 2011; Han
and Sun, 2011; Ratinov et al., 2011; Kozareva et
al., 2011; Dalton and Dietz, 2013).

Te
ch

Fo
od

Lo
cat

ion
Me

dia
Ho

use Ba
nk
Ot
he
rs

To
ur

An
im
al

Av
iat

ion
Ph

ras
e

Un
ive

rsi
ty

Pla
nt

0%

5%

10%

15%

20%

25%

30%

35%

40%

45%

50%

Figure 2: Statistics on the ambiguity in stocks.

3.2 Graph-based Method

Based on these assumptions, we build a graph to
represent the documents and their relations, and
we perform a TrustRank-like algorithm on the
graph. We are given a graph G = (V, E) that con-
sists of a set V ofN documents (vertices) and a set
E of directed edges, where each edge (di, dj) ‚àà E
denotes that di points to dj , and o(di) is the out-
neighbors of di.

3.2.1 Similarity Measurement

We constructed the edges of the graph accord-
ing to the similarity relations between docu-
ments. Most similarity measurements (Artiles et
al., 2010; Miller, 1995) could be used in the pro-
posed method. After some exploration, we found
that Jaccard similarity performs better. It can be
efficiently calculated through simple set opera-
tions:

œâJij = J(di, dj) =

‚à£‚à£Wdi ‚ãÇWdj ‚à£‚à£‚à£‚à£Wdi ‚ãÉWdj ‚à£‚à£ (1)
where œâJij denotes the weight of edge (di, dj) us-
ing Jaccard similarity, andWdi is the set of words
contained in di. œâij varies from 0 to 1, where a val-
ue closer to 1 indicates that its two nodes are more
similar. We link only similar nodes by choosing an
appropriate threshold Œ∑. In other words, we have
(di, dj) ‚àà E , only if œâJij > Œ∑. This is the founda-
tion to construct the graph locally using inverted
indexes.

3.2.2 Inverted Index

When the scale becomes excessively large, such
as the 350,000 pieces in our dataset, the number
of edges will increase into the billions, produc-
ing computational and storage-based difficulties.
Considering that the propagation begins with doc-
ument di and that we are required to find all of
its out-neighbors o(di) through a traversal of the
entire dataset, then the complexity is O(n2). Al-
ternatively, we can represent the entire graph with
a matrix; however, its billions of elements would
be difficult to store and calculate.

To address the large scale problem, we con-
struct the graph locally via inverted index tech-
nology. During propagation, the neighbors of
the documents are obtained by searching the in-
dexes in real time. Two types of indexes are
used: the document-to-word index and the word-
to-document index. The former index recordsWdi
of each document di ‚àà D; the latter index record-
s the occurrence of each word Dwk = {dj |wk ‚àà
W}, where W = {w1, . . . , wN} is the word dic-
tionary. Combining these two indexes, the total
out-neighbors of a document can be obtained in
constant time as follows:

1. Obtain all of the wordsWdi of di via search-
ing the document-to-word index.

2. Find the occurrences of each word wk ‚àà
Wdi in the word-to-document index: Ddi =
{dj | ‚à™wt‚ààWdi Dwt}. Each document dj ‚àà
Ddi shares at least one common word with
di.

3. Count the frequency of dj , which indicates
the number of overlapping words between di
and dj . Then, the frequency fij = |Wdi ‚à©
Wdj |, i 6= j and Equation 1 becomes:

657



œâJij =
fij

|Wdi |+ |Wdj | ‚àí fij
(2)

4. Calculate the similarity weight. We obtain
the out-neighbors o(di) = {dj |œâJij ‚â• Œ∑}.

3.2.3 Trust Propagation
Similar to TrustRank, an individual documen-
t propagates the trust score to its neighbors and
those who have more neighbors will receive more
trust. Intuitively, trust attenuates along the edge.
There are several ways for attenuation to take
place, such as trust dampening, trust splitting, and
a combination of them (GyoÃàngyi et al., 2004). For
example, each document di ‚àà D has a trust score
ri, and its out-neighbors obtain Œ± ¬∑ ri (or ri|o(di)| )
through trust dampening (or splitting). We adopt
the third method, in which a document dampens
its split trust with the attenuation coefficient Œ±.

The final trust score is determined by two parts:
the trust from a document‚Äôs neighbors and its pri-
or estimation. Then, TremenRank can iteratively
propagate via the following function:

ri = Œ± ¬∑
‚àë
o(di)

rj
|o(dj)| + (1‚àí Œ±) ¬∑ T (di) (3)

where T (di) is the prior estimation of the docu-
ment di and is a constant during iterative propaga-
tion. Here we simply assign a uniform distribution
to all of the documents T (di) = 1|D| . A more pre-
cise estimation will be discussed later.

At the beginning of the propagation, we initial-
ize the trust scores of the documents with the pri-
or estimation. Then, the trust scores of the docu-
ments are updated iteratively until convergence1.
True mentions receive high scores because they
are likely to connect with more trustworthy doc-
uments and thus receive more trust through the
first term in Equation 3. In contrast, false men-
tions are dissimilar to most other documents; thus,
their scores gradually attenuate (the second term
in Equation 3). These scores are globally compa-
rable. One can normalize the final scores by di-
viding them by the largest score, but the relative
ordering of the documents will not change. Thus,
one document that is more likely to contain any

1We select the attenuation coefficient Œ± = 0.85 and the
number of iterations to be 20, which have been regarded as
the standards in the PageRank literature(Page et al., 1999;
Krishnan and Raj, 2006). Our experiments also show that 20
iterations are sufficient to achieve convergence.

true mention will receive a higher trust score and
be ranked higher.

3.3 Multi-layer Directed Graph

During propagation, all of the documents are ini-
tialized with the same trust score and attenuate
their trust at the same level. However, different
documents should be treated differently. For ex-
ample, the tweet ‚ÄúI drive a big car suburban‚Äù is
more trustworthy than ‚Äúdoctors use GMC report
system to process harmful patients‚Äù, because the
former tweet contains the credible context feature
‚Äúcar‚Äù, which is the name of the target domain. The
latter clearly irrelevant text should not be trusted or
even be regarded as noise. In this subsection, we
first discuss how to make more precise prior esti-
mation of documents based on some of the charac-
teristics of the target domain. Additionally, based
on the prior estimation, an MLD graph is built to
assign different trust levels to the documents.

3.3.1 Prior Estimation
Ideally, a true mention is similar to true mentions
only. To take advantage of the approximate isola-
tion of true mentions, we first extract a set of true
mentions to start the propagation. The documents
that contain these true mentions are called seeds.

Formally, the entire set of documents D
is divided into two groups: (i) the seed set
Ds = {d1, . . . , ds}, and (ii) a subset D‚àó =
{ds+1, . . . , dn}. We estimate a prior trust score
for each document via the function T :

T (di) =
{

1‚àíÔøΩ
|D‚àó| di ‚àà D‚àó,
ÔøΩ
|Ds| di ‚àà Ds.

(4)

where |Ds| and |D‚àó| represent the size of the two
sets for normalization. ÔøΩ ‚àà (0, 1] is used for s-
moothing and indicates the likelihood that we can
trust the seeds that actually contain true mentions.
In the experiments, we set ÔøΩ = 0.9 based on the
accuracy of the seeds extraction method.

There are several methods for extracting seed-
s, such as manual annotation and pattern-based
method. Patterns could be easily derived from the
characteristics of the target domain, such as the
domain name, product type or unique ID2. These
methods can typically identify entities with a high
accuracy, but their low recall limits the portion of

2The exact patterns used in our datasets are detailed in
Section 4

658



true mentions that can be found. We use the result-
s of the pattern-based method as our seeds, which
have an accuracy higher than 90%. We do not
present the experimental results here due to space
limitations.

3.3.2 Graph Construction
The similarity measurement does not consider di-
rections, and thus, the documents are mutually
connected. In this section, we construct a layered
structure in the graph, where each layer denotes a
document trust level that contains any true men-
tion. Thus, we define that the propagation direc-
tion from the high trust level to the low trust level.

Figure 3 shows an example of an MLD graph.
The blue nodes of the seeds in the top layer are
the most trustworthy, and the other white nodes in
the higher layer are less similar to the seeds which
implies that they are at lower trust levels. Thus, as
we move farther away from the seeds, trust atten-
uates at a constant speed Œ± along with the layers.
The nodes in the same layer are also connected.

Œ±

ùíÖùüè
ùüé

. . . Layer-1

Layer-2

.
.
.

Seeds

. . .

. . .

ùíÖùüí
ùüè

ùíÖùüî
ùüê

ùíÖùüê
ùüé ùíÖùüë

ùüé

ùíÖùüì
ùüè

ùíÖùüï
ùüê ùíÖùüñ

ùüê

Œ±

ùíÖ1
0

ùíÖ2
0

ùíÖ3
0

Figure 3: Example of an MLD graph.

The construction algorithm3 is presented in Al-
gorithm 1, where Dl = {dli|l ‚â• 0} is the set of
nodes in layer l, and the nodes in DÃÑ = {dÃÑj |dÃÑj /‚àà
Dl, l ‚â• 0} are not connected. To simplify our no-
tation, the seeds are set to be in layer 0. Note that
(‚à©l‚â•0Dl) ‚à© DÃÑ = D.

TremenRank is different from the standard
TrustRank and MentionRank in several respect-
s. First, TremenRank is designed to process short
texts at a large scale. Second, through a well-
designed MLD graph, we consider documents to
consist of different trust levels rather than be repre-
sented by a unified distribution. Third, TrustRank

3A key parameter for the structure of MLD Graph is Œ∑,
which will be discussed in Section 4.2.

Algorithm 1: Construction of an MLD graph.
Input: Ds, D‚àó, Œ∑, indexesWD,DW
Output: G
Initialize seeds in layer l = 0, DÃÑ = D‚àó;
foreach layer l ‚â• 0 do

Find the out-neighbors of Dl from DÃÑ;
foreach document dli ‚àà Dl do

put o(dli) in the next layer D
l+1;

update DÃÑ = DÃÑ ‚àíDl+1;
end
if |DÃÑ| = 0 or |Dl+1| = 0 then

break;
else

l = l + 1;
end

end

randomly samples a set of seeds and checks them
manually, which limits the number of seeds avail-
able; however, the proposed method extracts the
seeds automatically and uses large amounts of
seeds to produce better prior estimations. Final-
ly, disambiguation occurs at the document level in
the proposed method; we assign the same scores
to the entities that occur in one document, because
they typically share common context features in
short texts.

4 Empirical Evaluation

4.1 Data Preparation

Because there is no publicly available benchmark
dataset for TED, we constructed three datasets of
different domains: Stock, Car and Herb4. Al-
l of these datasets came from the needs of real-life
projects.

1. Stock - We collected 2,468 stock names
from a stock exchange website, and identified their
candidate mentions by string matching from Sina
Weibo (Counterpart of Twitter in China). Each s-
tock has a unique ID, which has little ambiguity.
Thus, we used this regular expression pattern to
extract seeds.

2. Car - We collected 32 car brands of General
Motors and a group of tweets that contain at least
one mention of these brands via the Twitter API. In
the seeds extraction step, we use the domain name
‚Äúcar‚Äù as the patterns.

4All of the dataset related sources used in this study are
listed at http://newsminer.org/TEDs.

659



3. Herb - We randomly selected 1,119 Chinese
herb names and collected 40 pieces of text descrip-
tions in the search results per entity from Google.
To extract the seeds, we simply added the domain
name to the key words (e.g., ‚ÄúU√Å(Worms)¬•√∫
¬Ü(Chinese Medical Herb)‚Äù).

Table 1: Statistics on the datasets.
E D #edges Positive(%)

Stock 2,468 350,498 2.049B 43
Car 32 37,379 4.91M 7
Herb 1,119 34,018 9.66M 68

Table 1 shows some of the statistics of the
datasets created. We chose these datasets because
(i) the identification of entities in these domains
meets the practical requirements of many applica-
tions, (ii) the target entities are ambiguous and (iii)
there is little information in the existing KBs. Be-
fore applying TremenRank, we preprocessed the
plain texts through word segmentation, low fre-
quency words filtering and stop words filtering.

4.2 Experiment

Baseline Methods TED in short texts is a relative-
ly new problem, and there are few specific meth-
ods for solving it. To validate the performance of
TremenRank and the improvement produced by
the MLD graph, we selected the baseline from
three different perspectives: (i) a context-based
method that identifies target entities separately; (i-
i) a classic supervised method SVM to classify a
document by whether it contains any true mention-
s; and (iii) the only state-of-the-art MentionRank
for TED, which is a collective ranking method.

‚Ä¢ The Context-based method mines a fre-
quent item set of true mentions and identi-
fies the documents that contain more frequent
items.

‚Ä¢ SVM classifies the documents into two class-
es: documents that are in the target domain
and those that are not. Using context word-
s as features, we train and test SVM on the
labeled set with a 10-fold cross validation.

‚Ä¢ MentionRank is a graph-based method that
disambiguates at the entity level. Difficult to
apply to the entire large datasets directly, it
has been applied to the labeled set.

Evaluation Metrics The performance of the dis-
ambiguation task is typically evaluated by accura-
cy, but in TEDs we are also interested in precision,
recall and the F1-measure because different appli-
cations focus on different aspects. For example, in
the application of CFA, a company expects a high-
er recall to collect as many reviews as possible,
while in financial news recommendations, users
prefer to read more accurate microblogs. Because
the entire dataset is too large to evaluate direct-
ly, we randomly sampled 800 mentions for each
dataset and labelled them manually to calculate the
performance metrics5.

Results and Analysis
The overall performances of TremenRank and

those of the baseline methods on all of the datasets
are shown in Table 2. The following is indicated
in the results:

‚Ä¢ TremenRank+MLD outperforms all of the
baselines with all of the datasets, because it
collectively identifies target entities and treats
the documents differently based on a precise
prior estimation.

‚Ä¢ The collective methods tend to perform bet-
ter. Although, on the Car dataset, SVM
achieves the second best performance, the
use of MLD graph in TremenRank outper-
forms all of the methods tested. This is
because false mentions that occupy a large
proportion of the dataset produce too much
noise, whose negative impacts can be reduced
through the train set in the supervised method
or a precise prior estimations.

‚Ä¢ Compared with MentionRank, TremenRank
processes a larger number of entities and
documents while achieving a similar per-
formance. Combined with a MLD graph,
TremenRank shows significant improve-
ments including an average gain of 24.8% in
accuracy and 15.2% in the F1-measure on the
three datasets.

We also investigate the influence of the main el-
ements in TremenRank below.

Similarity Threshold Different similarity thresh-
olds result in various structures of the MLD graph,
and have a great impact on the performance of our

5A detailed explanation about these metrics can be found
in http://en.wikipedia.org/wiki/Precision and Recall.

660



Table 2: Overall results on the three datasets
Method Stock Car HerbAccu Prec Recall F-score Accu Prec Recall F-score Accu Prec Recall F-score
Context 0.704 0.332 0.545 0.410 0.733 0.333 0.256 0.476 0.670 0.200 0.074 0.108
SVM 0.746 0.377 0.998 0.547 0.839 0.371 0.923 0.529 0.567 0.988 0.564 0.718

MentionRank 0.458 0.424 0.828 0.561 0.464 0.301 0.715 0.423 0.644 0.691 0.824 0.752
TremenRank 0.477 0.424 0.803 0.555 0.542 0.310 0.720 0.433 0.737 0.736 0.827 0.779

TremenRank+MLD 0.683 0.575 0.844 0.684 0.827 0.624 0.731 0.673 0.800 0.774 0.908 0.836

0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

%

Accuracy Precision Recall F1-measure

0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
Œ∑

0.5

0.6

0.7

0.8

0.9

1.0

C
o
ve

ra
g

e
(%

)

coverage

layer

0

1

2

3

4

5

6

#
L

a
ye

rs

Figure 4: Selection of the Similarity Threshold

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.40

0.50

0.60

0.70

0.80

0.90

C
a
r(

%
)

Accuracy Precision Recall F1-measure

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

seeds scale(%)

0.70

0.75

0.80

0.85

0.90

H
e
rb

(%
)

Figure 5: Influence of Seed Scale

method. In this subsection, we study a heuristical
method that help choose the best eta according t-
wo factors (Both of them can be obtained before
propagation). Figure 4 presents the experimental
results for different values of Œ∑ on the dataset of
Herb. The performance of TremenRank is showed
on the top, and the bottom is the corresponding
graph structure represented by two factors: the
graph coverage and the number of layers. We
can see that precision increases until it becomes
steady with the growth of Œ∑, and other measure-
ments reach their peaks when Œ∑ = 0.1.

This is in accordance with the change of the
graph structure. On one hand, the number of lay-
ers l declines sharply when Œ∑ is less than 0.1, this
indicates little difference in the trust level of doc-
uments in the propagation. On the other hand, our
method has no effect on the vertices outside of the
graph, so the performance is directly proportional
to the graph coverage rate. Therefore, a proper Œ∑
should ensure a high coverage as well as adequate
layers. In experiments, we choose Œ∑ as 0.1, 0.15,
0.1 for the datasets of Stock, Car and Herb respec-
tively.

Influence of the Seed Scale As the basis of the
prior estimation, the seeds have significant influ-

ence on the performance of the proposed method
via the MLD graph. Intuitively, a larger set of
seeds should lead to a more precise estimation and
thus better performance. In the experiments on the
Car and Herb datasets, we split their seed set into
ten parts, and add one part each time. As Fig-
ure 5 shows, when increasing the percentage of
the seeds gradually, the performance has an over-
all upward trend (e.g., a 14.6% and 4.4% gain in
the F1-measure for the Car and Herb datasets, re-
spectively). This trend occurs because the poten-
tial context features of the seeds utilized for prop-
agation increase as the absolute number of seed
documents rises. For example, the tweet ‚ÄúCadil-
lac Uber airport is classy‚Äù obtains a low score of
0.013 with 50% of the seeds, whereas it is identi-
fied successfully with the score of 0.588 when all
of the seeds are used for propagation because more
context features are introduced, such as ‚Äúairport‚Äù
and ‚ÄúUber‚Äù. Thus, the proposed method achieves
better performance as the seed set grows. This ar-
rangement is helpful for a company that seldom
changes its business area and then accumulates
seeds continuously to improve performance.

Robustness to Noise Because seeds play an im-
portant role in the MLD graph, we further test the

661



Stock Car Herb
0.4

0.5

0.6

0.7

0.8

0.9

1.0
%

Accuracy Precision Recall F1 noise

Figure 6: Robustness to Noise

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Cut-off Value(%)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

H
e
rb

(%
)

Max F1

High Precision

Accuracy Precision Recall F1-measure

Figure 7: Cut-off Value

robustness of our method with respect to the qual-
ity of the seeds. We randomly sample documents
outside of the seed set (considered as noise) to re-
place 20% of the seeds on all the datasets; these re-
sults are shown in Figure 6. The introduced noise
only leads to a limited decrease in performance
(average 1.3%, 1% and 1% in the F1), which is
much better than when only using TremenRank
(Table 2). More specifically, the experiments that
use artificial noise occasionally achieve a high-
er recall (e.g., on the Stock dataset); this result
could occur because the unknown true mentions
add some useful edges to the graph, which is help-
ful when finding more true mentions of the target
entities.

Cut-off Value According to the globally compa-
rable trust scores, we can (i) rank all of the doc-
uments, and trade off the performance metrics by
choosing an appropriate cut-off value Œ≥ for various
applications, or (ii) rank the documents of an indi-
vidual entity separately and obtain its top-k men-
tions.

In the experiments, we set Œ≥ as different per-
centage of the ranked documents. As Figure 7
shows, the recommendation system could use Œ≥ =
30% to achieve a 93.2% precision and a 64.2% re-
call, or a company could use Œ≥ = 60% for more
reviews that have a relatively high precision.

Efficiency We implemented TremenRank in Java
and ran it on a single PC with the Windows 8.1 64-
bit operation system. With an Intel(R) Core(TM)
i3-3240 (3.40GHz) CPU and 4GB memory, our
program converges within 5 iterations, and on-
ly consumes approximately 700MB of memory

when running steadily. The overall identification
times of the Stock, Car and Herb datasets are 12h,
5min and 18min, respectively. The computation
time increases exponentially with the increase of
the amount of data due to the excessive computa-
tions required to search the indexes, which can be
optimized in future work.

5 Conclusions and Future Work

In this paper, we addressed a new and increasing-
ly important problem in social content analysis in
a challenging scenario: disambiguation of a list
of homogenous entities in short texts using names
only. We proposed a graph-based method called
TremenRank to identify target entities collective-
ly; this method can also hold an arbitrary number
of target entities and documents. The performance
of this method can be further improved via a well-
designed MLD graph. The experimental results
show that the proposed method has a significant
improvement compared to other approaches.

In the future, we are interested in refining the
prior estimation by using the ontology and extend-
ing this work to detect the target entities that are
not in a list while performing the disambiguation
task.

Acknowledgments We‚Äôd like to acknowledge Lei
Hou, Chi Wang and Hongzhao Huang for their im-
pressive discussions on this paper. The work is
supported by 973 Program (No. 2014CB340504),
NSFC-ANR (No. 61261130588), Tsinghua Uni-
versity Initiative Scientific Research Program (No.
20131089256), Science and Technology Support
Program (No. 2014BAK04B00), and THU-NUS
NExT Co-Lab.

662



References
Javier Artiles, Andrew Borthwick, Julio Gonzalo,

Satoshi Sekine, and Enrique AmigoÃÅ. 2010. Weps-3
evaluation campaign: Overview of the web people
search clustering and attribute extraction tasks. In
CLEF (Notebook Papers/LABs/Workshops).

Razvan C Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In EACL, volume 6, pages 9‚Äì16.

Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441‚Äì456. Citeseer.

Zheng Chen and Heng Ji. 2011. Collaborative rank-
ing: A case study on entity linking. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 771‚Äì781. Association
for Computational Linguistics.

Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL, pages 708‚Äì716.

Silviu Cucerzan. 2011. Tac entity linking by perform-
ing full-document entity extraction and disambigua-
tion. In Proceedings of the Text Analysis Confer-
ence, volume 2011.

Jeffrey Dalton and Laura Dietz. 2013. A neighbor-
hood relevance model for entity linking. In Proceed-
ings of the 10th Conference on Open Research Areas
in Information Retrieval, pages 149‚Äì156. LE CEN-
TRE DE HAUTES ETUDES INTERNATIONALES
D‚ÄôINFORMATIQUE DOCUMENTAIRE.

Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings of
the 23rd International Conference on Computation-
al Linguistics, pages 277‚Äì285. Association for Com-
putational Linguistics.

Norberto Fernandez, Jesus A Fisteus, Luis Sanchez,
and Eduardo Martin. 2010. Webtlab: A cooc-
curencebased approach to kbp 2010 entity-linking
task. In Proc. TAC 2010 Workshop.

Yuhang Guo, Wanxiang Che, Ting Liu, and Sheng Li.
2011. A graph-based method for entity linking. In
IJCNLP, pages 1010‚Äì1018. Citeseer.

Weiwei Guo, Hao Li, Heng Ji, and Mona T Diab. 2013.
Linking tweets to news: A framework to enrich short
text data in social media. In ACL (1), pages 239‚Äì
249. Citeseer.

ZoltaÃÅn GyoÃàngyi, Hector Garcia-Molina, and Jan Ped-
ersen. 2004. Combating web spam with trustrank.
In Proceedings of the Thirtieth international con-
ference on Very large data bases-Volume 30, pages
576‚Äì587.

Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945‚Äì
954. Association for Computational Linguistics.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in Infor-
mation Retrieval, pages 765‚Äì774. ACM.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen FuÃàrstenau, Manfred Pinkal, Marc S-
paniol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782‚Äì792. Association for Computational
Linguistics.

Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via re-
lated instances. In Proceedings of the conference on
empirical methods in natural language processing,
pages 118‚Äì128. Association for Computational Lin-
guistics.

Vijay Krishnan and Rashmi Raj. 2006. Web spam de-
tection with anti-trust rank. In AIRWeb, volume 6,
pages 37‚Äì40.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective annota-
tion of wikipedia entities in web text. In Proceed-
ings of the 15th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 457‚Äì466. ACM.

Yang Li, Chi Wang, Fangqiu Han, Jiawei Han, Dan
Roth, and Xifeng Yan. 2013. Mining evidences
for named entity disambiguation. In KDD‚Äô13, pages
1070‚Äì1078.

Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou,
Furu Wei, and Yi Lu. 2013. Entity linking for tweet-
s. In ACL (1), pages 1304‚Äì1311.

Rada Mihalcea and Andras Csomai. 2007. Wiki-
fy!: linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 233‚Äì242. ACM.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39‚Äì41.

David Milne and Ian H Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th ACM
conference on Information and knowledge manage-
ment, pages 509‚Äì518. ACM.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.

663



Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web.

Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume 1,
pages 238‚Äì247. Association for Computational Lin-
guistics.

Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithm-
s for disambiguation to wikipedia. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1375‚Äì1384. Associ-
ation for Computational Linguistics.

Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Linden: linking named entities with knowl-

edge base via semantic knowledge. In Proceedings
of the 21st international conference on World Wide
Web, pages 449‚Äì458. ACM.

Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2013. Linking named entities in tweets with knowl-
edge base via user interest modeling. In KDD, pages
68‚Äì76.

Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Sura-
jit Chaudhuri. 2012. Targeted disambiguation of ad-
hoc, homogeneous sets of named entities. In WWW,
pages 719‚Äì728.

Radford Will, Hachey Ben, Nothman Joel, Honnibal
Matthew, and R.Curran James. 2010. Cmcrc at
tac10: Document-level entity linking with graph-
based re-ranking. In In Proc. Text Analysis Confer-
ence (TAC 2010).

664


