



















































XRCE: Hybrid Classification for Aspect-based Sentiment Analysis


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 838–842,
Dublin, Ireland, August 23-24, 2014.

XRCE: Hybrid Classification for Aspect-based Sentiment Analysis

Caroline Brun, Diana Nicoleta Popa, Claude Roux
Xerox Research Centre Europe

6, chemin de Maupertuis
38240 Meylan, France

{caroline.brun, diana.popa, claude.roux}@xrce.xerox.com

Abstract

In this paper, we present the system we
have developed for the SemEval-2014
Task 4 dedicated to Aspect-Based Senti-
ment Analysis. The system is based on
a robust parser that provides information
to feed different classifiers with linguis-
tic features dedicated to aspect categories
and aspect categories polarity classifica-
tion. We mainly present the work which
has been done on the restaurant domain1

for the four subtasks, aspect term and cat-
egory detection and aspect term and cate-
gory polarity.

1 Introduction

Aspect Based Sentiment Analysis aims at discov-
ering the opinions or sentiments expressed by a
user on the different aspects of a given entity ((Hu
and Liu, 2004); (Liu, 2012)). A wide range of
methods and techniques have been proposed to ad-
dress this task, among which systems that use syn-
tactic dependencies to link source and target of the
opinion, such as in (Kim and Hovy, 2004), (Bloom
et al., 2007), or (Wu et al., 2009). We have devel-
oped a system that belongs to this family, (Brun,
2011), as we believe that syntactic processing of
complex phenomena (negation, comparison, ...)
is a crucial step to perform aspect-based opinion
mining. In this paper, we describe the adaptations
we have made to this system for SemEval, and the
way it is applied to category and polarity classifi-
cation.

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

1We have not performed any domain adapation for the
laptop corpus and only submitted a run for the subtask 1, term
detection.

2 Description of the System

In this section, we describe the different compo-
nents of the system.

2.1 Existing System
In order to tackle the Semeval’14 Task 4, (Pon-
tiki et al., 2014), we used our existing aspect-
based opinion detection system. The opinion de-
tection system we built relies on a robust deep
syntactic parser, (Ait-Mokhtar et al., 2001), as a
fundamental component, from which semantic re-
lations of opinion are calculated. Parsing here
includes tokenization, morpho-syntactic analysis,
tagging which is performed via a combination of
hand-written rules and HMM, Named Entity De-
tection, chunking and finally, extraction of depen-
dency relations between lexical nodes. These re-
lations are labeled with deep syntactic functions.
More precisely, a predicate (verbal or nominal) is
linked with what we call its deep subject (SUBJ-
N), its deep object (OBJ-N), and modifiers. In
addition, the parser calculates more sophisticated
and complex relations using derivational morpho-
logic properties, deep syntactic properties (subject
and object of infinitives in the context of control
verbs), and some limited lexical semantic coding.

Syntactic relations already extracted by a
general dependency grammar, lexical information
about word polarities, sub categorization informa-
tion and syntactic dependencies are all combined
within our robust parser to extract the semantic
relations. The polarity lexicon has been built
using existing resources and also by applying
classification techniques over large corpora, while
the semantic extraction rules are handcrafted, see
(Brun, 2011) and (Brun, 2012) for the complete
description of these different components. The
system outputs a semantic dependency called
SENTIMENT which can be binary, i.e. linking
opinionated terms and their targets, or unary,
i.e. just the polar term in case the target of the

838



opinion hasn’t been detected. For example, when
parsing I was highly disappointed by their service
and food., the systems outputs the following
dependencies:

SUBJ N(disappointed,food)
SUBJ N(disappointed,service)
OBJ N(disappointed,I)
MANNER PRE(disappointed,highly)
SENTIMENT NEGATIVE(disappointed,service)
SENTIMENT NEGATIVE(disappointed,food)

In this system, aspects terms are not explic-
itly extracted, however all non-polar arguments of
the SENTIMENT dependency are potential aspect
terms. Moreover, this system considers only posi-
tive and negative opinions, but does not cover the
neutral and conflict polarities.

2.2 System Adaptation
The opinion detection system described in the
previous section has been adapted for the Se-
mEval2014 Task4, in two ways: some lexical ac-
quisition has been performed in order to detect the
terms of the domain, and some rules have been de-
veloped to detect multi-word terms and to output
semantic dependencies associating their polarity
to terms and categories.

2.2.1 Lexical Enrichment and Term
Detection

As said before, the existing system encodes a rea-
sonable amount of polar vocabulary. However, as
the task implies domain knowledge to detect the
terms, we have first extracted the terms from the
training corpus and encoded their words into our
lexicons, assigning to them the semantic features
food, service, ambiance and price. We have then
extended the list with Wordnet synonyms. To im-
prove coverage, we have also extracted and fil-
tered food term lists from Wikipedia pages and en-
coded them. More precisely, the list of food terms
has been extracted from the Wikipedia ”Food Por-
tal”, from the category ”Lists of foods”2. At the
end of this process, our lexicon has the following
coverage: Polar words: 1265 negative, 1082 posi-
tive and Domain words: 761 food words, 31 price
words, 105 ambiance words, 42 service words.

In order to detect the terms, some local grammar
rules (based on regular expressions) have been de-
veloped taking into account the lexical semantic

2http://en.wikipedia.org/wiki/Category:Lists of foods

information encoded in the previous step. These
rules detect the multi-words terms, e.g. pas-
trami sandwiches, group them under the appropri-
ate syntactic category (noun, verb) and associate
them with the corresponding lexical semantic fea-
ture, food, service, ambiance, price. In addition to
this, in order to prepare the aspect category clas-
sification (c.f. section 2.3.3), a layer of semantic
dependencies has been added to the grammar: If
a domain term is detected in a sentence, a unary
dependency corresponding to its category (FOOD,
SERVICE, PRICE, AMBIANCE) is built.

2.2.2 Grammar Adaptation for Polarity
Detection

The English grammar, which had been previously
developed to detect sentiments, has also been
adapted in order to extract the opinions associated
to the terms and categories detected at the previous
step.

If an aspect term is the second argument of a
SENTIMENT relation, 2 dependencies, one for the
term (OPINION ON TERM) and one for the corre-
sponding category (OPINION ON CATEGORY) are
built. They inherit the polarity (positive or nega-
tive) of the SENTIMENT dependency. If these de-
pendencies target the same term and category and
if they have opposite polarity, they are modified in
order to bear the feature ”conflict”.

Then, if a sentence contains a term and
if no SENTIMENT dependency has been de-
tected, the OPINION ON TERM and OPIN-
ION ON CATEGORY are created with the polarity
”neutral”. Finally, if no terms have been de-
tected in a sentence, there are two cases: (1)
a SENTIMENT dependency has been detected
somewhere in the sentence, the dependency
OPINION ON CATEGORY(anecdote/misc), is
created with the corresponding polarity (positive
or negative); (2) no SENTIMENT dependency
has been detected, the dependency OPIN-
ION ON CATEGORY(anecdote/misc), is created
with polarity ”neutral”.

The dependency OPINION ON TERM links the
terms to their polarities in the sentences and serves
as input for the subtasks 1 and 3.

2.3 Classification
2.3.1 KiF (Knowledge in Frame)
The whole system, training and prediction, has
been implemented in KiF (Knowledge in Frame),
a script language that has been implemented into

839



the very fabric of the rule-based Xerox Incremen-
tal Parser (XIP). KiF offers a very simple way to
hybridize a rule-based parser with machine learn-
ing technique. For instance, a KiF function, which
evaluates a set a features to predict a class, can be
called from a rule, which could then be fired along
the output of that function. KiF is a multi-threaded
programming language, which is available for all
platforms (Windows, Mac OS, Linux). It pro-
vides all the necessary objects (strings, containers
or classes) and many encapsulations of dynamic
libraries from different C programs such as classi-
fiers (liblinear and libsvm), database (SQLite), or
XML (libxml2), which can be loaded on the fly.
All internal XIP linguistic structures are wrapped
up into KiF objects. For example, linguistic fea-
tures are available as maps, which can be modi-
fied and re-injected into their own syntactic nodes.
The language syntax is a mix between Java (types
are static) and Python (in the way containers are
handled), but provides many implicit conversions
to avoid code overloading with too many func-
tions. KiF allows for an efficient integration of
all aspects of linguistic analysis into a very sim-
ple framework, where XML documents can be an-
alyzed and modified both with linguistic parsing
and classifiers into a few hundred lines of code.

2.3.2 General Methodology
We focus on four main tasks: detecting the as-
pect terms and aspect categories and their corre-
sponding polarities. While the detection of aspect
terms and their corresponding polarities occurs at
the grammar level, for the detection of aspect cate-
gories and their corresponding polarities we make
use of the liblinear library (Fan et al., 2008) to
train our models. We train one classifier for detect-
ing the categories and further, for each category
we train a separate classifier for detecting the po-
larities corresponding to that particular category.
For both settings, we use 10-fold cross-validation.
The two modules for aspect category classification
and aspect category polarity classification are de-
scribed in details further.

2.3.3 Aspect Category Classification
The sentence classification module is used to as-
sign aspect categories to sentences. For each sen-
tence, the module takes as input features the bag
of words in the sentence as well as the information
provided by the syntactic parser. The output con-
sists of a list of categories corresponding to each

sentence.
In the pre-processing stage stop words are re-

moved (determinants, conjunctions). Further, we
use the L2-regularized logistic regression solver
from the liblinear library to train a model. The
features considered are the word lemmas from the
sentence along with their frequencies (term fre-
quency). Apart from this, the information pro-
vided by the rule based component is also taken
into account to increase the term frequency for
terms belonging to the detected categories.

Such information can consist of: dependencies
denoting the category to which a detected aspect
term belongs (Food, Service, Price, Ambiance)
and dependencies denoting the opinions on the
detected aspect terms and categories (OPIN-
ION ON CATEGORY, OPINION ON TERM). For
example for the following sentence: “Fab-
ulous service, fantastic food, and a chilled
out atmosphere and environment”, the salient
dependencies produced by the syntactic parser are:

FOOD(food), AMBIANCE(atmosphere),
SERVICE(service), AMBIANCE(environment),
OPINION ON CATEGORY POSITIVE(food),
OPINION ON CATEGORY POSITIVE(service),
OPINION ON CATEGORY POSITIVE(ambiance),
OPINION ON TERM POSITIVE(food),
OPINION ON TERM POSITIVE(service),
OPINION ON TERM POSITIVE(atmosphere).

This yields the following features having an
increase in their frequencies: food (+3), service
(+3), atmosphere (+2), environment (+1), am-
biance (+1).

Once the logistic regression is performed, each
category is predicted with a certain probability.
Since in one sentence there may be entities that re-
fer to different categories, we set a threshold with
respect to the probability values to be taken into
account. We have tried different approaches to set
this threshold. The best results on the training and
trial data were obtained with a threshold of 0.25,
(i.e. we kept only the categories with a probability
over 0.25).

2.3.4 Aspect Category Polarity Classification
The approach to predict the polarity for each cate-
gory is similar to the one predicting the categories
for each sentence, with some differences as will
be further detailed. The classification uses for fea-
tures, the bag of words (term frequency), but also

840



the polarity provided by XIP by the following de-
pendencies: OPINION ON CATEGORY and SEN-
TIMENT. Whenever these dependencies are de-
tected, a feature is added to the classification of
the form polarity category. Thus for the previ-
ous example sentence: Fabulous service, fantastic
food, and a chilled out atmosphere and environ-
ment, the additional dependencies considered are
SENTIMENT POSITIVE(atmosphere, chilled out),
SENTIMENT POSITIVE(food, fantastic), SENTI-
MENT POSITIVE(service, Fabulous). After map-
ping back the terms to their corresponding cate-
gories, the added features are: positive ambiance,
positive food and positive service. Since the de-
pendency OPINION ON CATEGORY is also de-
tected by the parser for these categories, each
of the above mentioned features will have a fre-
quency of 2 in this case. Moreover, the polarity
alone is also added as a feature. The training is
performed using the L2-regularized L2-loss sup-
port vector classification solver from the same li-
brary (liblinear) and a model is generated for each
category. Thus, depending on the categories de-
tected within a certain sentence, the correspond-
ing model is used to make the prediction regarding
their polarities. The classifier’s output represents
the predicted polarity for one given category.

3 Evaluation

The corpus used for evaluating the system con-
tains 800 sentences, 1134 aspect term occurrences,
1025 aspect category occurrences, 5 different as-
pect categories and 555 distinct aspect terms. All
these belong to the restaurant domain.

3.1 Terms and Category Detection

When evaluating aspect terms and aspect cate-
gories detection, three measures were taken into
account: precision, recall and the f1-measure.

For both aspect term extraction and aspect cat-
egory detection, the baseline methodologies are
presented in (Pontiki et al., 2014). Table 1 shows
the results obtained using our approach as com-
pared to the baseline for aspect term detection,
whereas Table 2 outlines the results regarding as-
pect category detection in terms of the previously
mentioned measures.

Furthermore, it is interesting to notice the in-
crease in performance obtained by combining the
bag-of-words features with the output of the parser
as opposed to just using the bag-of words. These

Method Precision Recall F-Measure
Baseline 0.627329 0.376866 0.470862
XRCE 0.862453 0.818342 0.839818

Table 1: Aspect term detection.

Method Precision Recall F-Measure
Baseline 0.637500 0.483412 0.549865

BOW 0.77337 0.799024 0.785988
XRCE 0.832335 0.813658 0.822890

Table 2: Aspect category detection.

differences are outlined for aspect category detec-
tion in Table 2, where BOW denotes the system
using the same settings, but just the bag-of-words
features and XRCE denotes the submitted system
where the bag-of-words features are augmented
with parser output features.

For both tasks of aspect term and aspect cate-
gory detection, our system clearly outperforms the
baseline, resulting in being ranked among the first
3 in the competition for the restaurant corpus.

3.2 Terms and Category Polarity Detection

Similarly, Table 3 shows the results in terms of
accuracy on aspect term polarity detection and on
aspect category polarity detection. Here, baseline
methodologies are similar to the ones used for as-
pect category detection and also described in (Pon-
tiki et al., 2014). Again, our system ranks high in
the competition, achieving an overall accuracy of
0.77 for aspect term polarity detection and 0.78 for
aspect category polarity detection. Furthermore, a
comparison is also made between the current sys-
tem and one that, using the same settings, would
not take into account the features provided by the
parser (BOW). The results emphasize the impor-
tance of using the merged version.

Method Task Accuracy
Baseline Term polarity 0.552239
XRCE Term polarity 0.776895

Baseline Category polarity 0.563981
BOW Category polarity 0.681951
XRCE Category polarity 0.781463

Table 3: Aspect term and aspect category polarity.

841



Label Precision Recall F-measure
conflict NaN 0 NaN
negative 0.7857 0.7296 0.7566
neutral 0.5833 0.3214 0.4145
positive 0.7998 0.9272 0.8588

Table 4: Aspect term polarity (2).

Label Precision Recall F-measure
conflict 0.5333 0.1538 0.2388
negative 0.726 0.6802 0.7023
neutral 0.5119 0.4574 0.4831
positive 0.8343 0.9117 0.8713

Table 5: Aspect category polarity (2).

3.3 Error Analysis

The results obtained with our system are unar-
guably competitive, but some remarks can be
made regarding the most frequent causes of er-
rors. In the task of aspect category classification,
the choice of the threshold (0.25) may have con-
stituted a factor impacting the performance. In
the task of aspect term detection, the lexical cov-
erage is one of the factors to explain the difference
in performance between training/trial data and test
data.

Table 4 contains the results obtained in terms
of precision, recall and F-measure for each of the
possible polarities for terms (positive, negative,
neutral and conflict) and similarly does Table 5
for category polarities. In both cases we notice a
clear decrease for these measures when predicting
the conflict and neutral classes, with a higher de-
crease in the case of aspect term polarity detection.
This can be explained by the fact that the syntactic
parser was primarily customized to detect the neg-
ative and positive labels. This obviously had an
impact on the final results as the information from
the parser constituted some of the input features
for the classification.

4 Conclusion

The combination of a symbolic parser, customized
with specialized lexicons, with SVM classifiers
proved to be an interesting platform to implement
a category/polarity detection system. The sym-
bolic parser on the one hand provides a versatile
architecture to add lexical and multi-words infor-
mation, augmented with specific rules, in order to
feed classifiers with high quality features. How-

ever, some work will be needed to improve per-
formances on the neutral and conflict polarities,
which rely less on specific words, than on a more
global interpretation of the content.

Acknowledgements

We would like to thank the Semeval task 4
organizers, as well as our colleague, Vassilina
Nikoulina, for her help on this project.

References
Salah Ait-Mokhtar, Jean-Pierre Chanod, and Claude

Roux. 2001. A multi-input dependency parser. In
IWPT.

Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In In HLT-
NAACL 2007, pages 308–315.

Caroline Brun. 2011. Detecting opinions using deep
syntactic analysis. In RANLP, pages 392–398.

Caroline Brun. 2012. Learning opinionated patterns
for contextual opinion detection. In COLING, pages
165–174.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In KDD, pages 168–177.

Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, COLING ’04, Stroudsburg, PA, USA.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In International
Workshop on Semantic Evaluation (SemEval).

Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533–1541. ACL.

842


