




































Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1909â€“1919,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

1909

Who Is Speaking to Whom? Learning to Identify Utterance Addressee
in Multi-Party Conversations

Ran Le13âˆ—, Wenpeng Hu2âˆ—, Mingyue Shang1âˆ—, Zhenjun You2âˆ—,
Lidong Bing 4, Dongyan Zhao13 and Rui Yan13â€ 

1 Center for Data Science, AAIS, Peking University, Beijing, China
2 School of Mathematical Sciences, Peking University, Beijing, China

3 Wangxuan Institute of Computer Technology, Peking University, Beijing, China,
4 R&D Center Singapore, Machine Intelligence Technology, Alibaba DAMO Academy

{leran.pku}@gmail.com {l.bing}@alibaba-inc.com
{wenpeng.hu,shangmy,youzhenjunpku,zhaody,ruiyan}@pku.edu.cn

Abstract
Previous research on dialogue systems gen-
erally focuses on the conversation between
two participants, yet multi-party conversa-
tions which involve more than two partici-
pants within one session bring up a more com-
plicated but realistic scenario. In real multi-
party conversations, we can observe who is
speaking, but the addressee information is not
always explicit. In this paper, we aim to
tackle the challenge of identifying all the miss-
ing addressees in a conversation session. To
this end, we introduce a novel who-to-whom
(W2W) model which models users and ut-
terances in the session jointly in an interac-
tive way. We conduct experiments on the
benchmark Ubuntu Multi-Party Conversation
Corpus and the experimental results demon-
strate that our model outperforms baselines
with consistent improvements.

1 Introduction

As an essential aspect of artificial intelligence, dia-
logue systems have attracted extensive attention in
recent studies (Vinyals and Le, 2015; Serban et al.,
2016). Researchers have paid great efforts to un-
derstand conversations between two participants,
either single-turn (Li et al., 2016a; Shang et al.,
2015; Vinyals and Le, 2015) or multi-turn (Zhou
et al., 2016; Yan et al., 2016; Tao et al., 2019a,b),
and achieved encouraging results. A more gen-
eral and challenging scenario is that a conversa-
tion may involve more than two interlocutors con-
versing among each other (Uthus and Aha, 2013;
Hu et al., 2019), which is known as multi-party
conversation. Ubuntu Internet Relay Chat chan-
nel (IRC) is a multi-party conversation scenario
as shown in Table 1. Generally, each utterance
is associated with a speaker and one or more ad-
dressees in the conversation. Such a characteristic

âˆ—Equal contribution.
â€ Corresponding author.

Table 1: An example of the multi-party conversation in
the IRC dataset. Not all the addressees are specified.

Speaker Utterance Addressee
User 1 â€Good point, tmux is the thing I miss.â€ â€“
User 1 â€Cool thanks for ur help.â€ @User 4 User 4
User 2 â€Ahha, you r using something like cpanel.â€ â€“
User 3 â€Yeah 1.4.0 exactly.â€ @User 2 User 2
User 4 â€my pleasure :)â€ â€“

leads to complex speaker-addressee interactions.
As a result, the speaker and addressee roles as-
sociated with utterances are constantly changing
among multiple users across different turns. Such
speaker and addressee information could be essen-
tial in many multi-party conversation scenarios in-
cluding group meeting, debating and forum dis-
cussion. Therefore, compared to two-party con-
versations, a unique issue of multi-party conversa-
tions is to understand who is speaking to whom.

In real scenarios of multi-party conversations,
an interesting phenomenon is that the speakers do
not usually designate an addressee explicitly. This
phenomenon also accords with our statistic analy-
sis on the IRC dataset. We found that around 66%
utterances missing explicit addressee information.
That means when modeling such multi-party con-
versations, one may have to guess who is speak-
ing to whom in order to understand the utterance
correspondence as well as the stream structure of
multi-party conversations.

Given a multi-party conversation where part
of the addressees are unknown, previous work
mainly focuses on predicting the addressee of only
the last utterance. Ouchi and Tsuboi (2016) pro-
posed to scan the conversation session and track
the speakerâ€™s state based on the utterance content
at each step. On this basis, Zhang et al. (2017)
introduced a speaker interaction model that tracks
all usersâ€™ states according to their roles in the ses-
sion. They both fused the representations of the
last speaker and utterance as a query, and a match-



1910

ing network is utilized to calculate the matching
degree between the query and each listener. The
listener with the highest matching score is selected
as the predicted addressee.

However, in practice, it is more helpful to pre-
dict all the missing addressees rather than only
the last one in understanding the whole conver-
sation. And it also benefits for both building a
group-based chatbot and clustering users based on
what they have said. Therefore, we propose a
new task of identifying the addressees of all the
missing utterances given a multi-party conversa-
tion session where part of the addressees are un-
specified. To this end, we propose a novel Who-to-
Whom (W2W) model which jointly models users
and utterances in the multi-party conversation and
predicts all the missing addressees in a uniform
framework.1 Our contributions are as follows:
â€¢ We introduce a new task of understanding

who speaks to whom given an entire conversation
session as well as a benchmark system.
â€¢ To capture the correlation within users and ut-

terances in multi-party conversations, we propose
an interactive representation learning approach to
jointly learn the representations of users and utter-
ances and enhance them mutually.
â€¢ The proposed approach (W2W) considers

both previous and subsequent information in the
session while incorporating the correlation with
users and utterances. For conversations with com-
plex structures, W2W models them in a uniform
way and could handle any kind of occasion even
when all the addressee information is missing.

2 Related Work

In this section, we briefly review recent works and
progresses on multi-party conversations.

Multi-party conversations, as a general case of
multi-turn conversations (Li et al., 2017, 2016c;
Yan et al., 2016; Serban et al., 2016) involve more
than two participants. In addition to the represen-
tation of learning for utterances, another key issue
is to model multiple participants in the conversa-
tions. It is intuitive to introduce multiple user em-
beddings for multi-party conversations, either as
persona-dependent embeddings (Li et al., 2016b),
or as persona-independent embeddings (Ouchi and
Tsuboi, 2016; Zhang et al., 2017; Meng et al.,
2017). Recently, some researchers utilized usersâ€™

1To make the model practical in learning, we assume that
one utterance is associated with only one addressee.

information based on different roles in conversa-
tions, such as senders and recipients (Chen et al.,
2017; Chi et al., 2017; Luan et al., 2016).

In multi-party conversations, identifying the re-
lationship among users is also an important task.
It can be categorized into two topics, 1) predicting
who will be the next speaker (Meng et al., 2017)
and 2) who is the addressee (Ouchi and Tsuboi,
2016; Zhang et al., 2017). For the first topic, Meng
et al. (2017) investigated a temporal-based and a
content-based method to jointly model the users
and context. For the second topic, which is closely
related to ours, Ouchi and Tsuboi (2016) proposed
to predict the addressee and utterance given a con-
text with all available information. Later, Zhang
et al. (2017) proposed a speaker-interactive model,
which takes usersâ€™ role information into consider-
ation and implements a role-sensitively state track-
ing process.

In our task, the addressee identification prob-
lem is quite different from (Ouchi and Tsuboi,
2016) and (Zhang et al., 2017). Both of their stud-
ies aimed to make predictions on whom the last
speaker addresses to. While in this paper, we fo-
cus on the whole session and aim to identify all the
missing addressees. By contrast, our task is a more
challenging scenario since it relies on the correla-
tion within all users and utterances to identify the
speaker-addressee structure of the entire session.

3 Overview

3.1 Problem Formulation

Given an entire multi-party conversation S with
length T , the sequence of utterances in it is de-
fined as {ut}Tt=1. Each utterance is associated with
a speaker aSPRt and an addressee a

ADR
t . a

SPR
t is

observable across the entire session while aADRt
is mostly unspecified as shown in Table 1. Our
task is to identify the addressees for all utterances
within the conversation session. The predicted ad-
dressee is denoted as aÌ‚ADRt . Formally, we have
following formulations:

QUERY : {(aSPRt , ut)}Tt=1
PREDICTIONS : {aÌ‚ADRt }Tt=1

(1)

Let A(S) denote the user set in the session S, thus
A(S)\{aSPRt } denotes the listeners at the t-th turn
(aLSRjt denotes the j-th listener). The listeners are
also referred as candidate addressees for each turn
and the identified addressee aÌ‚ADRt should be one



1911

Candidate Addressees

Candidate 4

QUERY
Score = 0.90 

Speaker

Utterance

Matching
Prediction by Ranking

Interactive Representation Learning

Score = 0.85 

Score = 0.76 

Score = 0.70 

Candidate 3

Candidate 2

Candidate 1

Addressee
= Candidate 1

Figure 1: The system architecture of W2W model.

of them.2

3.2 Architecture Overview of W2W Model

Figure 1 illustrates the overview picture of the pro-
posed W2W model which consists of a represen-
tation learning module and a matching module.

Concretely, the representation learning module
is designed to jointly learn the representation of
users and utterances in an interactive way after ini-
tializing them separately. The representations of
users (also denoted as user states) and utterance
embeddings are mutually enhanced. With the rep-
resentations of users and utterances, a network is
utilized to fuse them up into a query representa-
tion. In this way, we jointly capture who is speak-
ing what at each step.

After the representations of users and utterances
are learned, we feed them into a matching mod-
ule. In this module, a matching network is learned
to score the matching degrees between the query
and each candidate. According to the matching
scores, the model ranks all addressee candidates in
A(S)\{aSPRt } and selects the one with the high-
est matching score as the identified addressee. For
each utterance in the multi-party conversation, we
repeat the above steps until the addressees of all
utterances are identified.

4 Our W2W Model

In this section, we first describe each part of the
W2W model in details: (1) Initialization of utter-
ance and user representations; (2) Interactive rep-
resentation learning of users and utterances; (3)
Matching procedure for identifying the addressee.
We finally describe the training procedure of the
W2W model.

2In this paper, we denote vectors with bold lower-case
(like ui) and matrices with bold upper-case like (U and W ).

4.1 Initialization

W2W models utterance and user embeddings sep-
arately before interactive representation learning
and gets the representation of each utterance and
user as initialization.

4.1.1 Utterance Initialization Encoder

Suppose that in a conversation session S with T
utterances denoted as {u1, u2, . . . , uT }. An ut-
terance ut that contains n tokens is denoted as
{w1, w2, . . . , wn}, where {wi} is word embed-
dings3 of the i-th token. We first utilize a word
level bi-directional RNN with Gated Recurrent
Units (GRUs) (Cho et al., 2014) to encode each
utterance and take the concatenation of the hid-
den states of the last step from both sides as the
sentence embedding. Then, a sentence level bi-
directional GRU is applied with each sentence em-
bedding as input to obtain the global context of the
session. The utterance representation ut is repre-
sented by the concatenation of hidden states from
both sides at t-th time step. 4

4.1.2 Position-Based User Initialization

In multi-party conversation, position information
of different participants in the session is crucial in
the addressee identification task. For example, a
speaker is more likely to address his direct pre-
ceding or subsequent speaker. On this basis, we
define the initialization user matrix A(0) based on
the speaking order of users in the session (Ouchi
and Tsuboi, 2016). Concretely, all users in a ses-
sion are sorted in a descending order according to
the first time when they speak, and the i-th user is
assigned with the i-th row of A(0) as ai(0). The user
matrix A(0) is trained as parameters along with
other weight matrices in the neural network.

Users of the same order in different sessions
share the same initialization user embedding. Note
that the user representations are independent of
each personality (unique user). Such strategy
guarantees the initialization user embeddings to
carry position information as well as handle new
users unseen in training data during addressee
identification.

3 We use GloVe(Pennington et al., 2014), but it can be any
word embeddings(Mikolov et al., 2013; Hu et al., 2016).

4Such a hierarchical framework (Serban et al., 2016) takes
into account the context of all previous and future sentences
in the whole session, thus enables the model to learn a strong
representation.



1912

a(ğŸ)
ğŸ a(ğŸ)

ğŸ

a(ğŸ)
ğŸ

a(ğŸ)
ğŸ‘

a(ğŸ)
ğŸ

a(ğŸ)
ğŸ‘

a(ğŸ)
ğŸ

a(ğŸ)
ğŸ

a(ğŸ)
ğŸ‘ a(ğŸ‘)

ğŸ‘

a(ğŸ‘)
ğŸ

a(ğŸ‘)
ğŸ

(ğ’–1, a 0
ğ‘ƒğ´ğ·ğ‘…) (ğ’–2

ğ‘ƒ1 , a 1
ğ‘ƒğ´ğ·ğ‘…1) (ğ’–3

ğ‘ƒ1 , a 2
ğ‘ƒğ´ğ·ğ‘…1)

(ğ’–2, a 1
ğ‘ƒğ´ğ·ğ‘…) (ğ’–3

ğ‘ƒ2 , a 2
ğ‘ƒğ´ğ·ğ‘…2)(ğ’–1

ğ‘ƒ2 , a 0
ğ‘ƒğ´ğ·ğ‘…2)

(ğ’–1
ğ‘ƒ3 , a 0

ğ‘ƒğ´ğ·ğ‘…3) (ğ’–2
ğ‘ƒ3 , a 1

ğ‘ƒğ´ğ·ğ‘…3) (ğ’–3, a 2
ğ‘ƒğ´ğ·ğ‘…)

à·¥ğ’–ğŸ à·¥ğ’–ğŸ à·¥ğ’–ğŸ‘

(ğ’–1, ğ‘¨(0)) (ğ’–2, ğ‘¨(1)) (ğ’–3, ğ‘¨(2))

à·¥ğ’–ğŸ à·¥ğ’–ğŸ à·¥ğ’–ğŸ‘

BackwardForward

ağŸ

ağŸ

ağŸ‘

ağŸ

ağŸ

ağŸ‘

ğ‘ˆğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘ğ‘’
ğ‘…ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘ğ‘–ğ‘œğ‘›

ğ‘¨(0)

ğ‘ˆğ‘ ğ‘’ğ‘Ÿ
ğ‘…ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘ğ‘–ğ‘œğ‘›

User Utterance

ğ‘1 ğ‘¢1

ğ‘2 ğ‘¢2

ğ‘3 ğ‘¢3

SGRU

LGRU

UGRU

Figure 2: Interactive representation learning in W2W: At each utterance turn, SGRU tracks the speakerâ€™s state,
LGRU tracks listenersâ€™ states and UGRU fuses usersâ€™ states into utterances. W2W scans the conversation session
from two directions and the representation of each user and utterance is the concatenation of both sides.

4.2 Interactive Representation Learning

To better capture who speaks what at each turn
through the whole session, we propose to inter-
actively learn the representation of utterances and
users. Different from prior studies (Ouchi and
Tsuboi, 2016; Zhang et al., 2017) which only track
the usersâ€™ states but neglecting the usersâ€™ impact
on utterances. We propose the W2W model which
learns user and utterance representations interac-
tively by tracking usersâ€™ states with utterance em-
beddings as well as fusing usersâ€™ states into the
utterance embeddings.

4.2.1 Users Representation Learning
Role-sensitive User State Tracking. Suggested
by (Zhang et al., 2017), an utterance could have
different degrees of impact on the states of the
corresponding speaker and listeners. In order to
capture the usersâ€™ role information, we utilize two
kinds of GRU-based cells represented as Speaker-
GRU (SGRU) and Listener-GRU (LGRU) to track
the states of the speaker and listeners respectively
at each turn of the session.5 At the t-th tran-
sition step, the SGRU tracks the speaker rep-
resentation aSPR(t) , from the former state of him
aSPR(tâˆ’1), the utterance representation ut, as well as a
pseudo addressee representation aPADR(tâˆ’1) calculated
via PAM (Person Attention Mechanism) which is

5We denote a user embedding tracked until tth time step
as a(t), with aSPR(t) as the representation of the speaker at tth
turn and aLSRj(t) as the representation of the jth listener at tth
turn.

a weighted sum of all the listenersâ€™ representa-
tions. Details on PAM will be elaborated in the
next part. The state tracking procedure for the i-
th step is formulated as Eq (2). The main idea of
SGRU is to incorporate two reset gates, each of
which controls the information fusion from the lis-
teners and speaker respectively, denoted as ri and
pi. W , U and V are learnable parameters.

ri = Ïƒ(Wrui +UraSPR(iâˆ’1) +Vra
PADR
(iâˆ’1) )

pi = Ïƒ(Wpui +Upa
SPR
(iâˆ’1) +Vpa

PADR
(iâˆ’1) )

zi = Ïƒ(Wzui +UzaSPR(iâˆ’1) +Vza
PADR
(iâˆ’1) )

aÌƒSPR(i) =tanh(Wui +U(ri ï¿½ aSPR(iâˆ’1)) + V(pi ï¿½ a
PADR
(iâˆ’1) ))

aSPR(i) =zi ï¿½ aSPR(iâˆ’1) + (1âˆ’ zi)ï¿½ aÌƒSPR(i)
(2)

Symmetrically, LGRU incorporates the embed-
dings of a certain listener as well as a pseudo
speaker and a pseudo utterance representation
(also calculated via PAM) as inputs and tracks
the state of each listener. SGRU and LGRU have
symmetric updating functions as Eq (2) except for
the difference on pseudo representation incorpo-
rated in the cell.6 The parameters of SGRU and
LGRU are not shared, which guarantees W2W to
learn role-dependent features in usersâ€™ state track-
ing procedure. The whole structure of SGRU and
LGRU are illustrated in Figure 3.
Person Attention Mechanism. We propose a per-
son attention mechanism (PAM) (Eq (3) to model

6SGRU incorporates pseudo addressee aPADR(iâˆ’1) for speaker
state tracking. LGRU incorporates the pseudo speaker
aPSPRj(iâˆ’1) and pseudo utterance u

j
i to track each jth listener



1913

ğ’–ğ’Š

a(ğ’Šâˆ’ğŸ)
ğ‘ºğ‘·ğ‘¹

a(ğ’Šâˆ’ğŸ)
ğ‘³ğ‘ºğ‘¹ a(ğ’Š+1)

ğ‘³ğ‘ºğ‘¹

ğ’“ğ’”

ğ’‘ğ’”

ğ’‘ğ‘³

ğ’–ğ’Š+ğŸ

a(ğ’Š)
ğ‘ºğ‘·ğ‘¹

à·¤a(ğ’Š)
ğ‘³ğ‘ºğ‘¹

à·¤a(ğ’Šâˆ’ğŸ)
ğ‘ºğ‘·ğ‘¹

a(ğ’Š)
ğ‘³ğ‘ºğ‘¹

ğ’›ğ’”

ğ’“ğ‘³

ğ’›ğ‘³

SGRU

LGRU

SGRU

LGRU

a(ğ’Š+ğŸ)
ğ‘ºğ‘·ğ‘¹

Turn ğ’Š-1 Turn ğ’Š Turn ğ’Š+1

Figure 3: Structures of SGRU and LGRU.

the state tracking process exactly. Each element
Î²ji measures how likely the model estimates the
j-th listener to be the addressee for the i-th turn
based on the user representations tracked until turn
i. Wp is the parameter.

Î²ji = Ïƒ(a
LSRj
(iâˆ’1)Wp[a

SPR
(iâˆ’1); ui]

T
) (3)

Then for each turn i, a pseudo addressee aPADR(iâˆ’1) is
generated as the weighted sum of all listener rep-
resentations tracked until step i as Eq (4). Intu-
itively, a listener with a higher matching score is
more likely to be the addressee at the current step.
The pseudo addressee aPADR(iâˆ’1) is incorporated into
the state tracking of the speaker as Eq (2).

aPADR(iâˆ’1) =

âˆ‘
j Î²

j
i Â· a

LSRj
(iâˆ’1)âˆ‘

j Î²
j
i

(4)

aPSPRj(iâˆ’1) = Î²
j
i Â· a

SPRj
(iâˆ’1) (5)

uPji = Î²
j
i Â· ui (6)

Symmetrically, the pseudo speaker aPSPRj(iâˆ’1) and

pseudo utterance uPji are generated through Eq (5)
and Eq (6) for each listener j at the i-th turn of the
conversation.

4.2.2 Utterances Representation Learning
We design a UGRU (Utterance-GRU) cell7, which
has the same structure as SGRU/LGRU to fuse
the utterance embedding, current speaker embed-
ding and the user-summary vector into an en-
hanced utterance embedding. The user matrix
initialized with A(0) (as described in 4.2.1) and
tracked until step t âˆ’ 1 is denoted as A(tâˆ’1). The

7Note that although UGRU has the same structure as
SGRU/LGRU, it acts on each utterance for only once instead
of recurrently tracking.

Input: Initial representations of utterances {ut}Tt=1
Initial user matrix A(0)

1 for i = 0; i < T ; i++ do
2 Calculate current matching scores through PAM

Eq (3);
3 Generate pseudo embeddings using Eq (4,5,6);
4 Track the speaker state through SGRU using

Eq (2);
5 Track each listenerâ€™s state with LGRU using Eq (2);
6 Fuse usersâ€™ information into utterance

representation using Eq (7) ;
7 end
8 return 1) User matrix of the last turn

âˆ’â†’
A(T );

9 2) Enhanced utterance embeddings {âˆ’â†’Ìƒu t}
T

t=1.

Algorithm 1: Interactive Representation
Learning Algorithm (Forward Pass).

user-summary vector is calculated through max-
pooling over users on A(tâˆ’1) as a summary of all
usersâ€™ current states.

hs = Max-Pool(A(tâˆ’1))

uÌƒt = UGRU(ut, aSPR(tâˆ’1), hs)
(7)

4.2.3 Forward-and-Backward Scanning.
Considering that the addressee of an utterance can
be the speaker of the preceding utterances or the
subsequent ones, it is important to capture the
dependency from both sides for users and utter-
ances. We propose a forward-and-backward scan-
ning schema to enhance the interactive represen-
tation learning. For forward pass, W2W model
outputs the forward user matrix of the last time
step, denoted as

âˆ’â†’
A(T ) as well as all the forward-

enhanced utterance embeddings {âˆ’â†’Ìƒu t}
T

t=1 as illus-
trated in Algorithm 1. The backward pass initial-
izes users and utterances in the same way as the
forward pass and scans the conversation session
in the reversed order. Representations from both
sides are concatenated correspondingly as the fi-
nal representation as Eq (8).

uÌƒi = [
âˆ’â†’Ìƒ
ui;
â†âˆ’Ìƒ
ui]; aj = [

âˆ’âˆ’â†’
aj(T);

â†âˆ’âˆ’
aj(T)] (8)

4.3 Matching

Matching Network. We first fuse the speaker em-
bedding and the utterance embedding into a query
representation as q, then measures the embedding
similarity s between the query and each listener:

q = tanh(WsaSPR + WuuÌƒ)

s = Ïƒ(aLSRWmqT)
(9)



1914

Table 2: Data statistics: sample size of datasets with
different session lengths (i.e., 5, 10, 15) from the
Ubuntu dataset (Ouchi and Tsuboi, 2016).

Dataset Train Dev Test

Len-5 461,120 28,570 32,668
Len-10 495,226 30,974 35,638
Len-15 489,812 30,815 35,385

where Ws,Wu,Wm denote weight matrices. For
simplicity, we use a short-handedMatch(.) to de-
note the Equation (9) when there is no ambiguity.
Addressee Identification. For each turn in the
session, we score the matching degree between the
query and each listener and select the best matched
aÌ‚ADRi as the addressee prediction as Eq (10). s

j
i

denotes the matching score between the j-th lis-
tener aLSRj and the query of the i-th turn. For the
entire conversation, we repeat the above steps until
the addressee for each utterance is identified.

sji = Match(a
SPR
i , a

LSRj
i , uÌƒi)

aÌ‚ADRi = argmaxj({s
j
i})

(10)

4.4 Learning

We utilize the cross-entropy loss to train our model
(Ouchi and Tsuboi, 2016). The objective is to min-
imize the loss as follows:

loss = âˆ’
âˆ‘
k

âˆ‘
i

(log(s+i ) + log (1âˆ’ s
âˆ’
i )) +

Î»

2
â€–Î¸â€–2

(11)

Each subscript k denotes a session, and subscript
i is taken from the utterances that have ground
truths of addressee information. s+i denotes the
matching score between the query and the ground
truth addressee, sâˆ’i denotes the score of the neg-
ative matching, where the candidate addressee is
negatively sampled. All parameters in our W2W
model are jointly trained via Back-Propagation
(BP) (Rumelhart et al., 1986).

5 Experimental Setups

Dataset. We run experiments using the bench-
mark Ubuntu dataset released by Ouchi and
Tsuboi (2016). The corpus consists of a huge
amount of records including response utterances,
user IDs and their posting time. We organize the
dataset as samples of conversation sessions.

We filter out the sessions without a single ad-
dressee ground truth, which means no label is
available in these sessions. We also filter out

session samples with one or more blank utter-
ance. Moreover, we separate the conversation ses-
sions into three categories according to the session
length. Len-5 indicates the sessions with 5 turns
and it is similar for Len-10 and Len-15. Such
a splitting strategy is adopted in related studies
as (Ouchi and Tsuboi, 2016; Zhang et al., 2017).
The dataset is split into train-dev-test sets and the
statistics are shown in Table 2.
Comparison Methods. We utilize several al-
gorithms including heuristic and state-of-the-art
methods as baselines. As there is no existing
method that can perform the new task, we have
to adapt baselines below into our scenario.
â€¢ Preceding: The addressee is designated as the

preceding speaker of the current speaker.
â€¢ Subsequent: The addressee is designated as

the next speaker.
â€¢ Dynamic RNN (DRNN): The model is origi-

nally designed to predict the addressee of the last
utterance given the whole context available (Ouchi
and Tsuboi, 2016). We adapt it to our scenario
which is to identify addressees for all utterances
in the conversation session. Concretely, the repre-
sentations of users and context are learned in the
same way as DRNN. While during the matching
procedure, the representations of speaker and con-
text are utilized to calculate the matching degree
with candidate addressees at each turn.
â€¢ Speaker Interactive RNN (SIRNN): SIRNN

is an extension model on DRNN, which is more
interaction-sensitive (Zhang et al., 2017). Since
all addressee information is totally unknown in
our scenario, we also adapt the model with only
speaker-role and observer-role into this situation.
User states are tracked recurrently according to
their roles at each turn, i.e. two distinct net-
works (IGRUS and IGRUO) are utilized to track
the status of the speaker and observers at each
turn. Since there is no addressee-role observable
through the session, we also make some adaption
on the updating cell here. At each turn, IGRUS

updates the speaker embedding from the previous
speaker embedding and the utterance embedding,
IGRUO updates the observer embedding from the
previous observer embedding and the utterance
embedding. During matching procedure, we make
the prediction on each turn instead of only predict-
ing the addressee for the last turn.
Implementation and Parameters. For fair com-
parison, we choose the hyper-parameters spec-



1915

Table 3: Addressee identification results (in %) on datasets (Len-5, Len-10 and Len-15) where ? denotes p-value
< 0.01 in the significance test against all the baselines.

Len-5 Len-10 Len-15
Model p@1 p@2 p@3 Acc. p@1 p@2 p@3 Acc. p@1 p@2 p@3 Acc.
Preceding 63.50 90.05 98.83 40.46 56.84 80.15 91.86 21.06 54.97 77.19 88.75 13.08
Subsequent 61.03 88.86 98.54 40.25 54.57 73.60 87.26 20.26 53.07 69.85 81.93 12.79
DRNN 72.75 93.21 99.24 58.18 65.58 85.85 94.92 34.47 62.60 82.68 92.14 22.58
SIRNN 75.98 94.49 99.39 62.06 70.88 89.14 96.10 40.66 68.13 85.82 93.52 28.05
W2W 77.55? 95.11? 99.57 63.81? 73.52? 90.33? 96.64 44.14? 73.42? 89.44? 95.51? 34.23?

Table 4: Consistency comparison between human in-
ference and model predictions on overlapping rate (%).
? denotes p-value< 0.01 in the significance test against
all the baselines.

Model Len-5 Len-10 Len-15
DRNN 75.60 67.54 63.06
SIRNN 78.94 71.59 67.22
W2W 80.86? 74.05? 71.14?

ified by Ouchi and Tsuboi (2016) and Zhang
et al. (2017). We represent the words with 300-
dimensional GloVe vectors, which are fixed dur-
ing training. The dimension of speaker embed-
dings and hidden states are set to 50. The joint
cross-entropy loss function with L2 weight decay
as 0.001 is minimized by Adam (Kingma and Ba,
2014) with a batch size of 128.
Evaluation Metrics. To examine the effective-
ness of our model on the addressee identification
task, we compare it with baselines in terms of pre-
cision@n (i.e. p@n) (Yan et al., 2017). For pre-
dicting an addressee of an utterance, our model
actually provides a ranking list for all candidate
addressees.8 We also evaluate the performance on
the session level: we mark a session as a positive
sample if and only if all ground truth labels are
correctly identified on the top 1 of rankings, and
calculate the ratio as accuracy.

As we discussed before, only a part of utter-
ances in multi-party conversations have explicit
addressee, which limits the completeness of auto-
matic evaluation metrics. In order to evaluate the
performance on unlabeled utterances, we leverage
human inference results and calculate the consis-
tency between the model predictions and human
results. Due to the labor cost limit, we randomly
sample 100 sessions from the test set of Len-5,
Len-10 and Len-15 respectively and recruit three

8Intuitively, p@1 is the precision at the highest ranked
position and should be the most natural way to indicate the
performance. We also provide p@2 and p@3 to illustrate the
potential of different systems to identify the correct addressee
on top of the lists.

volunteers to annotate the addressee label for un-
labeled utterances by reasoning through content
and addressee information. We leave blank on
the utterance where three annotators give differ-
ent inference results. Finally around 81.4% of the
unlabeled utterances have two or more annotators
given them same annotation. With the human in-
ference results and model predictions, we use the
overlapping rate9 as the consistency metric.

6 Results and Discussion

We first give an overall comparison between W2W
and baselines followed by ablation experiments
to demonstrate the effectiveness of each part in
W2W model. We then confirm the robustness of
W2W with several factors including the numbers
of users, the position of the utterance. Further-
more, we evaluate how W2W and baseline models
perform on both labeled and unlabeled utterance.

Overall Performance. For automatic evalua-
tion shown in Table 3, end-to-end deep learning
approaches outperform heuristic ones, which in-
dicates that simple intuition is far from satisfac-
tion and further confirm the value of this work.
Among the deep-learning based approaches, our
W2W model outperforms the state-of-the-art mod-
els by all evaluation metrics. Direct adaption from
approaches on identifying the last addressee of the
session may not work fine for our scenario.

As shown in Figure 4, the performance of all
methods drops as the context length increases
(from Len-5 to Len-15) since the task becomes
more difficult with more context information to
encode and more candidate addressees to rank.
However, the improvement of our W2W model is
rather more obvious with longer context length. In
particular, for the dataset Len-15, W2W improves
5% on p@1 and 10% on session accuracy over
SIRNN as shown in Figure 4, which indicate the
robustness of W2W model in complex scenarios.

9The calculation formula of the overlapping rate is de-
scribed in Appendix.



1916

Len-5 Len-10 Len-15
Session Length

62

64

66

68

70

72

74

76

78

80
P@

1

72.75

65.58

62.60

75.98

70.88

68.13

77.55

73.52 73.42

DRNN
SIRNN
W2W

Figure 4: The comparison be-
tween W2W model and two state-
of-the-art baselines on p@1.

2 3 4 5 6 7 8 9 10 11 12
# of Participant Users

0

1000

2000

3000

4000

5000

6000

7000

#
 o

f S
es

sio
ns

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

P@
1

W2W
DRNN
SIRNN

Figure 5: p@1 performance vs.
number (#) of participants within
a session. Results are tested on
Len-15.

1 2 3 4 5 6 7 8 9 10 11 12 13 14
Position in Session

0

5

10

15

20

25

#
 o

f U
tte

ra
nc

es

*1000

0.50

0.55

0.60

0.65

0.70

0.75

0.80

P@
1

W2W
DRNN
SIRNN

Figure 6: p@1 performance vs.
positions of addressees within a
session. Results are tested on Len-
15.

Table 4 shows the consistency between human
inference and model predictions. W2W also out-
performs the baselines with a larger margin on
longer conversation scenarios, which is consis-
tent with the phenomenon of automatic evaluation.
The advantage on unlabeled data of our W2W
model demonstrates the superiority for detecting
the latent speaker-addressee structure unspecified
in the conversation stream, and that it could help
find out the relationship between and across users
in the session.

Ablation Test. Table 5 shows the results of
ablation test. First, we replace the bi-directional
scanning schema with the forward scanning one.
The result shows that the bi-directional scanning
schema captures the information in long conver-
sations more sufficiently. Besides, we investigate
the effectiveness of PAM by replacing it with the
simple mean-pooling approach as Eq (12):

aPADR(iâˆ’1) =
1

n
Â·
âˆ‘
j

aLSRj(iâˆ’1)

aPSPRj(iâˆ’1) =
1

n
Â· aSPRj(iâˆ’1)

uPji =
1

n
Â· ui

(12)

The result shows that it is more beneficial to cap-
ture the correlation between the user-utterance pair
and each listener and implement the state tracking
correspondingly at each turn with our PAM mech-
anism.

To investigate the effectiveness of interactive
representation learning module, we first remove
the UGRU cell and fix the utterance representa-
tions in the state tracking procedure (referred as
w/o Utterance Interaction in Table 5). Symmetri-
cally, we fix the user representations in the session
by removing the SGRU and LGRU cell and main-
tain only the interaction affect from the users to

Table 5: Ablation test on the effectiveness of W2W
model parts in dataset Len-15.

Model p@1 p@2 p@3 Acc.
W2W w/ Forward Scanning Only 71.60 87.99 94.80 31.39
W2W w/o PAM 72.56 88.83 95.21 32.78
W2W w/o Utterance Interaction 72.94 88.89 95.28 33.04
W2W w/o User Interaction 49.18 72.38 86.81 18.24
W2W w/o Interaction 46.39 71.66 86.14 15.15
W2W 73.42 89.41 95.50 33.89

the utterances (referred as w/o User Interaction in
Table 5). We also conduct an experiment on tak-
ing off the whole interactive representation mod-
ule by removing UGRU, SGRU and LGRU, where
the addressee identification is totally dependent on
the initial representations of users and utterances.
The result demonstrates that each part of them has
an important contribution to our W2W model, es-
pecially the interaction affect on users.

Number of Participants. The task be-
comes more difficult as the number of partici-
pants involved in the conversation increases since
more participants correspond to more complicated
speaker-addressee relationship. We investigate
how the performance correlates with the number
of speakers in the dataset Len-15. The results in
terms of p@1 are shown in Figure 5. In con-
versations with few participants, all methods have
rather high performance. As the participant num-
ber increases, W2W constantly outperforms the
baselines. The performance gap becomes larger
especially when there are 6 users and more, which
indicates the capability of our W2W model in han-
dling complex conversation situations.

Position of Addressee-to-Predict. As men-
tioned above in 4.1.2, position information of
utterances is a crucial factor in identifying ad-
dressees for multi-party conversation. We inves-
tigate how the system performance correlates with
the position of the addressee to be predicted. In



1917

Figure 6, we show the p@1 performance of the
W2W and baselines when predicting the addressee
of ui at the ith turn. Again, W2W shows consis-
tently better performance than the other baselines
no matter where the turn to predict addressee is.

We can observe that all the methods perform
relatively poor at the beginning or the end. Com-
pared with the middle part of a long conversation,
the beginning and the ending contains less con-
text information, which makes the addressee of
these part more difficult to predict. The result in
Figure 6 shows that the gap between W2W and
other methods is even larger where the addressee-
to-predict is at the beginning or the end, which
indicates that W2W is better at capturing key in-
formation and has stronger robustness in difficult
scenarios.

Variance of Matching Scores. In real multi-
party conversations, the utterances without ad-
dressee information can be divided into two cases.
Sometimes an utterance has an explicit addressee
while the speaker doesnâ€™t specify whom he/she
is speaking to. We denote these cases as NP
which refers to NULL-Positive. Another case is
that the utterances donâ€™t address to any user in
the conversation (denoted as NN which means
Null-Negative), such as utterances â€™Hi, everyone!â€™
and â€™Can anyone here help me?â€™ In Ubuntu IRC
dataset, unlabeled of the NN case and NP case are
mixed and are difficult to distinguish without man-
ual annotation.

Meanwhile, our W2W model and baseline ap-
proaches predict matching scores on each listener
for every utterance no matter it has addressee in-
formation or not. For each utterance, the variance
of the matching scores on all listeners represents
how certain the model is on its addressee identifi-
cation decision. A larger variance corresponds to
a more confident prediction. Table 6 demonstrates
the variance comparison on labeled and unlabeled
cases in the test set Len-15. On utterances with ad-
dressee labels, the variance of our W2W model is
significantly larger than the state-of-the-art base-
line, which indicates that W2W has a higher de-
gree of certainty about its own predictions when
the conversation content is referring to someone
explicitly. For utterances without addressee la-
bels, the difference of variance between W2W and
SIRNN is significantly reduced. Considering that
unlabeled sets consist of NN ones as well as NP
ones on which W2W has much larger variance

Table 6: Variance of matching scores on labeled and
unlabeled utterances in the set Len-15.

Model Labeled Unlabeled
W2W 0.111 0.080
SIRNN 0.088 0.077

than SIRNN just as the labled utterance scenario,
we can infer that W2W has much lower variance
than SIRNN on the NN case. Such phenomenon
reflects that our W2W model wonâ€™t make a pre-
diction recklessly on occasions where there is no
clear addressee. Therefore, the variance on match-
ing scores of all listeners in our W2W model, to
some extent, provides a signal of whether the ut-
terance has a explicit addressee even if we do not
provide any supervision information on this aspect
during training.

7 Conclusion

In this paper, we aim at learning to identify the ut-
terance addressees in multi-party conversations by
predicting who is speaking to whom, which is a
new task.. To perform the new task, we propose
the W2W model which learns the user and utter-
ance representations interactively. To handle the
uncertainty in the conversation session, we design
PAM which captures matching degree between
current query and each candidate addressees. The
experimental results show prominent and consis-
tent improvement over heuristic and state-of-the-
art baselines.

In the future, we will further investigate better
utterance models associated with additional infor-
mation such as topics or knowledge. With the help
of learned addressee structure, we can build a gen-
eral structural dialogue system for complex multi-
party conversations.

Acknowledgments

We would like to thank the reviewers for their con-
structive comments. We would also like to thank
Bing Liu and Zhangming Chan from Peking Uni-
versity for their suggestion and help on this paper.
This work was supported by the National Key Re-
search and Development Program of China (No.
2017YFC0804001), the National Science Founda-
tion of China (NSFC No. 61876196 and NSFC
No. 61672058). Rui Yan and Wenpeng Hu were
sponsored by Alibaba Innovative Research (AIR)
Grant.



1918

References
Po-Chun Chen, Ta-Chung Chi, Shang-Yu Su, and Yun-

Nung Chen. 2017. Dynamic time-aware attention
to speaker roles and contexts for spoken language
understanding. In Automatic Speech Recognition
and Understanding Workshop (ASRU), 2017 IEEE,
pages 554â€“560, Okinawa Japan. IEEE.

Ta Chung Chi, Po Chun Chen, Shang-Yu Su, and Yun-
Nung Chen. 2017. Speaker role contextual model-
ing for language understanding and dialogue policy
learning. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 163â€“168,
Taipei Taiwan. IJCNLP.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoderâ€“decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724â€“
1734, Doha Qatar. EMNLP.

Wenpeng Hu, Zhangming Chan, Bing Liu, Dongyan
Zhao, Jinwen Ma, and Rui Yan. 2019. Gsn: A
graph-structured network for multi-party dialogues.
In Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence, IJCAI-
19, pages 5010â€“5016. International Joint Confer-
ences on Artificial Intelligence Organization.

Wenpeng Hu, Jiajun Zhang, and Nan Zheng. 2016.
Different contexts lead to different word embed-
dings. In Proceedings of COLING 2016, the 26th
International Conference on Computational Lin-
guistics: Technical Papers, pages 762â€“771, Osaka,
Japan. The COLING 2016 Organizing Committee.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 9:15.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110â€“119, Berlin Germany. ACL.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A
persona-based neural conversation model. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, volume 1, pages
994â€“1003, Berlin Germany. ACL.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016c. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192â€“
1202, Berlin Germany. ACL.

Jiwei Li, Will Monroe, Tianlin Shi, SeÌ‡bastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2157â€“2169,
Copenhagen Denmark. EMNLP.

Yi Luan, Yangfeng Ji, and Mari Ostendorf. 2016.
Lstm based conversation models. arXiv preprint
arXiv:1603.09457, 1.

Zhao Meng, Lili Mou, and Zhi Jin. 2017. Towards
neural speaker modeling in multi-party conversa-
tion: The task, dataset, and models. arXiv preprint
arXiv:1708.03152, 1.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Hiroki Ouchi and Yuta Tsuboi. 2016. Addressee and
response selection for multi-party conversation. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages
2133â€“2143, Austin Texas USA. EMNLP.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing, pages 1532â€“1543, Doha Qutar. EMNLP.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. nature, 323(6088):533.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Build-
ing end-to-end dialogue systems using generative hi-
erarchical neural network models. In Proceedings
of the Thirtieth AAAI Conference on Artificial Intel-
ligence, pages 3776â€“3783, Phoenix Arizona USA.
AAAI Press, AAAI.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, volume 1, pages 1577â€“1586,
Beijing China. ACL.

Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu,
Dongyan Zhao, and Rui Yan. 2019a. Multi-
representation fusion network for multi-turn re-
sponse selection in retrieval-based chatbots. In Pro-
ceedings of the Twelfth ACM International Confer-
ence on Web Search and Data Mining, WSDM â€™19,
pages 267â€“275, New York, NY, USA. ACM.

Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu,
Dongyan Zhao, and Rui Yan. 2019b. One time of
interaction may not be enough: Go deep with an
interaction-over-interaction network for response se-
lection in dialogues. In Proceedings of the 57th

https://doi.org/10.24963/ijcai.2019/696
https://doi.org/10.24963/ijcai.2019/696
https://www.aclweb.org/anthology/C16-1073
https://www.aclweb.org/anthology/C16-1073
https://doi.org/10.1145/3289600.3290985
https://doi.org/10.1145/3289600.3290985
https://doi.org/10.1145/3289600.3290985


1919

Conference of the Association for Computational
Linguistics, pages 1â€“11.

David C Uthus and David W Aha. 2013. The ubuntu
chat corpus for multiparticipant chat analysis. In
Proceedings of the 27th AAAI Conference on Artifi-
cial Intelligence, Bellevue Washington USA. AAAI.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869, 1.

Rui Yan, Yiping Song, and Hua Wu. 2016. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of the 39th International ACM SIGIR
conference on Research and Development in Infor-
mation Retrieval, pages 55â€“64, Pisa Tuscany Italy.
ACM, SIGIR.

Rui Yan, Dongyan Zhao, et al. 2017. Joint learning
of response ranking and next utterance suggestion in
human-computer conversation system. In Proceed-
ings of the 40th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 685â€“694, Tokyo Japan. ACM, SI-
GIR.

Rui Zhang, Honglak Lee, Lazaros Polymenakos, and
Dragomir Radev. 2017. Addressee and response se-
lection in multi-party conversations with speaker in-
teraction rnns. arXiv preprint arXiv:1709.04005, 1.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan.
2016. Multi-view response selection for human-
computer conversation. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 372â€“381, Austin Texas
USA. EMNLP.


