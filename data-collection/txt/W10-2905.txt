










































Identifying Patterns for Unsupervised Grammar Induction


Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 38–45,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Identifying Patterns for Unsupervised Grammar Induction

Jesús Santamarı́a
U. Nacional de Educación a Distancia

NLP-IR Group, Madrid, Spain.
jsant@lsi.uned.es

Lourdes Araujo
U. Nacional de Educación a Distancia

NLP-IR Group, Madrid, Spain.
lurdes@lsi.uned.es

Abstract

This paper describes a new method for un-
supervised grammar induction based on
the automatic extraction of certain pat-
terns in the texts. Our starting hypoth-
esis is that there exist some classes of
words that function as separators, mark-
ing the beginning or the end of new con-
stituents. Among these separators we dis-
tinguish those which trigger new levels in
the parse tree. If we are able to detect these
separators we can follow a very simple
procedure to identify the constituents of a
sentence by taking the classes of words be-
tween separators. This paper is devoted to
describe the process that we have followed
to automatically identify the set of sepa-
rators from a corpus only annotated with
Part-of-Speech (POS) tags. The proposed
approach has allowed us to improve the re-
sults of previous proposals when parsing
sentences from the Wall Street Journal cor-
pus.

1 Introduction

Most works dealing with Grammar Induction (GI)
are focused on Supervised Grammar Induction,
using a corpus of syntactically annotated sen-
tences, or treebank, as a reference to extract the
grammar. The existence of a treebank for the lan-
guage and for a particular type of texts from which
we want to extract the grammar is a great help to
GI, even taking into account the theoretical limi-
tations of GI, such as the fact that grammars can-
not be correctly identified from positive examples
alone (Gold, 1967). But the manual annotation
of thousands of sentences is a very expensive task
and thus there are many languages for which there
are not treebanks available. Even in languages for
which there is a treebank, it is usually composed

of a particular kind of texts (newspaper articles,
for example) and may not be appropriate for other
kind of texts, such as tales or poetry. These rea-
sons have led to the appearance of several works
focused on unsupervised GI.

Thanks to our knowledge of the language we
know that some classes of words are particularly
influential to determine the structure of a sentence.
For example, let us consider the tree in Figure 1,
for which the meaning of the POS tags appears in
Table 1. We can observe that the tag MD (Modal)
breaks the sentence into two parts. Analogously,
in the tree appearing in Figure 2 the POS tag VBZ
breaks the sentence. In both cases, we can see that
after the breaking tag, a new level appears in the
parse tree. A similar effect is observed for other
POS tags, such as VB in the tree of Figure 1 and
IN in the tree of Figure 2. We call these kind of
POS tags separators. There are also other POS
tags which are frequently the beginning or the end
of a constituent1. For example in the tree in Fig-
ure 1 we can find the sequences (DT NN) and (DT
JJ NN), which according to the parse tree are con-
stituents. In the tree in Figure 2 we find the se-
quence (DT NNP VBG NN). In both trees we can
also find sequences beginning with the tag NNP:
(NNP NNP) and (NNP CD) in the tree in Figure 1
and (NNP NNP), which appears twice, in the tree
in Figure 2. This suggests that there are classes
of words with a trend to be the beginning or the
end of constituents without giving rise to new lev-
els in the parse tree. We call these POS tags sub-
separators. These observations reflect some of our
intuitions, such as the fact that most sentences are
composed of a noun phrase and a verb phrase, be-
ing frequently the verb the beginning of the verbal
phrase, which usually leads to a new level of the
parse tree. We also know that determiners (DT)
are frequently the beginning of the noun phrases.

1Constituents are language units in which we can arrange
the structure of a sentence.

38



S

NP-SBJ

NP

NNP NNP

ADJP

NP

CD NNS

JJ

VP

MD VP

VB NP

DT NN

PP-CLR

IN NP

DT JJ NN

NP-TMP

NNP CD

Figure 1: Parse tree for the sentence Pierre Vinken,
61 years old, will join the board as a nonexecutive
director Nov. 29. from the Penn Treebank

S

NP-SBJ

NNP NNP

VP

VBZ NP-PRD

NP

NN

PP

IN NP

NP

NNPNNP

NP

DT NNPVBG NN

Figure 2: Parse tree for the sentence Mr. Vinken
is chairman of Elsevier N.V., the Dutch publishing
group. from the Penn Treebank

At this point we could either try to figure out
what is the set of tags which work as separators, or
to compute them from a parsed corpus for the con-
sidered language, provided it is available. How-
ever, because we do not want to rely on the exis-
tence of a treebank for the corresponding language
and type of texts we have done something differ-
ent: we have devised a statistical procedure to au-
tomatically capture the word classes which func-
tion as separators. In this way our approach can
be applied to most languages, and apart from pro-
viding a tool for extracting grammars and parsing
sentences, it can be useful to study the different
classes of words that work as separators in differ-
ent languages.

Our statistical mechanism to detect separators
is applied to a corpus of sentences annotated with
POS tags. This is not a strong requirement since
there are very accurate POS taggers (about 97%)
for many languages. The grammar that we obtain
does not specify the left-hand-side of the rules, but
only sequences of POS tags that are constituents.

CC Coordinating conjunction
CD Cardinal number
DT Determiner
EX Existential there
FW Foreign word
IN Preposition / subordinating conjunction
JJ Adjective
JJR Adjective, comparative
JJS Adjective, superlative
LS List item marker
MD Modal
NN Noun, singular or mass
NNS Noun, plural
NNP Proper noun, singular
NNPS Proper noun, plural
PDT Predeterminer
POS Possessive ending
PRP Personal pronoun
PP$ Possessive pronoun
RB Adverb.
RBR Adverb, comparative
RBS Adverb., superlative
RP Particle
SYM Symbol (mathematical or scientific)
TO To
UH Interjection
VB Verb, base form
VBD Verb, past tense
VBG Verb, gerund / present participle
VBN Verb, past participle
VBP Verb, non-3rd ps. sing. present
VBZ Verb, 3rd ps. sing. present
WDT wh-determiner
WP wh-pronoun
WP$ Possessive wh-pronoun
WRB wh-adverb

Table 1: Alphabetical list of part-of-speech tags
used in the Penn Treebank, the corpus used in our
experiments

At this point we have followed the Klein and Man-
ning (2005) setting for the problem, which allows
us to compare our results to theirs. As far as we
know these are the best results obtained so far for
unsupervised GI using a monolingual corpus. As
they do, we have used the Penn treebank (Mar-
cus et al., 1994) for our experiments, employing
the syntactic annotations that it provides for eval-
uation purposes only. Specifically, we have used
WSJ10, composed of 6842 sentences, which is the
subset of the Wall Street Journal section of the
Penn Treebank, containing only those sentences of
10 words or less after removing punctuation and
null elements, such as $, ”, etc.

The rest of the paper is organized as follows:
section 2 reviews some related works; section 3
describes the details of the proposal to automati-
cally extract the separators from a POS tagged cor-
pus; section 4 is devoted to describe the procedure
to find a parse tree using the separators; section

39



5 presents and discusses the experimental results,
and section 6 draws the main conclusions of this
work.

2 State of the Art

A growing interest in unsupervised GI has been
observed recently with the appearance of several
works in the topic. Some of these works have fo-
cused on finding patterns of words (Solan et al.,
2005) more than syntactic structures. It has been
noted that the rules produced by GI can also be in-
terpreted semantically (David et al., 2003), where
a non-terminal describes interchangeable elements
which are instances of the same concepts.

Distributional approaches to unsupervised GI
exploit the principle of substitutability: con-
stituents of the same type may be exchanged with
one another without affecting the syntax of the
surrounding context. Distributional approaches to
grammar induction fall into two categories, de-
pending on their treatment of nested structure. The
first category covers Expectation-Maximization
(EM) systems (Dempster et al., 1977). These sys-
tems propose constituents based on analysis of the
text, and then select a non-contradictory combina-
tion of constituents for each sentence that maxi-
mizes a given metric, usually parsing probability.
One of the most successful proposals in this area
is the one by Klein and Manning (2005), which,
as mentioned before, starts from a corpus labelled
only with POS tags. The key idea of the model
proposed in this work is that constituents appear
in constituent contexts. However, the EM algo-
rithm presents some serious problems: it is very
slow (Lari and Young, 1990), and is easily trapped
in local maxima (Carroll and Charniak, 1992).
Alignment Based Learning (ABL) (van Zaanen
and Leeds, 2000) is the only EM system applied
directly to raw text. However, ABL is relatively
inefficient and has only been applied to small cor-
pora. Brooks (Brooks, 2006) reverses the notion
of distributional approaches: if we can identify
“surrounding context” by observation, we can hy-
pothesize that word sequences occurring in that
context will be constituents of the same type. He
describes a simplified model of distributional anal-
ysis (for raw test) which uses heuristics to reduce
the number of candidate constituents under con-
sideration. This is an interesting idea in spite that
Brook showed that the system was only capable of
learning a small subset of constituent structures in

a large test corpus.
The second category is that of incremental

learning systems. An incremental system analyzes
a corpus in a bottom-up fashion: each time a new
constituent type is found it is inserted into the cor-
pus to provide data for later learning. The EMILE
(Adriaans, 1999) and ADIOS (David et al., 2003)
systems are examples for this category, not yet
evaluated on large corpora.

Bilingual experiments have been also conducted
with the aim to exploit information from one lan-
guage to disambiguate another. Usually such a
setting requires a parallel corpus or another an-
notated data that ties the two languages. Co-
hen and Smith (2009) use the English and Chi-
nese treebanks, which are not parallel corpora, to
train parsers for both languages jointly. Their re-
sults shown that the performance on English im-
proved in the bilingual setting. Another related
work (Snyder et al., 2009) uses three corpora of
parallel text. Their approach is closer to the un-
supervised bilingual parsing model developed by
Kuhn (2004), which aims to improve monolingual
performance.

The approach considered in this work follows a
different direction, trying to identify certain pat-
terns that can determine the structure of the parse
trees.

3 Extracting Separators from the Corpus

To automatically extract the set of separators and
sub-separators from a corpus of POS tagged sen-
tences we start from some assumptions:

• The most frequent sequence (of any length)
of POS tags in the corpus is a constituent,
that we call safe constituent (sc). It is quite a
sensible assumption, since we can expect that
at least for the most frequent constituent the
number of occurrences overwhelms the num-
ber of sequences appearing by chance.

• We also assume that the POS tag on the left,
Lsc, and on the right, Rsc, of the safe con-
stituent are a kind of context for other se-
quences that play the same role. Accord-
ing to this, other extended sequences with
Lsc and Rsc at the ends but with other POS
tags inside are also considered constituents.
This assumption is somehow related to the
Klein and Manning’s (2005) idea underlying
their unsupervised GI proposal. According

40



to them, constituents appear in constituent
contexts. Their model exploits the fact that
long constituents often have short, common
equivalents, which appear in similar contexts
and whose constituency as a grammar rule is
more easily found.

• According to the previous point, we use the
tag on the left (Lsc) and on the right (Rsc) of
the safe constituent as discriminant with re-
spect to which to study the behavior of each
POS tag. A POS tag E can have a bias to be
inside the safe constituent, to be outside the
safe constituent (separator), or not to have a
bias at all (sub-separator). We define the de-
termining side of a tag E, as the end tag, Lsc
or Rsc, of the sc with the greater difference
on the number of occurrences of E on both
sides of the end tag. For example, if the ra-
tio of occurrences of E on the left and on the
right of Lsc is smaller (they are more differ-
ent) than the ratio of E on the left and on the
right of Rsc, then Lsc is the determining side
of E, ds(E)2. Then:

– E is considered a separator in the fol-
lowing cases:
∗ if Lsc is the determining side for E

and E appears a 75% more often to
the left of Lsc than to the right (the
75% has been fixed after some esti-
mates described below), or

∗ if Rsc is the determining side for E
and E appears a 75% more often to
the right of Rsc than to the left.

– E is considered a sub-separator if the
following conditions hold:
∗ if Lsc is the determining side for E

and E appears a 75% less often to
the left of Lsc than to the right (the
ratios are very similar) , or

∗ if Rsc is the determining side for E
and E appears a 75% less often to
the right of Rsc than to the left.

– In the remaining cases E is considered
to be part of a constituent (the prefer-
ence is to be inside).

Let us introduce some notation to define more
formally the separators and sub-separators. Let

2If the number of occurrences of E on any side of Lsc
or Rsc is zero, then we compare differences between occur-
rences instead of ratios.

#(E1, · · · , En) be the number of occurrences of
the sequence of tags (E1, · · · , En). We define a
predicate sim to denote the similarity between the
number of occurrences of a sequence of two tags
and the one with reverse order, as

sim(E1, E2)) =

#(E1, E2)

#(E2, E1)
≥ 0.75 if #(E1, E2) ≤ #(E2, E1)

#(E2, E1)

#(E1, E2)
≥ 0.75 if #(E2, E1) ≤ #(E1, E2)

Then a tag E is considered a separator if the
following predicate is true:

sep(Lsc, E, Rsc) =

(sd(Lsc) ∧ (#(E, Lsc) > #(Lsc, E)∧

¬sim(E, Lsc)))∨

(sd(Rsc) ∧ (#(Rsc, E) > #(E, Rsc) ∧ ¬sim(E, Rsc))

. A tag is considered a sub-separator when the
following predicate is true:

subsep(Lsc, E, Rsc) =

(sd(Lsc) ∧ sim(E, Lsc))∨

(sd(Rsc) ∧ sim(E, Rsc))

.

We have computed the number of occurrences
of every sequence of POS tags in the corpus, find-
ing that the most frequent sequence of tags is
(DT,NN). This sequence, which is our safe con-
stituent, appears 2222 times in the considered cor-
pus WSJ10.

Applying our procedure to the corpus we have
obtained the following sets of separators and sub-
separators:

Separators MD, PRP, IN, RB, RBR,
CC, TO, VB, VBD, VBN,
VBZ, VBP, VBG, EX, LS,
RP, UH, WP, WRB, WDT

Sub-separators DT, PDT, POS, SYM, NN,
NNS, NNP, NNPS

For selecting a threshold value to discriminate
the preference of a POS tag to be inside or out-
side of a constituent we have studied the results
obtained for different threshold values greater than
50%. Table 2 shows the results. We can observe
all of them are very similar for all the thresholds,
as long as they are greater than 50%. Analyzing
the set of POS-tags that have been classified as
separators and sub-separators with each threshold
we have found that the only differences are that the
tag POS (Possessive ending), which is classified
as sub-separator using a threshold between 50%

41



and 75%, is classified as separator using higher
thresholds, and the tag SYM (Symbol), which is
classified as sub-separator using a threshold be-
tween 50% and 75%, is classified neither as a sep-
arator nor as a sub-separator using higher thresh-
olds. We have adopted a threshold value of 75%
because higher values can be too restrictive, and in
fact provide worse results.

Similarity F1
55% 74.55
65% 74.55
75% 74.55
85% 72.24
95% 72.24

Table 2: F-measure results obtained for different
values of the threshold used to classify the set of
POS-tags.

Sub-separators can be grouped to their right or
to their left, depending on the case. In order to
measure the bias of each of them for one direc-
tion or another we have compared the number of
occurrences of the most frequent sequence com-
posed of the sub-separator and a POS tag on the
right and on the left. We choose as preference di-
rection for a sub-separator the corresponding to
the most frequent sequence. Table 3 shows the
results obtained, the preference direction of each
sub-separator appearing in the last column. In the
case of NNP, for which the frequency of the most
frequent tag to the right and to the left are the
same, we have looked at the second most frequent
sequence to choose the grouping direction.

sub-sep left freq. right freq. D
DT (DT, NN)(2222) (IN,DT)(894) L
PDT (PDT,DT)(28) (NN,PDT)(14) L
POS (POS, NN)(169) (NNP, POS)(223) R
SYM (SYM, IN)(11) (NN,SYM)(4) L
NN (NN, IN)(892) (DT,NN)(2222) R
NNS (NNS, VBP)(591) (JJ,NNS)(797) R
NNP (NNP, NNP)(2127) (NNP,NNP)(2127) R
NNPS (NNPS, NNP)(42) (NNP,NNPS)(82) R

Table 3: Preference direction to which each sub-
separator clusters. The first column corresponds
to the sub-separator, the second one to the most
frequent sequence composed of the sub-separator
and a tag on its right, the third one to the most fre-
quent sequence of the sub-separator and a tag on
its left, and the last column to the resulting direc-
tion.

4 Identifying Constituents

Once we have the sets of separators and sub-
separators the procedure to identify the con-
stituents of each sentence is as follows:

• We identify the separators in the sentence.
For example, if we consider the sentence:

CC DT NN IN NNP NNP POS NN VBZ

the separators are marked in boldface:

CC DT NN IN NNP NNP POS NN VBZ

• The next step is to split the sentence ac-
cording to the separators. The first separator
which is a verb, if any, is used to split the sen-
tence into two parts. Each separator can give
rise to two groups: one composed of the tag
sequence between the separator and the next
separator, and another one which includes the
separator and the POS tags up to the end of
the part of the sentence in which it appears
(usually sentences are divided into two parts
using the first separator which is a verb). In
our example, this mechanism leads to the fol-
lowing structure:

[[CC [DT NN] [IN [NNP NNP POS NN]]]
[VBZ]]

• Now it is the turn of the sub-separators (DT,
PDT, POS, SYM, NN, NNS, NNP, NNPS),
which are underlined in the sentence:

[[CC [DT NN] [IN [NNP NNP POS NN]]]
[VBZ]]

• Finally, each group of the sentence is split
according to the sub-separators. Each sub-
separator has been assigned a preference di-
rection to form the group with the next POS
tag. Looking at Table 3, which tells us the
direction in which each sub-separator forms
the group, we apply this step to our sentence
example, obtaining:

[[CC [DT NN] [IN [[NNP NNP POS] NN]]]
[VBZ]]

42



The sub-separator DT is grouped with the
tags on its right, while NN is grouped with
the tags on its left, thus composing the group
(DT NN). When two or more sub-separators
appear in a sequence, they are grouped to-
gether in a unique constituent whenever they
have the same grouping direction. In our sen-
tence example this criterion leads to [NNP
NNP POS] instead of [NNP[NNP[POS]]]. A
constituent finishes if the next POS tag is a
separator or if it is a sub-separator that makes
groups towards the left. Since POS (Pos-
sessive ending) tends to be grouped with the
POS tag on its left, it is the end of the con-
stituent.

Figure 3 represents the obtained structure as a
parse tree. Figure 4 represents the correct parse
tree according to the Penn treebank. We can ob-
serve that both structures are very similar. The
method based on separators has been able to cap-
ture most of the constituent appearing in the parse
tree: (DT, NN), (NNP, NNP, POS), (NNP, NNP,
POS, NN), (IN, NNP, NNP, POS, NN). The differ-
ences between both trees come from our criterion
of splitting the sequence of tags into two subse-
quences using the first verb. This problem will be
tackled in the future in a more refined model.

C

C

CC C

DT NN

C

IN C

C

NNP NNP POS

NN

VBZ

Figure 3: Parse tree for the sentence And the nose
on Mr. Courter’s face grows from the Penn tree-
bank (WSJ), obtained with our separators method.

5 Evaluation

Our proposal has been evaluated by comparing the
tree structures produced by the system to the gold-
standard trees produced by linguists, which can be
found in the Penn Treebank. Because we do not
assign class name to our constituents, i.e. a left
hand side symbol for the grammar rules, as the lin-
guists do in treebanks, the comparison ignores the
class labels, considering only groups of tags.

S

CC NP-SBJ

NP

DT NN

PP-LOC

IN NP

NP

NNP NNP POS

NN

VP

VBZ

Figure 4: Parse tree appearing in the Penn tree-
bank (WSJ) for the sentence And the nose on Mr.
Courter’s face grows.

The results presented in the work by Klein and
Manning (2005) have been our reference, since as
far we know they are the best ones obtained so far
for unsupervised GI. For the sake of comparison,
we have considered the same corpus and the same
measures. Accordingly, we performed the experi-
ments on the 6842 sentences3 of the WSJ10 selec-
tion from the Penn treebank Wall Street Journal
section.

In order to evaluate the quality of the obtained
grammar we have used the most common mea-
sures for parsing and grammar induction evalua-
tion: recall, precision, and their harmonic mean
(F-measure). They are defined assuming a bracket
representation of a parse tree.

Precision is given by the number of brackets
in the parse to evaluate which match those in the
correct tree and recall measures how many of the
brackets in the correct tree are in the parse. These
measures have counterparts for unlabeled trees,
the ones considered in this work – in which the
label assigned to each constituent is not checked.
Constituents which could not be wrong (those of
size one and those spanning the whole sentence)
have not been included in the measures.

The definitions of Unlabeled Precision (UP) and
Recall (UR) of a proposed corpus P = [Pi] against
a gold corpus G = [Gi] are:

UP (P, G) =

∑
i
|brackets(Pi) ∩ brackets(Gi)|∑

i
|brackets(Pi)|

,

UR(P, G) =

∑
i
|brackets(Pi) ∩ brackets(Gi)|∑

i
|brackets(Gi)|

.

Finally, UF (Unlabeled F-measure) is given by:

UF =
2 · UP (P, G) · UR(P, G)

UP (P, G) + UR(P, G)
.

3More precisely sequences of POS tags

43



1 2 3 4 5 6 7 8 9 10
Constituent size

60

70

80

90

100
%

Recall
Precision
F-measure

Figure 5: Results obtained per constituent size:
unlabeled recall, precision, and F-measure.

Figure 5 shows the results of unlabeled recall,
precision and F-measure obtained per constituent
size. We can observe that recall and precision, and
thus the corresponding F-measure, are quite sim-
ilar for every constituent size. This is important,
because obtaining a high F-measure thanks to a
very high recall but with a poor precision, is not
so useful. We can also observe that the best results
are obtained for short and long constituents, with
lower values for middle lengths, such as 5 and 6.
We believe that this is because intermediate size
constituents present more variability. Moreover,
for intermediate sizes, the composition of the con-
stituents is more dependent on sub-separators, for
which the statistical differences are less significant
than for separators.

2 3 4 5 6 7 8 9
Constituent size

50

60

70

80

90

F-
m

ea
su

re

Separator approach
Klein-Manning approach

Figure 6: Comparison of the separator approach
and Klein and Manning’s approach per constituent
size.

We have compared our results to those obtained
by Klein and Manning (2005) for the same corpus.

Table 4 shows the obtained results for WSJ10. We
can observe that we have obtained more balanced
values of recall and precision, as well as a better
value for the F-measure. Thus the method pro-
posed in this work, that we expect to refine by
assigning different probabilities to separators and
sub-separators, depending on the context they ap-
pear in, provides a very promising approach.

UR UP UF
Separ. A. 77,63% 71,71% 74,55%
KM 80.2% 63.8% 71.1%

Table 4: Results (unlabeled recall, precision, and
F-measure), obtained with the separator approach
(first row) and with the Klein and Manning ap-
proach (second row) for the WSJ10 corpus.

Figure 6 compares the F-measure for the two
approaches by constituents length. We can ob-
serve that the separator approach obtains better re-
sults for all the lengths. The figure also shows that
the results per constituent length follow the same
trend in both approaches, thus reflecting that the
difficulty for middle length constituents is greater.

6 Conclusions

We have proposed a novel approach for unsuper-
vised grammar induction which is based on iden-
tifying certain POS tags that very often divide the
sentences in particular manners. These separators
are obtained from POS tagged texts, thus making
the model valid for many languages. The con-
stituents corresponding to a sentence are found by
means of a simple procedure based on the sepa-
rators. This simple method has allowed us to im-
prove the results of previous proposals.

We are currently working in defining a more re-
fined statistical model which takes into account
the probability of a tag to be a separator or sub-
separator, depending on its context. We plan to
apply a similar study to other languages, in order
to study the different classes of words that func-
tion as separator in each of them.

Acknowledgements

This paper has been funded in part by the Span-
ish MICINN project QEAVis-Catiex (Spanish
Ministerio de Educación y Ciencia - TIN2007-
67581), as well as by the Regional Government of
Madrid under the Research Network MA2VICMR
(S2009/TIC-1542).

44



References

Pieter Adriaans. 1999. Learning Shallow Context-Free
Languages under Simple Distributions. Technical
Report, Institute for Logic, Language, and Compu-
tation, Amsterdam.

David J. Brooks. 2006. Unsupervised grammar in-
duction by distribution and attachment. In CoNLL-X
’06: Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, pages 117–
124. Association for Computational Linguistics.

Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. In Working Notes of the Work-
shop Statistically-Based NLP Techniques, pages 1–
13. AAAI.

Shay B. Cohen and Noah A. Smith. 2009. Shared
logistic normal distributions for soft parameter ty-
ing in unsupervised grammar induction. In NAACL
’09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 74–82. Association for
Computational Linguistics.

Zach Solan David, David Horn, and Shimon Edelman.
2003. Unsupervised efficient learning and represen-
tation of language structure. In Proc. 25th Confer-
ence of the Cognitive Science Society, pages 2577–
3596. Erlbaum.

A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM
algorithm. Royal statistical Society B, 39:1–38.

E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10(5):447–474.

Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context model. Pattern Recognition,
38(9):1407–1419.

Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In ACL ’04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 470. Association for Com-
putational Linguistics.

K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35–56.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL-IJCNLP ’09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the

ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1, pages 73–81. Association for Computational
Linguistics.

Zach Solan, David Horn, Eytan Ruppin, and Shi-
mon Edelman. 2005. Unsupervised learning of
natural languages. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 102(33):11629–11634.

Menno van Zaanen and Ls Jt Leeds. 2000. Learning
structure using alignment based learning. In Univer-
sities of Brighton and Sussex, pages 75–82.

45


