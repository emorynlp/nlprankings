










































Commitments to Preferences in Dialogue


Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 204–215,
Portland, Oregon, June 17-18, 2011. c©2011 Association for Computational Linguistics

Commitments to Preferences in Dialogue

Anais Cadilhac*, Nicholas Asher*, Farah Benamara*, Alex Lascarides**

*IRIT, University of Toulouse, **School of Informatics, University of Edinburgh

Abstract

We propose a method for modelling how dialogue

moves influence and are influenced by the agents’

preferences. We extract constraints on preferences

and dependencies among them, even when they are

expressed indirectly, by exploiting discourse struc-

ture. Our method relies on a study of 20 dia-

logues chosen at random from the Verbmobil cor-

pus. We then test the algorithms predictions against

the judgements of naive annotators on 3 random un-

seen dialogues. The average annotator-algorithm

agreement and the average inter-annotator agree-

ment show that our method is reliable.

1 Introduction

Dialogues are structured by various moves that the

participants make—e.g., answering questions, asking

follow-up questions, elaborating prior claims, and so

on. Such moves come with commitments to certain at-

titudes such as intentions and preferences. While map-

ping utterances to their underlying intentions is well

studied through the application of plan recognition tech-

niques (e.g., Grosz and Sidner (1990), Allen and Litman

(1987)), game-theoretic models of rationality generally

suggest that intentions result from a deliberation to find

the optimal tradeoff between one’s preferences and one’s

beliefs about possible outcomes (Rasmusen, 2007). So

mapping dialogue moves to preferences is an important

task: for instance, they are vital in decisions on how to

re-plan and repair should the agents’ current plan fail, for

they inform the agents about the relative importance of

their various goals. Classical game theory, however, de-

mands a complete and cardinal representation of prefer-

ences for the optimal intention to be defined. This is not

realistic for modelling dialogue because agents often lack

complete information about preferences prior to talking:

they learn about the domain, each other’s preferences and

even their own preferences through dialogue exchange.

For instance, utterance (1) implies that the speaker wants

to go to the mall given that he wants to eat, but we do not

know his preferences over “go to the mall” if he does not

want to eat.

(1) I want to go to the mall to eat something.

Existing formal models of dialogue content either do not

formalise a link between utterances and preferences (e.g.,

Ginzburg (to appear)), or they encode such links in a

typed feature structure, where desire is represented as a

feature that takes conjunctions of values as arguments

(e.g., Poesio and Traum (1998)), making the language

too restricted to express dependencies among preferences

of the kind we just described. Existing implemented

dialogue systems likewise typically represent goals as

simple combinations of values on certain information

‘slots’ (e.g., He and Young (2005), Lemon and Pietquin

(2007)); thus (1) yields a conjunction of preferences, to

go to the mall and to eat something. But such a system

could lead to suboptimal dialogue moves—e.g., to help

the speaker go to the mall even if he has already received

food.

What’s required, then, is a method for extracting par-

tial information about preferences and the dependencies

among them that are expressed in dialogue, perhaps indi-

rectly, and a method for exploiting that partial informa-

tion to identify the next optimal action. This paper pro-

poses a method for achieving these tasks by exploiting

discourse structure.

We exploited the corpus of Baldridge and Lascarides

(2005a), who annotated 100 randomly chosen sponta-

neous face-to-face dialogues from the Verbmobil cor-

pus (Wahlster, 2000) with their discourse structure ac-

cording to Segmented Discourse Representation Theory

(SDRT, Asher and Lascarides (2003))—these structures

represent the types of (relational) speech acts that the

agents perform. Here’s a typical fragment:

(2) a. A: Shall we meet sometime in the next

week?

b. A: What days are good for you?

c. B: Well, I have some free time on almost

every day except Fridays.

204



d. B: In fact, I’m busy on Thursday too.

e. A: So perhaps Monday?

Across the corpus, more than 30% of the discourse units

are either questions or assertions that help to elaborate a

plan to achieve the preferences revealed by a prior part

of the dialogue—these are marked respectively with the

discourse relations Q-Elab and Plan-Elab in SDRT, and

utterances (2b) and (2e) and the segments (2c) and (2d)

invoke these relations (see Section 2). Moreover, 10% of

the moves revise or correct prior preferences (like (2d)).

We will model the interaction between dialogue con-

tent and preferences in two steps. The first maps ut-

terances and their rhetorical connections into a partial

description of the agents’ preferences. The mapping is

compositional and monotonic over the dialogue’s logi-

cal form (i.e., the description of preferences for an ex-

tended segment is defined in terms of and always sub-

sumes those for its subsegments): it exploits recursion

over discourse structure. The descriptions partially de-

scribe ceteris paribus preference nets or CP-nets with

Boolean variables (Boutilier et al., 2004). We chose CP-

nets over alternative logics of preferences, because they

provide a compact, computationally efficient, qualitative

and relational representation of preferences and their de-

pendencies, making them compatible with the kind of

partial information about preferences that utterances re-

veal. Our mapping from the logical form of dialogue

to partial descriptions of Boolean CP-nets proceeds in a

purely linguistic or domain independent way (e.g., it ig-

nores information such as Monday and Tuesday cannot

co-refer) and will therefore apply to dialogue generally

and not just Verbmobil.

In a second stage, we “compress” and refine our descrip-

tion making use of constraints proper to CP-nets (e.g.,

that preference is transitive) and constraints provided by

the domain—in this case constraints about times and

places, as well as constraints from deep semantics. This

second step reduces the complexity of inferring which

CP-net(s) satisfy the partial description and allows us to

identify the minimal CP-net that satisfies the domain-

dependent description of preferences. We can thus ex-

ploit dependencies between dialogue moves and mental

states in a compact, efficient and intuitive way.

We start by motivating and describing the semantic repre-

sentation of dialogue from which our CP-net descriptions

and then our CP-nets will be constructed.

2 The Logical Form of Dialogue

Our starting point for representing dialogue con-

tent is SDRT. Like Hobbs et al. (1993) and

Mann and Thompson (1987), it structures discourse

into units that are linked together with rhetorical re-

lations such as Explanation, Question Answer Pair

(QAP), Q-Elab, Plan-Elab, and so on. Logical forms

in SDRT consist of Segmented Discourse Representation

Structures (SDRSs). As defined in Asher and Lascarides

(2003), an SDRS is a set of labels representing discourse

units, and a mapping from each label to an SDRS-formula

representing its content—these formulas are based on

those for representing clauses or elementary discourse

units (EDUs) plus rhetorical relation symbols between

labels. Lascarides and Asher (2009) argue that to make

accurate predictions about acceptance and denial, both

of which can be implicated rather than linguistically

explicit, the logical form of dialogue should track each

agent’s commitments to content, including rhetorical

connections. They represent a dialogue turn (where turn

boundaries occur whenever the speaker changes) as a

set of SDRSs—one for each agent representing all his

current commitments, from the beginning of the dialogue

to the end of that turn. The representation of the dialogue

overall—a Dialogue SDRS or DSDRS—is that of each of

its turns. Each agent constructs the SDRSs for all other

agents as well as his own. For instance, (2) is assigned

the DSDRS in Table 1, with the content of the EDUs

omitted for reasons of space (see Lascarides and Asher

(2009) for details). We adopt a convention of indexing

the root label of the nth turn, spoken by agent d, as

nd; and π : φ means that φ describes π’s content (we’ll

sometimes also write φπ to identify this description).

We now return to our example (2). Intuitively, (2a) com-

mits A to a preference for meeting next week but it does

so indirectly: the preference is not asserted, or equiva-

lently entailed at the level of content from the semantics

of Q-Elab(a,b). Accordingly, responding with "I do too"
(meaning "I want to meet next week too") is correctly pre-

dicted to be highly anomalous. A’s SDRS for turn 1 in Ta-

ble 1 commits him to the questions (2a) and (2b) because

Q-Elab is veridical: i.e. Q-Elab(a,b) entails the dynamic
conjunction φa ∧φb. Since intuitively (2a) commits A to
the implicature that he prefers next week, our algorithm

for eliciting preferences from dialogue must ascribe this

preference to A on the basis of his move Q-Elab(a,b).
Furthermore, Q-Elab(a,b) entails that any answer to (2b)
must elaborate a plan to achieve the preference revealed

by (2a); this makes φb paraphrasable as “What days next

week are good for you?”, which does not add new prefer-

ences.

B’s contribution in the second turn attaches to (2b) with

QAP and also Plan-Elab—he answers with a non-empty

extension for what days. Lascarides and Asher (2009) ar-

gue that this means that B is also committed to the illo-

cutionary contribution of (2b), as shown in Table 1 by

the addition of Q-Elab(a,b) to B’s SDRS. This addition
commits B also to the preference of meeting next week,

with his answer making the preference more precise: (2c)

reveals that B prefers any day except Friday; by linking

(2d) with Plan-Correction he retracts the preference for

Thursday. This compels A to revise his inferences about

205



Turn A’s SDRS B’s SDRS

1 π1A : Q-Elab(a,b) /0
2 π1A : Q-Elab(a,b) π2B : Q-Elab(a,b)∧QAP(b,π)∧Plan-Elab(b,π)

π : Plan-Correction(c,d)
3 π3A : Q-Elab(a,b)∧QAP(b,π)∧Plan-Elab(b,π)∧ π2B : Q-Elab(a,b)∧QAP(b,π)∧Plan-Elab(b,π)

Plan-Elab(π,e) π : Plan-Correction(c,d)

Table 1: The DSDRS for Dialogue (2).

B’s preference for meeting on Thursday. A’s Plan-Elab

move (2e) in the third turn reveals another preference for

Monday. This may not match his preferred day when the

dialogue started: perhaps that was Friday. He may con-

tinue to prefer that day. But engaging in dialogue can

compel agents to revise their commitments to preferences

as they learn about the domain and each other.

The above discussion of (2) exhibits how different types

of rhetorical relations between utterances rather than

Searle-like speech acts like question, construed as a prop-

erty of an utterance, are useful for encoding how pref-

erences evolve in a dialogue and how they relate to

one another. While the Grounding Acts dialogue model

(Poesio and Traum, 1998) and the Question Under Dis-

cussion (QUD) model (Ginzburg, to appear) both have

many attractive features, they do not encode as fine-

grained a taxonomy of types of speech acts and their se-

mantic effects as SDRT: in SDRT each rhetorical relation

is a different kind of (relational) speech act, so that, for

instance, the speech act of questioning is divided into the

distinct types Q-Elab, Plan-Correction, and others. For

the QUD model to encode such relations would require

implicit questions of all sorts of different types to be in-

cluded in the taxonomy, in which case the result may be

equivalent to the SDRT taxonomy of dialogue moves. We

have not explored this eventual equivalence here.

3 CP-nets and CP-net descriptions

A preference is standardly understood as an ordering by

an agent over outcomes; at the very least it entails a com-

parison between one entity and another (outcomes being

one sort of entity among others). As indicated in the in-

troduction, we are interested in an ordinal definition of

preferences, which consists in imposing an ordering over

all (relevant) possible outcomes. Among these outcomes,

some are acceptable for the agent, in the sense that the

agent is ready to act in such a way as to realize them;

and some outcomes are not acceptable. Amongst the ac-

ceptable outcomes, the agent will typically prefer some

to others. Our method does not try to determine the most

preferred outcome of an agent but follows rather the evo-

lution of their commitments to certain preferences as the

dialogue proceeds. To give an example, if an agent pro-

poses to meet on a certain day X and at a certain time Y,

we infer that among the agent’s acceptable outcomes is a

meeting on X at Y, even if this is not his most preferred

outcome (see earlier discussion of (2e)).

A CP-net (Boutilier et al., 2004) offers a compact rep-

resentation of preferences. It is a graphical model that

exploits conditional preferential independence so as to

structure the decision maker’s preferences under a ceteris

paribus assumption.

Although CP-nets generally consider variables with a fi-

nite range of values, to define the mapping from dialogue

turns to descriptions of CP-nets in a domain indepen-

dent and compositional way, we use Boolean proposi-

tional variables: each variable describes an action that an

agent can choose to perform, or not. We will then refine

the CP-net description by using domain-specific informa-

tion, transforming CP-nets with binary valued variables

to CP-nets with multiple valued variables. This reduces

the complexity of the evaluation of the CP-net by a large

factor.

More formally, let V be a finite set of propositional vari-

ables and LV the description language built from V via

Boolean connectives and the constants ⊤ (true) and ⊥
(false). Formulas of LV are denoted by φ,ψ, etc. 2

V is the

set of interpretations for V , and as usual for M ∈ 2V and
x∈V , M gives the value true to x if x∈M and false other-
wise. Where X ⊆V , let 2X be the set of X-interpretations.
X-interpretations are denoted by listing all variables of

X , with a ¯ symbol when the variable is set to false: e.g.,

where X = {a,b,d}, the X-interpretation M = {a,d} is
expressed as abd.

A preference relation � is a reflexive and transitive bi-
nary relation on 2V with strict preference ≻ defined in
the usual way (i.e., M � M′ but M′ 6� M). Note that
preference orderings are not necessarily complete, since

some candidates may not be comparable by a given agent.

An agent is said to be indifferent between two options

M,M′ ∈ 2V , written M ∼M′, if M �M′ and M′ �M.

As we stated earlier, CP-nets exploit conditional pref-

erential independence to compute a preferential ranking

over outcomes:

Definition 1 Let V be a set of propositional variables

and {X ,Y,Z} a partition of V . X is conditionally pref-
erentially independent of Y given Z if and only if ∀z ∈
2Z , ∀x1,x2 ∈ 2

X and ∀y1,y2 ∈ 2
Y we have: x1y1z �

206



x2y1z iff x1y2z� x2y2z.

For each variable X , the agent specifies a set of parent

variables Pa(X) that can affect his preferences over the
values of X . Formally, X is conditionally preferentially

independent of V \ ({X}∪Pa(X)). This is then used to
create the CP-net.

Definition 2 Let V be a set of propositional variables.

N = 〈G ,T 〉 is a CP-net on V , where G is a directed
graph over V , and T is a set of Conditional Preference

Tables (CPTs) with indifference. That is, T = {CPT(X j):
X j ∈ V}, where CPT(X j) specifies for each instantiation

p ∈ 2Pa(X j) either x j ≻p x j, x j ≻p x j or x j ∼p x j.

The following simple example illustrates these defini-

tions. Suppose our agent prefers to go from Paris to

Hong Kong by day rather than overnight. If he takes an

overnight trip, he prefers a non stop flight, but if he goes

by day, he prefers a flight with a stop. Figure 1 shows the

associated CP-net. The variable T stands for the prefer-

ence over the period of travel. Its values are Td for a day

trip and Tn for a night one. The variable St stands for the

preference over stops. Its values are S for a trip with stops

and S without.

T

St

CPT(T) = Td ≻ Tn

CPT(St) =
Td : S ≻ S

Tn : S≻ S

Figure 1: Travel CP-net

With CP-nets defined, we proceed to a description lan-

guage for them. The description language formula w ≻
y(CPT ) describes a CP-net where a CPT contains an en-
try of the form w ≻p y for some possibly empty list of
parent variables p. A CP-net description is a set of such

formulas. The CP-net N |= x1, . . .xn : w≻ y(CPT ) iff the
CP-net N ’s CPT T contains an entry w≻~u y—also writ-
ten~u : w≻ y—where x1, . . .xn figure in~u. Satisfaction of a
description formula by a CP-net yields a notion of logical

consequence between a CP-net descriptionD N and a de-

scription formula in the obvious way. Dialogue turns also

sometimes inform us that certain variables enter into pref-

erence statements. We’ll express the fact that the vari-

ables x1, . . . ,xn are associated with discourse constituent

π by the formula x1, . . . ,xn(P(π)), where P(π) refers to
the partial description of the preferences expressed by the

discourse unit π (see Section 4).

The description language allows us to impose constraints

on the CP-nets that agents commit to without specifying

the CP-net completely, as is required for utterances like

(1). In section 6, we describe how to construct a min-

imal CP-net from a satisfiable CP-net description. One

can then use the forward sweep procedure for outcome

optimisation (Boutilier et al., 2004). This is a proce-

dure of linear complexity, which consists in instantiating

variables following an order compatible with the graph,

choosing for each variable (one of) its preferred values

given the value of the parents.

4 From EDUs to Preferences

EDUs are described in SDRT using essentially Boolean

formulas over labels (Asher and Lascarides, 2003); thus

φ(π)∧ψ(π) means that φ and ψ describe aspects of π’s
content. Not(π1,π)∧ φ(π1) means that the logical form
of the EDU π is of the form ¬π1 and that π1 is described
by φ; so π has the content ¬φ. Our task is to map such
descriptions of content into descriptions of preferences.

Our preference descriptions will use Boolean connectives

and operators over preference entries (e.g., of the form

x ≻ y): namely, &,▽, 7→, and a modal operator ✸. The
rules below explain the semantics of preference opera-

tors (they are in effect defined in terms of the semantics

of buletic attitudes and Boolean connectives) and how

to recursively calculate preference descriptions from the

EDU’s logical structure.

Simple EDUs can provide atomic preference statements

(e.g., I want X or We need X). This means that with this

EDU the speaker commits to a preference for X . X will

typically involve a Boolean variable and a preference en-

try for its CPT. P(π) is the label of the preference descrip-
tion associated with discourse unit π. Hence for a sim-

ple EDU π, we have X(P(π)) as its description. Simple
EDUs also sometimes express preferences in an indirect

way (see (2a)).

More generally, P recursively exploits the logical struc-

ture of an EDU’s logical form to produce an EDU pref-

erence representation (EDUPR). For instance, since the

logical form of the EDU I want fish and wine features

conjunction, likewise so does its preference description:

φ&ψ(P(π)) means that among the preferences included
in π, the agent prefers to have both φ and ψ and prefers ei-

ther one if he can’t have both.1 We also have disjunctions

(let’s meet Thursday or Friday), and negations (I don’t

want to meet on Friday), whose preferences we’ll express

respectively as Thurs▽Fri(P(π)) and ¬Fri(P(π)).
Some EDUs express commitments to dependencies

among preferences. For example, in the sentence What

about Monday, in the afternoon?, there are two prefer-

ences: one for the day Monday, and, given the Monday

preference, one for the time afternoon (of Monday), at

least on one syntactic disambiguation. We represent this

dependency as Mon 7→ Aft(P(π)). Note that 7→ is not
expressible with just Boolean operators. Finally, EDUs

can express commitment to preferences via free choice

1The full set of rules also includes a stronger conjunction φ∆ψ(P(π))
(the agent prefers both φ and ψ, but is indifferent if he can’t have both).

207



modalities; I am free on Thursday, or ✸Thurs(P(π)), tells
us that Thursday is a possible day to meet. ✸φ says that φ

is an acceptable outcome (as described earlier, this means

the agent is ready to act so as to realize an outcome that

entails φ). Thus, ✸φ(π) entails φ(π), and ✸-embedded
preferences obey reduction axioms permitting ✸ to be

eliminated when combined with other preference oper-

ators. But a ✸ preference statement does affect a prefer-

ence description when is is conjoined in Boolean fashion

with another ✸ preference statement in an EDU or com-

bined via a discourse relation like Continuation. This is

because ✸ is a free choice modality and obeys the equiv-

alence (3) below, which in turn yields a disjunctive pref-

erence φ▽ψ(P(π)) from what appeared to be a conjunc-
tion.2

(3) (✸φ(P(π))∧✸ψ(P(π)))↔✸(φ▽ψ)(P(π))

The variables introduced by a discourse segment π are

integrated into the CP-net description D N via the oper-

ation Commit(π,D N ). The following seven rules cover
the different possible logical structures for the EDU pref-

erence representation. In the following, X ,Y,Z,W denote

propositional variables and φ, ψ propositional formulas

from EDUPR. Var(φ) are the variables in φ, and ≻X
the preference relation describing CPT (X). Sat(φ) (or
non-Sat(φ)) is a conjunction of literals from Var(φ) that
satisfy (or do not satisfy) φ. Sat(φ)−X is the formula that
results from removing the conjunct with X from Sat(φ).

1. Where X(P(π)) (X is a variable of P(π), e.g., I want
X), Commit(π,D N ) adds the description D N |=
X ≻ X(CPT (X)).3

2. Where φ&ψ(P(π)), Commit(π,D N ) adds descrip-
tions as follows:

• For each X ∈Var(φ), add Var(ψ) to Pa(X) and
modify CPT (X) as follows:
If Sati(ψ), Sat j(φ) ⊢ X (resp. X), then Sati(ψ),
Sat j(φ)−X : X ≻ X (resp. X ≻ X), for all sat-
isfiers i and j.
• Similarly for each Y ∈Var(ψ).

If φ and ψ are literals X and Y we get: D N |= Y ≻
Y (CPT (Y )) and D N |= X ≻ X(CPT (X)). Graph-
ically, this yields the following preference relation

(where one way arrows denote preference, two way

2We provide here the reduction axioms over preference descriptions

1. ✸(φ&ψ)(P(π))↔ (φ&ψ)(P(π))

2. ✸(φ 7→ ψ)(P(π))↔ (φ 7→ ψ)(P(π))

3. ✸(φ▽ψ)↔ (φ▽ ψ)(P(π))

4. ✸✸φ(P(π))↔✸φ(P(π))

3Given our description language semantics, this means that any

CP-net which satisfies the description D N contains a preference ta-

ble CPT (X) with an entry X ≻ X with at least one instantiation of the
variables in Pa(X).

arrows denote indifference or equal preference, and

no arrow means the options are incomparable):
XY

XY XY
XY

3. Where φ▽ ψ(P(π)) (the agent prefers to have at
least one of φ and ψ satisfied). If φ and ψ are X

and Y , we get:

• Var(X) ∈ Pa(Var(Y )) and D N |= X : Y ∼
Y (CPT (Y )), D N |= X : Y ≻ Y (CPT (Y )).
• Var(Y ) ∈ Pa(Var(X)) and D N |= Y : X ∼

X(CPT (X)), D N |= Y : X ≻ X(CPT (X)).

This corresponds to the following preference rela-

tion:

XY

XY XY XY

As before, the use of indifference allows us to find

the best outcomes (XY , XY and XY ) easily.

4. Where φ 7→ ψ(P(π)) (the agent prefers that φ is sat-
isfied and if so that ψ is also satisfied. If φ is not

satisfied, it is not possible to define preferences on

ψ). If φ and ψ are X and Y , we get:

• D N |= X ≻ X(CPT (X))

• Var(X) ∈ Pa(Var(Y)) and
D N |= X : Y ≻ Y (CPT (Y )).

Note that this description is also produced by

Elab(πi,π j) below where X(P(πi)) and Y (P(π j))
(see rule 8). Thus the implication symbol 7→ is a
"shortcut" in that it represents elaborations whose

arguments are in the same EDU.

5. Where ✸φ(P(π)) (the agent prefers a free choice of
φ). Given the behaviour of ✸, this reduces to treating

φ(P(π)).

6. Where ¬φ(P(π)). We can apply rules 1-5 by con-
verting ¬φ into conjunctive normal form.

7. Where φ(P(π))∧ψ(P(π)), with φ and ψ nonmodal,
we simply apply the rule for φ and that for ψ.

5 From Discourse Structure to Preferences

We must now define how the agents’ preferences, repre-

sented as a partial description of a CP-net, are built com-

positionally from the discourse structure over EDUs. The

constraints are different for different discourse relations,

reflecting the fact that the semantics of connections be-

tween segments influences how their preferences relate

to one another.

We will add rules for defining Commit over la-

bels π whose content φπ express rhetorical relations

R(πi,π j)—indeed, we overload the notation and write
Commit(R(πi,π j),D N ). Since Commit applies com-
positionally, starting with the EDUs and working up

208



the discourse structure towards the unique root la-

bel of the SDRS, we can assume in our definition of

Commit(R(πi,π j),D N ) that the EDUPRs are already de-
fined. We give rules for all the relations in the Verbmobil

corpus, though we will be very brief with those that are

less prevalent. A complete example using our rules is in

appendix A.

IExplanation, Elab, Plan-Elab, Q-elab

IExplanation(πi, π j): i.e., π j’s preferences explain πi’s

(e.g., see (1), where P(πi) would be going to the mall
and P(π j) is eating something). With Elab(πi, π j) a
preference in πi is elaborated on or developed in π j,

as in: I want wine. I want white wine. That is, a

preference for white wine depends on a preference for

wine. Plan-Elab(πi,π j) means that π j describes a plan
for achieving the preferences expressed by πi, and with

Q-Elab we have a similar dependence between prefer-

ences, but the second constituent is a question (so often

in practice this means preference commitments from πi
transfer from one agent to another).

Plan-Elab(π j,πi), Elab(π j,πi) and IExplanation(πi,π j)
all follow the same two-step rule, and so from the point

of view of preference updates they are equivalent:

8. i Firstly, preference description D N is up-

dated according to P(π j) by applying
Commit(π j,D N ), if π j expresses a new
preference. If not go to step (ii).

ii. Secondly, description D N is modified so that

each variable in P(πi) depends on each vari-
able in P(π j): i.e., ∀X ∈ Var(P(πi)), ∀Y ∈
Var(P(π j)), Y ∈ Pa(X). Then, D N is enriched
according to P(πi), if πi expresses a preference.
If it does not, then end.

We now give some details concerning step (ii) above. To

this end, let φ denote a formula with SDRS description

predicates, φ′ its corresponding boolean (preference) for-

mula and φ′ its negation. Then for φ=Y , we define φ′=Y
and φ′ = Y ; for φ = Y 7→ Z we define φ′ = Y ∧ Z and
φ′ = Y ∨ Z; and for φ = Y ▽ Z and φ = Y&Z, we have
φ′ = Y ∨Z and φ′ = Y ∧Z.

a. X(P(πi)) and φ(P(π j)). The agent explains his pref-
erences on X by φ. So, if no preferences on X are

already defined, φ is a reason to prefer X . That is,

D N |= φ′: X ≻ X(CPT (X)). However, it is not pos-
sible to define preferences on X if φ is false. If, on

the other hand, preferences on X are already defined,

the agent prefers X if φ is satisfied, and does not

modify his preferences otherwise—i.e.,≻X ,φ′= X ≻

X , ≻
X ,φ′

=≻X .
4

4If we have ≻X such that Z: X ≻ X , Z: X ≻ X , ≻X ,φ′ represents

preferences defined by Z∧φ′ and Z∧φ′, whereas ≻
X ,φ′

represents pref-

erences defined by Z∧φ′ and Z∧φ′.

For φ = Y , if ≻X is not already defined, we obtain
the following preference relation (no information on

the preference for X if Y is false makes XY and XY

incomparable):
XY

XYXY

XY

b. X▽Z(P(πi)) and φ(P(π j)). The agent explains his
preferences X▽Z by φ: he wants to satisfy X or Z
if φ is satisfied.

First, we set Var(Z) ∈ Pa(Var(X)), Var(X) ∈
Pa(Var(Z)). If ≻X is not already defined, we have:
D N |= φ′ ∧ Z: X ∼ X(CPT (X)), D N |= φ′ ∧ Z:
X ≻ X(CPT (X)).
Otherwise, ≻X ,φ′,Z= X ∼ X , ≻X ,φ′,Z= X ≻ X ,
≻

X ,φ′,Z
= ≻

X ,φ′,Z
= ≻X .

CPT (Z) is defined as CPT (X) by inverting X and Z.
For φ =Y , if ≻X and≻Z are not already defined, we
obtain the following preference relation (again, the

lack of preference information on X and Z when Y

is false yields incomparability among states where Y

is false):

XY ZXY Z

XY Z

XY Z

XY Z

XY Z

XY ZXY Z

c. X&Z(P(πi)) and φ(P(π j)). The agent explains his
preferences on X&Z by φ.

• If ≻X is not already defined, we have: D N |=
φ′ : X ≻ X(CPT (X)).
Otherwise, ≻X ,φ′= X ≻ X , ≻X ,φ′= ≻X ,

• CPT (Z) is defined as CPT (X) by replacing X
by Z.

d. X 7→ Z(P(πi)) and φ(P(π j)). The agent explains his
preferences on X 7→ Z by φ: he wants to satisfy X
and after Z if φ is satisfied.

If ≻X is not already defined, we have D N |= φ
′ :

X ≻ X(CPT (X)) and we set Var(X)∈ Pa(Var(Z)).5

If ≻Z is not yet defined, we have : D N |= φ
′ ∧X :

Z ≻ Z(CPT (Z)), D N |= φ′∧X : Z ∼ Z(CPT (Z)).
Else, ≻Z,(φ′∧X)= Z ≻ Z, ≻Z,(φ′∧X)= Z ∼ Z,
≻

Z,(φ′∧X)= ≻Z,(φ′∧X)=≻Z.

e. ψ(P(πi)) and φ(P(π j)). We can apply rules 8 by
decomposing ψ.

5Otherwise, there is no need to modify ≻X . This is what we call a
‘partial elaboration’. Variables that were evoked since preferences on

X were introduced are parents of Z but not of X . For example, if an

agent commits to a preference for Monday then Afternoon, and later in

the discourse he commits to 2oclock, then Afternoon is 2oclock’s parent

but not Monday’s.

209



f. ✸(ψ)(P(πi)) and ✸(φ)(P(π j)). We treat this like a
free choice EDU (see rule 5).

g. ✸(ψ)(P(πi)) and φ(P(π j)), where φ is non modal.
We treat this like ψ(P(πi)) and φ(P(π j)) (see rule
8.e)

Let’s briefly look at how the rule changes for

Q-elabA(π1,π2) (where the subscript A identifies the
speaker of π2):

9. Q-ElabA(π1,π2) implies that we update A’s CP-
net description D N by applying the rule for

Elab(π1,π2), where if π2 expresses no preferences
on their own, we simply make the P(π2) description
equal to the P(π1) description. Thus A’s CP-net de-
scription is updated with the preferences expressed

by utterance π1, regardless of who said π1.

QAP Answers to questions affect preferences in complex

ways:

10. The first case concerns yes/no questions and there

are two cases, depending on whether B replies yes

or no:

Yes QAPB(π1,π2) where π2 is yes. B’s pref-
erence descriptions are updated by apply-

ing Commit(ElabB(π1,π2),D N ) (and so B’s
preference description include preferences ex-

pressed by π1 and π2).

No QAPB(π1,π2) where π2 is no. If P(π1)
and P(π2) are consistent, then B’s pref-
erence descriptions are updated by ap-

plying CommitB(ElabB(π1,π2),D N );
otherwise, they are updated by applying

Commit(Correction(π1,π2),D N ) (see rule
13).

11. When π1 is a wh-question and QAPB(π1,π2), B’s
preferences over variables in π1 and π2 are ex-

actly the same as the ones defined for a yes/no

question where the answer is yes. Variables in π2
will refine preferences over variables in π1. So,

B’s preference descriptions are updated by applying

CommitB(ElabB(π1,π2),D N ).

In previous rules, it is relatively clear how to update the

preference commitments. However, in some cases it’s not

clear what the answer in a QAP targets: in Could we meet

the 25 in the morning? No, I can’t., we do not know if

No is about the 25 and the morning, or only about the

morning. So, we define the following rule for managing

cases where the target is unknown :

12. If we know the target, we can change the description

of the CP-net. Otherwise, we wait to learn more.

Correction and Plan-Correction allow a speaker to rec-

tify a prior commitment to preferences. Self-corrections

also occur in the corpus: I could do it on the 27th. No I

can not make it on the 27th, sorry I have a seminar. Cor-

rection and Plan-Correction can have several effects on

the preferences. For instance, they can correct preference

entries. That is, given Correction(π1,π2), some variables
in P(π1) are replaced by variables in P(π2) (in the self-
correction example, every occurrence of 27 in P(π1) is
replaced with 27 and vice versa). We have a set of rules

of the form X ←{Y1, . . . ,Ym}, which means that the vari-
able X ∈ Var(P(π1)) is replaced by the set of variables
{Y1, . . . ,Ym} ⊆ Var(P(π2)). We assume that X can’t de-
pend on {Y1, . . . ,Ym} before the Correction is performed.
Then replacement proceeds as follows:

13. If Pa(X) = /0, we add the description D N |= Yk ≻
Y k(CPT (Yk)) for all k ∈ {1, . . . ,m} and remove X ≻
X(CPT (X)) (or X ≻ X(CPT (X))). Otherwise, we
replace every description of CPT (X) with an equiv-
alent statement using Yk (to describe CPT (Yk)), for
all k ∈ {1, . . .m}.

The specific target of the correction behaves similarly to

the target of a QAP. In some cases we don’t know the

target, in which case we apply rule 12.

Plan-Correction can also lead to the modification of an

agent’s own plan because of other agent’s proposals. In

this case it corrects the list of parent variables on which

a preference depends. We call that list of variables the

operative variables. Once the operative variables are

changed, Plan-Correction can elaborate a plan if some

new preferences are expressed. For example, all agents

have agreed to meet next week, so in their CP-net descrip-

tion, there is the entry Week1≻Week1. Then discussion
shows that their availabilities are not compatible and one

of them says "okay, that week is not going to work.". That

does not mean the agent prefers Week1 to Week1 because

both agreed on Week1 as preferable. Rather, Week1 has

been removed as an operative variable in the following

discourse segments. This leads us to the following rule:

14. For Plan-Correction(π1,π2) which corrects
the list of parent variables, the operative vari-

able list becomes the intersection of all Pa(X)
where X ∈ Var(P(π1)). We can now apply
Commit(Plan-Elab(π1,π2),D N ), if P(π2) contains
some new preferences φ. If the CPT affected by a

rule has no entry for the current operative variable

list O , then O : φ has to be added to D N .

Continuation, Contrast and Q-Cont pattern with the

rule for Elab. Alternation patterns with rule 8.b.6 Expla-

nation, Explanation*, Result, Qclar (clarification ques-

tion), Commentary, Summary and Acknowledgment

6The rule for Alternative questions like Do you want fish or chicken?

is a special case yielding φ▽ψ(P(π)), but we don’t offer details here.

210



either do nothing or have the same effect on preference

elicitation as Elab. Sometimes, adding these preferences

via the Elab rule may yield an unsatisfiable CP-net de-

scription, because an implicit correction is involved. If an

evaluation of the CP-net (see next section) is performed

after a processing of one of these rules shows that the

CP-net description is not satisfiable, then we apply the

rule 13, associated with Correction.

6 From Descriptions to Models

Each dialogue turn adds constraints monotonically to the

descriptions of the CP-nets to which the dialogue partic-

ipants commit. We have interpreted each new declared

variable in our rules as independent, which allows us to

give a domain independent description of preference elic-

itation. However, when it comes to evaluating a CP-net

description for satisfiability, we need to take into account

various axioms about preference (irreflexivity and transi-

tivity), and axioms for the domain of conversation: in our

case, temporal designations (Wednesdays are not Tues-

days and so on). This typically adds dependencies among

the variables in the description. In the case of the Verb-

mobil domain, since the variable Monday means essen-

tially "to meet on Monday", Monday implies Meet , and

this must be reflected via a dependency in the CP-net: we

must view the variable Meet as filling a hidden slot in

the variable Monday in the preference description, Meet :

Mon≻Mon. This likewise allows us to fill in the negative
clauses of the CP-net description: we can now infer that

Meet : Mon ≻ Mon. These axioms also predict certain
preference descriptions to be unsatisfiable. For instance,

if we have Mon ≻Mon, our axioms imply Mon ≻ Tues,
Mon ≻Wed, etc. At this point we can calculate, ceteris
paribus, inconsistencies on afternoons and mornings of

particular days.

Domain knowledge also allows us to collapse Boolean

valued variables that all denote, say, days or times of the

day into multiple valued variables. So for instance, our

domain independent algorithm from dialogue moves to

preference descriptions might yield:

(4) Meet∧31.01∧30.01∧02.02: am≻ am

Domain knowledge collapses all Boolean variables for

distinct days into one variable with values for days to get:

(5) Meet∧02.02: am≻ pm

This leads to a sizeable reduction in the set of variables

that are used in the CP-net.

We can test any CP-net description for satisfiability by

turning the description formulas into CP-net entries. Our

description automatically produces a directed graph over

the parent variables. We have to check that the ≻ state-
ments form an irreflexive and transitive relation and that

each variable introduced into the CP-net has a preference

entry consistent given these constraints. If the description

does not yield a preference entry for a given variable X ,

we will add the indifference formula X ∼ X as the entry.
If our CP-net description meets these requirements, this

procedure yields a minimal CP-net. Testing for satisfia-

bility is useful in eliciting preferences from several dis-

course moves like Explanation, Qclar or Result, since in

the case of unsatisfiability, we will exploit the Correction

rule 13 with these moves.

7 Evaluation of the proposed method

We evaluate our method by testing it against the judg-

ments of three annotators on three randomly chosen un-

seen test dialogues from the Verbmobil corpus. The

test corpus contains 75 EDUs and the proportion of dis-

course relations is the same as in the corpus overall. The

three annotators were naive in the sense that they were

not familiar with preference representations and prefer-

ence reasoning strategies. For each dialogue segment,

we checked if the judges had the same intuitions that we

did on: (i) how commitments to preferences are extracted

from EDUs, and (ii) how preferences evolve through dia-

logue exchange.

The judges were given a manual with all the instructions

and definitions needed to make the annotations. For ex-

ample, the manual defined preference to be "a notion of

comparison between one thing at least one other". The

manual also instructs annotators to label each EDU with

the following four bits of information: (1) preferences

(if any) expressed in the EDU; (2) dependencies between

preferences expressed in the EDU; (3) dependencies be-

tween preferences in the current EDU and previous ones;

and (4) preference evolution (namely, the appearance of

a new factor that affects preferred outcomes, update to

preferences over values for an existing factor, and so on).

For each of these four components, example dialogues

were given for each type of decision they would need to

make, and instructions were given on the format in which

to code their judgements. Appendix A shows an example

of an annotated dialogue.

Table 2 presents results of the evaluation of (i). For each

EDU, we asked the annotator to list the preferences ex-

pressed in the EDU and we compared the preferences ex-

tracted by each judge with those extracted by our algo-

rithm. The triple (a, b, c) respectively indicates the pro-

portion of common preferences (two preference sets Γi
and Γ j are common if (Γi = Γ j) or (∃x ∈ Γi,y ∈ Γ j ,x→
y)—for example, the preference MeetBefore2≻MeetAt2
implies MeetAt2 ≻ MeetAt2), the proportion of prefer-
ences that one judge extracts and the other judge or our al-

gorithm misses and the proportion of preferences missed

by one judge and extracted by the other judge or by our

algorithm. The average annotator-algorithm agreement

(AAA) is 75.6% and the average inter-annotator agree-

211



Our algorithm J1 J2 J3 % of EDUs that commit to preferences

Our algorithm (83, 4, 13) (91, 0, 9) (91, 0, 9) 76%

J1 (83, 13, 4) (85, 7, 8) (91, 4, 5) 80%

J2 (91, 9, 0) (85, 8, 7) (92, 4, 4) 86%

J3 (91, 9, 0) (91, 5, 4) (92, 4, 4) 84%

Table 2: Evaluating how preferences are extracted from EDUs.

Our algorithm J1 J2 J3

Our algorithm (85, 71) (96, 100) (93, 86)

J1 (85, 71) (89, 71) (91, 86)

J2 (96, 100) (89, 71) (98, 86)

J3 (93, 86) (91, 86) (98, 86)

Table 3: Evaluating how preferences evolve through dialogue.

ment (IAA) is 77.9%; this shows that our method for ex-

tracting preferences from EDUs is reliable.

The evaluation (ii) proceeds as follows. For each EDU, we

ask the judge if the segment introduces new preferences

or if it updates, corrects or deletes preferences commited

in previous turns. As in (i), judges have to justify their

choices. Table 3 presents the preliminary results where

the couple (a,b) indicates respectively the proportion of

common elaborations (preference updates or new prefer-

ences) and the proportion of common corrections. Since

elaboration is also applied in case of other discourse re-

lations (e.g., Q-Elab), the measure a evaluates the rules

8, 9, 10 (yes) and 11. Similarly, the measure b evalu-

ates the rules 10 (no), 13 and 14. We obtain AAA=91%

IAA=92.7% for elaboration and AAA=85.7% IAA=81%

for correction.

8 Conclusion

We have proposed a compositional method for elicit-

ing preferences from dialogue consisting of a domain-

independent algorithm for constructing a partial CP-net

description of preferences, followed by a domain-specific

method for identifying the minimal CP-net satisfying the

partial description and domain constraints. The method

supports qualitative and partial information about prefer-

ences, with CP-nets benefiting from linear algorithms for

computing the optimal outcome from a set of preferences

and their dependencies. The need to compute intentions

from partially defined preferences is crucial in dialogue,

since preferences are acquired and change through dia-

logue exchange.

Our work partially confirms that CP-nets have a certain

naturalness, as the map from dialogue moves to prefer-

ences using the CP-net formalism is relatively intuitive.

The next step is to implement our method. This depends

on extracting discourse structure from text, which, though

difficult, is becoming increasingly tractable for simple

domains (Baldridge and Lascarides, 2005b). We plan to

extract CP-net descriptions from EDUs and to evaluate

these descriptions using "multi-valued variables" auto-

matically. We will then evaluate our method on a large

number of dialogues.

Our work here is also and more generally a first step to-

wards modelling the complex interaction between what

agents say, what their preferences are, and what they take

the preferences of other dialogue agents to be. It leads

to a conception of dialogue that’s more general than one

based purely on Gricean cooperative principles (Grice,

1975). On a purely Gricean approach, conversation is

cooperative in at least two ways: a basic level concern-

ing the conventions that govern linguistic meaning (ba-

sic cooperativity); and a level concerning shared attitudes

towards what is said, including shared intentions (con-

tent cooperativity). While basic cooperation is needed

for communication to work at all, content cooperativ-

ity involves strongly cooperative axioms like Coopera-

tivity (interlocutors normally adopt the speaker’s inten-

tions) (Allen and Litman, 1987, Grosz and Sidner, 1990,

Lochbaum, 1998). Our approach allows for divergent

preferences and divergent intentions, i.e. conversations

that aren’t based on content cooperativity. This will al-

low us to exploit information about conflicting agents’

preferences and game-theoretic techniques that are inher-

ent in the logics of CP-nets for computing optimal moves

(Bonzon, 2007). And in contrast to Franke et al. (2009),

who analyse conversations where content cooperativity

doesn’t hold using a game-theoretic framework, our ap-

proach allows for partial and qualitative representations

of preferences rather than demanding complete and quan-

titative representations of them.

212



References

J. Allen and D. Litman. A plan recognition model for

subdialogues in conversations. Cognitive Science, 11

(2):163–200, 1987.

N. Asher and A. Lascarides. Logics of Conversation.

Cambridge University Press, 2003.

J. Baldridge and A. Lascarides. Annotating discourse

structures for robust semantic interpretation. In Pro-

ceedings of the Sixth International Workshop on Com-

putational Semantics (IWCS), Tilburg, The Nether-

lands, 2005a.

J. Baldridge and A. Lascarides. Probabilistic head-driven

parsing for discourse structure. In Proceedings of

the Ninth Conference on Computational Natural Lan-

guage Learning (CoNLL), pages 96–103, 2005b.

E. Bonzon. Modélisation des Interactions entre Agents

Rationnels: les Jeux Booléens. PhD thesis, Université

Paul Sabatier, Toulouse, 2007.

C. Boutilier, R.I. Brafman, C. Domshlak, H.H. Hoos, and

David Poole. Cp-nets: A tool for representing and

reasoning with conditional ceteris paribus preference

statements. Journal of Artificial Intelligence Research,

21:135–191, 2004.

M. Franke, T. de Jager, and R. van Rooij. Relevance in

cooperation and conflict. Journal of Logic and Lan-

guage, 2009.

J. Ginzburg. The Interactive Stance: Meaning for Con-

versation. CSLI Publications, to appear.

H. P. Grice. Logic and conversation. In P. Cole and

J. L. Morgan, editors, Syntax and Semantics Volume

3: Speech Acts, pages 41–58. Academic Press, 1975.

B. Grosz and C. Sidner. Plans for discourse. In J. Mor-

gan P. R. Cohen and M. Pollack, editors, Intentions in

Communication, pages 365–388. MIT Press, 1990.

Y. He and S. Young. Spoken language understsanding

using the hidden vector state model. Speech Commu-

nication, 48(3-4):262–275, 2005.

J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Inter-

pretation as abduction. Artificial Intelligence, 63(1–2):

69–142, 1993.

A. Lascarides and N. Asher. Agreement, disputes and

commitment in dialogue. Journal of Semantics, 26(2):

109–158, 2009.

O. Lemon and O. Pietquin. Machine learning for spoken

dialogue systems. In Interspeech, 2007.

K. E. Lochbaum. A collaborative planning model of in-

tentional structure. Computational Linguistics, 24(4):

525–572, 1998.

W. C. Mann and S. A. Thompson. Rhetorical structure

theory: A framework for the analysis of texts. Interna-

tional Pragmatics Association Papers in Pragmatics,

1:79–105, 1987.

M. Poesio and D. Traum. Towards an axiomatisation of

dialogue acts. In J. Hulstijn and A. Nijholt, editors,

Proceedings of the Twente Workshop on the Formal Se-

mantics and Pragmatics of Dialogue. 1998.

E. Rasmusen. Games and Information: An Introduction

to Game Theory. Blackwell Publishing, 2007.

W. Wahlster, editor. Verbmobil: Foundations of Speech-

to-Speech Translation. Springer, 2000.

213



Appendix A : Treatment of an example

We illustrate in this section how our rules work on an

example. Since this dialogue was also evaluated by our

judges (cf section 7), we give where relevant some details

on those annotations. The example is as follows:

(6) π1. A: so, I guess we should have another meet-

ing

π2. A: how long do you think it should be for.

π3. B: well, I think we have quite a bit to talk

about.

π4. B: maybe, two hours?

π5. B: how does that sound.

π6. A: deadly,

π7. A: but, let us do it anyways.

π8. B: okay, do you have any time next week?

π9. B: I have got, afternoons on Tuesday and

Thursday.

π10. A: I am out of Tuesday Wednesday Thurs-

day,

π11. A: so, how about Monday or Friday

Table 4 is the DSDRS associated with (6).

Relation(πi, [π j − πk]) indicates that a rhetorical re-
lation holds between the segment πi and a segment

consisting of π j, π j+1, . . . , πk

π1 provides an atomic preference. We apply the rule

1 and so CommitA(π1,D N A) adds the description
D N A |= M ≻M(CPT (M)) where M means Meet.

π2 We have Q-Elab(π1, π2). A continues to commit to

M in π2 and no new preferences are introduced by

π2. We apply rule 9, which makes the P(π2) de-
scription the same as P(π1)’s.

π3 is linked to π2 with QAP. B accepts A’s preference

and we apply the rule 11 since π2 is a wh-question.

Thus CommitB(ElabB(π2,π3),D N B) adds the de-
scription D N B |= M ≻M(CPT (M)). It is interest-
ing to note that some judges consider that agent’s

utterance in π3 indicates a preference towards "talk-

ing a long time" while other judges consider, as our

method predicts, that this segment does not convey

any preference.

π4 is linked to π3 by Q-Elab. B commits to a new

preference. We apply rule 9, rule 8 and then rule

8.a. The preference on the hour is now dependent

on the preference on meeting; i.e., D N B |= M :
2h ≻ 2h(CPT (2h)), where the variable 2h means
two hours.

π5 is related to π4 with the Q-Cont relation. We

then follow the same rule as the continued relation,

namely Q-Elab. We apply rule 9 which does not

change the CP-net description of B because π5 does

not convey any preference.

π6 is related to π5 with QAP relation. In this case, it’s

not clear what is the QAP target and so we apply

rule 12: we wait to learn more and we do not change

B’s CP-net description.

All the Judges indicated that segments π5 and π6
are ambiguous and therefore hesitated to say if they

commit to preferences. For example in π6, do we

have a preference for meeting more than 2 hours

or less than 2 hours? This indecision is compatible

with the predictions of rule 12.

π7 A accepts B’s preference. We apply rule 9 and then

rule 8 to obtain:

D N A |= M ≻M(CPT (M)),
D N A |= M : 2h≻ 2h(CPT (2h)).

π8 is linked to π7 by Q-Elab. B introduces a new pref-

erence for meeting next week.

We apply rule 9 and then 8 to obtain:

D N B |= M ≻M(CPT (M)),
D N B |= M : 2h≻ 2h(CPT (2h)),
D N B |= M∧2h : NW ≻NW (CPT (NW )) where the
variable NW means next week.

π9 is linked to π8 by Plan-Elab. π9 expresses com-

mitments to preference that already involve a

CP-net description. B introduces three prefer-

ences: one for meeting on Tuesday, the other

for meeting on Thursday and given the conjunc-

tion of preferences Tues ∧ T hurs, one for time
afternoon (of Tuesday and Thursday). That is,

((✸(Tues)∧✸(T hurs)) 7→ Aft)(P(π9)). We apply
the equivalence (3) and obtain :

(✸(Tues▽Thurs)→ Aft)(P(π9)).
Then, we apply rules 8.g, 8.b and 8.d. The CP-net

description of B is thus updated as follows:

D N B |= M ∧ 2h ∧ NW ∧ Tues : Thurs ≻

T hurs(CPT (T hurs)),
D N B |= M ∧ 2h ∧ NW ∧ Tues : Thurs ∼
T hurs(CPT (T hurs)),
D N B |= M ∧ 2h ∧ NW ∧ T hurs : Tues ≻
Tues(CPT (Tues)),
D N B |= M ∧ 2h ∧ NW ∧ T hurs : Tues ∼
Tues(CPT (Tues)),
D N B |= M ∧ 2h ∧ NW ∧ (T hurs ∨ Tues) : Aft ≻
Aft(CPT (Aft)).

Most judges express here a preference ranking over

outcomes. For instance, if B elaborates by adding

the preference "I have got Monday morning too"

(as it is in the test corpus), some consider the rank-

ing "(Tuesday or Thursday afternoons) ≻ (Monday

214



Turn A’s SDRS B’s SDRS

1 π1A : Q-Elab(π1,π2) /0

2 π1A:is the same as in turn 1 π2B : Q-Elab(π1, [π2−π5])∧QAP(π2, [π3−π5])∧

Q-Elab(π3,π)

π : Q-Cont(π4,π5)

3 π3A : Q-Elab(π1, [π2−π7])∧QAP(π2, [π3−π7])∧ π2B: is the same as in turn 2

Q-Elab(π3, [π4,π7])∧QAP(π,π
′)

π : Q-Cont(π4,π5),π
′ : Contrast(π6,π7)

4 π3A: is the same as in turn 3 π4B : Q-Elab(π1, [π2−π9])∧QAP(π2, [π3−π9])∧

Q-Elab(π3, [π4−π9])∧QAP(π, [π6−π9])∧

Q-Elab(π′,π′′)

π : Q-Cont(π4,π5),π
′ : Contrast(π6,π7)

π′′ : Plan-Elab(π8,π9)

5 π5A : Q-Elab(π1, [π2−π11])∧QAP(π2, [π3−π11])∧ π4B: is the same as in turn 4

Q-Elab(π3, [π4−π11])∧QAP(π, [π6−π11])∧

Q-Elab(π′, [π8−π11])∧QAP(π
′′,π′′′)

π : Q-Cont(π4,π5),π
′ : Contrast(π6,π7)

π′′ : Plan-Elab(π8,π9),π
′′′ : Q-Elab(π10,π11)

Table 4: The DSDRS for Dialogue (6).

morning)≻ (other days)", while others consider the
ranking "(Tuesday or Thursday afternoon) or (Mon-

day morning)≻ (other days)". We did not treat such
preference ranking.

π10 is related to π9 by QAP where A answers no to B’s

question asked in π8. We apply rule 10 (no). Since

Tues&Weds&Thurs(P(π10)) is not consistent with
((✸(Tues) ∧✸(T hurs)) 7→ Aft)(P(π9)), we apply
CommitA(Correction(π9,π10),D N A), which adds
the preference Weds to A’s description and then

the rule 13 where Tues and T hurs are respectively

replaced by Tues and T hurs :

D N A |= M∧2h∧NW : Tues≻ Tues(CPT (Tues)),
D N A |= M ∧ 2h ∧ NW : T hurs ≻
T hurs(CPT (T hurs)),
D N A |= M ∧ 2h ∧ NW : Weds ≻
Weds(CPT (Weds)).

π11 Finally, this segment is linked to π10 with Q-Elab

where Mond▽Fri(P(π11)). We apply rules 9 and
8.b and update A’s CP-net description as follows:

D N A |=M∧2h∧NW ∧Tues∧T hurs∧Weds∧Fri :
Mond ≻Mond(CPT (Mond)),
D N A |=M∧2h∧NW ∧Tues∧T hurs∧Weds∧Fri :
Mond ∼Mond(CPT (Mond)),
D N A |= M ∧ 2h ∧ NW ∧ Tues ∧ T hurs ∧Weds ∧
Mond : Fri≻ Fri(CPT (Fri)),
D N A |= M ∧ 2h ∧ NW ∧ Tues ∧ T hurs ∧Weds ∧
Mond : Fri∼ Fri(CPT (Fri)).

The evaluation of this dialogue also reveals to what extent

naive annotators reason with binary (Monday preferred

to not Monday) or multi-valued variables (Monday pre-

ferred to Tuesday). Most judges use multi-valued vari-

ables to express the preference extracted from an EDU,

and the way in which our method exploits domain knowl-

edge to yield the minimal CP-net satisfying the descrip-

tion reflects this. In addition, some judges use a small

set of variables (for example the variable time of meeting

that groups together the notion of week, day, hours, etc.)

while others use a distinct variable for each preference.

Finally, we also noticed that judges do not describe the

same preference dependencies. For example, in:

(7) We could have lunch together and then have the

meeting from one to three?

some consider that the preference on having lunch is in-

dependent from the preference on the meeting (in this

case, they consider that the preference on the period one

to three is independent from the preference on meeting)

while others consider that the two preferences are depen-

dent.

215


