



















































ELDEN: Improved Entity Linking Using Densified Knowledge Graphs


Proceedings of NAACL-HLT 2018, pages 1844‚Äì1853
New Orleans, Louisiana, June 1 - 6, 2018. c¬©2018 Association for Computational Linguistics

ELDEN: Improved Entity Linking using Densified Knowledge Graphs

Priya Radhakrishnan+, Partha Talukdar* and Vasudeva Varma+

+IIIT Hyderabad, India
*Indian Institute of Science, Bangalore, India

priya.r@research.iiit.ac.in, ppt@iisc.ac.in, vv@iiit.ac.in

Abstract

Entity Linking (EL) systems aim to automati-
cally map mentions of an entity in text to the
corresponding entity in a Knowledge Graph
(KG). Degree of connectivity of an entity in
the KG directly affects an EL system‚Äôs abil-
ity to correctly link mentions in text to the
entity in KG. This causes many EL systems
to perform well for entities well connected
to other entities in KG, bringing into focus
the role of KG density in EL. In this paper,
we propose Entity Linking using Densified
Knowledge Graphs (ELDEN). ELDEN is an
EL system which first densifies the KG with
co-occurrence statistics from a large text cor-
pus, and then uses the densified KG to train
entity embeddings. Entity similarity measured
using these trained entity embeddings result
in improved EL. ELDEN outperforms state-
of-the-art EL system on benchmark datasets.
Due to such densification, ELDEN performs
well for sparsely connected entities in the KG
too. ELDEN‚Äôs approach is simple, yet effec-
tive. We have made ELDEN‚Äôs code and data
publicly available.

1 Introduction

Entity Linking (EL) is the task of mapping men-
tions of an entity in text to the corresponding entity
in Knowledge Graph (KG) (Hoffart et al., 2011;
Dong et al., 2014; Chisholm and Hachey, 2015).
EL systems primarily exploit two types of infor-
mation: (1) similarity of the mention to the candi-
date entity string, and (2) coherence between the
candidate entity and other entities mentioned in
the vicinity of the mention in text. Coherence es-
sentially measures how well the candidate entity is
connected, either directly or indirectly, with other
KG entities mentioned in the vicinity (Milne and
Witten, 2008; Globerson et al., 2016). In the state-

‚úì

KN
O

W
LE

D
G

E 
G

R
AP

H

Andrei Broder won best paper award at WWWTEX
T

W
EB

 C
O

R
PU

S

World_Wide_Web

WWW_conference

Program Committee

PERTH, Australia, 5 April 2017: ... affiliation are Andrei Broder (Google/AltaVista),.. WWW9 
Program Committee... WWW conference with more than 3,500 citations‚Ä¶.

?
?

Andrei_Broder

Figure 1: Improving entity disambiguation per-
formance using KG densification: edges to
WWW conference, a sparsely-connected entity
in the KG, is increased by adding edges from pseudo
entity Program Committee whose mention co-
occurs with it in web corpus. ELDEN, the system
proposed in this paper, uses such densified KG to
successfully link ambiguous mention WWW to the
correct entity WWW conference, instead of the more
popular entity World Wide Web.

of-the-art EL system by (Yamada et al., 2016), co-
herence is measured as distance between embed-
dings of entities. This system performs well on
entities which are densely-connected in KG, but
not so well on sparsely-connected entities in the
KG.

We demonstrate this problem using the exam-
ple sentence in Figure 1. This sentence has two
mentions: Andrei Broder and WWW. The figure
also shows mention-entity linkages, i.e., mentions
and their candidate entities in KG. Using a conven-
tional EL system, the first mention Andrei Broder1

can be easily linked to Andrei Broder using
string similarity between the mention and candi-
date entity strings. String similarity works well
in this case as this mention is unambiguous in
the given setting. However, the second mention

1We use italics to denote textual mentions and
typewriter to indicate an entity in KG.

1844



WWW has two candidates, World Wide Web
and WWW conference, and hence is ambiguous.
In such cases, coherence measure between the
candidate entity and other unambiguously linked
entity(ies) is used for disambiguation.

State-of-the-art EL systems measure coherence
as similarity between embeddings of entities. The
entity embeddings are trained based on the num-
ber of common edges in KG2. In our example,
common edges are edges World Wide Web
shares with Andrei Broder and
edges WWW conference shares with
Andrei Broder. But WWW conference has
less number of edges (it is a sparsely-connected
entity) compared to World Wide Web. This
leads to poor performance3 whereby WWW is
erroneously linked to World Wide Web instead
of linking to WWW conference.

In this paper, we propose ELDEN, an EL sys-
tem which increases nodes and edges of the KG by
using information available on the web about enti-
ties and pseudo entities. Pseudo Entities are words
and phrases that frequently occur in Wikipedia,
and co-occur with mentions of KG entities in
the web corpus. Thus ELDEN uses a web cor-
pus to find pseudo entities and refines the co-
occurrences with Pointwise Mutual Information
(PMI) (Church and Hanks, 1989) measure. EL-
DEN then adds edges to the entity from pseudo en-
tities. In Figure 1, pseudo entity Program Commit-
tee co-occurs with mentions of Andrei Broder
and WWW conference in web corpus and has
a positive PMI value with both. So EL-
DEN adds edges from Program Committee to
Andrei Broder and WWW conference, den-
sifying neighborhood of the entities. Coherence,
now measured as similarity between entity embed-
dings where embeddings are trained on densified
KG, leads to improved EL performance.

Density (number of KG edges) of candidate en-
tity affects EL performance. In our analysis of
density and number of entities having that density
in the Wikipedia KG, we find that entities with 500
edges or less make up more than 90%. Thus, creat-
ing an EL system that performs well on densely as
well as sparsely-connected entities is a challeng-
ing, yet unavoidable problem.

2Wikipedia Link based Measure (WLM) (Milne and Wit-
ten, 2008) used in Yamada et al.‚Äôs system is based on number
of common edges in KG.

3This paper focuses on mention disambiguation. We as-
sume mention and candidate entities are detected already.

We make the following contributions:

‚Ä¢ ELDEN presents a simple yet effective graph
densification method which may be applied
to improve EL involving any KG.

‚Ä¢ By using pseudo entities and unambiguous
mentions of entity in a corpus, we demon-
strate how non-entity-linked corpus can be
used to improve EL performance.

‚Ä¢ We have made ELDEN‚Äôs code and data pub-
licly available4.

2 Related Work

Entity linking: Most EL systems use coherence
among entities (Cheng and Roth, 2013) to link
mentions. We studied coherence measures and
datasets used in six recent5 EL systems (He et al.,
2013; Huang et al., 2015; Sun et al., 2015; Ya-
mada et al., 2016; Globerson et al., 2016; Bar-
rena et al., 2016). We see that the two popu-
lar datasets used for evaluating EL (Chisholm and
Hachey, 2015) on documents are CoNLL (Hoffart
et al., 2011) and TAC2010 (Ji et al., 2010), here
after TAC. The popular coherence measures used
are (1) WLM, (2) Entity Embedding Similarity
and (3) Jaccard Similarity (Chisholm and Hachey,
2015; Guo et al., 2013). WLM is widely acknowl-
edged as most popular (Hoffart et al., 2012) with
almost all the six approaches analyzed above using
WLM or its variants. Entity embedding similarity
(Yamada et al., 2016) is reported to give highest
EL6 performance and is the baseline of ELDEN.
Enhancing entity disambiguation: Among
methods proposed in literature to enhance entity
disambiguation utilizing KG (Bhattacharya and
Getoor) uses additional relational information be-
tween database references; (Han and Zhao, 2010)
uses semantic relatedness between entities in other
KGs; and (Shen et al., 2018) uses paths con-
sisting of defined relations between entities in the
KG (IMDB and DBLP). All these methods utilize
structured information, while our method shows
how unstructured data (web corpus about the en-
tity to be linked) can be effectively used for entity
disambiguation.
Entity Embeddings: ELDEN presents a method
to enhance embedding of entities and words in a

4https://github.com/
priyaradhakrishnan0/ELDEN

5 (Shen et al., 2015) presents a survey of EL systems.
6Named Entity Disambiguation (NED) and EL are syn-

onymous terms in research (Hoffart et al., 2011)

1845



common vector space. Word embedding meth-
ods like word2vec (Mikolov et al., 2013) and
Glove (Pennington et al., 2014) have been ex-
tended to entities in EL (Yamada et al., 2016; Fang
et al., 2016; Zwicklbauer et al., 2016; Huang et al.,
2015). These methods use data about entity-entity
co-occurrences to improve the entity embeddings.
In ELDEN, we improve it with web corpus co-
occurrence statistics. Ganea and Hofmann (2017)
present a very interesting neural model for jointly
learning entity embedding along with mentions
and contexts.
KG densification with pseudo entities: KG den-
sification using external corpus has been studied
by Kotnis et al. (2015) and Hegde and Taluk-
dar (2015). Densifying edge graph is also studied
as ‚Äòlink prediction‚Äô in literature (Martƒ±ÃÅnez et al.,
2016). Kotnis et al. augment paths between KG
nodes using ‚Äòbridging entities‚Äô which are noun
phrases mined from an external corpus. ELDEN
has a similar approach as it proposes densifying
the KG edges of entities by adding edges from
pseudo entities. However, densification is used for
relation inference in the former methods whereas
it is used for entity coherence measurement in EL-
DEN.
Word co-occurrence measures: Chaudhari et
al. (2011) survey several co-occurrence mea-
sures for word association including PMI, Jac-
card (Dice, 1945) and Co-occurrence Significance
Ratio (CSR). Damani (2013) proves that consid-
ering corpus level significant co-occurrences, PMI
is better than others. Budiu et al. (2007) compare
Latent Semantic Analysis (LSA), PMI and Gener-
alized Latent Semantic Analysis (GLSA) and con-
clude that for large corpora like web corpus, PMI
works best on word similarity tests. Hence, we
chose PMI to refine co-occurring mentions of en-
tities in web corpus.

3 Definitions and Problem Formulation
In this section, we present a few definitions and
formulate the EL problem.
Knowledge Graph (KG): A Knowledge Graph is
defined as G = (E,F ) with entities E as nodes
and F as edges. In allegiance to EL literature
and baselines (Milne and Witten, 2008; Glober-
son et al., 2016), we use the Wikipedia hyperlink
graph as the KG in this paper, where nodes corre-
spond to Wikipedia articles and edges are incom-
ing links from one Wikipedia article to another.
ELDEN ultimately uses a densified version of this

Wikipedia KG, as described in Section 4.
Sparsely connected entities: Following Hoffart
et al. (2012), we define an entity to be a sparsely
connected entity, if the number of edges incident
on the entity node in the KG is less than thresh-
old Œ∑7. Otherwise, the entity is called a densely-
connected entity.
Entity Linking (EL): Given a set of mentions
MD = {m1, ...,mn} in a document D, and a
knowledge graph G = (E,F ), the problem of en-
tity linking is to find the assignment Œõ : MD ‚Üí
ED, where ED is the set of entities linked to men-
tions in document D such that ED ‚äÜ E.

For mention mi ‚àà MD, let the set of possible
entities it can link to (candidate entities) be Ci.
Then, the solution to the EL problem is an assign-
ment Œõ where,

Œõ(mi) = arg max
e‚ààCi

[œÜ(mi, e) + Œ≤ ¬∑ œà(e, ED)]
(1)

Here, œÜ(mi, e) ‚àà [0, 1] measures the contextual
compatibility of mentionmi and entity e. œÜ(mi, e)
is obtained by combining prior probability and
context similarity. œà(e, ED) measures the coher-
ence of e with entities in ED. Œ≤ is a variable con-
trolling inclusion of œà in the assignment Œõ.
Problem Formulation : (Yamada et al., 2016) is a
recently proposed state-of-the-art EL system. We
consider it as a representative EL system and use
it as the main baseline for the experiments in this
paper. In this section, we briefly describe Yamada
et al. (2016)‚Äôs two-step approach that solves the
EL problem presented above.

Step 1: A mention mi ‚àà MD is defined to be
unambiguous if ‚àÉe ‚àà Ci such that œÜ(mi, e) ‚â• Œ≥.
Let M (u)D ‚äÜ MD be the set of such unambigu-
ous mentions in document D. For all unambigu-
ous mentions m ‚àà M (u)D , Yamada et al. freeze
the assignment by solving Equation 1 after set-
ting Œ≤ = 0. In other words, œà(e, ED) is not used
while assigning entities to unambiguous mentions.
Assigning entities first to unambiguous mentions
has also been found to be helpful in prior research
(Milne and Witten, 2008; Guo and Barbosa, 2014).
Let AD be the set of entities linked to in this step.
In Figure 1, mention Andrei Broder is unambigu-
ous8.

7Like (Hoffart et al., 2012) we set Œ∑ = 500 for the exper-
iments in this paper.

8Between mention Andrei Broder and entity
Andrei Broder, string similarity and prior probabil-
ity are 1.0. In the experiments we use a Œ≥ value of 0.95.

1846



Notation Definition
ve entity embedding of entity e with dimension 1 ‚àó d
E set of all titles in Wikipedia

S set of all pseudo entities considered in ELDEN

E+ set of all entities considered in ELDEN

V word vectors ofE+ where V ‚àà Rk‚àód

œàELDEN Embedding similarity measured using V trained onGdense
œàYamada Embedding similarity measured using V trained on input

KGG

Table 1: Notation used in KG densification and learn-
ing entity embeddings (Please see Sec 4 for more de-
tails).

Step 2: In this step, Yamada et al. links all am-
biguous mentions by solving Equation 2.

Œõ(mi) =

arg max
e‚ààCi

Ô£´
Ô£≠œÜ(mi, e) +

1

|AD|
‚àë

ej‚ààAD
ve ¬∑ vej

Ô£∂
Ô£∏

‚àÄmi ‚ààMD \M (u)D (2)

where ve, vej ‚àà Rd are d-dimensional embed-
dings of entities e and ej respectively. Please
note that the equation above is a reformulation
of Equation 1 with Œ≤ = 1 and œà(e, ED) =
1
|AD|

‚àë
ej‚ààAD ve ¬∑ vej , where AD is derived from

ED as described in Step 1. As coherence
œà(e, ED) is applied only in disambiguation of am-
biguous mentions, we apply densification to only
selective nodes of KG.

Embeddings of entities are generated using
word2vec model and trained using WLM (Details
in Section 5). Given a graph G = (E,F ), the
WLM coherence measure œàwlm(ei, ej) between
two entities ei and ej is defined in Equation 3
where Ce is the set of entities with edge to entity
e.

œàwlm(ei, ej) = 1‚àí
log(max(|Cei |, |Cej |))‚àí log(|Cei ‚à© Cej )|)

log(|E|)‚àí log(min(|Cei |, |Cej |))
(3)

4 Our Approach: ELDEN

In this section, we present ELDEN, our proposed
approach. ELDEN extends (Yamada et al., 2016),
with one important difference: rather than work-
ing with the input KG directly, ELDEN works
with the densified KG, created by selective den-
sification of the KG with co-occurrence statistics
extracted from a large corpus. Even though this

is a simple change, this results in improved EL
performance. The method performs well even for
sparsely connected entities.
Overview : Overview of the ELDEN system is
shown in Figure 2. ELDEN starts off with densi-
fication of the input KG, using statistics from web
corpus. Embeddings of entities are then learned
utilizing the densified KG in the next step. Em-
bedding similarity estimated using the learned en-
tity embeddings is used in calculating coherence
measure in subsequent EL. Notation used is sum-
marized in Table 1.
(i)KG Densification Figure 2 depicts densifica-
tion of KG in ‚ÄòInput KG‚Äô and ‚ÄòDensified KG‚Äô. It
shows two Wikipedia titles Andrei Broder and
WWW conference from our running example
(Figure 1). There are no edges common between
Andrei Broder and WWW conference. In a
web corpus, mentions of Andrei Broder and
WWW conference co-occur with Program com-
mittee and it has a positive PMI value with both
the entities. So ELDEN adds an edge from Pro-
gram committee to both the entities. Here Pro-
gram committee is a pseudo entity. Thus, ELDEN
densifies the KG by adding edges from pseudo en-
tities when the mentions of Wikipedia entity and
pseudo entity co-occur in a web corpus and the
pseudo entity has a positive PMI value with given
entity.

Taking a closer look, KG densification pro-
cess starts from ‚Äòinput KG‚Äô which is Wikipedia
hyperlink graph G = (E,F ), where the nodes
are Wikipedia titles (E) and edges are hyperlinks
(F ). ELDEN processes Wikipedia text corpus and
identifies phrases (unigrams and bi-grams) that oc-
cur frequently, i.e. more than 10 times in it. We
denote these phrases as pseudo entities (S) and
add them as nodes to the KG. Let E+ = E ‚à™ S
be the resulting set of nodes.

ELDEN then adds edges connecting entities in
E+ to entities in E. This is done by processing
a web text corpus looking for mentions of enti-
ties in E+, and linking the mentions to entities in
KG G

‚Ä≤
= (E+, F ). ELDEN uses Equation 1 with

Œ≤ = 0 for this entity linking, i.e. only mention-
entity similarity œÜ(m, e) is used during this link-
ing9. Based on this entity linked corpus, a co-
occurrence matrix M of size |E+| √ó |E+| is con-
structed. Each cell Mi,j is set to the PMI between

9Since prior probabilities of pseudo entities are not avail-
able, only mention-entity similarity component of œÜ(m, s) is
used while linking a mention m to a pseudo entity s ‚àà S.

1847



Input KG Densified KG

Densification

Web Corpus

Andrei_Broder

WWW_conference

Andrei_Broder

WWW_conference

Program 
Committee

Embedding similarity = ùùçELDEN = Cosine (vEntity1 , vEntity2)
Cosine (vAndrei_Broder , vWWW_conference)   > Cosine (vAndrei_Broder, vWorld_Wide_Web)

Learn Entity 
Representation

.0.02,..0.11‚Ä¶0.14..

.0.05,..0.01...0.06..

vAndrei_Broder

vWWW_conference

.0.03,..0.12‚Ä¶0.15.. vWorld_Wide_Web

Entity Embeddings (V)

Figure 2: ELDEN consists of KG densification, training entity embeddings on densified KG and building EL
system that uses similarity between the trained embeddings as coherence measure. ELDEN‚Äôs difference from
baseline method is that while baseline method uses input KG for training ve, ELDEN uses densified KG for
training ve. Hence, the improved performance of ELDEN is solely attributed to densification.

Objective function
component

Feature
Group

Feature Explanation

Contextual
compati-
bility
œÜ(mi, ei)

Base

Entity Prior Fraction of edges that links to the entity
Prior Probability Of all possible pages a mention can link to, the probability that it links to a

given page
Max Prior Probability Maximum value of prior probabilities for a mention
Max Prior Probability in document Maximum value of prior probabilities for a document
Num of candidates Candidate count of the mention

String
Similarity

Exact match Whether mention is an exact match of candidate
Partial match Whether mention is part of candidate

Coherence
œà(ei, ej)

Coherence
measures

œàwlm WLM (Please see Equation 3)
œàYamada Similarity between entity embeddings that are learned using œàwlm
œàdense œàwlm calculated on densified KB
œàELDEN Similarity between entity embeddings that are trained using œàdense
œàELDEN++ Combination of œàwlm, œàYamada, œàdense, and œàELDEN. ELDEN uses

this coherence feature.

Table 2: Features used by various EL systems. Context compatibility œÜ(mi, ei) and coherence œà(ei, ej) of Equa-
tion 1 are parameterized using features as shown above. Context compatibility features are used by all systems.
Coherence features used by Yamada16 are œàwlm and œàYamada. ELDEN uses œàELDEN++ as coherence feature.

the entities e and e
‚Ä≤
.

Me,e‚Ä≤ = PMI(e, e
‚Ä≤
) = log

f(e, e
‚Ä≤
)√óN

f(e)√ó f(e‚Ä≤)
where f(e) is the frequency of entity e in web cor-
pus, f(e, e

‚Ä≤
) is the sentence-constrained pair fre-

quency of the entity pair (e, e
‚Ä≤
) in web corpus, and

N =
‚àë

e,e‚Ä≤‚ààE+ f(e, e
‚Ä≤
). Please note that PMI, and

there by M , are symmetric. The expanded set of
edges, F+, is now defined as

F+ = F‚à™{(e, e
‚Ä≤
), (e

‚Ä≤
, e) | e‚Ä≤ ‚àà E+, e ‚àà E,Me,e‚Ä≤ > 0}

In other words, we augment the set of initial edges
F with additional edges connecting entities in E+
with entities in E such that PMI between the enti-
ties is positive.

ELDEN now constructs the KG Gdense =
(E+, F+), which is a densified version of the in-
put KG G = (E,F ). ELDEN uses this densified
KG Gdense for subsequent processing and entity
linking.
(ii)Learning Embeddings of Densified KG En-
tities ELDEN derives entity embeddings using
the same setup, corpus and Word2vec skip-gram
with negative sampling model as in Yamada et al.,
However, instead of training embeddings over the
input KG, ELDEN trains embeddings of entities in
the densified KG Gdense. Let V be the word2vec
matrix containing embeddings of entities in E+
where V ‚àà Rk‚àód. ve is the embedding of entity
e in E+ with dimension 1 ‚àó d.

In word2vec model, entities in context are used

1848



to predict the target entity. ELDEN maximizes
the objective function (Goldberg and Levy, 2014)
of word2vec skip-gram model with negative sam-
pling, L =

‚àë
(t,c)‚ààP Lt,c where

Lt,c = log Œ∏(vc ¬∑ vt) +
‚àë

n‚ààN(t,c)
log Œ∏(‚àívn ¬∑ vt)

Here vt and vc are the entity embeddings of tar-
get entity t and context entity c. P is the set
of target-context entity pairs considered by the
model. N(t,c) is a set of randomly sampled enti-
ties used as negative samples with pair (t, c). This
objective is maximized with respect to variables
vt‚Äôs and vc‚Äôs, where Œ∏(x) = 11+e‚àíx .
P and N are derived usingGdense. t and c are en-

tities inE+ such that c shares a common edge with
t. vn is randomly sampled from V, for entities that
do not share a common edge with t. Entity em-
bedding similarity measured using V trained this
way on Gdense is œàELDEN. Embedding similarity
is measured as cosine distance between ves. Em-
beddings of S are trained using positive and nega-
tive word contexts derived using context length.
(iii) Bringing it All Together: ELDEN

ELDEN is a supervised EL system which uses
two sets of features: (1) contextual compatibility
œÜ(m, e); and (2) coherence œà(ei, ej). These fea-
tures are summarized in Table 2. Similarity be-
tween entity embeddings is measured as cosine
similarity between ves.

5 Experiments

In this section, we evaluate the following:

‚Ä¢ Is ELDEN‚Äôs corpus co-occurrence statistics-
based densification helpful in disambiguating
entities better? (Sec. 6.1)

‚Ä¢ Where does ELDEN‚Äôs selective densification
of KG nodes link entities better? (Sec. 6.3)

Setup : ELDEN is implemented using Random
Forest ensemble 10 (Breiman, 1998). Parameter
values were set using CoNLL development set.
Feature limit of 3 with number of estimators as
100 yielded best performance.
Knowledge Graph: Wikipedia Following prior
EL literature, we use Wikipedia hypergraph as
our KG (Milne and Witten, 2008; Globerson
et al., 2016). This KG is enhanced with pseudo
entities as explained in Section 4. We process

10http://scikit-learn.org

Specifics Value
Number of titles after cleaning (|E|) 4.6 M
Number of pseudo entities (|S|) 1.3 M
Total number of entities (|E+|) 5.9 M
Number of epochs 3

Learning rate 0.25 linearly reducing to 0.01

Number of negative samples (|N|) 5
Context window size 3

Dimensions of embedding (d) 100

Table 3: Parameters used in Wikipedia processing and
training KG.

the Wikipedia corpus following the same proce-
dure as in Yamada et al. (2016). We cleaned 11

Wikipedia dump dated Nov 2015. We then parsed
the Wikipedia article text to identify pseudo enti-
ties. More details on KG and parameters used for
training embeddings are in Table 3. Training took
4 days on gpu with 2 cores.
Preprocessing: Web corpus and Densified KG
For our experiments, we created a web corpus by
querying Google12. Candidate entities of all men-
tions in the dataset are queried in Google and top
ten search results are considered for unigram and
bigram frequencies. This corpus occupied 6.8GB
for candidate entities of TAC and CoNLL dataset
mentions (54336 entities). Even for sparsely con-
nected entities, an average corpus size of 670 lines
or more13 was collected. We note that though
some of the entities mentioned in this dataset are
ten or more years old, we are able to collect, on an
average more than 670 lines of web content. Thus
corpus proves to be a good source of additional
links for densification, for both common and rare
entities. As Taneva and Weikum (2013) also note,
it is not hard to find content about sparsely con-
nected entities on the web.

The web corpus is analyzed for mentions and
pseudo entities. Co-occurrence matrix M is cre-
ated14 for mention and pseudo entities occurring
within window of size 10 for PMI calculation15.
Edges are added from pseudo entities with positive
PMI to mention of given entity. In experiments we
add edges from top 10 pseudo entities ordered by

11by removing disambiguation, navigation, maintenance
and discussion pages.

12https://www.google.com/
13A detailed analysis of knowledge gained from crawling

for common versus less common entities is present in Figure
1 of supplementary material.

14This co-occurrence matrix is downloadable with source
code.

15We experimented with window sizes 10, 25 and 50. We
chose 10 that gave best results

1849



PMI values16.
Evaluation Dataset: In line with prior work
on EL, we test the performance of ELDEN on
CoNLL and TAC datasets. As this paper fo-
cuses on entity disambiguation, we tested ELDEN
against datasets and baseline methods for disam-
biguation. We note that the entity disambiguation
evaluation part of other recent datasets like ERD
2014 and TAC 2015 is exactly same as the TAC
2010 evaluation (Ellis et al., 2014) 17.
Training: ELDEN‚Äôs parameters were tuned us-
ing training (development) sets of CoNLL and
TAC datasets. CoNLL and TAC datasets consist
of documents where mentions are marked and en-
tity to which the mention links to, is specified. We
use only mentions that link to a valid Wikipedia
title (non NIL entities) and report performance on
test set. Some aspects of these datasets relevant to
our experiments are provided below.
CoNLL: In CoNLL test set (5267 mentions), we
report Precision of topmost candidate entity, ag-
gregated over all mentions (P-micro) and aggre-
gated over all documents (P-macro), i.e., if tp, fp
and p are the individual true positives, false posi-
tives and precision for each document in a dataset
of Œ¥ documents, then

Pmicro =

Œ¥‚àë
i=1

tpi

Œ¥‚àë
i=1

tpi+
Œ¥‚àë
i=1

fpi

and Pmacro =

Œ¥‚àë
i=1

pi

Œ¥ .

For CoNLL candidate entities, we use (Pershina
et al., 2015) dataset18.
TAC: In TAC dataset, we report P-micro of top-
ranked candidate entity on 1,020 mentions. P-
macro is not applicable to TAC as most documents
have only one mention as query mention ( or ‚Äômen-
tion to be linked‚Äô). For TAC candidate entities, we
index the Wikipedia word tokens and titles using
solr19. We index terms in (1) title of the entity, (2)
title of another entity redirecting to the entity, and
(3) names of anchors that point to the entity, in line
with baselines. We are making this TAC candidate
set publicly available.
Baseline: Yamada16 Our baseline is the Yamada
et al. system explained in Section 3. Entity em-
bedding distance measured using ve trained on the

16This is a tunable parameter.
17These recent datasets consist of other evaluations, e.g.,

mention detection, multilinguality etc. which is beyond the
scope of the paper and hence we didnt focus on them in the
paper.

18https://github.com/masha-p/PPRforNED
19http://lucene.apache.org/solr/

Method CONLL

(P-micro)

CONLL

(P-macro)

TAC

(P-micro)
(Hoffart et al., 2011) 82.5 81.7 -
(He et al., 2013) 85.6 84.0 81.0

(Ling et al., 2015) 67.5 - 86.8
(Barrena et al., 2016) 88.32 - -
(Chisholm and Hachey,
2015)

88.7 - 80.7

(Pershina et al., 2015) 91.8 89.9 -
(Globerson et al., 2016) 91.7 - 87.2
(Yamada et al., 2016) 93.1 92.6 85.2
ELDEN 93.0 93.7 89.6

Table 4: Performance comparison with other recent EL
approaches. ELDEN matches best results in CoNLL
and outperforms the state-of-the-art in TAC dataset.
(Please see Section 6.1 for details and œàELDEN++ row
of Table 5 for ELDEN results.)

input KG G is œàYamada.

6 Results

6.1 Does ELDEN‚Äôs selective densification
help in disambiguation in EL?

In Table 4, we compare ELDEN‚Äôs EL performance
with results of other recently proposed state-of-
the-art EL methods that use coherence models. We
see that ELDEN results matches best results on
CoNLL and outperforms state-of-the-art in TAC
dataset. In the table, the last four rows uses the
Pershina et al. (2015) candidate set and hence, we
provide a comparison of their disambiguation per-
formance. Improved results of ELDEN over base-
line is attributed to the improved disambiguation
due to KG densification.

6.2 Why does ELDEN‚Äôs selective
densification work?

We conduct ablation analysis using various fea-
ture and feature combinations and present perfor-
mance of ELDEN and baseline in Table 5. Start-
ing with base features, we add various features to
ELDEN incrementally and report their impact on
performance. The results when using base feature
group alone, and base and string similarity groups
together (œÜ) are presented in first and second rows
for each dataset. We compare œàELDEN to three co-
herence measures: œàwlm, œàYamada and œàdense, de-
tails of which are provided in Table 2. The perfor-
mance improvement from each of the four coher-
ence measures are in the next four rows. Perfor-
mance of ELDEN from using all four coherence
features is given in œàELDEN++ row.

On CoNLL dataset, œàdense combined with œÜ,

1850



Dataset Features P-micro P-macro

CoNLL

Base 87.0 88.3

œÜ 90.0 91.2

œÜ+ œàwlm 91.0* 91.8

œÜ+ œàYamada 90.0* 91.1

œÜ+ œàdense 92.0* 93.0

œÜ+ œàELDEN 91.0* 91.2

œÜ+ œàwlm + œàYamada 91.0 91.8

œÜ+ œàdense + œàELDEN 91.0 92.6

œÜ+ œàELDEN++ 93.0 93.7

TAC

Base 78.2

œÜ 80.2

œÜ+ œàwlm 82.5*

œÜ+ œàYamada 83.1*

œÜ+ œàdense 85.5*

œÜ+ œàELDEN 87.3*

œÜ+ œàwlm + œàYamada 84.4

œÜ+ œàdense + œàELDEN 88.7

œÜ+ œàELDEN++ 89.6

Table 5: Ablation analysis involving various coher-
ence measures (see Table 2 for definitions of these
measures). Statistically significant improvements over
œÜ are marked with an asterisk. ELDEN‚Äôs coherence
measure, œàELDEN++, achieves the best overall perfor-
mance. P-macro is not applicable to TAC as most docu-
ments have only one mention marked as query. (Please
see Section 6.2).

gave an improvement of 2.0 and 1.9 (P-micro and
P-macro) over Yamada16 results. We note that Ya-
mada16 results are from our re-implementation of
(Yamada et al., 2016) system 20 and we are able
to almost reproduce the baseline results. We also
present the results combining baseline‚Äôs œàYamada
and œàwlm versus ELDEN‚Äôs œàELDEN and œàdense in
next two rows. We find the ELDEN‚Äôs KG densifi-
cation features perform better than baselines.

On TAC dataset also, combined with œÜ, œàdense
is found to do better than œàwlm and œàELDEN gives
a significant P-micro improvement of 4.2 over
œàYamada. The œàELDEN++ P-micro in TAC dataset
is statistically significant21. In short, we find the
KG densification features, œàdense and œàELDEN,
as the features causing better performance of EL-
DEN on both datasets.

6.3 Where does ELDEN‚Äôs selective
densification work better?

While most EL systems give higher precision on
CoNLL dataset than TAC dataset, ELDEN per-
forms with high precision on TAC dataset too.

20We have re-implemented the Yamada et al system us-
ing hyper-parameters specified in the paper and these are our
best-effort results.

21We performed two tailed t-test, with 2-tail 95% value of
1.96.

% sparsely connected Entities

Dataset Train Test
TAC 78.8 63.6

CoNLL 48.4 48.2

Table 6: Percentage of sparsely connected entities in
evaluation datasets. TAC has higher composition of
sparsely connected entities than CoNLL. Hence, EL-
DEN results are better in TAC over CoNLL (Please see
Table 4).

This is explained by analyzing distribution of
densely-connected and sparsely connected entities
in TAC and CoNLL datasets as presented in Ta-
ble 6. We see that CoNLL test set has almost half
as densely-connected and half as sparsely con-
nected entities, whereas in TAC test set, 63.6%
are sparsely connected entities. This higher con-
stitution of sparsely connected entities in TAC, ex-
plains ELDEN‚Äôs better results in TAC relative to
CoNLL dataset. As the number of sparsely con-
nected entities is more than the number of densely-
connected entities in most KGs (Reinanda et al.,
2016), our method is expected to be of significance
for most KGs.

6.4 What type of EL errors are best fixed
with ELDEN‚Äôs selective densification ?

We analyzed errors fixed by ELDEN on TAC
dataset. We categorize the errors into four classes
in line with error classes of Ling et al. (2015). We

0

10

20

30

40

50

60

70

Synonym Acronym Specific	label Miscellaneous

False	Positive	Analysis

FP FP	after	densification

Figure 3: False positives of ELDEN before and after
KG densification. Errors reduce with use of œàdense and
œàELDEN measures. (Please see Sec 6.4)

manually analyzed 240 wrong predictions of Ya-
mada16 and compared it with that of ELDEN, and
the results are presented in Figure 3. We found
errors to reduce with use of KG densification fea-
tures and most of the errors eliminated were in
‚ÄúSpecific label‚Äù class. Errors in this class called
for better modeling of mention‚Äôs context and link-
based similarity (Ling et al., 2015).(More details
of this analysis in the supplementary document.)

1851



7 Conclusion

We started this study by analyzing the perfor-
mance of a state-of-the-art Entity Linking (EL)
system and found that its performance was low
when linking entities sparsely-connected in the
KG. We saw that this can be addressed by densi-
fying the KG with respect to the given entity. We
proposed ELDEN, which densifies edge graph of
entities using pseudo entities and mentions of en-
tities in a large web corpus. Through our experi-
ments, we find that ELDEN outperforms state-of-
the-art baseline on benchmark datasets.

We believe that ELDENs combination of KG
densification and entity embeddings is novel. Poor
performance of EL systems on sparsely connected
entities has been recognized as one of the open
challenges by prior research. ELDEN performs
well on sparsely connected entities too, as a val-
idation of our method of combining KG densifica-
tion followed by embedding. Our approach may
be applied to any KG as the densification is per-
formed with the help of unstructured data, and
not any specific KG. We hope the simple graph
densification method utilized in ELDEN will be
of much interest to the research community.

Pseudo entities can be looked at as entity can-
didates for KG expansion, as also noted by Farid
et al. (2016). In future, we plan to enhance EL-
DEN using EL of pseudo entities to estimate en-
tity prior of entities not present in KG. We also
plan to explore entity embeddings obtained using
other graph densifying methods.

8 Acknowledgments

We thank the Microsoft Research India Travel
Grants for generous travel funds to attend and
present this paper at NAACL.

References
Ander Barrena, Aitor Soroa, and Eneko Agirre. 2016. Allevi-

ating poor context with background knowledge for named
entity disambiguation. In ACL (1). The Association for
Computer Linguistics.

Indrajit Bhattacharya and Lise Getoor. ???? Online collective
entity resolution.

Leo Breiman. 1998. Arcing classifier (with discussion and
a rejoinder by the author). Ann. Statist. 26(3):801‚Äì849.
https://doi.org/10.1214/aos/1024691079.

Raluca Budiu, Christiaan Royer, and Peter Pirolli. 2007.
Modeling information scent: A comparison of lsa, pmi
and glsa similarity measures on common tests and cor-
pora. In Large Scale Semantic Access to Content

(Text, Image, Video, and Sound). Paris, France, France,
RIAO ‚Äô07, pages 314‚Äì332. http://dl.acm.org/
citation.cfm?id=1931390.1931422.

Dipak Chaudhari, Om P. Damani, and Srivatsan Laxman.
2011. Lexical co-occurrence, statistical significance, and
word association. CoRR abs/1008.5287.

Xiao Cheng and Dan Roth. 2013. Relational inference for
wikification. In EMNLP. ACL, pages 1787‚Äì1796.

Andrew Chisholm and Ben Hachey. 2015. Entity
disambiguation with web links. Transactions of
the Association for Computational Linguistics 3:145‚Äì
156. https://tacl2013.cs.columbia.edu/
ojs/index.php/tacl/article/view/494.

Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. ACL ‚Äô89, pages 76‚Äì83. https://doi.org/10.
3115/981623.981633.

Om P. Damani. 2013. Improving pointwise mutual infor-
mation (PMI) by incorporating significant co-occurrence.
CoRR abs/1307.0596.

Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology 26(3):297‚Äì302.
https://doi.org/10.2307/1932409.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn,
Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun,
and Wei Zhang. 2014. Knowledge vault: A web-scale
approach to probabilistic knowledge fusion. ACM, New
York, NY, USA, KDD ‚Äô14, pages 601‚Äì610. https:
//doi.org/10.1145/2623330.2623623.

Joe Ellis, Jeremy Getman, and Stephanie Strassel. 2014.
Overview of linguistic resource for the tac kbp 2014 eval-
uations: Planning, execution, and results. Linguistic Data
Consortium, University of Pennsylvania.

Weiyi Fang, Jianwen Zhang, Dilin Wang, Zheng Chen, and
Ming Li. 2016. Entity disambiguation by knowledge and
text jointly embedding. In CoNLL.

Mina Farid, Ihab F. Ilyas, Steven Euijong Whang, and Cong
Yu. 2016. Lonlies: Estimating property values for long
tail entities. In Proceedings of the 39th International
ACM SIGIR Conference on Research and Development
in Information Retrieval. ACM, New York, NY, USA, SI-
GIR ‚Äô16, pages 1125‚Äì1128. https://doi.org/10.
1145/2911451.2911466.

Octavian-Eugen Ganea and Thomas Hofmann. 2017. Deep
joint entity disambiguation with local neural attention.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing. Association for
Computational Linguistics, pages 2609‚Äì2619. http:
//www.aclweb.org/anthology/D17-1276.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti, Amar-
nag Subramanya, Michael Ringaard, and Fernando
Pereira. 2016. Collective entity resolution with multi-
focal attention. In ACL.

Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.‚Äôs negative-sampling word-
embedding method. CoRR abs/1402.3722. http://
arxiv.org/abs/1402.3722.

1852



Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013. To link or not to link? a study on end-to-
end tweet entity linking. In NAACL-HLT 2013.
http://research.microsoft.com/apps/
pubs/default.aspx?id=183909.

Zhaochen Guo and Denilson Barbosa. 2014. Robust entity
linking via random walks. ACM, New York, NY, USA,
CIKM ‚Äô14, pages 499‚Äì508. https://doi.org/10.
1145/2661829.2661887.

Xianpei Han and Jun Zhao. 2010. Structural semantic relat-
edness: A knowledge-based method to named entity dis-
ambiguation. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Stroudsburg, PA,
USA, ACL ‚Äô10, pages 50‚Äì59. http://dl.acm.org/
citation.cfm?id=1858681.1858687.

Zhengyan He, Shujie Liu, Yang Song, Mu Li, Ming
Zhou, and Houfeng Wang. 2013. Efficient col-
lective entity linking with stacking. EMNLP.
http://research.microsoft.com/apps/
pubs/default.aspx?id=202249.

Manjunath Hegde and Partha P. Talukdar. 2015. An entity-
centric approach for overcoming knowledge graph spar-
sity. Association for Computational Linguistics, Lis-
bon, Portugal, EMNLP ‚Äô15, pages 530‚Äì535. http:
//aclweb.org/anthology/D15-1061.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin
Theobald, and Gerhard Weikum. 2012. Kore: Keyphrase
overlap relatedness for entity disambiguation. ACM, New
York, NY, USA, CIKM ‚Äô12, pages 545‚Äì554. https:
//doi.org/10.1145/2396761.2396832.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Ha-
gen FuÃàrstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stefan Thater, and Gerhard Weikum. 2011. Ro-
bust disambiguation of named entities in text. Association
for Computational Linguistics, Stroudsburg, PA, USA,
EMNLP ‚Äô11, pages 782‚Äì792. http://dl.acm.org/
citation.cfm?id=2145432.2145521.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015. Lever-
aging deep neural networks and knowledge graphs for
entity disambiguation. CoRR abs/1504.07678. http:
//arxiv.org/abs/1504.07678.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and
Joe Ellis. 2010. Overview of the tac 2010 knowledge base
population track. In In Third Text Analysis Conference
(TAC).

Bhushan Kotnis, Pradeep Bansal, and Partha P. Talukdar.
2015. Knowledge base inference using bridging enti-
ties. In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015. pages 2038‚Äì
2043. http://aclweb.org/anthology/D/D15/
D15-1241.pdf.

Xiao Ling, Sameer Singh, and Dan Weld. 2015. Design chal-
lenges for entity linking. Transactions of the Association
for Computational Linguistics (TACL) 3.

Vƒ±ÃÅctor Martƒ±ÃÅnez, Fernando Berzal, and Juan-Carlos Cubero.
2016. A survey of link prediction in complex networks.
ACM Comput. Surv. 49(4):69:1‚Äì69:33. https://doi.
org/10.1145/3012704.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Cor-
rado, and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality. In NIPS
2013. pages 3111‚Äì3119.

David Milne and Ian H. Witten. 2008. Learning to Link with
Wikipedia. ACM, CIKM‚Äô08, pages 509‚Äì518.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word represen-
tation. In Empirical Methods in Natural Language Pro-
cessing (EMNLP). pages 1532‚Äì1543. http://www.
aclweb.org/anthology/D14-1162.

Maria Pershina, Yifan He, and Ralph Grishman. 2015. Per-
sonalized page rank for named entity disambiguation,
Association for Computational Linguistics (ACL), pages
238‚Äì243. NAACL HLT 2015.

Ridho Reinanda, Edgar Meij, and Maarten de Rijke. 2016.
Document filtering for long-tail entities. ACM, CIKM‚Äô16.

W. Shen, J. Han, J. Wang, X. Yuan, and Z. Yang. 2018.
Shine+: A general framework for domain-specific en-
tity linking with heterogeneous information networks.
IEEE Transactions on Knowledge and Data Engineer-
ing 30(2):353‚Äì366. https://doi.org/10.1109/
TKDE.2017.2730862.

W. Shen, J. Wang, and J. Han. 2015. Entity linking with
a knowledge base: Issues, techniques, and solutions.
IEEE Transactions on Knowledge and Data Engineer-
ing 27(2):443‚Äì460. https://doi.org/10.1109/
TKDE.2014.2327028.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-
zhou Ji, and Xiaolong Wang. 2015. Modeling men-
tion, context and entity with neural networks for en-
tity disambiguation. In Qiang Yang and Michael
Wooldridge, editors, IJCAI. AAAI Press, pages 1333‚Äì
1339. http://dblp.uni-trier.de/db/conf/
ijcai/ijcai2015.html#SunLTYJW15.

Bilyana Taneva and Gerhard Weikum. 2013. Gem-based
entity-knowledge maintenance. ACM, New York, NY,
USA, CIKM ‚Äô13, pages 149‚Äì158. https://doi.
org/10.1145/2505515.2505715.

Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and
Yoshiyasu Takefuji. 2016. Joint learning of the embed-
ding of words and entities for named entity disambigua-
tion. CoNLL 2016, pages 250‚Äì259. http://aclweb.
org/anthology/K/K16/K16-1025.pdf.

Stefan Zwicklbauer, Christin Seifert, and Michael Gran-
itzer. 2016. Robust and collective entity disambiguation
through semantic embeddings. In Proceedings of the
39th International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM, New
York, NY, USA, SIGIR ‚Äô16, pages 425‚Äì434. https:
//doi.org/10.1145/2911451.2911535.

1853


