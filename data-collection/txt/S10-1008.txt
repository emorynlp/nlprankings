



















































SemEval-2010 Task 10: Linking Events and Their Participants in Discourse


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45–50,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse

Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics

Saarland University
{josefr,csporled}@coli.uni-sb.de

Roser Morante
CNTS

University of Antwerp
Roser.Morante@ua.ac.be

Collin Baker
ICSI

Berkeley, CA 94704
collin@icsi.berkeley.edu

Martha Palmer
Department of Linguistics

University of Colorado at Boulder
martha.palmer@colorado.edu

Abstract

We describe the SemEval-2010 shared
task on “Linking Events and Their Partic-
ipants in Discourse”. This task is an ex-
tension to the classical semantic role label-
ing task. While semantic role labeling is
traditionally viewed as a sentence-internal
task, local semantic argument structures
clearly interact with each other in a larger
context, e.g., by sharing references to spe-
cific discourse entities or events. In the
shared task we looked at one particular as-
pect of cross-sentence links between ar-
gument structures, namely linking locally
uninstantiated roles to their co-referents
in the wider discourse context (if such
co-referents exist). This task is poten-
tially beneficial for a number of NLP ap-
plications, such as information extraction,
question answering or text summarization.

1 Introduction

Semantic role labeling (SRL) has been defined as
a sentence-level natural-language processing task
in which semantic roles are assigned to the syntac-
tic arguments of a predicate (Gildea and Jurafsky,
2002). Semantic roles describe the function of the
participants in an event. Identifying the seman-
tic roles of the predicates in a text allows knowing
who did what to whom when where how, etc.

However, semantic role labeling as it is cur-
rently defined misses a lot of information due to
the fact that it is viewed as a sentence-internal
task. Hence, relations between different local se-
mantic argument structures are disregarded. This
view of SRL as a sentence-internal task is partly
due to the fact that large-scale manual annotation

projects such as FrameNet1 and PropBank2 typ-
ically present their annotations lexicographically
by lemma rather than by source text.

It is clear that there is an interplay between lo-
cal argument structure and the surrounding dis-
course (Fillmore, 1977). In early work, Palmer et
al. (1986) discussed filling null complements from
context by using knowledge about individual pred-
icates and tendencies of referential chaining across
sentences. But so far there have been few attempts
to find links between argument structures across
clause and sentence boundaries explicitly on the
basis of semantic relations between the predicates
involved. Two notable exceptions are Fillmore and
Baker (2001) and Burchardt et al. (2005). Fillmore
and Baker (2001) analyse a short newspaper arti-
cle and discuss how frame semantics could benefit
discourse processing but without making concrete
suggestions of how to model this. Burchardt et al.
(2005) provide a detailed analysis of the links be-
tween the local semantic argument structures in a
short text; however their system is not fully imple-
mented either.

With the shared task, we aimed to make a first
step towards taking SRL beyond the domain of
individual sentences by linking local semantic ar-
gument structures to the wider discourse context.
The task addresses the problem of finding fillers
for roles which are neither instantiated as direct
dependents of our target predicates nor displaced
through long-distance dependency or coinstantia-
tion constructions. Often a referent for an unin-
stantiated role can be found in the wider context,
i.e. in preceding or following sentences. An ex-
ample is given in (1), where the CHARGES role

1http://framenet.icsi.berkeley.edu/
2http://verbs.colorado.edu/˜mpalmer/

projects/ace.html

45



(ARG2 in PropBank) of cleared is left empty but
can be linked to murder in the previous sentence.

(1) In a lengthy court case the defendant was
tried for murder. In the end, he was
cleared.

Another very rich example is provided by (2),
where, for instance, the experiencer and the ob-
ject of jealousy are not overtly expressed as depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.

(2) Watson won’t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.

This paper is organized as follows. In Section 2
we define how the concept of Null Instantiation
is understood in the task. Section 3 describes the
tasks to be performed, and Section 4, how they
are evaluated. Section 5 presents the participant
systems, and Section 6, their results. Finally, in
Section 7, we put forward some conclusions.

2 Null Instantiations

The theory of null complementation used here is
the one adopted by FrameNet, which derives from
the work of Fillmore (1986).3 Briefly, omissions
of core arguments of predicates are categorized
along two dimensions, the licensor and the in-
terpretation they receive. The idea of a licensor
refers to the fact that either a particular lexical item
or a particular grammatical construction must be
present for the omission of a frame element (FE)
to occur. For instance, the omission of the agent in
(3) is licensed by the passive construction.

(3) No doubt, mistakes were made
0Protagonist.

The omission is a constructional omission be-
cause it can apply to any predicate with an appro-
priate semantics that allows it to combine with the
passive construction. On the other hand, the omis-
sion in (4) is lexically specific: the verb arrive al-
lows the Goal to be unspecified but the verb reach,
also a member of the Arriving frame, does not.

(4) We arrived 0Goal at 8pm.

3Palmer et al.’s (1986) treatment of uninstantiated ‘essen-
tial roles’ is very similar (see also Palmer (1990)).

The above two examples also illustrate the sec-
ond major dimension of variation. Whereas, in (3)
the protagonist making the mistake is only existen-
tially bound within the discourse (instance of in-
definite null instantiation, INI), the Goal location
in (4) is an entity that must be accessible to speaker
and hearer from the discourse or its context (def-
inite null instantiation, DNI). Finally, note that
the licensing construction or lexical item fully and
reliably determines the interpretation. Whereas
missing by-phrases have always an indefinite in-
terpretation, whenever arrive omits the Goal lexi-
cally, the Goal has to be interpreted as definite, as
it is in (4).

The import of this classification to the task here
is that we will concentrate on cases of DNI, be
they licensed lexically or constructionally.

3 Description of the Task

3.1 Tasks

We originally intended to offer the participants a
choice of two different tasks: a full task, in which
the test set was only annotated with gold stan-
dard word senses (i.e., frames) for the target words
and the participants had to perform role recogni-
tion/labeling and null instantiation linking, and a
NI only task, in which the test set was already
annotated with gold standard semantic argument
structures and the participants only had to recog-
nize definite null instantiations and find links to
antecedents in the wider context (NI linking).

However, it turned out that the basic semantic
role labeling task was already quite challenging
for our data set. Previous shared tasks have shown
that frame-semantic SRL of running text is a hard
problem (Baker et al., 2007), partly due to the fact
that running text is bound to contain many frames
for which no or little annotated training data are
available. In our case the difficulty was increased
because our data came from a new genre and do-
main (i.e., crime fiction, see Section 3.2). Hence,
we decided to add standard SRL, i.e., role recogni-
tion and labeling, as a third task (SRL only). This
task did not involve NI linking.

3.2 Data

The participants were allowed to make use of a va-
riety of data sources. We provided a training set
annotated with semantic argument structure and
null instantiation information. The annotations
were originally made using FrameNet-style and

46



later mapped semi-automatically to PropBank an-
notations, so that participants could choose which
framework they wanted to work in. The data for-
mats we used were TIGER/SALSA XML (Erk
and Padó, 2004) (FrameNet-style) and a modified
CoNLL-format (PropBank-style). As it turned
out, all participants chose to work on FrameNet-
style annotations, so we will not describe the Prop-
Bank annotation in this paper (see Ruppenhofer et
al. (2009) for more details).

FrameNet-style annotation of full text is ex-
tremely time-consuming. Since we also had to an-
notate null instantiations and co-reference chains
(for evaluation purposes, see Section 4), we could
only make available a limited amount of data.
Hence, we allowed participants to make use of ad-
ditional data, in particular the FrameNet and Prop-
Bank releases.4 We envisaged that the participants
would want to use these additional data sets to
train SRL systems for the full task and to learn
something about typical fillers for different roles
in order to solve the NI linking task. The anno-
tated data sets we made available were meant to
provide additional information, e.g., about the typ-
ical distance between an NI and its filler and about
how to distinguish DNIs and INIs.

We annotated texts from two of Arthur Conan
Doyle’s fiction works. The text that served as
training data was taken from “The Adventure of
Wisteria Lodge”. Of this lengthy, two-part story
we annotated the second part, titled “The Tiger of
San Pedro”. The test set was made up of the last
two chapters of “The Hound of the Baskervilles”.
We chose fiction rather than news because we be-
lieve that fiction texts with a linear narrative gen-
erally contain more context-resolvable NIs. They
also tend to be longer and have a simpler structure
than news texts, which typically revisit the same
facts repeatedly at different levels of detail (in the
so-called ‘inverted pyramid’ structure) and which
mix event reports with commentary and evalua-
tion, thus sequencing material that is understood
as running in parallel. Fiction texts should lend
themselves more readily to a first attempt at inte-
grating discourse structure into semantic role la-
beling. We chose Conan Doyle’s work because
most of his books are not subject to copyright any-
more, which allows us to freely release the anno-
tated data. Note, however, that this choice of data

4For FrameNet we provided an intermediate release,
FrameNet 1.4 alpha, which contained more frames and lexi-
cal units than release 1.3.

means that our texts come from a different domain
and genre than many of the examples in FrameNet
and PropBank as well as making use of a some-
what older variety of English.5

Table 1 provides basic statistics of the data sets.
The training data had 3.1 frames per sentence and
the test data 3.2, which is lower than the 8.8 frames
per sentence in the test data of the 2007 SemEval
task on Frame Semantic Structure Extraction.6 We
think this is mainly the result of switching to a do-
main different from the bulk of what FrameNet
has made available in the way of full-text anno-
tation. In doing so, we encountered many new
frames and lexical units for which we could not
ourselves create the necessary frames and pro-
vide lexicographic annotations. The statistics also
show that null-instantiation is relatively common:
in the training data, about 18.7% of all FEs are
omitted, and in the test set, about 18.4%. Of the
DNIs, 80.9% had an antecedent in the training
data, and 74.2% in the test data.

To ensure a high quality of the annotations, both
data sets were annotated by more than one person
and then adjudicated. The training set was an-
notated independently by two experienced anno-
tators and then adjudicated by the same two peo-
ple. The test set was annotated by three annota-
tors and then adjudicated by the two experienced
annotators. Throughout the annotation and adju-
dication process, we discussed difficult cases and
also maintained a wiki. Additionally, we created a
software tool that checked the consistency of our
annotations against the frame, frame element and
FE-relation specifications of FrameNet and alerted
annotators to problems with their annotations. The
average agreement (F-score) for frame assignment
for pairs of annotators on the two chapters in the
test set ranges from 0.7385 to 0.7870. The agree-
ment of individual annotators with the adjudicated
gold standard ranges from 0.666 to 0.798. Given
that the gold standard for the two chapters features
228 and 229 different frame types, respectively,
this level of agreement seems quite good.

5While PropBank provides annotations for the Penn Tree-
bank and is thus news-based, the lexicographic annotations
in FrameNet are extracted from the BNC, a balanced cor-
pus. The FrameNet full-text annotations, however, only cover
three domains: news, travel guides, and nuclear proliferation
reports.

6The statistics in Table 1 and all our discussion of the
data includes only instances of semantic frames and ignores
the instances of the Coreference, Support, and Relativization
frames, which we labeled on the data as auxiliary informa-
tion.

47



data set sentences tokens frame inst. frame types overt FEs DNIs (resolved) INIs
train 438 7,941 1,370 317 2,526 303 (245) 277
test 525 9,131 1,703 452 3,141 349 (259) 361

Table 1: Statistics for the provided data sets

For the annotation of NIs and their links to the
surrounding discourse we created new guidelines
as this was a novel annotation task. We adopted
ideas from the annotation of co-reference informa-
tion, linking locally unrealized roles to all men-
tions of the referents in the surrounding discourse,
where available. We marked only identity rela-
tions but not part-whole or bridging relations be-
tween referents. The set of unrealized roles un-
der consideration includes only the core arguments
but not adjuncts (peripheral or extra-thematic roles
in FrameNet’s terminology). Possible antecedents
are not restricted to noun phrases but include all
constituents that can be (local) role fillers for
some predicate plus complete sentences (which
can sometimes fill roles such as MESSAGE).

4 Evaluation

As noted above, we allowed participants to ad-
dress three different tasks: SRL only, NI only,
full task. For role recognition and labeling we
used a standard evaluation set-up, i.e., accuracy for
role labeling and precision, recall, F-Score for role
recognition.

The NI linkings were evaluated slightly differ-
ently. In the gold standard, we identified refer-
ents for null instantiations in the discourse con-
text. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system is given credit
if the NI is linked to any of these expressions. To
achieve this we create equivalence sets for the ref-
erents of NIs (by annotating coreference chains).
If the NI is linked to any item in the equivalence
set, the link is counted as a true positive. We can
then define NI linking precision as the number
of all true positive links divided by the number of
links made by a system, and NI linking recall as
the number of true positive links divided by the
number of links between an NI and its equivalence
set in the gold standard. NI linking F-Score is
then the harmonic mean between NI linking preci-
sion and recall.

Since it may sometimes be difficult to deter-

mine the correct extent of the filler of an NI, we
score an automatic annotation as correct if it in-
cludes the head of the gold standard filler in the
predicted filler. However, in order to not favor sys-
tems which link NIs to very large spans of text to
maximize the likelihood of linking to a correct ref-
erent, we introduce a second evaluation measure,
which computes the overlap (Dice coefficient) be-
tween the words in the predicted filler (P) of an NI
and the words in the gold standard one (G):

NI linking overlap =
2|P ∩ G|
|P | + |G| (5)

Example (6) illustrates this point. The verb
won in the second sentence evokes the Fin-
ish competition frame whose COMPETITION role
is omitted. From the context it is clear that the
competition role is semantically filled by their first
TV debate (head: debate) and last night’s debate
(head: debate) in the previous sentences. These
two expressions form the equivalence set for the
COMPETITION role in the last sentence. Any sys-
tem that would predict a linkage to a filler that
covers the head of either of these two expressions
would score a true positive for this NI. However,
a system that linked to last night’s debate would
have an NI linking overlap of 1 (i.e., 2*3/(3+3))
while a system linking the whole second sentence
Last night’s debate was eagerly anticipated to the
NI would have an overlap of 0.67 (i.e., 2*3/(6+3))

(6) US presidential rivals Republican John
McCain and Democrat Barack Obama
have yesterday evening attacked each
other over foreign policy and the econ-
omy, in [their first TV debate]Competition.
[Last night’s debate]Competition was ea-
gerly anticipated. Two national flash
polls suggest that [Obama]Competitor
wonFinish competition 0Competition.

5 Participating Systems

While a fair number of people expressed an inter-
est in the task and 26 groups or individuals down-
loaded the data sets, only three groups submitted

48



results for evaluation. Feedback from the teams
that downloaded the data suggests that this was
due to coinciding deadlines and to the difficulty
and novelty of the task. Only the SEMAFOR
group addressed the full task, using a pipeline of
argument recognition followed by NI identifica-
tion and resolution. Two groups (GETARUNS++
and SEMAFOR) tackled the NI only task, and
also two groups, the SRL only task (CLR and SE-
MAFOR7).

All participating systems were built upon ex-
isting systems for semantic processing which
were modified for the task. Two of the groups,
GETARUNS++ and CLR, employed relatively
deep semantic processing, while the third, SE-
MAFOR, employed a shallower probabilistic sys-
tem. Different approaches were taken for NI link-
ing. The SEMAFOR group modeled NI linking as
a variant of role recognition and labeling by ex-
tending the set of potential arguments beyond the
locally available arguments to also include noun
phrases from the previous sentence. The system
then uses, among other information, distributional
semantic similarity between the heads of potential
arguments and role fillers in the training data. The
GETARUNS++ group applied an existing system
for deep semantic processing, anaphora resolution
and recognition of textual entailment, to the task.
The system analyzes the sentences and assigns its
own set of labels, which are subsequently mapped
to frame semantic categories. For more details of
the participating systems please consult the sepa-
rate system papers.

6 Results and Analysis

6.1 SRL Task

Argument Recognition Label
Prec. Rec. F1 Acc.

SHA 0.6332 0.3884 0.4812 0.3471
SEM 0.6528 0.4674 0.5448 0.4184
CLR 0.6702 0.1121 0.1921 0.1093

Table 2: Shalmaneser (SHA), SEMAFOR (SEM)
and CLR performance on the SRL task (across
both chapters)

The results on the SRL task are shown in Table
2. To get a better sense of how good the perfor-
mance of the submitted systems was on this task,

7For SEMAFOR, this was the first step of their pipeline.

we applied the Shalmaneser statistical semantic
parser (Erk and Padó, 2006) to our test data and
report the results. Note, however, that we used a
Shalmaneser trained only on FrameNet version 1.3
which is different from the version 1.4 alpha that
was used in the task, so its results are lower than
what can be expected with release 1.4 alpha.

We observe that although the SEMAFOR and
the CLR systems score a higher precision than
Shalmaneser for argument recognition, the SE-
MAFOR system scores considerably higher recall
than Shalmaneser, whereas the CLR system scores
a much lower recall.

6.2 NI Task

Tackling the resolution of NIs proved to be a dif-
ficult problem due to a variety of factors. First,
the NI sub-task was completely new and involves
several steps of linguistic processing. It also is
inherently difficult in that a given FE is not al-
ways omitted with the same interpretation. For
instance, the Content FE of the Awareness frame
evoked by know is interpreted as indefinite in
the blog headline More babbling about what it
means to know but as definite in a discourse
like Don’t tell me you didn’t know!. Second,
prior to this SemEval task there was no full-text
training data available that contained annotations
with all the kinds of information that is relevant
to the task, namely overt FEs, null-instantiated
FEs, resolutions of null-instantiations, and coref-
erence. Third, the data we used also represented
a switch to a new domain compared to existing
FrameNet full-text annotation, which comes from
newspapers, travel guides, and the nuclear pro-
liferation domain. Our most frequent frame was
Observable bodyparts, whereas it is Weapons in
FrameNet full-text. Fourth, it was not well un-
derstood at the beginning of the task that, in cer-
tain cases, FrameNet’s null-instantiation annota-
tions for a given FE cannot be treated in isolation
of the annotations of other FEs. Specifically, null-
instantiation annotations interact with the set of re-
lations between core FEs that FrameNet uses in its
analyses. As an example, consider the CoreSet re-
lation, which specifies that from a set of core FEs
at least one must be instantiated overtly, though
more of them can be. As long as one of the FEs
in the set is expressed overtly, null-instantiation is
not annotated for the other FEs in the set. For
instance, in the Statement frame, the two FEs

49



Topic and Message are in one CoreSet and the
two FEs Speaker and Medium are in another. If
a frame instance occurs with an overt Speaker and
an overt Topic, the Medium and Message FEs are
not marked as null-instantiated. Automatic sys-
tems that treat each core FE separately, may pro-
pose DNI annotations for Medium and Message,
resulting in false positives.

Therefore, we think that the evaluation that we
initially defined was too demanding for a novel
task. It would have been better to give sepa-
rate scores for 1) ability to recognize when a core
FE has to be treated as null-instantiated; 2) abil-
ity to distinguish INI and DNI; and 3) ability to
find antecedents. The systems did have to tackle
these steps anyway and an analysis of the sys-
tem output shows that they did so with different
success. The two chapters of our test data con-
tained a total of 710 null instantiations, of which
349 were DNI and 361 INI. The SEMAFOR sys-
tem recognized 63.4% (450/710) of the cases of
NI, while the GETARUNS++ system found only
8.0% (57/710). The distinction between DNI and
INI proved very difficult, too. Of the NIs that
the SEMAFOR system correctly identified, 54.7%
(246/450) received the correct interpretation type
(DNI or INI). For GETARUNS++, the percentage
is higher at 64.2% (35/57), but also based on fewer
proposed classifications. A simple majority-class
baseline gives a 50.8% accuracy. Interestingly, the
SEMAFOR system labeled many more INIs than
DNIs, thus often misclassifying DNIs as INI. The
GETARUNS++ system applied both labels about
equally often.

7 Conclusion

In this paper we described the SemEval-2010
shared task on “Linking Events and Their Partic-
ipants in Discourse”. The task is novel, in that it
tackles a semantic cross-clausal phenomenon that
has not been treated before in a task, namely, link-
ing locally uninstantiated roles to their coreferents
at the text level. In that sense the task represents
a first step towards taking SRL beyond the sen-
tence level. A new corpus of fiction texts has been
annotated for the task with several types of seman-
tic information: semantic argument structure, co-
reference chains and NIs. The results scored by
the systems in the NI task and the feedback from
participant teams shows that the task was more dif-
ficult than initially estimated and that the evalua-

tion should have focused on more specific aspects
of the NI phenomenon, rather than on the com-
pleteness of the task. Future work will focus on
modeling the task taking this into account.

Acknowledgements

Josef Ruppenhofer and Caroline Sporleder are supported

by the German Research Foundation DFG (under grant PI

154/9-3 and the Cluster of Excellence Multimodal Comput-

ing and Interaction (MMCI), respectively). Roser Morante’s

research is funded by the GOA project BIOGRAPH of the

University of Antwerp. We would like to thank Jinho Choi,

Markus Dräger, Lisa Fuchs, Philip John Gorinski, Russell

Lee-Goldman, Ines Rehbein, and Corinna Schorr for their

help with preparing the data and/or implementing software

for the task. Thanks also to the SemEval-2010 Chairs Katrin

Erk and Carlo Strapparava for their support during the task

organization period.

References
C. Baker, M. Ellsworth, K. Erk. 2007. SemEval-2007

Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.

A. Burchardt, A. Frank, M. Pinkal. 2005. Building text
meaning representations from contextually related
frames – A case study. In Proceedings of IWCS-6.

K. Erk, S. Padó. 2004. A powerful and versatile XML
format for representing role-semantic annotation. In
Proceedings of LREC-2004.

K. Erk, S. Padó. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC-06.

C. Fillmore, C. Baker. 2001. Frame semantics for text
understanding. In Proc. of the NAACL-01 Workshop
on WordNet and Other Lexical Resources.

C. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In A. Zampolli, ed.,
Fundamental Studies in Computer Science, No. 59,
55–88. North Holland Publishing.

C. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual
Meeting of the Berkeley Liguistics Society.

D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.

M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL-1986.

M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.

J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. Semeval-2010 task 10: Linking
events and their participants in discourse. In The
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-09).

50


