



















































A quantitative view of feedback lexical markers in conversational French


Proceedings of the SIGDIAL 2013 Conference, pages 87–91,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

A quantitative view of feedback lexical markers in conversational French

Laurent Prévot Brigitte Bigi
Aix Marseille Université & CNRS

Laboratoire Parole et Langage
Aix-en-Provence (France)

firstname.lastname@lpl-aix.fr

Roxane Bertrand

Abstract

This paper presents a quantitative descrip-
tion of the lexical items used for linguis-
tic feedback in the Corpus of Interactional
Data (CID). The paper includes the raw
figures for feedback lexical item as well
as more detailed figures concerning inter-
individual variability. This effort is a first
step before a broader analysis including
more discourse situations and featuring
communicative function annotation.

Index Terms: Feedback, Backchannel, Corpus,
French Language

1 Objectives

Conversational feedback is mostly performed
through short utterances such as yeah, mh, okay
not produced by the main speaker but by one of
the other participants of a conversation. Such ut-
terances are among the most frequent in conver-
sational data (Stolcke et al., 2000). They also
have been described in psycho-linguistic models
of communication as a crucial communicative tool
for achieving coordination or alignment in dia-
logue (Clark, 1996).

The general objective of the project (ANR
CoFee: Conversational Feedback)1(Prévot and
Bertrand, 2012) in which this work takes place
is to propose a fine grained model of the
form/function relationship concerning feedback
behaviors in conversation. The present study is
first exploration aiming at knowing better the dis-
tribution of these items in one of our corpus. More
precisely, we would to verify how much inter-
individual variability we will face in further study
and whether we can identify a structure in this
variability (e.g speaker profiles). Second, we tried

1See the project website: http://cofee.hypotheses.org

to check there some strong trends in terms of evo-
lution of use of these items in the course of the
conversation. This later point was not conclusive
and is not developed in this paper.

Some data-intensive works exist for English
(Gravano et al., 2012), Japanese (Kamiya et al.,
2010; Misu et al., 2011) or Swedish (Allwood et
al., 1992; Cerrato, 2007; Neiberg et al., 2013) but
not on many other languages such as French for
example. On French, the work of (Muller and
Prévot, 2003; Muller and Prévot, 2009) concerned
a smaller scale (A hour corpus) and very specific
task. (Bertrand et al., 2007) was focussed on the
feedback inviting cues and also on a smaller scale
(2 × 15 minutes). They showed that particular
pitch contours and discursive markers play a sys-
tematic role as inviting-cues both for vocal and
gestural back-channels.

The paper is structured as follow. Section 2
presents the conversational corpus used for this
study, then section 3 presents how this corpus has
been processed. Section 4 is related to general fig-
ures for the feedback lexical items, followed by
more detailed information about inter-individual
variability (section 5).

2 The corpus

The Corpus of Interactional Data (CID) (Bertrand
et al., 2008; Blache et al., 2009)2 is an audio-video
recording of 8 hours of spontaneous French dia-
logues, 1 hour of recording per session. Each di-
alogue involved two participants of the same gen-
der. One of the following two topics of conver-
sation was suggested to participants: conflicts in
their professional environment or unusual situa-
tions in which participants may have found them-
selves. It features a nearly free conversational
style with only a single theme proposed to the par-
ticipants at the beginning of the experiment. This

2http://www.sldr.org/sldr000027/en

87



corpus is fully transcribed and forced-aligned at
phone level. Moreover, it has been annotated with
various linguistic information (Prosodic Phrasing,
Discourse units, Syntactic tags, ...) (Blache et al.,
2010) which will allow us later to take advantage
of these levels of analysis.

Numerous studies have been carried out in pre-
pared speech. However, conversational speech
refers to a more informal activity, in which par-
ticipants have constantly to manage and negotiate
turn-taking, topic changes (among other things)
without any preparation. As a consequence, nu-
merous phenomena appear such as hesitations, re-
peats, backchannels, etc. Phonetic phenomena
such as non-standard elision, reduction phenom-
ena, truncated words, and more generally, non-
standard pronunciations are also very frequent.
All these phenomena can impact on the phoneti-
zation, then on alignment.

3 Processing the corpus

The transcription process is done following spe-
cific conventions derived from that of the GARS
(Blanche-Benveniste and Jeanjean, 1987). The
result is what we call an enriched orthographic
transcription (EOT), from which two derived tran-
scriptions are generated automatically : the stan-
dard orthographic transcription (the list of ortho-
graphic tokens) and a specific transcription from
which the phonetic tokens are obtained to be
used by the grapheme-phoneme converter. From
the phoneme sequence and the audio signal, the
aligner outputs for each phoneme its time localiza-
tion. This corpus has been processed with several
aligners. The first and main one (Brun et al., 2004)
is HMM-based, it uses a set of 10 macro-classes
of vowel (7 oral and 3 nasal), 2 semi-vowels and
15 consonants. Finally, from the time aligned
phoneme sequence plus the EOT, the orthographic
tokens is time-aligned.

The alignment for this paper is another ver-
sion that has been carried out using SPPAS3 (Bigi,
2012). SPPAS is a tool to produce automatic anno-
tations which include utterance, word, syllabic and
phonemic segmentations from a recorded speech
sound and its transcription.

Alignment of items of the list given in (1) were
then manually verified. Largest errors were cor-
rected to obtain reliable alignments.

DM prononciations are the standard ones except
3http://www.lpl-aix.fr/∼bigi/sppas/

for a few cases. There are only two items with
non standard cases that are over 2 occurrences:
sampa: m.w.e.) that is an hybrid between mh
and ouais, and sampa w.a.l.a, a reduction of
v.w.a.l.a voilà.

The extraction themselves have been realized
by the authors with a Python script and all the
statistical analyses and plots have been produced
with R statistical analysis tool.

4 Descriptive statistics for the lexical
markers used in feedback

All the lexical items of the list given in (1) were
automatically extracted and categorized into two
categories: (i) Isolated items are items or sequence
of items surrounded by pauses of at least 200 ms
and not including any extra material than the items
of this list ; (ii) Initial items (or sequence items)
are located in front of some other items (but there
is no other material within the sequence). Most
of these items also occur in final or even sur-
rounded positions but we did not consider these
cases since they do are not clearly related to feed-
back. More precisely surrounded items are mostly
consisting in breaks of disfluencies or genuinely
integrated construction (e.g j’étais d’accord avec
lui / I agreed with him). Final ones can play a
role in eliciting feedback or sometimes bring some
kind of closure at the end of the utterance (what
has been described as Pivot Ending in (Gravano et
al., 2012)).

(1) ah (ah), bon (well), ben (well), euh (err,
uh), mh (mh), ouais (yeah), oui (yes), non
(no), d’accord (agreed), OK (okay), voilà
(that’s it, right)

Strictly speaking, the list (1) is not exhaustive.
However, other items are already in the thin part
of the distribution’s tail. Moreover, some of the
items such as euh / err are not necessarily related
to feedback. However, by crossing lexical values
with position we expect to get close enough the
full set of tokens involved in feedback. For exam-
ple, initial euh not followed by a feedback related
item will not be included in the final dataset. This
is also an objective of the present work to identify
these situations.

The different markers exhibit very different fig-
ures with regard to their location as it can be seen
in 1. While some are specialized in isolated feed-
back such as the continuer mh which is most of the

88



time backchanneled, others are found at the begin-
ning of utterances such as euh, ah. The later makes
sense since euh is also a filled pause.

ah
 

ah
_o

ua
is

 

ah
_o

ui
 

be
n 

bo
n 

da
cc

or
d 

eu
h 

m
h 

m
h_

m
h 

no
n 

ou
ai

s 

ou
ai

s_
ou

ai
s 

ou
i 

vo
ila

 

0

100

200

300

400

500

initial
isolated

Figure 1: Distribution of isolated vs. initial posi-
tion for the most frequent lexical items

In total 197 different combinations of the ba-
sic markers were identified. The most frequent
are the simple repetitions of items such as ouais
(up to nine times) or mh. There are also more
complex structures as exhibited in (2) that seem
to mix two kinds of items: base ones and mod-
ifiers (ah, euh). The base ones seem by default
to carry general purpose communicative functions
as described in (Bunt, 2009; Bunt, 2012) while the
others can also be produced alone but are generally
dealing specific dimension such as turn-taking, at-
titude expression or time management.

(2) a. ah ouais d’accord ok (ah yeah right
okay)

b. voilà oui non (that’s it yes no)

With regard to duration, the data is rather messy
concerning the very long items. There are extreme
lengthening on these units. Aside that and the filler
uh that exhibit a wide spread, the other items are
not produced with huge variations. Monosyllabic
remain well centered around 150-250 ms while di-
syllabic and repeated items are distributed in the
250-500 ms range. This is important for our next
step in which automatic acoustic analysis of these
items will be performed.

●

●

●
●

●

●

●
●
●

●

●

●

●

●●
●●●

●
●

●
●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●●
●

●

●

●

●●

●

●●

●

●
●
●

●

●

●

●

●

●

●

●

●●●
●

●

●

●

●●
●

●
●
●

●

●

●

●

●
●●

●●●●
●
●●●

●

●

●

●

●

●

●●●●●
●●

●
●
●

●

●

●

●

●●
●●
●

●

●

●●●

●

●

●

●
●
●

●●

●

●
●

●

●

●

●

●

●

●

●

●

ah
 

ah
_o

ua
is

 

ah
_o

ui
 

be
n 

bo
n 

da
cc

or
d 

eu
h 

m
h 

m
h_

m
h 

no
n 

ou
ai

s 

ou
ai

s_
ou

ai
s 

ou
i 

vo
ila

 

0.0

0.5

1.0

1.5

2.0

Figure 2: Duration (in seconds) of each lexical
type

5 Inter-individual variability

Inter-individual variation is a big issue on the way
to the generalizability. We would like to under-
stand some of the feedback producing profiles.
Our intuitions coming from familiarity of the data
is that there are strong variation but they corre-
spond to a few different speaking styles. In fu-
ture work, we would like to see in a second step
whether we can identify and characterize these
styles.

●

15
0

20
0

25
0

30
0

35
0

40
0

45
0

Figure 3: Number of feedback items per speaker

Figure 3 illustrates the total figures of feedback
per speaker. As expected variation is huge, from
132 to 425 but with in fact with few outliers with
a nice batch of speaker in the 200 − 300 range.
The wider spread of the distribution in the high
range comes from two factors. First of all, there
are participants producing a high quantity of feed-

89



back items. They produce a massive amount of
light backchannels (mh, ouais) compared to low-
quantity feedback producers. The later also pro-
duce feedback during the long pauses of the main
speaker but they produce much less overlapping
backchannels. This should be double checked
with a specific measure (adding overlapping as a
factor). However, a second effect seems important
for at least one speaker (the outlier): the amount
to time holding the floor. In fact the speaker pro-
ducing the most feedback did so because she was
rarely the main speaker.

In order to get a global idea of the different uses
of these items, Figure 5 represents the proportion
of each item per speaker. As expected, the varia-
tion is important but one can spot some tendencies.
For examples for the most frequent items, the rank
seems to preserved across speakers.

C
ID

_A
B

 
C

ID
_A

C
 

C
ID

_A
G

 
C

ID
_A

P
 

C
ID

_B
X

 
C

ID
_C

M
 

C
ID

_E
B

 
C

ID
_I

M
 

C
ID

_L
J 

C
ID

_L
L 

C
ID

_M
B

 
C

ID
_M

G
 

C
ID

_M
L 

C
ID

_N
H

 
C

ID
_S

R
 

C
ID

_Y
M

 

voila
oui
ouais_ouais
ouais
non
mh_mh
mh
euh
daccord
bon
ben
ah_oui
ah_ouais
ah

0.0

0.2

0.4

0.6

0.8

1.0

Figure 4: Distribution of the lexical items

Based on their feedback profile (proportion of
use of each items as illustrated in Figure 5), we
attempted to cluster the participants as showed in
5. While the lower parts of the dendrogram are
hard to interpret the higher part matches well with
the impression acquired by listening to the corpus
(no backchannels and rather formal feedback vs.
lots of backchannels and very colloquial style).

6 Current and Future Work

About this first batch of analyses, we will com-
plete the analysis of the evolution during the con-
versation. More precisely, we will go at the
individual level looking for time-based changes

A
B

A
G

IM M
L

Y
M

B
X

A
P

E
B

LJ N
H

C
M

S
R

A
C

M
B

LL M
G

0.
1

0.
2

0.
3

0.
4

0.
5

Dendrogram of  diana(x = distSpForm)

diana (*, "NA")
distSpForm

H
ei

gh
t

Figure 5: Dendrogram of the participants cluster
based on their feedback profile

in their profiles as well as looking at the pairs
for tracking potential convergence effect either in
terms of distribution of lexical marker types or in
their duration.

In parallel to this work, we are launching in-
dependent prosodic and kinesic analyses of the
forms, as well as a discourse analysis of the func-
tions. Moreover the work is being extended by
adding two corpora in the study in order to allow
for a better situation generalisability: A French
MapTask; and a third corpus consisting in a less
cooperative situation. The idea is later to bring
together the observations from the different levels
in order to propose a multidimensional model for
feedback in French dialogues.

Those are steps toward more extensive studies
in the spirit of (Gravano et al., 2012) or (Neiberg
et al., 2013) on French language and in which we
hope to address more directly the issue of dis-
course situation generalisability.

Acknowledgment

This work has been realized with the support of
the ANR (Grant Number: ANR-12-JCJC-JSH2-
006-01) and exploited aligned data produced in
the framework of the ANR project (Grant Number
ANR-08-BLAN-0239). We would like to thank all
the members of these two projects.

90



References
J. Allwood, J. Nivre, and E. Ahlsen. 1992. On the se-

mantics and pragmatics of linguistic feedback. Jour-
nal of Semantics, 9.

R. Bertrand, G. Ferré, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Proceedings of Auditory-
visual Speech Processing. Citeseer.

R. Bertrand, P. Blache, R. Espesser, G. Ferré, C. Me-
unier, B. Priego-Valverde, and S. Rauzy. 2008.
Le cid-corpus of interactional data-annotation et ex-
ploitation multimodale de parole conversationnelle.
Traitement Automatique des Langues, 49(3):1–30.

B. Bigi. 2012. SPPAS: a tool for the phonetic segmen-
tation of speech. In Language Resource and Evalu-
ation Conference, pages 1748–1755, ISBN 978–2–
9517408–7–7, Istanbul (Turkey).

P. Blache, R. Bertrand, and G. Ferré. 2009. Creating
and exploiting multimodal annotated corpora: the
toma project. Multimodal corpora, pages 38–53.

P. Blache, R. Bertrand, B. Bigi, E. Bruno, E. Cela,
R. Espesser, G. Ferré, M. Guardiola, D. Hirst,
E. Muriasco, J.-C. Martin, C. Meunier, M.-A. Morel,
I. Nesterenko, P. Nocera, B. Palaud, L. Prévot,
B. Priego-Valverde, J. Seinturier, N. Tan, M. Tel-
lier, and S. Rauzy. 2010. Multimodal annotation
of conversational data. In Proceedings of Linguistic
Annotation Workshop.

C. Blanche-Benveniste and C. Jeanjean. 1987. Le
français parlé. Edition et transcription. Paris, Di-
dier Erudition.

A. Brun, C. Cerisara, D. Fohr, I. Illina, D. Langlois,
O. Mella, and K. Smaı̈li. 2004. Ants: le système de
transcription automatique du loria. In Actes des XXV
Journées d’Etudes sur la Parole, Fès, Morocco.

H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue act annotation. In Proceedings of
DiaHolmia, SEMDIAL.

H. Bunt. 2012. The semantics of feedback. In
16th Workshop on the Semantics and Pragmatics of
Dialogue (SEMDIAL 2012), pages 118–127, Paris
(France).

L. Cerrato. 2007. Investigating Communicative Feed-
back Phenomena across Languages and Modalities.
Ph.D. thesis.

H.H. Clark. 1996. Using language. Cambridge: Cam-
bridge University Press.

A. Gravano, J. Hirschberg, and Š. Beňuš. 2012. Af-
firmative cue words in task-oriented dialogue. Com-
putational Linguistics, 38(1):1–39.

Y. Kamiya, T. Ohno, and S. Matsubara. 2010. Coher-
ent back-channel feedback tagging of in-car spoken
dialogue corpus. In Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 205–208. Association for Com-
putational Linguistics.

T. Misu, E. Mizukami, Y. Shiga, S. Kawamoto,
H. Kawai, and S. Nakamura. 2011. Toward con-
struction of spoken dialogue system that evokes
users’ spontaneous backchannels. In Proceedings
of the SIGDIAL 2011 Conference, pages 259–265.
Association for Computational Linguistics.

P. Muller and L. Prévot. 2003. An empirical study
of acknowledgement structures. In Proceedings od
Diabruck, 7th workshop on semantics and pragmat-
ics of dialogue, Saarbrucken.

P. Muller and L. Prévot. 2009. Grounding information
in route explanation dialogues. In Spatial Language
and Dialogue. Oxford University Press.

D. Neiberg, G. Salvi, and J. Gustafson. 2013. Semi-
supervised methods for exploring the acoustics of
simple productive feedback. Speech Communica-
tion.

L. Prévot and R. Bertrand. 2012. Cofee-toward a mul-
tidimensional analysis of conversational feedback,
the case of french language. In Proceedings of the
Workshop on Feedback Behaviors. (poster).

A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.

91


