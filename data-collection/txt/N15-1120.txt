



















































Idest: Learning a Distributed Representation for Event Patterns


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1140–1149,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

IDEST: Learning a Distributed Representation for Event Patterns

Sebastian Krause∗
LT Lab, DFKI
Alt-Moabit 91c

10559 Berlin, Germany
skrause@dfki.de

Enrique Alfonseca Katja Filippova Daniele Pighin
Google Research

Brandschenkestrasse 110
8810 Zurich, Switzerland

{ealfonseca,katjaf,biondo}@google.com

Abstract

This paper describes IDEST, a new method for
learning paraphrases of event patterns. It is
based on a new neural network architecture
that only relies on the weak supervision sig-
nal that comes from the news published on the
same day and mention the same real-world en-
tities. It can generalize across extractions from
different dates to produce a robust paraphrase
model for event patterns that can also cap-
ture meaningful representations for rare pat-
terns. We compare it with two state-of-the-art
systems and show that it can attain compara-
ble quality when trained on a small dataset.
Its generalization capabilities also allow it to
leverage much more data, leading to substan-
tial quality improvements.

1 Introduction

Most Open Information Extraction (Open-IE) sys-
tems (Banko et al., 2007) extract textual relational
patterns between entities automatically (Fader et al.,
2011; Mausam et al., 2012) and optionally organize
them into paraphrase clusters. These pattern clusters
have been found to be useful for Question Answer-
ing (Lin & Pantel, 2001; Fader et al., 2013) and re-
lation extraction (Moro & Navigli, 2012; Grycner &
Weikum, 2014), among other tasks.

A related Open-IE problem is that of automati-
cally extracting and paraphrasing event patterns:
those that describe changes in the state or attribute
values of one or several entities. An existing ap-
proach lo learn paraphrases of event patterns is
to build on the following weak supervision signal:

∗Work performed during an internship at Google

news articles that were published on the same day
and mention the same entities should contain good
paraphrase candidates. Two state-of-the-art event
paraphrasing systems that are based on this assump-
tion are NEWSSPIKE (Zhang & Weld, 2013) and
HEADY (Alfonseca et al., 2013; Pighin et al., 2014).

These two systems have a lot in common, yet they
have never been compared with each other. They
have specific weak and strong points, and there are
many ways in which they are substantially different:

• Scope of generalization. In NEWSSPIKE the
paraphrase clusters are learned separately for
each publication day and entity set, and the sys-
tem cannot generalize across events of the same
type involving different entities occurring on
the same or on different days. For example, if
the event verbs has married and wed appear in
news about two entitiesA andB marrying, and
has married and tied the knot with appear in
news involving two different entities C and D,
NEWSSPIKE is not able to infer that wed and
tied the knot with are also paraphrases, unless a
post-processing is done.

HEADY overcomes this limitation thanks to a
global model that learns event representations
across different days and sets of entities. How-
ever, the global nature of the learning problem
can incur into other drawbacks. First, training a
global model is more costly and more difficult
to parallelize. Second, relatively frequent pat-
terns that erroneously co-occur with other pat-
terns may have a negative impact on the final
models, potentially resulting in noisier clusters.
Lastly, low-frequency patterns are likely to be

1140



discarded as noisy in the final model. Over-
all, HEADY is better at capturing paraphrases
from the head of the pattern distribution, and
is likely to ignore most of the long tail where
useful paraphrases can still be found.

• Simplifying assumptions. We already men-
tioned that the two systems share a common un-
derlying assumption, i.e., that good paraphrase
candidates can be found by looking at news
published on the same day and mentioning the
same entities. On top of this, NEWSSPIKE also
assumes that better paraphrases are reported
around spiky entities, verb tenses may not dif-
fer, there is one event mention per discourse,
and others. These restrictions are not enforced
by HEADY, where the common assumption is
indeed even relaxed across days and entity sets.

• Annotated data. NEWSSPIKE requires hand-
annotated data to train the parameters of a
supervised model that combines the different
heuristics, whereas HEADY does not need an-
notated data.

This paper describes IDEST, a new method for
learning paraphrases of event patterns that is de-
signed to combine the advantages of these two sys-
tems and compensate for their weaknesses. It is
based on a new neural-network architecture that, like
HEADY, only relies on the weak supervision signal
that comes from the news published on the same day,
requiring no additional heuristics or training data.
Unlike NEWSSPIKE, it can generalize across differ-
ent sets of extracted patterns, and each event pattern
is mapped into a low-dimensional embedding space.
This allows us to define a neighborhood around a
pattern to find the ones that are closer in meaning.

IDEST produces a robust global model that can
also capture meaningful representations for rare pat-
terns, thus overcoming one of HEADY’s main lim-
itations. Our evaluation of the potential trade-off
between local and global paraphrase models shows
that comparably good results to NEWSSPIKE can be
attained without relying on supervised training. At
the same time, the ability of IDEST to produce a
global model allows it to benefit from a much larger
news corpus.

2 Related work

Relational Open-IE In an early attempt to move
away from domain-specific, supervised IE systems,
Riloff (1996) attempted to automatically find rela-
tional patterns on the web and other unstructured re-
sources in an open domain setting. This idea has
been further explored in more recent years by Brin
(1999), Agichtein & Gravano (2000), Ravichan-
dran & Hovy (2002) and Sekine (2006), among
the others. Banko et al. (2007) introduced Open-
IE and the TEXTRUNNER system, which extracted
binary patterns using a few selection rules ap-
plied on the dependency tree. More recent sys-
tems such as REVERB (Fader et al., 2011) and OL-
LIE (Mausam et al., 2012) also define linguistically-
motivated heuristics to find text fragments or depen-
dency structures that can be used as relational pat-
terns.

A natural extension to the previous work is to au-
tomatically identify which of the extracted patterns
have the same meaning, by producing either a hard
or a soft clustering. Lin & Pantel (2001) use the
mutual information between the patterns and their
observed slot fillers. Resolver (Yates & Etzioni,
2007) introduces a probabilistic model called the
Extracted Shared Property (ESP) where the proba-
bility that two instances or patterns are paraphrases
is based on how many properties or instances they
share. USP (Poon & Domingos, 2009) produces a
clustering by greedily merging the extracted rela-
tions. Yao et al. (2012) employ topic models to learn
a probabilistic model that can capture also the am-
biguity of polysemous patterns. More recent work
also organizes patterns in clusters or taxonomies
using distributional methods on the pattern con-
texts or entities extracted (Moro & Navigli, 2012;
Nakashole et al., 2012), or implicitly clusters rela-
tional text patterns via the learning of latent feature
vectors for entity tuples and relations, in a setting
similar to knowledge-base completion (Riedel et al.,
2013).

A shared difficulty for systems that cluster pat-
terns based on the arguments they select is that it is
very hard for them to distinguish between identity
and entailment. If one pattern entails another, both
are likely to be observed in the corpus involving the
same entity sets. A typical example illustrating this
problem is the two patterns e1 married e2 and e1

1141



John Smith married Mary Brown in Baltimore yesterday after a long courtship
person person location

subj dobj pobj
prep

tmod
prep

pobj

amod
det

Figure 1: Example sentence, and extraction (the nodes connected through solid dependency edges).

· married ·
person person

subj dobj

Figure 2: Example pattern that encodes a wedding event
between two people.

dated e2, which can be observed involving the same
pairs of entities, but which carry a different mean-
ing. As discussed below, relying on the temporal di-
mension (given by the publication date of the input
documents) is one way to overcome this problem.

Event patterns and Open-IE Although some ear-
lier work uses the temporal dimension of text as
filters to improve precision of relational pattern
clusters, NEWSSPIKE (Zhang & Weld, 2013) and
HEADY (Alfonseca et al., 2013; Pighin et al., 2014)
fully rely on it as its main supervision signal. In or-
der to compare the two approaches, we will start by
defining some terms:

• An event pattern encodes an expression that
describes an event. It can be either a linear sur-
face pattern or a lexico-syntactic pattern, and
can possibly include entity-type restrictions on
the arguments. For example, Figure 2 rep-
resents a binary pattern that corresponds to a
wedding event between two people.

• An extraction is a pattern instance obtained
from an input sentence, involving specific en-
tities. For example, the subgraph represented
with solid dependency edges in Figure 1 is an
extraction corresponding to the pattern in Fig-
ure 2.

• An Extracted Event Candidate Set (EEC-
Set (Zhang & Weld, 2013), or just EEC for
brevity) is the set of extractions obtained from

news articles published on the same day, and
involving the same set of entities.

• Two extractions are co-occurrent if there is at
least one EEC that contains both of them.

NEWSSPIKE produces extractions from the in-
put documents using REVERB (Fader et al., 2011).
The EECs are generated from the titles and all the
sentences of the first paragraph of the documents
published on the same day. From each EEC, po-
tentially one paraphrase cluster may be generated.
The model is a factor graph that captures several
additional heuristics. Integer Lineal Programming
(ILP) is then used to find the Maximum a Posteriori
(MAP) solution for each set of patterns, and model
parameters are trained using a labeled corpus that
contains 500 of these sets.

Regarding HEADY, it only considers titles and
first sentences for pattern extraction and trains a
two-layer Noisy-OR Bayesian Network, in which
the hidden nodes represent possible event types, and
the observed nodes represent the textual patterns.
A maximum-likelihood model is the one in which
highly co-occurring patterns are generated by the
same latent events. The output is a global soft clus-
tering, in which two patterns may also be clustered
together even if they never co-occur in any EEC,
as long as there is a chain of co-occurring patterns
generated by the same hidden node. HEADY was
evaluated using three different extraction methods:
a heuristic-based pattern extractor, a sentence com-
pression algorithm and a memory-based method.

While this model produces a soft clustering of
patterns, HEADY was evaluated only on a headline
generation task and not intrinsically w.r.t. the quality
of the clustering itself.

Neural networks and distributed representations
Another related field aims to learn continuous vec-
tor representations for various abstraction levels of

1142



natural language. In particular the creation of so-
called word embeddings has attracted a lot of atten-
tion in the past years, often by implementing neural-
network language models. Prominent examples in-
clude the works by Bengio et al. (2003) and Mikolov
et al. (2013), with the skip-gram model of the lat-
ter providing a basis for the vector representations
learned in our approach.

Also closely related to IDEST are approaches
which employ neural networks capable of handling
word sequences of variable length. For example,
Le & Mikolov (2014) extend the architectures of
Mikolov et al. (2013) with artificial paragraph to-
kens, which accumulate the meaning of words ap-
pearing in the respective paragraphs.

In contrast to these shallow methods, other ap-
proaches employ deep multi-layer networks for the
processing of sentences. Examples include Kalch-
brenner et al. (2014), who employ convolutional
neural networks for analyzing the sentiment of sen-
tences, and Socher et al. (2013), who present a spe-
cial kind of recursive neural network utilizing ten-
sors to model the semantics of a sentence in a com-
positional way, guided by the parse tree.

A frequent issue with the deeper methods de-
scribed above is the high computational complex-
ity coming with the large numbers of parameters in
a multi-layer neural network or in the value prop-
agation in unfolded recursive neural networks. To
circumvent this problem, our model is inspired by
Mikolov’s simpler skip-gram model, as described
below.

3 Proposed model

Similarly to HEADY and NEWSSPIKE, our model is
also based on the underlying assumption that if sen-
tences from two news articles were published on the
same day and mention the same entity set, then they
are good paraphrase candidates. The main novelty
is in the way we train the paraphrase model from
the source data. We propose a new neural-network
architecture which is able to learn meaningful dis-
tributed representations of full patterns.

3.1 Skip-gram neural network

The original Skip-gram architecture (Mikolov et al.,
2013) is a feed-forward neural network that is
trained on distributional input examples, by assum-

ing that each word should be able to predict to some
extent the other words in its context. A skip-gram
architecture consists of:

1. An input layer, usually represented as a one-of-
V or one-hot-spot layer. This layer type has as
many input nodes as the vocabulary size. Each
training example will activate exactly one input
node corresponding to the current word wi, and
all the other input nodes will be set to zero.

2. A first hidden layer, the embedding or projec-
tion layer, that will learn a distributed represen-
tation for each possible input word.

3. Zero or more additional hidden layers.

4. An output layer, expected to pre-
dict the words in the context of wi:
wi−K , ..., wi−1, wi+1, ..., wi+K .

In practice, when training based on this architecture,
the network converges towards representing words
that appear in similar contexts with vectors that are
close to each other, as close vectors will produce a
similar distribution of output labels in the network.

3.2 IDEST neural network
Figure 3 shows the network architecture we use for
training our paraphrase model in IDEST. In our case,
the input vocabulary is the set ofN unique event pat-
terns extracted from text, and our supervision signal
is the co-occurrence of event patterns in EECs. We
set the input to be a one-hot-spot layer with a di-
mensionality of N , and for each pair of patterns that
belong to the same EECs, we will have these pat-
terns predict each other respectively, in two separate
training examples. The output layer is also a one-
of-V layer, because for each training example only
one output node will be set to 1, corresponding to a
co-occurring pattern.

After training, if two patterns Pi and Pj have
a large overlap in the set of entities they co-occur
with, then they should be mapped onto similar in-
ternal representations. Note that the actual entities
are only used for EEC construction, but they do not
play a role in the training itself, thus allowing the
network to generalize over specific entity instantia-
tions. To exemplify, given the two EECs {“[Alex]
married [Leslie]”, “[Leslie] tied the knot with

1143



Input Embedding SoftMax

P0

))

##

��

O0

P1 //

))

##

e0

44

//

w′0j

$$

��

O1

... e1

::

44

w′1j

**

��

...

Pi

wi0

;;

wi1
55

wiE

))

... Oj

... eE

CC

??

w′Ej
55

))

...

PV

DD

AA

55

OV

Figure 3: Model used for training. V is the total number
of unique patterns, which are used both in the one-of-V
input and output. E is the dimensionality of the embed-
ding space.

[Alex]”} and {“[Carl] and [Jane] wed”, “[Carl]
married [Jane]”}, IDEST could learn an embedding
space in which “[Per] tied the know with [Per]” and
“[Per] and [Per] wed” are relatively close, even
though the two patterns never co-occur in the
same EEC. This is possible because both pat-
terns have been trained to predict the same pattern
{“[Per] married [Per]”}.

Reported representations of word embeddings
typically use between 50 and 600 dimensions
(Mikolov et al., 2013; Levy & Goldberg, 2014). For
our pattern embeddings we have opted for an em-
bedding layer size of 200 nodes. We also experi-
mented with larger sizes and with adding more in-
termediate hidden layers, but while the added cost
in terms of training time was substantial we did not
observe a significant difference in the results.

4 Experimental settings

4.1 Pattern extraction methods used
In previous work we can find three different pattern
extraction methods from a sentence:

• Heuristic-based, where a number of hand-
written rules or regular expressions based on

part-of-speech tags or dependency trees are
used to select the most likely pattern from the
source sentence (Fader et al., 2011; Mausam
et al., 2012; Alfonseca et al., 2013).

• Sentence compression, which takes as input the
original sentence and the entities of interest and
produces a shorter version of the sentence that
still includes the entities (Pighin et al., 2014).

• Memory-based, that tries to find the shortest
reduction of the sentence that still includes
the entities, with the constraint that its lexico-
syntactic structure has been seen previously as
a full sentence in a high-quality corpus (Pighin
et al., 2014).

It is important to note that the final purpose of
the system may impact the decision of which ex-
traction method to choose. Pighin et al. (2014) use
the event models to generate headlines, and using
the memory-based method resulted in more gram-
matical headlines at the cost of coverage. If the
purpose of the patterns is information extraction for
knowledge base population, then the importance of
having well-formed complete sentences as patterns
becomes less obvious, and higher coverage meth-
ods become more attractive. For these reasons, in
this paper we focus on the first two approaches,
which are very well-established and can produce
high-coverage output. More specifically, we use
REVERB extractions and a statistical compression
model trained on (sentence, compression) pairs im-
plemented after Filippova & Altun (2013).

4.2 Generating clusters from the embedding
vectors

IDEST does not produce a clustering like
NEWSSPIKE and HEADY, so in order to be able to
compare against them we have used the algorithm
described in Figure 4 to build paraphrase clusters
from the pattern embeddings. Given a similarity
threshold on the cosine similarity of embedding vec-
tors, we start by sorting the patterns by extraction
frequency and proceed in order along the sorted vec-
tor by keeping the most similar pattern of each. Used
patterns are removed from the original set to make
sure that a pattern is not added to two clusters at the
same time.

1144



function COMPUTECLUSTERS(P , θ)
Result = {}
SORTBYFREQUENCY(P )
while |P | > 0 do

p = POP(P ) . Take highest-frequency pattern
Cp = {p} . Initialize cluster around p
N = NEIGHBORS(p, P, θ) . n ∈ P, sim(n, p) > θ
for all n ∈ N do

Cp = Cp ∪ {n}
REMOVE(P, n) . Remember n has been used

Result = Result ∪ {Cp}
return Result

Figure 4: Pseudocode of the algorithm for producing a
clustering from the distributed representation of the ex-
tracted patterns. P is the set of extracted patterns, and θ
is the similarity threshold to include two patterns in the
same cluster.

5 Evaluation results

This section opens with a quantitative look at the
clusterings obtained with the different methods to
understand their implications with respect to the dis-
tribution of event clusters and their internal diversity.
In 5.2, we will complement these figures with the re-
sults of a manual quality evaluation.

5.1 Quantitative analysis
5.1.1 NEWSSPIKE vs. IDEST-ReV-NS

This section compares the clustering models that
were output by NEWSSPIKE and IDEST when us-
ing the same set of extractions, to evaluate the
performance of the factor graph-based method and
the neural-network method on exactly the same
EECs. We have used as input the dataset released
by Zhang & Weld (2013)1, which contains 546,713
news articles, from which 2.6 million REVERB
extractions were reportedly produced. 84,023 of
these are grouped into the 23,078 distributed EECs,
based on mentions of the same entities on the same
day. We compare here the released output clusters
from NEWSSPIKE and a clustering obtained from
a IDEST-based distributed representation trained on
the same EECs.

Figure 5 shows a comparative analysis of the two
sets of clusters. As can be seen, IDEST generates
somewhat fewer clusters for every cluster size than
NEWSSPIKE. We have also computed a lexical di-
versity ratio, defined as the percentage of root-verb

1http://www.cs.washington.edu/node/9473

Figure 5: Cluster size (log-scale) and ratio of unique verb
lemmas in the clusters generated from NEWSSPIKE and
IDEST with the REVERB extractions as input.

lemmas in a cluster that are unique. This met-
ric captures whether a cluster mainly contains the
same verb with different inflections or modifiers, or
whether it contains different predicates. The fig-
ure shows that IDEST generates clusters with much
more lexical diversity. These results make sense in-
tuitively, as a global model should be able to pro-
duce more aggregated clusters by merging patterns
originating from different EECs, resulting in fewer
clusters with a higher lexical diversity. A higher lex-
ical diversity may be a signal of richer paraphrases
or noisier clusters. The manual evaluation in Sec-
tion 5.2 will address this issue by comparing the
quality of the clusterings.

5.1.2 NEWSSPIKE vs. IDEST-Comp-NS
Figure 6 compares NEWSSPIKE’s clusters against

IDEST clusters obtained using sentence compres-
sion instead of REVERB for extracting patterns.
Both systems were trained on the same set of input
news. Using sentence compression, the total num-
ber of extracted patterns was 321,130, organized in
41,740 EECs. We can observe that IDEST produced
larger clusters than NEWSSPIKE. For cluster sizes
larger or equal to 4, this configuration of IDEST
produced more clusters than NEWSSPIKE. At the
same time, lexical diversity remained consistently
on much higher levels, well over 60%.

5.1.3 IDEST-Comp-NS vs. IDEST-Comp-All
In order to evaluate the impact of the size of train-

ing data, we produced a clustering from embedding
vectors trained from a much larger dataset. We used

1145



Figure 6: Cluster size (log-scale) and ratio of unique verb
lemmas in the clusters generated from NEWSSPIKE and
IDEST with compression-based pattern extraction.

Figure 7: Cluster size (log-scale) and ratio of unique
verb lemmas in the clusters generated from IDEST with
compression-based pattern extraction, using only the
500,000 NEWSSPIKE articles, or the large dataset.

our own crawl of news collected between 2008 and
2014. Using sentence compression, hundreds of
millions of extractions have been produced.

In order to keep the dataset at a reasonable size,
and aiming at producing a model of comparable size
to the other approaches, we applied a filtering step
in which we removed all the event patterns that were
not extracted at least five times from the dataset. Af-
ter this filtering, 28,014,423 extractions remained,
grouped in 8,340,162 non-singleton EECs.

Figure 7 compares the resulting clusterings. In
the all-data setting, clusters were generally smaller
and showed less lexical variability. We believe that
this is due to the removal of the long tail of low-
frequency and noisy patterns. Indeed, while high
lexical variability is desirable it can also be a sign

of noisy, unrelated patterns in the clusters. The co-
hesiveness of the clusters, which we will evaluate in
Section 5.2, must also be considered to tell construc-
tive and destructive lexical variability apart.

5.1.4 HEADY
HEADY produces a soft-clustering from a gener-

ative model, and expects the maximum number of
clusters to be provided beforehand. The model then
tries to approximate this number. In our experi-
ments, 5,496 clusters were finally generated. One
weak point of HEADY, mentioned above, is that low-
frequency patterns do not have enough evidence and
Noisy-OR Bayesian Networks tend to discard them;
in our experiments, only 4.3% of the unique ex-
tracted patterns actually ended up in the final model.

5.2 Qualitative analysis

The clusters obtained with different systems and
dataset have been evaluated by five expert raters with
respect to three metrics, according to the following
rating workflow:

1. The rater is shown the cluster, and is asked to
annotate which patterns are meaningless or un-
readable2. This provides us with a Readabil-
ity score, which measures at the same time the
quality of the extraction algorithm and the abil-
ity of the method to filter out noise.

2. The rater is asked whether there is a majority
theme in the cluster, defined as having at least
half of the readable patterns refer to the same
real-world event happening. If the answer is
No, the cluster is annotated as noise. We call
this metric Cohesiveness.

3. If a cluster is cohesive, the rater is finally asked
to indicate which patterns are expressing the
main theme, and which ones are unrelated to
it. The third metric, Relatedness, is defined
as the percentage of patterns that are related to
the main cluster theme. All the patterns in a
non-cohesive cluster are automatically marked
as unrelated.

2In the data released by NewsSpike, REVERB patterns are
lemmatized, but the original inflected sentences are also pro-
vided. We have restored the original inflection of all the words
to make those patterns more readable for the raters.

1146



The inter-annotator agreement on the three metrics,
measured as the intraclass correlation (ICC), was
strong (Cicchetti, 1994; Hallgren, 2012). More pre-
cisely, the observed ICC scores (with 0.95 confi-
dence intervals) were 0.71 [0.70, 0.72] for cohe-
siveness, 0.71 [0.70, 0.73] for relatedness and 0.66
[0.64, 0.67] for readability.

For the evaluation, from each model we se-
lected enough clusters to achieve an overall size
(number of distinct event patterns) comparable to
NEWSSPIKE’s. For HEADY and IDEST, the stop-
ping condition in Figure 4 was modified accordingly.

Table 1 shows the outcome of the annotation.
As expected, using a global model (that can merge
patterns from different EECs into single clusters)
and using the whole news dataset both led to larger
clusters. At the same time, we observe that using
REVERB extractions generally led to smaller clus-
ters. This is probably because REVERB produced
fewer extractions than sentence compression from
the same input documents.

On REVERB extractions, NEWSSPIKE outper-
formed IDEST in terms of cohesiveness and related-
ness, but NEWSSPIKE’s lowest cluster size and lex-
ical diversity makes it difficult to prefer any of the
two models only w.r.t. the quality of the clusters. On
the other hand, the patterns retained by IDEST-ReV-
NS were generally more readable (65.16 vs. 56.66).

On the same original news data, using IDEST
with sentence compression produced comparable
results to IDEST-ReV-NS, Cohesiveness being the
only metric that improved significantly.

More generally, in terms of readability all the
models that rely on global optimization (i.e., all
but NEWSSPIKE) showed better readability than
NEWSSPIKE, supporting the intuition that global
models are more effective in filtering out noisy ex-
tractions. Also, the more data was available to
IDEST, the better the quality across all metrics.
IDEST model using all data, i.e, IDEST-Comp-All,
was significantly better (with 0.95 confidence) than
all other configurations in terms of cluster size, co-
hesiveness and pattern readability. Pattern related-
ness was higher, though not significantly better, than
NEWSSPIKE, whose clusters were on average more
than ten times smaller.

We did not evaluate NEWSSPIKE on the whole
news dataset. Being a local model, extending the

System Ext Data Size Coh(%) Rel(%) Read(%)

HEADY Comp All 12.66bcd 34.40! 27.70! 60.70
NEWSSPIKE ReV NS 3.40! 56.20ac 66.42acd 56.66
IDEST ReV NS 3.62b 40.00 47.10a 65.16b

IDEST Comp NS 5.54bc 50.31ac 46.58a 66.04b

IDEST Comp All 44.09∗ 87.93∗ 68.28acd 80.13∗

Table 1: Results of the manual evaluation, averaged over
all the clusters produced by each configuration listed. Ex-
traction algorithms: ReV = REVERB; Comp = Com-
pression; Data sets: NS = NewsSpike URLs; All = news
2008-2014. Quality metrics: Size: average cluster size;
Coh: cohesiveness; Rel: relatedness; Read: readability.
Statistical significance: a: better than HEADY; b: bet-
ter than NEWSSPIKE; c: better than IDEST-ReV-NS; d:
better than IDEST-Comp-NS; ∗: better than all others; !:
worse than all others (0.95 confidence intervals, bootstrap
resampling).

dataset to cover six years of news would only lead
to many more EECs, but it would not affect the re-
ported metrics as each final cluster would still be
generated from one single EEC.

It is interesting to see that, even though they
were trained on the same data, IDEST outperformed
HEADY significantly across all metrics, sometimes
by a very large margin. Given the improvements
on cluster quality, it would be interesting to evalu-
ate IDEST performance on the headline-generation
task for which HEADY was initially designed, but
we leave this as future work.

6 Conclusions

We described IDEST, a new approach based on neu-
ral networks to map event patterns into an embed-
ding space. We show that it can be used to construct
high quality pattern clusters based on neighborhood
in the embedding space. On a small dataset, IDEST
produces comparable results to NEWSSPIKE, but its
main strength is in its ability to generalize extrac-
tions into a single global model. It scales to hun-
dreds of millions of news, leading to larger clusters
of event patterns with significantly better coherence
and readability. When compared to HEADY, IDEST
outperforms it significantly on all the metrics tried.

Acknowledgments

The first author was partially supported by the Ger-
man Federal Ministry of Education and Research,
project ALL SIDES (contract 01IW14002).

1147



References

Agichtein, E. & L. Gravano (2000). Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on
Digital libraries, pp. 85–94.

Alfonseca, E., D. Pighin & G. Garrido (2013).
HEADY: News headline abstraction through
event pattern clustering. In Proc. of ACL-13, pp.
1243–1253.

Banko, M., M. J. Cafarella, S. Soderland, M. Broad-
head & O. Etzioni (2007). Open information ex-
traction from the Web. In Proc. of IJCAI-07, pp.
2670–2676.

Bengio, Y., R. Ducharme & P. Vincent (2003). A
neural probabilistic language model. Journal of
Machine Learning Research, 3:1137–1155.

Brin, S. (1999). Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases, pp. 172–183. Springer.

Cicchetti, D. V. (1994). Guidelines, criteria, and
rules of thumb for evaluating normed and stan-
dardized assessment instruments in psychology.
Psychological Assessment, 6(4):284.

Fader, A., S. Soderland & O. Etzioni (2011). Iden-
tifying relations for open information extraction.
In Proc. of EMNLP-11, pp. 1535–1545.

Fader, A., L. S. Zettlemoyer & O. Etzioni (2013).
Paraphrase-driven learning for open question an-
swering. In Proc. of ACL-13, pp. 1608–1618.

Filippova, K. & Y. Altun (2013). Overcoming the
lack of parallel data in sentence compression. In
Proc. of EMNLP-13, pp. 1481–1491.

Grycner, A. & G. Weikum (2014). Harpy: Hyper-
nyms and alignment of relational paraphrases. In
Proc. of COLING-14, pp. 2195–2204.

Hallgren, K. A. (2012). Computing inter-rater re-
liability for observational data: An overview and
tutorial. Tutorials in quantitative methods for psy-
chology, 8(1):23.

Kalchbrenner, N., E. Grefenstette & P. Blunsom
(2014). A convolutional neural network for mod-
elling sentences. In Proc. of ACL-14.

Le, Q. & T. Mikolov (2014). Distributed represen-
tations of sentences and documents. In Proc. of
ICML-14.

Levy, O. & Y. Goldberg (2014). Linguistic regular-
ities in sparse and explicit word representations.
In Proc. of CoNLL-14.

Lin, D. & P. Pantel (2001). Discovery of inference
rules for question-answering. Natural Language
Engineering, 7(4):343–360.

Mausam, M. Schmitz, R. Bart, S. Soderland &
O. Etzioni (2012). Open language learning for in-
formation extraction. In Proc. of EMNLP-12, pp.
523–534.

Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado
& J. Dean (2013). Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Sys-
tems, pp. 3111–3119.

Moro, A. & R. Navigli (2012). Wisenet: Building
a wikipedia-based semantic network with ontolo-
gized relations. In Proc. of CIKM-12, pp. 1672–
1676.

Nakashole, N., G. Weikum & F. Suchanek (2012).
Patty: a taxonomy of relational patterns with se-
mantic types. In Proc. of EMNLP-12, pp. 1135–
1145.

Pighin, D., M. Colnolti, E. Alfonseca & K. Filippova
(2014). Modelling events through memory-based,
Open-IE patterns for abstractive summarization.
In Proc. of ACL-14, pp. 892–901.

Poon, H. & P. Domingos (2009). Unsupervised se-
mantic parsing. In Proc. of EMNLP-09, pp. 1–10.

Ravichandran, D. & E. H. Hovy (2002). Learning
surface text patterns for a question answering sys-
tem. In Proc. of ACL-02, pp. 41–47.

Riedel, S., L. Yao, B. M. Marlin & A. McCallum
(2013). Relation extraction with matrix factor-
ization and universal schemas. In Proc. of HLT-
NAACL-13.

Riloff, E. (1996). Automatically generating extrac-
tion patterns from untagged text. In Proc. of
AAAI-96, pp. 1044–1049.

1148



Sekine, S. (2006). On-demand information extrac-
tion. In Proc. of COLING-ACL-06 Poster Session,
pp. 731–738.

Socher, R., A. Perelygin, J. Wu, J. Chuang, C. Man-
ning, A. Ng & C. Potts (2013). Recursive deep
models for semantic compositionality over a sen-
timent treebank. In Proc. of EMNLP-13.

Yao, L., S. Riedel & A. McCallum (2012). Unsuper-
vised relation discovery with sense disambigua-
tion. In Proc. of ACL-12, pp. 712–720.

Yates, A. & O. Etzioni (2007). Unsupervised resolu-
tion of objects and relations on the Web. In Proc.
of NAACL-HLT-07, pp. 121–130.

Zhang, C. & D. S. Weld (2013). Harvesting paral-
lel news streams to generate paraphrases of event
relations. In Proc. of EMNLP-13, pp. 1776–1786.

1149


