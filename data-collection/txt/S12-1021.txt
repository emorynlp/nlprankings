










































An Unsupervised Ranking Model for Noun-Noun Compositionality


First Joint Conference on Lexical and Computational Semantics (*SEM), pages 132–141,
Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

An Unsupervised Ranking Model for Noun-Noun Compositionality

Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman
Department of Computer Science

University of Oxford
Wolfson Building, Parks Road

Oxford OX1 3QD, UK
{karl.moritz.hermann,phil.blunsom,stephen.pulman}@cs.ox.ac.uk

Abstract

We propose an unsupervised system that
learns continuous degrees of lexicality for
noun-noun compounds, beating a strong base-
line on several tasks. We demonstrate that the
distributional representations of compounds
and their parts can be used to learn a fine-
grained representation of semantic contribu-
tion. Finally, we argue such a representation
captures compositionality better than the cur-
rent status-quo which treats compositionality
as a binary classification problem.

1 Introduction

A Multiword Expressions (MWE) can be defined as
a sequence of words whose meaning cannot nec-
essarily be derived from the meaning of the words
making up that sequence, for example:

Rat Race — self-defeating or pointless pursuit1

MWEs are considered a “key problem for the de-
velopment of large-scale, linguistically sound nat-
ural language processing technology” (Sag et al.,
2002). The challenge posed by MWEs is three-
fold, consisting of MWE identification, classifica-
tion and interpretation. Following the identification
of a MWE, it needs to be established whether the
expression should be treated as lexical (idiomatic)
or as compositional. The final step, learning the se-
mantics of the MWE, strongly depends on this deci-
sion.

1Definition taken from Wikipedia, and clearly not recover-
able if one only knows the meaning of the words ‘rat’ and ‘race’.

The problem posed by MWEs is considered hard,
but at the same time it is highly relevant and inter-
esting. MWEs occur frequently in language and in-
terpreting them correctly would directly improve re-
sults in a number of tasks in NLP such as translation
and parsing (Korkontzelos and Manandhar, 2010).
By extension this makes deciding the lexicality of
MWEs an important challenge for various fields in-
cluding machine translation, question answering and
information retrieval. In this paper we discuss com-
positionality with respect to noun-noun compounds.

Most Computational Linguistics literature treats
compositionality as a binary problem, classifying
compounds as either lexical or compositional. We
show that this approach is too simplistic and argue
for the real-valued treatment of compositionality.

We propose two unsupervised models that learn
compositionality rankings for compounds, placing
them on a scale between lexical and compositional
extremes. We develop a fine-grained representa-
tion of compositionality using a novel generative ap-
proach that models context as generated by com-
pound constituents. This representation differenti-
ates between the semantic contribution of both com-
pound constituents as well as the compound itself.

Comparing it with existing work in the field, we
demonstrate the competitiveness of our approach.
We evaluate on an existing corpus of noun com-
pounds with ranked compositionality data, as well
as on a large corpus with a binary annotation for lex-
ical and compositional compounds. We analyse the
impact of data sparsity and propose an interpolation
approximation which significantly reduces the effect
of sparsity on model performance.

132



2 Related Work

Interpreting MWEs is a difficult task as “compound
nouns can be freely constructed” (Spärck Jones,
1985), and are thus able to proliferate infinitely. At
the same time, semantic composition can take many
different forms, making uniform interpretation of
compounds impossible (Zanzotto et al., 2010).

Most current work on MWEs focuses on inter-
preting compounds and sidesteps the task of deter-
mining whether a compound is compositional in the
first place (Butnariu et al., 2010; Kim and Baldwin,
2008). Such methods, aimed at learning the seman-
tics of compounds, can roughly be divided into two
major strands of research.

One group relies on data intensive methods to ex-
tract semantics vectors from large corpora (Baroni
and Zamparelli, 2010; Zanzotto et al., 2010; Gies-
brecht, 2009). The focus of these approaches is to
develop methods for composing the vectors of un-
igrams into a semantic vector representing a com-
pound. Some of the work in this area touches on the
issue of lexicality, as models learning distributional
representations of MWEs ideally would first estab-
lish whether a given MWE is compositional or not
(Mitchell and Lapata, 2010).

The other group are knowledge intensive ap-
proaches collecting linguistic features (Kim and
Baldwin, 2005; Korkontzelos and Manandhar,
2009). Tratz and Hovy (2010), for instance, train
a classifier for noun compound interpretation on a
large set of WORDNET and Thesaurus features.

Combined approaches include Kim and Baldwin
(2008), who interpret noun compounds by extrapo-
lating their semantics from observations where the
two nouns forming a compound are in an intransi-
tive relationship. For example extracting the phrase
‘the family owns a car’ from the training data would
help learn that the compound ‘family car’ describes
a POSSESSOR-OWNED/POSSESSED relationship.

Some of these supervised classifiers include lexi-
cality as a classification option, considering it jointly
with the actual compound interpretation.

Next to the work on MWE interpretation there has
been some work focused on determining lexicality
in its own right (Reddy et al., 2011; Bu et al., 2010;
Kim and Baldwin, 2007).

One possibility is to exploit special properties of

lexical MWEs such as high statistical association
of their constituents (Pedersen, 2011) or syntactic
rigidity (Fazly et al., 2009; McCarthy et al., 2007).
However, these approaches are limited in their ap-
plicability to compound nouns (Reddy et al., 2011).

Another method is to compare the semantics of
a compound and its constituents to decide com-
positionality. The approaches used to determine
those semantics can again be divided into knowl-
edge intensive and data-driven methods. Depending
on the chosen representation of semantics these ap-
proaches can either be used for supervised classifiers
or together with a distance metric comparing vector
space representations of semantics. In a binary set-
ting, a threshold would then be applied to the result
of that distance function (Korkontzelos and Man-
andhar, 2009). In a real-valued setting the distance
metric itself can be used as a measure for compo-
sitionality (Reddy et al., 2011). Related to the vec-
tor space based models, some research focuses on
improving the distance metrics used to compare in-
duced semantics (Bu et al., 2010).

3 Methodology

English noun-noun compounds are majority left-
branching (Lauer, 1995), with a head (the second
element), modified by an attributive noun (first el-
ement). For example:

Ground Floor — The floor of a building at or near-
est ground level.2

In this paper, we will use the terms attributive noun
(AN) and head noun (HN) to refer to the first and
second noun in a noun compound.

3.1 Real-Valued Representation

Lexicality of MWEs is frequently treated as a bi-
nary property (Tratz and Hovy, 2010; Ó Séaghdha,
2007). We argue that lexicality should instead be
treated as a graded property, as most compound se-
mantics exhibit a mixture of compositional and lexi-
cal influences. For example, ‘cocktail dress’ derives
a large part of its semantics from ‘dress’, but the
compound also contributes an idiosyncratic element
to its meaning.

2Definition from http://www.thefreedictionary.com

133



We define lexicality as the degree to which id-
iosyncrasy contributes to a compound’s semantics.
Inversely phrased, the compositionality of a com-
pound can be defined as the degree to which its sense
is related to the senses of its constituents.3

This graded representation follows Spärck Jones
(1985), who argued that “it is not possible to main-
tain a principled distinction between lexicalised and
non-lexicalised compounds”. Some recent work
also supports this view (Reddy et al., 2011; Bu et
al., 2010; Baldwin, 2006). From a practical per-
spective, a real-valued representation of composi-
tionality should help improve interpretation of com-
pounds. This is especially true when factoring in the
respective semantic contributions of its parts.

3.2 Context Generation
According to the distributional hypothesis, the se-
mantics of a lexical item can be expressed by its
context. We apply this hypothesis to the problem of
noun compound compositionality by using a genera-
tive model on compound context. Our model allows
context to be generated by the compound itself or by
either one of its constituents. By learning which el-
ement of the compound generates which part of its
context we effectively determine the semantic con-
tribution of each element. This in turn gives us a
fine-grained, graded representation of a compound’s
lexicality.

4 Corpora for Evaluation

4.1 Ranked Corpus — REDDY
As we want to evaluate our models’ ability to learn
lexicality as a real-valued property, we require an
annotated data set of noun compounds ranked by
lexicality. To the best of our knowledge the only
such data set was developed by Reddy et al. (2011).
This data set contains 90 distinct noun compounds
with real-valued gold standard scores ranking from
0 (lexical) to 5 (compositional). The compounds
are nearly linearly distributed across the [0;5] range,
with inter annotator agreement (Spearman’s ρ) of

3For example, the meaning of ‘gravy train’ has hardly any
relation to either ‘gravy’ or ‘train’. Its semantics are thus highly
dependent on the compound in its own right. On the other end
of the spectrum, ‘climate change’ is significantly related to both
‘climate’ and ‘change’, contributing little inherent semantics to
its overall meaning.

0.522. We refer to this data set and evaluation as
REDDY throughout this paper.

4.2 Binary Corpora — TRATZ

We also apply our models to a second, binary classi-
fication task. Tratz and Hovy (2010) compiled a data
set for noun compound interpretation, which classi-
fies noun compounds based on their internal struc-
ture. We use this corpus to extract lexical and com-
positional noun compounds.

After some pre-processing4 the data set contains
18,858 compositional and 118 lexical noun com-
pounds. We believe this to more accurately represent
the real world distribution of lexical and composi-
tional noun compounds: Tratz and Hovy (2010) ex-
tracted noun compounds from several large corpora
including the Wall Street Journal section of the Penn
Treebank, thus obtaining a reasonable approxima-
tion of real world occurrence. Other collections of
noun compounds (Ó Séaghdha, 2007) feature sim-
ilar proportions of lexical and compositional noun
compounds.

The large bias towards compositional noun com-
pounds does not support the status-quo of treating
compositionality as a binary property. As discussed
earlier, we assume that most compounds have a
compositional as well as a lexical element. While
the compositional aspect may be larger for most
compounds this alone does not suffice as a reason
to disregard the lexical element contained in these
compounds.

In order to evaluate our system on the TRATZ
data, we use receiving operator characteristic (ROC)
curves. ROC analysis enables us to evaluate a rank-
ing model without setting an artificial threshold for
the compositionality/lexicality decision.

5 Baseline Approach

We develop a set of advanced baselines related to
the semi-supervised models presented by Reddy et
al. (2011). We define the context K of a noun com-
pound as all words in all sentences the compound
appears in. From this we calculate distributional
representations of a compound (c = 〈a, h〉) and its
constituent elements a, h. We refer to these repre-
sentations as ~c for the compound and ~a, ~h for the

4We removed trigrams from the data set.

134



Name ⊕ r ρ
ADD w.Sac + (1− w).Shc .323 .567
MULT Sac.Shc .379 .551
MIN min(Sac, Shc) .343 .550
MAX max(Sac, Shc) .299 .505
COMB w1.Sac+w2.Shc+w3.Sac.Shc .366 .556

Table 1: Results of COSLEX with different operators on
the REDDY data set, reporting Pearson’s r and Spear-
man’s ρ correlations. Weights for operators ADD (w =
0.3) and COMB (w = 〈0.3, 0.1, 0.6〉) are manually opti-
mised. Values range from -1 (negative correlation) to +1
(perfect correlation) with 0 describing random data.

attributive and head noun, respectively. We can cal-
culate the cosine similarity based lexicality score
(COSLEX) by combining the cosine similarity of the
compound’s distribution with each of its two con-
stituents (Reddy et al., 2011).

Sac = sim(~a,~c)

Shc = sim(~h,~c)
COSLEX(c) = Sac ⊕ Shc

We evaluate a number of alternative operators ⊕ for
combining Sac and Shc. Results for this baseline
on the REDDY corpus are in Table 1,5 with weights
wi on the combination operators manually optimised
for Spearman’s ρ on that data set. In effect this
renders this baseline into a supervised approach, so
we would expect it to perform very well. We use
the best performing operators (ADD with w = 0.3,
MULT) as baselines for this paper.

6 Generative Models

We exploit the distributional hypothesis to model
the semantic contribution of the different elements
of a noun compound. For this, we require a sys-
tem that treats a noun compound as a vector of three
semantics-bearing units: the compound itself, its
head and its attributive noun. This system should
then model the relationship between the context of
the compound and these three units, deciding which
of them is responsible for each context element.

5Reddy et al. (2011) report higher figures on our baseline
models. The differences are attributed to differences in training
data and parametrization.

6.1 3-way Compound Mixture

We model a corpus D of tuples d = {c, k1, ..., kn}.
Each tuple d contains a noun compound c = 〈a, h〉
and its context words K = (k1, ..., kn). We use vo-
cabularies Vc for noun compounds, Va for attributive
nouns, Vh for head nouns and Vk for context.

We condition our generative model on the noun
compounds. Given an observation d of a compound
c, we generate each context word in two steps. First,
we choose one of the compounds three elements6 to
generate the next context word. Second, we gener-
ate a new context word conditioned on that element.
Formally, the context is generated as follows.

We draw three multinomial parameters Ψc, Ψa

and Ψh from Dirichlet distributions with parameters
αc, αa and αh. Ψc represents the distribution over
context words Vk given compound c. Ψa and Ψh

are distributions over Vk given attributive noun a and
head noun h, respectively. These three distributions
form the mixture components of our model.

A fourth multinomial parameter Ψz , drawn from
a Dirichlet distribution with parameter αz , controls
the distribution over the mixture components. Ψz is
specific to each compound c, so multiple observa-
tions of the same compound share this parameter.

For each context word we draw a mixture compo-
nent zc,i ∈ {č, ǎ, ȟ} from the multinomial distribu-
tion with parameter Ψz . zc,i determines which dis-
tribution the context word itself will be drawn from.
Finally, we draw the context word:

∀i: ki | Ψ{zc,i} ∼ Multi(Ψ{zc,i})

Thus, for each observation of a compound noun we
have a vector zc = 〈z1, ..., zn〉 detailing how its
context words were created either by the compound
itself or by one of its constituents. To determine lex-
icality, we are interested in learning the multinomial
parameter Ψz , which describes to what extent the
compound and its constituents contribute to the gen-
eration of the context (i.e. semantics). We can ap-
proximate Ψz from the vector zc.

We define the lexicality score Lex(c) for a com-
pound as the percentage of context words created by

6The compound itself, its attributive noun and its head noun

135



Figure 1: Plate diagram illustrating the MULT-CMPD
model with context words ki drawn from a mixture model
with three components controlled by zi.

the compound and not one of its constituents:

Lex(c) = p(z=č|〈a, h〉), (1)
where c = 〈a, h〉

Figure 1 shows a plate diagram of this model, which
we will refer to as MULT-CMPD.

One hypothesis encoded in model MULT-CMPD
is that deciding which part of a compound (the com-
pound itself, the head or the attributive noun) gen-
erates context is a single decision. An alternative
representation could treat this as a two-step process,
which we encode in a second model BIN-CMPD.
The intuition behind the BIN-CMPD model is that
there are two distinct decisions. First, whether a
compound is compositional or not. Second, whether
(in the compositional case) its semantics stem from
its head or attributive noun

Where MULT-CMPD uses a three component mix-
ture to determine which multinomial distribution to
use, BIN-CMPD uses two cascaded binary mixtures
(see Figure 2). The BIN-CMPD model first chooses
whether to treat a compound as compositional or
lexical. If the compound is determined as composi-
tional, a second binary mixture determines whether
to generate a context word using the attributive (Ψa)
or head multinomial (Ψh). For the lexical case, the
model remains unchanged.

Figure 2: Schematic description of compositional-
ity/lexicality decision for models MULT-CMPD and BIN-
CMPD.

Model r ρ
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551
MULT-CMPD .141 .435
BIN-CMPD .168 .410

Table 2: Results on the REDDY data set, reporting Pear-
son’s r and Spearman’s ρ correlations. Values range from
-1 (negative correlation) to +1 (perfect correlation).

6.1.1 Inference and Sampling
We use Gibbs sampling to learn the vectors z for

each instance d, integrating out the parameters Ψx.
We train our models on the British National Corpus
(BNC), extracting all noun-noun compounds from a
parsed version of the corpus.

In order to speed up convergence of the sampler,
we use simulated annealing over the first 20 iter-
ations (Kirkpatrick et al., 1983), helping the ran-
domly initialised model reach a mode faster. We re-
port results using marginal distributions after a fur-
ther 130 iterations, excluding the counts of the an-
nealing stage.

6.1.2 Evaluation
We evaluate our two models on the REDDY data

set by comparing its scores for lexicality (Lex(c))
with the annotated gold standard. The aim of this
evaluation is to determine how accurately the mod-
els can capture gradual distinctions in lexicality. The
ROC analysis on the TRATZ data set furthermore in-
forms us how precise the models are at distinguish-
ing lexical from compositional compounds.

Results of the REDDY evaluation are in Table 2.
We use Spearman’s ρ to measure the monotonic cor-
relation of our data to the gold standard. Pearson’s r
additionally captures the linear relationship between
the data, taking into account the relative differences
in Lex(c) scores among noun compounds.

136



Figure 3: ROC analysis of models MULT-CMPD and
BIN-CMPD versus the best COSLEX baseline (ADD) on
the TRATZ data set

While both models, BIN-CMPD and MULT-
CMPD, clearly learn a correlation with lexical-
ity rankings, they underperform the strong, semi-
supervised COSLEX baselines described earlier in
this paper. The second evaluation, on the binary
TRATZ data set shows a different picture (see Fig-
ure 3). The best COSLEX baseline (ADD with
w = 0.2) fails to outperform random choice on this
task. Both generative models clearly beat COSLEX
on this task, with MULT-CMPD in particular per-
forming very well for low sensitivity.

There is no clear distinction in performance be-
tween the two generative approaches. Further anal-
ysis might help us to separate the two more clearly,
and we will continue using both models throughout
this paper.

It is important to note the different performance of
the generative models vs. the cosine similarity ap-
proach on two tasks. The REDDY data set has a
nearly linear distribution of compositionality scores,
while the TRATZ data set is overwhelmingly com-
positional, which more closely represents the real
world distribution of compounds. The poor perfor-
mance of the cosine similarity approach (COSLEX)
on the TRATZ evaluation suggests the limitations
of this approach when applied to more realistic data
such as this data set. An additional explanation for
the semi-supervised baseline’s poorer result is that
the effect of parameter tuning decreases on larger
data.

Investigating the errors made by the models
MULT-CMPD and BIN-CMPD gives rise to a number
of possible explanations for their performance. The
most promising lead is related to data sparsity, with
many of the evaluated noun-noun compounds only
appearing once or twice in the corpus. This makes it
harder for our generative approach to learn sensible
context distributions for these instances.

We will next investigate how to reduce the effects
encountered by sparsity.

6.2 Interpolation

Working on problems related to non-unigram data,
sparsity is a frequently encountered problem. As al-
ready explored in the previous section, this is also
the case for our generative models of lexicality.

It would be possible to use an even larger training
corpus, but there are limitations as to what extent
this is possible. The BNC, containing 100 million
words, is already one of the largest corpora regu-
larly used in Computational Linguistics. However,
adding more data in an unsupervised sense is un-
likely to significantly improve results (Brants et al.,
2007).

Alternatively, it would be possible to add spe-
cific training data that included the noun compounds
from the evaluation data sets. This would, how-
ever, compromise the unsupervised nature of our ap-
proach, and it thus not an option either.

In this paper, we will instead focus on extenuat-
ing the effects of data sparsity through other unsu-
pervised means. For this purpose we investigate in-
terpolating on a larger set of noun compounds.

Kim and Baldwin (2007) observed that seman-
tic similarity of verb-particle compounds correlates
with their lexicality. We extend this observation for
noun compounds, hypothesising that the lexicality
of similar words will be similar. We combine this
with the assumption that noun compounds sharing a
constituent are likely to be semantically similar (Ko-
rkontzelos and Manandhar, 2009).

Using this idea, we can approximate the lexical-
ity of a given compound with the lexicality scores of
all compounds sharing either of its constituents. So
far we have calculated the lexicality of a given com-
pound using the formula Lex(c) in Equation 1. The
formula Clex(c) in Equation 2 averages the lexical-
ity scores of a compound with those of its related

137



Function and Model r ρ
COSLEX (ADD) .323 .567
COSLEX (MULT) .379 .551

Lex(c)
MULT-CMPD .141 .435
BIN-CMPD .168 .410

Clex(c)
MULT-CMPD .357 .596
BIN-CMPD .400 .592

Ilex(c)
MULT-CMPD .422 .621
BIN-CMPD .538 .623

Table 3: Results on the REDDY data set, reporting
Pearson’s r and Spearman’s ρ correlations, comparing
Ilex(c) and Clec(c) interpolations with Lex(c).

compounds. As p(z=1|〈a, h〉) directly influences
both p(z=1|〈a, ·〉) and p(z=1|〈·, h〉), we can also
consider dropping it from the approximation such as
in Equation 3. This approach trades some specificity
in favour of reducing sparsity, as we observe more
instances of such related compounds than of a par-
ticular noun compound itself only.

Lex(c) ≈ Clex(c) (2)

Clex(c) =
p(z=1|〈a, ·〉) + p(z=1|〈·, h〉) + p(z=1|〈a, h〉)

3
,

where c = 〈a, h〉
Lex(c) ≈ Ilex(c) (3)

Ilex(c) =
p(z=1|〈a, ·〉) + p(z=1|〈·, h〉)

2
,

where c = 〈a, h〉

Both formulations enable us to better deal with
sparse data as decisions are made based on a wider
range of observations. At the same time, we avoid a
loss of specificity as the models and scores are still
highly dependent on the individual noun compound.

We avoid introducing additional degrees of free-
dom by using uniform weights only. However, it
would be simple to turn this approach into a semi-
supervised model by tuning the weights for the dif-
ferent probabilities involved in calculating Clex(c)
and Lex(c). That approach would be comparable to
the operators used on our COSLEX baselines.

Results on the REDDY data set using Clex(c)
and Ilex(c) are in Table 3. Figure 4 shows the im-
pact of these approximations on the Tratz data for
the BIN-CMPD model. These interpolations suggest
strong improvements in performance. It should es-
pecially be noted that Ilex(c) consistently outper-
forms Clex(c), which indicates the strength of the

Figure 4: ROC analysis of model BIN-CMPD on the
TRATZ data set, comparing Ilex(c) and Clec(c) inter-
polations with Lex(c).

related-compound probabilities over the individual
compound probabilities.

These results confirm our suspicion that sparsity
was a major factor affecting our models’ perfor-
mance. Furthermore, they strengthen our hypothe-
sis about the relatedness of semantic similarity and
lexicality and demonstrate a sensible approach for
exploiting this relationship.

7 Analysis

We use this section for qualitative evaluation, com-
plementing the quantitative evaluation in the previ-
ous sections. The purpose of the qualitative evalu-
ation is to better understand exactly what it is our
models are learning.

Table 5 lists the compounds that model BIN-
CMPD considers the most lexical and the most com-
positional. The list of compounds with the high lex-
icality scores is dominated by proper nouns such as
countries, companies and persons. This is in line
with expectation as compounds of proper nouns are
fully lexical. Removing proper nouns (also in Table
5), we get a slightly more ambiguous list. For exam-
ple, ‘study design’ is not considered a lexical com-
pound, but rather a highly institutionalized, com-
positional MWE (Sag et al., 2002). Using Lex(c)
‘study design’ is ranked as such, so this appears to
be a case where interpolation has a negative impact.

In this paper we argued for a finer grained analysis
of compositionality, taking into account the differ-

138



Context of ‘flea market’ generated by
flea market flea market
canal, wall, incline,
campsite

stall, Paris, sale,
Saturday, week,
Sunday, quarter,
damage, change

barter, souvenir,
launderette,
Lamine, Canet,
Kouyate, Plage

Context of ‘night owl’ generated by
night owl night owl
court, fee, guest,
early, day, Baden,
membership, life,
game

waive, player,
Halikarnas, bar,
bird, unbooked,
Vienna

adventurous

Context of ‘memory lane’ generated by
memory lane memory lane
take, story, about,
tell, real, glimpse,
Britain, reminis-
cence

village, protection,
drive, catwalk,
plant

war, justify, bill,
Campbell, rude-
boys

Context of ‘melting pot’ generated by
melting pot melting pot
forest, racial,
caribbean, plan,
programme, real-
ity, arrangement

in, into, put, polit-
ical, community,
prepare

ethnic, greatest,
drawing, liaise,
pan-european,
myth

Table 4: Overview over context words generated by model BIN-CMPD. We list a selection of words predominately
generated by each of the mixture components of the given noun-noun compound.

Most Compositional
labour union, tax authority, health council,
market counterparty, employment policy

Most Lexical
study design, family motto, wood shaving,
avoidance behaviour, smash hit

Most Lexical (including Proper Nouns)
Vo Quy, Bonito Oliva, Mamur Zapt, Evander
Holyfield, Saudi Arabia

Table 5: Top lexical and compositional nouns for the
BIN-CMPD model using Ilex(c)

ent impact of both constituents. We tried to achieve
this by modelling a compound’s context as gener-
ated from its various semantic constituents. Table 4
highlights the impact of this method for a number
of noun compounds, showing which context words
were predominately generated by each constituent.

Due to the nature of the context used, some of
the links are semantically not obvious (e.g. the rela-
tionship between owls and Vienna). In some cases
the semantic contribution of the parts is more clearly
separated, such as the contributions of ‘memory’ and
‘lane’ to the semantics of ‘memory lane’. In sum-
mary, these examples clearly suggest that our mod-
els learn to associate context with compound ele-
ments and that this association is an informed one.

8 Conclusion

We proposed a novel approach for learning lexicality
scores for noun compounds and empirically demon-
strated the feasiblity of this approach. Using a gen-

erative model we were able to beat a strong, semi-
supervised baseline with an unsupervised model.

We discussed the issue of data sparsity in depth
and proposed several approaches for overcoming
this problem. Focusing on unsupervised approaches,
we demonstrated how interpolation can be used to
tackle sparsity. The two interpolation methods that
we implemented helped us to strongly improve over-
all model performance. Our empirical evaluation of
interpolation metricsClex(c) and Ilex(c) also gives
credence to the hypothesis that lexicality is related to
semantic similarity.

On the theoretical side, we offered further support
to the real-valued treatment of lexicality.

Further work will include using larger training
corpora. While the BNC is a popular corpus in Com-
putational Linguistics, it proved to be too small to
learn sensible representations for a number of com-
pounds encountered in the test data. Using larger
corpora will also allow us to further study and re-
duce the sparsity issues encountered.

To study the relationship between constituent and
compound compositionality in greater depth, we
will also investigate alternative approaches for in-
terpolation. Similarity measures that consider the
semantic relevance of individual context elements
should also be considered as a next step.

Another obvious source of future work is to ap-
ply our approach to general collocations beyond the
special case of noun compounds only.

Acknowledgments

The authors would like to acknowledge the use of
the Oxford Supercomputing Centre (OSC) in carry-
ing out this work.

139



References
Timothy Baldwin. 2006. Compositionality and mul-

tiword expressions: Six of one, half a dozen of the
other? In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying
Properties, page 1, Sydney, Australia. Association for
Computational Linguistics.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 1183–1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language Mod-
els in Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 858–
867.

Fan Bu, Xiaoyan Zhu, and Ming Li. 2010. Measuring
the non-compositionality of multiword expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ’10, pages 116–
124, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid Ó. Séaghdha, Stan Szpakowicz, and Tony Veale.
2010. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ’10, pages 39–
44, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61–103.

Eugenie Giesbrecht. 2009. In search of semantic com-
positionality in vector spaces. In Proceedings of the
17th International Conference on Conceptual Struc-
tures: Conceptual Structures: Leveraging Semantic
Technologies, ICCS ’09, pages 173–184, Berlin, Hei-
delberg. Springer-Verlag.

Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using wordnet simi-
larity. In In Proceedings of the 2nd International Joint
Conference on Natural Language Processing, Jeju Is-
land, South Korea, 1113, pages 945–956.

Su Nam Kim and Timothy Baldwin. 2007. Detect-
ing compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of the

7th Meeting of the Pacific Association for Computa-
tional Linguistics, PACLING ’07, pages 40–48.

Su Nam Kim and Timothy Baldwin. 2008. An unsu-
pervised approach to interpreting noun compounds. In
Natural Language Processing and Knowledge Engi-
neering, 2008. NLP-KE ’08. International Conference
on, pages 1–7.

S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983.
Optimization by simulated annealing. Science,
220(4598):671–680.

Ioannis Korkontzelos and Suresh Manandhar. 2009. De-
tecting compositionality in multi-word expressions.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, ACLShort ’09, pages 65–68, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ’10, pages 636–644, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Mark Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ’95, pages 47–54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Diana McCarthy, Sriram Venkatapathy, and Aravind
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369–379, Prague, Czech Republic. As-
sociation for Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.

Diarmuid Ó Séaghdha. 2007. Annotating and learning
compound noun semantics. In Proceedings of the 45th
Annual Meeting of the ACL: Student Research Work-
shop, ACL ’07, pages 73–78, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Ted Pedersen. 2011. Identifying collocations to mea-
sure compositionality: shared task system description.
In Proceedings of the Workshop on Distributional Se-
mantics and Compositionality, DiSCo ’11, pages 33–
37, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-

140



pound nouns. In Proceedings of The 5th Interna-
tional Joint Conference on Natural Language Process-
ing 2011 (IJCNLP 2011), Chiang Mai, Thailand.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In In Proc.
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002, pages 1–15.

Karen Spärck Jones. 1985. Compound noun interpre-
tation problems. In Frank Fallside and William A.
Woods, editors, Computer speech processing, pages
363–381. Prentice Hall International (UK) Ltd., Hert-
fordshire, UK, UK.

Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’10, pages 678–687, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ’10, pages 1263–1271, Stroudsburg,
PA, USA. Association for Computational Linguistics.

141


