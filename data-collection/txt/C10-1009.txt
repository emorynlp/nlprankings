Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71–79,

Beijing, August 2010

Fluency Constraints for Minimum Bayes-Risk Decoding
of Statistical Machine Translation Lattices

Graeme Blackwood and Adrià de Gispert and William Byrne
Machine Intelligence Laboratory, Department of Engineering, Cambridge University

{gwb24|ad465|wjb31}@cam.ac.uk

Abstract

A novel and robust approach to improv-
ing statistical machine translation fluency
is developed within a minimum Bayes-
risk decoding framework. By segment-
ing translation lattices according to con-
fidence measures over the maximum like-
lihood translation hypothesis we are able
to focus on regions with potential transla-
tion errors. Hypothesis space constraints
based on monolingual coverage are ap-
plied to the low confidence regions to im-
prove overall translation fluency.

1 Introduction and Motivation

Translation quality is often described in terms of
fluency and adequacy. Fluency reflects the ‘na-
tiveness’ of the translation while adequacy indi-
cates how well a translation captures the meaning
of the original text (Ma and Cieri, 2006).

From a purely utilitarian view, adequacy should
be more important than fluency. But fluency and
adequacy are subjective and not easy to tease apart
(Callison-Burch et al., 2009; Vilar et al., 2007).
There is a human tendency to rate less fluent trans-
lations as less adequate. One explanation is that
errors in grammar cause readers to be more crit-
ical. A related phenomenon is that the nature of
translation errors changes as fluency improves so
that any errors in fluent translations must be rel-
atively subtle. It is therefore not enough to fo-
cus solely on adequacy. SMT systems must also
be fluent if they are to be accepted and trusted.
It is possible that the reliance on automatic met-
rics may have led SMT researchers to pay insuffi-
cient attention to fluency: BLEU (Papineni et al.,
2002), TER (Snover et al., 2006), and METEOR
(Lavie and Denkowski, 2009) show broad corre-
lation with human rankings of MT quality, but are

incapable of fine distinctions between fluency and
adequacy.

There is concern that the fluency of current
SMT is inadequate (Knight, 2007b). SMT is ro-
bust, in that a translation is nearly always pro-
duced. But unlike translators who should be
skilled in at least one of the languages, SMT sys-
tems are limited in both source and target lan-
guage competence. Fluency and accuracy there-
fore tend to suffer together as translation quality
degrades. This should not be the case. Ideally, an
SMT system should never be any less fluent than
the best stochastic text generation system avail-
able in the target language (Oberlander and Brew,
2000). What is needed is a good way to enhance
the fluency of SMT hypotheses.

The maximum likelihood (ML) formulation
(Brown et al., 1990) of translation of source lan-
guage sentence F to target language sentence Ê

Ê = argmax
E

P (F |E)P (E) (1)

makes it clear why improving SMT fluency is a
difficult modelling problem. The language model
P (E), the closest thing to a ‘fluency component’
in the original formulation, only affects candidates
likely under the translation model P (F |E). Given
the weakness of current translation models this is
a severe limitation. It often happens that SMT sys-
tems assign P (F |Ē) = 0 to a correct reference
translation Ē of F (see the discussion in Section
9). The problem is that in ML decoding the lan-
guage model can only encourage the production
of fluent translations; it cannot easily enforce con-
straints on fluency or introduce new hypotheses.

In Hiero (Chiang, 2007) and syntax-based SMT
(Knight and Graehl, 2005; Knight, 2007a), the
primary role of syntax is to drive the translation
process. Translations produced by these systems
respect the syntax of their translation models, but

71



this does not force them to be grammatical in the
way that a typical human sentence is grammati-
cal; they produce many translations which are not
fluent. The problem is robustness. Generating
fluent translations demands a tightly constraining
target language grammar but such a grammar is at
odds with broad-coverage parsing needed for ro-
bust translation.

We have described two problems in transla-
tion fluency: (1) SMT may fail to generate flu-
ent hypotheses and there is no simple way to in-
troduce them into the search; (2) SMT produces
many translations which are not fluent but enforc-
ing constraints to improve fluency can hurt robust-
ness. Both problems are rooted in the ML decod-
ing framework in which robustness and fluency
are conflicting objectives.

We propose a novel framework to improve the
fluency of any SMT system, whether syntactic or
phrase-based. We will perform Minimum Bayes-
risk search (Kumar and Byrne, 2004) over a space
of fluent hypotheses H:

ÊMBR = argmin
E′∈H

∑

E∈E
L(E,E′)P (E|F ) (2)

In this approach the MBR evidence space E is
generated by an SMT system as a k-best list or lat-
tice. The system runs in its best possible config-
uration, ensuring both translation robustness and
good baselines. Rather than decoding in the out-
put of the baseline SMT system, translations will
be sought among a collection of fluent sentences
that are close to the top SMT hypotheses as deter-
mined by the loss function L(E,E′).

Decoupling the MBR hypothesis space from
first-pass translation offers great flexibility. Hy-
potheses in H may be arbitrarily constrained ac-
cording to lexical, syntactic, semantic, or other
considerations, with no effect on translation ro-
bustness. This is because constraints on fluency
do not affect the production of the evidence space
by the baseline system. Robustness and fluency
are no longer conflicting objectives. This frame-
work also allows the MBR hypothesis space to be
augmented with hypotheses produced by an NLG
system, although this is beyond the scope of the
present paper.

This paper focuses on searching out fluent

strings amongst the vast number of hypotheses en-
coded in SMT lattices. Oracle BLEU scores com-
puted over k-best lists (Och et al., 2004) show
that many high quality hypotheses are produced
by first-pass SMT decoding. We propose reducing
the difficulty of enhancing the fluency of complete
hypotheses by first identifying regions of high-
confidence in the ML translations and using these
to guide the fluency refinement process. This has
two advantages: (1) we keep portions of the base-
line hypotheses that we trust and search for alter-
natives elsewhere, and (2) the task is made much
easier since the fluency of sentence fragments can
be refined in context.

In what follows, we use posterior probabilities
over SMT lattices to identify useful subsequences
in the ML translations (Sections 2 & 3). These
subsequences drive the segmentation and transfor-
mation of lattices into smaller subproblems (Sec-
tions 4 & 5). Subproblems are mined for fluent
strings (Section 6), resulting in improved transla-
tion fluency (Sections 7 & 8). Our results show
that, when guided by the careful selection of sub-
problems, fluency can be improved with no real
degradation of the BLEU score.

2 Lattice MBR Decoding
The formulation of the MBR decoder in Equation
(2) separates the hypothesis space from the evi-
dence space. We apply the linearised lattice MBR
decision rule (Tromble et al., 2008)

ÊLMBR = argmax
E′∈H

{
θ0|E′|+

∑

u∈N
θu#u(E

′)p(u|E)
}
,

(3)
whereH is the hypothesis space, E is the evidence
space, N is the set of all n-grams in H (typically,
n = 1 . . . 4), and θ are constants estimated on
held-out data. The quantity p(u|E) is the path pos-
terior probability of n-gram u

p(u|E) =
∑

E∈Eu
P (E|F ), (4)

where Eu = {E ∈ E : #u(E) > 0} is the sub-
set of paths containing n-gram u at least once.
The path posterior probabilities p(u|E) of Equa-
tion (4) can be efficiently calculated (Blackwood
et al., 2010) using general purpose WFST opera-
tions (Mohri et al., 2002).

72



0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Posterior probability threshold β

A
v
e
r
a
g
e
 
p
e
r
−
s
e
n
t
e
n
c
e
 
p
r
e
c
i
s
i
o
n
 
p
n
,

β

 

 

1−gram

2−gram

3−gram

4−gram

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0

5

10

15

20

25

30

Posterior probability threshold β

A
v
e
r
a
g
e
 
p
e
r
−
s
e
n
t
e
n
c
e
 
n
−
g
r
a
m
 
c
o
u
n
t
s

 

 

1−grams

2−grams

3−grams

4−grams

Figure 1: Average n-gram precisions (left) and counts (right) for 2075 sentences of NIST
Arabic→English ML translations at a range of posterior probability thresholds 0 ≤ β ≤ 1. The left
plot shows at β = 0 the n-gram precisions used in the BLEU score of the ML baseline system.

3 Posterior Probability Confidence
Measures

In the formulation of Equations (3) and (4) the
path posterior n-gram probabilities play a crucial
role. MBR decoding under the linear approxima-
tion to BLEU is driven mainly by the presence
of high posterior n-grams in the lattice; the low
posterior n-grams contribute relatively little to the
MBR decision criterion. Here we investigate the
predictive power of these statistics. We will show
that the n-gram posterior is a good predictor as to
whether or not an n-gram is to be found in a set of
reference translations.

Let Nn denote the set of n-grams of order n
in the ML hypothesis Ê, and let Rn denote the
set of n-grams of order n in the union of the ref-
erences. For confidence threshold β, let Nn,β =
{u∈Nn : p(u|E) ≥ β} denote the n-grams inNn
with posterior probability greater than or equal to
β, where p(u|E) is computed using Equation (4).
This is equivalent to identifying all substrings of
length n in the translation hypotheses for which
the system assigns a posterior probability of β or
higher. The precision at order n for threshold β is
the proportion of n-grams in Nn,β also present in
the references:

Pn,β =
|Rn ∩ Nn,β|
|Nn,β|

(5)

The left plot in Figure 1 shows average per-
sentence n-gram precisions Pn,β at orders 1. . .4
for an Arabic→English translation task at a range

of thresholds 0 ≤ β ≤ 1. Sentence start and end
tokens are ignored when computing unigram pre-
cisions. We note that precision at all orders im-
proves as the threshold β increases. This confirms
that these intrinsic measures of translation confi-
dence have strong predictive power.

The right-hand side of the figure shows the av-
erage number of n-grams per sentence for the
same range of β. We see that for high β, there are
few n-grams with p(u|E) ≥ β; this is as expected.
However, even at a high threshold of β = 0.9
there are still on average three 4-grams per sen-
tence with posterior probabilities that exceed β.
Even at this very high confidence level, high pos-
terior n-grams occur frequently enough that we
can expect them to be useful.

These precision results motivate our use of path
posterior n-gram probabilities as a confidence
measure. We assign confidence p(Êji |E) to sub-
sequences Êi . . . Êj of the ML hypothesis.

Prior work focuses on word-level confidence
extracted from k-best lists and lattices (Ueffing
and Ney, 2007), while Zens and Ney (2006)
rescore k-best lists with n-gram posterior proba-
bilities. Similar experiments with a slightly dif-
ferent motivation are reported by DeNero et al.
(2009); they show that expected n-gram counts in
a lattice can be used to predict which n-grams ap-
pear in the references.

4 Lattice Segmentation
We have shown that current SMT systems, al-
though flawed, can identify with confidence par-

73



the newspaper “ constitution ” quoted brigadier abdullah krishan , the chief of police in karak governorate ( 521 km
south @-@ west of amman ) as saying that the seizure took place after police received information that there were
attempts by the group to sell for more than $ 100 thousand dollars , the police rushed to the arrest in possession .

0 1
1/313.69

2

3

3

7/4.3574

4

23/0.068359

5
3580

6
3

7

23/0.0097656

8
1300

9
3/4.6934

104/5.7402

11

5/4.7529

12

23

38

1300/2.9102

13
23

14

3580/2.248

15

1300

18

3580

16

4/4.0488

17
23/2.5654

1300

19

23/1.2598

3/0.66016

2023

23/1.5156

32

3

231300

21

3/2.8027

225/6.2256

4423

24
1300

23

4/2.5117
25

23

30
23

26
23

29

3580

27

3/2.1895

2810/3.9199

1300

1300
3/1.1123

1300

23

33

4/4.4336

31

1300

3

4/1.6416

23
3580

3/2.1895

3

36
5614

34
3580

353580

37
5614

4/1.8594

23

39

23

41
5614

425614

43
12316

55
12316

575614

56
5614

69
12316

49
12316

50

12/2.9502

52

7359

53

12272/1.9209

51

42/1.04

45
12/2.9502

46

42/1.3105

47

7359

48

12272/1.9209

54

8615/6.5205

58

12316

40

12/2.9502

42/1.3105

7359

12272/1.9209

5942

60

7359

618615/3.6523

62

12272/0.66504

112

58332/5.0586

63

4/0.5752

11036/6.5117

64

309/6.1533

65

755/6.3652

66

58332

68

58332

67

4/0.12109

7359

12272/1.9209
42/1.04

707359

7142

58332

72

4/0.5752

58332

1234/0.12109

7359

12272/0.66504

12316

58332/3.2891

74
7359

758615/3.9463

76

12272/0.91699

78

58332

774/0.69727

79

12

58332

80
4/0.52637

4

58332/4.6484

132

3

81

515/4.1318

82

755/2.9941

58332

83

515

84
5

4

85

3/4.2812
86

309/3.6953
87

515/3.8184

88

755/3.9072

4

755/3.9072
89

3/4.2812

90309/3.6953

91

515/3.8184

58332/3.1895

755/4.7812

3

515/3.9678

58332

7359

12272/0.91699

3

755/2.9941

3

94

12

10565

12272/1.9209

42/1.3105

73

7359

4/0.5752

58332

9358332

92
4/0.69727

58332/0.086914

12

58332

4/0.52637

4

755/3.9072

3/4.2812

309/3.6953

96515/3.8184

58332/4.7764

3

755/3.123

95

515/4.2607

97
10565

58332/3.1895

3
515/3.9678

139
755

98
5

99

5

100

5

101

309

755

102

515/1.4756

103

515

104

755/1.0029

309/2.4023

105

5

106

5

755

755/1.0029

107

515

108

5

4

755/3.9072

515/3.8184 109

309/3.6953

3

755/3.123

111

5

5

58332

4/1.5498

113

3

114

309

115

3

116

95850/0.21289
138

9

117
5

5

4/2.9111

95850/0.44141

118

95850309

119
3/0.63672

120
95850/1.582

121

309

5

95850/0.44141

309

3/0.63672

515

122
3

95850/0.75391

124
309

9

95850/1.751

95850

125

8716

1263/0.24902

127

95850

309/0.16016

128

3

129
8716

95850/0.39355

130

309

1311008

9

95850/0.91602

309

58332/0.84375

95850/0.12402

133

9

134

15/2.5664

153

16

309

95850/0.9082

16

95850/0.12402

9

136309

95850/0.074219

141

3

14213267

157
13267

135
8716

1505

143

16/6.207

137
8716

2035/4.6826

15/2.5664

16

144

13267/5.418

16

140
5

146
8716

147
5108

148
95850

13267

149
5108

145

309

9

95850/6.04

5

168

8/2.209

151

368

154
368

1703

171

12
176

1001/1.9658

15/1.5693

16

13267/3.3818

12 159
1001

160

5

152 156
5 1627375

163
5108

155
5

161

7375/4.4355

166
7375

181

15

16715

8/4.0586

14226/5.498

169

368

158 164
12

172
1001

177

5

165
7375/5.1436

180
7375

173
15

174
15

183
20

175

20

12

1001/2.3379

5

4/0.94141 20

4/0.9873

20 179
1976

368

1861976 189

13

184
3/4.5869

16/6.084

178
15

4/0.9873

20

1821976/5.3818 185
13

192
3

1874854

188
3

13

190714

191
4854

196
336

195714

193

31/1.3125

197

57/1.459

198
4854

205
185

204336

199

4/5.2324

200

185/1.6523

201626

202
236

714

31/1.3125

57/1.459

194

63/6.7617

206
185

207

3

20813/3.3818

209
24/1.8857

626/0.54102

210

90/0.18164

211

309/1.875

212505

203

626

214

505

215
185

217309

216

3/0.79297

213336

218

309

220
309

219

90/3.8145

221
3

222

505

223
31

225505

224

90/1.124

226

90

227
185

288

90

309

3/0.79297

228

24/5.6748

626/4.7695

229

90/4.7461

233

505

232

90/4.0713

231

309

230

90/3.8027

505

505

90/2.3574

234

57

235

90

236

90

237505

239
90

23831

240

5/4.6641
2418

242
3

2978

243
505

24431

505

90/3.7266

2455

8/0.95117

266

57/1.7461

247
90

246
31

248
50

5

8

5/2.1689

8

249

3/3.0986

250

505

251
13

2523

2533

255

309

254
90/4.8203

256
90

257505

258
3

278

50

259
505

260
13

261505

309 262
13

263

83

264309

265
309

505

57

267

8

8

268
5/2.1689

269309

270

13

271

83

272

3029/2.8291

273
8

27483

275

29

337

63/1.9453

276
13

277
13

279
3

308
3

2803

281
4713

282
83 63

283
29/0.66406

28429

3

29

2853029

356
3029

28683

83

2873029/2.8857

505

289
309

290
309

291
13

63/0.8457

29
292

3029

293

194

36217

29

63/1.5664

295
29

296
63/0.29004

298

13

299
13

300
83

301
17

302

87

303

3

30410/0.21484

306

194

307
194

309
83

31083 29

63/1.7354

3

305140/5.4287

312

10/0.21484

313

17

294
3

10/0.65234

140/6.0684

314

140

315

1188/5.5186

316

140

317

1188/3.1943

318

5

319
87

320
87

321
309

63

29/1.043

63

140

1188/2.3926

323

3

32410/0.125

326

8

325
5/0.27246

327

11/2.5967

328

1188/0.80762

329140

330
5

331

11/5.2559

332

1188/2.5459

333

140

334

1188

335
17

336
17

311
13

322
83 29/0.15527

63

400

140

1188/2.4033

338140

341

3/2.9023

342

1188/6.0127

343

3996

339
1188

340

2710/4.4717

401

3

344

5/4.9404

345

8/1.1172

346

11

347

3848/3.9785

348

8

2710/4.2734

349

1188

350

3

3848/3.3076

402

11

351

8/0.44629

8

352

11/2.667

354

8

353
4/0.53027

3

355
10/0.125

3

10/0.20996

11/2.8037

357

5

358

8/2.1064

359

1188/1.2041

360

5

361
1188/2.4062

4/1.6289

365

8

363

3/5.5244

364

5/3.4756

366

11/1.3096

367

1579/6.1328

368

3848/3.5664

8
369

5/1.5518

370

11/1.7031

3711579

3848 374

11

372

3/3.1328

373

9/3.7803

375

81/4.0898

376

1188/2.4893

377

1704/4.7871

378

2710/5.6816

379

1579

1579

3848/0.19922

3/0.019531

380

3996

3996/4.1592

382
1579

381

3/2.3301

383

1649/4.8389

384

3848/1.6953

385

11

3996

3512/3.4209

3996/2.5566

4/3.2705

388

8

386
3/3.7451

387

5/2.584

389

11/3.0176

390

1579/1.8506

391

1649/5.4414

392

3848/0.073242

393

1579

3/2.3301

395

1579

3963848/1.6953

3943996

1579/0.65039

397

3

399

3996

398

8

140

17

1188

11

8/0.44629

1579/6.5

3848/6.1123

3996

406

3/5.6807

403

1579

1579/1.1162

3848/1.3164

404

3

405

1649/2.5039

3996/4.21391649/4.8936

407

3

408

1579/0.054688

409

3848/1.75

410

11

411

11

3

1579

412

5

413

9/2.7051

414

11/2.5723

415

1188/3.8848

416

1704/4.084

81

421

38/4.04

422

143/4.9678

423

188/6.5078

417

1188

4182710/0.94434

419
569

420

775/0.1084

714

163

775/2.3652

425

11

424
9/0.68457

426
20/4.0977

427

130/1.0742

4281704/0.60352

429

3431/4.0811

430
8/0.40527

431

21

432

9

4331704/1.2285

9/3.7803

438

11

439

1704/4.7871

775/5.4775

443

11441
5/4.5166

442

9/1.3047

444
20/5.0928

445

130/3.2822

446

1704/1.4395

440

1579

447

1704

81/3.7598

449

11

448

9/2.5488

450

130/3.375

451

1704/1.54

452

8246/3.0615

81

3848/6.1123

3996

3/5.6807

453

1579

1579/2.0078

3848/2.208

454

3

3/0.21191 3848/1.9619

455

1579

775/4.3457

5/3.6152

11

456
9/1.7539

457

1704/1.4248

458

1704

81/5.1875

11

459

9/4.9961

460
1704/4.542

461

5

11

462
1579

463
3996

9/3.7803

81/5.3926

1704/4.7871

20/6.6523

464

11

1188

3996

8/1.1172

11

1704/1.0518

465

11

3848/2.8662

4661579

1704

467

1579

468

1579

5/4.5166

9/1.3047

20/5.0928

1704/1.4395

469

11

470

130/3.2822

11

9/2.5488

1704/1.54

471

130/3.375

472

8246/3.0615

81

473

38/3.3545

474

143/3.3359

81

475

38/4.7412

476

143/4.5361

81/0.95898

542

1188

477

2710/2.8506

478

569/0.63574

479

775

81

38/1.4326

481

1704

480

9/0.081055

21

775/1.7617

9/0.25488

20/3.3711

1704

482

11/1.3154

483

130/0.80176

11

1704/2.5254

484

9/0.31934

485
11

48611

487

561

488

74

489

188/1.9863

490

3250/0.032227

491

3745/1.8066

492

4816/0.54199

493

143

494
143

81

38/1.3213

497

3/3.3994

496

775

495

569/0.53906

498

3431

81

499
3/1.8145

500
38/1.2832

501

143/0.49512

502

3745/0.70996

503

8

504

21/0.38281

505

8

81

3/1.2666

50638/0.38281

81

507

3/1.6318

775/0.014648

508

569

8

434
5

435
9/4.2754

436

11/3.8105

437

1188/3.6299

81/2.2656

1188

2710/2.8506

775

81/0.10449

509

38

130/0.4707

511

1704

510
9/0.081055

81

38/4.04

512

143/4.9678

21

513

5

5149/1.4326

515

11/0.9668

516

1704/2.2139

81

38/2.5791

3/5.5254

519

143/3.0596

775/0.79395

81

517

1704/0.10059

775/1.0488

518

569

520
3431

81/1.75

522

38

521

3/2.3906

523
143/0.81934

524
8

525

21/1.2314

526

8

81

38/3.6436

528

775

527

569/0.38184

81/0.98145

38

3/0.7793

21/1.4082

529
8

81

38/1.166

530

3/0.90234

531

143/0.37305

532

561/0.8584

5/1.2695

9/0.99316

11

533

1704/1.4629

3848/3.7578

534

1579

536
11

535

9/1.3047
537

1704/1.7061

569

775/0.43359

539

21

5388/0.4082

21

5408/0.33887

775

569/0.38184

21/0.69336

541
8

1188

11

9/1.3047

130/3.2822

1704/1.4395

11

1704/1.54

5

54311

81

38/4.04

188/6.5078

143/4.9678

38

5/2.417

9/1.4287

544

11

545

1704/2.7559

5

11/0.92969

20/4.6992

5

9/1.4326

11/0.9668

1704/2.2139

546

130/5.1309

81

38/2.5791

143/3.0596

81/1.75

38

143/0.81934

38

81

143/0.37305

547

561

74/0.45117

3250

4816/0.50977

548

561

3250

4816/0.50977

549

74/0.71582

775/2.4541

20/3.7178

1704/0.69238

3431/3.6172

550

9

551

11/0.39551

552

130/0.37207

553

9

55411

555

11

556

8

775

81

38/0.22363

3/2.1543

81

38/1.2832

143/0.49512

3745/0.70996

557
3/0.82227

775/0.99414

569

81/0.38965

558
10

81

559

38/1.5928

560

5/2.4922

561

3745

562

188

563

1191

565

1191

564

143/0.625

566143

567

1191/0.20703

1191

568

1191

571
11

570
11

572

561

8

81/0.56348

573

561

574

561

188/1.9551

3250

3745/1.7744

4816/0.50977

575143

81
577

38/0.38281

576

3/2.0371

81

578

3/1.6318

81/0.09082

38

579

561

580

561

581

11

582

561

21/0.38281

8

775

569/0.53906

74

3250/0.032227

4816/0.54199

775/2.917
81

583

11/3.3545

584
569/3.9336

585

1704/2.4365

775

586

569/0.63574

81/0.10449

3/3.4795

38

21/0.51562

587

8

5888

589

11

188/2.8662

3250

3745/0.82031

4816/1.4219

590

74/0.55664

591
8

593561

592
561

3250

3745/0.82031

4816/1.4219

81/1.4834

3/3.4424

38

81

594

3/0.89746

38

596
11

595

11

81/0.71484

38

597

561

3250

4816/0.50977

3745

598

5/0.037109

599
4816/0.43164

21

8/0.6377

5/2.0967

9/1.4287

11

1704/2.7559

81

38/2.8457

775/1.0488

600
569

21/0.96484

601
8

81

3/1.5

81

3/2.3506

38/0.38281

81

38/0.15137

81

38/0.38281

81

38/4.5273

81

38/1.9385

21

8/0.6377

38

602

3745

3745

188

603

81/0.34082

775/0.014648

610

569

81/0.25586

10

81

3/2.1406

38/1.5928

81

38/0.38281

81/0.56348

611

561

612

2339

613

561

614
188

615

143

616
1191

617

3356

619
3356

618
1191

620
1191

621

3356

622

3356

5691191

623

3356

81/0.28613

10

81

3/3.3311

38/1.5928

5

624
5

625

4816/0.8916

3745

5/1.5254

626
1191

628
561

627
561

629
561

630

3745

631

5

10

5/4.0293

3745

81/0.56543

38

632

11

21/1.7168

633

8

634

11

81/1.1543

38

81/1.4834

38

81/0.25586

635

10

636
38/3.7988

188

637

3250/1.4678 638
3745/0.75

81/0.84473

38

5

639

3745

640

561

81

10/0.13965

81

3/1.7939

38/1.5928

5

4816/0.048828

641

188

642

1191

643
11

81/1.2168

38

775

604569/0.53906

81

38/1.6504

3/3.9385

81

38/1.2832

3745/0.70996

606

143/0.49512

605

3/1.5205

607

561/3.8506

608

3250/3.5908

609

4593/3.8887

1191/3.2842

143

163

644

11

188/1.9551

3745/1.7744

646

3250

647

4816/0.50977

81/0.56348

645

561

3745

5/0.037109

648143

5

649

11

5

4816/0.8916

650
9/1.6494

737

5

5

3745/0.52344

652

1191

651
143/2.3555

653

1191

654

3356

655

1032

656

3356

657

1032

658
3356

659

1032

660
1032

661
1032

662

2295/6.8135

663

188

1191/0.25391

664

143

6653356

5

6664816/1.8076

5/3.1465

3745

5

667

3745/1.3447

1191/3.7744

143

668

188

81

10/1.3828

81/1.2715

38

81/0.25586

669

10

670

2339

671

561

1191/0.5625
672

143

143/1.8193

1191

1191/2.8828

143

4816/2.0596

5

3745/1.3447

1191

6733356

81/0.25586

674

10

81/0.28613

10

38/3.8291

3745/2.3125

5

4816/0.8916

9/1.6494

1191

143/0.625

675

4376/3.2324

1191

676

143/2.5205

677
4376/2.5254

678

1191

81/0.52148

10

679
4593

680

81/2.3428

681
116/4.3838

682

188

6843356

6831191

6853356

686

1032

687

4

689

4

688

3/2.4316

690

71/1.3447

692

4

691

3/1.8115

693

4

3/1.8115

694

4

4

71/3.583

3/1.8115

695

4

696

6/6.2324

697

7/6.3906

698
9/5.1299

699

24/3.6904

700

77/4.2246

701

119/3.668

702

1032

143/1.918

1191

7031191

704

1032

143

1191/0.83887

143

1191

143/1.918

705

2339

5

706
9/4.5615

740

3745/1.1836

5

3745/0.57617

707
1191

7081032

709

2339

710

1032

7111191

712
1032

7133356

715

5

7173356

7181032

3/3.6035

71/2.5166

4

4

3/1.8115

7193

720

309/4.5137

721

9331/2.3145

309/2.1367

723

9331

724

22194/2.8037

722
309

7259331

3

9331/2.3145

726

309

309/2.1367

9331

22194/2.8037

727
3/2.9238

3

71/4.9521

524/5.0078

119/4.9316

3

309/4.5137

9331/2.3145

22194/5.1182

728

24/5.543

48 729
9331

730

3

731

9331

732

22194/1.9717

22194/1.3242

733

9331

734

3

735
3

736

3356

3/1.8115

4

5

3745/1.5127

738

4593

1191/1.6035

143

739
3356

4

5

3745/1.4502

741

4

742

3356 3/0.46582

743

4

744

1032

1191

143/0.66602

716

163

1191

143/1.6924

8

1191

143/1.5361
3/2.4316

71/1.3447

4

4

71/3.583

3/1.8115

119/3.668

745

24/3.6904
746

77/4.2246

747

309

749

9331

748

1899/1.582

750

8
751

309/4.4062

752
9331

8

309/4.4062

753
3/4.7422

754

8

755
8

757

9331

756

1899/4.4609

758309

22194/0.54492

759

9331

760

8

761

309

762

8

3/0.42383

8

763
8

764

309

765

309

71/1.3447

8044

766

5

4

854
3

767
9331/0.080078

4

524/0.36816

3

9331/0.080078

4

9331

9331

1899/4.3027

803
9331

768
2925/6.7461

770

8

769
2925

309/1.9824

771

3

772

8

773
8

309

309

309/2.1914

7743

776

8

775
2925

777

9331

778

8

779

3

780

9331

309/0.55273

3

309/0.96387

3

781
8/1.3115

782
21215

9331

9331

783

4376

784
309

785

8

787
8

786

1716

788
8

1716/2.3975 789

3

790

2318/1.6875

791

309

792

2318

795

3

793
309

794
8

1716/2.4541

3

796
2318/1.7441

797

8

309/1.1377

3

798
309

799

8

800
2318

801
8

802

1032

3

805

9

807
2318

809
2318

808

1716/0.81934

810
7/0.6084

811

161

8/3.6729

8121716

813

11070/2.5137

161

11070/3.251

816
2318

815

1716/0.81934

814
124/3.165

817

5763/4.4824

818

7235/5.5088

819

11744/5.4629

1716

11070/2.5137

820

2318

7/0.6084

823161

821

3/4.7285

879

4/3.4551

822

9/0.92676

3

2318/1.7441

1716

3

2318/1.7441

824

7

826

2318

825

3/2.9131

827

4

9331

828

3005

1716/2.4541

3

2318/1.7441

806

45/5.8838

829

2318

161

830

7/0.77637

833

9

832

7/0.30078

831

9

57/3.4434

834

9

835

109/2.0273

8363005/0.50586

837

9

9

7/4.2412

838

161/4.2617

839

9

161

7/0.98242

845

9/0.92676

9

843

7/0.30078

842

4/3.3135

4/5.543

9 841

7/4.8145

8405

844
9

9

9/0.125

846
7

9

848
4/3.5527

8497/2.3818

880

3005

881
9

45/2.6279

847

3005

850
21215

9/0.92676

852

7/0.6084

853

161

851
2318

3

5

8846/0.25

7/0.55762

9

9

45/4.8506

3005

45/2.6006

3005

57/2.3096

9

855

3/1.5703

856

5/0.34961

857

3005/0.019531

45

858

3005/0.84277
859

63

5/2.6631

6

45/0.20508

860

3005
9

45/1.249

3005

861

45

45/0.48828

8629

21/3.7686

45/5.5586

57/2.3096

9

109/4.7979

3/3.0703

5/1.8926

3005/0.019531

864

63/5.4697

7/3.0781

863

9

865

3005

3005

9

6

5/0.94043

6

7/0.5791

9

3005/2.1006

866

9

867
99

868

7/0.30078

9

109/2.0273

9

869

7/2.3818

882
2

870
3005

3

5/3.1211

6

6

883
5/0.24609

871

9

5/0.89355

6

872

2318

45

873

3005/0.84277

45

874

3005/2.4316
875

9

5/0.99414
6

45

45

3005/1.2275

45
3005/1.5234

9

3005/0.019531

9

5/2.0693

6

161/2.6025
6

876
3005

877

7

5/0.24609

6

5

878

3005

5/0.28711

6

3/2.0576

57/1.9053

9

5

6/0.25098

H1 H2 H3 H4 H5 H6 H7 H8 H9

0 1
1

23

3

7

4
23

5

3580

6
3

7

23 8

1300

9

3

10

4

115

12
23

13

1300

23

14
3580

25
1300

15

4

1623

17

3580

1300

18
23

3

19
23

23

203

21
3

22

5 23

1300 24

23

23

4

26
23

27
23

28

3

2910

30

3580

31

23

1300

1300

3

1300

321300

3

23

33

4
34

5614

3 35
3580

363580

37
5614

38
5614

23

4

3923

40
12316

3

3580

41

5614

425614

43

12316

44
12316

45

5614

46

12

47

42

48

7359

49

12272

50

12316

51
12316

52

12

53

42

64
7359

12272

12

42

7359

12272

548615

55

12316

56
42

7359

12272

57

8615
63

58332

58332

58

4

36

58332

4

42

7359

12272

7359

59
42

7359

12272

58332

6012

7359

12272

42

12272

7359

58332

61

8615 62

12

58332

7359

12272

10565

58332
12

10565

0 1
4

2
3

3
755

4
5

5
309

6
9

0

13

2
95850

38716

52035

8716

45 95850
0 1

16
2

13267
3

5108
4

368
5

12
6

1001
7

5
8

7375
9

15
10

20
11

1976
12

13
13

3
14

4854

0 1
31

2
57

3

714

44

5

185

6236

7
336

10
185

3

8336

185

9185

3

0 1
309

2
505

3
90

4
13

0

183

2

3029

329

63

4
29

53029

6

194

717

887

9

3

1010

11

140

1217

13

140

14

1188

15140

16

1188

17

5

18

3

1910

205

21

8

2211

23

1188

24

140

255

2611

27

1188

28140

29

1188

30
140

1188

31

140

32

1188

332710

343

35

1188

36

3996

373

38
5

39

8

40

11

41

3848

42

8

2710

43

1188

443

3848

45

8

46

11

8

47
11

48
4

49

8

11

505

51

8

52

1188

535

54

1188

4

55

3

56
5

57
8

58

11

59

1579

603848

8

61
5

62

11

631579

3848

64

3

65

9

66

11

67

81

68

1188

69

1704

70

2710

711579

1579

3848

3

72

3996

3996

73

3

74

1579

751649

76

3848

77

11

3996

3996

3512

4

78

3

79

5

808

81

11
82

1579

83

1649

84

3848

851579

86

3996

3

87

1579

883848

1579

89

3

90

8

91

3996

1188

3996

8

11

1188

8

11

92

1579

1579

3848

93

3

94
1649

1579

3848

3996

95

3

3996

1649

96

3

97

1579

98

3848

99

11

100

11

3

1579

101

5

102

9

103

11

1041188

105

1704

1061188

1072710

108569

109

775

81

110

38

111

143

112

188

113

163775

9

114

11
115

20

116

130

117

1704

118

3431

1198

120

21

121

9

122

1704

5

123

9

124

11

125
1188

9

126

11

127

1704

128

1579

775

129
5

130

9

131

11

132

20

133

130
134

1704

1351704

81

9

136

11

137

130

138

1704

1398246

81

140

1579

1579

3848

3

3848

3996

3

3

3848

141

1579

775

5

11
9

1704

1704

81

11

1704

142

9

1435

11

9

11

130

1704

11

1704

144

1579

3996

9

81

1704

20

145

11

1704

146

11

3848

147

1579

1704

148
1579

149

1579

5

9

20

1704

150

11

151

130

9

11

1704

152

130

153

8246

81

154

38

155

143

81

156

38

157

143

81

158

1188

159

2710

569

160

775

81

38

1619

162

1704

21

775

9

20

1704

163

11

164

130

11

1704

165

9

166

11

167

11

168

561

169

74

170
188

171

3250

172

3745

173

4816

143

174

143

81

38

175
3

176

3431

81

177

3

178

38

179
143

180

3745

181

8

182

21

8

81

38

3

81

183

3

775

184

569

8

775

81

185

38

130

9

186

1704

81

38

143

21

187

5
188

9

189

11

190
1704

775

81

191
1704

775

192

569

81

38

3

193

143

3431

81

38

194

3

195

143

8

196

21

8

81

38

81

38

3

21

8

81

38

197

3

198

143

199

561

5

9

11

1704

11

9

1704

775

569

1188

5

81

38

188

143

38

5

9

11

200

1704

5

11

20

130

5

9

11

1704

81

38

143

81

38

143

38

81

143

201

561

74

3250

4816

202

561

3250

4816

203

74

775

20

1704
3431

11

2049

205
130

9

11

775

206

8

81

38

3

81

38

143

3745

207

3

775

569

81

208

10

81

38

209

5

2103745

188

211
1191

212

143

213

1191

1191

214
143

1191

2151191

216

561

8

81

217

561

218

561

188

3250

3745

4816

143

81

3

21938

81

3

220

561

22111

222

561

21

8

775

81

569

11

1704

775

223
569

81

3

38

8

21

8

22411

188

3250

3745

4816

225

74

226

561

3250

3745

4816

81

227

3

228

561

3250

4816

5

4816

3745

8

21

229
3745

3745

188

230

81

775

231
569

81

3250

38

3745

561

232

3

233

143

234

4593

81

38

81

235

561

236

2339

237

188

238

143

266

3356

239

1191

240

3356

2411191

2423356

5

4816

243
5

5

3745

244

561

5

10

5

3745

245

11

81

38

246

10

3250

3745

188

5

247

561

5

4816

1191

143

163

248

11

81

249

561

188

3745

250

3250

251

4816

5

48165

2529

253

5 1191

143

254

1191

267

1032

268

3356

255

1032

3356

256

1032

2572295

188

5

3745

81

25810

259

2339

4816

5

3745

81

38

10

4816

3745

5

9

143

1191

4376

1191

4376

2604593

81

188

261

116

3356

4

4

4

9

1032

262

2339

3745

5

263

9

5

8

3745

5

2644593 2655

4376

0 1
3

2
309

3
9331

4
8

0

1

3

2

45

30

1716

32318

5763

7235

4
124

51716

6

2318

7

11070

811744

9
2318

35

9

10

3

11

4

12

7

13

9

14

161

155

4

9

16
7

9

174

18

7

9

7

9

7

9

19

3005

20

9

57

21

9

22

109

37

3005

45

3005

9

23

4

24
7

25
45

45
9

7

26

9

63

21

45

57

3

9

109

3005

27

5

3005

28

6

45

45

29

3005

63

5

6

7

9

9

3005

312318

45

32
3005

3

33
2

6

34
5

36
7

5

161

6

3

57

9

433 1 4 1 6 1 6860 1 76

Figure 2: ML translation Ê, word lattice E , and decomposition as a sequence of four string and five
sublattice regions H1 . . .H9 using n-gram posterior probability threshold p(u|E)≥0.8.

tial hypotheses that can be trusted. We wish to
constrain MBR decoding to include these trusted
partial hypotheses but allow decoding to consider
alternatives in regions of low confidence. In this
way we aim to improve the best possible output of
the best available systems.

We use the path posterior n-gram probabilities
of Equation (4) to segment lattice E into regions of
high and low confidence. As shown in the exam-
ple of Figure 2, the lattice segmentation process
is performed relative to the ML hypothesis Ê, i.e.
relative to the best path through E .

For confidence threshold β, we find all 4-grams
u = Êi, . . . , Êi+3 in the ML translation for which
p(u|E) > β. We then segment Ê into regions
of high and low confidence where the high confi-
dence regions are identified by consecutive, over-
lapping high confidence 4-grams. The high confi-
dence regions are contiguous strings of words for
which there is consensus amongst the translations
in the lattice. If we trust the path posterior n-gram
probabilities, any hypothesised translation should
include these high confidence substrings. This ap-
proach differs from simple posterior-based prun-
ing in that we discard paths, rather than words

or n-grams, which are not consistent with high-
confidence regions of the ML hypothesis.

The hypothesis string Ê is in this way seg-
mented into R alternating subsequences of high
and low confidence. The segment boundaries are
ir and jr so that Ê

jr
ir

is either a high confidence
or a low confidence subsequence. Each subse-
quence is associated with an unweighted subspace
Hr; this subspace has the form of a string for high
confidence regions and the form of a lattice for
low confidence regions.

If the rth segment is a high confidence region
then Hr accepts only the string Êjrir . If the rth
segment is a region of low confidence, then Hr
is built to accept relevant substrings from E . It is
constructed as follows. The rth low confidence
region Êjrir has a high confidence left context êr−1
and a high confidence right context êr+1 formed
from subsequences of the ML translation hypoth-
esis Ê as

êr−1 = Ê
jr−1
ir−1 , êr+1 = Ê

jr+1
ir+1

Note that when r = 1 the left context êr−1 is the
empty string and when r = R the right context
êr+1 is the empty string. We build a transducer

74



Tr for the regular expression /. ∗ êr−1(.∗)êr+1. ∗
/\1/.1 Composition with E yieldsHr = E◦Tr , so
that Hr contains all the reasonable alternatives to
Êjrir in E consistent with the high confidence left
and right contexts êr−1 and êr+1. IfHr is aligned
to a high confidence subsequence of Ê, we call
it a string region since it contains a single path;
if it is aligned to a low confidence region it is a
lattice and we call it a sublattice region. The se-
ries of high and low confidence subspace regions
H1, . . . ,HR defines the lattice segmentation.

5 Hypothesis Space Construction
We now describe a general framework for improv-
ing the fluency of the MBR hypothesis space.

The segmentation of the lattice described in
Section 4 considerably simplifies the problem of
improving the fluency of its hypotheses since each
region of low confidence may be considered in-
dependently. The low confidence regions can be
transformed one-by-one and then reassembled to
form a new MBR hypothesis space.

In order to transform the hypothesis region Hr
it is important to know the context in which it oc-
curs, i.e. the sequences of words that form its pre-
fix and suffix. Some transformations might need
only a short context; others may need a sentence-
level context, i.e. the full sequence of ML words
Ê

jr−1
1 and Ê

N
ir+1

to the left and right of the region
Hr that is to be transformed.

To put this formally, each low confidence sub-
lattice region is transformed by the application of
some function Ψ:

Hr ← Ψ(Êjr−11 , Hr, ÊNir+1) (6)

The hypothesis space is then constructed from the
concatenation of high confidence string and trans-
formed low confidence sublattice regions

H = E ◦
⊗

1≤r≤R
Hr (7)

The composition with the original lattice E dis-
cards any new hypotheses that might be created
via the unconstrained concatenation of strings
from theHr. It may be that in some circumstances

1In this notation parentheses indicate string matches so
that /. ∗ y(a∗)w. ∗ /\1/ applied to xyaaawzz yields aaa.

the introduction of new paths is good, but in what
follows we test the ability to improve fluency by
searching among existing hypotheses, and this en-
sures that nothing new is introduced.

Size of the Hypothesis Space If no new hy-
potheses are introduced by the operations Ψ, the
size of the hypothesis space H is determined by
the posterior probability threshold β. Only the
ML hypothesis remains at β = 0, since all its
subsequences are of high confidence, i.e. can be
covered by n-grams with non-zero path posterior
probability. At the other extreme, for β = 1, it
follows that H = E and no paths are removed,
since any string regions created are formed from
subsequences that occur on every path in E .

We can therefore use β to tighten or relax
constraints on the LMBR hypothesis space. At
β = 0, LMBR returns only the ML hypothesis;
at β = 1, LMBR is done over the full transla-
tion lattice. This is shown in Table 1, where the
BLEU score approaches the BLEU score of un-
constrained LMBR as β increases.

Note also that the size of the resulting hypoth-
esis space is the product of the number of se-
quences in the sublattice regions. For Figure 2 at
β = 0.8, this product is ∼5.4 billion hypotheses.
Even for fairly aggressive constraints on the hy-
pothesis space, many hypotheses remain.

6 Monolingual Coverage Constraints

This section describes one implementation of the
transformation function Ψ that we will show leads
to improved fluency of machine translation out-
put. This transformation is based on n-gram cov-
erage in a large target language text collection:
where possible, we filter the sublattice regions
so that they contain only long-span n-grams ob-
served in the text. Our motivation is that large
monolingual text collections are good guides to
fluency. If a hypothesis is composed entirely of
previously seen high order n-grams, it is likely to
be fluent and should be favoured.

Initial attempts to identify fluent hypotheses in
sublattice regions by ranking according to n-gram
LM scores were ineffective. Figure 3 shows the
difficulties. We see that both the 4-gram Kneser-
Ney and 5-gram stupid-backoff language models

75



LM Translation hypothesis E and n-gram orders used by the LM to score each word Score

4g
<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 atomic3 bomb2 .3 </s>4 -22.59
<s>1 the2 reactor3 produces3 plutonium2 needed2 to3 manufacture4 the4 atomic2 bomb3 .4 </s>4 -23.61

5g <s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 atomic5 bomb2 .3 </s>4 -16.04
<s>1 the2 reactor3 produces4 plutonium5 needed3 to3 manufacture4 the4 atomic4 bomb5 .4 </s>5 -17.96

Figure 3: Scores and n-gram orders for hypotheses using 4-gram Kneser-Ney and 5-gram stupid-
backoff (estimated from 1.1B and 6.6B tokens, resp.) LMs. Low confidence regions are in italics.

favour the shorter but disfluent hypothesis; nor-
malising by length was not effective. However,
the stupid-backoff LM has better coverage and the
backing-off behaviour is a clue to the presence
of disfluency. Similar cues have been observed
in ASR analysis (Chase, 1997). The shorter hy-
pothesis backs off to a bigram for “atomic bomb”,
whereas the longer hypothesis covers the same
words with 4-grams and 5-grams. We therefore
disregard the language model scores and focus on
n-gram coverage. This is an example where ro-
bustness and fluency are at odds. The n-gram
models are robust, but often favour less fluent hy-
potheses.

Let S denote the set of all n-grams in the mono-
lingual training data. To identify partial hypothe-
ses in sublattice regions that have complete mono-
lingual coverage at the maximum order n, we
build a coverage acceptor Cn with a similar form
to the WFST representation of an n-gram backoff
language model (Allauzen et al., 2003). Cn as-
signs a penalty to every n-gram not found in S .
In Cn word arcs have no cost and backoff arcs are
assigned a fixed cost of 1. Firstly, arcs from the
start state are added for each unigram w ∈ N1:

w
w/0∅

Then for n-grams u ∈ S ∩ {∪ni=2 Ni}, where
u = wn1 consisting of history h = w

n−1
1 and target

word wn, arcs are added

wn/0h h+

where h+ = wn−12 if u has order n and h
+ = wn1

if u has order less than n. Backoff arcs are added
for each u as

φ/1
h h−

where h− = wn−12 if u has order > 2, and bi-
grams backoff to the null history start state ∅.

For each sublattice region Hr, we wish to pe-
nalise each path proportionally to the number of

its n-grams not found in the monolingual text col-
lection S . We wish to do this in context, so that
we include the effect of the neighbouring high
confidence regions Hr−1 and Hr+1. Given that
we are counting n-grams at order n we form the
left context machine Lr which accepts the last
n − 1 words in Hr−1; similarly, Rr accepts the
first n − 1 words of Hr+1. The concatenation
Xr = Lr⊗Hr⊗Rr represents the partial transla-
tion hypotheses inHr padded with n−1 words of
left and right context from the neighbouring high
confidence regions. Composing Xr ◦ Cn assigns
each partial hypothesis a cost equal to the number
of times it was necessary to back off to lower order
n-grams while reading the string. Partial hypothe-
ses with cost 0 did not back off at all and contain
only maximum order n-grams.

In the following experiments, we look at each
Xn ◦ Cn and if there are paths with cost 0, only
these are kept and all others discarded. We intro-
duce this as a constraint on the hypothesis space
which we will evaluate for improvement on flu-
ency. Here the transformation function Ψ returns
Hr as Xr ◦Cn after pruning. If Xr ◦Cn has no zero
cost paths, the transformation function Ψ returns
Hr as we find it, since there is not enough mono-
lingual coverage to guide the selection of fluent
hypotheses. After applying monolingual coverage
constraints to each region, the modified hypothe-
sis space used for MBR search is formed by con-
catenation using Equation (7).

We note that Cn is a simplistic NLG system. It
generates strings by concatenating n-grams found
in S . We do not allow it to run ‘open loop’ in these
experiments, but instead use it to find the strings
in Xr with good n-gram coverage.

7 LMBR Over Segmented Lattices

The effect of fluency constraints on LMBR de-
coding is evaluated in the context of the NIST
Arabic→English MT task. The set tune consists

76



ML ... view , especially with the open chinese economy to the world and ...
+LMBR ... view , especially with the open chinese economy to the world and ...
+LMBR+CC ... view , especially with the opening of the chinese economy to the world and ...

ML ... revision of the constitution of the japanese public , which dates back ...
+LMBR ... revision of the constitution of the japanese public , which dates back ...
+LMBR+CC ... revision of the constitution of japan , which dates back ...

Figure 4: Improved fluency through the application of monolingual coverage constraints to the hypoth-
esis space in MBR decoding of NIST MT 08 Arabic→English newswire lattices.

of the odd numbered sentences of the MT02–
MT05 testsets; the even numbered sentences form
test. MT08 performance on nw08 (newswire) and
ng08 (newsgroup) data is also reported.

First-pass translation is performed using HiFST
(Iglesias et al., 2009), a hierarchical phrase-based
decoder. The first-pass LM is a modified Kneser-
Ney (Kneser and Ney, 1995) 4-gram estimated
over the English side of the parallel text and an
881M word subset of the English GigaWord 3rd
Edition. Prior to LMBR, the first-pass lattices are
rescored with zero-cutoff stupid-backoff 5-gram
language models (Brants et al., 2007) estimated
over more than 6B words of English text. The
LMBR factors θ0, . . . , θ4 are set as in Tromble et
al. (2008) using unigram precision p = 0.85 and
recall ratio r = 0.74.

The effect of performing LMBR over the seg-
mented hypothesis space is shown in Table 1. The
hypothesis subspaces Hr are constructed at var-
ious confidence thresholds as described in Sec-
tion 4 with H formed via Equation (7); no cover-
age constraints are applied yet. Constraining the
search space using β = 0.6 leads to little degra-
dation in LMBR performance under BLEU. This
shows lattice segmentation works as intended.

We next investigate the effect of monolingual
coverage constraints on BLEU. We build accep-
tors Cn as described in Section 6 with S con-
sisting of all n-grams in the English GigaWord.
At β = 0.6 we found 181 sentences with sub-
lattices Hr spanned by maximum order n-grams
from S , i.e. for which Xr ◦ Cn have paths with
cost 0; these are filtered as described. LMBR
over these coverage-constrained sublattices is de-
noted LMBR+CC. On nw08 the BLEU score for
LMBR+CC is 52.0 which is +0.7 over the ML de-
coder and only -0.2 BLEU below unconstrained
LMBR decoding. Done in this way, constraining
hypotheses to have 5-grams from the GigaWord

tune test nw08 ng08
ML 54.2 53.8 51.3 36.3

β

0.0 54.2 53.8 51.3 36.3
0.2 54.3 53.8 51.3 36.3
0.4 54.6 54.2 51.6 36.7
0.6 54.9 54.4 52.1 36.6
0.8 54.9 54.4 52.1 36.6
1.0 54.9 54.4 52.2 36.7

LMBR 54.9 54.4 52.2 36.8

Table 1: BLEU scores for ML hypotheses and
LMBR decoding inH over 0 ≤ β ≤ 1.

has little impact on BLEU.
At this value of β, 116 of the 813 nw08 sen-

tences have a low confidence region (1) com-
pletely covered by 5-grams, and (2) within which
the ML hypothesis and the LMBR+CC hypothe-
sis differ. It is these regions which we will inspect
for improved fluency.

8 Human Fluency Evaluation
We asked 17 native speakers to judge the fluency
of sentence fragments from nw08. We compared
hypotheses from the ML and the LMBR+CC de-
coders. Each fragment consisted of the partial
translation hypothesis from a low confidence re-
gion together with its left and right high confi-
dence contexts (examples given in Figure 4). For
each sample, judges were asked: “Could this frag-
ment occur in a fluent sentence?”

The results are shown in Table 2. Most of the
time, the ML and LMBR+CC sentence fragments
were both judged to be fluent; it often happened
that they differed by only a single noun or verb
substitution which didn’t affect fluency. In a small
number of cases, both ML and LMBR+CC were
judged to be disfluent. We are most interested in
the ‘off-diagonal’ cases. In cases when one sys-
tem was judged to be fluent and the other was not,
LMBR+CC was preferred about twice as often as
the ML baseline (26.9% to 9.7%). In other words,
the monolingual fluency constraints were judged

77



LMBR+CC
Fluent Not Fluent

ML
Fluent 1175 (59.6%) 192 (9.7%)

Not Fluent 530 (26.9%) 75 (3.8%)

Table 2: Partial hypothesis fluency judgements.

to have improved the fluency of the low confi-
dence region more than twice as often as a fluent
hypothesis was made disfluent.

Some examples of improved fluency are shown
in Figure 4. Although both the ML and un-
constrained LMBR hypotheses might satisfy ad-
equacy, they lack the fluency of the LMBR+CC
hypotheses generated using monolingual fluency
constraints.

9 Summary and Discussion
We have described a general framework for im-
proving SMT fluency. Decoupling the hypothesis
space from the evidence space allows for much
greater flexibility in lattice MBR search.

We have shown that high path posterior proba-
bility n-grams in the ML translation can be used to
guide the segmentation of a lattice into regions of
high and low confidence. Segmenting the lattice
simplifies the process of refining the hypothesis
space since low confidence regions can be refined
in the context of their high confidence neighbours.
This can be done independently before reassem-
bling the refined regions. Lattice segmentation
facilitates the application of post-processing and
rescoring techniques targeted to address particu-
lar deficiencies in ML decoding.

The techniques we presented are related to con-
sensus decoding and system combination for SMT
(Matusov et al., 2006; Sim et al., 2007), and to
segmental MBR for automatic speech recognition
(Goel et al., 2004). Mohit et al. (2009) describe
an alternative approach to improving specific por-
tions of translation hypotheses. They use an SVM
classifier to identify a single phrase in each source
language sentence that is “difficult to translate”;
such phrases are then translated using an adapted
language model estimated from parallel data. In
contrast to their approach, our approach is able
to exploit large collections of monolingual data to
refine multiple low confidence regions using pos-
terior probabilities obtained from a high-quality
evidence space of first-pass translations.

Testset Sentences Reachability
tune 2075 15%
test 2040 14%

nw08 813 11%
ng08 547 9%

Table 3: Arabic→English reference reachability.

We applied hypothesis space constraints based
on monolingual coverage to low confidence re-
gions resulting in improved fluency with no real
degradation in BLEU score relative to uncon-
strained LMBR decoding. This approach is lim-
ited by the coverage of sublattices using monolin-
gual text. We expect this to improve with larger
text collections or in tightly focused scenarios
where in-domain text is less diverse.

However, fluency will be best improved by inte-
grating more sophisticated natural language gen-
eration. NLG systems capable of generating sen-
tence fragments in context can be incorporated di-
rectly into this framework. If the MBR hypothe-
sis spaceH contains a generated hypothesis Ē for
which P (F |Ē) = 0, Ē could still be produced as
a translation, since it can be ‘voted for’ by nearby
hypotheses produced by the underlying system.

Table 3 shows the proportion of NIST testset
sentences that can be aligned to any of the ref-
erence translations using our high quality base-
line hierarchical decoder with a powerful gram-
mar. The low level of reachability suggests that
NLG may be required to achieve high levels of
translation quality and fluency. Other rescoring
approaches (Kumar et al., 2009; Li et al., 2009)
may also benefit from NLG when the baseline is
incapable of generating the reference.

We note that our approach could also be used to
improve the fluency of ASR, OCR and other lan-
guage processing tasks where the goal is to pro-
duce fluent natural language output.

Acknowledgments
We would like to thank Matt Gibson and the
human judges who participated in the evalua-
tion. This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022 and the European Union Seventh
Framework Programme (FP7-ICT-2009-4) under
Grant Agreement No. 247762.

78



References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark. 2003.

Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL 2003.

Blackwood, Graeme, Adrià de Gispert, and William Byrne.
2010. Efficient path counting transducers for minimum
Bayes-risk decoding of statistical machine translation lat-
tices. In Proceedings of ACL 2010.

Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in ma-
chine translation. In Proceedings of the EMNLP 2007.

Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational
Linguistics, 16(2):79–85.

Callison-Burch, Chris, Philipp Koehn, Christof Monz, and
Josh Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In WMT 2009.

Chase, Lin Lawrance. 1997. Error-responsive feed-
back mechanisms for speech recognizers, Ph.D. Thesis,
Carnegie Mellon University.

Chiang, David. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.

DeNero, John, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL-IJCNLP 2009.

Goel, V., S. Kumar, and W. Byrne. 2004. Segmental mini-
mum Bayes-risk decoding for automatic speech recogni-
tion. IEEE Transactions on Speech and Audio Process-
ing, 12:234–249.

Iglesias, Gonzalo, Adrià de Gispert, Eduardo R. Banga, and
William Byrne. 2009. Hierarchical phrase-based trans-
lation with weighted finite state transducers. In Proceed-
ings of the 2009 Annual Conference of the NAACL.

Kneser, R. and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Acoustics, Speech, and
Signal Processing.

Knight, K and J Graehl. 2005. An overview of probabilis-
tic tree transducers for natural language processing. In
Proceedings of CICLING 2005.

Knight, K. 2007a. Capturing practical natural language
transformations. Machine Translation, 21(2).

Knight, Kevin. 2007b. Automatic language translation gen-
eration help needs badly. In MT Summit XI Workshop on
Using Corpora for NLG: Keynote Address.

Kumar, Shankar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine translation. In
NAACL 2004.

Kumar, Shankar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL-IJCNLP 2009.

Lavie, Alon and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine trans-
lation. Machine Translation Journal.

Li, Zhifei, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine translation.
In Proceedings of ACL-IJCNLP 2009.

Ma, Xiaoyi and Christopher Cieri. 2006. Corpus support for
machine translation at LDC. In LREC 2006.

Matusov, Evgeny, Nicola Ueffing, and Hermann Ney. 2006.
Computing consensus translation from multiple machine
translation systems using enhanced hypotheses align-
ment. In 11th Conference of the EACL.

Mohit, B., F. Liberato, and R. Hwa. 2009. Language model
adaptation for difficult-to-translate phrases. In Proceed-
ings of the 13th Annual Conference of the EAMT.

Mohri, Mehryar, Fernando Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition.
In CSL, volume 16, pages 69–88.

Oberlander, Jon and Chris Brew. 2000. Stochastic text gen-
eration. In Philosophical Transactions of the Royal Soci-
ety.

Och, F., D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features
for statistical machine translation. In Proceedings of the
HLT Conference of the NAACL.

Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of
machine translation. In Proceedings of ACL 2002.

Sim, K.-C., W. Byrne, M. Gales, H. Sahbi, and P.C. Wood-
land. 2007. Consensus network decoding for statisti-
cal machine translation system combination. In ICASSP
2007.

Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, , and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Pro-
ceedings of AMTA.

Tromble, Roy, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decoding
for statistical machine translation. In Proceedings of the
2008 Conference on EMNLP.

Ueffing, Nicola and Hermann Ney. 2007. Word-level confi-
dence estimation for machine translation. Computational
Linguistics, 33(1):9–40.

Vilar, D, G Leusch, H Ney, and R Banchs. 2007. Human
evaluation of machine translation through binary system
comparisons. In Proceedings of WMT 2007.

Zens, Richard and Hermann Ney. 2006. N -gram posterior
probabilities for statistical machine translation. In Pro-
ceedings of WMT 2006.

79


