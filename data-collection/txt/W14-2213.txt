



















































Data Warehouse, Bronze, Gold, STEC, Software


Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 91–99,
Baltimore, Maryland, USA, 26 June 2014. c©2014 Association for Computational Linguistics

Data Warehouse, Bronze, Gold, STEC, Software  

Doug Cooper 
Center for Research in Computational Linguistics 
doug.cooper.thailand@gmail.com 

 
Abstract 

We are building an analytical data warehouse 
for linguistic data – primarily lexicons and 
phonological data – for languages in the 
Asia-Pacific region.  This paper briefly out-
lines the project, making the point that the 
need for improved technology for endangered 
and low-density language data extends well 
beyond completion of fieldwork.  We suggest 
that shared task evaluation challenges 
(STECs) are an appropriate model to follow 
for creating this technology, and that stocking 
data warehouses with clean bronze-standard 
data and baseline tools – no mean task – is an 
effective way to elicit the broad collaboration 
from linguists and computer scientists needed 
to create the gold-standard data that STECs 
require.  

1 Introduction 
The call for this workshop mentions the first step 
of the language documentation process, pointing 
out that the promise of new technology in docu-
menting endangered languages remains unful-
filled, particularly in the context of modern re-
cording technologies. 

But lack of tools extends far beyond this first 
step.  It encompasses the accessibility of data 
long since gathered and (usually, but not always) 
published, as well as applications for the data by 
its most voracious consumer:  the study of com-
parative and historical linguistics.     

We encounter these problems daily in prelimi-
nary development of data and software resources 
for a planned Asia-Pacific Linguistic Data Ware-
house.  Briefly, our initial focus is on five phyla 
(~2,000 languages):  Austroasiatic, Austronesian, 
Hmong-Mien, Kra-Dai, and Sino-Tibetan, which 
form a Southeast Asian convergence area, and 
individually extend well into China, India, the 
Himalayas, and the Pacific.  Data for languages 
of Australia and New Guinea will follow. 

Not all of these languages are endangered, but 
many are; not all are low-density, but most are. 

Our data are preferentially drawn from the sort 
of lexicography gathered for comparative pur-
poses (ideally 2,500 items per language), and the 

phonological, semantic, and phylogenetic data 
that can be found for, or inferred from, them.  
These are the only kind of data for which we are 
likely to find near-complete language representa-
tion.  We include smaller lexicons when neces-
sary, and intra-language dialect surveys when 
available.  All available metadata are incorpo-
rated, including typological and phonotactic fea-
tures, (phylogenetic) character sets, geo-physical 
and demographic data, details of lexicon cover-
age, extent, or quality, and bibliographic or 
source data. 

Such data are not always easily found.  Their 
delivery packages – primarily books and journals 
– may be discoverable via bibliographic meta-
data, but details of the datasets themselves are 
not.  As a result, traditional bibliographic docu-
mentation, accessed via portals like OLAC 
(Simons and Bird, 2000) and Glottolog (Nord-
hoff and Hammarström, 2011), tends to have low 
recall and precision in regard to data resource 
discovery. 

Our experience in acquiring and performing 
methodical data audits of  large quantities of 
published and unpublished materials reveals sets 
of lexical, grammatical, phonological, corpus, 
and other materials that are regular enough in 
form, and extensive enough in content, to com-
prise aggregable linguistic data supersets for the 
Asia-Pacific region. 

These ongoing data audits take a three-tiered 
approach, separately documenting texts (to en-
able source recovery), their abstract data content 
(to enable high-recall resource discovery), and 
any concrete, transcribed data instances (to en-
able high-precision data aggregation).   

Discovery and aggregation only open the door.  
Many datasets are hand-crafted for a researcher’s 
specific needs and interests, even if they fall into 
larger research categories.  Yet far from having 
reliable algorithms for central concerns (such as 
proto-language reconstruction, or subgrouping of 
linguistic phyla in family trees or networks) the 
field has not yet had to grapple with basic prob-
lems – such as normalizing phonological tran-
scription or gloss semantics, or accurately as-
sembling large-scale cognate sets – that will be 

91



presented by datasets that include millions of 
data items for thousands of languages, and many 
more thousands of dialectal variants. 

The central issue we face is the gap between: 
 the results of published and unpublished 

fieldwork, and 
 their usability in downstream research and 

reference applications. 
In some cases this gap is painfully obvious – 

as in the backlog of carefully elicited wordlists 
still awaiting phonetic transcription.  In others, 
the gap becomes evident when we begin to as-
semble large comparable datasets from published 
data; deceptively difficult, and never accom-
plished for collections broader than a single lan-
guage family, or larger than about 200 words per 
language.  Such tasks are still basically hand 
work; often requiring the specialized knowledge 
of the field researcher. 

1.1 Data life cycle:  anticipate or participate 
We see the need for tools as part of a new sort of 
data life cycle management that extends the con-
cerns of content, format, discovery, access, cita-
tion, preservation, and rights as usually articu-
lated, notably in Bird and Simons (2003).   

Simply put, producing publishable or “cor-
rect” results is not sufficient to guarantee the 
downstream usability of data.  Rather, data must 
undergo a series of transformations as it travels 
from one research specialty to the next.  We hope 
there will be an increasing expectation that the 
data producer either anticipate or participate in 
this process.  

At one end of the cycle, this often requires 
small, specialized datasets of the sort needed to 
support software development for tasks like 
automated transcription or phonemic analysis – 
still open problems in the context of under-
resourced languages. 

At the other, building massive datasets that are 
suitable for improving and extending quantitative 
comparative linguistic applications – or discover-
ing the scales at which different methods might 
be most useful – has not been a priority for the 
linguistics community:  if a few representative 
items demonstrate a relationship or support a 
reconstruction convincingly, then exhaustive 
coverage does not make the argument stronger. 

We face a classic resource deadlock.  High-
quality “last-user” datasets are not constructed 
because traditional methods are too expensive 
and time-consuming.  However, tools for refin-
ing “first-producer” data on an industrial scale 

are not built because the high-quality datasets 
needed to validate them do not exist.  Develop-
ment of computational methods for problems like 
subgrouping tends to focus on a small number of 
available datasets, while their results are criti-
cized for precisely this. 

2 STECs and gold-standard data 
Log jams in natural language processing are 
nothing new.  A shared task evaluation chal-
lenge (STEC) presents an open challenge to the 
field in the context of evaluating performance on 
a specific task.  Originally developed in the con-
text of the TIPSTER Text Program (which initi-
ated the long-running MUC and TREC confer-
ence series) as discussed in Belz and Kilgarriff 
(2006), see also Hirschmann (1998) “Over the 
past twenty years, virtually every field of re-
search in human language technology (HLT) has 
introduced STECS.”   

The STEC is the culmination of a series of ef-
forts intended to focus and advance progress by 
asking such questions as: 
 what problems need to be solved in order 

to advance the field?  Where are we trying 
to go, and what is standing in our way? 

 what kinds of necessary data are not gener-
ally available?  What kinds of datasets are 
too difficult for individual researchers to 
create? 

 what kind of functional decomposition into 
simpler goals will help demonstrates and 
measure progress in quantitative and quali-
tative terms?   

Both data, and evaluation metrics, are made 
available well before the STEC, which is often 
held in conjunction with a major conference.  
The task is typically initiated by the release of a 
dataset; results are submitted by some deadline, 
and the results of evaluation are announced be-
fore or at the conference.  

The terms gold-standard and more recently, 
silver-standard (for machine-generated sets) are 
used to describe datasets created for use in 
STECs and NLP applications.  These can be 
thought of as being “correct answers” for quanti-
tative evaluation (Kilgarriff 1998).  

Gold-standard datasets are built to enable 
comparable evaluation of alternative algorithms 
or implementations.  Frequently, part of the set 
will be publicly released in advance to serve as 
training data, while part of it is held back to pro-
vide test data (and is released at a later date).  

92



Gold-standard datasets reflect the state of the 
art in an area, such as the specification of word 
senses, delineation of word boundaries, or 
evaluation of message sentiment, for which there 
may not be any purely objective ground truth.  
We can reasonably expect to allow alternative 
formulations of gold-standard sets in areas in 
which the state of the art may be uncertain, even 
in the eyes of experts.  And we can anticipate 
increased critical scrutiny of previously accepted 
judgments as more base data and better investi-
gative tools become available; see e.g. Round 
(2013, 2014). 

2.1 STECs for low-density languages 
In our opinion, all of the reasons for which 
STECs are devised and gold-standard datasets 
defined apply equally to the low-density lan-
guage problems we touched on in Section 1.  
These include: 
 normalization and syllabification of tran-

scribed data, 
 phonetic transcription of audio and ortho-

graphic data, 
 morphemic analysis of transcribed data, 
 extraction of a phonemic analysis from 

phonetic data, 
 identification of internal cognates and/or 

derivationally related forms, as well as 
loan-word identification and stratification, 

 automated reglossing / translation (to a 
standardized gloss set) of glosses and/or 
definitions. 

 automated inference of phylogenetic sub-
grouping. 

 automated generation of proto-forms, 
All are characterized by the same requirement 

for human judgment in processing, and lack of 
absolute certainty as to outcomes. 

The critical difference is that (as far as we 
know) STECs in NLP invariably focus on high-
density languages for which both data and exper-
tise are readily available.  In contrast, low-
density languages – which presumably includes 
the entire range of endangered languages – are 
by their nature specialty realms, for which exper-
tise, even within a single phylum, is often widely 
dispersed.   

Thus, the problem we face in creating success-
ful STECs for documentary linguistics is not 
simply a matter of thinking up tasks, and relying 
on in-house expertise to develop gold-standard 
datasets.  Rather, advancing development of 
computational tools requires participation from a 
large community of independently working lin-
guists as well.  

3 Cast bronze to net gold 
Our approach to achieving this begins by laying 
the groundwork for collaboration between:  
 computer scientists who recognize the need 

for better data, and will join the challenge 
of solving practical problems in building 
massive, comparable datasets, and  

 linguists willing to help create and validate 
the gold-standard reference sets and train-
ing data needed to establish quality metrics 
for improving software tools.   

We think this collaboration is best motivated 
in the old-fashioned way:  reduce participants’ 

vapor   no data could be located (useful when documenting data availability by ISO code) 
water untranscribed audio recording only 
paper   print/image/PDF data are in hand, but not transcribed or extracted 
tin   raw e-orthography and definitions (as in typical documentary dictionaries) 
copper   raw e-forms and glosses (as in purpose-collected comparative lexicons; e.g. Holle lists)

bronze 

clean electronic data and metadata, ready for hand or machine processing, 
naive normalization of forms and glosses, cognate sets partially specified, 
capable of demonstrating preliminary data warehouse functionality 
(Software:  baseline vanilla algorithms) 

silver  machine-normalized or grouped data, not yet verified by humans (Software:  better than baseline) 

gold   human-verified/accepted, machine-usable comparable datasets (Software:  (best) able to produce gold-standard results) 
Table 1. Data quality standards re lexicons, cognate sets, reconstructions, and subgrouping, with par-
allels to software tools. Silver- and gold-standard are the only terms commonly used in this context.

93



startup costs, flatten their learning curves, high-
light expected outcomes that will advance col-
laborators’ self-interests, and help provide the 
data, tools, and/or metrics that collaborators will 
need to seek funding themselves. 

This in itself as a long-term effort – easily 5–8 
years for our region, with optimal funding – 
whose thrust can be summarized as cast bronze 
to net gold (see Table 1).  

Locating data, and bringing it to the minimal 
state required for computer applications requires 
a massive amount of work.  Consider just the 
discovery aspect, for which the data audit men-
tioned earlier entails an ongoing, two-pronged 
effort.   

On one hand, we identify potential data con-
tent by acquiring as much published and unpub-
lished print material as possible, including com-
plete journal runs, monograph series, informally 
published “gray literature,” extensive sets of un-
published field notes, and regular publication 
backlists (notably, a half-century of works from 
Pacific Linguistics, which will be added to our 
on-line repository later this year).1 

On the other, we systematically work through 
the complete ISO 639-3 inventory (as a proxy for 
the on-the ground truth, and as a means of help-
ing to perfect the standard, as well as identifying 
documentary shortfalls that might be short-listed 
for fieldwork) of our region, attempting to find at 
least lexical content for every language.  

Overall, our summary project development 
plan has four steps, which relate to content and 
scale, and determined our choice of a regional 
focus – for which we could take responsibility – 
rather than either working at greater depth on a 
single phylum, or attempting to build a global 
framework, and then relying primarily on outside 
contributors.  

First, define an area that is broad enough to be 
of wide linguistic interest, and able to supply a 
range of control and alternative test conditions 
for both traditional and computational methods.  
Even allowing for typological variation that may 
be found in individual phyla, we think this usu-
ally requires a regional perspective. 

                                                 
1 For New Guinea, this required a special sub-project 
dubbed INGA, dedicated to tracking down “invisible” 
New Guinea archives held in libraries and file cabi-
nets around the world!  As implied, when possible we 
negotiate rights to scan and make all materials freely 
available in an on-line repository, and will begin to 
register DOI names (when appropriate) for texts and 
data this year. 

Second, locate and prepare raw data of suffi-
cient breadth and depth.  We think that aiming 
for blanket rather than selective coverage is ap-
propriate – it enables the broadest range of re-
search agendas by reflecting the natural state of 
human migration and constant language contact.  

Third, establish research goals that capture the 
interest of both fields – documentary / compara-
tive / historical linguistics and computer science.  
This extends the argument for complete regional 
coverage, especially in convergence areas.  But it 
also argues for limiting scope to an area in which 
it is realistically possible to actively recruit in-
volvement, conference by conference. 

Finally, we need to lower barriers to participa-
tion  We think we can do this by providing a 
framework that allows data owners to take ad-
vantage of existing software tools, and which 
provides software developers with easily custom-
ized data test beds – the analytical data ware-
house.  

4 The data warehouse 
A data warehouse is an integrated collection of 
databases that incorporates tools for sampling, 
analyzing, and visualizing query results.  Unlike 
repository databases intended for storage and 
retrieval of prepared values (perhaps for off-line 
processing), data warehouses assume that data 
filtering, transformation, and analysis are essen-
tial to satisfying every query.  In the context of 
comparative lexicons, such tasks are well beyond 
the scope of existing virtual research environ-
ments such as WebLicht (Hinrichs et al 2010) 
and TextGrid (Neuroth et al 2011), which focus 
primarily on text corpora. 

Because sampling filters allow selection of 
homogeneous or representative subsamples, we 
can be as inclusive as possible in regard to data 
acquisition.  We are not talking about data qual-
ity; rather (working within our overall criterion 
of comparative lexical data) we want to avoid 
excluding sets because of concerns about dataset 
size or content disparity, or over-representation 
of dialect survey data.  

Many operations we wish to perform on or 
with data involve open research questions.  Al-
though users may perceive the warehouse as 
providing access to tools, we intend to present it 
to tool developers as a tunable test bed of data 
that does not require them to deal with data man-
agement, as well as a means of using, and en-
couraging development of, open-source toolkits 
such as the pioneering work of Kleiweg (2009) 

94



and List and Moran (2013).  We return to the 
idea of plug-and-play operations on lexicons in 
Section 6. 

The warehouse also helps provide added value 
to potential data contributors.  Even if software is 
freely available, preparing data or setting up 
tools can impose substantial, even insurmount-
able, burdens on data creators, particularly in 
regions in which cooperation between linguists 
and computer scientists is less common than in 
the US or Europe. 

4.1 Data warehouse query-flow 
In our test warehouse implementation, function-
ality is divided as follows: 
 filter: define a search universe based on phy-

logenetic or phonotactic properties, geo-
physical or proximal location, lexicon char-
acteristics, or other data or metadata features. 

 frame: specify data and/or metadata to be 
returned, e.g. specific aspects of the form 
and/or gloss, or metadata details that might 
be useful for correlation testing. 

 analyze: extract phone inventories, calculate 
functional load, investigate lexical neighbor-
hoods, cluster data by phonological similar-
ity, etc.  

 visualize: provide alternatives to tables as 
appropriate, e.g. tree/graph/map layouts. 

 recycle: search within returned data, use 
faceting to extend searches, or let the visu-
alization serve as a chooser for a new search. 

For brevity we discuss just one feature:  filter-
ing.  This lets the search universe be defined in 
as much detail as possible, and is partly common 
sense:  our overall data universe is decidedly 
lumpy due to the decision to include small sam-
ples (some <100 items) when necessary, and dia-
lect surveys (perhaps with only minor differences 
between doculects) when possible. 

It is also intended to take advantage of the 
large quantities of available metadata, whether it 
is explicit / external – that is, related to the lan-
guage or doculect, or implicit / internal, i.e. can 
be derived from individual datasets or samples. 

Such metadata includes proposed phylogenetic 
relations, typological features, geophysical and 
demographic data, characteristics of lexicon 
composition, extent, or quality, bibliographic or 
source data, and phonological properties of the 
doculect itself.  Some of this metadata may be 
returned with individual items as part of the data 
frame. 

Filter targets may be specified if appropriate.  
For example, a filter might limit a search to lan-
guages that contain sesquisyllables, or instead 
require that returned items be sesquisyllabic. 

4.2 An example query and result 
Figure 1 shows the result of a relatively simple 
warehouse query (using our unreleased explora-
tory implementation):  a geo-constrained phy-
logenetic tree for Trans-New Guinea languages.  
Tree topology follows Ethnologue 16 (Lewis, 
2009) as provided by the MultiTree project 
(Aristar and Ratliff, 2005); other analyses are 

Figure 1  A geo-constrained phylogenetic tree (analysis by Ethnologue via MultiTree).  This cluster 
tree keeps low-level group nodes near their daughters, but raises the root nodes.  Dialects are green, 
languages yellow, and groups blue  

95



readily specified.  In this example dialects (from 
the same sources) are arranged in a circular pat-
tern around the ISO 639-3 hub language (and, 
again, other analyses could be used instead).  The 
same filtering and visualization routines are used 
in a different manner in Figure 2, which shows 
words for “bone” in Austronesian languages as 
provided by ABVD (Greenhill et al, 2008).    

5 Data comparability and reusability 
We will finish the discussion of data warehouses 
with a quick look at data comparability and re-
use.  Comparability or equivalence of datasets 
can be looked at in two ways 
 at the content level, e.g. to ensure that the 

same systems of transcription and glossing 
are used for all datasets, and  

 at the structural level, in identifying datasets 
of comparable complexity, structure, or 
available detail. 

At the content level, normalization of forms 
and glosses is the critical transformation in the 
journey to gold-standard quality. We will briefly 
describe our systems for normalization, Meta-
gloss and Metaphon, because they are ripe for 
computational assistance.  The discussion ends 
with a quick introduction to Etyset, the frame-
work we intend to use to describe and distribute 
structured datasets, such as those that incorporate 
subgroup and cognate detail.  

5.1 Gloss, Metagloss, Etygloss 
In most of our applications, a gloss is semantic 
annotation provided by the wordlist author in 
order to index phonological forms.  Unfortu-
nately, these may be elicitation terms rather than 
glosses (green? “grue.”  blue? “grue”), or local 

vernacular rather than common or scientific 
terms for flora and fauna. Phrasing varies wildly, 
and proper reading may depend on having the list 
context available (short/tall, short/long).  Trans-
lation may be lossy (strew or scatter as nouns) 
due to differences in grammaticalization or lexi-
fication.  All of these undermine comparability.  

We have begun to define an intermediate, 
standardized metagloss layer to express the au-
thor’s intent (if discernable).  A third layer, the 
etygloss, will help account for semantic shift in 
labeling cognate groups; i.e. glossing empty 
placeholders for proto-language reconstructions.  
In the simple case all three layers are identical. 

Metagloss provides a controlled vocabulary 
for re-annotating or translating existing lexicon 
glosses; it foregrounds the critical design link 
between glossing and searching.  We map this to 
WordNet senses, creating a low-overhead tool for 
word-sense disambiguation and facet generation.   

The Metagloss controlled vocabulary can be 
extended; it uses attributes to specify predictable 
relationships (sheep:male:castrated for wether) 
and solve lexicalization problems that arise in 
gloss translation (e.g. n@strew is the noun form 
of strew).  Additionally, it allows definition of 
lightweight ontologies; relations between Meta-
glosses that clarify semantic relations and im-
prove search fallback performance.  

5.2 Phon, Metaphon, Etyphon 
Phonological forms present similarly difficult 
search problems; these go beyond easily fixed 
notational convention.  For example, absence of 
marked syllable boundaries can make phonologi-
cal searches difficult when we are interested in 
the phoneme’s role (such as pre-nasalization) 
rather than its sign (/n/, /m/ etc.).   

Figure 2  A search for “bone” in ABVD Austronesian data (again, relations by Ethnologue via Mul-
tiTree), constrained to locations in Indonesia, and projected onto a map

96



The same holds true for other context-
sensitive symbols (e.g. “h” as /h/, /h/, or as a pre-
pended indicator of unvoiced phonemes).  A 
greater problem arises from parsimonious nota-
tions that rely on commentary to clarify unwrit-
ten content, e.g. predictable vowel insertion – 
these must be made explicit.   

We define an intermediate layer of standard-
ized notation called metaphon:  a conventional 
notation that allows consistent search, while 
clearly documenting (and minimizing, in com-
parison to wild-card searches) the scope of any 
unavoidable approximation.  A third layer, the 
etyphon, allows temporary specification of a 
(possibly sub-lexical) phonemic rendition prior 
to any formal reconstruction.   

Metaphon, like metagloss, is intimately tied to 
search functionality.  Normalized transcription 
enables consistent extraction of phonological and 
phonotactic data.  It lets the search universe be 
restricted to languages (or items) that have par-
ticular phonemes or features.  This dynamic, 
data-driven process lets us weigh relative signifi-
cance – frequency, salience, functional load – of 
features in sets that are themselves results drawn 
from a restricted search universe; e.g. to consider 
the functional load of tones in sesquisyllables.   

5.3 Structural comparability:  EtySet 
The discussion thus far has focused on the form 
and quality of data items.  We are equally con-
cerned with what might be called structural com-
parability of data sets, because this determine the 
approach we take to systematic description, dis-
semination, and re-use of cognate sets, phyloge-
netic trees, or sets of proto-form reconstructions.  

This has nothing to do with tagging or inter-
change standards, which can be handled with 
borrowed schemes designed for similar purposes, 
e.g. Newick notation (Felsenstein, 1986) or suc-
cessors (Nakhleh, 2003).  Rather, we require 
nomenclature that might be used to describe their 
contents, or to enable identification of sets of 

comparable complexity, structure, or detail. 
We think such comparison is crucial to help 

research in quantitative historical linguistics 
move beyond its current state, which many lin-
guists view as interesting but nevertheless ad hoc 
experimentation.  In other words, we would like 
to see computational approaches to cognate iden-
tification, subgrouping, and proto-language re-
construction be developed and tested in envi-
ronments for which the controlled variable is 
linguistic typology, with as many other factors as 
possible held equal. 

Similarly, we would like to be able to vary 
starting conditions.  For example Bouchard-Côté 
et al (2013) report on a computational approach 
to reconstruction given (assumed) prior knowl-
edge of subgrouping in Austronesian.  However, 
any one or two variables from amongst cognate 
grouping, reconstruction, and phylogenetic sub-
grouping may be used to test approaches to infer-
ring or generating the third.   

We refer to cognate sets, phylogenetic trees, 
and reconstructed proto-forms as etysets.  The 
key terms of our working descriptive nomencla-
ture are outlined in Table 2.   

Etysets may be bare (links only), or supported  
by reconstructed forms or semantics; note that 
the phylogenetic analyses provided by Eth-
nologue, Glottolog, or MultiTree may be repre-
sent with bare etysets.  An internal cognate etyset 
has depth (number of internal sets) and size 
(number of forms in each set).  A regular cognate 
etyset has depth (the number of sets / implicit 
number of root proto-forms) and breadth (the 
number of lects represented in each cognate set).  

For example a  bare cognate etyset of Bah-
naric, breadth Eth:80% / depth MSEA:90% 
depth includes data from 32 (of 40. according to 
the Ethnologue analysis) Bahnaric languages, 
and at least 450 of the 500-odd terms in the 
MSEA (SIL 2002) elicitation list.  Cognate 
groupings are provided, but not reconstructions 
or etyglosses.  

breadth number of nodes or leaves at any level of a phylogenetic tree. 
depth number of branch levels supplied. 
degree branchy-ness – the number of branches / degree of diversity at a given node. 
density a joint measure of breadth, depth, and degree. 
size # of cited or reconstructed forms associated with a leaf or branch node. 
coverage describes the extent of an etyset in terms of a fixed reference inventory.   
phylogenetic etyset described in term of breadth, depth, degree, and size. 
documented node includes metadata for approximate time depth and geographic location.  
cognate etyset may be internal or regular, and contains internal or regular cognate sets. 
Table 2.  Outline of the EtySet descriptive vocabulary. 

97



6 Operations on lexicons 
We end with a brief note about computational 
tasks for and by a data warehouse that is: 
 stocked primarily with lexical, phonological, 

and phylogenetic data and relevant metadata,  
 intended to support research in comparative 

and historical linguistics.   
These fall under the general heading of opera-

tions on lexicons. We do not draw a strict divid-
ing line between software employed to prepare 
data for use in a warehouse, and software used by 
the warehouse.  We do exclude operations whose 
implementation is likely to be closely tied to a 
particular database implementation. 

All would benefit from being implemented as 
plug-and-play functions, requiring some, but not 
excessive, programmer effort.  This:  
 allows head-to-head comparison of alterna-

tive algorithms, implementations, or inter-
pretations of how measurements or actions 
should be carried out, 

 allows encapsulation and offloading of com-
putationally expensive algorithms; this is an 
important issue for some quantitative or sta-
tistical comparative methods, and  

 encourages re-use of code in building new, 
alternative platforms for linguistic research. 

We assume that all of these can be specified in 
terms of functionality, required data inputs, and 
expected data outputs, sticking to a Unix-like 
model in which data can be minimally formatted 
plain-text streams which, with the assistance of 
tabs, parentheses, and newlines, can be inter-
preted as bags, lists, vectors, matrices, trees, and 
the like.  Higher-level streams (JSON, XML, 
RDF, HTML) are also reasonable outputs. 

For brevity’s sake, we limit examples to op-
erations on phonological forms.  We could easily 
list similar sets of operations – some straightfor-
ward, some not – on morphology, semantics, 
alternatives for visualization, cognate identifica-
tion, phylogenetic subgrouping, proto-form gen-
eration, and the like. 

Operations on phonological strings / lists 
Conversion and markup of transcription 
 between standardized and/or special-purpose 

notations, 
 to novel notations, e.g. gestural scores,  
 unambiguous conversion of notation from 

historical (e.g. Americanist) to IPA, 

 potentially ambiguous normalization (e.g. 
interpretation of /h/), 

 phonetic to phonemic conversion, 
 marking of syllable boundaries, 
 marking of syllable-internal features (e.g. 

onset, nucleus, coda), 
 marking of morpheme boundaries. 

Extraction / recognition of phonological features 
 sonority sequence tagging. 
 extraction/recognition of phones, phonation, 

co-articulatory, suprasegmental features, 
 count/extraction of phone/feature n-grams, 
 extraction or identification of arbitrary collo-

cational features (e.g. sesquisyllable+tone), 

Calculation of distance/similarity measures be-
tween strings, lists, and vectors 
 weighted and unweighted edit distances,  
 substring matching measures, 
 vector cosine distance,  
 phonologically based distance/similarity, 
 language-internal distance/similarity, 
 information content distance/similarity.  

Clustering 
 subgrouping list contents, 
 “sounds like...” search (for very large sets). 

Neighborhood measures  
 generation of phonological neighborhoods, 
 identification of neighbors, 
 calculation of neighborhood size, density, 

clustering coefficients. 

Load measures  
 calculation of functional load of phonemes, 

features, collocations, 
 calculation of salience of phonemes, fea-

tures, collocations, 
 use in pseudo-word generation. 

7 Conclusion 
The call for this workshop foregrounds develop-
ment of software to aid in initial documentation 
of endangered languages, seeks models for col-
lection and management of endangered-language 
data, and means of encouraging productive inter-
action between documentary linguists and com-
puter scientists. 

98



We suggest that these same needs exist all 
down the line, encompassing low-resource lan-
guages in general, documentation long-since 
completed, and analytical applications far re-
moved from fieldwork settings.  We propose that 
addressing them in downstream environments, 
such as data warehouses and STECs, may be an 
effective way to meet our common “preeminent 
grand challenge:”  integration of linguistic theo-
ries and analyses, relying on massive scaling up 
of datasets and new computational methods, as 
articulated by Bender and Good (2010). 

References 
Anthony Aristar and Martha Ratliff. 2005.  MultiTree: 

A digital library of language relationships. Insti-
tute for Language Information and Technology: 
Ypsilanti, MI.  http://multitree.org. 

Anja Belz and Adam Kilgarriff. 2006. Shared-task 
evaluations in HLT: Lessons for NLG.  In Proceed-
ings of INLG-2006. 

Emily Bender and Jeff Good. 2010. A Grand Chal-
lenge for Linguistics:  Scaling Up and Integrating 
Models. White paper contributed to NSF SBE 2020 
initiative. http://www.nsf.gov/sbe/sbe_2020/-
2020_pdfs/Bender_Emily_81.pdf 

Steven Bird and Gary Simons. 2003.  Seven Dimen-
sions of Portability for Language Documentation 
and Description.  Language 79:2003, 557-5822. 

Alexandre Bouchard-Côté,  David Hall,  Thomas L. 
Griffiths and  Dan Klein.  2013.  Automated recon-
struction of ancient languages using probabilistic 
models of sound change.  Proceedings of the Na-
tional Academy of Sciences.  http://-
www.pnas.org/content/110/11/4224 

Joseph Felsenstein. 1986. The newick tree format.   
http://evolution.genetics.washington.edu/phylip/ne
wicktree.html 

Simon Greenhill, Robert Blust, and Russell D.. Gray. 
2008.  The Austronesian Basic Vocabulary Data-
base: From Bioinformatics to Lexomics. Evolu-
tionary Bioinformatics, 4:271-283. http://-
language.psy.auckland.ac.nz/austronesian  

Erhard W. Hinrichs, Marie Hinrichs and Thomas Zas-
trow. 2010. WebLicht: Web-Based LRT Services 
for German. In: Proceedings of the ACL 2010 Sys-
tem Demonstrations. pages 25–29. 

Lynette Hirschman. 1998. The evolution of evalua-
tion: Lessons from the Message Understanding 
Conferences. Computer Speech and Language, 
12:283–285. 

Adam Kilgarriff. 1998. Gold Standard Datasets for 
Evaluating Word Sense Disambiguation Programs. 
Computer Speech and Language, 12 (3) Special Is-

sue on Evaluation of Speech and Language Tech-
nology, edited by Robert Gaizauskas. 453-472.  
http://www.kilgarriff.co.uk/Publications/1998-K-
CompSL.pdf  For TREC see http://trec.nist.gov.   
The TIPSTER site has been preserved here: 
http://www.nist.gov/itl/div894/894.02/related_proj
ects/tipster/ 

Peter Kleiweg. 2006. RuG/L04 Software for dialecto-
metrics and cartography. Rijksuniversiteit Gronin-
gen. Faculteit der Letteren. http://www.let.rug.nl-
/kleiweg/L04/ 

M. Paul Lewis. 2009.  Ethnologue: Languages of the 
World, Sixteenth Edition.  SIL International, Dal-
las, Texas. 

Johann-Mattis List and Steven Moran. 2013. An 
Open-Source Toolkit for Quantitative Historical 
Linguistics.  Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, 13–18, Sofia, Bulgaria.  http://-
www.zora.uzh.ch/84667/1/P13-4003.pdf  

Luay Nakhleh, Daniel Miranker, and Francois Bar-
bancon. 2003. Requirements of Phylogenetic Data-
bases.  In Third IEEE Symposium on BioInformat-
ics and BioEngineering (BIBE’03). 141-148.  IEEE 
Press. http://www.cs.rice.edu/~nakhleh/Papers/-
bibe03_final.pdf 

Heike Neuroth, Felix Lohmeier, Kathleen Marie 
Smith: TextGrid. Virtual Research Environment for 
the Humanities. In: The International Journal of 
Digital Curation. 6, Nr. 2, 2011, S. 222–231. 

Sebastian Nordhoff and Harald Hammarström. 2011. 
Glottolog/Langdoc: Defining dialects, languages, 
and language families as collections of resources.  
783 CEUR Workshop, Proceedings of the First In-
ternational Workshop on Linked Science 2011 
http://iswc2011.semanticweb.org/fileadmin/iswc/-
Papers/Workshops/LISC/nordhoff.pdf 

Erich R. Round. 2013. ‘Big data’ typology and lin-
guistic phylogenetics: Design principles for valid 
datasets.  Presented 25 May 2013 at 21st Manches-
ter Phonology Meeting. Accessible via 
https://uq.academia.edu/ErichRound. 

Erich R. Round. 2014. The performance of STRUC-
TURE on linguistic datasets & ‘researcher degrees 
of freedom’.  Presented 15 Jan 2014 at TaSil, Aar-
hus, Denmark. Accessible via 
https://uq.academia.edu/ErichRound 

SIL Mainland Southeast Asia Group. 2002. Southeast 
Asia 436 Word List revised November 2002.  
http://msea-ling.info/digidata/495/b11824.pdf 

Gary Simons and Steven Bird. 2000. The seven pillars 
of open language archiving:A vision statement.  
http://www.language-archives.org/docs/vision.-
html. 

 

99


