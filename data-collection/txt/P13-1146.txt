



















































Fine-grained Semantic Typing of Emerging Entities


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1488–1497,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Fine-grained Semantic Typing of Emerging Entities

Ndapandula Nakashole, Tomasz Tylenda, Gerhard Weikum
Max Planck Institute for Informatics

Saarbrücken, Germany
{nnakasho,ttylenda,weikum}@mpi-inf.mpg.de

Abstract

Methods for information extraction (IE)
and knowledge base (KB) construction
have been intensively studied. However, a
largely under-explored case is tapping into
highly dynamic sources like news streams
and social media, where new entities are
continuously emerging. In this paper, we
present a method for discovering and se-
mantically typing newly emerging out-of-
KB entities, thus improving the freshness
and recall of ontology-based IE and im-
proving the precision and semantic rigor
of open IE. Our method is based on a prob-
abilistic model that feeds weights into in-
teger linear programs that leverage type
signatures of relational phrases and type
correlation or disjointness constraints. Our
experimental evaluation, based on crowd-
sourced user studies, show our method
performing significantly better than prior
work.

1 Introduction

A large number of knowledge base (KB) con-
struction projects have recently emerged. Promi-
nent examples include Freebase (Bollacker 2008)
which powers the Google Knowledge Graph, Con-
ceptNet (Havasi 2007), YAGO (Suchanek 2007),
and others. These KBs contain many millions of
entities, organized in hundreds to hundred thou-
sands of semantic classes, and hundred millions
of relational facts between entities. However, de-
spite these impressive advances, there are still ma-
jor limitations regarding coverage and freshness.
Most KB projects focus on entities that appear in
Wikipedia (or other reference collections such as
IMDB), and very few have tried to gather entities
“in the long tail” beyond prominent sources. Vir-

tually all projects miss out on newly emerging en-
tities that appear only in the latest news or social
media. For example, the Greenlandic singer Nive
Nielsen has gained attention only recently and is
not included in any KB (a former Wikipedia article
was removed because it “does not indicate the im-
portance or significance of the subject”), and the
resignation of BBC director Entwistle is a recently
new entity (of type event).

Goal. Our goal in this paper is to discover emerg-
ing entities of this kind on the fly as they become
noteworthy in news and social-media streams. A
similar theme is pursued in research on open infor-
mation extraction (open IE) (Banko 2007; Fader
2011; Talukdar 2010; Venetis 2011; Wu 2012),
which yields higher recall compared to ontology-
style KB construction with canonicalized and se-
mantically typed entities organized in prespecified
classes. However, state-of-the-art open IE meth-
ods extract all noun phrases that are likely to de-
note entities. These phrases are not canonical-
ized, so the same entity may appear under many
different names, e.g., “Mr. Entwistle”, “George
Entwistle”, “the BBC director”, “BBC head En-
twistle”, and so on. This is a problem because
names and titles are ambiguous, and this hampers
precise search and concise results.

Our aim is for all recognized and newly dis-
covered entities to be semantically interpretable
by having fine-grained types that connect them
to KB classes. The expectation is that this will
boost the disambiguation of known entity names
and the grouping of new entities, and will also
strengthen the extraction of relational facts about
entities. For informative knowledge, new entities
must be typed in a fine-grained manner (e.g., gui-
tar player, blues band, concert, as opposed to crude
types like person, organization, event).

Strictly speaking, the new entities that we cap-

1488



ture are typed noun phrases. We do not attempt
any cross-document co-reference resolution, as
this would hardly work with the long-tail na-
ture and sparse observations of emerging entities.
Therefore, our setting resembles the established
task of fine-grained typing for noun phrases (Fleis-
chmann 2002), with the difference being that we
disregard common nouns and phrases for promi-
nent in-KB entities and instead exclusively focus
on the difficult case of phrases that likely denote
new entities. The baselines to which we compare
our method are state-of-the-art methods for noun-
phrase typing (Lin 2012; Yosef 2012).
Contribution. The solution presented in this
paper, called PEARL, leverages a repository of
relational patterns that are organized in a type-
signature taxonomy. More specifically, we har-
ness the PATTY collection consisting of more
than 300,000 typed paraphrases (Nakashole 2012).
An example of PATTY’s expressive phrases is:
〈musician〉 * cover * 〈song〉 for a musician per-
forming someone else’s song. When extract-
ing noun phrases, PEARL also collects the co-
occurring PATTY phrases. The type signatures of
the relational phrases are cues for the type of the
entity denoted by the noun phrase. For example,
an entity named Snoop Dogg that frequently co-
occurs with the 〈singer〉 * distinctive voice in *
〈song〉 pattern is likely to be a singer. Moreover,
if one entity in a relational triple is in the KB and
can be properly disambiguated (e.g., a singer), we
can use a partially bound pattern to infer the type
of the other entity (e.g., a song) with higher confi-
dence.

In this line of reasoning, we also leverage the
common situation that many input sentences con-
tain one entity registered in the KB and one novel
or unknown entity. Known entities are recognized
and mapped to the KB using a recent tool for
named entity disambiguation (Hoffart 2011). For
cleaning out false hypotheses among the type can-
didates for a new entity, we devised probabilistic
models and an integer linear program that consid-
ers incompatibilities and correlations among entity
types.

In summary, our contribution in this paper is
a model for discovering and ontologically typ-
ing out-of-KB entities, using a fine-grained type
system and harnessing relational paraphrases with
type signatures for probabilistic weight computa-

tion. Crowdsourced quality assessments demon-
strate the accuracy of our model.

2 Detection of New Entities

To detect noun phrases that potentially refer to en-
tities, we apply a part-of-speech tagger to the in-
put text. For a given noun phrase, there are four
possibilities: a) The noun phrase refers to a gen-
eral concept (a class or abstract concept), not an
individual entity. b) The noun phrase is a known
entity that can be directly mapped to the knowl-
edge base. c) The noun phrase is a new name for
a known entity. d) The noun phrase is a new entity
not known to the knowledge base at all. In this pa-
per, our focus is on case d); all other cases are out
of the scope of this paper.

We use an extensive dictionary of surface forms
for in-KB entities (Hoffart 2012), to determine if
a name or phrase refers to a known entity. If a
phrase does not have any match in the dictionary,
we assume that it refers to a new entity. To decide
if a noun phrase is a true entity (i.e., an individ-
ual entity that is a member of one or more lexi-
cal classes) or a non-entity (i.e., a common noun
phrase that denotes a class or a general concept),
we base the decision on the following hypothesis
(inspired by and generalizing (Bunescu 2006): A
given noun phrase, not known to the knowledge
base, is a true entity if its headword is singular
and is consistently capitalized (i.e., always spelled
with the first letter in upper case).

3 Typing Emerging Entities

To deduce types for new entities we propose to
align new entities along the type signatures of pat-
terns they occur with. In this manner we use the
patterns to suggest types for the entities they occur
with. In particular, we infer entity types from pat-
tern type signatures. Our approach builds on the
following hypothesis:

Hypothesis 3.1 (Type Alignment Hypothesis)
For a given pattern such as 〈actor〉’s character
in 〈movie〉, we assume that an entity pair (x, y)
frequently occurring with the pattern in text
implies that x and y are of the types 〈actor〉 and
〈movie〉, respectively.
Challenges and Objective. While the type align-
ment hypothesis works as a starting point, it in-
troduces false positives. Such false positives stem

1489



from the challenges of polysemy, fuzzy pattern
matches, and incorrect paths between entities.
With polysemy, the same lexico-syntactic pattern
can have different type signatures. For example,
the following are three different patterns: 〈singer〉
released 〈album〉, 〈music band〉 released 〈album〉,
〈company〉 released 〈product〉. For an entity pair
(x, y) occurring with the pattern “released”, x can
be one of three different types.

We cannot expect that the phrases we extract in
text will be exact matches of the typed relational
patterns learned by PATTY. Therefore, for better
recall, we must accept fuzzy matches. Quite often
however, the extracted phrase matches multiple re-
lational patterns to various degrees. Each of the
matched relational patterns has its own type sig-
nature. The type signatures of the various matched
patterns can be incompatible with one another.

The problem of incorrect paths between entities
emerges when a pair of entities occurring in the
same sentence do not stand in a true subject-object
relation. Dependency parsing does not adequately
solve the issue. Web sources contain a plethora
of sentences that are not well-formed. Such sen-
tences mislead the dependency parser to extract
wrong dependencies.

Our solution takes into account polysemy, fuzzy
matches, as well as issues stemming from poten-
tial incorrect-path limitations. We define and solve
the following optimization problem:

Definition 1 (Type Inference Optimization)
Given all the candidate types for x, find the
best types or “strongly supported” types for x.
The final solution must satisfy type disjointness
constraints. Type disjointness constraints are
constraints that indicate that, semantically, a pair
of types cannot apply to the same entity at the
same time. For example, a 〈university〉 cannot be
a 〈person〉.
We also study a relaxation of type disjointness
constraints through the use of type correlation con-
straints. Our task is therefore twofold: first, gen-
erate candidate types for new entities; second, find
the best types for each new entity among its can-
didate types.

4 Candidate Types for Entities

For a given entity, candidate types are types that
can potentially be assigned to that entity, based on

the entity’s co-occurrences with typed relational
patterns.

Definition 2 (Candidate Type) Given a new en-
tity x which occurs with a number of patterns
p1, p2, ..., pn, where each pattern pi has a type sig-
nature with a domain and a range: if x occurs on
the left of pi, we pick the domain of pi as a candi-
date type for x; if x occurs on the right of pi, we
pick the range of pi as a candidate type for x.

For each candidate type, we compute confi-
dence weights. Ideally, if an entity occurs with
a pattern which is highly specific to a given type
then the candidate type should have high con-
fidence. For example “is married to” is more
specific to people then “expelled from”. A per-
son can be expelled from an organization but a
country can also be expelled from an organization
such as NATO. There are various ways to com-
pute weights for candidate types. We first intro-
duce a uniform weight approach and then present a
method for computing more informative weights.

4.1 Uniform Weights

We are given a new entity x which occurs with
phrases (x phrase1 y1), (x phrase2 y2), ..., (x
phrasen yn). Suppose these occurrences lead
to the facts (x, p1, y1), (x, p2, y2),..., (x, pn, yn).
The pis are the typed relational patterns extracted
by PATTY. The facts are generated by matching
phrases to relational patterns with type signa-
tures. The type signature of a pattern is denoted
by:

sig(pi) = (domain(pi), range(pi))

We allow fuzzy matches, hence each fact comes
with a match score. This is the similarity degree
between the phrase observed in text and the typed
relational pattern.

Definition 3 (Fuzzy Match Score) Suppose we
observe the surface string: (x phrase y) which
leads to the fact: x, pi, y. The fuzzy match similar-
ity score is: sim(phrase, pi), where similarity is
the n-gram Jaccard similarity between the phrase
and the typed pattern.

The confidence that x is of type domain is de-
fined as follows:

Definition 4 (Candidate Type Confidence)
For a given observation (x phrase y), where

1490



phrase matches patterns p1, ..., pn, with domains
d1, ..., db which are possibly the same:

typeConf(x, phrase, d) =
∑

{pi:domain(pi)=d}

(
sim(phrase, pi)

)

Observe that this sums up over all patterns that
match the phrase.

To compute the final confidence for
typeConf(x, domain), we aggregate the
confidences over all phrases occurring with x.
Definition 5 (Aggregate Confidence) For
a set of observations (x, phrase1, y1),
(x, phrase2, y2), ..., (x, phrasen, yn), the
aggregate candidate type confidence is given by:

aggTypeConf(x, d) =
∑

phrasei

typeConf(x, phrasei, d)

=
∑

phrasei

∑

{pj :domain(pj)=d}
(sim(phrasei, pj))

The confidence for the range
typeConf(x, range) is computed analogously.
All confidence weights are normalized to values
in [0, 1].

The limitation of the uniform weight approach
is that each pattern is considered equally good for
suggesting candidate types. Thus this approach
does not take into account the intuition that an en-
tity occurring with a pattern which is highly spe-
cific to a given type is a stronger signal that the
entity is of the type suggested. Our next approach
addresses this limitation.

4.2 Co-occurrence Likelihood Weight
Computation

We devise a likelihood model for computing
weights for entity candidate types. Central to this
model is the estimation of the likelihood of a given
type occurring with a given pattern.

Suppose using PATTY methods we mined a
typed relational pattern 〈t1〉 p 〈t2〉. Suppose that
we now encounter a new entity pair (x, y) occur-
ring with a phrase that matches p. We can com-
pute the likelihood of x and y being of types t1
and t2, respectively, from the likelihood of p co-
occurring with entities of types t1, t2. Therefore
we are interested in the type-pattern likelihood,
defined as follows:

Definition 6 (Type-Pattern Likelihood) The
likelihood of p co-occurring with an entity pair
(x, y) of the types (t1, t2) is given by:

P [t1, t2|p] (1)

where t1 and t2 are the types of the arguments ob-
served with p from a corpus such as Wikipedia.
P [t1, t2|p] is expanded as follows:

P [t1, t2|p] =
P [t1, t2, p]

P [p]
. (2)

The expressions on the right-hand side of Equa-
tion 2 can be directly estimated from a corpus.
We use Wikipedia (English), for corpus-based es-
timations. P [t1, t2, p] is the relative occurrence
frequency of the typed pattern among all entity-
pattern-entity triples in a corpus (e.g., the frac-
tion of 〈musican〉 plays 〈song〉 among all triples).
P[p] is the relative occurrence frequency of the un-
typed pattern (e.g., plays) regardless of the argu-
ment types. For example, this sums up over both
〈musican〉 plays 〈song〉 occurrences and 〈actor〉
plays 〈fictional character〉. If we observe a fact
where one argument name can be easily disam-
biguated to a knowledge-base entity so that its type
is known, and the other argument is considered to
be an out-of-knowledge-base entity, we condition
the joint probability of t1, p, and t2 in a different
way:

Definition 7 (Conditional Type-PatternLikelihood)
The likelihood of an entity of type t1 occurring
with a pattern p and an entity of type t2 is given
by:

P [t1|t2, p] =
P [t1, t2, p]

P [p, t2]
(3)

where the P [p, t2] is the relative occurrence fre-
quency of a partial triple, for example, 〈*〉 plays
〈song〉.

Observe that all numbers refer to occurrence
frequencies. For example, P [t1, p, t2] is a frac-
tion of the total number of triples in a corpus.

Multiple patterns can suggest the same type for
an entity. Therefore, the weight of the assertion
that y is of type t, is the total support strength from
all phrases that suggest type t for y.

Definition 8 (Aggregate Likelihood) The aggre-
gate likelihood candidate type confidence is given

1491



by:

typeConf(x, domain)) =
∑

phrasei

∑

pj

(
sim(phrasei, pj) ∗Υ

)

Where Υ = P [t1, t2|p] or P [t1|t2, p] or P [t2|t1, p]

The confidence weights are normalized to values
in [0, 1]. So far we have presented a way of gener-
ating a number of weighted candidate types for x.
In the next step we pick the best types for an entity
among all its candidate types.

4.3 Integer Linear Program Formulation

Given a set of weighted candidate types, our goal
is to pick a compatible subset of types for x. The
additional asset that we leverage here is the com-
patibility of types: how likely is it that an entity
belongs to both type ti and type tj . Some types
are mutually exclusive, for example, the type loca-
tion rules out person and, at finer levels, city rules
out river and building, and so on. Our approach
harnesses these kinds of constraints. Our solution
is formalized as an Integer Linear Program (ILP).
We have candidate types for x: t1, .., tn. First, we
define a decision variable Ti for each candidate
type i = 1, . . . , n. These are binary variables:
Ti = 1 means type ti is selected to be included
in the set of types for x, Ti = 0 means we discard
type ti for x.

In the following we develop two variants of this
approach: a “hard” ILP with rigorous disjointness
constraints, and a “soft” ILP which considers type
correlations.

“Hard” ILP with Type Disjointness Con-
straints. We infer type disjointness constraints
from the YAGO2 knowledge base using occur-
rence statistics. Types with no overlap in entities
or insignificant overlap below a specified thresh-
old are considered disjoint. Notice that this intro-
duces hard constraints whereby selecting one type
of a disjoint pair rules out the second type. We de-
fine type disjointness constraints Ti + Tj ≤ 1 for
all disjoint pairs ti, tj (e.g. person-artifact, movie-
book, city-country, etc.). The ILP is defined as
follows:

objective
max

∑
i Ti × wi

type disjointness constraint
∀(ti, tj)disjoint Ti + Tj ≤ 1

The weights wi are the aggregrated likelihoods
as specified in Definition 8.
“Soft” ILP with Type Correlations. In many
cases, two types are not really mutually exclusive
in the strict sense, but the likelihood that an en-
tity belongs to both types is very low. For exam-
ple, few drummers are also singers. Conversely,
certain type combinations are boosted if they are
strongly correlated. An example is guitar players
and electric guitar players. Our second ILP con-
siders such soft constraints. To this end, we pre-
compute Pearson correlation coefficients for all
type pairs (ti, tj) based on co-occurrences of types
for the same entities. These values vij ∈ [−1, 1]
are used as weights in the objective function of
the ILP. We additionally introduce pair-wise deci-
sion variables Yij , set to 1 if the entity at hand be-
longs to both types ti and tj , and 0 otherwise. This
coupling between the Yij variables and the Ti, Tj
variables is enforced by specific constraints. For
the objective function, we choose a linear combi-
nation of per-type evidence, using weights wi as
before, and the type-compatibility measure, using
weights vij . The ILP with correlations is defined
as follows:

objective
max α

∑
i Ti × wi + (1− α)

∑
ij Yij × vij

type correlation constraints
∀i,j Yij + 1 ≥ Ti + Tj
∀i,j Yij ≤ Ti
∀i,j Yij ≤ Tj

Note that both ILP variants need to be solved
per entity, not over all entities together. The “soft”
ILP has a size quadratic in the number of candidate
types, but this is still a tractable input for modern
solvers. We use the Gurobi software package to
compute the solutions for the ILP’s. With this de-
sign, PEARL can efficiently handle a typical news
article in less than a second, and is well geared for
keeping up with high-rate content streams in real
time. For both the “hard” and “soft” variants of
the ILP, the solution is the best types for entity x
satisfying the constraints.

1492



5 Evaluation

To define a suitable corpus of test data, we ob-
tained a stream of news documents by subscrib-
ing to Google News RSS feeds for a few topics
over a six-month period (April 2012 – Septem-
ber 2012). This produced 318, 434 documents.
The topics we subscribed to are: Angela Merkel,
Barack Obama, Business, Entertainment, Hillary
Clinton, Joe Biden, Mitt Romney, Newt Gingrich,
Rick Santorum, SciTech and Top News. All our ex-
periments were carried out on this data. The type
system used is that of YAGO2, which is derived
from WordNet. Human evaluations were carried
out on Amazon Mechanical Turk (MTurk), which
is a platform for crowd-sourcing tasks that require
human input. Tasks on MTurk are small question-
naires consisting of a description and a set of ques-
tions.
Baselines. We compared PEARL against two
state-of-the-art baselines: i). NNPLB (No Noun
Phrase Left Behind), is the method presented in
(Lin 2012), based on the propagation of types
for known entities through salient patterns occur-
ring with both known and unknown entities. We
implemented the algorithm in (Lin 2012) in our
framework, using the relational patterns of PATTY
(Nakashole 2012) for comparability. For assess-
ment we sampled from the top-5 highest ranked
types for each entity. In our experiments, our im-
plementation of NNPLB achieved precision values
comparable to those reported in (Lin 2012). ii).
HYENA (Hierarchical tYpe classification for En-
tity NAmes), the method of (Yosef 2012), based
on a feature-rich classifier for fine-grained, hierar-
chical type tagging. This is a state-of-the-art rep-
resentative of similar methods such as (Rahman
2010; Ling 2012).
Evaluation Task. To evaluate the quality of types
assigned to emerging entities, we presented turk-
ers with sentences from the news tagged with out-
of-KB entities and the types inferred by the meth-
ods under test. The turkers task was to assess the
correctness of types assigned to an entity mention.
To make it easy to understand the task for the turk-
ers, we combined the extracted entity and type into
a sentence. For example if PEARL inferred that
Brussels Summit is an political event, we generate
and present the sentence: Brussels Summit is an
event. We allowed four possible assessment val-

ues: a) Very good output corresponds to a perfect
result. b) Good output exhibits minor errors. For
instance, the description G20 Summit is an orga-
nization is wrong, because the summit is an event,
but G20 is indeed an organization. The problem in
this example is incorrect segmentation of a named
entity. c) Wrong for incorrect types (e.g., Brussels
Summit is a politician). d) Not sure / do not know
for other cases.
Comparing PEARL to Baselines. Per method,
turkers evaluated 105 entity-type pair test sam-
ples. We first sampled among out-of-KB entities
that were mentioned frequently in the news cor-
pus: in at least 20 different news articles. Each
test sample was given to 3 different turkers for as-
sessment. Since the turkers did not always agree
if the type for a sample is good or not, we ag-
gregate their answers. We use voting to decide
whether the type was assigned correctly to an en-
tity. We consider the following voting variants:
i) majority “very good” or “good”, a conservative
notion of precision: precisionlower. ii) at least
one “very good” or “good”, a liberal notion of
precision: precisionupper. Table 1 shows preci-
sion for PEARL-hard, PEARL-soft, NNPLB, and
HYENA, with a 0.9-confidence Wilson score in-
terval (Brown 2001). PEARL-hard outperformed
PEARL-soft and also both baselines. HYENA’s
relatively poor performance can be attributed to
the fact that its features are mainly syntactic such
as bi-grams and part-of-speech tags. Web data is
challenging, it has a lot of variations in syntac-
tic formulations. This introduces a fair amount
of ambiguity which can easily mislead syntactic
features. Leveraging semantic features as done
by PEARL could improve HYENA’s performance.
While the NNPLB method performs better than
HYENA, in comparison to PEARL-hard, there is
room for improvement. Like HYENA, NNPLB
assigns negatively correlated types to the same en-
tity. This limitation could be addressed by apply-
ing PEARL’s ILPs and probabilistic weights to the
candidate types suggested by NNPLB.

To compute inter-judge agreement we calcu-
lated Fleiss’ kappa and Cohen’s kappa κ, which
are standard measures. The usual assumption for
Fleiss’κ is that labels are categorical, so that each
disagreement counts the same. This is not the case
in our settings, where different labels may indicate
partial agreement (“good”, “very good”). There-

1493



Precisionlower Precisionupper
PEARL-hard 0.77±0.08 0.88±0.06
PEARL-soft 0.53±0.09 0.77±0.09
HYENA 0.26±0.08 0.56±0.09
NNPLB 0.46±0.09 0.68±0.09

Table 1: Comparison of PEARL to baselines.

κ F leiss Cohen

0.34 0.45

Table 2: Lower bound estimations for inter-judge
agreement kappa: Fleiss’ κ & adapted Cohen’s κ.

fore the κ values in Table 2 are lower-bound esti-
mates of agreement in our experiments; the “true
agreement” seems higher. Nevertheless, the ob-
served Fleiss κ values show that the task was fairly
clear to the turkers; values > 0.2 are generally
considered as acceptable (Landis 1977). Cohen’s
κ is also not directly applicable to our setting. We
approximated it by finding pairs of judges who as-
sessed a significant number of the same entity-type
pairs.

Precisionlower Precisionupper
Freq. mentions 0.77±0.08 0.88±0.06
All mentions 0.65±0.09 0.77±0.08

Table 3: PEARL-hard performance on a sample of
frequent entities (mention frequency≥ 20) and on
a sample of entities of all mention frequencies.

Mention Frequencies. We also studied PEARL-
hard’s performance on entities of different men-
tion frequencies. The results are shown in Ta-
ble 3. Frequently mentioned entities provide
PEARL with more evidence as they potentially oc-
cur with more patterns. Therefore, as expected,
precision when sampling over all entities drops
a bit. For such infrequent entities, PEARL does
not have enough evidence for reliable type assign-
ments.
Variations of PEARL. To quantify how various
aspects of our approach affect performance, we
studied a few variations. The first method is the
full PEARL-hard. The second method is PEARL
with no ILP (denoted No ILP), only using the
probabilistic model. The third variation is PEARL
without probabilistic weights (denoted Uniform

Figure 1: Variations of the PEARL method.

Weights). From Figure 1, it is clear that both the
ILP and the weighting model contribute signifi-
cantly to PEARL’s ability to make precise type as-
signments. Sample results from PEARL-hard are
shown in Table 4.
NDCG. For a given entity mention e, an entity-
typing system returns a ranked list of types
{t1, t2, ..., tn}. We evaluated ranking quality us-
ing the top-5 ranks for each method. These assess-
ments were aggregated into the normalized dis-
counted cumulative gain (NDCG), a widely used
measure for ranking quality. The NDCG values
obtained are 0.53, 0.16, and 0.16, for PEARL-
hard, HYENA, and NNPLB, respectively. PEARL
clearly outperforms the baselines on ranking qual-
ity, too.

6 Related Work

Tagging mentions of named entities with lexical
types has been pursued in previous work. Most
well-known is the Stanford named entity recog-
nition (NER) tagger (Finkel 2005) which assigns
coarse-grained types like person, organization, lo-
cation, and other to noun phrases that are likely to
denote entities. There is fairly little work on fine-
grained typing, notable results being (Fleischmann
2002; Rahman 2010; Ling 2012; Yosef 2012).
These methods consider type taxonomies similar
to the one used for PEARL, consisting of several
hundreds of fine-grained types. All methods use
trained classifiers over a variety of linguistic fea-
tures, most importantly, words and bigrams with
part-of-speech tags in a mention and in the textual
context preceding and following the mention. In
addition, the method of (Yosef 2012) (HYENA)
utilizes a big gazetteer of per-type words that oc-
cur in Wikipedia anchor texts. This method out-
performs earlier techniques on a variety of test

1494



Entity Inferred Type Sample Source Sentence (s)

Lochte medalist Lochte won America’s lone gold ...
Malick director ... the red carpet in Cannes for Malick’s 2011 movie ...
Bonamassa musician Bonamassa recorded Driving Towards the Daylight in Las Vegas ...

... Bonamassa opened for B.B. King in Rochester , N.Y.
Analog Man album Analog Man is Joe Walsh’s first solo album in 20 years.
Melinda Liu journalist ... in a telephone interview with journalist Melinda Liu of the Daily Beast.
RealtyTrac publication Earlier this month, RealtyTrac reported that ...

Table 4: Sample types inferred by PEARL.

cases; hence it served as one of our baselines.
Closely related to our work is the recent ap-

proach of (Lin 2012) (NNPLB) for predicting
types for out-of-KB entities. Noun phrases in the
subject role in a large collection of fact triples
are heuristically linked to Freebase entities. This
yields type information for the linked mentions.
For unlinkable entities the NNPLB method (in-
spired by (Kozareva 2011)) picks types based on
co-occurrence with salient relational patterns by
propagating types of linked entities to unlinkable
entities that occur with the same patterns. Unlike
PEARL, NNPLB does not attempt to resolve in-
consistencies among the predicted types. In con-
trast, PEARL uses an ILP with type disjointness
and correlation constraints to solve and penalize
such inconsistencies. NNPLB uses untyped pat-
terns, whereas PEARL harnesses patterns with
type signatures. Furthermore, PEARL computes
weights for candidate types based on patterns and
type signatures. Weight computations in NNPLB
are only based on patterns. NNPLB only assigns
types to entities that appear in the subject role of
a pattern. This means that entities in the object
role are not typed at all. In contrast, PEARL in-
fers types for entities in both the subject and object
role.

Type disjointness constraints have been studied
for other tasks in information extraction (Carlson
2010; Suchanek 2009), but using different formu-
lations.

7 Conclusion

This paper addressed the problem of detecting and
semantically typing newly emerging entities, to
support the life-cycle of large knowledge bases.
Our solution, PEARL, draws on a collection of
semantically typed patterns for binary relations.
PEARL feeds probabilistic evidence derived from

occurrences of such patterns into two kinds of
ILPs, considering type disjointness or type corre-
lations. This leads to highly accurate type predic-
tions, significantly better than previous methods,
as our crowdsourcing-based evaluation showed.

References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-

niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722–735,
Busan, Korea, 2007.

M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670–2676, Hyderabad, India, 2007.

K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.

Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta:
Interval Estimation for a Binomial Proportion. Sta-
tistical Science 16: pages 101–133, 2001.

R. C. Bunescu, M. Pasca: Using Encyclopedic Knowl-
edge for Named entity Disambiguation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy, 2006.

A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101–110, New
York, NY, USA, 2010.

S. Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-

1495



CoNLL), pages 708–716, Prague, Czech Republic,
2007.

A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535–1545, Edinburgh, UK, 2011.

J.R. Finkel, T. Grenager, C. Manning. 2005. Incorpo-
rating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 363–370,
Ann Arbor, Michigan, 2005.

Michael Fleischman, Eduard H. Hovy: Fine Grained
Classification of Named Entities. In Proceedings
the International Conference on Computational Lin-
guistics, COLING 2002.

X. Han, J. Zhao: Named Entity Disambiguation by
Leveraging Wikipedia Semantic Knowledge. In Pro-
ceedings of 18th ACM Conference on Information
and Knowledge Management (CIKM), pages 215 –
224,Hong Kong, China, 2009.

C. Havasi, R. Speer, J. Alonso. ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.

Sebastian Hellmann, Claus Stadler, Jens Lehmann,
Sren Auer: DBpedia Live Extraction. OTM Confer-
ences (2) 2009: 1209-1223.

J. Hoffart, M. A. Yosef, I.Bordino and H. Fuerstenau,
M. Pinkal, M. Spaniol, B.Taneva, S.Thater, Gerhard
Weikum: Robust Disambiguation of Named Entities
in Text. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 782–792, Edinburgh, UK, 2011.

J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229–232, Hyderabad, In-
dia. 2011.

J. Hoffart, F. Suchanek, K. Berberich, G. Weikum:
YAGO2: A Spatially and Temporally Enhanced
Knowledge Base from Wikipedia. Artificial Intelli-
gence 2012.

Z. Kozareva, L. Voevodski, S.-H.Teng: Class Label
Enhancement via Related Instances. EMNLP 2011:
118-128

J. R. Landis, G. G. Koch: The measurement of observer
agreement for categorical data in Biometrics. Vol.
33, pp. 159174, 1977.

C. Lee, Y-G. Hwang, M.-G. Jang: Fine-grained
Named Entity Recognition and Relation Extraction
for Question Answering. In Proceedings of the 30th
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR), pages 799–800, Amsterdam, The Nether-
lands, 2007.

T. Lin, Mausam , O. Etzioni: No Noun Phrase Left
Behind: Detecting and Typing Unlinkable Entities.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 893–903, Jeju, South Ko-
rea, 2012.

Xiao Ling, Daniel S. Weld: Fine-Grained Entity
Recognition. In Proceedings of the Conference on
Artificial Intelligence (AAAI), 2012

D. N. Milne, I. H. Witten: Learning to Link with Wi-
kipedia. In Proceedings of 17th ACM Conference on
Information and Knowledge Management (CIKM),
pages 509-518, Napa Valley, California, USA, 2008.

N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.

V. Nastase, M. Strube, B. Boerschinger, Cäcilia Zirn,
Anas Elghafari: WikiNet: A Very Large Scale
Multi-Lingual Concept Network. In Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation(LREC), Malta, 2010.

H. T. Nguyen, T. H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. In Pro-
ceedings of the IEEE International Conference on
Research, Innovation and Vision for the Future in
Computing & Communication Technologies (RIVF),
pages 247–254, Ho Chi Minh City, Vietnam, 2008.

Feng Niu, Ce Zhang, Christopher Re, Jude W. Shav-
lik: DeepDive: Web-scale Knowledge-base Con-
struction using Statistical Learning and Inference. In
the VLDS Workshop, pages 25-28, 2012.

A. Rahman, Vincent Ng: Inducing Fine-Grained Se-
mantic Classes via Hierarchical and Collective Clas-
sification. In Proceedings the International Con-
ference on Computational Linguistics (COLING),
pages 931-939, 2010.

F. M. Suchanek, G. Kasneci, G. Weikum: Yago: a
Core of Semantic Knowledge. In Proceedings of the
16th International Conference on World Wide Web
(WWW) pages, 697-706, Banff, Alberta, Canada,
2007.

1496



F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631–640,
Madrid, Spain, 2009.

P.P. Talukdar, F. Pereira: Experiments in Graph-Based
Semi-Supervised Learning Methods for Class-
Instance Acquisition. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1473-1481, 2010.

P. Venetis, A. Halevy, J. Madhavan, M. Pasca, W. Shen,
F. Wu, G. Miao, C. Wu: Recovering Semantics of
Tables on the Web. In Proceedings of the VLDB En-
dowment, PVLDB 4(9), pages, 528–538. 2011.

W. Wu, H. Li, H. Wang, K. Zhu: Probase: A
Probabilistic Taxonomy for Text Understanding. In
Proceedings of the International Conference on
Management of Data (SIGMOD), pages 481–492,
Scottsdale, AZ, USA, 2012.

M. A. Yosef, S. Bauer, J. Hoffart, M. Spaniol, G.
Weikum: HYENA: Hierarchical Type Classifica-
tion for Entity Names. In Proceedings the In-
ternational Conference on Computational Linguis-
tics(COLING), to appear, 2012.

1497


