










































Summarization of Historical Articles Using Temporal Event Clustering


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 631–635,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Summarization of Historical Articles Using Temporal Event Clustering

James Gung
Department of Computer Science

Miami University
Oxford, Ohio 45056

gungjm@muohio.edu

Jugal Kalita
Department of Computer Science

University of Colorado
Colorado Springs CO 80920
jkalita@uccs.edu

Abstract

In this paper, we investigate the use of tempo-
ral information for improving extractive sum-
marization of historical articles. Our method
clusters sentences based on their timestamps
and temporal similarity. Each resulting clus-
ter is assigned an importance score which
can then be used as a weight in traditional
sentence ranking techniques. Temporal im-
portance weighting offers consistent improve-
ments over baseline systems.

1 Introduction
Extensive research has gone into determining

which features of text documents are useful for cal-
culating the importance of sentences for extractive
summarization, as well as how to use these features
(Gupta and Lehal, 2010). Little work, however, has
considered the importance of temporal information
towards single document summarization. This is
likely because many text documents have very few
explicit time features and do not necessarily describe
topics in chronological order.

Historical articles, such as Wikipedia articles de-
scribing wars, battles, or other major events, tend to
contain many explicit time features. Historical arti-
cles also tend to describe events in chronological or-
der. In addition, historical articles tend to focus on a
single central event. The importance of other events
can then be judged by their temporal distance from
this central event. Finally, important events in an ar-
ticle will be described in greater detail, employing
more sentences than less important events.

This paper investigates the value of a temporal-
based score towards automatic summarization,
specifically focusing on historical articles. We in-
vestigate whether or not such a score can be used as
a weight in traditional sentence ranking techniques
to improve summarization quality.

2 Related Work

Event-based summarization is a recent approach
to summary generation. (Filatova and Hatzivas-
siloglou, 2004) introduced atomic events, which are
named entities connected by a relation such as a verb
or action noun. Events are selected for summary by
applying a maximum coverage algorithm to mini-
mize redundancy while maintaining coverage of the
major concepts of the document. (Vanderwende et
al., 2004) identify events as triples consisting of two
nodes and a relation. PageRank is then used to de-
termine the relative importance of these triples rep-
resented in a graph. Sentence generation techniques
are applied towards summarization.

Limited work has explored the use of temporal
information for summarization. (Lim et al., 2005)
use the explicit time information in the context of
multi-document summarization for sentence extrac-
tion and detection of redundant sentences, ordering
input documents by time. They observe that impor-
tant sentences tend to occur in in time slots contain-
ing more documents and time slots occurring at the
end and beginning of the documents set. They se-
lect topic sentences for each time slot, giving higher
weights based on the above observation.

(Wu et al., 2007) extract event elements, the ar-
guments in an event, and event terms, the actions.
Each event is placed on a timeline divided into in-
tervals consistent with the timespan of the article.
Each element and event term receives a weight cor-
responding to the total number of elements and event
terms located in each time interval the event element
or term occupies. Each sentence is scored by the to-
tal weight of event elements and terms it contains.

Clustering of events based on time has also re-
ceived little attention. (Foote and Cooper, 2003) in-
vestigate clustering towards organizing timestamped
digital photographs. They present a method that first

631



calculates the temporal similarity between all pairs
of photographs at multiple time scales. These values
are stored in a chronologically ordered matrix. Clus-
ter boundaries are determined by calculating novelty
scores for each set of similarity matrices. These are
used to form the final clusters. We adopt this clus-
tering method for clustering timestamped sentences.

3 Approach
The goal of our method is to give each sentence

in an article a temporal importance score that can
be used as a weight in traditional sentence ranking
techniques. To do this, we need to gain an idea
of the temporal structure of events in an article. A
score must then be assigned to each group corre-
sponding to the importance of the group’s timespan
to the article as a whole. Each sentence in a partic-
ular group will be assigned the same temporal im-
portance score, necessitating the use of a sentence
ranking technique to find a complete summary.

3.1 Temporal Information Extraction

We use Heideltime, a rule-based system that
uses sets of regular expressions, to extract explicit
time expressions in the article and normalize them
(Strötgen and Gertz, 2010). Events that occur be-
tween each Heideltime-extracted timestamp are as-
signed timestamps consisting of when the prior
timestamp ends and the subsequent timestamp be-
gins. The approach is naive and is described in
(Chasin et al., 2011). This method of temporal ex-
traction is not reliable, but serves the purposes of
testing as a reasonable baseline for temporal extrac-
tion systems. As the precision increases, the perfor-
mance of our system should also improve.

3.2 Temporal Clustering

To cluster sentences into temporally-related
groups, we adopt a clustering method proposed by
Foote et al. to group digital photograph collections.

Inter-sentence similarity is calculated between ev-
ery pair of sentences using Equation (1).

SK(i, j) = exp

(
−|ti − tj |

K

)
(1)

The similarity measure is based inversely on the
distance between the central time of the sentences.
Similarity scores are calculated at varying granular-
ities. If the article focuses on a central event that

Figure 1: Similarity matrices at varying k displayed as
heat maps, darker representing more similar entries

occurs over only a few hours, such as the assassi-
nation of John F. Kennedy, the best clustering will
generally be found from similarities calculated us-
ing a smaller time granularity. Conversely, articles
with central events spanning several years, such as
the American Civil War, will be clustered using sim-
ilarities calculated at larger time granularities.

The similarities are placed in a matrix and orga-
nized chronologically in order of event occurrence
time. In this matrix, entries close to the diagonal
are among the most similar and the actual diagonal
entries are maximally similar (diagonal entries cor-
respond to similarities between the same sentences).

To identify temporal event boundaries, (Foote and
Cooper, 2003) calculate novelty scores. A checker-
board kernel in which diagonal regions contain all
positive weights and off-diagonal regions contain all
negative weights is correlated along the diagonal of
the similarity matrix. The weights of each entry in
the kernel are calculated from a Gaussian function
such that the most central entries have the highest (or
lowest in the off-diagonal regions) values. The result
is maximized when the kernel is located on tempo-
ral event boundaries. In relatively uniform regions,
the positive and negative weights cancel each other
out, resulting in small novelty scores. Where there
is a gap in similarity, presumably at an event bound-
ary, off diagonal squares are dissimilar, increasing
the novelty score. In calculating novelty scores with
each set of similarity scores, we obtain a hierarchi-
cal set of boundaries. With each time granularity, we
have a potential clustering option.

In order to choose the best clustering, we calcu-
late a confidence score C for each boundary set,
then choose the clustering with the highest score, as
suggested in (Foote and Cooper, 2003). This score
is the sum of intercluster similarities (IntraS) be-
tween adjacent clusters subtracted from the sum of
intracluster (InterS) similarities as seen in Equa-
tion (4). A high confidence score suggests low inter-

632



cluster similarity and high intracluster similarity.

IntraS(BK)S =

|Bk|−1∑
l=1

bl+1∑
i,j=bl

SK(i, j)

(bl+1 − bl)2
(2)

InterS(BK)S =

|Bk|−2∑
l=1

bl+1∑
i=bl

bl+2∑
j=bl+1

SK(i, j)

(bl+1 − bl)(bl+2 − bl+1)
(3)

CS(BK) = IntraS(BK)S − InterSBK)S (4)

3.3 Estimating Clustering Paramaters

Historical articles describing wars generally have
much larger timespans than articles describing bat-
tles. Looking at battles at a broad time granularity
applicable to wars may not produce a meaningful
clustering. Thus, we should estimate the temporal
structure of each article before clustering. The time
granularity for each clustering is controlled by the
k parameter in the similarity function between sen-
tences. To find multiple clusterings, we start at a
base k, then increment k by a multiplier for each new
clustering. We calculate the base k using the stan-
dard deviation for event times in the article. Mea-
suring the spread of events in the article gives us an
estimate of what time scale we should use.

3.4 Calculating Temporal Importance
We use three novel metrics to calculate the impor-

tance of a cluster towards a summary. The first met-
ric is based on the size of the cluster (Eqn 5). This
is motivated by the assumption that more important
events will be described in greater detail, thus pro-
ducing larger clusters. The second metric (Eqn 6) is
based on the distance from the cluster’s centroid to
the centroid of the largest cluster, corresponding to
the central event of the article. This metric is moti-
vated by the assumption that historical articles have
a central event which is described in the greatest de-
tail. The third metric is based on the spread of the
cluster (Eqn 7). Clusters with large spreads are un-
likely to pertain to the same event, and should there-
fore be penalized.

Size(Ci) =
|Ci|
|Cmax|

(5)

Sim(Ci) = exp

(
−|tCiCentroid − tMaxClusterCentroid|

m

)
(6)

Spread(Ci) = exp

(
− σCi
n ∗ (tmax − tmin)

)
(7)

The parameters m and n serve to weight the impor-
tance of these measures and are assigned based on
the spread of events in an article. For n, we used the
standard deviation of event times in the article. For
m, we used the cluster similarity score from Equa-
tion (4). The three measures work in tandem to en-
sure that the importance measure will be valid even
if the largest cluster does not correspond to the cen-
tral event of the article.

3.5 Final Sentence Ranking

Each sentence is assigned a temporal importance
weight equal to the importance score of the clus-
ter to which it belongs. To find a complete ranking
of the sentences, we apply a sentence ranking tech-
nique. Any automatic summarization technique that
ranks its sentences with numerical scores can poten-
tially be augmented with our temporal importance
weight. We multiply the base scores from the rank-
ing by the associated temporal importance weights
for each sentence to find the final ranking.

WS(Vi) = (1− d) (8)

+d ∗
∑

Vj∈In(Vi)

wj,i∑
vk∈Out(Vj)wj,k

WS(Vj)

Like several graph-based methods for sentence rank-
ing for summarization (e.g., (Erkan and Radev,
2004)), we use Google’s PageRank algorithm
(Equation 8) with a damping factor d of 0.85.

Similarity(Si, Sj) =
|{wk|wk ∈ Si&wk ∈ Sj}|
log(|Si|) + log(|Sj |)

(9)
We use TextRank (Mihalcea and Tarau, 2004) in
our experiments. Our similarity measure is calcu-
lated using the number of shared named entities and
nouns between sentences as seen in equation 9. For
identification of named entities, we use Stanford
NER (Finkel et al., 2005). It is straightforward to
weight the resulting TextRank scores for each sen-
tence using their cluster’s temporal importance.

4 Experimental Results
We test on a set of 13 Wikipedia articles describ-

ing historical battles. The average article length is
189 sentences and 4,367 words. The longest ar-
ticle is 545 sentences and contains 11,563 words.
The shortest article is 51 sentences and contains

633



1,476 words. Each article has at least two human-
annotated gold standard summaries. Volunteers
were asked to choose the most important sentences
from each article. We evaluate using ROUGE-2 bi-
gram matching (Lin, 2004).

4.1 Clustering

Each Wikipedia article contains a topic sentence
stating the timespan of the main event in the article.
This provides an easy way to determine whether a
clustering is successful. If the largest cluster con-
tains the timespan of the main event described by
the topic sentence, we consider the clustering to be
successful. The articles vary greatly in length. Also,
the ratio of sentences with time features to sentences
without is considerably varied. In 92% of the arti-
cles, there were successful clusterings. An exam-
ple of an article that didn’t cluster is Nickel Grass,
where the main event was divided into two clusters.
It is of interest to note that this article had one of
lowest time feature to sentence ratios, which possi-
bly explains the poor clustering.

4.2 Temporal Importance Weighting

We test our TextRank implementation with and
without temporal importance weighting.

We observe improvements in general using the
TextRank system with temporal importance weight-
ing. The ROUGE-2 score increased by 15.72%
across all the articles. The lowest increase was
0% and the highest was 128.86%. The average
ROUGE-2 scores were 0.2575 weighted and 0.2362
unweighted, a statistically significant increase with
a 95% confidence interval of 0.0066 to 0.0360.

In particular, we see significant improvements
in articles that contain sentences TextRank ranked
highly but have events occurring at significantly dif-
ferent times than the central event of the article. Al-
though the content of these sentences is highly re-
lated to the rest of the article, they should not be
included in the summary since their events happen
nowhere near the main event temporally.

Our random ranking system, which randomly
assigns base importance scores to each sentence,
observed only small improvements, of 4.27% on
average, when augmented with temporal impor-
tance weighting. It is likely that additional human-
annotated summaries are necessary for conclusive

results.

5 Conclusions and Future Work
The novelty-based clustering method worked ex-

tremely well for our purposes. These results can
likely be improved upon using more advanced tem-
poral extraction and interpolation methods, since we
used a naive method for interpolating between time
features prone to error. The temporal importance
weighting worked very well with TextRank and rea-
sonably well with random ranking.

It may also be fairly easy to predict the success of
using this temporal weight a priori to summarization
of an article. A small ratio of explicit time features to
sentences (less than 0.15) indicates that the temporal
interpolation process may not be very accurate. The
linearity of time features is also a good indication
of the success of temporal extraction. Finally, the
spread of time features in an article is a clue to the
success of our weighting method.

Acknowledgements
Research reported here has been funded partially

by NSF grants CNS-0958576 and CNS-0851783.

References
R. Chasin, D. Woodward, and J. Kalita, 2011. Machine

Intelligence: Recent Advances, chapter Extracting and
Displaying Temporal Entities from Historical Articles.
Narosa Publishing, Delhi.

G. Erkan and D.R. Radev. 2004. Lexrank: Graph-based
lexical centrality as salience in text summarization. J.
Artif. Intell. Res., 22:457–479.

E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In ACL Workshop on Sum-
marization.

J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL, pages
363–370.

J. Foote and M. Cooper. 2003. Media segmentation us-
ing self-similarity decomposition. In SPIE, volume
5021, pages 167–175.

V. Gupta and G.S. Lehal. 2010. A survey of text sum-
marization extractive techniques. Journal of Emerging
Technologies in Web Intelligence, 2(3):258–268.

J.M. Lim, I.S. Kang, J.H. Bae, and J.H. Lee. 2005. Sen-
tence extraction using time features in multi-document
summarization. Information Retrieval Technology,
pages 82–93.

634



C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Workshop on text summariza-
tion, pages 25–26.

R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In EMNLP, pages 404–411.

J. Strötgen and M. Gertz. 2010. Heideltime: High qual-
ity rule-based extraction and normalization of tempo-
ral expressions. In 5th International Workshop on Se-
mantic Evaluation, pages 321–324.

L. Vanderwende, M. Banko, and A. Menezes. 2004.
Event-centric summary generation. Working notes of
DUC.

M. Wu, W. Li, Q. Lu, and K.F. Wong. 2007. Event-
based summarization using time features. Compu-
tational Linguistics and Intelligent Text Processing,
pages 563–574.

635


