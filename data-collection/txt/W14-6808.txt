



















































Problematic Situation Analysis and Automatic Recognition for Chinese Online Conversational System


Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 43–51,
Wuhan, China, 20-21 October 2014

Problematic Situation Analysis and Automatic Recognition for Chi-

nese Online Conversational System 

 

Yang Xiang
†
, Yaoyun Zhang, Xiaoqiang Zhou

‡
, Xiaolong Wang,  

Yang Qin
 

Key Laboratory of Network Oriented Intelligent Computation, 

Harbin Institute of Technology Shenzhen Graduate School, China 
†
windseedxy@gmail.com 

‡
xiaoqiang.jeseph@gmail.com 

 

 

 

Abstract 

Automatic problematic situation recogni-

tion (PSR) is important for an online 

conversational system to constantly im-

prove its performance. A PSR module is 

responsible of automatically identifying 

users’ un-satisfactions and then sending 

feedbacks to conversation managers. In 

this paper, we collect dialogues from a 

Chinese online chatbot, annotate the 

problematic situations and propose a 

framework to predict utterance-level 

problematic situations by integrating in-

tent and sentiment factors. Different from 

previous work, the research field is set as 

open-domain in which very few domain 

specific textual features could be used 

and the method is easy to be adapted to 

other domains. Experimental results 

show that integrating both intent and sen-

timent factors gains the best performance. 

1 Introduction 

Automatic conversational systems are computer 

programs that interact with human users based 

on their knowledge bases. Developers of conver-

sational systems devote plenty of efforts and 

time in collecting and verifying knowledge so as 

to maximize the information needs of potential 

users. However, problematic situations are inevi-

table due to several reasons (i.e. human verifiers 

would make mistakes or omissions, or quality of 

some answers couldn’t be judged without certain 

contexts). So it is necessary to equip a conversa-

tional system with an automatic PSR module to 

keep its performance constantly improved. The 

program is responsible of monitoring whether the 

dialogue or some utterances are problematic dur-

ing interactions and then providing feedbacks to 

the dialogue managers.  

Problematic situations reflect that a human us-

er is not satisfied with answers that a conversa-

tional system offers. From one perspective, some 

of these un-satisfactions can be captured through 

a human user’s dialogue acts. For example, if a 

user repeats requesting the same question or fre-

quently changes topics, it is likely that the sys-

tem provides unsatisfactory answers (Chai et al., 

2006). From another perspective, some explicit 

manners (i.e. sentiment-related expressions or 

dissatisfied feelings) that reflect the change of a 

user’s mentality would also indicate a problemat-

ic situation occurs. Some previous systems use 

surveys to capture users’ satisfactions: they let 

users to vote or evaluate whether the system has 

perfectly help them complete certain tasks (Has-

tie et al., 2002; Higashinaka et al., 2010) so as to 

collect users’ satisficing scores. However, for a 

real-world conversational application, there are 

very few users who are willing to provide this 

kind of feedbacks. 

The dialogue materials for this research come 

from a Chinese online chatting robot—BIT, 

which is developed for chatting and entertain-

ment. It also integrates real-time data query func-

tions about share price, weather report, post-code 

and telephone area code lookup. In addition to 

queries about real-time data, the corpus is totally 

open-domain and the number of topics that a dia-

logue could be related is unlimited. We annotat-

ed problematic situation labels in the utterance 

level (whether a question-answer pair is prob-

lematic/whether an answer is problematic) and 

took a deeper analysis towards different cases. 

Finally, we introduce the PSR framework. This 

framework is simple but efficient: we mapped 

the user intent and user sentiment categories to 

two groups of representative features and pre-

dicted problematic situations with supervised 

learners.  

43



Our main contributions stem from the features, 

domains and language: Unlike most previous 

researchers who considered only user intent 

(Chai et al., 2006) or took offline satisfaction 

scores provided by users as user sentiment (Has-

tie et al., 2002; Higashinaka et al., 2010), our 

method integrates intent and sentiment in an 

online manner, which automatically identifies 

these two factors and gives the managers real-

time feedbacks. The domain of the dialogue is 

open which is different from (Hastie et al., 2002; 

Chai et al., 2006). Another contribution is that 

this is the first work that solves this issue on the 

Chinese language, which has very different lan-

guage specific features and resources from Eng-

lish.  

We experimented on the corpus through 10-

fold cross validation. In each individual fold, we 

compare our method with two baselines and with 

four popular classifiers. Results show that inte-

grating both user intent and user sentiment fac-

tors gains the best performance with an average 

F1 of 0.62 (by SVM). 

Following, we first introduce related work o 

PSR from different perspectives. Introduction to 

the corpus are arranged next. The feature selec-

tions and the recognition framework are pro-

posed in Section 4. Experiments, future work and 

conclusions constitute the rest. 

2 Related Work 

Previous researches in this literature differed in 

research grains, input features and research do-

mains. 

2.1 Dialog-level vs. Utterance-level 

Most early work focused on the prediction of a 

complete dialogue. Hastie et al.(2002) predicted 

problematic dialogues from a series of DARPA 

Communicator dialogues according to user satis-

faction rates, task completion predictors and 

some interaction based features. Walker et al. 

(2002) presented their prediction model on the 

basis of information the system collected early in 

the dialogue and in real time. Oulasvirta et al. 

(2006) reported relations between users’ satisfac-

tion rates among the goal-level, concept-level, 

task-level and command-level, and captured a 

number of qualified user features. Möller et al. 

(2008) evaluated performance of different mod-

els including linear regression models and classi-

fication trees on predicting dialog-level user sat-

isfaction in three spoken dialogue datasets.  

Although the predictions of progress towards 

dialogue completion might be used as a cue to 

the dialogue manager, the results couldn’t reflect 

in which position a dialogue began to become 

problematic. Chai et al. (2006) proposed the def-

inition of user intent and incorporate a few 

matching features to predict utterance-level prob-

lematic situations (whether an immediate answer 

is satisfactory). Engelbrecht et al. (2009) employ 

the Hidden Markov Model (HMM) to model the 

whole dialogue into a sequence where each node 

of the sequence corresponds to the quality of the 

utterance. Higashinaka et al. (2010a; 2010b) also 

use HMM to model the good/bad sequence and 

testing the effects of turn-wise and overall rat-

ings. Similar spirit also exists in (Hara et al., 

2010). Support Vector Machines (SVM) are used 

by Schmitt et al. (2011) for the quality prediction 

on the CMU’s Let’s Go Bus Information system 

(Raux et al., 2006) and ASR features are com-

pared in their experiments. 

2.2 Features 

There are many factors that could affect the 

performance of judging whether a dialogue is 

problematic or not, i.e. time attributes like the 

total time of a dialogue and the time delays be-

tween utterances (Hastie et al., 2002; Walker et 

al., 2002; Möller et al., 2008), dialogue acts that 

may reflect user intents (Hastie et al., 2002) and 

users’ satisfaction ratings toward the system’s 

performance (Hastie et al., 2002). To avoid the 

side effects by Automatic Speech Recognition 

(ASR) and concentrate on the pure textual fea-

tures in dialogues, several researchers only study 

the effect of dialogue acts and users’ satisfaction 

ratings (Chai et al., 2006; Higashinaka et al., 

2010). However, it has also proved that users’ 

satisfaction ratings could not be always relied on 

since different groups of users may have differ-

ent predictive powers (i.e. from novices to ex-

perts) (Möller et al., 2005).  

2.3 Research Domains 

Another main difference among previous re-

searches is domain restriction. Specific domains 

or tasks simplify the PSR task and features are 

easy to be defined by employing domain experts. 

However, this restriction limits the ability of fea-

ture adaption from certain domains/tasks to oth-

ers. In a way, domain-specific knowledge and 

user surveys are not easy to be adapted. As far as 

we know, most previous related work restricted 

their researches on specific domains such as 

travel plan making (Hastie et al., 2002), restrict-

44



ed scenarios (Chai et al., 2006), bus schedule 

information (Schmitt et al., 2011), music infor-

mation (Hara et al., 2010), animal discussion and 

attentive listening (Higashinaka et al., 2010a; 

Higashinaka et al., 2010b). 

3 Problematic Situation Analysis 

This section will first introduce the characteris-

tics of the corpus we construct and then provide 

definitions and examples for what we have 

learned from the dialogues. 

3.1 Corpus Description 

The corpus includes 479 dialogues with totally 

3111 QA pairs. The dialogues are extracted from 

log files of the BIT robot from May to June, 

2013. Each dialogue has a specific session ID, 

identifying that the dialogues are collected from 

different users or on different time. Chatting (> 

2/3), stock real-time inquiries (<1/6) and weather 

report inquiries (<1/7) account for the largest 

proportion. The dialogues are almost original 

which contains a number of curse words (alt-

hough we have removed some too dirty words), 

facial expressions (by expressing moods through 

several punctuations such as ―:)‖), boring state-

ments (i.e. I am boring uh) as well as duplicate 

questions, indicating the irregular and informal 

characteristics of the online chatting contexts. 

The language of the corpus is Chinese, with very 

few English utterances (<1/100). The length of 

dialogues ranges from 1 to 64 QA pairs
1
. 

To collect users’ un-satisfactions, the develop-

ers place good/bad comment buttons for each 

answer but it seems to be useless (seldom users 

would click the buttons). Through observation, 

we found that users’ behaviors, including the 

type of both inquiries and responses, provide 

important cues to determine whether a user is 

satisfied with an answer or not. To show this re-

lation, two examples are listed in Table 1. 

We asked two annotators to label whether 

each answer is problematic or not. They gave 

their judgment according to whether they consid-

ered the answer provided by the chatting system 

was reasonable or not, but ignored whether it 

was not precise (i.e. a factual answer that is out 

of date). When labeling the current pair, they 

were asked to refer to the above interactions so 

as to take the context restrains into consideration. 

By using Cohen’s kappa coefficient, the inter 

                                                 
1 In this paper, we will use the notion ―utterance‖ and ―QA 

pair‖ exchangeablely. 

agreement of the two annotators is 
^

0.933oP  ,
^

0.602eP  , and 0.833   which is relatively high. 

Most conflicts occur when the two annotators 

have different under standings towards a ques-

tion or one of them couldn’t understand what a 

question really means. The conflict labels are 

resolved by a third annotator. Finally, 832 out of 

3111 pairs (26.7%) are labeled as problematic, 

indicating that the chatting system still has a lot 

of room for improvement. 

 

Table 1. Examples of problematic situations in 

BIT. 

3.2 Corpus Deeper Analysis 

According to observation, the style of sen-

tences raised by users could be roughly divided 

into two groups: questions and state-

ments(corresponding to inquiries and responses 

in the previous section). Questions are sentences 

that send inquiries to the system, indicating that 

users have some information needs. Contrarily, 

statements are sentences that reflect no infor-

mation needs, but could express complains, ex-

clamations or some other affections. We’ve also 

observed that a specific group of features is 

much related to questions while another group is 

more likely to co-occur with statements.  

Type. Question 

by User 

Answer by BIT Explanation 

S
en

ti
m

en
t-

re
la

te
d
 

你会…？
(Can you 

do…?) 

这也不会,那也不

会(I can’t do 
anything.) -- 

problematic 

The second 

question which 

is a curse sen-

tence implies 

that the user is 

not satisfied 

with the robot’s 

previous an-

swer. 

你真是个

废物 (You 
are really a 

good-for-

nothing.) 

我是比特 
(I am BIT.) 

 

In
te

n
t-

re
la

te
d
 

你儿子

呢? 
(Where’s 

your son?) 

 

你要闹哪样啊? 
(What’re you 

going to do?)  

--Problematic 

Adjacent utter-

ances. Repeat 

questions re-

flect the unsat-

isfactory of the 

user towards 

the last answer. 

Probably due to 

irrelevant an-

swers. On the 

left, the first 

answer is ir-

relavant.  

你儿子

呢?  
(Where’s 

your son?) 

天若赐我辉煌,

我必比天猖狂 
(If the God fa-

vours me, I would 

be crazier than 

the God.[Chinese 

network catch- 

words]) -- Prob-

lematic 

45



 Category Utterance Explanation 

Int. switch 1. 中国(China) The current question belongs to a different topic from the 
last one. The beginning of a new dialogue (other than 

greeting) is classified to switch. 

retry 2. 中华人民共和国 
(People’s Republic of China) 

The current question has the same idea as the last one but 

may be expressed in a different style.  

continue 3. 中国首都 
(The capital of China) 

The current question belongs to the same topic as the last 

one. The example is a detailed question about the topic 

―China‖. 

clarify 4. 中国首都在哪里? 
(Which city is the capital of China?) 

Negotiate with the system to refine or coarsen the last 

question for a clearer intent.  

Sen. greeting 早上好(Morning)/ 

亲爱的! (Honey!) 

Usually a beginning or ending of a dialog. Intimate 

speeches are also categorized into greeting. 

criticize/ 

response 
你好聪明!(You are so clever!) / 你

说对了(You are right) 

Criticism or response towards the last answer. Positive or 

negative criticisms frequently occur in the corpus, indicat-

ing users’ (un)satisfactions. 

exclaim/ 

statement 
好烦啊!(It’s so boring!) / 我喜欢
**. (I love someone.) 

Exclaims or statements that the user delivers which are not 

aiming at the chatbot. 

curse Dirty words. Explicit curse words that are inevitable in chatting dia-

logues. They sometimes show unsatisfactory, but some-

times occur dues to that the user has been ridiculed by the 

robot. 

order 讲个笑话!(Tell me a joke!) Order the system to provide information or do something. 

other 。。。/ !!!  Utterances other than the above such as punctuations or 
symbols that might show speechless(。。。), exclaiming / 
warning(!!!) or some facial expressions. 

Table 2: Examples and definitions for user intent (Int.) and user sentiment (Sen.).

Based on this intuition, we define two con-

cepts as: 

User Intent – the action of a user when rais-

ing a question, indicating that the user is execut-

ing an inquiry to the system. 

User Sentiment – the sentiment or affection 

that a user expresses through his/her utterances, 

including negative and non-negative. 

The definition of user intent follows (Chai et 

al., 2006). It mainly contains four lower-level 

types: switch, continue, retry, and clarify. Switch 

means to start a new topic or a new dialog. Con-

tinue, retry and clarify are restricted in the same 

topic, with different dialogue acts. User senti-

ment is associated with the following cases: 

greeting, criticize/response, exclaim/statement, 

curse, order and other. Other contains punctua-

tions, facial expressions and special symbols that 

are frequently used in Chinese daily chatting. 

Examples with explanations for user intent and 

sentiment are listed in Table 2.  

The annotations towards the lower-level cate-

gories have more conflicts (with an average κ 

about 0.5) than the problematic labels. The disa-

greements are solved after declaring some issues: 

1) if intent and sentiment characteristics both 

occur, label according to the type of the sentence 

(question correlates with intent and statement 

with sentiment) 2) Criticizes are towards the sys-

tem’s last response while curses are not.  

Problematic situations that originate from the 

following types are more direct and easier to un-

derstand: a) repeat the last question (retry, 

4.95%-45.45%); b) change the topic (switch, 

32.27%-32.17% with 6.97% at the beginning); c) 

try to clarify what the user intended to ask (clari-

fy, 1.29%-50%) d) negative criticisms towards 

the last answer (criticize, 13.79%-15.85%); e) 

negative words toward the robot (curse, 6.59%-

11.7%). The percentages 4.95%-45.45% stand 

for that retry accounts for 4.95% in all, and 

among all the retry cases, 45.45% are problemat-

ic. We also have the polarity (negative or non-

negative) of each user provided utterance anno-

tated and find that nearly all the negative occur 

in statements. The rest problematic situations 

mostly come from the other type (8.61%-48.13% 

with 36.19% facial expressions that the system is 

not able to recognize), continue (8.01%-28.4%), 

exclaim (10.83%-19.58%) and order (7.23%-

19.55%).   

We also notice that in several cases, although 

users hadn’t received satisfactory answers, they 

didn’t mean to negotiate with the system any 

more, indicating that many users are not patient 

enough to provide cues. These cases bring about 

difficulties for the prediction. Another special 

case we notice is from the disagreements of an-

notators, that is, sentiment and intent characteris-

tics could co-occur (i.e. repeated curses). This 

46



inspires us to synthesize both user intent and user 

sentiment attributes for an utterance. 

4 Recognition Framework for Problem-
atic Situations 

Based on a simple dependency analysis for a dia-

logue, we first map user intent and user senti-

ment into related feature groups, and then use the 

features to predict problematic situations.  

4.1 Utterance Dependency Analysis 

A dialogue could be modeled using a directed 

graph constituted by the question sequence Q 

and the answer sequence A. In the graph, a node 

stands for an utterance (question/answer), and 

edges are drawn from each Qi-1 to Qi, Qi to Ai, Ai-

1 to Ai and Ai-1 to Qi. The edges stand for depend-

encies or constrains between utterances (Figure 

1).  

 
Figure 1. Dependencies or constrains in dia-

logues 

 

In this work, the edges from Ai-1 to Qi and from 

Qi-1 to Qi are the main dependency types we re-

search. Ai-1 to Qi shows the last answer affects the 

current question in a dialog, always reflected by 

user sentiment. Constrains between questions are 

more related to user intent, i.e. the current ques-

tion would have a high similarity with the last 

one if one attempts to retry an inquiry. The fol-

lowing example typically shows the two types of 

constrains: 

Qi-1: Who did you go with yesterday?  

Ai-1: My advantage is that I am handsome. 

Qi:  Who did you go with yesterday? 

Ai:  If the God favours me, I would be crazier 

than the God. 

Qi+1: You are an idiot. 

In the example, the retry case from Qi-1 to Qi  

implies that Ai-1 should not be a good answer. 

The negative curse Qi+1 indicates that Ai may be 

problematic.  

4.2 Mapping to problematic situations 

To avoid cascade errors brought about by low-

er-level classifications, we weaken the category 

constrains by mapping the taxonomy to related 

features. The four types for user intent could be 

distinguished by features considering about simi-

larity between sentences, which descends from 

retry, clarify to continue and switch. For the six 

types in user sentiment, we define word features, 

word polarity features and pattern features to 

make the types distinguished.  

In our proposed framework, the automatic 

PSR problem is simplified into a one level binary 

classification task in which utterances are mod-

elled with general features, user intent specific 

features and sentiment specific features. General 

features are textual and non-textual features that 

have nothing to do with user intent or user sen-

timent, including: whether the answer is from the 

system’s default response list to underdeveloped 

knowledge, whether the question is a real-time 

inquiry, the number of utterances before and fol-

low (especially to distinguish the beginning or 

ending of a dialog), the similarity between the 

question and its corresponding answer.  

User intent specific features are those extract-

ed from the perspective of user intent, mainly 

related to the similarity between two adjacent 

questions. User sentiment specific features are 

those extracted from the perspective of user sen-

timent, which focus on whether a user-raised ut-

terance contains any sentiment information.  

4.3 Intent  Specific Feature Selection 

Specifically, we tag whether the current ques-

tion is retry because retry always corresponds to 

a very high similarity which is easy to be identi-

fied and many of them are related to problematic 

situations. We also use the similarity between 

two questions to distinguish the other types of 

intents. Typical features are listed in Table 3(NE 

stands for Name Entity). 

The semantic similarity measure between 

questions (labeled by * in Table 3) is based on a 

Chinese semantic web, HowNet (Dong and Dong, 

2006). The defined semantic similarity in 

HowNet is a normalized real value ([0,1]) of the 

shortest path connecting two words in the 

HowNet Concept Relation Net. Suppose two 

questions P and Q (word sequence size m and n, 

respectively), the semantic similarity between 

them is defined as: 

… … 

Qi+1 Qi Qi-1 

Ai+1 Ai Ai-1 

Q-A 

Q-Q 

A-Q 

A-A 

47



111( , ) ( )
2

nm

ji
ji

QP

ssim P Q
m n

 


 

where iP and jQ are denoted as: 

1 2max( ( , ), ( , ),..., ( , ))i i i i nP ssim P Q ssim P Q ssim P Q  

1 2max( ( , ), ( , ),..., ( , ))j j j m jQ ssim P Q ssim P Q ssim P Q  

ssim(Pi,Qm) denotes the semantic similarity of 

the ith word in Question P and the mth word in 

Question Q. If two words are the same, the simi-

larity is set to 1. 

The final similarity is defined as: 

     1 2,   ,  ,  sim P Q im P Q im P Qns ss    
nsim(P,Q) is the normalized real value of the 

number of words the two questions share. λ1 and 

λ2 are the weighted parameters (set to be 0.5,0.5 

in our experiment). 

 

Feature Description 

Exact match 

(Boolean) 

After removing punctua-

tions and stop words. 

No. of NEs 

 

By analyzing results of 

LTP. 

NE similarity 

 

The match No. and con-

tents for NEs. 

Ques. Similarity 

 

Weighted similarity 

based on lexicon and se-

mantics*. 

Ques. similarity  

without NEs 

 

Target word 

Weighted similarity 

based on lexicon and se-

mantics*. 

The target word in a 

question. 

Dependency 

similarity 

Dependency pattern simi-

larity.  

Table 3. User intent specific features. 

 

The target words, name entities and dependen-

cy trees are identified or generated by LTP (LTP, 

Liu et al., 2011). Target words are defined as the 

direct objects that the root verb governs in a de-

pendency parse tree in questioning sentences. 

The dependency similarity is computed by count-

ing the number of common dependency relations 

(normalized to [0,1]). 

4.4 Sentiment Specific Feature Selection 

User sentiment is a good reflection of a user’s 

current mood. The difficulty lie on that curse 

sentences and negative criticisms are not easy to 

be distinguished, especially for the Chinese lan-

guage where many sentences have no subjects at 

all. A solution is that considering both the similar 

key words between the last answer and the cur-

rent statement, and whether a second person pro-

noun (i.e. you/BIT) exists. 

This work models the possible relations from 

sentiments to problematic situations by defining 

a series of sentiment related features. We employ 

dictionary-based method (Zhao et al., 2010) to 

judge the polarity of words in a sentence. Typical 

features are shown in Table 4. 

 

Feature Example 

Key words 

 
弱智(stupid), 次(weak) 

Question word/ 

question mark 
为什么(why), 是什么

(what), 是谁(who), ? 

Target word 天气(weather), 人名
(person name) 

Ending word 好吗(is it ok?), 吗(modal) 

Sent. pattern 你好/真/太傻(you’re 
quite/very/too stupid) 

Part-of-Speech Adjectives, nouns 

Polarity  Polarity of a word 

Person pronoun 你(you), 比特(the name 
of the robot) 

Dependency Subject-verb-object (SBV 

and VOB by LTP) 

Table 4. User sentiment specific features. 

 

Cursing sentences or negative criticisms are 

usually expressed in certain patterns which could 

be captured through regular expressions after 

removing adverbs and modals. Adjective and 

noun words are good indicators for sentiment 

which could be looked up in sentiment dictionar-

ies. We employ two general Chinese sentiment 

dictionaries (NTUSD
2
 and HowNet) to determine 

the polarity of a word (including both nouns and 

adjectives for the consideration of both You’re a 

fool and You’re foolish.). In addition, we tag the 

sentence as negative if it only contains negative 

words (key words) after removing useless com-

ponents. Real-time inquiries are special cases 

that we should filter out through key words 

matching.  

There are also something special that we 

should consider. Suppose there are three contin-

uous pairs: A->B->C: If the question in B con-

tains negative criticism information but A is a 

real-time inquiry, we couldn’t directly judge A is 

problematic. A typical example is that the answer 

is closely related but is not precise (i.e. out of 

date). Inquiry includes questions about weather, 

                                                 
2 http://nlg18.csie.ntu.edu.tw:8080/lwku/pub1.html 

48



stock, post code, telephone and identity code in 

this system. 

In addition to un-satisfactions for not achiev-

ing the desired answer, curse/criticism sentences 

could also grow out from some other cases: (1) 

the user has been ridiculed by the system thereby 

becomes irritated; (2) the user just wants to ex-

press his/her feelings to the system through re-

peated statements. These cases are not directly 

related to problematic situations, which, however, 

haven’t been well recognized yet, hindering the 

improvements of the learners. 

4.5 The Recognition Framework 

We expected that the lower-level category in-

formation could be well modeled through fea-

tures and classifiers. General features, user intent 

specific features and user sentiment specific fea-

tures are extracted for each QA pair. Intuitively, 

the feature groups for user intent and user senti-

ment have relatively different emphasis and the 

hybrid features should naturally increase the sys-

tem’s recall.  

Suppose the sequences are Q and A, in which 

Qi is to be determined (see Figure 1). The auto-

matic PSR model is described as the follows: 

a) Pre-processing: tokenization, POS tagging, 
parsing, removing stop words, and filtering 

system specified inquires (weather, stock, 

post code, telephone and identity code); 

b) Extract sentiment specific features for Qi 
based on Qi; 

c) Extract intent specific features for Qi based 
on Qi-1 and Qi; 

d) Tag whether Qi is retry or not,  tag whether 
Qi is negative or not; 

e) Determine problematic of Qi according to 
sentiment (retry or not) and intent labels 

(negative or not), specific features (Table 3 

and 4) and general features (§4.2), as well as 

the labels for Qi-1 (retry, negative, and prob-

lematic); 

f) Post-processing: For the last QA pair in a 
dialog, if a same pair exists before and is la-

beled as problematic, Qi is labeled problem-

atic. 

The reason why we also take the labels of Qi-1 

into account is based on the fact that the labels of 

Qi-1 may help determine the current label. For 

example, if the last intent indicates a retry and 

the current question indicates a switch (a much 

lower similarity with the last one), it is very like-

ly that the user has tried at least twice but hasn’t 

received a satisfactory answer. In this case, the 

previous retry could also increase the probability 

of switch, which is helpful for the final determi-

nation.  

Post processing mainly deals with the last ut-

terance in a dialogue which doesn’t have any 

followings. 

5 Experiments and Analysis 

To prove the effectiveness of our model, we 

compare it with two baselines on four classical 

classifiers through 10-fold cross-validation. 

The baselines include the model with general 

features (GF) and intent specified features (ISF), 

the model with GF and sentiment specified fea-

tures (SSF). We name our hybrid model that with 

hybrid features as GF+ISF+SSF. We report the 

detailed performance gains of the GF+ISF+SSF 

model compared with the two baselines with in-

tense experiments on the corpus. General fea-

tures (GF) only contains little useful information 

towards our task and has very poor performance, 

therefore we didn’t set it as a baseline. We test 

the model with SVM, Naïve Bayes, Decision 

Tree and CRF so as to find out an efficient and 

stable learner for the task. 

 

GF+SSF 

 Prec. Rec. F1 

SVM 92.97  44.44  60.05  
J48 85.03  22.94  35.85  
NB 95.37  22.53  36.37  
CRF 89.20  40.06  55.01  

GF+ISF 

 Prec. Rec. F1 

SVM 93.77  43.80  59.57  
J48 88.76  21.72  34.67  
NB 96.39  23.24  36.42  
CRF 88.74  44.89  59.46  

GF+ISF+SSF 

 Prec. Rec. F1 

SVM 85.73  49.38  62.19  
J48 79.15  24.89  37.75  
NB 85.97  29.09  43.35  
CRF 91.08  45.02  60.16  

Table 5. Average performance by cross-

validation. 

 

10-fold cross validations are performed on the 

dataset. To specify, the corpus should be divided 

in the unit of dialogues rather than utterances for 

the sake of integrating sequential features (i.e. 

the previous labels). LibSVM (Chang and Lin, 

2011), Naïve Bayes and Decision Tree (J48) 

were provided by the Weka toolkit (Hall, et al.,  

49



fold Prec. Rec. F1 Percent. Best Learner im-in im-sen 

1 89.21 49.01 63.27 26.69  intent CRF -0.03 +6.1 
2 84.57 50.51 63.25 30.24  hybrid SVM +3.16 +1.74 
3 92.41 42.07 57.82 29.51  hybrid CRF +0.12 +6.38 
4 85.47 47.52 61.08 28.85  hybrid SVM +3.16 +3.16 
5 84.82 54.92 66.67 28.41  hybrid SVM +3.61 +3.61 
6 95.42 44.17 60.39 25.94  sentiment SVM -0.07 -0.07 
7 78.45 55.69 65.14 26.90  hybrid SVM +3.12 +3.72 
8 85.33 52.46 64.97 32.50  hybrid SVM +2.93 +3.42 
9 83.03 51.31 63.43 26.90  hybrid SVM +5.47 +4.0 

10 94.29 39.87 56.05 28.55  sentiment SVM +1.3 -0.2 

Table 6. Detailed results in 10-fold cross validation. 

―im-in‖ and ―im-sen‖ stand for the improvements of the hybrid model than intent and sentiment spe-

cific models. ―Percent.‖ stands for the proportion% of problematic utterances in this fold of data.

 

2009). CRF is provided by CRF++
3
, a C++ im-

plementation. Metrics of precision, recall and F1 

are used for evaluation.
 We list results for the average performance of 

cross-validation in Table 5. From the data we 

notice, all the four learning models perform well 

in precision but a little poor in recall (no matter 

for which model). And the case of Naïve Bayes 

is especially obvious. According to analysis to-

wards the output, the performance of high preci-

sion and low recall mainly due to the following 

reasons: Firstly, we select features empirically 

which may generate strong rules: if some condi-

tion is satisfied, some conclusion is drawn. Sec-

ondly, there are still a number of situations that 

we couldn’t resolve by training our models. For 

example, not all retry result in problematic situa-

tions, and sometimes the users’ intents are hard 

to understand. Finally, there are many negative 

sentences that are not related to problematic situ-

ations which could confuse the learners.  

 We also notice that SVM and CRF have much 

better results than J48 and Naïve Bayes, imply-

ing the effectiveness of the two classifiers. The 

hybrid model outperforms the two baselines 

mainly by recall, reflecting the reasonability of 

considering both user intent and sentiment.  

More evidence for the robustness of the hybrid 

features and the learners can be recognized 

through a detailed report of the cross validation 

(Table 6). From the table we observe two im-

portant things: one is that SVM performs much 

more stable than other classifiers, and CRF is not 

so good as what we have expected, considering 

there are sequential features; the other is that the 

hybrid model outperforms other baselines in 

most cases, and it also has comparative results in  

                                                 
3 http://crfpp.googlecode.com/svn/trunk/doc/index.html 

 

other cases (fold 1, 6, and 10). 

What we have also noticed is that although 

Naïve Bayes doesn’t achieve a better score in F1, 

it always performs well in precision (Table 5). Its 

characteristics of running fast, easy implemented 

and with high precision enable the developers to 

integrate the automatic recognizer in the system 

and send back precise predictions in real time.  

6 Future Work 

We left two problems for future work. Firstly, 

although we have defined lower-level categories 

for user sentiment and user intent, we failed to 

well identify each of them. More representative 

features (maybe word embedding or something 

else) should be extracted to clearly identify their 

boundaries. Secondly, there is much noise in the 

original corpus which may affect the model per-

formance. An automatic sieve should be devel-

oped to deal with the noisy information. 

7 Conclusion 

This paper analyses different problematic situ-

ations under the chatting context for the Chinese 

language. Other than previous work, we propose 

the problematic situation recognition model from 

two perspectives—user sentiment and user intent, 

and test the proposed model on a totally open-

domain corpus. Experiments verify that integrat-

ing both the two factors gains the best predicting 

result. More representative features and more 

efficient approaches will be developed for further 

improvement.   

Acknowledgements 

This work is supported in part by the National 

Natural Science Foundation of China (No. 612-

72383 and 61173075). And the foundations of 

50



Shenzhen(JC201005260118A, ZDSY20120613-

125401420, JCYJ20120613151940045, and 

JC201005260175A). 

Reference 

Joyce Y. Chai, Chen Zhang, and Tylor Baldwin. 

2006. Towards Conversational QA: Automatic 

Identification of Problematic Situations and 

User Intent. In Proceedings of COLING/ACL.  

C. C. Chang and C. J. Lin. 2011. LIBSVM : A 

Library for Support Vector Machines. ACM 

Transactions on Intelligent Systems and Tech-

nology, 2(27)1-27. 

Zhendong Dong, and Qiang Dong. 2006. 

HowNet and the Computation of Meaning. 

River Edge, NJ: World Scientific, 25-76.  

Klaus-Peter Engelbrech, et al. 2009. Modeling 

User Satisfaction with Hidden Markov Model. 

In Proceedings of the SIGDIAL 2009 Confer-

ence. Association for Computational Linguis-

tics. 

Sunao Hara, Norihide Kitaoka, and Kazuya 

Takeda. 2010. Estimation Method of User Sat-

isfaction Using N-gram-based Dialogue Histo-

ry Model for Spoken Dialogue System. LREC.  

Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-

hard Pfahringer, Peter Reutemann, Ian H. Wit-

ten. 2009. The WEKA Data Mining Software: 

An Update; SIGKDD Explorations, Volume 

11, Issue 1. 

Helen Wright Hastie, Rashmi Prasad, Marilyn-

Walker. 2002. What's the Trouble: Automati-

cally Identifying Problematic Dialogues in  

DARPA Communicator Dialogue Systems. In 

Proceedings of ACL.  

Ryuichiro Higashinaka, et al. 2010. Issues in 

Predicting User Satisfaction Transitions in Di-

alogues: Individual Differences, Evaluation 

Criteria, and Prediction Models. Spoken Dia-

logue Systems for Ambient Environments. 48-

60. 

Ryuichiro Higashinaka, et al. 2010. Modeling 

User Satisfaction Transitions in Dialogues 

from Overall Ratings. In Proceedings of the 

SIGDIAL 2010 Conference. 

Ting Liu, Wanxiang Che, Zhenghua Li. 2011. 

Language Technology Platform. Journal of 

Chinese Information Processing. 25(6): 53-62. 

Sebastian Möller, et al. 2005. Quality of Tele-

phone-based Spoken Dialogue Systems. 

Sebastian Möller, Klaus-Peter Engelbrecht, and 

Robert Schleicher. 2008. Predicting the Quali-

ty and Usability of Spoken Dialogue Services. 

Speech Communication 50.8: 730-744. 

A.Oulasvirta, S.Möller, S. Engelbrecht, et al. 

2006. The Relationship of User Errors to Per-

ceived Usability of a Spoken Dialogue System. 

In Proceedings of the 2nd ISCA/DEGA Tuto-

rial and Research Workshop on Perceptual 

Quality of Systems, Berlin. 

A. Raux, D. Bohus, B. Langner, A. W. Black, 

and M. Eskenazi. 2006. Doing Research on a 

Deployed Spoken Dialogue System: One Year 

of Let’s Go! Experience. In Proceedings of the 

International Conference on Speech and Lan-

guage Processing.  

Alexander Schmitt, Benjamin Schatz, and Wolf-

gang Minker. 2011. Modeling and Predicting 

Quality in Spoken Human Computer Interac-

tion. In Proceedings of the SIGDIAL 2011 

Conference.  

Marilyn A. Walker, et al. 2002. Automatically 

Training a Problematic Dialogue Predictor for 

a Spoken Dialogue System. Journal of Artifi-

cial Intelligence Research, Vol.16(1): 293-319.  

Yanyan Zhao, Bing Qin, and Ting Liu. 2010. 

Sentiment Analysis. Journal of Software, 

21(8): 1834-1848.DARPA Communicator Di-

alogue Systems. In Proceedings of ACL.  

51


