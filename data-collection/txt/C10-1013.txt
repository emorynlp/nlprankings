Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 107–115,

Beijing, August 2010

107

Towards an optimal weighting of context words based on distance

Bernard Brosseau-Villeneuve*#, Jian-Yun Nie*, Noriko Kando#

* Université de Montréal, Email: {brosseab, nie}@iro.umontreal.ca

# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp

Abstract

Word Sense Disambiguation (WSD) of-
ten relies on a context model or vector
constructed from the words that co-occur
with the target word within the same text
windows.
In most cases, a ﬁxed-sized
window is used, which is determined by
trial and error. In addition, words within
the same window are weighted uniformly
regardless to their distance to the target
word. Intuitively, it seems more reason-
able to assign a stronger weight to con-
text words closer to the target word. How-
ever, it is difﬁcult to manually deﬁne the
optimal weighting function based on dis-
tance. In this paper, we propose a unsu-
pervised method for determining the op-
timal weights for context words accord-
ing to their distance. The general idea is
that the optimal weights should maximize
the similarity of two context models of the
target word generated from two random
samples. This principle is applied to both
English and Japanese. The context mod-
els using the resulting weights are used
in WSD tasks on Semeval data. Our ex-
perimental results showed that substantial
improvements in WSD accuracy can be
obtained using the automatically deﬁned
weighting schema.

1 Introduction

The meaning of a word can be deﬁned by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and Véronis,
1998; Navigli, 2009).
In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context

count(x,t)

words. For example, under the unigram bag-of-
words assumption, this means building p(x|t) =
(cid:2)x(cid:2) count(x(cid:2),t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and
t should co-occur within a window of up to k
words or sentences. The bounds are usually se-
lected in an ad-hoc fashion to maximize system
performance. Occurrences inside the window of-
ten weight the same without regard to their po-
sition. This is counterintuitive.
Indeed, a word
closer to the target word generally has a greater
semantic constraint on the target word than a more
distant word.
It is however difﬁcult to deﬁne
the optimal weighting function manually. To get
around this, some systems add positional features
for very close words. In information retrieval, to
model the strength of word relations, some studies
have proposed non-uniform weighting methods of
context words, which decrease the importance of
more distant words in the context vector. How-
ever, the weighting functions are deﬁned manu-
ally. It is unclear that these functions can best cap-
ture the impact of the context words on the target
word.

In this paper, we propose an unsupervised
method to automatically learn the optimal weight
of a word according to its distance to the target
word. The general principle used to determine
such weight is that, if we randomly determine
two sets of windows containing the target word
from the same corpus, the meaning – or mixture
of meanings for polysemic words – of the target
word in the two sets should be similar. As the con-
text model – a probability distribution for the con-
text words – determines the meaning of the target
word, the context models generated from the two
sets should also be similar. The weights of con-
text words at different distance are therefore de-

108

termined so as to maximize the similarity of con-
text models generated from the two sets of sam-
ples. In this paper, we propose a gradient descent
method to ﬁnd the optimal weights. We will see
that the optimal weighting functions are different
from those used in previous studies. Experimenta-
tion on Semeval-2007 English and Semeval-2010
Japanese lexical sample task data shows that im-
provements can be attained using the resulting
weighting functions on simple Naïve Bayes (NB)
systems in comparison to manually selected func-
tions. This result validates the general principle
we propose in this paper.

The remainder of this paper is organized as fol-
lows:
typical uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3.
In Section 4 to 6,
we show experimental results on English and
Japanese WSD. We conclude in Section 7 with
discussion and further possible extensions.

2 Uses of text windows

Modeling the distribution of words around one
target word, which we call context model, has
many uses. For instance, one can use it to deﬁne
a co-occurrence-based stemmer (Xu and Croft,
1998), which uses window co-occurrence statis-
tics to calculate the best equivalence classes for a
group of word forms. In the study of Xu and Croft,
they suggest using windows of up to 100 words.
Context models are also widely used in WSD.
For example, top performing systems on English
WSD tasks in Semeval-2007, such as NUS-ML
(Cai et al., 2007), all made use of bag-of-words
features around the target word. In this case, they
found that the best results can be achieved using a
window size of 3.

Both systems limit the size of their windows for
different purposes. The former uses a large size in
order to model the topic of the documents contain-
ing the word rather than the word’s meaning. The
latter would limit the size because bag-of-words
features further from the target word would not be
sufﬁciently related to its meaning (Ide and Véro-
nis, 1998). We see that there is a compromise be-
tween taking fewer, highly related words, or tak-
ing more, lower quality words. However, there is
no principled way to determine the optimal size

of windows. The size is determined by trial and
error.

A more questionable aspect in the above sys-
tems is that for bag-of-words features, all words
in a window are given equal weights. This is
counterintuitive. One can easily understand that
a context word closer to the target word gener-
ally imposes a stronger constraint on the meaning
of the latter, than a more distant context word. It
is then reasonable to deﬁne a weighting function
that decreases along with distance. Several studies
in information retrieval (IR) have proposed such
functions to model the strength of dependency be-
tween words. For instance, Gao et al.
(2002)
proposed an exponential decay function to capture
the strength of dependency between words. This
function turns out to work better than the uniform
weighting in the IR experiments.

Song and Bruza (2003) used a ﬁxed-size slid-
ing window to determine word co-occurrences.
This is equivalent to deﬁne a linear decay func-
tion for context words. The context vectors de-
ﬁned this way are used to estimate similarity be-
tween words. A use of the resulting similarity in
query expansion in IR turned out to be successful
(Bai et al., 2005).

In a more recent study, Lv and Zhai (2009) eval-
uated several kernel functions to determine the
weights of context words according to distance,
including Gaussian kernel, cosine kernel, and so
on. As for the exponential and linear decaying
functions, all these kernel functions have ﬁxed
shapes, which are determined manually.

Notice that the above functions have only been
tested in IR experiments.
It is not clear how
these functions perform in WSD. More impor-
tantly, all the previous studies have investigated
only a limited number of weighting functions for
context words. Although some improvements us-
ing these functions have been observed in IR, it
is not clear whether the functions can best capture
the true impact of the context words on the mean-
ing of the target word. Although the proposed
functions comply with the general principle that
closer words are more important than more dis-
tant words, no principled way has been proposed
to determine the particular shape of the function
for different languages and collections.

109

In this paper, we argue that there is indeed a hid-
den weighting function that best capture the im-
pact of context words, but the function cannot be
deﬁned manually. Rather, the best function should
be the one that emerges naturally from the data.
Therefore, we propose an unsupervised method to
discover such a function based on the following
principle:
the context models for a target word
generated from two random samples should be
similar. In the next section, we will deﬁne in detail
how this principle is used.

3 Computing weights for distances

In this section, we present our method for choos-
ing how much a word occurrence should count in
the context model according to its distance to the
target word. In this study, for simplicity, we as-
sume that all word occurrences at a given distance
count equally in the context model. That is, we
ignore other features such as POS-tags, which are
used in other studies on WSD.

Let C be a corpus, W a set of text windows for
the target word w, cW,i,x the count of occurrences
of word x at distance i in W , cW,i the sum of these
counts, and αi the weight put on one word occur-
rence at distance i. Then,

PM L,W (x) = (cid:2)i αicW,i,x
(cid:2)i αicW,i

(1)

is the maximum likelihood estimator for x in the
context model of w. To counter the zero probabil-
ity problem, we apply Dirichlet smoothing with
the collection language model as a prior:

PDir,W (x) = (cid:2)i αicW,i,x + μW P (x|C)

(cid:2)i αicW,i + μW

The pseudo-count μW can be a constant, or can be
found by using Newton’s method, maximizing the
log likelihood via leave-one-out estimation:

(2)

• Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.

• We want to ﬁnd α(cid:3) that maximizes the sim-
ilarity of the models obtained from the two
sets, by minimizing their mutual cross en-
tropy:

l(α) = H(PM L,A, PDir,B) + (3)

H(PM L,B, PDir,A)

In other words, we want αi to represent how much
an occurrence at distance i models the context
better than the collection language model, whose
counts are weighted by the Dirichlet parameter.
We hypothesize that target words occur in limited
contexts, and as we get farther from them, the pos-
sibilities become greater, resulting in sparse and
less related counts. Since two different sets of the
same word are essentially noisy samples of the
same distribution, the weights maximizing their
mutual generation probabilities should model this
phenomenon.

One may wonder why we do not use a distri-
bution similarity metric such as Kullback–Leibler
(KL) divergence or Information Radius (IRad).
The reason is that with enough word occurrences
(big windows or enough samples), the most sim-
ilar distributions are found with uniform weights,
when all word counts are used. KL divergence
is especially problematic as, since it requires
smoothing, the weights will converge to the de-
generate weights α = 0, where only the identical
smoothing counts remain. Entropy minimization
is therefore needed in the objective function.

To determine the optimal weight of αi, we pro-
pose a simple gradient descent minimizing (3)
over α. The following are the necessary deriva-
tives:

L−1(μ|W,C) =
(cid:2)i(cid:2)x∈V αicW,i,x log αicW,i,x−αi+μP (x|C)
(cid:2)j αj cW,j−αi+μ

The general process, which we call automatic
Dirichlet smoothing, is similar to that described
in (Zhai and Lafferty, 2002).

To ﬁnd the best weights for our model we pro-

pose the following process:

∂l
∂αi

=

∂H(PM L,A, PDir,B)

∂αi

∂H(PM L,B, PDir,A)

+

∂αi

∂H(cid:3)PM L,W , PDir,(T−W )(cid:4)

∂αi

=

110

−(cid:5)x∈V(cid:6) ∂PM L,W (x)

∂αi

∂PDir,(T−W )(x)

∂αi

∂PM L,W (x)

∂αi

∂PDir,W (x)

∂αi

=

=

log PDir,(T−W )(x)+

×

PM L,W (x)

cW,i,x − PM L,W (x)cW,i

PDir,(T−W )(x)(cid:7)
(cid:2)j αjcW,j
cW,i,x − PDir,W (x)cW,i
(cid:2)j αjcW,j + μW

We use stochastic gradient descent: one word is
selected randomly, it’s gradient is computed, a
small gradient step is done and the process is re-
peated. A pseudo-code of the process can be
found in Algorithm 1.

Algorithm 1 LearnWeight(C, η, )

α ← 1k
repeat

T ←{Get windows for next word}
(A, B) ←RandomPartition(T )
for W in A, B do
PM L,W ←MakeML(W ,α)
μW ←ComputePseudoCount(W ,C)
PDir,W ←MakeDir( PM L,W , μW ,C)
end for
grad ← ∇H(PM L,A, PDir,B) +
∇H(PM L,B, PDir,A)
α ← α − η grad
(cid:3)grad(cid:3)
until ∃αi < 
return α/ max{αi}

Now, as the objective function would eventu-
ally go towards putting nearly all weight on α1,
we hypothesize that the farthest distances should
have a near-zero contribution, and determine the
stop criterion as having one weight go under a
small threshold. Alternatively, a control set of
held out words can be used to observe the progress
of the objective function or the gradient length.
When more and more weight is put on the few
closest positions, the objective function and gra-
dient depends on less counts and will become less
stable. This can be used as a stop criterion.

The above weight learning process is applied
on an English collection and a Japanese collection

with η =  = 0.001, and μ = 1000. In the next
sections, we will describe both resulting weight-
ing functions in the context of WSD experiments.

4 Classiﬁers for supervised WSD tasks

Since we use the same systems for both English
and Japanese experiments, we will brieﬂy discuss
the used classiﬁers in this section. In both tasks,
the objective is to maximize WSD accuracy on
held-out data, given that we have a set of training
text passages containing a sense-annotated target
word.

The ﬁrst of our baselines, the Most Frequent
Sense (MFS) system always selects the most fre-
quent sense in the training set. It gives us a lower
bound on system accuracies.

Naïve Bayes (NB) classiﬁers score classes us-
ing the Bayes formula under a feature indepen-
dence assumption. Let w be the target word in a
given window sample to be classiﬁed, the scoring
formula for sense class S is:

Score(w, S) = P (S)PT ar(w|S)λT ar×
(cid:8)xi∈context(w) PCon(xi|S)λConαdist(xi)

where dist(xi) is the distance between the context
word xi and the target word w. The target word
being an informative feature present in all sam-
ples, we use it in a target word language model
PT ar. The surrounding words are summed in the
context model PCon as shown in equation (1). As
we can see with the presence of α in the equation,
the scoring follows the same weighting scheme as
we do when accumulating counts, since the sam-
ples to classify follow the same distribution as the
training ones. Also, when a language model uses
automatic Dirichlet smoothing, the impact of the
features against the prior is controlled with the
manual parameters λT ar or λCon. When a man-
ual smoothing parameter is used, it also handles
impact control. Our systems use the following
weight functions:

Uniform: αi = 11≤i≤δ, where δ is a window size

and 1 the indicator function.

Linear: αi = max{0, 1 − (i − 1)δ}, where δ is

the decay rate.

111

Exponential: αi = e−(i−1)δ, where δ is the ex-

ponential parameter.

Learned: αi is the weight learned as shown pre-

viously.

The parameters for NB systems are identical for
all words of a task and were selected by exhaustive
search, maximizing leave-one-out accuracy on the
training set. For each language model, we tried
Laplace, manual Dirichlet and automatic Dirichlet
smoothing.

For the sake of comparison, also we provide a
Support Vector Machine (SVM) classiﬁer, which
produces the best results in Semeval 2007. We
used libSVM with a linear kernel, and regular-
ization parameters were selected via grid search
maximizing leave-one-out accuracy on the train-
ing set. We tested the following windows limits:
all words in sample, current sentence, and various
ﬁxed window sizes. We used the same features
as the NB systems, testing Boolean, raw count,
log-of-counts and counts from weight functions
representations. Although non-Boolean features
had good leave-one-out precision on the training
data, since SVM does not employ smoothing, only
Boolean features kept good results on test data, so
our SVM baseline uses Boolean features.

5 WSD experiments on Semeval-2007

English Lexical Sample

The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al.,
2007). The task is to maximize WSD accuracy on
a selected set of polysemous words, 65 verbs and
35 nouns, for which passages were taken from the
WSJ Tree corpus. Passages contain a couple of
sentences around the target word, which is manu-
ally annotated with a sense taken from OntoNotes
(Hovy et al., 2006). The sense inventory is quite
coarse, with an average of 3.6 senses per word.
Instances count are listed in Table 1.

Train
8988
13293
22281

Test
2292
2559
4851

Verb
Noun
Total

Total
11280
15852

Table 1: Number of instances in the ELS data

Figure 1: Weight curve for AP88-90

Since there are only 100 target words and in-
stances are limited in the Semeval collection, we
do not have sufﬁcient samples to estimate the op-
timal weights for context words. Therefore, we
used the AP88-90 corpus of the TREC collection
(CD 1 & 2) in our training process. The AP col-
lection contains 242,918 documents. Since our
classiﬁers use word stems, the collection was also
stemmed with the Porter stemmer and sets of win-
dows were built for all word stems. To get near-
uniform counts in all distances, only full win-
dows with a size of 100, which was considered
big enough without any doubt, were kept. In order
to get more samples, windows to the right and to
the left were separated. For each target word, we
used 1000 windows. A stoplist of the top 10 fre-
quent words was used, but place holders were left
in the windows to preserve the distances. Mul-
tiple consecutive stop words (ex: “of the”) were
merged, and the target word stem, being the same
for all samples of a set, was ignored in the con-
struction of context models. The AP collection re-
sults in 32,650 target words containing 5,870,604
windows. The training process described in Sec-
tion 3 is used to determine the best weights of con-
text words. Figure 1 shows the ﬁrst 40 elements
of the resulting weighting function curve.

As we can see, the curve is neither exponen-
tial, linear, or any of the forms used by Lv and
Zhai. Its form is rather similar to x−δ, or rather
log−1(δ + x) minus some constant. The decrease

112

Cross-Val (%) Test set (%)
System
MFS
78.66
Uniform NB 86.04
85.53
SVM
Linear NB
86.89
Exp. NB
87.80
Learned NB 88.46

77.76
84.52
85.03
85.71
86.23
86.70

Table 2: WSD accuracy on Semeval-2007 ELC

rate is initially very high and then reduces as it
becomes closer to zero. This long tail is not
present in any of the previously suggested func-
tions. The large difference between the above op-
timal weighting function and the functions used
in previous studies would indicate that the latter
are suboptimal. Also, as we can see, the rela-
tion between context words and the target word
is mostly gone after a few words. This would
motivate the commonly used very small windows
when using a uniform weights, since using a big-
ger window would further widen the gap between
the used weight and the optimal ones.

Now for the system settings, the context words
were processed the same way as the external cor-
pus. The target word was used without stemming
but had the case stripped. The NB systems used
the concatenation of the AP collection and the
Semeval data for the collection language model.
This is motivated by the fact that the Semeval data
is not balanced: it contains only a small number of
passages containing the target words. This makes
words related to them unusually frequent. The
class priors used an absolute discounting of 0.5 on
class counts. Uniform NB uses a window of size 4,
a Laplace smoothing of 0.65 on PT ar and an au-
tomatic Dirichlet with λCon = 0.7 on PCon. Lin-
ear NB has δ = 0.135, uses a Laplace smoothing
of 0.85 on PT ar and an automatic Dirichlet with
λCon = 0.985 on PCon. Exp NB has δ = 0.27,
uses a Laplace smoothing of 2.8 on PT ar and an
automatic Dirichlet with λCon = 1.01 on PCon.
The SVM system uses a window of size 3. Our
system, Learned NB uses a Laplace smoothing of
1.075 on PT ar, and an automatic Dirichlet with
λCon = 1.025 on PCon. The results on WSD are
listed in Table 2. WSD accuracy is measured by

the proportion of correctly disambiguated words
among all the word samples. The cross-validation
is performed on the training data with leave-one-
out and is shown as a hint of the capacity of the
models. A randomization test comparing Expo-
nential NB and Learned NB gives a p-value of
0.0508, which is quite good considering the exten-
sive trials used to select the exponential parameter
in comparison to a single curve computed from a
different corpus. This performance is comparable
to the current state of the art. It outperforms most
of the systems participating in the task (Pradhan et
al., 2007). Out of 14 systems, the best results had
accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (*
indicates post-competition submissions). Notice
that most previous systems used SVM with ad-
ditional features such as local collocations, posi-
tional word features and POS tags. Our approach
only uses bag-of-words in a Naïve Bayes classi-
ﬁer. Therefore, the performance of our method is
sub-optimal. With additional features and better
classiﬁcation methods, we can expect that better
performance can be obtained. In future work, we
will investigate the applications of SVM with our
new term weighting scheme, together with addi-
tional types of features.

6 WSD experiments on Semeval-2010

Japanese Lexical Sample

The Semeval-2010 Japanese WSD task (Okumura
et al., 2010) consists of 50 polysemous words
for which examples were taken from the BCCWJ
corpus (Maekawa, 2008).
It was manually seg-
mented, POS-tagged, and annotated with senses
taken from the Iwanami Kokugo dictionary. The
selected words have 50 samples for both the train-
ing and test set. The task is identical to the ELS
of the previous experiment.

Since the data was again insufﬁcient to com-
pute the optimal weighting curve, we used the
Mainichi-2005 corpus of NTCIR-8. We tried to
reproduce the same kind of segmentation as the
training data by using the Chasen parser with Uni-
Dic, which nevertheless results in different word
segments as the training data. For the corpus and
Semeval data, conjugations (setsuzoku-to, jodô-
shi, etc.), particles (all jo-shi), symbols (blanks,
kigô, etc.), and numbers were stripped. When a

113

Cross-Val (%) Test set (%)
System
75.23
MFS
SVM
82.55
Uniform NB 82.47
Linear NB
82.63
Exp. NB
82.68
Learned NB 82.67

68.96
74.92
76.16
76.48
76.44
76.52

Table 3: WSD accuracy on Semeval-2010 JWSD

smoothing with λCon = 65 on PCon. Exp NB
has δ = 2.675, uses a manual Dirichlet smooth-
ing of 6.5 on PT ar and a manual Dirichlet of 70
on PCon. The SVM system uses a window size of
1 and Boolean features. Learned NB used a man-
ual Dirichlet smoothing of 4 for PT ar and auto-
matic Dirichlet smoothing with λCon = 0.6 for
PCon. We believe this smoothing is beneﬁcial
only on this system because it uses more words
(the long tail), that makes the estimation of the
pseudo-count more accurate. Results on WSD are
listed in Table 3. As we can see, the difference be-
tween the NB models is less substantial than for
English. This may be due to differences in the
segmentation parameters of our external corpus:
we used the human-checked segmentation found
in the Semeval data for classiﬁcation, but used a
parser to segment our external corpus for weight
learning. We are positive that the Chasen parser
with the UniDic dictionary was used to create the
initial segmentation in the Semeval data, but there
may be differences in versions and the initial seg-
mentation results were further modiﬁed manually.
Another reason for the results could be that the
systems use almost the same weights: Uniform
NB and SVM both used windows of size 1, and
the Japanese curve is steeper than the English one,
making the context model account to almost only
immediately adjacent words. So, even if our con-
text model contains more context words at larger
distances, their weights are very low. This makes
all context model quite similar. Nevertheless, we
still observe some gain in WSD accuracy. These
results show that the curves work as expected even
in different languages. However, the weighting
curve is strongly language-dependent.
It could
also be collection-dependent – we will investigate

Figure 2: Weight curve for Mainichi 2005

base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(Chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.

As we can see, the general form of the curve
is similar to that of the English collection, but
is steeper. This suggests that the meaning of
Japanese words can be determined using only
the closest context words. Words further than a
few positions away have very small impact on
the target word. This can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-Verb-
Complement
language, Japanese is considered
Subject-Complement-Verb. Verbs, mostly found
at the end of a sentence, can be far apart from their
subject, and vice versa. The window distance is
therefore less useful to capture the relatedness in
Japanese than in English since Japanese has more
non-local dependencies.

The Semeval Japanese test data being part of a
balanced corpus, untagged occurrences of the tar-
get words are plenty, so we can beneﬁt from using
the collection-level counts for smoothing. Uni-
form NB uses a window of size 1, manual Dirich-
let smoothing of 4 for PT ar and 90 for the PCon.
Linear NB has δ = 0.955, uses a manual Dirichlet
smoothing of 6.25 on PT ar and manual Dirichlet

114

this aspect in the future, using different collec-
tions.

7 Conclusions

The deﬁnition of context vector and context model
is critical in WSD. In previous studies in IR, de-
caying weight along with distance within a text
window have been proposed. However, the de-
caying functions are deﬁned manually. Although
some of the functions produced better results than
the uniform weighting, there is no evidence show-
ing that these functions best capture the impact
of the context words on the meaning of the tar-
get word. This paper proposed an unsupervised
method for ﬁnding optimal weights for context
words according to their distance to the target
word. The general idea was to ﬁnd the weights
that best ﬁt the data, in such a way that the context
models for the same target word generated from
two random windows samples become similar. It
is the ﬁrst time that this general principle is used
for this purpose. Our experiments on WSD in En-
glish and Japanese suggest the validity of the prin-
ciple.

In this paper, we limited context models to bag-
of-words features, excluding additional features
such as POS-tags. Despite this simple type of fea-
ture and the use of a simple Naïve Bayes classiﬁer,
the WSD accuracy we obtained can rival the other
state-of-the-art systems with more sophisticated
features and classiﬁcation algorithms. This result
indicates that a crucial aspect in WSD is the def-
inition of an appropriate context model, and our
weighting method can generate more reasonable
weights of context words than using a predeﬁned
decaying function.

Our experiments also showed that the optimal
weighting function is language-dependent. We
obtained two different functions for English and
Japanese, although their general shapes are simi-
lar. In fact, the optimal weighting function reﬂects
the linguistic properties: as dependent words in
Japanese can be further away from the target word
due to its linguistic structure, the optimal weight-
ing quickly decays, meaning that we can rely less
on distant context words. This also shows a lim-
itation of this study: distance is not the sole cri-
terion to determine the impact of a context word.

Other factors, such as POS-tag and syntactic de-
pendency, can play an important role in the con-
text model. These additional factors are comple-
mentary to the distance criterion and our approach
can be extended to include such additional fea-
tures. This extension is part of our future work.

Another limitation of straight window distance
is that all words introduce the same distance, re-
gardless of their nature.
In our experiments, to
make the distance a more sensible metric, we
merged consecutive stop words in one placeholder
token. The idea behind this it that some words,
such as stop words, should introduce less distance
than others. On the opposite, we can easily un-
derstand that tokens such as commas, full stops,
parentheses and paragraph should introduce a big-
ger distance than regular words. We could there-
fore use a congruence score for a word, an indi-
cator showing on average how much what comes
before is similar to what comes after the word.

Also, we have combined our weighting schema
with NB classiﬁer. Other classiﬁers such as SVM
could lead to better results. The utilization of our
new weighting schema with SVM is another fu-
ture work.

Finally, the weights computed with our method
has been used in WSD tasks. The weights could
be seen as the expected strength of relation be-
tween two words in a document according to their
distance. The consideration of word relationships
in documents and queries is one of the endeav-
ors in current research in IR. The new weighting
schema could be easily integrated with a depen-
dency model in IR. We plan to perform such inte-
gration in the future.

Acknowledgments

The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work.
This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientiﬁc
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.

115

Zhai, ChengXiang and John Lafferty. 2002. Two-
stage language models for information retrieval. In
SIGIR ’02 Proceedings, pages 49–56, New York,
NY, USA. ACM.

References
Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In CIKM ’05 Proceedings, pages 688–695,
New York, NY, USA. ACM.

Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features.
In SemEval ’07 Proceedings,
pages 249–252, Morristown, NJ, USA. Association
for Computational Linguistics.

Cheung, Percy and Pascale Fung. 2004. Translation
disambiguation in mixed language queries. Ma-
chine Translation, 18(4):251–273.

Gao, Jianfeng, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations.
In SI-
GIR ’02 Proceedings, pages 183–190, New York,
NY, USA. ACM.

Ide, Nancy and Jean Véronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2–40.

Lv, Yuanhua and ChengXiang Zhai. 2009. Positional
language models for information retrieval. In SIGIR
’09 Proceedings, pages 299–306, New York, NY,
USA. ACM.

Maekawa, Kikuo.

2008. Compilation of the bal-
anced corpus of contemporary written japanese in
the kotonoha initiative (invited paper).
In ISUC
’08 Proceedings, pages 169–172, Washington, DC,
USA. IEEE Computer Society.

Navigli, Roberto. 2009. Word sense disambiguation:

A survey. ACM Comput. Surv., 41(2):1–69.

Okumura, Manabu, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ’10 Proceedings. Asso-
ciation for Computational Linguistics.

Pradhan, Sameer S., Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words.
In Se-
mEval ’07 Proceedings, pages 87–92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.

Song, D. and P. D. Bruza. 2003. Towards context sen-
sitive information inference. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 54(4):321–334.

Xu, Jinxi and W. Bruce Croft.

1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61–81.

