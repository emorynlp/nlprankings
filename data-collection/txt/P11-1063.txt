










































Incremental Syntactic Language Models for Phrase-based Translation


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 620–631,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

Incremental Syntactic Language Models for Phrase-based Translation
Lane Schwartz

Air Force Research Laboratory
Wright-Patterson AFB, OH USA

lane.schwartz@wpafb.af.mil

Chris Callison-Burch
Johns Hopkins University

Baltimore, MD USA
ccb@cs.jhu.edu

William Schuler
Ohio State University
Columbus, OH USA

schuler@ling.ohio-state.edu

Stephen Wu
Mayo Clinic

Rochester, MN USA
wu.stephen@mayo.edu

Abstract

This paper describes a novel technique for in-
corporating syntactic knowledge into phrase-
based machine translation through incremen-
tal syntactic parsing. Bottom-up and top-
down parsers typically require a completed
string as input. This requirement makes it dif-
ficult to incorporate them into phrase-based
translation, which generates partial hypothe-
sized translations from left-to-right. Incre-
mental syntactic language models score sen-
tences in a similar left-to-right fashion, and are
therefore a good mechanism for incorporat-
ing syntax into phrase-based translation. We
give a formal definition of one such linear-
time syntactic language model, detail its re-
lation to phrase-based decoding, and integrate
the model with the Moses phrase-based trans-
lation system. We present empirical results
on a constrained Urdu-English translation task
that demonstrate a significant BLEU score im-
provement and a large decrease in perplexity.

1 Introduction

Early work in statistical machine translation viewed
translation as a noisy channel process comprised of
a translation model, which functioned to posit ad-
equate translations of source language words, and
a target language model, which guided the fluency
of generated target language strings (Brown et al.,

This research was supported by NSF CAREER/PECASE
award 0447685, NSF grant IIS-0713448, and the European
Commission through the EuroMatrixPlus project. Opinions, in-
terpretations, conclusions, and recommendations are those of
the authors and are not necessarily endorsed by the sponsors or
the United States Air Force. Cleared for public release (Case
Number 88ABW-2010-6489) on 10 Dec 2010.

1990). Drawing on earlier successes in speech
recognition, research in statistical machine trans-
lation has effectively used n-gram word sequence
models as language models.

Modern phrase-based translation using large scale
n-gram language models generally performs well
in terms of lexical choice, but still often produces
ungrammatical output. Syntactic parsing may help
produce more grammatical output by better model-
ing structural relationships and long-distance depen-
dencies. Bottom-up and top-down parsers typically
require a completed string as input; this requirement
makes it difficult to incorporate these parsers into
phrase-based translation, which generates hypothe-
sized translations incrementally, from left-to-right.1

As a workaround, parsers can rerank the translated
output of translation systems (Och et al., 2004).

On the other hand, incremental parsers (Roark,
2001; Henderson, 2004; Schuler et al., 2010; Huang
and Sagae, 2010) process input in a straightforward
left-to-right manner. We observe that incremental
parsers, used as structured language models, pro-
vide an appropriate algorithmic match to incremen-
tal phrase-based decoding. We directly integrate in-
cremental syntactic parsing into phrase-based trans-
lation. This approach re-exerts the role of the lan-
guage model as a mechanism for encouraging syn-
tactically fluent translations.

The contributions of this work are as follows:

• A novel method for integrating syntactic LMs
into phrase-based translation (§3)

• A formal definition of an incremental parser for
1While not all languages are written left-to-right, we will

refer to incremental processing which proceeds from the begin-
ning of a sentence as left-to-right.

620



statistical MT that can run in linear-time (§4)

• Integration with Moses (§5) along with empiri-
cal results for perplexity and significant transla-
tion score improvement on a constrained Urdu-
English task (§6)

2 Related Work

Neither phrase-based (Koehn et al., 2003) nor hierar-
chical phrase-based translation (Chiang, 2005) take
explicit advantage of the syntactic structure of either
source or target language. The translation models in
these techniques define phrases as contiguous word
sequences (with gaps allowed in the case of hierar-
chical phrases) which may or may not correspond
to any linguistic constituent. Early work in statisti-
cal phrase-based translation considered whether re-
stricting translation models to use only syntactically
well-formed constituents might improve translation
quality (Koehn et al., 2003) but found such restric-
tions failed to improve translation quality.

Significant research has examined the extent to
which syntax can be usefully incorporated into sta-
tistical tree-based translation models: string-to-tree
(Yamada and Knight, 2001; Gildea, 2003; Imamura
et al., 2004; Galley et al., 2004; Graehl and Knight,
2004; Melamed, 2004; Galley et al., 2006; Huang
et al., 2006; Shen et al., 2008), tree-to-string (Liu
et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi
and Huang, 2008; Huang and Mi, 2010), tree-to-tree
(Abeillé et al., 1990; Shieber and Schabes, 1990;
Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan
et al., 2006; Nesson et al., 2006; Zhang et al., 2007;
DeNeefe et al., 2007; DeNeefe and Knight, 2009;
Liu et al., 2009; Chiang, 2010), and treelet (Ding
and Palmer, 2005; Quirk et al., 2005) techniques
use syntactic information to inform the translation
model. Recent work has shown that parsing-based
machine translation using syntax-augmented (Zoll-
mann and Venugopal, 2006) hierarchical translation
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al., 2009). In
contrast to the above tree-based translation models,
our approach maintains a standard (non-syntactic)
phrase-based translation model. Instead, we incor-
porate syntax into the language model.

Traditional approaches to language models in

speech recognition and statistical machine transla-
tion focus on the use of n-grams, which provide a
simple finite-state model approximation of the tar-
get language. Chelba and Jelinek (1998) proposed
that syntactic structure could be used as an alterna-
tive technique in language modeling. This insight
has been explored in the context of speech recogni-
tion (Chelba and Jelinek, 2000; Collins et al., 2005).
Hassan et al. (2007) and Birch et al. (2007) use
supertag n-gram LMs. Syntactic language models
have also been explored with tree-based translation
models. Charniak et al. (2003) use syntactic lan-
guage models to rescore the output of a tree-based
translation system. Post and Gildea (2008) investi-
gate the integration of parsers as syntactic language
models during binary bracketing transduction trans-
lation (Wu, 1997); under these conditions, both syn-
tactic phrase-structure and dependency parsing lan-
guage models were found to improve oracle-best
translations, but did not improve actual translation
results. Post and Gildea (2009) use tree substitution
grammar parsing for language modeling, but do not
use this language model in a translation system. Our
work, in contrast to the above approaches, explores
the use of incremental syntactic language models in
conjunction with phrase-based translation models.

Our syntactic language model fits into the fam-
ily of linear-time dynamic programming parsers de-
scribed in (Huang and Sagae, 2010). Like (Galley
and Manning, 2009) our work implements an in-
cremental syntactic language model; our approach
differs by calculating syntactic LM scores over all
available phrase-structure parses at each hypothesis
instead of the 1-best dependency parse.

The syntax-driven reordering model of Ge (2010)
uses syntax-driven features to influence word order
within standard phrase-based translation. The syn-
tactic cohesion features of Cherry (2008) encour-
ages the use of syntactically well-formed translation
phrases. These approaches are fully orthogonal to
our proposed incremental syntactic language model,
and could be applied in concert with our work.

3 Parser as Syntactic Language Model in
Phrase-Based Translation

Parsing is the task of selecting the representation τ̂
(typically a tree) that best models the structure of

621



ÀÁÂÃÄÅÆ

〈s〉
τ̃0

ÊÁÂÃÄÅÆ

〈s〉 the
τ̃11

ÊÁÂÃÄÅÆ

〈s〉 that
τ̃12

ÀËÂÃÄÅÆ

〈s〉 president
τ̃13

. . .

ÊËÂÃÄÅÆ

the president

τ̃21

ÊËÂÃÄÅÆ

that president

τ̃22

ÀËÂÍÄÅÆ

president Friday

τ̃23

. . .

ÊËÌÃÄÅÆ

president meets

τ̃31

ÊËÌÃÄÅÆ

Obama met
τ̃32

. . .

Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German
sentence Der Präsident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the
application of a translation option, and includes the source sentence coverage vector, target language n-
gram state, and syntactic language model state τ̃th . Hypothesis combination is also shown, indicating
where lattice paths with identical n-gram histories converge. We use the English translation The president
meets the board on Friday as a running example throughout all Figures.

sentence e, out of all such possible representations
τ . This set of representations may be all phrase
structure trees or all dependency trees allowed by
the parsing model. Typically, tree τ̂ is taken to be:

τ̂ = argmax
τ

P(τ | e) (1)

We define a syntactic language model P(e) based
on the total probability mass over all possible trees
for string e. This is shown in Equation 2 and decom-
posed in Equation 3.

P(e) =
∑
τ∈τ

P(τ, e) (2)

P(e) =
∑
τ∈τ

P(e | τ)P(τ) (3)

3.1 Incremental syntactic language model

An incremental parser processes each token of in-
put sequentially from the beginning of a sentence to
the end, rather than processing input in a top-down
(Earley, 1968) or bottom-up (Cocke and Schwartz,
1970; Kasami, 1965; Younger, 1967) fashion. After

processing the tth token in string e, an incremen-
tal parser has some internal representation of possi-
ble hypothesized (incomplete) trees, τt. The syntac-
tic language model probability of a partial sentence
e1...et is defined:

P(e1...et) =
∑
τ∈τt

P(e1...et | τ)P(τ) (4)

In practice, a parser may constrain the set of trees
under consideration to τ̃t, that subset of analyses or
partial analyses that remains after any pruning is per-
formed. An incremental syntactic language model
can then be defined by a probability mass function
(Equation 5) and a transition function δ (Equation
6). The role of δ is explained in §3.3 below. Any
parser which implements these two functions can
serve as a syntactic language model.

P(e1...et) ≈ P(τ̃ t) =
∑
τ∈τ̃ t

P(e1...et | τ)P(τ) (5)

δ(et, τ̃ t−1)→ τ̃ t (6)

622



3.2 Decoding in phrase-based translation

Given a source language input sentence f , a trained
source-to-target translation model, and a target lan-
guage model, the task of translation is to find the
maximally probable translation ê using a linear
combination of j feature functions h weighted ac-
cording to tuned parameters λ (Och and Ney, 2002).

ê = argmax
e

exp(
∑
j

λjhj(e,f)) (7)

Phrase-based translation constructs a set of trans-
lation options — hypothesized translations for con-
tiguous portions of the source sentence — from a
trained phrase table, then incrementally constructs a
lattice of partial target translations (Koehn, 2010).
To prune the search space, lattice nodes are orga-
nized into beam stacks (Jelinek, 1969) according to
the number of source words translated. An n-gram
language model history is also maintained at each
node in the translation lattice. The search space
is further trimmed with hypothesis recombination,
which collapses lattice nodes that share a common
coverage vector and n-gram state.

3.3 Incorporating a Syntactic Language Model

Phrase-based translation produces target language
words in an incremental left-to-right fashion, gen-
erating words at the beginning of a translation first
and words at the end of a translation last. Similarly,
incremental parsers process sentences in an incre-
mental fashion, analyzing words at the beginning of
a sentence first and words at the end of a sentence
last. As such, an incremental parser with transition
function δ can be incorporated into the phrase-based
decoding process in a straightforward manner. Each
node in the translation lattice is augmented with a
syntactic language model state τ̃t.

The hypothesis at the root of the translation lattice
is initialized with τ̃ 0, representing the internal state
of the incremental parser before any input words are
processed. The phrase-based translation decoding
process adds nodes to the lattice; each new node
contains one or more target language words. Each
node contains a backpointer to its parent node, in
which τ̃ t−1 is stored. Given a new target language
word et and τ̃ t−1, the incremental parser’s transi-
tion function δ calculates τ̃ t. Figure 1 illustrates

S

NP

DT

The

NN

president

VP

VP

VB

meets

NP

DT

the

NN

board

PP

IN

on

NP

Friday

Figure 2: Sample binarized phrase structure tree.

S

S/NP

S/PP

S/VP

NP

NP/NN

DT

The

NN

president

VP

VP/NN

VP/NP

VB

meets

DT

the

NN

board

IN

on

NP

Friday

Figure 3: Sample binarized phrase structure tree af-
ter application of right-corner transform.

a sample phrase-based decoding lattice where each
translation lattice node is augmented with syntactic
language model state τ̃t.

In phrase-based translation, many translation lat-
tice nodes represent multi-word target language
phrases. For such translation lattice nodes, δ will
be called once for each newly hypothesized target
language word in the node. Only the final syntac-
tic language model state in such sequences need be
stored in the translation lattice node.

4 Incremental Bounded-Memory Parsing
with a Time Series Model

Having defined the framework by which any in-
cremental parser may be incorporated into phrase-
based translation, we now formally define a specific
incremental parser for use in our experiments.

The parser must process target language words
incrementally as the phrase-based decoder adds hy-
potheses to the translation lattice. To facilitate this
incremental processing, ordinary phrase-structure
trees can be transformed into right-corner recur-

623



r1t−1

r2t−1

r3t−1

s1t−1

s2t−1

s3t−1

r1t

r2t

r3t

s1t

s2t

s3t

et−1 et

. . .

. . .

. . .

. . .

Figure 4: Graphical representation of the depen-
dency structure in a standard Hierarchic Hidden
Markov Model with D = 3 hidden levels that can
be used to parse syntax. Circles denote random vari-
ables, and edges denote conditional dependencies.
Shaded circles denote variables with observed val-
ues.

sive phrase structure trees using the tree transforms
in Schuler et al. (2010). Constituent nontermi-
nals in right-corner transformed trees take the form
of incomplete constituents cη/cηι consisting of an
‘active’ constituent cη lacking an ‘awaited’ con-
stituent cηι yet to come, similar to non-constituent
categories in a Combinatory Categorial Grammar
(Ades and Steedman, 1982; Steedman, 2000). As
an example, the parser might consider VP/NN as a
possible category for input “meets the”.

A sample phrase structure tree is shown before
and after the right-corner transform in Figures 2
and 3. Our parser operates over a right-corner trans-
formed probabilistic context-free grammar (PCFG).
Parsing runs in linear time on the length of the input.
This model of incremental parsing is implemented
as a Hierarchical Hidden Markov Model (HHMM)
(Murphy and Paskin, 2001), and is equivalent to a
probabilistic pushdown automaton with a bounded
pushdown store. The parser runs in O(n) time,
where n is the number of words in the input. This
model is shown graphically in Figure 4 and formally
defined in §4.1 below.

The incremental parser assigns a probability
(Eq. 5) for a partial target language hypothesis, using
a bounded store of incomplete constituents cη/cηι.
The phrase-based decoder uses this probability value
as the syntactic language model feature score.

4.1 Formal Parsing Model: Scoring Partial
Translation Hypotheses

This model is essentially an extension of an HHMM,
which obtains a most likely sequence of hidden store
states, ŝ1..D1..T , of some length T and some maxi-
mum depth D, given a sequence of observed tokens
(e.g. generated target language words), e1..T , using
HHMM state transition model θA and observation
symbol model θB (Rabiner, 1990):

ŝ1..D1..T
def
= argmax

s1..D1..T

T∏
t=1

PθA(s
1..D
t | s1..Dt−1 )·PθB(et | s

1..D
t )

(8)
The HHMM parser is equivalent to a probabilis-
tic pushdown automaton with a bounded push-
down store. The model generates each successive
store (using store model θS) only after considering
whether each nested sequence of incomplete con-
stituents has completed and reduced (using reduc-
tion model θR):

PθA(s
1..D
t | s1..Dt−1 )

def
=∑

r1t ..r
D
t

D∏
d=1

PθR(r
d
t | rd+1t sdt−1s

d−1
t−1 )

· PθS(s
d
t | rd+1t rdt sdt−1s

d−1
t ) (9)

Store elements are defined to contain only the
active (cη) and awaited (cηι) constituent categories
necessary to compute an incomplete constituent
probability:

sdt
def
= 〈cη, cηι〉 (10)

Reduction states are defined to contain only the
complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a
flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):

rdt
def
= 〈crdt , frdt 〉 (11)

The model probabilities for these store elements
and reduction states can then be defined (from Mur-
phy and Paskin 2001) to expand a new incomplete
constituent after a reduction has taken place (frdt =
1; using depth-specific store state expansion model
θS-E,d), transition along a sequence of store elements

624



s11

s21

s31

e1

t=1

r12

r22

r32

s12

s22

s32

e2

t=2

r13

r23

r33

s13

s23

s33

e3

t=3

r14

r24

r34

s14

s24

s34

e4

t=4

r15

r25

r35

s15

s25

s35

e5

t=5

r16

r26

r36

s16

s26

s36

e6

t=6

r17

r27

r37

s17

s27

s37

e7

t=7

r18

r28

r38

=DT

=NP/NN
=NP

=NN

=S/VP

=VB

=S/VP

=VP/NP

=DT

=VP/NN

=S/VP

=NN

=VP

=S/PP

=IN

=S/NP

=S

=NP

=The =president =meets =the =board =on =Friday

Figure 5: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The
president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized
right-corner tree structure of Figure 3.

if no reduction has taken place (frdt =0; using depth-
specific store state transition model θS-T,d): 2

PθS(s
d
t | rd+1t rdt sdt−1s

d−1
t )

def
=

if frd+1t =1, frdt =1 : PθS-E,d(s
d
t | sd−1t )

if frd+1t =1, frdt =0 : PθS-T,d(s
d
t | rd+1t rdt sdt−1s

d−1
t )

if frd+1t =0, frdt =0 : Jsdt = sdt−1K
(12)

and possibly reduce a store element (terminate
a sequence) if the store state below it has re-
duced (frd+1t = 1; using depth-specific reduction
model θR,d):

PθR(r
d
t | rd+1t sdt−1s

d−1
t−1 )

def
={

if frd+1t =0 : Jrdt = r⊥K
if frd+1t =1 : PθR,d(r

d
t | rd+1t sdt−1 s

d−1
t−1 )

(13)

where r⊥ is a null state resulting from the failure of
an incomplete constituent to complete, and constants
are defined for the edge conditions of s0t and r

D+1
t .

Figure 5 illustrates this model in action.
These pushdown automaton operations are then

refined for right-corner parsing (Schuler, 2009),
distinguishing active transitions (model θS-T-A,d, in
which an incomplete constituent is completed, but
not reduced, and then immediately expanded to a

2An indicator function J·K is used to denote deterministic
probabilities: JφK = 1 if φ is true, 0 otherwise.

new incomplete constituent in the same store el-
ement) from awaited transitions (model θS-T-W,d,
which involve no completion):

PθS-T,d(s
d
t | rd+1t rdt sdt−1s

d−1
t )

def
={

if rdt 6=r⊥ : PθS-T-A,d(sdt | s
d−1
t r

d
t )

if rdt =r⊥ : PθS-T-W,d(s
d
t | sdt−1r

d+1
t )

(14)

PθR,d(r
d
t | rd+1t sdt−1s

d−1
t−1 )

def
={

if crd+1t 6=xt : Jrdt = r⊥K
if crd+1t =xt : PθR-R,d(r

d
t | sdt−1s

d−1
t−1 )

(15)

These HHMM right-corner parsing operations are
then defined in terms of branch- and depth-specific
PCFG probabilities θG-R,d and θG-L,d: 3

3Model probabilities are also defined in terms of left-
progeny probability distribution EθG-RL∗,d which is itself defined
in terms of PCFG probabilities:

EθG-RL∗,d(cη
0→ cη0 ...)

def
=

∑
cη1

PθG-R,d(cη → cη0 cη1) (16)

EθG-RL∗,d(cη
k→ cη0k0 ...)

def
=

∑
c
η0k

EθG-RL∗,d(cη
k−1→ cη0k ...)

·
∑
c
η0k1

PθG-L,d(cη0k → cη0k0 cη0k1) (17)

EθG-RL∗,d(cη
∗→ cηι ...)

def
=

∞∑
k=0

EθG-RL∗,d(cη
k→ cηι ...) (18)

EθG-RL∗,d(cη
+→ cηι ...)

def
= EθG-RL∗,d(cη

∗→ cηι ...)

− EθG-RL∗,d(cη
0→ cηι ...) (19)

625



ÊËÌÃÄÅÆ

president meets

τ̃31

. . .

ÊËÌÃÄÏÐ

the board

τ̃51

. . .

s13

s23

s33

e3

r14

r24

r34

s14

s24

s34

e4

r15

r25

r35

s15

s25

s35

e5=meets =the =board

Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation op-
tion the board of source phrase den Vorstand. Syntactic language model state τ̃31 contains random variables
s1..33 ; likewise τ̃51 contains s

1..3
5 . The intervening random variables r

1..3
4 , s

1..3
4 , and r

1..3
5 are calculated by

transition function δ (Eq. 6, as defined by §4.1), but are not stored. Observed random variables (e3..e5) are
shown for clarity, but are not explicitly stored in any syntactic language model state.

• for expansions:

PθS-E,d(〈cηι, c
′
ηι〉 | 〈−, cη〉)

def
=

EθG-RL∗,d(cη
∗→ cηι ...) · Jxηι = c′ηι = cηιK (20)

• for awaited transitions:

PθS-T-W,d(〈cη, cηι1〉 | 〈c
′
η, cηι〉 cηι0)

def
=

Jcη = c′ηK ·
PθG-R,d(cηι → cηι0 cηι1)

EθG-RL∗,d(cηι
0→ cηι0 ...)

(21)

• for active transitions:

PθS-T-A,d(〈cηι, cηι1〉 | 〈−, cη〉 cηι0)
def
=

EθG-RL∗,d(cη
∗→ cηι ...) · PθG-L,d(cηι → cηι0 cηι1)

EθG-RL∗,d(cη
+→ cηι0 ...)

(22)

• for cross-element reductions:

PθR-R,d(cηι,1 | 〈−, cη〉 〈c
′
ηι,−〉)

def
=

Jcηι = c′ηιK ·
EθG-RL∗,d(cη

0→ cηι ...)

EθG-RL∗,d(cη
∗→ cηι ...)

(23)

• for in-element reductions:

PθR-R,d(cηι,0 | 〈−, cη〉 〈c
′
ηι,−〉)

def
=

Jcηι = c′ηιK ·
EθG-RL∗,d(cη

+→ cηι ...)

EθG-RL∗,d(cη
∗→ cηι ...)

(24)

We use the parser implementation of (Schuler,
2009; Schuler et al., 2010).

5 Phrase Based Translation with an
Incremental Syntactic Language Model

The phrase-based decoder is augmented by adding
additional state data to each hypothesis in the de-

626



coder’s hypothesis stacks. Figure 1 illustrates an ex-
cerpt from a standard phrase-based translation lat-
tice. Within each decoder stack t, each hypothe-
sis h is augmented with a syntactic language model
state τ̃th . Each syntactic language model state is
a random variable store, containing a slice of ran-
dom variables from the HHMM. Specifically, τ̃th
contains those random variables s1..Dt that maintain
distributions over syntactic elements.

By maintaining these syntactic random variable
stores, each hypothesis has access to the current
language model probability for the partial transla-
tion ending at that hypothesis, as calculated by an
incremental syntactic language model defined by
the HHMM. Specifically, the random variable store
at hypothesis h provides P(τ̃th) = P(e

h
1..t, s

1..D
1..t ),

where eh1..t is the sequence of words in a partial hy-
pothesis ending at h which contains t target words,
and where there are D syntactic random variables in
each random variable store (Eq. 5).

During stack decoding, the phrase-based decoder
progressively constructs new hypotheses by extend-
ing existing hypotheses. New hypotheses are placed
in appropriate hypothesis stacks. In the simplest
case, a new hypothesis extends an existing hypothe-
sis by exactly one target word. As the new hypothe-
sis is constructed by extending an existing stack ele-
ment, the store and reduction state random variables
are processed, along with the newly hypothesized
word. This results in a new store of syntactic ran-
dom variables (Eq. 6) that are associated with the
new stack element.

When a new hypothesis extends an existing hy-
pothesis by more than one word, this process is first
carried out for the first new word in the hypothe-
sis. It is then repeated for the remaining words in
the hypothesis extension. Once the final word in
the hypothesis has been processed, the resulting ran-
dom variable store is associated with that hypoth-
esis. The random variable stores created for the
non-final words in the extending hypothesis are dis-
carded, and need not be explicitly retained.

Figure 6 illustrates this process, showing how a
syntactic language model state τ̃51 in a phrase-based
decoding lattice is obtained from a previous syn-
tactic language model state τ̃31 (from Figure 1) by
parsing the target language words from a phrase-
based translation option.

In-domain Out-of-domain
LM WSJ 23 ppl ur-en dev ppl

WSJ 1-gram 1973.57 3581.72
WSJ 2-gram 349.18 1312.61
WSJ 3-gram 262.04 1264.47
WSJ 4-gram 244.12 1261.37
WSJ 5-gram 232.08 1261.90
WSJ HHMM 384.66 529.41

Interpolated WSJ
5-gram + HHMM 209.13 225.48

Giga 5-gram 258.35 312.28
Interp. Giga 5-gr
+ WSJ HHMM 222.39 123.10

Interp. Giga 5-gr
+ WSJ 5-gram 174.88 321.05

Figure 7: Average per-word perplexity values.
HHMM was run with beam size of 2000. Bold in-
dicates best single-model results for LMs trained on
WSJ sections 2-21. Best overall in italics.

Our syntactic language model is integrated into
the current version of Moses (Koehn et al., 2007).

6 Results

As an initial measure to compare language models,
average per-word perplexity, ppl, reports how sur-
prised a model is by test data. Equation 25 calculates
ppl using log base b for a test set of T tokens.

ppl = b
−logbP(e1...eT )

T (25)

We trained the syntactic language model from
§4 (HHMM) and an interpolated n-gram language
model with modified Kneser-Ney smoothing (Chen
and Goodman, 1998); models were trained on sec-
tions 2-21 of the Wall Street Journal (WSJ) tree-
bank (Marcus et al., 1993). The HHMM outper-
forms the n-gram model in terms of out-of-domain
test set perplexity when trained on the same WSJ
data; the best perplexity results for in-domain and
out-of-domain test sets4 are found by interpolating

4In-domain is WSJ Section 23. Out-of-domain are the En-
glish reference translations of the dev section , set aside in
(Baker et al., 2009) for parameter tuning, of the NIST Open
MT 2008 Urdu-English task.

627



Sentence Moses +HHMM +HHMM
length beam=50 beam=2000

10 0.21 533 1143
20 0.53 1193 2562
30 0.85 1746 3749
40 1.13 2095 4588

Figure 8: Mean per-sentence decoding time (in sec-
onds) for dev set using Moses with and without syn-
tactic language model. HHMM parser beam sizes
are indicated for the syntactic LM.

HHMM and n-gram LMs (Figure 7). To show the
effects of training an LM on more data, we also re-
port perplexity results on the 5-gram LM trained for
the GALE Arabic-English task using the English Gi-
gaword corpus. In all cases, including the HHMM
significantly reduces perplexity.

We trained a phrase-based translation model on
the full NIST Open MT08 Urdu-English translation
model using the full training data. We trained the
HHMM and n-gram LMs on the WSJ data in order
to make them as similar as possible. During tuning,
Moses was first configured to use just the n-gram
LM, then configured to use both the n-gram LM and
the syntactic HHMM LM. MERT consistently as-
signed positive weight to the syntactic LM feature,
typically slightly less than the n-gram LM weight.

In our integration with Moses, incorporating a
syntactic language model dramatically slows the de-
coding process. Figure 8 illustrates a slowdown
around three orders of magnitude. Although speed
remains roughly linear to the size of the source sen-
tence (ruling out exponential behavior), it is with an
extremely large constant time factor. Due to this
slowdown, we tuned the parameters using a con-
strained dev set (only sentences with 1-20 words),
and tested using a constrained devtest set (only sen-
tences with 1-20 words). Figure 9 shows a statis-
tically significant improvement to the BLEU score
when using the HHMM and the n-gram LMs to-
gether on this reduced test set.

7 Discussion

This paper argues that incremental syntactic lan-
guages models are a straightforward and appro-

Moses LM(s) BLEU
n-gram only 18.78
HHMM + n-gram 19.78

Figure 9: Results for Ur-En devtest (only sentences
with 1-20 words) with HHMM beam size of 2000
and Moses settings of distortion limit 10, stack size
200, and ttable limit 20.

priate algorithmic fit for incorporating syntax into
phrase-based statistical machine translation, since
both process sentences in an incremental left-to-
right fashion. This means incremental syntactic LM
scores can be calculated during the decoding pro-
cess, rather than waiting until a complete sentence is
posited, which is typically necessary in top-down or
bottom-up parsing.

We provided a rigorous formal definition of in-
cremental syntactic languages models, and detailed
what steps are necessary to incorporate such LMs
into phrase-based decoding. We integrated an incre-
mental syntactic language model into Moses. The
translation quality significantly improved on a con-
strained task, and the perplexity improvements sug-
gest that interpolating between n-gram and syntactic
LMs may hold promise on larger data sets.

The use of very large n-gram language models is
typically a key ingredient in the best-performing ma-
chine translation systems (Brants et al., 2007). Our
n-gram model trained only on WSJ is admittedly
small. Our future work seeks to incorporate large-
scale n-gram language models in conjunction with
incremental syntactic language models.

The added decoding time cost of our syntactic
language model is very high. By increasing the
beam size and distortion limit of the baseline sys-
tem, future work may examine whether a baseline
system with comparable runtimes can achieve com-
parable translation quality.

A more efficient implementation of the HHMM
parser would speed decoding and make more exten-
sive and conclusive translation experiments possi-
ble. Various additional improvements could include
caching the HHMM LM calculations, and exploiting
properties of the right-corner transform that limit the
number of decisions between successive time steps.

628



References
Anne Abeillé, Yves Schabes, and Aravind K. Joshi.

1990. Using lexicalized tree adjoining grammars for
machine translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics.

Anthony E. Ades and Mark Steedman. 1982. On the
order of words. Linguistics and Philosophy, 4:517–
558.

Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). SCALE summer workshop final report, Hu-
man Language Technology Center Of Excellence.

Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9–16.

Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).

Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, John Lafferty,
Robert Mercer, and Paul Roossin. 1990. A statisti-
cal approach to machine translation. Computational
Linguistics, 16(2):79–85.

Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of the Ninth Ma-
chine Translation Summit of the International Associ-
ation for Machine Translation.

Ciprian Chelba and Frederick Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In Pro-
ceedings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics, pages 225–
231.

Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283–332.

Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University.

Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 72–80.

David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263–270.

David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452.

John Cocke and Jacob Schwartz. 1970. Program-
ming languages and their compilers. Technical report,
Courant Institute of Mathematical Sciences, New York
University.

Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 507–514.

Brooke Cowan, Ivona Kuc̆erová, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 232–241.

Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 727–736.

Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755–763.

Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 541–548.

Jay Earley. 1968. An efficient context-free parsing algo-
rithm. Ph.D. thesis, Department of Computer Science,
Carnegie Mellon University.

Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In The Companion Vol-
ume to the Proceedings of 41st Annual Meeting of
the Association for Computational Linguistics, pages
205–208.

Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773–781.

Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In

629



Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 273–
280.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961–
968.

Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 849–857.

Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 80–87.

Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 105–112.

Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 288–295.

James Henderson. 2004. Lookahead in deterministic
left-corner parsing. In Proceedings of the Workshop
on Incremental Parsing: Bringing Engineering and
Cognition Together, pages 26–33.

Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 273–283.

Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086.

Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
conference of the Association for Machine Translation
in the Americas.

Kenji Imamura, Hideo Okuma, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Example-based machine transla-
tion based on syntactic transfer with statistical models.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 99–105.

Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, pages 675–685.

T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory.

Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 127–133.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 177–180.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609–616.

Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 704–711.

Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558–566.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.

I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Meeting of
the Association for Computational Linguistics, pages
653–660.

Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 206–214.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 192–199.

630



Kevin P. Murphy and Mark A. Paskin. 2001. Linear time
inference in hierarchical HMMs. In Proceedings of
Neural Information Processing Systems, pages 833–
840.

Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Pro-
ceedings of the 7th Biennial conference of the Associ-
ation for Machine Translation in the Americas, pages
128–137.

Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302.

Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
161–168.

Matt Post and Daniel Gildea. 2008. Parsers as language
models for statistical machine translation. In Proceed-
ings of the Eighth Conference of the Association for
Machine Translation in the Americas, pages 172–181.

Matt Post and Daniel Gildea. 2009. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning.

Arjen Poutsma. 1998. Data-oriented translation. In
Ninth Conference of Computational Linguistics in the
Netherlands.

Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 271–279.

Lawrence R. Rabiner. 1990. A tutorial on hid-
den Markov models and selected applications in
speech recognition. Readings in speech recognition,
53(3):267–296.

Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249–276.

William Schuler, Samir AbdelRahman, Tim Miller, and
Lane Schwartz. 2010. Broad-coverage incremental
parsing using human-like memory constraints. Com-
putational Linguistics, 36(1):1–30.

William Schuler. 2009. Positive results for parsing with a
bounded stack using a model-based right-corner trans-

form. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 344–352.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 577–585.

Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree adjoining grammars. In Proceedings of the 13th
International Conference on Computational Linguis-
tics.

Stuart M. Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Interna-
tional Workshop on Tree Adjoining Grammar and Re-
lated Formalisms.

Mark Steedman. 2000. The syntactic process. MIT
Press/Bradford Books, Cambridge, MA.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523–530.

D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n cubed. Information
and Control, 10(2):189–208.

Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Seng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of the 11th Machine Translation Summit
of the International Association for Machine Transla-
tion, pages 535–542.

Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138–141.

631


