



















































Topic Models + Word Alignment = A Flexible Framework for Extracting Bilingual Dictionary from Comparable Corpus


Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Topic Models + Word Alignment = A Flexible Framework for Extracting
Bilingual Dictionary from Comparable Corpus

Xiaodong Liu, Kevin Duh and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology

8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{xiaodong-l,kevinduh,matsu}@is.naist.jp

Abstract
We propose a flexible and effective frame-
work for extracting a bilingual dictionary
from comparable corpora. Our approach
is based on a novel combination of topic
modeling and word alignment techniques.
Intuitively, our approach works by con-
verting a comparable document-aligned
corpus into a parallel topic-aligned cor-
pus, then learning word alignments us-
ing co-occurrence statistics. This topic-
aligned corpus is similar in structure to the
sentence-aligned corpus frequently used in
statistical machine translation, enabling us
to exploit advances in word alignment re-
search. Unlike many previous work, our
framework does not require any language-
specific knowledge for initialization. Fur-
thermore, our framework attempts to han-
dle polysemy by allowing multiple trans-
lation probability models for each word.
On a large-scale Wikipedia corpus, we
demonstrate that our framework reliably
extracts high-precision translation pairs on
a wide variety of comparable data condi-
tions.

1 Introduction

A machine-readable bilingual dictionary plays a
very important role in many natural language pro-
cessing tasks. In machine translation (MT), dic-
tionaries can help in the domain adaptation set-
ting (Daume III and Jagarlamudi, 2011). In
cross-lingual information retrieval (CLIR), dictio-
naries serve as efficient means for query trans-
lation (Resnik et al., 2011). Many other multi-
lingual applications also rely on bilingual dictio-
naries as integral components.

One approach for building a bilingual dictio-
nary resource uses parallel sentence-aligned cor-
pora. This is often done in the context of Statis-
tical MT, using word alignment algorithms such
as the IBM models (Brown et al., 1993; Och and
Ney, 2003). Unfortunately, parallel corpora may
be scarce for certain language-pairs or domains of
interest (e.g., medical and microblog).

Thus, the use of comparable corpora for bilin-
gual dictionary extraction has become an active
research topic (Haghighi et al., 2008; Vulić et
al., 2011). Here, a comparable corpus is defined
as collections of document pairs written in dif-
ferent languages but talking about the same topic
(Koehn, 2010), such as interconnected Wikipedia
articles. The challenge with bilingual dictionary
extraction from comparable corpus is that exist-
ing word alignment methods developed for paral-
lel corpus cannot be directly applied.

We believe there are several desiderata for bilin-
gual dictionary extraction algorithms:

1. Low Resource Requirement: The approach
should not rely on language-specific knowl-
edge or a large scale seed lexicon.

2. Polysemy Handling: One should handle the
fact that a word form may have multiple
meanings, and such meanings may be trans-
lated differently.

3. Scalability: The approach should run effi-
ciently an massively large-scale datasets.

Our framework addresses the above desired
points by exploiting a novel combination of topic
models and word alignment, as shown in Figure 1.
Intuitively, our approach works by first converting
a comparable document-aligned corpus into a par-

212



Figure 1: Proposed Framework

allel topic-aligned corpus, then apply word align-
ment methods to model co-occurence within top-
ics. By employing topic models, we avoid the
need for seed lexicon and operate purely in the
realm of unsupervised learning. By using word
alignment on topic model results, we can easily
model polysemy and extract topic-dependent lexi-
cons.

Specifically, let we be an English word and
wf be a French word. One can think of tradi-
tional bilingual dictionary extraction as obtaining
(we, wf ) pairs in which the probability p(we|wf )
or p(wf |we) is high. Our approach differs
by modeling p(we|wf , t) or p(wf |we, t) instead,
where t is a topic. The key intuition is that it is
easier to tease out the translation of a polysemous
word e given p(wf |we, t) rather than p(wf |we).
A word may be polysemous, but given a topic,
there is likely a one-to-one correspondence for the
most appropriate translation. For example, un-
der the simple model p(wf |we), the English word
“free“ may be translated into the Japanese word
自由 (as in free speech) or 無料 (as in free
beer) with equal 0.5 probability; this low proba-
bility may cause both translation pairs to be re-
jected by the dictionary extraction algorithm. On
the other hand, given p(wf |we, t), where t is “pol-
itics“ or “shopping“, we can allow high probabili-
ties for both words depending on context.

Our contribution is summarized as follows:

• We propose a bilingual dictionary extrac-
tion framework that simultaneously achieves
all three of the desiderata: low resource re-
quirement, polysemy handling, and scalabil-
ity. We are not aware of any previous works
that address all three.

• Our framework is extremely flexible and
simple-to-implement, consisting of a novel
combination of existing topic modeling tools
from machine learning and word alignment
tools from machine translation.

2 Related Work

There is a plethora of research on bilingual lexi-
con extraction from comparable corpora, starting

with seminal works of (Rapp, 1995; Fung and Lo,
1998). The main idea is to assume that translation
pairs have similar contexts, i.e. the distributional
hypothesis, so extraction consists of 3 steps: (1)
identify context windows around words, (2) trans-
late context words using a seed bilingual dictio-
nary, and (3) extract pairs that have high result-
ing similarity. Methods differ in how the seed
dictionary is acquired (Koehn and Knight, 2002;
Déjean et al., 2002) and how similarity is defined
(Fung and Cheung, 2004; Tamura et al., 2012).
Projection-based approaches have also been pro-
posed, though they can be shown to be related
to the aforementioned distributional approaches
(Gaussier et al., 2004); for example, Haghighi
(2008) uses CCA to map vectors in different lan-
guages into the same latent space. Laroche (2010)
presents a good summary.

Vulić et al. (2011) pioneered a new approach
to bilingual dictionary extraction based on topic
modeling approach which requires no seed dictio-
nary. While our approach is motivated by (Vulić
et al., 2011), we exploit the topic model in a very
different way (explained in Section 4.2). They do
not use word alignments like we do and thus can-
not model polysemy. Further, their approach re-
quires training topic models with a large number
of topics, which may limit the scalability of the
approach.

Recently, there has been much interest in mul-
tilingual topic models (MLTM) (Jagarlamudi and
Daume, 2010; Mimno et al., 2009; Ni et al., 2009;
Boyd-Graber and Blei, 2009). Many of these mod-
els give p(t|e) and p(t|f), but stop short of extract-
ing a bilingual lexicon. Although topic models can
group related e and f in the same topic cluster, the
extraction of a high-precision dictionary requires
additional effort. One of our contributions here is
an effective way to do this extraction using word
alignment methods.

3 System Components: Background

This section reviews MLTMs and Word Align-
ment, the main components of our framework.
The knowledgeable readers may wish to skim this
section for notation and move to Section 4, which
describes our contribution.

3.1 Multilingual Topic Model

Any multilingual topic model may be used with
our framework. We use the one by Mimno et

213



al. (2009), which extends the monolingual La-
tent Dirichlet Allocation model (Blei et al., 2003).
Given a comparable corpus E in English and F
in a foreign language, we assume that the docu-
ment pair boundaries are known. For each doc-
ument pair di = [dei , d

f
i ] consisting of English

document dei and Foreign document d
f
i (where

i ∈ {1, . . . , D}, D is number of document pairs),
we know that dei and d

f
i talk about the same

topics. While the monolingual topic model lets
each document have its own so-called document-
specific distribution over topics, the multilingual
topic model assumes that documents in each tu-
ple share the same topic prior (thus the compara-
ble corpora assumption) and each topic consists of
several language-specific word distributions. The
generative story is shown in Algorithm 1.

for each topic k do
for l ∈ {e, f} do

sample ϕlk ∼ Dirichlet(βl);
end

end
for each document pair di do

sample θi ∼ Dirichlet(α);
for l ∈ {e, f} do

sample zl ∼Multinomial(θi);
for each word wl in dli do

sample wl ∼ p(wl|zl, ϕl);
end

end
end

Algorithm 1: Generative story for (Mimno et al.,
2009). θi is the topic proportion of document
pair di. Words wl are drawn from language-
specific distributions p(wl|zl, ϕl), where lan-
guage l indexes English e or Foreign f . Here
pairs of language-specific topics ϕl are drawn
from Dirichlet distributions with prior βl.

3.2 Statistical Word Alignment
For a sentence-pair (e,f), let e =
[we1, w

e
2, . . . w

e
|e|] be the English sentence with |e|

words and f = [wf1 , w
f
2 , . . . w

f
|f |] be the foreign

sentence with |f | words. For notation, we will
index English words by i and foreign words
by j. The goal of word alignment is to find an
alignment function a : i → j mapping words in e
to words in f (and vice versa).

We will be using IBM Model 1 (Brown et al.,

1993; Och and Ney, 2003), which proposes the
following probabilistic model for alignment:

p(e, a, |f) ≈
|e|∏

i=1

p(wei |wfa(i)) (1)

Here, p(wei |wfa(i)) captures the translation prob-
ability of the English word at position i from the
foreign word at position j = a(i), where the ac-
tual alignment a is a hidden variable, and training
can be done via EM. Although this model does not
incorporate much linguistic knowledge, it enables
us to find correspondence between distinct objects
from paired sets. In machine translation, the dis-
tinct objects are words from different languages
while the paired sets are sentence-aligned corpora.
In our case, our distinct objects are also words
from distinct languages but our pair sets will be
topic-aligned corpora.

4 Proposed Framework for Bilingual
Dictionary Extraction

The general idea of our proposed framework is
sketched in Figure 1: First, we run a multilin-
gual topic model to convert the comparable cor-
pora to topic-aligned corpora. Second, we run
a word alignment algorithm on the topic-aligned
corpora in order to extract translation pairs. The
innovation is in how this topic-aligned corpora is
defined and constructed, the link between the two
stages. We describe how this is done in Section 4.1
and show how existing approaches are subsumed
in our general framework in Section 4.2.

4.1 Topic-Aligned Corpora
Suppose the original comparable corpus has D
document pairs [dei , d

f
i ]i=1,...,D. We run a mul-

tilingual topic model with K topics, where K
is user-defined (Section 3.1). The topic-aligned
corpora is defined hierarchically as a set of sets:
On the first level, we have a set of K topics,
{t1, . . . , tk, . . . , tK}. On the second level, for
each topic tk, we have a set of D “word col-
lections“ {Ck,1, . . . , Ck,i, . . . , Ck,D}. Each word
collection Ck,i represents the English and foreign
words that occur simultaneously in topic tk and
document di.

For clarity, let us describe the topic-aligned
corpora construction process step-by-step together
with a flow chart in Figure 2:

1. Train a multilingual topic model.

214



Figure 2: Construction of topic-aligned corpora.

2. Infer a topic assignment for each token in the
comparable corpora, and generate a list of word
collections Ck,i occurring under a given topic.

3. Re-arrange the word collections such that Ck,i
belonging to the same topic are grouped together.
This resulting set of sets is called topic-aligned
corpora, since it represents word collections linked
by the same topics.

4. For each topic tk, we run IBM Model 1 on
{Ck,1, . . . , Ck,i, . . . , Ck,D}. In analogy to statis-
tical machine translation, we can think of this
dataset as a parallel corpus of D “sentence pairs“,
where each “sentence pair“ contains the English
and foreign word tokens that co-occur under the
same topic and the same document. Note that
word alignment is run independently for each
topic, resulting in K topic-dependent lexicons
p(we|wf , tk).
5. To extract a bilingual dictionary, we find pairs
(we, wf ) with high probability under the model:

p(we|wf ) =
∑

k

p(we|wf , tk)p(tk|wf ) (2)

The first term is the topic-dependent bilingual lex-
icon from Step 4; the second term is the topic pos-
terior from the topic model in Step 1.

In practice, we will compute the probabilities
of Equation 2 in both directions: p(we|, wf ) as in
Eq. 2 and p(wf |we) = ∑k p(wf |we, tk)p(tk|we).
The bilingual dictionary can then be extracted
based on a probabilities threshold or some bidirec-
tional constraint. We choose to use a bidirectional
constraint because it gives very high-precision

dictionaries and avoid the need to tune probability
thresholds. A pair (ẽ, f̃) is extracted if the
following holds:

ẽ = argmax
e

p(e|f = f̃); f̃ = argmax
f

p(f |e = ẽ)
(3)

To summarize, the main innovation of our ap-
proach is that we allow for polysemy as topic-
dependent translation explicitly in Equation 2, and
use a novel combination of topic modeling and
word alignment techniques to compute the term
p(we|wf , tk) in an unsupervised fashion.

4.2 Alternative Approaches
To the best of our knowledge, (Vulić et al., 2011)
is the only work focuses on using topic models
for bilingual lexicon extraction like ours, but they
exploit the topic model results in a different way.
Their “Cue Method“ computes:

p(we|wf ) =
∑

k

p(we|tk)p(tk|wf ) (4)

This can be seen as a simplification of
our Eq. 2, where Eq. 4 replaces p(we|tk, wf )
with the simpler p(we|tk). Another vari-
ant is the so-called Kullback-Liebler (KL)
method, which scores translation pairs by
−∑k p(tk|we) log p(tk|we)/p(tk|wf ). In either
case, their contribution is the use of topic-word
distributions like p(tk|wf ) or p(wf |tk) to compute
translation probabilities.1 Our formulation can be
considered more general because we do not have
the strong assumption that we is independent of

1A third variant uses TF-IDF weighting, but is conceptu-
ally similar and have similar results.

215



wf given tk, and focus on estimating p(we|wf , tk)
directly with word alignment methods.

5 Experimental Setup

5.1 Data Set
We perform experiments on the Kyoto Wiki Cor-
pus2. We chose this corpus because it is a parallel
corpus, where the Japanese edition of Wikipedia
is translated manually into English sentence-by-
sentence. This enables us to use standard word
alignment methods to create a gold-standard lexi-
con for large-scale automatic evaluation.3

From this parallel data, we prepared several
datasets at successively lower levels of compara-
bility. As shown in Table 1, Comp100% is a com-
parable version of original parallel data, deleting
all the sentence alignments but otherwise keeping
all content on both Japanese and English sides.
Comp50% and Comp20% are harder datasets
that keep only 50% and 20% (respectively) of ran-
dom English sentences per documents. We further
use a real comparable corpus (Wiki)4, which is
prepared by crawling the online English editions
of the corresponding Japanese articles in the Ky-
oto Wiki Corpus. The Comp datasets are con-
trolled scenarios where all English content is guar-
anteed to have Japanese translations; no such guar-
antee exists in our Wiki data.

5.2 Experimental Results
1. How does the proposed framework compare
to previous work?

We focus on comparing with previous topic-
modeling approaches to bilingual lexicon extrac-
tion, namely (Vulić et al., 2011). The methods are:

• Proposed: The proposed method which
exploits a combination of topic modeling
and word alignment to incorporate topic-
dependent translation probabilities (Eq. 2).

• Cue: From (Vulić et al., 2011), i.e. Eq. 4.
2http://alaginrc.nict.go.jp/WikiCorpus/index E.html
3We trained IBM Model 4 using GIZA++ for both direc-

tions p(e|f) and p(f |e). Then, we extract word pair (ẽ, f̃) as
a “gold standard“ bilingual lexicon if it satisfies Eq. 3. Due
to the large data size and the strict bidirectional requirement
imposed by Eq. 3, these “gold standard“ bilingual dictionary
items are of high quality (94% precision by a manual check
on 100 random items). Note sentence alignments are used
only for creating this gold-standard.

4The English corresponding dataset, gold-standard and
ML-LDA software used in our experiments are available at
https://sites.google.com/site/buptxiaodong/home/resource

Dataset #doc #sent(e/j) #voc(e/j)
Comp100% 14k 472k/472k 152k/116k
Comp50% 14k 236k/472k 100k/116k
Comp20% 14k 94k/472k 62k/116k
Wiki 3.6k 127k/163k 88k/61k

Table 1: Datasets: the number of document
pairs (#doc), sentences (#sent) and vocabulary
size (#voc) in English (e) and Japanese (j). For
pre-processing, we did word segmentation on
Japanese using Kytea (Neubig et al., 2011) and
Porter stemming on English. A TF-IDF based
stop-word lists of 1200 in each language is ap-
plied. #doc is smaller for Wiki because not all
Japanese articles in Comp100% have English ver-
sions in Wikipedia during the crawl.

• JS: From (Vulić et al., 2011). Symmetrizing
KL by Jensen-Shannon (JS) divergence im-
proves results, so we report this variant.5

We also have a baseline that uses no topic models:
IBM-1 runs IBM Model 1 directly on the compa-
rable dataset, assuming each document pair is a
“sentence pair“.

Figure 3 shows the ROC (Receiver Operat-
ing Characteristic) Curve on the Wiki dataset.
The ROC curve lets us observe the change in
Recall as we gradually accept more translation
pairs as dictionary candidates. In particular,
it measures the true positive rate (i.e. recall =
|{Gold(e, f)}⋂{Extracted(e, f)}|/#Gold)
and false positive rate (fraction of false extractions
over total number of extractions) at varying levels
of thresholds. This is generated by first computing
p(e|f) + p(f |e) as the score for pair (e, f) for
each method, then sorting the pairs by this score
and successive try different thresholds.

The curve of the Proposed method dominates
those of all other methods. It is also the best
in Area-Under-Curve scores (Davis and Goadrich,
2006), which are 0.96, 0.90, 0.85 and 0.71, for
Proposed, IBM-1, Cue, and JS, respectively.6

ROC is insightful if we are interested in com-
paring methods for all possible thresholds, but in
practice we may desire a fixed operating point.
Thus we apply the bidirectional heuristic of Eq.

5Topic model hyperparameters for Proposed, Cue, and
JS are α = 50/K and β = 0.1 following (Vulić et al., 2011).

6The Precision-Recall curve gives a similar conclusion.
We do not show it here since the extremely low precision of
JS makes the graph hard to visualize. Instead see Table 2.

216



Figure 3: ROC curve on the Wiki dataset. Curves
on upper-left is better. Cue, JS, Proposed all use
K=400 topics. Note that Proposed is best.

K Method Prec ManP #Extracted

100

Cue 0.027 0.02 3800
JS 0.013 0.01 3800

Proposed 0.412 0.36 3800

400

Cue 0.059 0.02 2310
JS 0.075 0.02 2310

Proposed 0.631 0.56 2310
- IBM-1 0.514 0.42 2310
- IBM-1* 0.493 0.39 3714

Table 2: Precision on the Wiki dataset.
K=number of topics. Precision (Prec) is defined
as |{Gold(e,f)}

⋂{Extracted(e,f)}|
#Extracted . ManP is preci-

sion evaluated manually on 100 random items.

3 to extract a fixed set of lexicon for Proposed.
For the other methods, we calibrated the thresh-
olds to get the same number of extractions. Then
we compare the precision, as shown in Table 2.

1. Proposed outperforms other methods,
achieving 63% (automatic) precision and
56% (manual) precision.

2. The JS and Cue methods suffer from ex-
tremely poor precision. We found that this
is due to insufficient number of topics, and
is consistent with the results by (Vulić et al.,
2011) which showed best results with K >
2000. However, we could not train JS/Cue
on such a large number of topics since it is
computationally-demanding for a corpus as
large as ours.7 In this regard, the Proposed

7The experiments in (Vulić et al., 2011) has vocabulary

Figure 4: Robustness of method under different
data conditions.

method is much more scalable, achieving
good results with low K, satisfying one of
original desiderata.8

3. IBM-1 is doing surprisingly well, consider-
ing that it simply treats document pairs as
sentence pairs. This may be due to some
extent to the structure of the Kyoto Wiki
dataset, which contains specialized topics
(about Kyoto history, architecture, etc.), lead-
ing to a vocabulary-document co-occurrence
matrix with sparse block-diagonal structure.
Thus there may be enough statistics train
IBM-1 on documents.

2. How does the proposed method perform un-
der different degrees of “comparability“?

We next examined how our methods perform un-
der different data conditions. Figure 4 plots the re-
sults in terms of Precision evaluated automatically.
We observe that Proposed (K=400) is relatively
stable, with a decrease of 14% Precision going
from fully-comparable to real Wikipedia compa-
rable corpora. The degradation for K=100 is much
larger (31%) and therefore not recommended. We
believe that robustness depends on K, because the

size of 10k, compared to 150k in our experiments. We have
attempted large K ≥ 1000 but Cue did not finish after days.

8We have a hypothesis as to why Cue and JS depend on
largeK. Eq. 2 is a valid expression for p(we|wf ) that makes
little assumptions. We can view Eq. 4 as simplifying the first
term of Eq. 2 from p(we|tk, wf ) to p(we|tk). Both prob-
ability tables have the same output-space (we), so the same
number of parameters is needed in reality to describe this dis-
tribution. By throwing outwf , which has large cardinality, tk
needs to grow in cardinality to compensate for the loss of ex-
pressiveness.

217



5 10 15 20

0
10
00
0

30
00
0

5 10 15 20

0
10
00
0

30
00
0

W
or

d 
C

ou
nt

Topic Count

en
jp

Figure 5: Power-law distribution of number of
word types with X number of topics.

topic model of (Mimno et al., 2009) assumes one
topic distribution per document pair. For low-
levels of comparability, a small number of topics
may not sufficiently model the differences in top-
ical content. This suggests the use of hierarchical
topic models (Haffari and Teh, 2009) or other vari-
ants in future work.
3. What are the statistical characteristics of
topic-aligned corpora?

First, we show the word-topic distribution from
multilingual topic modeling in the K = 400 sce-
nario (first step of Proposed, Cue, and JS). For
each word type w, we count the number of topics
it may appear in, i.e. nonzero probabilities accord-
ing to p(w|t). Fig. 5 shows the number of word
types that have x number of topics. This power-
law is expected since we are modeling all words.9

Next we compute the statistics after construct-
ing the topic-aligned corpora (Step 3 of Fig. 2).
For each part of the topic-aligned corpora, we
compute the ratio of distinct English word types
vs. distinct Japanese word types. If the ratio is
close to one, that means the partition into topic-
aligned corpora effectively separates the skewed
word-topic distribution of Fig 5. We found that
the mean ratio averaged across topics is low at
1.721 (variance is 1.316), implying that within
each topic, word alignment is relatively easy.
4. What kinds of errors are made?

We found that the proposed method makes sev-
eral types of incorrect lexicon extractions. First,
Word Segmentation “errors“ on Japanese could

9This means that it is not possible to directly extract lexi-
con by taking the cross-product (wf , we) of the top-n words
in p(wf |tk) and p(we|tk) for the same topic tk, as suggested
by (Mimno et al., 2009). When we attempted to do this, us-
ing top-2 words per p(wf |tk) and p(we|tk), we could only
obtain precision of 0.37 for 1600 extractions. This skewed
distribution similarly explains the poor performance of Cue.

make it impossible to find a proper English trans-
lation (e.g., 高市皇子 should translate to “Prince-
Takechi“ but system proposes “Takechi“). Sec-
ond, an unrelated word pair (we, wf ) may be in-
correctly placed in the same topic, leading to an
Incorrect Topic error. Third, even if (we, wf ) in-
tuitively belong to the same topic, they may not be
direct translations; an extraction in this case would
be a Correct Topic, Incorrect Alignment error
(e.g. もんじゃ焼き, a particular panfried snack,
is incorrectly translated as “panfry“).

Table 3 shows the distribution of error types by
a manual classification. Incorrect Alignment er-
rors are most frequent, implying the topic models
are doing a reasonable job of generating the topic-
aligned corpus. The amount of Incorrect Topic is
not trivial, though, so we would still imagine more
advanced topic models to help. Segmentation er-
rors are in general hard to solve, even with a better
word segmenter, since in general one-to-one cross-
lingual word correspondence is not consistent–we
believe the solution is a system that naturally han-
dles multi-word expressions (Baldwin, 2011).

Word Segmentation Error 14
Incorrect Topic 29
Correct Topic, Incorrect Alignment 40
Reason Unknown 7
Table 3: Counts of various error types.

5. What is the computation cost?

Timing results on a 2.4GHz Opteron CPU for var-
ious steps of Proposed and Cue are shown in Ta-
ble 5. The proposed method is 5-8 times faster
than Cue. For Proposed, computation time is
dominated by topic modeling while GIZA++ on
topic-aligned corpora is extremely fast. Cue addi-
tionally suffers from computational complexity in
calculating Eq.4, especially when both p(we|tk)
and p(tk|wf ) have high cardinality. In compari-
son, calculating Eq.2 is fast since p(we|wf , tk) is
in practice quite sparse.
6. What topic-dependent lexicons are learned
and do they capture polysemy?

In our evaluation so far, we have only produced an
one-to-one bilingual dictionary (due to the bidirec-
tionality constraint of Eq.3). We have seen how
topic-dependent translation models p(wf |we, tk)
is important in achieving good results. However,
Eq.2 marginalizes over the topics so we do not
know what topic-dependent lexicons are learned.

218



English Japanese1(gloss), Japanese2(gloss)
interest 関心 (a sense of concern),利息 (a charge of money borrowing)
count 数え(act of reciting numbers),伯爵 (nobleman)
free 自由(as in “free“ speech),無料 (as in “free“ beer)
blood 血縁(line of descent),血 (the red fluid)
demand 需要(as noun),要求(as verb)
draft 提案(as verb),草稿 (as noun)
page ページ (one leaf of e.g. a book),侍童 (youthful attendant)
staff スタッフ(general personel),参謀 (as in political “chief of staff“)
director 長官 (someone who controls),理事 (board of directors)監督 (movie director)
beach 浜(area of sand near water),海水浴(leisure spot at beach)
actor 役者 (theatrical performer),俳優 (movie actor)

Table 4: Examples of topic-dependent translations given by p(wf |we, tk). The top portion shows ex-
amples of polysemous English words. The bottom shows examples where English is not decisively
polysemous, but indeed has distinct translations in Japanese based on topic.

K topic giza Eq.2 Eq.4 Prp Cue
100 180 3 20 1440 203 1620
200 300 3 33 2310 336 2610
400 780 5 42 3320 827 4100

Table 5: Wall-clock times in minutes for Topic
Modeling (topic), Word Alignment (giza), and
p(we|wf ) calculation. Overall time for Pro-
posed (Prp) is topic+giza+Eq.2 and for Cue is
topic+Eq.4.

Here, we explore the model p(wf |we, tk) learned
at Step 4 of Figure 2 to see whether it captures
some of the polysemy phenomenon mentioned in
the desiderata. It is not feasible to automatically
evaluate topic-dependent dictionaries, since this
requires “gold standard“ of the form (e, f, t). Thus
we cannot claim whether our method successfully
extracts polysemous translations. Instead we will
present some interesting examples found by our
method. In Table 4, we look at potentially pol-
ysemous English words we, and list the highest-
probability Japanese translations wf conditioned
on different tk. We found many promising cases
where the topic identification helps divide the dif-
ferent senses of the English word, leading to the
correct Japanese translation achieving the highest
probability.

6 Conclusion

We proposed an effective way to extract bilin-
gual dictionaries by a novel combination of topic
modeling and word alignment techniques. The
key innovation is the conversion of a compara-

ble document-aligned corpus into a parallel topic-
aligned corpus, which allows word alignment
techniques to learn topic-dependent translation
models of the form p(we|wf , tk). While this kind
of topic-dependent translation has been proposed
for the parallel corpus (Zhao and Xing, 2007),
we are the first to enable it for comparable cor-
pora. Our large-scale experiments demonstrated
that the proposed framework outperforms existing
baselines under both automatic metrics and man-
ual evaluation. We further show that our topic-
dependent translation models can capture some of
the polysemy phenomenon important in dictionary
construction. Future work includes:

1. Exploring other topic models (Haffari and Teh,
2009) and word alignment techniques (DeNero
and Macherey, 2011; Mermer and Saraclar, 2011;
Moore, 2004) in our framework.

2. Extract lexicon from massive multilingual col-
lections. Mausum (2009) and Shezaf (2010) show
that language pivots significantly improve the pre-
cision of distribution-based approaches. Since
multilingual topic models can easily be trained on
more than 3 languages, we expect it will give a big
boost to our approach.

Acknowledgments

We thank Mamoru Komachi, Shuhei Kondo and
the anonymous reviewers for valuable discussions
and comments. Part of this research was executed
under the Commissioned Research of National In-
stitute of Information and Communications Tech-
nology (NICT), Japan.

219



References
Timothy Baldwin. 2011. Mwes and topic mod-

elling: enhancing machine learning with linguis-
tics. In Proceedings of the Workshop on Multiword
Expressions: from Parsing and Generation to the
Real World, MWE ’11, pages 1–1, Stroudsburg, PA,
USA. Association for Computational Linguistics.

D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.

Jordan Boyd-Graber and David M. Blei. 2009. Multi-
lingual topic models for unaligned text. In UAI.

P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2).

Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
407–412, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Jesse Davis and Mark Goadrich. 2006. The relation-
ship between precision-recall and ROC curves. In
ICML.

Hervé Déjean, Éric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th international conference
on Computational linguistics - Volume 1, COLING
’02, pages 1–7.

John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL).

Pascale Fung and Percy Cheung. 2004. Mining
verynon-parallel corpora: Parallel sentence and lex-
icon extraction via bootstrapping and em. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.

Pascale Fung and Yuen Yee Lo. 1998. Translating un-
known words using nonparallel, comparable texts.
In COLING-ACL.

Eric Gaussier, J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL’04), Main Vol-
ume, pages 526–533, Barcelona, Spain, July.

Ghloamreza Haffari and Yee Whye Teh. 2009. Hier-
archical dirichlet trees for information retrieval. In
NAACL.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771–779, Columbus, Ohio, June.
Association for Computational Linguistics.

Jagadeesh Jagarlamudi and Hal Daume. 2010. Ex-
tracting multilingual topics from unaligned compa-
rable corpora. In ECIR.

Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.

Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
617–625, Beijing, China, August. Coling 2010 Or-
ganizing Committee.

Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In ACL.

Coskun Mermer and Murat Saraclar. 2011. Bayesian
word alignment for statistical machine translation.
In ACL.

David Mimno, Hanna Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.

Robert Moore. 2004. Improving IBM word alignment
model 1. In ACL.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT) Short Paper Track, pages 529–533, Portland,
Oregon, USA, 6.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from wikipedia.
In WWW.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.

Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics.

220



Philip Resnik, Douglas Oard, and Gina Levow. 2011.
Improved cross-language retrieval using backoff
translation. In Proceedings of the First International
Conference on Human Language Technology.

Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 98–107. Association for Computational Lin-
guistics.

Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 24–36,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.

Ivan Vulić, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compa-
rable corpora using latent topic models. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 479–484, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.

Bing Zhao and Eric P. Xing. 2007. HM-BiTAM:
Bilingual Topic Exploration, Word Alignment, and
Translation. In NIPS.

221


