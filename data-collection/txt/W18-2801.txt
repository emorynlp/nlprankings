



















































Predicting Brain Activation with WordNet Embeddings


Proceedings of the Eighth Workshop on Cognitive Aspects of Computational Language Learning and Processing, pages 1–5
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

1

Predicting Brain Activation with WordNet Embeddings

João António Rodrigues, Ruben Branco, João Ricardo Silva, Chakaveh Saedi, António Branco
University of Lisbon

NLX-Natural Language and Speech Group, Department of Informatics
Faculdade de Ciências

Campo Grande, 1749-016 Lisboa, Portugal
{joao.rodrigues, ruben.branco, jsilva, chakaveh.saedi, antonio.branco}@di.fc.ul.pt

Abstract

The task of taking a semantic represen-
tation of a noun and predicting the brain
activity triggered by it in terms of fMRI
spatial patterns was pioneered by Mitchell
et al. (2008). That seminal work used
word co-occurrence features to represent
the meaning of the nouns. Even though the
task does not impose any specific type of
semantic representation, the vast majority
of subsequent approaches resort to feature-
based models or to semantic spaces (aka
word embeddings). We address this task,
with competitive results, by using instead
a semantic network to encode lexical se-
mantics, thus providing further evidence
for the cognitive plausibility of this ap-
proach to model lexical meaning.

1 Introduction

Neurosemantics studies the mapping between con-
cepts and the corresponding brain activity, bring-
ing together neuroscientists doing brain imaging
research and linguists doing research on the se-
mantics of natural language expressions.

The task introduced by Mitchell et al. (2008)
consists of taking a semantic representation of a
noun and predicting the functional magnetic res-
onance imaging (fMRI) spatial activation patterns
in the brain triggered by that noun. That is, given
a meaning representation of a word, it should be
the basis to predict the activation strength at each
point (voxel) in the 3D volume of the brain asso-
ciated to the cognitive handling of that word. This
allows to make testable predictions of fMRI activ-
ity, even for nouns for which there is no fMRI data
available, as long as there is some way to model
and represent the semantics of a lexicon.

In lexical semantics, three broad families of ap-
proaches have emerged to model meaning, namely
(i) semantic networks, (ii) feature-based models,
and (iii) semantic spaces. The models of the lex-
icon produced under these approaches have been
embedded in wider models of the whole grammar
or in language technology applications and tasks,
including synonym identification, analogies detec-
tion a.o., were they have been tested on behav-
ioral data sets. The prediction of brain activation
considered here is agnostic regarding the approach
used to model lexical meaning, thus providing an-
other way of assessing the cognitive plausibility of
lexical semantic representations of different sorts.

While most approaches to this task have re-
sorted to feature-based models or to semantic
spaces (aka word embeddings), here we address
the task of prediciting the brain activation triggred
by nouns rather by using a semantic network, thus
providing further evidence for the cognitive plau-
sibility of this approach to model lexical meaning.

In this paper, we report on the competitive
results of resolving the brain activation task by
taking a mainstream lexical semantics network,
WordNet (Fellbaum, 1998), and resorting to inter-
mediate word embeddings obatined with a novel
methodology (Saedi et al., 2018) for generating se-
mantic spaces from semantic networks.

2 The brain activation prediction task

The seminal work of Mitchell et al. (2008) intro-
duced the task consisting of predicting the fMRI
activation patterns triggered by a noun-picture pair
from a semantic representation of that noun. The
language of the data used was English.

Each wordw was represented by a set of seman-
tic features given by the normalized co-occurrence
counts of w with a set of 25 verbs. These counts
were obtained from the Web 1T 5-gram data set



2

(Brants and Franz, 2006), using the n-grams up to
length 5 generated from 1 trillion tokens of text.

The 25 verbs were manually selected due to
their correspondence to basic sensory and motor
activities.1 Sensory-motor features should be par-
ticularly relevant for the representation of objects
and, in fact, alternative features based on a random
selection of 25 frequent words performed worse.

The fMRI activation pattern at every voxel in
the brain is calculated as a weighted sum of each
of the 25 semantic features, where the weights are
learned by regression to maximum likelihood esti-
mates given observed fMRI data.

To produce the fMRI data, 9 participants were
shown 60 different word-picture pairs,2 the stim-
uli, each presented 6 times. For each participant,
a representative fMRI image for each stimulus
was calculated by determining the mean fMRI re-
sponse from the 6 repetitions and subtracting from
each the mean of all 60 stimuli.

Separate models were learned for each of the
9 participants. These models were evaluated us-
ing leave-two-out cross-validation, where in each
cross-validation iteration the model was asked to
predict the fMRI activation for the two held-out
words. The two predictions were matched against
the two observed activations for those words using
cosine similarity over the 500 most stable voxels.

Randomly assigning the two predictions to the
two observations would yield a 0.50 accuracy. The
models in the seminal paper (Mitchell et al., 2008)
achieve a mean accuracy of 0.77, with all individ-
ual accuracies significantly above chance.

These results support the plausibility of the
two key assumptions underlying the task, namely
that (i) brain activation patterns can be predicted
from semantic representations of words; and that
(ii) lexical semantics can be captured by co-
occurrence statistics, the assumption underlying
semantics space models of the lexicon.

3 Related work

Several authors have addressed this brain activa-
tion prediction task, keeping up with its basic as-
sumptions and resorting to the same data sets for

1The verbs are: approach, break, clean, drive, eat, en-
ter, fear, fill, hear, lift, listen, manipulate, move, near, open,
push, ride, rub, run, say, see, smell, taste, touch, and wear.

2The 60 pairs are composed of 5 items from each of the 12
concrete semantic categories (animals, body parts, buildings,
building parts, clothing, furniture, insects, kitchen items,
tools, vegetables, vehicles, and other man-made items).

the sake of the comparability of the performance
scores obtained.

In an initial period, different authors sought to
explore the experimental space of the task by fo-
cusing on different ways to set up the features.

Devereux et al. (2010) find that choosing the set
of verbs used for the semantic features under an
automatic approach can lead to predictions that are
equally good as when using the manually selected
set of verbs. Jelodar et al. (2010) use the same set
of 25 features to represent a word, but instead of
basing the features on co-occurrence counts they
resort to relatedness measures based on WordNet.
Fernandino et al. (2015) use instead a set of fea-
tures with 5 sensory-motor experience based at-
tributes (sound, color, visual motion, shape, and
manipulation). The relatedness scores between the
stimulus word and the attributes are based on hu-
man ratings instead of corpus data.

Subsequently, as distributional semantics be-
came increasingly popular, authors moved from
feature-based representations of the meaning of
words to experiment with different vector based
representation models (aka word embeddings).

Murphy et al. (2012) compare different corpus-
based models to derive word embeddings. They
find the best results with dependency-based em-
beddings, where words inside the context window
are extended with grammatical functions. Binder
et al. (2016) use word representations based on
65 experiential attributes with relatedness scores
crowdsourced from over 1,700 participants. Xu
et al. (2016) present BrainBench, a workbench
to test embedding models on both behavioral and
brain imaging data sets. Anderson et al. (2017) use
a linguistic model based on word2vec embeddings
and a visual model built with a deep convolutional
neural network on the Google Images data set.

Recently, Abnar et al. (2018) evaluated 8 dif-
ferent embeddings regarding their usefulness in
predicting neural activation patterns: the co-
occurrence embeddings of (Mitchell et al., 2008);
the experiential embeddings of (Binder et al.,
2016); the non-distributional feature-based em-
beddings of (Faruqui and Dyer, 2015); and
5 different distributional embeddings, namely
word2vec (Mikolov et al., 2013), Fasttext
(Bojanowski et al., 2016), dependency-based
word2vec (Levy and Goldberg, 2014), GloVe
(Pennington et al., 2014) and LexVec (Salle et al.,
2016). These authors found that dependency-



3

based word2vec achieves the best performance
among the approaches resorting to word embed-
dings, while the seminal approach resorting to 25
features “is doing slightly better on average” with
respect to all the approaches experimented with.

The rationale guiding the various works pre-
sented in this Section is that the better the per-
formance of the system the higher is the cogni-
tive plausibility of the lexical semantics model re-
sorted to. It is also important to note, however, that
there is not always a clearly better method since
results show that different methods have different
error patterns (Abnar et al., 2018).

4 WordNet embeddings

The previous Sections indicate that approaches to
the brain activation task typically resort to feature-
based models or to semantic spaces to represent
the meaning of words.

In this paper, we address this task by using in-
stead a semantic network as the base repository
of lexical semantic knowledge, namely WordNet.
We then resort to a novel methodology developed
by us (Saedi et al., 2018) for generating seman-
tic space embeddings from semantic networks,
and use it to obtain WordNet embeddings. This
method is based on the intuition that the larger the
number of paths and the shorter the paths connect-
ing any two nodes in a network the stronger is their
semantic association.

The conversion method begins by representing
the semantic graph as an adjacency matrix M ,
where element Mij is set to 1 if there is an edge
between word wi and word wj , and 0 otherwise.
Then, this initial relatedness of immediately adja-
cent words is “propagated” through the matrix by
iterating the following cumulative addition

M
(n)
G = I + αM + α

2M2 + · · ·+ αnMn (1)

where I is the identity matrix, the n-th power of
the transition matrix, Mn, is the matrix where
each Mij counts the number of paths of lenght n
between nodes i and j, and α is a decay factor.

The limit of this sum is given by the following
closed expression (see Newman, 2010, Eq. 7.63):

MG =
∞∑
e=0

(αM)e = (I − αM)−1 (2)

Matrix MG is subsequently submitted to a Pos-
itive Point-wise Mutual Information transforma-
tion, each line is L2-normalized and, finally, Prin-
cipal Component Analysis is applied, reducing

each line to the size of the desired embedding
space. Row i of matrix MG is then taken as the
embedding for word wi.

Using the methodology outlined above, embed-
dings with size 850 were extracted from a subset
of 60k words in version 3 of English WordNet.3

When run on the mainstream semantic similarity
data set SimLex-999 (Hill et al., 2016), the re-
sulting embeddings showed highly competitive re-
sults, outperforming word2vec by some 15% We
refer to our embeddings as wnet2vec.4

5 Experiment

The good results obtained with wnet2vec in the
semantic similarity task lead to experiment with
them also in the brain activation prediction task.

5.1 System training

We resorted to the framework implementation5 by
Abnar et al. (2018). Training ran for 1,000 epochs,
with a batch size of 29 and a learning rate of 0.001.
The loss function is calculated by adding the Hu-
ber loss, the mean pairwise squared error and the
L2-norm (on weights and bias). Like in previ-
ous works, only the 500 most stable voxels are se-
lected. Training was done on a Tesla K40m GPU
and took 54 hours (6 hours per subject).

Figure 1 shows an example for Participant 1,
with the model prediction and the observed fMRI
activation pattern for the word eye. The brain acti-
vation images were generated with Nibabel (Brett
et al., 2017) and Nilearn (Abraham et al., 2014).

5.2 Evaluation and discussion

We followed the usual evaluation procedure for
this framework. The cross-validated, leave-two-
out mean accuracy was 0.71. The full scores, to-
gether with the scores from the original paper, are
summarized in Table 1 and shown graphically in
Figure 2 (0.50 corresponds to chance).6

This indicates that wnet2vec has a competitive
performance in this task as the mean score ob-
tained is in the range of the scores found for all ap-

3We used less than half of the 1̃50k words in WordNet due
to computational limitations as the matrix inverse in (2) faces
substantial challenges in terms of the memory footprint.

4Available at https://github.com/nlx-group/
WordNetEmbeddings

5https://github.com/samiraabnar/
NeuroSemantics/

6Materials for replication available at https://
github.com/nlx-group/BrainActivation

https://github.com/nlx-group/WordNetEmbeddings
https://github.com/nlx-group/WordNetEmbeddings
https://github.com/samiraabnar/NeuroSemantics/
https://github.com/samiraabnar/NeuroSemantics/
https://github.com/nlx-group/BrainActivation
https://github.com/nlx-group/BrainActivation


4

(a) predicted (b) observed

Figure 1: fMRI activations for Participant 1, word eye

Embeddings P1 P2 P3 P4 P5 P6 P7 P8 P9 mean

(Mitchell et al., 2008) 0.83 0.76 0.78 0.72 0.78 0.85 0.73 0.68 0.82 0.77
wnet2vec 0.84 0.72 0.86 0.75 0.60 0.67 0.70 0.53 0.74 0.71

Table 1: Accuracy results for the 9 subjects

Figure 2: Accuracy results for the 9 subjects

proaches resorting to word embeddings, systemat-
ically tested by (Abnar et al., 2018).

In line with all approaches resorting to word
embeddings (Abnar et al., 2018), the mean score
obtained is also not outperforming the original 25
verb-based co-occurrence features model reported
in the seminal paper (Mitchell et al., 2008).

When comparing the scores per participant, the
bulk of the wnet2vec losses are due to P5, P6 and
P8. For the other subjects, results are close or, in
three cases, even better than those from the semi-
nal paper. This highlights the point already made
in (Abnar et al., 2018), that different methods have
different error patterns, which suggests that an en-
semble of classifiers could lead to better overall
accuracy. And also, that a dataset with only 9 sub-
jects — the dataset used in the literature on this
task since (Mitchell et al., 2008) — may be hin-
dering better empirically grounded conclusions.

Finally, it should be noted that these competitive
results were obtained with wnet2vec generated on
the basis of 60k words only, thus less than half of

WordNet. It will be very interesting to see how
the performance of this approach progresses when
larger portions of WordNet are taken into account
as computational limitations can be overcome.

6 Conclusions

We report on an experiment with the task of pre-
dicting the fMRI spatial activation patterns in the
brain associated with a given noun.

We resorted to a semantic network of lexical
knowledge, viz. WordNet, and thus to a represen-
tation of the meaning of the input nouns as ele-
ments of concept nodes in a graph of semantically
related edges. We also resorted to a derived inter-
mediate vectorial semantic representation (word
embeddings) for the input nouns that was obtained
by a novel methodology to convert semantic net-
works into semantic spaces, applied to WordNet.

The results indicate that this model has a com-
petitive performance as its scores are within the
range of the results obtained with state of the art
models based on corpus-based word embeddings
reported in the literature. Though for one third of
the 9 subjects this model surpasses Mitchell et al.
(2008), on average it did not outperform that sem-
inal model, which used hand-selected features.

The fact that less than half of the words in
WordNet were used allows a positive expectation
with respect to the strength of the proposed ap-
proach, and points towards future work that will
seek to use larger portions of WordNet, and fur-
ther lexical semantics networks and ontologies.



5

References
Samira Abnar, Rasyan Ahmed, Max Mijnheer, and

Willem Zuidema. 2018. Experiential, distributional
and dependency-based word embeddings have com-
plementary roles in decoding brain activity. In Pro-
ceedings of the 8th Workshop on Cognitive Model-
ing and Computational Linguistics (CMCL 2018) ,
pages 5766.

Alexandre Abraham, Fabian Pedregosa, Michael Eick-
enberg, Philippe Gervais, Andreas Mueller, Jean
Kossaifi, Alexandre Gramfort, Bertrand Thirion, and
GaelVaroquaux. 2014. Machine learning for neu-
roimaging with scikit-learn. Frontiers in Neuroin-
formatics, 8:14.

Andrew J. Anderson, Douwe Kiela, Stephen Clark, and
Massimo Poesio. 2017. Visually grounded and tex-
tual semantic models differentially decode brain ac-
tivity associated with concrete and abstract nouns.
Transactions of the Association of Computational
Linguistics, 5(1):17–30.

Jeffrey R. Binder, Lisa L. Conant, Colin J. Humphries,
Leonardo Fernandino, Stephen B. Simons, Mario
Aguilar, and Rutvik H. Desai. 2016. Toward a brain-
based componential semantic representation. Cog-
nitive neuropsychology, 33(3-4):130–174.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.

Matthew Brett, Michael Hanke, et al. 2017.
nipy/nibabel: 2.2.0.

Barry Devereux, Colin Kelly, and Anna Korhonen.
2010. Using fMRI activation to conceptual stim-
uli to evaluate methods for extracting conceptual
representations from corpora. In Proceedings of
the NAACL HLT 2010 First Workshop on Compu-
tational Neurolinguistics, pages 70–78. Association
for Computational Linguistics.

Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representations. arXiv
preprint arXiv:1506.05230.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.

Leonardo Fernandino, Colin J. Humphries, Mark S.
Seidenberg, William L. Gross, Lisa L. Conant, and
Jeffrey R. Binder. 2015. Predicting brain activation
patterns associated with individual lexical concepts
based on five sensory-motor attributes. Neuropsy-
chologia, 76:17–26.

Felix Hill, Roi Reichart, and Anna Korhonen. 2016.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41:665–695.

Ahmad Babaeian Jelodar, Mehrdad Alizadeh, and
Shahram Khadivi. 2010. WordNet based features
for predicting brain activity associated with mean-
ings of nouns. In Proceedings of the NAACL HLT
2010 First Workshop on Computational Neurolin-
guistics, pages 18–26. Association for Computa-
tional Linguistics.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), volume 2, pages 302–308.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tom M. Mitchell, Svetlana V. Shinkareva, Andrew
Carlson, Kai-Min Chang, Vicente L. Malave,
Robert A. Mason, and Marcel Adam Just. 2008.
Predicting human brain activity associated with the
meanings of nouns. Science, 320(5880):1191–1195.

Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Selecting corpus-semantic models for neu-
rolinguistic decoding. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 114–123. Association for Computa-
tional Linguistics.

Mark Newman. 2010. Networks: An Introduction. Ox-
ford University Press.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

Chakaveh Saedi, Antnio Branco, Joo Antnio Ro-
drigues, and Joo Ricardo Silva. 2018. Wordnet
embeddings. In Proceedings of the ACL2018 3rd
Workshop on Representation Learning for Natural
Language Processing (RepL4NLP). Association for
Computational Linguistics.

Alexandre Salle, Marco Idiart, and Aline Villavicencio.
2016. Matrix factorization using window sampling
and negative sampling for improved word represen-
tations. arXiv preprint arXiv:1606.00819.

Haoyan Xu, Brian Murphy, and Alona Fyshe. 2016.
BrainBench: A brain-image test suite for distribu-
tional semantic models. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 2017–2021.

https://doi.org/10.5281/zenodo.1011207

