



















































UMCCDLSI: Sentiment Analysis in Twitter using Polirity Lexicons and Tweet Similarity


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 727–731,
Dublin, Ireland, August 23-24, 2014.

UMCC DLSI: Sentiment Analysis in Twitter using Polirity Lexicons and
Tweet Similarity

Pedro Aniel Sánchez-Mirabal,
Yarelis Ruano Torres,

Suilen Hernández Alvarado
University of Matanzas / Cuba

pedroasm@umcc.cu
yara@umcc.cu

suilen.alvarado@umcc.cu

Yoan Gutiérrez,
Andrés Montoyo,

Rafael Muñoz
University of Alicante/Spain
ygutierrez@dlsi.ua.es
montoyo@dlsi.ua.es
rafael@dlsi.ua.es

Abstract

This paper describes a system sub-
mitted to SemEval-2014 Task 4B:
Sentiment Analysis in Twitter, by the
team UMCC DLSI Sem integrated by
researchers of the University of Matanzas,
Cuba and the University of Alicante,
Spain. The system adopts a cascade
classification process that uses two classi-
fiers, K-NN using the lexical Levenshtein
metric and a Dagging model trained over
attributes extracted from annotated cor-
pora and sentiment lexicons. Phrases that
fit the distance thresholds were automat-
ically classified by the KNN model, the
others, were evaluated with the Dagging
model. This system achieved over 52.4%
of correctly classified instances in the
Twitter message-level subtask.

1 Introduction

Nowadays, one of the most important sources of
data to extract useful and heterogeneous knowl-
edge is Textual Information. Daily, millions
of Tweets, SMS and blog comments increase
the huge volume of information available for re-
searchers. Texts can provide factual information,
such as: descriptions, lists of characteristics, or
even instructions to opinion-based information,
which would include reviews, emotions, or feel-
ings (Gutiérrez et al., 2013). These facts have
motivated that dealing with the identification and
extraction of opinions and sentiments in texts re-
quires special attention. Applications of Senti-
ment Analysis are now more common than ever
in fields like politics and business. More than 50

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

systems participating in this task, clearly indicate
the increase of interest in the scientific community.

Twitter messages can be found among of the
most used corpora nowadays for Sentiment Anal-
ysis (SA). This kind of messages involves an evi-
dent informality which has been addressed in dif-
ferent ways. For example, there are some works
like (Gutiérrez et al., 2013) that apply normali-
sation textual tools to reduce the informality of
the twitter messages. Authors such as (Go et al.,
2009), (Gutiérrez et al., 2013), (Fernández et al.,
2013) and others are focused on the application
of preprocessing processes and feature reduction
to be able to standardise twitter messages and re-
duce different types of elements like hashtags, user
nicks, urls, etc.

In terms of those techniques that can be used
for SA, we can cite (Pang et al., 2002) who built
a lexicon with associated polarity value, starting
with a set of classified seed adjectives and using
conjunctions (and) disjunctions (or, but) to deduce
the orientation of new words in a corpus. This re-
search was based on machine learning techniques
to address Sentiment Classification. Other inter-
esting research is (Turney, 2002), which classi-
fies words according to their polarity based on
the idea that terms with similar orientation tend
to co-occur in documents. There are a large quan-
tity of approaches to deal with SA, and basically
most of them are based on word bags and/or an-
notated corpora as knowledge base. Based on this
information the SA systems are able to apply dif-
ferent types of evaluation techniques such as ma-
chine learning or statistic formulas to predict the
correct classification. As part of machine learn-
ing approaches we would like to mention those
works such as (Go et al., 2009), (Mohammad et
al., 2013) and others that were based on feature
vectors and which cover a wide range settings of
SA. As a starting point, we based this work on
the (Mohammad et al., 2013) approach, adding

727



new features extracted from the sentiment repos-
itories Sentiment 140 1 and NRC-Hashtag Senti-
ment (Mohammad and Turney, 2013).

The remainder of this paper is structured as fol-
lows: section 2 describes in detail the approach
presented. In section 3 we explain the experiments
we carried out. Finally in section 4 conclusions
and future works are expounded.

2 System Description

In this section we present our system in detail
which is able to classify the polarity of tweets as
positive, negative, or neutral.

The system is structured in two main stages.
The first stage consists of classifying a given
tweet. For that, we first recovered all the tweets
from the training corpus that have a similarity
value greater than a fixed threshold T . The sec-
ond stage consists of classifying using the K-NN
rule (Coomans and Massart, 1982), considering as
K all tweets recovered. The process begins with
T = 0.9 decreasing it until T = 0.6. In section 3
we will explain how these values were determined.

As similarity metric we use the Levenshtein
(Levenshtein, 1966) lexical distance. In case that
we cannot find any tweet fulfilling the condition,
the tweet polarity is assigned using a second clas-
sifier trained using Dagging which combines sev-
eral Logistic classifiers set by WEKA as default.

2.1 Preprocessing
The first step in our system is to pre-process all
tweets. The following operations were applied in
the given order.

- Replacing emoticons: Each emoticon is
replaced by a word according to a
lexicon of emoticons. The mean-
ings of the emoticons were taken from
http://en.wikipedia.org/wiki/
List_of_emoticons.

- Replacing acronyms: Each acronym is re-
placed by its meaning. The meanings of the
acronyms were taken from http://www.
acronymfinder.com/.

- Cleaning text: Remove not alphanumeric char-
acters from the tweet.

- Replacing abbreviations: Each abbrevia-
tion is replaced by its respective words.

1http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip

The abbreviations were taken from
http://en.wikipedia.org/wiki/
Abbreviation.

- Lemmatising: Each word is replaced by its
lemma. We use Freeling 3.0 (Padró and
Stanilovsky, 2012) for this purpose. We only
retain lemmas corresponding to adjectives,
adverbs, interjections, nouns and verbs.

- Expanding contractions: Each contraction
is replaced by its respective word. The
contractions were taken from http://
www.softschools.com/language_
arts/grammar/contractions/
contractions_list/.

- Deleting punctuation marks.

- Deleting stop words. The stop words
were taken from http://www.ranks.
nl/stopwords.

2.2 Recovering tweets from similarity

As it was explained before, in a first step we tried
to classify tweets using the K-NN rule. To recover
the K similar tweets we used the Levenshtein met-
ric (Levenshtein, 1966). This measure allows to
compute the similarity of two strings of symbols
counting the minimum number of deletions, sub-
stitutions and insertions necessary to transform
one string into another. In our case, each word in
the string is considered as a symbol. In the future
we plan to improve this metric using Levenshtein
at word level and then at sentence level. This met-
ric is known as DLED (Double Levenshteins Edit
Distance) and will be taken from (Fernández et al.,
2012).

2.3 Features for Dagging classifier

We represented each tweet as a vector of features
based in (Mohammad et al., 2013) plus other new
ones. Also we used the lexicons Sentiment 140
and NRC-Hashtag Sentiment as it was defined
by Mohammad.

Also two new lexicons, named NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0 were
derived from the NRC Emotion Lexicon (Mo-
hammad and Turney, 2013). In the first case we
associated to each word just the values in the
columns positive and negative of NRC Emotion
Lexicon, thus, no sentiment score was computed.

728



For the second lexicon, the positive score was cal-
culated as the sum of the values for the classifica-
tions positive, anticipation, joy, surprise and trust.
On the other hand, the negative score was com-
puted as the sum of the values for the classifica-
tions negative, anger, disgust, fear, sadness and
trust.

In each case we computed the following at-
tributes:

- Pos: Sum of the positive scores of each token
in the tweet over the number of tokens in the
tweet.

- Neg: Sum of the negative scores of each token
in the tweet over the number of tokens in the
tweet.

- PercentPos: 100∗PosPos+Neg
- MissNGram: Percent of tokens in the tweet that

were not found in the lexicon.

For the Sentiment 140 and NRC-Hashtag Sen-
timent lexicons we also computed the feature:

- SSE: Sum of the sentiment score of each token
in the tweet over the number of tokens in the
tweet.

Based on the information involved into Senti-
ment 140 and NRC-Hashtag Sentiment lexicons,
unigrams, bigrams and pairs were tokenised in-
volving any non-contiguous combination of the
previous n-grams. With respect to the pairs extrac-
tion were considered the following possibilities:
unigram-unigram, unigram-bigram and bigram-
bigram. Similar to (Mohammad et al., 2013) dif-
ferent set of attributes were generated for each
type of token. As result an initial set of 50 at-
tributes were obtained.

In the case of the new lexicons (NRC Emotion
Lexicon 1.0 and NRC Emotion Lexicon 2.0), only
unigrams were considered. Moreover, the feature
SSE was not computed. So, another 8 features
were taken into account with respect to these lexi-
cons.

Finally we computed:

- NCL: Percent of tokens in capital letters.

- NoE: Number of emoticons in the tweet.

- NoA: Number of acronyms in the tweet.

In general the system works with a total of 61
attributes.

2.4 Classifier Design
As training set, we joined the preprocessed tweets
from both the train and development sets pro-
vided by the Task9B of Semeval-2014. The
Dagging classifier was trained using this set
with the following parameters -F 15 -S 1 -W
weka.classifiers.functions.Logistic – -R 1.0E-8 -
M -1 using a 10 fold cross-validation as evaluation
method.

3 Experiments

The experiments were evaluated over the training
dataset provided by Task 9: Sentiment Analysis in
Twitter, subtask B. Based on the explanation pro-
vided in section 2 according to the initialisation of
the threshold T to ensure that the K similar tweets
are in fact similar enough, we carried out an exper-
iment for different values of T . These experiments
refer an analysis to know how the variation of T
affects the classification results.

T % CCI
0.9 86.7
0.8 83.3
0.7 74.1
0.6 67.2
0.5 61.1
0.4 55.0
0.3 56.0

Table 1: Results of the K-NN classifier using Lev-
enshtein metric.

T % CCI
0.9 81.2
0.8 83.3
0.7 74.1
0.6 66.7
0.5 63.1
0.4 60.6
0.3 54.2

Table 2: Results of the K-NN classifier using
Matching Coefficient metric.

The first stage of the system was applied to
compute the number of instances which have at
least one instance with a similarity value greater
than T . We computed the percent of instances
correctly classified (%CCI). Table 1 shows the
behaviour of the system when T changes. Table
2 shows the results of the K-NN classifier using

729



System LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm
Best result 74.8 70.3 72.1 71.0 58.2

Average result 63.5 55.6 59.8 60.6 45.4
UMCC-DLSI-Sem 53.1 50.0 52.0 55.4 42.8

Worse result 29.3 24.6 34.2 33.0 29.0

Table 3: Results in the SemEval-2014 Task 4B.

Matching Coefficient metric (http://www.
coli.uni-saarland.de/courses/LT1/
2011/slides/stringmetrics.pdf).
This metric counts the quantity of matched
symbols (words in this case) between two
sentences.

Furthermore, we repeated this experiment using
the Matching Coefficient similarity metric to bet-
ter tunning the algorithm and to evaluate if the re-
sults behave in a similar way when T changes. In
both cases, we use the implementation provided in
the SimMetrics library.

As those results shows, when T decrease the ac-
curacy decrease too. In practice, for the values of
T lower than 0.6 the results are worse than 61.4%
using the Dagging classifier in the 10 fold cross-
validation. For that reason, as was mentioned in 2,
we only tried to apply the first stage for values of
T ≥ 0.6 .

We evaluated our system in the challenge Task
4B: Sentiment Analysis in Twitter, using the pro-
vided training and test data of this challenge.
Based on the classifier obtained in the training pro-
cess we tested our system over the test dataset
achieving values of %CCI up to 55.4. Table 3
show detailed results for each of the 5 different
sources.

4 Conclusions and Future Works

Our system was based on an approach that follows
two stages to classify the polarity of tweets. Re-
gardless the fact that our system behaves worse
than the average, we consider that the approach is
suitable to deal with SA, since our results are close
to the average. As future works we will study
other approaches in order to encourage further de-
velopments of this proposal. Several issues could
be adjusted, for example, other distances should be
tested and evaluated such as DLED (Double Lev-
enshteins Edit Distance) (Fernández et al., 2012).
Also, features that encode information about the
presence of negation and opposition words could
be very useful.

Acknowledgements

This research work has been partially funded
by the University of Alicante, Generalitat Va-
lenciana, Spanish Government and the European
Commission through the projects, ”Tratamiento
inteligente de la informacin para la ayuda a la toma
de decisiones” (GRE12-44), ATTOS (TIN2012-
38536-C03-03), LEGOLANG (TIN2012-31224),
SAM (FP7-611312), FIRST (FP7-287607) and
ACOMP/2013/067.

References
D. Coomans and D.L. Massart. 1982. Alternative k-

nearest neighbour rules in supervised pattern recog-
nition : Part 1. k-nearest neighbour classification by
using alternative voting rules. Analytica Chimica
Acta, 136(0):15–27.

Antonio Fernández, Yoan Gutiérrez, Héctor Dávila,
Alexander Chávez, Andy González, Rainel Estrada,
Yenier Castañeda, Sonia Vázquez, Andrés Montoyo,
and Rafael Muñoz. 2012. Umcc dlsi: Multidimen-
sional lexical-semantic textual similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
608–616, Montréal, Canada, 7-8 June. Association
for Computational Linguistics.

Javi Fernández, Yoan Gutiérrez, José M Gómez, Patri-
cio Martınez-Barco, Andrés Montoyo, and Rafael
Munoz. 2013. Sentiment analysis of spanish tweets
using a ranking algorithm and skipgrams. Proc. of
the TASS workshop at SEPLN 2013. IV Congreso
Español de Informática, pages 17–20.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1–6.

Yoan Gutiérrez, Andy González, Roger Pérez, José I.
Abreu, Antonio Fernández Orquı́n, Alejandro Mos-
quera, Andrés Montoyo, Rafael Muñoz, and Franc
Camara. 2013. Umcc dlsi-(sa): Using a ranking
algorithm and informal features to solve sentiment
analysis in twitter. In Second Joint Conference on
Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International

730



Workshop on Semantic Evaluation (SemEval 2013),
pages 443–449, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.

Vladimir Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and rever-
sals. Cybernetics and Control Theory, 10(8):707–
710. Original in Doklady Akademii Nauk SSSR
163(4): 845–848 (1965).

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.

Lluı́s Padró and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ’02,
pages 79–86, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 417–424, Stroudsburg, PA,
USA. Association for Computational Linguistics.

731


