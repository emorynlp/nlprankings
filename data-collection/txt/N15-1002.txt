



















































Predicate Argument Alignment using a Global Coherence Model


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 11–20,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Predicate Argument Alignment using a Global Coherence Model

Travis Wolfe, Mark Dredze, and Benjamin Van Durme
Johns Hopkins University

Baltimore, MD, USA

Abstract

We present a joint model for predicate argu-
ment alignment. We leverage multiple sources
of semantic information, including temporal
ordering constraints between events. These
are combined in a max-margin framework
to find a globally consistent view of entities
and events across multiple documents, which
leads to improvements over a very strong local
baseline.

1 Introduction

Natural language understanding (NLU) requires
analysis beyond the sentence-level. For example,
an entity may be mentioned multiple times in a dis-
course, participating in various events, where each
event may itself be referenced elsewhere in the
text. Traditionally the task of coreference resolution
has been defined as finding those entity mentions
within a single document that co-refer, while cross-
document coreference resolution considers a wider
discourse context across many documents, yet still
pertains strictly to entities.

Predicate argument alignment, or entity-event
cross-document coreference resolution, enlarges the
set of possible co-referent elements to include the
mentions of situations in which entities participate.
This expanded definition drives practitioners to-
wards a more complete model of NLU, where sys-
tems must not only consider who is mentioned, but
also what happened. However, despite the drive to-
wards an expanded notion of discourse, models typ-
ically are formulated with strong notions of local-
independence: viewing a multi-document task as
one limited to individual pairs of sentences. This
creates a mis-match between the goals of such work
– considering entire documents – with the systems –
consider individual sentences.

In this work, we consider a system that takes a
document level view in considering coreference for
entities and predictions: the task of predicate ar-
gument linking. We treat this task as a global in-
ference problem, leveraging multiple sources of se-
mantic information identified at the document level.
Global inference for this problem is mostly unex-
plored, with the exception of Lee et al. (2012) (dis-
cussed in § 8). Especially novel here is the use of
document-level temporal constraints on events, rep-
resenting a next step forward on the path to full un-
derstanding.

Our approach avoids the pitfalls of local infer-
ence while still remaining fast and exact. We use
the pairwise features of a very strong predicate argu-
ment aligner (Wolfe et al., 2013) (competitive with
the state-of-the-art (Roth, 2014)), and add quadratic
factors that constrain local decisions based on global
document information. These global factors lead
to superior performance compared to the previous
state-of-the-art. We release both our code and data.1

2 Model

Consider the two sentences from the document pair
shown in Figure 1. These sentences describe the
same event, although with different details. The
source sentence has four predicates and four ar-
guments, while the target has three predicates and
three arguments. In this case, one of the predicates
from each sentence aligns, as do three of the argu-
ments. We also show additional information poten-
tially helpful to determining alignments: temporal
relations between the predicates. The goal of predi-
cate argument alignment is to assign these links in-
dicating coreferent predicates and arguments across
a document pair (Roth and Frank, 2012).

Previous work by Wolfe et al. (2013) formulated

1https://github.com/hltcoe/parma2

11



Figure 1: An example analysis and predicate argument alignment task between a source and target document.
Predicates appear as hollow ovals, have blue mentions, and are aligned considering their arguments (dashed lines).
Arguments, in black diamonds with green mentions, represent a document-level entity (coreference chain), and are
aligned using their predicate structure and mention-level features. The alignment choices appear in the middle in red.
Temporal relation information is lifted into the global inference over alignments.

this as a binary classification problem: given a pair
of arguments or predicates, construct features and
score the pair, where scores above threshold indi-
cate links. A binary classification framework has ad-
vantages: it’s fast since individual decisions can be
made quickly, but it comes at the cost of global in-
formation across links. The result may be links that
conflict in their interpretation of the document. Fig-
ure 1 makes clear that jointly considering all links at
once can aid individual decisions, for example, by
including temporal ordering of predicates.

The global nature of this task is similar to word
alignment for machine translation (MT). Many sys-
tems consider alignment links between words indi-
vidually, selecting the best link for each word inde-
pendently of the other words in the sentence. Just as
with an independent linking strategy in predicate ar-
gument alignment, this can lead to inconsistencies in
the output. Lacoste-Julien et al. (2006) introduced a
model that jointly resolved word alignments based
on the introduction of quadratic variables, factors
that depend on two alignment decisions which char-
acterize patterns that span word-word links. Their
approach achieved improved results even in the pres-
ence of little training data.

We present a global predicate argument alignment
model based on considering quadratic interactions
between alignment variables to captures patterns we
expect in coherent discourse. We introduce factors
which are comprised of a binary variable, multiple
quadratic constraints on that variable, and features
that determine the cost associated with that vari-
able in order to characterize the dependence between
alignment decisions.

While the mathematical framework we use is sim-
ilar to Lacoste-Julien et al. (2006), predicate argu-
ment alignment greatly differs from word alignment;
thus our joint factors are based on different sources
of regularity. Word alignment favors monotonic-
ity in word order, but this effect is very weak in
predicate argument alignment: aligned items can be
spread throughout a document, and are often nested,
gapped, or shuffled. Instead, we encode assump-
tions about consistency of temporal relations be-
tween coreferent events, coherence between predi-
cates and arguments that appear in both documents,
and fertility (to prevent over-alignment). We also
note that our setting has much less data than typical
word alignment tasks, as well as richer features that
utilize semantic resources.

12



Notation An alignment between an item indexed
by i in the source document and j in the target docu-
ment is represented by variable zij ∈ {0, 1}, where
zij = 1 indicates that items i and j are aligned. In
some cases, we will explicitly indicate when the two
items are predicates as zpij ; an argument alignment
will be zaij . We represent all alignments for a docu-
ment pair as matrix z.

For clarity, we omit any variable representing
observed data when discussing feature functions;
alignment variables are endowed with this informa-
tion. For each pair of items we use “local” fea-
ture functions f(·) and corresponding parameters
w, which capture the similarity between two items
without the context of other alignments.

sij = w · f(zij) (1)

where sij is the score of linking items i and j.
Using only local features, our system would

greedily select alignments. To capture global as-
pects we add joint factors that capture effects be-
tween alignment variables. Each joint factor φ is
comprised of a constrained binary variable zφ asso-
ciated with features f(φ) that indicates when the fac-
tor is active. Together with parameters w these form
additional scores sφ for the objective:

sφ = w · f(φ) (2)

The full linear scoring function on alignments
sums over both local similarity and joint factors:∑

ij

sijzij +
∑
φ∈Φ

sφzφ. (3)

Lastly, it is convenient to describe the local fea-
ture functions and their corresponding alignment
variable as factors with no constraints, and we will
do so when describing the full score function.

3 Local Factors

Local factors encode features based on the men-
tion pair, which include a wide variety of simi-
larity measures, e.g. whether two headwords ap-
pear as synonyms in WordNet, gender agreement
based on possessive pronouns. We adopt the fea-
tures of Wolfe et al. (2013), a strong baseline system

which doesn’t use global inference.2 These features
are built on top of a variety of semantic resources
(PPDB (Ganitkevitch et al., 2013), WordNet (Miller,
1995), FrameNet (Baker et al., 1998)) and methods
for comparing mentions (tree edit distance (Yao et
al., 2013), string transducer (Andrews et al., 2012)).

4 Joint Factors

Our goal is to develop joint factors that improve over
the feature rich local factors baseline by considering
global information.

Fertility A common mistake when making inde-
pendent classification decisions is to align many
source items to a single target item. While each link
looks promising on its own, they clearly cannot all
be right. Empirically, the training set reveals that
many to one alignments are uncommon; thus many
to one predictions are likely errors. We add a fertility
factor for predicates and arguments, where fertility
is defined as the number of links to an item. Higher
fertilities are undesired and are thus penalized. For-
mally, for matrix z, the fertility of a row i or column
j is the sum of that row or column. We discuss fer-
tility in terms of rows below.

We include two types of fertility factors. First,
factor φfert1 distinguishes between rows with at least
one link from those with none. For row i, we add one
instance of the linear factor φfert1 with constraints

zφfert1 ≥ zij ∀j (4)

The cost associated with zφfert1 , which we will re-
fer to as sfert1, will be incurred any time an item is
mentioned in both documents. For data sets with
many singletons, sfert1 more strongly penalizes non-
singleton rows, reflecting this pattern in the training
data. We make sfert1 parametric, where the features
of the φfert1 factor allow us to learn different weights
for predicates and arguments, as well as the size of
the row, i.e. number of items in the pairing.

The second fertility factory φfert2 considers items
with a fertility greater than one, penalizing items for
having too many links. Its binary variable has the

2Some features inspect the apparent predicate argument
structure, based on things like dependency parses, but the model
may not inspect more than one of its own decisions (joint fac-
tors) while scoring an alignment.

13



quadratic constraints:

zφfert2 ≥ zijzik ∀j < k (5)

This factor penalizes rows that have fertility of at
least two, but does not distinguish beyond that. An
alternative would be to introduce a factor for every
pair of variables in a row, each with one constraint.
This would heavily penalize fertilities greater than
two. We found that the resulting quadratic program
took longer to solve and gave worse results.

Since documents have been processed to identify
in-document coreference chains, we do not expect
multiple arguments from a source document to align
to a single target item. For this reason, we expect
φfert2 for arguments to have a large negative weight.
In contrast, since predicates do not form chains, we
may have multiple source predicates for one target.

We note an important difference between our
fertility factor compared with Lacoste-Julien et al.
(2006). We parameterize fertility for only two cases
(1 and 2) whereas they consider fertility factors from
2 to D. We do not parameterize fertilities higher
than two because they are not common in our dataset
and come at a high computational cost.

The features f(φ) for both φfert1 and φfert2 are an
intercept feature (which always fires), indicator fea-
tures for whether this row corresponds to an argu-
ment or a predicate, and a discretized feature for how
many alignments are in this row.

Predicate Argument Structure We expect struc-
ture among links that involve a predicate and its as-
sociated arguments. Therefore, we add joint factors
that consider a predicate and its associated align-
ments: the predicate argument structure. We deter-
mine this structure from a dependency parse, though
the idea is general to any semantic binding, e.g.
FrameNet or Propbank style parses. Given a co-
herent discourse, there are several expected types of
patterns in the PAS; we add factors for these.

Predicate-centric We begin with a predicate-
centric factor, which views scores an alignment be-
tween predicates based on their arguments, i.e. the
two predicates share the same arguments. Ideally,
two predicates can only align when their arguments
are coreferent. However, in practice we may in-
correctly resolve argument links, or there may be

implicit arguments that do not appear as syntactic
dependencies of the predicate trigger. Therefore,
we settle for a weaker condition, that there should
be some overlap in the arguments of two coreferent
predicates.

For every predicate alignment zpij , we add a factor
φpsa whose score spsa is a penalty for having no ar-
gument overlap; predicates share arguments (psa).
To constrain the variable of φpsa, we add a quadratic
constraint that considers every possible pair of argu-
ment alignments that might overlap:

zφpsa ≥ zpij
(
1− max

k∈args(pi)
l∈args(pj)

zakl
)

(6)

where args(pi) finds the indices of all arguments
governed by the predicate pi.

Entity-centric We expect similar behavior from
arguments (entities). If an entity appears in two doc-
uments, it is likely that this entity will be mentioned
in the context of a common predicate, i.e. arguments
share predicates (asp). For a given argument align-
ment zaij we add quadratic constraints so that zφasp
represents a penalty for two arguments not sharing a
single predicate:

zφasp ≥ zaij
(
1− max

k∈preds(ai)
l∈preds(aj)

zpkl
)

(7)

where preds(ai) finds the indices of all predicates
that govern any mention of argument ai.

The features f(φ) for both psa and asp are an
intercept feature and a bucketed count of the size of
args(pi)× args(pj) or preds(ai)×preds(aj) respec-
tively.

Temporal Information Temporal ordering, in
contrast to textual ordering, can indicate when pred-
icates cannot align: we expect aligned predicates
in both documents to share the same temporal re-
lations. SemEval 2013 included a task on predict-
ing temporal relations between events (UzZaman et
al., 2013). Many systems produced partial rela-
tions of events in a document based on lexical as-
pect and tense, as well as discourse connectives like
“during” or “after”. We obtain temporal relations
with CAEVO, a state-of-the-art sieve-based system
(Chambers et al., 2014).

14



TimeML (Pustejovsky et al., 2003), the format for
specifying temporal relations, defines relations be-
tween predicates (e.g. immediately before and si-
multaneous), each with an inverse (e.g. immediately
after and simultaneous respectively). We will refer
to a relation as R and its inverse as R−1. Suppose
we had pa and pb in the source document, px and py
in the target document, and paR1pb, pxR2py. Given
this configuration the following alignments conflict
with the in-doc relations:

zax zby zay zbx In-Doc Relations
* * 1 1 R1 = R2
1 1 * * R1 = R−12

where 1 means there is a link and * means there is
a link or no link (wildcard). The simplest example
that fits this pattern is: ‘a before b’, ‘x before y’, ‘a
corefers with y’, and ‘b corefers with x’ implies a
conflict.

We introduce a factor that penalizes these conflict-
ing configurations. In every instance where the pre-
dicted temporal relation for a pair of predicate align-
ments matches one of the conflict patterns above, we
add a factor using zφtemp :

zφtemp ≥ zayzbx
if paR1pb, pxR2py, R1 = R2

zφtemp ≥ zaxzby
if paR1pb, pxR2py, R1 = R−12

(8)

Thus sφtemp is the cost of disagreeing with the in-
doc temporal relations. This is a general technique
for incorporating relational information into coref-
erence decisions. It only requires specifying when
two relations are incompatible, e.g. spouseOf and
siblingOf are incompatible relations (in most
states). We leave this for future work.

Since CAEVO gives each relation prediction a
probability, we incorporate this into the feature by
indicating the probability of a conflict not arising:

f(φtemp) = log
(
1− p(R1)p(R2) + �

)
(9)

� avoids large negative values since CAEVO proba-
bilities are not perfectly calibrated. We use � = 0.1,
allowing feature values of at most −2.3.
Summary The objective is a linear function over
binary variables. There is a local similarity score

def train(alignments):
w = init_weights()
working_set = set()
while True:
xi = solve_ILP(w, working_set)
c = most_violated_constraint(w, alignments)
working_set.add(c)
if hinge(c, w) < xi:
break

def most_violated_constraint(w, alignments):
delta_features = vector()
loss = 0
for z in alignments:
z_mv = make_ILP(z)
for phi in factors:
costs = dot(w, phi.features)
z_mv.add_terms(costs, phi.vars)
z_mv.add_constraints(phi.constraints)

solve_ILP(z_mv)
mu = (z.size + k) / (avg_z_size + k)
delta_features += mu * (f(z) - f(z_mv))
loss += mu * Delta(z, z_mv)

return Constraint(delta_features, loss)

def hinge(c, w):
return max(0, c.loss - dot(w, c.delta_features))

Figure 2: Learning algorithm (caching and ILP solver
not shown). The sum in each constraint is performed once
when finding the constraint, and implicitly thereafter.

coefficient on every alignment variable, and a joint
factor similarity score on every quadratic variable.
These quadratic variables are constrained by prod-
ucts of the original alignment variables. Decoding
an alignment requires solving this quadratically con-
strained integer program; in practice is can be solved
quickly without relations.

5 Inference

Learning We use the supervised structured SVM
formulation of Joachims et al. (2009). As is common
in structure prediction we use margin rescaling and
1 slack variable, with the structural SVM objective:

min
w
||w||22 + Cξ

s.t. ξ ≥ 0

ξ +
N∑
i=1

w · f(zi) ≥
N∑
i=1

w · f(ẑi) + ∆(zi, ẑi)

∀ẑi ∈ Zi
(10)

where Zi is the set of all possible alignments that
have the same shape as zi.

15



The score function for an alignment uses three
types of terms: weights, features, and alignment
variables. When we decode, we take the product
of the weights and the features to get the costs for
the ILP (e.g. sφ = w · f(φ)). When we optimize our
SVM objective, we take the product of the alignment
variables and the features to get modified features
for the SVM:

f(z) =
∑
ij

zijf(zij) +
∑
φ∈Φ

zφf(φ) (11)

Since we cannot iterate over the exponentially
many margin constraints, we solve for this optimiza-
tion using the cutting-plane learning algorithm. This
algorithm repeatedly asks the “separation oracle” for
the most violated SVM constraint, which finds this
constraint by solving:

arg max
ẑ1...ẑN

∑
i

w · f(ẑi) + ∆(zi, ẑi) (12)

subject to the constraints defined by the joint fac-
tors. When the separation oracle returns a constraint
that is not violated or is already in the working set,
then we have a guarantee that we solved the original
SVM problem with exponentially many constraints.
This is the most time-consuming aspect of learning,
but since the problem decomposes over document
alignments, we cache solutions on a per document
alignment basis. With caching, we only call the sep-
aration oracle around 100-300 times.

We implement the separation oracle using an ILP
solver, CPLEX,3 due to complexity of the discrete
optimization problem: there are 2m

n
possible align-

ments for and m×n alignment grid. In practice this
is solved very efficiently, taking less than a third of
a second per document alignment on average. We
would like ∆ to be F1, but we need a decomposable
loss to include it in a linear objective (Taskar et al.,
2003). Instead, we use Hamming loss as a surrogate,
as in Lacoste-Julien et al. (2006).

Our training data is heavily biased towards nega-
tive examples, performing poorly on F1 since preci-
sion and recall are unbalanced. We use an asym-
metric version of Hamming loss that incurs cFP
cost for predicting an alignment for two unaligned

3http://www-01.ibm.com/software/
commerce/optimization/cplex-optimizer/

items and cFN for predicting no alignment for two
aligned items. We fixed cFP = 1 and tuned cFN ∈
{1, 2, 3, 4} on dev data. Additionally we found it
useful to tune the scale of the loss function across
{12 , 1, 2, 4}. Previous work, such as Joachims et al.
(2009), use a hand-chosen constant for the scale of
the Hamming loss, but we observe some sensitivity
in this parameter and choose to optimize it.

Decoding Following Wolfe et al. (2013), we tune
the threshold for classification τ on dev data to max-
imize F1 (via linesearch). For SVMs τ is typically
fixed at 0: this is not necessarily good practice when
your training loss differs from test loss (Hamming
vs F1). In our case this extra parameter is worth al-
locating a portion of training data to enable tuning.
Tuning τ addresses the same problem as using an
asymmetric Hamming loss, but we found that do-
ing both led to better results.4 Since we are using a
global scoring function rather than a set of classifi-
cations, τ is implemented as a test-time unary factor
on every alignment.

6 Experiments

Data We consider two datasets for evaluation. The
first is a cross-document entity and event corefer-
ence resolution dataset called the Extended Event
Coref Bank (EECB) created by Lee et al. (2012) and
based on a corpus from Bejan and Harabagiu (2010).
The dataset contains clusters of news articles taken
from Google News with annotations about corefer-
ence over entities and events. Following the proce-
dure of Wolfe et al. (2013), we select the first doc-
ument in every cluster and pair it with every other
document in the cluster.

The second dataset (RF) comes from Roth and
Frank (2012). The dataset contains pairs of news
articles that describe the same news story, and are
annotated for predicate links between the document
pairs. Due to the lack of annotated arguments, we
can only report predicate linking performance and
the psa and asp factors do not apply. Lastly, the
size of the RF data should be noted as it is much
smaller than EECB: the test set has 60 document
pairs and the dev set has 10 document pairs.

4Only tuning τ performed almost as well as tuning τ and
the Hamming loss, but not tuning τ performed much worse than
only tuning the Hamming loss at train time.

16



Both datasets are annotated with parses and in-
document coreference labels provided by the toolset
of Napoles et al. (2012)5 and are available with our
code release. Due to the small data size, we use k-
fold cross validation for both datasets. We choose
k = 10 for RF due to its very small size (more
folds give more training examples) and k = 5 on
EECB to save computation time (amount of training
data in EECB is less of a concern). Hyperparam-
eters were chosen by hand using using cross vali-
dation on the EECB dataset using F1 as the crite-
ria (rather than Hamming). Figures report averages
across these folds.

Systems Following Roth and Frank (2012) and
Wolfe et al. (2013) we include a Lemma baseline
for identifying alignments which will align any two
predicates or arguments that have the same lemma-
tized head word.6 The Local baseline uses the same
features as Wolfe et al., but none of our joint fac-
tors. In addition to running our joint model with all
factors, we measure the efficacy of each individual
factor by evaluating each with the local features.

For evaluation we use a generous version of F1
that is defined for alignment labels composed of
sure, Gs, and possible links, Gp and the system’s
proposed links H (following Cohn et al. (2008),
Roth and Frank (2012) and Wolfe et al. (2013)).

P =
|H ∩Gp|
|H| R =

|H ∩Gs|
|Gs| F =

2PR
P +R

Note that the EECB data does not have a sure and
possible distinction, so Gs = Gp, resulting in stan-
dard F1. In addition to F1, we separately measure
predicate and argument F1 to demonstrate where our
model makes the largest improvements.

We performed a one-sided paired-bootstrap test
where the null hypothesis was that the joint model
was no better than the Local baseline (described in
Koehn (2004)). Cases where p < 0.05 are bolded.

5https://github.com/cnap/anno-pipeline
6The lemma baseline is obviously sensitive to the lemma-

tizer used. We used the Stanford CoreNLP lemmatizer (Man-
ning et al., 2014) and found it yielded slightly better results than
previously reported as the lemma baseline (Roth and Frank,
2012), so we used it for all systems to ensure fairness and that
the baseline is as strong as it could be.

7 Results

Results for EECB and RF are reported in Table 7. As
previously reported, using just local factors (features
on pairs) improves over lemma baselines (Wolfe et
al., 2013). The joint factors make statistically sig-
nificant gains over local factors in almost all experi-
ments. Fertility factors provide the largest improve-
ments from any single constraint. A fertility penalty
actually allows the pairwise weights to be more op-
timistic in that they can predict more alignments
for reasonable pairs, allowing the fertility penalty to
ensure only the best is chosen. This penalty also
prevents the “garbage collecting” effect that arises
for instances that have rare features (Brown et al.,
1993).

Temporal constraints are relatively sparse, ap-
pearing just 2.8 times on average. Nevertheless,
it was very helpful across all experiments, though
only statistically significantly on the RF dataset.
This is one of the first results to demonstrate ben-
efits of temporal relations affecting an downstream
task. Perhaps surprisingly, these improvements re-
sult from a a temporal relation system that has rela-
tively poor absolute performance. Despite this, im-
provements are possibly due to the orthogonal na-
ture of temporal information; no other feature cap-
tures this signal. This suggests that future work on
temporal relation prediction may yield further im-
provements and deserves more attention as a useful
feature for semantic tasks in NLP.

The predicate-centric factors improved perfor-
mance significantly on both datasets. For the
predicate-centric factor, when a predicate was
aligned there is a 72.3% chance that there was
at least one argument aligned as well, compared
to only 14.1% of case of non-aligned predicates.
As mentioned before, the reason the former num-
ber isn’t 100% is primarily due to implicit argu-
ments and errors in argument identification. The
argument-centric features helped almost as much as
the predicate-centric version, but the improvements
were not significant on the EECB dataset. Run-
ning the same diagnostic as the predicate-centric
feature reveals similar support: in 57.1% of the cases
where an argument was aligned, at least one pred-
icate it partook in was aligned too, compared to
7.6% of cases for non-aligned arguments. Both the

17



EECB
F1 P R Arg F1 Arg P Arg R Pred F1 Pred P Pred R

Lemma 68.1 79.3 * 59.6 61.7 79.1 * 50.6 75.0 87.3 * 65.7
Local 73.0 75.8 70.5 67.7 76.3 60.8 78.7 81.4 76.2
+Fertility 77.1 * 83.9 * 71.3 66.6 80.9 * 56.6 82.8 * 87.4 * 78.7 *
+Predicate-centric 74.1 * 80.7 * 68.6 67.4 81.6 * 57.3 79.7 * 85.0 * 75.1
+Argument-centric 73.7 81.2 * 67.5 66.8 83.0 * 55.9 79.3 85.1 * 74.3
+Temporal 73.7 78.2 * 69.7 67.9 80.6 * 58.7 79.0 82.1 76.1
+All Factors 77.5 * 86.3 * 70.3 65.8 83.1 * 54.5 83.7 * 89.7 * 78.4 *

RF
Pred F1 Pred P Pred R

Lemma 52.4 47.6 58.2 *
Local 58.1 63.5 53.6
+Fertility 60.0 57.4 62.4 *
+Predicate-centric NA NA NA
+Argument-centric NA NA NA
+Temporal 59.0 57.4 60.6 *
+All factors 59.4 56.9 62.2 *

Figure 3: Cross validation results for EECB (above)
(Lee et al., 2012) and RF (left) (Roth and Frank,
2012). Statistically significant improvements from Lo-
cal marked * (p < 0.05 using a one-sided paired-
bootstrap test) and best results are bolded.

predicate- and argument-centric improve similarly
across both predicates and arguments on EECB.

While each of the joint factors all improve over
the baselines on RF, the full model with all the joint
factors does not perform as well as with some fac-
tors excluded. Specifically, the fertility model per-
forms the best. We attribute this small gap to lack
of training data (RF only contains 64 training docu-
ment pairs in our experiments), as this is not a prob-
lem on the larger EECB dataset.

Additionally, the joint models seem to trade pre-
cision for recall on the RF dataset compared to the
Local baseline. Note that both models are tuned to
maximize F1, so this tells you more about the shape
of the ROC curve as opposed to either models’ abil-
ity to achieve either high precision or recall. Since
we don’t see this behavior on the EECB corpus, it is
more likely that this is a property of the data than the
model.

8 Related Work

The task of predicate argument linking was intro-
duced by Roth and Frank (2012), who used a graph
parameterized by a small number of semantic fea-
tures to express similarities between predicates and
used min-cuts to produce an alignment. This was
followed by Wolfe et al. (2013), who gave a locally-
independent, feature-rich log-linear model that uti-
lized many lexical semantic resources, similar to the

sort employed in RTE challenges.
Lee et al. (2012) considered a similar problem

but sought to produce clusters of entities and events
rather than an alignment between two documents
with the goal of improving coreference resolution.
They used features which consider previous event
and entity coreference decisions to make future
coreference decisions in a greedy manner. This dif-
fers from our model which is built on non-greedy
joint inference, but much of the signal indicating
when two mentions corefer or are aligned is similar.

In the context of in-document coreference reso-
lution, Recasens et al. (2013) sought to overcome
the problem of opaque mentions7 by finding high-
precision paraphrases of entities by pivoting off
verbs mentioned in similar documents. We address
the issue of opaque mentions not by building a para-
phrase table, but by jointly reasoning about entities
that participate in coreferent events (c.f. §4); the ap-
proaches are complementary.

In this work we incorporate ordering information
of events. Though we consider it an upstream task,
there is a line of work trying to predict temporal rela-
tions between events (Pustejovsky et al., 2003; Mani
et al., 2006; Chambers et al., 2014). Our results in-
dicate this is a useful source of information, one of
the first results to show an improvement from this

7A lexically disparate description of an entity.

18



type of system (Glavaš and Šnajder, 2013).
We utilize an ILP to improve upon a pipelined

system, similar to Roth and Yih (2004), but our work
differs in that we do not use piecewise-trained clas-
sifiers. Our local similarity scores are calibrated ac-
cording to a global objective by propagating the gra-
dient back from the loss to every parameter in the
model. When using piecewise training, local clas-
sifiers must focus more on recall (in the spirit of
Weiss and Taskar (2010)) than they would for an or-
dinary classification task with no global objective.
Our method trains classifiers jointly with a global
convex objective. While our training procedure re-
quires decoding an integer program, the parameters
we learn are globally optimal.

9 Conclusion

We presented a max-margin quadratic cost model
for predicate argument alignment, seeking to ex-
ploit discourse level semantic features to improve
on previous, locally independent approaches. Our
model includes factors that consider fertility of pred-
icates and arguments, the predicate argument struc-
ture present in coherent discourses, and soft con-
straints on predicate coreference determined by a
temporal relation classifier. We have shown that this
model significantly improves upon prior work which
uses extensive lexical resources but without the ben-
efit of joint inference. Additionally, this is one of the
first demonstrations of the benefits of temporal rela-
tion identification. Overall, this work demonstrates
the benefits of considering global document infor-
mation as part of natural language understanding.

Future work should extend the problem formu-
lation of predicate argument alignment to consider
incremental linking: starting with a pair of docu-
ments, perform linking, and then continue to add
in documents over time. This problem formula-
tion would capture the evolution of a breaking news
story, which closely matches the type of data (news
articles) considered in this work (EECB and RF
datasets). This formulation ties into existing work
on news summarization, topic detection and track-
ing, an multi-document NLU. This goes hand with
work on better intra-document relation prediction
methods, such as the temporal relation model used
in this work, to lead to better joint linking decisions.

References
Nicholas Andrews, Jason Eisner, and Mark Dredze.

2012. Name phylogeny: A generative model of string
variation. In EMNLP-CoNLL, pages 344–355. ACL.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
ACL ’98, pages 86–90, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’10, pages 1412–1422, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the
Workshop on Human Language Technology, HLT ’93,
pages 202–205, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering with
a multi-pass architecture. Transactions of the Associ-
ation for Computational Linguistics, 2.

Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Comput. Linguist.,
34(4):597–614, December.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages 758–
764, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

Goran Glavaš and Jan Šnajder. 2013. Recognizing
identical events with graph kernels. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 797–803, Sofia, Bulgaria, August. Association
for Computational Linguistics.

Thorsten Joachims, Thomas Finley, and Chun-Nam John
Yu. 2009. Cutting-plane training of structural svms.
Mach. Learn., 77(1):27–59, October.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Simon Lacoste-Julien, Benjamin Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via

19



quadratic assignment. In Robert C. Moore, Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,
editors, HLT-NAACL. The Association for Computa-
tional Linguistics.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL ’12, pages 489–500, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 753–760. Associa-
tion for Computational Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.

George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39–41.

Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.

James Pustejovsky, Jos Castao, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. Timeml: Robust specification of event
and temporal expressions in text. In in Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5).

Marta Recasens, Matthew Can, and Daniel Jurafsky.
2013. Same referent, different words: Unsupervised
mining of opaque coreferent mentions. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 897–906,
Atlanta, Georgia, June. Association for Computational
Linguistics.

Michael Roth and Anette Frank. 2012. Aligning pred-
icate argument structures in monolingual comparable
texts: a new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics - Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ’12, pages 218–227,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In In Proceedings of CoNLL-2004, pages 1–8.

Michael Roth. 2014. Inducing Implicit Arguments via
Cross-document Alignment: A Framework and its Ap-
plications. Ph.D. thesis, Heidelberg University, June.

Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin markov networks. MIT Press.

Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluat-
ing time expressions, events, and temporal relations.
In Second Joint Conference on Lexical and Compu-
tational Semantics (*SEM), Volume 2: Proceedings of
the Seventh International Workshop on Semantic Eval-
uation (SemEval 2013), pages 1–9, Atlanta, Georgia,
USA, June. Association for Computational Linguis-
tics.

David Weiss and Benjamin Taskar. 2010. Structured pre-
diction cascades. Journal of Machine Learning Re-
search - Proceedings Track, 9:916–923.

Travis Wolfe, Benjamin Van Durme, Mark Dredze,
Nicholas Andrews, Charley Bellar, Chris Callison-
Burch, Jay DeYoung, Justin Snyder, Jonathann Weese,
Tan Xu, and Xuchen Yao. 2013. Parma: A predicate
argument aligner. In Proceedings of the 51th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers). Association for Compu-
tational Linguistics, July.

Xuchen Yao, Benjamin Van Durme, Chris Callison-
burch, and Peter Clark. 2013. Answer extraction as
sequence tagging with tree edit distance. In In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL.

20


