



















































Question Condensing Networks for Answer Selection in Community Question Answering


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1746‚Äì1755
Melbourne, Australia, July 15 - 20, 2018. c¬©2018 Association for Computational Linguistics

1746

Question Condensing Networks for Answer Selection in Community
Question Answering

Wei Wu1, Xu Sun1, Houfeng Wang1,2

1MOE Key Lab of Computational Linguistics, Peking University, Beijing, 100871, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, 221009, China

{wu.wei, xusun, wanghf}@pku.edu.cn

Abstract

Answer selection is an important subtask
of community question answering (CQA).
In a real-world CQA forum, a question
is often represented as two parts: a sub-
ject that summarizes the main points of
the question, and a body that elaborates on
the subject in detail. Previous researches
on answer selection usually ignored the
difference between these two parts and
concatenated them as the question repre-
sentation. In this paper, we propose the
Question Condensing Networks (QCN) to
make use of the subject-body relationship
of community questions. In this model,
the question subject is the primary part of
the question representation, and the ques-
tion body information is aggregated based
on similarity and disparity with the ques-
tion subject. Experimental results show
that QCN outperforms all existing models
on two CQA datasets.

1 Introduction

Community question answering (CQA) has seen a
spectacular increase in popularity in recent years.
With the advent of sites like Stack Overflow1 and
Quora2, more and more people can freely ask any
question and expect a variety of answers. With
the influx of new questions and the varied qual-
ity of provided answers, it is very time-consuming
for a user to inspect them all. Therefore, develop-
ing automated tools to identify good answers for a
question is of practical importance.

A typical example for CQA is shown in Table 1.
In this example, Answer 1 is a good answer, be-
cause it provides helpful information, e.g., ‚Äúcheck

1https://stackoverflow.com/
2https://www.quora.com/

it to the traffic dept‚Äù. Although Answer 2 is rele-
vant to the question, it does not contain any useful
information so that it should be regarded as a bad
answer.

From this example, we can observe two charac-
teristics of CQA that ordinary QA does not pos-
sess. First, a question includes both a subject that
gives a brief summary of the question and a body
that describes the question in detail. The question-
ers usually convey their main concern and key in-
formation in the question subject. Then, they pro-
vide more extensive details about the subject, seek
help, or express gratitude in the question body.
Second, the problem of redundancy and noise is
prevalent in CQA (Zhang et al., 2017). Both ques-
tions and answers contain auxiliary sentences that
do not provide meaningful information.

Previous researches (Tran et al., 2015; Joty
et al., 2016) usually treat each word equally in
the question and answer representation. How-
ever, due to the redundancy and noise problem,
only part of text from questions and answers is
useful to determine the answer quality. To make
things worse, they ignored the difference between
question subject and body, and simply concate-
nated them as the question representation. Due
to the subject-body relationship described above,
this simple concatenation can aggravate the re-
dundancy problem in the question. In this paper,
we propose the Question Condensing Networks
(QCN) to address these problems.

In order to utilize the subject-body relationship
in community questions, we propose to treat the
question subject as the primary part of the ques-
tion, and aggregate the question body information
based on similarity and disparity with the ques-
tion subject. The similarity part corresponds to
the information that exists in both question sub-
ject and body, and the disparity part corresponds
to the additional information provided by the ques-

https://stackoverflow.com/
https://www.quora.com/


1747

Question Subject Checking the history of the car.
Question body How can one check the history of the car like maintenance, accident or service

history. In every advertisement of the car, people used to write ‚ÄúAccident Free", but
in most cases, car have at least one or two accident, which is not easily detectable
through Car Inspection Company. Share your opinion in this regard.

Answer1 Depends on the owner of the car.. if she/he reported the accident/s i believe u can
check it to the traffic dept.. but some owners are not doing that especially if its only
a small accident.. try ur luck and go to the traffic dept..

Answer2 How about those who claim a low mileage by tampering with the car fuse box? In
my sense if you‚Äôre not able to detect traces of an accident then it is probably not
worth mentioning... For best results buy a new car :)

Table 1: An example question and its related answers in CQA. The text is shown in its original form,
which may contain errors in typing.

tion body. Both information can be important for
question representation. In our model, they are
processed separately and the results are combined
to form the final question representation.

In order to reduce the impact of redundancy
and noise in both questions and answers, we pro-
pose to align the question-answer pairs using the
multi-dimensional attention mechanism. Differ-
ent from previous attention mechanisms that com-
pute a scalar score for each token pair, multi-
dimensional attention, first proposed in Shen et al.
(2018), computes one attention score for each di-
mension of the token embedding. Therefore, it can
select the features that can best describe the word‚Äôs
specific meaning in the given context. Therefore,
we can learn the interaction between questions and
answers more accurately.

The main contributions of our work can be sum-
marized as follows:

‚Ä¢ We propose to treat the question subject and
the question body separately in community
question answering. We treat the question
subject as the primary part of the question,
and aggregate the question body information
based on similarity and disparity with the
question subject.

‚Ä¢ We introduce a new method that uses the
multi-dimensional attention mechanism to
align question-answer pair. With this at-
tention mechanism, the interaction between
questions and answers can be learned more
accurately.

‚Ä¢ Our proposed Question Condensing Net-
works (QCN) achieves the state-of-the-art

performance on two SemEval CQA datasets,
outperforming all exisiting SOTA models by
a large margin, which demonstrates the effec-
tiveness of our model.3

2 Task Description

A community question answering consists of four
parts, which can be formally defined as a tuple of
four elements (S,B,C, y). S = [s1, s2, ..., sl] de-
notes the subject of a question whose length is l,
where each si is a one-hot vector whose dimen-
sion equals the size of the vocabulary. Similarly,
B = [b1, b2, ..., bm] denotes the body of a ques-
tion whose length is m. C = [c1, c2, ..., cn] de-
notes an answer corresponding to that question
whose length is n. y ‚àà Y is the label represent-
ing the degree to which it can answer that ques-
tion. Y = {Good,PotentiallyUseful,Bad} where
Good indicates the answer can answer that ques-
tion well, PotentiallyUseful indicates the answer
is potentially useful to the user, and Bad indi-
cates the answer is just bad or useless. Given
{S,B,C}, the task of CQA is to assign a label to
each answer based on the conditional probability
Pr(y|S,B,C).

3 Proposed Model

In this paper, we propose Question Condensing
Networks (QCN) which is composed of the fol-
lowing modules. The overall architecture of our
model is illustrated in Figure 1.

3An implementation of our model is available at https:
//github.com/pku-wuwei/QCN.

https://github.com/pku-wuwei/QCN
https://github.com/pku-wuwei/QCN


1748

MLP

ùëÜùëíùëöùëè

ùêµùëíùëöùëè

ùê∂ùëíùëöùëè

ùê∂ùëüùëíùëù

ùëÜùëùùëéùëüùëé
ùëÜrùëíùëùùëÜùëúùëüùë°‚Ñé

ùê∂ùëéùë°ùë°

ùëÜùëéùë°ùë°

ùíîùë†ùë¢ùëö

ùíÑùë†ùë¢ùëö

ùë¶
ùëêùëúùëõùëêùëéùë°

ùëùùëüùëúùëóùëíùëêùë°

ùëêùëúùëõùëêùëéùë°

Figure 1: Architecture for Question Condensing Network (QCN). Each block represents a vector.

3.1 Word-Level Embedding

Word-level embeddings are composed of two
components: GloVe (Pennington et al., 2014)
word vectors trained on the domain-specific unan-
notated corpus provided by the task 4, and con-
volutional neural network-based character embed-
dings which are similar to (Kim et al., 2016). Web
text in CQA forums differs largely from normal-
ized text in terms of spelling and grammar, so
specifically trained GloVe vectors can model word
interactions more precisely. Character embedding
has proven to be very useful for out-of-vocabulary
(OOV) words, so it is especially suitable for noisy
web text in CQA.

We concatenate these two embedding vectors
for every word to generate word-level embeddings
Semb ‚àà Rd√ól,Bemb ‚àà Rd√óm,Cemb ‚àà Rd√ón,
where d is the word-level embedding size.

3.2 Question Condensing

In this section, we condense the question repre-
sentation using subject-body relationship. In most
cases, the question subject can be seen as a sum-
mary containing key points of the question, the
question body is relatively lengthy in that it needs
to explain the key points and add more details
about the posted question. We propose to cheat the
question subject as the primary part of the question
representation, and aggregate question body infor-
mation from two perspectives: similarity and dis-
parity with the question subject. To achieve this
goal, we use an orthogonal decomposition strat-
egy, which is first proposed by Wang et al. (2016),
to decompose each question body embedding into
a parallel component and an orthogonal compo-

4http://alt.qcri.org/semeval2015/
task3/index.php?id=data-and-tools

nent based on every question subject embedding:

bi,jpara =
bjemb ¬∑ s

i
emb

siemb ¬∑ siemb
siemb (1)

bi,jorth = b
j
emb ‚àí b

i,j
para (2)

All vectors in the above equations are of length d.
Next we describe the process of aggregating the
question body information based on the parallel
component in detail. The same process can be ap-
plied to the orthogonal component, so at the end
of the fusion gate we can obtain Sorth and Sorth
respectively.

The decomposed components are passed
through a fully connected layer to compute the
multi-dimensional attention weights. Here we use
the scaled tanh activation, which is similar to Shen
et al. (2018), to prevent large difference among
scores while it still has a range large enough for
output:

ai,jpara = c ¬∑ tanh
([
Wp1b

i,j
para + bp1

]
/c
)

(3)

where Wp1 ‚àà Rd√ód and bp1 ‚àà Rd are parame-
ters to be learned, and c is a hyper-parameter to be
tuned.

The obtained word-level alignment tensor
Apara ‚àà Rd√ól√óm is then normalized along the
third dimension to produce the attention weights
over the question body for each word in the ques-
tion subject. The output of this attention mecha-
nism is a weighted sum of the question body em-
beddings for each word in the question subject:

wi,jpara =
exp

(
ai,jpara

)
‚àëm

j=1 exp
(
ai,jpara

) (4)
siap =

m‚àë
j=1

wi,jpara ÔøΩ b
j
emb (5)

http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools
http://alt.qcri.org/semeval2015/task3/index.php?id=data-and-tools


1749

where ÔøΩ means point-wise product. This multi-
dimensional attention mechanism has the advan-
tage of selecting features of a word that can best
describe the word‚Äôs specific meaning in the given
context. In order to determine the importance be-
tween the original word in the question subject and
the aggregated information from the question body
with respect to this word, a fusion gate is utilized
to combine these two representations:

Fpara = œÉ (Wp2Semb +Wp3Sap + bp2) (6)

Spara = Fpara ÔøΩ Semb + (1‚àí Fpara)ÔøΩ Sap (7)

where Wp2,Wp3 ‚àà Rd√ód, and bp2 ‚àà Rd
are learnable parameters of the fusion gate, and
Fpara, Semb, Sap, Spara ‚àà Rd√ól. The final ques-
tion representation Srep ‚àà R2d√ól is obtained by
concatenating Spara and Sorth along the first di-
mension.

3.3 Answer Preprocessing

This module has two purposes. First, we try to
map each answer word from embedding space
Cemb ‚àà Rd√ón to the same interaction space
Crep ‚àà R2d√ón as the question. Second, similar to
Wang and Jiang (2017), a gate is utilized to con-
trol the importance of different answer words in
determining the question-answer relation:

Crep =œÉ (Wc1Cemb + bc1)ÔøΩ
tanh (Wc2Cemb + bc2)

(8)

where Wc1,Wc2 ‚àà Rd√ó2d and bc1, bc2 ‚àà R2d are
parameters to be learned.

3.4 Question Answer Alignment

We apply the multi-dimensional attention mech-
anism to the question and answer representa-
tion Srep and Crep to obtain word-level align-
ment tensor Aalign ‚àà R2d√ól√ón. Similar to the
multi-dimensional attention mechanism described
above, we can compute attention weights and
weighted sum for both the question representation

and the answer representation :

aÃÉi,jalign =Wa1s
i
rep +Wa2c

j
rep + ba (9)

ai,jalign = c ¬∑ tanh
(
aÃÉi,jalign/c

)
(10)

siai =
n‚àë

j=1

exp
(
ai,jalign

)
‚àën

j=1 exp
(
ai,jalign

) ÔøΩ cjrep (11)
cjai =

l‚àë
i=1

exp
(
ai,jalign

)
‚àël

i=1 exp
(
ai,jalign

) ÔøΩ sirep (12)
where Wa1,Wa2 ‚àà R2d√ó2d and ba ‚àà R2d are
parameters to be learned. To attenuate the effect
of incorrect attendance, input and output of this
attention mechanism are concatenated and fed to
the subsequent layer. Finally, we obtain the ques-
tion and answer representation Satt ‚àà R4d√ól =
[Srep;Sai], Catt ‚àà R4d√ón = [Crep;Cai].

3.5 Interaction Summarization
In this layer, the multi-dimensional self-attention
mechanism is employed to summarize two se-
quences of vectors (Satt and Catt) into two fixed-
length vectors ssum ‚àà R4d and csum ‚àà R4d.

As =Ws2tanh (Ws1Satt + bs1) + bs2 (13)

ssum =

n‚àë
i=1

exp
(
ais
)‚àën

i=1 exp (ais)
ÔøΩ siatt (14)

where Ws1,Ws2 ‚àà R4d√ó4d and bs1, bs2 ‚àà R4d are
parameters to be learned. The same process can
be applied to Catt and obtain csum.

3.6 Prediction
In this component, ssum and csum are concate-
nated and fed into a two-layer feed-forward neural
network. At the end of the last layer, the softmax
function is applied to obtain the conditional prob-
ability distribution Pr(y|S,B,C).

4 Experimental Setup

4.1 Datasets
We use two community question answering
datasets from SemEval (Nakov et al., 2015, 2017)
to evaluate our model. The statistics of these
datasets are listed in Table 2. The corpora contain
data from the QatarLiving forum 5, and are pub-
licly available on the task website. Each dataset

5http://www.qatarliving.com/forum

http://www.qatarliving.com/forum


1750

Statistics SemEval 2015 SemEval 2017
Train Dev Test Train Dev Test

Number of questions 2376 266 300 5124 327 293
Number of answers 15013 1447 1793 38638 3270 2930
Average length of subject 6.36 6.08 6.24 6.38 6.16 5.76
Average length of body 39.26 39.47 39.53 43.01 47.98 54.06
Average length of answer 35.82 33.90 37.33 37.67 37.30 39.50

Table 2: Statistics of two CQA datasets. We can see from the statistics that the question body is much
lengthier than the question subject. Thus, it is necessary to condense the question representation.

consists of questions and a list of answers for each
question, and each question consists of a short ti-
tle and a more detailed description. There are also
some metadata associated with them, e.g., user ID,
date of posting, and the question category. We do
not use the metadata because they failed to boost
performance in our model. Since the SemEval
2017 dataset is an updated version of SemEval
2016 6, and shares the same evaluation metrics
with SemEval 2016, we choose to use the SemEval
2017 dataset for evaluation.

4.2 Evaluation Metrics
In order to facilitate comparison, we adopt the
evaluation metrics used in the official task or prior
work. For the SemEval 2015 dataset, the offi-
cial scores are macro-averaged F1 and accuracy
over three categories. However, many recent re-
searches (Barr√≥n-Cede√±o et al., 2015; Joty et al.,
2015, 2016) switched to a binary classification set-
ting, i.e., identifying Good vs. Bad answers. Be-
cause binary classification is much closer to a real-
world CQA application. Besides, the Potential-
lyUseful class is both the smallest and the noisiest
class, making it the hardest to predict. To make
it worse, its impact is magnified by the macro-
averaged F1. Therefore, we adopt the F1 score
and accuracy on two categories for evaluation.

SemEval 2017 regards answer selection as a
ranking task, which is closer to the application sce-
nario. As a result, mean average precision (MAP)
is used as an evaluation measure. For a perfect
ranking, a system has to place all Good answers
above the PotentiallyUseful and Bad answers. The
latter two are not actually distinguished and are
considered Bad in terms of evaluation. Addition-

6The SemEval 2017 dataset provides all the data from
2016 for training , and fresh data for testing, but it does not
include a development set. Following previous work (Filice
et al., 2017), we use the 2016 official test set as the develop-
ment set.

ally, standard classification measures like accuracy
and F1 score are also reported.

4.3 Implementation Details

We use the tokenizer from NLTK (Bird, 2006)
to preprocess each sentence. All word embed-
dings in the sentence encoder layer are initial-
ized with the 300-dimensional GloVe (Pennington
et al., 2014) word vectors trained on the domain-
specific unannotated corpus, and embeddings for
out-of-vocabulary words are set to zero. We use
the Adam Optimizer (Kingma and Ba, 2014) for
optimization with a first momentum coefficient of
0.9 and a second momentum coefficient of 0.999.
We perform a small grid search over combina-
tions of initial learning rate [1 √ó 10‚àí6, 3 √ó 10‚àí6,
1√ó 10‚àí5], L2 regularization parameter [1√ó 10‚àí7,
3 √ó 10‚àí7, 1 √ó 10‚àí6], and batch size [8, 16, 32].
We take the best configuration based on perfor-
mance on the development set, and only evalu-
ate that configuration on the test set. In order to
mitigate the class imbalance problem, median fre-
quency balancing Eigen and Fergus (2015) is used
to reweight each class in the cross-entropy loss.
Therefore, the rarer a class is in the training set, the
larger weight it will get in the cross entropy loss.
Early stopping is applied to mitigate the problem
of overfitting. For the SemEval 2017 dataset, the
conditional probability over the Good class is used
to rank all the candidate answers.

5 Experimental Results

In this section, we evaluate our QCN model on two
community question answering datasets from Se-
mEval shared tasks.

5.1 SemEval 2015 Results

Table 3 compares our model with the following
baselines:



1751

Methods F1 Acc
(1) JAIST 78.96 79.10
(2) HITSZ-ICRC 76.52 76.11
(3) Graph-cut 80.55 79.80
(4) FCCRF 81.50 80.50
(5) BGMN 77.23 78.40
(6) CNN-LSTM-CRF 82.22 82.24
(7) QCN 83.91 85.65

Table 3: Comparisons on the SemEval 2015
dataset.

‚Ä¢ JAIST (Tran et al., 2015): It used an SVM
classifier to incorporate various kinds of fea-
tures , including topic model based features
and word vector representations.

‚Ä¢ HITSZ-ICRC (Hou et al., 2015): It pro-
posed ensemble learning and hierarchical
classification method to classify answers.

‚Ä¢ Graph-cut (Joty et al., 2015): It modeled the
relationship between pairs of answers at any
distance in the same question thread, based
on the idea that similar answers should have
similar labels.

‚Ä¢ FCCRF (Joty et al., 2016): It used locally
learned classifiers to predict the label for each
individual node, and applied fully connected
CRF to make global inference.

‚Ä¢ CNN-LSTM-CRF (Xiang et al., 2016): The
question and its answers are linearly con-
nected in a sequence and encoded by CNN.
An attention-based LSTM with a CRF layer
is then applied on the encoded sequence.

‚Ä¢ BGMN (Wu et al., 2017b): It used the mem-
ory mechanism to iteratively aggregate more
relevant information which is useful to iden-
tify the relationship between questions and
answers.

Baselines include top systems from SemEval
2015 (1, 2), systems relying on thread level infor-
mation to make global inference (3, 4), and neu-
ral network based systems (5, 6). We observe that
our proposed QCN can achieve the state-of-the-art
performance on this dataset, outperforming previ-
ous best model (6) by 1.7% in terms of F1 and
3.4% in terms of accuracy.

Methods MAP F1 Acc
(1) KeLP 88.43 69.87 73.89
(2) Beihang-MSRA 88.24 68.40 51.98
(3) ECNU 86.72 77.67 78.43
(4) LSTM 86.32 74.41 75.69
(5) LSTM-subject-body 87.11 74.50 77.28
(6) QCN 88.51 78.11 80.71

Table 4: Comparisons on the SemEval 2017
dataset.

Notably, Systems (1, 2, 3, 4) have heavy feature
engineering, while QCN only uses automatically-
learned feature vectors, demonstrating that our
QCN model is concise as well as effective. Fur-
thermore, our model can outperform systems rely-
ing on thread level information to make global in-
ference (3, 4), showing that modeling interaction
between the question-answer pair is useful enough
for answer selection task. Finally, neural network
based systems (5, 6) used attention mechanism in
sentence representation but ignored the subject-
body relationship in community questions. QCN
can outperform them by a large margin, showing
that condensing question representation helps in
the answer selection task.

5.2 SemEval 2017 Results

Table 4 compares our model with the following
baselines:

‚Ä¢ KeLP (Filice et al., 2017): It used syn-
tactic tree kernels with relational links be-
tween questions and answers, together with
some standard text similarity measures lin-
early combined with the tree kernel.

‚Ä¢ Beihang-MSRA (Feng et al., 2017): It used
gradient boosted regression trees to combine
traditional NLP features and neural network-
based matching features.

‚Ä¢ ECNU (Wu et al., 2017a): It combined a su-
pervised model using traditional features and
a convolutional neural network to represent
the question-answer pair.

‚Ä¢ LSTM: It is a simple neural network based
baseline that we implemented. In this model,
the question subject and the question body
are concatenated, and an LSTM is used to ob-
tain the question and answer representation.



1752

‚Ä¢ LSTM-subject-body: It is another neural
network based baseline that we implemented.
LSTM is applied on the question subject and
body respectively, and the results are concate-
nated to form question representation.

Baselines include top systems from the Se-
mEval 2017 CQA task (1, 2, 3) and two neural net-
work based baselines (4, 5) that we implemented.
(5) can outperform (4), showing that treating ques-
tion subject and body differently can indeed boot
model performance. Comparing (6) with (5), we
can draw the conclusion that orthogonal decom-
position is more effective than simple concatena-
tion, because it can flexibly aggregate related in-
formation from the question body with respect to
the main subject. In the example listed in Table 1,
attention heatmap ofAorth indicates that QCN can
effectively find additional information like ‚Äúmain-
tenance, accident or service history‚Äù, while (5)
fails to do so.

QCN has a great advantage in terms of accu-
racy. We hypothesize that QCN focuses on mod-
eling interaction between questions and answers,
i.e., whether an answer can match the correspond-
ing question. Many pieces of previous work fo-
cus on modeling relationship between answers in
a question thread, i.e., which answer is more suit-
able in consideration of all other answers. As a
consequence, their models have a greater advan-
tage in ranking while QCN has a greater advan-
tage in classification. Despite all this, QCN can
still obtain better ranking performance.

5.3 Ablation Study

For thorough comparison, besides the preceding
models, we implement nine extra baselines on the
SemEval 2017 dataset to analyze the improve-
ments contributed by each part of our QCN model:

‚Ä¢ w/o task-specific word embeddings where
word embeddings are initialized with the
300-dimensional GloVe word vectors trained
on Wikipedia 2014 and Gigaword 5.

‚Ä¢ w/o character embeddings where word-
level embeddings are only composed of 600-
dimensional GloVe word vectors trained on
the domain-specific unannotated corpus.

‚Ä¢ subject-body alignment where we use the
same attention mechanism as Question An-
swer Alignment to obtain weighted sum of

Model Acc
(1) w/o task-specific word embeddings 78.81
(2) w/o character embeddings 78.05
(3) subject-body alignment 77.38
(4) subject-body concatenation 76.06
(5) w/o multi-dimensional attention 78.33
(6) subject only 74.02
(7) body only 75.57
(8) similarity only 79.11
(9) disparity only 78.24
(10) QCN 80.71

Table 5: Ablation studies on the SemEval 2017
dataset.

the question body for each question subject
word, and then the result is concatenated with
Semb to obtain question representation Srep.

‚Ä¢ subject-body concatenation where we con-
catenate question subject and body text, and
use the preprocessing step described in sec-
tion 3.3 to obtain Srep.

‚Ä¢ w/o multi-dimensional attention where the
multi-dimensional attention mechanism is re-
placed by vanilla attention in all modules,
i.e., attention score for each token pair is a
scalar instead of a vector.

‚Ä¢ subject only where only question subject is
used as question representation.

‚Ä¢ body only where only question body is used
as question representation.

‚Ä¢ similarity only where the parallel component
alone is used in subject-body interaction.

‚Ä¢ disparity only where the orthogonal compo-
nent alone is used in subject-body interaction.

The results are listed in Table 5. We can see that
using task-specific embeddings and character em-
beddings both contribute to model performance.
This is because CQA text is non-standard. There
are quantities of informal language usage, such
as abbreviations, typos, emoticons, and grammati-
cal mistakes. Using task-specific embeddings and
character embeddings can help to attenuate the
OOV problem.

Using orthogonal decomposition (10) instead of
subject-body alignment (3) can bring about signif-
icant performance gain. This is because not only



1753

An
y

re
co

m
m

en
da

tio
n fo
r

Tr
av

el
ag

en
cie

s in
Do

ha to
ar

ra
ng

e
pa

ck
ag

es

Which
airline
offers

the
best

fares
or

low
cost

to
Kuala

Lumpur

(a) Apara

An
y

re
co

m
m

en
da

tio
n fo
r

Tr
av

el
ag

en
cie

s in
Do

ha to
ar

ra
ng

e
pa

ck
ag

es

Which
airline
offers

the
best

fares
or

low
cost

to
Kuala

Lumpur

(b) Aorth

Gu
lf ai
r

wa
s

th
e

ch
ea

pe
st It

tra
ns

its vi
a

Ba
hr

ai
n

Which
airline
offers

the
best

fares
or

low
cost

to
Kuala

Lumpur

(c) Aalign

Figure 2: Attention probabilities in Apara, Aorth and Aalign. In order to visualize the multi-dimensional
attention vector, we use the L2 norm of the attenion vector for representation.

the similar part of the question body to the ques-
tion subject is useful for the question representa-
tion, the disparity part can also provide additional
information. In the example listed in Table 1, ad-
ditional information like ‚Äúmaintenance, accident
or service history‚Äù is also important to determine
answer quality.

QCN outperforms (4) by a great margin,
demonstrating that subject-body relationship in
community questions helps to condense question
representation. Therefore, QCN can identify the
meaningful part of the question representation that
helps to determine answer quality.

Using the multi-dimensional attention can fur-
ther boost model performance, showing that the
multi-dimensional attention can model the inter-
action between questions and answers more pre-
cisely.

Comparing QCN with (6) and (7), we can con-
clude that both the subject and the body are indis-
pensable for question representation. (8) outper-
forms (9), demonstrating the parallel component
is more useful in subject-body interaction.

6 Qualitative Study

To gain a closer view of what dependencies are
captured in the subject-body pair and the question-
answer pair, we visualize the attention probabili-
ties Apara, Aorth and Aalign by heatmap. A train-
ing example from SemEval 2015 is selected for
illustration.

In Figure 2, we can draw the following con-
clusions. First, orthogonal decomposition helps
to divide the labor of identifying similar parts in
the parallel component and collecting related in-
formation in the question body in the orthogonal
component. For instance, for the word ‚ÄúKuala‚Äù in

the question subject, its parallel alignment score
focuses more on ‚ÄúDoha‚Äù and ‚ÄúTravel‚Äù, while its
orthogonal alignment score focuses on ‚Äúarrange‚Äù
and ‚Äúpackage‚Äù, which is the purpose of the travel
and therefore is also indispensable for sentence
representation. Second, semantically important
words such as ‚Äúairline‚Äù and ‚Äúfares‚Äù dominate the
attention weights, showing that our QCN model
can effectively select words that are most repre-
sentative for the meaning of the whole sentence.
Lastly, words that are useful to determine answer
quality stand out in the question-answer interac-
tion matrix, demonstrating that question-answer
relationship can be well modeled. For example,
‚Äúbest‚Äù and ‚Äúlow‚Äù are the words that are more im-
portant in the question-answer relationship, they
are emphasized in the question-answer alignment
matrix.

7 Related Work

One main task in community question answering
is answer selection, i.e., to rate the answers ac-
cording to their quality. The SemEval CQA tasks
(Nakov et al., 2015, 2016, 2017) provide univer-
sal benchmark datasets for evaluating researches
on this problem.

Earlier work of answer selection in CQA relied
heavily on feature engineering, linguistic tools,
and external resource. Nakov et al. (2016) investi-
gated a wide range of feature types including sim-
ilarity features, content features, thread level/meta
features, and automatically generated features for
SemEval CQA models. Tran et al. (2015) studied
the use of topic model based features and word
vector representation based features in the answer
re-ranking task. Filice et al. (2016) designed var-
ious heuristic features and thread-based features



1754

that can signal a good answer. Although achiev-
ing good performance, these methods rely heav-
ily on feature engineering, which requires a large
amount of manual work and domain expertise.

Since answer selection is inherently a ranking
task, a few recent researches proposed to use local
features to make global ranking decision. Barr√≥n-
Cede√±o et al. (2015) was the first work that ap-
plies structured prediction model on CQA answer
selection task. Joty et al. (2016) approached the
task with a global inference process to exploit the
information of all answers in the question-thread
in the form of a fully connected graph.

To avoid feature engineering, many deep learn-
ing models have been proposed for answer selec-
tion. Among them, Zhang et al. (2017) proposed
a novel interactive attention mechanism to address
the problem of noise and redundancy prevalent in
CQA. Tay et al. (2017) introduced temporal gates
for sequence pairs so that questions and answers
are aware of what each other is remembering or
forgetting. Simple as their model are, they did not
consider the relationship between question subject
and body, which is useful for question condensing.

8 Conclusion and Future Work

We propose Question Condensing Networks
(QCN), an attention-based model that can utilize
the subject-body relationship in community ques-
tions to condense question representation. By or-
thogonal decomposition, the labor of identifying
similar parts and collecting related information in
the question body can be well divided in two dif-
ferent alignment matrices. To better capture the
interaction between the subject-body pair and the
question-answer pair, the multi-dimensional atten-
tion mechanism is adopted. Empirical results on
two community question answering datasets in Se-
mEval demonstrate the effectiveness of our model.
In future work, we will try to incorporate more
hand-crafted features in our model. Furthermore,
since thread-level features have been explored in
previous work (Barr√≥n-Cede√±o et al., 2015; Joty
et al., 2015, 2016), we will verify their effective-
ness in our architecture.

9 Acknowledgments

We would like to thank anonymous reviewers for
their insightful comments. Our work is supported
by National Natural Science Foundation of China
under Grant No.61433015 and the National Key

Research and Development Program of China un-
der Grant No.2017YFB1002101. The correspond-
ing author of this paper is Houfeng Wang.

References
Alberto Barr√≥n-Cede√±o, Simone Filice, Giovanni

Da San Martino, Shafiq R. Joty, Llu√≠s M√†rquez,
Preslav Nakov, and Alessandro Moschitti. 2015.
Thread-level information for comment classification
in community question answering. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers. pages 687‚Äì693.

Steven Bird. 2006. NLTK: the natural language toolkit.
In ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference, Sydney, Aus-
tralia, 17-21 July 2006.

David Eigen and Rob Fergus. 2015. Predicting depth,
surface normals and semantic labels with a com-
mon multi-scale convolutional architecture. In 2015
IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015.
pages 2650‚Äì2658.

Wenzheng Feng, Yu Wu, Wei Wu, Zhoujun Li, and
Ming Zhou. 2017. Beihang-msra at semeval-2017
task 3: A ranking system with neural matching fea-
tures for community question answering. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 280‚Äì286.

Simone Filice, Danilo Croce, Alessandro Moschitti,
and Roberto Basili. 2016. Kelp at semeval-2016
task 3: Learning semantic relations between ques-
tions and answers. In Proceedings of the 10th
International Workshop on Semantic Evaluation,
SemEval@NAACL-HLT 2016, San Diego, CA, USA,
June 16-17, 2016. pages 1116‚Äì1123.

Simone Filice, Giovanni Da San Martino, and Alessan-
dro Moschitti. 2017. Kelp at semeval-2017 task 3:
Learning pairwise patterns in community question
answering. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval@ACL
2017, Vancouver, Canada, August 3-4, 2017. pages
326‚Äì333.

Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: exploiting classification approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval@NAACL-HLT 2015,
Denver, Colorado, USA, June 4-5, 2015. pages 196‚Äì
202.



1755

Shafiq R. Joty, Alberto Barr√≥n-Cede√±o, Giovanni
Da San Martino, Simone Filice, Llu√≠s M√†rquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment classifi-
cation in community question answering. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2015,
Lisbon, Portugal, September 17-21, 2015. pages
573‚Äì578.

Shafiq R. Joty, Llu√≠s M√†rquez, and Preslav Nakov.
2016. Joint learning with global inference for com-
ment classification in community question answer-
ing. In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016. pages 703‚Äì713.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, February 12-
17, 2016, Phoenix, Arizona, USA.. pages 2741‚Äì
2749.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980.

Preslav Nakov, Doris Hoogeveen, Llu√≠s M√†rquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. Semeval-2017
task 3: Community question answering. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 27‚Äì48.

Preslav Nakov, Llu√≠s M√†rquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in com-
munity question answering. In Proceedings of the
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 269‚Äì281.

Preslav Nakov, Llu√≠s M√†rquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim
Freihat, Jim Glass, and Bilal Randeree. 2016.
Semeval-2016 task 3: Community question answer-
ing. In Proceedings of the 10th International Work-
shop on Semantic Evaluation, SemEval@NAACL-
HLT 2016, San Diego, CA, USA, June 16-17, 2016.
pages 525‚Äì545.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL. pages 1532‚Äì1543.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-

rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI Conference on Artifi-
cial Intelligence.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. CoRR abs/1711.07656.

Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen,
and Son Bao Pham. 2015. JAIST: combining
multiple features for answer selection in commu-
nity question answering. In Proceedings of the
9th International Workshop on Semantic Evalua-
tion, SemEval@NAACL-HLT 2015, Denver, Col-
orado, USA, June 4-5, 2015. pages 215‚Äì219.

Shuohang Wang and Jing Jiang. 2017. A compare-
aggregate model for matching text sequences. In
Proceedings of the International Conference on
Learning Representations (ICLR).

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016. Sentence similarity learning by lexical de-
composition and composition. In Proceedings of
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers.

GuoShun Wu, Yixuan Sheng, Man Lan, and Yuanbin
Wu. 2017a. ECNU at semeval-2017 task 3: Us-
ing traditional and deep learning methods to ad-
dress community question answering task. In Pro-
ceedings of the 11th International Workshop on Se-
mantic Evaluation, SemEval@ACL 2017, Vancou-
ver, Canada, August 3-4, 2017. pages 365‚Äì369.

Wei Wu, Houfeng Wang, and Sujian Li. 2017b. Bi-
directional gated memory networks for answer se-
lection. In Chinese Computational Linguistics and
Natural Language Processing Based on Naturally
Annotated Big Data. LNAI 10565, Springer. pages
251‚Äì262.

Yang Xiang, Xiaoqiang Zhou, Qingcai Chen, Zhihui
Zheng, Buzhou Tang, Xiaolong Wang, and Yang
Qin. 2016. Incorporating label dependency for an-
swer quality tagging in community question an-
swering via CNN-LSTM-CRF. In COLING 2016,
26th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, December 11-16, 2016, Osaka, Japan.
pages 1231‚Äì1241.

Xiaodong Zhang, Sujian Li, Lei Sha, and Houfeng
Wang. 2017. Attentive interactive neural networks
for answer selection in community question answer-
ing. In Proceedings of the Thirty-First AAAI Confer-
ence on Artificial Intelligence, February 4-9, 2017,
San Francisco, California, USA.. pages 3525‚Äì3531.


