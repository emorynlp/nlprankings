










































Measuring Ideological Proportions in Political Speeches


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 91–101,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Measuring Ideological Proportions in Political Speeches

Yanchuan Sim∗ Brice D. L. Acree†
∗Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

{ysim,nasmith}@cs.cmu.edu

Justin H. Gross† Noah A. Smith∗
†Department of Political Science

University of North Carolina at Chapel Hill
Chapel Hill, NC 27599, USA

{brice.acree,jhgross}@unc.edu

Abstract
We seek to measure political candidates’ ideo-
logical positioning from their speeches. To ac-
complish this, we infer ideological cues from
a corpus of political writings annotated with
known ideologies. We then represent the
speeches of U.S. Presidential candidates as se-
quences of cues and lags (filler distinguished
only by its length in words). We apply a
domain-informed Bayesian HMM to infer the
proportions of ideologies each candidate uses
in each campaign. The results are validated
against a set of preregistered, domain expert-
authored hypotheses.

1 Introduction

The artful use of language is central to politics, and
the language of politicians has attracted consider-
able interest among scholars of political commu-
nication and rhetoric (Charteris-Black, 2005; Hart,
2009; Deirmeier et al., 2012; Hart et al., 2013)
and computational linguistics (Thomas et al., 2006;
Fader et al., 2007; Gerrish and Blei, 2011, in-
ter alia). In American politics, candidates for of-
fice give speeches and write books and manifestos
expounding their ideas. Every political season,
however, there are accusations of candidates “flip-
flopping” on issues, with opinion shows, late-night
comedies, and talk radio hosts replaying clips of
candidates contradicting earlier statements. Pres-
idential candidate Mitt Romney’s own aide infa-
mously proclaimed in 2012: “I think you hit a reset
button for the fall campaign [i.e., the general elec-
tion]. Everything changes. It’s almost like an Etch-
a-Sketch. You can kind of shake it up and we start
all over again.”

A more general observation, often stated but not
yet, to our knowledge, tested empirically, is that

successful primary candidates “move to the cen-
ter” before a general election. The expectation fol-
lows directly from long-standing and widely influen-
tial theories of political competition that are collec-
tively referred to in their simplest form as the “me-
dian voter theorem” (Hotelling, 1929; Black, 1948;
Downs, 1957). Thus it is to be expected that when
a set of voters that are more ideologically concen-
trated are replaced by a set who are more widely
dispersed across the ideological spectrum, as occurs
in the transition between the United States primary
and general elections, that candidates will present
themselves as more moderate in an effort to capture
enough votes to win.

Do political candidates in fact stray ideologically
at opportune moments? More specifically, can we
measure candidates’ ideological positions from their
prose at different times? Following much work
on classifying the political ideology expressed by a
piece of text (Laver et al., 2003; Monroe and Maeda,
2004; Hillard et al., 2008), we start from the as-
sumption that a candidate’s choice of words and
phrases reflects a deliberate attempt to signal com-
mon cause with a target audience, and as a broader
strategy, to respond to political competitors. Our
central hypothesis is that, despite candidates’ in-
tentional vagueness, differences in position—among
candidates or over time—can be automatically de-
tected and described as proportions of ideologies ex-
pressed in a speech.

In this work, we operationalize ideologies in a
novel empirical way, exploiting political writings
published in explicitly ideological books and mag-
azines (§2).1 The corpus then serves as evidence for

1We consider general positions in terms of broad ideolog-
ical groups that are widely discussed in current political dis-
course (e.g., “Far Right,” “Religious Right,” “Libertarian,”’

91



BACKGROUND

LEFT RIGHT
CENTER

PROGRESSIVE

RELIGIOUS LEFT

FAR LEFT

RELIGIOUS RIGHT
CENTER-LEFT

FAR RIGHT

CENTER-RIGHT

LIBERTARIAN

POPULIST

Figure 1: Ideology tree showing the labels for the ide-
ological corpus in §2.1 (excluding BACKGROUND) and
corresponding to states in the HMM (§3.3).

a probabilistic model that allows us to automatically
infer compact, human-interpretable lexicons of cues
strongly associated with each ideology.

These lexicons are used, in turn, to create a low-
dimensional representation of political speeches: a
speech is a sequence of cues interspersed with lags.
Lags correspond to the lengths of sequences of non-
cue words, which are treated as irrelevant to the in-
ference problem at hand. In other words, a speech is
represented as a series alternating between cues sig-
naling ideological positions and uninteresting filler.

Our main contribution is a probabilistic technique
for inferring proportions of ideologies expressed by
a candidate (§3). The inputs to the model are the
cue-lag representation of a speech and a domain-
specific topology relating ideologies to each other.
The topology tree (shown in Figure 1) encoding
the closeness of different ideologies and, by exten-
sion, the odds of transitioning between them within a
speech. Bayesian inference is used to manage uncer-
tainty about the associations between cues and ide-
ologies, probabilities of traversing each of the tree’s
edges, and other parameters.

We demonstrate the usefulness of the measure-
ment model by showing that it accurately recov-
ers pre-registered beliefs regarding narratives widely
accepted—but not yet tested empirically—about the
2008 and 2012 U.S. Presidential elections (§4).

2 First Stage: Cue Extraction

We first present a data-driven technique for automat-
ically constructing “cue lexicons” from texts labeled
with ideologies by domain experts.

etc.). Analysis of positions on specific issues is left for future
work.

Total tokens 32,835,190
Total types 138,235
Avg. tokens per book 77,628
Avg. tokens per mag. issue 31,713
Breakdown by ideology: Documents Tokens
LEFT 0 0
FAR LEFT 112 3,334,601
CENTER-LEFT 196 7,396,264
PROGRESSIVE LEFT 138 7,257,723
RELIGIOUS LEFT 7 487,844
CENTER 5 429,480
RIGHT 97 3,282,744
FAR RIGHT 211 7,392,163
LIBERTARIAN RIGHT 88 1,703,343
CENTER-RIGHT 9 702,444
POPULIST RIGHT 5 407,054
RELIGIOUS RIGHT 6 441,530

Table 1: Ideology corpus statistics. Note that some docu-
ments are not labeled with finer-grained ideologies.

2.1 Ideological Corpus

We start with a collection of contemporary political
writings whose authors are perceived as represen-
tative of one particular ideology. Our corpus con-
sists of two types of documents: books and maga-
zines. Books are usually written by a single author,
while each magazine consists of regularly published
issues with collections of articles written by several
authors. A political science domain expert who is
a co-author of this work manually labeled each ele-
ment in a collection of 112 books and 10 magazine
titles2 with one of three coarse ideologies: LEFT,
RIGHT, or CENTER. Documents that were labeled
LEFT and RIGHT were further broken down into
more fine-grained ideologies, shown in Fig. 1.3 Ta-
ble 1 summarizes key details about the ideological
corpus.

In addition to ideology labels, individual chapters
within the books were manually tagged with topics
that the chapter was about. For instance, in Barack
Obama’s book The Audacity of Hope, his chapter

2There are 765 magazine issues, which are published bi-
weekly to quarterly, depending on the magazine. All of a mag-
azine’s issues are labeled with the same ideology.

3We cannot claim that these texts are “pure” examples of
the ideologies they are labeled with (i.e., they may contain parts
that do not match the label). By finding relatively few terms
strongly associated with texts sharing a label, our model should
be somewhat robust to impurities, focusing on those terms that
are indicative of whatever drew the expert to identify them as
(mostly) sharing an ideology.

92



titled “Faith” is labeled as RELIGIOUS. Not all
chapters have clearly defined topics, and as such,
these chapters are simply labeled MISC. Maga-
zines are not labeled with topics because each issue
of a magazine generally touches on multiple top-
ics. There are a total of 61 topics; the full list can
be found in the supplementary materials, along with
a table summarizing key details about the corpus,
which contains 32.8 million tokens.

2.2 Cue Discovery Model
We use the ideological corpus to infer ideological
cues: terms that are strongly associated with an ide-
ology. Because our ideologies are organized hierar-
chically, we required a technique that can account
for multiple effects within a single text. We further
require that the sets of cue terms be small, so that
they can be inspected by domain experts. We there-
fore turn to the sparse additive generative (SAGE)
models introduced by Eisenstein et al. (2011).

Like other probabilistic language models, SAGE
assigns probability to a text as if it were a bag of
terms. It differs from most language models in pa-
rameterizing the distribution using a generalized lin-
ear model, so that different effects on the log-odds
of terms are additive. In our case, we define the
probability of a term w conditioned on attributes of
the text in which it occurs. These attributes include
both the ideology and its coarsened version (e.g., a
FAR RIGHT book also has the attribute RIGHT).
For simplicity, let A(d) denote the set of attributes
of document d and A =

⋃
d A(d). The parametric

form of the distribution is given, for term w in doc-
ument d, by:

p(w | A(d);η) =
exp

(
η0w +

∑
a∈A(d) η

a
w

)
Z(A(d),η)

Each of the η weights can be a positive or negative
value influencing the probability of the word, condi-
tioned on various properties of the document. When
we stack an attribute a’s weights into a vector across
all words, we get an ηa vector, understood as an ef-
fect on the term distribution. (We use η to refer to
the collection of all of these vectors.) The effects in
our model, described in terms of attributes, are:
• η0, the background (log) frequencies of words,

fixed to the empirical frequencies in the corpus.

Hence the other effects can be understood as de-
viations from this background distribution.
• ηic , the coarse ideology effect, which takes differ-

ent values for LEFT, RIGHT, and CENTER.
• ηif , the fine ideology effect, which takes different

values for the fine-grained ideologies correspond-
ing to the leaves in Fig. 1.
• ηt, the topic effect, taking different values for

each of the 61 manually assigned topics. We fur-
ther include one effect for each magazine series
(of which there are 10) to account for each maga-
zine’s idiosyncrasies (topical or otherwise).
• ηd, a document-specific effect, which captures id-

iosyncratic usage within a single document.
Note that the effects above are not mutually exclu-
sive, although some effects never appear together
due to constraints imposed by their semantics (e.g.,
no book is labeled both LEFT and RIGHT).

When estimating the parameters of the model (the
η vectors), we impose a sparsity-inducing `1 prior
that forces many weights to zero. The objective is:

max
η

∑
d

∑
w∈d

log p(w | A(d);η)−
∑
a∈A

λa‖ηa‖1

This objective function is convex but requires spe-
cial treatment due to non-differentiability when any
elements are zero; we use the OWL-QN algorithm
to solve it (Andrew and Gao, 2007). To reduce the
complexity of the hyperparameter space (the possi-
ble values of all λa) and to encourage similar levels
of sparsity across the different effect vectors, we let,
for each ideology attribute a,

λa = λ · |V(a)| /maxa′∈A |V(a′)|

where V(a) is the set of term types appearing in
the data with attribute a (i.e., its vocabulary) , and
λ is a hyperparameter we can adjust to control the
amount of sparsity in the SAGE vectors. For the
non-ideology effects, we fix λa = 10 (not tuned).

2.3 Bigram and Trigram Lexicons
After estimating parameters, we are left with sparse
ηa for each attribute. We are only interested, how-
ever, in the ideological attributes I ⊂ A. For an
ideological attribute i ∈ I, we take the terms with
positive elements of this vector to be the cues for
ideology i; call this set L(i) and let L =

⋃
i∈I L(i).

93



Because political texts use a fair amount of multi-
word jargon, we initially represented each document
as a bag of unigrams, bigrams, and trigrams, ignor-
ing the fact that these “overlap” with each other.4

While this would be inappropriate in language mod-
eling and is inconsistent with our model’s indepen-
dence assumptions among words, it is sensible since
our goal is to identify cues that are statistically asso-
ciated with attributes like ideologies.

Preliminary trials revealed that unigrams tend to
dominate in such a model, since their frequency
counts are so much higher. Further, domain ex-
perts found them harder to interpret out of context
compared to bigrams and trigrams. We therefore in-
cluded only bigrams and trigrams as terms in our cue
discovery model.

2.4 Validation

The term selection method we have described can
be understood as a form of feature selection that
reasons globally about the data and tries to con-
trol for some effects that are not of interest (topic
or document idiosyncrasies). We compared the
approach to two classic, simple methods for fea-
ture selection: ranking based on pointwise mu-
tual information (PMI) and weighted average PMI
(WAPMI) (Schneider, 2005; Cover and Thomas,
2012). Selected features were used to classify the
ideologies of held-out documents from our cor-
pus.5 We evaluated these feature selection methods
within naı̈ve Bayes classification in a 5-fold cross-
validation setup. We vary λ for the SAGE model
and compare the results to equal-sized sets of terms
selected by PMI and WAPMI. We consider SAGE
with and without topic effects.

Figure 2 visualizes accuracy against the num-
ber of features for each method. Bigrams and
trigrams consistently outperform unigrams (McNe-
mar’s, p < 0.05). Otherwise, there are no sig-
nificant differences in performance except WAPMI

4Generative models that produce the same evidence more
than once are sometimes called “deficient,” but model defi-
ciency does not necessarily imply that the model is ineffective.
Some of the IBM models for statistical machine translation pro-
vide a classic example (Brown et al., 1993).

5The text was tokenized and stopwords removed. Punctu-
ation, numbers, and web addresses were normalized. Tokens
appearing less than 20 times in training data, or in fewer than 5
documents were removed.

26 27 28 29 210 211 212 213 214 ∞
0.5

0.55

0.6

0.65

0.7

PMI
WAPMI

SAGE
SAGE w/ topics

Figure 2: Plot of average classification accuracy for
5-fold cross validation against the number of features.
Dashed lines refer to using only unigram features, while
solid lines refer to using bigram and trigram features.

with bigrams/trigrams at its highest point. SAGE
with topics is slightly (but not significantly) bet-
ter than without. We conclude that SAGE is a
competitive choice for cue discovery, noting that a
principled way of controlling for topical and doc-
ument effects—offered by SAGE but not the other
methods—may be even more relevant to our task
than classification accuracy.

2.5 Cue Lexicon

We ran SAGE on the the full ideological book cor-
pus, including topic effects, and setting λ = 30, ob-
tained a set of |L| = 8, 483 cue terms. The supple-
mentary materials include top cue terms associated
with various ideologies and a heatmap of similarities
among SAGE vectors.

We conducted a small, relatively informal study
in which seven subjects (including four scholars of
American politics) were asked to match brief de-
scriptions of the classes, including prominent proto-
typical individuals exemplifying each, to cue terms.
About 70% of ideologies were correctly matched
by experts, with relatively few confusions between
LEFT and RIGHT. More details are given in sup-
plementary materials.

3 Second Stage: Cue-Lag Ideological
Proportions

The main contribution of this paper is a technique
for measuring ideology proportions in the prose of
political candidates. We adopt a Bayesian approach
that manages our uncertainty about the cue lexi-

94



con L, the tendencies of political speakers to “flip-
flop” among ideological types, and the relative “dis-
tances” among different ideologies. The representa-
tion of a candidate’s ideology as a mixture among
discrete, hierarchically related categories can be dis-
tinguished from continuous representations (“scal-
ing” or “spatial” models) often used in political sci-
ence, especially to infer positions from Congres-
sional roll-call voting patterns (Poole and Rosen-
thal, 1985; Poole and Rosenthal, 2000; Clinton
et al., 2004). Moreover, the ability to draw in-
ferences about individual policy-makers’ ideologies
from their votes on proposed legislation is severely
limited by institutional constraints on the types of
legislation that is actually subject to recorded votes.

3.1 Political Speeches Corpus

We gathered transcribed speeches given by candi-
dates of the two main parties (Democrats and Re-
publicans) during the 2008 and 2012 Presidential
election seasons. Each election season is comprised
of two stages: (i) the primary elections, where can-
didates seek the support of their respective parties to
be nominated as the party’s Presidential candidate,
and (ii) the general elections where the parties’ cho-
sen candidates travel across the states to garner sup-
port from all citizens. Each candidate’s speeches are
partitioned into epochs for each election; e.g., those
that occur before the candidate has secured enough
pledged delegates to win the party nomination are
“from the primary.” Table 2 presents a breakdown
of the candidates and speeches in our corpus.

3.2 Cue-Lag Representation

Our measurement model only considers ideological
cues; other terms are treated as filler. We therefore
transform each speech into a cue-lag representation.

The representation is a sequence of alternating
cues (elements from the ideological lexicon L) and
integer “lags” (counts of non-cue terms falling be-
tween two cues). This will allow us to capture the in-
tuition that a candidate may use longer lags between
evocations of different ideologies, while nearby cues
are likely to be from similar ideologies.

To map a speech into the cue-lag representation,
we simply match all elements of L in the speech and
replace sequences of other words by their lengths.
When a trigram cue strictly includes a bigram cue,

Party Pri’08 Gen’08 Pri’12 Gen’12
Democrats∗ 167 - - -
Republicans† 50 - 49 -
Obama (D) 78 81 - 99
McCain (R) 9 159 - -
Romney (R) 8 ‡(13) 19 19
∗Democrats in our corpus are: Joe Biden, Hillary Clinton, John
Edwards, and Bill Richardson in 2008 and Barack Obama in
both 2008 and 2012.
†Republicans in our corpus are: Rudy Giuliani, Mike Huck-
abee, John McCain, and Fred Thompson in 2008, Michelle
Bachmann, Herman Cain, Newt Gingrich, Jon Huntsman, Rick
Perry, and Rick Santorum in 2012, and Ron Paul and Mitt Rom-
ney in both 2008 and 2012.
‡For Romney, we have 13 speeches which he gave in the period
2008-2011 (between his withdrawal from the 2008 elections
and before the commencement of the 2012 elections). While
these speeches are not technically part of the regular Presiden-
tial election campaign, they can be seen as his preparation to-
wards the 2012 elections, which is particularly interesting as
Romney has been accused of having inconsistent viewpoints.

Table 2: Breakdown of number of speeches in our polit-
ical speech corpus by epoch. On average, 2,998 tokens,
and 95 cue terms are found in each speech document.

we take only the trigram. When two cues partially
overlap, we treat them as consecutive cue terms and
set the lag to 0. Figure 3 shows an example of our
cue-lag representation.

3.3 CLIP: An Ideology HMM

The model we use to infer ideologies, cue-lag ide-
ological proportions (CLIP), is a hidden Markov
model. Each state corresponds to an ideology
(Fig. 1) or BACKGROUND. The emission from a state
consists of (i) a cue from L and (ii) a lag value. The
high-level generative story for a single speech with
T cue-lag pairs is as follows:

1. Parameters are drawn from conjugate priors
(details in §3.3.3).
2. Let the initial state be the BACKGROUND
state.
3. For t ∈ {1, 2, . . . , T}:6

(a) Transition to state St based on the
transition distribution, discussed in §3.3.1.
This transition is conditioned on the previ-
ous state St−1 and the lag at timestep t−1,
denoted by Lt−1.

6The length of the sequence is assumed to be exogenous, so
that no stop state needs to be defined.

95



Original sentence Just compare this President’s record with Ronald Reagan’s first term. President Reagan also faced
an economic crisis. In fact, in 1982, the unemployment rate peaked at nearly 11 percent. But in the
two years that followed, he delivered a true recovery economic growth and job creation were three
times higher than in the Obama Economy.

Cue-lag representation . . . 6−→ ronald reagan 2−→ presid reagan 3−→ econom crisi 5−→ unemploy rate 17−→ econom growth 1−→
job creation 9−→ . . .

Figure 3: Example of the cue-lag representation.

(b) Emit cue term Wt from the lexicon L
and lag Lt based on the emission distribu-
tion, discussed in §3.3.2.

We turn next to the transitions and emissions.

3.3.1 Ideology Topology and Transition
Parameterization

CLIP assumes that each cue term uttered by a
politician is generated from a hidden state corre-
sponding to an ideology. The ideologies are orga-
nized into a tree based on their hierarchical relation-
ships; see Fig. 1. In this study, the tree is fixed ac-
cording to our domain knowledge of current Ameri-
can politics; in future work it might be enriched with
greater detail or its structure learned automatically.

The ideology tree is used in defining the transition
distribution in the HMM, but not to directly define
the topology of the HMM. Importantly, each state
may transition to any other state, but the transition
distribution is defined using the graph, so that ide-
ologies that are closer to each other will tend to be
more likely to transition to each other. To transition
between two states si and sj , a walk must be taken
in the tree from vertex si to vertex sj . We emphasize
that the walk corresponds to a single transition—
the speaker does not emit anything from the states
passed through along the path.

A simplified version of our transition distribution,
for exposition, is given as follows:

ptree(sj | si; ζ,θ)

=
(∏
〈u,v〉∈Path(si,sj)(1− ζu)θu,v

)
ζsj

Path(si, sj) refers to the sequence of edges in the
tree along the unique path from si to sj . Each of
these edges 〈u, v〉 must be traversed, and the prob-
ability of doing so, conditioned on having already
reached u, is (1−ζu)—i.e., not stopping in u—times
θu,v—i.e., selecting vertex v from among those that
share an edge with u. Eventually, sj is reached, and
the walk ends, incurring probability ζsj .

In order to capture the intuition that a longer lag
after a cue term should increase the entropy over the
next ideology state, we introduce a restart probabil-
ity, which is conditioned on the length of the most
recent lag, `. The probability of restarting the walk
from the BACKGROUND state is a noisy-OR model
with parameter ρ. This gives the transition distribu-
tion:

p(sj | si, `; ζ,θ, ρ) = (1− ρ)`+1ptree(sj | si; ζ,θ)
+ (1− (1− ρ)`+1)ptree(sj | sBACKGROUND ; ζ,θ)

Note that, if ρ = 1, there is no Markovian depen-
dency between states (i.e., there is always a restart),
so CLIP reverts to a mixture model.

This approach allows us to parameterize the full
set of |I|2 transitions with O(|I|) parameters.7 Since
the graph is a tree and the walks are not allowed
to backtrack, the only ambiguity in the transition
is due to the restart probability; this distinguishes
CLIP from other algorithms based on random walks
(Brin and Page, 1998; Mihalcea, 2005; Toutanova et
al., 2004; Collins-Thompson and Callan, 2005).

3.3.2 Emission Parameterization
Recall that, at time step t, CLIP emits a cue from

the lexicon L and an integer-valued lag. For each
state s, we let the probability of emitting cue w
be denoted by ψs,w; ψs is a multinomial distribu-
tion over the entire lexicon L. This allows our ap-
proach to handle ambiguous cues that can associate
with more than one ideology, and also to associate a
cue with a different ideology than our cue discovery
method proposed, if the signal from the data is suffi-
ciently strong. We assume each lag to be generated
by a Poisson distribution with global parameter ν.

7More precisely, there are |I| edges (since there are |I| + 1
vertices including BACKGROUND), each with a θ-parameter in
each direction. For a vertex with degree d, however, there are
only d−1 degrees of freedom, so that there are 2|I|−(|I|+1) =
|I|−1 degrees of freedom for θ. There are |I| ζ-parameters and
a single ρ, for a total of 2|I| degrees of freedom.

96



3.3.3 Inference and Learning
Above we described CLIP’s transitions and emis-

sions. Because our interest is in measuring
proportions—and, as we will see, in comparing
those proportions across speakers and campaign
periods—we require a way to allow variation in pa-
rameters across different conditions. Specifically,
we seek to measure differences in time spent in each
ideology state. This can be captured by allowing
each speaker to have a different θ and ζ in each stage
of the campaign. On the other hand, we expect that a
speaker draws from his ideological lexicon similarly
across different epochs—there is a single ψ shared
between different epochs.

In order to manage uncertainty about the param-
eters of CLIP, to incorporate prior beliefs based on
our ideology-specific cue lexicons {L(i)}i, and to
allow sharing of statistical strength across condi-
tions, we adopt a Bayesian approach to inference.
This will allow principled exploration of the poste-
rior distribution over the proportions of interest.

We place a symmetric Dirichlet prior on the tree
walk probabilities θ; its parameter is α. For the
cue emission distribution associated with ideology
i, ψsi , we use an informed Dirichlet prior with two
different values, βcue for cues in L(i), and a smaller
βdef for those in L \ L(i).8

Learning proceeds by collapsed Gibbs sampling
for the hidden states and slice sampling (with vague
priors) for the hyperparameters (α, β, ρ, and ζ). De-
tails of the sampler are given in the supplementary
materials. At each Gibbs step, we resample the ide-
ology state and restart indicator variable for every
cue term in every speech.

We ran our Gibbs sampler for 75,000 iterations,
discarding the first 25,000 iterations for burn-in, and
collected samples at every 10 iterations. Further, we
perform the slice sampling step at every 5,000 itera-
tions. For each candidate, we collected 5,000 poste-
rior samples which we use to infer his/her ideologi-
cal proportions.

In order to determine the amount of time a candi-
date spends in each ideology, we denote the unit of
time in terms of half the lag before and after each cue

8This implies that a term can, in the posterior distribution,
be associated with an ideology i of whose L(i) it was not a
member. In fact, this occurred frequently in our runs of the
model.

term, i.e., when a candidate draws a cue term from
ideology i during timestep t, we say that he spends
1
2(Lt−1 + Lt) amount of time in ideology i. Aver-
aging over all the samples returned by our sampler
and normalizing it by the length of the documents in
each epoch, we obtain a candidate’s expected ideo-
logical proportions within the epoch.

4 Pre-registered Hypotheses

The traditional way to evaluate a text analysis model
in NLP is, of course, to evaluate its output against
gold-standard judgements by humans. In the case
of recent political speeches, however, we are doubt-
ful that such judgments can be made objectively at
a fine-grained level. While we are confident about
gross categorization of books and magazines in our
ideological corpus (§2.1), many of which are overtly
marked by their ideological assocations, we believe
that human estimates of ideological proportions, or
even association of particular tokens with ideologies
they may evoke, may be overly clouded by the vari-
ation in annotator ideology and domain expertise.

We therefore adopt a different method for evalua-
tion. Before running our model, we identified a set
of hypotheses, which we pre-registered as expec-
tations. These are categorized into groups based on
their strength and relevance to judging the validity of
the model. Strong hypotheses are those that consti-
tute the lowest bar for face validity; if violated, they
suggest a flaw in the model. Moderate hypotheses
are those that match the intuition of domain experts
conducting the research, or extant theory. Violations
suggest more examination is required, and may raise
the possibility that further testing might be pursued
to demonstrate the hypothesis is false. Our 13 prin-
cipal hypotheses are enumerated in Table 3.

5 Evaluation

We compare the posterior proportions inferred by
CLIP with several baselines:
• HMM: rather than §3.3.1, a fully connected, tra-

ditional transition matrix is used.
• MIX: a mixture model; at each timestep, we al-

ways restart (ρ = 1). This eliminates Marko-
vian dependencies between ideologies at nearby
timesteps, but still uses the ideology tree in defin-
ing the probabilities of each state through θ.

97



Hypotheses CLIP HMM MIX NORES
Sanity checks (strong):
S1. Republican primary candidates should tend to draw more from RIGHT than

from LEFT.
*12/12 10/13 13/13 12/13

S2. Democratic primary candidates should tend to draw more from LEFT than
from RIGHT.

4/5 5/5 5/5 5/5

S3. In general elections, Democrats should draw more from the LEFT than the
Republicans and vice versa for the RIGHT.

4/4 4/4 3/4 0/4

S total 20/21 19/22 21/22 17/22
Primary hypotheses (strong):
P1. Romney, McCain and other Republicans should almost never draw from FAR

LEFT, and extremely rarely from PROGRESSIVE.
29/32 *21/31 27/32 29/32

P2. Romney should draw more heavily from the RIGHT than Obama in both stages
of the 2012 campaign.

2/2 2/2 1/2 1/2

Primary hypotheses (moderate):
P3. Romney should draw more heavily on words from the LIBERTARIAN,

POPULIST, RELIGIOUS RIGHT, and FAR RIGHT in the primary com-
pared to the general election. In the general election, Romney should draw
more heavily on CENTER, CENTER-RIGHT and LEFT vocabularies.

2/2 2/2 0/2 2/2

P4. Obama should draw more heavily on words from the PROGRESSIVE in the
2008 primary than in the 2008 general election.

0/1 0/1 0/1 1/1

P5. In the 2008 general election, Obama should draw more heavily on the
CENTER, CENTER-LEFT, and RIGHT vocabularies than in the 2008 primary.

1/1 1/1 1/1 1/1

P6. In the 2012 general election, Obama should sample more from the LEFT than
from the RIGHT, and should sample more from the LEFT vocabularies than
Romney.

2/2 2/2 0/2 0/2

P7. McCain should draw more heavily from the FAR RIGHT, POPULIST, and
LIBERTARIAN in the 2008 primary than in the 2008 general election.

0/1 1/1 1/1 1/1

P8. In the general 2008, McCain should draw more heavily from the CENTER,
CENTER-RIGHT, and LEFT vocabularies than in the 2008 primary.

1/1 1/1 1/1 1/1

P9. McCain should draw more heavily from the RIGHT than Obama in both stages
of the campaign.

2/2 2/2 2/2 1/2

P10.Obama and other Democrats should very rarely draw from FAR RIGHT. 6/7 5/7 7/7 4/7
P total 45/51 37/50 40/51 41/51

Table 3: Pre-registered hypotheses used to validate the measurement model; number of statements evaluated correctly
by different models. *Some differences were not significant at p = 0.05 and are not included in the results.

• NORES, where we never restart (ρ = 0). This
strengthens the Markovian dependencies.

In MIX, there are no temporal effects between cue
terms, although the structure of our ideology tree
encourages the speaker to draw from coarse-grained
ideologies over fine-grained ideologies. On the other
hand, the strong Markovian dependency between
states in NORES would encourage the model to stay
local within the ideology tree. In our experiments,
we will see how that the ideology tree and the ran-
dom treatment of restarting both contribute to our
model’s inferences.

Table 3 presents a summary of which hypothe-
ses the models’ inferences are in accordance with.
CLIP is not consistently outperformed by any of the

competing baselines.

Sanity checks (S1–3) CLIP correctly identifies
sixteen LEFT/RIGHT alignments of primary candi-
dates (S1, S2), but is unable to determine one can-
didate’s orientation; it finds Jon Huntsman to spend
roughly equal proportions of speech-time drawing
on LEFT and RIGHT cue terms. Interestingly,
Huntsman, who had served as U.S. Ambassador to
China under Obama, was considered the one mod-
erate in the 2012 Republican field. MIX correctly
identifies all thirteen Republicans, while NORES
places McCain from the 2008 primaries as mostly
LEFT-leaning and HMM misses three of thirteen,
including Perry and Gingrich, who might be deeply

98



disturbed to find that they are misclassified as LEFT-
leaning. As for the Democratic primary candidates
(S2), CLIP’s one questionable finding is that John
Edwards spoke slightly more from the RIGHT than
the LEFT. For the general elections (S3), CLIP and
HMM correctly identify the relative amount of time
spent in LEFT/RIGHT between Obama and his Re-
publican competitors. NORES had the most trou-
ble, missing all four. CLIP finds Obama spend-
ing slightly more time on the RIGHT than on the
LEFT in the 2008 general elections but nevertheless,
Obama is still found to spend more time engaging in
LEFT-speak than McCain.

Name interference When we looked at the cue
terms actually used in the speeches, we found one
systematic issue: the inclusion of candidates’ names
as cue terms. Terms mentioning John McCain are
associated with the RIGHT, so that Obama’s men-
tions of his opponent are taken as evidence for
rightward positioning; in total, mentions of McCain
contributed 4% absolute to Obama’s RIGHT ide-
ological proportion. Similarly, barack obama and
presid obama are LEFT cues (though senat obama
is a RIGHT cue). In future work, we believe filtering
candidate names in the first stage will be beneficial.

Strong hypotheses P1 and P2 CLIP and the vari-
ants making use of the ideology tree were in agree-
ment on most of the strong primary hypotheses.
Most of these involved our expectation that the
Republican candidates would rarely draw on FAR
LEFT and PROGRESSIVE LEFT. Our qualitative
hypotheses were not specific about how to quantify
“rare” or “almost never.” We chose to find a result
inconsistent with a P1 hypothesis any time a Repub-
lican had proportions greater than 5% for either ide-
ology. The notable deviations for CLIP were Fred
Thompson (13% from the PROGRESSIVE LEFT
during the 2008 primary) and Mitt Romney (12%
from the PROGRESSIVE LEFT between the 2008
and 2012 elections, 13% from the FAR LEFT dur-
ing the 2012 general election). This model did no
worse than other variants here and much better than
one: HMM had 10 inconsistencies out of 32 oppor-
tunities, suggesting the importance of the ideology
tree.

M
cC

ai
n

Primaries 2008 General 2008
Far Left

Religious (L)
Center-Left

Center-Right

Libertarian (R)

Religious (R)

Progressive (L)
Left

Center

Right

Populist (R)

Far Right

R
om

ne
y

Primaries 2008 2008-2011 Primaries 2012 General 2012
Far Left

Religious (L)

Center
Center-Right

Libertarian (R)

Religious (R)

Progressive (L)

Left

Center-Left

Right

Populist (R)
Far Right

O
ba

m
a

Primaries 2008 General 2008 General 2012
Far Left

Religious (L)

Left

Center-Left

Center-Right

Libertarian (R)
Populist (R)

Religious (R)

Progressive (L)

Center

Right

Far Right

Figure 4: Proportion of time spent in each ideology by
McCain, Romney, and Obama during the 2008 and 2012
Presidential election seasons.

“Etch-a-Sketch” hypotheses Hypotheses P3, P4,
P5, P7, and P8 are all concerned with differences
between the primary and general elections: success-
ful primary candidates are expected to “move to the
center.” A visualization of CLIP’s proportions for
McCain, Romney, and Obama is shown in Figure 4,
with their speeches grouped together by different
epochs. The model is in agreement with most of
these hypotheses. It did not confirm P4—Obama
appears to CLIP to be more PROGRESSIVE in the
2008 general election than in the primary, though the
difference is small (3%) and may be within the mar-
gin of error. Likewise, in P7, the difference between
McCain drawing from FAR RIGHT, POPULIST
and LIBERTARIAN between the 2008 primary and
general elections is only 2% and highly uncertain,
with a 95% credible interval of 44–50% during the
primary (vs. 47–50% in the general election).

Fine-grained ideologies Fine-grained ideologies
are expected to account for smaller proportions, so
that making predictions about them is quite difficult.
This is especially true for primary elections, where a
broader palette of ideologies is expected to be drawn
from, but we have fewer speeches from each candi-

99



date. CLIP’s inconsistency with P10, for example,
comes from assigning 5.4% of Obama’s 2008 pri-
mary cues to FAR RIGHT.

CLIP’s inferences on the corpus of political
speeches can be browsed at http://www.ark.
cs.cmu.edu/CLIP. We emphasize that CLIP
and its variants are intended to quantify the ideo-
logical content candidates express in speeches, not
necessarily their beliefs (which may not be perfectly
reflected in their words), or even how they are de-
scribed by pundits and analysts (who draw on far
more information than is expressed in speeches).
CLIP’s deviations from the hypotheses are sug-
gestive of potential improvements to cue extraction
(§2), but also of incorrect hypotheses. We expect
future research to explore a richer set of linguistic
cues and attributes beyond ideology (e.g., topics and
framing on various issues). We plan to use CLIP
as a text analysis method to support substantive in-
quiry in political science, such as following trends
in expressed ideology over time.

6 Related Work

As early as the 1960s, there has been research on
modeling ideological beliefs using automated sys-
tems (Abelson and Carroll, 1965; Carbonell, 1978;
Sack, 1994). These early works model ideology at a
sophisticated level, involving the actors, actions and
goals; they require manually constructed knowledge
bases. Poole and Rosenthal (1985) used congres-
sional roll call data to demonstrate the ideological
divide in Congress, and provided a methodology for
measuring ideological positions. Gerrish and Blei
(2011; 2012) augmented the methodology with text
from congressional bills using probabilistic models
to uncover lawmakers’ positions on specific polit-
ical issues, putting them on a left-right spectrum,
while Thomas et al. (2006) made use of floor de-
bate speeches to predict votes. Likewise, taking ad-
vantage of the proliferation of text today, numer-
ous techniques have been developed to identify top-
ics and perspectives in the media (Gentzkow and
Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009;
Gentzkow and Shapiro, 2010); determine the polit-
ical leanings of a document or author (Laver et al.,
2003; Efron, 2004; Mullen and Malouf, 2006; Fader
et al., 2007); or recognize stances in debates (So-

masundaran and Wiebe, 2009; Anand et al., 2011).
Going beyong lexical indicators, Greene and Resnik
(2009) investigated syntactic features to identify per-
spectives or implicit sentiment.

7 Conclusions

We introduced CLIP, a domain-informed, Bayesian
model of ideological proportions in political lan-
guage. We showed how ideological cues could be
discovered from a lightly labeled corpus of ideolog-
ical writings, then incorporated into CLIP. The re-
sulting inferences are largely consistent with a set
of preregistered hypotheses about candidates in the
2008 and 2012 Presidential elections.

Acknowledgments

For thoughtful feedback on this research, the authors
thank: several anonymous reviewers, Amber Boydstun,
Philip Resnik, members of the ARK group at CMU, and
participants in Princeton University’s Political Methodol-
ogy Colloquium and PolMeth XXX hosted by The Uni-
versity of Virginia. This work was supported in part by an
A∗STAR fellowship to Y. Sim, NSF grants IIS-1211201
and IIS-1211277, and Google’s support of the Reading is
Believing project at CMU.

References

Robert P. Abelson and J. Douglas Carroll. 1965. Com-
puter simulation of individual belief systems. Ameri-
can Behavioral Scientist, 8(9):24–30.

Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: Classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.

Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of ICML.

Duncan Black. 1948. On the rationale of group decision-
making. The Journal of Political Economy, 56(1):23–
34.

Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1):107–117.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.

100



Jaime G. Carbonell. 1978. Politics: Automated ideolog-
ical reasoning. Cognitive Science, 2(1):27–51.

Jonathan Charteris-Black. 2005. Politicians and
Rhetoric: The Persuasive Power of Metaphor.
Palgrave-MacMillan.

Joshua Clinton, Simon Jackman, and Douglas Rivers.
2004. The statistical analysis of roll call data. Ameri-
can Political Science Review, 98(2):355–370.

Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In Pro-
ceedings of CIKM.

Thomas M. Cover and Joy A. Thomas. 2012. Elements
of Information Theory. Wiley-Interscience.

Daniel Deirmeier, Jean-Francois Godbout, Bei Yu, and
Stefan Kaufmann. 2012. Language and ideology
in congress. British Journal of Political Science,
42(1):31–55.

Anthony Downs. 1957. An Economic Theory of Democ-
racy. Harper, New York.

Miles Efron. 2004. Cultural orientation: Classifying
subjective documents by cociation analysis. In AAAI
Fall Symposium on Style and Meaning in Language,
Art, and Music.

Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.

Anthony Fader, Dragomir R. Radev, Michael H. Crespin,
Burt L. Monroe, Kevin M. Quinn, and Michael Co-
laresi. 2007. MavenRank: Identifying influential
members of the US senate using lexical centrality. In
Proceedings of EMNLP-CoNLL.

Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statistical
learning methods. In Ashok N. Srivastava and Mehran
Sahami, editors, Text Mining: Classification, Cluster-
ing, and Applications, chapter 2, pages 27–50. Chap-
man & Hall/CRC.

Matthew Gentzkow and Jesse Shapiro. 2005. Media bias
and reputation. Technical report, National Bureau of
Economic Research.

Matthew Gentzkow and Jesse M. Shapiro. 2010. What
drives media slant? evidence from u.s. daily newspa-
pers. Econometrica, 78(1):35–71.

Sean M. Gerrish and David M. Blei. 2011. Predict-
ing legislative roll calls from text. In Proceedings of
ICML.

Sean M. Gerrish and David M. Blei. 2012. How they
vote: Issue-adjusted models of legislative behavior. In
Advances in NIPS 25.

Stephan Greene and Philip Resnik. 2009. More than
words: syntactic packaging and implicit sentiment. In
Proceedings of NAACL.

Roderick P. Hart, Jay P. Childers, and Colene J. Lind.
2013. Political Tone: How Leaders Talk and Why.
University of Chicago Press.

Roderick P. Hart. 2009. Campaign talk: Why elections
are good for us. Princeton University Press.

Dustin Hillard, Stephen Purpura, and John Wilker-
son. 2008. Computer-assisted topic classification for
mixed-methods social science research. Journal of In-
formation Technology & Politics, 4(4):31–46.

Harold Hotelling. 1929. Stability in competition. The
Economic Journal, 39(153):41–57.

Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. The American Political Science Review,
97(2):311–331.

Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.

Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
EMNLP.

Burt L. Monroe and Ko Maeda. 2004. Talk’s cheap:
Text-based estimation of rhetorical ideal-points. Pre-
sented at the Annual Meeting of the Society for Politi-
cal Methodology.

Tony Mullen and Robert Malouf. 2006. A preliminary
investigation into sentiment analysis of informal polit-
ical discourse. In AAAI Symposium on Computational
Approaches to Analysing Weblogs.

Keith T. Poole and Howard Rosenthal. 1985. A spatial
model for legislative roll call analysis. American Jour-
nal of Political Science, 29(2):357–384.

Keith T. Poole and Howard Rosenthal. 2000. Congress:
A Political-Economic History of Roll Call Voting. Ox-
ford University Press.

Warren Sack. 1994. Actor-role analysis: ideology, point
of view, and the news. Master’s thesis, Massachusetts
Institute of Technology, Cambridge, MA.

Karl-Michael Schneider. 2005. Weighted average point-
wise mutual information for feature selection in text
categorization. In Proceedings of PKDD.

Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
EMNLP.

Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In Proceed-
ings of ICML.

101


