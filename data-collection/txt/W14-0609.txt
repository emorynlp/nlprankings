



















































Mining the Twentieth Centurys History from the Time Magazine Corpus


Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 62–70,
Gothenburg, Sweden, April 26 2014. c©2014 Association for Computational Linguistics

Mining the Twentieth Century’s History from the Time Magazine Corpus

Mike Kestemont
University of Antwerp
Prinsstraat 13, D.188

B-2000, Antwerp
Belgium

mike.kestemont
@uantwerpen.be

Folgert Karsdorp
Meertens Institute

Postbus 94264
1090 GG Amsterdam

The Netherlands
Folgert.Karsdorp
@meertens.knaw.nl

Marten Düring
University of North-Carolina

551 Hamilton Hall
CB 3195, Chapel Hill
North Carolina 27599

United States
marten@live.unc.edu

Abstract
In this paper we report on an explorative
study of the history of the twentieth cen-
tury from a lexical point of view. As
data, we use a diachronic collection of
270,000+ English-language articles har-
vested from the electronic archive of the
well-known Time Magazine (1923–2006).
We attempt to automatically identify sig-
nificant shifts in the vocabulary used in
this corpus using efficient, yet unsuper-
vised computational methods, such as Par-
simonious Language Models. We offer a
qualitative interpretation of the outcome
of our experiments in the light of momen-
tous events in the twentieth century, such
as the Second World War or the rise of
the Internet. This paper follows up on a
recent string of frequentist approaches to
studying cultural history (‘Culturomics’),
in which the evolution of human culture is
studied from a quantitative perspective, on
the basis of lexical statistics extracted from
large, textual data sets.

1 Introduction: Culturomics

Although traditionally, the Humanities have been
more strongly associated with qualitative rather
than quantitative methodologies, it is hard to miss
that ‘hipster’ terms like ‘Computational Analy-
sis’, ‘Big Data’ and ‘Digitisation’, are currently
trending in Humanities scholarship. In the interna-
tional initiative of Digital Humanities, researchers
from various disciplines are increasingly explor-
ing novel, computational means to interact with
their object of research. Often, this is done in col-
laboration with researchers from Computational
Linguistics, who seem to have adopted quantita-
tive approaches relatively sooner than other Hu-
manities disciplines. The subfield of Digital His-
tory (Zaagsma, 2013), in which the present paper

is to be situated, is but one of the multiple Human-
ities disciplines in which rapid progress is being
made as to the application of computational meth-
ods. Although the vibrant domain of Digital His-
tory cannot be exhaustively surveyed here due to
space limits, it is nevertheless interesting to refer
to a recent string of frequentist lexical approaches
to the study of human history, and the evolution of
human culture in particular: ‘Culturomics’.

This line of computational, typically data-
intensive research seeks to study various aspects of
human history, by researching the ways in which
(predominantly cultural) phenomena are reflected
in, for instance, word frequency statistics extracted
from large textual data sets. The field has been ini-
tiated in a lively, yet controversial publication by
Michel et al. (2011), which – while it has invited
a lot of attention in popular media – has not gone
uncriticized in the international community of Hu-
manities.1 In this paper, the authors show how a
number of major historical events show interest-
ing correlations with word counts in a vast corpus
of n-grams extracted from the Google Books, al-
legedly containing 4% percent of all books ever
printed.

In recent years, the term ‘Culturomics’ seems
to have become an umbrella term for studies en-
gaging, often at an increasing level of complex-
ity, with the seminal, publicly available Google
Books NGram Corpus (Juola, 2013; Twenge et al.,
2012; Acerbi et al., 2013b). Other studies, like
the inspiring contribution by Leetaru (2011) have
independently explored other data sets for simi-
lar purposes, such as the retroactive prediction of
the Arab Spring Revolution using news data. In

1Consult, for instance, the critical report by A. Grafton
on the occasion of a presentation by Michel and Lieberman
Aiden at one of the annual meetings of the American His-
torical Association (https://www.historians.
org/publications-and-directories/
perspectives-on-history/march-2011/
loneliness-and-freedom).

62



the present paper, we seek to join this recent line
of Culturomics research: we will discuss a series
of quantitative explorations of the Time Magazine
Corpus (1923–2006), a balanced textual data set
covering a good deal of the twentieth (and early
twenty-first) century.

The structure of the paper is as follows: in the
following section 2, we will discuss the data set
used. In section 3, we will introduce some of
the fundamental assumptions underlying the Cul-
turomics approach for Big Data, and report on
an experiment that replicates an earlier sentiment-
related analysis of the Google Books Corpus
(Acerbi et al., 2013b) using our Time data. Subse-
quently, we will apply a Parsimonious Language
Model to our data (section 4) and assess from a
qualitative perspective how and whether this tech-
nique can be used to extract the characteristic vo-
cabulary from specific time periods. To conclude,
we will use these Parsimonious Language Mod-
els in a variability-based neighbor clustering (sec-
tion 5), in an explorative attempt to computation-
ally identify major turning points in the twentieth
century’s history.

2 Data: Time Magazine Corpus

For the present research, we have used a collection
of electronic articles harvested from the archive of
the well-known weekly publication Time Maga-
zine. The magazine’s online archive is protected
by international copyright law and it can only be
consulted via a paying subscription.2 Therefore,
the corpus cannot be freely redistributed in any
format. To construct the corpus, we have used
metadata provided by corpus linguist Mark Davies
who has published a searchable interface to the
Time Corpus (Davies, 2013). For the present
paper, we were only dependent on the unique
identification number and publication year which
Davies provides for each article. Users who are in-
terested in downloading (a portion of) the corpus
which we used, can use this metadata to replicate
our findings.

We have used the Stanford CoreNLP Suite to
annotate this collection (with its default settings
for the English language).3 We have tokenized
and lemmatized the corpus with this tool suite.
Additionally, we have applied part-of-speech tag-

2http://content.time.com/time/archive.
3http://nlp.stanford.edu/software/

corenlp.shtml

Period # Documents # Word forms # Unique forms
1920s 24,332 11,155,681 158,443
1930s 32,788 20,622,526 222,777
1940s 41,832 22,547,958 234,918
1950s 42,249 25,638,032 251,658
1960s 35,440 27,355,389 258,276
1970s 27,804 25,449,488 218,322
1980s 25,651 24,185,889 208,678
1990s 23,300 20,637,179 204,393
2000s 17,299 14,151,399 176,515

Overall 270,695 191,743,541 867,399

Table 1: General word frequency statistics on the
reconstructed version of the Time Corpus (1923-
2006).

ging (Toutanova et al., 2003) and named entity
recognition (Finkel et al., 2005). In the end, our
reconstructed version of the Time Corpus in to-
tal amounted to 270,695 individual articles. In
its entirety, the corpus counted 191,743,541 dis-
tinct word forms (including punctuation marks),
867,399 forms of which proved unique in their
lowercased format. Some general statistics about
our reconstructed version of the Time Corpus are
given in Table 1. In addition to the cumulative
word count statistics about the corpus, we have
included the frequency information per decade
(1920s, 1930s, etc.), as this periodisation will
prove important for the experiments described in
section 4.

In its entirety, the corpus covers the period
March 1923 throughout December 2006. It only
includes articles from the so-called ‘U.S. edition’
of Time (i.e., it does not contain articles which
only featured in the e.g. European edition of the
Magazine). Because of Time’s remarkably contin-
uous publication history, as well as the consider-
able attention the magazine traditionally pays to
international affairs and politics, the Time Cor-
pus can be expected to offer an interesting, al-
beit exclusively American perspective on the re-
cent world history. As far as we know, the cor-
pus has only been used so far in corpus linguistic
publications and we do not know of any advanced
studies in the field of cultural history that make
extensive use of the corpus.

3 Assumption: Lexical frequency

Previous contributions to the field of Culturomics
all have in common that they attempt to establish a
correlation between word frequency statistics and
cultural phenomena. While this is rarely explicitly
voiced, the broader assumption underlying these
studies is that frequency statistics extracted from

63



the texts produced by a society at specific mo-
ment in history, will necessarily reflect that soci-
ety’s cultural specific (e.g. cultural) concerns at
that time. As such, it can for instance be expected
that the frequency of conflict-related terminology
will tend to be more elevated in texts produced by
a society at war than one at peace. (Needless to
say, this need not imply that a society e.g. sup-
ports that war, since the same conflict-related ter-
minology will be frequent in texts that oppose a
particular conflict.) Obviously, the resulting as-
sumption is that the study of developments in the
vocabulary of a large body of texts should enable
the study of the evolution of the broader histori-
cal concerns that exist(ed) in the culture in which
these texts were produced.

Frequency has been considered a key measure
in recent studies into cultural influence (Skiena
and Ward, 2013). The more frequent a word
in a corpus, the more weighty the cultural con-
cerns which that word might be related to. A
naive illustration of this frequency effect can be
gleaned from Figure 1. In the subplots of the fig-
ure, we have plotted the absolute frequency with
which the last names of U.S. presidents have been
yearly mentioned throughout the Time Corpus (in
their lowercased form, and only when tagged as
a named entity). The horizontal axis represents
time, with grey zones indicating start and end
dates of the administration periods. The absolute
frequencies have been normalised in each year, by
taking their ratio over the frequency of the defi-
nite article the. Before plotting, these relative fre-
quencies have been mean-normalised. (Readers
are kindly requested to zoom in on the digital PDF
to view more detail for all figures.) Although this
is by no means a life-changing observation, each
presidential reign is indeed clearly characterised
by a significant boost in the frequency of the cor-
responding president’s last name. Nevertheless,
the graph also displays some notable deficiencies,
such the confusion of father and son Bush, or the
increase in frequency right before an administra-
tion period, which seems related to the presidential
election campaigns.

Importantly, it has been stressed that reliable
frequency information can only be extracted from
large enough corpora, in order to escape the bias
caused by limiting oneself to e.g. too restricted a
number of topics or text varieties. This has caused
studies to stress the importance of so-called ‘Big

Figure 1: Diachronic visualisation of mean-
normalised frequencies-of-mention of the last
names of U.S. presidents in the Time corpus, to-
gether with their administration periods.

Coolidge

Hoover

Roosevelt

Truman

Eisenhower

Kennedy

Johnson

Nixon

Ford

Carter

Reagan

Bush

Clinton

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Bush

Data’ when it comes to Culturomics, reviving the
old adagium from the field of Machine Learning
‘There’s no data like more data’, attributed to Mer-
cer. In terms of data size, it is therefore an impor-
tant question whether the Time Corpus is a reli-
able enough resource for practicing Culturomics.
While the Time Corpus (just under 200 million to-
kens) is not a small data set, it is of course orders
of magnitude smaller than the Google Books cor-
pus with its intimidating 361 billion words. As
such, the Time Corpus might hardly qualify as
‘Big Data’ in the eyes of many contemporary data
scientists. One distinct advantage which the Time
Corpus might offer to counter-balance the disad-
vantage of its limited size, is the high quality, both
of the actual text, as well as the metadata (OCR-
errors are for instance extremely rare).

In order to assess whether a smaller, yet higher-
quality corpus like the Time Corpus might yield
valid results when it comes to Culturomics we
have attempted to replicate an interesting experi-
ment reported by Acerbi et al. (2013b) in the con-
text of a paper on the expression of emotions in
twentieth century books. For their research, they
used the publicly available Google Books unigram
corpus. In our Figure 2 we have reproduced their
‘Figure 1: Historical periods of positive and neg-
ative moods’. For this analysis, they used the so-
called LIWC-procedure: a methodology which at-
tempts to measure the presence of particular emo-
tions in texts by calculating the relative occur-
rences of a set of key words (Tausczik and Pen-

64



nebaker, 2010).4 In the authors’ own words, the
graph “shows that moods tracked broad histori-
cal trends, including a ‘sad’ peak corresponding
to Second World War, and two ‘happy’ peaks, one
in the 1920’s and the other in the 1960’s.”

We have exactly re-engineered their methodol-
ogy and applied it to the Time Corpus. The result
of this entirely parallel LIWC-analysis (Tausczik
and Pennebaker, 2010) of the Time Corpus is visu-
alized in Figure 3. While our data of course only
starts in 1923 instead of 1900 (cf. grey area), it
is clear that our experiment has produced a sur-
prisingly similar curve, especially when it comes
to the ‘sad’ and ‘happy’ periods in the 1940s and
1960s respectively. These pronounced similarities
are especially remarkable because, to our knowl-
edge, the Time Corpus is not only much smaller
but also completely unrelated to the Google Books
corpus. This experiment thus serves to emphasise
the remarkable stability of certain cultural trends
as reflected across various text types and unrelated
text corpora.5 Moreover, these results suggest that
the Time Corpus, in spite of limited size, might
still yield interesting and valid results in the con-
text of Culturomics research.

4 Parsimonious Language Models

As discussed above, Michel et al. (2011) have
proposed a methodology in their seminal paper,
whereby, broadly speaking, they try to establish
a correlation between historical events and word
counts in corpora. They show, for instance, that
the term ‘Great War’ is only frequent in their data
until the 1940s: at that point the more distinc-
tive terms ‘World War I’ and ‘World War II’ sud-
denly become more frequent. One interesting is-
sue here is that this methodology is characterised
by a modest form a ‘cherry picking’: with this
way of working, a researcher will only try out
word frequency plots of which (s)he expects be-
forehand that they will display interesting trends.
Inevitably, this fairly supervised approach might
lower one’s chance to discover new phenomena,
and thus reduces the chance for scientific serendip-
ity to occur. An equally interesting, yet much less

4We would like to thank Ben Verhoeven for sharing
his LIWC-implementation. The methodology adopted by
Acerbi et al. (2013b) has been detailed in the follow-
ing blog post: http://acerbialberto.wordpress.
com/tag/emotion/.

5Acerbi et al. (2013a) have studied the robustness of their
own experiments recently, using different metrics.

Figure 2: Figure reproduced from Acerbi et
al. (2013b): ‘Figure 1: Historical periods of posi-
tive and negative moods’. Also see Figure 3.

supervised approach might therefore be to auto-
matically identify which terms are characteristic
for a given time span in a corpus.

In this respect, it is interesting to refer to Par-
simonious Language Models (PLMs), a fairly re-
cent addition to the field of Information Retrieval
(Hiemstra et al., 2004). PLMs can be used to cre-
ate a probabilistic model of a text collection, de-
scribing the relevance of words in individual docu-
ments in contrast to all other texts in the collection.
From the point of view of indexing in Information
Retrieval, the question which a PLM in reality tries
to answer is: ‘Suppose that in the future, a user
will be looking for this document, which search
terms is (s)he most likely to use?’ As such, PLMs
offers a powerful alternative to the established TF-
IDF metric, in that they are also able to estimate
which words are most characteristic of a given
document. While PLMs are completely unsuper-
vised (i.e. no manual annotation of documents is
needed), they do require setting the λ parameter
beforehand. The λ parameter will of course have
a major influence on the final results, since it will
control the rate at which the language of each doc-
ument will grow different from that of all other
documents, during the subsequent updates of the
model. (For the mathematical details on λ, con-
sult Hiemstra et al. (2004).) Thus, PLMs can be
expected to single out more characteristic vocab-

65



Figure 3: LIWC-analysis carried on the Time Cor-
pus (cf. Figure 2), attempting to replicate the
trends found by Acerbi et al. (2013b). Plotted
is the absolute difference between the z-scores
for the LIWC-categories ‘Positive emotions’ and
‘Negative emotions’. The same smoother (‘Fried-
man’s supersmoother’) has been applied (R Core
Team, 2013).

●

●

●

●

●

●

●
●
●
●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●
●
●

●

●

●

●

●
●

●

●
●

●

●

●

●
●

●

●

●

●
●●

●

●

●
●

●

●●

●●

●

●

●

●

●

●

●

●

●
●

●
●

●

●●
●

●

●

●

●

●

●

●

1900 1920 1940 1960 1980 2000

0.
00

2
0.

00
4

0.
00

6
0.

00
8

0.
01

0

Year

Jo
y−

S
ad

ne
ss

 (
z−

sc
or

es
)

ulary than simpler frequentist approaches and, in-
terestingly, they are more lightweight to run than
e.g. temporal topic models.

In a series of explorative experiments, we have
applied PLMs to the Time Corpus. In particu-
lar, we have build PLMs for this data, by com-
bining individual articles into much larger docu-
ments: both for each year in the data, as well as all
‘decades’ (e.g. 1930 = 1930−1939) we have con-
structed such large, multi-article documents. For
both document types (years and decades), we have
subsequently generated PLMs. In Figure 4 to Fig-
ure 12 we have plotted the results for the PLMs
based on the decade documents (for λ = 0.1).
In the left subpanel, we show the 25 words (tech-
nically, the lowercased lemma’s) which the PLM
estimated to be most discriminative for a given
decade. In the right subpanel, we have plotted the
evolution of the relevance scores for each of these
25 words in the year-based PLM (the grey zone in-
dicates the decade). Higher scores indicate a more
pronounced relevance for a given decade. For the
sake of interpretation, we have restricted our anal-
ysis to words which were tagged as nouns (‘NN’

Figure 4: PLM for the 1920s.

man
play

newspaper
name

son
bill

gentleman
motor
letter

daughter
tariff

p.
railroad

lady
foot

interest
speech

statement
governor

person
ship

condition
honor

automobile
a.

Highest PML scores (1920s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 5: PLM for the 1930s.

week
p.

cent
fortnight

a.
picture

president
son

governor
cinema

name
author

employe
friend

automobile
no.

gold
railroad

m.
bank

daughter
newshawk

ed
wife

depression

Highest PML scores (1930s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

& ‘NNS’).
It is not immediately clear how the output of

these PLMs can be evaluated using quantitative
means. A concise qualitative discussion seems
most appropriate to assess the results. For this
reason, we have combined individual articles into
larger decade documents in these experiments,
since this offers a very intuitive manner of ar-
ranging the available sources from a historical,
interpretative point of view. Often, when peo-
ple address the periodisation of the twentieth cen-
tury they will use decades, where terms like e.g.
‘the seventies’ or ‘the twenties’ refer to a fairly
well-delineated concept in people’s minds, associ-
ated with a particular set of political events, peo-
ple and cultural phenomena, etc. By sticking
to this decade-based periodisation, we can verify
fairly easily to what extent the top 25 yielded by
the PLM corresponds to commonplace historical

66



Figure 6: PLM for the 1940s.

war
week
man

plane
day
air

ship
soldier

radio
jap

production
battle

enemy
admiral

labor
front

officer
bomber

bomb
mile
ton
job

tank
troops

plant

Highest PML scores (1940s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 7: PLM for the 1950s.

man
week
time
day

tv
story

production
picture

hand
record

boy
red

world
army

end
party

ft.
committee

wife
pp

mile
show

girl
farm

trouble

Highest PML scores (1950s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 8: PLM for the 1960s.

man
p.m.

student
nation
school

city
church
negro
state

art
century

space
world

girl
university

today
area

college
jet
life

moon
fact

ft.
market

rights

Highest PML scores (1960s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 9: PLM for the 1970s.

%
price

oil
country
problem

nation
inflation

government
energy

state
black

official
area

court
rate

woman
example

tax
policy

increase
leader

aide
city

people
group

Highest PML scores (1970s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 10: PLM for the 1980s.

%
official

country
government

computer
leader
missile
policy

firm
people
budget

company
rate

program
weapon

arm
deficit

system
drug
force

problem
hostage

group
television

aide

Highest PML scores (1980s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

Figure 11: PLM for the 1990s.

people
child

no
kid

family
movie

drug
woman

computer
parent

issue
tv

way
film

director
lot

show
guy

decade
media
thing

group
phone

network
sex

Highest PML scores (1990s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

67



Figure 12: PLM for the 2000s.

kid
people

company
parent

lot
movie

drug
guy

thing
family

cell
phone

site
internet

risk
technology

attack
study

official
intelligence

way
source

website
terrorism

group

Highest PML scores (2000s)

1920 1930 1940 1950 1960 1970 1980 1990 2000 2010

stereotypes about the decades in the twentieth cen-
tury.

Let us start by inspecting the top 25 for the
1940s, a decade in which the Second War II natu-
rally played a major role. Already at first glance,
it is clear that the top 25 is dominated by war-
related terminology (war, soldier, enemy, . . . ). In-
terestingly, the list also contains words referring
to WWII, but not from the politically correct jar-
gon which we would nowadays use to address the
issue (e.g. jap). Remarkable is the pronounced
position of aviary vocabulary (bomber, air, plane,
. . . ), which is perhaps less surprising if we con-
sider the fact that WWI was one of the first inter-
national conflicts in which aircrafts played a major
military role.

Interestingly, the 1920s are hardly characterised
by an equally focused set of relevant words. Al-
though mobility does seem to play an important
role (cf. the recently invented automobile, but
also ship and railroad), a number of less mean-
ingful abbreviations (such as p. for ‘page’) pop
up that seem connected to superficial changes in
Time’s editorial policies, rather than cultural de-
velopments. (Future analyses might want to re-
move such words manually.) On the other hand,
the use of the terms lady and honour might be
rooted in a cultural climate that is different from
ours (‘lady’ seems the equivalent of woman to-
day). A number of parallel observations can be
made for the 1930s, although here, the high rank-
ing word depression is of course striking (cf. the
economic crisis of 1929). Fascinatingly, a variety
of denominations for (popular) media play a ma-
jor role throughout the decade PLMs. Note, that

while the 1920s’ top 25 mentioned the radio as the
primary communication medium, the popular cin-
ema and (moving?) picture show up in the 1930s.
Interestingly, the popular media of tv and record
make their appearance in the 1950s. (In the 1980s
and 1990s top 25, television moreover continues
to show up.)

The PLM also seems to offer an excellent cul-
tural characterisation of the 1960s and the associ-
ated baby boom, with an emphasis on the contro-
versies of the time, debate involving human rights
(rights, negro, nation), and in particular educa-
tional (college, university, school). The use of
‘educational’ words might well be related to the
social unrest, much of which took place in and
around universities. Does the striking presence of
the word ‘today’ in the list reveal an elevated hic et
nunc mentality in the contemporary States? Amer-
ica’s well-documented interest in space traveling
at the time is also appropriately reflected (space,
moon). Perhaps unexpectedly, this seemingly op-
timistic ‘Zeitgeist’ is more strongly associated in
the Time Corpus with the sixties, than with the
seventies: in the flower-power era, Time displays a
remarkable focus on political and especially eco-
nomic issues. Rather, the oil crisis seems to domi-
nate Time’s lexis in the seventies.

In the 1990s and 2000s, we can observe a fo-
cus on what one might unrespectfully call ‘first-
world problems’, involving for instance family re-
lations (family, kid, parent, child, etc.). Apart
from the fact that Time’s vocabulary seems to grow
more colloquial in general in this period (at least
in our eyes, e.g. guy, lot, thing), a number of
controversial taboo subjects seem to have become
discussable: sex, drug. ‘Terrorism’ and ‘intelli-
gence’ seem to have become major concerns in
post-09/11 America, and perhaps the presence of
the word ‘attack’ and ‘technology’ might be (par-
tially) interpreted in the same light. Again, we
see how vocabulary related to media absolutely
dominates the final rankings in the corpus: Hol-
lywood seems to have enjoyed an increasing pop-
ularity (film, director, movie, . . . ) but it is in-
formation technology that seems to have had the
biggest cultural impact: mobile communication
devices (phone, cell) and Internet-related termi-
nology (network, computer, internet, . . . ) seem to
have caused a major turning point in Time’s lexis.6

6Due to lack of space, we only report results for λ = 0.1
applied to nouns, but highly similar results could be obtained

68



5 Twentieth Century Turning Points?

An interesting technique in this respect is a clus-
tering method called VNC or ‘Variability-Based
Neighbor Clustering’ (Gries and Hilpert, 2008).
The technique has been introduced in the field of
historical linguistics as an aid in the automated
identification of temporal stages in diachronic
data. The method will apply a fairly straightfor-
ward clustering algorithm to a data set (with e.g.
Ward linkage applied to a Cosine distance matrix)
but, importantly, it will add the connectivity con-
straint that (clusters of) data points can only merge
with each other at the next level in a dendrogram,
if they are immediately adjacent. That is to say
that e.g. in a series of yearly observations 1943
would be allowed to merge with 1942 and 1944,
but not with 1928 (even if 1943 would be much
more similar to 1928 than to 1943). We have ap-
plied VNC (with Ward linkage applied to a plain
Cosine distance table) to a series of vectors which
for each year in our data (1923-2006) contained
the PML scores of 5,000 words deemed most rel-
evant for that year by the model.

The dendrogram resulting from the VNC pro-
cedure is visualised in Figure 13. The early his-
tory of Time Magazine (1923-1927) does not re-
ally seem to fit in with the rest and takes up a fairly
deviant position. However, the most attention-
grabbing feature of this tree structure is the major
divide which the dendrogram suggests (cf. red vs.
green-blue cluster) between the years before and
after 1945, the end of the Second World War. An-
other significant rupture seem to be present before
and after 1996: the discussion leaves us to won-
der whether this turning point might related to the
recent introduction of new communication tech-
nologies, in particular the rise of the Internet.

Historically speaking, these turning points do
not come as a surprise. There is, for instance,
widespread acceptance among historians WWII
has indeed been the single most influential event
in the twentieth century. What does surprise, how-
ever, is the relative easy with which a completely
unsupervised procedure has managed to suggest

using other part-of-speech categories and settings for λ. An
interesting effect was associated with ‘fiddling the knob’ of
this last parameter: for lower values (0.01, 0.001 etc.), the
model would come up with perhaps increasingly characteris-
tic, but also increasingly obscure and much less frequent vo-
cabulary. For the fourties, for instance, instead of returning
the word ‘bomber’ the analysis would return the exact name
of a particular bomber type which was used at the time. This
parameter setting deserves further exploration.

Figure 13: Dendrogram resulting from apply-
ing Variability-Based Neighbor Analysis to vec-
tors which contain for each year the 5,000 words
deemed most relevant by the PML.

19
23

19
24

19
25

19
26

19
27

19
28

19
29

19
30

19
31

19
32

19
33

19
34

19
35

19
36

19
37

19
38

19
39

19
40

19
41

19
42

19
43

19
44

19
45

19
46

19
47

19
48

19
49

19
50

19
51

19
52

19
53

19
54

19
55

19
56

19
57

19
58

19
59

19
60

19
61

19
62

19
63

19
64

19
65

19
66

19
67

19
68

19
69

19
70

19
71

19
72

19
73

19
74

19
75

19
76

19
77

19
78

19
79

19
80

19
81

19
82

19
83

19
84

19
85

19
86

19
87

19
88

19
89

19
90

19
91

19
92

19
93

19
94

19
95

19
96

19
97

19
98

19
99

20
00

20
01

20
02

20
03

20
04

20
05

20
06

0.000

0.005

0.010

0.015

0.020

Variability-Based Neighbor Clustering Dendrogram
(5000 highest PML scores per year)

this identification. Arguably, this is where we
leave the realm of the obvious when it comes to
the computational study of cultural history. The
identification of major events and turning points in
human history is normally a task which requires a
good deal of formal education and some advanced
reasoning skills. Here, we might be nearing a
modest form of Artificial Intelligence when we ap-
ply computational methods to achieve a fairly sim-
ilar goal. Hopefully, these analyses, as well as the
ones reported above, illustrate the huge potential
of computational methods in the study of cultural
history, even if only as a discovery tool.

6 Conclusion and criticism

In this paper we have discussed a series of analy-
ses that claim to mine a data-driven cultural char-
acterization of the ‘Zeitgeist’ of some of the main
periods in the twentieth century. Nevertheless, we
must remain vigilant not to overstate the achieve-
ment of these techniques: it remains to be deter-
mined to which extent can we truly call these ap-
plications Digital History and whether these anal-
yses have taught us anything which we did not
know before. Because the twentieth century is
so well known to most of us, the evidence often
tends to be self-referential and self-explanatory,
and merely confirms that which we already knew
intuitively. Like with most distant reading ap-
proaches, the results urge us to go back to the orig-
inal material for the close reading of individual
sources in their historical context, in order to ver-

69



ify the macro-hypotheses that might be suggested
at a higher level. Therefore, the proposed method
might in fact be more suitable for the study of time
periods and corpora of which we know less.

Nevertheless, our methodology seems promis-
ing for future applications in Digital History: our
naı̈ve periodisation in decades, for instance, might
be hugely fine-tuned by processing the results of
a VNC-dendrogram. Breaking up history into
meaningful units is a much more complex, and of-
ten controversial matter (e.g. ‘When does moder-
nity start?’). In this light, it would be helpful to
have at our disposal unbiased, computational tools
that might help us to identify cultural ruptures or
even turning points in history. Our results reported
in the final section do show that this application
yields interesting results, and again, the method
seems promising for the analysis of lesser known
corpora.

Acknowledgments

The authors would like to thank the anonymous re-
viewers and Kalliopi Zervanou for their valuable
feedback on earlier drafts of this paper, as well as
Walter Daelemans and Antal van den Bosch for
the inspiring discussions on the topic. For this
study, Mike Kestemont was funded as a postdoc-
toral research fellow for the Research Foundation
of Flanders (FWO). Folgert Karsdorp was sup-
ported as a Ph.D. candidate by the Computational
Humanities Programme of the Royal Netherlands
Academy of Arts and Sciences, as part of the
Tunes & Tales project.

References
Alberto Acerbi, Vasileios Lampos, and Alexander R.

Bentley. 2013a. Robustness of emotion extraction
from 20th century English books. In BigData ’13.
IEEE, IEEE.

Alberto Acerbi, Vasileios Lampos, Philip Garnett, and
Alexander R. Bentley. 2013b. The Expression
of Emotions in 20th Century Books. PLoS ONE,
8(3):e59030.

Mark Davies. 2013. TIME Magazine Corpus: 100
million words, 1920s-2000s.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Stefan Th. Gries and Martin Hilpert. 2008. The iden-
tification of stages in diachronic data: variability-
based neighbour clustering. Corpora, 3(1):59–81.

Djoerd Hiemstra, Stephen E. Robertson, and Hugo
Zaragoza. 2004. Parsimonious language models for
information retrieval. In Mark Sanderson, Kalervo
Jrvelin, James Allan, and Peter Bruza, editors, SI-
GIR, pages 178–185. ACM.

Patrick Juola. 2013. Using the Google N-Gram corpus
to measure cultural complexity. Literary and Lin-
guistic Computing, 28(4):668–675.

Kalev H. Leetaru. 2011. Culturomics 2.0: Forecasting
large-scale human behavior using global news media
tone in time and space. First Monday, 16(9).

Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale
Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant,
Steven Pinker, Martin A. Nowak, and Erez Lieber-
man Aiden. 2011. Quantitative Analysis of Cul-
ture Using Millions of Digitized Books. Science,
331(6014):176–182.

R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.

Steve Skiena and Charles Ward. 2013. Who Belongs
in Bonnie’s Textbook? Cambridge University Press.

Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24–54.

Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich Part-of-
speech Tagging with a Cyclic Dependency Network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ’03, pages 173–180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Jean M. Twenge, Keith W. Campbell, and Brittany
Gentile. 2012. Increases in Individualistic Words
and Phrases in American Books, 19602008. PLoS
ONE, 7(7):e40181.

Gerben Zaagsma. 2013. On Digital History. BMGN –
Low Countries Historical Review, 128(4):3–29.

70


