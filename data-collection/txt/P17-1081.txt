



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 873–883
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1081

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 873–883
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1081

Context-Dependent Sentiment Analysis in User-Generated Videos
Soujanya Poria

Temasek Laboratories
NTU, Singapore

sporia@ntu.edu.sg

Erik Cambria
School of Computer Science and

Engineering, NTU, Singapore
cambria@ntu.edu.sg

Devamanyu Hazarika
Computer Science and

Engineering, NITW, India
devamanyu@sentic.net

Navonil Mazumder
Centro de Investigacin en
Computacin, IPN, Mexico
navonil@sentic.net

Amir Zadeh
Language Technologies
Institute, CMU, USA

abagherz@cs.cmu.edu

Louis-Philippe Morency
Language Technologies
Institute, CMU, USA

morency@cs.cmu.edu

Abstract

Multimodal sentiment analysis is a de-
veloping area of research, which involves
the identification of sentiments in videos.
Current research considers utterances as
independent entities, i.e., ignores the inter-
dependencies and relations among the ut-
terances of a video. In this paper, we pro-
pose a LSTM-based model that enables
utterances to capture contextual informa-
tion from their surroundings in the same
video, thus aiding the classification pro-
cess. Our method shows 5-10% perfor-
mance improvement over the state of the
art and high robustness to generalizability.

1 Introduction

Sentiment analysis is a ‘suitcase’ research prob-
lem that requires tackling many NLP sub-tasks,
e.g., aspect extraction (Poria et al., 2016a), named
entity recognition (Ma et al., 2016), concept ex-
traction (Rajagopal et al., 2013), sarcasm detec-
tion (Poria et al., 2016b), personality recognition
(Majumder et al., 2017), and more.

Sentiment analysis can be performed at differ-
ent granularity levels, e.g., subjectivity detection
simply classifies data as either subjective (opin-
ionated) or objective (neutral), while polarity de-
tection focuses on determining whether subjec-
tive data indicate positive or negative sentiment.
Emotion recognition further breaks down the in-
ferred polarity into a set of emotions conveyed by
the subjective data, e.g., positive sentiment can be
caused by joy or anticipation, while negative sen-
timent can be caused by fear or disgust.

Even though the primary focus of this paper is
to classify sentiment in videos, we also show the
performance of the proposed method for the finer-
grained task of emotion recognition.

Emotion recognition and sentiment analysis
have become a new trend in social media, help-
ing users and companies to automatically extract
the opinions expressed in user-generated content,
especially videos. Thanks to the high availability
of computers and smartphones, and the rapid rise
of social media, consumers tend to record their re-
views and opinions about products or films and
upload them on social media platforms, such as
YouTube and Facebook. Such videos often con-
tain comparisons, which can aid prospective buy-
ers make an informed decision.

The primary advantage of analyzing videos over
text is the surplus of behavioral cues present in vo-
cal and visual modalities. The vocal modulations
and facial expressions in the visual data, along
with textual data, provide important cues to bet-
ter identify affective states of the opinion holder.
Thus, a combination of text and video data helps to
create a more robust emotion and sentiment anal-
ysis model (Poria et al., 2017a).

An utterance (Olson, 1977) is a unit of speech
bound by breathes or pauses. Utterance-level sen-
timent analysis focuses on tagging every utterance
of a video with a sentiment label (instead of as-
signing a unique label to the whole video). In par-
ticular, utterance-level sentiment analysis is use-
ful to understand the sentiment dynamics of dif-
ferent aspects of the topics covered by the speaker
throughout his/her speech.

Recently, a number of approaches to multi-
modal sentiment analysis, producing interesting
results, have been proposed (Pérez-Rosas et al.,
2013; Wollmer et al., 2013; Poria et al., 2015).
However, there are major issues that remain un-
addressed. Not considering the relation and de-
pendencies among the utterances is one of such is-
sues. State-of-the-art approaches in this area treat
utterances independently and ignore the order of
utterances in a video (Cambria et al., 2017b).

873

https://doi.org/10.18653/v1/P17-1081
https://doi.org/10.18653/v1/P17-1081


Every utterance in a video is spoken at a distinct
time and in a particular order. Thus, a video can
be treated as a sequence of utterances. Like any
other sequence classification problem (Collobert
et al., 2011), sequential utterances of a video may
largely be contextually correlated and, hence, in-
fluence each other’s sentiment distribution. In our
paper, we give importance to the order in which
utterances appear in a video.

We treat surrounding utterances as the con-
text of the utterance that is aimed to be classi-
fied. For example, the MOSI dataset (Zadeh et al.,
2016) contains a video, in which a girl reviews
the movie ‘Green Hornet’. At one point, she says
“The Green Hornet did something similar”. Nor-
mally, doing something similar, i.e., monotonous
or repetitive might be perceived as negative. How-
ever, the nearby utterances “It engages the audi-
ence more”, “they took a new spin on it”, “and I
just loved it” indicate a positive context.

The hypothesis of the independence of tokens
is quite popular in information retrieval and data
mining, e.g., bag-of-words model, but it has a lot
limitations (Cambria and White, 2014). In this pa-
per, we discard such an oversimplifying hypothe-
sis and develop a framework based on long short-
term memory (LSTM) that takes a sequence of ut-
terances as input and extracts contextual utterance-
level features.

The other uncovered major issues in the lit-
erature are the role of speaker-dependent versus
speaker-independent models, the impact of each
modality across the dataset, and generalization
ability of a multimodal sentiment classifier. Leav-
ing these issues unaddressed has presented diffi-
culties in effective comparison of different multi-
modal sentiment analysis methods. In this work,
we address all of these issues.

Our model preserves the sequential order of
utterances and enables consecutive utterances to
share information, thus providing contextual infor-
mation to the utterance-level sentiment classifica-
tion process. Experimental results show that the
proposed framework has outperformed the state of
the art on three benchmark datasets by 5-10%.

The paper is organized as follows: Section 2
provides a brief literature review on multimodal
sentiment analysis; Section 3 describes the pro-
posed method in detail; experimental results and
discussion are shown in Section 4; finally, Sec-
tion 5 concludes the paper.

2 Related Work

The opportunity to capture people’s opinions has
raised growing interest both within the scientific
community, for the new research challenges, and
in the business world, due to the remarkable bene-
fits to be had from financial market prediction.

Text-based sentiment analysis systems can
be broadly categorized into knowledge-based
and statistics-based approaches (Cambria et al.,
2017a). While the use of knowledge bases
was initially more popular for the identification
of polarity in text (Cambria et al., 2016; Poria
et al., 2016c), sentiment analysis researchers have
recently been using statistics-based approaches,
with a special focus on supervised statistical meth-
ods (Socher et al., 2013; Oneto et al., 2016).

In 1974, Ekman (Ekman, 1974) carried out
extensive studies on facial expressions which
showed that universal facial expressions are able
to provide sufficient clues to detect emotions. Re-
cent studies on speech-based emotion analysis
(Datcu and Rothkrantz, 2008) have focused on
identifying relevant acoustic features, such as fun-
damental frequency (pitch), intensity of utterance,
bandwidth, and duration.

As for fusing audio and visual modalities for
emotion recognition, two of the early works
were (De Silva et al., 1997) and (Chen et al.,
1998). Both works showed that a bimodal system
yielded a higher accuracy than any unimodal sys-
tem. More recent research on audio-visual fusion
for emotion recognition has been conducted at ei-
ther feature level (Kessous et al., 2010) or decision
level (Schuller, 2011). While there are many re-
search papers on audio-visual fusion for emotion
recognition, only a few have been devoted to mul-
timodal emotion or sentiment analysis using tex-
tual clues along with visual and audio modalities.
(Wollmer et al., 2013) and (Rozgic et al., 2012)
fused information from audio, visual, and textual
modalities to extract emotion and sentiment.

Poria et al. (Poria et al., 2015, 2016d, 2017b)
extracted audio, visual and textual features us-
ing convolutional neural network (CNN); concate-
nated those features and employed multiple kernel
learning (MKL) for final sentiment classification.
(Metallinou et al., 2008) and (Eyben et al., 2010a)
fused audio and textual modalities for emotion
recognition. Both approaches relied on a feature-
level fusion. (Wu and Liang, 2011) fused audio
and textual clues at decision level.

874



3 Method

In this work, we propose a LSTM network that
takes as input the sequence of utterances in a video
and extracts contextual unimodal and multimodal
features by modeling the dependencies among the
input utterances. M number of videos, comprising
of its constituent utterances, serve as the input. We
represent the dataset as U = u1, u2, u3..., uM and
each ui = ui,1, ui,2, ..., ui, Li where Li is the num-
ber of utterances in video ui. Below, we present
an overview of the proposed method in two major
steps.

A. Context-Independent Unimodal Utterance-
Level Feature Extraction

Firstly, the unimodal features are extracted
without considering the contextual informa-
tion of the utterances (Section 3.1).

B. Contextual Unimodal and Multimodal
Classification

Secondly, the context-independent unimodal
features (from Step A) are fed into a LSTM
network (termed contextual LSTM) that al-
lows consecutive utterances in a video to
share information in the feature extraction
process (Section 3.2).

We experimentally show that this pro-
posed framework improves the performance
of utterance-level sentiment classification over
traditional frameworks.

3.1 Extracting Context-Independent
Unimodal Features

Initially, the unimodal features are extracted from
each utterance separately, i.e., we do not consider
the contextual relation and dependency among the
utterances. Below, we explain the textual, audio,
and visual feature extraction methods.

3.1.1 text-CNN: Textual Features Extraction
The source of textual modality is the transcrip-
tion of the spoken words. For extracting features
from the textual modality, we use a CNN (Karpa-
thy et al., 2014). In particular, we first repre-
sent each utterance as the concatenation of vec-
tors of the constituent words. These vectors are
the publicly available 300-dimensional word2vec
vectors trained on 100 billion words from Google
News (Mikolov et al., 2013).

The convolution kernels are thus applied to
these concatenated word vectors instead of indi-
vidual words. Each utterance is wrapped to a win-
dow of 50 words which serves as the input to the
CNN. The CNN has two convolutional layers; the
first layer has two kernels of size 3 and 4, with 50
feature maps each and the second layer has a ker-
nel of size 2 with 100 feature maps.

The convolution layers are interleaved with
max-pooling layers of window 2 × 2. This is
followed by a fully connected layer of size 500
and softmax output. We use a rectified linear
unit (ReLU) (Teh and Hinton, 2001) as the acti-
vation function. The activation values of the fully-
connected layer are taken as the features of utter-
ances for text modality. The convolution of the
CNN over the utterance learns abstract representa-
tions of the phrases equipped with implicit seman-
tic information, which with each successive layer
spans over increasing number of words and ulti-
mately the entire utterance.

3.1.2 openSMILE: Audio Feature Extraction
Audio features are extracted at 30 Hz frame-rate
and a sliding window of 100 ms. To compute
the features, we use openSMILE (Eyben et al.,
2010b), an open-source software that automati-
cally extracts audio features such as pitch and
voice intensity. Voice normalization is performed
and voice intensity is thresholded to identify sam-
ples with and without voice. Z-standardization is
used to perform voice normalization.

The features extracted by openSMILE con-
sist of several low-level descriptors (LLD), e.g.,
MFCC, voice intensity, pitch, and their statistics,
e.g., mean, root quadratic mean, etc. Specifically,
we use IS13-ComParE configuration file in openS-
MILE. Taking into account all functionals of each
LLD, we obtained 6373 features.

3.1.3 3D-CNN: Visual Feature Extraction
We use 3D-CNN (Ji et al., 2013) to obtain vi-
sual features from the video. We hypothesize
that 3D-CNN will not only be able to learn rele-
vant features from each frame, but will also learn
the changes among given number of consecutive
frames.

In the past, 3D-CNN has been successfully
applied to object classification on tridimensional
data (Ji et al., 2013). Its ability to achieve state-
of-the-art results motivated us to adopt it in our
framework.

875



Let vid ∈ Rc×f×h×w be a video, where c = num-
ber of channels in an image (in our case c = 3,
since we consider only RGB images), f = num-
ber of frames, h = height of the frames, and w =
width of the frames. Again, we consider the 3D
convolutional filter filt ∈ Rfm×c×fd×fh×fw , where
fm = number of feature maps, c = number of chan-
nels, fd = number of frames (in other words depth
of the filter), fh = height of the filter, and fw =
width of the filter. Similar to 2D-CNN, filt slides
across video vid and generates output convout ∈
Rfm×c×(f−fd+1)×(h−fh+1)×(w−fw+1). Next, we ap-
ply max pooling to convout to select only relevant
features. The pooling will be applied only to the
last three dimensions of the array convout.

In our experiments, we obtained best results
with 32 feature maps (fm) with the filter-size of
5 × 5 × 5 (or fd × fh × fw). In other words, the
dimension of the filter is 32 × 3 × 5 × 5 × 5 (or
fm × c × fd × fh × fw). Subsequently, we apply
max pooling on the output of convolution opera-
tion, with window-size being 3 × 3 × 3. This is
followed by a dense layer of size 300 and softmax.
The activation values of this dense layer are finally
used as the video features for each utterance.

3.2 Context-Dependent Feature Extraction

In sequence classification, the classification of
each member is dependent on the other members.
Utterances in a video maintain a sequence. We hy-
pothesize that, within a video, there is a high prob-
ability of inter-utterance dependency with respect
to their sentimental clues.

In particular, we claim that, when classifying
one utterance, other utterances can provide impor-
tant contextual information. This calls for a model
which takes into account such inter-dependencies
and the effect these might have on the target ut-
terance. To capture this flow of informational
triggers across utterances, we use a LSTM-based
recurrent neural network (RNN) scheme (Gers,
2001).

3.2.1 Long Short-Term Memory
LSTM (Hochreiter and Schmidhuber, 1997) is a
kind of RNN, an extension of conventional feed-
forward neural network. Specifically, LSTM cells
are capable of modeling long-range dependencies,
which other traditional RNNs fail to do given the
vanishing gradient issue. Each LSTM cell consists
of an input gate i, an output gate o, and a forget
gate f , to control the flow of information.

Current research (Zhou et al., 2016) indicates
the benefit of using such networks to incorporate
contextual information in the classification pro-
cess. In our case, the LSTM network serves the
purpose of context-dependent feature extraction
by modeling relations among utterances. We term
our architecture ‘contextual LSTM’. We propose
several architectural variants of it later in the pa-
per.

3.2.2 Contextual LSTM Architecture
Let unimodal features have dimension k, each
utterance is thus represented by a feature vec-
tor xi,t ∈ Rk, where t represents the tth utter-
ance of the video i. For a video, we collect the
vectors for all the utterances in it, to get Xi =[xi,1,xi,2, ...,xi,Li] ∈ RLi×k, where Li represents
the number of utterances in the video. This ma-
trix Xi serves as the input to the LSTM. Figure 1
demonstrates the functioning of this LSTM mod-
ule.

In the procedure, getLstmFeatures(Xi) of Al-
gorithm 1, each of these utterance xi,t is passed
through a LSTM cell using the equations men-
tioned in line 32 to 37. The output of the LSTM
cell hi,t is then fed into a dense layer and finally
into a softmax layer (line 38 to 39). The activa-
tions of the dense layer zi,t are used as the context-
dependent features of contextual LSTM.

3.2.3 Training
The training of the LSTM network is performed
using categorical cross-entropy on each utter-
ance’s softmax output per video, i.e.,

loss = − 1(∑Mi=1Li)
M∑
i=1

Li∑
j=1

C∑
c=1 y

j
i,c log2(ŷji,c),

where M = total number of videos, Li = number
of utterances for ith video, yji,c = original output
of class c, and ŷji,c = predicted output for jth utter-
ance of ith video.

As a regularization method, dropout between
the LSTM cell and dense layer is introduced to
avoid overfitting. As the videos do not have the
same number of utterances, padding is introduced
to serve as neutral utterances. To avoid the prolif-
eration of noise within the network, bit masking is
done on these padded utterances to eliminate their
effect in the network. Hyper-parameters tuning is
done on the training set by splitting it into train
and validation components with 80/20% split.

876



Softmax Output

Dense Layer Output

Contextual features

sc-LSTM

Utterance 1 Utterance 2 Utterance n

LSTMLSTM LSTM

Utterance 3

LSTM

...

...

...

...

...

Figure 1: Contextual LSTM network: input features are passed through an unidirectional LSTM layer, followed by a dense and
then a softmax layer. The dense layer activations serve as the output features.

RMSprop has been used as the optimizer which
is known to resolve Adagrad’s radically dimin-
ishing learning rates (Duchi et al., 2011). After
feeding the training set to the network, the test
set is passed through it to generate their context-
dependent features. These features are finally
passed through an SVM for the final classification.

Different Network Architectures We consider
the following variants of the contextual LSTM ar-
chitecture in our experiments.

sc-LSTM This variant of the contextual
LSTM architecture consists of unidirectional
LSTM cells. As this is the simple variant of the
contextual LSTM, we termed it as simple contex-
tual LSTM (sc-LSTM1).

h-LSTM We also investigate an architecture
where the dense layer after the LSTM cell is omit-
ted. Thus, the output of the LSTM cell hi,t pro-
vides our context-dependent features and the soft-
max layer provides the classification. We call this
architecture hidden-LSTM (h-LSTM).

bc-LSTM Bi-directional LSTMs are two uni-
directional LSTMs stacked together having oppo-
site directions. Thus, an utterance can get infor-
mation from utterances occurring before and after
itself in the video. We replaced the regular LSTM
with a bi-directional LSTM and named the result-
ing architecture as bi-directional contextual LSTM
(bc-LSTM). The training process of this architec-
ture is similar to sc-LSTM.

1http://github.com/senticnet/sc-lstm

uni-SVM In this setting, we first obtain the
unimodal features as explained in Section 3.1,
concatenate them and then send to an SVM for the
final classification. It should be noted that using a
gated recurrent unit (GRU) instead of LSTM did
not improve the performance.

3.3 Fusion of Modalities
We accomplish multimodal fusion through two
different frameworks, described below.

3.3.1 Non-hierarchical Framework
In this framework, we concatenate context-
independent unimodal features (from Section 3.1)
and feed that into the contextual LSTM networks,
i.e., sc-LSTM, bc-LSTM, and h-LSTM.

3.3.2 Hierarchical Framework
Contextual unimodal features can further improve
performance of the multimodal fusion framework
explained in Section 3.3.1. To accomplish this, we
propose a hierarchical deep network which con-
sists of two levels.

Level-1 Context-independent unimodal fea-
tures (from Section 3.1) are fed to the proposed
LSTM network to get context-sensitive unimodal
feature representations for each utterance. Individ-
ual LSTM networks are used for each modality.

Level-2 This level consists of a contextual
LSTM network similar to Level-1 but independent
in training and computation. Output from each
LSTM network in Level-1 are concatenated and
fed into this LSTM network, thus providing an in-
herent fusion scheme (see Figure 2).

877



Figure 2: Hierarchical architecture for extracting context-
dependent multimodal utterance features (see Figure 1 for the
LSTM module).

The performance of the second level banks on
the quality of the features from the previous level,
with better features aiding the fusion process. Al-
gorithm 1 describes the overall computation for ut-
terance classification. For the hierarchical frame-
work, we train Level-1 and Level-2 successively
but separately, i.e., the training is not performed
“end-to-end”.

Weight Bias
Wi,Wf ,Wc,Wo ∈ Rd×k bi, bf , bc, bo ∈ Rd
Pi, Pf , Pc, PoVo ∈ Rd×d bz ∈ Rm

Wz ∈ Rm×d bsft ∈ Rc
Wsft ∈ Rc×m

Table 1: Summary of notations used in Algorithm 1. Leg-
enda: d = dimension of hidden unit; k = dimension of input
vectors to LSTM layer; c = number of classes.

4 Experiments

4.1 Dataset details
Most of the research in multimodal sentiment
analysis is performed on datasets with speaker
overlap in train and test splits. Because each in-
dividual has a unique way of expressing emotions
and sentiments, however, finding generic, person-
independent features for sentiment analysis is very
important.

Algorithm 1 Proposed Architecture
1: procedure TRAINARCHITECTURE( U, V)
2: Train context-independent models with U
3: for i:[1,M] do ▷ extract baseline features
4: for j:[1,Li] do
5: xi,j ← TextFeatures(ui,j)
6: x

′
i,j ← V ideoFeatures(ui,j)

7: x”i,j ← AudioFeatures(ui,j)
8: Unimodal:
9: Train LSTM at Level-1 with X,X

′
andX”.

10: for i:[1,M] do ▷ unimodal features
11: Zi ← getLSTMFeatures(Xi)
12: Z

′
i ← getLSTMFeatures(X ′i)

13: Z”i ← getLSTMFeatures(X”i )
14: Multimodal:
15: for i:[1,M] do
16: for j:[1,Li] do
17: if Non-hierarchical fusion then
18: x∗i,j ← (xi,j ∣∣x′i,j ∣∣x”i,j) ▷

concatenation
19: else
20: if Hierarchical fusion then
21: x∗i,j ← (zi,j ∣∣z′i,j ∣∣z”i,j) ▷

concatenation
22: Train LSTM at Level-2 with X∗.
23: for i:[1,M] do ▷ multimodal features
24: Z∗i ← getLSTMFeatures(X∗i )
25: testArchitecture( V)
26: return Z∗
27: procedure TESTARCHITECTURE( V)
28: Similar to training phase. V is passed through the

learnt models to get the features and classification out-
puts. Table 1 shows the trainable parameters.

29: procedure GETLSTMFEATURES(Xi) ▷ for ith video
30: Zi ← φ
31: for t:[1,Li] do ▷ Table 1 provides notation
32: it ← σ(Wixi,t + Pi.ht−1 + bi)
33: C̃t ← tanh(Wcxi,t + Pcht−1 + bc)
34: ft ← σ(Wfxt + Pfht−1 + bf)
35: Ct ← it ∗ C̃t + ft ∗Ct−1
36: ot ← σ(Woxt + Poht−1 + VoCt + bo)
37: ht ← ot ∗ tanh(Ct) ▷ output of lstm cell
38: zt ← ReLU(Wzht + bz) ▷ dense layer
39: prediction← softmax(Wsftzt + bsft)
40: Zi ← Zi ∪ zt
41: return Zi

In real-world applications, the model should be
robust to person idiosyncrasy but it is very diffi-
cult to come up with a generalized model from the
behavior of a limited number of individuals. To
this end, we perform person-independent experi-
ments to study generalization of our model, i.e.,
our train/test splits of the datasets are completely
disjoint with respect to speakers.

Multimodal Sentiment Analysis Datasets

MOSI The MOSI dataset (Zadeh et al., 2016)
is a dataset rich in sentimental expressions where
93 people review topics in English. The videos

878



are segmented with each segments sentiment label
scored between +3 (strong positive) to -3 (strong
negative) by 5 annotators. We took the average
of these five annotations as the sentiment polarity
and, hence, considered only two classes (positive
and negative). The train/validation set consists of
the first 62 individuals in the dataset. The test set
contains opinionated videos by rest 31 speakers.
In particular, 1447 and 752 utterances are used in
training and test, respectively.

MOUD This dataset (Pérez-Rosas et al.,
2013) contains product review videos provided by
55 persons. The reviews are in Spanish (we used
Google Translate API2 to get the English tran-
scripts). The utterances are labeled to be either
positive, negative or neutral. However, we drop
the neutral label to maintain consistency with pre-
vious work. Out of 79 videos in the dataset, 59
videos are considered in the train/val set.

Multimodal Emotion Recognition Datasets

IEMOCAP The IEMOCAP (Busso et al.,
2008) contains the acts of 10 speakers in a two-
way conversation segmented into utterances. The
medium of the conversations in all the videos is
English. The database contains the following cat-
egorical labels: anger, happiness, sadness, neutral,
excitement, frustration, fear, surprise, and other,
but we take only the first four so as to compare
with the state of the art (Rozgic et al., 2012).
Videos by the first 8 speakers are considered in
the training set. The train/test split details are pro-
vided in Table 2, which provides information re-
garding train/test split of all the datasets. Table 2
also provides cross-dataset split details where the
datasets MOSI and MOUD are used for training
and testing, respectively. The proposed model be-
ing used on reviews from different languages al-
lows us to analyze its robustness and generalizabil-
ity.

4.1.1 Characteristic of the Datasets
In order to evaluate the robustness of our proposed
method, we employ it on multiple datasets of
different kinds. Both MOSI and MOUD are
used for the sentiment classification task but
they consist of review videos spoken in different
languages, i.e., English and Spanish, respectively.

2http://translate.google.com

IEMOCAP dataset is different from MOSI and
MOUD since it is annotated with emotion la-
bels. Apart from this, IEMOCAP dataset was
created using a different method than MOSI and
MOUD. These two datasets were developed by
crawling consumers’ spontaneous online product
review videos from popular social websites and
later labeled with sentiment labels. To curate the
IEMOCAP dataset, instead, subjects were pro-
vided affect-related scripts and asked to act.

As pointed out by Poria et al. (Poria et al.,
2017a), acted dataset like IEMOCAP can suffer
from biased labeling and incorrect acting which
can further cause the poor generalizability of the
models trained on the acted datasets.

Dataset Train Testuttrnce video uttrnce video
IEMOCAP 4290 120 1208 31

MOSI 1447 62 752 31
MOUD 322 59 115 20

MOSI→MOUD 2199 93 437 79
Table 2: uttrnce: Utterance; Person-Independent Train/Test
split details of each dataset (≈ 70/30 % split). Legenda: X→Y
represents train: X and test: Y; Validation sets are extracted
from the shuffled training sets using 80/20 % train/val ratio.

It should be noted that the datasets’ individ-
ual configuration and splits are same throughout
all the experiments (i.e., context-independent uni-
modal feature extraction, LSTM-based context-
dependent unimodal and multimodal feature ex-
traction and classification).

4.2 Performance of Different Models

In this section, we present unimodal and multi-
modal sentiment analysis performance of differ-
ent LSTM network variants as explained in Sec-
tion 3.2.3 and comparison with the state of the art.

Hierarchical vs Non-hierarchical Fusion
Framework As expected, trained contextual
unimodal features help the hierarchical fusion
framework to outperform the non-hierarchical
framework. Table 3 demonstrates this by com-
paring the hierarchical and the non-hierarchical
frameworks using the bc-LSTM network.

For this reason, we the rest of the analysis only
leverages on the hierarchical framework. The
non-hierarchical model outperforms the baseline
uni-SVM, which confirms that it is the context-
sensitive learning paradigm that plays the key role
in improving performance over the baseline.

879



Comparison of Different Network Variants It
is to be noted that both sc-LSTM and bc-LSTM
perform quite well on the multimodal emotion
recognition and sentiment analysis datasets. Since
bc-LSTM has access to both the preceding and
following information of the utterance sequence,
it performs consistently better on all the datasets
over sc-LSTM. The usefulness of the dense layer
in increasing the performance is evident from the
experimental results shown in Table 3. The per-
formance improvement is in the range of 0.3%
to 1.5% on MOSI and MOUD datasets. On the
IEMOCAP dataset, the performance improvement
of bc-LSTM and sc-LSTM over h-LSTM is in the
range of 1% to 5%.

Comparison with the Baselines Every LSTM
network variant has outperformed the baseline
uni-SVM on all the datasets by the margin of
2% to 5% (see Table 3). These results prove
our initial hypothesis that modeling the contex-
tual dependencies among utterances (which uni-
SVM cannot do) improves the classification. The
higher performance improvement on the IEMO-
CAP dataset indicates the necessity of modeling
long-range dependencies among the utterances as
continuous emotion recognition is a multiclass se-
quential problem where a person does not fre-
quently change emotions (Wöllmer et al., 2008).
We have implemented and compared with the cur-
rent state-of-the-art approach proposed by (Po-
ria et al., 2015). In their method, they extracted
features from each modality and fed these to a
MKL classifier. However, they did not conduct
the experiment in a speaker-independent manner
and also did not consider the contextual relation
among the utterances. In Table 3, the results in
bold are statistically significant (p < 0.05) in com-
pare to uni-SVM. Experimental results in Table 4
show that the proposed method outperformes (Po-
ria et al., 2015) by a significant margin. For the
emotion recognition task, we have compared our
method with the current state of the art (Rozgic
et al., 2012), who extracted features in a similar
fashion to (Poria et al., 2015) (although they used
SVM trees (Yuan et al., 2006) for the fusion).

4.3 Importance of the Modalities

As expected, in all kinds of experiments, bimodal
and trimodal models have outperformed unimodal
models. Overall, audio modality has performed
better than visual on all the datasets.

On MOSI and IEMOCAP datasets, the tex-
tual classifier achieves the best performance over
other unimodal classifiers. On IEMOCAP dataset,
the unimodal and multimodal classifiers obtained
poor performance to classify neutral utterances.
The textual modality, combined with non-textual
modes, boosts the performance in IEMOCAP by
a large margin. However, the margin is less in the
other datasets.

On the MOUD dataset, the textual modality per-
forms worse than audio modality due to the noise
introduced in translating Spanish utterances to En-
glish. Using Spanish word vectors3 in text-CNN
results in an improvement of 10%. Nonetheless,
we report results using these translated utterances
as opposed to utterances trained on Spanish word
vectors, in order to make fair comparison with
(Poria et al., 2015).

4.4 Generalization of the Models
To test the generalizability of the models, we have
trained our framework on complete MOSI dataset
and tested on MOUD dataset (Table 5). The per-
formance was poor for audio and textual modal-
ity as the MOUD dataset is in Spanish while
the model is trained on MOSI dataset, which is
in English language. However, notably the vi-
sual modality performs better than the other two
modalities in this experiment, which means that
in cross-lingual scenarios facial expressions carry
more generalized, robust information than audio
and textual modalities. We could not carry out a
similar experiment for emotion recognition as no
other utterance-level dataset apart from the IEMO-
CAP was available at the time of our experiments.

4.5 Qualitative Analysis
The need for considering context dependency (see
Section 1) is of prime importance for utterance-
level sentiment classification. For example, in the
utterance “What would have been a better name
for the movie”, the speaker is attempting to com-
ment the quality of the movie by giving an appro-
priate name. However, the sentiment is expressed
implicitly and requires the contextual knowledge
about the mood of the speaker and his/her general
opinion about the film. The baseline unimodal-
SVM and state of the art fail to classify this utter-
ance correctly4.

3http://crscardellino.me/SBWCE
4RNTN classifies it as neutral. It can be seen here

http://nlp.stanford.edu:8080/sentiment/rntnDemo.html

880



Modality
MOSI MOUD IEMOCAP

hierarchical (%)

no
n-

hi
er

(%
) hierarchical (%)

no
n-

hi
er

(%
) hierarchical (%)

no
n-

hi
er

(%
)

un
i-

SV
M

h-
L

ST
M

sc
-L

ST
M

bc
-L

ST
M

un
i-

SV
M

h-
L

ST
M

sc
-L

ST
M

bc
-L

ST
M

un
i-

SV
M

h-
L

ST
M

sc
-L

ST
M

bc
-L

ST
M

T 75.5 77.4 77.6 78.1 49.5 50.1 51.3 52.1 65.5 68.9 71.4 73.6
V 53.1 55.2 55.6 55.8 46.3 48.0 48.2 48.5 47.0 52.0 52.6 53.2
A 58.5 59.6 59.9 60.3 51.5 56.3 57.5 59.9 52.9 54.4 55.2 57.1

T + V 76.7 78.9 79.9 80.2 78.5 50.2 50.6 51.3 52.2 50.9 68.5 70.3 72.3 75.4 73.2
T + A 75.8 78.3 78.8 79.3 78.2 53.1 56.9 57.4 60.4 55.5 70.1 74.1 75.2 75.6 74.5
V + A 58.6 61.5 61.8 62.1 60.3 62.8 62.9 64.4 65.3 64.2 67.6 67.8 68.2 68.9 67.3

T + V + A 77.9 78.1 78.6 80.3 78.1 66.1 66.4 67.3 68.1 67.0 72.5 73.3 74.2 76.1 73.5

Table 3: Comparison of models mentioned in Section 3.2.3. The table reports the accuracy of classification. Legenda: non-hier← Non-hierarchical bc-lstm. For remaining fusion, hierarchical fusion framework is used (Section 3.3.2).

Modality Sentiment (%) Emotion on IEMOCAP (%)MOSI MOUD angry happy sad neutral
T 78.12 52.17 76.07 78.97 76.23 67.44
V 55.80 48.58 53.15 58.15 55.49 51.26
A 60.31 59.99 58.37 60.45 61.35 52.31

T + V 80.22 52.23 77.24 78.99 78.35 68.15
T + A 79.33 60.39 77.15 79.10 78.10 69.14
V + A 62.17 65.36 68.21 71.97 70.35 62.37

A + V + T 80.30 68.11 77.98 79.31 78.30 69.92
State-of

73.551 63.251 73.10 2 72.402 61.902 58.102-the-art
1by (Poria et al., 2015),2by (Rozgic et al., 2012)

Table 4: Accuracy % on textual (T), visual (V), audio (A)
modality and comparison with the state of the art. For the
fusion, the hierarchical fusion framework was used.

Modality MOSI→MOUDuni-SVM h-LSTM sc-LSTM bc-LSTM
T 46.5% 46.5% 46.6% 46.9%
V 43.3% 45.5% 48.3% 49.6%
A 42.9% 46.0% 46.4% 47.2%

T + V 49.8% 49.8% 49.8% 49.8%
T + A 50.4% 50.9% 51.1% 51.3%
V + A 46.0% 47.1% 49.3% 49.6%

T + V + A 51.1% 52.2% 52.5% 52.7%

Table 5: Cross-dataset comparison in terms of classification
accuracy.

However, information from neighboring ut-
terances, e.g., “And I really enjoyed it” and
“The countryside which they showed while go-
ing through Ireland was astoundingly beautiful”
indicate its positive context and help our contex-
tual model to classify the target utterance cor-
rectly. Such contextual relationships are prevalent
throughout the dataset.

In order to have a better understanding of the
roles of each modality for the overall classifica-
tion, we have also done some qualitative analy-
sis. For example, the utterance “who doesn’t have

any presence or greatness at all” was classified
as positive by the audio classifier (as “presence
and greatness at all” was spoken with enthusiasm).
However, the textual modality caught the negation
induced by “doesn’t” and classified it correctly.
The same happened to the utterance “amazing spe-
cial effects”, which presented no jest of enthusi-
asm in the speaker’s voice nor face, but was cor-
rectly classified by the textual classifier.

On other hand, the textual classifier classified
the utterance “that like to see comic book charac-
ters treated responsibly” as positive (for the pres-
ence of “like to see” and “responsibly”) but the
high pitch of anger in the person’s voice and the
frowning face helps to identify this as a negative
utterance. In some cases, the predictions of the
proposed method are wrong because of face oc-
clusion or noisy audio. Also, in cases where sen-
timent is very weak and non contextual, the pro-
posed approach shows some bias towards its sur-
rounding utterances, which further leads to wrong
predictions.

5 Conclusion

The contextual relationship among utterances in a
video is mostly ignored in the literature. In this pa-
per, we developed a LSTM-based network to ex-
tract contextual features from the utterances of a
video for multimodal sentiment analysis. The pro-
posed method has outperformed the state of the art
and showed significant performance improvement
over the baseline.

As future work, we plan to develop a LSTM-
based attention model to determine the importance
of each utterance and its specific contribution to
each modality for sentiment classification.

881



References
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe

Kazemzadeh, Emily Mower, Samuel Kim, Jean-
nette N Chang, Sungbok Lee, and Shrikanth S
Narayanan. 2008. Iemocap: Interactive emotional
dyadic motion capture database. Language re-
sources and evaluation 42(4):335–359.

Erik Cambria, Dipankar Das, Sivaji Bandyopadhyay,
and Antonio Feraco. 2017a. A Practical Guide to
Sentiment Analysis. Springer, Cham, Switzerland.

Erik Cambria, Devamanyu Hazarika, Soujanya Po-
ria, Amir Hussain, and RBV Subramanyam. 2017b.
Benchmarking multimodal sentiment aanlysis. In
CICLing.

Erik Cambria, Soujanya Poria, Rajiv Bajpai, and Björn
Schuller. 2016. SenticNet 4: A semantic resource
for sentiment analysis based on conceptual primi-
tives. In COLING. pages 2666–2677.

Erik Cambria and Bebo White. 2014. Jumping NLP
curves: A review of natural language processing re-
search. IEEE Computational Intelligence Magazine
9(2):48–57.

Lawrence S Chen, Thomas S Huang, Tsutomu
Miyasato, and Ryohei Nakatsu. 1998. Multimodal
human emotion/expression recognition. In Proceed-
ings of the Third IEEE International Conference
on Automatic Face and Gesture Recognition. IEEE,
pages 366–371.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.

Dragos Datcu and L Rothkrantz. 2008. Seman-
tic audio-visual data fusion for automatic emotion
recognition. Euromedia’2008 .

Liyanage C De Silva, Tsutomu Miyasato, and Ryohei
Nakatsu. 1997. Facial emotion recognition using
multi-modal information. In Proceedings of ICICS.
IEEE, volume 1, pages 397–401.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12(Jul):2121–2159.

Paul Ekman. 1974. Universal facial expressions of
emotion. Culture and Personality: Contemporary
Readings/Chicago .

Florian Eyben, Martin Wöllmer, Alex Graves, Björn
Schuller, Ellen Douglas-Cowie, and Roddy Cowie.
2010a. On-line emotion recognition in a 3-d
activation-valence-time continuum using acoustic
and linguistic cues. Journal on Multimodal User In-
terfaces 3(1-2):7–19.

Florian Eyben, Martin Wöllmer, and Björn Schuller.
2010b. Opensmile: the munich versatile and fast
open-source audio feature extractor. In Proceedings
of the 18th ACM international conference on Multi-
media. ACM, pages 1459–1462.

Felix Gers. 2001. Long Short-Term Memory in Re-
current Neural Networks. Ph.D. thesis, Universität
Hannover.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2013.
3d convolutional neural networks for human action
recognition. IEEE transactions on pattern analysis
and machine intelligence 35(1):221–231.

Andrej Karpathy, George Toderici, Sanketh Shetty,
Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.
2014. Large-scale video classification with convolu-
tional neural networks. In Proceedings of the IEEE
conference on Computer Vision and Pattern Recog-
nition. pages 1725–1732.

Loic Kessous, Ginevra Castellano, and George Cari-
dakis. 2010. Multimodal emotion recognition in
speech-based interaction using facial expression,
body gesture and acoustic analysis. Journal on Mul-
timodal User Interfaces 3(1-2):33–48.

Yukun Ma, Erik Cambria, and Sa Gao. 2016. Label
embedding for zero-shot fine-grained named entity
typing. In COLING. Osaka, pages 171–180.

Navonil Majumder, Soujanya Poria, Alexander Gel-
bukh, and Erik Cambria. 2017. Deep learning based
document modeling for personality detection from
text. IEEE Intelligent Systems 32(2):74–79.

Angeliki Metallinou, Sungbok Lee, and Shrikanth
Narayanan. 2008. Audio-visual emotion recogni-
tion using gaussian mixture models for face and
voice. In Tenth IEEE International Symposium on
ISM 2008. IEEE, pages 250–257.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

David Olson. 1977. From utterance to text: The bias
of language in speech and writing. Harvard educa-
tional review 47(3):257–281.

Luca Oneto, Federica Bisio, Erik Cambria, and Davide
Anguita. 2016. Statistical learning theory and ELM
for big social data analysis. IEEE Computational
Intelligence Magazine 11(3):45–55.

Verónica Pérez-Rosas, Rada Mihalcea, and Louis-
Philippe Morency. 2013. Utterance-level multi-
modal sentiment analysis. In ACL (1). pages 973–
982.

882



Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017a. A review of affective computing:
From unimodal analysis to multimodal fusion. In-
formation Fusion .

Soujanya Poria, Erik Cambria, and Alexander Gel-
bukh. 2015. Deep convolutional neural network
textual features and multiple kernel learning for
utterance-level multimodal sentiment analysis. In
Proceedings of EMNLP. pages 2539–2544.

Soujanya Poria, Erik Cambria, and Alexander Gel-
bukh. 2016a. Aspect extraction for opinion min-
ing with a deep convolutional neural network.
Knowledge-Based Systems 108:42–49.

Soujanya Poria, Erik Cambria, D Hazarika, and Prateek
Vij. 2016b. A deeper look into sarcastic tweets using
deep convolutional neural networks. In COLING.
pages 1601–1612.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Fed-
erica Bisio. 2016c. Sentic LDA: Improving on LDA
with semantic similarity for aspect-based sentiment
analysis. In IJCNN. pages 4465–4473.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and
Amir Hussain. 2016d. Convolutional mkl based
multimodal emotion recognition and sentiment anal-
ysis. In Data Mining (ICDM), 2016 IEEE 16th In-
ternational Conference on. IEEE, pages 439–448.

Soujanya Poria, Haiyun Peng, Amir Hussain, Newton
Howard, and Erik Cambria. 2017b. Ensemble appli-
cation of convolutional neural networks and multiple
kernel learning for multimodal sentiment analysis.
Neurocomputing .

Dheeraj Rajagopal, Erik Cambria, Daniel Olsher, and
Kenneth Kwok. 2013. A graph-based approach to
commonsense concept extraction and semantic sim-
ilarity detection. In WWW. Rio De Janeiro, pages
565–570.

Viktor Rozgic, Sankaranarayanan Ananthakrishnan,
Shirin Saleem, Rohit Kumar, and Rohit Prasad.
2012. Ensemble of svm trees for multimodal emo-
tion recognition. In Signal & Information Pro-
cessing Association Annual Summit and Conference
(APSIPA ASC), 2012 Asia-Pacific. IEEE, pages 1–4.

Björn Schuller. 2011. Recognizing affect from linguis-
tic information in 3d continuous space. IEEE Trans-
actions on Affective Computing 2(4):192–205.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP. pages 1631–1642.

Vee Teh and Geoffrey E Hinton. 2001. Rate-coded re-
stricted boltzmann machines for face recognition. In
T Leen, T Dietterich, and V Tresp, editors, Advances
in neural information processing system. volume 13,
pages 908–914.

Martin Wöllmer, Florian Eyben, Stephan Reiter,
Björn W Schuller, Cate Cox, Ellen Douglas-Cowie,
Roddy Cowie, et al. 2008. Abandoning emo-
tion classes-towards continuous emotion recognition
with modelling of long-range dependencies. In In-
terspeech. volume 2008, pages 597–600.

Martin Wollmer, Felix Weninger, Timo Knaup, Bjorn
Schuller, Congkai Sun, Kenji Sagae, and Louis-
Philippe Morency. 2013. Youtube movie reviews:
Sentiment analysis in an audio-visual context. IEEE
Intelligent Systems 28(3):46–53.

Chung-Hsien Wu and Wei-Bin Liang. 2011. Emo-
tion recognition of affective speech based on mul-
tiple classifiers using acoustic-prosodic information
and semantic labels. IEEE Transactions on Affective
Computing 2(1):10–21.

Xun Yuan, Wei Lai, Tao Mei, Xian-Sheng Hua, Xiu-
Qing Wu, and Shipeng Li. 2006. Automatic video
genre categorization using hierarchical svm. In Im-
age Processing, 2006 IEEE International Confer-
ence on. IEEE, pages 2905–2908.

Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-
Philippe Morency. 2016. Multimodal sentiment in-
tensity analysis in videos: Facial gestures and verbal
messages. IEEE Intelligent Systems 31(6):82–88.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In The 54th Annual
Meeting of the Association for Computational Lin-
guistics. pages 207–213.

883


	Context-Dependent Sentiment Analysis in User-Generated Videos

