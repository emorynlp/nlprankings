



















































Searching for Effective Neural Extractive Summarization: What Works and What’s Next


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049–1058
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1049

Searching for Effective Neural Extractive Summarization:
What Works and What’s Next

Ming Zhong∗, Pengfei Liu∗, Danqing Wang, Xipeng Qiu†, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China

{mzhong18,pfliu14,dqwang18,xpqiu,xjhuang}@fudan.edu.cn

Abstract

The recent years have seen remarkable suc-
cess in the use of deep neural networks on text
summarization. However, there is no clear un-
derstanding of why they perform so well, or
how they might be improved. In this paper, we
seek to better understand how neural extractive
summarization systems could benefit from dif-
ferent types of model architectures, transfer-
able knowledge and learning schemas. Addi-
tionally, we find an effective way to improve
current frameworks and achieve the state-of-
the-art result on CNN/DailyMail by a large
margin based on our observations and analy-
ses. Hopefully, our work could provide more
clues for future research on extractive sum-
marization. Source code will be available on
Github1.

1 Introduction

Recent years has seen remarkable success in the
use of deep neural networks for text summariza-
tion (See et al., 2017; Celikyilmaz et al., 2018;
Jadhav and Rajan, 2018). So far, most research
utilizing the neural network for text summariza-
tion has revolved around architecture engineer-
ing (Zhou et al., 2018; Chen and Bansal, 2018;
Gehrmann et al., 2018).

Despite their success, it remains poorly under-
stood why they perform well and what their short-
comings are, which limits our ability to design bet-
ter architectures. The rapid development of neural
architectures calls for a detailed empirical study
of analyzing and understanding existing models.
In this paper, we primarily focus on extractive
summarization since they are computationally ef-
ficient, and can generate grammatically and coher-
ent summaries (Nallapati et al., 2017). and seek to

∗These two authors contributed equally.
†Corresponding author.

1https://github.com/fastnlp/fastNLP

better understand how neural network-based ap-
proaches to this task could benefit from different
types of model architectures, transferable knowl-
edge, and learning schemas, and how they might
be improved.

Architectures Architecturally, the better perfor-
mance usually comes at the cost of our under-
standing of the system. To date, we know little
about the functionality of each neural component
and the differences between them (Peters et al.,
2018b), which raises the following typical ques-
tions: 1) How does the choice of different neu-
ral architectures (CNN, RNN, Transformer) influ-
ence the performance of the summarization sys-
tem? 2) Which part of components matters for
specific dataset? 3) Do current models suffer from
the over-engineering problem?

Understanding the above questions can not only
help us to choose suitable architectures in different
application scenarios, but motivate us to move for-
ward to more powerful frameworks.

External Transferable Knowledge and Learn-
ing schemas Clearly, the improvement in accu-
racy and performance is not merely because of the
shift from feature engineering to structure engi-
neering, but the flexible ways to incorporate exter-
nal knowledge (Mikolov et al., 2013; Peters et al.,
2018a; Devlin et al., 2018) and learning schemas
to introduce extra instructive constraints (Paulus
et al., 2017; Arumae and Liu, 2018). For this part,
we make some first steps toward answers to the
following questions: 1) Which type of pre-trained
models (supervised or unsupervised pre-training)
is more friendly to the summarization task? 2)
When architectures are explored exhaustively, can
we push the state-of-the-art results to a new level
by introducing external transferable knowledge or
changing another learning schema?

To make a comprehensive study of above an-

https://github.com/fastnlp/fastNLP


1050

Perspective Content Sec.ID

Learning Schemas Sup. & Reinforce. 4.4

Structure
Dec. Pointer & SeqLab. 4.3.1
Enc. LSTM & Transformer 4.3.2

Knowledge
Exter. GloVe BERT NEWS.

4.3.3Inter. Random

Table 1: Outline of our experimental design. Dec. and
Enc. represent decoder and encoder respectively. Sup.
denotes supervised learning and NEWS. means super-
vised pre-training knowledge.

alytical perspectives, we first build a testbed for
summarization system, in which training and test-
ing environment will be constructed. In the train-
ing environment, we design different summariza-
tion models to analyze how they influence the
performance. Specifically, these models differ
in the types of architectures (Encoders: CNN,
LSTM, Transformer (Vaswani et al., 2017); De-
coders: auto-regressive2, non auto-regressive), ex-
ternal transferable knowledge (GloVe (Penning-
ton et al., 2014), BERT (Devlin et al., 2018),
NEWSROOM (Grusky et al., 2018)) and different
learning schemas (supervised learning and rein-
forcement learning).

To peer into the internal working mechanism of
above testing cases, we provide sufficient evalu-
ation scenarios in the testing environment. Con-
cretely, we present a multi-domain test, sentence
shuffling test, and analyze models by different
metrics: repetition, sentence length, and position
bias, which we additionally developed to provide
a better understanding of the characteristics of dif-
ferent datasets.

Empirically, our main observations are summa-
rized as:

1) Architecturally speaking, models with auto-
regressive decoder are prone to achieving bet-
ter performance against non auto-regressive de-
coder. Besides, LSTM is more likely to suffer
from the architecture overfitting problem while
Transformer is more robust.

2) The success of extractive summarization sys-
tem on the CNN/DailyMail corpus heavily relies
on the ability to learn positional information of the
sentence.

3) Unsupervised transferable knowledge is
more useful than supervised transferable knowl-

2Auto-regressive indicates that the decoder can make cur-
rent prediction with knowledge of previous predictions.

edge since the latter one is easily influenced by the
domain shift problem.

4) We find an effective way to improve the cur-
rent system, and achieving the state-of-the-art re-
sult on CNN/DailyMail by a large margin with
the help of unsupervised transferable knowledge
(42.39 R-1 score). And this result can be further
enhanced by introducing reinforcement learning
(42.69 R-1 score).

Hopefully, this detailed empirical study can pro-
vide more hints for the follow-up researchers to
design better architectures and explore new state-
of-the-art results along a right direction.

2 Related Work

The work is connected to the following threads of
work of NLP research.

Task-oriented Neural Networks Interpreting
Without knowing the internal working mechanism
of the neural network, it is easy for us to get
into a hobble when the performance of a task has
reached the bottleneck. More recently, Peters et al.
(2018b) investigate how different learning frame-
works influence the properties of learned contex-
tualized representations. Different from this work,
in this paper, we focus on dissecting the neural
models for text summarization.

A similar work to us is Kedzie et al. (2018),
which studies how deep learning models perform
context selection in terms of several typical sum-
marization architectures, and domains. Compared
with this work, we make a more comprehensive
study and give more different analytic aspects. For
example, we additionally investigate how transfer-
able knowledge influence extractive summariza-
tion and a more popular neural architecture, Trans-
former. Besides, we come to inconsistent con-
clusions when analyzing the auto-regressive de-
coder. More importantly, our paper also shows
how existing systems can be improved, and we
have achieved a state-of-the-art performance on
CNN/DailyMail.

Extractive Summarization Most of recent
work attempt to explore different neural compo-
nents or their combinations to build an end-to-end
learning model. Specifically, these work instan-
tiate their encoder-decoder framework by choos-
ing recurrent neural networks (Cheng and Lapata,
2016; Nallapati et al., 2017; Zhou et al., 2018)
as encoder, auto-regressive decoder (Chen and



1051

Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al.,
2018) or non auto-regressive decoder (Isonuma
et al., 2017; Narayan et al., 2018; Arumae and
Liu, 2018) as decoder, based on pre-trained word
representations (Mikolov et al., 2013; Pennington
et al., 2014). However, how to use Transformer in
extractive summarization is still a missing issue.
In addition, some work uses reinforcement learn-
ing technique (Narayan et al., 2018; Wu and Hu,
2018; Chen and Bansal, 2018), which can provide
more direct optimization goals. Although above
work improves the performance of summarization
system from different perspectives, yet a compre-
hensive study remains missing.

3 A Testbed for Text Summarization

To analyze neural summarization system, we pro-
pose to build a Training-Testing environment, in
which different text cases (models) are firstly gen-
erated under different training settings, and they
are further evaluated under different testing set-
tings. Before the introduction of our Train-Testing
testbed, we first give a description of text summa-
rization.

3.1 Task Description

Existing methods of extractive summarization di-
rectly choose and output the salient sentences (or
phrases) in the original document. Formally, given
a document D = d1, · · · , dn consisting of n sen-
tences, the objective is to extract a subset of sen-
tences R = r1, · · · , rm from D, m is determinis-
tic during training while is a hyper-parameter in
testing phase. Additionally, each sentence con-
tains |di| words di = x1, · · · , x|di|.

Generally, most of existing extractive summa-
rization systems can be abstracted into the follow-
ing framework, consisting of three major modules:
sentence encoder, document encoder and de-
coder. At first, a sentence encoder will be utilized
to convert each sentence di into a sentential repre-
sentation di. Then these sentence representations
will be contextualized by a document encoder to
si. Finally, a decoder will extract a subset of sen-
tences based on these contextualized sentence rep-
resentations.

3.2 Setup for Training Environment

The objective of this step is to provide typical and
diverse testing cases (models) in terms of model
architectures, transferable knowledge and learning

schemas.

3.2.1 Sentence Encoder
We instantiate our sentence encoder with CNN
layer (Kim, 2014). We don’t explore other options
as sentence encoder since strong evidence of pre-
vious work (Kedzie et al., 2018) shows that the dif-
ferences of existing sentence encoder don’t matter
too much for final performance.

3.2.2 Document Encoder
Given a sequence of sentential representation
d1, · · · ,dn, the duty of document encoder is to
contextualize each sentence therefore obtaining
the contextualized representations s1, · · · , sn. To
achieve this goal, we investigate the LSTM-based
structure and the Transformer structure, both of
which have proven to be effective and achieved the
state-of-the-art results in many other NLP tasks.
Notably, to let the model make the best of its struc-
tural bias, stacking deep layers is allowed.

LSTM Layer Long short-term memory net-
work (LSTM) was proposed by (Hochreiter and
Schmidhuber, 1997) to specifically address this is-
sue of learning long-term dependencies, which has
proven to be effective in a wide range of NLP
tasks, such as text classification (Liu et al., 2017,
2016b), semantic matching (Rocktäschel et al.,
2015; Liu et al., 2016a), text summarization (Rush
et al., 2015) and machine translation (Sutskever
et al., 2014).

Transformer Layer Transformer (Vaswani
et al., 2017) is essentially a feed-forward self-
attention architecture, which achieves pairwise
interaction by attention mechanism. Recently,
Transformer has achieved great success in many
other NLP tasks (Vaswani et al., 2017; Dai et al.,
2018), and it is appealing to know how this neural
module performs on text summarization task.

3.2.3 Decoder
Decoder is used to extract a subset of sentences
from the original document based on contextu-
alized representations: s1, · · · , sn. Most exist-
ing architecture of decoders can divide into auto-
regressive and non auto-regressive versions, both
of which are investigated in this paper.

Sequence Labeling (SeqLab) The models,
which formulate extractive summarization task as
a sequence labeling problem, are equipped with
non auto-regressive decoder. Formally, given a



1052

documentD consisting of n sentences d1, · · · , dn,
the summaries are extracted by predicting a se-
quence of label y1, · · · , yn (yi ∈ {0, 1}) for the
document, where yi = 1 represents the i-th sen-
tence in the document should be included in the
summaries.

Pointer Network (Pointer) As a representative
of auto-regressive decoder, pointer network-based
decoder has shown superior performance for ex-
tractive summarization (Chen and Bansal, 2018;
Jadhav and Rajan, 2018). Pointer network se-
lects the sentence by attention mechanism using
glimpse operation (Vinyals et al., 2015). When it
extracts a sentence, pointer network is aware of
previous predictions.

3.2.4 External transferable knowledge

The success of neural network-based models on
NLP tasks cannot only be attributed to the shift
from feature engineering to structural engineer-
ing, but the flexible ways to incorporate external
knowledge (Mikolov et al., 2013; Peters et al.,
2018a; Devlin et al., 2018). The most common
form of external transferable knowledge is the pa-
rameters pre-trained on other corpora.

To investigate how different pre-trained models
influence the summarization system, we take the
following pre-trained knowledge into considera-
tion.

Unsupervised transferable knowledge Two
typical unsupervised transferable knowledge are
explored in this paper: context independent word
embeddings (Mikolov et al., 2013; Pennington
et al., 2014) and contextualized word embeddings
(Peters et al., 2018a; Devlin et al., 2018), have put
the state-of-the-art results to new level on a large
number of NLP taks recently.

Supervised pre-trained knowledge Besides
unsupervised pre-trained knowledge, we also can
utilize parameters of networks pre-trained on other
summarization datasets. The value of this inves-
tigation is to know transferability between differ-
ent dataset. To achieve this, we first pre-train our
model on the NEWSROOM dataset (Grusky et al.,
2018), which is one of the largest datasets and con-
tains samples from different domains. Then, we
fine-tune our model on target domains that we in-
vestigate.

3.2.5 Learning Schemas
Utilizing external knowledge provides a way to
seek new state-of-the-art results from the perspec-
tive of introducing extra data. Additionally, an al-
ternative way is resorting to change the learning
schema of the model. In this paper, we also ex-
plore how different learning schemas influence ex-
tractive summarization system by comparing su-
pervised learning and reinforcement learning.

3.3 Setup for Testing Environment

In the testing environment, we provide sufficient
evaluation scenarios to get the internal working
mechanism of testing models. Next, we will make
a detailed deception.

ROUGE Following previous work in text sum-
marization, we evaluate the performance of dif-
ferent architectures with the standard ROUGE-1,
ROUGE-2 and ROUGE-L F1 scores (Lin, 2004)
by using pyrouge package3.

Cross-domain Evaluation We present a multi-
domain evaluation, in which each testing model
will be evaluated on multi-domain datasets based
on CNN/DailyMail and NEWSROOM. Detail of
the multi-domain datasets is descried in Tab. 2.

Repetition We design repetition score to test
how different architectures behave diversely on
avoiding generating unnecessary lengthy and re-
peated information. We use the percentage of re-
peated n-grams in extracted summary to measure
the word-level repetition, which can be calculated
as:

REPn =
CountUniq(ngram)

Count(ngram)
(1)

where Count is used to count the number of n-
grams and Uniq is used to eliminate n-gram dupli-
cation. The closer the word-based repetition score
is to 1, the lower the repeatability of the words in
summary.

Positional Bias It is meaningful to study
whether the ground truth distribution of the
datasets is different and how it affects different ar-
chitectures. To achieve this we design a positional
bias to describe the uniformity of ground truth dis-
tribution in different datasets, which can be calcu-

3pypi.python.org/pypi/pyrouge/0.1.3

pypi.python.org/pypi/pyrouge/0.1.3


1053

lated as:

PosBias =
k∑

i=1

−p(i) log(p(i)) (2)

We divide each article into k parts (we choose
k = 30 because articles from CNN/DailyMail and
NEWSROOM have 30 sentences by average) and
p(i) denotes the probability that the first golden
label is in part i of the articles.

Sentence Length Sentence length will affect
different metrics to some extent. We count the av-
erage length of the k-th sentence extracted from
different decoders to explore whether the decoder
could perceive the length information of sen-
tences.

Sentence Shuffling We attempt to explore the
impact of sentence position information on differ-
ent structures. Therefore, we shuffle the orders of
sentences and observe the robustness of different
architectures to out-of-order sentences.

4 Experiment

4.1 Datasets

Instead of evaluating model solely on a single
dataset, we care more about how our testing mod-
els perform on different types of data, which al-
lows us to know if current models suffer from the
over-engineering problem.

Domains Train Valid Test

CNN/DailyMail 287,227 13,368 11,490
NYTimes 152,981 16,490 16,624
WashingtonPost 96,775 10,103 10,196
FoxNews 78,795 8,428 8,397
TheGuardian 58,057 6,376 6,273
NYDailyNews 55,653 6,057 5,904
WSJ 49,968 5,449 5,462
USAToday 44,921 4,628 4,781

Table 2: Statistics of multi-domain datasets based on
CNN/DailyMail and NEWSROOM.

CNN/DailyMail The CNN/DailyMail question
answering dataset (Hermann et al., 2015) modi-
fied by (Nallapati et al., 2016) is commonly used
for summarization. The dataset consists of on-
line news articles with paired human-generated
summaries (3.75 sentences on average). For the

data prepossessing, we use the data with non-
anonymized version as (See et al., 2017), which
doesn’t replace named entities.

NEWSROOM Recently, NEWSROOM is con-
structed by (Grusky et al., 2018), which contains
1.3 million articles and summaries extracted from
38 major news publications across 20 years. We
regard this diversity of sources as a diversity of
summarization styles and select seven publica-
tions with the largest number of data as different
domains to do the cross-domain evaluation. Due
to the large scale data in NEWSROOM, we also
choose this dataset to do transfer experiment.

4.2 Training Settings

For different learning schemas, we utilize cross
entropy loss function and reinforcement learning
method close to Chen and Bansal (2018) with a
small difference: we use the precision of ROUGE-
1 as a reward for every extracted sentence instead
of the F1 value of ROUGE-L.

For context-independent word representations
(GloVe, Word2vec), we directly utilize them to
initialize our words of each sentence, which can
be fine-tuned during the training phase.

For BERT, we truncate the article to 512 to-
kens and feed it to a feature-based BERT (without
gradient), concatenate the last four layers and get
a 128-dimensional token embedding after passing
through a MLP.

4.3 Experimental Observations and Analysis

Next, we will show our findings and analyses in
terms of architectures and external transferable
knowledge.

4.3.1 Analysis of Decoders
We understand the differences between decoder
Pointer and SeqLab by probing their behaviours
in different testing environments.

Domains From Tab. 3, we can observe that
models with pointer-based decoder are prone to
achieving better performance against SeqLab-
based decoder. Specifically, among these eight
datasets, models with pointer-based decoder out-
perform SeqLab on six domains and achieves
comparable results on the other two domains. For
example, in “NYTimes”, “WashingtonPost”
and “TheGuardian” domains, Pointer sur-
passes SeqLab by at least 1.0 improvment (R-1).



1054

Model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L

Dec. Enc. CNN/DM (2/3) NYTimes (2) WashingtonPost (1) Foxnews (1)

Lead 40.11 17.64 36.32 28.75 16.10 25.16 22.21 11.40 19.41 54.20 46.60 51.89
Oracle 55.24 31.14 50.96 52.17 36.10 47.68 42.91 27.11 39.42 73.54 65.50 71.46

SeqLab LSTM 41.22 18.72 37.52 30.26 17.18 26.58 21.27 10.78 18.56 59.32 51.82 56.95Transformer 41.31 18.85 37.63 30.03 17.01 26.37 21.74 10.92 18.92 59.35 51.82 56.97

Pointer LSTM 41.56 18.77 37.83 31.31 17.28 27.23 24.16 11.84 20.67 59.53 51.89 57.08Transformer 41.36 18.59 37.67 31.34 17.25 27.16 23.77 11.63 20.48 59.35 51.68 56.90

Dec. Enc. TheGuardian (1) NYDailyNews (1) WSJ (1) USAToday (1)

Lead 22.51 7.69 17.78 45.26 35.53 42.70 39.63 27.72 36.10 29.44 18.92 26.65
Oracle 41.08 21.49 35.80 73.99 64.80 72.09 57.15 43.06 53.27 47.17 33.40 44.02

SeqLab LSTM 23.02 8.12 18.29 53.13 43.52 50.53 41.94 29.54 38.19 30.30 18.96 27.40Transformer 23.49 8.43 18.65 53.66 44.19 51.07 42.98 30.22 39.02 30.97 19.77 28.03

Pointer LSTM 24.71 8.55 19.30 53.31 43.37 50.52 43.29 30.20 39.12 31.73 19.89 28.50Transformer 24.86 8.66 19.45 54.30 44.70 51.67 43.30 30.17 39.07 31.95 20.11 28.78

Table 3: Results of different architectures over different domains, where Enc. and Dec. represent document en-
coder and decoder respectively. Lead means to extract the first k sentences as the summary, usually as a competitive
lower bound. Oracle represents the ground truth extracted by the greedy algorithm (Nallapati et al., 2017), usually
as the upper bound. The number k in parentheses denotes k sentences are extracted during testing and choose
lead-k as a lower bound for this domain. All the experiments use word2vec to obtain word representations.

We attempt to explain this difference from the fol-
lowing three perspectives.

Repetition For domains that need to extract
multiple sentences as the summary (first two do-
mains in Tab. 3), Pointer is aware of the previ-
ous prediction which makes it to reduce the du-
plication of n-grams compared to SeqLab. As
shown in Fig. 1(a), models with Pointer always
get higher repetition scores than models with Se-
qLab when extracting six sentences, which indi-
cates that Pointer does capture word-level infor-
mation from previous selected sentences and has
positive effects on subsequent decisions.

Positional Bias For domains that only need to
extract one sentence as the summary (last six do-
mains in Tab. 3), Pointer still performs better
than SeqLab. As shown in Fig. 1(b), the perfor-
mance gap between these two decoders grows as
the positional bias of different datasets increases.
For example, from the Tab. 3, we can see in the
domains with low-value positional bias, such as
“FoxNews(1.8)”, “NYDailyNews(1.9)”,
SeqLab achieves closed performance against
Pointer. By contrast, the performance gap
grows when processing these domains with high-
value positional bias (“TheGuardian(2.9)”,
“WashingtonPost(3.0)”). Consequently,
SeqLab is more sensitive to positional bias, which
impairs its performance on some datasets.

Sentence length We find Pointer shows the abil-
ity to capture sentence length information based
on previous predictions, while SeqLab doesn’t.
We can see from the Fig. 1(c) that models with
Pointer tend to choose longer sentences as the first
sentence and greatly reduce the length of the sen-
tence in the subsequent extractions. In compar-
ison, it seems that models with SeqLab tend to
extract sentences with similar length. The ability
allows Pointer to adaptively change the length of
the extracted sentences, thereby achieving better
performance regardless of whether one sentence
or multiple sentences are required.

4.3.2 Analysis of Encoders
In this section, we make the analysis of two en-
coders LSTM and Transformer in different testing
environments.

Domains From Tab. 3, we get the following ob-
servations:

1) Transformer can outperform LSTM on some
datasets “NYDailyNews” by a relatively large
margin while LSTM beats Transformer on some
domains with closed improvements. Besides, dur-
ing different training phases of these eight do-
mains, the hyper-parameters of Transformer keep
unchanged4 while for LSTM, many sets of hyper-
parameters are used5.

44 layers 512 dimensions for Pointer and 12 layers 512
dimensions for SeqLab

5the number of layers searches in (2, 4, 6, 8) and dimen-



1055

REP2 REP3 REP4 REP5

0.9

0.95

Sc
or

e

SLSTM STransformer PLSTM PTransformer

(a) Repetition score

1.8 1.9 2.3 2.6 2.9 3.0

0

0.5

1

1.5

2

2.5

Positional Bias

∆
R

R-1
R-2
R-L

(b) Positional bias

1 2 3 4

20

25

30

35

#Sent

Av
g.

Le
ng

th

SLSTM
STransformer

PLSTM
PTransformer

(c) Average length

Figure 1: Different behaviours of two decoders (SeqLab and Pointer) under different testing environment. (a)
shows repetition scores of different architectures when extracting six sentences on CNN/DailyMail. (b) shows the
relationship between ∆R and positional bias. The abscissa denotes the positional bias of six different datasets and
∆R denotes the average ROUGE difference between the two decoders under different encoders. (c) shows average
length of k-th sentence extracted from different architectures.

R-1 R-2 R-L

10

15

20

∆
R

(%
)

LSTM Transformer

Figure 2: Results of different document encoders with
Pointer on normal and shuffled CNN/DailyMail. ∆R
denotes the decrease of performance when the sen-
tences in document are shuffled.

Above phenomena suggest that LSTM easily
suffers from the architecture overfitting problem
compared with Transformer. Additionally, in our
experimental setting, Transformer is more effi-
cient to train since it is two or three times faster
than LSTM.

2) When equipped with SeqLab decoder, Trans-
former always obtains a better performance com-
pared with LSTM, the reason we think is due to the
non-local bias (Wang et al., 2018) of Transformer.

Shuffled Testing In this settings, we shuffle the
orders of sentences in training set while test set
keeps unchanged. We compare two models with
different encoders (LSTM, Transformer) and the
results can be seen in Fig. 2. Generally, there is
significant drop of performance about these two
models. However, Transformer obtains lower de-
crease against LSTM, suggesting that Transformer

sion searches in (512, 1024, 2048)

α β R-1 R-2 R-L

1 0 37.90 15.69 34.31√
d 1 40.93 18.49 37.24

1 1 41.31 18.85 37.63
1

√
d 40.88 18.42 37.19

0 1 40.39 17.67 36.54

Nallapati et al. (2017) 39.6 16.2 35.3
Narayan et al. (2018) 40.2 18.2 36.6

Table 4: Results of Transformer with SeqLab using
different proportions of sentence embedding and po-
sitional embedding on CNN/DailyMail. The input of
Transformer is α ∗ sentence embedding plus β ∗ posi-
tional embedding6. The bottom half of the table con-
tains models that have similar performance with Trans-
former that only know positional information.

are more robust.

Disentangling Testing Transformer provides us
an effective way to disentangle position and con-
tent information, which enables us to design a spe-
cific experiment, investigating what role positional
information plays.

As shown in Tab. 4, we dynamically regulate
the ratio between sentence embedding and posi-
tional embedding by two coefficients α and β.

Surprisingly, we find even only utilizing po-
sitional embedding (the model is only told
how many sentences the document contains),
our model can achieve 40.08 on R-1, which
is comparable to many existing models. By

6In Vaswani et al. (2017), the input of Transformer is
√
d

∗ word embedding plus positional embedding, so we design
the above different proportions to carry out the disentangling
test.



1056

Model R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L

Dec. Enc. Baseline + GloVe + BERT + NEWSROOM

SeqLab
LSTM 41.22 18.72 37.52 41.33 18.78 37.64 42.18 19.64 38.53 41.48 18.95 37.78
Transformer 41.31 18.85 37.63 40.19 18.67 37.51 42.28 19.73 38.59 41.32 18.83 37.63

Pointer
LSTM 41.56 18.77 37.83 41.15 18.38 37.43 42.39 19.51 38.69 41.35 18.59 37.61
Transformer 41.36 18.59 37.67 41.10 18.38 37.41 42.09 19.31 38.41 41.54 18.73 37.83

Table 5: Results of different architectures with different pre-trained knowledge on CNN/DailyMail, where Enc.
and Dec. represent document encoder and decoder respectively.

contrast, once the positional information is re-
moved, the performance dropped by a large mar-
gin. This experiment shows that the success of
such extractive summarization heavily relies on
the ability of learning the positional information
on CNN/DailyMail, which has been a benchmark
dataset for most of current work.

4.3.3 Analysis of Transferable Knowledge
Next, we show how different types of transferable
knowledge influences our summarization models.

Unsupervised Pre-training Here, as a base-
line, word2vec is used to obtain word repre-
sentations solely based on the training set of
CNN/DailyMail.

As shown in Tab. 5, we can find that context-
independent word representations can not con-
tribute much to current models. However, when
the models are equipped with BERT, we are ex-
cited to observe that the performances of all types
of architectures are improved by a large margin.
Specifically, the model CNN-LSTM-Pointer
has achieved a new state-of-the-art with 42.11 on
R-1, surpassing existing models dramatically.

Supervised Pre-training In most cases, our
models can benefit from the pre-trained parame-
ters learned from the NEWSROOM dataset. How-
ever, the model CNN-LSTM-Pointer fails and
the performance are decreased. We understand
this phenomenon by the following explanations:
The transferring process from CNN/DailyMail to
NEWSROOM suffers from the domain shift prob-
lem, in which the distribution of golden labels’ po-
sitions are changed. And the observation from Fig.
2 shows that CNN-LSTM-Pointer is more sen-
sitive to the ordering change, therefore obtaining a
lower performance.

Why does BERT work? We investigate two dif-
ferent ways of using BERT to figure out from

Models R-1 R-2 R-L

Chen and Bansal (2018) 41.47 18.72 37.76
Dong et al. (2018) 41.50 18.70 37.60
Zhou et al. (2018) 41.59 19.01 37.98
Jadhav and Rajan (2018)7 41.60 18.30 37.70

LSTM + PN 41.56 18.77 37.83
LSTM + PN + RL 41.85 18.93 38.13
LSTM + PN + BERT 42.39 19.51 38.69
LSTM + PN + BERT + RL 42.69 19.60 38.85

Table 6: Evaluation on CNN/DailyMail. The top half
of the table is currently state-of-the-art models, and the
lower half is our models.

where BERT has brought improvement for extrac-
tive summarization system.

In the first usage, we feed each individual sen-
tence to BERT to obtain sentence representation,
which does not contain contextualized informa-
tion, and the model gets a high R-1 score of 41.7.
However, when we feed the entire article to BERT
to obtain token representations and get the sen-
tence representation through mean pooling, model
performance soared to 42.3 R-1 score.

The experiment indicates that though BERT
can provide a powerful sentence embedding, the
key factor for extractive summarization is con-
textualized information and this type of informa-
tion bears the positional relationship between sen-
tences, which has been proven to be critical to ex-
tractive summarization task as above.

4.4 Learning Schema and Complementarity

Besides supervised learning, in text summariza-
tion, reinforcement learning has been recently
used to introduce more constraints. In this paper,
we also explore if several advanced techniques be
complementary with each other.

We first choose the based model
7trained and evaluated on the anonymized version.



1057

LSTM-Pointer and LSTM-Pointer +
BERT, then the reinforcement learning are intro-
duced aiming to further optimize our models. As
shown in Tab. 6, we observe that even though
the performance of LSTM+PN has been largely
improved by BERT, when applying reinforce-
ment learning, the performance can be improved
further, which indicates that there is indeed a com-
plementarity between architecture, transferable
knowledge and reinforcement learning.

5 Conclusion

In this paper, we seek to better understand how
neural extractive summarization systems could
benefit from different types of model archi-
tectures, transferable knowledge, and learning
schemas. Our detailed observations can provide
more hints for the follow-up researchers to design
more powerful learning frameworks.

Acknowledgment

We thank Jackie Chi Kit Cheung, Peng Qian for
useful comments and discussions. We would
like to thank the anonymous reviewers for their
valuable comments. The research work is sup-
ported by National Natural Science Foundation
of China (No. 61751201 and 61672162), Shang-
hai Municipal Science and Technology Commis-
sion (16JC1420401 and 17JC1404100), Shang-
hai Municipal Science and Technology Major
Project(No.2018SHZDZX01)and ZJLab.

References
Kristjan Arumae and Fei Liu. 2018. Reinforced ex-

tractive summarization with question-focused re-
wards. In Proceedings of ACL 2018, Student Re-
search Workshop. pages 105–111.

Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers). volume 1, pages 1662–1675.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers). volume 1, pages 675–686.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.

In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers). volume 1, pages 484–494.

Zihang Dai, Zhilin Yang, Yiming Yang, William W
Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. 2018. Transformer-xl: Language
modeling with longer-term dependency .

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .

Yue Dong, Yikang Shen, Eric Crawford, Herke van
Hoof, and Jackie Chi Kit Cheung. 2018. Bandit-
sum: Extractive summarization as a contextual ban-
dit. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing.
pages 3739–3748.

Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing. pages
4098–4109.

Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Newsroom: A dataset of 1.3 million summaries with
diverse extractive strategies. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers). volume 1, pages 708–719.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka
Matsuo, and Ichiro Sakata. 2017. Extractive sum-
marization using multi-task learning with document
classification. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing. pages 2101–2110.

Aishwarya Jadhav and Vaibhav Rajan. 2018. Extrac-
tive summarization with swap-net: Sentences and
words from alternating pointer networks. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). volume 1, pages 142–151.

Chris Kedzie, Kathleen McKeown, and Hal Daume III.
2018. Content selection in deep learning models of
summarization. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing. pages 1818–1828.



1058

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out .

Pengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion lstms for text seman-
tic matching. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). volume 1, pages
1034–1043.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016b.
Recurrent neural network for text classification with
multi-task learning. In Proceedings of IJCAI. pages
2873–2879.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). volume 1, pages 1–10.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. In Thirty-First AAAI Conference on Artificial
Intelligence.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Ça glar Gulçehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond. CoNLL 2016 page 280.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summariza-
tion with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers). volume 1, pages 1747–1759.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304 .

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP). pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language

Technologies, Volume 1 (Long Papers). volume 1,
pages 2227–2237.

Matthew Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b. Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1499–1509.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
arXiv preprint arXiv:1509.06664 .

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. pages 379–389.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). volume 1,
pages 1073–1083.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems. pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems. pages 5998–6008.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
2015. Order matters: Sequence to sequence for sets.
arXiv preprint arXiv:1511.06391 .

Xiaolong Wang, Ross Girshick, Abhinav Gupta, and
Kaiming He. 2018. Non-local neural networks. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pages 7794–7803.

Yuxiang Wu and Baotian Hu. 2018. Learning to extract
coherent summary via deep reinforcement learning.
In Thirty-Second AAAI Conference on Artificial In-
telligence.

Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao. 2018. Neural docu-
ment summarization by jointly learning to score and
select sentences. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). volume 1, pages
654–663.


