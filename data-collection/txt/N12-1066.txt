








































Behavioral Factors in Interactive Training of Text Classifiers

Burr Settles
Machine Learning Department

Carnegie Mellon University
Pittsburgh PA 15213, USA
bsettles@cs.cmu.edu

Xiaojin Zhu
Computer Sciences Department

University of Wisconsin
Madison WI 53715, USA

jerryzhu@cs.wisc.edu

Abstract

This paper describes a user study where hu-
mans interactively train automatic text clas-
sifiers. We attempt to replicate previous re-
sults using multiple “average” Internet users
instead of a few domain experts as annotators.
We also analyze user annotation behaviors to
find that certain labeling actions have an im-
pact on classifier accuracy, drawing attention
to the important role these behavioral factors
play in interactive learning systems.

1 Introduction

There is growing interest in methods that incorpo-
rate human domain knowledge in machine learning
algorithms, either as priors on model parameters or
as constraints in an objective function. Such ap-
proaches lend themselves well to natural language
tasks, where input features are often discrete vari-
ables that carry semantic meaning (e.g., words). A
feature label is a simple but expressive form of do-
main knowledge that has received considerable at-
tention recently (Druck et al., 2008; Melville et al.,
2009). For example, a single feature (word) can be
used to indicate a particular label or set of labels,
such as “excellent”⇒ positive or “terrible”⇒ neg-
ative, which might be useful word-label rules for a
sentiment analysis task.

Contemporary work has also focused on mak-
ing such learning algorithms active, by enabling
them to pose “queries” in the form of feature-based
rules to be labeled by annotators in addition to —
and sometimes lieu of — data instances such as
documents (Attenberg et al., 2010; Druck et al.,

2009). These concepts were recently implemented
in a practical system for interactive training of text
classifiers called DUALIST1. Settles (2011) reports
that, in user experiments with real annotators, hu-
mans were able to train near state of the art classi-
fiers with only a few minutes of effort. However,
there were only five subjects, who were all com-
puter science researchers. It is possible that these
positive results can be attributed to the subjects’ im-
plicit familiarity with machine learning and natural
language processing algorithms.

This short paper sheds more light on previous ex-
periments by replicating them with many more hu-
man subjects, and of a different type: non-experts
recruited through the Amazon Mechanical Turk ser-
vice2. We also analyze the impact of annotator be-
havior on the resulting classifiers, and suggest rela-
tionships to recent work in curriculum learning.

2 DUALIST

Figure 1 shows a screenshot of DUALIST, an inter-
active machine learning system for quickly build-
ing text classifiers. The annotator is allowed to
take three kinds of actions: À label query docu-
ments (instances) by clicking class-label buttons in
the left panel, Á label query words (features) by
selecting them from the class-label columns in the
right panel, or Â “volunteer” domain knowledge by
typing labeled words into a text box at the top of
each class column. The underlying classifier is a
naı̈ve Bayes variant combining informative priors,

1http://code.google.com/p/dualist/
2http://mturk.com



1

2

3

Figure 1: The DUALIST user interface.

maximum likelihood estimation, and the EM algo-
rithm for fast semi-supervised training. When a
user performs action À or Á, she labels queries that
should help minimize the classifier’s uncertainty on
unlabeled documents (according to active learning
heuristics). For action Â, the user is free to volun-
teer any relevant word, whether or not it appears in
a document or word column. For example, the user
might volunteer the labeled word “oscar” ⇒ posi-
tive in a sentiment analysis task for movie reviews
(leveraging her knowledge of domain), even if the
word “oscar” does not appear anywhere in the in-
terface. This flexibility goes beyond traditional ac-
tive learning, which restricts the user to feedback on
items queried by the learner (i.e., actions À and Á).
After a few labeling actions, the user submits her
feedback and receives the next set of queries in real
time. For more details, see Settles (2011).

3 Experimental Setup

We recruited annotators through the crowdsourcing
marketplace Mechanical Turk. Subjects were shown
a tutorial page with a brief description of the clas-
sification task, as well as a cartoon of the interface
similar to Figure 1 explaining the various annotation
options. When they decided they were ready, users
followed a link to a web server running a customized
version of DUALIST, which is an open source web-
based application. At the end of each trial, subjects
were given a confirmation code to receive payment.

We conducted experiments using two corpora
from the original DUALIST study: Science (a subset
of the 20 Newsgroups benchmark: cryptography,
electronics, medicine, and space) and Movie Re-

views (a sentiment analysis collection). These are
not specialized domains, i.e., we could expect av-
erage Internet users to be knowledgable enough to
perform the annotations. While both are generally
accessible, these corpora represent different types of
tasks and vary both in number of categories (four
vs. two) and difficulty (Movie Reviews is known to
be harder for learning algorithms). We replicated
the same experimental conditions as previous work:
DUALIST (the full interface in Figure 1), active-doc
(the left-hand À document panel only), and passive-
doc (the À document panel only, but with texts se-
lected at random and not queried by active learning).

For each condition, we recruited 25 users for the
Science corpus (75 total) and 35 users for Movie Re-
views (105 total). We were careful to publish tasks
on MTurk in a way that no one user annotated more
than one condition. Some users experienced techni-
cal difficulties that nullified their work, and four ap-
peared to be spammers3. After removing these sub-
jects from the analysis, we were left with 23 users
for the Science DUALIST condition, 25 each for the
two document-only conditions (73 total), 32 users
for the Movie Reviews DUALIST condition, and
33 each for the document-only conditions (98 total).
DUALIST automatically logged data about user ac-
tions and model accuracies as training progressed,
although users could not see these statistics. Trials
lasted 6 minutes for the Science corpus and 10 min-
utes for Movie Reviews. We did advertise a “bonus”
for the user who trained the best classifier to encour-
age correctness, but otherwise offered no guidance
on how subjects should prioritize their time.

4 Results

Figure 2(a) shows learning curves aggregated across
all users in each experimental condition. Curves are
LOESS fits to classifier accuracy over time: locally-
weighted polynomial regressions (Cleveland et al.,
1992) ±1 standard error, with the actual user data
points omitted for clarity. For the Science task (top),
DUALIST users trained significantly better classi-
fiers after about four minutes of annotation time.
Document-only active learning also outperformed

3A spammer was ruled to be one whose document error rate
(vs. the gold standard) was more than double the chance error,
and whose feature labels appeared to be arbitrary clicks.



0.20

0.30

0.40

0.50

0.60

0.70

 0  60  120  180  240  300  360

S
c
ie

n
c
e

 

DUALIST
active-doc
passive-doc

0.49

0.52

0.55

0.58

0.61

0.64

 0  120  240  360  480  600

M
o

v
ie

 R
e

v
ie

w
s

annotation time (sec)

DUALIST
active-doc
passive-doc

(a) learning curves

DUALIST active-doc passive-doc

0
.3

0
.5

0
.7

DUALIST active-doc passive-doc

0
.5
0

0
.6
0

0
.7
0

(b) final classifier accuracies

0.20

0.30

0.40

0.50

0.60

0.70

 0  60  120  180  240  300  360

S
c
ie

n
c
e

 

DV++ (5)
DV+ (9)
DV- (9)

0.49

0.52

0.55

0.58

0.61

0.64

 0  120  240  360  480  600

M
o

v
ie

 R
e

v
ie

w
s

annotation time (sec)

DV++ (8)
DV+ (13)
DV- (11)

(c) behavioral subgroup curves

Figure 2: (a) Learning curves plotting accuracy vs. actual annotation time for the three conditions. Curves are LOESS
fits (±1 SE) to all classifier accuracies at that point in time. (b) Box plots showing the distribution of final accuracies
under each condition. (c) Learning curves for three behavioral subgroups found in the DUALIST condition. The
DV++ group volunteered many labeled words (action Â), DV+ volunteered some, and DV- volunteered none.

standard passive learning, which is consistent with
previous work. However, for Movie Reviews (bot-
tom), there is little difference among the three set-
tings, and in fact models trained with DUALIST ap-
pear to lag behind active learning with documents.

Figure 2(b) shows the distribution of final classi-
fier accuracies in each condition. For Science, the
DUALIST users are significantly better than either
of the baselines (two-sided KS test, p < 0.005).
While the differences for Movie Reviews are not
significantly different, we can see that the top quar-
tile does much better than the two baselines. Clearly
some DUALIST users are making better use of the
interface and training better classifiers. How?

It is important to note that users in the active-
doc and passive-doc conditions can only choose ac-
tion À (label documents), whereas those in the DU-
ALIST condition must allocate their time among
three kinds of actions. It turns out that the anno-
tators exhibited very non-uniform behavior in this
respect. In particular, activity of action Â (volunteer
labeled words) follows a power law, and many sub-
jects volunteered no features at all. By inspecting
the distribution of these actions for natural break-
points, we identified three subgroups of DUALIST
users: DV++ (many volunteered words), DV+ (some
words), and DV- (none; labeled queries only). Note

Movie Reviews Science
Group # Words Users # Words Users

DV++ 21–62 8 24–42 5
DV+ 1–15 13 2–19 9
DV- 0 11 0 9

Table 1: The range of volunteered words and number of
users in each behavioral subgroup of DUALIST subjects.

that DV- is not functionally equivalent to the active-
doc condition, as users in the DV- group could still
view and label word queries. The three behavioral
subgroups are summarized in Table 1.

Figure 2(c) shows learning curves for these three
groups. We can see that the DV++ and DV+ groups
ultimately train better classifiers than the DV- group,
and DV++ also dominates both the active and pas-
sive baselines from Figure 2(a). The DV++ group is
particularly effective on the Movie Reviews corpus.
This suggests that a user’s choice to volunteer more
labeled features — by occasionally side-stepping the
queries posed by the active learner and directly in-
jecting their domain knowledge — is a good predic-
tor of classifier accuracy on this task.

To tease apart the relative impact of other behav-
iors, we conducted an ordinary least-squares regres-
sion to predict classifier accuracy at the end of a trial.
We included the number of user events for each ac-



tion as independent variables, plus two controls: the
subject’s document error rate in [0,1] with respect to
the gold standard, and class entropy in [0, logC] of
all labeled words (whereC is the number of classes).
The entropy variable is meant to capture how “bal-
anced” a user’s word-labeling activity was for ac-
tions Á and Â, with the intuition that a skewed set of
words could confuse the learner, by biasing it away
from categories with fewer labeled words.

Table 2 summarizes these results. Surprisingly,
query-labeling actions (À and Á) have a relatively
small impact on accuracy. The number of volun-
teered words and entropy among word labels appear
to be the only two factors that are somewhat signif-
icant: the former is strongest in the Movie Reviews
corpus, the latter in Science4. Interestingly, there is a
strong positive correlation between these two factors
in the Movie Reviews corpus (Spearman’s ρ = 0.51,
p = 0.02) but not in Science (ρ = 0.03). When we
consider change in word label entropy over time, the
Science DV++ group is balanced early on and be-
comes steadily more so on average , whereas
DV+ goes for several minutes before catching up
(and briefly overtaking) . This may account
for DV+’s early dip in accuracy in Figure 2(c). For
Movie Reviews, DV++ is more balanced than DV+
throughout the trial. DV++ labeled many words that
were also class-balanced, which may explain why
it is the best consistently-performing group. As is
common in behavior modeling with small samples,
the data are noisy and the regressions in Table 2 only
explain 33%–46% of the variance in accuracy.

5 Discussion

We were able to partially replicate the results from
Settles (2011). That is, for two of the same data sets,
some of the subjects using DUALIST significantly
outperformed those using traditional document-only
interfaces. However, our results show that the
gains come not merely from the interface itself, but
from which labeling actions the users chose to per-
form. As interactive learning systems continue to
expand the palette of interactive options (e.g., la-

4Science has four labels and a larger entropy range, which
might explain the importance of the entropy factor here. Also,
labels are more related to natural clusterings in this corpus
(Nigam et al., 2000), so class-balanced priors might be key for
DUALIST’s semi-supervised EM procedure to work well.

Movie Reviews Science
Action β SE β SE

(intercept) 0.505 0.038 *** 0.473 0.147 **
À label query docs 0.001 0.001 0.005 0.005
Á label query words -0.001 0.001 0.000 0.001
Â volunteer words 0.002 0.001 * 0.000 0.002
human error rate -0.036 0.109 -0.328 0.230
word label entropy 0.053 0.051 0.201 0.102 .

R2 = 0.4608 ** R2 = 0.3342
*** p < 0.001 ** p < 0.01 * p < 0.05 . p < 0.1

Table 2: Linear regressions estimating the accuracy of a
classifier as a function of annotator actions and behaviors.

beling and/or volunteering features), understanding
how these options impact learning becomes more
important. In particular, training a good classifier
in our experiments appears to be linked to (1) vol-
unteering more labeled words, and (2) maintaining
a class balance among them. Users who exhibited
both of these behaviors — which are possibly arti-
facts of their good intuitions — performed the best.

We posit that there is a conceptual connection be-
tween these insights and curriculum learning (Ben-
gio et al., 2009), the commonsense notion that learn-
ers perform better if they begin with clear and unam-
biguous examples before graduating to more com-
plex training data. A recent study found that some
humans use a curriculum strategy when teaching a
1D classification task to a robot (Khan et al., 2011).
About half of those subjects alternated between ex-
treme positive and negative instances in a relatively
class-balanced way. This behavior was explained by
showing that it is optimal under an assumption that,
in reality, the learning task has many input features
for which only one is relevant to the task.

Text classification exhibits similar properties:
there are many features (words), of which only a few
are relevant. We argue that labeling features can be
seen as a kind of training by curriculum. By volun-
teering labeled words in a class-balanced way (espe-
cially early on), a user provides clear, unambiguous
training signals that effectively perform feature se-
lection while biasing the classifier toward the user’s
hypothesis. Future research on mixed-initiative user
interfaces might try to detect and encourage these
kinds of annotator behaviors, which can potentially
improve interactive machine learning outcomes.



Acknowledgments

This work was funded in part by DARPA, the
National Science Foundation (under grants IIS-
0953219 and IIS-0968487), and Google.

References
J. Attenberg, P. Melville, and F. Provost. 2010. A uni-

fied approach to active dual supervision for labeling
features and examples. In Proceedings of the Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 40–55. Springer.

Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 119–126. Omnipress.

W.S. Cleveland, E. Grosse, and W.M. Shyu. 1992. Lo-
cal regression models. In J.M. Chambers and T.J.
Hastie, editors, Statistical Models in S. Wadsworth &
Brooks/Cole.

G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595–602. ACM.

G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81–90. ACL.

F. Khan, X. Zhu, and B. Mutlu. 2011. How do humans
teach: On curriculum learning and teaching dimen-
sion. In Advances in Neural Information Processing
Systems (NIPS), volume 24, pages 1449–1457. Mor-
gan Kaufmann.

P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275–1284. ACM.

K. Nigam, A.K. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using em. Machine Learning, 39:103–134.

B. Settles. 2011. Closing the loop: Fast, interactive
semi-supervised annotation with queries on features
and instances. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1467–1478. ACL.


