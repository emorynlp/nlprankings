










































Reproducible Results in Parsing-Based Machine Translation: The JHU Shared Task Submission


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 177–182,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Reproducible Results in Parsing-Based Machine Translation:
The JHU Shared Task Submission

Lane Schwartz ∗
University of Minnesota

Minneapolis, MN
lane@cs.umn.edu

Abstract

We present the Johns Hopkins Univer-
sity submission to the 2010 WMT shared
translation task. We describe processing
steps using open data and open source
software used in our submission, and pro-
vide the scripts and configurations re-
quired to train, tune, and test our machine
translation system.

1 Introduction

Research investigating natural language process-
ing and computational linguistics can and should
have an extremely low barrier to entry. The data
with which we work is customarily available in
common electronic formats. The computational
techniques which we apply can typically be per-
formed on commodity computing resources which
are widely available. In short, there should be no
reason why small research groups and even lone
researchers should not be able to join and make
substantive contributions furthering our field. The
reality is less encouraging.

Many published articles describe novel tech-
niques and provide interesting results, yet fail to
describe technical details in sufficient detail to al-
low their results to be reproduced by other re-
searchers. While there are notable and laudable
exceptions, many publications fail to provide the
source code and scripts necessary to reproduce re-
sults. The use of restricted data, not freely avail-
able for download by any interested researcher
only compounds these problems. Pedersen (2008)
rightly argues that the implementation details so
often ignored in publications are in fact essential
for our research to be reproducible science.

Reproducibility in machine translation is made
more challenging by the complexity of experi-
mental workflows. Results in machine translation

∗Research conducted as a visiting researcher at Johns
Hopkins University

tasks are dependent on a cascade of processing
steps and configurations. While interesting sub-
sets of these usually appear in experimental de-
scriptions, many steps (preprocessing techniques,
alignment parameters, translation rule extraction
parameters, language model parameters, list of
features used) are invariably omitted, even though
these configurations are often critical to reproduc-
ing results.

This paper describes the Johns Hopkins Univer-
sity submission to the 2010 Workshop on Statis-
tical Machine Translation shared translation task.
Links to the software, scripts, and configurations
used to run the experiments described herein are
provided. The remainder of this paper is struc-
tured as follows. Section 2 lists the major ex-
amples of publicly available open source machine
translation systems, parallel corpora, and machine
translation workflow management systems. Sec-
tion 3 describes the experimental workflow used
to run the shared task translations, with the corre-
sponding experimental design in section 4. Sec-
tion 5 presents the shared task results.

2 Related Work

The last four years have witnessed the implemen-
tation and release of numerous open source ma-
chine translation systems. The widely used Moses
system (Koehn et al., 2007) implements the stan-
dard phrase-based translation model. Parsing-
based translation models are implemented by
Joshua (Li et al., 2009), SAMT (Zollmann and
Venugopal, 2006), and cdec (Dyer et al., 2010).
Cunei (Phillips and Brown, 2009) implements
statistical example-based translation. Olteanu et
al. (2006) and Schwartz (2008) respectively pro-
vide additional open-source implementations of
phrase-based and hierarchical decoders.

The SRILM (Stolcke, 2002), IRSTLM (Fed-
erico et al., 2008), and RandLM (Talbot and Os-
borne, 2007) toolkits enable efficient training and

177



N
or

m
al

iz
e

R
un

M
E

R
T

R
un

M
B

R
on

Tr
ue

ca
se

d
Tr

an
sl

at
io

ns

U
nz

ip
D

at
a

To
ke

ni
ze

H
ie

ro

Tr
an

sl
at

e

Su
bs

am
pl

e
fo

rT
ru

ec
as

in
g

Tr
ai

n
L

M
E

xt
ra

ct
Tr

ue
ca

se
G

ra
m

m
ar

Tr
ai

n
Tr

ue
ca

se
L

M

W
M

T
Sc

ri
pt

s

SR
IL

M

R
un

M
B

R
on

Tr
an

sl
at

io
ns

E
xt

ra
ct

G
ra

m
m

ar

A
lig

n
Tr

ue
ca

se
d

D
at

a
A

lig
n

Su
bs

am
pl

e

R
es

to
re

to
Tr

ue
ca

seJ
os

hu
a

D
ow

nl
oa

d
D

at
a

R
em

ov
e

X
M

L

B
er

ke
le

y
A

lig
ne

r

Figure 1: Machine translation workflow. Square nodes in grey indicate software and scripts.
The scripts and configuration files used to implement and run this workflow are available
for download at http://sourceforge.net/projects/joshua/files/joshua/1.3/
wmt2010-experiment.tgz/download

178



querying of n-gram language models.
Freely available parallel corpora for numer-

ous European languages have also been released
in recent years. These include the Europarl
(Koehn, 2005) and JRC-Acquis (Steinberger et al.,
2006) legislative corpora, each of which includes
data for most EU language pairs. The smaller
News Commentary corpora (Callison-Burch et al.,
2007; Callison-Burch et al., 2008) provide smaller
amounts of parallel data in the news genre. The re-
cent Fr-En 109 (Callison-Burch et al., 2009) cor-
pus aggregates huge numbers of parallel French-
English sentences from the web.

Open source systems to address the complex
workflows required to run non-trivial machine
translation experiments have also been developed.
These include experiment.perl (Koehn et
al., 2010), developed as a workflow management
system at the University of Edinburgh, and Loony-
Bin (Clark et al., 2010), a general hyperworkflow
management utility from Carnegie Melon Univer-
sity.

3 Managing Experiment Workflows

Running a statistical machine translation system to
achieve state-of-the-art performance involves the
configuration and execution of numerous interde-
pendent intermediate tools. To manage task de-
pendencies and tool configuration, our shared task
workflow consists of a set of dependency scripts
written for GNU Make (Stallman et al., 2006).

Figure 1 shows a graph depicting the steps in
our experimental workflow, and the dependencies
between steps. Each node in the graph represents
a step in the workflow; each step is implemented
as a Make script that defines how to run the tools
required in that step. In each experiment, an ad-
ditional configuration script is provided for each
experimental step, defining the parameters to be
used when running that step in the current experi-
ment. Optional front-end wrapper scripts can also
be provided, allowing for a complete experiment
to be run - from downloading data and software
through truecasing translated results - by execut-
ing a single make file.

This framework is also conducive to paralleliza-
tion. Many tasks, such as preprocessing numerous
training files, are not dependent on one another.
In such cases make can be configured to exe-
cute multiple processes simultaneously on a single
multi-processor machine. In cases where sched-

uled distributed computing environments such as
the Sun Grid Engine are configured, make files can
be processed by scheduler-aware make variants
(distmake, SGE qmake, Sun Studio dmake)
which distribute outstanding tasks to available dis-
tributed machines using the relevant distributed
scheduler.

4 Experimental Configuration

Experimental workflows were configured1 and run
for six language pairs in the translation shared
task: English-French, English-German, English-
Spanish, French-English, German-English, and
Spanish-English.

In all experiments, only data freely available
for download was used. No restricted data from
the LDC or other sources was used. Table 1 lists
the parallel corpora used in training the translation
model for each experiment. The monolingual cor-
pora used in training each target language model
are listed in table 2. In all experiments, news-
test2008 was used as a development tuning corpus
during minimum error rate training; newstest2009
was used as a development test set. The shared
task data set newstest2010 was used as a final blind
test set.

All data was automatically downloaded, un-
zipped, and preprocessed prior to use. Files pro-
vided in XML format were converted to plain text
by selecting lines with <seg> tags, then removing
the beginning and end tags for each segment; this
processing was applied using GNU grep and sed.
The tokenize.perl and lowercase.perl
scripts provided for the shared task2 were applied
to all data.

Interpolated n-gram language models for the
four target languages were built using the SRI
Language Model Toolkit3, with n-gram order set
to 5. The Chen and Goodman (1998) technique
for modified Kneser-Ney discounting (Kneser and
Ney, 1995) was applied during language model
training.

Following Li et al. (2009), a subset of the avail-
able training sentences was selected via subsam-

1http://sourceforge.net/projects/joshua/files/joshua/1.3/wmt2010-
experiment.tgz/download

2http://www.statmt.org/wmt08/scripts.tgz with md5sum:
tokenize.perl 45cd1832827131013245eca76481441a
lowercase.perl a1958ab429b1e29d379063c3b9cd7062

3http://www-speech.sri.com/projects/srilm
SRILM version 1.5.7. Our experimental workflow requires
that SRILM be compiled separately, with the $SRILM envi-
ronment variable set to the install location.

179



Source Target Parallel Corpora
German English news-commentary10.de-en europarl-v5.de-en
English German news-commentary10.de-en europarl-v5.de-en
French English news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr
English French news-commentary10.fr-en europarl-v5.fr-en giga-fren.release2 undoc.2000.en-fr
Spanish English news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es
English Spanish news-commentary10.es-en europarl-v5.es-en undoc.2000.en-es

Table 1: Parallel training data used for training translation model, per language pair

Target Monolingual Corpora
English europarl-v5.en news-commentary10.en news.en.shuffled undoc.2000.en-fr.en giga-fren.release2.en
French europarl-v5.fr news-commentary10.fr news.fr.shuffled undoc.2000.en-fr.fr giga-fren.release2.fr
German europarl-v5.de news-commentary10.de news.de.shuffled
Spanish europarl-v5.es news-commentary10.es news.es.shuffled undoc.2000.en-es.es

Table 2: Monolingual training data used for training language model, per target language

pling; training sentences are selected based on the
estimated likelihood of each sentence being useful
later for translating a particular test corpus.

Given a subsampled parallel training corpus,
word alignment is performed using the Berkeley
aligner4 (Liang et al., 2006).

For each language pair, a synchronous context
free translation grammar is extracted for a particu-
lar test set, following the methods of Lopez (2008)
as implemented in (Schwartz and Callison-Burch,
2010). For the largest training sets (French-
English and English-French) the original (Lopez,
2008) implementation included with Hiero was
used to save time during training5.

Because of the use of subsampling, the ex-
tracted translation grammars are targeted for use
with a specific test set. Our experiments were be-
gun prior to the release of the blind newstest2010
shared task test set. Subsampling was performed
for the development tuning set, news-test2008,
and the development test set, newstest2009. Once
the newstest2010 test set was released, the process
of subsampling, alignment, and grammar extrac-
tion was repeated to obtain translation grammars
targeted for use with the shared task test set.

Our experiments used hierarchical phrase-based
grammars containing exactly two nonterminals -
the wildcard nonterminal X, and S, used to glue

4http://berkeleyaligner.googlecode.com/files/berkeleyaligner
unsupervised-2.1.tar.gz — Berkeley aligner version 2.1

5It is expected that using the Joshua implementation
should result in nearly identical results, albeit with somewhat
more time required to extract the grammar.

together neighboring constituents. Recent work
has shown that parsing-based machine translation
using SAMT (Zollmann and Venugopal, 2006)
grammars with rich nonterminal sets can demon-
strate substantial gains over hierarchical grammars
for certain language pairs (Baker et al., 2009).
Joshua supports such grammars; the experimental
workflow presented here could easily be extended
in future research to incorporate the use of SAMT
grammars with additional language pairs.

The Z-MERT implementation (Zaidan, 2009) of
minimum error rate training (Och, 2003) was used
for parameter tuning. Tuned grammars were used
by Joshua to translate all test sets. The Joshua de-
coder produces n-best lists of translations.

Rather than simply selecting the top candidate
from each list, we take the preferred candidate af-
ter perform minimum Bayes risk rescoring (Ku-
mar and Byrne, 2004).

Once a single translation has been extracted
for each sentence in the test set, we repeat the
procedures described above to train language and
translation models for use in translating lower-
cased results into a more human-readable true-
cased form. A truecase language model is
trained as above, but on the tokenized (but not
normalized) monolingual target language corpus.
Monotone word alignments are deterministically
created, mapping normalized lowercase training
text to the original truecase text. As in bilin-
gual translation, subsampling is performed for
the training set, and a translation grammar for
lowercase-to-truecase is extracted. No tuning is

180



performed. The Joshua decoder is used to trans-
late the lowercased target language test results into
truecase format. The detokenize.perl and
wrap-xml.perl scripts provided for the shared
task were manually applied to truecased transla-
tion results prior to final submission of results.

The code used for subsampling, grammar ex-
traction, decoding, minimum error rate training,
and minimum Bayes risk rescoring is provided
with Joshua6, with the exception of the original
(Lopez, 2008) grammar extraction implementa-
tion.

5 Experimental Results

The experiments described in sections 3 and
4 above provided truecased translations for
six language pairs in the translation shared
task: English-French, English-German, English-
Spanish, French-English, German-English, and
Spanish-English. Table 3 lists the automatic met-
ric scores for the newstest2010 test set, accord-
ing to the BLEU (Papineni et al., 2002) and TER
(Snover et al., 2006) metrics.

Source Target BLEU BLEU- TER
cased

German English 21.3 19.5 0.660
English German 15.2 14.6 0.738
French English 27.7 26.4 0.614
English French 23.8 22.8 0.681
Spanish English 29.0 27.6 0.595
English Spanish 28.1 26.5 0.596

Table 3: Automatic metric scores for the test set
newstest2010

The submitted system ranked highest among
shared task participants for the German-English
task, according to TER.

In order to provide points of comparison with
the 2009 Workshop on Statistical Machine Trans-
lation shared translation task participants, table
4 lists automatic metric scores for our systems’
translations of the newstest2009 test set, which we
used as a development test set.

6 Steps to Reproduce

The experiments in this paper can be reproduced
by running the make scripts provided in the

6http://sourceforge.net/projects/joshua/files/joshua/1.3/joshua-
1.3.tgz/download — Joshua version 1.3

Source Target BLEU
German English 18.19
English German 13.57
French English 26.41
English French 25.28
Spanish English 25.28
English Spanish 24.02

Table 4: Automatic metric scores for the develop-
ment test set newstest2009

following file: http://sourceforge.net/
projects/joshua/files/joshua/1.3/
wmt2010-experiment.tgz/download.
The README file details how to configure the
workflow for your environment. Note that SRILM
must be downloaded and compiled separately
before running the experimental steps.

Acknowledgements

This work was supported by the DARPA GALE
program (Contract No HR0011-06-2-0001).

References
Kathy Baker, Steven Bethard, Michael Bloodgood,

Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07).

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).

Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation (WMT09), March.

Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, Cambridge, MA, USA, August.

181



Jonathan Clark, Jonathan Weese, Byung Gyu Ahn,
Andreas Zollman, Qin Gao, Kenneth Heafield, and
Alon Lavie. 2010. The machine translation tool-
pack for LoonyBin: Automated management of
experimental machine translation hyperworkflows.
The Prague Bulletin of Mathematical Linguistics,
93:117–126, January.

C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese,
F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and
P. Resnik. 2010. cdec: A decoder, alignment, and
learning framework for finite-state and context-free
translation models. In Proc. ACL (Demonstration
Track), Uppsala, Sweden.

Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: An open source toolkit for
handling large scale language models. In Proc. In-
terspeech, Brisbane, Australia.

Reinhard Kneser and Hermann Ney. 1995. Improved
smoothing for mgram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech and Signal Processing.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL-2007 Demo and Poster Sessions.

Philipp Koehn, Anthony Rousseau, Ben Gottesmann,
Aurora Marsye, Frédéric Blain, and Eun-Jin Park,
2010. An Experiment Management System. Fourth
Machine Translation Marathon, Dublin, Ireland,
January.

Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.

Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
135–139, Athens, Greece, March. Association for
Computational Linguistics.

P. Liang, B. Taskar, and D. Klein. 2006. Align-
ment by agreement. In North American Association
for Computational Linguistics (NAACL), pages 104–
111.

Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. thesis, University of Maryland.

Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.

Marian Olteanu, Chris Davis, Ionut Volosen, and Dan
Moldovan. 2006. Phramer an open source statis-
tical pharse-based translator. In HLT-NAACL 2006:
Proceedings of the Workshop on Statistical Machine
Translation, New York, June.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.

Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465–470.

Aaron B. Phillips and Ralf D. Brown. 2009. Cunei ma-
chine translation platform: System description. In
3rd Workshop on Example-Based Machine Transla-
tion, Dublin, Ireland.

Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua
suix arrays and prex trees. The Prague Bulletin of
Mathematical Linguistics, 93:157–166.

Lane Schwartz. 2008. An open-source hierarchical
phrase-based translation system. In Proceedings of
the 5th Midwest Computational Linguistics Collo-
quium (MCLC’08), May.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA.

Richard M. Stallman, Roland McGrath, and Paul D.
Smith, 2006. GNU Make. Free Software Founda-
tion, Boston, MA, 0.70 edition, April.

Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-acquis: A multi-
lingual aligned parallel corpus with 20+ languages.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.

Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.

David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.

Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.

182


