



















































Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2461–2470,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Multi- and Cross-Modal Semantics Beyond Vision:
Grounding in Auditory Perception

Douwe Kiela
Computer Laboratory

University of Cambridge
douwe.kiela@cl.cam.ac.uk

Stephen Clark
Computer Laboratory

University of Cambridge
stephen.clark@cl.cam.ac.uk

Abstract

Multi-modal semantics has relied on fea-
ture norms or raw image data for per-
ceptual input. In this paper we examine
grounding semantic representations in raw
auditory data, using standard evaluations
for multi-modal semantics, including mea-
suring conceptual similarity and related-
ness. We also evaluate cross-modal map-
pings, through a zero-shot learning task
mapping between linguistic and auditory
modalities. In addition, we evaluate multi-
modal representations on an unsupervised
musical instrument clustering task. To our
knowledge, this is the first work to com-
bine linguistic and auditory information
into multi-modal representations.

1 Introduction

Although distributional models (Turney and Pan-
tel, 2010; Clark, 2015) have proved useful for a
variety of NLP tasks, the fact that the meaning of
a word is represented as a distribution over other
words implies that they suffer from the ground-
ing problem (Harnad, 1990); i.e. they do not ac-
count for the fact that human semantic knowledge
is grounded in the perceptual system (Louwerse,
2008). Motivated by human concept acquisition,
multi-modal semantics enhances linguistic repre-
sentations with extra-linguistic perceptual input.
These models outperform language-only models
on a range of tasks, including modelling semantic
similarity and relatedness, and predicting compo-
sitionality (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013; Bruni et al., 2014). Al-
though feature norms have also been used, raw
image data has become the de-facto perceptual
modality in multi-modal models.

However, if the objective is to ground seman-
tic representations in perceptual information, why

stop at image data? The meaning of violin is
surely not only grounded in its visual properties,
such as shape, color and texture, but also in its
sound, pitch and timbre. To understand how per-
ceptual input leads to conceptual representation,
we should use as many perceptual modalities as
possible. A recent preliminary study by Lopopolo
and van Miltenburg (2015) found that it is possible
to derive uni-modal semantic representations from
sound data. Here, we explore taking multi-modal
semantics beyond its current reliance on image
data and experiment with grounding semantic rep-
resentations in the auditory perceptual modality.

Multi-modal models that rely on raw image
data have typically used “bag of visual words”
(BoVW) representations (Sivic and Zisserman,
2003). We follow a similar approach for the
auditory modality and construct bag of audio
words (BoAW) representations. Following pre-
vious work in multi-modal semantics, we evalu-
ate these models on measuring conceptual simi-
larity and relatedness, and inducing cross-modal
mappings between modalities to perform zero-
shot learning. In addition, we evaluate on an
unsupervised musical instrument clustering task.
Our findings indicate that multi-modal representa-
tions enriched with auditory information perform
well on relatedness and similarity tasks, particu-
larly on words that have auditory assocations. To
our knowledge, this is the first work to combine
linguistic and auditory representations in multi-
modal semantics.

2 Related Work

Information processing in the brain can be roughly
described to occur on three levels: perceptual in-
put, conceptual representation and symbolic rea-
soning (Gazzaniga, 1995). While research in AI
has made great progress in understanding the first
and last of these, understanding the middle level is
still more of an open problem: how is it that per-

2461



ceptual input leads to conceptual representations
that can be processed and reasoned with?

A key observation is that concepts are, through
perception, grounded in physical reality and sen-
sorimotor experience (Harnad, 1990; Louwerse,
2008), and there has been a surge of recent work
on perceptually grounded semantic models that try
to account for this fact. These models learn se-
mantic representations from both textual and per-
ceptual input, using either feature norms (Silberer
and Lapata, 2012; Roller and Schulte im Walde,
2013; Hill and Korhonen, 2014) or raw image
data (Feng and Lapata, 2010; Leong and Mihal-
cea, 2011; Bruni et al., 2014) as the source of per-
ceptual information. A popular approach in the
latter case is to collect images associated with a
concept, and then lay out each image as a set of
keypoints on a dense grid, where each keypoint
is represented by a robust local feature descriptor
such as SIFT (Lowe, 2004). These local descrip-
tors are subsequently clustered into a set of “vi-
sual words” using a standard clustering algorithm
such as k-means and then quantized into vector
representations by comparing the descriptors with
the centroids. An alternative to this bag of vi-
sual words (BoVW) approach is transferring fea-
tures from convolutional neural networks (Kiela
and Bottou, 2014).

Various ways of aggregating images into visual
representations have been proposed, such as tak-
ing the mean or the elementwise maximum. Ide-
ally, one would jointly learn multi-modal repre-
sentations from parallel multi-modal data, such as
text containing images (Silberer and Lapata, 2014)
or images described with speech (Synnaeve et al.,
2014), but such data is hard to obtain, has limited
coverage and can be noisy. Hence, image repre-
sentations are often learned independently. Ag-
gregated visual representations are subsequently
combined with a traditional linguistic space to
form a multi-modal model. This mixing can be
done in a variety of ways, ranging from simple
concatenation to more sophisticated fusion meth-
ods (Bruni et al., 2014).

Cross-modal semantics, instead of being con-
cerned with improving semantic representations
through grounding, focuses on the problem of ref-
erence. Using, for instance, mappings between
visual and textual space, the objective is to learn
which words refer to which objects (Lazaridou
et al., 2014). This problem is very much re-

MEN score SimLex-999 score

automobile-car 1.00 taxi-cab 0.92

rain-storm 0.98 plane-jet 0.81

cat-feline 0.96 horse-mare 0.83

jazz-musician 0.88 sheep-lamb 0.84

bird-eagle 0.88 bird-hawk 0.79

highway-traffic 0.88 band-orchestra 0.71

guitar-piano 0.86 music-melody 0.70

Table 1: Examples of pairs in the datasets where
auditory is relevant, with the similarity score.

lated to the object recognition task in computer
vision, but instead of using just visual data and
labels, these cross-modal models also utilize tex-
tual information (Socher et al., 2014; Frome et al.,
2013). This allows for zero-shot learning, where
the model can predict how an object relates to
other concepts just from seeing an image of the
object, but without ever having previously encoun-
tered an image of that particular object (Lazaridou
et al., 2014). Multi-modal and cross-modal ap-
proaches have outperformed state-of-the-art text-
based methods on a variety of tasks (Bruni et al.,
2014; Silberer and Lapata, 2014).

3 Evaluations

Following previous work in multi-modal seman-
tics, we evaluate on two standard similarity and re-
latedness datasets: SimLex-999 (Hill et al., 2014)
and the MEN test collection (Bruni et al., 2014).
These datasets consist of concept pairs together
with a human-annotated similarity or relatedness
score, where the former dataset focuses on gen-
uine similarity (e.g., teacher-instructor) and the
latter focuses more on relatedness (e.g., river-
water). In addition, following previous work in
cross-modal semantics, we evaluate on the zero-
shot learning task of inducing a cross-modal map-
ping to the correct label in the auditory modality
from the linguistic one and vice-versa.

3.1 Multi-modal Semantics
Evidence suggests that the inclusion of visual rep-
resentations only improves performance for cer-
tain concepts, and that in some cases the introduc-
tion of visual information is detrimental to perfor-
mance on similarity and relatedness tasks (Kiela
et al., 2014). The same is likely to be true for
other perceptual modalities: in the case of com-
parisons such as guitar-piano, the auditory modal-

2462



Dataset MEN AMEN SLex ASLex

Linguistic 3000 258 999 296

Auditory 2590 233 534 216

Table 2: Number of concept pairs for which repre-
sentations are available in each modality.

ity is certainly meaningful, whereas in the case
of democracy-anarchism it is probably less so.
Therefore, we had two graduate students annotate
the datasets according to whether auditory percep-
tion is relevant to the pairwise comparison. The
annotation criterion was as follows: if both con-
cepts in a pairwise comparison have a distinctive
associated sound, the modality is deemed rele-
vant. Inter-annotator agreement was high: κ =
0.93 for MEN and κ = 0.92 for SimLex-999.
Some examples of relevant pairs can be found in
Table 1. Hence, we now have four evaluation
datasets: the MEN test collection MEN and its
auditory-relevant subset AMEN; and the SimLex-
999 dataset SLex and its auditory-relevant sub-
set ASLex. Due to the nature of the auditory
data sources, it is not possible to build auditory
representations for all concepts in the test sets.
Hence, unless stated otherwise, we report results
for the covered subsets (using the same subsets
when comparing across modalities, to ensure a fair
comparison). Table 2 shows how much of the test
sets are covered for each modality.1

3.2 Cross-modal Semantics

In addition to evaluating our models on the MEN
and SimLex tasks, we evaluate on the cross-
modal task of zero-shot learning. In the case
of vision, Lazaridou et al. (2014) studied the
possibility of predicting from “we found a cute,
hairy wampimuk sleeping behind the tree” that a
“wampimuk” will probably look like a small furry
animal, even though a wampimuk has never been
seen before. We evaluate zero-shot learning, using
partial least squares regression (PLSR) to obtain
cross-modal mappings from the linguistic to audi-
tory space and vice versa.2 Thus, given a linguistic
representation for e.g. guitar, the task is to map it
to the appropriate place in auditory space without

1To facilitate further work in multi-modal semantics be-
yond vision, our code and data have been made publicly
available at http://www.cl.cam.ac.uk/˜dk427/audio.html.

2To avoid introducing another parameter, we set the num-
ber of latent variables in the cross-modal PLSR map to a third
of the number of dimensions of the perceptual representation.

ever having heard a guitar; or map it to the appro-
priate place in linguistic space without ever having
read about a guitar (having only heard it).

4 Approach

One reason for using raw image data in multi-
modal models is that there is a wide variety of re-
sources that contain tagged images, such as Im-
ageNet (Deng et al., 2009) and the ESP Game
dataset (Von Ahn and Dabbish, 2004). However,
such resources do not exist for audio files, and
so we follow a similar approach to Fergus et al.
(2005) and Bergsma and Goebel (2011), who use
Google Images to obtain images. We use the
online search engine Freesound3 to obtain audio
files. Freesound is a collaborative database re-
leased under Creative Commons licenses, in the
form of snippets, samples and recordings, that is
aimed at sound artists. The Freesound API allows
users to easily search for audio files that have been
tagged using certain keywords.

For each of the concepts in the evaluation
datasets, we used the Freesound API to obtain
samples encoded in the standard open source OGG
format4. Because the database contains variable
numbers of files, with varying duration per indi-
vidual file, we restrict the search to a maximum of
50 files and a maximum of 1 minute duration per
file. The Freesound API allows for various degrees
of keyword matching: we opted for the strictest
keyword matching, in that the audio file needs to
have been purposely tagged with the given word
(the alternative includes searching the text descrip-
tion for matching keywords). For example, if we
are searching for audio files of cars, we retrieve up
to 50 files with a maximum duration of 1 minute
per file that have been tagged with the label “car”.

4.1 Linguistic Representations
For the linguistic representations we use the con-
tinuous vector representations from the log-linear
skip-gram model of Mikolov et al. (2013). Specifi-
cally, we trained 300-dimensional vector represen-
tations trained on a dump of the English Wikipedia
plus newswire (8 billion words in total).5 These
types of representations have been found to yield
the highest performance on a variety of semantic
similarity tasks (Baroni et al., 2014).

3http://www.freesound.org.
4http://www.vorbis.com.
5We used the demo-train-big-model-v1.sh script from

http://word2vec.googlecode.com to obtain this corpus.

2463



4.2 Auditory Representations

A common approach to obtaining acoustic fea-
tures of audio files is the Mel-scale Frequency
Cepstral Coefficient (MFCC) (O’Shaughnessy,
1987). MFCC features are abundant in a wide
variety of applications in audio signal process-
ing, ranging from audio information retrieval, to
speech and speaker recognition, and music analy-
sis (Eronen, 2003). Such features are derived from
the mel-frequency cepstrum representation of an
audio fragment (Stevens et al., 1937). In MFCC,
frequency bands are spaced along the mel scale,
which has the advantage that it approximates hu-
man auditory perception more closely than e.g.
linearly-spaced frequency bands. Hence, MFCC
takes human perceptual sensitivity to audio fre-
quencies into consideration, which makes it suit-
able for e.g. compression and recognition tasks,
but also for our current objective of modelling au-
ditory perception. We obtain MFCC descriptors
for frames of audio files using librosa, a popu-
lar library for audio and music analysis written
in Python.6 After having obtained the descrip-
tors, we cluster them using mini-batch k-means
(Sculley, 2010) and quantize the descriptors into a
“bag of audio words” (BoAW) (Foote, 1997) rep-
resentation by comparing the MFCC descriptors
to the cluster centroids. This gives us BoAW rep-
resentations for each of the audio files. Auditory
representations are obtained by taking the mean
of the BoAW representations of the relevant au-
dio files, and finally weighting them using positive
point-wise mutual information (PPMI), a standard
weighting scheme for improving vector represen-
tation quality (Bullinaria and Levy, 2007). We set
k = 300, which equals the number of dimensions
for the linguistic representations.

4.3 Multi-modal Fusion Strategies

Since multi-modal semantics relies on two or more
modalities, there are several ways of combining
or fusing linguistic and perceptual cues (Bruni et
al., 2014). When computing similarity scores, for
instance, we can either jointly learn the represen-
tations; learn them independently, combine (e.g.
concatenate) them and compute similarity scores;
or learn them independently, compute similarity
scores independently and combine the scores. We
call these possibilities early, middle and late fu-
sion, respectively, and evaluate multi-modal mod-

6http://bmcfee.github.io/librosa.

els in each category.

4.3.1 Early Fusion
A good example of early fusion is the recently
introduced multi-modal skip-gram model (Lazari-
dou et al., 2015). This model behaves like a nor-
mal skip-gram, but instead of only having a train-
ing objective for the linguistic representation, it in-
cludes an additional training objective for the vi-
sual context, which consists of an aggregated rep-
resentation of images associated with the given
target word. The skip-gram training objective for
a sequence of training words w1, w2, ..., wT and a
context size c is:

1
T

T∑
t=1

Jθ(wt)

where Jθ is the log-likelihood∑
−c≤j≤c log p(wt+j |wt) and p(wt+j |wt) is

obtained via the softmax:

p(wt+j |wt) = exp
u>wt+j vwt∑W

w′=1 exp
u>

w′vwt

where uw and vw are the context and target vec-
tor representations for the word w respectively,
and W is the vocabulary size. The objective for
the multi-modal skip-gram has an additional vi-
sual objective Jvis (in this case a margin criterion):

1
T

T∑
t=1

Jθ(wt) + Jvis(wt)

Here, we take a similar but more straightfor-
ward approach by making the auditory context a
part of the initial training objective, which is pos-
sible because linguistic and auditory representa-
tions have the same dimensionality. That is, we
modified word2vec to predict additional auditory
contexts that have been set to the corresponding
BoAW representation. We jointly learn linguistic
and audio representations by taking the aggregated
mean µaw of the auditory vectors for a given word
w, and adding this mean vector to the context:

1
T

T∑
t=1

Jθ(wt) + log p(µawt |wt)

The intuition is that the induced vector for the
target word now has to predict an auditory vec-
tor as part of its context, as well as the linguis-
tic ones. As an alternative, we also investigate re-

2464



placing the mean µawt with an auditory vector ob-
tained by uniformly sampling from the set of au-
ditory representations for the target word. We re-
fer to these two alternatives as MMSG-MEAN and
MMSG-SAMPLED, respectively. For this model,
auditory BoAW representations are built for the
ten thousand most frequent words in our corpus,
based on 10 audio files retrieved from FreeSound
for each word (or fewer when 10 are not available).

4.3.2 Middle and Late Fusion
Whereas early fusion requires a joint training ob-
jective that takes into account both modalities,
middle fusion allows for individual training ob-
jectives and independent training data. Similarity
between two multi-modal representations is calcu-
lated as follows:

sim(u, v) = g(f(ul, ua), f(vl, va))

where g is some similarity function, ul and vl are
linguistic representations, and ua and va are the
auditory representations. A typical formulation in
multi-modal semantics for f(x, y) is αx‖(1−α)y,
where ‖ is concatenation (see e.g. Bruni et al.
(2014) and Kiela and Bottou (2014)).

Late fusion can be seen as the converse of mid-
dle fusion, in that the similarity function is com-
puted first before the similarity scores are com-
bined:

sim(u, v) = h(g(ul, vl), g(ua, va))

where g is some similarity function and h is a way
of combining similarities, in our case a weighted
average: h(x, y) = 12(αx+(1−α)y); and we use
g = x·y|x||y| (cosine similarity). Since cosine simi-
larity is the normalized dot-product, and the uni-
modal representations are themselves normalized,
middle and late fusion are equivalent if α = 0.5,
which we call MM. However, when α 6= 0.5, we
distinguish between the two models, calling them
MM-MIDDLE and MM-LATE respectively.

5 Results

5.1 Conceptual Similarity and Relatedness

We evaluate performance by calculating the Spear-
man ρs correlation between the ranking of the
concept pairs produced by the automatic similar-
ity metric (cosine between the derived vectors)
and that produced by the gold-standard similarity
scores. To ensure a fair comparison, we evaluate

Modality MEN AMEN SLex ASLex

Linguistic 0.687 0.603 0.320 0.314

Auditory 0.325 0.510 0.161 0.201

MMSG-MEAN 0.612 0.537 0.274 0.266

MMSG-SAMPLED 0.690 0.602 0.321 0.314
MM 0.680 0.662 0.314 0.345

Table 3: Spearman ρs correlation comparison of
uni-modal and multi-modal representations. The
MMSG models perform early fusion, MM repre-
sents middle and late fusion with α = 0.5 (see
Section 4.3.2).

on the common subsets for which there are repre-
sentations in both modalities (see Table 2).

The results are reported in Table 3. We find
that, while performance decreases for linguistic
representations on the auditory-relevant subsets of
the two datasets, performance increases for the
uni-modal auditory representations on those sub-
sets. This indicates that our auditory representa-
tions are better at judging auditory-relevant com-
parisons than they are at non-auditory ones, as we
might expect.

For all datasets, the accuracy scores for multi-
modal models are at least as high as those for the
purely linguistic representations. In the case of
the full datasets this difference is only marginal,
which is to be expected given how few of the
words in the datasets are auditory-relevant. How-
ever, the results indicate that adding auditory in-
put even for words that are not directly auditory-
relevant is not detrimental to overall performance.

In the case of the auditory-relevant subsets, we
see a large increase in performance when using
multi-modal representations. It is also interesting
that this performance increase is found in the sim-
ple MM model, compared to the more complicated
MMSG models, which seems to indicate that the
latter models are still too reliant on linguistic in-
formation, which harms their performance when
performing auditory-specific comparisons. The
model which performs consistently well across the
four datasets is MM, the middle-late fusion model
with α = 0.5.

5.2 Cross-modal Zero-shot Learning

We learn a cross-modal mapping between the
linguistic and auditory spaces using partial least
squares regression, taking out each concept, train-
ing on the others, and then projecting from one

2465



Mapping P@1 P@5 P@20 P@50

Chance 0.00 0.93 4.01 8.49

Auditory⇒ Ling. 0.77 6.48 17.54 31.33
Ling. ⇒ Auditory 0.73 6.71 22.16 37.32

Table 4: Cross-modal zero-shot learning accuracy.

space into the other. Zero-shot performance is
evaluated using the average percentage correct at
N (P@N ), which measures how many of the test
instances were ranked within the top N highest
ranked nearest neighbors. Results are shown in
Table 4, with the chance baseline obtained by ran-
domly ranking a concept’s nearest neighbors. In-
sofar as it is possible to make a direct comparison
with linguistic-visual zero-shot learning (which
uses entirely different data), it appears that the cur-
rent task may be more difficult: Lazaridou et al.
(2014) report a P@1 of 2.4 and P@20 of 33.0 for
their linguistic-visual model.

5.3 Qualitative Analysis

We also performed a small qualitative analysis of
the BoAW representations for the words in MEN
and SLex. As Table 5 shows, the nearest neighbors
are remarkably semantically coherent. For exam-
ple, the model groups together sounds produced
by machines, or by water. It even finds that dinner,
meal, lunch and breakfast are closely related. In
contrast, nearest neighbors for the linguistic model
tend to be of a more abstract nature: where we find
mouth and throat as auditory neighbors for lan-
guage, the linguistic model gives us concepts like
word and dictionary; while auditory gossip sounds
like maids and is something you might do in the
corridor, it is linguistically associated with more
abstract concepts like news and newspaper.

6 Parameter Tuning

There are many parameters that were left fixed in
the main results that could have been adjusted to
improve performance, particularly in the middle
and late fusion models. It is useful to investigate
some of the parameters that are likely to have an
impact on performance: what the effect of the α
mixing parameter is, whether a different k would
have yielded better auditory representations, and
whether the number and duration of the audio files
from FreeSound has any effect.

Figure 1: Performance of middle and late multi-
modal fusion models compared to linguistic rep-
resentations on the four datasets when varying the
α mixing parameter on the x-axis.

6.1 Mixing with α

The mixing parameter α plays an important role
in the middle and late fusion models. We kept
it fixed at 0.5 for the MM model above, but here
we experiment with varying the parameter, yield-
ing results for two different models, MM-MIDDLE
and MM-LATE. The results are shown in Figure 1,
where moving to the right on the x-axis uses more
linguistic input and moving to the left uses more
auditory input. The late model consistently out-
performs the middle fusion model, which is prob-
ably because it is less susceptible to any noise in
the auditory representation. Optimal performance
seems to be around α = 0.6 for both fusion strate-
gies on all four datasets, indicating that it is bet-
ter to include a little more linguistic than auditory
input. It appears that any 0.5 ≤ α < 1 (i.e.,
where we have more linguistic input but still some
auditory signal), outperforms the purely linguis-
tic representation, substantially in the case of the
auditory-relevant subsets.

6.2 Number of Auditory Dimensions

We experimented with different values for the
number of audio words k (i.e. the number of clus-
ters in the k-means clustering that determines the
number of “audio words”). As Figure 2 shows, the
quality of the uni-modal auditory representations
is highly robust to the number of dimensions. In
fact, any choice of k in the range shown provides
similar results across the datasets.

2466



Auditory Linguistic

navy language gossip dinner navy language gossip dinner

army mouth maid meal army word news lunch

aviation man guest lunch military words newspaper wedding

plane father elevator writer vessel literature cute meal

jet adult danger breakfast sunk dictionary sexy breakfast

cannon throat corridor couch ship tongue mirror cocktail

monster motor water dawn monster motor water dawn

orchestra engine stream summer zombie vehicle droplets dusk

demon rain bath child demon automobile salt sunrise

guitar beach river victor dragon car cold moon

beast boat bathroom morning beast motorcycle sunlight night

pilot car rain garden creatures truck milk misty

Table 5: Example nearest neighbors for auditory (BoAW) representations and linguistic representations.

Figure 2: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
k parameter.

6.3 Number and Duration of Audio Files

We experimented with the number of audio files
by querying FreeSound for up to 100 audio files
per search word, while keeping k = 300. The
results are shown in Figure 3. It appears that “the
more the better”, although performance does not
increase significantly after around 40 audio files.

In order to examine the effect of audio file du-
ration, we experimented with specifying the du-
ration of audio files when querying the database,
either taking very short (up to 5 seconds), medium
length (up to 1 minute) or files of any duration.
The results can be found in Figure 4, showing that
performance generally increases as the files get
longer (except on AMEN where a duration of 1
minute provides optimal performance).

7 Case Study: Musical Instruments

To strengthen the finding that multi-modal repre-
sentations perform well on the auditory-relevant
subsets of the datasets, we evaluate on an alto-
gether different task, namely that of musical in-

Figure 3: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
number of audio files per target word.

strument classification. We used Wikipedia to col-
lect a total of 52 instruments and divided them into
5 classes: brass, percussion, piano-based, string
and woodwind instruments. For each of the in-
struments, we collected as many audio files from
FreeSound as possible, and used the MM-MIDDLE
model with parameter settings that yielded good
results in the previous experiments (k = 300 and
α = 0.6). We then performed k-means cluster-
ing with five cluster centroids and compared re-
sults between auditory, linguistic and multi-modal,
evaluating the clustering quality using the standard
V-measure clustering evaluation metric (Rosen-
berg and Hirschberg, 2007).

This is an interesting problem because instru-
ment classes are determined somewhat by conven-

2467



Figure 4: Performance of uni-modal auditory rep-
resentations on the four datasets when varying the
maximum duration.

tion (is a saxophone a brass or a woodwind in-
strument?). What is more, how instruments ac-
tually sound is rarely described in detail in text,
so corpus-based linguistic representations cannot
take this information into account. The results are
in Table 6, clearly showing that the multi-modal
representation which utilizes both linguistic infor-
mation and auditory input performs much better
on this task than the uni-modal representations. It
is interesting to observe that the linguistic repre-
sentations perform better than the auditory ones: a
possible explanation for this is that audio files in
FreeSound are rarely samples of a single individ-
ual instrument, so if a bass is often accompanied
by a drum this will affect the overall representa-
tion. The table also shows, for the 5 clusters under
both models, the nearest instruments to the cluster
centroids, qualitatively demonstrating the greater
cluster coherence for the multi-modal model.

8 Conclusions

We have studied grounding semantic representa-
tions in raw auditory perceptual information, us-
ing a bag of audio words model to obtain au-
ditory representations, and combining them into
multi-modal representations using a variety of fu-
sion strategies. Following previous work in multi-
modal semantics, we evaluated on conceptual sim-
ilarity and relatedness datasets, and on the cross-
modal task of zero-shot learning. We presented
a short case study showing that multi-modal rep-
resentations perform much better than auditory or
linguistic representations on a musical instrument
clustering task. It may well be the case that the

Model Auditory Linguistic MM-MIDDLE

V-measure 0.39 0.47 0.54

Linguistic

1 baritone

2 lute, zither, xylophone, lyre, cymbals

3 piano, trombone, clarinet, cello, violin

4 castanets, tambourine, claves, maracas

5 trumpet, horn, bugle, cowbell, carillon

Multi-modal

1 drum, claves, bongo, bass, conga

2 xylophone, glockenspiel, tambourine, cymbals

3 cello, piano, clarinet, trombone, violin,

4 chimes, bell

5 mandolin, banjo, harmonica, guitar, sitar

Table 6: V-measure performance for clustering
musical instruments, together with instruments
closest to cluster centroid for linguistic and multi-
modal.

auditory modality is better suited for other evalua-
tions, but we have chosen to follow standard eval-
uations in multi-modal semantics to allow for a di-
rect comparison.

In future work, it would be interesting to inves-
tigate different sampling strategies for the early
fusion joint-learning approach and to investigate
more sophisticated mixing strategies for the mid-
dle and late fusion models, e.g. using the “audio
dispersion” of a word to determine how much au-
ditory input should be included in the multi-modal
representation (Kiela et al., 2014). Another in-
teresting possibility is to improve auditory repre-
sentations by training a neural network classifier
on the audio files and subsequently transferring
the hidden representations to tasks in semantics.
Lastly, now that the perceptual modalities of vi-
sion, audio and even olfaction (Kiela et al., 2015)
have been investigated in the context of distribu-
tional semantics, the logical next step for future
work is to explore different fusion strategies for
multi-modal models that combine various sources
of perceptual input into a single grounded model.

Acknowledgments

DK is supported by EPSRC grant EP/I037512/1.
SC is supported by ERC Starting Grant DisCoTex
(306920) and EPSRC grant EP/I037512/1. We are
grateful to Xavier Serra, Frederic Font Corbera,
Alessandro Lopopolo and Emiel van Miltenburg

2468



for useful suggestions and thank the anonymous
reviewers for their helpful comments.

References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.

Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP, pages 399–405.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.

John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting Semantic Representations from Word Co-
occurrence Statistics: A computational study. Be-
havior Research Methods, 39:510–526.

Stephen Clark. 2015. Vector Space Models of Lexical
Meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, chapter 16.
Wiley-Blackwell, Oxford.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Proceedings of Com-
puter Vision and Pattern Recognition (CVPR), pages
248–255.

A. Eronen. 2003. Musical instrument recognition us-
ing ICA-based transform of features and discrimina-
tively trained HMMs. In Proceedings of the Seventh
International Symposium on Signal Processing and
Its Applications, volume 2, pages 133–136.

Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of NAACL, pages 91–99.

Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew
Zisserman. 2005. Learning object categories from
Google’s image search. In Proceedings of ICCV,
pages 1816–1823.

Jonathan T Foote. 1997. Content-based retrieval of
music and audio. In Voice, Video, and Data Com-
munications, pages 138–147.

Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. DeViSE: A Deep
Visual-Semantic Embedding Model. In Proceedings
of NIPS, pages 2121–2129.

Michael S. Gazzaniga, editor. 1995. The Cognitive
Neurosciences. MIT Press, Cambridge, MA.

Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335–346.

Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can’t see what I mean. In Pro-
ceedings of EMNLP, pages 255–265.

Felix Hill, Roi Reichart, and Anna Korhonen.
2014. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.

Douwe Kiela and Léon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In Proceedings of
EMNLP, pages 36–45.

Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of ACL, pages 835–841.

Douwe Kiela, Luana Bulat, and Stephen Clark. 2015.
Grounding semantics in olfactory perception. In
Proceedings of ACL, pages 231–236, Beijing, China,
July.

Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In Proceedings of ACL, pages 1403–1414.

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skipgram model. In Proceedings of
NAACL.

Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.

A. Lopopolo and E. van Miltenburg. 2015. Sound-
based distributional models. In Proceedings of the
11th International Conference on Computational
Semantics (IWCS 2015).

Max M. Louwerse. 2008. Symbol interdependency in
symbolic and embodied cognition. Topics in Cogni-
tive Science, 59(1):617–645.

David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of ICLR,
Scottsdale, Arizona, USA.

D. O’Shaughnessy. 1987. Speech communication: hu-
man and machine. Addison-Wesley series in electri-
cal engineering: digital signal processing. Universi-
ties Press (India) Pvt. Limited.

Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of
EMNLP, pages 1146–1157.

2469



Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 410–
420.

David Sculley. 2010. Web-scale k-means clustering.
In Proceedings of WWW, pages 1177–1178. ACM.

Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of EMNLP, pages 1423–1433.

Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL, pages 721–732.

Josef Sivic and Andrew Zisserman. 2003. Video
google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470–
1477.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
ACL, 2:207–218.

Stanley Smith Stevens, John Volkmann, and Edwin B.
Newman. 1937. A scale for the measurement of
the psychological magnitude pitch. Journal of the
Acoustical Society of America, 8(3):185—190.

Gabriel Synnaeve, Maarten Versteegh, and Emmanuel
Dupoux. 2014. Learning words from images and
speech. In NIPS Workshop on Learning Semantics,
Montreal, Canada.

Peter D. Turney and Patrick Pantel. 2010. From
Frequency to Meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188, January.

Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319–326. ACM.

2470


