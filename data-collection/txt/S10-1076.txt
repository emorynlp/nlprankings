



















































NCSU: Modeling Temporal Relations with Markov Logic and Lexical Ontology


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341–344,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

NCSU: Modeling Temporal Relations  
with Markov Logic and Lexical Ontology 

 
Eun Young Ha Alok Baikadi Carlyle Licata  James C. Lester 

Department of Computer Science 
North Carolina State University 

Raleigh, NC, USA 
{eha,abaikad,cjlicata,lester}@ncsu.edu 

 
  

 

Abstract 

As a participant in TempEval-2, we ad-
dress the temporal relations task consist-
ing of four related subtasks. We take a su-
pervised machine-learning technique us-
ing Markov Logic in combination with 
rich lexical relations beyond basic and 
syntactic features. One of our two submit-
ted systems achieved the highest score for 
the Task F (66% precision), untied, and 
the second highest score (63% precision) 
for the Task C, which tied with three other 
systems.  

1 Introduction 

Time plays a key role in narrative. However, cor-
rectly recognizing temporal order among events 
is a challenging task. As a follow-up to the first 
TempEval competition, TempEval-2 addresses 
this challenge. Among the three proposed tasks 
of TempEval-2, we address the temporal rela-
tions task consisting of four subtasks: predicting 
temporal relations that hold between events and 
time expressions in the same sentence (Task C), 
events and the document creation time (Task D), 
main events in adjacent sentences (Task E), and 
main events and syntactically dominated events, 
such as those in subordinated clauses (Task F). 
We are primarily concerned with Task C, E, and 
F, because D is not relevant to our application 
domain.1 However, rather than eliminating Task 
D altogether, we build a very simple model for 
this task by using only those features that are 
shared with other task models (i.e., the document 

                                                
1 Our application domain concerns analysis of narrative 
stories written by middle school students, with the analysis 
being conducted a single story at a time. 

creation time data are not used because none of 
the other task models need them as features). It 
was expected that this approach would support 
more interesting comparisons with other systems 
that take a more sophisticated approach to the 
task. Further, we experiment with a joint model-
ing technique to examine if the communication 
with other task models brings a boost to a per-
formance of the simple model. 

Taking a supervised machine-learning ap-
proach with Markov Logic (ML) (Richardson and 
Domingos, 2006), we constructed two systems, 
NCSU-INDI and NCSU-JOINT. NCSU-INDI con-
sists of four independently trained classifiers, 
one for each task, whereas NCSU-JOINT models 
all four tasks jointly. The choice of ML as learn-
ing technique for temporal relations is motivated 
both theoretically and practically. Theoretically, 
it is a statistical relational learning framework 
that does not make the i.i.d. assumption for the 
data. This is a desirable characteristic for com-
plex problems such as temporal relation classifi-
cation, as well as many other natural language 
problems, in which the features representing a 
given problem are often correlated with one an-
other. Practically, ML allows us to build both 
individual and joint models in a uniform frame-
work; individual models can be easily combined 
together into a joint model with a set of global 
formulae governing over them.  

In previous work (Yoshikawa et al., 2009), 
ML was successfully applied to temporal relation 
classification task. Our approach is different 
from this work in two primary respects. First, we 
introduce new lexical relation features derived 
from English lexical ontologies. Second, our 
model addresses a new task introduced in Tem-
pEval-2, which is to identify temporal relations 
between main and syntactically dominated 
events in the same sentence. We also employ 
phrase-based syntactic features (Bethard and 

341



Martin 2007) rather than dependency-based syn-
tactic features. 

2 Features 
We consider three types of features: basic, syn-
tactic, and lexical relation features. Basic fea-
tures represent the information directly available 
from the original data provided by the task orga-
nizer; syntactic features are extracted from syn-
tactic parses generated by Charniak parser 
(Charniak, 2000); and lexical semantic relations 
that are derived from two external lexical data-
bases, VERBOCEAN (Chklovski and Pantel, 
2004) and WordNet (Fellbaum, 1998). 

2.1 Basic Features 
Basic features include the word tokens, stems of 
the words, and the manually annotated attributes 
of events and time expressions. In the TempEval-
2 data, an event always consists of a single word 
token, but time expressions often consist of mul-
tiple tokens. We treat each word in time expres-
sions as a different feature. For example, two 
word features, ‘this’ and ‘afternoon’, are ex-
tracted from a given time expression ‘this after-
noon’. Stemming is done with the Porter 
Stemmer in NLTK (Loper and Bird, 2002). The 
value attributes of time expressions are treated as 
symbolic features, rather than being decomposed 
into actual integer values representing dates and 
times.  

2.2 Syntactic Features 
Our syntactic features draw upon the features 
previously shown to be effective for temporal 
relation classification (Bethard and Martin, 
2007), including the following: 

• pos: the part-of-speech (pos) tags of the 
event and the time expression word to-
kens, assigned by Charniak parser.  

• gov-prep: any prepositions governing 
the event or time expression (e.g., ‘for’ in 
‘for ten years’). 

• gov-verb: the verb governing the 
event or time expression, similar to gov-
prep. 

• gov-verb-pos: the pos tag of the 
governing verb. 

We also investigate both full and partial syn-
tactic paths between a pair of event and time ex-
pressions, but including these features does not 
improve the classification results on our devel-
opment data set. 

2.3 Lexical Relation Features 
VERBOCEAN is a graph of semantic relations 
between verbs. There are 22,306 relations be-
tween 3,477 verbs that have been mined using 
Google searches for lexico-syntactic patterns. 
VERBOCEAN contains five different types of re-
lations (Table 1). Verbs are stored in the lemma-
tized forms and senses are not disambiguated. A 
connection between two verbs indicates that the 
relation holds between some senses of the verbs. 

VERBOCEAN’S database is presented as a list 
of verb pair relations, along with a confidence 
score. Both the transitive and symmetric closure 
over the relations were taken before storage in a 
SQLite database for queries. The transitive clo-
sure was calculated using the Warshall algorithm 
(Agrawal and Jagadish, 1990). The confidence 
score for the new arc was calculated as the aver-
age of the two constituents. The symmetric clo-
sure was calculated using a simple pass. The 
confidence score is the same as the reflected 
edge for symmetric relations. A set of VER-
BOCEAN features were calculated for each target 
event pair within each of the temporal relations 
tasks. Each verb was lemmatized using the 
WordNet lemmatizer in NLTK before being 
compared against the database. Rather than fo-
cusing only on HAPPENS-BEFORE relation as in 
Mani et al. (2006), we consider all five verb rela-
tions in two different versions, unweighted and 
weighted. The unweighted version is a binary 
feature indicating the existence of an arc between 
the two target verbs in VERBOCEAN. In the 
weighted version, the existence of an arc is 
weighted by the associated confidence score.  

In addition to VerbOcean, WordNet was used 
for its conceptual relations. WordNet is a large 
lexical database, which contains information on 
verbs, nouns, adjectives and adverbs, grouped 
into hierarchically organized cognitive synonym 

                                                
2 Examples are taken from 
http://demo.patrickpantel.com/Content/Verbocean/. 

Relation Example  
SIMILARITY ‡† produce :: create 
STRENGTH † wound :: kill 
ANTONYMY ‡ open :: close  
ENABLEMENT  fight :: win 
HAPPENS-BEFORE † buy :: own 
 

Table 1: Semantic relations between verbs in 
VERBOCEAN (‡ and † denotes symmetric and 
transitive closure, respectively, holds for the 
given relation)2 

342



sets (synsets). WordNet was accessed through 
the WordNetCorpusReader module of NLTK. 
For each target event pair within each of the 
temporal relations tasks, a semantic distance be-
tween the associated tokens was computed using 
the path-similarity metric present within the API. 
The synset chosen was simply the first synset 
returned by the reader. Similar to the VER-
BOCEAN features, we consider both unweighted 
and weighted versions of the feature.   

3 The Systems 

ML is a probabilistic extension of first-order 
logic that allows formulae to be violated. It as-
signs a weight to each formula, reflecting the 
strength of the constraint represented by the for-
mula. A Markov logic network (MLN) is a set of 
weighted first-order clauses, which, together 
with constants, defines a Markov network.  We 
constructed two systems, NCSU-INDI and 
NCSU-JOINT using an off-the-shelf tool for ML 
(Riedel, 2008). 

3.1 NCSU-INDI 
NCSU-INDI consists of four independently 
trained MLNs, one for each task. Each MLN is 
defined by a set of local formulae that are con-
junctions of predicates representing the features. 
An example local formula used for Task C is 
 

eventTimex(e, t)  eventWord(e, w)  
          relEventTimex(e, t, r)        (1) 

 

If a pair of event e and time expression t exists 
and the event consists of a word token w, for-
mula (1) assigns a temporal relation t to the 
given pair of e and t with some weights. 

For each task, the features described in Sec-
tion 2 were examined on a held-out development 
data set (about 10% of the training data) for their 
effectiveness in predicting temporal relations and 
removed if they do not improve the results. Ta-
ble 2 lists the features actually used for the tasks. 
Interestingly, none of the time expression fea-
tures were effective on the development data. 

3.2 NCSU-JOINT 
As well as the local formulae from the four local 
MLNs, a set of global formulae are added to 
NCSU-JOINT as hard constraints to ensure the 
consistency between the classification decisions 
of local MLNs. For example, formula (2) ensures 
that if an event e1 happens before the document 
creation time (dct) and another event e2 happens 

after dct, then e1 happens before e2 and vice ver-
sa. 

 

relDctEvent(e1,t,BEFORE) relDctEvent(e2,t, AFTER)     
                    relEvents(e1, e2, BEFORE)       (2) 
 

A set of global constraints is defined between 
Tasks C and F, D and F, as well as D and E, re-
spectively. 

4 Results and Discussion 

The predicted outputs from our systems exhibit 
mixed results. NCSU-INDI achieves the highest 
precision score on the test data for Task F by a 
relatively large margin (6%) from the second-
place system, as well as the second highest preci-
sion score on Task C, tied with three other sys-
tems. Given the encouraging result for Task F, 
we would preliminarily conclude that the VE-
BOCEAN relations are effective predictors of 
temporal relations between main and syntacti-
cally dominated events. However, the same sys-
tem does not achieve the same level of accuracy 

Task Feature 
C  D E F 

event-word √ √ √e2 √e1,e2 Event 
event-stem √ √ √e1.e2 √e1,e2 
event-polarity √ √ √e1.e2 √e1,e2 
event-modal √ √ √e1.e2 √e1,e2 
event-pos √ √ √e1.e2 √e2 
event-tense √  √e1.e2 √e1,e2 
event-aspect √ √ √e1.e2 √e1,e2 

Event  
Attribute 

event-class √ √ √e1.e2 √e1,e2 
timex-word     Timex 
timex-stem     
timex-type     Timex  

Attribute timex-value     
pos  √e √e1.e2  
gov-prep √e,t √e √e1.e2 √e1,e2 
gov-verb √e,t √e √e1.e2 √e1,e2 

Syntactic 
Parse 

gov-verb-pos √e,t  √e1.e2 √e1,e2 
verb-rel    √ Verb-

Ocean verb-rel-w   √  
word-dist   √  WordNet 
word-dist-w     

 

Table 2: Features used for each task (subscripts 
e and t mean event and time expression, re-
spectively. Subscripts e1 and e2 mean the first 
and the second main events for the Task E and 
the main and the syntactically dominated 
events for the Task F, respectively) 
 
 

343



for Task E, even though it is closely related to 
Task F. The major difference between the mod-
els of Task E and F is that the Task E model uses 
weighted VERBOCEAN relations along with a 
WordNet feature, while the Task F model uses 
unweighted VERBOCEAN relations without the 
WordNet feature. We suspect these two features 
might negatively impact the classification deci-
sions on the test data, even though they prelimi-
narily appeared to be effective predictors on the 
development data.  

NCSU-JOINT also yields mixed results. The 
performance on both Task D and F dramatically 
drops with the joint modeling approach, while 
there is a modest improvement on Task E. Man-
ual examination of the results on the test data 
revealed that the majority of the relations in Task 
D and F were classified as OVERLAP, which may 
be due to overly strict global constraints; rather 
than violating global constraints, the system re-
sorted to rather neutral predictions.  

5 Conclusions 
Temporal event order recognition is a challeng-
ing task. Using basic, syntactic, and lexical rela-
tion features, we built two systems with ML: 
NCSU-INDI models each subtask independently, 
and NCSU-JOINT models all four tasks jointly. 
NCSU-INDI was most effective in predicting 
temporal relations between main events and syn-
tactically dominated events (66% precision), as 
well as temporal relations between time expres-
sions and events (63% precision). Future direc-
tions include conducting a more rigorous exami-
nation of the predictive power of the features, as 
well as the impact of global formulae for the 
joint model. 
 
Acknowledgments 
This research was supported by the National Sci-
ence Foundation under Grant IIS-0757535.  Any 
opinions, findings, and conclusions or recom-
mendations expressed in this material are those 
of the authors and do not necessarily reflect the 
views of the National Science Foundation. 

 

References  
R. Agrawal, S. Dar, and H. V. Jagadish. 1990. Direct 

transitive closure algorithms: design and perform-
ance evaluation. ACM Transactions on Database 
Systems, 15(3): 427-458. 

S. Bethard and J. H. Martin. 2007. CU-TMP: tempo-
ral relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 
129-132, Prague, Czech Republic. 

E. Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings of the 1st North American 
chapter of the Association for Computational Lin-
guistics conference, pages 132-139, Seattle, WA. 

Y. Cheng, M. Asahara, and Y. Matsumoto. 2007. 
NAIST.Japan: Temporal relation identification us-
ing dependency parsed tree. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, pages 245-248, Prague, Czech Republic. 

T. Chklovski and P. Pantel. 2004.VerbOcean: Mining 
the Web for Fine-Grained Semantic Verb Rela-
tions. In Proceedings of Conference on Empirical 
Methods in Natural Language Processing, pages 
33-40, Barcelona, Spain. 

C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 

E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of ACL Workshop 
on Effective Tools and Methodologies for Teaching 
Natural Language Processing and Computational 
Linguistics, pages 62–69, Philadelphia, PA. 

I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. 
Pustejovsky. 2006. Machine learning of temporal 
relations. In Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the Association for Compu-
tational Linguistics, pages 753-760, Sydney, Aus-
tralia. 

M. Richardson and P. Domingos. 2006. Markov Log-
ic Networks. Machine Learning, 62(1): 107-136. 

S. Riedel. 2008. Improving the accuracy and effi-
ciency of MAP inference for Markov Logic. In 
Proceedings of the 24th Conference in Uncertainty 
in Artificial Intelligence, pages 468-475, Helsinki, 
Finland. 

K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-
moto. 2009. Jointly Identifying Temporal Relations 
with Markov Logic. In Proceedings of the Joint 
Conference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages 
405-413, Suntec, Singapore. 

Precision / Recall (%) System 
Task C Task D Task E Task F 

NCSU-INDI 63/63 68/68 48/48 66/66 
NCSU-JOINT 62/62 21/21 51/51 25/25 

 

Table 3: Accuracy of the systems on each task 

344


