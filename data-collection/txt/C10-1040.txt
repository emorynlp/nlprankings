Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 349–357,

Beijing, August 2010

349

EMDC: A Semi-supervised Approach for Word Alignment

Qin Gao

Language Technologies Institute

Carnegie Mellon University

qing@cs.cmu.edu

Francisco Guzman

Centro de Sistemas Inteligentes
Tecnol´ogico de Monterrey
guzmanhe@gmail.com

Stephan Vogel

Language Technologies Institute

Carnegie Mellon University

stephan.vogel@cs.cmu.edu

Abstract

1

semi-
This paper proposes a novel
technique
supervised word alignment
called EMDC that integrates discrimina-
tive and generative methods. A discrim-
inative aligner is used to ﬁnd high preci-
sion partial alignments that serve as con-
straints for a generative aligner which
implements a constrained version of the
EM algorithm. Experiments on small-size
Chinese and Arabic tasks show consistent
improvements on AER. We also experi-
mented with moderate-size Chinese ma-
chine translation tasks and got an aver-
age of 0.5 point improvement on BLEU
scores across ﬁve standard NIST test sets
and four other test sets.
Introduction
Word alignment is a crucial component in sta-
tistical machine translation (SMT). From a Ma-
chine Learning perspective, the models for word
alignment can be roughly categorized as gener-
ative models and discriminative models. The
widely used word alignment tool, i.e. GIZA++
(Och and Ney, 2003), implements the well-known
IBM models (Brown et al., 1993) and the HMM
model (Vogel et al., 1996), which are genera-
tive models. For language pairs such as Chinese-
English, the word alignment quality is often un-
satisfactory. There has been increasing interest on
using manual alignments in word alignment tasks,
which has resulted in several discriminative mod-
els.
Ittycheriah and Roukos (2005) proposed to
use only manual alignment links in a maximum
entropy model, which is considered supervised.
Also, a number of semi-supervised word align-
ers have been proposed (Taskar et al., 2005; Liu
et al., 2005; Moore, 2005; Blunsom and Cohn,
2006; Niehues and Vogel, 2008). These methods

use held-out manual alignments to tune weights
for discriminative models, while using the model
parameters, model scores or alignment links from
unsupervised word aligners as features. Callison-
Burch et. al. (2004) proposed a method to interpo-
late the parameters estimated by sentence-aligned
and word-aligned corpus. Also, there are recent
attempts to combine multiple alignment sources
using alignment conﬁdence measures so as to im-
prove the alignment quality (Huang, 2009).

In this paper,

the question we address is
whether we can jointly improve discriminative
models and generative models by feeding the in-
formation we get from the discriminative aligner
back into the generative aligner. Examples of
this line of research include Model 6 (Och and
Ney, 2003) and the EMD training approach pro-
posed by Fraser and Marcu (2006) and its ex-
tension called LEAF aligner (Fraser and Marcu,
2007). These approaches use labeled data to tune
additional parameters to weight different compo-
nents of the IBM models such as the lexical trans-
lation model, the distortion model and the fertility
model. These methods are proven to be effective
in improving the quality of alignments. However,
the discriminative training in these methods is re-
stricted in using the model components of gener-
ative models, in other words, incorporating new
features is difﬁcult.

Instead of using discriminative training meth-
ods to tune the weights of generative models,
in this paper we propose to use a discrimina-
tive word aligner to produce reliable constraints
for the EM algorithm. We call this new train-
ing scheme EMDC (Expectation-Maximization-
Discrimination-Constraint).
The methodology
can be viewed as a variation of bootstrapping. It
enables the generative models to interact with dis-
criminative models at the data level instead of the
model level. Furthermore, with a discriminative

350

word aligner that uses generative word aligner’s
output as features, we create a feedback loop that
can iteratively improve the quality of both align-
ers. The major contributions of this paper are: 1)
The EMDC training scheme, which ties the gen-
erative and discriminative aligners together and
enables future research on integrating other dis-
criminative aligners. 2) An extended generative
aligner based on GIZA++ that allows to perform
constrained EM training.

In Section 2, we present the EMDC training
scheme. Section 3 provides details of the con-
strained EM algorithm.
In Section 4, we intro-
duce the discriminative aligner and link ﬁltering.
Section 5 provides the experiment set-up and the
results. Section 6 concludes the paper.
2 EMDC Training Scheme

The EMDC training scheme consists of
three parts, namely EM, Discrimination, and
Constraints. As illustrated in Figure 1, a large
unlabeled training set is ﬁrst aligned with a gen-
erative aligner (GIZA++ for the purpose of this
paper). The generative aligner outputs the model
parameters and the Viterbi alignments for both
source-to-target and target-to-source directions.
Afterwards, a discriminative aligner (we use the
one described in (Niehues and Vogel, 2008)),
takes the lexical translation model, fertility model
and Viterbi alignments from both directions as
features, and is tuned to optimize the AER on a
small manually aligned tuning set. Afterwards,
the alignment links generated by the discrimina-
tive aligner are ﬁltered according to their likeli-
hood, resulting in a subset of links that has high
precision and low recall. The next step is to put
these high precision alignment links back into the
generative aligner as constraints. A conventional
generative word aligner does not support this type
of constraints. Thus we developed a constrained
EM algorithm that can use the links from a partial
alignment as constraints and estimate the model
parameters by marginalizing likelihoods.

After the constrained EM training is performed,
we repeat the procedure and put the updated gen-
erative models and Viterbi alignment back into the
discriminative aligner. We can either ﬁx the num-
ber of iterations, or stop the procedure when the
gain on AER of a small held-out test set drops be-

Figure 1: Illustration of EMDC training scheme

low a threshold.

The key components for the system are:
1. A generative aligner that can make use of re-
liable alignment links as constraints and im-
prove the models/alignments.

2. A discriminative aligner that outputs con-
ﬁdence scores for alignment links, which
allows to obtain high-precision-low-recall
alignments.

While in this paper we derive the reliable links
by ﬁltering the alignment generated by a discrimi-
native aligner, such partial alignments may be ob-
tained from other sources as well: manual align-
ments, speciﬁc named entity aligner, noun-phrase
aligner, etc.

As we mentioned in Section 1, the discrimina-
tive aligner is not restricted to use features param-
eters of generative models and Viterbi alignments.
However, including the features from generative
models is required for iterative training, because
the improvement on the quality of these features
can in turn improve the discriminative aligner. In
our experiments, the discriminative aligner makes
heavy use of the Viterbi alignment and the model
parameters from the generative aligner. Nonethe-
less, one can easily replace the discriminative
aligner or add new features to it without modify-
ing the training scheme. The open-ended prop-
erty of the training scheme makes it a promising
method to integrate different aligners.

In the next two sections, we will describe the

key components of this framework in detail.
3 Constrained EM algorithm

In this section we will brieﬂy introduce the con-
strained EM algorithm we used in the experiment,

351

further details of the algorithm can be found in
(Gao et al., 2010).

The IBM Models (Brown et al., 1993) are a
series of generative models for word alignment.
GIZA++ (Och and Ney, 2003), the most widely
used implementation of IBM models and HMM
(Vogel et al., 1996), employs EM algorithm to es-
timate the model parameters. For simpler models
such as Model 1 and Model 2, it is possible to
obtain sufﬁcient statistics from all possible align-
ments in the E-step. However, for fertility-based
models such as Models 3, 4, and 5, enumerating
all possible alignments is NP-complete. To over-
come this limitation, GIZA++ adopts a greedy
hill-climbing algorithm, which uses simpler mod-
els such as HMM or Model 2 to generate a “center
alignment” and then tries to ﬁnd better alignments
among its neighbors. The neighbors of an align-
ment aJ
1 = [a1, a2,··· , aJ ] with aj ∈ [0, I] are
deﬁned as alignments that can be generated from
1 by one of the following two operators:
aJ
1. The move operator m[i,j], that changes aj :=
i, i.e. arbitrarily sets word fj in the target
sentence to align to the word ei in source sen-
tence;

2. The swap operator s[j1,j2] that exchanges aj1

and aj2.

The algorithm will update the center alignment
as long as a better alignment can be found, and
ﬁnally outputs a local optimal alignment. The
neighbor alignments of the ﬁnal center alignment
are then used in collecting the counts for the M-
Step. Och and Ney (2003) proposed a fast imple-
mentation of the hill-climbing algorithm that em-
ploys two matrices, i.e. Moving Matrix MI×J and
Swapping Matrix SJ×J. Each cell of the matrices
stores the value of likelihood difference after ap-
plying the corresponding operator.

1 , eI

1) as a set of links: αJ

We deﬁne a partial alignment constraint of a
sentence pair (f J
I =
{(i, j)|0 ≤ i < I, 0 ≤ j < J}. Given a set of
constraints, an alignment aJ
1 = [a1, a2,··· , aj]
on the sentence pair f J
1, the translation proba-
1) will be zero if the alignment
bility of P r(f J
is inconsistent with the constraints. Constraints
(0, j) or (i, 0) are used to explicitly represent that
word fj or ei is aligned to the empty word.

1 |eI

1 , eI

Under the assumptions of the IBM models,
1 is inconsistent with

there are two situations that aJ
I :
αJ
1. Target word misalignment: The IBM mod-
els assume that one target word can only be
aligned to one source word. Therefore, if the
target word fj aligns to a source word ei,
while the constraint αJ
I suggests fj should be
aligned to ei0, the alignment violates the con-
straint and thus is considered inconsistent.

2. Source word to empty word misalignment: if
a source word is aligned to the empty word,
it cannot be aligned to any concrete target
word.

However, the partial alignments, which allow
n-to-n alignments, may already violate the 1-to-n
alignment restriction of the IBM models. In these
cases, we relax the condition in situation 1 that if
the alignment link aj∗ is consistent with any one
of the conﬂicting target-to-source constraints, it
will be considered consistent. Also, we arbitrarily
assign the source word to empty word constraints
higher priorities than other constraints, because
unlike situation 1, it does not have the problem
of conﬂicting with other constraints.

3.1 Constrained hill-climbing algorithm

To ensure that resulting center alignment be
consistent with the constraints, we need to split
the hill-climbing algorithm into two stages: 1) op-
timize towards the constraints and 2) optimize to-
wards the optimal alignment under the constraints.
From a seed alignment, we ﬁrst move the align-
ment towards the constraints by choosing a move
or swap operator that:

1. produces the alignment that has the highest
likelihood among alignments generated by
other operators,

2. eliminates at least one inconsistent link.
We iteratively update the alignment until no
other inconsistent link can be removed. The algo-
rithm implies that we force the seed alignment to
be closer to the constraints while trying to ﬁnd the
best consistent alignment. Figure 2 demonstrates
the idea, given the constraints shown in (a), and
the seed alignment shown as solid links in (b), we

352

Figure 2: Illustration of Algorithm 1

move the inconsistent link to the dashed link by a
move operation.

After we ﬁnd the consistent alignment, we pro-
ceed to optimize towards the optimal alignment
under the constraints. The algorithm sets the value
of the cells in moving/swapping matrices to nega-
tive if the corresponding operators will lead to an
inconsistent alignment. The moving matrix needs
to be processed only once, whereas the swapping
matrix needs to be updated every iteration, since
once the alignment is updated, the possible viola-
tions will also change.

If a source word ei is aligned to the empty word,
we set Mi,j = −1,∀j. The swapping matrix does
not need to be modiﬁed in this case because the
swapping operator will not introduce new links.

Because the cells that can lead to violations are
set to negative, the operators will never be picked
when updating the center alignments. This en-
sures the consistency of the ﬁnal center alignment.
3.2 Count Collection

After ﬁnding the center alignment, we need to
collect counts from neighbor alignments so that
the M-step can normalize the counts to produce
the model parameters for the next step.
In this
stage, we want to make sure all the inconsistent
alignments in the neighbor set of the center align-
ment be ruled out from the sufﬁcient statistics, i.e.
have zero probability. Similar to the constrained
hill climbing algorithm, we can manipulate the
moving/swapping matrices to effectively exclude
inconsistent alignments. Since the original count
collection algorithm depends only on moving and
swapping matrices, we just need to bypass all the
cells which hold negative values, i.e. represent in-
consistent alignments.

We can also view the algorithm as forcing
the posteriors of inconsistent alignments to zero,
and therefore increase the posteriors of consistent
alignments. When no constraint is given, the algo-

rithm falls back to conventional EM, and when all
the alignments are known, the algorithm becomes
fully supervised. And if the alignment quality
can be improved if high-precision partial align-
ment links is given as constraints. In (Gao et al.,
2010) we experimented with using a dictionary to
generate such constraints, and in (Gao and Vogel,
2010) we experimented with manual word align-
ments from Mechanical Turk. And in this paper
we try to use an alternative method that uses a dis-
criminative aligner and link ﬁltering to generate
such constraints.

4 Discriminative Aligner and Link

The

Filtering
We employ the CRF-based discriminative word
aligner described in (Niehues and Vogel, 2008).
The aligner can use a variety of knowledge
sources as features, such as: the fertility and lex-
ical translation model parameters from GIZA++,
the Viterbi alignment from both source-to-target
and target-to-source directions. It can also make
use of ﬁrst-order features which model the depen-
dency between different links, the Parts-of-Speech
tagging features, the word form similarity feature
and the phrase features. In this paper we use all
the features mentioned above except the POS and
phrase features.
aligner

trained using a belief-
propagation (BP) algorithm, and can be optimized
to maximize likelihood or directly optimize to-
wards AER on a tuning set. The aligner outputs
conﬁdence scores for alignment
links, which
allows us to control
the precision and recall
rate of the resulting alignment. Guzman et al.
(2009) experimented with different alignments
produced by adjusting the ﬁltering threshold for
the alignment links and showed that they could
get high-precision-low-recall alignments by hav-
ing a higher threshold. Therefore, we replicated
the conﬁdence ﬁltering procedures to produce
the partial alignment constraints. Afterwards
we iterate by putting the partial alignments back
to the constrained word alignment algorithm
described in section 3.

is

Although the discriminative aligner performs
well in supplying high precision constraints, it
does not model
the null alignment explicitly.

2005(cid:5284)

the

2005(cid:5284)

the

2005(cid:5284)

the

(cid:11444)

summer

of

(cid:3903)(cid:3929)

2005
Manual Alignment Link

(cid:11444)

(cid:3903)(cid:3929)

summer

Of

2005

(cid:11444)

(cid:3903)(cid:3929)

summer

of

2005

Seed Alignment Î Consistent Alignment Î Center Alignment

(a)

(b)                    

(c)

353

Num. of
Sentences

Num. of Words
Target
Source

Num. of
Links

Ch-En
Ar-En

21,863
29,876

424,683
630,101

524,882
821,938

687,247
830,349

Table 1: Corpus statistics of the manual aligned
corpora

Threshold

P

Ch-En

Ar-En

0.6
0.7
0.8
0.9
0.6
0.7
0.8
0.9

71.30
75.24
85.66
93.70
72.35
77.55
80.07
83.74

R

58.12
54.03
44.19
37.95
59.87
55.58
50.89
44.16

AER

35.96
37.11
41.70
45.98
34.48
35.25
37.77
42.17

Table 2: The qualities of the constraints

Hence we are currently not able to provide source
word to empty word alignment constraints which
have been proven to be effective in improving the
alignment quality in (Gao et al., 2010). Due to
space limitation, please refer to: (Niehues and Vo-
gel, 2008; Guzman et al., 2009) for further details
of the aligner and link ﬁltering, respectively.
5 Experiments

To validate the proposed training scheme, we
performed two sets of experiments. First of all,
we experimented with a small manually aligned
corpus to evaluate the ability of the algorithm to
improve the AER. The experiment was performed
on Chinese to English and Arabic to English tasks.
Secondly, we experimented with a moderate size
corpus and performed translation tasks to observe
the effects in translation quality.
5.1 Effects on AER

In order to measure the effects of EMDC in
alignment quality, we experimented with Chinese-
English and Arabic-English manually aligned cor-
pora. The statistics of these sets are shown in Ta-
ble 1. We split the data into two fragments, the
ﬁrst 100 sentences (Set A) and the remaining (Set
B). We trained generative IBM models using the
Set B, and tuned the discriminative aligner using
the Set A. We evaluated the AER on Set B, but in
any of the training steps the manual alignments of

Set B were not used.

In each iteration of EDMC, we load the model
parameters from the previous step and continue
training using the new constraints. Therefore, it is
important to compare the performance of contin-
uous training against an unconstrained baseline,
because variation in alignment quality could be
attributed to either the effect of more training it-
erations or to the effect of semi-supervised train-
ing scheme.
In Figures 3 and 4 we show the
alignment quality for each iteration. Iteration 0 is
the baseline, which comes from standard GIZA++
training1. The grey dash curves represent uncon-
strained Model 4 training, and the curves with
start, circle, cross and diamond markers are con-
strained EM alignments with 0.6, 0.7, 0.8 and
0.9 ﬁltering thresholds respectively. As we can
see from the results, when comparing only the
mono-directional trainings, the alignment quali-
ties improve over the unconstrained training in all
the metrics (precision, recall and AER). From Ta-
ble 2, we observe that the quality of discrimina-
tive aligner also improved. Nonetheless, when
we consider the heuristically symmetrized align-
ment2, we observe mixed results. For instance,
for the Chinese-English case we observe that AER
improves over iterations, but this is the result of
a increasingly higher recall rate in detriment of
precision. Ayan and Dorr (2006) pointed out
that grow-diag-ﬁnal symmetrization tends to out-
put alignments with high recall and low precision.
However this does not fully explain the tendency
we observed between iterations. The character-
istics of the alignment modiﬁed by EDMC that
lead to larger improvements in mono-directional
trainings but a precision drop with symmetrization
heuristics needs to be addressed in future work.

Another observation is how the ﬁltering thresh-
olds affect the results. As we can see in Table 3,
for Chinese to English word alignment, the largest
gain on the alignment quality is observed when
the threshold was set to 0.8, while for Arabic to
English, the threshold of 0.7 or 0.6 works better.
Table 2 shows the precision, recall, and AER of
the constraint links used in the constrained EM al-

1We run 5, 5, 3, 3 iterations of Model 1, HMM, Model 3

and Model 4 respectively.

2We used grow-diag-ﬁnal-and

354

(a) Arabic-English

(b) English-Arabic

(c) Heuristically-symmetrized

Figure 3: Alignment qualities of each iteration for Arabic-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 ﬁltering thresholds respectively.

Source-Target
AER

R

P

Target-Source
AER

R

P

Heuristic

P

R

AER

Discriminative
AER
P

R

Ch

Ar

BL
68.22 46.88 44.43
NC +0.73 +0.71 -0.74
+2.17 +2.28 -2.32
0.6
0.7
+2.57 +2.32 -2.48
+3.78 +3.27 -3.55
0.8
+0.98 +1.13 -1.11
0.9
BL
58.41 50.42 45.88
NC +2.98 +2.92 -2.96
+6.69 +8.02 -7.47
0.6
+8.38 +7.93 -8.16
0.7
+6.48 +6.27 -6.39
0.8
0.9
+4.02 +4.07 -4.07

65.35 55.05 40.25
+1.14 +1.14 -1.15
+1.17 +2.51 -1.97
+1.94 +2.34 -2.19
+2.94 +3.32 -3.18
+1.48 +1.85 -1.71
59.08 64.84 38.17
+1.40 +2.06 -1.70
+3.45 +6.70 -4.90
+3.65 +5.26 -4.38
+2.18 +3.54 -2.80
+1.70 +3.10 -2.33

69.15 57.47 37.23
+0.06 +1.07 -0.66
-0.64 +2.65 -1.27
-0.34 +2.30 -1.20
-0.52 +3.32 -1.70
-0.55 +1.94 -0.90
60.35 66.99 36.50
+0.97 +2.14 -1.49
+2.62 +4.71 -3.55
+2.83 +4.70 -3.67
+1.81 +3.81 -2.70
+0.62 +3.82 -2.03

67.45 59.77 36.62
+0.15 +0.64 -0.42
-0.39 +1.89 -0.87
-0.28 +1.60 -0.76
+0.69 +0.14 -0.89
-0.58 +1.45 -0.54
68.93 63.94 33.66
-0.87 +2.37 -0.83
+0.58 -0.55 +0.03
+2.46 -0.42 -0.88
+1.67 +2.30 -2.01
+1.33 +2.70 -2.06

Table 3: Improvement on word alignment quality on small corpus after 8 iterations. BL stands for
baseline, and NC represents unconstrained Model 4 training, and 0.9, 0.8, 0.7, 0.6 are the thresholds
used in alignment link ﬁltering.

gorithm, the numbers are averaged across all iter-
ations, the actual numbers of each iteration only
have small differences. Although one might ex-
pect that the quality of resulting alignment from
constrained EM be proportional to the quality of

constraints, from the numbers in Table 2 and 3,
we are not able to induce a clear relationship be-
tween them, and it could be language- or corpus-
dependent. However, in practice we nonetheless
use a held-out test set to tune this parameter. The

%

66

64

62

60

 

0

Precision

 

Unconstrained
Filtered 0.6
Filtered 0.7
Filtered 0.8
Filtered 0.9

2

4

6

8

60

58

56

54

52

50
0

Recall

AER

46

44

42

40

38

2

4

6

8

0

2

4

6

8

Precision

Recall

AER

62

61

%

60

 

59
0

2

4

6

 

72

70

68

66

64
0

8

39

38

37

36

35

34

2

4

6

33
0

8

2

4

6

8

Precision

Recall

AER

%

63

62.5

62

61.5

61

60.5
0

 

2

4

6

 

72

70

68

66
0

8

37

36

35

34

33

2

4

6

32
0

8

2

4

6

8

355

(a) Chinese-English

(b) English-Chinese

(c) Heuristically-symmetrized

Figure 4: Alignment qualities of each iteration for Chinese-English word alignment task. The grey dash
curves represent unconstrained Model 4 training, and the curves with star, circle, cross and diamond
markers are constrained EM alignments with 0.6, 0.7, 0.8 and 0.9 ﬁltering thresholds respectively.

Ch-En
R

AER

P

En-Ch
R

AER

P

Heuristic

P

R

AER

Discriminative
AER
P

R

BL 73.51 50.14 40.38
NC 73.23 50.38 40.30
0.8
76.27 52.90 37.53

68.82 57.66 37.31
68.30 58.00 37.27
70.26 60.26 35.11

72.98 60.23 34.01
72.39 60.99 33.80
72.75 63.49 32.19

72.10 61.63 33.55
72.07 61.81 33.45
72.64 63.29 32.35

Table 4: Improvement on word alignment quality on moderate-size corpus, where BL and NC represents
baseline and non-constrained Model 4 training

relationship between quality of constraints and
alignment results is an interesting topic for future
research.
5.2 Effects on translation quality

In this experiment we run the whole machine
translation pipeline and evaluate the system on
BLEU score. We used the corpus LDC2006G05
which contains 25 million words as training set,
the same discriminative tuning set as previously
used (100 sentence pairs) and the remaining
21,763 sentence pairs from the hand-aligned cor-
pus of the previous experiment are held-out test
set for alignment qualities. A 4-gram language

model trained from English GigaWord V1 and V2
corpus was used. The AER scores on the held-
out test set are also provided for every iteration.
Based on the observation in last experiment, we
adopt the ﬁltering threshold of 0.8.

Similar to previous experiment, the heuristi-
cally symmetrized alignments have lower preci-
sions than their EMDC counterparts, however the
gaps are smaller as shown in Table 4. We observe
2.85 and 2.21 absolute AER reduction on two di-
rections, after symmetrization the gain on AER
is 1.82. Continuing Model 4 training appears to
have minimal effect on AER, and the improve-

Precision

Recall

AER

72

71

%

70

69

 

0

 

51

50

49

48

47

46
0

8

Unconstrained
Filtered 0.6
Filtered 0.7
Filtered 0.8
Filtered 0.9

2

4

6

45

44

43

42

41

2

4

6

40
0

8

2

4

6

8

Precision

Recall

AER

%

68

67.5

67

66.5

66

65.5
0

 

2

4

6

 

59

58

57

56

55
0

8

41

40

39

38

2

4

6

37
0

8

2

4

6

8

Precision

Recall

AER

%

69.2

69

68.8

68.6

 

0

2

4

6

 

61

60

59

58

57
0

8

38

37

36

2

4

6

35
0

8

2

4

6

8

356

I M

0 G
1 D
G
2 D
G
3 D
G
4 D
G
5 D
G

NIST

GALE

mt06 mt02 mt03 mt04 mt05 mt08

ain db-nw db-wb dd-nw dd-wb

aia

31.00 31.80 29.89 32.63 29.33 24.24
30.65 31.60 30.04 32.89 29.34 24.52
31.35 31.91 30.35 32.75 29.40 24.16
31.61 32.31 30.40 33.06 29.49 24.11
31.14 31.94 30.42 32.86 29.49 24.15
31.29 32.39 30.28 33.19 29.60 24.41
30.94 31.95 30.15 32.71 29.38 24.22
30.80 32.04 30.51 33.24 29.49 24.61
30.68 31.81 30.33 33.05 29.28 24.41
30.93 31.89 29.96 32.89 29.37 24.50
31.16 32.28 30.72 33.30 29.83 24.30

26.92
0.12 27.43
0.15 27.39
0.33 28.17
0.20 27.31
0.43 27.64
0.12 27.63
0.46 27.61
0.26 27.20
0.17 27.75
0.51 27.32

24.48
24.72
24.50
24.42
24.51
25.32
24.61
25.27
24.79
24.50
25.05

28.44
28.32
28.22
28.58
27.50
28.55
28.80
28.72
28.43
29.05
28.60

24.26
24.30
24.60
24.36
24.02
24.71
25.05
24.98
24.50
24.90
25.44

0.14
0.15
0.34
0.03
0.47
0.29
0.53
0.24
0.33
0.54

Table 5: Improvement on translation alignment quality on moderate-size corpus, The column ain shows
the average improvement of BLEU scores for all NIST test sets (excluding the tuning set MT06), and
column aia is the average improvement on all unseen test sets. The column M indicates the alignment
source, G means the alignment comes from generative aligner, and D means discriminative aligner
respectively. The number of iterations is shown in column I.

ment mainly comes from the constraints.

In the experiment, we use the Moses toolkit to
extract phrases, tune parameters and decode. We
use the NIST MT06 test set as the tuning set,
NIST MT02-05 and MT08 as unseen test sets.
We also include results for four additional unseen
test sets used in GALE evaluations: DEV07-Dev
newswire part (dd-nw, 278 sentences) and We-
blog part (dd-wb, 345 sentences), Dev07-Blind
newswire part (db-nw, 276 sentences and Weblog
part (db-wb, 312 sentences). Table 5 presents the
average improvement on BLEU scores in each it-
eration. As we can see from the results, in all iter-
ations we got improvement on BLEU scores, and
the largest gain we have gotten is on the ﬁfth it-
eration, which has 0.51 average improvement on
ﬁve NIST test sets, and 0.54 average improvement
across all nine test sets.
6 Conclusion

In this paper we presented a novel training
scheme for word alignment task called EMDC.
We also presented an extension of GIZA++ that
can perform constrained EM training. By inte-
grating it with a CRF-based discriminative word
aligner and alignment link ﬁltering, we can im-
prove the alignment quality of both aligners itera-
tively. We experimented with small-size Chinese-
English and Arabic English and moderate-size
Chinese-English word alignment tasks, and ob-

served in all four mono-directional alignments
more than 3% absolute reduction on AER, with
the largest improvement being 8.16% absolute on
Arabic-to-English comparing to the baseline, and
5.90% comparing to Model 4 training with the
same numbers of iterations. On a moderate-size
Chinese-to-English tasks we also evaluated the
impact of the improved alignment on translation
quality across nine test sets. The 2% absolute
AER reduction resulted in 0.5 average improve-
ment on BLEU score.

Observations on the results raise several inter-
esting questions for future research, such as 1)
What is the relationship between the precision of
the constraints and the quality of resulting align-
ments after iterations, 2) The effect of using dif-
ferent discriminative aligners, 3) Using aligners
that explicitly model empty words and null align-
ments to provide additional constraints. We will
continue exploration on these directions.

The extended GIZA++ is released to the re-
search community as a branch of MGIZA++ (Gao
and Vogel, 2008), which is available online3.
Acknowledgement

This work is supported by NSF CluE Project

(NSF 08-560) and DARPA GALE project.

3Accessible

the URL:
http://sourceforge.net/projects/mgizapp/

on Source Forge, with

357

References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt.
In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 9–16.

Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random ﬁelds.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 65–72.

Brown, Peter F., Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation.
In Computational Linguistics,
volume 19(2), pages 263–331.

Guzman, Francisco, Qin Gao, and Stephan Vogel.
2009. Reassessment of the role of phrase extrac-
tion in pbsmt. In The twelfth Machine Translation
Summit.

Huang, Fei.

2009. Conﬁdence measure for word
alignment. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 932–940.

Ittycheriah, Abraham and Salim Roukos. 2005. A
maximum entropy word aligner for arabic-english
machine translation. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 89–96.

Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In ACL ’05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 459–466.

Callison-Burch, C., D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-
and sentence-aligned parallel corpora. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 175–183.

Moore, Robert C.

2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 81–88.

Niehues, Jan. and Stephan. Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 18–25.

Och, Franz Joseph and Hermann Ney.

2003. A
systematic comparison of various statistical align-
ment models.
In Computational Linguistics, vol-
ume 1:29, pages 19–51.

Taskar, Ben, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 73–80.

Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM based word alignment in statis-
tical machine translation. In Proceedings of 16th In-
ternational Conference on Computational Linguis-
tics), pages 836–841.

Fraser, Alexander and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 769–776.

Fraser, Alexander and Daniel Marcu.

2007. Get-
ting the structure right for word alignment: LEAF.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 51–60.

Gao, Qin and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the ACL 2008 Software Engineering, Testing, and
Quality Assurance Workshop, pages 49–57.

Gao, Qin and Stephan Vogel. 2010. Consensus ver-
sus expertise : A case study of word alignment with
mechanical turk. In NAACL 2010 Workshop on Cre-
ating Speech and Language Data With Mechanical
Turk, pages 30–34.

Gao, Qin, Nguyen Bach, and Stephan Vogel. 2010.
A semi-supervised word alignment algorithm with
partial manual alignments.
In In Proceedings of
the ACL 2010 joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR (ACL-2010
WMT).

