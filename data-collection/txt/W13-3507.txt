



















































Spectral Learning of Refinement HMMs


Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Spectral Learning of Refinement HMMs

Karl Stratos1 Alexander M. Rush2 Shay B. Cohen1 Michael Collins1

1Department of Computer Science, Columbia University, New-York, NY 10027, USA
2MIT CSAIL, Cambridge, MA, 02139, USA

{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu

Abstract

We derive a spectral algorithm for learn-
ing the parameters of a refinement HMM.
This method is simple, efficient, and can
be applied to a wide range of supervised
sequence labeling tasks. Like other spec-
tral methods, it avoids the problem of lo-
cal optima and provides a consistent esti-
mate of the parameters. Our experiments
on a phoneme recognition task show that
when equipped with informative feature
functions, it performs significantly better
than a supervised HMM and competitively
with EM.

1 Introduction

Consider the task of supervised sequence label-
ing. We are given a training set where the j’th
training example consists of a sequence of ob-
servations x(j)1 ...x

(j)
N paired with a sequence of

labels a(j)1 ...a
(j)
N and asked to predict the cor-

rect labels on a test set of observations. A
common approach is to learn a joint distribu-
tion over sequences p(a1 . . . aN , x1 . . . xN ) as a
hidden Markov model (HMM). The downside of
HMMs is that they assume each label ai is inde-
pendent of labels before the previous label ai−1.
This independence assumption can be limiting,
particularly when the label space is small. To re-
lax this assumption we can refine each label ai
with a hidden state hi, which is not observed in
the training data, and model the joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This re-
finement HMM (R-HMM), illustrated in figure 1,
is able to propagate information forward through
the hidden state as well as the label.

Unfortunately, estimating the parameters of an
R-HMM is complicated by the unobserved hid-
den variables. A standard approach is to use the
expectation-maximization (EM) algorithm which

a1, h1 a2, h2 aN , hN

x1 x2 xN

(a)

a1 a2 aN

h1 h2 hN

x1 x2 xN

(b)

Figure 1: (a) An R-HMM chain. (b) An equivalent
representation where labels and hidden states are
intertwined.

has no guarantee of finding the global optimum of
its objective function. The problem of local op-
tima prevents EM from yielding statistically con-
sistent parameter estimates: even with very large
amounts of data, EM is not guaranteed to estimate
parameters which are close to the “correct” model
parameters.

In this paper, we derive a spectral algorithm for
learning the parameters of R-HMMs. Unlike EM,
this technique is guaranteed to find the true param-
eters of the underlying model under mild condi-
tions on the singular values of the model. The al-
gorithm we derive is simple and efficient, relying
on singular value decomposition followed by stan-
dard matrix operations.

We also describe the connection of R-HMMs
to L-PCFGs. Cohen et al. (2012) present a spec-
tral algorithm for L-PCFG estimation, but the
naı̈ve transformation of the L-PCFG model and
its spectral algorithm to R-HMMs is awkward and
opaque. We therefore work through the non-trivial
derivation the spectral algorithm for R-HMMs.

We note that much of the prior work on spec-
tral algorithms for discrete structures in NLP has
shown limited experimental success for this fam-
ily of algorithms (see, for example, Luque et al.,
2012). Our experiments demonstrate empirical

56



success for the R-HMM spectral algorithm. The
spectral algorithm performs competitively with
EM on a phoneme recognition task, and is more
stable with respect to the number of hidden states.

Cohen et al. (2013) present experiments with a
parsing algorithm and also demonstrate it is com-
petitive with EM. Our set of experiments comes as
an additional piece of evidence that spectral algo-
rithms can function as a viable, efficient and more
principled alternative to the EM algorithm.

2 Related Work

Recently, there has been a surge of interest in spec-
tral methods for learning HMMs (Hsu et al., 2012;
Foster et al., 2012; Jaeger, 2000; Siddiqi et al.,
2010; Song et al., 2010). Like these previous
works, our method produces consistent parameter
estimates; however, we estimate parameters for a
supervised learning task. Balle et al. (2011) also
consider a supervised problem, but our model is
quite different since we estimate a joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed
to a conditional distribution and use feature func-
tions over both the labels and observations of the
training data. These feature functions also go be-
yond those previously employed in other spectral
work (Siddiqi et al., 2010; Song et al., 2010). Ex-
periments show that features of this type are cru-
cial for performance.

Spectral learning has been applied to related
models beyond HMMs including: head automata
for dependency parsing (Luque et al., 2012),
tree-structured directed Bayes nets (Parikh et al.,
2011), finite-state transducers (Balle et al., 2011),
and mixture models (Anandkumar et al., 2012a;
Anandkumar et al., 2012b).

Of special interest is Cohen et al. (2012), who
describe a derivation for a spectral algorithm for
L-PCFGs. This derivation is the main driving
force behind the derivation of our R-HMM spec-
tral algorithm. For work on L-PCFGs estimated
with EM, see Petrov et al. (2006), Matsuzaki et al.
(2005), and Pereira and Schabes (1992). Petrov
et al. (2007) proposes a split-merge EM procedure
for phoneme recognition analogous to that used in
latent-variable parsing.

3 The R-HMM Model

We decribe in this section the notation used
throughout the paper and the formal details of R-
HMMs.

3.1 Notation
We distinguish row vectors from column vectors
when such distinction is necessary. We use a
superscript > to denote the transpose operation.
We write [n] to denote the set {1, 2, . . . , n} for
any integer n ≥ 1. For any vector v ∈ Rm,
diag(v) ∈ Rm×m is a diagonal matrix with en-
tries v1 . . . vm. For any statement S , we use [[S]]
to refer to the indicator function that returns 1 if S
is true and 0 otherwise. For a random variable X ,
we use E[X] to denote its expected value.

A tensor C ∈ Rm×m×m is a set of m3 val-
ues Ci,j,k for i, j, k ∈ [m]. Given a vector v ∈
Rm, we define C(v) to be the m × m matrix
with [C(v)]i,j =

∑
k∈[m]Ci,j,kvk. Given vectors

x, y, z ∈ Rm, C = xy>z> is anm×m×m tensor
with [C]i,j,k = xiyjzk.

3.2 Definition of an R-HMM
An R-HMM is a 7-tuple 〈l,m, n, π, o, t, f〉 for in-
tegers l,m, n ≥ 1 and functions π, o, t, f where
• [l] is a set of labels.

• [m] is a set of hidden states.

• [n] is a set of observations.

• π(a, h) is the probability of generating a ∈
[l] and h ∈ [m] in the first position in the
labeled sequence.

• o(x|a, h) is the probability of generating x ∈
[n], given a ∈ [l] and h ∈ [m].

• t(b, h′|a, h) is the probability of generating
b ∈ [l] and h′ ∈ [m], given a ∈ [l] and
h ∈ [m].

• f(∗|a, h) is the probability of generating the
stop symbol ∗, given a ∈ [l] and h ∈ [m].

See figure 1(b) for an illustration. At any time step
of a sequence, a label a is associated with a hidden
state h. By convention, the end of an R-HMM
sequence is signaled by the symbol ∗.

For the subsequent illustration, let N be the
length of the sequence we consider. A full se-
quence consists of labels a1 . . . aN , observations
x1 . . . xN , and hidden states h1 . . . hN . The model
assumes

p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = π(a1, h1)×
N∏

i=1

o(xi|ai, hi)×
N−1∏

i=1

t(ai+1, hi+1|ai, hi)× f(∗|aN , hN )

57



Input: a sequence of observations x1 . . . xN ; operators〈
Cb|a, C∗|a, c1a, c

a
x

〉

Output: µ(a, i) for all a ∈ [l] and i ∈ [N ]

[Forward case]

• α1a ← c1a for all a ∈ [l].

• For i = 1 . . . N − 1

αi+1b ←
∑

a∈[l]
Cb|a(caxi)× α

i
a for all b ∈ [l]

[Backward case]

• βN+1a ← C∗|a(caxN ) for all a ∈ [l]

• For i = N . . . 1

βia ←
∑

b∈[l]
βi+1b × Cb|a(caxi) for all a ∈ [l]

[Marginals]

• µ(a, i)← βia × αia for all a ∈ [l], i ∈ [N ]

Figure 2: The forward-backward algorithm

A skeletal sequence consists of labels a1 . . . aN
and observations x1 . . . xN without hidden states.
Under the model, it has probability

p(a1 . . . aN , x1 . . . xN )

=
∑

h1...hN

p(a1 . . . aN , x1 . . . xN , h1 . . . hN )

An equivalent definition of an R-HMM is
given by organizing the parameters in matrix
form. Specifically, an R-HMM has parameters〈
πa, oax, T

b|a, fa
〉

where πa ∈ Rm is a column
vector, oax is a row vector, T

b|a ∈ Rm×m is a ma-
trix, and fa ∈ Rm is a row vector, defined for all
a, b ∈ [l] and x ∈ [n]. Their entries are set to
• [πa]h = π(a, h) for h ∈ [m]
• [oax]h = o(x|a, h) for h ∈ [m]
• [T b|a]h′,h = t(b, h′|a, h) for h, h′ ∈ [m]
• [fa]h = f(∗|a, h) for h ∈ [m]

4 The Forward-Backward Algorithm

Given an observation sequence x1 . . . xN , we want
to infer the associated sequence of labels under
an R-HMM. This can be done by computing the
marginals of x1 . . . xN

µ(a, i) =
∑

a1...aN : ai=a

p(a1 . . . aN , x1 . . . xN )

for all labels a ∈ [l] and positions i ∈ [N ]. Then
the most likely label at each position i is given by

a∗i = arg max
a∈[l]

µ(a, i)

The marginals can be computed using a tensor
variant of the forward-backward algorithm, shown
in figure 2. The algorithm takes additional quanti-
ties

〈
Cb|a, C∗|a, c1a, c

a
x

〉
called the operators:

• Tensors Cb|a ∈ Rm×m×m for a, b ∈ [l]
• Tensors C∗|a ∈ R1×m×m for a ∈ [l]
• Column vectors c1a ∈ Rm for a ∈ [l]
• Row vectors cax ∈ Rm for a ∈ [l] and x ∈ [n]

The following proposition states that these opera-
tors can be defined in terms of the R-HMM param-
eters to guarantee the correctness of the algorithm.
Proposition 4.1. Given an R-HMM with param-
eters

〈
πa, oax, T

b|a, fa
〉
, for any vector v ∈ Rm

define the operators:

Cb|a(v) = T b|adiag(v) c1a = π
a

C∗|a(v) = fadiag(v) cax = o
a
x

Then the algorithm in figure 2 correctly computes
marginals µ(a, i) under the R-HMM.
The proof is an algebraic verification and deferred
to the appendix. Note that the running time of the
algorithm as written is O(l2m3N).1

Proposition 4.1 can be generalized to the fol-
lowing theorem. This theorem implies that the op-
erators can be linearly transformed by some invert-
ible matrices as long as the transformation leaves
the embedded R-HMM parameters intact. This
observation is central to the derivation of the spec-
tral algorithm which estimates the linearly trans-
formed operators but not the actual R-HMM pa-
rameters.
Theorem 4.1. Given an R-HMM with parameters〈
πa, oax, T

b|a, fa
〉
, assume that for each a ∈ [l] we

have invertible m ×m matrices Ga and Ha. For
any vector v ∈ Rm define the operators:

Cb|a(v) = GbT b|adiag(vHa)(Ga)−1 c1a = G
aπa

C∗|a(v) = fadiag(vHa)(Ga)−1 cax = o
a
x(H

a)−1

Then the algorithm in figure 2 correctly computes
marginals µ(a, i) under the R-HMM.
The proof is similar to that of Cohen et al. (2012).

1We can reduce the complexity to O(l2m2N) by pre-
computing the matricesCb|a(cax) for all a, b ∈ [l] and x ∈ [n]
after parameter estimation.

58



5 Spectral Estimation of R-HMMs

In this section, we derive a consistent estimator for
the operators

〈
Cb|a, C∗|a, c1a, c

a
x

〉
in theorem 4.1

through the use of singular-value decomposition
(SVD) followed by the method of moments.

Section 5.1 describes the decomposition of the
R-HMM model into random variables which are
used in the final algorithm. Section 5.2 can be
skimmed through on the first reading, especially
if the reader is familiar with other spectral algo-
rithms. It includes a detailed account of the deriva-
tion of the R-HMM algorithm.

For a first reading, note that an R-HMM se-
quence can be seen as a right-branching L-PCFG
tree. Thus, in principle, one can convert a se-
quence into a tree and run the inside-outside algo-
rithm of Cohen et al. (2012) to learn the parame-
ters of an R-HMM. However, projecting this trans-
formation into the spectral algorithm for L-PCFGs
is cumbersome and unintuitive. This is analo-
gous to the case of the Baum-Welch algorithm for
HMMs (Rabiner, 1989), which is a special case of
the inside-outside algorithm for PCFGs (Lari and
Young, 1990).

5.1 Random Variables
We first introduce the random variables un-
derlying the approach then describe the opera-
tors based on these random variables. From
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an R-
HMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN )
and choose a time step i uniformly at random from
[N ]. The random variables are then defined as

X = xi

A1 = ai and A2 = ai+1 (if i = N , A2 = ∗)
H1 = hi and H2 = hi+1
F1 = (ai . . . aN , xi . . . xN ) (future)
F2 = (ai+1 . . . aN , xi+1 . . . xN ) (skip-future)
P = (a1 . . . ai, x1 . . . xi−1) (past)
R = (ai, xi) (present)
D = (a1 . . . aN , x1 . . . xi−1, xi+1 . . . xN ) (destiny)
B = [[i = 1]]

Figure 3 shows the relationship between the ran-
dom variables. They are defined in such a way
that the future is independent of the past and the
present is independent of the destiny conditioning
on the current node’s label and hidden state.

Next, we require a set of feature functions over
the random variables.

• φ maps F1, F2 to φ(F1), φ(F2) ∈ Rd1 .

a1 ai−1 ai ai+1 aN

x1 xi−1 xi xi+1 xN

P

F1

F2

(a)

a1 ai−1 ai ai+1 aN

x1 xi−1 xi xi+1 xN

D R

(b)

Figure 3: Given an R-HMM sequence, we define
random variables over observed quantities so that
conditioning on the current node, (a) the future F1
is independent of the past P and (b) the present R
is independent of the density D.

• ψ maps P to ψ(P ) ∈ Rd2 .
• ξ maps R to ξ(R) ∈ Rd3 .
• υ maps D to υ(D) ∈ Rd4 .

We will see that the feature functions should be
chosen to capture the influence of the hidden
states. For instance, they might track the next la-
bel, the previous observation, or important combi-
nations of labels and observations.

Finally, we require projection matrices

Φa ∈ Rm×d1 Ψa ∈ Rm×d2

Ξa ∈ Rm×d3 Υa ∈ Rm×d4

defined for all labels a ∈ [l]. These matrices
will project the feature vectors of φ, ψ, ξ, and υ
from (d1, d2, d3, d4)-dimensional spaces to an m-
dimensional space. We refer to this reduced di-
mensional representation by the following random
variables:

F 1 = Φ
A1φ(F1) (projected future)

F 2 = Φ
A2φ(F2) (projected skip-future: if i = N , F 2 = 1)

P = ΨA1ψ(P ) (projected past)

R = ΞA1ξ(R) (projected present)

D = ΥA1υ(D) (projected destiny)

Note that they are all vectors in Rm.

59



5.2 Estimation of the Operators

Since F 1, F 2, P , R, and D do not involve hid-
den variables, the following quantities can be di-
rectly estimated from the training data of skeletal
sequences. For this reason, they are called observ-
able blocks:

Σa = E[F 1P
>|A1 = a] ∀a ∈ [l]

Λa = E[R D>|A1 = a] ∀a ∈ [l]
Db|a = E[[[A2 = b]]F 2P

>R>|A1 = a] ∀a, b ∈ [l]
dax = E[[[X = x]]D

>|A1 = a] ∀a ∈ [l], x ∈ [n]

The main result of this paper is that under cer-
tain conditions, matrices Σa and Λa are invert-
ible and the operators

〈
Cb|a, C∗|a, c1a, c

a
x

〉
in the-

orem 4.1 can be expressed in terms of these ob-
servable blocks.

Cb|a(v) = Db|a(v)(Σa)−1 (1)

C∗|a(v) = D∗|a(v)(Σa)−1 (2)

cax = d
a
x(Λ

a)−1 (3)

c1a = E[[[A1 = a]]F 1|B = 1] (4)

To derive this result, we use the following defini-
tion to help specify the conditions on the expecta-
tions of the feature functions.

Definition. For each a ∈ [l], define matrices
Ia ∈ Rd1×m, Ja ∈ Rd2×m, Ka ∈ Rd3×m,W a ∈
Rd4×m by

[Ia]k,h = E[[φ(F1)]k|A1 = a,H1 = h]
[Ja]k,h = E[[ψ(P )]k|A1 = a,H1 = h]
[Ka]k,h = E[[ξ(R)]k|A1 = a,H1 = h]

[W a]k,h = E[[υ(D)]k|A1 = a,H1 = h]

In addition, let Γa ∈ Rm×m be a diagonal matrix
with [Γa]h,h = P (H1 = h|A1 = a).
We now state the conditions for the correctness of
Eq. (1-4). For each label a ∈ [l], we require that

Condition 6.1 Ia, Ja,Ka,W a have rank m.

Condition 6.2 [Γa]h,h > 0 for all h ∈ [m].

The conditions lead to the following proposition.

Proposition 5.1. Assume Condition 6.1 and 6.2
hold. For all a ∈ [l], define matrices

Ωa1 = E[φ(F1)ψ(P )
>|A1 = a] ∈ Rd1×d2

Ωa2 = E[ξ(R)υ(D)
>|A1 = a] ∈ Rd3×d4

Let ua1 . . . u
a
m ∈ Rd1 and va1 . . . vam ∈ Rd2 be the

top m left and right singular vectors of Ωa. Sim-
ilarly, let la1 . . . l

a
m ∈ Rd3 and ra1 . . . ram ∈ Rd4 be

the top m left and right singular vectors of Ψa.
Define projection matrices

Φa = [ua1 . . . u
a
m]
> Ψa = [va1 . . . v

a
m]
>

Ξa = [la1 . . . l
a
m]
> Υa = [ra1 . . . r

a
m]
>

Then the following m×m matrices

Ga = ΦaIa Ga = ΨaJa
Ha = ΞaKa Ha = ΥaW a

are invertible.

The proof resembles that of lemma 2 of Hsu et al.
(2012). Finally, we state the main result that shows〈
Cb|a, C∗|a, c1a, c

a
x

〉
in Eq. (1-4) using the projec-

tions from proposition 5.1 satisfy theorem 4.1. A
sketch of the proof is deferred to the appendix.

Theorem 5.1. Assume conditions 6.1 and 6.2
hold. Let 〈Φa,Ψa,Ξa,Υa〉 be the projection ma-
trices from proposition 5.1. Then the operators in
Eq. (1-4) satisfy theorem 4.1.

In summary, these results show that with the
proper selection of feature functions, we can con-
struct projection matrices 〈Φa,Ψa,Ξa,Υa〉 to ob-
tain operators

〈
Cb|a, C∗|a, c1a, c

a
x

〉
which satisfy

the conditions of theorem 4.1.

6 The Spectral Estimation Algorithm

In this section, we give an algorithm to estimate
the operators

〈
Cb|a, C∗|a, c1a, c

a
x

〉
from samples of

skeletal sequences. Suppose the training set con-
sists of M skeletal sequences (a(j), x(j)) for j ∈
[M ]. ThenM samples of the random variables can
be derived from this training set as follows

• At each j ∈ [M ], choose a position
ij uniformly at random from the positions
in (a(j), x(j)). Sample the random vari-
ables (X,A1, A2, F1, F2, P,R,D,B) using
the procedure defined in section 5.1.

This process yields M samples

(x(j), a
(j)
1 , a

(j)
2 , f

(j)
1 , f

(j)
2 , p

(j), r(j), d(j), b(j)) for j ∈ [M ]

Assuming (a(j), x(j)) are i.i.d. draws from
the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal se-
quences under an R-HMM, the tuples obtained
through this process are i.i.d. draws from the joint
PMF over (X,A1, A2, F1, F2, P,R,D,B).

60



Input: samples of (X,A1, A2, F1, F2, P,R,D,B); feature
functions φ, ψ, ξ, and υ; number of hidden states m
Output: estimates

〈
Ĉb|a, Ĉ∗|a, ĉ1a, ĉ

a
x

〉
of the operators

used in algorithm 2

[Singular Value Decomposition]

• For each label a ∈ [l], compute empirical estimates of

Ωa1 = E[φ(F1)ψ(P )
>|A1 = a]

Ωa2 = E[ξ(R)υ(D)
>|A1 = a]

and obtain their singular vectors via an SVD. Use
the top m singular vectors to construct projections〈

Φ̂a, Ψ̂a, Ξ̂a, Υ̂a
〉

.

[Sample Projection]

• Project (d1, d2, d3, d4)-dimensional samples of

(φ(F1), φ(F2), ψ(P ), ξ(R), υ(D))

with matrices
〈

Φ̂a, Ψ̂a, Ξ̂a, Υ̂a
〉

to obtain m-
dimensional samples of

(F 1, F 2, P ,R,D)

[Method of Moments]

• For each a, b ∈ [l] and x ∈ [n], compute empirical
estimates

〈
Σ̂a, Λ̂a, D̂b|a, d̂ax

〉
of the observable blocks

Σa = E[F 1P
>|A1 = a]

Λa = E[R D>|A1 = a]
Db|a = E[[[A2 = b]]F 2P

>R>|A1 = a]
dax = E[[[X = x]]D

>|A1 = a]

and also ĉ1a = E[[[A1 = a]]F 1|B = 1]. Finally, set

Ĉb|a(v)← D̂b|a(v)(Σ̂a)−1

Ĉ∗|a(v)← D̂∗|a(v)(Σ̂a)−1

ĉax ← d̂ax(Λ̂a)−1

Figure 4: The spectral estimation algorithm

The algorithm in figure 4 shows how to derive
estimates of the observable representations from
these samples. It first computes the projection
matrices 〈Φa,Ψa,Ξa,Υa〉 for each label a ∈ [l]
by computing empirical estimates of Ωa1 and Ω

a
2

in proposition 5.1, calculating their singular vec-
tors via an SVD, and setting the projections in
terms of these singular vectors. These projection
matrices are then used to project (d1, d2, d3, d4)-

0 5 10 15 20 25 30
hidden states (m)

54.0

54.5

55.0

55.5

56.0

56.5

57.0

57.5

ac
cu

ra
cy

Spectral

EM

Figure 5: Accuracy of the spectral algorithm and
EM on TIMIT development data for varying num-
bers of hidden states m. For EM, the highest scor-
ing iteration is shown.

dimensional feature vectors
(
φ(f

(j)
1 ), φ(f

(j)
2 ), ψ(p

(j)), ξ(r(j)), υ(d(j))
)

down to m-dimensional vectors
(
f (j)
1
, f (j)

2
, p(j), r(j), d(j)

)

for all j ∈ [M ]. It then computes correlation
between these vectors in this lower dimensional
space to estimate the observable blocks which are
used to obtain the operators as in Eq. (1-4). These
operators can be used in algorithm 2 to compute
marginals.

As in other spectral methods, this estimation al-
gorithm is consistent, i.e., the marginals µ̂(a, i)
computed with the estimated operators approach
the true marginal values given more data. For
details, see Cohen et al. (2012) and Foster et al.
(2012).

7 Experiments

We apply the spectral algorithm for learning
R-HMMs to the task of phoneme recognition.
The goal is to predict the correct sequence of
phonemes a1 . . . aN for a given a set of speech
frames x1 . . . xN . Phoneme recognition is often
modeled with a fixed-structure HMM trained with
EM, which makes it a natural application for spec-
tral training.

We train and test on the TIMIT corpus of spoken
language utterances (Garofolo and others, 1988).
The label set consists of l = 39 English phonemes
following a standard phoneme set (Lee and Hon,
1989). For training, we use the sx and si utter-
ances of the TIMIT training section made up of

61



φ(F1) ai+1 × xi, ai+1, xi, np(ai . . . aN )
ψ(P ) (ai−1, xi−1), ai−1, xi−1, pp(a1 . . . ai)
ξ(R) xi
υ(D) ai−1 × xi−1, ai−1, xi−1, pp(a1 . . . ai),

pos(a1 . . . aN )

iy r r r r r r ow . . .. . .

pp b m e np

Figure 6: The feature templates for phoneme
recognition. The simplest features look only at the
current label and observation. Other features in-
dicate the previous phoneme type used before ai
(pp), the next phoneme type used after ai (np),
and the relative position (beginning, middle, or
end) of ai within the current phoneme (pos). The
figure gives a typical segment of the phoneme se-
quence a1 . . . aN

M = 3696 utterances. The parameter estimate is
smoothed using the method of Cohen et al. (2013).

Each utterance consists of a speech signal
aligned with phoneme labels. As preprocessing,
we divide the signal into a sequence of N over-
lapping frames, 25ms in length with a 10ms step
size. Each frame is converted to a feature repre-
sentation using MFCC with its first and second
derivatives for a total of 39 continuous features.
To discretize the problem, we apply vector quanti-
zation using euclidean k-means to map each frame
into n = 10000 observation classes. After pre-
processing, we have 3696 skeletal sequence with
a1 . . . aN as the frame-aligned phoneme labels and
x1 . . . xN as the observation classes.

For testing, we use the core test portion of
TIMIT, consisting of 192 utterances, and for de-
velopment we use 200 additional utterances. Ac-
curacy is measured by the percentage of frames
labeled with the correct phoneme. During infer-
ence, we calculate marginals µ for each label at
each position i and choose the one with the highest
marginal probability, a∗i = arg maxa∈[l] µ(a, i).

The spectral method requires defining feature
functions φ, ψ, ξ, and υ. We use binary-valued
feature vectors which we specify through features
templates, for instance the template ai × xi corre-
sponds to binary values for each possible label and
output pair (ln binary dimensions).

Figure 6 gives the full set of templates. These
feature functions are specially for the phoneme
labeling task. We note that the HTK baseline
explicitly models the position within the current

Method Accuracy
EM(4) 56.80
EM(24) 56.23
SPECTRAL(24), no np, pp, pos 55.45
SPECTRAL(24), no pos 56.56
SPECTRAL(24) 56.94

Figure 7: Feature ablation experiments on TIMIT
development data for the best spectral model (m =
24) with comparisons to the best EM model (m =
4) and EM with m = 24.

Method Accuracy
UNIGRAM 48.04
HMM 54.08
EM(4) 55.49
SPECTRAL(24) 55.82
HTK 55.70

Figure 8: Performance of baselines and spectral
R-HMM on TIMIT test data. Number of hidden
states m optimized on development data (see fig-
ure 5). The improvement of the spectral method
over the EM baseline is significant at the p ≤ 0.05
level (and very close to significant at p ≤ 0.01,
with a precise value of p ≤ 0.0104).

phoneme as part of the HMM structure. The spec-
tral method is able to encode similar information
naturally through the feature functions.

We implement several baseline for phoneme
recognition: UNIGRAM chooses the most likely
label, arg maxa∈[l] p(a|xi), at each position;
HMM is a standard HMM trained with maximum-
likelihood estimation; EM(m) is an R-HMM
with m hidden states estimated using EM; and
SPECTRAL(m) is an R-HMM with m hidden
states estimated with the spectral method de-
scribed in this paper. We also compare to HTK,
a fixed-structure HMM with three segments per
phoneme estimated using EM with the HTK
speech toolkit. See Young et al. (2006) for more
details on this method.

An important consideration for both EM and the
spectral method is the number of hidden states m
in the R-HMM. More states allow for greater label
refinement, with the downside of possible overfit-
ting and, in the case of EM, more local optima.
To determine the best number of hidden states, we
optimize both methods on the development set for
a range of m values between 1 to 32. For EM,

62



we run 200 training iterations on each value of m
and choose the iteration that scores best on the de-
velopment set. As the spectral algorithm is non-
iterative, we only need to evaluate the develop-
ment set once per m value. Figure 5 shows the
development accuracy of the two method as we
adjust the value of m. EM accuracy peaks at 4
hidden states and then starts degrading, whereas
the spectral method continues to improve until 24
hidden states.

Another important consideration for the spectral
method is the feature functions. The analysis sug-
gests that the best feature functions are highly in-
formative of the underlying hidden states. To test
this empirically we run spectral estimation with a
reduced set of features by ablating the templates
indicating adjacent phonemes and relative posi-
tion. Figure 7 shows that removing these features
does have a significant effect on development ac-
curacy. Without either type of feature, develop-
ment accuracy drops by 1.5%.

We can interpret the effect of the features in
a more principled manner. Informative features
yield greater singular values for the matrices Ωa1
and Ωa2, and these singular values directly affect
the sample complexity of the algorithm; see Cohen
et al. (2012) for details. In sum, good feature func-
tions lead to well-conditioned Ωa1 and Ω

a
2, which in

turn require fewer samples for convergence.
Figure 8 gives the final performance for the

baselines and the spectral method on the TIMIT
test set. For EM and the spectral method, we
use best performing model from the develop-
ment data, 4 hidden states for EM and 24 for
the spectral method. The experiments show that
R-HMM models score significantly better than a
standard HMM and comparatively to the fixed-
structure HMM. In training the R-HMM models,
the spectral method performs competitively with
EM while avoiding the problems of local optima.

8 Conclusion

This paper derives a spectral algorithm for the
task of supervised sequence labeling using an R-
HMM. Unlike EM, the spectral method is guar-
anteed to provide a consistent estimate of the pa-
rameters of the model. In addition, the algorithm
is simple to implement, requiring only an SVD
of the observed counts and other standard ma-
trix operations. We show empirically that when
equipped with informative feature functions, the

spectral method performs competitively with EM
on the task of phoneme recognition.

Appendix
Proof of proposition 4.1. At any time step i ∈ [N ] in the al-
gorithm in figure 2, for all label a ∈ [l] we have a column
vector αia ∈ Rm and a row vector βia ∈ Rm. The value of
these vectors at each index h ∈ [m] can be verified as

[αia]h =
∑

a1...ai,h1...hi:
ai=a,hi=h

p(a1 . . . ai, x1 . . . xi−1, h1 . . . hi)

[βia]h =∑

ai...aN ,hi...hN :
ai=a,hi=h

p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai, hi)

Thus βiaαia is a scalar equal to

∑

a1...aN ,h1...hN :
ai=a

p(a1 . . . aN , x1 . . . xN , h1 . . . hN )

which is the value of the marginal µ(a, i).

Proof of theorem 5.1. It can be verified that c1a = Gaπa. For
the others, under the conditional independence illustrated in
figure 3 we can decompose the observable blocks in terms of
the R-HMM parameters and invertible matrices

Σa = GaΓa(Ga)> Λa = HaΓa(Ha)>

Db|a(v) = GbT b|adiag(vHa)Γa(Ga)>

D∗|a(v) = fadiag(vHa)Γa(Ga)> dax = oaxΓa(Ha)>

using techniques similar to those sketched in Cohen et al.
(2012). By proposition 5.1, Σa and Λa are invertible, and
these observable blocks yield the operators that satisfy theo-
rem 4.1 when placed in Eq. (1-3).

References
A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K.

Liu. 2012a. Two svds suffice: Spectral decompositions
for probabilistic topic modeling and latent dirichlet allo-
cation. Arxiv preprint arXiv:1204.6703.

A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A
method of moments for mixture models and hidden
markov models. Arxiv preprint arXiv:1203.0683.

B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral
learning algorithm for finite state transducers. Machine
Learning and Knowledge Discovery in Databases, pages
156–171.

S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2012. Spectral learning of latent-variable PCFGs. In
Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics. Association for Computa-
tional Linguistics.

S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2013. Experiments with spectral learning of latent-
variable pcfgs. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies.

63



D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spec-
tral dimensionality reduction for hmms. Arxiv preprint
arXiv:1203.6130.

J. S. Garofolo et al. 1988. Getting started with the darpa
timit cd-rom: An acoustic phonetic continuous speech
database. National Institute of Standards and Technology
(NIST), Gaithersburgh, MD, 107.

D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral al-
gorithm for learning hidden markov models. Journal of
Computer and System Sciences.

H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371–
1398.

K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer speech & language, 4(1):35–56.

K.F. Lee and H.W. Hon. 1989. Speaker-independent phone
recognition using hidden markov models. Acoustics,
Speech and Signal Processing, IEEE Transactions on,
37(11):1641–1648.

F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012.
Spectral learning for non-deterministic dependency pars-
ing. In EACL, pages 409–419.

T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg
with latent annotations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Linguis-
tics, pages 75–82. Association for Computational Linguis-
tics.

A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algo-
rithm for latent tree graphical models. In Proceedings of
the 28th International Conference on Machine Learning.

F. Pereira and Y. Schabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In Proceedings
of the 30th annual meeting on Association for Computa-
tional Linguistics, pages 128–135. Association for Com-
putational Linguistics.

S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages
433–440. Association for Computational Linguistics.

Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learn-
ing structured models for phone recognition. In Proc. of
EMNLP-CoNLL.

L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Proceed-
ings of the IEEE, 77(2):257–286.

S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reduced-
rank hidden Markov models. In Proceedings of the Thir-
teenth International Conference on Artificial Intelligence
and Statistics (AISTATS-2010).

L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola.
2010. Hilbert space embeddings of hidden markov mod-
els. In Proceedings of the 27th International Conference
on Machine Learning. Citeseer.

S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,
XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al.
2006. The htk book (for htk version 3.4).

64


