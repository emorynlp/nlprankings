



















































Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized EM


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Unsupervised Word Alignment Using Frequency Constraint in Posterior
Regularized EM

Hidetaka Kamigaito1,2, Taro Watanabe2, Hiroya Takamura1, Manabu Okumura1
1Tokyo Institute of Technology, Precision and Intelligence Laboratory

4259 Nagatsuta-cho Midori-ku Yokohama, Japan
2National Institute of Information and Communication Technology

3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan

Abstract

Generative word alignment models, such
as IBM Models, are restricted to one-
to-many alignment, and cannot explicitly
represent many-to-many relationships in
a bilingual text. The problem is par-
tially solved either by introducing heuris-
tics or by agreement constraints such that
two directional word alignments agree
with each other. In this paper, we fo-
cus on the posterior regularization frame-
work (Ganchev et al., 2010) that can force
two directional word alignment models
to agree with each other during train-
ing, and propose new constraints that can
take into account the difference between
function words and content words. Ex-
perimental results on French-to-English
and Japanese-to-English alignment tasks
show statistically significant gains over the
previous posterior regularization baseline.
We also observed gains in Japanese-to-
English translation tasks, which prove the
effectiveness of our methods under gram-
matically different language pairs.

1 Introduction

Word alignment is an important component in sta-
tistical machine translation (SMT). For instance
phrase-based SMT (Koehn et al., 2003) is based
on the concept of phrase pairs that are automat-
ically extracted from bilingual data and rely on
word alignment annotation. Similarly, the model
for hierarchical phrase-based SMT is built from
exhaustively extracted phrases that are, in turn,
heavily reliant on word alignment.

The Generative word alignment models, such as
the IBM Models (Brown et al., 1993) and HMM
(Vogel et al., 1996), are popular methods for au-
tomatically aligning bilingual texts, but are re-
stricted to represent one-to-many correspondence

of each word. To resolve this weakness, vari-
ous symmetrization methods are proposed. Och
and Ney (2003) and Koehn et al. (2003) propose
various heuristic methods to combine two direc-
tional models to represent many-to-many relation-
ships. As an alternative to heuristic methods, fil-
tering methods employ a threshold to control the
trade-off between precision and recall based on
a score estimated from the posterior probabili-
ties from two directional models. Matusov et al.
(2004) proposed arithmetic means of two mod-
els as a score for the filtering, whereas Liang et
al. (2006) reported better results using geometric
means. The joint training method (Liang et al.,
2006) enforces agreement between two directional
models. Posterior regularization (Ganchev et al.,
2010) is an alternative agreement method which
directly encodes agreement during training. DeN-
ero and Macherey (2011) and Chang et al. (2014)
also enforce agreement during decoding.

However, these agreement models do not take
into account the difference in language pairs,
which is crucial for linguistically different lan-
guage pairs, such as Japanese and English: al-
though content words may be aligned with each
other by introducing some agreement constraints,
function words are difficult to align.

We focus on the posterior regularization frame-
work and improve upon the previous work by
proposing new constraint functions that take into
account the difference in languages in terms of
content words and function words. In particular,
we differentiate between content words and func-
tion words by frequency in bilingual data, follow-
ing Setiawan et al. (2007).

Experimental results show that the proposed
methods achieved better alignment qualities on the
French-English Hansard data and the Japanese-
English Kyoto free translation task (KFTT) mea-
sured by AER and F-measure. In translation eval-
uations, we achieved statistically significant gains

153



in BLEU scores in the NTCIR10.

2 Statistical word alignment with
posterior regularization framework

Given a bilingual sentence x = (xs,xt) where xs

andxt denote a source and target sentence, respec-
tively, the bilingual sentence is aligned by a many-
to-many alignment of y. We represent posterior
probabilities from two directional word alignment
models as −→p θ(−→y |x) and←−p θ(←−y |x) with each ar-
row indicating a particular direction, and use θ to
denote the parameters of the models. For instance,−→y is a subset of y for the alignment from xs to
xt under the model of p(xt,−→y |xs). In the case of
IBM Model 1, the model is represented as follows:

p(xt,−→y |xs) =
∏
i

1

|xs|+ 1pt(x
t
i|xs−→y i). (1)

where we define the index of xt, xs as i, j(1 ≤
i ≤ |xt|, 1 ≤ j ≤ |xs|) and the posterior probabil-
ity for the word pair (xti, x

s
j) is defined as follows:

−→p (i, j|x) = pt(x
t
i|xsj)∑

j′ pt(x
t
i|xsj′)

. (2)

Herein, we assume that the posterior probabil-
ity for wrong directional alignment is zero (i.e.,−→p (←−y |x) = 0).1 Given the two directional mod-
els, Ganchev et al. defined a symmetric feature for
each target/source position pair, i, j as follows:

φi,j(x,y) =

 +1 (
−→y ⊂ y) ∩ (−→y i = j),

−1 (←−y ⊂ y) ∩ (←−y j = i),
0 otherwise.

(3)

The feature assigns 1 for the subset of word align-
ment for −→y , but assigns −1 for ←−y . As a result,
if a word pair i, j is aligned with equal posterior
probabilities in two directions, the expectation of
the feature value will be zero. Ganchev et al. de-
fined a joint model that combines two directional
models using arithmetic means:

pθ(y|x) = 1
2
−→p θ(y|x) + 1

2
←−p θ(y|x). (4)

Under the posterior regularization framework, we
instead use q that is derived by maximizing the fol-
lowing posterior probability parametrized by λ for
each bilingual data x as follows (Ganchev et al.,
2010):

qλ(y|x) =
−→p θ(−→y |x) +←−p θ(←−y |x)

2
(5)

·exp{−λ · φ(x,y)}
Z

1No alignment is represented by alignment into a special
token ”null”.

=
−→q (−→y |x) Z−→q−→p θ(x) +

←−q (←−y |x) Z←−q←−p θ(x)
2Z

,

Z =
1
2
(
Z−→q
−→p θ

+
Z←−q
←−pθ

),

−→q (−→y |x) = 1
Z−→q
−→p θ(−→y ,x)exp{−λ · φ(x,y)},

Z−→q =
∑
−→y

−→p θ(−→y ,x)exp{−λ · φ(x,y)},

←−q (←−y |x) = 1
Z←−q
←−p θ(←−y ,x)exp{−λ · φ(x, y)},

Z←−q =
∑
←−y

←−p θ(←−y ,x)exp{−λ · φ(x,y)},

such that Eqλ [φi,j(x,y)] = 0. In the E-step of
EM-algorithm, we employ qλ instead of pθ to ac-
cumulate fractional counts for its use in the M-
step. λ is efficiently estimated by the gradient as-
cent for each bilingual sentence x. Note that pos-
terior regularization is performed during parame-
ter estimation, and not during testing.

3 Posterior Regularization with
Frequency Constraint

The symmetric constraint method represented in
Equation (3) assumes a strong one-to-one rela-
tion for any word, and does not take into account
the divergence in language pairs. For linguisti-
cally different language pairs, such as Japanese-
English, content words may be easily aligned one-
to-one, but function words are not always aligned
together. In addition, Japanese is a pro-drop lan-
guage which can easily violate the symmetric con-
straint when proper nouns in the English side have
to be aligned with a “null” word. In addition, low
frequency words may cause unreliable estimates
for adjusting the weighing parameters λ.

In order to solve the problem, we improve
Ganchev’s symmetric constraint so that it can con-
sider the difference between content words and
function words in each language. In particular, we
follow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-
tion words by their frequencies. We propose con-
straint features that take into account the differ-
ence between content words and function words,
determined by a frequency threshold.

3.1 Mismatching constraint

First, we propose a mismatching constraint that
penalizes word alignment between content words
and function words by decreasing the correspond-
ing posterior probabilities.

154



The constraint is represented as f2c (function to
content) constraint:

φf2ci,j (x,y) = (6)

+1 (−→y ⊂ y) ∩ (−→y i = j) ∩ ((xti ∈ Ct ∩ xsj ∈ Fs)
∪(xti ∈ F t ∩ xsj ∈ Cs)) ∩ (δi,j(x,y) > 0),

0 (←−y ⊂ y) ∩ (←−y j = i) ∩ ((xti ∈ Ct ∩ xsj ∈ Fs)
∪(xti ∈ F t ∩ xsj ∈ Cs)) ∩ (δi,j(x,y) > 0),

0 (−→y ⊂ y) ∩ (−→y i = j) ∩ ((xti ∈ Ct ∩ xsj ∈ Fs)
∪(xti ∈ F t ∩ xsj ∈ Cs)) ∩ (δi,j(x,y) < 0),

−1 (←−y ⊂ y) ∩ (←−y j = i) ∩ ((xti ∈ Ct ∩ xsj ∈ Fs)
∪(xti ∈ F t ∩ xsj ∈ Cs)) ∩ (δi,j(x,y) < 0).

where δi,j(x,y) = −→p θ(i, j|x) − ←−p θ(i, j|x) is
the difference in the posterior probabilities be-
tween the source-to-target and the target-to-source
alignment. Cs and Ct represent content words in
the source sentence and target sentence, respec-
tively. Similarly, Fs and F t are function words
in the source and target sentence, respectively. In-
tuitively, when there exists a mismatch in content
word and function word for a word pair (i, j), the
constraint function returns a non-zero value for
the model with the highest posterior probability.
When coupled with the constraint such that the ex-
pectation of the feature value is zero, the constraint
function decreases the posterior probability of the
highest direction and discourages agreement with
each other.

Note that when this constraint is not fired, we
fall back to the constraint function in Equation (3)
for each word pair.

3.2 Matching constraint

In contrast to the mismatching constraint, our
second constraint function rewards alignment for
function to function word matching, namely f2f.
The f2f constraint function is defined as follows:

φf2fi,j (x,y) = (7)

+1 (−→y ⊂ y) ∩ (−→y i = j)∩
(xti ∈ F t ∩ xsj ∈ Fs) ∩ (δi,j(x,y) > 0),

0 (←−y ⊂ y) ∩ (←−y j = i)∩
(xti ∈ F t ∩ xsj ∈ Fs) ∩ (δi,j(x,y) > 0),

0 (−→y ⊂ y) ∩ (−→y i = j)∩
(xti ∈ F t ∩ xsj ∈ Fs) ∩ (δi,j(x,y) < 0),

−1 (←−y ⊂ y) ∩ (←−y j = i)∩
(xti ∈ F t ∩ xsj ∈ Fs) ∩ (δi,j(x,y) < 0).

This constraint function returns a non-zero value
for a word pair (i, j) when they are function
words. As a result, the pair of function words
are encouraged to agree with each other, but not
other pairs. The content to content word matching
function c2c can be defined similarly by replac-
ing F s and F t by Cs and Ct, respectively. Like-
wise, the function to content word matching func-

tion f2c is defined by considering the matching
of content words and function words in two lan-
guages. As noted in the mismatch function, when
no constraint is fired, we fall back to Eq (3) for
each word pair.

4 Experiment

4.1 Experimental Setup
The data sets used in our experiments are the
French-English Hansard Corpus, and two data sets
for Japanese-English tasks: the Kyoto free trans-
lation task (KFTT) and NTCIR10. The Hansard
Corpus consists of parallel texts drawn from of-
ficial records of the proceedings of the Canadian
Parliament. The KFTT (Neubig, 2011) is derived
from Japanese Wikipedia articles related to Ky-
oto, which is professionally translated into En-
glish. NTCIR10 comes from patent data employed
in a machine translation shared task (Goto et al.,
2013). The statistics of these data are presented in
Table 1.

Sentences of over 40 words on both source and
target sides are removed for training alignment
models. We used a word alignment toolkit ci-
cada 2 for training the IBM Model 4 with our
proposed methods. Training is bootstrapped from
IBM Model 1, followed by HMM and IBM Model
4. When generating the final bidirectional word
alignment, we use a grow-diag-final heuristic for
the Japanese-English tasks and an intersection
heuristic in the French-English task, judged by
preliminary studies.

Following Bisazza and Federico (2012), we
automatically decide the threshold for word fre-
quency to discriminate between content words and
function words. Specifically, the threshold is de-
termined by the ratio of highly frequent words.
The threshold th is the maximum frequency that
satisfies the following equation:∑

w∈(freq(w)>th) freq(w)∑
w∈all freq(w)

> r. (8)

Here, we empirically set r = 0.5 by preliminary
studies. This method is based on the intuition that
content words and function words exist in a docu-
ment at a constant rate.

4.2 Word alignment evaluation
We measure the impact of our proposed meth-
ods on the quality of word alignment measured

2https://github.com/tarowatanabe/cicada

155



Table 1: The statistics of the data sets

hansard kftt NTCIR10
French English Japanese English Japanese English

train sentence 1.13M 329.88K 2.02M
word 23.3M 19.8M 6.08M 5.91M 53.4M 49.4M
vocabulary 78.1K 57.3K 114K 138K 114K 183K

dev sentence 1.17K 2K
word 26.8K 24.3K 73K 67.3K
vocabulary 4.51K 4.78K 4.38K 5.04K

test WA sentence 447 582
word 7.76K 7.02K 14.4K 12.6K
vocabulary 1,92K 1.69K 2.57K 2.65K

TR sentence 1.16K 8.6K
word 28.5K 26.7K 334K 310K
vocabulary 4.91K 4.57K 10.4K 12.7K

Figure 1: Precision Recall graph in Hansard
French-English

Figure 2: Precision Recall graph in KFTT

Figure 3: AER in Hansard French-English Figure 4: AER in KFTT

156



Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)

KFTT Hansard (French-English)
method precision recall AER F precision recall AER F
symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822
f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844
c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804
f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848

by AER and F-measure (Och and Ney, 2003).
Since there exists no distinction for sure-possible
alignments in the KFTT data, we use only sure
alignment for our evaluation, both for the French-
English and the Japanese-English tasks. Table 2
summarizes our results.

The baseline method is symmetric constraint
(Ganchev et al., 2010) shown in Table 2. The num-
bers in bold and in italics indicate the best score
and the second best score, respectively. The dif-
ferences between f2f,f2c and baseline in KFTT are
statistically significant at p < 0.05 using the sign-
test, but in hansard corpus, there exist no signifi-
cant differences between the baseline and the pro-
posed methods. In terms of F-measure, it is clear
that the f2f method is the most effective method
in KFTT, and both f2f and f2c methods exceed the
original posterior regularized model of Ganchev et
al. (2010).

We also compared these methods with filtering
methods (Liang et al., 2006), in addition to heuris-
tic methods. We plot precision/recall curves and
AER by varying the threshold between 0.1 and
0.9 with 0.1 increments. From Figures, it can be
seen that our proposed methods are superior to
the baseline in terms of both precision-recall and
AER.

4.3 Translation evaluation

Next, we performed a translation evaluation, mea-
sured by BLEU (Papineni et al., 2002). We
compared the grow-diag-final and filtering method
(Liang et al., 2006) for creating phrase tables.
The threshold for the filtering factor was set to
0.1 which was the best setting in the word align-
ment experiment in section 4.2 under KFTT. From
the English side of the training data, we trained a
word using the 5-gram model with SRILM (Stol-
cke and others, 2002). “Moses” toolkit was used
as a decoder (Koehn et al., 2007) and the model
parameters were tuned by k-best MIRA (Cherry
and Foster, 2012). In order to avoid tuning insta-
bility, we evaluated the average of five runs (Hop-
kins and May, 2011). The results are summarized

Table 3: Results of translation evaluation

KFTT NTCIR10
GDF Filtered GDF Filtered

symmetric 19.06 19.28 28.3 29.71
f2f 19.15 19.17 28.36 29.74
c2c 19.26 19.02 28.36 29.92
f2c 18.91 19.20 28.36 29.67

in Table 3. Our proposed methods achieved large
gains in NTCIR10 task with the filtered method,
but observed no gain in the KFTT with the filtered
method. In NTCIR10 task with GDF, the gain in
BLEU was smaller than that of KFTT. We cal-
culate p-values and the difference between sym-
metric and c2c (the most effective proposed con-
straint) are lower than 0.05 in kftt with GDF and
NTCIR10 with filtered method. There seems to
be no clear tendency in the improved alignment
qualities and the translation qualities, as shown in
numerous previous studies (Ganchev et al., 2008).

5 Conclusion

In this paper, we proposed new constraint func-
tions under the posterior regularization frame-
work. Our constraint functions introduce a
fine-grained agreement constraint considering the
frequency of words, a assuming that the high
frequency words correspond to function words
whereas the less frequent words may be treated
as content words, based on the previous work of
Setiawan et al. (2007). Experiments on word
alignment tasks showed better alignment quali-
ties measured by F-measure and AER on both the
Hansard task and KFTT. We also observed large
gain in BLEU, 0.2 on average, when compared
with the previous posterior regularization method
under NTCIR10 task.

As our future work, we will investigate more
precise methods for deciding function words and
content words for better alignment and translation
qualities.

157



References
Arianna Bisazza and Marcello Federico. 2012. Cutting

the long tail: Hybrid language models for translation
style adaptation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 439–448. Associ-
ation for Computational Linguistics.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A constrained viterbi
relaxation for bidirectional word alignment. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1481–1490, Baltimore, Maryland,
June. Association for Computational Linguistics.

Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427–436. Association for Computational Lin-
guistics.

John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 420–429, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Kuzman Ganchev, João V. Graça, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986–993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Kuzman Ganchev, Joao Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001–2049.

Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352–1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North

American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.

Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104–111, New York City,
USA, June. Association for Computational Linguis-
tics.

E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. In Proceedings of COLING 2004, pages 219–
225, Geneva, Switzerland, August 23–27.

Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th annual meeting on associ-
ation for computational linguistics, pages 712–719.
Association for Computational Linguistics.

Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836–
841. Association for Computational Linguistics.

158


