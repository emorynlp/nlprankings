



















































Sentence-level Rewriting Detection


Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 149–154,
Baltimore, Maryland USA, June 26, 2014. c©2014 Association for Computational Linguistics

Sentence-level Rewriting Detection

Fan Zhang
University of Pittsburgh
Pittsburgh, PA, 15260

zhangfan@cs.pitt.edu

Diane Litman
University of Pittsburgh
Pittsburgh, PA, 15260

litman@cs.pitt.edu

Abstract

Writers usually need iterations of revisions
and edits during their writings. To bet-
ter understand the process of rewriting,
we need to know what has changed be-
tween the revisions. Prior work mainly fo-
cuses on detecting corrections within sen-
tences, which is at the level of words
or phrases. This paper proposes to de-
tect revision changes at the sentence level.
Looking at revisions at a higher level al-
lows us to have a different understanding
of the revision process. This paper also
proposes an approach to automatically de-
tect sentence revision changes. The pro-
posed approach shows high accuracy in an
evaluation using first and final draft essays
from an undergraduate writing class.

1 Introduction

Rewriting is considered to be an important process
during writing. However, conducting successful
rewriting is not an easy task, especially for novice
writers. Instructors work hard on providing sug-
gestions for rewriting (Wells et al., 2013), but usu-
ally such advice is quite general. We need to un-
derstand the changes between revisions better to
provide more specific and helpful advice.

There has already been work on detecting cor-
rections in sentence revisions (Xue and Hwa,
2014; Swanson and Yamangil, 2012; Heilman
and Smith, 2010; Rozovskaya and Roth, 2010).
However, these works mainly focus on detecting
changes at the level of words or phrases. Ac-
cording to Faigley’s definition of revision change
(Faigley and Witte, 1981), these works could help
the identification of Surface Changes (changes
that do not add or remove information to the orig-
inal text). However, Text Changes (changes that
add or remove information) will be more difficult

to identify if we only look at revisions within sen-
tences. According to Hashemi and Schunn (2014),
when instructors were presented a comparison of
differences between papers derived from words,
they felt the information regarding changes be-
tween revisions was overwhelming.

This paper proposes to look at the changes be-
tween revisions at the level of sentences. Com-
paring to detecting changes at the word level, de-
tecting changes at the sentence level contains less
information, but still keeps enough information
to understand the authors’ intention behind their
modifications to the text. The sentence level edits
could then be grouped and classified into differ-
ent types of changes. The long-term goal of this
project is to allow us to be able to identify both
Text Changes and Surface Changes automatically.
Students, teachers, and researchers could then per-
form analysis on the different types of changes and
have a better understanding of the rewriting pro-
cess. As a preliminary work, this paper explores
steps toward this goal: First, automatically gener-
ate the description of changes based on four prim-
itives: Add, Delete, Modify, Keep; Second, merge
the primitives that come from the same purpose.

2 Related work

Hashemi and Schunn (2014) presented a tool
to help professors summarize students’ changes
across papers before and after peer review. They
first split the original documents into sentences
and then built on the output of Compare Suite
(CompareSuite, 2014) to count and highlight
changes in different colors. Figure 1 shows a
screenshot of their work. As we can see, the mod-
ifications to the text are misinterpreted. Line 66
in the final draft should correspond to line 55 and
line 56 in the first draft, while line 67 and line 68
should be a split of line 57 in the first draft. How-
ever, line 67 is aligned to line 56 wrongly in their
work. This wrong alignment caused many mis-

149



recognized modifications. According to Hashemi,
the instructors who use the system think that the
overwhelming information of changes make the
system less useful. We hypothesize that since their
work is based on analysis at the word level, al-
though their approach might work for identifying
differences within one sentence, it makes mistakes
when sentence analysis is the primary concern.

Our work avoids the above problem by detect-
ing differences at the sentence level. Sentence
alignment is the first step of our method; fur-
ther inferences about revision changes are then
based on the alignments generated. We borrow
ideas from the research on sentence alignment for
monolingual corpora. Existing research usually
focuses on the alignment from the text to its sum-
marization or its simplification (Jing, 2002; Barzi-
lay and Elhadad, 2003; Bott and Saggion, 2011).
Barzilay and Elhadad (2003) treat sentence align-
ment as a classification task. The paragraphs are
clustered into groups, and a binary classifier is
trained to decide whether two sentences should be
aligned or not. Nelken (2006) further improves
the performance by using TF*IDF score instead of
word overlap and also utilizing global optimiza-
tion to take sentence order information into con-
sideration. We argue that summarization could
be considered as a special form of revision and
adapted Nelken’s approach to our approach.

Edit sequences are then inferred based on the
results of sentence alignment. Fragments of ed-
its that come from the same purpose will then be
merged. Related work to our method is sentence
clustering (Shen et al., 2011; Wang et al., 2009).
While sentence clustering is trying to find and
cluster sentences similar to each other, our work
is to find a cluster of sentences in one document
that is similar to one sentence in the other docu-
ment after merging.

3 Sentence-level changes across revisions

3.1 Primitives for sentence-level changes

Previous work in educational revision analysis
(Faigley and Witte, 1981; Connor and Asenav-
age, 1994) categorized revision changes to be ei-
ther surface changes or text-based changes. With
both categories, six kinds of changes were defined
as shown in Table 1.

Different from Faigley’s definition, we define
only 4 primitives for our first step of edit sequence
generation: Add, Delete, Modify and Keep. This

Code Explanation
Addition Adding a word or phrase
Deletion Omitting a word or phrase
Substitutions exchange words with synonyms
Permutation rearrange of words or phrases
Distribution one segment divided into two
Consolidation combine two segments into one

Table 1: Code Definition by L.Faigley and S.Witte

definition is similar to Bronner’s work (Bronner
and Monz, 2012). We choose this definition be-
cause these 4 primitives only correspond to one
sentence at a time. Add, Delete, Modify indicates
that the writer has added/deleted/modified a sen-
tence. Keep means the original sentence is not
modified. We believe Permutation, Distribution
and Consolidation as defined by Faigley could be
described with these four primitives, which could
be recognized in the later merge step.

3.2 Data and annotation

The corpus we choose consists of paired first and
final drafts of short papers written by undergradu-
ates in a course “Social Implications of Comput-
ing Technology”. Students are required to write
papers on one topic and then revise their own pa-
pers. The revisions are guided by other students’
feedback based on a grading rubric, using a web-
based peer review system. Students first submitted
their original paper into the system, and then were
randomly assigned to review and comment others’
work according to the writing rubric. The authors
would receive the others’ anonymous comments,
and then could choose to revise their work based
on others’ comments as well as their own insights
obtained by reviewing other papers.

The papers in the corpus contain two topics.
In the first topic, the students discussed the role
that Big Data played in Obama’s presidential cam-
paign. This topic contains 11 pairs of first and final
drafts of short papers. We name this C1. The other
topic, named C2, talks about intellectual property
and contains 10 pairs of paper drafts. The students
involved in these two topics are from the same
class. Students make more modifications to their
papers in C2. More details can be seen in Table 2.

Our revision change detection approach con-
tains three steps: sentence alignment, edit se-
quence generation and merge of edit sequences.
Thus we annotated for these three steps.

150



(a) first draft (b) final draft

(c) Revision detection using Hashemi’s approach

Figure 1: Fragments of a paper in corpus C2 discussing intellectual property, (c) is Hashemi’s work,
green for recognized modifications, blue for insertions and red for deletion

For sentence alignment, each sentence in the fi-
nal draft is assigned the index of its aligned sen-
tence in the original draft. If a sentence is newly
added, it will be annotated as ADD. Sentence
alignment is not necessarily one-to-one. It can
also be one-to-many (Consolidation) and many-
to-one (Distribution). Table 3 shows a fragment
of the annotation for the text shown in Figure 1.

For edit sequences, the annotators do the anno-
tation based on the initial draft. For the same frag-
ment in Table 3, the annotated sequence is: Keep,
Modify, Delete, Modify, Add1.

For edit sequence merging, we further annotate
Consolidation and Distribution based on the edit
sequences. In our example, 66 consolidates 55 and
56, while 57 distributes to 67 and 68.

pairs #D1 #D2 Avg1 Avg2
C1 11 761 791 22.5 22.7
C2 10 645 733 24.7 24.5

Table 2: Detailed information of corpora. #D1 and
#D2 are the number of sentences in the first and
final draft, Avg1 and Avg2 are the average number
of words in one sentence in the first and final draft

As a preliminary work, we only have one anno-
tator doing all the annotations. But for the anno-
tation of sentence alignments, we have two anno-

166 consolidates 55, 56; while 57 distributes to 67, 68.
Notice that Consolidation is illustrated as Modify, Delete and
Distribution is illustrated as Modify, Add. As the annotators
annotate based on the first draft, Modify always appears be-
fore Add or Delete

tators annotating on one pair of papers. The paper
contains 76 sentences, and the annotators only dis-
agree in one sentence. The kappa is 0.794 2, which
suggests that the annotation is reliable based on
our annotation scheme.

4 Automatic detection of revision
changes

The detection of revision changes contains three
parts: sentence alignment, edit sequence genera-
tion and edit sequence merging. The first two parts
generate edit sequences detected at the sentence
level, while the third part groups edit sequences
and classifies them into different types of changes.
Currently the third step only covers the identifica-
tion of Consolidation and Distribution.

Sentence Index (Final) 65 66 67 68
Sentence Index (First) 54 55,56 57 57

Table 3: An example of alignment annotation

Sentence alignment We adapted Nelken’s ap-
proach to our problem.

Alignment based on sentence similarity
The alignment task goes through three stages.
1. Data preparation: for each sentence in the an-

notated final draft, if it is not a new sentence, cre-
ate a sentence pair with its aligned sentence in the

2We calculate the Kappa value following Macken’s idea
(Macken, 2010), where the aligned sentences are categorized
as direct-link, while new added sentences are categorized as
null-link (ADD).

151



first draft. The pair is considered to be an aligned
pair. Also, randomly select another sentence from
the first draft to make a negative sentence pair.
Thus we ensure there are nearly equal numbers of
positive and negative cases in the training data.

2. Training: according to the similarity met-
ric defined, calculate the similarity of the sentence
pairs. A logistic regression classifier predicting
whether a sentence pair is aligned or not is trained
with the similarity score as the feature. In addi-
tion to classification, the classifier is also used to
provide a similarity score for global alignment.

3. Alignment: for each pair of paper drafts, con-
struct sentence pairs using the Cartesian product
of sentences in the first draft and sentences in the
final. Logistic regression classifier is used to deter-
mine whether the sentence pair is aligned or not.

We added Levenshtein distance (LD) (Leven-
shtein, 1966) as another similarity metric in ad-
dition to Nelken’s metrics. Together three similar-
ity metrics were compared: Levenshtein Distance,
Word Overlap(WO), and TF*IDF.

Global alignment
Sentences are likely to preserve the same or-

der between rewritings. Thus, sentence or-
dering should be an important feature in sen-
tence alignment. Nelken’s work modifies the
Needleman-Wunsch alignment (Needleman and
Wunsch, 1970) to find the sentence alignments and
goes in the following steps.

Step1: The logistic regression classifier previ-
ously trained assigns a probability value from 0 to
1 for each sentence pair s(i, j). Use this value as
the similarity score of sentence pair: sim(i, j).

Step2: Starting from the first pair of sen-
tences, find the best path to maximize the likeli-
hood between sentences according to the formula
s(i, j) = max{s(i − 1, j − 1) + sim(i, j), s(i −
1, j) + sim(i, j) , s(i, j − 1) + sim(i, j)}

Step3: Infer the sentence alignments by back
tracing the matrix s(i, j).

We found out that changing bolded parts in the
formula to s(i, j) = max{s(i − 1, j − 1) +
sim(i, j), s(i − 1, j) + insertcost , s(i, j − 1) +
deletecost} shows better performance in our prob-
lem. According to our experiment with C1, insert-
cost and deletecost are both set to 0.1 as they are
found to be the most effective during practice.

Edit sequence generation This step is an inter-
mediate step, which tries to generate the edit se-
quence based on the sentence alignment results

from the previous step. The edit sequences gen-
erated would later be grouped together and clas-
sified into different types. In our current work, a
rule-based method is proposed for this step.

Step1: The index of original document i and the
index of the modified document j both start from
0. If sentence i in the original document is aligned
to sentence j in the modified one, go to step 2, if
not go to step 3.

Step2: If the two sentences are exactly the same,
add Keep to the edit sequence, if not, add Modify.
Increase i and j by 1, go to step 1.

Step3: Check the predicted alignment index of
sentence j, if the predicted index is larger than sen-
tence i in the original document, add Delete and
increase i by 1, otherwise, mark as Add and in-
crease j by 1, go to step 1.

Edit sequence merging Distribution means
splitting one sentence into two or more sentences,
while Consolidation means merging two or more
sentences into one sentence. These two operations
can be derived with primitives Modify, Add and
Delete. They follow the following patterns:

Consolidation: Modify-Delete-Delete-...
Distribution: Modify-Add-Add-...
These sequences both start with Modify fol-

lowed with a repetitive number of Delete or Add.
A group of edit sequences can be merged if they
can be merged to a sentence close to the sentence
in the other draft. We applied a rule-based ap-
proach based on our observations.

We first scan through the sequence generated
above. Sequences with Modify-Add-... or Mod-
ify-Delete-... are extracted. For each sequence ex-
tracted, if there are n consecutive Add or Delete
following Modify, create n groups, Groupi(i ≤
n) contains sentences from the modified sentence
to the next consecutive i sentences. For each
group, merge all the sentences, and use the clas-
sifier trained above to get the similarity score
Simgroupi between the merged sentence and the
original one. If there are multiple groups classi-
fied as aligned, choose group i that has the largest
Simgroupi , merge the basic edit operations into
Consolidation or Distribution. If none of the
groups are classified as aligned, do not merge.

5 Evaluation

Sentence alignment We use accuracy as the
evaluation metric. For each pair of drafts, we
count the number of sentences in the final draft

152



N1. For each sentence in the final draft, we count
the number of sentences that get the correct align-
ment as N2. The accuracy of the sentence align-
ment is N2N1 .

3

We use Hashemi’s approach as the baseline.
Compare Suite colors the differences out, as
shown in Figure 1. We treat the green sentences
as Modify and aligned to the original sentence.

For our method, we tried four groups of set-
tings. Group 1 and group 2 perform leave-one-out
cross validation on C1 and C2 (test on one pair of
paper drafts and train on the others). Group 3 and
group 4 train on one corpus and test on the other.

Group LD WO TF*IDF Baseline
1 0.9811 0.9863 0.9931 0.9427
2 0.9649 0.9593 0.9667 0.9011
3 0.9727 0.9700 0.9727 0.9045
4 0.9860 0.9886 0.9798 0.9589

Table 4: Accuracy of our approach vs. baseline

Table 4 shows that all our methods beat the
baseline 4. Among the three similarity metrics,
TF*IDF is the most predictive.

Edit sequence generation We use WER (Word
Error Rate) from speech recognition for evaluat-
ing the generated sequence by comparing the gen-
erated sequence to the gold standard.

WER is calculated based on edit distances be-
tween sequences. The ratio is calculated as:
WER = S+D+IN , where S means the number of
modifications, D means the number of deletes, I
means the number of inserts.

We apply our method on the gold standard of
sentence alignment. The generated edit sequence
is then compared with the gold standard edit se-
quence to calculate WER. Hashemi’s approach is
chosen as the baseline. The WER of our method is
0.035 on C1 and 0.017 on C2, comparing to 0.091
on C1 and 0.153 on C2 for the baseline, which
shows that our rule-based method has promise.

3Notice that we have the case that one sentence is aligned
to two sentences (i.e. Consolidation, as sentence 66 in Table
3). In our evaluation, an alignment is considered to be correct
only if the alignment covers all the sentences that should be
covered. For example, if Sentence 66 in Table 3 is aligned to
Sentence 55 in the first draft, it is counted as an error.

4For Groups 1 and 2, we calculate the accuracy of
Hashemi’s approach under a leave-one-out setting, each time
remove one pair of document and calculate the accuracy. A
significance test is also conducted, the worst metric LD in
Group 1 and WO in Group 2 both beat the baseline signifi-
cantly ( p1 = 0.025,p2 = 0.017) in two-tailed T-test.

Applying our method on the predicted alignment
on the first step gets 0.067 on C1 and 0.025 on C2,
which although degraded still beats the baseline.

Edit sequence merging There are only a limited
number of Consolidation and Distribution exam-
ples in our corpus. Together there are 9 Consolida-
tion and 5 Distribution operations. In our current
data, the number of sentences involved in these
operations is always 2. Our rule-based method
achieved 100% accuracy in the identification of
these operations. It needs further work to see if
this method would perform equally well in more
complicated corpora.

6 Conclusion

This paper presents a preliminary work in the ef-
fort of describing changes across revisions at a
higher level than words, motivated by a long term
goal to build educational applications to support
revision analysis for writing. Comparing to revi-
sion analysis based on words or phrases, our ap-
proach is able to capture higher level revision op-
erations. We also propose algorithms to detect re-
vision changes automatically. Experiments show
that our method has a reliable performance.

Currently we are investigating applying se-
quence merging on the automatic generated edit
sequences based on edit distances directly. Our
next plan is to develop a tool for comparing drafts,
and conduct user studies to have extrinsic evalua-
tions on whether our method would provide more
useful information to the user. We are also plan-
ning to do further analysis based on the revisions
detected, and ultimately be able to distinguish be-
tween surface changes and text-based changes.

Acknowledgments

We would like to thank W. Wang, W. Luo, H. Xue,
and the ITSPOKE group for their helpful feedback
and all the anonymous reviewers for their sugges-
tions.

This research is supported by the Institute of
Education Sciences, U.S. Department of Educa-
tion, through Grant R305A120370 to the Univer-
sity of Pittsburgh. The opinions expressed are
those of the authors and do not necessarily repre-
sent the views of the Institute or the U.S. Depart-
ment of Education.

153



References
Regina Barzilay and Noemie Elhadad. 2003. Sentence

alignment for monolingual comparable corpora. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 25–
32. Association for Computational Linguistics.

Stefan Bott and Horacio Saggion. 2011. An un-
supervised alignment algorithm for text simplifica-
tion corpus construction. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 20–26. Association for Computational Lin-
guistics.

Amit Bronner and Christof Monz. 2012. User edits
classification using document revision histories. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 356–366. Association for Computa-
tional Linguistics.

CompareSuite. 2014. Compare suite, feature-rich
file and folder compare tool. http://www.
comparesuite.com.

Ulla Connor and Karen Asenavage. 1994. Peer re-
sponse groups in esl writing classes: How much im-
pact on revision? Journal of Second Language Writ-
ing, 3(3):257–276.

Lester Faigley and Stephen Witte. 1981. Analyzing
revision. College composition and communication,
pages 400–414.

Homa B. Hashemi and Christian D. Schunn. 2014.
A tool for summarizing students’ shanges across
drafts. In International Conference on Intelligent
Tutoring Systems(ITS).

Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019.
Association for Computational Linguistics.

Hongyan Jing. 2002. Using hidden markov modeling
to decompose human-written summaries. Computa-
tional linguistics, 28(4):527–543.

Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.

Lieve Macken. 2010. An annotation scheme and
gold standard for dutch-english word alignment.
In 7th conference on International Language Re-
sources and Evaluation (LREC 2010), pages 3369–
3374. European Language Resources Association
(ELRA).

Saul B Needleman and Christian D Wunsch. 1970.
A general method applicable to the search for simi-
larities in the amino acid sequence of two proteins.
Journal of molecular biology, 48(3):443–453.

Rani Nelken and Stuart M Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In EACL.

Alla Rozovskaya and Dan Roth. 2010. Annotating
esl errors: Challenges and rewards. In Proceedings
of the NAACL HLT 2010 fifth workshop on innova-
tive use of NLP for building educational applica-
tions, pages 28–36. Association for Computational
Linguistics.

Chao Shen, Tao Li, and Chris HQ Ding. 2011. Inte-
grating clustering and multi-document summariza-
tion by bi-mixture probabilistic latent semantic anal-
ysis (plsa) with sentence bases. In AAAI.

Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an esl educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357–361. Association for Computa-
tional Linguistics.

Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 297–300. Association for Computational Lin-
guistics.

Jaclyn M. Wells, Morgan Sousa, Mia Martini, and
Allen Brizee. 2013. Steps for revising your pa-
per. http://owl.english.purdue.edu/
owl/resource/561/05.

Huichao Xue and Rebecca Hwa. 2014. Improved cor-
rection detection in revised esl sentences. In Pro-
ceedings of The 52nd Annual Meeting of the Associ-
ation for Computational Linguistics(ACL).

154


