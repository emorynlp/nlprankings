










































Learning-based Multi-Sieve Co-reference Resolution with Knowledge


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1234–1244, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Learning-based Multi-Sieve Co-reference Resolution with Knowledge∗

Lev Ratinov
Google Inc.†

ratinov@google.com

Dan Roth
University of Illinois at Urbana-Champaign

danr@illinois.edu

Abstract

We explore the interplay of knowledge and
structure in co-reference resolution. To inject
knowledge, we use a state-of-the-art system
which cross-links (or “grounds”) expressions
in free text to Wikipedia. We explore ways
of using the resulting grounding to boost the
performance of a state-of-the-art co-reference
resolution system. To maximize the utility of
the injected knowledge, we deploy a learning-
based multi-sieve approach and develop novel
entity-based features. Our end system outper-
forms the state-of-the-art baseline by 2 B3 F1
points on non-transcript portion of the ACE
2004 dataset.

1 Introduction

Co-reference resolution is the task of grouping men-
tions to entities. For example, consider the text snip-
pet in Fig. 11. The correct output groups the men-
tions {m1, m2, m5} to one entity while leaving m3

∗We thank Nicholas Rizzolo and Kai Wei Chang for their
invaluable help with modifying the baseline co-reference sys-
tem. We thank the anonymous EMNLP reviewers for con-
structive comments. This research was supported by the Army
Research Laboratory (ARL) under agreement W911NF-09-2-
0053 and by the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, conclusions or recommendations are
those of the authors and do not necessarily reflect the view of
the ARL, DARPA, AFRL, or the US government.

† The majority of this work was done while the first author
was at the University of Illinois.

1Throughout this paper, curly brackets {} denote the extent
and square brackets [] denote the head.

“After the {[vessel]}m1 suffered a catastrophic torpedo
detonation, {[Kursk]}m2 sank in the waters of {[Barents
Sea]}m3 with all hands lost. Though rescue attempts were
offered by a nearby {Norwegian [ship]}m4 , Russia declined
initial rescue offers, and all 118 sailors and officers aboard
{[Kursk]}m5 perished.”

Figure 1: Example illustrating the challenges in co-reference
resolution.

and m4 as singletons. Resolving co-reference is fun-
damental for understanding natural language. For
example in Fig. 1, to infer that Kusrk has suffered
a torpedo detonation, we have to understand that
{[vessel]}m1 refers to {[Kursk]}m2.

This inference is typically trivial for humans, but
proves extremely challenging for state-of-the-art co-
reference resolution systems. We believe that it is
world knowledge that gives people the ability to un-
derstand text with such ease. A human reader can in-
fer that since Kursk sank, it must be a vessel and ves-
sels which suffer catastrophic torpedo detonations
can sink. Moreover, some readers might just know
that Kursk is a Russian submarine named after the
city of Kursk, where the largest tank battle in his-
tory took place in 1943. In this work we are using
Wikipedia as a source of encyclopedic knowledge.
The key contributions of this work are:

(1) Using Wikipedia to assign a set of knowledge
attributes to mentions in a context-sensitive way.
For example, for the text in Fig. 1, our system as-
signs to the mention “Kursk” the nationalities: Rus-
sian, Soviet and the attributes ship, incident, subma-
rine, shipwreck (as opposed to city or battle). We
are using a publicly available system for context-

1234



sensitive disambiguation to Wikipedia. We then
extract attributes from the cross-linked Wikipedia
pages (described in Sec. 3.1), assign these attributes
to the document mentions (Sec. 3.2) and develop
knowledge-rich compatibility metric between men-
tions (Sec. 3.3)2.

(2) Integrating the strength of rule-based systems
such as (Haghighi and Klein, 2009; Raghunathan et
al., 2010) into a machine learning framework. We
are using a multi-sieve approach (Raghunathan et
al., 2010), which splits pairwise “co-reference” vs.
“non-coreference” decisions to different types and
attempts to make the easy decisions first (Goldberg
and Elhadad, 2010). Our multi-sieve approach is
different from (Raghunathan et al., 2010) in sev-
eral respects: (a) our sieves are machine-learning
classifiers, (b) the same pair of mentions can fall
into multiple sieves, (c) later sieves can override
the decisions made by earlier sieves, allowing to re-
cover from errors as additional evidence becomes
available. In our running example, the decision
of whether {[vessel]}m1 refers to {[Kursk]}m2 is
made before the decision of whether {[vessel]}m1
refers to {Norwegian [ship]}m4 since decisions in
the same sentence are believed to be easier than
cross-sentence ones. We describe our learning-
based multi-sieve approach in Sec. 4.

(3) A novel approach for entity-based features. As
sieves of classifiers are applied, our system attempts
to model entities and share the attributes between the
mentions belonging to the same entity. Once the de-
cision that {[vessel]}m1 and {[Kursk]}m2 co-refer is
made, we want the two mentions to share the Rus-
sian nationality. This allows us to avoid erroneously
linking {[vessel]}m1 to {Norwegian [ship]}m4 de-
spite vessel and ship being synonyms in Word-
Net. However, in this work we allow the sieves to
make conflicting decisions on the same pair of men-
tions. Hence, obtaining entities and their attributes
by straightforward transitive closure of co-reference
predictions is impossible. We describe our approach
for leveraging possibly contradicting predictions in
Sec. 5.

(4) By adding word-knowledge features and us-

2The extracted attributes and the related re-
sources are available for public download at
http://cogcomp.cs.illinois.edu/Data/
Ace2004CorefWikiAttributes.zip

Input: document d; mentions M = {m1, . . . , mN}
1) For each mi ∈ M , assign it a Wikipedia page pi in a
context-sensitive way (pi may be null).
- If pi 6= null: extract knowledge attributes from pi and
assign to m.
- Else extract knowledge attributes directly from m via
noun-phrase parsing techniques (Vadas and Curran, 2008).
3) Let Q = {(mi, mj)}i6=j , be the queue of mention
pairs approximately sorted by “easy-first” (Goldberg and
Elhadad, 2010).
4) Let G be a partial clustering graph.
5) While Q is not empty
- Extract a pair p = (mi, mj) from Q.
- Using the knowledge attributes of mi and mj as well as
the structure of G, classify whether p is co-referent.
- Update G with the classification decision.
6) Construct an end clustering from G.

Figure 2: High-level system architecture.

ing learning-based multi-sieve approach, we im-
prove the performance of the state-of-the-art system
of (Bengtson and Roth, 2008) by 3 MUC, 2 B3 and
2 CEAF F1 points on the non-transcript portion of
the ACE 2004 dataset. We report our experimen-
tal results in Sec. 6 and conclude with discussion in
Sec. 7.

We conclude the introduction by giving a high-
level overview of our system in Fig. 2.

2 Baseline System

In this work, we are using the state-of-the-art sys-
tem of (Bengtson and Roth, 2008), which relies
on a pairwise scoring function pc to assign an or-
dered pair of mentions a probability that they are
coreferential. It uses a rich set of features includ-
ing: string edit distance, gender match, whether the
mentions appear in the same sentence, whether the
heads are synonyms in WordNet etc. The function
pc is modeled using regularized averaged percep-
tron for a tuned number of training rounds, learn-
ing rate and margin. For the end system, we keep
these parameters intact, our only modifications will
be adding knowledge-rich features and adding inter-
mediate classification sieves to the training and the
inference, which we will discuss in the following
sections.

At inference time, given a document d and a
pairwise co-reference scoring function pc, (Bengt-
son and Roth, 2008) generate a graph Gd accord-

1235



ing to the Best-Link decision model (Ng and Cardie,
2002) as follows. For each mention m in docu-
ment d, let Bm be the set of mentions appearing
before m in d. Let a be the highest scoring an-
tecedent: a = argmaxb∈Bm(pc(b, m)). We will add
the edge (a, m) to Gd if pc(a, m) predicts the pair to
be co-referent with a confidence exceeding a chosen
threshold, then we take the transitive closure3.

The properties of the Best-Link inference are il-
lustrated in Fig. 3. At this stage, we ask the reader
to ignore the knowledge attributes at the bottom of
the figure. Let us assume that the pairwise classi-
fier labeled the mentions (m2, m5) co-referent be-
cause they have identical surface form; mentions
(m1, m4) are co-referred because the heads are syn-
onyms in WordNet. Let us assume that since m1
and m2 appear in the same sentence, the pairwise
classifier managed to leverage the dependency parse
tree to correctly co-ref the pair (m1, m2). The tran-
sitive closure would correctly link (m1, m5) despite
the incorrect prediction of the pairwise classifier on
(m1, m5), and would incorrectly link m4 with all
other mentions because of the incorrect pairwise
prediction on (m1, m4) and despite the correct pre-
dictions on (m2, m4) and (m4, m5).

Figure 3: A sample output of a pairwise co-reference classifier.
The full edges represent a co-ref prediction and the empty edges
represent a non-coref prediction. A set of knowledge attributes
for selected mentions is shown as well.

3 Wikipedia as Knowledge

In this section we describe our methodology for us-
ing Wikipedia as a knowledge resource. In Sec. 3.1
we cover the process of knowledge extraction from

3We use Platt Scaling while (Bengtson and Roth, 2008) used
the raw output value of pc.

Wikipedia pages. We describe how to inject this
knowledge into mentions in Sec. 3.2. The bottom
part of Fig. 3 illustrates the knowledge attributes our
system injects to two sample mentions at this stage.
Finally, in Sec. 3.3 we describe a compatibility met-
ric our system learns over the injected knowledge.

3.1 Wikipedia Knowledge Attributes

Our goal in this section is to extract from Wikipedia
pages a compact and highly-accurate set of knowl-
edge attributes, which nevertheless possesses dis-
criminative power for co-reference4. We concentrate
on three types of knowledge attributes: fine-grained
semantic categories, gender information and nation-
ality where applicable.

Each Wikipedia page is assigned a set of cat-
egories. There are over 100K categories in
Wikipedia, many are extremely fine-grained and
contain very few pages. The value of the Wikipedia
category structure for knowledge acquisition has
long been noticed in several influential works, such
as (Suchanek et al., 2007; Nastase and Strube, 2008)
to name a few. However, while the recall of the
above resources is excellent, we found their preci-
sion insufficient for our purposes. We have imple-
mented a simple high-precision low-recall heuris-
tic for extracting the head words of Wikipedia cat-
egories as follows.

We noticed that Wikipedia categories have a sim-
ple structure of either <noun-phrase> or <noun-
phrase><relation-token><noun-phrase>, where
in the second case the category information is al-
ways on the left. Therefore, we first remove the
text succeeding a set of carefully chosen relation to-
kens5. With this heuristic “Recipients of the Gold
Medal of the Royal Astronomical Society” becomes
just “Recipients”; “Populated places in Africa” be-
comes “places”; however “Institute for Advanced
Study faculty” becomes “Institute” (rather than
“faculty”). At the second step, we apply the Illi-
nois POS tagger and keep only the tokens labeled as
NNS. This step allows us to exclude singular nouns
incorrectly identified as heads, such as “Institute”
above. To further reduce the noise in the category

4We justify the reasons for our choice of high-precision low-
recall knowledge extraction in Sec. 3.2.

5The selected set was: {of, in, with, from, ”,”, at, who,
which, for, and, by}

1236



extraction, we also remove all rare category tokens
which appeared in less than 100 titles ending up with
2088 fine-grained entity types. We manually map
popular fine-grained categories to coarser-grained
ones, more consistent with ACE entity typing. A
sample of the mapping is shown in the table below:

Fine-grained Coarse-grained

departments, organizations, banks, . . . ORG

venues, trails, areas, buildings, . . . LOC

countries, towns, villages, . . . GPE

churches, highways, schools, . . . FACILITY
Manual inspection of the extracted category key-
words has led us to believe that this heuristic
achieves a higher precision at a considerable loss
of recall when compared to the more sophisticated
approach of (Nastase and Strube, 2008), which
correctly identifies “faculty” as the head of “Insti-
tute for Advanced Study faculty”, but incorrectly
identifies “statistical organizations” as the head of
“Presidents of statistical organizations” in about
half the titles containing the category6.

We assign gender to the titles using the follow-
ing simple heuristic. The first paragraph of each
Wikipedia article provides a very brief summary
of the entity in focus. If the first paragraph of a
Wikipedia page contains the pronoun “she”, but not
“he”, the article is considered to be about a female
(and vice-versa). However, when the page is as-
signed a non-person-related fine-grained NE type
(e.g. school) and at the same time is not assigned
a person-related fine-grained NE type (e.g. novel-
ist), we mark the page as inanimate regardless of the
presence of he/she pronouns. The nationality is as-
signed by matching the tokens in the original (un-
processed) categories of the Wikipedia page to a list
of countries. We assign nationality not only to the
Wikipedia titles, but also to single tokens. For each
token, we track the list of titles it appears in, and if
the union of the nationalities assigned to the titles it
appears in is less than 7, we mark the token compat-
ible with these nationalities. This allows us to iden-
tify Ivan Lebedev as Russian and Ronen Ben-Zohar
as Israeli, even though Wikipedia may not contain
pages for these specific people.

6 (Nastase and Strube, 2008) analyze a set of categories S
assigned to Wikipedia page p jointly, hence the same category
expression can be interpreted differently, depending on S.

3.2 Injecting Knowledge Attributes

Once we have extracted the knowledge attributes of
Wikipedia pages, we need to inject them into the
mentions. (Rahman and Ng, 2011) used YAGO for
similar purposes, but noticed that knowledge injec-
tion is often noisy. Therefore they used YAGO only
for mention pairs where one mention was an NE
of type PER/LOC/ORG and the other was a com-
mon noun. This implies that all MISC NEs were
discarded, and all NE-NE pairs were discarded as
well. We also note that (Rahman and Ng, 2011)
reports low utility of FrameNet-based features. In
fact, when incrementally added to other features in
cluster-ranking model the FrameNet-based features
sometimes led to performance drops. This observa-
tion has motivated our choice of high-precision low-
recall heuristic in Sec. 3.1 and will motivate us to
add features conservatively when building attribute
compatibility metric in Sec. 3.3.

Additionally, while (Rahman and Ng, 2011) uses
the union of all possible meanings a mention may
have in Wikipedia, we deploy GLOW (Ratinov et
al., 2011)7, a context-sensitive system for disam-
biguation to Wikipedia. Using context-sensitive dis-
ambiguation to Wikipedia as well as high-precision
set of knowledge attributes allows us to inject the
knowledge to more mention pairs when compared
to (Rahman and Ng, 2011). Our exact heuristic for
injecting knowledge attributes to mentions is as fol-
lows:
Named Entities with Wikipedia Disambiguation
If the mention head is an NE matched to a Wikipedia
page p by GLOW, we import all the knowledge at-
tributes from p. GLOW allows us to map “Ephraim
Sneh” to http://en.wikipedia.org/wiki/Efraim Sneh
and to assign it the Israeli nationality, male gender,
and the fine-grained entity types: {member, politi-
cian, person, minister, alumnus, physician, gen-
eral}.
Head and Extent Keywords
If the mention head is not mapped to Wikipedia by
GLOW and the head contains keywords which ap-
pear in the list of 2088 fine-grained entity types,
then the rightmost such keyword is added to the list
of mention knowledge attributes. If the head does

7Available at: http://cogcomp.cs.illinois.
edu/page/software_view/Wikifier

1237



not contain any entity-type keywords but the extent
does, we add the rightmost such keyword of the ex-
tent. In both cases, we apply the heuristic of re-
moving clauses starting with a select set of punctua-
tions, prepositions and pronouns, annotating what is
left with POS tagger and restricting to noun tokens
only8. This allows us to inject knowledge to men-
tions unmapped to Wikipedia, such as: “{current
Cycle World publisher [Larry Little]}”, which is as-
signed the attribute publisher but not world or cy-
cle. Likewise, “{[Joseph Conrad Parkhurst], who
founded the motorcycle magazine Cycle World in
1962 }”, is not assigned the attribute magazine,
since the text following “who” is discarded.

3.3 Learning Attributes Compatibility
In the previous section we have assigned knowledge
attributes to the mentions. Some of this information,
such as gender and coarse-grained entity types are
also modeled in the baseline system of (Bengtson
and Roth, 2008). Our goal is to build a compatibility
metric on top of this redundant, yet often inconsis-
tent information.

The majority of the features we are using are
straightforward, such as: (1) whether the two men-
tions mapped to the same Wikipedia page, (2)
gender agreement (both Wikipedia and dictionary-
based), (3) nationality agreement (here we measure
only whether the sets intersect, since mentions can
have multiple nationalities in the real world), (4)
coarse-grained entity type match, etc.

The only non-trivial feature is measuring com-
patibility between sets of fine-grained entity types,
which we describe below. Let us assume that men-
tion m1 was assigned the set of fine-grained entity
types S1 and the mention m2 was assigned the set
of fine-grained entity types S2. We record whether
S1 and S2 share elements. If they do, than, in addi-
tion to the Boolean feature, the list of the shared el-
ements also appears as a list of discrete features. We
do the same for the most similar and most dissimilar
elements of S1 and S2 (along with their discretized
similarity score) according to a WordNet-based sim-
ilarity metric of (Do et al., 2009). The reason for ex-
plicitly listing the shared, the most similar and dis-

8This heuristic is similar to the one we used for extracting
Wikipedia category headwords and seems to be a reasonable
baseline for parsing noun structures (Vadas and Curran, 2008).

similar elements is that the WordNet similarity does
not always correspond to co-reference compatibil-
ity. For example, the pair (company, rival) has a
low similarity score according to WordNet, but char-
acterizes co-referent mentions. On the other hand,
the pair (city, region) has a high WordNet simi-
larity score, but characterizes non-coreferent men-
tions. We want to allow our system to “memorize”
the discrepancy between the WordNet similarity and
co-reference compatibility of specific pairs.

We also note that we generate a set of selected
conjunctive features, most notably of fine-grained
categories with NER predictions. The reason is
that the pair of mentions “(Microsoft, Google)” are
not co-referent despite the fact that they both have
the company attribute. On the other hand “(Mi-
crosoft, Redmond-based company)” is a co-referent
pair. To capture this difference, we generate the
feature ORG-ORG&&share attribute for the first
pair, and ORG-O&&share attribute for the second
pair9. These features are also used in conjunction
with string edit distance. Therefore, if our system
sees two named entities which share the same fine-
grained type but have a large string edit distance, it
will label the pair as non-coref.

4 Learning-based Multi-Sieve Aproach

State-of-the-art machine-learning co-ref systems,
e.g. (Bengtson and Roth, 2008; Rahman and Ng,
2011) train a single model for predicting co-
reference of all mention pairs. However, rule-based
systems, e.g. (Haghighi and Klein, 2009; Raghu-
nathan et al., 2010) characterize mention pairs by
discourse structure and linguistic properties and ap-
ply rules in a prescribed order (high-precision rules
first). Somewhat surprisingly, such hybrid approach
of applying rules on top of structures produced by
statistical tools (such as dependency parse trees) per-
forms better than pure machine-learning approach10.

In this work, we attempt to integrate the strength
of linguistically motivated rule-based systems with
the robustness of a machine learning approach. We
started with a hypothesis that different types of men-

9The head of “Redmond-based company” is “company”,
which is not a named entity, and is marked O.

10(Raghunathan et al., 2010) recorded the best result on
CoNLL 2011 shared task.

1238



tion pairs may require a different co-ref model. For
example, consider the text below:

Queen Rania of Jordan , Egypt’s [Suzanne Mubarak]m1 and
others were using their charisma and influence to campaign
for equality of the sexes. [Mubarak]m2 , wife of Egyptian
President [Hosni Mubarak]m3 , and one of the conference
organizers, said they must find ways to . . .

There is a subtle difference between mention pairs
(m1, m2) and (m2, m3). One of the differences is
purely structural. The first pair appears in different
sentences, while the second pair – in the same sen-
tence. It turns out that string edit distance feature be-
tween two named entities has different “semantics”
depending on whether the two mentions appear in
the same sentence. The reason is that to avoid redun-
dancy, humans refer to the same entity differently
within the sentence, preferring titles, nicknames and
pronouns. Therefore, when a similar-looking named
entities appear in the same sentence, they are ac-
tually likely to refer to different entities. On the
other hand, in the sentence “Reggie Jackson, nick-
named Mr. October . . . ” we have to rely heavily on
sentence structure rather than string edit distance to
make the correct co-ref prediction.

Trained on Sieve-specific
Sieve All Data Training
AllSentencePairs 61.37 67.46
ClosestNonProDiffSent 60.71 63.33
NonProSameSentence 62.97 63.80
NerMentionsDiffSent 86.44 87.12
SameSentenceOneNer 64.10 68.88
Adjacent 71.00 78.80
SameSenBothNer 75.30 73.75
Nested 76.11 79.00

Table 1: F1 performance on co-referent mention pairs by sieve
type when trained with all data versus sieve-specific data only.

Our second intuition is that “easy-first” inference
is necessary to effectively leverage knowledge. For
example, in Fig. 3, our goal is to link vessel to
Kursk and assign it the Russian/Soviet nationality
prior to applying the pairwise co-reference classi-
fier on (vessel, Norwegian ship). Therefore, our
goal is to apply the pairwise classifier on pairs in
prescribed order and to propagate the knowledge
across mentions. The ordering should be such that
(a) maximum amount of information is injected at
early stages (b) the precision at the early stages is as

high as possible (Raghunathan et al., 2010). Hence,
we divide the mention pairs as follows:

Nested: are pairs such as “{{[city]m1} of [Jerusalem]m2}”
where the extent of one of the mentions contains the extent of

the other. For some mentions, the extent is the entire clause, so

we also added a requirement that mention heads are at most 7

tokens apart. Intuitively, it is the easiest case of co-reference.

There are 5,804 training samples and 992 testing samples, out

of which 208 are co-referent.
SameSenBothNer: are pairs of named entities which appear
in the same sentence. We already saw an example for this

case involving [Mubarak]m2 and [Hosni Mubarak]m3. There

are 13,041 training samples and 1,746 testing samples, out of

which 86 are co-referent.
Adjacent: are pairs of mentions which appear closest to each
other on the dependency tree. We note that most of the nested

pairs are also adjacent. There are training 5,872 samples and

895 testing samples, out of which 219 are co-referent.
SameSentenceOneNer: are pairs which appear in the same
sentence and exactly one of the mentions is a named entity, and

the other is not a pronoun. Typical pairs are “Israel-country”,

as opposed to “Bill Clinton - reporter”. This type of pairs is

fairly difficult, but our hope is to use encyclopedic knowledge

to boost the performance. There are 15,715 training samples

and 2,635 testing samples, out of which 207 are co-referent.
NerMentionsDiffSent: are pairs of mentions in different sen-
tences, both of which are named entities. There are 189,807

training samples and 24,342 testing samples, out of which 1,628

are co-referent.
NonProSameSentence: are pairs in the same sentence, where
both mentions are non-pronouns. This sieve includes all the

pairs in the SameSentenceOneNer sieve. Typical pairs are

“city-capital” and “reporter-celebrity”. There are 33,895

training samples and 5,393 testing samples, out of which 336

are co-referent..
ClosestNonProDiffSent: are pairs of mentions in different sen-
tences with no other mentions between the two. 3,707 train-

ing samples and 488 testing samples, out of which 38 are co-

referent.
AllSentencePairs: All mention pairs within same sentence.
There are 49,953 training samples and 7,809 testing samples,

out of which 846 are co-referent.
TopSieve: The set of mention pairs classified by the baseline
system. 525,398 training samples and 85,358 testing samples,

out of which 1,387 are co-referent.
In Tab. 1 we compare the performance at each

sieve in two scenarios11. First, we train with the en-
tire 525,398 training samples, and then we train on

11The data is described in Sec. 6.1.

1239



whatever training data is available for the specific
sieve12. We were surprised to see that the F1 on the
nested mentions, when trained on the 5,804 sieve-
specific samples improves to 79.00 versus 76.11
when trained on the 525,398 top sieve samples.

There are several things to note when interpreting
the results in Tab 1. First, the sheer ratio of positive
to negative samples fluctuates drastically. For exam-
ple, 208 out of the 992 testing samples at the nested
sieve are positive, while there are only 86 positive
samples out of 1,746 testing samples in the Same-
SenBothNer sieve. It seems unreasonable to use the
same model for inference at both sieves. Second, the
data for intermediate sieves is not always a subset of
the top sieve. The reason is that top sieve extracts
a positive instance only for the closest co-referent
mentions, while sieves such as AllSentencePairs ex-
tract samples for all co-referent pairs which appear
in the same sentence. Third, while our division to
sieves may resemble witchcraft, it is motivated by
the intuition that mentions appearing close to one
another are easier instances of co-ref as well as lin-
guistic insights of (Raghunathan et al., 2010).

5 Entity-Based Features

In this section we describe our approach for build-
ing entity-based features. Let {C1, C2, . . . CN} be
the set of sieve-specific classifiers. In our case, C1 is
the nested mention pairs classifier, C2 is the Same-
SenBothNer classifier, and C9 is the top sieve clas-
sifier. We design entity-based features so that the
subsequent sieves “see” the decisions of the previ-
ous sieves and use entity-based features based on the
intermediate clustering. However, unlike (Raghu-
nathan et al., 2010), we allow the subsequent sieves
to change the decisions made by the lower sieves
(since additional information becomes available).

5.1 Intermediate Clustering Features (IC)
Let Ri(m) be the set of all mentions which, when
paired with the mention m, form valid sample pairs
for sieve i. E.g. in our running example of Fig. 1,

12We report pairwise performance on mention pairs because
it is the more natural metric for the intermediate sieves. We
report only performance on co-referent pairs, because for many
sieves, such as the top sieve, 99% of the mention pairs are non-
coreferent, hence the baseline of labeling all samples as non-
coreferent would result in 99% accuracy. We are interested in a
more challenging baseline, the co-referent pairs.

R2([Kursk]m2) = {[Barents Sea]m3}, since both
m1 and m2 are NEs and appear in the same sen-
tence. Let R+i (m) be the set of all mentions which
were labeled as co-referent to the mention m by the
classifier Ci (including m, which is co-referent to
itself). We define R−i (m) similarly. We denote the
union of mentions co-refed to m during inference
up to sieve i as E+i (m) = ∪

i−1
j=1R

+
j (m). Similarly,

E−i (m) = ∪
i−1
j=1R

−
j (m). Using these definitions

we can introduce entity-based prediction features
which allow subsequent sieves to use information
aggregated from previous sieves:

ICRi (mj , mk) =


−1 mj ∈ R−i−1(mk)
+1 mj ∈ R+i−1(mk)
0 Otherwise

ICEi (mj , mk) =


−1 mj ∈ E−i−1(mk)
+1 mj ∈ E+i−1(mk)
0 Otherwise

ICRi stores the pairwise prediction history, thus
when classifying a pair (mj , mk) at sieve i, a
classifier can see the predictions of all the previous
sieves applicable on that pair. ICEi stores the
transitive closures of the sieve-specific predictions.
We note that both ICRi and IC

E
i can have the values

+1 and -1 active at the same time if intermediate
sieve classifiers generated conflicting predictions.
However, a classifier at sieve i will use as features
both ICR1 ,. . . IC

R
i−1 and IC

E
1 ,. . . IC

E
i−1, thus it

will know the lowest sieve at which the conflicting
evidence occurs. The classifier at sieve i also
uses set identity, set containment, set overlap and
other set comparison features between E+/−i−1 (mj)

and E+/−i−1 (mk). We check whether the sets have
symmetric difference, whether the size of the
intersection between the two sets is at least half
the size of the smallest set etc. We also generate
subtypes of set comparison features when restricting
the elements to NE-mentions and non-pronominal
mentions (e.g “what percent of named entities do
the sets have in common?”).

5.2 Surface Form Compatibility (SFC)

The intermediate clustering features do not allow us
to generalize predictions from pairs of mentions to
pairs of surface strings. For example, if we have
three mentions: {[vessel]m1 , [Kursk]m2 , [Kursk]m5},
then the prediction on the pair (m1, m2) will not be

1240



(B)aseline (B)+Knowledge (B)+Predictions (B)+Knowledge+Predictions
TopSieve 66.58 69.08 68.77 70.43
AllSentencePairs 67.46 71.79 69.59 73.50
ClosestNonProDiffSent 63.33 65.62 65.57 70.76
NonProSameSentence 63.80 69.62 67.03 71.11
NerMentionsDiffSent 87.12 88.23 88.68 89.07
SameSentenceOneNer 68.88 70.58 67.89 73.17
Adjacent 78.80 81.32 80.00 81.79
SameSenBothNer 73.75 80.50 77.21 80.98
Nested 79.00 83.59 80.65 83.37

Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves. Both knowledge and
entity-based features significantly and independently improve the performance for all sieves. The goal of entity-based features is
to propagate knowledge effectively, thus it is encouraging that the combination of entity-based and knowledge features performed
significantly better than any of the approaches individually at the top sieve.

used for the prediction on the pair (m1, m5), even
though in both pairs we are asking whether Kursk
can be referred to as vessel. The surface form com-
patibility features mirror the intermediate clustering
features, but relax mention IDs and replace them
by surface forms. Similarly to intermediate cluster-
ing features, both +1 and -1 values can be active at
the same time. We also generate subtypes of set-
comparison features for NE-mentions and optionally
stemmed non-pronominal mentions. For example,
in a text discussing President Clinton and President
Putin, some instances of the surface from president
will refer to Putin but not Clinton and vice-versa.
Therefore, both for (Putin, president) and for (Clin-
ton, president), the surface from compatibility will
be +1 and -1 simultaneously. This indicates to the
system that Putin can be referred to as president, but
president can refer to other entities in the document
as well.

6 Experiments and Results

6.1 Data
We use the official ACE 2004 English training
data (NIST, 2004). We started with the data split
used in (Culotta et al., 2007), which used 336 doc-
uments for training and 107 documents for testing.
We note that ACE data contains both newswire text
and transcripts. In this work, we are using NLP tools
such as POS tagger, named entity recognizer, shal-
low parser, and a disambiguation to Wikipedia sys-
tem to inject expressive features into a co-reference
system.

Unfortunately, current state-of-the-art NLP tools

do not work well on transcribed text. Therefore, we
discard all the transcripts. Our criteria was simple.
The ACE annotators have marked the named enti-
ties both in newswire and in the transcripts. We kept
only those documents which contained named en-
tities (according to manual ACE annotation) and at
least 1/3 of the named entities started with a capital
letter. After this pre-processing step, we were left
with 275 out of the original 336 training documents,
and 42 out of the 107 testing documents.

For the experiments throughout this paper, fol-
lowing Culotta et al. (Culotta et al., 2007) and much
other work, to make experiments more compara-
ble across systems, we assume that perfect mention
boundaries and mention type labels are given. How-
ever, we do not use the gold named entity types such
as person/location/facility etc. available in the data.
In all experiments we automatically split words and
sentences, and annotate the text with part-of-speech
tags, named entities and cross-link concepts from
the text to Wikipedia using publicly available tools.

6.2 Ablation Study

In Tab. 2 we report the pairwise F1 scores on co-
referent mention pairs broken down by sieve and
using different components. This allows us to see,
for example, that adding only the knowledge at-
tributes improved the performance at NonProSame-
Sentence sieve from 63.80 to 69.62. We have or-
dered the sieves according to our initial intuition of
“easy first”. We were surprised to see that co-ref res-
olution for named entities in the same sentence was
harder than cross-sentence (73.75 vs. 87.12 base-

1241



 75

 76

 77

 78

 79

 80

 81

 82

0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0

F
1
 -

 M
U

C

Confidence threshold for a positive prediction

Baseline
Knowledge&Predictions

KnowledgeOnly
PredictionsOnly

 81.5

 82

 82.5

 83

 83.5

 84

 84.5

 85

 85.5

0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0

F
1
 -

 B
3

Confidence threshold for a positive prediction

Baseline
Knowledge&Predictions

KnowledgeOnly
PredictionsOnly

 73

 74

 75

 76

 77

 78

 79

 80

0.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0

F
1

 -
 C

E
A

F

Confidence threshold for a positive prediction

Baseline
Knowledge&Predictions

KnowledgeOnly
PredictionsOnly

Figure 4: End performance for various systems.

line F1). We were also surprised to see that resolv-
ing all mention pairs within sentence when includ-
ing pronouns was easier than resolving pairs where
both mentions were non-pronouns (67.46 vs. 63.80
baseline F1).

We note that conceptually, the nested
(B)+Predictions sieve should be identical to
the baseline. However, in practice, the surface
form compatibility (SFC) features are generated
for the nested sieve as well. Given two mentions
m1 and m2, the SFC features capture how many
surface forms E+(m1) and E+(m2) share. At the
nested sieve, E+(m) and R+(m) are just m, which
is identical to string comparison features already
existing in the baseline system. While the SFC
features do not add new information, they influence
the weight the features get (essentially leading to
a different regularization), which in turn leads to
slightly different results.

6.3 End system performance
Recall that the Best-Link algorithm applies transi-
tive closure on the graph generated by thresholding
the pairwise co-reference scoring function pc. The
lower the threshold on the positive prediction, the
lower is the precision and the higher is the recall. In
Fig. 4 we compare the end clustering quality across
a variety of thresholds and for various system fla-
vors using three metrics: MUC (Vilain et al., 1995),
B3 (Bagga and Baldwin, 1998) and CEAF (Luo,
2005)13. The purpose of this comparison is to see
the impact of the knowledge and the prediction fea-
tures on the final output and to see whether the per-
formance gains are due to (mis-)tuning of one of
the systems or are they consistent across a variety
of thresholds.

The end performance of the baseline system
on our training/testing split peaks at around 78.39
MUC, 83.03 B3 and 77.52 CEAF, which is higher
(e.g. 3 B3 F1 points) than the originally reported
result on the entire dataset (which includes the tran-
scripts). This is expected, since well-formed text is
easier to process than transcripts. We note that our
baseline is a state-of-the art system which recorded
the highest B3 and BLANC scores at CoNLL 2011
shared task and took the third place overall. Fig. 4
shows a minimum improvement of 3 MUC, 2 B3

and 1.25 CEAF F1 points across all thresholds when
comparing the baseline to our end system. Surpris-
ingly, the knowledge features outperformed predic-
tion features on pairwise, MUC and B3 metrics, but
not on the CEAF metric. This shows that pairwise
performance is not always indicative of cluster-level
performance for all metrics.

7 Conclusions and Related Work

To illustrate the strengths of our approach, let us
consider the following text:

Another terminal was made available in {[Jiangxi]m1}, an
{inland [province]m2}. . . . The previous situation whereby
large amount of goods for {Jiangxi [province]m3} had to
be re-shipped through Guangzhou and Shanghai will be

changed completely.

The baseline system assigns each mention to a
separate cluster. The pairs (m1, m2) and (m1, m3)

13In the interest of space, we refer the reader to the literature
for details about the different metrics.

1242



are misclassified because the baseline classifier does
not know that Jiangxi is a province and the preposi-
tion an before m2 is interpreted to mean it is a pre-
viously unmentioned entity. The pair (m2, m3) is
misclassified because identical heads have different
modifiers, as in (big province, small province). Our
end system first co-refs (m1, m2) at the AllSameSen-
tence sieve due to the knowledge features, and then
co-refs (m1, m3) at the top sieve due to surface form
compatibility features indicating that province was
observed to refer to Jiangxi in the document. The
transitivity of Best-Link takes care of (m2, m3).

However, our approach has multiple limitations.
Entity-based features currently do not propagate
knowledge attributes directly, but through aggregat-
ing pairwise predictions at knowledge-infused inter-
mediate sieves. We rely on gold mention bound-
aries and exhaustive gold co-reference annotation.
This prevented us from applying our approach to
the Ontonotes dataset where singleton clusters and
co-referent nested mentions are removed. There-
fore the gold annotation for training several sieves
of our scheme is missing (e.g. nested mentions).
Another limitation is our somewhat preliminary di-
vision to sieves. (Vilalta and Rish, 2003) have ex-
perimented with approaches for automatic decom-
position of data to subclasses and learning multiple
models to improve data separability. We hope that
similar approach would be useful for co-reference
resolution. Ideally, we want to make “simple de-
cisions” first, similarly to what was done in (Gold-
berg and Elhadad, 2010) for dependency parsing,
and model clustering as a structured problem, sim-
ilarly to (Joachims et al., 2009; Wick et al., 2011).
However, our experience with multi-sieve approach
with classifiers suggests that a single model would
not perform well for both lower sieves with little
entity-based information and higher sieves with a lot
of entity-based features. Addressing the aforemen-
tioned challenges is a subject for future work.

There has been an increasing interest in
knowledge-rich co-reference resolution (Ponzetto
and Strube, 2006; Haghighi and Klein, 2010; Rah-
man and Ng, 2011). We use Wikipedia differently
from (Ponzetto and Strube, 2006) who focus on
using WikiRelate, a Wikipedia-based relatedness
metric (Strube and Ponzetto, 2006). (Rahman and
Ng, 2011) used the union of all possible inter-

pretations a mention may have in YAGO, which
means that Michael Jordan could be co-refed both
to a scientist and basketball player in the same
document. Additionally, (Rahman and Ng, 2011)
use exact word matching, relying on YAGO’s ability
to extract a comprehensive set of facts offline14. We
are the first to use context-sensitive disambiguation
to Wikipedia, which received a lot of attention re-
cently (Bunescu and Pasca, 2006; Cucerzan, 2007;
Mihalcea and Csomai, 2007; Milne and Witten,
2008; Ratinov et al., 2011). We extract context-
sensitive, high-precision knowledge attributes from
Wikipedia pages and apply (among other features)
WordNet similarity metric on pairs of knowledge
attributes to determine attribute compatibility.

We have integrated the strengths of rule-based
systems such as (Haghighi and Klein, 2009; Raghu-
nathan et al., 2010) into a multi-sieve machine learn-
ing framework. We show that training sieve-specific
models significantly increases the performance on
most intermediate sievesieves.

We develop a novel approach for entity-based in-
ference. Unlike (Rahman and Ng, 2011) who con-
struct entities left-to-right, and similarly to (Raghu-
nathan et al., 2010) we resolve easy instances of co-
ref to reduce error propagation in entity-based fea-
tures. Unlike (Raghunathan et al., 2010), we al-
low later stages of inference to change the decisions
made at lower stages as additional entity-based evi-
dence becomes available.

By adding word-knowledge features and refin-
ing the inference, we improve the performance of a
state-of-the-art system of (Bengtson and Roth, 2008)
by 3 MUC, 2 B3 and 2 CEAF F1 points on the non-
transcript portion of the ACE 2004 dataset.

References

A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.

E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.

14YAGO uses WordNet to expand its set of facts. For ex-
ample, if Martha Stewart is assigned the meaning personality
from category head words analysis, YAGO adds the meaning
celebrity because personality is a direct hyponym of celebrity in
WordNet. However, this is done offline in a context-insensitive
way, which is inherently limited.

1243



R. C. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
EACL.

S. Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In EMNLP-
CoNLL.

A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL, pages 81–88.

Q. Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.
2009. Robust, light-weight approaches to compute
lexical similarity. Technical report, University of Illi-
nois at Urbana-Champaign.

A. Fader, S. Soderland, and O. Etzioni. 2009. Scaling
wikipedia-based named entity disambiguation to arbi-
trary web text. In WikiAI (IJCAI workshop).

Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In NAACL.

A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In EMNLP.

A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In HLT-ACL. As-
sociation for Computational Linguistics.

T. Joachims, T. Hofmann, Y. Yue, and C. Yu. 2009.
Predicting structured objects with support vector ma-
chines. Communications of the ACM, Research High-
light, 52(11):97–104, November.

X. Luo. 2005. On coreference resolution performance
metrics. In HLT.

R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.

D. Milne and I. H. Witten. 2008. Learning to link with
wikipedia. In CIKM.

V. Nastase and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In AAAI.

V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.

NIST. 2004. The ace evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.

S. P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, wordnet and wikipedia for coreference
resolution. In HLT-ACL.

K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.

A. Rahman and V. Ng. 2011. Coreference resolution
with world knowledge. In HLT-ACL.

L. Ratinov, D. Downey, M. Anderson, and D. Roth.
2011. Local and global algorithms for disambiguation
to wikipedia. In ACL.

M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the Twenty-First National Conference
on Artificial Intelligence, July.

F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A core of semantic knowledge. In WWW.

D. Vadas and J. R. Curran. 2008. Parsing noun phrase
structure with CCG. In ACL.

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6, pages 45–52.

R. Vilalta and I. Rish. 2003. A decomposition of classes
via clustering to explain and improve naive bayes. In
ECML.

M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, and
A. McCallum. 2011. Samplerank: Training factor
graphs with atomic gradients. In ICML.

1244


