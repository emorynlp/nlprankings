















































A New Concept of Deep Reinforcement Learning based Augmented General Tagging System


Proceedings of the 27th International Conference on Computational Linguistics, pages 1683â€“1693
Santa Fe, New Mexico, USA, August 20-26, 2018.

1683

A New Concept of Deep Reinforcement Learning based Augmented
General Sequence Tagging System

Yu Wang
Samsung Research America

Abhishek Patel
Samsung Research America

Hongxia Jin
Samsung Research America

Abstract

In this paper, a new deep reinforcement learning based augmented general sequence tagging
system is proposed. The new system contains two parts: a deep neural network (DNN) based
sequence tagging model and a deep reinforcement learning (DRL) based augmented tagger. The
augmented tagger helps improve system performance by modeling the data with minority tags.
The new system is evaluated on SLU and NLU sequence tagging tasks using ATIS and CoNLL-
2003 benchmark datasets, to demonstrate the new systemâ€™s outstanding performance on general
tagging tasks. Evaluated by F1 scores, it shows that the new system outperforms the current
state-of-the-art model on ATIS dataset by 1.9 % and that on CoNLL-2003 dataset by 1.4 %.

1 Introduction

Sequence tagging/labeling is one of the general techniques required for implementing different natu-
ral/spoken language understanding (NLU/SLU) tasks, such as: Named-entity recognition (NER), Part-
of-speech (POS) Tagging in NLU, or Slot filling in SLU. Though the purpose of the tasks and their
sequence labels are different, most of the state-of-the-art results for these tasks are generated using neu-
ral network based architectures. For example, Ma et al. demonstrates that their BLSTM-CNN-CRF
model (Ma and Hovy, 2016) can achieve state-of-the-art performance for the NER task (without using
extra features) on CoNLL-2003 dataset; Liu et al. gives an attention-based bi-directional LSTM model
(Liu and Lane, 2016) for the slot-filling task, which also gives the best performance on the popular ATIS
dataset (Hemphill et al., 1990) for SLU tasks (This result is obtained by intial paper submission, a bet-
ter result is published later by Yu et al. (Wang et al., 2018)). The details of these related works will be
given in next background section. The solution for general sequence tagging tasks (like NER/Slot filling)
should mainly contain two properties: the first one is that the model can handle the language sequence in
a word/token level manner, i.e., it can read in the sequence word by word and outputs their correspond-
ing labels; the second property is that the model should capture the contextual information/constraints
which helps understand the word/tokenâ€™s tags, either as named-entity or simply slot tags. In(Lample et
al., 2016; Ma and Hovy, 2016), the authors use a conditional random filed (CRF) to assist their basic
bidirectional LSTM model to capture the contextual constraints in NER task. Similarly, Liu et al. uses
an attention based mechanism to take advantages of the weighted collective information from the hidden
states of all words in an utterance.

Due to the imbalance of data distribution under different labels, normally the evaluations for different
models will be based on their precision and recall, or simply the F1 scores. It can be observed that,
though the neural network based models are becoming more and more complicated, it becomes harder to
make further improvement on these tagging tasks by using more advanced network structures. The main
reasons are from two aspects:
1. Though the tagging models are designed to be more and more complicated, they also becomes more
likely to be over-fitting by using more layers, hidden nodes or advance recurrent neural network structures

This work is licensed under a Creative Commons Attribution 4.0 International License. License details:
http://creativecommons.org/licenses/by/4.0/
*: The authorsâ€™ emails are {yu.wang1@samsung.com, abhishek.p@partner.samsung.com, hongxia.jin@samsung.com}.



1684

(like bidirectional LSTM). A modelâ€™s performance may easily reach a bottom-neck by simply tuning
hyper-parameters or changing network structures.
2. Considering the fact that the numbers of words/tokens under some wrongly labeled tags may be quite
small, it may not give us more improvement if we still use the same group of training data by simply
changing the model. Normally a model will focus on finding the pattern of the data under majority
tags instead of minority. Sometimes, even a new model can improve the performance on minority tagsâ€™
classes, at the cost of degrading the modelâ€™s performance on the majority tags with more training data,
which is a common issue resulting from the imbalanced data distribution under different tags (He and
Garcia, 2009; Chawla et al., 2004).

Weighted sampling is one general solution to resolve the imbalanced data issue by giving more weights
to minority tags and fewer to the majority ones (He and Garcia, 2009; Sun et al., 2009). The tech-
nique, however, gives the issue of distorting the original tagging distribution, which may adversely affect
modelâ€™s performance on majority tags. Similar approach, like adaptive synthetic sampling (He et al.,
2008), also suffers from the same problem.

This gives us the dilemma on dealing with imbalanced tagged data (either for slot filling or NER task):
On the one hand, it is a non-trivial task to re-organizing/sampling the data under different tags without
any distortion of information; on the other hand, the improvement by changing models has an upper
bound which is mainly decided by the data pattern itself instead of the model structure. Due to these two
reasons, in this paper, a new concept of augmented tagging system based on deep reinforcement learning
is proposed to furture improve the performances of sequence labeling tasks without sacrificing their
original data distributions. This novel system will use a deep reinforcement learning based compensatory
model to capture the wrong labeled tags and learn their correct labels, hence it can improve the whole
model system performance without affecting the original correctly labeled tags. Detailed experiments
will be further conducted on two sequence labeling tasks in the domain of SLU/NLU, as NER and slot
filling, using public datasets.

The paper is organized as following: In section 2, a brief overview of two common NLP/SLU tagging
tasks, i.e. NER and Slot filling, are given. Their current state-of-the-art models will also be explained
briefly. Section 3 gives a background of deep reinforcement learning (DRL) first, then illustrates our
DRL based augmented general tagging system (DAT) in detail. In section 4, two different tagging tasks
on different datasets are given, One is a slot filling task performed on the ATIS benchmark dataset. The
other is a named-entity recognition (NER) task, tested on the CoNLL-2003 benchmark dataset. Both
experiments will compare our new algorithm with their current best-performed models with state-of-the-
art results.

2 Background

In this section, a brief background overview on the models for two sequence labeling tasks will be given.
Their state-of-the-art neural network structures will also be discussed for further comparison purpose.

2.1 Slot filling in SLU

Slot filling task is one of the fundamental tasks in spoken language understanding. It sequentially labels
the words in an utterance using the pre-defined types of attributes or slot tags. The most straight-forward
approach for this task is by using a single recurrent neural network (RNN) to generate sequential tags
by reading an input sentence word by word. The current state-of-the-art result on ATIS is an attention
based bidirectional LSTM model (Liu and Lane, 2016). A brief overview about this approach is given
for a better illustration of our new algorithm. The model introduced in (Liu and Lane, 2016) covers both
of the intent detection and slot filling tasks. Considering the focus of this paper, we will only discuss the
slot filling part of the model. The structure of the attention based BLSTM model for slot filling task is as
shown in Figure 1.



1685

ğ’™ğŸ: from ğ’™ğŸ: Denver ğ’™ğŸ‘: to ğ’™ğŸ’: Philadelphia

ğ ğ…ğ«ğ¨ğ¦ğ‹ğ¨ğœ ğ“ğ¨ğ‹ğ¨ğœğ

ğ’„ğŸ ğ’„ğŸ ğ’„ğŸ‘ ğ’„ğŸ’

Bidirectional 

LSTM

ğ’‰ğŸ ğ’‰ğŸ ğ’‰ğŸ‘ ğ’‰ğŸ’

Figure 1: Attention based Bidirectional LSTM for Slot
Filling

Bidirectional 

LSTM

CRF Layer

CNN based

Character-level 

representation

ğ âˆ’ ğğ„ğ‘ ğ„ âˆ’ ğğ„ğ‘ ğ’ âˆ’ ğ‹ğğ‚ğ

ğ’™ğŸ: Mark ğ’™ğŸ: Watney ğ’™ğŸ‘: visited ğ’™ğŸ’: Mars

Figure 2: BLSTM-CNN-CRF Structure for Named-entity
Recognition

As shown in the figure, a contextual vector ci(Â·) is defined using the attention of hidden states hj :

ci =
Lâˆ‘
j=1

Î±i,jhj (1)

where Î±i,j is the attention coefficient defined as:

Î±i,j =
eei,jâˆ‘L
k=1 e

ei,k

ei,k = Ï†(siâˆ’1, hk)

(2)

where Ï†(Â·) is a feed-forward neural network and siâˆ’1 is the decoder hidden state.
This model consists of tw0 main properties:
1. It uses the Bi-direction LSTM (BLSTM) structure to capture the long-term dependencies in both
directions.
2. The attention vector ci gives additional contextual information, for which cannot be captured by the
hidden states in BLSTM.

The model gives the state-of-the-art slot filling performance on ATIS dataset. In this paper, we will
also use this model as a baseline for comparison purpose. Also, this attention based model is chosen as
the DNN part of our new system, i.e.fdnn, to generate the filtered input data to the deep reinforcement
learning based augmented tagger (DAT) sub-model of the system.

2.2 Named-entity recognition in NLU
Another very common NLU sequence labeling task is named-entity recognition (NER), which seeks to
locate and classify entities into some pre-defined named labels. In this section, a brief overview about
the neural network based NER model as in (Ma and Hovy, 2016) will be given. The model gives the
state-of-the-art performance on CoNLL-2003 dataset (without lexicon features).

The structure contains a convolutional neural network for generating character-level representations,
which is used as an input to a bidirectional LSTM network followed by a conditional random field layer.
The conditional random field layer is designed to capture the strong dependency across the labels, which
can be formulated as:

s(X, y) =

nâˆ‘
i=0

Ayi,yi+1 +

nâˆ‘
i=0

Pi,yi

p(y|X) = e
s(X,y)âˆ‘

yÌƒâˆˆYx e
s(X,yÌƒ)

(3)

where s(X, y) is the score defined a predicted labeling sequence y given X . Ayi,yj represents transition
score from the tag yi to tag yj , and Pi,yi is the probability of the yi output tag of the i

th input word



1686

Table 1: Comparison between Minority Tags and Majority Tags on ATIS and CoNLL-2003

ATIS CoNLL-2003
Training Data Test Data Training Data Test Data

# of tag
types

# of
tokens

% of wrongly labeled data # of tag
types

# of
tokens

% of wrongly labeled data

Minority Tags 119 6,323 92% 2 2,312 78%
(<1% of total # of data)

Majority Tags 8 38,707 8% 7 202,255 22%
(â‰¥ 1% of total # of data)

Total 127 45,030 100% 9 204,567 100%

generated by the bidirectional LSTM. YX represents all possible tag sequences for an input sentence
X , and p(y|X) is the predicted output yâ€™s tagging probability giving an input words sequence X =
{x1, x2, Â· Â· Â· , xn}.

The model shown above gives the state-of-the-art result on the English NER task with an F1 score of
91.2 on CoNLL-2003 dataset (without extra features).

Though decent experimental results are obtained by using deep learning models on the slot filling and
NER tagging tasks, the potentials of these models are still limited by the imbalanced data distribution,
since most of the tokens donâ€™t have any entity names (labeled as â€™Oâ€™). The model can easily cover the
majority tags but not the minority ones. Table 1 gives a summary of minority tags (each tag has less
than 1% of entire data) and majority tags on ATIS and CoNLL-2003 datasets, and their corresponding
occurrence ratio among the wrongly labeled data in test dataset.

In Table 1, it shows that about 92% of the wrongly labeled data are from minority tags in ATIS dataset
and 78% in CoNLL03 dataset. This gives us an indication that, in order to further improve the model
performance, the new model needs to figure out how to better label the minority tags and wrongly labeled
tags without sacrificing the model performance on majority tags.

3 Deep Reinforcement Learning

In this section, a novel deep reinforcement learning (DRL) based augmented tagging system is proposed
to address the issue as described earlier. The system contains two parts: one is the original deep neural
network(NN) based tagger (fdnn) as in section 2.1 for slot filling and section 2.2 for NER, the second
part is a deep reinforcement learning (DRL) based augmented tagger (fdat), which can be trained to learn
the correct tags that are labeled wrongly by the deep learning based tagger, such that it can compensate
the weakness of the original single tagger.

3.1 Reinforcement Learning and Deep Q Network (DQN)
Reinforcement learning (RL) is a bit different from the supervised learning and unsupervised learning
algorithm. Formulated as a markov decision process (MDP) , an RL based model mainly contains several
key elements: state (st), action (at), rewards (rt) and policy (Ï€). In a given a state of a stochastic
environment, an agent is seeking its best action to perform in order to maximize its expected rewards to
be obtained, by following some policy. The main target of a RL based model is to seek the best policy,
hence the corresponding action, for an agent to perform. There are mainly three types of reinforcement
learning algorithms: value-based, policy gradient and actor-critic. In our scenario, due to the necessity to
define rewards at each state and small discrete action space, a value-based model design usingQ-learning
is proposed for our problem. Its optimal action-value function QÏ€, which is also the maximum expected
reward obtained by selecting the best policy Ï€, is defined as:

QÏ€(st, at) = max
Ï€

E[Rt | at, st, Ï€] (4)

One well-known issue of applying reinforcement learning in real problems is that, in order to compute
this optimal action-value function QÏ€, it is necessary to store all of the Q values for each state action
pair (st, at) in a table, which is not very practical if the state or action space is large. This problem is
described as â€œthe curse of dimensionalityâ€ by Bellman (Powell, 2007). One approach to overcome it is



1687

Word 

Sequence

Pre-trained Deep NN 

based Tagger ğ’‡ğ’…ğ’ğ’

DRL based Augmented 

Tagger (DAT) ğ’‡ğ’…ğ’‚ğ’•
States

ğ’šğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’•

actionğ’‚ğ’•

ğ’™ğ’•ğ’“ğ’‚ğ’Šğ’

ğ’™ğ’•ğ’“ğ’‚ğ’Šğ’

ğ‘·ğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’• = [ğ’‘ğŸ, ğ’‘ğŸ, â€¦ , ğ’‘ğ’˜]

One hot labels ğ’ğ’âˆ—

Ground Truth ğ’šğ’•ğ’“ğ’‚ğ’Šğ’

ğ’”ğ’• = (ğ’—ğ’Š, ğ’ğ’Š)

Figure 3: Training Model of DRL based Augmented Tag-
ger

Pre-trained Deep NN 

based Tagger ğ’‡ğ’…ğ’ğ’

DRL based Augmented 

Tagger (DAT) ğ’‡ğ’…ğ’‚ğ’•

States

ğ’šğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’•

action ğ’‚ğ’•

ğ’™ğ’•ğ’†ğ’”ğ’•

ğ’”ğ’• = (ğ’—ğ’Š, ğ’ğ’Š)

Filter ğ’šğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’•_ğ’…ğ’ğ’

ğ’šğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’•_ğ’…ğ’‚ğ’•

> ğ‘‡ğ‘Ÿ

â‰¤ ğ‘‡ğ‘Ÿğ’™ğ’…ğ’‚ğ’• = ğ’™ğ’•ğ’†ğ’”ğ’•\ğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’•_ğ’…ğ’ğ’

Combine ğ’šğ’“ğ’†ğ’”ğ’–ğ’ğ’•

Figure 4: Inference Model of DRL based Augmented Tag-
ger

by training a deep neural network based estimatorQ(st, at|Î¸t) to estimateQÏ€(st, at) at time t. However,
these types of estimators tend to be unstable since the convergence cannot be fully guaranteed. Recently,
the deep Q network (DQN) (Mnih et al., 2013; Mnih et al., 2015) demonstrates much better convergence
performance on large state space, by taking advantage of a novel training technique called â€œexperience
replayâ€. In this paper, we will use DQN as our selected deep reinforcement learning structure to satisfy
the model requirement due to large state space.

In DQN structure, the optimal action-value functionQÏ€(st, at) is estimated by a neural network based
estimator Q(st, at|Î¸t), i.e.

Q(st, at|Î¸t) âˆ¼ QÏ€(st, at) (5)

where Î¸t is the neural network parameter, and the state st serves as the input of the network. In next
section we will describe how to use the techniques in DQN to build our augmented tagger from a system
perspective. Also, the states, actions and rewards of the DRL based tagger are also going to be carefully
designed next.

3.2 DRL based Augmented Tagger (DAT) System
The design of DRL based augmented tagging (DAT) system is as given in Figure 3 and Figure 4. Due to
the differences of using augmented tagger in training and inference, we discuss these two parts separately.

3.2.1 Training DAT System
In order to train the entire tagging system, one may need to first pre-train a deep NN based tagger fdnn,
this tagger will follow the design as discussed in section 2.1 for slot filling task or section 2.2 for the
NER task. Once fdnn is fully trained, the output labels of fdnn using training input xtrain are stored as
ypredict with a probability distribution Pypredict = [p1, p2, Â· Â· Â· , pw], where w is the total number of tags
in the entire training dataset. Following is the DRL based design of our new DAT structure:

States (st): The DAT modelâ€™s state st is shown in Figure 5. The state is defined by each word/tokenwi
in sentences, it mainly contains two parts: the first part contains the word level information represented
by an n-gram (n is odd) averaged vector vi, and the other part is a given label li of wi. During the
generation of training states, li uses all possible tags for the word/token wi. vi is defined as the average
of the vector of word sequences from wiâˆ’(nâˆ’1)/2 to wi+(nâˆ’1)/2, where wj is the center of this sequence:

vi =
1

n

j=i+(nâˆ’1)/2âˆ‘
j=iâˆ’(nâˆ’1)/2

wj (6)

Remarks: The reason to use an n-gram vector vi to substitute wi as a word level information in a state
is due to that we want to better capture the contextual information compared to that using a single word
vector. When n = 1, vi is the same as word vector wi. Also, the average of fewer words/tokens can be
used if the index of word sequences is out of the boundary of an input sentence.



1688

Sğ­ğšğ­ğ ğ’”ğ’•

ğ‹ğšğ›ğğ¥ ğ’ğ’Šğ’—ğ’Š =
ğŸ

ğ’
à·

ğ’‹=ğ’Šâˆ’(ğ’âˆ’ğŸ)/ğŸ

ğ’‹=ğ’Š+(ğ’âˆ’ğŸ)/ğŸ

ğ’˜ğ’Š

ğ’˜ğŸ: from ğ’˜ğŸ: Denver ğ’˜ğŸ‘: to
(ğ’ = ğŸ‘, ğ’Š = ğŸ)

ğœğğ§ğ­ğğ« ğ°ğ¨ğ«ğ ğ¢ğ¬ ğ’˜ğ’Š

Averaged 

vector

Figure 5: State of DAT

Action ğ’‚ğ’•

Current state ğ’”ğ’•â€™s label ğ’ğ’Š ğ’ğ’‹New label

Figure 6: Action of DAT

Actions (at): The DAT modelâ€™s action at at time step t is defined as in Figure 6. The action gives a
transition signal such that the state will change from its current label li to its next label lj by keep the
same n-gram vector vi. Simply speaking, the action set A contains all possible labels/tags for the word
vector wi or its correspond n-gram substitute vi, i.e. A = {l1, Â· Â· Â· , lk}. At each time step t, the action at
with highest predicted action-reward value is chosen as:

at = argmax
a

Q(st, a|a âˆˆ A, Î¸t) (7)

Then the state will transit from its current label to the label directed by action at.
Rewards (rt): The reward defined at a state st containing an n-gram vector vi (with a center

word/token wi) will use the one-hot representation olâˆ—i of wiâ€™s true label l
âˆ—
i , the one hot vector oli of

the label li in current state st, and the predicted probability pi for the word wi using fdnn as:

rt = tanh(log(
||olâˆ—i âˆ’ pi||2
||oli âˆ’ olâˆ—i ||2 + ï¿½

)) (8)

where tanh(Â·) is used to normalize the reward to be within -1 and 1, || Â· ||2 is the L2 norm, and ï¿½ is a very
small value added to avoid zero denominator.

The insight behind our reward design is using the ratio of the distance from fdnn predicted label to
wâ€²is true label and that from current stateâ€™s label to its true label. Based on our definition, a higher reward
will be assigned to a state in which its label is more closer to the true label compared with the fdnnâ€™s
predicted one. The reward function is one of the key factors for further improvement using our new
augmented tagger.

Remarks: It is worth noticing that pi should be generated by fdnn using the same word sequence as
preparing vi. Similarly, lâˆ—i is the true label of wi in the same sentence of preparing pi and vi.

Training: The training algorithm for DAT is as shown in Algorithm 1. The loss function is defined
based on the difference of two estimated expected rewards at state st using an unsupervised approach as:

Lst = (QÌ‚(st, at|Î¸t)âˆ’Q(st, at|Î¸t))2

= (rt + Î³max
at+1

Q(st+1, at+1|Î¸t)âˆ’Q(st, at|Î¸t))2
(9)

where Q(st, at|Î¸t) is the predicted expected reward using (st, at) generated by the neural network-based
estimator in DAT directly, and QÌ‚(st, at|Î¸t) is an estimation of Q(st, at|Î¸t) using the current state reward
rt and its neighborsâ€™ estimations Q(st+1, at+1|Î¸t) as in (9).

One training technique we borrowed from (Mnih et al., 2013) is the experience replay used in DQN.
It improves the convergence issue in neural network-estimator based Q-learning by storing the state st
visited before, action performed at, stateâ€™s reward rt and the next state st+1 after performing the action
at in an experience tuple (st,rt, at, st+1). This tuple is then pushed into the experience replay memory
queue M . The size of M is pre-defined based on experiment.

Whenever an action is performed then a new state is arrived, the past experience tuple will be pushed
into the replay memory queue M if M is not full, otherwise M will pop out the first tuple and push in
the latest one as First-in First-out (FIFO). At each training iteration, a random tuple is selected from M
and the loss function value is calculated based on the st, rt, at and st+1 given by the tuple.



1689

Algorithm 1 DAT Training using Experience Replay
1: Epochs: N
2: Batch Size: K
3: Initialize a replay memory M with size Âµ
4: for n=1â†’ N do
5: Time step in state space: tâ† 0
6: Randomly select an initial state st = (vi, li), lâˆ—i â† true label of vi
7: while li!=lâˆ—i do
8: Q(st, at|Î¸t)â† DAT(st)
9: at=argmaxaQ(st, a|Î¸t)

10: Perform action at, generating next state st+1
11: Push tuple (st, rt, at, st+1) into replay memory M
12: for b=1â†’K do
13: Randomly select a tuple m from M
14: st â† m(0), rt â† m(1), st+1 â† m(3)
15: Q(st, at|Î¸t)â† DAT(st)
16: Q(st+1, at+1|Î¸t)â† DAT(st+1)
17: Obtain pi from fdnn and oli in state st, olâˆ—i from the ground truth in training data

18: rtâ† tanh(log(
||olâˆ—i âˆ’ pi||2
||oli âˆ’ olâˆ—i ||2 + ï¿½

))

19: Update QÌ‚(st, at|Î¸t) in (9)
20: Update the network by minimizing loss Lst in (9)
21: end for
22: tâ† t+ 1
23: end while
24: end for

Remarks: 1. It is worth noticing that it is necessary to use reinforcement learning (RL) in our problem,
since RL based model can learn the long-term dependency much better than the â€œconventionalâ€ deep
learning approach. In our case, since we split a sentence into multiple word level vectors and select them
randomly during training, not only we need to consider what the states current label is, but also how
this state/word level vector is affected by its connected state in the network, such that it wont have any
difficulty to find the correct label from its current wrong label.
2. In our model, we use a cascaded multiple model structure to boost the system performance. The
concept of using information from multiple models (with the same or different structures) to achieve
better performance has been widely used in deep learning ((Dean et al., 2012; Ngiam et al., 2011; Wang
et al., 2018)), system identification ((Narendra et al., 2014; Narendra et al., 2015b; Murray-Smith and
Johansen, 1997; Narendra et al., 2015c; Wang, 2017a; Wang, 2017b)) and also reinforcement learning
field ((Narendra et al., 2015a; Wang and Jin, 2018; Narendra et al., 2016)) recently . Instead of using
collective information, in this paper, we takes advantage of two models (DNN-based and DRL-based) to
compensate each other, such that minor tags can be taken care of and learned.

3.2.2 Model Inference
As shown in Figure 4, the inference part of the DAT model is a bit different from training the DAT. Since
during the training procedure, the entire training dataset (xtrain, ytrain) is used to train both DNN and
DAT models. Comparatively, during inference, only partial of test data with â€œunsatisfied performanceâ€
will be collected and further evaluated by DAT. This is mainly because that we use DAT as an augmented
tagger to compensate the minority cases when the original DNN based tagger fdnn does not perform
well. For the majority test data xpredict dnn, on which fdnn can perform very well, we still use fdnn to
generate their tags ypredict dnn . In order to filter those data with â€œunsatisfied performanceâ€, a threshold
value Tr is defined. All the tokens with their predicted tagsâ€™ probabilities below Tr are filtered as the



1690

minority cases and further used as the inference input of DAT, i.e. xdat = xtest\predict dnn. The outputs
of DAT are the actions that will transfer the states from their current labels li to the target label lâˆ—i , which
gives the output of minority cases, i.e. ypredict dat.

Remarks: The choice of Tr will slightly affect the performance of DAT as different percentage of data
will be filtered. A general recommended Tr is to use a similar percentage of data under minority tags
in training data as shown in section 2.2. For example, we use a Tr to filter around 14%(=6,323/45,030)
percent of test data in ATIS for DAT since the training tokens under minority tags in ATIS are roughly
around this ratio, as shown in Table 1.

4 Experiment

4.1 Data Sets

In our experiment, we will evaluate our deep reinforcement learning based augmented tagging system
on two sequence labeling tasks: Slot filling task and NER task.
Slot Filling: For Slot filling task, we use the public ATIS dataset which follows the same format as in
(Liu and Lane, 2015; Mesnil et al., 2015; Xu and Sarikaya, 2013; Liu and Lane, 2016). The training set
contains 4,978 utterances/sequences, and the test dataset contains 893 utterances. There are totally 127
slot tags.
NER: For NER task, we use the public CoNLL-2003 dataset as in (Lample et al., 2016; Chiu and
Nichols, 2016; Ma and Hovy, 2016; Xu et al., 2017; Huang et al., 2015). The training set contains
14,987 sentences, and the test dataset contains 3,684 sentences. The dataset contains four different types
of named entities: PERSON, LOCATION, ORGANIZATION, and MISC. By using the BIO2 tagging
scheme, the total number of different labels is 9. There are a total of 204,567 tokens in training set, and
46,666 tokens in test set.

4.2 Training Setup

Slot Filling: For slot filling task, the pre-trained DNN model fdnn has the same set-up as in (Liu and
Lane, 2016), by using an attention based bi-directional LSTM. The number of states in LSTM cell is
128. The randomly initialized word embedding size is also 128.The batch size is 16 and the dropout rate
for non-recurrent connection is 0.5.
NER: For NER task, the pre-trained DNN model fdnn follows the BLSTM-CNNS-CRF structure as in
(Ma and Hovy, 2016), which gives the current state-of-the-art result on CoNLL-2003 dataset. The word
embedding is chosen as the GloVe 100-dimensional embedding (Pennington et al., 2014). The CNNâ€™s
window size is chosen as 3 and the number of filters is 30. The number of states in LSTM cell is 200.
The batch size is 10 and the dropout rate is 0.5.

The DAT model fdat used for Slot fill and NER tasks are almost the same except the batch size. The
network structure chosen to estimate the action-value function Q is an LSTM structure with 100 states.
The averaged word vector in a reinforcement learning state is chosen as a trigram, i.e. n=3. The discount
factor Î³ in (9) is selected as 0.5, 0.7 and 0.9 for difference experiments. The minibatch size is K = 16
for slot filling task and K = 10 for NER in order to keep the same training batch size as fdnn in different
tasks, and the replay memory size is pre-defined as Âµ = 5, 000. The thresholds for both experiment are
set as Tr = 0.95.

4.3 Performance on ATIS dataset

Our first experiment is performed on ATIS dataset and compared with other existing approaches, by
evaluating their slot filling F1 scores. A detailed comparison is given in Table 2.

By using the same DNN based model for fdnn as in (Liu and Lane, 2016), our new model surpassed the
previous state-of-the-art result by 0.9%,1.2% and 1.9% for Î³= 0.5, 0.7 and 0.9 separately. It is also worth
noticing that, a larger discount factor Î³ gives a better performance as shown in Table. One empirical
explanation is that when a larger discount factor is used, the model puts more weights on future states
during training, hence it can search the correct label for the current word within a state in a faster manner.



1691

Table 2: Performance of Different Models on ATIS
Dataset

Model F1 Score

Recursive NN (Guo et al., 2014) 93.96%
RNN with Label Sampling (Liu and Lane, 2015) 94.89%

Hybrid RNN (Mesnil et al., 2015) 95.06%
RNN-EM (Peng and Yao, 2015) 95.25%

CNN CRF (Xu and Sarikaya, 2013) 95.35%
Encoder-labeler Deep LSTM (Kurata et al., 2016) 95.66%

Attention Encoder-Decoder NN (Liu and Lane, 2016) 95.87%
Attention BiRNN (Liu and Lane, 2016) 95.98%

DRL based Augmented Tagging System (Î³ = 0.5) 96.85%
DRL based Augmented Tagging System (Î³ = 0.7) 97.23%
DRL based Augmented Tagging System (Î³ = 0.9) 97.86%

Table 3: Performance of Different Models on CoNLL-
2003 Dataset

Model F1 Score

NN+SLL+LM2(Collobert et al., 2011) 88.67%
NN+SLL+LM2+Gazetterâˆ— (Collobert et al., 2011) 89.59%

BI-LSTM-CRFâˆ— (Huang et al., 2015) 90.10%
ID-CNN-CRF (Strubell et al., 2017) 90.54%

FOFE (Xu et al., 2017) 90.71%
BLSTM-CNN+emb (Chiu and Nichols, 2016) 90.91%

BLSTM-CRF(Lample et al., 2016) 90.94%
BLSTM-CNN-CRFs (Ma and Hovy, 2016) 91.21%

BLSTM-CNN+emb+lexâˆ— (Chiu and Nichols, 2016) 91.62%

DRL based Augmented Tagging System (Î³ = 0.5) 91.92%
DRL based Augmented Tagging System (Î³ = 0.7) 92.23%
DRL based Augmented Tagging System (Î³ = 0.9) 92.67%

Table 4: Change on Tagsâ€™ Distribution of Wrongly Labeled Data on ATIS and CoNLL-2003 Test Datasets

ATIS Test Dataset CoNLL-2003 Test Dataset

% of wrongly labeled data % of wrongly labeled data % of wrongly labeled data % of wrongly labeled data
using fdnn using fdnn+fdat using fdnn using fdnn+fdat

Minority Tags 92% 56% 78% 48%
(<1% of total # of data)

Majority Tags 8% 44% 22% 52%
(â‰¥ 1% of total # of data)

Total 100% 100% 100% 100%

4.4 Performance on CoNLL-2003 dataset

Our second experiment is conducted on the CoNLL-2003 dataset and compared with current existing
neural network-based approaches for the NER tasks. The metric is also using their F1 scores, and the
result is as shown in Table 3 (The results are collected before initial submission, some better results are
released during final submission (Peters et al., 2018), despite our modelâ€™s performance is still better).

The methods marked using âˆ— are using extra features like lexicons and etc. It can be observed that our
DAT systemâ€™s result outperforms the state-of-the-art results in (Ma and Hovy, 2016) (without lexicon
features) and (Chiu and Nichols, 2016) (lexicon features) by 1% and 1.4% using Î³ = 0.9 separately.
Similar to the SLU task, a larger discount factor Î³ also boosts our systemâ€™s performance.

4.5 Change on Result Distribution

Another observation from our experimentâ€™s result is that the tagsâ€™ distribution of wrongly labeled data
changed. Table 4 gives a summary about these changes on ATIS and CoNLL-2003 two test dataset.

It can be observed that the percentage of wrongly labeled data with minority tags decreases for both
ATIS and CoNLL-2003 datasets by adding the DAT model fdat to the DNN based model fdnn. It
indicates that the DAT model fdat can help improve the performance of general sequence tagging model
by correctly labeling data with minority tags, which is also the weakness of a single DNN based model
fdnn.

5 Conclusion

In this paper, a new DRL based augmented general tagging system is designed. The system use two
sequence labeling models: one is a â€œconventionalâ€ DNN based tagger, and the other is a novel DRL
based augmented tagger, i.e. DAT. By filtering DNNâ€™s output and picking out the â€œunsatisfied dataâ€,
DAT can further improve sequence labeling tasksâ€™ performance by correctly relabeling the data below
the threshold Tr, especially for those under minority tags. The experiment results on two sequence
labeling tasks, i.e. Slot fillings and NER in SLU and NLU, both outperform the current state-of-the-art
models. Besides the decent experimental performance obtained, more importantly, the new augmented
approach can be generalized to more general sequence labeling models without changing their original
model setups.



1692

References
Nitesh V Chawla, Nathalie Japkowicz, and Aleksander Kotcz. 2004. Special issue on learning from imbalanced

data sets. ACM Sigkdd Explorations Newsletter, 6(1):1â€“6.

Jason PC Chiu and Eric Nichols. 2016. Named entity recognition with bidirectional lstm-cnns. Transactions of
the Association for Computational Linguistics, 4:357â€“370.

Ronan Collobert, Jason Weston, LeÌon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493â€“
2537.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker,
Ke Yang, Quoc V Le, et al. 2012. Large scale distributed deep networks. In Advances in neural information
processing systems, pages 1223â€“1231.

Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey Zweig. 2014. Joint semantic utterance classification and
slot filling with recursive neural networks. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pages
554â€“559. IEEE.

Haibo He and Edwardo A Garcia. 2009. Learning from imbalanced data. IEEE Transactions on knowledge and
data engineering, 21(9):1263â€“1284.

Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. 2008. Adasyn: Adaptive synthetic sampling approach
for imbalanced learning. In Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational
Intelligence). IEEE International Joint Conference on, pages 1322â€“1328. IEEE.

Charles T Hemphill, John J Godfrey, George R Doddington, et al. 1990. The atis spoken language systems pilot
corpus. In Proceedings of the DARPA speech and natural language workshop, pages 96â€“101.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint
arXiv:1508.01991.

Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu. 2016. Leveraging sentencelevel information with encoder
lstm for natural language understanding. arXiv preprint.

Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural
architectures for named entity recognition. arXiv preprint arXiv:1603.01360.

Bing Liu and Ian Lane. 2015. Recurrent neural network structured output prediction for spoken language under-
standing. In Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions.

Bing Liu and Ian Lane. 2016. Attention-based recurrent neural network models for joint intent detection and slot
filling. arXiv preprint arXiv:1609.01454.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint
arXiv:1603.01354.

GreÌgoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He, Larry
Heck, Gokhan Tur, Dong Yu, et al. 2015. Using recurrent neural networks for slot filling in spoken language
understanding. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 23(3):530â€“539.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves,
Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep
reinforcement learning. Nature, 518(7540):529.

Roderick Murray-Smith and T Johansen. 1997. Multiple model approaches to nonlinear modelling and control.
CRC press.

Kumpati S Narendra, Yu Wang, and Wei Chen. 2014. Stability, robustness, and performance issues in second level
adaptation. In American Control Conference (ACC), 2014, pages 2377â€“2382. IEEE.

Kumpati S Narendra, Snehasis Mukhopadyhay, and Yu Wang. 2015a. Improving the speed of response of learning
algorithms using multiple models. arXiv preprint arXiv:1510.05034.



1693

Kumpati S Narendra, Yu Wang, and Wei Chen. 2015b. Extension of second level adaptation using multiple models
to siso systems. In American Control Conference (ACC), 2015, pages 171â€“176. IEEE.

Kumpati S Narendra, Yu Wang, and Wei Chen. 2015c. The rationale for second level adaptation. arXiv preprint
arXiv:1510.04989.

Kumpati S Narendra, Yu Wang, and Snehasis Mukhopadhay. 2016. Fast reinforcement learning using multiple
models. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pages 7183â€“7188. IEEE.

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. 2011. Multimodal
deep learning. In Proceedings of the 28th international conference on machine learning (ICML-11), pages
689â€“696.

Baolin Peng and Kaisheng Yao. 2015. Recurrent neural networks with external memory for language understand-
ing. arXiv preprint arXiv:1506.00195.

Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word represen-
tation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),
pages 1532â€“1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-
moyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 2227â€“2237.

Warren B Powell. 2007. Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703.
John Wiley & Sons.

Emma Strubell, Patrick Verga, David Belanger, and Andrew McCallum. 2017. Fast and accurate sequence labeling
with iterated dilated convolutions. arXiv preprint arXiv:1702.02098.

Yanmin Sun, Andrew KC Wong, and Mohamed S Kamel. 2009. Classification of imbalanced data: A review.
International Journal of Pattern Recognition and Artificial Intelligence, 23(04):687â€“719.

Yu Wang and Hongxia Jin. 2018. A boosting-based deep neural networks algorithm for reinforcement learning.
In Proceedings of the 2018 American Control Conference.

Yu Wang, Yilin Shen, and Hongxia Jin. 2018. A bi-model based rnn semantic frame parsing model for intent de-
tection and slot filling. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2, volume 2, pages 309â€“314.

Yu Wang. 2017a. Adaptive control and learning using multiple models. Ph.D. thesis, Yale University.

Yu Wang. 2017b. A new concept using lstm neural networks for dynamic system identification. In American
Control Conference (ACC), 2017, pages 5324â€“5329. IEEE.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neural network based triangular crf for joint intent detection
and slot filling. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages
78â€“83. IEEE.

Mingbin Xu, Hui Jiang, and Sedtawut Watcharawittayakul. 2017. A local detection approach for named entity
recognition and mention detection. In Proceedings of the 55th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), volume 1, pages 1237â€“1247.


