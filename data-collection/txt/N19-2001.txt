



















































Enabling Real-time Neural IME with Incremental Vocabulary Selection


Proceedings of NAACL-HLT 2019, pages 1‚Äì8
Minneapolis, Minnesota, June 2 - June 7, 2019. c¬©2019 Association for Computational Linguistics

1

Enabling Real-time Neural IME with Incremental Vocabulary Selection

Jiali Yao
Microsoft

jiayao@microsoft.com

Raphael Shu
The University of Tokyo

shu@nlab.ci.i.u-tokyo.ac.jp

Xinjian Li
Carnegie Mellon University
xinjianl@andrew.cmu.edu

Katsutoshi Ohtsuki
Microsoft

Katsutoshi.Ohtsuki@microsoft.com

Hideki Nakayama
The University of Tokyo

nakayama@ci.i.u-tokyo.ac.jp

Abstract

Input method editor (IME) converts sequen-
tial alphabet key inputs to words in a target
language. It is an indispensable service for
billions of Asian users. Although the neural-
based language model is extensively studied
and shows promising results in sequence-to-
sequence tasks, applying a neural-based lan-
guage model to IME was not considered feasi-
ble due to high latency when converting words
on user devices. In this work, we articu-
late the bottleneck of neural IME decoding
to be the heavy softmax computation over a
large vocabulary. We propose an approach
that incrementally builds a subset vocabulary
from the word lattice. Our approach always
computes the probability with a selected sub-
set vocabulary. When the selected vocabu-
lary is updated, the stale probabilities in pre-
vious steps are fixed by recomputing the miss-
ing logits. The experiments on Japanese IME
benchmark shows an over 50x speedup for the
softmax computations comparing to the base-
line, reaching real-time speed even on com-
modity CPU without losing conversion accu-
racy1. The approach is potentially applica-
ble to other incremental sequence-to-sequence
decoding tasks such as real-time continuous
speech recognition.

1 Introduction

Input Method Editors (IME) run on every desk-
top and mobile devices that allows users to type
the scripts in their language. Though Latin users
can type directly without conversion as a second
step, some common languages such as Chinese
and Japanese require users to convert the keyboard
input sequence as there are thousands of charac-
ters in these languages. The conversion task of
an IME takes a key sequence and converts it to a

1The source code of the implementation is available from
https://github.com/jiali-ms/JLM

sequence of words in the target language. In the
ideal case, the conversion results shall fit the in-
tention of users. The accuracy of the conversion
task directly affects the typing efficiency and user
experiences.

The conversion task is a sequence decoding
task similar to speech recognition, machine trans-
lation, and optical character recognition. Con-
ventionally, an n-gram language model toolkit is
used to evaluate the path probability during decod-
ing (Stolcke, 2002; Heafield, 2011). Due to the
ability of leveraging context information without
hitting data sparsity issue, neural language mod-
els as an alternative option have been extensively
studied in the past (Bengio et al., 2003; Schwenk,
2007; Mikolov et al., 2010; Mikolov and Zweig,
2012), which achieve state-of-the-art performance
on many tasks (Sundermeyer et al., 2012; Luong
et al., 2015; Jozefowicz et al., 2016). With emerg-
ing dedicated hardware processing unit such as
custom ASIC (Jouppi and Young, 2017), neural-
based models are promising to be even more
widely applied to user devices.

However, neural-based language models were
not considered feasible for the IME conversion
task. The main reason is that an IME has to
run interactively on various user devices, whereas
speech recognition and machine translation ser-
vices are normally provided on servers. Further-
more, the neural model has to meet following
requirements in order to be adopted in practice:
1) low-latency incremental conversion; 2) word
lattice post-editing. First, the conversion task for
IME is an incremental process that needs to re-
turn the best paths immediately when receiving
each key input. Existing speed optimization meth-
ods (Deoras et al., 2011; Hori et al., 2014) nor-
mally increase the speed of processing sequences
in batch. Some methods incorporate prefix tree
(Si et al., 2013), which doesn‚Äôt ensure the worse



2

case latency still meets the real-time requirement.
Second, IME allows users to post-edit the con-
verted results at word lattice by manually selecting
candidates. It limits the choice of many end-to-
end neural network architectures (Cho et al., 2014;
Vaswani et al., 2017) as they do not provide a way
for users to select partial conversion results.

In this work, we enhance a neural language
model tailored for IMEs to meet real-time infer-
ence speed requirements on the conversion task.
Our baseline model is composed of a LSTM-
based language model (Hochreiter and Schmidhu-
ber, 1997) with a Viterbi decoder (Forney, 1973)
as Figure 1 shows. We articulate the bottleneck
of run-time speed as the heavy linear transforma-
tion in the softmax layer. We propose an incre-
mental vocabulary selection approach that builds
a subset vocabulary Vt at each decoding step t.
By only computing softmax over Vt, the cost of
softmax significantly drops since |Vt| is usually a
small number that is less than 1% of the original
vocabulary size. We evaluate the speedup compar-
ing to other softmax optimizations on a Japanese
benchmark. The contributions of this work can be
summarized as:

1. We propose a novel incremental vocabulary
selection approach, which significantly re-
duces the latency of lattice decoding in IME
conversion task.

2. We provide an extensive comparison among
different approaches for speeding up the soft-
max computation in lattice decoding.

3. We demonstrate that with our proposed ac-
celeration method helps the neural models to
meet the requirement for real-world applica-
tions.

2 Related Work

Applying a neural-based model for input method
is studied in a few previous works. Chen et al.
(2015) proposed a MLP architecture for Chinese
Pinyin input method to re-score the n-best list of
n-gram decoded results similar to speech recogni-
tion solutions (Deoras et al., 2011; Si et al., 2013).
Though not in literature, we have found the im-
plementation of RNN-based input method2. The

2Yoh Okuno. Neural ime: Neural input method engine.
https://github.com/yohokuno/neural ime, 2016

decoding results on Japanese corpus show promis-
ing accuracy improvement comparing to the n-
gram model. Huang et al. (2018) treats the Chi-
nese input conversion as machine translation and
apply sequence-to-sequence models with attention
mechanism. The conversion and other features are
served as cloud services. Our work focus on en-
abling real-time neural-based models on devices
with limited computation resources.

The softmax layer with a large vocabulary size
is the most computational-heavy operation of a
neural-based language model. Differentiated soft-
max (Chen et al., 2016) and its variation (Joulin
et al., 2017) decrease the amount of computa-
tion by reducing the embedding size of long tail
words in the vocabulary with frequency based seg-
mentation. For prediction tasks where only top-k
words are necessary, softmax approximation such
as SVD softmax (Shim et al., 2017) uses a low-
rank matrix in the softmax layer for a first pass
fast ranking. Zhang et al. (2018) proposes a word
graph traverse approach to find top-k hypothesis in
logarithmic complexity. In a word lattice decoder,
where the target words are given, structured output
layer (Mnih and Hinton, 2009; Le et al., 2011; Shi
et al., 2013) is often applied to avoid calculating
probability distributions from the full vocabulary.
Character-based RNN (Mikolov et al., 2010) is a
good alternative for word-based language models
that can significantly reduce the vocabulary size.
However, there are still tens of thousands of Chi-
nese characters.

Another approach to solving the softmax bot-
tleneck is vocabulary manipulation. In machine
translation, given a source sentence, the vocab-
ulary of target words can be largely limited be-
fore translation. It is possible to compute the soft-
max on a subset of the full vocabulary (Jean et al.,
2015; Mi et al., 2016). However, for the input
method, the conversion task takes user input incre-
mentally. Our proposed incremental vocabulary
selection can work without predicting the full vo-
cabulary beforehand, which is designed for reduc-
ing the latency for incremental sequence decoding
tasks with a large vocabulary.

3 Neural-based Input Method Editor

Our proposed approach for neural-based IME is
illustrated in Figure 1. We use a neural-based
language model to predict the word probabili-
ties in the lattice decoder. Although there are



3

kyouhaii

„Åç „Çá „ÅÜ „ÅØ „ÅÑ „ÅÑ

...

Today is good‰ªäÊó•„ÅØ„ÅÑ„ÅÑ

‰ªäÊó•„ÅØ„ÅÑ„ÅÑ

weatherÂ§©Ê∞ó

is„ÅØ

„ÅÑ„ÅÑ good

today‰ªäÊó•

(a) (b) (c)
to

 k
an

a
le

x
ic

o
n

 l
o

o
k

u
p

word lattice

context

vector

best path

next word prob distribution

input context

context

vector

top candidates

Neural-based language model

weatherÂ§©Ê∞ó

feelingÊ∞óÊåÅ„Å°

feelingÊ∞óÊåÅ„Å°

Figure 1: Illustration of a neural-based IME with LSTM language model and word lattice decoder. (a) Input
context. (b) Conversion with a LSTM-based language model and Viterbi lattice decoder. (c) Word prediction.

various choices for the model architecture, we
choose a single layer LSTM (Hochreiter and
Schmidhuber, 1997) model considering the run-
time speed constraint. Other model architectures
such as sequence-to-sequence models (Cho et al.,
2014) and the bi-directional LSTM-based mod-
els (Huang et al., 2015) are not considered as they
cannot generate conversion results incrementally.

Conversion task is illustrated in Figure 1(b).
User key inputs are first converted into Japanese
Kana (also known as ‚Äúfifty sounds‚Äù) with pre-
defined rules. Given a partial observed Kana char-
acter input sequence (x1, ..., xt), we search for a
set of words in the dictionary that match any suf-
fix of the observation as lattice words at step t:

Dt =
t‚ãÉ

i=1

match(xi, ..., xt), (1)

where the function match(¬∑) returns all lexicon
items matching the partial sequence. For exam-
ple, given the observation ‚Äúha ru ka‚Äù, the lexicon
set Dt contains all words matching ‚Äúha ru ka‚Äù or
‚Äúru ka‚Äù or ‚Äúka‚Äù.

Given a word wt ‚àà Dt, to construct a hypothe-
sis œÄt ending with wt, previous hypotheses œÄt‚àíl
are used as the context to evaluate p(wt|œÄt‚àíl),
where l is the length of the Kana representation
of wt. Since l is a variable for wt, only aligned
hypotheses can be connected. To find the best hy-
potheses, we use a Viterbi decoder to decode with
a beam size B.

We use a LSTM-based language model directly
here to evaluate p(wt|œÄt‚àíl). The conversion task

has to compute B √ó |V | √ó T steps of LSTM com-
putation for one input sequence, where |V | is the
vocabulary size, T is the sequence length. Heavy
computation cost of LSTM model comes from two
operations: the matrix operations inside the LSTM
cell and the matrix projection at softmax. In the
case of a LSTM model with a vocabulary size of
100K, a hidden size of 512, and an embedding size
of 256, estimated by simple matrix multiplication,
the number of multiplication operations for LSTM
cell is about 1.5M, while the softmax has 50M op-
erations. In practice, the softmax occupies 97% of
the total computation cost, which is the bottleneck
for the conversion task with large vocabulary size.

4 Incremental Vocabulary Selection

To solve the challenges, we propose an incremen-
tal vocabulary selection algorithm, which signif-
icantly reduces the amount of softmax computa-
tion during decoding. The algorithm of incremen-
tal vocabulary selection is given in Alg. 1. Let
Œ† be the hypothesis dictionary that stores the best
hypotheses at each step with LSTM states. We de-
fine Œ†[¬∑] as the dictionary lookup operation to get
the best hypotheses at a specified step. Œ† is ini-
tialized with a start hypothesis that contains the
LSTM state carried over from the previously con-
verted sequence.

In current conversion step, when a new input
character xt arrives, we construct a subset vocab-
ulary Vt that covers all possible output words for



4

Algorithm 1 Incremental vocabulary selection
1: Initialize:

B ‚Üê beam size
Œ†‚Üê hypothesis dictionary
VÃÉ ‚Üê samples from vocabulary
t‚Üê current step t

2: Vt ‚Üê sub vocabulary with Eq. 2
3: Vt ‚Üê Vt ‚à™ VÃÉ
4: Vfix ‚Üê empty set
5: for k ‚Üê t‚àí 1 to 1 do
6: Vfix ‚Üê Vfix ‚à™ (Vt \ Vk)
7: Vk ‚Üê Vt
8: Re-compute the logits on Vfix for all past hyps
9: Evaluate Œ†[t]

10: Œ†[t]‚Üê best B hyps in Œ†[t]
11: Update LSTM state for Œ†[t]
12: output Œ†[t]

the sequence until step t as:

Vt =
t‚ãÉ

i=1

Di. (2)

To evaluate a newly formed hypothesis
(œÄt‚àíl, wt), we need to query the probability
p(wt|œÄt‚àíl), which is already calculated and
cached in Œ†[t ‚àí l]. However, as Figure 2 il-
lustrates, since the Vt is built incrementally, the
softmax distribution calculated in previous steps
may not contain the word wt. To evaluate Œ†[t],
we need to fix the missing vocabulary items in
previously calculated softmax distributions. In
principle, we can correct a stale probability in
step k by adding the logits of missing vocabulary
to the denominator as:

P (yk = i|hk)

=
exp(h>k Wi)‚àë

j ‚àà Vk
exp(h>k Wj) +

‚àë
j ‚àà d

exp(h>k Wj)
. (3)

where W is the projection matrix of the output
layer. hk is the cached LSTM state for a hypoth-
esis œÄk that can be re-used to calculate missing
logits. d is the difference of vocabulary between
steps Vt \ Vk. In practice, as each path has differ-
ent missing vocabularies, we compute a union of
all missing vocabularies Vfix and then re-compute
the logits of them in one batch.

As the vocabulary in initial steps is fairly small,
we introduce VÃÉ as a subset vocabulary sampled

... ... ...

(a) (b)

lattice 

OOV

lattice 

word

top 

samples

random 

samples

lattice vocab

step ùë°step t ‚àí 1step t ‚àí 2

diff vocab

latent

vector

path ‚Ñéùë°

Figure 2: Previously aligned hypotheses have missing
vocabulary to evaluate words in current step.

from the full vocabulary. In contrast to the beam
search in machine translation, we have to rank
paths that contain a different number of words. We
use Vt jointly with VÃÉ to make the word probabili-
ties closer to their original probabilities.

After the best hypotheses are decided, we im-
mediately update the LSTM state for B hypothe-
ses in Œ†[t]. The single best hypothesis in Œ†[t] is
finally returned to the IME engine as the output of
the conversion task.

5 Experiments

5.1 Dataset

We use BCCWJ (Balanced Corpus of Contempo-
rary Written Japanese) corpus (Maekawa et al.,
2014) for evaluating our model. The corpus is well
balanced with various sources of text representing
contemporary written Japanese. This corpus con-
tains 5.8M sentences, which are segmented into
127M tokens. In our experiments, all words are
further segmented into short unit words. Each
word has a format of ‚Äúdisplay/reading/POS‚Äù. The
reading and part-of-speech (POS) attributes are at-
tached to indicate different usages of the same
word. Among the 611K unique words, we choose
top 50K frequent ones which cover 97.3% of the
token appearances as an appropriate vocabulary
size for the IME task. The words in the vocab-
ulary are ranked with frequency. Most frequent
words are at the top.

5.2 Experiment Settings

The BCCWJ dataset is split into training, valid and
test set. The ratio is 70%, 20%, and 10% respec-
tively. We randomly sample 2000 sentences from
the test set for evaluating the conversion accuracy.



5

For input method task, we evaluate the conversion
accuracy using a Viterbi decoder with a beam size
of 10. In Japanese, there are often more than one
correct conversion results. For instance, a verb
may have two acceptable styles, one in original
Japanese Kana, the other in Chinese characters. To
better evaluate the model performance, we also re-
port the top-10 conversion accuracy in addition to
top-1 conversion accuracy.

We implement using TensorFlow3. We use a
batch size of 384. A dropout with a drop rate
of 0.9 is applied before the LSTM layer. Adam
is used with a fixed learning rate of 0.001. The
hyper-parameters are shared for all experiments.

A replica of the same model is written in numpy
to work with a Viterbi decoder in python. It uses
the weights learned with TensorFlow model. The
inference performance is measure with numpy on
a single Intel E5 CPU. We also apply the underline
BLAS library to accelerate matrix operation.

5.3 Evaluation of Neural-based Input
Method Editor

We first compared the neural model performance
with a conventional n-gram model. We evaluate
the perplexity of the n-gram model with SRILM
package (Stolcke, 2002). We choose modified
Kneser Ney (Kneser and Ney, 1995; James, 2000)
as the smoothing algorithm when learning the n-
gram model. No cut-offs or pruning is applied.
The learned language model is plugged into our
input method pipeline for evaluating the sentence
conversion accuracy. The prediction accuracy is
not provided as the perplexity directly reflects it.

The LSTM baseline model has a standard archi-
tecture. The embedding size is 256, and the hidden
size is 256. The size of LSTM cells is selected em-
pirically on a validation dataset. In practice, using
a network size bigger than 256 cannot gain sig-
nificant improvement over perplexity. In all the
following experiments, we bind the input embed-
ding and output embedding according to (Press
and Wolf, 2017). The idea is proven to save space
and almost loss-less. We treat it as the baseline
model in the following experiments.

As shown in Table 1, the LSTM baseline
achieved significant improvement on perplexity,
comparing to conventional n-gram based models.
The top-1 and top-10 path conversion accuracy

3We implement a standard LSTM cell to avoid any unex-
pected customization from published version

model pp top1 top10hit % hit %
uni-gram 833.55 26.95 45.85

bi-gram KN 99.30 51.15 78.10
tri-gram KN 68.11 55.60 79.65

LSTM baseline 41.39 61.20 88.30

Table 1: Performance comparison of baseline LSTM
model with conventional n-gram model.

were increased by 5.6% and 8.65% respectively
comparing to tri-gram KN. In real products, bi-
gram is often used for decoding while tri-gram is
only used for re-ranking the best paths. Please
note that in this evaluation, we did not apply any
pruning for n-gram. In practice, the n-gram model
takes over 1GB storage size.

5.4 Evaluation of Run-time Speed

In this section, we compare the inference speed of
various methods for accelerating the computation.
We measure the execution speed only for the com-
ponent that computes the language model proba-
bilities. For neural-based methods, the component
includes the LSTM and softmax layers. For the
n-gram model, the computation of probability is
only a lookup in the hash tables. Other compo-
nents such as lattice construction are not included
as they heavily depend on implementation.

We report the decoding time in each step receiv-
ing a key input, as the per-step latency is critical
for the real-time user experience. The computa-
tion cost for decoding the whole sequence is linear
to the number of steps. For comparison, we also
report the computation time of the softmax alone.

As Table 2 shows, our proposed incremental vo-
cabulary selection (IVS) achieves an 84x speedup
for softmax computation comparing to the LSTM
baseline. The lattice vocabulary in our experi-
ments contains only a few hundred words, while
the full vocabulary has 50k words. IVS only takes
3 ms to handle a new coming key. Such a high
processing speed meets the real-time latency re-
quirement even on low-end devices. In contrast,
the non-incremental vocabulary selection is less
efficient since it recalculates from the beginning
of each step. After solving the speed bottleneck of
computing the softmax layer, the majority of the
computational cost comes from the LSTM cells.

We also evaluate various sampling methods
with IVS. We find top sampling help close the ac-



6

model total time softmax time top-1 hit top-10 hit(ms) (ms) (%) (%)
tri-gram Kneser-Ney 0.0025 - 55.6 79.6

LSTM baseline w/o batch 526 513 61.2 88.3
LSTM baseline w/ batch 87 84 61.2 88.3

Char LSTM 63 21 53.2 79.8
Char LSTM w/ large beam4 152 55 54.4 85.2

D-Softmax 38 35 60.8 86.8
D-Softmax‚àó 37 35 60.9 87.5

IVS 3 1 58.8 86.9
w/ top sampling 4 2 60.1 88.2

w/ uniform sampling 4 2 58.9 87.2
w/ self-norm 3 1 60.1 88.2

non-incremental VS 11 2 58.8 86.9

Table 2: Evaluation of different model acceleration approaches in terms of computation time and resultant accu-
racy. The computation time is reported for each step.

curacy gap between the baseline and softmax ap-
proximation. In our experiments, the number of
samples used in top sampling and uniform sam-
pling methods is both 400.

If we are able to train the neural language model
from scratch, then the self-normalization (De-
vlin et al., 2014) approach can be applied as
a softmax approximation. When applying self-
normalization, the model is trained with additional
normalization terms, which force the exponential
logits to sum up to 1. Therefore, it eliminates
the necessity of performing vocabulary sampling.
However, if one only has access to a pre-trained
language model, then applying self-normalization
is not an option.

Differentiated softmax (D-Softmax) (Chen
et al., 2016) and its variation (D-softmax‚àó) (Joulin
et al., 2017) can also reduce the amount of soft-
max computation by over a half depending on the
segmentation strategy. However, since the vocab-
ulary size is still large, the room for speedup is
limited. For character-based LSTM models, we
use a hidden size of 1024 and embedding size of
512. Since there is no direct mapping between a
single Kana and a single Chinese character, we use
the word lattice and evaluate the path probability
by character-based LSTM. It reduces the vocab-
ulary size from 50K to 3717 in this experiment.
However, the amount of vocabulary is still non-
trivial. Furthermore, the integration of character-
based models is not as efficient as word-based

4The large beam size is set to 50 in our experiments.

models in this task. Consequently, we have to use
a large beam size to improve accuracy.

The cost to loop previous steps during fixing
the vocabulary is not given here due to the imple-
mentation detail. In a real scenario, users do not
type the full sentence in one effort. Instead, they
type and convert in fragments, where each frag-
ment contains a few words. It is trivial compared
to softmax computation.

We confirmed that batching is critical for ac-
celerating matrix operations on CPU. The LSTM
computation without batching is almost 7x slower
with a beam size 10. Therefore, we batch all the
softmax computations when fixing the vocabulary
in the second pass to achieve the best speed.

6 Conclusion

IME plays an important role in improving typing
efficiency and user experience. It has to work on
various devices with extremely low latency. We
study the key challenges to apply neural-based in-
put method on real commodity devices. The pro-
posed incremental vocabulary selection approach
reduces the cost of computing the softmax layer
without losing accuracy. Our proposed method
sets a strong baseline in a real-time IME conver-
sion task. More importantly, as the computation
has low latency, the model is production-ready to
be used on real devices.



7

References
Yoshua Bengio, ReÃÅjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137‚Äì1155.

Shenyuan Chen, Hai Zhao, and Rui Wang. 2015. Neu-
ral network language model for chinese pinyin in-
put method engine. In Proceedings of the 29th Pa-
cific Asia Conference on Language, Information and
Computation, pages 455‚Äì461.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1975‚Äì1985.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder‚Äìdecoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724‚Äì
1734.

Anoop Deoras, Tomas Mikolov, and Kenneth Church.
2011. A fast re-scoring strategy to capture long-
distance dependencies. In EMNLP.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard M. Schwartz, and John Makhoul.
2014. Fast and robust neural network joint models
for statistical machine translation. In ACL.

G David Forney. 1973. The viterbi algorithm. Pro-
ceedings of the IEEE, 61(3):268‚Äì278.

Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In WMT@EMNLP.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.

Takaaki Hori, Yotaro Kubo, and Atsushi Nakamura.
2014. Real-time one-pass decoding with recurrent
neural network language model for speech recog-
nition. 2014 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 6364‚Äì6368.

Yafang Huang, Zuchao Li, Zhuosheng Zhang, and
Hai Zhao. 2018. Moon ime: Neural-based chinese
pinyin aided input method with customizable associ-
ation. In Proceedings of ACL 2018, System Demon-
strations, pages 140‚Äì145. Association for Computa-
tional Linguistics.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Frankie James. 2000. Modified kneser-ney smoothing
of n-gram models. Technical report.

Sebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
ACL-IJCNLP 2015, pages 1‚Äì10. Association for
Computational Linguistics (ACL).

Armand Joulin, Moustapha CisseÃÅ, David Grangier,
HerveÃÅ JeÃÅgou, et al. 2017. Efficient softmax approx-
imation for gpus. In International Conference on
Machine Learning, pages 1302‚Äì1310.

Norman P. Jouppi and Cliff Young. 2017. In-datacenter
performance analysis of a tensor processing unit.
2017 ACM/IEEE 44th Annual International Sympo-
sium on Computer Architecture (ISCA), pages 1‚Äì12.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
icassp, volume 1, page 181e4.

Hai Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franois Yvon. 2011. Struc-
tured output layer neural network language model.
2011 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
5524‚Äì5527.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In EMNLP.

Kikuo Maekawa, Makoto Yamazaki, Toshinobu
Ogiso, Takehiko Maruyama, Hideki Ogura, Wakako
Kashino, Hanae Koiso, Masaya Yamaguchi, Makiro
Tanaka, and Yasuharu Den. 2014. Balanced cor-
pus of contemporary written japanese. Language
resources and evaluation, 48(2):345‚Äì371.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Vocabulary manipulation for neural machine trans-
lation. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), volume 2, pages 124‚Äì129.

TomaÃÅsÃå Mikolov, Martin KarafiaÃÅt, LukaÃÅsÃå Burget, Jan
CÃåernockyÃÄ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association.

Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
2012 IEEE Spoken Language Technology Workshop
(SLT), pages 234‚Äì239.



8

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081‚Äì1088.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Volume 2, Short Papers, volume 2, pages 157‚Äì163.

Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21:492‚Äì
518.

Yongzhe Shi, Weiqiang Zhang, Jia Liu, and Michael T.
Johnson. 2013. Rnn language model with word
clustering and class-based output layer. EURASIP
J. Audio, Speech and Music Processing, 2013:22.

Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo,
and Wonyong Sung. 2017. Svd-softmax: Fast soft-
max approximation on large vocabulary neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 5463‚Äì5473.

Yujing Si, Qingqing Zhang, Ta Li, Jielin Pan, and
Yonghong Yan. 2013. Prefix tree based n-best list
re-scoring for recurrent neural network language
model used in speech recognition system. In IN-
TERSPEECH.

Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Seventh international confer-
ence on spoken language processing.

Martin Sundermeyer, Ralf SchluÃàter, and Hermann Ney.
2012. Lstm neural networks for language modeling.
In INTERSPEECH.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Minjia Zhang, Xiaodong Liu, Wenhan Wang, Jian-
feng Gao, and Yuxiong He. 2018. Navigating with
graph representations for fast and scalable decoding
of neural language models. CoRR, abs/1806.04189.


