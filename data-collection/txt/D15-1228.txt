



















































Extractive Summarization by Maximizing Semantic Volume


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1961–1966,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Extractive Summarization by Maximizing Semantic Volume
Dani Yogatama

School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dyogatama@cs.cmu.edu

Fei Liu
Electrical Engineering & Computer Science

University of Central Florida
Orlando, FL 32816, USA

feiliu@cs.ucf.edu

Noah A. Smith
Computer Science & Engineering

University of Washington
Seattle, WA 98195, USA

nasmith@cs.washington.edu

Abstract
The most successful approaches to extrac-
tive text summarization seek to maximize
bigram coverage subject to a budget con-
straint. In this work, we propose instead
to maximize semantic volume. We em-
bed each sentence in a semantic space and
construct a summary by choosing a sub-
set of sentences whose convex hull max-
imizes volume in that space. We provide
a greedy algorithm based on the Gram-
Schmidt process to efficiently perform
volume maximization. Our method out-
performs the state-of-the-art summariza-
tion approaches on benchmark datasets.

1 Introduction

In artificial intelligence, changes in representation
sometimes suggest new algorithms. For example,
increased attention to distributed meaning repre-
sentations suggests that existing combinatorial al-
gorithms for NLP might be supplanted by alterna-
tives designed specifically for embeddings. In this
work, we consider summarization.

Classical approaches to extractive summariza-
tion represent each sentence as a bag of terms
(typically bigrams) and seek a subset of sentences
from the input document(s) that either (a) trade off
between high relevance and low redundancy (Car-
bonell and Goldstein, 1998; McDonald, 2007), or
(b) maximize bigram coverage (Yih et al., 2007;
Gillick et al., 2008). The sentence representa-
tion is fundamentally discrete, and a range of
greedy (Carbonell and Goldstein, 1998), approx-
imate (Almeida and Martins, 2013), and exact op-
timization algorithms (McDonald, 2007; Martins
and Smith, 2009; Berg-Kirkpatrick et al., 2011)
have been proposed.

Recent studies have explored continuous sen-
tence representations, including the paragraph
vector (Le and Mikolov, 2014), a convolutional
neural network architecture (Kalchbrenner et al.,
2014), and a dictionary learning approach (Jenat-
ton et al., 2011). If sentences are represented as
low-dimensional embeddings in a distributed se-
mantic space, then we begin to imagine a geomet-
ric relationship between a summary and a doc-
ument. We propose that the volume of a sum-
mary (i.e., the semantic subspace spanned by the
selected sentences) should ideally be large. We
therefore formalize a new objective function for
summarization based on semantic volume (§2),
and we provide a fast greedy algorithm that can
be used to maximize it (§3). We show that our
method outperforms competing extractive base-
lines under similar experimental conditions on
benchmark summarization datasets (§4).

2 Extractive Summarization Models

Assume we are given a set of N sentences: D =
{s1, s2, . . . , sN} from one or many documents,
and the goal is to produce a summary by choos-
ing a subset S of M sentences, where S ⊆ D and
M ≤ N , and the length of the summary is less
than or equal to L words. In this work, we as-
sume no summaries are available as training data.
Denote a binary indicator vector y ∈ RN , where
sentence i is included if and only if yi = 1 and 0
otherwise. Extractive summarization can be writ-
ten as an optimization problem:

max score(S) = score(D, y)
with respect to S equivalently y

subject to length(S) ≤ L

1961



with a scoring function score(D, y). A good scor-
ing function should assign higher scores to bet-
ter summaries. In the following, we describe two
commonly used scoring functions and our pro-
posed scoring function.

2.1 Maximal Marginal Relevance
The Maximal Marginal Relevance (MMR) method
(Carbonell and Goldstein, 1998) considers the fol-
lowing scoring function:

score(D, y) =
N∑

i=1

yiRel(si)−
N∑

i,j=1

yiyjSim(si, sj)

where Rel(si) measures the relevancy of sentence
i and Sim(si, sj) measures the (e.g., cosine) simi-
larity between sentence i and sentence j. The in-
tuition is to choose sentences that are highly rel-
evant to the document(s) and avoid redundancy.
The above maximization problem has been shown
to be NP-hard, solvable exactly using ILP (Mc-
Donald, 2007). A greedy algorithm that approxi-
mates the global solution by adding one sentence
at a time to maximize the overall score (Lin and
Bilmes, 2010) is often used in practice.

2.2 Coverage-Based Summarization
Another popular scoring function aims to give
higher scores for covering more diverse concepts
in the summary. Gillick et al. (2008) use bigrams
as a surrogate for concepts. Following convention,
we extract bigrams from each sentence si ∈ D.
Denote the number of unique bigrams extracted
from all sentences by B. We introduce another
binary vector z ∈ RB to indicate the presence or
absence of a bigram in the summary, and a binary
indicator matrix M ∈ RN×B , where mi,j is 1 if
and only if bigram j is present in sentence i and 0
otherwise. The scoring function is:

score(D, y, z) =
B∑

j=1

bjzj

and the two additional constraints are:

∀j ∈ [B],∀i ∈ [N ] yimi,j ≤ zj

∀j ∈ [B]
N∑

i=1

yimi,j ≥ zj

where we use [B] as a shorthand for {1, 2, . . . , B}.
The first constraint makes sure that selecting a sen-
tence implies selecting all its bigrams, whereas the

Figure 1: A toy example of seven sentences
projected into a two-dimensional semantic space.
Consider the case when the maximum summary
length is four sentences. Our scoring function is
optimized by chooseing the four sentences in red
as the summary, since they maximize the volume
(area in two dimensions).

second constraint makes sure that selecting a bi-
gram implies selecting at least one of the sentences
that contains it. In this formulation, there is no ex-
plicit penalty on redundancy. However, insofar as
redundant sentences cover fewer bigrams, they are
implicitly discouraged. Although the above scor-
ing function also results in an NP-hard problem,
an off-the-shelf ILP solver (Gillick et al., 2008)
or a dual decomposition algorithm (Almeida and
Martins, 2013) can be used to solve it in practice.

2.3 Semantic Volume
We introduce a new scoring function for summa-
rization. The main idea is based on the notion of
coverage, but in a distributed semantic space: a
good summary should have broad semantic cover-
age with respect to document contents. For every
sentence si, i ∈ [N ], we denote its continuous se-
mantic representation in a K-dimensional seman-
tic space by Ω(si) = ui ∈ RK , where Ω is a func-
tion that takes a sentence and returns its semantic
vector representation. We denote embeddings of
all sentences in D with the function Ω by Ω(D).
We will return to the choice of Ω later. We propose
to use a scoring function that maximizes the vol-
ume of selected sentences in this semantic space:

score(D, y) = Volume(Ω(D), y) = Volume(Ω(S))

In the case when K = 2, this scoring function
maximizes the area of a polytope, as illustrated in
Figure 1. In the example, there exists a maximum
number of sentences that can be selected such that
adding more sentences does not increase the score,
i.e., the set of selected sentences forms a convex
hull of the set of all sentences. The sentences
forming a convex hull may together be longer than

1962



L words, so we seek to maximize the volume of
the summary under this constraint.

There are many choices of Ω that we can use to
produce sentence embeddings. As an exploratory
study, we construct a vector of bigrams for each
sentence, that is, si ∈ RB,∀i ∈ [N ]. If bigram b
is present in si, we let si,b be the number of doc-
uments in the corpus that contain bigram b, and
zero otherwise. We stack these vectors in columns
to produce a matrix S ∈ RN×B , where N is the
number of sentences in the corpus and B is the
number of bigrams. We then perform singular
value decomposition (SVD) on S = UΣV>. We
use UK ∈ RN×K as the sentence representations,
where K is a parameter that specifies the number
of latent dimensions. Instead of performing SVD,
we can also take si ∈ RB as our sentence repre-
sentation, which makes our method resemble the
bigram coverage-based summarization approach.
However, this makes si a very sparse vector. Pro-
jecting to a lower dimensional space makes sense
to allow the representation to incorporate informa-
tion from (bigram) cooccurrences and share infor-
mation across bigrams.

3 Volume Maximization

Given the semantic coverage scoring function in
§2.3, our optimization problem is:

max score(S) = Volume(Ω(S))
with respect to S

subject to length(S) ≤ L
For computational considerations, we propose to
use a greedy algorithm that approximates the so-
lution by iteratively adding a sentence that max-
imizes the current semantic coverage, given that
the length constraint is still satisfied. The main
steps in our algorithm are as follows. We first
find the sentence that is farthest from the cluster
centroid and add it to S. Next, we find the sen-
tence that is farthest from the first sentence and
add it to S. Given a set of already selected sen-
tences, we choose the next one by finding the sen-
tence farthest from the subspace spanned by sen-
tences already in the set. We repeat this process
until we have gone through all sentences, break-
ing ties arbitrarily and checking whether adding
a sentence to S will result in a violation of the
length constraint. This method is summarized in
Algorithm 1. We note that related variants of our
method for maximizing volume have appeared in

Algorithm 1 Greedy algorithm for approximately
maximizing the semantic volume given a budget
constraint.
Input: Budget constraint L, sentence representa-

tions R = {u1, u2, . . . , uN}
S = {}, B = {}
Compute the cluster centroid c: 1N

∑N
i=1 ui.

p← index of sentence that is farthest from c.
S = S ∪ {sp}. I add first sentence
q ← index of sentence that is farthest from sp.
S = S ∪ {sq}. I add second sentence
b0 =

uq
‖uq‖ , B = B ∪ {u0}

total length = length(sp) + length(sq)
for i = 1, . . . , N − 2 do

r ← index of sentence that is farthest from
the subspace of Span(B). I see text
if total length + length(sr) ≤ L then

S = S ∪ {sr}.
br = ur‖ur‖ , B = B ∪ {br}.
total length = total length + length(sr)

end if
end for

other applications, such as remote sensing (Nasci-
mento and Dias, 2005; Gomez et al., 2007) and
topic modeling (Arora et al., 2012; Arora et al.,
2013).

Computing Distance to a Subspace Our algo-
rithm involves finding a point farthest from a sub-
space (except for the first and second sentences,
which can be selected by computing pointwise dis-
tances). In order for this algorithm to be efficient,
we need this operation to be fast, since it is ex-
ecuted frequently. There are several established
methods to compute the distance between a point
to a subspace spanned by sentences in S. For com-
pleteness, we describe one method based on the
Gram-Schmidt process (Laplace, 1812) here.

We maintain a set of basis vectors, denoted by
B. Our first basis vector consists of one element:
b0 =

uq
‖uq‖ , where q is the second sentence chosen

above. Next, we project each candidate sentence i
to this basis vector:

Projb0(ui) = (u
>
i b0)b0,

and find the distance by computing
Distance(ui, B) = ‖ui − Projb0(ui)‖. Once we
find the farthest sentence r, we add a new basis
vector B = B ∪ {br}, where br = ur‖ur‖ and
repeat this process. When there are more than one

1963



basis vectors, we find the distance by computing:

Distance(ui, B) =

∥∥∥∥∥∥ui −
∑
bj∈B

Projbj (ui)

∥∥∥∥∥∥ .
4 Experiments

4.1 Setup
We evaluate our proposed method on the non-
update portion of TAC-2008 and TAC-2009. The
datasets contain 48 and 44 multi-document sum-
marization problems, respectively. Each problem
has 10 news articles as input; each is to be sum-
marized in a maximum of L = 100 words. There
are 4 human reference summaries for each prob-
lem, against which an automatically generated
summary is compared. We compare our method
with two baselines: Maximal Marginal Relevance
(MMR, §2.1) and the coverage-based summariza-
tion method (CBS, §2.2). ROUGE (Lin, 2004) is
used to evaluate the summarization results.

For preprocessing, we tokenize, stem with the
Porter (1980) stemmer, and split documents into
sentences. We remove bigrams consisting of only
stopwords and bigrams which appear in less than
3 sentences. As a result, we have 2,746 and 3,273
bigrams for the TAC-2008 and TAC-2009 datasets
respectively. Unlabeled data can help generate
better sentence representations. For each sum-
marization problem in each dataset, we use other
problems in the same dataset as unlabeled data.
We concatenate every problem in each dataset and
perform SVD on this matrix (§2.3). Note that this
also means we only need to do one SVD for each
dataset.

4.2 Results
Table 1 shows results on the TAC-2008 and TAC-
2009 datasets. We report results for our method
with K = 500 (Volume 500), and K = 600 (Vol-
ume 600). We also include results for an oracle
model that has access to the human reference sum-
maries and extracts sentences that maximize bi-
gram recall as an upper bound. Similar to previous
findings, CBS is generally better than MMR. Our
method outperforms other competing methods, al-
though the optimal value of K is different in each
dataset. The improvements with our proposed ap-
proach are small in terms of R-2. This is likely
because the R-2 score computes bigram overlaps,
and the CBS method that directly maximizes bi-
gram coverage is already a resonable approach to

optimizing this metric (although still worse than
the best of our methods).

Methods TAC-2008 TAC-2009R-1 R-2 R-1 R-2
MMR 34.08 9.30 31.87 7.99
CBS 35.83 9.43 32.70 8.84
Volume 500 37.40 9.17 34.08 8.91
Volume 600 37.50 9.58 34.37 8.76
Oracle 46.06 19.33 46.77 16.99

Table 1: Results on the TAC-2008 and TAC-2009
datasets. “Volume” refers to our method, shown
with two embedding sizes.

0 200 400 600 800 1000
11
.0

12
.0

13
.0

Our method
CBS
MMR

number of dimensions

R-
SU
4

Figure 2: R-SU4 scores as we vary the number of
dimensions (K) on the TAC-2008 datasets.

5 Discussion

Runtime comparisons In terms of inference
running time, all methods perform reasonably fast.
MMR is the slowest, on average it takes 0.38 sec-
onds per problem, followed by our method at 0.17
seconds per problem, and CBS at 0.15 seconds
per problem. However, our implementations of
MMR and Algorithm 1 are in Python, whereas we
use an optimzed solver from Gurobi for our CBS
baseline. For preprocessing, our method is the
slowest, since we need to compute sentence em-
beddings using SVD. There are about 10,000 sen-
tences and 3,000 bigrams for each dataset. SVD
takes approximately 2.5 minutes (150 seconds) us-
ing Matlab on our 12-core machine with 24GB
RAM. Our method introduces another hyperpa-
rameter, the number of latent dimensions K for
sentence embeddings. We observe that the optimal
value depends on the dataset, although a value in
the range of 400 to 800 seems best. Figure 2 shows
R-SU4 scores on the TAC-2008 dataset as we vary
K.

Other sentence projection methods We use
SVD in this study for computing sentence embed-
dings. As mentioned previously, our summariza-

1964



tion approach can benefit from advances in neural-
network-based sentence representations (Jenatton
et al., 2011; Le and Mikolov, 2014; Kalchbrenner
et al., 2014). These models can also produce vec-
tor representations of sentences, so Algorithm 1
can be readily applied to the learned representa-
tions. Our work opens up a possibility to make
summarization a future benchmark task for evalu-
ating the quality of sentence representations.

Our method is related to determinantal point
processes (DPPs; Gillenwater et al., 2012; Kulesza
and Taskar, 2012) in that they both seek to maxi-
mize the volume spanned by sentence vectors to
produce a summary. In DPP-based approaches,
quality and selectional diversity correspond to
vector magnitude and angle respectively. In this
work, the length of a sentence vector is not tai-
lored to encode quality in terms of representative-
ness directly. In contrast, we rely on sentence em-
bedding methods to produce a semantic space and
assume that a good summary should have a large
volume in the semantic space. We show that a sim-
ple singular value decomposition embedding of
sentences—one that is not especially tuned for this
task—produces reasonably good results. We leave
exploration of other sentence embedding methods
to future work.

Future work Our method could be extended for
compressive summarization, by simply including
compressed sentences in the embedded space and
running Algorithm 1 without any change. This re-
sembles the summarization methods that jointly
extracts and compresses (Berg-Kirkpatrick et al.,
2011; Woodsend and Lapata, 2012; Almeida and
Martins, 2013). Another alternative is a pipeline
approach, where extractive summarization is fol-
lowed or preceded by a sentence compression
module, which can be built and tuned indepen-
dent of our proposed extractive method (Knight
and Marcu, 2000; Lin, 2003; Zajic et al., 2007;
Wang et al., 2013; Li et al., 2013).

We are also interested in exploring volume as
a relevance function within MMR. MMR avoids
redundancy by penalizing redundant sentences,
whereas in our method semantic redundancy is
inherently discouraged since the method chooses
sentences to maximize volume. Depending on
the method used to embed sentences, this might
not translate directly into avoiding n-gram redun-
dancy. Plugging our scoring function to an MMR
objective is a simple way to enforce diversity.

Finally, an interesting future direction is find-
ing an exact tractable solution to the volume max-
imization problem (or demonstrating that one does
not exist).

6 Conclusion

We introduced a summarization approach based
on maximizing volume in a semantic vector space.
We showed an algorithm to efficiently perform
volume maximization in this semantic space. We
demonstrated that our method outperforms exist-
ing state-of-the-art extractive methods on bench-
mark summarization datasets.

Acknowledgments
We thank anonymous reviewers for helpful sug-
gestions. This work was supported by the Defense
Advanced Research Projects Agency through
grant FA87501420244 and by NSF grant SaTC-
1330596. This work was completed while the au-
thors were at CMU.

References
Miguel B. Almeida and Andre F. T. Martins. 2013.

Fast and robust compressive summarization with
dual decomposition and multi-task learning. In
Proc. of ACL.

Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur
Moitra. 2012. Computing a nonnegative matrix fac-
torization – provably. In Proc. of STOC.

Sanjeev Arora, Rong Ge, Yoni Halpren, David Mimno,
Ankur Moitra, David Sontag, Yichen Wu, and
Michael Zu. 2013. A practical algorithm for topic
modeling with provable guarantees. In Proc. of
ICML.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of ACL.

Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR.

Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proc. of EMNLP-CoNLL.

Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008. The ICSI summarization system at TAC 2008.
In Proc. of TAC.

C. Gomez, H. Le Borgne, P. Allemand, C. Delacourt,
and P. Ledru. 2007. N-findr method versus indepen-
dent component analysis for lithological identifica-
tion in hyperspectral imagery. International Journal
of Remote Sensing, 28(23):5315–5338.

1965



Rodolphe Jenatton, Julien Mairal, Gullaume Obozin-
ski, and Francis Bach. 2011. Proximal methods
for hierarchical sparse coding. Journal of Machine
Learning Research, 12:2297–2334.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proc. of ACL.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization – step one: Sentence compres-
sion. In Proc. of AAAI.

Alex Kulesza and Ben Taskar. 2012. Determinantal
point processes for machine learning. Foundations
and Trends in Machine Learning, 5(2–3):123–286.

Pierre-Simon Laplace. 1812. Theorie analytique des
probabilites. Courcier, Paris.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proc. of
ICML.

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proc. of EMNLP.

Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proc. of NAACL-HLT.

Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression – A pilot study.
In Proc. of Workshop on Information Retrieval with
Asian Language.

Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proc. of the ACL Work-
shop on Text Summarization Branches Out.

Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proc. of the ACL Workshop on
Integer Linear Programming for Natural Language
Processing.

Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proc. of ECIR.

Jose M. P. Nascimento and Jose M. Bioucas Dias.
2005. Vertex component analysis: A fast algorithm
to unmix hyperspectral data. Proc. of IEEE Transac-
tion on Geoscience and Remote Sensing, 43(4):898–
910.

M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.

Lu Wang, Hema Raghavan Vittorio Castelli Radu Flo-
rian, and Claire Cardie. 2013. A sentence compres-
sion based framework to query-focused multidocu-
ment summarization. In Proc. of ACL.

Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proc. of EMNLP.

Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proc. of IJCAI.

David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing and Manage-
ment, 43(6):1549–1570.

1966


