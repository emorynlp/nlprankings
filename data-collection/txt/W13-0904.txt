










































Semantic Signatures for Example-Based Linguistic Metaphor Detection


Proceedings of the First Workshop on Metaphor in NLP, pages 27–35,
Atlanta, Georgia, 13 June 2013. c©2013 Association for Computational Linguistics

Semantic Signatures for Example-Based Linguistic Metaphor Detection

Michael Mohler and David Bracewell and David Hinote and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA

{michael,david,dhinote,marc}@languagecomputer.com

Abstract

Metaphor is a pervasive feature of human lan-
guage that enables us to conceptualize and
communicate abstract concepts using more
concrete terminology. Unfortunately, it is
also a feature that serves to confound a com-
puter’s ability to comprehend natural human
language. We present a method to detect
linguistic metaphors by inducing a domain-
aware semantic signature for a given text and
compare this signature against a large index
of known metaphors. By training a suite of
binary classifiers using the results of several
semantic signature-based rankings of the in-
dex, we are able to detect linguistic metaphors
in unstructured text at a significantly higher
precision as compared to several baseline ap-
proaches.

1 Introduction

Metaphor is a widely-used literary mechanism
which allows for the comparison of seemingly un-
related concepts. It has been thoroughly studied in
both the linguistics literature (Ahrens et al., 2003;
Lakoff and Johnson, 1980; Tourangeau and Stern-
berg, 1982; Wilks, 1978) and more recently within
the field of computational linguistics.1 Although
there have been many influential theories regarding
the cognitive basis of metaphor, the most promi-
nent among them is Lakoff’s Contemporary The-
ory of Metaphor (Lakoff and Johnson, 1980; Lakoff,
1993), which popularized the idea of a conceptual

1For a broad survey of the relevant literature, see Shutova
(2010).

metaphor mapping. Within the cognitive framework
of a given conceptual mapping, terms pertaining to
one concept or domain (the source) can be used fig-
uratively to express some aspect of another concept
or domain (the target). For example, the conceptual
metaphor “Life is a Journey” indicates a medium
within which the target concept “life” may be more
easily discussed and understood. This particular
mapping allows us to speak of one being stuck in a
“dead-end” job, a crucial decision as being a “fork in
the road”, or someone’s life “taking a wrong turn”.

By allowing us to discuss an abstract target con-
cept using the vocabulary and world knowledge
associated with a more familiar source concept,
metaphor serves as a vehicle for human communica-
tion and understanding, and as such, has been found
to be extremely prevalent in natural language, oc-
curring as often as every third sentence (Shutova et
al., 2010). As a consequence of this ubiquity, it is
crucial that any system tasked with the understand-
ing of natural language be capable of detecting the
presence of metaphor in text and of modeling the
intended semantic content of the metaphoric expres-
sion. In this work, we first induce a domain-sensitive
semantic signature which we define as a set of highly
related and interlinked WordNet (Fellbaum, 1998)
senses drawn and augmented from a text that may
be used to place the text within the semantic space
of a metaphoric concept. We then employ a suite
of binary classifiers to detect metaphoricity within
a text by comparing its semantic signature to a set
of known metaphors. If the semantic signature of
the text closely matches the signature of a known
metaphor, we propose that it is likely to represent

27



Example Metaphor
Obama heard a bomb ticking in his left ear. No
Obama heard another political bomb ticking, this time in his left ear. Yes

Table 1: The top sentence describes a literal bomb ticking, while the bottom sentence uses metaphoric language to
describe an impending political disaster.

an instance of the same conceptual metaphor. To fa-
cilitate this work, we have built an index of known
metaphors within a particular target domain. We
have selected the domain of Governance which we
define broadly to include electoral politics, the set-
ting and enactment of economic policy, and the
creation, application, and enforcement of rules and
laws.

The problem of metaphor as it relates to computer
understanding is illustrated in the example sentences
of Table 1. A strictly literal reading suggests that the
two sentences are describing something very similar.
At the very least, the semantics of the phrases “bomb
ticking” and “in his left ear” are indistinguishable
without the added knowledge that the second sen-
tence is using metaphor to convey information about
something altogether different from explosives and
body parts. From the context of the full sentences,
it is clear that while the first sentence is straight-
forwardly describing Obama and his perception of
a literal bomb, the second is describing an impend-
ing political crisis as though it were a bomb. Rather
than a literal “ear” this sentence uses the phrase “in
his left ear” to suggest that the source of the crisis in
on the political “left”. In order for an automated sys-
tem to correctly understand the intended meaning of
these sentences, it must first be aware that the text
under consideration is not to be taken literally, and
given this knowledge, it must employ all available
knowledge of the underlying conceptual mapping to
appropriately interpret the text in context.

The remainder of this work is organized as fol-
lows. In Section 2, we survey related work in se-
mantic representation and linguistic metaphor iden-
tification. Section 3 describes in detail our approach
to metaphor identification through the use of seman-
tic signatures. In Section 4, we discuss the setup of
our experiment which includes the creation of our
metaphor index as well as the extraction and anno-
tation of our training and testing data sets. Finally,

we show the results of our experiments in Section 5
and share our conclusions in Section 6.

2 Related Work

The phenomenon of metaphor has been studied
by researchers across multiple disciplines, includ-
ing psychology, linguistics, sociology, anthropol-
ogy, and computational linguistics. A number of
theories of metaphor have been proposed, includ-
ing the Contemporary Theory of Metaphor (Lakoff,
1993), the Conceptual Mapping Model (Ahrens et
al., 2003), the Structure Mapping Model (Wolff and
Gentner, 2000), and the Attribute Categorization
Hypothesis (McGlone, 1996). Based on these the-
ories, large collections of metaphors have been as-
sembled and published for use by researchers. The
Master Metaphor List (MML) (Lakoff, 1994) groups
linguistic metaphors together according to their
conceptual mapping, and the Hamburg Metaphor
Database (HMD) (Eilts and Lönneker, 2002) for
French and German fuses EuroWordNet synsets
with the MML source and target domains for a ro-
bust source of metaphoric semantics in those lan-
guages.

In recent years, the computational linguistics
community has seen substantial activity in the de-
tection of figurative language (Bogdanova, 2010;
Li and Sporleder, 2010; Peters and Wilks, 2003;
Shutova, 2011) one aspect of which is the iden-
tification of metaphoric expressions in text (Fass,
1991; Shutova et al., 2010; Mason, 2004). Much of
the early work on the identification of metaphor re-
lied upon hand-crafted world knowledge. The met*
(Fass, 1991) system sought to determine whether an
expression was literal or figurative by detecting the
violation of selectional preferences. Figurative ex-
pressions were then classified as either metonymic,
using hand-crafted patterns, or metaphoric, us-
ing a manually constructed database of analogies.
The CorMet (Mason, 2004) system determined the

28



source and target concepts of a metaphoric expres-
sion using domain-specific selectional preferences
mined from Internet resources. More recent work
has examined noun-verb clustering (Shutova et al.,
2010) which starts from a small seed set of one-
word metaphors and results in clusters that rep-
resent source and target concepts connected via a
metaphoric relation. These clusters are then used to
annotate the metaphoricity of text.

Similar to our work, the Metaphor Interpreta-
tion, Denotation, and Acquisition System (MIDAS)
(Martin, 1990) employed a database of conventional
metaphors that could be searched to find a match
for a metaphor discovered in text. If no match
was found, the metaphoric text was replaced with a
more abstract equivalent (e.g. a hypernym) and the
database was searched again. If a match was found,
an interpretation mapping was activated, and the
novel metaphor would be added to the database for
use in future encounters. Unfortunately, this tech-
nique was limited to interpreting known metaphors
(and descendants of known metaphors) and was un-
able to detect truly novel usages. By expanding the
metaphors using a more robust semantic signature,
we attempt to transcend this limitation thereby pro-
ducing a more durable system for metaphoric exam-
ple linking.

An additional vein of metaphor research has
sought to model the human processing of metaphor
as a semantic space within which source and tar-
get concepts can be placed such that the similar-
ity between their representations within this space
(i.e. semantic vectors) can be sensibly quantified
(Katz, 1992; Utsumi, 2011). One computational
example of this approach (Kintsch, 2000) has em-
ployed latent semantic analysis (LSA) (Landauer
and Dumais, 1997) to represent the semantic space
of the metaphors in a reduced dimensionality (i.e.
using singular value decomposition). In their ap-
proach, metaphors were represented as a set of terms
found using a spreading activation algorithm in-
formed by the terms’ independent vector related-
ness to the source and target concepts within some
LSA space. By contrast, we have chosen to rep-
resent the metaphoric space using WordNet senses
which have been shown in previous work(Lönneker,
2003) to represent a viable representation language
for metaphor. We believe that the ontological knowl-

edge encoded in the semantic relationships of Word-
Net represents an improvement over the distribu-
tional relatedness encoded within an LSA vector.

Also of relevance to the construction and use of
semantic signatures is current research on the induc-
tion of topic signatures. A topic signature is a set of
related words with associated weights which define
and indicate the distinct topics within a text. In their
work on automated summarization, Lin and Hovy
(2000) developed a method for the construction of
topic signatures which were mined from a large cor-
pus. Similarly, Harabagiu and Lacatusu (2005) ex-
plored the use of topic signatures and enhanced topic
signatures for their work on multi-document sum-
marization. By contrast, we explore the use of se-
mantic signatures which serve to enrich the seman-
tics of the source and target frame concepts being
expressed in a text for the purpose of detecting the
presence of metaphor.

3 Methodology

In this work, we approach the task of linguis-
tic metaphor detection as a classification problem.
Starting from a known target domain (i.e. Gover-
nance), we first produce a target domain signature
which represents the target-specific dimensions of
the full conceptual space. Using this domain sig-
nature, we are able to separate the individual terms
of a sentence into source frame elements and tar-
get frame elements and to independently perform a
semantic expansion for each set of elements using
WordNet and Wikipedia as described in our earlier
work (Bracewell et al., 2013). Taken together, the
semantic expansions of a text’s source frame ele-
ments and target frame elements make up the full se-
mantic signature of the text which can then be com-
pared to an index of semantic signatures generated
for a collection of manually detected metaphors. We
use as features for our classifiers a set of metrics that
are able to quantify the similarity between the given
semantic signature and the signatures of metaphors
found within the index.

3.1 Constructing a Target Domain Signature

In order to produce a semantic representation of the
text, we first build a target domain signature, which
we define as a set of highly related and interlinked

29



Figure 1: Focused crawling of Wikipedia articles pertaining to the target concept using intra-wiki links

Figure 2: Constructing the domain signature of the target concept from Wikipedia articles pertaining to the target
concept

WordNet senses that correspond to our particular
target domain with statistical reliability. For ex-
ample, in the domain of Governance the concepts
of “law”, “government”, and “administrator”, along
with their associated senses in WordNet, are present
in the domain signature. We generate this signa-
ture using semantic knowledge encoded in the fol-
lowing resources: (1) the semantic network encoded
in WordNet; (2) the semantic structure implicit in
Wikipedia; and (3) collocation statistics taken from
the statistical analysis of a large corpora. In par-
ticular, we use Wikipedia as an important source
of world knowledge which is capable of provid-
ing information about concepts, such as named en-
tities, that are not found in WordNet as shown in
several recent studies (Toral et al., 2009; Niemann
and Gurevych, 2011). For example, the organi-
zation “Bilderberg Group” is not present in Word-
Net, but can easily be found in Wikipedia where
it is listed under such categories as “Global trade
and professional organizations”, “International busi-
ness”, and “International non-governmental orga-
nizations”. From these categories we can deter-
mine that the “Bilderberg Group” is highly related
to WordNet senses such as “professional organiza-
tion”, “business”, “international”, and “nongovern-
mental organization”.

We begin our construction of the domain signa-
ture by utilizing the semantic markup in Wikipedia
to collect articles that are highly related to the tar-
get concept by searching for the target concept (and

optionally content words making up the definition
of the target concept) in the Wikipedia article titles
and redirects. These articles then serve as a “seed
set” for a Wikipedia crawl over the intra-wiki links
present in the articles. By initiating the crawl on
these links, it becomes focused on the particular do-
main expressed in the seed articles. The crawling
process continues until either no new articles are
found or a predefined crawl depth (from the set of
seed articles) has been reached. The process is illus-
trated in Figure 1. The result of the crawl is a set
of Wikipedia articles whose domain is related to the
target concept. From this set of articles, the domain
signature can be built by exploiting the semantic in-
formation provided by WordNet.

The process of going from a set of target concept
articles to a domain signature is illustrated in Fig-
ure 2 and begins by associating the terms contained
in the gathered Wikipedia articles with all of their
possible WordNet senses (i.e. no word sense disam-
biguation is performed). The word senses are then
expanded using the lexical (e.g. derivationally re-
lated forms) and semantic relations (e.g. hypernym
and hyponym) available in WordNet. These senses
are then clustered to eliminate irrelevant senses us-
ing the graph-based Chinese Whispers algorithm
(Biemann, 2006). We transform our collection of
word senses into a graph by treating each word sense
as a vertex of an undirected, fully-connected graph
where edge weights are taken to be the product of
the Hirst and St-Onge (1998) WordNet similarity be-

30



tween the two word senses and the first-order cor-
pus cooccurrence of the two terms. In particular, we
use the normalized pointwise mutual information as
computed using a web-scale corpus.

The clusters resulting from the Chinese Whispers
algorithm contain semantically and topically similar
word senses such that the size of a cluster is directly
proportional to the centrality of the concepts within
the cluster as they pertain to the target domain. After
removing stopwords from the clusters, any clusters
below a predefined size are removed. Any cluster
with a low2 average normalized pointwise mutual
information (npmi) score between the word senses
in the cluster and the word senses in the set of terms
related to the target are likewise removed. This set
of target-related terms used in calculating the npmi
are constructed from the gathered Wikipedia articles
using TF-IDF (term frequency inverse document fre-
quency), where TF is calculated within the gathered
articles and IDF is calculated using the entire textual
content of Wikipedia. After pruning clusters based
on size and score, the set of word senses that remain
are taken to be the set of concepts that make up the
target domain signature.

3.2 Building Semantic Signatures for
Unstructured Text

After constructing a signature that defines the do-
main of the target concept, it is possible to use this
signature to map a given text (e.g. a sentence) into
a multidimensional conceptual space which allows
us to compare two texts directly based on their con-
ceptual similarity. This process begins by mapping
the words of the text into WordNet and extracting
the four most frequent senses for each term. In or-
der to improve coverage and to capture entities and
terms not found in WordNet, we also map terms
to Wikipedia articles based on a statistical measure
which considers both the text of the article and the
intra-wiki links. The Wikipedia articles are then
mapped back to WordNet senses using the text of
the categories associated with the article.

In the next step, source and target frame ele-
ments of a given text are separated using the Word-
Net senses contained in the target domain signature.

2We define low as being below an empirically defined
threshold, τ .

Terms in the text which have some WordNet sense
that is included in the domain signature are clas-
sified as target frame elements while those that do
not are considered source frame elements. Figure 3
shows an overview of the process for determining
the source and target concepts within a text. The
remainder of the signature induction process is per-
formed separately for the source and target frame el-
ements. In both cases, the senses are expanded using
the lexical and semantic relations encoded in Word-
Net, including hypernymy, domain categories, and
pertainymy. Additionally, source frame elements
are expanded using the content words found in the
glosses associated with each of the noun and verb
senses. Taken together, these concepts represent the
dimensions of a full conceptual space which can be
separately expressed as the source concept dimen-
sions and target concept dimensions of the space.

Figure 3: Example of a generated conceptual space for a
given text. In this work, only one iteration of the sense
expansion is performed.

In order to determine the correct senses for in-
clusion in the semantic signature of a text, cluster-
ing is performed using the same methodology as
in the construction of the domain signature. First,
a graph is built from the senses with edge weights
assigned based on WordNet similarity and cooccur-
rence. Then, the Chinese Whispers algorithm is used
to cluster the graph which serves to disambiguate the
senses and to prioritize which senses are examined
and incorporated into the source concept dimensions
of the conceptual space. Word senses are prioritized
by ranking the clusters based on their size and on the
highest scoring word sense contained in the cluster
using:

rank(c) = size(c) ·
(∑

s score(s)

|c|

)
(1)

where c is the cluster, s is a word sense in the clus-

31



ter, and |c| is the total number of word senses in the
cluster. The senses are scored using: (1) the degree
distribution of the sense in the graph (more central
word senses are given a higher weight); and (2) the
length of the shortest path to the terms appearing in
the given text with concepts closer to the surface
form given a higher weight. Formally, score(s) is
calculated as:

score(s) =
degree(s) + dijkstra(s,R)

2
(2)

where degree(s) is degree distribution of s and
dijkstra(s,R) is the length of the shortest path in
the graph between s and some term in the original
text, R.

Clusters containing only one word sense or with
a score less than the average cluster score (µc) are
ignored. The remaining clusters and senses are
then examined for incorporation into the concep-
tual space with senses contained in higher ranked
clusters examined first. Senses are added as con-
cepts within the conceptual space when their score is
greater than the average word sense score (µs). To
decrease redundancy in the dimensions of the con-
ceptual space, neighbors of the added word sense in
the graph are excluded from future processing.

3.3 Classification
Given a semantic signature representing the place-
ment of a text within our conceptual space, it is pos-
sible to measure the conceptual distance to other sig-
natures within the same space. By mapping a set
of known metaphors into this space (using the pro-
cess described in Section 3.2), we can estimate the
likelihood that a given text contains some metaphor
(within the same target domain) by using the seman-
tic signature of the text to find the metaphors with
the most similar signatures and to measure their sim-
ilarity with the original signature.

We quantify this similarity using five related mea-
sures which are described in Table 2. Each of these
features involves producing a score that ranks ev-
ery metaphor in the index based upon the seman-
tic signature of the given text in a process similar to
that of traditional information retrieval. In particu-
lar, we use the signature of the text to build a query
against which the metaphors can be scored. For each

word sense included in the semantic signature, we
add a clause to the query which combines the vector
space model with the Boolean model so as to prefer
a high overlap of senses without requiring an identi-
cal match between the signatures.3

Three of the features simply take the score of
the highest ranked metaphor as returned by a query.
Most simply, the feature labeled Max Score (naı̈ve)
uses the full semantic signature for the text which
should serve to detect matches that are very simi-
lar in both the source concept dimensions and the
target concept dimensions. The features Max Score
(source) and Max Score (target) produce the query
using only the source concept dimensions of the
signature and the target concept dimensions respec-
tively.

The remaining two features score the metaphors
within the source dimensions and the target dimen-
sions separately before combining the results into a
joint score. The feature Max Score (joint) calculates
the product of the scores for each metaphor using the
source- and target-specific queries described above
and selects the maximum value among these prod-
ucts. The final feature, Joint Count, represents the
total number of metaphors with a score for both the
source and the target dimensions above some thresh-
old (µj). Unlike the more naı̈ve features for which a
very good score in one set of dimensions may incor-
rectly lead to a high overall score, these joint similar-
ity features explicitly require metaphors to match the
semantic signature of the text within both the source
and target dimensions simultaneously.

Altogether, these five features are used to train
a suite of binary classifiers to make a decision on
whether a given text is or is not a metaphor.

4 Experimental Setup

One crucial component of our linguistic metaphor
detection system is the index of metaphors (in the
domain of Governance) against which we com-
pare our candidate texts. As a part of this project,
we have produced an ever-growing, metaphor-rich
dataset taken from political speeches, political web-
sites (e.g. Communist Party USA, Tea Party sites,

3This functionality comes standard with the search function-
ality of Apache Lucene which we employ for the production of
our index.

32



Measure Description
Max Score (naı̈ve) Find the score of the metaphor that best matches the full semantic signature
Max Score (source) Find the score of the metaphor that best matches the source side of the semantic signature
Max Score (target) Find the score of the metaphor that best matches the target side of the semantic signature

Max Score (joint)
Independently score the metaphors by the target side and by the source side.
Find the metaphor with the highest product of the scores.

Joint Count
Independently score the metaphors by the target side and by the source side.
Count the number of metaphors that receive a positive score for both.

Table 2: The five features used by our metaphoricity classifiers.

etc.), and political commentary in web-zines and on-
line newspapers. Three annotators have analyzed
the raw texts and manually selected snippets of text
(with context) whenever some element in the text
seemed to have been used figuratively to describe
or stand in for another element not represented in
the text.4 Each of these metaphors is projected into
a conceptual space using the process described in
Section 3.2 and assembled into a searchable index.

For evaluation purposes, we have selected a sub-
set of our overall repository which consists of
500 raw documents that have been inspected for
metaphoricity by our annotators. We allocate 80%
of these documents for the training of our classi-
fiers and evaluate using the remaining 20%. In total,
our training data consists of 400 documents contain-
ing 1,028 positive examples of metaphor and around
16,000 negative examples. Our test set consists of
100 documents containing 4,041 sentences with 241
positive examples of metaphor and 3,800 negative
examples. For each sentence in each document, our
system attempts to determine whether the sentence
does or does not contain a metaphor within the do-
main of Governance.

We have experimented with several flavors of ma-
chine learning classification. In addition to an in-
house implementation of a binary maximum en-
tropy (MaxEnt) classifier, we have evaluated our re-
sults using four separate classifiers from the popu-
lar Weka machine learning toolkit.5 These include
an unpruned decision tree classifier (J48), a support
vector machine (SMO) approach using a quadratic

4Generally speaking, each annotator operated within a re-
gion of high precision and low recall, and the overlap between
individual annotators was low. As such, we have selected the
union of all metaphors detected by the annotators.

5http://www.cs.waikato.ac.nz/ml/weka/

kernel with parameters tuned via grid search, a rule-
based approach (JRIP), and a random forest clas-
sifier (RF). In addition, we have combined all five
classifiers into an ensemble classifier which uses a
uniformly-weighted voting methodology to arrive at
a final decision.

5 Results

We have evaluated our methodology in two ways.
First, we have performed an evaluation which high-
lights the discriminatory capabilities of our features
by testing on a balanced subset of our test data.
Next, we performed an evaluation which shows the
utility of each of our classifiers as they are applied
to real world data with a natural skew towards literal
usages.6 In both cases, we train on a balanced sub-
set of our training data using all 1,028 positive ex-
amples and a set of negative examples selected ran-
domly such that each document under consideration
contains the same number of positive and negative
examples. In an initial experiment, we trained our
classifiers on the full (skewed) training data, but the
results suggested that an error-minimizing strategy
would lead to all sentences being classified as “lit-
eral”.

As shown in Table 3, the choice of classifier ap-
pears significant. Several of the classifiers (J48,
JRIP, and MaxEnt) maintain a high recall suggest-
ing the ability of the tree- and rule-based classifiers
to reliably “filter out” non-metaphors. On the other
hand, other classifiers (SMO and ENSEMBLE) op-
erate in a mode of high precision suggesting that a
high confidence can be associated with their positive
classifications. In all cases, performance is signifi-

6Note that metaphors that are not related to the domain of
Governance are classified as “literal”.

33



Classifier Precision Recall F-Measure
J48 56.1% 93.0% 70.0%
JRIP 57.7% 79.3% 66.8%
MaxEnt 59.9% 72.6% 65.7%
ENSEMBLE 72.0% 42.7% 53.7%
RF 55.8% 47.7% 51.5%
SMO 75.0% 33.6% 46.4%
All metaphor 50.0% 100.0% 66.7%
Random baseline 50.0% 50.0% 50.0%

Table 3: The results of our experiments using several ma-
chine learning classifiers while evaluating on a dataset
with 241 positive examples and 241 negative examples.

cantly better than chance as reported by our random
baseline.7

Table 4 shows the result of evaluating the same
models on an unbalanced dataset with a natural
skew towards “literal” sentences which reflects a
more realistic use case in the context of linguistic
metaphor detection. The results suggest that, once
again, the decision tree classification accepts the
vast majority of all metaphors (93%), but also pro-
duces a significant number of false positives mak-
ing it difficult to usefully employ this classifier as
a complete metaphor detection system despite its
top-performing F-measure on the balanced dataset.
More useful is the SMO approach, which shows a
precision over twice that of the random baseline. Put
another way, a positive result from this classifier is
more than 110% more likely to be correct than a
random classification. From the standpoint of util-
ity, joining these classifiers in an ensemble config-
uration seems to combine the high precision of the
SMO classifier with the improved recall of the other
classifiers making the ensemble configuration a vi-
able choice in a real world scenario.

6 Conclusions

We have shown in this work the potential utility
of our example-based approach to detect metaphor
within a domain by comparing the semantic signa-
ture of a text with a set of known metaphors. Al-
though this technique is necessarily limited by the
coverage of the metaphors in the index, we believe
that it is a viable technique for metaphor detection

7According to Fisher’s exact test (one-tailed): RF (p <
0.02); all others (p < 0.0001).

Classifier Precision Recall F-Measure
SMO 12.7% 33.6% 18.4%
ENSEMBLE 11.2% 42.7% 17.8%
MaxEnt 8.7% 72.6% 15.6%
JRIP 8.1% 79.3% 14.8%
J48 7.6% 93.0% 14.0%
RF 7.4% 47.7% 12.7%
All metaphor 6.0% 100.0% 11.3%
Random baseline 6.0% 50.0% 10.7%

Table 4: The results of our experiments using several ma-
chine learning classifiers while evaluating on naturally
skewed dataset with 241 positive examples and 3,800
negative examples.

as more and more examples become available. In
future work, we hope to supplement our existing fea-
tures with such information as term imageability, the
transmission of affect, and selectional preference vi-
olation we believe will result in a robust system for
linguistic metaphor detection to further aid in the
computer understanding of natural language.

Acknowledgments

This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Labora-
tory contract number W911NF-12-C-0025. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. We would also like to thank our annotators
whose efforts have made this work possible.

References

K. Ahrens, S.F. Chung, and C. Huang. 2003. Concep-
tual metaphors: Ontology-based representation and
corpora driven mapping principles. In Proceedings
of the ACL 2003 workshop on Lexicon and figura-
tive language-Volume 14, pages 36–42. Association
for Computational Linguistics.

C. Biemann. 2006. Chinese whispers: an efficient graph
clustering algorithm and its application to natural lan-

34



guage processing problems. In Proceedings of the
First Workshop on Graph Based Methods for Natural
Language Processing, pages 73–80. Association for
Computational Linguistics.

D. Bogdanova. 2010. A framework for figurative lan-
guage detection based on sense differentiation. In Pro-
ceedings of the ACL 2010 Student Research Workshop,
pages 67–72. Association for Computational Linguis-
tics.

D. Bracewell, M. Tomlinson, and M. Mohler. 2013. De-
termining the conceptual space of metaphoric expres-
sions. In Computational Linguistics and Intelligent
Text Processing, pages 487–500. Springer.

C. Eilts and B. Lönneker. 2002. The Hamburg Metaphor
Database.

D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49–90.

C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.

S. Harabagiu and F. Lacatusu. 2005. Topic themes for
multi-document summarization. In Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 202–209. ACM.

G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, Word-
Net: An Electronic Lexical Database, pages 305–332.
MIT Press.

A.N. Katz. 1992. Psychological studies in metaphor pro-
cessing: extensions to the placement of terms in se-
mantic space. Poetics Today, pages 607–632.

W. Kintsch. 2000. Metaphor comprehension: A com-
putational theory. Psychonomic Bulletin & Review,
7(2):257–266.

G. Lakoff and M. Johnson. 1980. Metaphors we live by,
volume 111. Chicago London.

G. Lakoff. 1993. The contemporary theory of metaphor.
Metaphor and thought, 2:202–251.

G. Lakoff. 1994. Master metaphor list. University of
California.

T.K. Landauer and S.T. Dumais. 1997. A solution to
Plato’s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review; Psychological Review,
104(2):211.

L. Li and C. Sporleder. 2010. Using gaussian mixture
models to detect figurative language in context. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 297–300.
Association for Computational Linguistics.

C. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics-Volume 1, pages 495–501. Association for
Computational Linguistics.

B. Lönneker. 2003. Is there a way to represent metaphors
in WordNets?: insights from the Hamburg Metaphor
Database. In Proceedings of the ACL 2003 workshop
on Lexicon and figurative language-Volume 14, pages
18–27. Association for Computational Linguistics.

J.H. Martin. 1990. A computational model of metaphor
interpretation. Academic Press Professional, Inc.

Z.J. Mason. 2004. CorMet: A computational, corpus-
based conventional metaphor extraction system. Com-
putational Linguistics, 30(1):23–44.

M.S. McGlone. 1996. Conceptual metaphors and figura-
tive language interpretation: Food for thought? Jour-
nal of memory and language, 35(4):544–565.

E. Niemann and I. Gurevych. 2011. The people’s web
meets linguistic knowledge: Automatic sense align-
ment of Wikipedia and WordNet. In Proceedings of
the 9th International Conference on Computational
Semantics (IWCS), pages 205–214. Citeseer.

W. Peters and Y. Wilks. 2003. Data-driven detection
of figurative language use in electronic language re-
sources. Metaphor and Symbol, 18(3):161–173.

E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor
identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1002–1010. Associ-
ation for Computational Linguistics.

E. Shutova. 2010. Models of metaphor in NLP. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 688–697. Asso-
ciation for Computational Linguistics.

E.V. Shutova. 2011. Computational approaches to fig-
urative language. Ph.D. thesis, University of Cam-
bridge.

S.L. Toral, M.R. Martı́nez-Torres, F. Barrero, and
F. Cortés. 2009. An empirical study of the driving
forces behind online communities. Internet Research,
19(4):378–392.

R. Tourangeau and R.J. Sternberg. 1982. Understanding
and appreciating metaphors. Cognition, 11(3):203–
244.

A. Utsumi. 2011. Computational exploration of
metaphor comprehension processes using a semantic
space model. Cognitive science, 35(2):251–296.

Y. Wilks. 1978. Making preferences more active. Artifi-
cial Intelligence, 11(3):197–223.

P. Wolff and D. Gentner. 2000. Evidence for role-neutral
initial processing of metaphors. Journal of Experi-
mental Psychology: Learning, Memory, and Cogni-
tion, 26(2):529.

35


