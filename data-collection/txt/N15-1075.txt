



















































The Unreasonable Effectiveness of Word Representations for Twitter Named Entity Recognition


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 735–745,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

The Unreasonable Effectiveness of Word Representations
for Twitter Named Entity Recognition

Colin Cherry and Hongyu Guo
National Research Council Canada
first.last@nrc-cnrc.gc.ca

Abstract

Named entity recognition (NER) systems
trained on newswire perform very badly when
tested on Twitter. Signals that were reliable in
copy-edited text disappear almost entirely in
Twitter’s informal chatter, requiring the con-
struction of specialized models. Using well-
understood techniques, we set out to improve
Twitter NER performance when given a small
set of annotated training tweets. To lever-
age unlabeled tweets, we build Brown clus-
ters and word vectors, enabling generaliza-
tions across distributionally similar words. To
leverage annotated newswire data, we employ
an importance weighting scheme. Taken all
together, we establish a new state-of-the-art
on two common test sets. Though it is well-
known that word representations are useful for
NER, supporting experiments have thus far fo-
cused on newswire data. We emphasize the ef-
fectiveness of representations on Twitter NER,
and demonstrate that their inclusion can im-
prove performance by up to 20 F1.

1 Introduction

Named entity recognition (NER) is the task of find-
ing rigid designators as they appear in free text and
classifying them into coarse categories such as per-
son or location (Nadeau and Sekine, 2007). NER
enables many other information extraction tasks
such as relation extraction (Bunescu and Mooney,
2005) and entity linking (Ratinov et al., 2011).

There is considerable excitement at the prospect
of porting information extraction technology to so-
cial media platforms such as Twitter. Social media
reacts to world events faster than traditional news
sources, and its sub-communities pay close attention
to topics that other sources might ignore. An early

example of the potential inherent in social infor-
mation extraction is the Twitter Calendar (Ritter et
al., 2012), which detects upcoming events (concerts,
elections, video game releases, etc.) based on the
anticipatory chatter of Twitter users. Unfortunately,
processing social media text presents a unique set of
challenges, especially for technologies designed for
newswire: Twitter posts are short, the language is
informal, capitalization is inconsistent at best, and
spelling variations and abbreviations run rampant.

Armed with an affordable training set of 1,000
annotated tweets, we establish a strong baseline for
Twitter NER using well-understood techniques. We
build two unsupervised word representations in or-
der to leverage a large collection of unannotated
tweets, while a data-weighting technique allows us
to benefit from annotated newswire data. Taken
together, these two simple ideas establish a new
state-of-the-art for both our test sets. We rigorously
test the impact of both continuous and cluster-based
word representations on Twitter NER, emphasizing
the dramatic improvement that they bring. We also
bring the experimental methodology of the domain
adaptation community to Twitter NER, testing in-
domain, out-of-domain and combined training sce-
narios, and revealing that it is not trivial to benefit
from out-of-domain training data. Finally, an error
analysis helps us begin to understand which social
media challenges are being addressed by our adap-
tations, and which problems persist.

2 Background

Our work builds on a long line of research in dis-
criminative tagging (Collins, 2002), and its applica-
tion to named entity recognition (McCallum and Li,
2003). Our baseline tagger draws inspiration from
Sarawagi and Cohen (2004), who introduce the no-

735



tion of semi-Markov tagging for NER, and from de
Bruijn et al. (2011), who apply a similar tagger to
clinical information extraction.

A number of previous studies have closely ex-
amined the use of word representations in NER,
where one leverages unlabeled data to build features
that help the tagger generalize across similar words.
Miller et al. (2004) introduce this idea and provide
the framework to build representation features from
word clusters, while Lin and Wu (2009) extend this
technique with phrases and sheer masses of unla-
beled data. Turian et al. (2010) introduce continuous
vectors as alternative word representations, and pro-
vide several experiments comparing these with clus-
ters. Recently, Passos et al. (2014) have shown how
continuous representations can be tailored to NER
with a combination of context- and gazetteer-aware
objectives. All of these studies employ representa-
tions only in newswire scenarios. Ratinov and Roth
(2009) investigate cluster representations in a Web
NER task, but the performance of their baseline in-
dicates that it is not nearly so drastic a domain shift
as our Twitter task.

2.1 Adapting to Social Media

There has been much recent activity in adapting
NLP tools for social media. Ritter et al. (2011) col-
lect training data and adapt tools for a number of
tasks, including part-of-speech (POS) tagging, shal-
low parsing and NER. Owoputi et al. (2013) extends
a line of research on building robust POS taggers for
Twitter, and share our focus on the utility of word
representations in this domain.

Liu et al. (2011) carry out the first study to specif-
ically examine NER on Twitter. They use a nearest-
neighbour word classifier stacked with a CRF, along
with a boot-strapping scheme for semi-supervised
learning. Interestingly, they find no utility in us-
ing cluster-based word representations, perhaps be-
cause their model directly accounts for a type’s
global context with bag-of-word features. Ritter et
al. (2011) also examine Twitter NER, developing a
semi-supervised technique that uses labeled LDA to
project information from Freebase gazetteers onto
unlabeled tweets. Plank et al. (2014) suggest a
distant-supervision scheme, creating artificial train-
ing data by projecting reliable NER tags from web
pages onto the tweets that link to those pages.

Fromreide et al. (2014) and Plank et al. (2014)
point out that NER performance can be over-
estimated when a system is tested on data extracted
from the same pool as its training data. Temporal
effects and annotation biases can result in gains that
disappear when shifting to another test set. We fol-
low their lead by testing on data that was annotated
independently from our training data.

3 Methods

Our named entity recognizer is a discriminative,
semi-Markov tagger, trained online using large-
margin updates. It differs from word-based CRF
systems in three ways: its inference algorithm, its
tag structure, and its learning algorithm. This tag-
ger allows us to develop new systems quickly, but it
is important to emphasize that the adaptation strate-
gies described later in this section can just as easily
be applied to word-based CRFs.

Semi-Markov Inference

Sarawagi and Cohen (2004) describe a straight-
forward extension to the Viterbi algorithm that en-
ables the tagging of contiguous phrases instead of
words. Because each phrasal entity is tagged as a
unit, we can recover entity boundaries without dis-
tinguishing between Begin and Inside tags, leaving
the tagger to track only entity classes and Outside
tags. This in turn allows us to run our tagger with-
out Markov features. Since most entities are sur-
rounded by Outside tags, conditioning on previous
tag assignments has only limited utility. Finally, our
phrasal tags enable useful features that consider en-
tire entities, such as phrase-identity indicators.

Phrasal and Word-level Tags

In word-based models, it is beneficial to not only
identify words that Begin entities, but also those that
are in the middle (Inside) or at the end of entities
(Last), as well as entities that consist of exactly one
Unique word (Ratinov and Roth, 2009). Since we
tag entire phrases at once, we can easily assign each
word in the phrase to one of these four entity-relative
positions. Therefore, even though our tagger tracks
only entity class, its word-level features are anno-
tated as if we maintained a full BILUO tag set.

736



Passive-Aggressive Learning
We train our model with a structured version of

the Passive-Aggressive (PA) algorithm (Crammer et
al., 2006). The benefits of using PA in place of a
CRF are that we require only Viterbi inference, and
memory requirements are minimized, as we update
the model one training sentence at a time.

PA is an online, large-margin learning algorithm
that attempts to separate correct sequences from in-
correct ones by a margin of 1. For each update to
the weight vector w, we select a training sentence
x and its gold-standard tag sequence y. We use dy-
namic programming to search for a response ŷ that
maximizes the structured hinge loss:1

ŷ = arg max
y′ 6=y

=
[
1 + wT

(
Φ(x, y′)− Φ(x, y))]

(1)
where Φ() maps an (x, y) pair to a feature vector. If
the loss is greater than 0, we update our model:

w = w + τ
(
Φ(x, y)− Φ(x, ŷ)) (2)

where τ is an adaptive learning rate that scales the
update to the smallest step size that achieves 0 loss:

τ = min

(
C,

1 + wT
(
Φ(x, ŷ)− Φ(x, y))

||Φ(x, y)− Φ(x, ŷ)||

)
(3)

C is a hyper-parameter that truncates large steps to
prevent over-fitting. It is related to the C-parameter
of an SVM (Martins et al., 2010). To further
guard against over-fitting, we use the average of all
vectors w seen during training when tagging new
text (Collins, 2002).

Features
The feature function Φ(x, y) must decompose

into the semi-Markov dynamic program:

Φ(x, y) =
∑

(s,t,yj)∈D(x,y)
φ(s, t, yj , x) (4)

where D is a derivation decomposing (x, y) into J
entity-tag assignments (s, t, yj), each asserting that
the phrase xs . . . xt−1 is assigned the tag yj . Tagged
spans are non-overlapping, and to eliminate spurious

1This can be done by running a 2-best tagger. If the 1-best
answer is not correct (y′ 6= y), then it maximizes the loss, oth-
erwise, the 2-best answer maximizes the loss.

Phrase:
[yj ], [yj , xs . . . xt−1],
[yj , lc(xs . . . xt−1)], [yj , ss(xs . . . xt−1)]

Word, for each i s.t. s ≤ i < t:
{[yj , xi+k, k]}2k=−2, {[yj , ers,t(i), xi+k, k]}2k=−2,
{[yj , lc(xi+k), k]}2k=−2,
{[yj , ers,t(i), lc(xi+k), k]}2k=−2,
{[yj , ss(xi+k), k]}2k=−2,
{[yj , ers,t(i), ss(xi+k), k]}2k=−2,
{[yj , pf(n, xi)]}3n=1, {[yj , ers,t(i), pf(n, xi)]}3n=1,
{[yj , sf(n, xi)]}3n=1, {[yj , ers,t(i), sf(n, xi)]}3n=1,

Table 1: Baseline features φ(s, t, yj , x). [str] stands for
an indicator feature with the name str; lc() maps a string
onto its lowercased form; ss() maps a string onto its word
shape (“Apple Inc.” becomes “Aa Aa.”); pf(n, xi) and
sf(n, xi) are n-character prefixes and suffixes of xi; and
ers,t(i) maps an absolute sentence position i (s ≤ i < t)
to a relative entity position drawn from {B, I, L, U}.

ambiguity, constrained so that Outside can tag only
single-word spans (t = s+ 1).

Our baseline feature set, shown in Table 1, closely
mimics the set proposed by Ratnaparkhi (1996),
covering word identity, prefixes, suffixes and sur-
rounding words. It has been augmented with phrase-
identity indicators and hierarchical word-level tags.
These conjoin the entity class yj with the word’s
entity-relative position, backing off to yj alone.
Most features look only at a single word xi, which
improves efficiency by allowing the tagger to re-use
word-level scores across many phrasal tags.

There are some standard NER features that we
chose not to include. We follow Lin and Wu (2009)
in omitting POS tags and gazetteers in order to re-
duce our dependence on linguistic resources. We ex-
pect similar information to be provided by unsuper-
vised word representations, and we test this assump-
tion in Section 5.2. We omit context aggregation,
which accounts for the repetition of entities (Ratinov
and Roth, 2009), because Twitter’s short message
length reduces the utility of document-level features.

3.1 Word Representations

Our primary tool for domain adaptation will be un-
supervised word representations, which convey in-
formation about a word’s distributional profile.

737



Brown clusters, for each i s.t. s ≤ i < t:
{[yj , brn(n, xi), n]}n∈{2,4,8,12},
{[yj , ers,t(i), brn(n, xi), n]}n∈{2,4,8,12}
Word vectors, for each i s.t. s ≤ i < t:
{[yj , n] = w2v(n, xi)}300n=1,
{[yj , ers,t(i), n] = w2v(n, xi)}300n=1

Table 2: Word representation features in φ(s, t, yj , x).
brn(n, xi) maps a word xi to the first n bits of its Brown
cluster bit sequence. w2v(n, xi) maps xi to the nth com-
ponent of its word vector, and [str] = v stands for a real-
valued feature with name str and value v.

Brown Clusters
The Brown clustering algorithm assigns types to

a deterministic, hierarchical clustering, which has
been trained to optimize the likelihood of a first-
order, class-based language model (Brown et al.,
1992). The clusters capture both syntactic and se-
mantic regularities, and have been shown to perform
well as unsupervised part-of-speech taggers (Blun-
som and Cohn, 2011).

The clusters are organized into a binary tree struc-
ture; therefore, each cluster can be represented as a
bit string that encodes the branching decisions re-
quired to reach its leaf from the root. By truncating
the bit string at different prefix lengths, one can ac-
cess different granularities of clusters. Cluster mem-
bership can then be used to create indicators similar
to the baseline’s word identity features. This results
in two feature templates, shown in Table 2.2

This technique has been previously applied to
both newswire NER (Miller et al., 2004; Turian et
al., 2010; Passos et al., 2014) and Twitter NER (Rit-
ter et al., 2011; Plank et al., 2014). But previous
work on Twitter NER has not directly tested the im-
pact of Brown clusters; instead, they generally ap-
pear as part of an adapted baseline.

Word Vectors
An alternative word representation maps each

word type deterministically to a low-dimensional
continuous vector space. This technique was orig-
inally used as the bottom layer for continuous-space
language models (Bengio et al., 2003), where the

2We also experimented with templates over clusters and vec-
tors for surrounding words, to no benefit.

type-to-vector mapping can be learned with back-
propagation. However, Mikolov et al. (2013) have
shown that useful vector representations can be
learned more efficiently by eschewing the language-
modeling objective. Their skip-gram model, which
we adopt here, optimizes for each token, the likeli-
hood of the tokens in a window surrounding it. This
training process creates a linear classifier that pre-
dicts words conditioned on the central token’s vec-
tor representation. The classifier and the word vec-
tors are learned simultaneously, but once training is
complete, the classifier is usually discarded, leaving
only the vectors.

These continuous representations project words
into a low-dimensional space. Words that tend to
have similar contexts, and therefore similar syntac-
tic and semantic properties, will tend to be near one
another in this space. We incorporate these represen-
tations into our NER system as real-valued features
of each word xi, as shown in Table 2.

3.2 Data Weighting

Our next tool for domain adaptation is a small pool
of in-domain, annotated data. The easiest way to
make use of this data is to append it to our large pool
of out-of-domain training data, which is what has
been done in previous work on Twitter NER (Rit-
ter et al., 2011; Plank et al., 2014). However, we
have the strong intuition that greater weight should
be placed on the in-domain data.

Assume that for each training pair (x, y) we also
have an importance weight η. In our case, all out-
of-domain pairs will share one value for η, and all
in-domain pairs will share another, higher η. We
modify our PA learner to calculate τ using a ver-
sion of Equation 3 that replaces C with ηC. Unlike
scaling τ directly, scaling C has the desirable prop-
erty of having even high-η examples stop updating
at precisely 0 loss, just as if we had duplicated that
training example η times (Karampatziakis and Lang-
ford, 2010). If we view C as a regularization term,
then this modification can also be interpreted as im-
plementing example-specific regularization. Impor-
tance weights can also be incorporated into CRFs
by modifying their training objective; however, this
is not a standard feature of most CRF packages.

738



Data Lines Types Tokens # PER # LOC # ORG
Fin10 (Train) 1,000 4,865 17,276 192 143 172
Fin10Dev (Test) 1,975 7,734 33,770 325 279 287
Rit11 (Test) 2,394 8,686 46,469 454 377 280
Fro14 (Test) 1,545 5,392 20,666 390 163 200
CoNLL (Train) 14,041 20,752 203,621 6,601 7,142 6,322
Unlabeled Tweets 98M 57M 1,995M – – –

Table 3: Details of our NER-annotated corpora. A line is a tweet in Twitter and a sentence in newswire.

4 Experimental Design

Vital statistics for all of our data sets are shown in
Table 3. For in-domain NER data, we use three col-
lections of annotated tweets: Fin10 was originally
crowd-sourced by Finin et al. (2010), and was man-
ually corrected by Fromreide et al. (2014), while
Rit11 (Ritter et al., 2011) and Fro14 (Fromreide et
al., 2014) were built by expert annotators. We divide
Fin10 temporally into a training set and a develop-
ment set, and we consider Rit11 and Fro14 to be our
test sets. This reflects a plausible training scenario,
with train and dev drawn from the same pool, but
with distinct tests drawn from later in time. These
three data sets were collected and unified by Plank et
al. (2014), who normalized the tags into three entity
classes: person (PER), location (LOC) and organi-
zation (ORG). The source text has also been normal-
ized; notably, all numbers are normalized to NUM-
BER, and all URLs and Twitter @user names have
been normalized to URL and @USER respectively.
In the gold-standard, we choose to reverse a tagging
normalization performed by Plank et al. (2014), who
had post-processed the data so that all @user names
are tagged as PER. These tags are trivial to replicate,
and we found that they inflate scores quite dramat-
ically. Therefore, all @user names are untagged in
both the gold standard and our system outputs.

We use the CoNLL 2003 newswire training set as
a source of out-of-domain NER annotations (Tjong
Kim Sang and De Meulder, 2003). The source text
has been normalized to match the Twitter NER data,
and we have removed the MISC tag from the gold-
standard, leaving PER, LOC and ORG.

Finally, we also use a large corpus of unannotated
tweets, collected from between May 2011 and April
2012. It has been tokenized by the CMU Twok-

enizer,3 but is otherwise unnormalized.

4.1 Hyper-parameter Configuration
Our NER system is trained for 10 epochs with its
regularization parameter C set to 0.01.

We train our word vectors with an in-house im-
plementation of word2vec (Mikolov et al., 2013),
with vector size set to 300, a hierarchical soft-max
objective, down-sampling frequent words at a rate
of 0.001, a window-size of 10 tokens, and a mini-
mum frequency count of 10. When run on our unan-
notated tweets, this produces vector representations
for 2.5M types. We generate a random vector, with
each component sampled from the standard normal,
to use as the representation for any word that did
not occur in our unlabeled data, including begin- and
end-of-sentence markers. We do not scale the vec-
tors before using them as NER features.

We train Brown clusters on the same data us-
ing the implementation by Liang (2005), with 1,000
clusters and a minimum frequency of 10, resulting
in cluster assignments for the same 2.5M types.

5 Results

We evaluate our various NER taggers using the
CoNLL 2003 metrics: phrase-level precision, recall,
and balanced F-measure (F1).

We begin by testing our system on the CoNLL
newswire task, both to confirm that our implemen-
tation is reasonable, and to help situate the Twitter
results that appear later. We train on the unmodi-
fied CoNLL training corpus, and report F1 on the
CoNLL development and test sets. We compare
our baseline to the baseline from Ratinov and Roth
(2009) (RR09), and we compare our representation-
enhanced system (+Reps) to their “All External

3http://www.ark.cs.cmu.edu/TweetNLP/

739



System Dev F1 Test F1
RR09 Baseline 89.2 83.6
Our Baseline 90.4 84.3
RR09 Base + All External 92.5 88.6
Our Base + Reps 91.6 88.0

Table 4: Performance on newswire (CoNLL) data.

System Fin10Dev Rit11 Fro14 Avg
CoNLL 27.3 27.1 29.5 28.0

+ Brown 38.4 39.4 42.5 40.1
+ Vector 40.8 40.4 42.9 41.4
+ Reps 42.4 42.2 46.2 43.6

Fin10 36.7 29.0 30.4 32.0
+ Brown 59.9 53.9 56.3 56.7
+ Vector 61.5 56.4 58.4 58.8
+ Reps 64.0 58.5 60.2 60.9

CoNLL+Fin10 44.7 39.9 44.2 42.9
+ Brown 54.9 52.9 58.5 55.4
+ Vector 58.9 55.2 59.9 58.0
+ Reps 58.9 56.4 61.8 59.0

+ Weights 64.4 59.6 63.3 62.4

Table 5: Impact of our components on Twitter NER per-
formance, as measured by F1, under 3 data scenarios.

Knowledge” system. Both use Brown clusters, but
RR09 uses Wikipedia gazetteers where we use word
vectors. Results are shown in Table 4.

We achieve broadly comparable scores in both
settings. Our external knowledge features are not
as useful as theirs, which may be due to our lack of
Wikipedia gazetteers, or due to a domain mismatch
in our unannotated training data. Their clusters are
trained on the 1996 Reuters corpus, a superset of the
CoNLL data, matching it in both era and domain.
Conversely, our clusters and vectors are both trained
on tweets from 2011, so it is somewhat surprising
that they help to the extent that they do.

5.1 Performance on Twitter

Our primary results are shown in Table 5, where we
compare our word representation and data weighting
techniques under three scenarios: training on out-of-
domain data only (CoNLL), on in-domain data only
(Fin10), and on both. Our data weighting technique
(+Weights, see Section 3.2) only applies when we
use both training sets. We used Fin10Dev to deter-

System Prec Rec F1
CoNLL 43.0 49.8 46.2
Fin10 75.3 50.2 60.2
CoNLL+Fin10 66.0 58.0 61.8

+Weights 73.8 55.4 63.3

Table 6: Precision, recall and F1 on the Fro14 test set
with the Base+All Reps feature set.

mine our importance weights, selecting η = 0.01
for CoNLL and η = 1 for Fin10. For these experi-
ments, we test Brown clusters (+Brown), word vec-
tors (+Vector), and both together (+Reps).

Our Twitter NER results are much lower than the
newswire results from Table 4, with our best Twit-
ter system scoring more than 25 F1 below our best
CoNLL system. But the picture would look much
worse without word representations, which boost
performance in every training scenario. Our best
representation-free system lags nearly 20 F1 behind
our best system that uses representations.

Looking across scenarios, we note that
CoNLL+Reps outperforms CoNLL+Fin10 on 2
out of 3 tests. This is interesting, as it shows that,
given the hypothetical choice between collecting
100 million unannotated tweets for word representa-
tions, and collecting one thousand annotated tweets
for NER training, we are better served by the unan-
notated data. Of course, it is even better to use both;
their combined benefit in CoNLL+Fin10+Reps is
more than additive.

Across all data scenarios and test sets, Brown
clusters help less than word vectors. This contradicts
the observations from Turian et al. (2010), who gen-
erally found Brown clusters to perform best. This
may be because of our domain adaptation scenario,
or it could be due to our use of word2vec, which
did not exist at the time of the Turian study. The
combination of Brown clusters and word vectors is
consistently better than using either alone. The two
representations track different sorts of information:
our use of a large window leads word2vec to build
topic-focused vectors (Turney, 2012), while Brown
clustering is naturally more local, creating very syn-
tactic, part-of-speech-like clusters. It is easy to see
how both types of information can be useful to NER.

Ritter et al. (2011) report that including out-of-

740



���
���
���
���
���
���
���
���
���

� �� ��� ����

��
��
��
��
��

��
��

������������������������������

����
����

������������

Figure 1: F1 averaged over all 3 test sets as we add Fin10
training data to CoNLL.

domain data hurts NER performance. Focusing on
the lines Fin10+Reps and CoNLL+Fin10+Reps, we
see the same problem. In the presence of word rep-
resentations, unweighted CoNLL data hurts perfor-
mance when added to a Fin10 system. Fortunately,
the inclusion of importance weights (+Weights) re-
verses this trend, giving us our best result on each
test. We saw no consistent improvement from im-
portance weights on the representation-free system.

To better understand the benefits of importance
weights, Table 6 reports detailed scores for the
Fro14 test set under the Base+Reps feature set, as
we vary training scenarios. Results on the other test
sets are similar. The Fin10 system achieves high pre-
cision but low recall, while the CoNLL+Fin10 does
the opposite. This is because the CoNLL data is
much more entity-dense than the Twitter data, which
biases systems trained on CoNLL to return too many
entities. By down-weighting the CoNLL data, we
reduce this bias and gain 6.8 points of precision at
the cost of only 2.6 points of recall.

Figure 1 gives learning curves as we add Fin10
data to CoNLL across several feature sets. There is a
steady improvement for all systems as the in-domain
data grows, and importance weighting increases the
impact of in-domain data even at very low quanti-
ties. Though the curves shows no sign of flattening
out, note that the x-axis is log-scaled.

We have access to roughly 100 million tweets for
unsupervised representation learning. Figure 2 pro-
vides a learning curve for our Reps+Weights system
as we increase the percentage of unlabeled tweets
used to train both Brown clusters and word vec-
tors from 1.5% to 100% of that data, doubling the

���
�����
���

�����
���

�����
���

�����
���

�����

��� � � ���� �� �� ���

��
��
��
��
��

��
��

������������������������������������
Figure 2: F1 averaged over all 3 test sets as we increase
the percentage of tweets used to build representations.

Test Set PER LOC ORG
Fin10Dev 71.3 72.4 48.8
Rit11 70.8 61.9 36.9
Fro14 69.4 70.2 42.6

Table 7: F1 for our All Data+Reps+Weights system, or-
ganized by entity class.

amount with each step. With only 1.5 million tweets,
average performance is already very good, and we
can see that the benefits of scale are starting to level
off after we clear 12.5 million.

Table 7 reports our best system’s performance by
entity class. For all three test sets, ORG is the most
difficult. ORG is perhaps the broadest entity class,
but we suspect it is also the most likely to be anno-
tated inconsistently, as it is rife with subtle distinc-
tions: bands (ORG) versus musicians (PER); com-
panies (ORG) versus their products (O); and teams
(ORG) versus their home cities (LOC). During an
inspection of 25 incorrect ORG predictions by our
best system, drawn from a test on Fin10Dev, we
found 10 cases where the gold standard was ques-
tionable. Two of these incorrectly placed “the”
inside a chunk, (“[the Mariners]” is wrong; “the
[Mariners]” is right), while the remaining 8 involved
company-product distinctions, which are tricky even
for human annotators. The NER task is not always
as intuitive as we would like, and organizations tend
to highlight these difficulties.

5.2 Comparison with Linguistic Resources

Thus far, we have restricted ourselves to a setting
without access to linguistic resources, but for some

741



Base +X Reps +X
∅ 42.9 62.4
[P]OS Tags 47.1 63.0
[G]azetteers 52.8 63.2
[P]+[G] 55.6 63.5

Table 8: Adding linguistic resources to our baseline
and representation-enabled systems, as measured by F1
averaged over 3 test sets. All systems are trained on
CoNLL+Fin10, and all but Base+∅ use data weighting.

languages, such as English, rich resources exist and
can be very useful. We now examine how word
representations compare and interact with gazetteers
and POS taggers.

For gazetteers, we use those included with the Illi-
nois NER system (Ratinov and Roth, 2009), gen-
erating features that indicate when a word appears
as part of a phrase found in a gazetteer. For POS
tags, we use the CMU Twitter Tagger (Owoputi et
al., 2013), and generate POS tag indicators for the
current word and for tags within a 2-word window.
For some of our corpora, notably CoNLL and Rit11,
the corpus tokenization did not match the POS tag-
ger’s tokenization. We resolve mismatches by al-
lowing the POS tagger to further tokenize the input
sentence to better match its assumptions. After POS
tagging, we merge any split tokens back to the orig-
inal tokenization, picking a representative tag from
among merged tags according to a priority list (verb
> noun > adjective, etc.). The POS tags may have
performed better if we had used the tagger’s native
tokenization throughout.4

Results of our comparison are shown in Table 8.
Comparing Base+∅ and Base+[P]+[G], we see
that linguistic resources boost the baseline’s perfor-
mance considerably. Turning to Reps+[P]+[G], we
see that adding word representations to linguistic re-
sources provides another substantial boost of 7.9 F1.
Conversely, adding linguistic resources to a system
that already has representations increases F1 by only
1.1 points, indicating that not much new information
is being added. The per-feature analysis indicates
that much of this boost comes from the gazetteers.

4Inconsistent tokenization also hinders the word representa-
tions, which were constructed from a corpus tokenized by the
CMU Twokenizer.

System Rit11 Fro14
PHMS14 Baseline 77.4 82.1
PHMS14 Dict ≺Web 78.5 83.9
All Data+Reps+Weights 82.3 86.4
All Data+Ling+Reps+Weights 82.6 86.9

Table 9: Comparison with the state-of-the-art, reporting
test F1. Both the gold-standard and the system outputs
have @user names deterministically tagged as PER.

5.3 Comparison with the State-of-the-Art

In Table 9, we compare our best system, including
linguistic resources, to the state-of-the-art results re-
ported by Plank et al. (2014).5 In order to create
a fair comparison, we post-process both our system
output and the gold-standard to tag all @user names
as PER, just as they do.

Like our system, their baseline includes CoNLL
and Twitter data, and uses Brown clusters trained
on a comparable number of unlabeled tweets. Their
strongest system uses distant supervision over linked
web-pages to create artificial training data. But we
are able to outperform it with our vector representa-
tions and importance weights. Note that this com-
parison is not perfect, as they train on a much larger
pool of crowd-sourced, NER-annotated tweets, con-
sisting of 170k tokens compared to our 17k. The
size of their training data is balanced by the fact that
its annotations were automatically correctly using
MACE (Hovy et al., 2013), where ours were cor-
rected manually, making it unclear which group has
the advantage. Nonetheless, our results establish a
new state-of-the-art for both test sets, and they do so
using only 1k annotated tweets.

6 Analysis

We inspected 100 tweets from the Rit11 test set,
focusing on the output from our primary system,
Base+Reps+Weights, and our baseline, Base, both
trained on the CoNLL+Fin10 data. We noted cases
where the primary system improved upon the base-
line, and cases where it failed to achieve the gold-
standard, and placed the phenomena we observed
into bins. In general, the baseline was observed

5We omit the Fin10 test set from this comparison, as Plank
et al. (2014) test on the entirety of Fin10, while we have divided
it into training and development sets.

742



(a) RT @USER : Christmas:PER was so much better when there was a santa :( #allteensthings
RT @USER : Christmas was so much better when there was a santa :( #allteensthings

(b) Lmao . I have a feeling Imma:ORG get yelled at tomorrow . Big time . XD Ehh oh well
Lmao . I have a feeling Imma get yelled at tomorrow . Big time . XD Ehh oh well

(c) I pray an give God glory even when im in pain , hurting , or crying .
I pray an give God:PER glory even when im in pain , hurting , or crying .

(d) Anyone know what days/times that you can smoke hookah at the mix ( cma center ) in corbin:PER .
Anyone know what days/times that you can smoke hookah at the mix ( cma center ) in corbin:LOC .

Figure 3: (a,b): examples where the baseline (top) is improved by our final system (bottom)
(c,d): examples where our final system (top) falls short of the gold-standard (bottom)

to rely heavily on local context and capitalization,
while the primary system has a much stronger global
prior on a given type’s entity assignment.

Reps+Weights improved the baseline in 54 out of
100 tweets. There were 31 cases where the primary
system corrected a baseline error caused by a mis-
leading capitalization cue. Some of these, such as
Figure 3(a) are patched by world knowledge pro-
vided by word representations, but many simply re-
flect a reduced reliance on capitalization. We were
surprised to find only 11 cases where Twitter’s infor-
mal language led to an error, often due to a vaguely
name-shaped colloquialism, such as in 3(b). 6 of
these 11 cases were fixed by the primary system.

Reps+Weights fell short of the gold-standard in
62 of 100 tweets. We observed 39 recall errors that
were difficult to divide into smaller bins. These
entities were often missed despite clear capitaliza-
tion cues, as in Figure 3(c). This particular exam-
ple is actually a symptom of inconsistent annota-
tion: CoNLL and Rit11 consistently annotate God as
a person, while our Fin10 training data leaves God
untagged. The next largest class of errors consists
of 11 problems caused by uniform casing (all caps
or all lowercase). We also have 5 remaining errors
due to informal language, which are interesting, as
they highlight gaps in our representations. These in-
clude cases where the system generates false entities
for variants of rare words (Tidying→ Tidyin), or un-
usual lengthenings (Yayaayayay, as opposed to the
well-attested Yayayayay). We also saw cases where
entities were missed due to creative punctuation (Go
V-I-K-I-N-G-S!). Finally, we found 4 cases where
the system actually over-relies on its word represen-

tations, such as in 3(d), where the global PER in-
terpretation of corbin overrides a fairly strong LOC
signal provided by the local context word in.

7 Discussion

We have shown that the combination of Brown clus-
ters, word vectors, and a simple data weighting
scheme is sufficient to establish a new state-of-the-
art on two Twitter NER test sets, using only 1,000
annotated tweets. We have designed our experi-
ments to emphasize the dramatic impact of word
representations in this domain, and to clarify the ef-
fects of in- and out-of-domain training sets.

Word representations learned on a large, unla-
beled Twitter corpus have addressed a surprising
number of issues with inconsistent capitalization
and informal language. However, our continuing
problems with uncased tweets and unusual colloqui-
alisms demonstrate that there are still many human-
readable words that remain a mystery to our sys-
tem. In response to these observations, we would
like to investigate more flexible representations, per-
haps similar to those of Botha and Blunsom (2014),
who use a linear combination of morpheme vectors
to create representations that can generalize across
words with similar forms.

Acknowledgments

Thanks to the anonymous reviewers for their helpful
comments, to Alan Ritter for providing us with our
corpus of unlabeled tweets, and to Barbara Plank for
providing us with our three labeled corpora.

743



References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155.

Phil Blunsom and Trevor Cohn. 2011. A hierarchi-
cal pitman-yor process hmm for unsupervised part of
speech induction. In ACL, pages 865–874, Portland,
Oregon, USA, June.

Jan A. Botha and Phil Blunsom. 2014. Compositional
Morphology for Word Representations and Language
Modelling. In ICML, Beijing, China.

Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In EMNLP, pages 724–731.

M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In EMNLP.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research.

Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel
Martin, and Xiaodan Zhu. 2011. Machine-learned
solutions for three stages of clinical information ex-
traction: the state of the art at i2b2 2010. Jour-
nal of the American Medical Informatics Association,
18(5):557–562.

Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk, pages 80–88.

Hege Fromreide, Dirk Hovy, and Anders Søgaard. 2014.
Crowdsourcing and annotating NER for Twitter #drift.
In LREC, pages 2544–2547, Reykjavik, Iceland.

Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with mace. In HLT-NAACL, pages 1120–1130, At-
lanta, Georgia, June.

Nikos Karampatziakis and John Langford. 2010. On-
line importance weight aware updates. arXiv preprint
arXiv:1011.1576.

Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Ph.D. thesis, Massachusetts Institute of
Technology.

Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the Joint

Conference of the ACL and the AFNLP, pages 1030–
1038, Singapore, August.

Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets. In
ACL, pages 359–367, Portland, Oregon, USA, June.

André F. T. Martins, Kevin Gimpel, Noah A. Smith,
Eric P. Xing, Pedro M. Q. Aguiar, and Mário A. T.
Figueiredo. 2010. Learning structured classifiers with
dual coordinate descent. Technical Report CMU-ML-
10-109, Carnegie Mellon University.

Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In CoNLL, pages 188–191. Association for Computa-
tional Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.

Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In HLT-NAACL, pages 337–342.

David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30(1):3–26.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In HLT-NAACL, pages
380–390.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In CoNLL, pages 78–86.

Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders
Søgaard. 2014. Adapting taggers to Twitter with not-
so-distant supervision. In COLING, pages 1783–1792,
Dublin, Ireland.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147–155.

Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In ACL, pages 1375–1384.

Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP, pages 133–
142.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An exper-
imental study. In EMNLP, pages 1524–1534, Edin-
burgh, Scotland, UK.

Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open
domain event extraction from twitter. In KDD, pages
1104–1112.

744



Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS, pages 1185–1192.

Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL,
pages 142–147.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL, pages 384–394.

Peter D Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, pages 533–
585.

745


