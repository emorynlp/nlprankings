















































CLPsych2019 Shared Task: Predicting Suicide Risk Level from Reddit Posts on Multiple Forums


Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 162‚Äì166
Minneapolis, Minnesota, June 6, 2019. c¬©2019 Association for Computational Linguistics

162

CLPsych2019 Shared Task: Predicting Users‚Äô Suicide Risk Levels from Their Reddit Posts 
on Multiple Forums 

Victor Ruiza*, Lingyun Shia*, Jorge Guerraa,b, Wei Quana,c,  Neal Ryand, Candice Biernesserd,  
David Brentd, and Fuchiang Tsuia-c 

aTsui Laboratory, Children‚Äôs Hospital of Philadelphia, Philadelphia, PA, USA, 
bInstitute for Biomedical Informatics, University of Pennsylvania, Philadelphia, PA, USA,  

cDrexel University, Philadelphia, PA, USA, 
dDepartment of Psychiatry, University of Pittsburgh, Pittsburgh, PA, USA 

*: Authors contributed equally 
 

Abstract 
We aimed to predict an individual suicide risk level from longi-
tudinal posts on Reddit discussion forums. Through participat-
ing in a shared task competition hosted by CLPsych2019, we 
received two annotated datasets: a training dataset with 496 
users (31,553 posts) and a test dataset with 125 users (9610 
posts). We submitted results from our three best-performing 
machine-learning models: SVM, Na√Øve Bayes, and an ensemble 
model. Each model provided a user‚Äôs suicide risk level in four 
categories, i.e., no risk, low risk, moderate risk, and severe risk. 
Among the three models, the ensemble model had the best 
macro-averaged F1 score 0.379 when tested on the holdout test 
dataset. The NB model had the best performance in two addi-
tional binary-classification tasks, i.e., no risk vs. flagged risk 
(any risk level other than no risk) with F1 score 0.836 and no 
or low risk vs. urgent risk (moderate or severe risk) with F1 
score 0.736. We conclude that the NB model may serve as a tool 
for identifying users with flagged or urgent suicide risk based 
on longitudinal posts on Reddit discussion forums. 

Keywords: suicide, Reddit, machine learning, predictive mod-
eling 

I. Introduction  
Suicide poses a challenge to our society. It is the 10th 
leading cause of death in the United States for all ages, 
and most importantly it is the second leading cause of 
death for 64 millions of youths between the ages of 10 and 
24.(NIMH, 2018)(Howden and Meyer, 2011)  
Meanwhile, the use of social media among the young 
population is getting more poupular.  

Social media websites such as Reddit discussion forums 
serve as a common platform for people to express their 
thoughts, and many people feel more comfortable dis-
cussing or sharing their mental state including suicidal 
thoughts on social media than they are in person. Moreo-
ver, people who can get access to the internet may not 
have adequate resources for mental health care. In con-
trast to the electronic health records that recorded the in-
teractions between patients and clinical care providers, 
on-line social media posts illustrate conversations be-
tween a user and an online audience mostly comprised of 
non-clinicians. In March 2019, Reddit was estimated to 
have 542 million monthly visitors and 234 million unique 
users, 53.9% of which with bases in the United 
States.(Wikipedia,  ) There is a need to study potential 

suicide risks based on social media posts as a part of pub-
lic health surveillance.(De Choudhury et al., 2017) 

Current state-of-art approaches for mental health condi-
tion prediction leveraged machine learning (ML) and nat-
ural language processing (NLP). Common ML algorithms 
include support vector machines, Na√Øve Bayes, etc. NLP 
techniques include part of speech, bag-of-words model-
ing, word embeddings, etc. The performance of those 
models measured by micro-averaged F1 score ranged be-
tween 0.4 and 0.76,(Calvo et al., 2017) and by macro-av-
eraged  F1 score ranged between 0.5 and 0.84.(Shing et 
al., 2018) A macro-averaged score computes the metric 
independently for each risk level (class) and then takes 
the average across all levels regardless of the number of 
samples in each risk-level group, whereas micro-average 
treats each post equally regardless of class. Thus, a 
macro-averaged score carries more per-post weight for 
those risk levels (categories) with fewer posts.      

In this study, we hypothesized that we can develop ad-
vanced data-driven predictive models that can predict in-
dividual suicide risk level from longitudinal posts on Red-
dit discussion forums. 

Our study has three key contributions. First, we devel-
oped 10 feature domains based on NLP and feature engi-
neering, described in Section II.2, including clinical find-
ings and semantic role labeling (those were not com-
monly included in previous shared tasks competition for 
social media data(Shing et al., 2018)) for the prediction of 
suicide risk from Reddit posts. Second, we developed 
several state-of-the-art machine learning models includ-
ing deep neural network models for the prediction task. 
Third, we developed a modeling strategy for improving 
prediction accuracy. 

II. Methods 
This section describes study datasets, text preprocessing, 
feature engineering, predictive modeling, and evaluation 
metrics. 

II.1 Datasets  

We received two datasets from the CLPsych2019(Zirikly 
et al., 2019): 1) a training dataset and 2) a test dataset.  
Both datasets comprised annotated posts on the Reddit 
discussion form and its sub-discussion forms, also known 



163

as subreddits. The training dataset study period is be-
tween 2005 and 2015, comprising 31,553 posts from 496 
Reddit users with the cohort definition: a user had at least 
one post on the SuicideWatch subreddit; users who posted 
on the SuicideWatch may not be of risk to suicide. The 
data elements in the training dataset included a user id, a 
subreddit name, a post title and body from the user‚Äôs posts 
in any subreddit, and post timestamp in a unified time 
zone. The CLPsych2019 organization provided the gold 
standard for the training dataset.(Shing et al., 2018; 
Zirikly et al., 2019)  Following the same cohort definition, 
the test dataset comprised 9,610 posts from 125 Reddit 
users. We received the training and test datasets one 
month and five days before the competition deadline, re-
spectively.   

The study is approved under the Children‚Äôs Hospital of 
Philadelphia IRB. 

II.2 Natural Language processing and Feature Engi-
neering 

II.2.1. Text preprocessing  

We performed a series of preprocessing pipeline includ-
ing sentence splitting, tokenization, removal of stop 
words, part of speech tagging, and lemmatization.(Posada 
et al., 2017) 

II.2.2 Feature domains from users‚Äô posts 

Similar to the work by Shing et. al.(Shing et al., 2018), 
we developed the following feature domains: 

Clinical findings: A social media post may contain clini-
cal findings such as depression, schizophrenia, cancer, 
etc. We utilized the clinical Text Analysis and Knowledge 
Extraction System (cTakes)(Savova et al., 2010) devel-
oped by the Mayo clinic, to extract clinical findings from 
each post. cTAKES extracts each finding with a Concept 
Unique Identifier (CUI) represented in the standard Uni-
fied Medical Language System (UMLS) developed by the 
National Library of Medicine (NLM). We also flagged 
suicide attempt related CUIs (SA CUIs) using a pre-de-
fined CUI list from our previous suicide attempt study 
with electronic health records (EHR).(Tsui et al., 2019) 

Social determinants of health (SDOH): We classified 
each sentence into one or more of the 11 social categories 
that we previously developed.(Quan et al., 2019; Liu et 
al., 2019)  The 11 categories included: 1) social environ-
ment, 2) education, 3) occupation, 4) housing, 5) eco-
nomic, 6) health care, 7) interaction with legal system, 8) 
social support circumstances and social network, 9) trans-
portation, 10) spirituality and 11) other (e.g., exposure to 
disaster, war, other hostilities, and access to weapons, 
etc.). 

Emotion and health-disorder association: We identified 
posts‚Äô lemmas that matched terms in the Word-Emotion 
Association Lexicon developed by Mohammad et. 
al.(Mohammad and Turney, 2013), as well as a lexicon 
compiled from terms available in the list of psychological 
disorders(,  ). We identified words in a post associated 
with emotion categories, e.g., joy, sadness, fear, etc. 

Readability score: Readability score provides a gauge for 
the level of understanding of a document. We used spaCy 
library to calculate 7 readability scores for each post: (1) 
automated readability index, (2) Coleman-Liau index, (3) 
Dale-Chall index, (4) Flesch-Kincaid grade level, (5) 
Flesch-Kincaid reading ease index, (6) forecast index and 
(7) smog index. 

Semantic role labeling (SRL): SRL is a linguistic process 
that identifies semantic roles, e.g., subject, object and 
verb, of a sentence. We used two latest state-of-the-art 
statistical SRL models: Bidirectional Long Short-Term 
Memory  (BiLSTM) model(He et al., 2017) and the Em-
beddings from Language Models (ELMo)(Peters et al., 
2018), which provides deep contextualized words repre-
sentations, to identify the semantic role labels and predi-
cate-argument structure from each sentence in a user‚Äôs 
post. The identified predicate-argument information indi-
cates detailed semantic structure and roles, i.e., ‚Äúwho‚Äù did 
‚Äúwhat‚Äù to ‚Äúwhom‚Äù at ‚Äúwhere‚Äù and ‚Äúwhen‚Äù. Table 1 shows 
an example. SRL plays a critical role for revealing self-
referential thinking. 

Table 1. Semantics analysis of a sample sentence from a 
Reddit forum. The right column in the table demonstrates 
the identified argument labels (subject and object labels), 
predicate and negation labels from the sentence on the left 
column after applying SRL process; the arg0 tag, the arg1 
tag, and argm-negation tag represent the subject ‚ÄúI‚Äù, the 
object ‚Äúthe loneliness and pain‚Äù, and the sentence nega-
tion, respectively.  
Sentence in a post Predicate-argument structure 

‚ÄúI can't handle 
the loneliness and 
pain anymore.‚Äù 

"arg0": " I", 
"argm-mod": " ca", 
"argm-negation": " n't", 
"predicate": " handle", 
"arg1": "the loneliness and pain" 

Sentiment levels: A sentiment level provides a gauge for 
the level of sentiment of a sentence. We used Stanford 
CoreNLP(Manning et al., 2014) to identify 5 sentiments: 
‚ÄúVery Negative", "Negative", "Neutral", "Positive", "Very 
Positive‚Äù for each post. To create the features, per user, 
we calculated the following averages: 1) micro average: 
the sum of all sentiments across all the post of a user di-
vided it by the total number of sentences across those post 
per that user; 2) macro average: the sum of each post level 
sentiment vector of a user divided by the total number of 
post by that use; 3) post-level vector: the sum of all senti-
ment vectors in a post divided by the total number of the 
sentence in that post. 

Topic modeling: Topic modeling provides an unsuper-
vised-based learning to map each post into a predefined 
number of topics. We used the unsupervised learning La-
tent Dirichlet Allocation (LDA) to identify 10, 20 and 30 
topics from all the posts.  

Empathy topics: We used Empathy text analysis tool to 
identify 196 pre-defined topics(Fast et al., 2016) from 
each of the posts, e.g., death, negative emotion, sadness, 
etc. Each post has an empathy vector, Ei196x1, where i rep-
resents a post, and each topic, ei,j ‚àà ùëç, [0,100]. 



164

Doc2Vec model: We built a Doc2Vec model via distrib-
uted bag of words (DBOW) based on the training Reddit 
posts, and represented each Reddit post as a 300x1vector. 

Aggregate Statistics (AS): We created summary statistics 
features that characterize users‚Äô posting habits. Table 2 
summarizes the list. 

Table 2. Aggregate statistics based on feature domains 
Feature 
Domain 

Statistics at the post and user levels 

Clinical 
Finding 

‚Ä¢ Individual CUI counts from all posts 
‚Ä¢ Average count of each CUI per post 
‚Ä¢ Average count of each CUI per CUI-post (CUI-

post refers to the post with at least one identi-
fied CUI) 

‚Ä¢ Total count of distinct CUIs from all posts 
‚Ä¢ Total count of SA CUIs per user 
‚Ä¢ Total count of SA CUI-posts per user (SA CUI-

post refers to the post with at least one identi-
fied SA CUI) 

‚Ä¢ Total count of distinct SA CUIs per user 
Semantic 
Role La-
beling 
(SRL) 

‚Ä¢ Average count of each arg0 and arg1 per post 
‚Ä¢ Minimum/Maximum counts of each arg0 and 

arg1 in one post 
‚Ä¢ Average count of ‚Äúnegative‚Äù-arg0 per post (An 

‚Äúnegative‚Äù-arg0 refers to the arg0 with an 
argm-negation modifier for the predicate as 
shown in Table 1) 

‚Ä¢ Minimum/Maximum count of each ‚Äúnegative‚Äù-
Arg0 in one post 

‚Ä¢ Count of distinct arg0 and distinct arg1 values 
per user 

‚Ä¢ Minimum/Maximum count of distinct arg0 and 
arg1 values in one post 

‚Ä¢ Average number of part-of-speech tags (nouns, 
verbs, adjectives, adverbs, etc.) in the last two 
years 

SDOH ‚Ä¢ Total number and percentage of sentences in 
each social determinants of health category 

Forum 
Posting 
Behavior 
(FPB) 

‚Ä¢ Number of total posts for the user in all subred-
dits 

‚Ä¢ Number of total posts for the user in in the last 
two years 

‚Ä¢ Number of weeks with posts to the Sui-
cideWatch subreddit 

‚Ä¢ Number of active days between the first and 
last posts 

‚Ä¢ Average post time difference between 2am 
(EST) and the post time in the last two years 

‚Ä¢ Average length (characters) of posts in the last 
two years 

‚Ä¢ Days since last post to the SuicideWatch sub-
reddit 

‚Ä¢ proportion of the user‚Äôs posts containing the 
word ‚Äòedit‚Äô in the last two years 

‚Ä¢ Proportion of posts made between 2a and 6m 
EST 

‚Ä¢ proportion of posts made during weekends 
(Saturday and Sunday) in the last two years 

‚Ä¢ Maximum number of consecutive weeks in 
which users‚Äô made posts to SuicideWatch in the 
last two years. 

‚Ä¢ All subreddits that the user posted to in the last 
two years 

‚Ä¢ Number of posts to SuicideWatch by week in 
the last two years (1x104 vector) 

‚Ä¢ Number of posts made by users to Sui-
cideWatch in the last two years. 

Senti-
ment 

‚Ä¢ Proportions of sentiment score at post and sen-
tence levels 

Readabil-
ity 

‚Ä¢ Averages of 7 readability at post and sentence 
levels 

Emotion ‚Ä¢ Average count of each emotion-related term 
across all posts 

Topic 
modeling 

‚Ä¢ Average count of each topic across all posts 

II.3 Predictive modeling and evaluation 

We developed seven machine learning models: Na√Øve 
Bayes (NB), gradient boosting (GB), random forest (RF), 
support vector machine (SVM), and deep neural networks 
including augmented convolution neural networks (CNN) 
and long short-term memory neural networks (LSTM). 
Unlike conventional deep neural networks, we developed 
augmented deep neural networks included input not only 
from freetext posts (Doc2Vec) but also the user-level ag-
gregate statistics defined in Section II.2. 

 
Figure 1. Experimental design and modeling process. 

Figure 1 outlines our modeling and evaluation process. 
We trained and optimized models in a nested 5-fold cross-
validation approach, and each model was optimized based 
on macro-averaged F1 score, which is the main measure 
for ranking models developed by the shared-task partici-
pants. First, we oversampled the sparsely-represented 
classes to alleviate the existing class imbalance.(Chawla 
et al., 2002) Then, we conducted imputed missing values 
with variable means, and either did not scale variables, or 
scaled values ‚àà ùëÖ to [0, 1]. Then we performed a two-
phase feature selection process. First, we applied a corre-
lation-based feature-selection filter(Hall, 1999), and then 
conducted a forward greedy search over an increasing 
number of features selected based on information gain 
feature ranking.(Tsui et al., 2017)  

For the competition, each team was limited to submit up 
to three models‚Äô results, we chose top two models and 
added an ensemble model based on our three best-per-
forming models. We used the 5-fold average of macro-
averaged F1 scores to evaluate each model. The models 
used to submit results to the competition were re-trained 
with the full training dataset following the same approach 
used during cross-validation. 

We created an ensemble classifier from our best 3 
performing models. Predictions from these models were 



165

used to tally votes and generate the final predictions of the 
ensemble classifier. Since there were more risk categories 
(4) than the number of classifiers (3) in the ensemble, it is 
possible that all models produce different predictions. In 
this scenario, we created a rule by favoring the classes that 
were likely to be misclassified. 

Besides macro-averaged F1, our evaluation metrics in-
clude macro-averaged accuracy, precision and recall. We 
further compared the performance based on binary classi-
fications, i.e., flagged risk (low, moderate, and severe 
risks) vs. no risk, and urgent risk (moderate, and severe 
risks) vs. others. 

III. Results  
Table 3.  Risk level distributions in two datasets. 

Table 3 shows the distributions of users in 4 different 
risk categories in the training and test datasets. Both 
datasets have low counts in the low risk level and 
share almost the same distribution. 
Table 4. Average 5-fold predictive model performance 
from the training dataset, measured by the macro-av-
eraged F1 score followed by the number of variables 
(features) used by a model in parentheses. 

 NB  GB RF SVM CNN LSTM 
Marco-
F1 score 

0.422 0.412 0.395 0.432 0.367 0.147 

# of var-
iables 

75 100 100 100 796 796 

 

  

Figure 2. Confusion matrix of the NB model (left) and 
the SVM model (right). 

 
Figure 3. Confusion matrix of the ensemble model. 

Our macro-averaged F1 scores from the training dataset 
ranged from 0.147 to 0.43. Table 4 summarizes the 

performance of the 6 models. SVM and NB had best F1 
scores. Based on these results, we applied three models to 
the test dataset: SVM, NB, and an ensemble model built 
from the top three models: NB, SVM, and GB. The rule 
for breaking the tie in the ensemble model was to set the 
order of the preference: B (highest), C, A, then D (lowest).  

Figures 2-3 show the confusion matrices of the 3 models, 
and Table 5 summarizes the performance of the three 
models submitted to the competition. These models‚Äô  
macro-averaged F1 scores on the holdout test dataset 
ranged from 0.338 to 0.379. The ensemle model had the 
best macro-F1 score 0.379, which was ranked 3rd among 
the particpating teams for this shared task 
competition.(Zirikly et al., 2019)  
Table 5. Model performance from the test dataset. 
Level A-D represent no risk, low risk, moderate risk, 
and severe risk, respectively. 

Table 6.  Top 10 features from the feature space 
Rank Domain Feature Description 
1 SRL Max. count of arg1 with value ‚ÄòI‚Äô in one post 
2 SRL Max. count of arg1 with value ‚Äòme‚Äô in one 

post 
3 FPB Number of posts to SuicideWatch in the last 

two years  
4 FPB Number of weeks with any SuicideWatch 

posts in the last two years 
5 SRL Max. count of arg1 with value ‚Äòmyself‚Äô in one 

post 
6 Empathy Max. value of negative emotion in a post 
7 SRL Average count of arg1 with value ‚ÄòI‚Äô 
8 Emotion Average count of 'disgust'-related terms 

across all posts 
9 Empathy Max. value of ‚Äòdeath‚Äô topic across all posts 
10 FPB Max. number of SuicideWatch posts in any 

week in the last two years 

The NB model had the best performance in two additional 
binary-classification tasks, i.e., no risk vs. flagged risk 
(any risk level other than no risk) with F1 score 0.836 and 
no or low risk vs. urgent risk (moderate or severe risk) 
with F1 score 0.736.  
We started modeling from a total of 7,603 features from 
10 feature domains in Section II.2, and Table 6 lists top 
10 features from the whole training dataset ranked in the 
order of information gain. Among the top 100 features, 
there were 35 clinical finding features, 25 Empathy fea-
tures, 17 SRL features, 14 user post-pattern features from 
forum posting behavior (FPB), 6 Readability features, and 

 Training Dataset Test Dataset 
No risk 127 (25.6%) 32 (25.6%) 
low risk 50 (10.08%) 13 (10.4%) 
moderate risk 113 (22.78%) 28 (22.4%) 
Severe risk 206 (41.53%) 52 (41.6%) 
Number Subred-
dits covered 

3662 1593 

 NB  SVM Ensemble 
Marco-F1 score  
(4 risk levels) 

0.338 0.370 0.379 

Accuracy 0.352 0.408 0.392 
F1 score  
(Flagged vs. no risk) 

0.836 0.789 0.818 

F1 score (Urgent vs. 
non-Urgent) 

0.736 0.603 0.648 

Level-A  
Precision/Recall/F1 

0.471/0.250/ 
0.327 

0.442/0.594/ 
0.507 

0.486/0.562/ 
0.522 

Level-B  
Precision/Recall/F1 

0.286/0.308/ 
0.296 

0.154/0.308/ 
0.205 

0.217/0.385/ 
0.278 

Level-C  
Precision/Recall/F1 

0.260/0.714/ 
0.381 

0.280/0.250/ 
0.264 

0.286/0.429/ 
0.343 

Level-D  
Precision/Recall/F1 

0.706/0.231/ 
0.348 

0.677/0.404/ 
0.506 

0.609/0.269/ 
0.373 



166

3 Emotion features. Among 17 SRL features, 6 of them 
were related to self-referencing. 

IV. Discussion and Limitations 
In this study, we developed a wealth of structured features 
from longitudinal freetext posts, built 6 state-of-the-art 
machine learning models, and tested 3 models in a test 
dataset from the CLPsych2019 organizers. We demon-
strated that data-driven machine learning models identi-
fied users with risk of suicide based on their Reddit posts. 
The SVM model had best macro-averaged F1 score for 
classifying 4 categories of suicide risk, which could be 
attributed by its hyperspace parameters and nonlinearity; 
the NB model had accurate macro-averaged F1 scores for 
classifying binary groups: flagged vs. no risk, and urgent 
risk vs. non-urgent risk groups. The NB performance may 
be attributed by its simple assumption and a relatively 
smaller number (75) of variables compared with others.  

Based on the top 100 features used by the SVM model, 
we found that SRL, Empathy, Readability, Clinical find-
ings, and user post patterns identified in FPB were im-
portant for classification. Most importantly, our top find-
ings revealed that frequent self-referencing like ‚ÄòI‚Äô, ‚Äòme‚Äô, 
and myself‚Äô (ranked 1, 2, 5, 7, 19) and negated self-refer-
encing (ranked 35) posed an elevated risk as illustrated in 
literatures.(Burke et al., 2017; Quevedo et al., 2016) 

On the other hand, LDA topic modeling, sentiment anal-
ysis, and social determinants of health did not play critical 
roles for classification in our experiments.  We attributed 
its low impact due to the variety of subreddits in the co-
hort, which possibly makes it challenge to effectively 
group certain topics for classifying suicide risk levels.  
Our sentiment tool was based on the context of movie re-
views, which may not be applicable to the suicide predic-
tion task from Reddit posts. For social determinants of 
health, we built the model based on clinical data, which 
may be limited for social media data. 

The oversampling strategy for model training improved 
predictive performance. Our conjecture is that over-
sampling enables a classifier to better tune its parameters 
for those rare occurrences.  

The deep neural networks (CNN and LSTM) did not per-
form well. Both DNNs employed all the features identi-
fied in the feature engineering section. The potential ex-
planation is that there were limited number of users in low 
and moderate risk levels and there were many input vari-
ables. Another factor we may consider in the future is the 
development of more complicated DNN structure and/or 
the use of multiple DNNs to catch the temporal, wide va-
riety of feature space, and system non-linearity.   

V. Conclusions  
In this study, the ensemble model had best macro-aver-
aged F1 score, and Na√Øve Bayes performed best for iden-
tifying users with flagged or urgent suicide risk based on 
longitudinal posts on Reddit discussion forums in con-
junction with features from clinical findings, empathy 
categories, semantic role labeling, user post-patterns, 
readability, and emotion. 

Correspondence: Fuchiang (Rich) Tsui: tsuif@chop.edu  
References 

Taylor A Burke, Samantha L Connolly, Jessica L Hamilton, Jonathan P Stange, 
Y Lyn, and Lauren B Alloy. 2017. Two Year Longitudinal Study in 
Adolescence. , 44(6):1145‚Äì1160. 

Rafael A. Calvo, David N. Milne, M. Sazzad Hussain, and Helen Christensen. 
2017. Natural language processing in mental health applications using non-
clinical texts. Natural Language Engineering. 

Nitesh V Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip 
Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique. 
Journal of Artificial Intelligence Research, 16:321‚Äì357. 

Munmun De Choudhury, Emre Kiciman, Mark Dredze, Glen Coppersmith, and 
Mrinal Kumar. 2017. Discovering shifts to suicide ideation from mental health 
content in social media. In Proc SIGCHI Conf Hum Factor Comput Syst., pages 
2098‚Äì2110. 

Ethan Fast, Binbin Chen, and Michael Bernstein. 2016. Empath: Understanding 
Topic Signals in Large-Scale Text. Technical report. 

Ma Hall. 1999. Correlation-based feature selection for machine learning. Diss. 
The University of Waikato. 

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2017. Deep 
Semantic Role Labeling: What Works and What‚Äôs Next. In  

Lindsay M Howden and Julie A Meyer. 2011. 2010 Census Briefs; Age and sex 
composition: 2010. Technical Report May. 

Haixia Liu, Lingyun Shi, Neal Ryan, David Brent, and Fuchiang Rich Tsui. 
2019. Developing an annotation guideline for the classification of social 
determinants of health from electronic healthrRecords. Technical report. 

Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven 
Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language 
Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association 
for Computational Linguistics: System Demonstrations, pages 55‚Äì60. 

Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-
emotion association lexicon. In Computational Intelligence. 

NIMH. 2018. Suicide. 

Matthew E. Peters, Mark Neumann, Mohit Iyyer, and Matt Gardner. 2018. 
Deep contextualized word representations. In Proceedings of NAACL-HLT 
2018, pages 2227‚Äì2237. 

Jose D. Posada, Amie J. Barda, Lingyun Shi, Diyang Xue, Victor Ruiz, Pei-
Han Kuan, Neal D. Ryan, and Fuchiang (Rich) Tsui. 2017. Predictive Modeling 
for Classification of Positive Valence System Symptom Severity from Initial 
Psychiatric Evaluation Records. Journal of Biomedical Informatics. 

Wei Quan, Haixia Liu, Lingyun Shi, Neal Ryan, David Brent, and Fuchiang 
Tsui. 2019. Classifying social determinants of health from electronic health 
records using deep neural networks. Technical report. 

Karina Quevedo, Rowena Ng, Hannah Scott, Jodi Martin, Garry Smyda, Matt 
Keener, and Caroline W. Oppenheimer. 2016. The neurobiology of self-face 
recognition in depressed adolescents with low or high suicidality. J Abnorm 
Psychol., 125(8):1185‚Äì1200. 

Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, 
Sunghwan Sohn, Karin C Kipper-Schuler, and Christopher G Chute. 2010. 
Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): 
architecture, component evaluation and applications. Journal of the American 
Medical Informatics Association, 17(5):507‚Äì513, September. 

Han-Chin Shing, Suraj Nair, Ayah Zirikly, Meir Friedenberg, Hal Daum√© III, 
and Philip Resnik. 2018. Expert, Crowdsourced, and Machine Assessment of 
Suicide Risk via Online Postings. In Proceedings of the Fifth Workshop on 
Computational Linguistics and Clinical Psychology: From Keyboard to Clinic, 
pages 25‚Äì36. 

Fuchiang Rich Tsui, Victor Ruiz, Amie Barda, Ye Ye, Srinivasan Suresh, and 
Andrew Urbach. 2017. Retrospective and Prospective Evaluations of the 
System for Hospital Adaptive Readmission Prediction and Management ( 
SHARP ) for All-Cause 30-Day Pediatric Readmission Prediction Children ‚Äô s 
Hospital of Pittsburgh of UPMC , Pittsburgh , PA. In AMIA 2017. 

Fuchiang Rich Tsui, Lingyun Shi, Victor Ruiz, Neal Ryan, Candice Biernesser, 
and David Brent. 2019. A large-data-driven approach for predicting suicide 
attempts and suicide deaths. In The 17th World Congress of Medical and Health 
Informatics, Lyon, France. 

Wikipedia. Reddit wikipedia. 

Ayah Zirikly, Philip Resnik, Ozlem Uzuner, and Kristy Hollingshead. 2019. 
{CLPsych} 2019 Shared Task: Predicting the Degree of Suicide Risk in 
{Reddit} Posts. In Proceedings of the Sixth Workshop on Computational 
Linguistics and Clinical Psychology: From Keyboard to Clinic. 

 Psychological disorders. 

 


