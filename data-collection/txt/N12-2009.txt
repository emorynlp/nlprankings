










































Deep Unsupervised Feature Learning for Natural Language Processing


Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 48–53,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Deep Unsupervised Feature Learning for Natural Language Processing

Stephan Gouws
MIH Media Lab, Stellenbosch University

Stellenbosch, South Africa
stephan@ml.sun.ac.za

Abstract

Statistical natural language processing (NLP) builds
models of language based on statistical features ex-
tracted from the input text. We investigate deep
learning methods for unsupervised feature learning
for NLP tasks. Recent results indicate that features
learned using deep learning methods are not a sil-
ver bullet and do not always lead to improved re-
sults. In this work we hypothesise that this is the
result of a disjoint training protocol which results
in mismatched word representations and classifiers.
We also hypothesise that modelling long-range de-
pendencies in the input and (separately) in the out-
put layers would further improve performance. We
suggest methods for overcoming these limitations,
which will form part of our final thesis work.

1 Introduction
Natural language processing (NLP) can be seen as build-
ing models h : X → Y for mapping an input encoding
x ∈ X representing a natural language (NL) fragment, to
an output encoding y ∈ Y representing some construct or
formalism used in the particular NLP task of interest, e.g.
part-of-speech (POS) tags, begin-, inside-, outside (BIO)
tags for information extraction, semantic role labels, etc.

Since the 90s, the predominant approach has been sta-
tistical NLP, where one models the problem as learning a
predictive function h for mapping from h : X → Y using
machine learning techniques. Machine learning consists
of a hypothesis function which learns this mapping based
on latent or explicit features extracted from the input data.

In this framework, h is usually trained in a supervised
setting from labelled training pairs (xi, yi) ∈ X × Y .
Additionally, the discriminant function h typically oper-
ates on a transformed representation of the data to a com-
mon feature space encoded as a feature vector φ(x), and
then learns a mapping from feature space to the output
space, h : φ(x) → y. In supervised learning, the idea

~x ∈ X the cat sits on the mat
φ(~x) φ(x1) φ(x2) φ(x3) φ(x4) φ(x5) φ(x6)
~y ∈ Y B-NP I-NP B-VP O B-NP I-NP
NE Tags [the cat] NP [sits] VP [on] O [the mat] NP

Table 1: Example NLP syntactic chunking task for the sentence
“the cat sits on the mat”. X represents the words in the input
space, Y represents labels in the output space. φ(~x) is a feature
representation for the input text ~x and the bottom row represents
the output named entity tags in a more standard form.

is generally that features represent strong discriminating
characteristics of the problem gained through manual en-
gineering and domain-specific insight.

As a concrete example, consider the task of syntactic
chunking, also called “shallow parsing”, (Gildea and Ju-
rafsky, 2002): Given an input string, e.g.

“the cat sits on the mat”,

the chunking problem consists of labelling segments of a
sentence with syntactic constituents such as noun or verb
phrases (NPs or VPs). Each word is assigned one unique
tag often encoded using the BIO encoding1. We repre-
sent the input text as a vector of words xi ∈ ~x, and each
word’s corresponding label is represented by yi ∈ ~y (see
Table 1). Given a feature generating function φ(xi) and
a set of labelled training pairs (xi, yi) ∈ X × Y , the task
then reduces to learning a suitable mapping h : φ(X ) →
Y .

Most previous works have focused on manually en-
gineered features and simpler, linear models, includ-
ing “shallow” model architectures, like the percep-
tron (Rosenblatt, 1957), linear SVM (Cortes and Vap-
nik, 1995) and linear-chain conditional random fields
(CRFs) (Lafferty, 2001). However, a shallow learning ar-
chitecture is only as good as its input features. Due to the
complex nature of NL, deeper architectures may be re-

1E.g. B-NP means “begin NP”, I-NP means “inside NP”, and O
means other/outside.

48



quired to learn data representations which contain the ap-
propriate level of information for the task at hand. Prior to
2006, it was computationally infeasible to perform infer-
ence in hierarchical (“deep”), non-linear models such as
multi-layer perceptrons with more than one hidden layer.
However, Hinton (2006) proposed an efficient, layer-wise
greedy method for learning the model parameters in these
architectures, which spurred a renewed interest in deep
learning research.

Still, creating annotated training data is labour-
intensive and costly, and manually designing and extract-
ing discriminating features from the data to be used in
the learning process is a costly procedure requiring sig-
nificant levels of domain expertise. Over the last two
decades, the growth of available unlabeled data x ∈ X
and the ubiquity of scalable computing power has shifted
research focus to unsupervised approaches for automat-
ically learning appropriate feature representations φ(x)
from large collections of unlabeled text.

Several methods have been proposed for unsuper-
vised feature learning, including simple k-means cluster-
ing (Lloyd, 1982), Brown clustering (Brown et al., 1992),
mutual information (Shannon and Weaver, 1962), princi-
pal components analysis (PCA) (Jolliffe, 2002), and in-
dependent component analysis (ICA) (Hyvärinen et al.,
2001).

However, natural language has complex mappings
from text to meaning, arguably involving higher-order
correlations between words which these simpler meth-
ods struggle to model adequately. Advances in the “deep
learning” community allow us to perform efficient unsu-
pervised feature learning in highly complex and high-
dimensional input feature spaces, making it an attrac-
tive method for learning features in e.g. vision or lan-
guage (Bengio, 2009).

The standard deep learning approach is to learn
lower-dimensional embeddings from the raw high-
dimensional2 input space X to lower dimensional (e.g.
50-dimensional) feature spaces in an unsupervised man-
ner, via repeated, layer-wise, non-linear transformation
of the input features, e.g.

ŷ = f (k)(· · · f (2)(f (1)(~x)) · · ·),

where f (i)(x) is some non-linear function (typically
tanh) for which the parameters are learned by back prop-
agating error gradients. This configuration is referred to
as a “deep” architecture with k layers (see Figure 1 for an
example).

For feature generation, we present a trained network
with a new vector ~x representing the input data on its

2E.g. a “one-hot” 50,000-dimensional vector of input words, with a
‘1’ indicating the presence of the word at that index, and a ‘0’ every-
where else.

Figure 1: Example of a deep model. The input vector x is trans-
formed into the hidden representation, here denoted as h1, using
an affine transformation W and a non-linearity. Each subse-
quent hidden layer hk takes as input the output of its preceding
layer h(k−1) (Bengio, 2009).

input layer. After performing one iteration of forward-
propagation through the network, we can then view the
activation values in the hidden layers as dense, so-called
“distributed representations” (features) of the input data.
These features can in turn be passed to an output clas-
sifier layer to produce some tagging task of interest. Re-
cent work in deep learning show state-of-the-art results in
part-of-speech parsing, chunking and named-entity tag-
ging (Collobert, 2011), however performance in more
complex NLP tasks like entity and event disambiguation
and semantic role labelling are still trailing behind.

In this work we focus specifically on extending current
state of the art deep neural models to improve their per-
formance on these more difficult tasks. In the following
section we briefly review and discuss the merits and lim-
itations of three of the current state of the art deep learn-
ing models for NLP. We then identify our primary re-
search questions and introduce our proposed future work
roadmap.

2 Current State-of-the-Art and
Limitations

Most work builds on the idea of a neural probabilistic
language model (NPLM) where words are represented
by learned real-valued embeddings, and a neural network
combines word embeddings to predict the most likely
next word. The first successful NPLM was introduced
by Bengio et al. in 2003 (Bengio et al., 2003).

49



Historically, training and testing these models were
slow, scaling linearly in the vocabulary size. However,
several recent approaches have been proposed which
overcome these limitations (Morin and Bengio, 2005),
including the work by Collobert and Weston (2008) and
Mnih and Hinton (2009) discussed next.

2.1 Collobert & Weston (2008)

Collobert and Weston (2008) present a discriminative,
non-probabilistic, non-linear neural language model that
can be scaled to train over billions of words since each
training iteration only computes a loss gradient over a
small stochastic sample of the training data.

All K-dimensional word embeddings are initially set
to a random state. During each training iteration, an n-
gram is read from the training data and each word is
mapped to its respective embedding. All embeddings are
then concatenated to form a nK-length positive training
vector. A corrupted n-gram is also created by replac-
ing the n’th (last) word by some word uniformly chosen
from the vocabulary. The training criterion is that the net-
work must predict positive training vectors with a score
at least some margin higher than the score predicted for
corrupted n-grams. Model parameters are trained simul-
taneously with word embeddings via gradient descent.

2.2 The Hierarchical Log Bilinear (HLBL) Model

Mnih and Hinton (2007) proposed a simple probabilis-
tic linear neural language model called the log bilin-
ear (LBL) model. For an n-gram context window, the
LBL model concatenates the first (n− 1)K-dimensional
word embeddings and then learns a linear mapping from
R(n−1)K to RK for predicting the embedding of the nth
word. For predicting the next word, the model outputs
a probability distribution over the entire vocabulary by
computing the dot product between the predicted embed-
ding and the embedding for each word in the vocabulary
in an output softmax layer. This softmax computation is
linear in the length of the vocabulary for each prediction,
and is therefore the performance bottleneck.

In follow-up work, Mnih and Hinton (2009) speed up
training and testing time by extending the LBL model to
predict the next word by hierarchically decomposing the
search through the vocabulary by traversing a binary tree
constructed over the vocabulary. This speeds up train-
ing and testing exponentially, but initially reduces model
performance as the construction of the binary partition-
ing has a strong effect on the model’s performance. They
introduce a method for bootstrapping the construction of
the tree by initially using a random binary tree to learn
word embeddings, and then rebuilding the tree based on
a clustering of the learned embeddings. Their final results
are superior to the standard LBL in both model perplexity
and model testing time.

2.3 Recursive Neural Networks (RNNs)
Socher (2010; 2011) introduces a recursive neural net-
work (RNN) framework for parsing natural language.
Previous approaches dealt with variable-length sentences
by either

i using a window approach (shifting a window of n
words over the input, processing each fixed-size win-
dow at a time), or

ii by using a convolutional layer where each word is
convolved with its neighbours within some sentence-
or window-boundary.

RNNs operate by recursively applying the same neural
network to segments of its input, thereby allowing RNNs
to naturally operate on variable-length inputs. Each pair
of neighbouring words is scored by the network to reflect
how likely these two words are considered to form part of
a phrase. Each such operation takes two K-dimensional
word vectors and outputs another K-dimensional vector
and a score. Socher (2010) proposes several strategies
(ranging from local and greedy to global and optimal) for
choosing which pairs of words to collapse into a new K-
dimensional vector representing the phrase comprised of
the two words. By viewing these collapsing operations
as branch merging decisions, one can construct a binary
parse tree over the words in a bottom-up fashion.

2.4 Discussion of Limitations
Neural language models are appealing since they can
more easily deal with missing data (unknown word com-
binations) due to their inherent continuous-space repre-
sentation, whereas n-gram language models (Manning et
al., 1999) need to employ (sometimes ad hoc) methods
for smoothing unseen and hence zero probability word
combinations.

The original NPLM performs well in terms of model
perplexity on held-out data; however, its training and test-
ing time is very slow. Furthermore, it provides no support
for handling multiple word senses, the property that any
word can have more than one meaning, since each word
is assigned an embedding based on its literal string repre-
sentation (i.e. from a lookup table).

The Collobert & Weston model still provides no mech-
anism for handling word senses, but improves on the
NPLM by adding several non-linear layers which in-
crease its modelling capacity, and a convolutional layer
for modelling longer range dependencies between words.
Recursive neural nets (RNNs) directly address the prob-
lem of longer-range dependencies by allowing neighbour
words to be combined into their phrasal equivalents in a
bottom-up process.

The LBL model, despite its very simple linear struc-
ture, provides very good performance in terms of model

50



perplexity, but shares the problem of slow training and
testing times and the inability to handle word senses or
dependencies between words (outside its n-gram con-
text).

In the HLBL model, Mnih and Hinton address the slow
testing performance of the LBL model by using a hierar-
chical search tree over the vocabulary to exponentially
speed up testing time, analogous to the concept of class-
based language models (Brown et al., 1992). The HLBL
model can also handle multiple word senses, but in their
evaluation they show that in practice the model learns
multiple senses (codes) for infrequently observed words
instead of words with more than one meaning (Mnih and
Hinton, 2009). The performance is strongly dependent
on the initialisation of the tree, for which they present an
iterative but non-optimal bootstrap-and-train procedure.
Despite being non-optimal, it is shown to outperform the
standard LBL model in terms of perplexity.

3 Mismatched Word Representations and
Classifiers

The deep learning ideal is to train deep, non-linear mod-
els over large collections of unlabeled data, and then use
these models to automatically extract information-rich,
higher-level features3 to integrate into standard NLP or
image processing systems as added features to improve
performance. However, several recent papers report sur-
prising and seemingly contradicting results for this ideal.

In the most direct comparison for NLP, Turian (2010)
compares features extracted using Brown clustering (a
hierarchical clustering technique for clustering words
based on their observed co-occurrence patterns), the hi-
erarchical log-bilinear (HLBL) embeddings (Mnih and
Hinton, 2007) and Collobert and Weston (C+W) em-
beddings (Collobert and Weston, 2008), by integrating
these as additional features in standard supervised condi-
tional random field (CRF) classification systems for NLP.
Somewhat surprisingly, they find that using the more
complex C+W and HLBL features do not improve signif-
icantly over Brown features. Indeed, under several con-
ditions the Brown features give the best results.

These results are important for several reasons (we
highlight these results in Table 2). The goal was to im-
prove classification performance in structured prediction
tasks in natural language by integrating features learned
in a deep, unsupervised approach within a standard lin-
ear classification framework. Yet these complex, deep
methods are outperformed by simpler unsupervised fea-
ture extraction methods.

3“Higher-level” features simply mean combining simpler features
extracted from a text to produce conceptually more abstract indicators,
e.g. combining word-indicators for “attack”, “soldier”, etc. to form an
indicator for WAR, even though “war” is not mentioned anywhere in the
text.

System Dev Test MUC7

Baseline 90.03 84.39 67.48
HLBL 100-dim 92.00 88.13 75.25
C&W 50-dim 92.27 87.93 75.74

Brown, 1000 clusters 92.32 88.52 78.84
C&W 200-dim 92.46 87.96 75.51

Table 2: Final NER F1 results reported by Turian (2010).

In a sense, these seem to be negative results for the
utility of deep learning in NLP. However, in this work we
argue that these seemingly anomalous results stem from
a mismatch between the feature learning function and the
classifier that was used in the classification (and hence
evaluation) process.

We consider the learning problem h : X → Y to
decompose into h = h′(φ(X )), where φ is the feature
learning function and h′ is a standard supervised classi-
fier. φ reads input from X and outputs encodings in fea-
ture space φ(x). h reads input in feature space φ(x) and
outputs encodings in the output label space Y .

Note that this easily extends to deep feature
learning models by simply replacing φ(X ) with
φ(k)(· · ·φ(2)(φ(1)(X )) · · ·), for a k-layer architecture,
where the first layer reads input inX and each subsequent
layer reads the output of the previous layer.

Within this view of the deep learning process, we can
see that unsupervised feature learning does not happen in
isolation. Instead, the learned features only make sense
within some learning framework, since the output of the
feature learning function φ (and each deep layer φ(k−1))
maps to a region in feature code space which becomes
in turn the input to the output classifier h′ (or subsequent
layer φ(k)) . We therefore argue that in a semi-supervised
or unsupervised classification problem, the feature learn-
ing function φ should be strongly dependent on the clas-
sifier h′ that interprets those features, and vice versa.

This notion ties in with the standard deep-learning
training protocol of unsupervised pre-training followed
by joint supervised fine-tuning (Hinton et al., 2006) of the
top classification layer and the deeper feature extraction
layers. We conjecture that jointly training a deep feature
extraction model with a linear output classifier leads to
better linearly separable feature vectors φ(x) than train-
ing both independently. Note that this is in contrast to
how Turian (2010) integrated the unsupervised features
into existing NLP systems via disjoint training.

4 Proposed Work and Research Questions

For simpler sequence tagging tasks such as part-of-
speech tagging and noun phrase chunking, the state-
of-the-art models introduced in Section 2 perform ade-
quately. However, in order to make use of the increased

51



modelling capacity of deep neural models, and to suc-
cessfully model more complex semantic tasks such as
anaphora resolution and semantic role labelling, we hy-
pothesise that the model needs to avoid modelling purely
local lexical semantics and needs to efficiently handle
multiple word senses and long-range dependencies be-
tween input words (or phrases) and output labels. We
propose to overcome the limitations of previous models
with regard to these design goals, by focusing on the fol-
lowing key areas:

Input language representation: Neural models rely
on vector representations of their input (as opposed to
discrete representations as in, for instance, HMMs). In
NLP, sentences are therefore encoded as real-valued em-
bedding vectors. These vectors are learned in either a
task-specific setting (as in the C+W model) or as part of
a language model (as in the LBL model), where the goal
is to predict the next word given the learned representa-
tions of the previous words. In order to maximise the
information available to the model, we need to provide
information-rich representations to the model. Current
approaches represent each word in a sentence using a dis-
tinct word vector based on its literal string representation.
However, as noted earlier, in NL the same words can have
different senses based on the context in which it appears
(polysemy). We propose to extend the hierarchical log-
bilinear (HLBL) language model (see Section 2.2) in two
important ways. We choose the HLBL model for its sim-
plicity and good performance compared to more complex
models.

Firstly, we propose to replace the iterative bootstrap-
and-train process for learning the hierarchical tree struc-
ture over the vocabulary with a modified self-balancing
binary tree. The tree rebalances itself from an initial
random tree to leave most frequently accessed words
near the root (for shorter codes and faster access times),
while moving words between clusters to maximise over-
all model perplexity.

Secondly, we propose to add a word sense disambigua-
tion layer capable of modelling long-range dependencies
between input words. For this layer we will compare a
modified RNN layer to a convolutional layer. The modi-
fied RNN will embed each focus word with its nmost dis-
criminative neighbour words (in a sentence context win-
dow) into a new K-dimensional, sense-disambiguated
embedding vector for the focus word. We will evaluate
and optimise the final model’s learned representations by
evaluating language model perplexity on held out data.

Model architecture and internal representation:
Deep models derive their modelling power from their hi-
erarchical structure. Each layer transforms the output
representation of its previous layer, allowing the model
to learn more general and abstract feature combinations
in the higher layers which are relevant for the current

task. The representations on the hidden layers serve
as transformed feature representations of the input data
for the output classifier. Enforcing sparsity on the hid-
den layers has been shown to produce stronger features
for certain tasks in vision (Coates et al., 2010). Ad-
ditionally, individual nodes might be highly correlated,
which can also reduce the performance of certain clas-
sifiers which make strong independence assumptions (for
instance naive Bayes). We propose to study the effect that
enforcing sparsity in the learned feature representations
has on task performance in NLP. Additionally, we pro-
pose to evaluate the effect that an even stronger training
objective – one that encourages statistical independence
between hidden nodes by learning factorial code repre-
sentations (Hochreiter and Schmidhuber, 1999) – has on
model performance.

Modelling structure in the output space: Tasks
in NLP mostly involve predicting labels which exhibit
highly regular structure. For instance, in part-of-speech
tagging, two determiners have a very low likelihood of
following directly on one another, e.g. “the the”. In or-
der to successfully model this phenomenon, a model must
take into account previous (and potentially future) predic-
tions when making the current prediction, e.g. as in hid-
den Markov models and conditional random fields. We
propose to include sequential dependencies in the output
labels and to compare this with including a convolutional
layer below the output layer, for predicting output labels
in complex NLP tasks such as coreference resolution and
event structure detection.

5 Conclusion

Deep learning methods offer an attractive unsupervised
approach for extracting higher-level features from large
quantities of text data to be used for NLP tasks. However
current attempts at integrating these features into existing
NLP systems do not produce the desired performance im-
provements. We conjecture that this is due to a mismatch
between the learned word representations and the classi-
fiers used as a result of disjoint training schemes, and our
thesis roadmap suggests three key areas for overcoming
these limitations.

References

Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. 2003. A
neural probabilistic language model. Journal of Machine
Learning Research, 3:1137–1155, March.

Y. Bengio. 2009. Learning deep architectures for ai. Founda-
tions and Trends R© in Machine Learning, 2(1):1–127.

P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C.
Lai. 1992. Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

52



A. Coates, H. Lee, and A.Y. Ng. 2010. An analysis of single-
layer networks in unsupervised feature learning. Ann Arbor,
1001:48109.

R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks with
multitask learning. In Proceedings of the 25th International
Conference on Machine Learning, pages 160–167. ACM.

R. Collobert. 2011. Deep learning for efficient discrimina-
tive parsing. In International Conference on Artificial In-
telligence and Statistics (AISTATS).

C. Cortes and V. Vapnik. 1995. Support-vector networks. Ma-
chine learning, 20(3):273–297.

D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245–288.

G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast learn-
ing algorithm for deep belief nets. Neural computation,
18(7):1527–1554.

S. Hochreiter and J. Schmidhuber. 1999. Feature extraction
through lococode. Neural Computation, 11(3):679–714.

A. Hyvärinen, J. Karhunen, and E. Oja. 2001. Independent
component analysis, volume 26. Wiley Interscience.

I.T. Jolliffe. 2002. Principal component analysis, volume 2.
Wiley Online Library.

J. Lafferty. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Pro-
ceedings of ICML, 2001.

S. Lloyd. 1982. Least squares quantization in pcm. IEEE
Transactions on Information Theory, 28(2):129–137.

C.D. Manning, H. Schütze, and MITCogNet. 1999. Founda-
tions of statistical natural language processing, volume 999.
MIT Press.

A. Mnih and G. Hinton. 2007. Three new graphical models for
statistical language modelling. In Proceedings of the 24th
international conference on Machine learning, pages 641–
648. ACM.

A. Mnih and G.E. Hinton. 2009. A scalable hierarchical dis-
tributed language model. Advances in neural information
processing systems, 21:1081–1088.

F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural
network language model. In AISTATS05, pages 246–252.

F. Rosenblatt. 1957. The Perceptron, a Perceiving and Recog-
nizing Automaton Project Para. Cornell Aeronautical Labo-
ratory.

C.E. Shannon and W. Weaver. 1962. The mathematical theory
of communication, volume 19. University of Illinois Press
Urbana.

R. Socher, C.D. Manning, and A.Y. Ng. 2010. Learning contin-
uous phrase representations and syntactic parsing with recur-
sive neural networks. In Proceedings of the NIPS-2010 Deep
Learning and Unsupervised Feature Learning Workshop.

R. Socher, C.C. Lin, A.Y. Ng, and C.D. Manning. 2011. Pars-
ing natural scenes and natural language with recursive neural
networks. In Proceedings of the 26th International Confer-
ence on Machine Learning (ICML), volume 2, page 7.

J. Turian, L. Ratinov, and Y. Bengio. 2010. Word represen-
tations: A simple and general method for semi-supervised
learning. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages 384–394.
Association for Computational Linguistics.

53


