










































Creating Disjunctive Logical Forms from Aligned Sentences for Grammar-Based Paraphrase Generation


Workshop on Monolingual Text-To-Text Generation, pages 74–83,

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 74–83,
Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics

Creating Disjunctive Logical Forms from Aligned Sentences for
Grammar-Based Paraphrase Generation

Scott Martin and Michael White
Department of Linguistics
The Ohio State University

Columbus, Ohio, USA
{scott,mwhite}@ling.ohio-state.edu

Abstract

We present a method of creating disjunctive
logical forms (DLFs) from aligned sentences
for grammar-based paraphrase generation us-
ing the OpenCCG broad coverage surface re-
alizer. The method takes as input word-level
alignments of two sentences that are para-
phrases and projects these alignments onto the
logical forms that result from automatically
parsing these sentences. The projected align-
ments are then converted into phrasal edits
for producing DLFs in both directions, where
the disjunctions represent alternative choices
at the level of semantic dependencies. The re-
sulting DLFs are fed into the OpenCCG re-
alizer for n-best realization, using a pruning
strategy that encourages lexical diversity. Af-
ter merging, the approach yields an n-best list
of paraphrases that contain grammatical alter-
natives to each original sentence, as well as
paraphrases that mix and match content from
the pair. A preliminary error analysis suggests
that the approach could benefit from taking the
word order in the original sentences into ac-
count. We conclude with a discussion of plans
for future work, highlighting the method’s po-
tential use in enhancing automatic MT evalu-
ation.

1 Introduction

In this paper, we present our initial steps towards
merging the grammar-based and data-driven para-
phrasing traditions, highlighting the potential of
our approach to enhance the automatic evaluation
of machine translation (MT). Kauchak and Barzi-
lay (2006) have shown that creating synthetic ref-
erence sentences by substituting synonyms from

Wordnet into the original reference sentences can
increase the number of exact word matches with
an MT system’s output and yield significant im-
provements in correlations of BLEU (Papineni et
al., 2002) scores with human judgments of trans-
lation adequacy. Madnani (2010) has also shown
that statistical machine translation technique can be
employed in a monolingual setting, together with
paraphrases acquired using Bannard and Callison-
Burch’s (2005) pivot method, in order to enhance
the tuning phase of training an MT system by aug-
menting a reference translation with automatic para-
phrases. Earlier, Barzilay and Lee (2003) and Pang
et al. (2003) developed approaches to aligning mul-
tiple reference translations in order to extract para-
phrases and generate new sentences. By starting
with reference sentences from multiple human trans-
lators, these data-driven methods are able to capture
subtle, highly-context sensitive word and phrase al-
ternatives. However, the methods are not particu-
larly adept at capturing variation in word order or
the use of function words that follow from general
principles of grammar. By contrast, grammar-based
paraphrasing methods in the natural language gen-
eration tradition (Iordanskaja et al., 1991; Elhadad
et al., 1997; Langkilde and Knight, 1998; Stede,
1999; Langkilde-Geary, 2002; Velldal et al., 2004;
Gardent and Kow, 2005; Hogan et al., 2008) have
the potential to produce many such grammatical al-
ternatives: in particular, by parsing a reference sen-
tence to a representation that can be used as the in-
put to a surface realizer, grammar-based paraphrases
can be generated if the realizer supports n-best out-
put. To our knowledge though, methods of using a
grammar-based surface realizer together with multi-
ple aligned reference sentences to produce synthetic

74



Source Liu Lefei says that [in the long term] , in terms of asset allocation, overseas in-
vestment should occupy a certain proportion of [an insurance company’s overall
allocation] .

Reference Liu Lefei said that in terms of capital allocation , outbound investment should make
up a certain ratio of [overall allocations for insurance companies] [in the long run]
.

Paraphrase Liu Lefei says that [in the long run], in terms of capital allocation, overseas invest-
ment should occupy the certain ratio of an [insurance company’s overall allocation]

Table 1: Zhao et al.’s (2009) similarity example, with italics added to show word-level substitutions, and square
brackets added to show phrase location or construction mismatches. Here, the source sentence (itself a reference
translation) has been paraphrased to be more like the reference sentence.

references have not been investigated.1

As an illustration of the need to combine gram-
matical paraphrasing with data-driven paraphrasing,
consider the example that Zhao et al. (2009) use
to illustrate the application of their paraphrasing
method to similarity detection, shown in Table 1.
Zhao et al. make use of a large paraphrase table,
similar to the phrase tables used in statistical MT, in
order to construct paraphrase candidates. (Like the-
sauri or WordNet, such resources are complemen-
tary to the ones we make use of here.) To test their
system’s ability to paraphrase reference sentences in
service of MT evaluation, they attempt to paraphrase
one reference translation to make it more similar to
another reference translation; thus, in Table 1, the
source sentence (itself a reference translation) has
been paraphrased to be more like the (other) refer-
ence sentence. As indicated by italics, their sys-
tem has successfully paraphrased term, asset and
proportion as run, capital and ratio, respectively
(though the certain seems to have been mistakenly
substituted for a certain). However, their system
is not capable of generating a paraphrase with in
the long run at the end of the sentence, nor can it
rephrase insurance company’s overall allocation as
overall allocations for insurance companies, which
would seem to require access to more general gram-
matical knowledge.

To combine grammar-based paraphrasing with
lexical and phrasal alternatives gleaned from mul-
tiple reference sentences, our approach takes advan-

1The task is not unrelated to sentence fusion in multidoc-
ument summarization (Barzilay and McKeown, 2005), except
there the goal is to produce a single, shorter sentence from mul-
tiple related input sentences.

tage of the OpenCCG realizer’s ability to generate
from disjunctive logical forms (DLFs), i.e. packed
semantic dependency graphs (White, 2004; White,
2006a; White, 2006b; Nakatsu and White, 2006; Es-
pinosa et al., 2008; White and Rajkumar, 2009). In
principle, semantic dependency graphs offer a better
starting point for paraphrasing than the syntax trees
employed by Pang et. al, as paraphrases can gener-
ally be expected to be more similar at the level of
unordered semantic dependencies than at the level
of syntax trees. Our method starts with word-level
alignments of two sentences that are paraphrases,
since the approach can be used with any alignment
method from the MT (Och and Ney, 2003; Haghighi
et al., 2009, for example) or textual inference (Mac-
Cartney et al., 2008, inter alia) literature in princi-
ple. The alignments are projected onto the logical
forms that result from automatically parsing these
sentences. The projected alignments are then con-
verted into phrasal edits for producing DLFs in both
directions, where the disjunctions represent alterna-
tive choices at the level of semantic dependencies.
The resulting DLFs are fed into the OpenCCG re-
alizer for n-best realization. In order to enhance
the variety of word and phrase choices in the n-best
lists, a pruning strategy is used that encourages lex-
ical diversity. After merging, the approach yields
an n-best list of paraphrases that contain grammat-
ical alternatives to each original sentence, as well
as paraphrases that mix and match content from the
pair.

The rest of the paper is organized as follows. Sec-
tion 2 provides background on surface realization
with OpenCCG and DLFs. Section 3 describes our

75



method of creating DLFs from aligned paraphrases.
Finally, Section 4 characterizes the recurring errors
and concludes with a discussion of related and future
work.

2 Surface Realization with OpenCCG

OpenCCG is an open source Java library for pars-
ing and realization using Baldridge’s multimodal
extensions to CCG (Steedman, 2000; Baldridge,
2002). In the chart realization tradition (Kay, 1996),
the OpenCCG realizer takes logical forms as input
and produces strings by combining signs for lexical
items. Alternative realizations are scored using in-
tegrated n-gram and perceptron models (White and
Rajkumar, 2009), where the latter includes syntac-
tic features from Clark and Curran’s (2007) normal
form model as well as discriminative n-gram fea-
tures (Roark et al., 2004). Hypertagging (Espinosa
et al., 2008), or supertagging for surface realiza-
tion, makes it practical to work with broad coverage
grammars. For parsing, an implementation of Hock-
enmaier and Steedman’s (2002) generative model is
used to select the best parse. The grammar is auto-
matically extracted from a version of the CCGbank
(Hockenmaier and Steedman, 2007) with Propbank
(Palmer et al., 2005) roles projected onto it (Boxwell
and White, 2008).

A distinctive feature of OpenCCG is the ability
to generate from disjunctive logical forms (White,
2006a). This capability has many benefits, such
as enabling the selection of realizations according
to predicted synthesis quality (Nakatsu and White,
2006), and avoiding repetition in the output of a dia-
logue system (Foster and White, 2007). Disjunctive
inputs make it possible to exert fine-grained control
over the specified paraphrase space. In the chart re-
alization tradition, previous work has not generally
supported disjunctive logical forms, with Shemtov’s
(Shemtov, 1997) more complex approach as the only
published exception.

An example disjunctive input from the COMIC
system appears in Figure 1(c).2 Semantic de-
pendency graphs such as these—represented in-
ternally in Hybrid Logic Dependency Semantics

2To simplify the exposition, the features specifying informa-
tion structure and deictic gestures have been omitted, as have
the semantic sorts of the discourse referents.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection<DET>the,<NUM>sg c 

<HASPROP> <CREATOR>

Funny_Day f v Villeroy_and_Boch

 

(a) Semantic dependency graph for The design (is|’s)
based on the Funny Day collection by Villeroy and
Boch.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

series<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

 

(b) Semantic dependency graph for The design (is|’s)
based on Villeroy and Boch’s Funny Day series.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection|series(<DET>the)?,<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

<CREATOR>

 

(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|’s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch’s Funny Day (collection|series)).

Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ collection ∧ 〈DET〉the ∧ 〈NUM〉sg ∧

〈HASPROP〉(f ∧ Funny Day) ∧
〈CREATOR〉(v ∧ V&B))))

(a)
...

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ 〈NUM〉sg ∧ (〈DET〉the)? ∧

(collection ∨ series) ∧
〈HASPROP〉(f ∧ Funny Day) ∧
(〈CREATOR〉v ∨ 〈GENOWNER〉v ))))

∧ @v(Villeroy and Boch)

(c)

Figure 2: HLDS for examples in Figure 1.

2 Disjunctive Logical Forms

As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. 〈NUM〉sg);
nodes are connected via dependency relations (e.g.
〈ARTIFACT〉).

Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the 〈CREATOR〉 and
〈GENOWNER〉 relations—lexicalized via by and
the possessive, respectively—connecting the head
c (collection or series) with the dependent v (for

1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-

mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection<DET>the,<NUM>sg c 

<HASPROP> <CREATOR>

Funny_Day f v Villeroy_and_Boch

 

(a) Semantic dependency graph for The design (is|’s)
based on the Funny Day collection by Villeroy and
Boch.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

series<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

 

(b) Semantic dependency graph for The design (is|’s)
based on Villeroy and Boch’s Funny Day series.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection|series(<DET>the)?,<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

<CREATOR>

 

(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|’s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch’s Funny Day (collection|series)).

Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ collection ∧ 〈DET〉the ∧ 〈NUM〉sg ∧

〈HASPROP〉(f ∧ Funny Day) ∧
〈CREATOR〉(v ∧ V&B))))

(a)
...

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ 〈NUM〉sg ∧ (〈DET〉the)? ∧

(collection ∨ series) ∧
〈HASPROP〉(f ∧ Funny Day) ∧
(〈CREATOR〉v ∨ 〈GENOWNER〉v ))))

∧ @v(Villeroy and Boch)

(c)

Figure 2: HLDS for examples in Figure 1.

2 Disjunctive Logical Forms

As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. 〈NUM〉sg);
nodes are connected via dependency relations (e.g.
〈ARTIFACT〉).

Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the 〈CREATOR〉 and
〈GENOWNER〉 relations—lexicalized via by and
the possessive, respectively—connecting the head
c (collection or series) with the dependent v (for

1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-

mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection<DET>the,<NUM>sg c 

<HASPROP> <CREATOR>

Funny_Day f v Villeroy_and_Boch

 

(a) Semantic dependency graph for The design (is|’s)
based on the Funny Day collection by Villeroy and
Boch.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

series<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

 

(b) Semantic dependency graph for The design (is|’s)
based on Villeroy and Boch’s Funny Day series.

be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>

based_on <DET>the,<NUM>sgdesign d p 

<SOURCE> 
<ARTIFACT>

collection|series(<DET>the)?,<NUM>sg c 

<HASPROP> <GENOWNER>

Funny_Day f v Villeroy_and_Boch

<CREATOR>

 

(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|’s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch’s Funny Day (collection|series)).

Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ collection ∧ 〈DET〉the ∧ 〈NUM〉sg ∧

〈HASPROP〉(f ∧ Funny Day) ∧
〈CREATOR〉(v ∧ V&B))))

(a)
...

@e(be ∧ 〈TENSE〉pres ∧ 〈MOOD〉dcl ∧
〈ARG〉(d ∧ design ∧ 〈DET〉the ∧ 〈NUM〉sg) ∧
〈PROP〉(p ∧ based on ∧

〈ARTIFACT〉d ∧
〈SOURCE〉(c ∧ 〈NUM〉sg ∧ (〈DET〉the)? ∧

(collection ∨ series) ∧
〈HASPROP〉(f ∧ Funny Day) ∧
(〈CREATOR〉v ∨ 〈GENOWNER〉v ))))

∧ @v(Villeroy and Boch)

(c)

Figure 2: HLDS for examples in Figure 1.

2 Disjunctive Logical Forms

As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. 〈NUM〉sg);
nodes are connected via dependency relations (e.g.
〈ARTIFACT〉).

Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the 〈CREATOR〉 and
〈GENOWNER〉 relations—lexicalized via by and
the possessive, respectively—connecting the head
c (collection or series) with the dependent v (for

1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-

mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.

Figure 1: Two similar logical forms from the COMIC
system as semantic dependency graphs, together with a
disjunctive logical form representing their combination
as a packed semantic dependency graph.

76



(Baldridge and Kruijff, 2002; White, 2006b), or
HLDS—constitute the input to the OpenCCG re-
alizer.3 This graph allows a free choice between
the domain synonyms collection and series, as in-
dicated by the vertical bar between their respec-
tive predications. The graph also allows a free
choice between the 〈CREATOR〉 and 〈GENOWNER〉
relations—lexicalized via by and the possessive,
respectively—connecting the head c (collection or
series) with the dependent v (for Villeroy and Boch);
this choice is indicated by an arc between the two
dependency relations. Finally, the determiner fea-
ture (〈DET〉the) on c is indicated as optional, via the
question mark. Note that as an alternative, the deter-
miner feature could have been included in the dis-
junction with the 〈CREATOR〉 relation (though this
would have been harder to show graphically); how-
ever, it is not necessary to do so, as constraints in the
lexicalized grammar will ensure that the determiner
is not generated together with the possessive.

3 Constructing DLFs from Aligned
Paraphrases

To develop our approach, we use the gold-standard
alignments in Cohn et al.’s (2008) paraphrase cor-
pus. This corpus is constructed from three monolin-
gual sentence-aligned paraphrase subcorpora from
differing text genres, with word-level alignments
provided by two human annotators. We parse each
corpus sentence pair using the OpenCCG parser to
yield a logical form (LF) as a semantic dependency
graph with the gold-standard alignments projected
onto the LF pair. Disjunctive LFs are then con-
structed by inspecting the graph structure of each LF
in comparison with the other. Here, an alignment is
represented simply as a pair 〈n1, n2〉 where n1 is a
node in the first LF and n2 a node in the second LF.
As Cohn et al.’s corpus contains some block align-
ments, there are cases where a single node is aligned

3To be precise, the HLDS logical forms are descriptions of
semantic dependency graphs, which in turn can be interpreted
model theoretically via translation to Discourse Representation
Theory (Kamp and Reyle, 1993), as White (2006b) explains. A
disjunctive logical form is thus a description of a set of seman-
tic dependency graphs. (As the LFs derived using CCGbank
grammars do not represent quantifier scope properly, it would
be more accurate to call them quasi-LFs; as this issue does not
appear to impact the realization or DLF creation algorithms,
however, we have employed the simpler term.)

to multiple nodes in the other sentence of the para-
phrase.

A semantic dependency is represented as graph
G = 〈N, E〉, where N = nodes(G) is the set of
nodes in G and E = edges(G) is the set of edges
in G. An edge e is a labeled dependency between
nodes, with source(e) denoting the source node,
target(e) the target node, and label(e) the relation
e represents. For n, n′ ∈ nodes(G) members of the
set of nodes for some graph G, n′ ∈ parents(n) if
and only if there is an edge e ∈ edges(G) with n′ =
source(e) and n = target(e). The set ancestors(n)
models the transitive closure of the ‘parent-of’ re-
lation: a ∈ ancestors(n) if and only if there is
some p ∈ parents(n) such that either a = p or
a ∈ ancestors(p). Nodes in a graph additionally
bear associated predicates and semantic features that
are derived during the parsing process.

3.1 The Algorithm

As a preprocessing step, we first characterize the dif-
ference between two LFs as a set of edit operations
via MAKEEDITS(g1, g2, alignments), as detailed
in Algorithm 1. An insert results when the second
graph contains an unaligned subgraph. Similarly, an
unaligned subgraph in the first LF is characterized
by a delete operation. For both inserts and deletes,
only the head of the inserted or deleted subgraph is
represented as an edit in order to reflect the fact that
these operations can encompass entire subgraphs. A
substitution occurs when a subgraph in the first LF
is aligned to one or more subgraphs in the second LF.
The case where subgraphs are block aligned corre-
sponds to a multi-word phrasal substitution (for ex-
ample, the substitution of Goldman for The US in-
vestment bank in paraphrase (2), below). The DLF
generation process is then driven by these edit oper-
ations.

DLFs are created for each sentence by DIS-
JUNCTIVIZE(g1, g2, alignments) and DISJUNC-
TIVIZE(g2, g1, alignments), respectively, where
g1 is the first sentence’s LF and g2 the LF of the
second (see Algorithm 2). The DLF construction
process takes as inputs a pair of dependency graphs
〈g1, g2〉 and a set of word-level alignments from
Cohn et al.’s (2008) paraphrase corpus projected
onto the graphs. This process creates a DLF by
merging or making optional material from the sec-

77



Algorithm 1 Preprocesses a pair of aligned LFs representing a paraphrase into edit operations.
1: procedure MAKEEDITS(g1, g2, alignments)
2: for all i ∈ {n ∈ nodes(g2) | ¬∃x.〈x, n〉 ∈ alignments} do . inserts
3: if ¬∃p.p ∈ parents(i) ∧ ¬∃x.〈x, p〉 ∈ alignments then
4: insert(i)

5: for all d ∈ {n ∈ nodes(g1) | ¬∃y.〈n, y〉 ∈ alignments} do . deletes
6: if ¬∃p.p ∈ parents(d) ∧ ¬∃y.〈p, y〉 ∈ alignments then
7: delete(d)

8: for all s ∈ nodes(g1) do . substitutions
9: if ∃y.〈s, y〉 ∈ alignments ∧ ¬∃z.z ∈ parents(y) ∧ 〈s, z〉 ∈ alignments then

10: substitution(s, y)

Algorithm 2 Constructs a disjunctive LF from an aligned paraphrase.
1: procedure DISJUNCTIVIZE(g1, g2, alignments)
2: MAKEEDITS(g1, g2, alignments)
3: for all i ∈ {n ∈ nodes(g2) | insert(n)} do
4: for all p ∈ {e ∈ edges(g2) | i = target(e)} do
5: for all 〈n1, n2〉 ∈ {〈x, y〉 ∈ alignments | y = source(p)} do
6: option(n1, p)

7: for all d ∈ {n ∈ nodes(g1) | delete(n)} do
8: for all p ∈ {e ∈ edges(g1) | d = target(e)} do
9: option(source(p), p)

10: for all s ∈ {n ∈ nodes(g1) | ∃y.substitution(n, y)} do
11: for all p ∈ parents(s) do
12: choice(p, {e ∈ edges(g2) | substitution(s, target(e)) ∧ 〈p, source(e)〉 ∈ alignments})

78



ond LF into the first LF.
As Algorithm 2 describes, first the inserts (line 3)

and deletes (line 7) are handled. In the case of in-
serts, for each node i in the second LF that is the
head of an inserted subgraph, we find every n2 that
is the source of an edge p whose target is i. The
edge p is added as an option for each node n1 in the
first LF that is aligned to n2. The process for deletes
is similar, modulo direction reversal. We find every
edge p whose target is d, where d is the head of an
unaligned subgraph in the first sentence, and make p
an option for the parent node source(p). With both
inserts and deletes, the intuitive idea is that an un-
aligned subgraph should be treated as an optional
dependency from its parent.

The following corpus sentence pair demonstrates
the handling of inserts/deletes:

(1) a. Justices said that the constitution allows
the government to administer drugs only
in limited circumstances.

b. In a 6-3 ruling, the justices said such
anti-psychotic drugs can be used only in
limited circumstances.

In the DLF constructed for (1a), the node represent-
ing the word drugs has two alternate children that
are not present in the first sentence itself (i.e., are in-
serted), such and anti-psychotic, both of which are in
the modifier relation to drugs. This happens because
drugs is aligned to the word drugs in (1b), which has
the modifier child nodes. The second sentence also
contains the insertion In a 6-3 ruling. This entire
subgraph is represented as an optional modifier of
said. Finally, the determiner the is inserted before
justices in the second sentence. This determiner is
also represented as an optional edge from justices.
Figure 2 shows the portion of the DLF reflecting the
optional modifier In a 6-3 ruling and optional deter-
miner the.

For substitutions (line 10), we consider each
subgraph-heading node s in the first LF that is sub-
stituted for some node y in the second LF that is
also a subgraph head. Then for each parent p of s,
the choices for p are contained in the set of edges
whose source is aligned to p and whose target is a
substitution for s. The intuition is that for each node
p in the first LF with an aligned subgraph c, there is a
disjunction between c and the child subgraphs of the

e say〈TENSE〉past

iin

rruling

aa x 6-3

j justices

t the

〈MOD〉

〈ARG1〉

〈DET〉

〈MOD〉

〈ARG0〉

〈DET〉

Figure 2: Disjunctive LF subgraph for the alternation (In
a 6-3 ruling)? (the)? justices said . . . in paraphrase (1).
The dotted lines represent optional edges, and some se-
mantic features are suppressed for readability.

node that p is aligned to in the second LF. For effi-
ciency, in the special case of substitutions involving
single nodes rather than entire subgraphs, only the
semantic predicates are disjoined.4

To demonstrate, consider the following corpus
sentence pair involving a phrasal substitution:

(2) a. The US investment bank said: we be-
lieve the long-term prospects for the en-
ergy sector in the UK remain attractive.

b. We believe the long-term prospects for
the energy sector in the UK remain at-
tractive, Goldman said.

In this paraphrase, the subtree The US investment
bank in (2a) is aligned to the single word Gold-
man in (2b), but their predicates are obviously differ-
ent. The constructed DLF contains a choice between
Goldman and The US investment bank as the subject
of said. Figure 3 illustrates the relevant subgraph of
the DLF constructed from the Goldman paraphrase
with a choice between subjects (〈ARG0〉). This dis-
junction arises because said in the first sentence is
aligned to said in the second, and The US investment
bank is the subject of said in the first while Gold-
man is its subject in the second. Note that, since
the substitution is a phrasal (block-aligned) one, the
constructed DLF forces a choice between Goldman
and the entire subgraph headed by bank, not between

4We leave certain more complex cases, e.g. multiple nodes
with aligned children, for future work.

79



e say〈TENSE〉past

ggoldman b bank

tthe
u us

i inv.

〈ARG0〉 〈ARG0〉

〈DET〉

〈MOD〉

〈MOD〉

Figure 3: Disjunctive LF subgraph for the alternation
(Goldman | The US investment bank) said . . . in para-
phrase (2). The arc represents the two choice edges for
the 〈ARG0〉 relation from say. Certain semantic dependen-
cies are omitted, and the word investment is abbreviated
to save space.

Goldman and each of bank’s dependents (the, US,
and investment).

4 Discussion and Future Work

With a broad coverage grammar, we have found that
most of the realization alternatives in an n-best list
tend to reuse the same lexical choices, with the dif-
ferences mostly consisting of alternate word orders
or use of function words or punctuation. Accord-
ingly, in order to enhance the variety of word and
phrase choices in the n-best lists, we have taken
advantage of the API-level support for plugging in
custom pruning strategies and developed a custom
strategy that encourages lexical diversity. This strat-
egy groups realizations that share the same open
class stems into equivalence classes, where new
equivalence classes are favored over new alterna-
tives within the same equivalence class in filling up
the n-best list.

Using this lexical diversity pruning strategy, an
example of the paraphrases generated after DLF cre-
ation appears in Table 2. In the example, the girl
and brianna are successfully alternated, as are her
mother’s and the (bedroom). The example also in-
cludes a reasonable Heavy-NP shift, with into the
bedroom appearing before the NP list. Without the
lexical diversity pruning strategy, the phrase her
mother’s does not find its way into the n-best list.
The paraphrases also include a mistaken change in
tense from had to has and a mysterious inclusion of
including. Interestingly though, these mistakes fol-

low in the n-best list alternatives that are otherwise
the same, suggesting that a final pruning of the list
may make it possible to keep only generally good
paraphrases. (Note that the appositive 33 in the sec-
ond reference sentence also has been dropped, most
likely since the pruning strategy does not include
numbers in the set of content words at present.)

Although we have not yet formally evaluated the
paraphrases, we can already characterize some re-
curring errors. Named entities are an issue since
we have not incorporated a named entity recognizer;
thus, the realizer is apt to generate O. Charles Prince
instead of Charles O. Prince, for example. Worse,
medical examiner ’s spokeswoman ellen borakove
is realized both correctly and as medical examiner
’s ellen spokeswoman borakove. Naturally, there
are also paraphrasing errors that stem from parser
errors. Certainly with named entities, though per-
haps also with parser errors, we plan to investigate
whether we can take advantage of the word order in
the reference sentence in order to reduce the number
of mistakes. Here, we plan to investigate whether
a feature measuring similarity in word order to the
original can be balanced against the averaged per-
ceptron model score in a way that allows new para-
phrases to be generated while sticking to the orig-
inal order in cases of uncertainty. Initial experi-
ments with adding to the perceptron model score
an n-gram precision score (approximating BLEU)
with an appropriate weight indicate that realizations
including the correct word order in names such as
Charles O. Prince can be pushed to the top of the
n-best list, though it remains to be verified that the
weight for the similarity score can be adequately
tuned with held-out data. Incorporating a measure of
similarity to the original reference sentences into re-
alization ranking is a form of what Madnani (2010)
calls a self-paraphrase bias, though a different one
than his method of adjusting the probability mass as-
signed to the original.

In future work, we plan to evaluate the gener-
ated paraphrases both intrinsically and extrinsically
in combination with MT evaluation metrics. With
the intrinsic evaluation, we expect to examine the
impact of parser and alignment errors on the para-
phrases, and the extent to which these can be miti-
gated by a self-paraphrase bias, along with the im-
pact of the lexical diversity pruning strategy on the

80



Reference 1 lee said brianna had dragged food , toys and other things into the bedroom .
Realizations lee said the girl had dragged food , toys and other things into the bedroom .

lee said brianna had dragged food , toys and other things into the bedroom .
lee said , the girl had dragged [into the bedroom] food , toys and other things .
lee said the girl has dragged into the bedroom food , toys and other things .
lee said , brianna had dragged into the bedroom food , toys and other things .
lee said the girl had dragged food , toys and other things into her mother ’s bedroom .
lee said , the girl had dragged into her mother ’s bedroom food , toys and other things .
lee said brianna had dragged food , toys and other things into her mother ’s bedroom .
lee said the girl had dragged food , toys and other things into including the bedroom .
lee said , brianna had dragged into her mother ’s bedroom food , toys and other things .

Reference 2 lee , 33 , said the girl had dragged the food , toys and other things into her mother ’s bedroom .
Realizations lee said the girl had dragged [into the bedroom] the food , toys and other things .

lee said , the girl had dragged into the bedroom the food , toys and other things .
lee said the girl has dragged into the bedroom the food , toys and other things .
lee said brianna had dragged the food , toys and other things into the bedroom .
lee said , brianna had dragged into the bedroom the food , toys and other things .
lee said the girl had dragged the food , toys and other things into her mother ’s bedroom .
lee said brianna had dragged into her mother ’s bedroom the food , toys and other things .
lee said , the girl had dragged into her mother ’s bedroom the food , toys and other things .
lee said brianna had dragged the food , toys and other things into her mother ’s bedroom .
lee said the girl had dragged the food , toys and other things into including the bedroom .

Table 2: Example n-best realizations starting from each reference sentence. Alternative phrasings from the other
member of the pair are shown in italics the first time, and alternative phrase locations are shown in square brackets.
Mistakes are underlined, and suppressed after the first occurrence in the list.

number of acceptable paraphrases in the n-best list.

With the extrinsic evaluation, we plan to investi-
gate whether n-best paraphrase generation using the
methods described here can be used to augment a
set of reference translations in such a way as to in-
crease the correlation of automatic metrics with hu-
man judgments. As Madnani observes, generated
paraphrases of reference translations may be either
untargeted or targeted to specific MT hypotheses.
In the case of targeted paraphrases, the generated
paraphrases then approximate the process by which
automatic translations are evaluated using HTER
(Snover et al., 2006), with a human in the loop, as
the closest acceptable paraphrase of a reference sen-
tence should correspond to the version of the MT
hypothesis with minimal changes to make it accept-
able. While in principle we might similarly acquire
paraphrase rules using the pivot method, as in Mad-
nani’s approach, such rules would be quite noisy, as
it is a difficult problem to characterize the contexts
in which words or phrases can be acceptably substi-
tuted. Thus, our immediate focus will be on gen-
erating synthetic references with high precision, re-

lying on grammatical alternations plus contextually
acceptable alternatives present in multiple reference
translations, given that metrics such as METEOR
(Banerjee and Lavie, 2005) and TERp (Snover et al.,
2010) can now employ paraphrase matching as part
of their scoring, complementing what can be done
with our methods. To the extent that we can main-
tain high precision in generating synthetic reference
sentences, we may expect the correlations between
automatic metric scores and human judgments to
improve as the task of the metrics becomes simpler.

Acknowledgements

This work was supported in part by NSF grant num-
ber IIS-0812297. We are also grateful to Trevor
Cohn for help with the paraphrase data.

References

Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.

Jason Baldridge. 2002. Lexically Specified Derivational

81



Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL-05 Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.

Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc.
ACL-05, pages 597–604.

Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of NAACL-HLT.

Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297–327.

Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.

Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493–552.

Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597–614.

M. Elhadad, J. Robin, and K. McKeown. 1997. Floating
constraints in lexical choice. Computational Linguis-
tics, 23(2):195–239.

Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183–191, Columbus, Ohio, June. Association
for Computational Linguistics.

Mary Ellen Foster and Michael White. 2007. Avoiding
repetition in generated text. In Proceedings of the 11th
European Workshop on Natural Language Generation
(ENLG 2007).

Claire Gardent and Eric Kow. 2005. Generating and se-
lecting grammatical paraphrases. In Proc. ENLG-05.

Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of ACL, pages 923–931,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.

Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proc. ACL-02.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.

Deirdre Hogan, Jennifer Foster, Joachim Wagner, and
Josef van Genabith. 2008. Parser-based retraining
for domain adaptation of probabilistic generators. In
Proc. INLG-08.

Lidija Iordanskaja, Richard Kittredge, and Alain
Polgúere. 1991. Lexical selection and paraphrase
in a meaning-text generation model. In Cécile L.
Paris, William R. Swartout, and William C. Mann, edi-
tors, Natural Language Generation in Artificial Intelli-
gence and Computational Linguistics, pages 293–312.
Kluwer.

Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.

David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of HLT-
NAACL.

Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde and Kevin Knight. 1998. The practical

value of n-grams in generation. In Proc. INLG-98.
Irene Langkilde-Geary. 2002. An empirical verification

of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.

Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 802–811, Honolulu, Hawaii,
October. Association for Computational Linguistics.

Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.

Crystal Nakatsu and Michael White. 2006. Learning to
say it well: Reranking realizations by predicted syn-
thesis quality. In Proc. COLING-ACL-06.

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 1(29):19–52.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.

Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proc. HLT/NAACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL-02.

Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proc. ACL-04.

82



Hadar Shemtov. 1997. Ambiguity Management in Natu-
ral Language Generation. Ph.D. thesis, Stanford Uni-
versity.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of the Association for Machine Translation in
the Americas (AMTA-06).

Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. TER-plus: Paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23:117–127.

M. Stede. 1999. Lexical Semantics and Knowledge Rep-
resentation in Multilingual Text Generation. Kluwer
Academic Publishers.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories.

Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410–419, Singapore,
August. Association for Computational Linguistics.

Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proc. INLG-04.

Michael White. 2006a. CCG chart realization from dis-
junctive logical forms. In Proc. INLG-06.

Michael White. 2006b. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language & Computation, 4(1):39–75.

Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 834–842, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.

83


