










































Chinese Spelling Checker Based on Statistical Machine Translation


Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 49â€“53,
Nagoya, Japan, 14 October 2013.

Chinese Spelling Checker Based on Statistical Machine Translation 

 
 

Hsun-wen Chiu Jian-cheng Wu Jason S. Chang 
Department of Institute of Information Systems and Applications 

National Tsing Hua University 
{chiuhsunwen, wujc86, jason.jschang}@gmail.com 

 
  

 

Abstract 

Chinese spelling check is an important com-
ponent for many NLP applications, including 
word processor and search engines. However, 
compared to checkers for alphabetical lan-
guages (e.g., English or French), Chinese 
spelling checkers are more difficult to develop, 
because there are no word boundaries in Chi-
nese writing system, and errors may be caused 
by various Chinese input methods. In this pa-
per, we proposed a novel method to Chinese 
spelling checking. Our approach involves error 
detection and correction based on the phrasal 
statistical machine translation framework. The 
results show that the proposed system achieves 
significantly better accuracy in error detecting 
and more satisfactory performance in error 
correcting. 

1 Introduction 
Chinese spelling check is a task involving auto-
matically detecting and correcting typos, roughly 
corresponding to misspelled words in English. 
Liu et al. (2011) show that people tend to unin-
tentionally generate typos that sound similar (e.g., 
â€œ*æªæŠ˜ cuo zheâ€ and â€œæŒ«æŠ˜ cuo zheâ€), or look 
similar (e.g., â€œ*å›ºé›£ gu nanâ€ and â€œå›°é›£ kun 
nanâ€). On the other hand, some typos found on 
the Web (such as forums or blogs) are used de-
liberately for the purpose of speed typing or just 
for fun. Therefore, spelling check is an important 
component for many applications such as com-
puter-aided writing and corpus cleanup.  

The methods of spelling check can be broadly 
classified into two types: rule-based methods 
(Ren et al., 2001; Jiang et al., 2012) and statisti-
cal methods (Hung and Wu, 2009; Chen and Wu, 
2010). Rule-based methods use knowledge re-
sources such as a dictionary to identify a word as 
a typo if the word is not in the dictionary, and 
provide similar words in the dictionary as sug-

gestions. However, simple rule-based methods 
have their limitations. Consider the sentence â€œå¿ƒ
æ˜¯å¾ˆé‡è¦çš„ã€‚ xin shi hen zhong yao deâ€ which 
is correct. However, the two single-character 
words â€œå¿ƒ xinâ€ and â€œæ˜¯ shiâ€ are likely to be re-
garded as an error by a rule-based model for the 
longer word â€œå¿ƒäº‹ xin shiâ€ with identical pro-
nunciation. 

Data driven, statistical spelling check ap-
proaches appear to be more robust and performs 
better. Statistical methods tend to use a large 
monolingual corpus to create a language model 
to validate the correction hypotheses. Consider-
ing â€œå¿ƒæ˜¯ xin shiâ€, the two characters â€œå¿ƒ xinâ€ 
and â€œæ˜¯ shiâ€ are a bigram which has high fre-
quency in a monolingual corpus, so we may de-
termine that â€œå¿ƒæ˜¯ xin shiâ€ is not a typo after all. 

In this paper, we propose a model, which 
combines rule-based with statistical approaches 
to detect errors and generate the most appropriate 
corrections in Chinese text. Once, an error is 
identified by the rule-based detection model, we 
use statistic machine translation (SMT) model 
(Koehn, 2010) to provide the most appropriate 
correction. Rule-based models tend to ignore 
context, so that we use SMT to deal with this 
problem. Our model treats spelling correction as 
a kind of translation, where typos are translated 
into correctly spelled words according to the 
translation probability and language model prob-
ability. Consider the same case â€œå¿ƒæ˜¯å¾ˆé‡è¦çš„ã€‚ 
xin shi hen zhong yao deâ€. The string â€œå¿ƒæ˜¯ xin 
shiâ€ will not be incorrectly replaced with â€œå¿ƒäº‹ 
xin shiâ€ because we would consider â€œå¿ƒæ˜¯ xin 
shiâ€ is highly probable according to the language 
model. 

The rest of the paper is organized as follows. 
We present the related work in the next section. 
Then we describe the proposed model for auto-
matically detecting the spelling errors and cor-
recting the found errors in Section 3. Section 4 

49



and Section 5 present the experimental data and 
evaluation results. And we conclude in Section 6. 

2 Related Work 
Chinese spelling check is a task involving auto-
matically detecting and correcting typos in a giv-
en Chinese sentence. Previous work typically 
takes the approach of combining a confusion set 
and a language model. Rule-based approach de-
pends on dictionary knowledge and a confusion 
set, a collection set of a certain character consists 
of visually and phonologically similar characters. 
On the other hand, statistical-based methods usu-
ally use a language model, which is generated 
from a reference corpus. Statistical language 
model assigns a probability to a sentence of 
words by means of ngram probability to compute 
the likelihood of a corrected sentence. 

Chang (1995) proposed a system that replaces 
each character in the sentence based on the con-
fusion set and estimates the probability of all 
modified sentences according to a bigram lan-
guage model built from a newspaper corpus, then 
comparing the probability before and after sub-
stitution. They used a confusion set consists of 
pairs of character with similar shape that are col-
lected by comparing the original text and its 
OCR results. Similarly, Zhuang et al. (2004) 
proposed an effective approach using OCR to 
recognize possible confusion set. In addition, 
Zhuang et al. (2004) also used a multi-
knowledge based statistical language model, the 
n-gram language model, and Latent Semantic 
Analysis. However, the experiments by Zhuang 
et al. (2004) seem to show that the simple n-gram 
model performs the best. 

In recent years, Chinese spelling checkers 
have incorporated word segmentation. The 
method proposed by Huang et al. (2007) incorpo-
rates Sinica Word Segmentation System (Ma and 
Chen, 2003) to detect typos. With a character-
based bigram language model and the rule-based 
methods of dictionary knowledge and confusion 
set, the method determines whether the word is a 
typo or not. There are many more systems that 
use word segmentation to detect errors. For ex-
ample, in Hung and Wu (2009), the given sen-
tence is segmented using a bigram language 
model. In addition, the method also uses confu-
sion set and common error templates manually 
edited and provided by Ministry of Education in 
Taiwan. Chen and Wu (2010) modified the sys-
tem proposed by Hung and Wu (2009), by com-
bining statistic-based methods and a template 

matching module generated automatically to de-
tect and correct typos based on language model. 

In a work closer to our method, Wu et al. 
(2010) adopts the noise channel model, a frame-
work used both in spell checkers and machine 
translation systems. The system combined statis-
tic-based method and template matching with the 
help of a dictionary and a confusion set. They 
also used word segmentation to detect errors, but 
they did not use an existing word segmentation 
as Huang et al. (2007) did, because it might re-
gard a typo as a new word. They used a back-
ward longest first approach to segment sentences 
with an online dictionary sponsored by MOE, 
and a templates with a confusion set. The system 
also treat Chinese spelling check as a kind of 
translation, they combine the template module 
and translation module to get a higher precision 
or recall. 

In our system, we also treat Chinese spelling 
checking problem as machine translation like Wu 
et al. (2010), with a different way of handling 
word segmentation to detect typos and transla-
tion model where typos are translated into cor-
rectly spelled words. 

3 Method 
In this section, we describe our solution to the 
problem of Chinese spelling check. In the error 
detection phase, the given Chinese sentence is 
segmented into words. (Section 3.1) The detec-
tion module then identifies and marks the words, 
which may be typos. (Section 3.2) In the error 
correction phase, we use a statistical machine 
translation (SMT) model to translate the sen-
tences containing typos into correct ones (Sec-
tion 3.3). In the rest of this section, we describe 
our solution to this problem in more details. 

3.1 Modified Chinese Word Segmentation 
System 

Unlike English text in which sentences are se-
quences of words delimited by spaces, Chinese 
texts are represented as strings of Chinese char-
acters (called Hanzi) with word delimiters. 
Therefore, word segmentation is a pre-processing 
step required for many Chinese NLP applications. 
In this study, we also perform word segment to 
reduce the search space and the probability of 
false alarm. After segmentation, sequences of 
two or more singleton words are considered like-
ly to contain an error. However, over-segmented 
might lead to falsely identified errors, which we 
will describe in Section 3.2. Considering the sen- 

50



Replaced character æ°£ ä»½ 
Translations æ±½ä»½   æ³£ä»½ 

å™¨ä»½   å¥‘ä»½ 
ä¼ä»½   æ†©ä»½ 

æ°£åˆ†   æ°£å¿¿ 
æ°£æ†¤   æ°£ç³ 
æ°£å¥®   æ°£æ°› 

Table 1. Sample â€œtranslationsâ€ for â€œæ°£ä»½ qi fenâ€. 
 

tence â€œé™¤äº†è¦æœ‰è¶…ä¸–ä¹‹æ‰ï¼Œä¹Ÿè¦æœ‰å …å®šçš„æ„å¿— 
chu le yao you chao shi zhi cai, ye yao you jian 
ding de yi zhiâ€, the sentence is segmented into 
â€œé™¤äº†/è¦/æœ‰/è¶…ä¸–/ä¹‹/æ‰/ï¼Œ/ä¹Ÿ/è¦/æœ‰/å …å®š/çš„/æ„
å¿—.â€ The part â€œè¶…ä¸–ä¹‹æ‰ chao shi zhi caiâ€ of the 
sentence is over-segmented and runs the risk of 
being identified as containing a typo. To solve 
the problem of over-segmentation, we used addi-
tional lexicon items and reduce the chance of 
generating false alarms. 

3.2 Error Detection 
Motivated by the observation that a typo often 

causes over-segmentation in the form of a se-
quence of single-character words, so we target 
the sequences of single-character words as can-
didates for typos. To identify the points of typos, 
we take all n-grams consist of single-character 
words in the segmented sentence into considera-
tion. In addition to a Chinese dictionary, we also 
include a list of web-based ngrams to reduce the 
false alarm due to the limited coverage of the 
dictionary. 

When a sequence of singleton word is not 
found in the dictionary, or in the web-based 
character ngrams, we regard the ngram as con-
taining a typo. For example, â€œæ£®æ— çš„ èŠ³ å¤š ç²¾ 
sen lin de fang duo jingâ€ is segmented into con-
secutive singleton words: bigrams such as â€œçš„ èŠ³ 
de fangâ€, and â€œèŠ³ å¤š fang duoâ€ and trigrams 
such as â€œçš„ èŠ³ å¤š de fang duoâ€ and â€œèŠ³ å¤š ç²¾ 
fang duo jingâ€ are all considered as candidates 
for typos since those ngrams are not found in the 
reference list. 

3.3 Error Correction 
Once we generate a list of candidates of typos, 

we attempt to correct typos, using a statistical 
machine translation model to translate typos into 
correct word. When given a candidate, we first 
generate all correction hypotheses by replacing 
each character of the candidate typo with similar 
characters, one character at a time.  

Take the candidate â€œæ°£ä»½ qi fenâ€ as example, 
the model generates all translation hypotheses, 
according to a visually and phonologically conf- 

Translations Freq. LM prob. tp 
æ°£æ†¤ 
æ°£æ°› 

48 
473 

-4.96 
-3.22 

-1.20 
-1.11 

Table 2. Translations for â€œæ°£ä»½ qi fenâ€. 
 
usion set. Table 1 shows some translation hy-
potheses. The translation hypotheses are then 
validated (or pruned from the viewpoint of SMT) 
using the dictionary. 

The translation probability tp is a probability 
indicates how likely a typo is translated into a 
correct word. tp of each correction translation is 
calculated using the following formula: 
 

ğ‘¡ğ‘ = log!"(
ğ‘“ğ‘Ÿğ‘’ğ‘(ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ )

ğ‘“ğ‘Ÿğ‘’ğ‘(ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ) âˆ’ ğ‘“ğ‘Ÿğ‘’ğ‘(ğ‘ğ‘ğ‘›ğ‘‘ğ‘–)
) âˆ— ğ›¾ 

 
where freq(trans) and freq(candi) are the fre-
quency of the translation and the candidate cor-
respondingly, and Î³ is the weight of different 
error types: visual or phonological. tp is set to 0 
if freq(trans) = 0. 

Take â€œæ°£ä»½ qi fenâ€ from â€œä¸/ä¸€æ¨£/çš„/æ°£/ä»½ 
bu/yi yang/de/qi/fenâ€ for instance, the transla-
tions with non-zero tp after filtering are shown in 
Table 2. Only two translations are possible for 
this candidate: â€œæ°£æ†¤ qi fenâ€ and â€œæ°£æ°› qi fenâ€. 

We use a simple, publicly available decoder 
written in Python to correct potential spelling 
errors found in the detection module. The de-
coder reads in a Chinese sentence at a time and 
attempts to â€œtranslateâ€ the sentence into a cor-
rectly spelled one. The decoder translates mono-
tonically without reordering the Chinese words 
and phrases using two models â€” translation 
probability model and the language model. These 
two models read from a data directory containing 
two text files containing a translation model in 
GIZA++ (Och and Ney, 2003) format, and a lan-
guage model in SRILM (Stolcke et al., 2011) 
format. These two models are stored in memory 
for quick access. 

The decoder invokes the two modules to load 
the translation and language models and decodes 
the input sentences, storing the result in output. 
The decoder computes the probability of the out-
put sentences according to the models. It works 
by summing over all possible ways that the 
model could have generated the corrected sen-
tence from the input sentence. Although in gen-
eral covering all possible corrections in the trans-
lation and language models is intractable, but a 
majority of error instances can be â€œtranslatedâ€ 

51



effectively by using the translation model and the 
language model. 

4 Experimental Setting 
To train our model, we used several corpora in-
cluding Sinica Chinese Balanced Corpus, 
TWWaC (Taiwan Web as Corpus), a Chinese 
dictionary, and a confusion set. We describe the 
data sets in more detail below. 

Sinica Corpus 
"Academia Sinica Balanced Corpus of Modern 
Chinese", or Sinica Corpus for short, is the first 
balanced Chinese corpus with part-of-speech 
tags (Huang et al., 1996). Current size of the cor-
pus is about 5 million words. Texts are seg-
mented according to the word segmentation 
standard proposed by the ROC Computational 
Linguistic Society. We use the corpus to generate 
the frequency of bigram, trigram and 4-gram for 
training translation model and to train the n-gram 
language model. 

TWWaC (Taiwan Web as Corpus) 
We use TWWaC for obtaining more language 
information. TWWaC is a corpus gathered from 
the Web under the .tw domain, containing 
1,817,260 Web pages, 30 billions Chinese char-
acters. We use the corpus to generate the fre-
quency of all character n-grams for n = 2, 3, 4 
(with frequency higher than 10). 

Words and Idioms in a Chinese Dictionary 
From the dictionaries and related books pub-
lished by Ministry of Education (MOE) of Tai-
wan, we obtained two lists, one is the list of 
64,326 distinct Chinese words1, and the other 
one is the list of 48,030 distinct Chinese idioms2. 
We combine the lists into a Chinese dictionary 
for validating words with lengths of 2 to 17 char-
acters. 

Confusion Set 
After analyzing erroneous Chinese word, Liu et 
al. (2011) found that more than 70% of typos 
were related to the phonologically similar char-
acter, about 50% are morphologically similar and 
almost 30% are both phonologically and mor-
phologically similar. We use the ratio as the 
weight for the translation probabilities. In this 
study, we used two confusion sets generated by 
Liu et al. (2011) and provided by SIGHAN 7 

                                                
1 Chinese Dictionary 
http://www.edu.tw/files/site_content/m0001/pin/yu7.htm?op
en 
2 Idioms http://dict.idioms.moe.edu.tw/cydic/index.htm 

Bake-off 2013: Chinese Spelling Check Shared 
Task as a full confusion set based on loosely 
similar relation. 

In order to improve the performance, we ex-
panded the sets slightly and also removed   some 
loosely similarly relations. For example, we re-
moved all relations based on non-identical pho-
nologically similarity. After that, we added the 
similar characters based on similar phonemes in 
Chinese phonetics, such as â€œã„£ï¼Œã„¥ en, engâ€, 
â€œã„¤ï¼Œã„¢ ang, anâ€, â€œã„•ï¼Œã„™ shi, siâ€ and so on. 
We also modify the similar shape set to a more 
strongly similar set. The characters are checked 
automatically by comparing corresponding 
Cangjie code (å€‰é ¡ç¢¼). Two characters which 
differ from each other by at most one symbol in 
Cangjie code are considered as strongly similar 
and are retained. For example, the code of â€œå¾µ 
zhengâ€ and â€œå¾®  weiâ€ are strongly similar in 
shape, since in their corresponding codes â€œç«¹äºº
å±±åœŸå¤§â€ and â€œç«¹äººå±±å±±å¤§â€, differ only in one 
place. 

5 Evaluation Results 
In Bake-off 2013, the evaluation includes two 
sub-tasks: error detection and error correction. 
For the error detection, sub-task1, there are 1000 
sentences with/without spelling errors. And sub-
task2 for the error correction, there are also con-
taining 1000 sentences but all with errors. The 
evaluation metrics, which computes false-alarm 
rate, accuracy, precision, recall, and F-Score is 
provided by SIGHAN 7 Bake-off 2013: Chinese 
Spelling Check Shared Task. In this paper, we 
describe Run3 system and results. 

On sub-task1, evaluation results as follows: 
 

Evaluation metrics Score 
False-Alarm Rate   
Detection Accuracy  
Detection Precision  
Detection Recall 
Detection F-Score  
Error Location Accuracy  
Error Location Precision  
Error Location Recall 
Error Location F-Score  

0.0514 
0.861 

0.8455 
0.6567 
0.7392 

0.82 
0.6695 

0.52 
0.5854 

Table 3. Evaluation metrics of Sub-task1. 
 

We obtain higher detection accuracy, error loca-
tion accuracy, and error location F-Score, which 
put our system in first place among 13 systems 
evaluated. On sub-task2, our system obtained 

52



location accuracy, correction accuracy, and cor-
rection precision of 0.454, 0.443, and 0.6998, 
respectively. 

6 Conclusions and Future Work 
Many avenues exist for future research and im-
provement of our system. For example, new 
terms can be automatically discovered and added 
to the Chinese dictionary to improve both detec-
tion and correction performance. Part of speech 
tagging can be performed to provide more in-
formation for error detection. Named entities can 
be recognized in order to avoid false alarms. Su-
pervised statistical classifier can be used to 
model translation probability more accurately. 
Additionally, an interesting direction to explore 
is using Web ngrams in addition to a Chinese 
dictionary for correcting typos. Yet another di-
rection of research would be to consider errors 
related to a missing or redundant character.  

In summary, we have introduced in this paper, 
we proposed a novel method for Chinese spelling 
check. Our approach involves error detection and 
correction based on the phrasal statistical ma-
chine translation framework. The error detection 
module detects errors by segmenting words and 
checking word and phrase frequency based on a 
compiled dictionary and Web corpora. The pho-
nological or morphological spelling errors found 
are then corrected by running a decoder based on 
statistical machine translation model (SMT). The 
results show that the proposed system achieves 
significantly better accuracy in error detecting 
and the evaluation results show that the method 
outperforms other system in Chinese Spelling 
Check Shared Task. 

References  
Chao-Huang Chang. 1995. A new approach for auto-

matic Chinese spelling correction. In Proceedings 
of Natural Language Processing Pacific Rim Sym-
posium, pp. 278-283.  

Yong-Zhi Chen and Shih-Hung Wu. 2010. Improve 
the detection of improperly used Chinese charac-
ters with noisy channel model and detection tem-
plate. Master thesis, Chaoyang University of Tech-
nology. 

Chu-Ren Huang, Keh-jiann Chen and Li-Li Chang. 
1996. Segmentation standard for Chinese natural 
language processing. Proceedings of the 1996 In-
ternational Conference on Computational Linguis-
tics (COLING 96), Vol. 2, pp. 1045-1048. 

Chuen-Min Huang, Mei-Chen Wu and Chang Ching-
Che. 2007. Error detection and correction based on 

Chinese phonemic alphabet in Chinese text. Pro-
ceedings of the 4th International Conference on 
Modeling Decisions for Artificial Intelligence 
(MDAI IV), pp. 463-476. 

Ta-Hung Hung and Shih-Hung Wu. 2009. Automatic 
Chinese character error detecting system based on 
n-gram language model and pragmatics knowledge 
base. Master thesis, Chaoyang University of Tech-
nology. 

Ying Jiang, Tong Wang, Tao Lin, Fangjie Wang, 
Wenting Cheng, Xiaofei Liu, Chenghui Wang and 
Weijian Zhang. 2012. A rule based Chinese 
spelling and grammar detection system utility. 
2012 International Conference on System Science 
and Engineering (ICSSE), pp. 437-440. 

Philipp Koehn. 2010. Statistical Machine Translation. 
United Kingdom: Cambridge University Press. 

Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, Yi-
Hsuan Chuang, Shih-Hung Wu and Chia-Ying Lee. 
2011. Visually and phonologically similar charac-
ters in incorrect Chinese words: Analyses, identifi-
cation, and applications. ACM Trans. Asian Lang, 
Inform. Process. 10, 2, Article 10, pp. 39. 

Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction 
to CKIP Chinese word segmentation system for the 
first international Chinese Word Segmentation 
Bakeoff. Proceedings of ACL, Second SIGHAN 
Workshop on Chinese Language Processing, Vol. 
17, pp. 168-171. 

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, Vol. 29, num-
ber 1, pp. 19-51. 

Fuji Ren, Hongchi Shi and Qiang Zhou. 2001. A hy-
brid approach to automatic Chinese text checking 
and error correction. 2001 IEEE International Con-
ference on Systems, Man, and Cybernetics, Vol. 3, 
pp. 1693-1698. 

Andreas Stolcke, Jing Zheng, Wen Wang and Victor 
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of IEEE Automatic Speech 
Recognition and Understanding Workshop. 

Shih-Hung Wu, Yong-Zhi Chen, Ping-che Yang, 
Tsun Ku and Chao-Lin Liu. 2010. Reducing the 
false alarm rate of Chinese character error detec-
tion and correction. Proceedings of CIPS-SIGHAN 
Joint Conference on Chinese Language Processing 
(CLP 2010), pp. 54-61. 

Li Zhuang, Ta Bao, Xiaoyan Zhu, Chunheng Wang 
and Satoshi Naoi. 2004. A Chinese OCR spelling 
check approach based on statistical language mod-
els. 2004 IEEE International Conference on Sys-
tems, Man and Cybernetics, Vol. 5, pp. 4727-4732. 

53


