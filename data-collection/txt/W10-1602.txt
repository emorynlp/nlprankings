










































Data-driven computational linguistics at FaMAF-UNC, Argentina


Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 8–14, Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Data-driven computational linguistics at FaMAF-UNC, Argentina

Laura Alonso i Alemany and Gabriel Infante-Lopez
Grupo de Procesamiento de Lenguaje Natural

Sección de Ciencias de la Computación
Facultad de Matemática, Astronomı́a y Fı́sica

Universidad Nacional de Córdoba
Córdoba, Argentina

{gabriel|alemany}@famaf.unc.edu.ar

Abstract

This paper provides a survey of some on-
going research projects in computational lin-
guistics within the group of Natural Language
Processing at the University of Córdoba, Ar-
gentina. We outline our future plans and spot-
light some opportunities for collaboration.

1 Introduction

In this paper we present our group, describe its
members, research agenda, interests and possible
collaboration opportunities. The research agenda
of the NLP group contains diverse lines of work.
As a group, we have a special interest in produc-
ing language technologies for our languages, at a
level comparable in performance with the state-of-
the-art technology for English. We are developing
such technology by deeply understanding its under-
ling models and either adapting them to our lan-
guages or by creating new ones.

In this paper we present only those related to Nat-
ural Language Parsing and data-driven characterisa-
tion of linguistic phenomena. For both lines we pro-
vide a small survey of our results so far, we describe
our current research questions and we spotlight pos-
sible opportunities of collaboration.

The paper is organized as follows. The follow-
ing Section describes the group, its composition,
projects and goals. Section 3 briefly introduces the
research agenda related to natural language pars-
ing and structure finding. Section 4 sketches the
work on data-driven characterisation of linguistic
phenomena in three main parts: semi-structured text

mining, characterisation of verbal behaviour and
mining of relations in biomedical text. Finally, Sec-
tion 5 presents outlines our overall vision for collab-
oration with other researchers in the Americas.

2 Description of the group

The NLP Group1 is part of the Computer Science
section at the Facultad de Matemática, Astronomı́a
y Fı́sica, at the Universidad Nacional de Córdoba.
The group was started in 2005, with two full time re-
searchers who had just got their doctorate degree in
Amsterdam and Barcelona. Then, in 2009 and 2010
three more full-time researchers joined the group,
coming from the Universities of Geneva and Nancy.

As of 2010, the group has 5 faculty researchers,
4 PhD students and several undergraduate students.
The computer science section has around 20 mem-
bers – including the NLP group, faculty members
and PhD students.

The faculty researchers are, by alphabetical order:

• Laura Alonso Alemany, working in text mining
and data-driven systematization of language.

• Carlos Areces, investigating different reason-
ing tasks and their applications in natural lan-
guage processing.

• Luciana Benotti, investigates the addition of
pragmatic abilities into dialogue systems.

• Paula Estrella, working in Machine Transla-
tion.

• Gabriel Infante-Lopez, working on Natural
Language Parsing and Structure Finding.

1http://www.cs.famaf.unc.edu.ar/˜pln/

8



One of the main aims of the group has been ed-
ucation, both at undergraduate and graduate lev-
els. Computer Science is an under-developed area
in Argentina, and Natural Language Processing even
more so. When the group was created, there were
very few NLP researchers in the country, and they
worked in isolation, with little connection to other
researchers from neighbouring countries. One of
the strategic goals of our University and of the NLP
group itself were to create a critical mass of re-
searchers in NLP. To that aim, we worked on in-
corporating researchers to our group and establish-
ing relations with other groups. Researchers were
incorporated via special programmes from both the
Faculty and the Argentinean Government to increase
the number of doctors in Computer Science in the
scientific system in Argentina.

Most of our efforts in the first years went to raise
awareness about the area and provide foundational
and advanced courses. This policy lead to a signifi-
cant number of graduation theses2 and to the incor-
poration of various PhD students to our group.

We taught several undergraduate and graduate
courses on various NLP topics at our own Univer-
sity, at the University of Rı́o Cuarto, at the Univer-
sity of Buenos Aires and at the Universidad de la
República (Uruguay), as well as crash courses at the
Society for Operative Investigations (SADIO) and
at the Conferencia Latinoamericana de Informática
(CLEI 2008). We also gave several talks at vari-
ous universities in the country, and participated in
local events, like JALIMI’05 (Jornadas Argentinas
de Lingüı́stica Informática: Modelización e Inge-
nierı́a) or the Argentinean Symposium on Artificial
Intelligence.

Since the beginning of its activities, the group
has received funding for two major basic research
projects, funded by the Argentinean Agency for the
Development of Science and Technology. A third
such project is pending approval.

We have a special interest in establishing work-
ing relations and strengthening the synergies with
the research community in NLP, both within South
America and the rest of the world. We have had sci-
entific and teaching exchanges with the NLP group

2http://cs.famaf.unc.edu.ar/˜pln/
Investigacion/tesis_grado/tesis_grado.html

in Montevideo, Uruguay. From that collaboration,
the Microbio project emerged3, bringing together
researchers on NLP from Chile, Brazil, Uruguay,
France and Argentina. This project was funded
by each country’s scientific institutions (MinCyT,
in the case of Argentina) within STIC-AmSud4,
a scientific-technological cooperation programme
aimed to promote and strengthen South America re-
gional capacities and their cooperation with France
in the area of Information Technologies and Com-
munication. Within this project, we hosted the kick-
off workshop on February 2008, with attendants rep-
resenting all groups in the project.

We have also had billateral international cooper-
ation in some smaller projects. Together with the
CNR-INRIA in Rennes, France, we have worked in
a project concerning the smallest grammar problem.
We tackle the same problem, finding small gram-
mars in two different domains: ADN sequences
and Natural Language sentences. In collaboration
with several universities in Spain (UB, UOC, UPC,
EHU/UPV), we have taken part in the major basic
research programme KNOW5, aiming to aggregate
meaning, knowledge and reasoning to current infor-
mation technologies. This project has now received
funding to carry on a continuating project6.

Moreover, we are putting forward some propos-
als for further international collaboration. Follow-
ing the path opened by the Microbio project, we
are working on a proposal to the Ecos Sud pro-
gramme for joint collaboration with research teams
in France7.

We are also working in strengthening relations
within Argentinean NLP groups. To that aim, we are
collaborating with the NLP group at the University
of Buenos Aires in the organization of the School
on Computational Linguistics ELiC8, with several
grants for students sponsored by NAACL. We are
also putting forward a proposal for a workshop on

3http://www.microbioamsud.net/
4http://www.sticamsud.org/
5KNOW project: http://ixa.si.ehu.es/know.
6Representation of Semantic Knowledge, TIN2009-14715-

C04-03 (Plan Nacional de I+D+i 2008-2011).
7ECOS-SUD programme: http://www.mincyt.gov.

ar/coopinter_archivos/bilateral/francia.
htm.

8ELiC school on Computational Linguistics: http://
www.glyc.dc.uba.ar/elic2010/.

9



NLP to be co-located with the IBERAMIA confer-
ence on Artificial Intelligence, to be held at Bahı́a
Blanca on November 2010.

3 Natural Language Parsing and
Structure Finding

3.1 Unsupervised Parsing
Unsupervised parsing of Natural Language Syntax
is a key technology for the development of lan-
guage technology. It is specially important for lan-
guages that have either small treebanks or none at
all. Clearly, there is a big difference between pro-
ducing or using a treebank for evaluation and pro-
ducing or using them for training. In the former
case, the size of the treebank can be significantly
smaller. In our group, we have investigated differ-
ent approaches to unsupervised learning of natural
language. and we are currently following two dif-
ferent lines, one that aims at characterizing the po-
tential of a grammar formalism to learn a given tree-
bank structure and a second that uses only regular
automata to learn syntax.

Characterization of Structures In (Luque and
Infante-Lopez, 2009) we present a rather unusual
result for language learning. We show an upper
bound for the performance of a class of languages
when a grammar from that class is used to parse
the sentences in any given treebank. The class of
languages we studied is the defined by Unambigu-
ous Non-Terminally Separated (UNTS) grammars
(Clark, 2006). UNTS grammars are interesting be-
cause, first, they have nice learnability properties
like PAC learnability (Clark, 2006), and, second,
they are used as the background formalism that won
the Omphalos competition (Clark, 2007). Our strat-
egy consists on characterizing all possible ways of
parsing all the sentences in a treebank using UNTS
grammars, then, we find the one that is closest to the
treebank. We show that, in contrast to the results ob-
tained for learning formal languages, UNTS are not
capable of producing structures that score as state-
of-the-art models on the treebanks we experimented
with.

Our results are for a particular, very specific type
of grammar. We are currently exploring how to
widen our technique to provide upper bounds to a
more general class of languages. Our technique does

not state how to actually produce a grammar that
performs as well as the upper bound, but it can be
useful for determining how to transform the training
material to make upper bounds go up. In particu-
lar we have defined a generalization of UNTS gram-
mars, called k-l-UNTS grammars, that transform a
word w in the training material in a 3-uple 〈α, w, β〉
where α contains the k previous symbols to w and
β contains the l symbols following w. Intuitively, k-
l-UNTS augments each word with a variable length
context. It turns out that the resulting class of lan-
guages is more general than UNTS grammars: they
are PAC learnable, they can be learned with the same
learning algorithm as UNTS and, moreover, their
upper bound for performance is much higher than
for UNTS. Still, it might be the case that the exist-
ing algorithm for finding UNTS is not the right one
for learning the structure of a treebank, it might be
the case that strings in the PTB have not been pro-
duced by a k-l-UNTS grammar. We are currently
investigating how to produce an algorithm that fits
better the structure given in a treebank.

Learning Structure Using Probabilistic Au-
tomata DMV+CCM (Klein and Manning, 2004;
Klein and Manning, 2002) is a probabilistic model
for unsupervised parsing, that can be successfully
trained with the EM algorithm to achieve state of
the art performance. It is the combination of the
Constituent-Context Model, that models unlabeled
constituent parsing, and the Dependency Model with
Valence, that models projective dependency parsing.
On the other hand, CCM encodes the probability that
a given string of POS tags is a constituent. DMV is
more of our interest in this work, because it encodes
a top-down generative process where the heads gen-
erate their dependents to both directions until there
is a decision to stop, in a way that resembles suc-
cessful supervised dependency models such as in
(Collins, 1999). The generation of dependents of
a head on a specific direction can be seen as an im-
plicit probabilistic regular language generated by a
probabilistic deterministic finite automaton.

Under this perspective, the DMV model is in fact
an algorithm for learning several automata at the
same time. All automata have in common that they
have the same number of states and the same num-
ber of arcs between states, which is given by the def-

10



inition of the DMV model. Automata differ in that
they have different probabilities assigned to the tran-
sitions. The simple observation that DMV actually
suppose a fixed structure for the automata it induces
might explain its poor performance with freer order
languages like Spanish. Using our own implementa-
tion (see (Luque, 2009)) we have empirically tested
that DMV+CMV works well in languages with strict
word order, like English, but for other languages
with freer word order, like Spanish, DMV+CMV
performance decreases dramatically. In order to
improve DMV+CCM performance for this type of
languages, the structure of the automaton might be
modified, but since the DMV model has an ad hoc
learning algorithm, a new parametric learning algo-
rithm has to be defined. We are currently investigat-
ing different automaton structures for different lan-
guages and we are also investigating not only the
induction of the parameters for fixed structure, but
also inducing the structure of the automata itself.

3.2 Smallest Grammar and Compression for
Natural Language

The smallest grammar problem has been widely
studied in the literature. The aim of the problem is
to find the smallest (smallest in the sense of number
of symbols that occur in the grammar) context free
grammar that produces only one given string. The
smallest grammar can be thought as a relaxation of
the definition of Kolmogorov Complexity where the
complexity is given by a context free grammar in-
stead of a Turing machine. It is believed that the
smallest grammar can be used both for computing
optimal compression codes and for finding meaning-
ful patterns in strings.

Moreover, since the procedure for finding the
smallest grammar is in fact a procedure that assigns
a tree structure to a string, the smallest grammar
problem is, in fact, a particular case of unsupervised
parsing that has a very particular objective function
to be optimized.

Since the search space is exponentially big, all
existing algorithms are in fact heuristics that look
for a small grammar. In (Carrascosa et al., 2010)
we presented two algorithms that outperform all ex-
isting heuristics. We have produce and algorithm
that produces 10% smaller grammars for natural lan-
guage strings and 1.5% smaller grammars for DNA

sequences.
Even more, we show evidence that it is possi-

ble to find grammars that share approximately the
same small score but that have very little structure
in common. Moreover, the structure that is found
by the smallest grammar algorithm for the sentences
in PTB have little in common with the structure that
the PTB defines for those sentences.

Currently, we are trying to find answers to two dif-
ferent questions. First, is there a small piece of struc-
ture that is common to all grammars having compa-
rable sizes? and second, can the grammars that are
found by our algorithms be used for improving com-
pression algorithms?

4 Data-driven characterisation of
linguistic phenomena

4.1 Semi-structured text mining

One of our lines of research is to apply standard text
mining techniques to unstructured text, mostly user
generated content like that found in blogs, social net-
works, short messaging services or advertisements.
Our main corpus of study is constituted by classi-
fied advertisements from a local newspaper, but one
of our lines of work within this project is to assess
the portability of methods and techniques to differ-
ent genres.

The goals we pursue are:

creating corpora and related resources, and mak-
ing them publicly available. A corpus of news-
paper advertisements and a corpus of short text
messages are underway.

normalization of text bringing ortographic vari-
ants of a word (mostly abbreviations) to a
canonical form. To do that, we apply machine
learning techniques to learn the parameters for
edit distances, as in (Gómez-Ballester et al.,
1997; Ristad and Yanilos, 1998; Bilenko and
Mooney, 2003; McCallum et al., 2005; Oncina
and Sebban, 2006). We build upon previous
work on normalization by (Choudhury et al.,
2007; Okazaki et al., 2008; Cook and Steven-
son, 2009; Stevenson et al., 2009). Prelimi-
nary results show a significant improvement of
learned distances over standard distances.

11



syntactic analysis applying a robust shallow pars-
ing approach aimed to identify entities and their
modifiers.

ontology induction from very restricted domains,
to aid generalization in the step of information
extraction. We will be following the approach
presented in (Michelson and Knoblock, 2009).

information extraction inducing templates from
corpus using unsupervised and semi-
supervised techniques, and using induced
templates to extract information to populate
a relational database, as in (Michelson and
Knoblock, 2006).

data mining applying traditional knowledge dis-
covery techniques on a relational database pop-
ulated by the information extraction techniques
used in the previous item.

This line of research has been funded for three
years (2009-2012) by the Argentinean Ministry for
Science and Technology, within the PAE project, as
a PICT project (PAE-PICT-2007-02290).

This project opens many opportunities for collab-
oration. The resulting corpora will be of use for lin-
guistic studies. The results of learning edit distances
to find abbreviations can also be used by linguists as
an input to study the regularities found in this kind
of genres, as proposed in (Alonso Alemany, 2010).

We think that some joint work on learning string
edit distances would be very well integrated within
this project. We are also very interested in collabo-
rations with researchers who have some experience
in NLP in similar genres, like short text messages or
abbreviations in medical papers.

Finally, interactions with data mining communi-
ties, both academic and industrial, would surely be
very enriching for this project.

4.2 Characterisation of verbal behaviour
One of our research interests is the empirical charac-
terization of the subcategorization of lexical items,
with a special interest on verbs. This line of work
has been pursued mainly within the KNOW project,
in collaboration with the UB-GRIAL group9.

Besides the theoretical interest of describing the
behaviour of verbs based on corpus evidence, this

9http://grial.uab.es/

line has an applied aim, namely, enriching syntac-
tic analyzers with subcategorization information, to
help resolving structural ambiguities by using lexi-
cal information. We have focused on the behaviour
of Spanish verbs, and implemented some of our find-
ings as a lexicalized enhancement of the dependency
grammars used by Freeling10. An evaluation of the
impact of this information on parsing accuracy is un-
derway.

We have applied clustering techniques to obtain
a corpus-based characterization of the subcatego-
rization behaviour of verbs (Alonso Alemany et al.,
2007; Castellón et al., 2007). We explored the be-
haviour of the 250 most frequent verbs of Spanish
on the SenSem corpus (Castellón et al., 2006), man-
ually annotated with the analysis of verbs at various
linguistic levels (sense, aspect, voice, type of con-
struction, arguments, role, function, etc.). Apply-
ing clustering techniques to the instances of verbs in
these corpus, we obtained coarse-grained classes of
verbs with the same subcategorization. A classifier
was learned from considering clustered instances as
classes. With this classifier, verbs in unseen sen-
tences were assigned a subcategorization behaviour.

Also with the aim of associating subcategoriza-
tion information to verbs using evidence found
in corpora, we developed IRASubcat (Altamirano,
2009). IRASubcat11. is a highly flexible system de-
signed to gather information about the behaviour of
verbs from corpora annotated at any level, and in
any language. It identifies patterns of linguistic con-
stituents that co-occur with verbs, detects optional
constituents and performs hypothesis testing of the
co-occurrence of verbs and patterns.

We have also been working on connecting pred-
icates in FrameNet and SenSem, using WordNet
synsets as an interlingua (Alonso Alemany et al.,
SEPLN). We have found many dissimilarities be-
tween FrameNet and SenSem, but have been able
to connect some of their predicates and enrich these
resources with information from each other.

We are currently investigating the impact of dif-
ferent kinds of information on the resolution of pp-
attachment ambiguities in Spanish, using the AN-
CORA corpus (Taulé et al., 2006). We are exploring

10http://www.lsi.upc.edu/˜nlp/freeling/
11http://www.irasubcat.com.ar/

12



the utility of various WordNet-related information,
like features extracted from the Top Concept Ontol-
ogy, in combination with corpus-based information,
like frequencies of occurrence and co-occurrence of
words in corpus.

The line of research of characterisation of verbal
behaviour presents many points for collaboration.
In collaboration with linguists, the tools and meth-
ods that we have explained here provide valuable in-
formation for the description and systematization of
subcategorization of verbs and other lexical pieces.
It would be very interesting to see whether these
techniques, that have been successfully applied to
Spanish, apply to other languages or with different
resources. We are also interested in bringing to-
gether information from different resources or from
different sources (corpora, dictionaries, task-specific
lexica, etc.), in order to achieve richer resources.
We also have an interest for the study of hypothe-
sis testing as applied to corpus-based computational
linguistics, to get some insight on the information
that these techniques may provide to guide research
and validate results.

4.3 Discovering relations between entitites

As a result of the Microbio project, we have devel-
oped a module to detect relations between entities
in biomedical text (Bruno, 2009). This module has
been trained with the GENIA corpus (Kim et al.,
2008), obtaining good results (Alonso Alemany and
Bruno, 2009). We have also explored different ways
to overcome the data sparseness problem caused by
the small amount of manually annotated examples
that are available in the GENIA corpus. We have
used the corpus as the initial seed of a bootstrapping
procedure, generalized classes of relations via the
GENIA ontology and generalized classes via clus-
tering. Of these three procedures, only generaliza-
tion via an ontology produced good results. How-
ever, we have hopes that a more insightful charac-
terization of the examples and smarter learning tech-
niques (semi-supervised, active learning) will im-
prove the results for these other lines.

Since this area of NLP has ambitious goals, op-
portunities for collaboration are very diverse. In
general, we would like to join efforts with other re-
searchers to solve part of these complex problems,
with a special focus in relations between entities and

semi-supervised techniques.

5 Opportunities for Collaboration

We are looking for opportunities of collaboration
with other groups in the Americas, producing a syn-
ergy between groups. We believe that we can artic-
ulate collaboration by identifying common interests
and writing joint proposals. In Argentina there are
some agreements for billateral or multi-lateral col-
laboration with other countries or specific institu-
tions of research, which may provide a framework
for starting collaborations.

We are looking for collaborations that promote
the exchange of members of the group, specially
graduate students. Our aim is to gain a level of col-
laboration strong enough that would consider, for
example, co-supervision of PhD students. Ideally,
co-supervised students would spend half of their
time in each group, tackle a problem that is common
for both groups and work together with two super-
visors. The standard PhD scholarship in Argentina,
provided by Conicet, allows such modality of doc-
torate studies, as long as financial support for travels
and stays abroad is provided by the co-supervising
programme. We believe that this kind of collabora-
tion is one that builds very stable relations between
groups, helps students learn different research id-
iosyncrasies and devotes specific resources to main-
tain the collaboration.

References

Laura Alonso Alemany and Santiago E. Bruno. 2009.
Learning to learn biological relations from a small
training set. In CiCLing, pages 418–429.

Laura Alonso Alemany, Irene Castellón, and Nevena
Tinkova Tincheva. 2007. Obtaining coarse-grained
classes of subcategorization patterns for spanish. In
RANLP’07.

Laura Alonso Alemany, Irene Castellón, Egoitz Laparra,
and German Rigau. SEPLN. Evaluación de métodos
semi-automáticos para la conexión entre FrameNet y
SenSem. In 2009.

Laura Alonso Alemany. 2010. Learning parameters
for an edit distance can learn us tendencies in user-
generated content. Invited talk at NLP in the So-
cial Sciences, Instituto de Altos Estudios en Psicolo-
gia y Ciencias Sociales, Buenos Aires, Argentina, May
2010.

13



I. Romina Altamirano. 2009. Irasubcat: Un sistema
para adquisición automática de marcos de subcatego-
rización de piezas léxicas a partir de corpus. Master’s
thesis, Facultad de Matemática, Astronomı́a y Fı́sica,
Universidad Nacional de Córdoba, Argentina.

Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD.

Santiago E. Bruno. 2009. Detección de relaciones entre
entidades en textos de biomedicina. Master’s thesis,
Facultad de Matemática, Astronomı́a y Fı́sica, Univer-
sidad Nacional de Córdoba, Argentina.

Rafael Carrascosa, François Coste, Matthias Gallé, and
Gabriel Infante-Lopez. 2010. Choosing Word Occur-
rences for the Smallest Grammar Problem. In Pro-
ceedings of LATA 2010. Springer.

Irene Castellón, Ana Fernández-Montraveta, Glòria
Vázquez, Laura Alonso, and Joanan Capilla. 2006.
The SENSEM corpus: a corpus annotated at the syntac-
tic and semantic level. In 5th International Conference
on Language Resources and Evaluation (LREC 2006).

Irene Castellón, Laura Alonso Alemany, and Nevena Tin-
kova Tincheva. 2007. A procedure to automatically
enrich verbal lexica with subcategorization frames. In
Proceedings of the Argentine Simposium on Artificial
Intelligence, ASAI’07.

Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. Int. J. Doc. Anal. Recognit.,
10(3):157–174.

Alexander Clark. 2006. Pac-learning unambiguous nts
languages. In International Colloquium on Grammat-
ical Inference, pages 59–71.

Alexander Clark. 2007. Learning deterministic context
free grammars: the omphalos competition. Machine
Learning, 66(1):93–110.

M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, PA.

Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Work-
shop on Computational Approaches to Linguistic Cre-
ativity. NAACL HLT 2009.

E. Gómez-Ballester, M. L. Micó-Andrés, J. Oncina,
and M. L. Forcada-Zubizarreta. 1997. An empir-
ical method to improve edit-distance parameters for
a nearest-neighbor-based classification task. In VII
Spanish Symposium on Pattern Recognition and Image
Analysis, Barcelona, Spain.

Jin D. Kim, Tomoko Ohta, and Jun’ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).

Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In ACL, pages 128–135.

Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL 42.

Franco Luque and Gabriel Infante-Lopez. 2009. Upper
bounds for unsupervised parsing with unambiguous
non-terminally. In International Workshop Compu-
tational Linguistic Aspects of Grammatical Inference.
EACL, Greece.

Franco M. Luque. 2009. Implementation of the
DMV+CCM parser. http://www.cs.famaf.
unc.edu.ar/˜francolq/en/proyectos/
dmvccm.

Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the Proceedings of the Twenty-First Conference An-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-05), pages 388–395, Arlington, Virginia.
AUAI Press.

Matthew Michelson and Craig A. Knoblock. 2006.
Phoebus: a system for extracting and integrating data
from unstructured and ungrammatical sources. In
AAAI’06: proceedings of the 21st national conference
on Artificial intelligence, pages 1947–1948. AAAI
Press.

Matthew Michelson and Craig A. Knoblock. 2009. Ex-
ploiting background knowledge to build reference sets
for information extraction. In Proceedings of the 21st
International Joint Conference on Artific ial Intelli-
gence (IJCAI-2009), Pasadena, CA.

Naoaki Okazaki, Sophia Ananiadou, and Jun’ichi Tsujii.
2008. A discriminative alignment model for abbrevia-
tion recognition. In COLING ’08: Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 657–664, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.

José Oncina and Marc Sebban. 2006. Learning stochas-
tic edit distance: Application in handwritten character
recognition. Pattern Recognition, 39(9):1575–1587.

E. S. Ristad and P. N. Yanilos. 1998. Learning string edit
distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20:522–532.

Mark Stevenson, Yikun Guo, Abdulaziz Al Amri, and
Robert Gaizauskas. 2009. Disambiguation of biomed-
ical abbreviations. In BioNLP ’09: Proceedings of the
Workshop on BioNLP, pages 71–79, Morristown, NJ,
USA. Association for Computational Linguistics.

M. Taulé, M.A. Martı́, and M. Recasens. 2006. Ancora:
Multilevel annotated corpora for catalan and spanish.
In LREC’06.

14


