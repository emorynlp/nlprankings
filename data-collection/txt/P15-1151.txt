



















































genCNN: A Convolutional Architecture for Word Sequence Prediction


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1567–1576,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

A Convolutional Architecture for Word Sequence Prediction

Mingxuan Wang1 Zhengdong Lu2 Hang Li2 Wenbin Jiang1 Qun Liu3,1
1Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences
{wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn

2Noah’s Ark Lab, Huawei Technologies
{Lu.Zhengdong,HangLi.HL}@huawei.com

3ADAPT Centre, School of Computing, Dublin City University

Abstract

We propose a convolutional neural net-
work, named genCNN, for word se-
quence prediction. Different from
previous work on neural network-
based language modeling and genera-
tion (e.g., RNN or LSTM), we choose
not to greedily summarize the history
of words as a fixed length vector. In-
stead, we use a convolutional neural
network to predict the next word with
the history of words of variable length.
Also different from the existing feed-
forward networks for language mod-
eling, our model can effectively fuse
the local correlation and global cor-
relation in the word sequence, with
a convolution-gating strategy specifi-
cally designed for the task. We argue
that our model can give adequate rep-
resentation of the history, and there-
fore can naturally exploit both the short
and long range dependencies. Our
model is fast, easy to train, and read-
ily parallelized. Our extensive exper-
iments on text generation and n-best
re-ranking in machine translation show
that genCNN outperforms the state-of-
the-arts with big margins.

1 Introduction

Both language modeling (Wu and Khudanpur,
2003; Mikolov et al., 2010; Bengio et al.,
2003) and text generation (Axelrod et al., 2011)
boil down to modeling the conditional proba-
bility of a word given the proceeding words.
Previously, it is mostly done through purely
memory-based approaches, such as n-grams,
which cannot deal with long sequences and has

to use some heuristics (called smoothing) for
rare ones. Another family of methods are based
on distributed representations of words, which
is usually tied with a neural-network (NN) ar-
chitecture for estimating the conditional prob-
abilities of words.

Two categories of neural networks have been
used for language modeling: 1) recurrent neu-
ral networks (RNN), and 2) feedfoward net-
work (FFN):

• The RNN-based models, including its
variants like LSTM, enjoy more popu-
larity, mainly due to their flexible struc-
tures for processing word sequences of ar-
bitrary lengths, and their recent empiri-
cal success(Sutskever et al., 2014; Graves,
2013). We however argue that RNNs,
with their power built on the recursive use
of a relatively simple computation units,
are forced to make greedy summarization
of the history and consequently not effi-
cient on modeling word sequences, which
clearly have a bottom-up structures.

• The FFN-based models, on the other
hand, avoid this difficulty by feeding di-
rectly on the history. However, the FFNs
are built on fully-connected networks,
rendering them inefficient on capturing
local structures of languages. Moreover
their “rigid” architectures make it futile to
handle the great variety of patterns in long
range correlations of words.

We propose a novel convolutional architec-
ture, named genCNN, as a model that can ef-
ficiently combine local and long range struc-
tures of language for the purpose of modeling
conditional probabilities. genCNN can be di-
rectly used in generating a word sequence (i.e.,

1567



αCNNβCNNβCNN…

“sandwich”?  

/     /   I was starving after this long meeting, so I rushed to wal-mart to buy ahistory:  

prediction:  

Figure 1: The overall diagram of a genCNN. Here “/” stands for a zero padding. In this example,
each CNN component covers 6 words, while in practice the coverage is 30-40 words.

text generation) or evaluating the likelihood of
word sequences (i.e., language modeling). We
also show the empirical superiority of genCNN
on both tasks over traditional n-grams and its
RNN or FFN counterparts.

Notations: We will use V to denote the vo-
cabulary, et (∈ {1, · · · , |V|}) to denote the tth
word in a sequence e1:t

def= [e1, · · · , et], and
e(n)t if the sequence is further indexed by n.

2 Overview

As shown in Figure 1, genCNN is overall re-
cursive, consisting of CNN-based processing
units of two types:

• αCNN as the “front-end”, dealing with
the history that is closest to the prediction;

• βCNNs (which can repeat), in charge of
more “ancient” history.

Together, genCNN takes history e1:t of arbi-
trary length to predict the next word et+1 with
probability

p(et+1 |e1:t; Θ̄), (1)

based on a representation φ(e1:t; Θ̄) produced
by the CNN, and a |V|-class soft-max:

p(et+1|e1:t; Θ̄) ∝ eµ
>
et+1

φ(e1:t)+bet+1 . (2)

genCNN is devised (tailored) fully for mod-
eling the sequential structure in natural lan-
guage, notably different from conventional
CNN (Lawrence et al., 1997; Hu et al., 2014)
in 1) its specifically designed weights-sharing
strategy (in αCNN), 2) its gating design, and
3) certainly its recursive architectures. Also
distinct from RNN, genCNN gains most of

its processing power from the heavy-duty pro-
cessing units (i.e.,αCNN and βCNNs), which
follow a bottom-up information flow and yet
can adequately capture the temporal structure
in word sequence with its convolutional-gating
architecture.

3 genCNN: Architecture

We start with discussing the convolutional ar-
chitecture of αCNN as a stand-alone sentence
model, and then proceed to the recursive struc-
ture. After that we give a comparative analysis
on the mechanism of genCNN.
αCNN, just like a normal CNN, has fixed

architecture with predefined maximum words
(denoted as Lα). History shorter than Lα will
filled with zero paddings, and history longer
than that will be folded to feed to βCNN after
it, as will be elaborated in Section 3.3. Similar
to most other CNNs, αCNN alternates between
convolution layers and pooling layers, and fi-
nally a fully connected layer to reach the repre-
sentation before soft-max, as illustrated by Fig-
ure 2. Unlike the toyish example in Figure 2, in
practice we use a larger and deeper αCNN with
Lα = 30 or 40, and two or three convolution
layers (see Section 4.1). Different from con-
ventional CNN, genCNN has 1) weight shar-
ing strategy for convolution, and 2)“external”
gating networks to replace the normal pooling
mechanism, both of which are specifically de-
signed for word sequence prediction.

3.1 αCNN: Convolution

Different from conventional CNN, the weights
of convolution units in αCNN is only partially
shared. More specifically, in the convolution
units there are two types feature-maps: TIME-
FLOW and the TIME-ARROW, illustrated re-

1568



probability of next word

what did you have for/               /               /

“dinner” 
“breakfast” 
“us” 
“the”

… 

A 3-layer αCNN

Time-Flow         Time-Arrow           Gating                  

Figure 2: Illustration of a 3-layer αCNN.
Here the shadowed nodes stand for the TIME-
ARROW feature-maps and the unfilled nodes
for the TIME-FLOW.

spectively with the unfilled nodes and filled
nodes in Figure 2. The parameters for TIME-
FLOW are shared among different convolution
units, while for TIME-ARROW the parame-
ters are location-dependent. Intuitively, TIME-
FLOW acts more like a conventional CNN (e.g.,
that in (Hu et al., 2014)), aiming to understand
the overall temporal structure in the word se-
quences; TIME-ARROW, on the other hand,
works more like a traditional NN-based lan-
guage model (Vaswani et al., 2013; Bengio et
al., 2003): with its location-dependent param-
eters, it focuses on capturing the direction of
time and prediction task.

For sentence input x = {x1, · · · ,xT }, the
feature-map of type-f on Layer-` is
if f ∈ TIME-FLOW:

z
(`,f)
i (x) = σ(w

(`,f)
TF ẑ

(`−1)
i + b

(`,f)
TF ), (3)

if f ∈ TIME-ARROW:
z

(`,f)
i (x) = σ(w

(`,f,i)
TA ẑ

(`−1)
i + b

(`,f,i)
TA ), (4)

where

• z(`,f)i (x) gives the output of feature-map
of type-f for location i in Layer-`;

• σ(·) is the activation function, e.g., Sig-
moid or Relu (Dahl et al., 2013)

• w(`,f)TF denotes the location-independent
parameters for f ∈TIME-FLOW on Layer-
`, while w(`,f,i)TA stands for that for f ∈
TIME-ARROW and location i on Layer-`;

• ẑ(`−1)i denotes the segment of Layer-`−1
for the convolution at location i , while

ẑ(0)i
def= [x>i , x

>
i+1, · · · , x>i+k1−1]>

concatenates the vectors for k1 words
from sentence input x.

3.2 Gating Network

Previous CNNs, including those for NLP
tasks (Hu et al., 2014; Kalchbrenner et al.,
2014), take a straightforward convolution-
pooling strategy, in which the “fusion” deci-
sions (e.g., selecting the largest one in max-
pooling) are based on the values of feature-
maps. This is essentially a soft template match-
ing, which works for tasks like classification,
but undesired for maintaining the composition
functionality of convolution. In this paper, we
propose to use separate gating networks to re-
lease the scoring duty from the convolution,
and let it focus on composition. Similar idea
has been proposed by (Socher et al., 2011) for
recursive neural networks on parsing, but never
been combined with a convolutional structure.

…

Layer-

Layer-

Layer-gating 

Figure 3: Illustration for gating network.

Suppose we have convolution feature-maps
on Layer-` and gating (with window size =
2) on Layer-`+1. For the jth gating win-
dow (2j−1, 2j), we merge ẑ(`−1)2j−1 and ẑ(`−1)2j as
the input (denoted as z̄(`)j ) for gating network,
as illustrated in Figure 3. We use a separate

1569



gate for each feature-map, but follow a differ-
ent parametrization strategy for TIME-FLOW
and TIME-ARROW. With window size = 2, the
gating is binary, we use a logistic regressor to
determine the weights of two candidates. For
f ∈ TIME-ARROW, with location-dependent
w(`,f,j)gate , the normalized weight for left side is

g
(`+1,f)
j = 1/(1 + e

−w(`,f,j)gate z̄
(`)
j ),

while for For f∈TIME-FLOW, the parameters
for the corresponding gating network, denoted
as w(`,f)gate , are shared. The gated feature map is
then a weighted sum to feature-maps from the
two windows:

z
(`+1,f)
j = g

(`+1,f)
j z

(`,f)
2j−1 + (1− g(`+1,f)j )z(`,f)2j . (5)

We find that this gating strategy works signifi-
cantly better than pooling directly over feature-
maps, and slightly better than a hard gate ver-
sion of Equation 5

3.3 Recursive Architecture
As suggested early on in Section 2 and Fig-
ure 1, we use extra CNNs with conventional
weight-sharing, named βCNN, to summarize
the history out of scope of αCNN. More specif-
ically, the output of βCNN (with the same di-
mension of word-embedding) is put before the
first word as the input to the αCNN, as il-
lustrated in Figure 4. Different from αCNN,
βCNN is designed just to summarize the his-
tory, with weight shared across its convolution
units. In a sense, βCNN has only TIME-FLOW
feature-maps. All βCNN are identical and re-
cursively aligned, enabling genCNN to handle
sentences with arbitrary length. We put a spe-
cial switch after each βCNN to turn it off (re-
placing a pading vector shown as “/” in Fig-
ure 4 ) when there is no history assigned to it.
As the result, when the history is shorter than
Lα, the recursive structure reduces to αCNN.

In practice, 90+% sentences can be mod-
eled by αCNN with Lα = 40 and 99+% sen-
tences can be contained with one extra βCNN.
Our experiment shows that this recursive strat-
egy yields better estimate of conditional den-
sity than neglecting the out-of-scope history
(Section 6.1.2). In practice, we found that a
larger (greater Lα) and deeper αCNN works

αCNN

e5       e6 e7 e8       e7       e8 e9

…

βCNN 

“/ ”            

/ / / e1       e2       e3 e4

prediction for e10

Figure 4: genCNN with recursive structure.

better than small αCNN and more recursion,
which is consistent with our intuition that the
convolutional architecture is better suited for
modeling the sequence.

3.4 Analysis

3.4.1 TIME-FLOW vs. TIME-ARROW

Both conceptually and systemically, genCNN
gives two interweaved treatments of word his-
tory. With the globally-shared parameters in
the convolution units, TIME-FLOW summa-
rizes what has been said. The hierarchi-
cal convolution+gating architecture in TIME-
FLOW enables it to model the composition in
language, yielding representation of segments
at different intermediate layers. TIME-FLOW
is aware of the sequential direction, inherited
from the space-awareness of CNN, but it is not
sensitive enough about the prediction task, due
to the uniform weights in the convolution.

On the other hand, TIME-ARROW, living
in location-dependent parameters of convolu-
tion units, acts like an arrow pin-pointing the
prediction task. TIME-ARROW has predictive
power all by itself, but it concentrates on cap-
turing the direction of time and consequently
short on modelling the long-range dependency.

TIME-FLOW and TIME-ARROW have to
work together for optimal performance in pre-
dicting what is going to be said. This intuition

1570



has been empirically verified, as our experi-
ments have demonstrated that TIME-FLOW or
TIME-ARROW alone perform inferiorly. One
can imagine, through the layer-by-layer convo-
lution and gating, the TIME-ARROW gradually
picks the most relevant part from the represen-
tation of TIME-FLOW for the prediction task,
even if that part is long distance ahead.

3.4.2 genCNN vs. RNN-LM
Different from RNNs, which recursively ap-
plies a relatively simple processing units,
genCNN gains its ability on sequence mod-
eling mostly from its flexible and power-
ful bottom-up and convolution architecture.
genCNN takes the “uncompressed” history,
therefore avoids

• the difficulty in finding the representation
for history, e.g., those end in the middle of
a chunk (e.g.,“the cat sat on the”),

• the damping effort in RNN when the
history-summarizing hidden state is up-
dated at each time stamp, which renders
the long-range memory rather difficult,

both of which can only be partially ameliorated
with complicated design of gates (Hochreiter
and Schmidhuber, 1997) and or more heavy
processing units (essentially a fully connected
DNN) (Sutskever et al., 2014).

4 genCNN: Training

The parameters of a genCNN Θ̄ consists of
the parameters for CNN Θnn, word-embedding
Θembed, and the parameters for soft-max
Θsoftmax. All the parameters are jointly
learned by maximizing the likelihood of ob-
served sentences. Formally the log-likelihood
of sentence Sn ( def= [e(n)1 , e(n)2 , · · · , e(n)Tn ]) is

log p(Sn; Θ̄) =
Tn∑
t=1

log p(e(n)t |e(n)1:t−1; Θ̄),

which can be trivially split into Tn training in-
stances during the optimization, in contrast to
the training of RNN that requires unfolding
through time due to the temporal-dependency
of the hidden states.

4.1 Implementation Details

Architectures: In all of our experiments
(Section 5 and 6) we set the maximum words
for αCNN to be 30 and that for βCNN to be 20.
αCNN have two convolution layers (both con-
taining TIME-FLOW and TIME-ARROW con-
volution) and two gating layers, followed by
a fully connected layer (400 dimension) and
then a soft-max layer. The numbers of feature-
maps for TIME-FLOW are respectively 150
(1st convolution layer) and 100 (2nd convolu-
tion layer), while TIME-ARROW has the same
feature-maps. βCNN is relatively simple, with
two convolution layer containing only TIME-
FLOW with 150 feature-maps, two gating lay-
ers and a fully connected layer. We use ReLU
as the activation function for convolution lay-
ers and switch to Sigmoid for fully connected
layers. We use word embedding with dimen-
sion 100.

Soft-max: Calculating a full soft-max is ex-
pensive since it has to enumerate all the words
in vocabulary (in our case 40K words) in the
denominator. Here we take a simple hierarchi-
cal approximation of it, following (Bahdanau
et al., 2014). Basically we group the words
into 200 clusters (indexed by cm), and factor-
ize (in an approximate sense) the conditional
probability of a word p(et|e1:t−1; Θ̄) into the
probability of its cluster and the probability of
et given its cluster

p(cm|e1:t−1; Θ̄) p(et|cm; Θsoftmax).

We found that this simple heuristic can speed-
up the optimization by 5 times with only slight
loss of accuracy.

Optimization: We use stochastic gradient
descent with mini-batch (size 500) for opti-
mization, aided further by AdaGrad (Duchi
et al., 2011). For initialization, we use
Word2Vec (Mikolov et al., 2013) for the start-
ing state of the word-embeddings (trained on
the same dataset as the main task), and set
all the other parameters by randomly sampling
from uniform distribution in [−0.1, 0.1]. The
optimization is done mainly on a Tesla K40
GPU, which takes about 2 days for the train-
ing on a dataset containing 1M sentences.

1571



5 Experiments: Sentence Generation

In this experiment, we randomly generate sen-
tences by recurrently sampling

e?t+1 ∼ p(et+1|e1:t; Θ̄),

and put the newly generated word into history,
until EOS (end-of-sentence) is generated. We
consider generating two types of sentences: 1)
the plain sentences, and 2) sentences with de-
pendency parsing, which will be covered re-
spectively in Section 5.1 and 5.2.

5.1 Natural Sentences

We train genCNN on Wiki data with 112M
words for one week, with some representative
examples randomly generated given in Table 1
(upper and middle blocks). We try two settings,
by letting genCNN generate a sentence 1)from
the very beginning (middle block), or 2) start-
ing with a few words given by human (upper
block). It is fairly clear that most of the time
genCNN can generate sentences that are syn-
tactically grammatical and semantically mean-
ingful. More specifically, most of the sentences
can be aligned to a parse tree with reasonable
structure. It is also worth noting that quotation
marks (‘‘ and ’’) are always generated in pairs
and in the correct order, even across a relatively
long distance, as exemplified by the first gener-
ated sentence in the upper block.

5.2 Sentences with Dependency Tags

For training, we first parse(Klein and Man-
ning, 2002) the English sentences and feed se-
quences with dependency tags as follows

( I ? like ( red ? apple ) )

to genCNN in training, where 1) each paired
parentheses contain a subtree, and 2) the sym-
bol “?” indicates that the word next to it is
the dependency head in the corresponding sub-
tree. Some representative examples gener-
ated by genCNN are given in Table 1 (bottom
block). As it suggests, genCNN is fairly ac-
curate on respecting the rules of parentheses,
and probably more remarkably, it can get the
dependency tree head right most of the time.

6 Experiments: Language Modeling

We evaluate our model as a language model in
terms of both perplexity (Brown et al., 1992)
and its efficacy in re-ranking the n-best can-
didates from state-of-the-art models in statisti-
cal machine translation, with comparison to the
following competitor language models.

Competitor Models we compare genCNN
to the following competitor models

• 5-gram: We use SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train
a 5-gram language model with modified
Kneser-Ney smoothing;

• FFN-LM: The neural language model
based on feedfoward network (Vaswani et
al., 2013). We vary the input window-size
from 5 to 20, while the performance stops
increasing after window size 20;

• RNN: we use the implementation1 of
RNN-based language model with hidden
size 600;

• LSTM: we adopt the code in Ground-
hog2, but vary the hyper-parameters,
including the depth and word-embedding
dimension, for best performance.
LSTM (Hochreiter and Schmidhuber,
1997) is widely considered to be the
state-of-the-art for sequence modeling.

6.1 Perplexity
We test the performance of genCNN on PENN
TREEBANK and FBIS, two public datasets
with different sizes.

6.1.1 On PENN TREEBANK
Although a relatively small dataset 3, PENN
TREEBANK is widely used as a language mod-
elling benchmark (Graves, 2013; Mikolov et
al., 2010). It has 930, 000 words in train-
ing set, 74, 000 words in validation set, and
82, 000 words in test set. We use exactly the
same settings as in (Mikolov et al., 2010),
with a 10, 000-words vocabulary (all out-of-
vocabulary words are replaced with unknown)

1http://rnnlm.org/
2https://github.com/lisa-groundhog/GroundHog
3http://www.fit.vutbr.cz/∼imikolov/rnnlm/simple-

examples.tgz

1572



‘‘ we are in the building of china ’s social development and the businessmen

audience , ’’ he said .

clinton was born in DDDD , and was educated at the university of edinburgh.

bush ’s first album , ‘‘ the man ’’ , was released on DD november DDDD .

it is one of the first section of the act in which one is covered in real

place that recorded in norway .

this objective is brought to us the welfare of our country

russian president putin delivered a speech to the sponsored by the 15th asia

pacific economic cooperation ( apec ) meeting in an historical arena on oct .

light and snow came in kuwait and became operational , but was rarely

placed in houston .

johnson became a drama company in the DDDDs , a television broadcasting

company owned by the broadcasting program .

( ( the two ? sides ) ? should ( ? assume ( a strong ? target ) ) ) . )

( it ? is time ( ? in ( every ? country ) ? signed ( the ? speech ) ) . )

( ( initial ? investigations ) ? showed ( ? that ( spot ? could ( ? be (

further ? improved significantly ) ) . )

( ( a ? book ( to ? northern ( the 21 st ? century ) ) ) . )

Table 1: Examples of sentences generated by genCNN. In the upper block (row 1-4) the underline
words are given by the human; In the middle block (row 5-8), all the sentences are generated
without any hint. The bottom block (row 9-12) shows the sentences with dependency tag generated
by genCNN trained with parsed examples.

and end-of-sentence token (EOS) at the end of
each sentence. In addition to the conventional
testing strategy where the models are kept un-
changed during testing, Mikolov et al. (2010)
proposes to also update the parameters in an
online fashion when seeing test sentences. This
new way of testing, named “dynamic evalua-
tion”, is also adopted by Graves (2013).

From Table 2 genCNN manages to give per-
plexity superior in both metrics, with about 25
point reduction over the widely used 5-gram,
and over 10 point reduction from LSTM, the
state-of-the-art and the second-best performer.

6.1.2 On FBIS
The FBIS corpus (LDC2003E14) is relatively
large, with 22.5K sentences and 8.6M English
words. The validation set is NIST MT06 and
test set is NIST MT08. For training the neural
network, we limit the vocabulary to the most
frequent 40,000 words, covering ∼ 99.4% of
the corpus. Similar to the first experiment,
all out-of-vocabulary words are replaced with
unknown and the EOS token is counted in the
sequence loss.

From Table 3 (upper block), genCNN

Model Perplexity Dynamic
5-gram, KN5 141.2 –
FFNN-LM 140.2 –
RNN 124.7 123.2
LSTM 126 117
genCNN 116.4 106.3

Table 2: PENN TREEBANK results, where the
3rd column are the perplexity in dynamic eval-
uation, while the numbers for RNN and LSTM
are taken as reported in the paper cited above.
The numbers in boldface indicate that the re-
sult is significantly better than all competitors
in the same setting.

clearly wins again in the comparison to com-
petitors, with over 25 point margin over LSTM
(in its optimal setting), the second best per-
former. Interestingly genCNN outperforms its
variants also quite significantly (bottom block):
1) with only TIME-ARROW (same number
of feature-maps), the performance deteriorates
considerably for losing the ability of capturing
long range correlation reliably; 2) with only
TIME-TIME the performance gets even worse,

1573



Model Perplexity
5-gram, KN5 278.6
FFN-LM(5-gram) 248.3
FFN-LM(20-gram) 228.2
RNN 223.4
LSTM 206.9
genCNN 181.2
TIME-ARROW only 192
TIME-FLOW only 203
αCNN only 184.4

Table 3: FBIS results. The upper block
(row 1-6) compares genCNN and the competi-
tor models, and the bottom block (row 7-9)
compares different variants of genCNN.

for partially losing the sensitivity to the predic-
tion task. It is quite remarkable that, although
αCNN (with Lα = 30) can achieve good re-
sults, the recursive structure in full genCNN
can further decrease the perplexity by over
3 points, indicating that genCNN can benefit
from modeling the dependency over range as
long as 30 words.

6.2 Re-ranking for Machine Translation

In this experiment, we re-rank the 1000-best
English translation candidates for Chinese sen-
tences generated by statistical machine transla-
tion (SMT) system, and compare it with other
language models in the same setting.

SMT setup The baseline hierarchical phrase-
based SMT system ( Chines→ English) was
built using Moses, a widely accepted state-
of-the-art, with default settings. The bilin-
gual training data is from NIST MT2012 con-
strained track, with reduced size of 1.1M sen-
tence pairs using selection strategy in (Axel-
rod et al., 2011). The baseline use conven-
tional 5-gram language model (LM), estimated
with modified Kneser-Ney smoothing (Chen
and Goodman, 1996) on the English side of the
329M-word Xinhua portion of English Giga-
word(LDC2011T07). We also try FFN-LM, as
a much stronger language model in decoding.
The weights of all the features are tuned via
MERT (Och and Ney, 2002) on NIST MT05,
and tested on NIST MT06 and MT08. Case-

Models MT06 MT08 Ave.
Baseline 38.63 31.11 34.87
RNN rerank 39.03 31.50 35.26
LSTM rerank 39.20 31.90 35.55
FFN-LM rerank 38.93 31.41 35.14
genCNN rerank 39.90 32.50 36.20
Base+FFN-LM 39.08 31.60 35.34
genCNN rerank 40.4 32.85 36.63

Table 4: The results for re-ranking the 1000-
best of Moses. Note that the two bottom rows
are on a baseline with enhanced LM.

insensitive NIST BLEU4 is used in evaluation.
Re-ranking with genCNN significantly im-

proves the quality of the final translation. In-
deed, it can increase the BLEU score by over
1.33 point over Moses baseline on average.
This boosting force barely slacks up on trans-
lation with a enhanced language model in de-
coding: genCNN re-ranker still achieves 1.29
point improvement on top of Moses with FFN-
LM, which is 1.76 point over the Moses (de-
fault setting). To see the significance of this
improvement, the state-of-the-art Neural Net-
work Joint Model (Devlin et al., 2014) usually
brings less than one point increase on this task.

7 Related Work

In addition to the long thread of work on neu-
ral network based language model (Auli et al.,
2013; Mikolov et al., 2010; Graves, 2013; Ben-
gio et al., 2003; Vaswani et al., 2013), our work
is also related to the effort on modeling long
range dependency in word sequence predic-
tion(Wu and Khudanpur, 2003). Different from
those work on hand-crafting features for incor-
porating long range dependency, our model can
elegantly assimilate relevant information in an
unified way, in both long and short range, with
the bottom-up information flow and convolu-
tional architecture.

CNN has been widely used in computer
vision and speech (Lawrence et al., 1997;
Krizhevsky et al., 2012; LeCun and Bengio,
1995; Abdel-Hamid et al., 2012), and lately
in sentence representation(Kalchbrenner and

4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-
v11b.pl

1574



Blunsom, 2013), matching(Hu et al., 2014) and
classification(Kalchbrenner et al., 2014). To
our best knowledge, it is the first time this is
used in word sequence prediction. Model-wise
the previous work that is closest to genCNN is
the convolution model for predicting moves in
the Go game (Maddison et al., 2014), which,
when applied recurrently, essentially gener-
ates a sequence. Different from the conven-
tional CNN taken in (Maddison et al., 2014),
genCNN has architectures designed for mod-
eling the composition in natural language and
the temporal structure of word sequence.

8 Conclusion

We propose a convolutional architecture for
natural language generation and modeling. Our
extensive experiments on sentence generation,
perplexity, and n-best re-ranking for machine
translation show that our model can signifi-
cantly improve upon state-of-the-arts.

References
[Abdel-Hamid et al.2012] Ossama Abdel-Hamid,

Abdel-rahman Mohamed, Hui Jiang, and Gerald
Penn. 2012. Applying convolutional neural
networks concepts to hybrid nn-hmm model
for speech recognition. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE
International Conference on, pages 4277–4280.
IEEE.

[Auli et al.2013] Michael Auli, Michel Galley,
Chris Quirk, and Geoffrey Zweig. 2013. Joint
language and translation modeling with recur-
rent neural networks. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1044–1054,
Seattle, Washington, USA, October.

[Axelrod et al.2011] Amittai Axelrod, Xiaodong
He, and Jianfeng Gao. 2011. Domain adapta-
tion via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 355–
362. Association for Computational Linguistics.

[Bahdanau et al.2014] Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio. 2014.
Neural machine translation by jointly learn-
ing to align and translate. arXiv preprint
arXiv:1409.0473.

[Bengio et al.2003] Yoshua Bengio, Rjean
Ducharme, Pascal Vincent, and Christian
Jauvin. 2003. A neural probabilistic lan-
guage model. Journal OF Machine Learning
Research, 3:1137–1155.

[Brown et al.1992] Peter F. Brown, Vincent J. Della
Pietra, Robert L. Mercer, Stephen A. Della
Pietra, and Jennifer C. Lai. 1992. An estimate of
an upper bound for the entropy of english. Com-
put. Linguist., 18(1):31–40, March.

[Chen and Goodman1996] Stanley F Chen and
Joshua Goodman. 1996. An empirical study of
smoothing techniques for language modeling.
In Proceedings of the 34th annual meeting
on Association for Computational Linguistics,
pages 310–318. Association for Computational
Linguistics.

[Dahl et al.2013] George E Dahl, Tara N Sainath,
and Geoffrey E. Hinton. 2013. Improving deep
neural networks for lvcsr using rectified linear
units and dropout. In Proceedings of ICASSP.

[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the
52nd Annual Meeting of the Association for
Computational Linguistics, pages 1370–1380.

1575



[Duchi et al.2011] John Duchi, Elad Hazan, and
Yoram Singer. 2011. Adaptive subgradient
methods for online learning and stochastic opti-
mization. The Journal of Machine Learning Re-
search, 12:2121–2159.

[Graves2013] Alex Graves. 2013. Generating se-
quences with recurrent neural networks. CoRR,
abs/1308.0850.

[Hochreiter and Schmidhuber1997] Sepp Hochre-
iter and Jürgen Schmidhuber. 1997. Long short-
term memory. Neural Comput., 9(8):1735–
1780, November.

[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang
Li, and Qingcai Chen. 2014. Convolutional
neural network architectures for matching natu-
ral language sentences. In NIPS.

[Kalchbrenner and Blunsom2013] Nal Kalchbren-
ner and Phil Blunsom. 2013. Recurrent contin-
uous translation models. In Proceedings of the
2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709,
Seattle, Washington, USA, October.

[Kalchbrenner et al.2014] Nal Kalchbrenner, Ed-
ward Grefenstette, and Phil Blunsom. 2014. A
convolutional neural network for modelling sen-
tences. ACL.

[Klein and Manning2002] Dan Klein and Christo-
pher D Manning. 2002. Fast exact inference
with a factored model for natural language pars-
ing. In Advances in neural information process-
ing systems, volume 15, pages 3–10.

[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey E Hinton. 2012.
Imagenet classification with deep convolutional
neural networks. In Advances in neural infor-
mation processing systems, pages 1097–1105.

[Lawrence et al.1997] Steve Lawrence, C Lee Giles,
Ah Chung Tsoi, and Andrew D Back. 1997.
Face recognition: A convolutional neural-
network approach. Neural Networks, IEEE
Transactions on, 8(1):98–113.

[LeCun and Bengio1995] Yann LeCun and Yoshua
Bengio. 1995. Convolutional networks for im-
ages, speech, and time series. The handbook of
brain theory and neural networks, 3361:310.

[Maddison et al.2014] Chris J. Maddison, Aja
Huang, Ilya Sutskever, and David Silver. 2014.
Move evaluation in go using deep convolutional
neural networks. CoRR, abs/1412.6564.

[Mikolov et al.2010] Tomas Mikolov, Martin
Karafit, Lukas Burget, Jan Cernocky, and
Sanjeev Khudanpur. 2010. In INTERSPEECH,
pages 1045–1048.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen,
Greg Corrado, and Jeffrey Dean. 2013. Effi-
cient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.

[Och and Ney2002] Franz Josef Och and Hermann
Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine
translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Lin-
guistics, pages 295–302.

[Socher et al.2011] Richard Socher, Cliff C. Lin,
Andrew Y. Ng, and Christopher D. Manning.
2011. Parsing Natural Scenes and Natural Lan-
guage with Recursive Neural Networks. In Pro-
ceedings of the 26th International Conference on
Machine Learning (ICML).

[Stolcke and others2002] Andreas Stolcke et al.
2002. Srilm-an extensible language modeling
toolkit. In Proceedings of the international
conference on spoken language processing, vol-
ume 2, pages 901–904.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V Le. 2014. Sequence to sequence
learning with neural networks. In NIPS.

[Vaswani et al.2013] Ashish Vaswani, Yinggong
Zhao, Victoria Fossum, and David Chiang.
2013. Decoding with large-scale neural lan-
guage models improves translation. In EMNLP,
pages 1387–1392. Citeseer.

[Wu and Khudanpur2003] Jun Wu and Sanjeev
Khudanpur. 2003. Maximum entropy language
modeling with non-local dependencies. Ph.D.
thesis, Johns Hopkins University.

1576


