



















































AMRICA: an AMR Inspector for Cross-language Alignments


Proceedings of NAACL-HLT 2015, pages 36–40,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

AMRICA: an AMR Inspector for Cross-language Alignments

Naomi Saphra
Center for Language and Speech Processing

Johns Hopkins University
Baltimore, MD 21211, USA
nsaphra@jhu.edu

Adam Lopez
School of Informatics

University of Edinburgh
Edinburgh, United Kingdom
alopez@inf.ed.ac.uk

Abstract

Abstract Meaning Representation (AMR), an
annotation scheme for natural language se-
mantics, has drawn attention for its simplic-
ity and representational power. Because AMR
annotations are not designed for human read-
ability, we present AMRICA, a visual aid for
exploration of AMR annotations. AMRICA
can visualize an AMR or the difference be-
tween two AMRs to help users diagnose in-
terannotator disagreement or errors from an
AMR parser. AMRICA can also automati-
cally align and visualize the AMRs of a sen-
tence and its translation in a parallel text. We
believe AMRICA will simplify and streamline
exploratory research on cross-lingual AMR
corpora.

1 Introduction

Research in statistical machine translation has be-
gun to turn to semantics. Effective semantics-based
translation systems pose a crucial need for a practi-
cal cross-lingual semantic representation. One such
schema, Abstract Meaning Representation (AMR;
Banarescu et al., 2013), has attracted attention for its
simplicity and expressive power. AMR represents
the meaning of a sentence as a directed graph over
concepts representing entities, events, and properties
like names or quantities. Concepts are represented
by nodes and are connected by edges representing
relations—roles or attributes. Figure 1 shows an ex-
ample of the AMR annotation format, which is opti-
mized for text entry rather than human comprehen-
sion.

For human analysis, we believe it is easier to visu-
alize the AMR graph. We present AMRICA, a sys-

(b / be-located-at-91 :li 4
:ARG1 (i / i)
:ARG2 (c / country

:name (n / name
:op1 "New" :op2 "Zealand"))

:time (w / week :quant 2
:time (p / past)))

Figure 1: AMR for “I’ve been in New Zealand the past
two weeks.” (Linguistic Data Consortium, 2013)

tem for visualizing AMRs in three conditions. First,
AMRICA can display AMRs as in Figure 2. Sec-
ond, AMRICA can visualize differences between
aligned AMRs of a sentence, enabling users to diag-
nose differences in multiple annotations or between
an annotation and an automatic AMR parse (Sec-
tion 2). Finally, to aid researchers studying cross-
lingual semantics, AMRICA can visualize differ-
ences between the AMR of a sentence and that of
its translation (Section 3) using a novel cross-lingual
extension to Smatch (Cai and Knight, 2013). The
AMRICA code and a tutorial are publicly available.1

2 Interannotator Agreement

AMR annotators and researchers are still exploring
how to achieve high interannotator agreement (Cai
and Knight, 2013). So it is useful to visualize a
pair of AMRs in a way that highlights their disagree-
ment, as in Figure 3. AMRICA shows in black those
nodes and edges which are shared between the anno-
tations. Elements that differ are red if they appear in
one AMR and blue if they appear in the other. This
feature can also be used to explore output from an

1http://github.com/nsaphra/AMRICA

36



Figure 2: AMRICA visualization of AMR in Figure 1.

Figure 3: AMRICA visualization of the disagreement
between two independent annotations of the sentence in
Figure 1.

automatic AMR parser in order to diagnose errors.
To align AMRs, we use the public implementation

of Smatch (Cai and Knight, 2013).2 Since it also
forms the basis for our cross-lingual visualization,
we briefly review it here.

AMR distinguishes between variable and con-
stant nodes. Variable nodes, like i in Figure 1, rep-
resent entities and events, and may have multiple in-
coming and outgoing edges. Constant nodes, like 2
in Figure 1, participate in exactly one relation, mak-
ing them leaves of a single parent variable. Smatch
compares a pair of AMRs that have each been de-
composed into three kinds of relationships:

2http://amr.isi.edu/download/smatch-v2.0.tar.gz

1. The set V of instance-of relations describe the
conceptual class of each variable. In Figure 1,
(c / country) specifies that c is an in-
stance of a country. If node v is an instance
of concept c, then (v, c) ∈ V .

2. The set E of variable-to-variable relations like
ARG2(b, c) describe relationships between
entities and/or events. If r is a relation from
variable v1 to variable v2, then (r, v1, v2) ∈ E.

3. The set C of variable-to-constant relations like
quant(w, 2) describe properties of entities
or events. If r is a relation from variable v to
constant x, then (r, v, x) ∈ C.

Smatch seeks the bijective alignment b̂ : V → V ′
between an AMR G = (V,E,C) and a larger AMR
G′ = (V ′, E′, C ′) satisfying Equation 1, where I is
an indicator function returning 1 if its argument is
true, 0 otherwise.

b̂ = arg max
b

∑
(v,c)∈V

I((b(v), c) ∈ V ′)+ (1)
∑

(r,v1,v2)∈E
I((r, b(v1), b(v2)) ∈ E′)+∑

(r,v,c)∈C
I((r, b(v), c) ∈ C ′)

Cai and Knight (2013) conjecture that this opti-
mization can be shown to be NP-complete by reduc-
tion to the subgraph isomorphism problem. Smatch
approximates the solution with a hill-climbing algo-
rithm. It first creates an alignment b0 in which each
node of G is aligned to a node in G′ with the same
concept if such a node exists, or else to a random
node. It then iteratively produces an alignment bi
by greedily choosing the best alignment that can be
obtained from bi−1 by swapping two alignments or
aligning a node in G to an unaligned node, stopping
when the objective no longer improves and returning
the final alignment. It uses random restarts since the
greedy algorithm may only find a local optimum.

3 Aligning Cross-Language AMRs

AMRICA offers the novel ability to align AMR an-
notations of bitext. This is useful for analyzing

37



AMR annotation differences across languages, and
for analyzing translation systems that use AMR as
an intermediate representation. The alignment is
more difficult than in the monolingual case, since
nodes in AMRs are labeled in the language of
the sentence they annotate. AMRICA extends the
Smatch alignment algorithm to account for this dif-
ficulty.

AMRICA does not distinguish between constants
and variables, since their labels tend to be grounded
in the words of the sentence, which it uses for align-
ment. Instead, it treats all nodes as variables and
computes the similarities of their node labels. Since
node labels are in their language of origin, exact
string match no longer works as a criterion for as-
signing credit to a pair of aligned nodes. There-
fore AMRICA uses a function L : V × V → R
indicating the likelihood that the nodes align. These
changes yield the new objective shown in Equation 2
for AMRs G = (V,E) and G′ = (V ′, E′), where V
and V ′ are now sets of nodes, and E and E′ are de-
fined as before.

b̂ = arg max
b

∑
v∈V

L(v, b(v))+ (2)∑
(r,v1,v2)∈E

I((r, b(v1), b(v2)) ∈ E′)

If the labels of nodes v and v′ match, then
L(v, v′) = 1. If they do not match, then L de-
composes over source-node-to-word alignment as,
source-word-to-target-word alignment a, and target-
word-to-node at, as illustrated in Figure 5. More
precisely, if the source and target sentences contain
n and n′ words, respectively, then L is defined by
Equation 3. AMRICA takes a parameter α to con-
trol how it weights these estimated likelihoods rela-
tive to exact matches of relation and concept labels.

L(v, v′) = α
n∑

i=1

Pr(as(v) = i)× (3)

n′∑
j=1

Pr(ai = j) · Pr(at(v′) = j)

Node-to-word probabilities Pr(as(v) = i) and
Pr(as(v′) = j) are computed as described in Sec-
tion 3.1. Word-to-word probabilities Pr(ai = j)

are computed as described in Section 3.2. AM-
RICA uses the Smatch hill-climbing algorithm to
yield alignments like that in Figure 4.

3.1 Node-to-word and word-to-node alignment
AMRICA can accept node-to-word alignments as
output by the heuristic aligner of Flanigan et al.
(2014).3 In this case, the tokens in the aligned span
receive uniform probabilities over all nodes in their
aligned subgraph, while all other token-node align-
ments receive probability 0. If no such alignments
are provided, AMRICA aligns concept nodes to to-
kens matching the node’s label, if they exist. A to-
ken can align to multiple nodes, and a node to multi-
ple tokens. Otherwise, alignment probability is uni-
formly distributed across unaligned nodes or tokens.

3.2 Word-to-word Alignment
AMRICA computes the posterior probability of the
alignment between the ith word of the source and jth
word of the target as an equal mixture between the
posterior probabilities of source-to-target and target-
to-source alignments from GIZA++ (Och and Ney,
2003).4 To obtain an approximation of the pos-
terior probability in each direction, it uses the m-
best alignments a(1) . . . a(m), where a(k)i = j indi-
cates that the ith source word aligns to the jth target
word in the kth best alignment, and Pr(a(k)) is the
probability of the kth best alignment according to
GIZA++. We then approximate the posterior proba-
bility as follows.

Pr(ai = j) =
∑m

k=1 Pr(a
(k))I[a(k)i = j]∑m

k=1 Pr(a(k))

4 Demonstration Script

AMRICA makes AMRs accessible for data explo-
ration. We will demonstrate all three capabilities
outlined above, allowing participants to visually ex-
plore AMRs using graphics much like those in Fig-
ures 2, 3, and 4, which were produced by AMRICA.
We will then demonstrate how AMRICA can be
used to generate a preliminary alignment for bitext

3Another option for aligning AMR graphs to sentences is
the statistical aligner of Pourdamghani et al. (2014)

4In experiments, this method was more reliable than using
either alignment alone.

38



Figure 5: Cross-lingual AMR example from Nianwen Xue et al. (2014). The node-to-node alignment of the high-
lighted nodes is computed using the node-to-word, word-to-word, and node-to-word alignments indicated by green
dashed lines.

Figure 4: AMRICA visualization of the example in Fig-
ure 5. Chinese concept labels are first in shared nodes.

AMRs, which can be corrected by hand to provide
training data or a gold standard alignment.

Information to get started with AMRICA is avail-
able in the README for our publicly available
code.

Acknowledgments

This research was supported in part by the National
Science Foundation (USA) under awards 1349902
and 0530118. We thank the organizers of the
2014 Frederick Jelinek Memorial Workshop and the
members of the workshop team on Cross-Lingual
Abstract Meaning Representations (CLAMR), who
tested AMRICA and provided vital feedback.

References

L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Grif-
fitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer,
and N. Schneider. 2013. Abstract meaning represen-
tation for sembanking. In Proc. of the 7th Linguistic

39



Annotation Workshop and Interoperability with Dis-
course.

S. Cai and K. Knight. 2013. Smatch: an evaluation met-
ric for semantic feature structures. In Proc. of ACL.

J. Flanigan, S. Thomson, C. Dyer, J. Carbonell, and N. A.
Smith. 2014. A discriminative graph-based parser for
the abstract meaning representation. In Proc. of ACL.

Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer,
Zdenka Uresova, and Xiuhong Zhang. 2014. Not an
interlingua, but close: Comparison of English AMRs
to Chinese and Czech. In Proc. of LREC.

F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51, Mar.

N. Pourdamghani, Y. Gao, U. Hermjakob, and K. Knight.
2014. Aligning english strings with abstract meaning
representation graphs.

Linguistic Data Consortium. 2013. DEFT phase 1 AMR
annotation R3 LDC2013E117.

40


