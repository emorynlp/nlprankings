










































Measuring the Effect of Discourse Relations on Blog Summarization


International Joint Conference on Natural Language Processing, pages 1401–1409,
Nagoya, Japan, 14-18 October 2013.

Measuring the Effect of Discourse Relations on Blog Summarization

Shamima Mithun
Concordia University

Montreal, Quebec, Canada
shamima.mithun@gmail.com

Leila Kosseim
Concordia University

Montreal, Quebec, Canada
kosseim@encs.concordia.ca

Abstract

The work presented in this paper attempts
to evaluate and quantify the use of dis-
course relations in the context of blog
summarization and compare their use to
more traditional and factual texts. Specif-
ically, we measured the usefulness of
6 discourse relations - namely compari-
son, contingency, illustration, attribution,
topic-opinion, and attributive for the task
of text summarization from blogs. We
have evaluated the effect of each relation
using the TAC 2008 opinion summariza-
tion dataset and compared them with the
results with the DUC 2007 dataset. The re-
sults show that in both textual genres, con-
tingency, comparison, and illustration re-
lations provide a significant improvement
on summarization content; while attribu-
tion, topic-opinion, and attributive rela-
tions do not provide a consistent and sig-
nificant improvement. These results indi-
cate that, at least for summarization, dis-
course relations are just as useful for in-
formal and affective texts as for more tra-
ditional news articles.

1 Introduction

It is widely accepted that in a coherent text, units
should not be understood in isolation but in rela-
tion with each other through discourse relations
that may or may not be explicitly marked. A text
is not a linear combination of textual units but a hi-
erarchial organized group of units placed together
based on informational and intentional relations to
one another. According to (Taboada, 2006), “Dis-
course relations - relations that hold together dif-
ferent parts (i.e. proposition, sentence, or para-
graph) of the discourse - are partly responsible for
the perceived coherence of a text”. For example,

in the sentence “If you want the full Vista expe-
rience, you’ll want a heavy system and graphics
hardware, and lots of memory”, the first and sec-
ond clauses do not bear much meaning indepen-
dently; but become more meaningful when we re-
alize that they are related through the discourse re-
lation condition.

Discourse relations have been found useful in
many NLP applications such as natural language
generation (e.g. (McKeown, 1985)) and news
summarization (e.g. (Blair-Goldensohn and McK-
eown, 2006; Bosma, 2004)) to improve coherence
and better simulate human writing. However, most
of these work have been developed for formal,
well-written and factual documents. Text avail-
able in the social media are typically written in a
more casual style, are opinionated and speculative
(Andreevskaia et al., 2007). Because of this, tech-
niques developed for formal texts, such as news
articles, often do not behave as well when deal-
ing with informal documents. In particular, news
articles are more uniform in style and structure;
whereas blogs often do not exhibit a stereotypi-
cal discourse structure. As a result, for blogs, it
is usually more difficult to identify and rank rel-
evant units for summarization compared to news
articles.

Several work have shown that discourse rela-
tions can improve the results of summarization in
the case of factual texts or news articles (e.g. (Ot-
terbacher et al., 2002)). However, to our knowl-
edge no work has evaluated the usefulness of dis-
course relations for the summarization of informal
and opinionated texts, as those found in the social
media. In this paper, we consider the most fre-
quent discourse relations found in blogs: namely
comparison, contingency, illustration, attribution,
topic-opinion, and attributive and evaluate the ef-
fect of each relation on informal text summariza-
tion using the Text Analysis Conference (TAC)

1401



2008 opinion summarization dataset1. We then
compare these results to those found with the news
articles of the Document Understanding Confer-
ence (DUC) 2007 Main task dataset2. The re-
sults show that in both types of texts, discourse
relations seem to be as useful: contingency, com-
parison, and illustration relations provide a statis-
tically significant improvement on the summary
content; while the attribution, topic-opinion, and
attributive relations do not provide a consistent
and significant improvement.

2 Related Work on Discourse Relations
for Summarization

The use of discourse relations for text sum-
marization is not new. Most notably, (Marcu,
1997) used discourse relations for single docu-
ment summarization and proposed a discourse re-
lation identification parsing algorithm. In some
work (e.g. (Bosma, 2004; Blair-Goldensohn and
McKeown, 2006)), discourse relations have been
exploited successfully for multi-document sum-
marization. In particular, (Otterbacher et al., 2002)
experimentally showed that discourse relations
can improve the coherence of multi-document
summaries. (Bosma, 2004) showed how dis-
course relations can be used effectively to incorpo-
rate additional contextual information for a given
question in a query-based summarization. (Blair-
Goldensohn and McKeown, 2006) used discourse
relations for content selection and organization of
automatic summaries and achieved an improve-
ment in both cases. Discourse relations were also
used successfully by (Zahri and Fukumoto, 2011)
for news summarization.

However, the work described above have been
developed for formal, well-written and factual
documents. Most of these work show how dis-
course relations can be used in text summarization
and show their overall usefulness. To the best of
our knowledge, our work is the first to measure the
effect of specific relations on the summarization of
informal and opinionated text.

3 Tagging Discourse Relations

To evaluate the effect of discourse relations on a
large scale, sentences need to be tagged automat-
ically with discourse relations. For example, the
sentence “Yesterday, I stayed at home because it

1http://www.nist.gov/tac/
2http://www-nlpir.nist.gov/projects/duc/guidelines/2007.html

was raining.” needs to be tagged as containing
a cause relation. One sentence can convey zero
or several discourse relations. For example, the
sentence “Starbucks has contributed to the popu-
larity of good tasting coffee” does not contain any
discourse relations of interest to us. On the other
hand, the sentence “While I like the Zillow inter-
face and agree it’s an easy way to find data, I’d
prefer my readers used their own brain to perform
a basic valuation of a property instead of relying
on zestimates.” contains 5 relations of interest:
one comparison, three illustrations, and one attri-
bution.

3.1 Most Frequent Discourse Relations

Since our work is performed within the frame-
work of blog summarization; we have only con-
sidered the discourse relations that are most useful
to this application. To find the set of the relations
needed for this task, we have first manually ana-
lyzed 50 summaries randomly selected from par-
ticipating systems at the TAC 2008 opinion sum-
marization track and 50 randomly selected blogs
from BLOG06 corpus3. In building our relation
taxonomy, we considered all main discourse rela-
tions listed in the taxonomy of Mann and Thomp-
son’s Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988). These discourse relations
are also considered in Grimes’ (Grimes, 1975) and
Williams’ predicate lists. From our corpus anal-
ysis, we have identified the six most prevalent
discourse relations in this blog dataset, namely
comparison, contingency, illustration, attribution,
topic-opinion, and attributive. The comparison,
contingency, and illustration relations are also
considered by most of the work in the field of dis-
course analysis such as the PDTB: Penn Discourse
TreeBank research group (Prasad et al., 2008) and
the RST Discourse Treebank research group (Carl-
son and Marcu, 2001). We considered three ad-
ditional classes of relations: attributive, attribu-
tion, and topic-opinion. These discourse relations
are summarized in Figure 1 while a description of
these relations is given below.

Illustration: Is used to provide additional infor-
mation or detail about a situation. For example:
“Allied Capital is a closed-end management in-
vestment company that will operate as a business
development concern.”

3http://ir.dcs.gla.ac.uk/test collections/blog06info.html

1402



Figure 1: Most Frequent Discourse Relations in
Blogs and their Sub-relations

As shown in Figure 1, illustration relations can
be sub-divided into sub-categories: joint, list, dis-
joint, and elaboration relations according to the
RST Discourse Treebank (Carlson and Marcu,
2001) and the Penn Discourse TreeBank (Prasad
et al., 2008).

Contingency: Provides cause, condition, reason
or evidence for a situation, result or claim. For
example: “The meat is good because they slice it
right in front of you.”

As shown in Figure 1, the contingency rela-
tion subsumes several more specific relations: ex-
planation, evidence, reason, cause, result, conse-
quence, background, condition, hypothetical, en-
ablement, and purpose relations according to the
Penn Discourse TreeBank (Prasad et al., 2008).

Comparison: Gives a comparison and contrast
among different situations. For example, “Its fast-
forward and rewind work much more smoothly
and consistently than those of other models I’ve
had.”

The comparison relation subsumes the contrast
relation according to the Penn Discourse Tree-
Bank (Prasad et al., 2008) and the analogy and
preference relations according to the RST Dis-
course Treebank (Carlson and Marcu, 2001).

Attributive: Relation provides details about an
entity or an event - e.g. “Mary has a pink coat.”. It
can be used to illustrate a particular feature about
a concept or an entity - e.g. “Picasa makes sure
your pictures are always organized.”. The at-
tributive relation, also included in Grimes’ pred-
icates (Grimes, 1975), is considered because it de-
scribes attributes or features of an object or event
and is often used in query-based summarization
and question answering.

Topic-opinion: We introduced topic-opinion re-
lations to represent opinions which are not ex-
pressed by reported speech. This relation can be
used to express an opinion: an internal feeling or
belief towards an object or an event. For example:
“Cage is a wonderfully versatile actor.”

Attribution: These relations are instances of re-
ported speech both direct and indirect which may
express feelings, thoughts, or hopes. For exam-
ple: “The legendary GM chairman declared that
his company would make “a car for every purse
and purpose.””

3.2 Automatic Discourse Tagging

Once the manual analysis identified the most
prevalent set of relations, we tried to measure their
frequency by tagging them automatically within
a larger corpus. Only recently, the HILDA (Her-
nault et al., 2010) and (Feng and Hirst, 2012)’s dis-
course parser were made publicly available. Both
of these parsers work at the text-level, as opposed
to the sentence-level, and hence currently achieve
the highest tagging performance when compared
to the state of the art. (Feng and Hirst, 2012)’s
work showed a significant improvement on the
performance of HILDA by enhancing its original
feature set. However, at the time this research was
done, the only publicly available discourse parser
was SPADE (Soricut and Marcu, 2003) which op-
erates on individual sentences. To identify illus-
tration, contingency, comparison, and attribution
relations, we have used SPADE discourse parser.
However, we have complemented this parser with
three other approaches: (Jindal and Liu, 2006)’s
approach is used to identify intra-sentence com-
parison relations; we have designed a tagger based
on (Fei et al., 2008)’s approach to identify topic-
opinion relations; and we have proposed a new ap-
proach to tag attributive relations (Mithun, 2012).
A description and evaluation of these approaches
can be found in (Mithun, 2012). By combining
these approaches, a sentence is tagged with all
possible discourse relations that it contains.

3.3 Distribution of Discourse Relations

To find the most prevalent discourse relations for
opinion summarization, we have used the TAC
2008 opinion summarization track input document
set (collection) which is a subset of BLOG06 and
the answer nuggets provided by TAC 2008 as the
reference summary (or model summaries), which

1403



had been created to evaluate participants’ sum-
maries at the TAC 2008 opinion summarization
track. The collection consists of 600 blogs on 28
different topics. The dataset of the model sum-
maries consists of 693 sentences.

Using the discourse parsers presented in Section
3.2, we computed the distribution of discourse re-
lations within the TAC 2008 opinion summariza-
tion collection and the model summaries. Illustra-
tion, contingency, comparison, attributive, topic-
opinion, and attribution are the most frequently
occuring relations in our data sets. The distribu-
tion is shown in Table 1 4.

Table 1: Distribution of Discourse Relations in the
TAC-2008 and DUC-2007 Datasets

Discourse TAC 2008 DUC 2007
Relation Coll. Model Coll. Model
Illustration 52% 46% 42% 38%
Contingency 31% 37% 34% 29%
Comparison 23% 18% 15% 12%
Attributive 12% 28% 3% 4%
Topic-opinion 14% 15% 4% 5%
Attribution 11% 9% 2% 3%
other 13% 9% 28% 31%
none 14% 10% 8% 7%

Table 1 shows that in the TAC 2008 input doc-
ument set, the illustration relation occurs in 52%
of the sentences; while attribution is the least fre-
quently occurring relation. In this dataset, other
relations, such as antithesis and temporal rela-
tions, occur in about 13% of the sentences and
about 14% of the sentences did not receive any re-
lation tag. As indicated in Table 1, the TAC model
summaries have a similar distribution as the col-
lection as a whole. The attributive relation seems,
however, to be more frequent in the summaries
(28%) than in the original texts (12%). We sus-
pect that the reason for this is due to the question
types of this track. To successfully generate query-
relevant summaries that answer the questions of
this track, candidate sentences need to contain at-
tributive relations. For example, to answer the
questions from this track “Why do people like Pi-
casa?” or “What features do people like about
Windows Vista?”, the summary needs to provide
details about these entities or illustrate a particular
feature about them. As a result, the summary will
be composed of many attributive relations since

4In Table 1, the percentages do not add up to 100 because
a sentence may contain more than one relation.

attributive relations help to model the required in-
formation.

To compare the distribution of discourse rela-
tions within more formal types of texts such as
news articles, we used the Document Understand-
ing Conference (DUC) 2007 Main Task input doc-
ument set (collection) and their associated model
summaries. The DUC 2007 dataset is a news arti-
cle based dataset from the AQUAINT corpus. The
DUC 2007 input document set contains 1125 news
articles on 45 different topics. The model sum-
maries were used to evaluate the DUC 2007 par-
ticipants’ summaries. The dataset of the model
summaries contains 180 summaries generated by
the National Institute of Standards and Technol-
ogy (NIST) assessors with a summary length of
about 250 words. The distribution of relations in
this dataset are shown in Table 1.

Table 1 shows that the most frequently occur-
ring relation in the DUC 2007 document collec-
tion and in the model summaries is illustration;
while the attribution relation is the least frequently
occurring relation. Here again, it is interesting to
note that the distribution of the discourse relations
in the document collection and in the model sum-
maries is generally comparable.

The distribution of the illustration, contingency,
and comparison relations in the DUC 2007 dataset
is comparable to those in the TAC 2008 opinion
summarization dataset. Indeed, Table 1 shows
that illustration, contingency, and comparison re-
lations occur quite frequently irrespective of the
textual genre. However, in contrast to the TAC
dataset, attributive, topic-opinion, and attribution
relations occur very rarely in DUC 2007. We
suspect that this is mostly due to the opinion-
ated nature of blogs. Another observation is that
temporal relations (included in “other”) occurred
very frequently (30%) in the DUC 2007 dataset
whereas this relation occurs rarely in the blog
dataset. This is inline with our intuition that news
articles present events that inherently contain tem-
poral information.

4 Evaluation of Discourse Relations

To measure the usefulness of discourse relations
for the summarization of informal texts, we have
tested the effect of each relation with four dif-
ferent summarizers: BlogSum (Mithun, 2012),
MEAD (Radev et al., 2004), the best scoring sys-

1404



tem at TAC 20085 and the best scoring system
at DUC 20076. We have evaluated the effect of
each discourse relation on the summaries gener-
ated and compared the results. Let us first describe
the BlogSum summarizer.

4.1 BlogSum

BlogSum is a domain-independent query-based
extractive summarization system that uses intra-
sentential discourse relations within the frame-
work based on text schemata. The heart of Blog-
Sum is based on discourse relations and text
schemata.

BlogSum works in the following way: First
candidate sentences are extracted and ranked
using the topic and question similarity to give
priority to topic and question relevant sentences.
Since BlogSum has been designed for blogs,
which are opinionated in nature, to rank a sen-
tence, the sentence polarity (e.g. positive, negative
or neutral) is calculated and used for sentence
ranking. To extract and rank sentences, BlogSum
thus calculates a score for each sentence using the
features shown below:

Sentence Score = w1 × Question Similarity +
w2 × Topic Similarity +
w3 × Subjectivity Score

where, question similarity and topic similarity
are calculated using the cosine similarity based on
words tf.idf and the subjectivity score is calcu-
lated using a dictionary-based approach based on
the MPQA lexicon7. Once sentences are ranked,
they are categorized based on the discourse rela-
tions that they convey. This step is critical because
the automatic identification of discourse relations
renders BlogSum independent of the domain. This
step also plays a key role in content selection and
summary coherence as schemata are designed us-
ing these relations.

In order not to answer all questions the same
way, BlogSum uses different schemata to gen-
erate a summary that answers specific types of
questions. Each schema is designed to give pri-
ority to its associated question type and subjec-
tive sentences as summaries for opinionated texts
are generated. Each schema specifies the types
of discourse relations and the order in which they
should appear in the output summary for a par-

5http://www.nist.gov/tac/
6http://www-nlpir.nist.gov/projects/duc/guidelines/2007.html
7MPQA: http://www.cs.pitt.edu/mpqa

ticular question type. Figure 2 shows a sample
schema that is used to answer reason questions
(e.g. “Why do people like Picasa?”). According
to this schema8, one or more sentences containing
a topic-opinion or attribution relation followed by
zero or many sentences containing a contingency
or comparison relation followed by zero or many
sentences containing a attributive relation should
be used.

Figure 2: A Sample Discourse Schema used in
BlogSum

Finally the most appropriate schema is selected
based on a given question type; and candidate sen-
tences fill particular slots in the selected schema
based on which discourse relations they contain in
order to create the final summary (details of Blog-
Sum can be found in (Mithun, 2012)).

4.2 Evaluation of Discourse Relations on
Blogs

To evaluate the effect of each discourse relation for
blog summarization, we performed several exper-
iments. We used as a baseline the original ranked
list of candidate sentences produced by BlogSum
before applying the discourse schemata, and com-
pared this to the BlogSum-generated summaries
with and without each discourse relation. We
used the TAC 2008 opinion summarization dataset
which consists of 50 questions on 28 topics; on
each topic one or two questions were asked and
9 to 39 relevant documents were given. For each
question, one summary was generated with no re-
gards to discourse relations and two summaries
were produced by BlogSum: one using the dis-
course tagger and the other without using the spe-
cific discourse tagger. The maximum summary
length was restricted to 250 words.

To measure the effect of each relation, we have
automatically evaluated how BlogSum performs
using the standard ROUGE-2 and ROUGE-SU4

8The notation / indicates an alternative, { } indicates op-
tionality, * indicates that the item may appear 0 to n times
and + indicates that the item may appear 1 to n times

1405



measures. For comparative purposes, Table 2
shows the official ROUGE-2 (R-2) and ROUGE-
SU4 (R-SU4) for all 36 submissions of the TAC
2008 opinion summarization track. In the table,
“TAC Average” refers to the mean performance of
all participant systems and “TAC-Best” refers to
the best-scoring system at TAC 2008.

Table 2: Results of the TAC 2008 Opinion Sum-
marization Track

System Name R-2 R-SU4
TAC Average 0.069 0.086
TAC-Best 0.130 0.139

Table 3: Effect of Discourse Relations on
ROUGE-2 with the TAC 2008 Dataset

System Name BlogSum MEAD TAC-Best
R-2 R-2 R-2

Baseline 0.102⇓ 0.041⇓ 0.130
w/o Illustration 0.107⇓ 0.022⇓ 0.112⇓
w/o Contingency 0.093⇓ 0.025⇓ 0.102⇓
w/o Comparison 0.103⇓ 0.033⇓ 0.113⇓
w/o Attributive 0.113⇓ 0.050 0.124
w/o Topic-opinion 0.112⇓ 0.049 0.123
w/o Attribution 0.118⇓ 0.051⇓ 0.128
with all Relations 0.125 .053 0.138

Table 4: Effect of Discourse Relations on
ROUGE-SU4 with the TAC 2008 Dataset

System Name BlogSum MEAD TAC-Best
R-SU4 R-SU4 R-SU4

Baseline 0.107⇓ 0.064⇓ 0.139
w/o Illustration 0.110⇓ 0.041⇓ 0.120⇓
w/o Contingency 0.102⇓ 0.046⇓ 0.110⇓
w/o Comparison 0.108⇓ 0.052⇓ 0.122⇓
w/o Attributive 0.115⇓ 0.072 0.130
w/o Topic-opinion 0.117 0.072 0.129
w/o Attribution 0.127⇓ 0.073⇓ 0.132
with all Relations 0.128 0.075 0.151

The results of our evaluation are shown in Ta-
bles 3 (ROUGE-2) and 4 (ROUGE-SU4). As
the tables show, BlogSum’s baseline is situated
below the best scoring system at TAC-2008, but
much higher than the average system (see Ta-
ble 2); hence, it represents a fair baseline. The
tables further show that using both the ROUGE-2
(R-2) and ROUGE-SU4 (R-SU4) metrics, with the
TAC 2008 dataset, BlogSum performs better when
taking discourse relations into account. Indeed,
when ignoring discourse relations, BlogSum has
a R2=0.102 and R-SU4=0.107 and misses many
question relevant sentences; whereas the inclusion
of these relations helps to incorporate those rele-
vant sentences into the final summary and brings

the R-2 score to 0.125 and R-SU4 to 0.128. In
order to verify if these improvements were sta-
tistically significant, we performed a 2-tailed t-
test. The results of this test are indicated with
the ⇓ symbol in Tables 3 and 4. For example,
the baseline setup of BlogSum performed signifi-
cantly lower for both R-2 and R-SU4 compared to
BlogSum with all relations. This result indicates
that the use of discourse relations as a whole helps
to include more question relevant sentences and
improve the summary content.

To ensure that the results were not specific to
our summarizer, we performed the same experi-
ments with two other systems: the MEAD sum-
marizer (Radev et al., 2004), a publicly available
and a widely used summarizer, and with the output
of the TAC best-scoring system. For MEAD, we
first generated candidate sentences using MEAD,
then these candidate sentences were tagged using
discourse relation taggers used under BlogSum.
Then these tagged sentences were filtered using
BlogSum so that no sentence with a specific re-
lation is used in summary generation for a par-
ticular experiment. We have calculated ROUGE
scores using the original candidate sentences gen-
erated by MEAD and also using the filtered candi-
date sentences. As a baseline, we used the origi-
nal candidate sentences generated by MEAD. As a
best case scenario, we have passed these candidate
sentences through the discourse schemata used by
BlogSum (see Section 4.1). In Tables 3 and 4, this
is referred to as “MEAD with all relations”. We
have applied the same approach with the output of
the TAC best-scoring system. In the tables, “TAC-
Best Baseline” refers to the original summaries
generated by the TAC-Best system and “TAC-Best
with all relations” refers to the summaries gen-
erated by applying discourse schemata using the
summary sentences generated by the TAC-Best
system.

When looking at individual relations, Tables 3
and 4 show that considering illustrations, contin-
gencies and comparisons make a statistically sig-
nificant improvement in all scenarios, and with all
summarisers. For example, if TAC-Best does not
consider illustration relations, then the R-2 score
decreases from 0.138 to 0.112, 0.102 and 0.113,
respectively. On the other hand, the relations of
topic-opinion, attribution, and attributive do not
consistently lead to a statistically significant im-
provement on ROUGE scores.

1406



It is interesting to note that although infor-
mal texts may not exhibit a clear discourse struc-
ture, the use of individual discourse relations such
as illustration, contingency and comparison is
nonetheless useful in the analysis of informal doc-
uments such as those found in the social media.

4.3 Effect of Discourse Relations on News
To compare the results found with blogs with more
formal types of texts, we have performed the same
experiments but, this time with the DUC 2007
Main Task dataset. In this task, given a topic
(title) and a set of 25 relevant documents, par-
ticipants had to create an automatic summary of
length 250 words from the input documents. In the
dataset, there were 45 topics and thirty teams par-
ticipated to this shared task. Table 5 shows the of-
ficial ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4)
scores of the DUC 2007 main task summarization
track. In Table 5, “DUC Average” refers to the
mean parformance of all participant systems and
“DUC-Best” refers to the best scoring system at
DUC 2007.

Table 5: DUC 2007 Main Task Summarization
Results

System Name R-2 R-SU4
DUC Average 0.095 0.157
DUC-Best 0.124 0.177

Table 6: Effect of Discourse Relations on
ROUGE-2 with the DUC 2007 Dataset

System Name BlogSum MEAD DUC-Best
R-2 R-2 R-2

Baseline 0.089 0.099 0.124⇓
w/o Illustration 0.079⇓ 0.061⇓ 0.103⇓
w/o Contingency 0.074⇓ 0.060⇓ 0.097⇓
w/o Comparison 0.086⇓ 0.078⇓ 0.114⇓
w/o Attributive 0.092 0.099 0.119⇓
w/o Topic-opinion 0.092 0.099 0.115⇓
w/o Attribution 0.093 0.099 0.120⇓
with all Relations 0.093 0.110 0.157

Tables 6 and 7 show the results with this
dataset with respect to ROUGE-2 and ROUGE-
SU4, respectively. As the tables show, Blog-
Sum’s performance with all discourse relations
(R2=0.093 and R-SU4=0.132) is similar to the
DUC average performance shown in Table 5
(R2=00.095 and R-SU4=0.157) which is much

Table 7: Effect of Discourse Relations on ROUGE
SU-4 with the DUC 2007 Dataset

System Name BlogSum MEAD DUC-Best
R-SU4 R-SU4 R-SU4

Baseline 0.110⇓ 0.142⇓ 0.177⇓
w/o Illustration 0.117⇓ 0.118⇓ 0.138⇓
w/o Contingency 0.113⇓ 0.118⇓ 0.123⇓
w/o Comparison 0.122⇓ 0.130⇓ 0.144⇓
w/o Attributive 0.131 0.141⇓ 0.159⇓
w/o Topic-opinion 0.130 0.141⇓ 0.153⇓
w/o Attribution 0.131 0.142⇓ 0.164⇓
with all Relations 0.132 0.168 0.196

lower than the DUC-Best performance (R2=0.124,
R-SU4=0.177) shown in Table 5). However, these
results show that even though BlogSum was de-
signed for informal texts, it still performs rela-
tively well with formal documents. Tables 6 and
7 further show that with the news dataset, the
same relations have the most effect as with blogs.
Indeed BlogSum generated summaries also ben-
efit most from the contingency, illustration, and
comparison relations; and all three relations bring
a statistically significant contribution to the sum-
mary content.

Here again, as shown in Tables 6 and 7, we
performed the same experiments with two other
systems: the MEAD summarizer and the output
of the DUC-Best system. Again, for the DUC
2007 dataset, each discourse relation has the
same effect on summarization with all systems as
with the blog dataset: contingency, illustration,
and comparison provide a statistically significant
improvement in content; while attributive, topic-
opinion and attribution do not reduce the content,
but do not see to bring a systematic and significant
improvement.

5 Conclusion and Future Work

In this paper, we have evaluated the effect of dis-
course relations on summarization. We have con-
sidered the six most frequent relations in blogs -
namely comparison, contingency, illustration, at-
tribution, topic-opinion, and attributive. First, we
have measured the distribution of discourse rela-
tions on blogs and on news articles and show that
the prevalence of these six relations is not genre
dependent. For example, the relations of illus-
tration, contingency, and comparison occur fre-
quently in both textual genres. We have then eval-
uated the effect of these six relations on summa-

1407



rization with the TAC 2008 opinion summariza-
tion dataset and the DUC 2007 dataset. We have
conducted these evaluations with our summariza-
tion system called BlogSum, the TAC best-scoring
system, the DUC best-scoring system, and the
MEAD summarizer. The results show that for both
textual genres, some relations have more effect on
summarization compared to others. In both types
of texts, the contingency, illustration, and compar-
ison relations provide a significant improvement
on summary content; while the attribution, topic-
opinion, and attributive relations do not provide
a systematic and statistically significant improve-
ment. These results seem to indicate that, at least
for summarization, discourse relations are just as
useful for informal and affective texts as for more
traditional news articles. This is interesting, be-
cause although informal texts may not exhibit a
clear discourse structure, the use of individual dis-
course relations is nonetheless useful in the analy-
sis of informal documents.

In the future, it would be interesting to evaluate
the effect of other relations such as the temporal
relation. Indeed, temporal relations occur infre-
quently in blogs but are very frequent in news arti-
cles. Such an analysis would allow us to tailor the
type of discourse relations to include in the final
summary as a function of the textual genre being
considered. In the future, it would also be inter-
esting to use other types of texts such as reviews
and evaluate the effect of discourse relations using
other measures than ROUGE-2 and ROUGE-SU4.
Finally, we would like to validate this work again
with the newly available discourse parsers of (Her-
nault et al., 2010) and (Feng and Hirst, 2012).

Acknowledgement

The authors would like to thank the anonymous
referees for their valuable comments on an earlier
version of the paper. This work was financially
supported by an NSERC grant.

References

Andreevskaia, A., Bergler, S., Urseanu, M.: All Blogs
are Not Made Equal: Exploring Genre Differences
in Sentiment Tagging of Blogs. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM-2007),(2007), Boulder, Colorado.

Blair-Goldensohn, S.J., McKeown, K.: Integrating
Rhetorical-Semantic Relation Models for Query-
Focused Summarization. In Proceedings of the Doc-

ument Understanding Conference (DUC) Workshop
at NAACL-HLT 2006, (2006), New York, USA.

Bosma, W.: Query-Based Summarization using
Rhetorical Structure Theory. In Proceedings of the
15th Meeting of Computational Linguistics in the
Netherlands CLIN, (2004), Leiden, Netherlands.

Carlson, L., Marcu, D.: Discourse Tagging Reference
Manual. University of Southern California Informa-
tion Sciences Institute, ISI-TR-545, 2001.

Fei, Z., Huang, X., Wu, L.: Mining the Relation be-
tween Sentiment Expression and Target Using De-
pendency of Words. Proceedings of the 20th Pa-
cific Asia Conference on Language, Information and
Computation, 257–264 (2008), Wuhan, China.

Feng, V. W., Hirst, G.: Text-level Discourse Parsing
with Rich Linguistic Features. In Proceedings of
ACL-2012, 60–68 (2012), Stroudsburg, USA.

Grimes, J. E.: The Thread of Discourse. Technical re-
port No. NSF-TR-1, NSF-GS-3180. Cornell Univer-
sity, Ithaca, New York, 1975.

Hernault, H., Prendinger. H., duVerle, D. A., Ishizuka,
M.: HILDA: A discourse parser using support vector
machine classification. J. Dialogue and Discourse,
1(3):1–33, 2010.

Jindal, N., Liu, B.: Identifying Comparative Sentences
in Text Documents. In Proceedings of SIGIR-2006,
244-251 (2006), Washington, USA.

Mann, W.C., Thompson, S. A.: Rhetorical Structure
Theory: Toward a Functional Theory of Text Organ-
isation. J. Text, 3(8):234–281, 1988.

Marcu, D.: From Discourse Structures to Text Sum-
maries. Proceedings of the ACL’97/EACL’97 Work-
shop on Intelligent Scalable Text Summarization.
1997, 82–88, Madrid, Spain.

McKeown, K.R.: Discourse Strategies for Generat-
ing Natural-Language Text. J. Artificial Intelligence,
27(1):1–41, 1985.

Mithun, S.: Exploiting Rhetorical Relations in Blog
Summarization. PhD Thesis, Department of Com-
puter Science and Software Engineering, Concordia
University, Montreal, Canada, 2012.

Otterbacher, J. C., Radev, D. R., Luo, A.: Revisions
that Improve Cohesion in Multi-document Sum-
maries: A Preliminary Study. In Proceedings of the
ACL-2002 Workshop on Automatic Summarization,
27–36 (2002), Philadelphia, USA.

Prasad, R., Miltsakaki, E., Dinesh, N., Lee, A., Joshi,
A., Robaldo, L., Webber, B.: The Penn Discourse
Treebank 2.0. Annotation Manual. University of
Pennsylvania, IRCS-08-01, 2008.

Radev, D. et al.: MEAD -A Platform for Multidocu-
ment Multilingual Text Summarization. In Proceed-
ings of LREC-2004, 1–4 (2004), Lisbon, Portugal.

1408



Soricut, R., Marcu, D.: Sentence Level Discourse Pars-
ing using Syntactic and Lexical Information. In Pro-
ceedings of NAACL/HLT 2003, 149–156 (2003), Ed-
monton, Canada.

Taboada, M.: Discourse Markers as Signals (or not) of
Rhetorical Relations. J. Pragmatics, 38(4):567–592,
2006.

Zahri, N. A. H. B., Fukumoto, F.: Multi-document
Summarization Using Link Analysis Based on
Rhetorical Relations between Sentences. In Pro-
ceedings of CICLing, 328–338 (2011), Tokyo,
Japan.

1409


