



















































D√©j√† Image-Captions: A Corpus of Expressive Descriptions in Repetition


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 504‚Äì514,
Denver, Colorado, May 31 ‚Äì June 5, 2015. c¬©2015 Association for Computational Linguistics

DeÃÅjaÃÄ Image-Captions: A Corpus of Expressive Descriptions in Repetition

Jianfu Chen‚Ä† and Polina Kuznetsova‚Ä† and David S. Warren‚Ä† and Yejin Choi‚Ä°
Stony Brook University‚Ä† University of Washington‚Ä°

{jianchen,pkuznetsova,warren}@cs.stonybrook.edu‚Ä†, yejin@cs.washington.edu‚Ä°

Abstract

We present a new approach to harvesting a
large-scale, high quality image-caption corpus
that makes a better use of already existing web
data with no additional human efforts. The
key idea is to focus on DeÃÅjaÃÄ Image-Captions:
naturally existing image descriptions that are
repeated almost verbatim ‚Äì by more than one
individual for different images. The resulting
corpus provides association structure between
4 million images with 180K unique captions,
capturing a rich spectrum of everyday narra-
tives including figurative and pragmatic lan-
guage. Exploring the use of the new corpus,
we also present new conceptual tasks of visu-
ally situated paraphrasing, creative image cap-
tioning, and creative visual paraphrasing.

1 Introduction
The use of multimodal web data has been a recur-
ring theme in many recent studies integrating lan-
guage and vision, e.g., image captioning (Ordonez
et al., 2011; Hodosh et al., 2013; Mason and Char-
niak, 2014; Kuznetsova et al., 2014), text-based im-
age retrieval (Rasiwasia et al., 2010; Rasiwasia et
al., 2007), and entry-level categorization (Ordonez
et al., 2013; Feng et al., 2015).

However, much research integrating complex tex-
tual descriptions to date has been based on datasets
that rely on substantial human curation or annota-
tion (Hodosh et al., 2013; Rashtchian et al., 2010;
Lin et al., 2014), rather than using the web data in
the wild as is (Ordonez et al., 2011; Kuznetsova et
al., 2014). The need for human curation limits the
potential scale of the multimodal dataset. Without
human curation, however, the web data introduces
significant noise. In particular, everyday captions

often contain extraneous information that is not di-
rectly relevant to what the image shows (Kuznetsova
et al., 2013b; Hodosh et al., 2013).

In this paper, we present a new approach to har-
vesting a large-scale, high quality image-caption
corpus that makes a better use of already existing
web data with no additional human efforts. Figure 1
shows sample captions in the resulting corpus, e.g.,
‚Äúbutterfly resting on a flower‚Äù and ‚Äúevening walk
along the beach‚Äù. Notably, some of these are figu-
rative, e.g., ‚Äúrippled sky‚Äù and ‚Äúsun is going to bed.‚Äù

The key idea is to focus on DeÃÅjaÃÄ Image-Captions,
i.e., naturally existing image captions that are re-
peated almost verbatim by more than one individ-
ual for different images. The hypothesis is that such
captions represent common visual content across
multiple images, hence are more likely to be free
of unwanted extraneous information (e.g., specific
names, time, or any other personal information) and
better represent visual concepts. A surprising as-
pect of our study is that such a strict data filtration
scheme can still result in a large-scale corpus; sifting
through 760 million image-caption pairs, we harvest
as many as 4 million image-caption pairs with 180K
unique captions.

The resulting corpus, DeÃÅjaÃÄ Image Captions, pro-
vides several unique properties that complement
human-curated or crowd-sourced datasets. First, as
our approach is fully automated, it can be readily
applied to harvesting a new dataset from the ever
changing multimodal web data. Indeed, a recent
internet report estimates that billions of new pho-
tographs are being uploaded daily (Meeker, 2014).
In contrast, human-annotated datasets are costly to
scale to different domains.

Second, datasets that are harvested from the web

504



Butterflies are self propelled flowers (198) 

butterfly resting on a flower (26) 

After the sun has set (9) 

Sun is going to bed (21)   

can you spot the butterfly (88) 

The sky looks like it is on fire (58) 

The sun sets for another day (12) 

Evening walk along the beach (9) 

Chillaxing at the beach (20) 

Walk by the beach (557)  Rippled sky (44) 

In the sky (1013) 

Figure 1: The image-caption association graph of DeÃÅjaÃÄ Image-Captions. Solid lines represent original captions and
dotted lines represent paraphrase captions. This corpus reflects a rich spectrum of everyday narratives people use in
online activities including figurative language (e.g., ‚ÄúSun is going to bed‚Äù), casual language (e.g., Chillaxing at the
beach‚Äù), and conversational language (e.g., ‚ÄúCan you spot the butterfly‚Äù). The numbers in the parenthesis show the
cardinality of images associated with each caption. Surprisingly, some of these descriptions are highly expressive,
almost creative, and yet not unique ‚Äî as all these captions are repeated almost verbatim by different individuals
describing different images.

can complement those based on prompted human
annotations. The latter in general are literal and
mechanical readings of the visual scenes, while the
former reflect a rich spectrum of natural language
utterances in everyday narratives, including figura-
tive, pragmatic, and conversational language, e.g.,
‚Äúcan you spot the butterfly‚Äù (Figure 1). Therefore,
this dataset offers unique opportunities for ground-
ing figurative and metaphoric expressions using vi-
sual context.

In conjunction with the new corpus, pub-
licly shared at http://www.cs.stonybrook.
edu/Àújianchen/deja.html, we also present
three new tasks: visually situated paraphrases (¬ß5);
creative image captioning (¬ß7), and creative visual
paraphrasing (¬ß7). The central algorithm compo-
nent in addressing all these tasks is a simple and yet
effective approach to image caption transfer that ex-
ploits the unique association structure of the result-
ing corpus (¬ß3).

Our empirical results collectively demonstrate
that when the web data is available at such scale,
it is possible to obtain a large-scale, high-quality
dataset with significantly less noise. We hope that
our approach would be only one of the first attempts,
and inspire future research to develop better ways of
making use of ever-growing multimodal web data.
Although it is unlikely that the automatically gath-
ered datasets can completely replace the curated de-
scriptions written in a controlled setting, our hope
is to find ways to complement human annotated
datasets in terms of both the scale and also the di-
versity of the domain and language.

The remainder of this paper is organized as fol-
lows. First we describe the dataset collection proce-
dure and insights (¬ß2). We then present a new ap-
proach to image caption transfer based on the asso-
ciation structure of the corpus (¬ß3) followed by ex-
perimental results (¬ß4). After then we present new
conceptual tasks: visual paraphrasing (¬ß5), creative
image captioning, and creative visual paraphrasing
(¬ß7), interleaved with corresponding experimental
results (¬ß6, ¬ß8).
2 Dataset - Captions in Repetition
Our corpus consists of three components (Table 1):
MAIN SET The first step is to crawl as many
image-caption pairs as possible. We use flickr.com
search API to crawl 760 million pairs in total. The
API allows searching images within a given time
window, which enables exhaustive search over any
time span. To ensure visual correspondence between
images and captions, we set query terms using 693
most frequent nouns from the dataset of Ordonez et
al. (2011), and systematically slide time windows
over the year 2013.1 For each image, we segment
its title and the first line of its description into sen-
tences.

The crawled dataset at this point includes a lot of
noise in the captions. Hence we apply initial filter-
ing rules to reduce the noise. We retain only those
image-sentence pairs in which the sentence contains
the query noun, and does not contain personal infor-
mation indicators such as first-person pronouns. We

1To ensure enough number of images are associated with
each caption, we further search captions with no more than 10
associated images across all years.

505



set # captions # images
MAIN 176,780 3,967,524

PARAPHRASE
7,570 human-annotated triples
353,560 auto-generated triples

FIGURATIVE
6,088 quotations 180,185
18,179 quotations +
predicted figurative captions

413,698

Table 1: Corpus Statistics

mean std 25% 50% 75% max
#imgs. 22.4 47.6 4 10 25 4617
#tokens 4.9 3.3 3 4 5 178

Table 2: Percentiles of the image count associated with
each caption and the number of tokens in each caption.

want captions that are more than simple keywords,
thus we discard trivial captions that do not include
at least one verb, preposition, or adjective.

The next step is to find captions in repetition. For
this purpose, we transform captions into canonical
forms. We lemmatize all words, convert prepositions
to a special token ‚ÄúIN‚Äù2, and discard function words,
numbers, and punctuations. For instance, ‚ÄúThe bird
flies in blue sky‚Äù and ‚ÄúA bird flying into the blue sky‚Äù
have the same canonical form, ‚Äúbird fly IN blue sky‚Äù.
We then retain only those captions that are repeated
with respect to their canonical forms by more than
one user, and for distinctly different images to en-
sure the generality of the captions.

Retaining only captions that are repeated verba-
tim may seem overly restrictive. Nonetheless, be-
cause we start with as many as 760 million pairs,
this procedure yields nearly 180K unique captions
associated with nearly 4M images.3 What is more
surprising, as will be shown later, is that many of
these captions are highly expressive. Table 2 shows
the distribution of the number of images associated
with each caption.4 The median and mean are 10
and 22.4 respectively, showing a high degree of con-
nectivities between captions and images.

PARAPHRASE SET Our dataset collection proce-
dure finds one-to-many relations between captions

2We do this transformation so as not to over-count unique
captions with trivial variations, but merging prepositions can
sometimes combine prepositions that are not semantically com-
patible. We therefore also keep original captions with original
prepositions.

3We also keep user annotated image tags if available.
4Without counting additional edges created by visual para-

phrasing (¬ß5).

- Hanging out with dad (*) 
- Snuggling with dad 
- Cuddles with dad 

- Life on the ocean waves (*) 
- Swimming in the ocean 
- Playing in the ocean 

Playing(in(the(ocean(

- Good morning sun (*) 
- Sun through the trees 
- Here comes the sun 

Automatic Visual Paraphrases  

- Fly high in the sky (*) 
- Stretching to the sky 
- Reaching out to the sky 

!smiling(children(

- Children see magic   
because they look for it (*) 
- The soul is healed by 
being with children 

Stretching(to(the(sky(.(

Reaching(out(to(the(sky(

- A bee collecting pollen (*) 
- Bumble bee on purple flower 
- Working bee 

!Bumble(bee((
on(purple(flower(

!Working(bee(

There(is(a(storm(rolling(in(
Storm(clouds(coming(over(
Big(storm(is(coming(

!Big(storm(is(coming(!The(soul(is(healed(by(
being(with(children((

Crowd-sourced Visual Paraphrases  

!Storm(clouds((
coming(over(

Figure 2: Example visual paraphrases: automatic (left)
and crowd-sourced (right). The first caption marked with
* indicates the original caption of the corresponding im-
age. Some paraphrases are not strictly equivalent to the
original caption if considered out of context, while they
are pragmatically adequate paraphrases given the image.

figure of speech #caps. example (#imgs.)
quotation&idiom 70 The early bird gets the worm (77)

personification 43 Meditating cat (38)
metaphor 24 Wine is the answer (7)
question 18 Do you see the moon (82)

dialog 11 Hello little flower (37)
anaphora 6 Beads, beads and more beads (62)

simile 5 The lake is like glass (23)
hyperbole 1 In the land of a billion lights (3)

Table 3: Distribution of figurative language out of 1000
random captions (171 figurative captions in total)

and images. To extend these relations to many-to-
many, we introduce visually-situated paraphrases
(or visual paraphrases for shorthand) (¬ß5). A visual
paraphrase relation is a triple (i, c, p), where image
i has an original caption c, caption p is the visual
paraphrase for c situated in image i. We collect vi-
sual paraphrases for sample images in our dataset,
using both crowd sourcing (7,570 triples) and an au-
tomatic algorithm (353,560 triples) (see ¬ß5 for de-
tails). Figure 2 shows example visual paraphrases.

Formally, our corpus represents a bipartite graph
G = (T, V,E), in which the set of captions T and
the set of images V are connected by typed edges
e(c, i, t), where caption c ‚àà T , image i ‚àà V , and
edge type t ‚àà {original, paraphrase}, which de-
notes whether the image-caption association is given
by the original caption or by a visual paraphrase.

FIGURATIVE SET We find that many repeating
captions are surprisingly lengthy and expressive,
most of which turn out to be idiomatic expres-
sions and quotations, e.g., ‚Äúfaith is the bird that
feels the light when the dawn is still dark‚Äù from
Tagore‚Äôs poem. We look up goodreads.com

506



tree light flower home sun rain sky water beach girl
0

20k

40k

60k

80k

100k

#images

tree light flower home sun rain sky water beach girl
0

1k

2k

3k

4k #captions

Figure 3: Top 10 queries with the largest number of im-
ages and unique captions

polarity
% in

all caps.
mean/median

#imgs. per cap.
example (#imgs)

pos. 8% 20 / 8
Happy bride and groom (282)
The rock and pool,
is nice and cool (4)

neg. 2% 19.5 / 7
Bad day at the office (269)
Crying lightning (147)

Table 4: Distribution of caption sentiment. The polar-
ity is determined by comparing number of positive words
and negative words (>: positive; <: negative) accord-
ing to a sentiment lexicon (Wilson et al., 2005) (counting
only words of strong polarity).

and brainyquotes.com to identify 6K quota-
tion captions illustrated by 180K images. We also
present a manual labeling on a small subset of the
data (Table 3) to provide better insights into the de-
gree and types of figurative speech used in natural
captions. Using these labels we build a classifier
(¬ß7) to further detect 18K figurative captions asso-
ciated with 410K images.
INSIGHTS As additional insights into the dataset,
Figure 3 shows statistics of the visual content, Ta-
ble 5 shows syntactic types of the captions, and Ta-
ble 4 shows positive and negative sentiment in cap-
tions.

3 Image Captioning using Association
Structure

We demonstrate the usefulness of the association be-
tween images and captions via retrieval-based image
captioning. Given a query image q and the corpus
G = (T, V,E), the task is to find a caption c ‚àà T
that maximizes an affinity function A(q, c), which
measures how well the caption c fits the query im-
age q,

c‚àó = arg max
c‚ààT

{A(q, c)} (1)

Visual Neighborhood: Each textual description,
e.g., ‚Äúreading a book‚Äù, can associate with many dif-

type %caps. %imgs. mean #imgs. std #imgs.

verb
45% 44% 22 9

be, have, do, look,
go, make, come, get,
wait, take, love, play,
walk, fly, see, watch,
find, live, sleep, fall

Sky is the limit (3057)
Home is where the heart is (2480)
Lunch is served (2443)
Let them eat cake (2193)
Follow the yellow brick road (2077)

prep 44% 41% 21 9
in, of, on,
at, with, for,
from, by,
over, through

On the road (4617)
After the rains (4450)
Under the bridge (3443)
At the beach (3203)

adj 11% 15% 30 15
old, little, new,
red, blue, more,
white, big, beautiful,
black

Home sweet home (2398)
Good morning sun (1122)
Cabbage white butterfly (976)
Next door neighbors (838)

Table 5: Statistics on the syntactic composition of cap-
tions. verb: captions with at least one verb. prep:
prepositional phrases (without any verbs). adj: adjective
phrases (without any verbs and prepositions). For each
caption type, we also show the top words that appear in
the most number of captions (left), and the top captions
that are associated with largest number of images (right).

ferent visual instantiations (Figure 4a). Our dataset
G = (T, V,E) serves as a database to navigate the
possible visual instantiations of descriptive captions
as observed in online photo sharing communities.
Let Nc = {i|e(c, i, original) ‚àà E} denote the
set of adjacent nodes (i.e., visual instantiations) of a
caption c. To quantify how well a caption c describe
a query image q, we propose to examine caption c‚Äôs
visual neighborhood Nc as provided in our dataset.
Concretely, the affinityA(q, c) of a query image q to
a caption c is a function œÜ(q,Nc) of q and the visual
neighborhood Nc defined as:

A(q, c) = œÜ(q,Nc) = 1
œÉ

œÉ‚àë
i=1

sim(q,N ic ) (2)

where œÉ is a parameter; sim(¬∑, ¬∑) is a similarity func-
tion of two images; and Nc = [N 1c ,N 2c , ...,N |Nc|c ]
is sorted by sim(q,N ic ) in descending order.

Figure 4a illustrates the key insight: instead of di-
rectly transferring the caption of the single image
with the closest visual similarity to the query im-
age (Ordonez et al., 2011), we propose to retrieve a
caption based on the aggregated visual similarity be-
tween its visual neighborhood and the query image.
The idea is to prefer a caption for which the query
image is likely to be a prototypical visual rendering
(Ordonez et al., 2013; Deselaers and Ferrari, 2011),
hence avoid an unusual association between the text

507



ùëê : Pelicans fly 
in formation

‚Ä¶

‚Ä¶

1

6

ùëê : Pelicans fly 
in formation

ùëê : Sunset 
over the seaQuery image q

Rank by image similarity

ùëñ

ùëñ

ùùì ùíí,ùëµùíÑùüè

ùùì ùíí,ùëµùë™ùüî

ùëê : Sunset 
over the sea

‚Ä¶

Final ranking

1

90

Rerank by neighborhood-based affinity

ùëÅ

ùëÅ

Original ranking

Reading a book

1. Can ùíÑ describe ùíí?

2. How well does ùê™ fits into ùëµùíÑ?

ùëµùíÑ: Visual Neighborhood of c
(a) (b)

‚Ä¶

‚Ä¶

‚Ä¶

ùëû

ùëû

ùëê

Figure 4: (a) Using the association structure, we retrieve a caption for which the query image is likely to be a
prototypical visual rendering. We hypothesize that there can be multiple visual prototypes of a caption. (b) Reranking
by visual neighborhood proximity.

and the visual information. Also, we hypothesize
that there could be several diverse visual prototypes
of any given textual description c, so we focus on
only the top œÉ nearest members of Nc.

We apply the neighborhood-based affinity for im-
age captioning via reranking (Figure 4b): first we
retrieve a pool of K candidate captions by find-
ing top K closest images based on their direct vi-
sual similarity to the query image, then compute the
neighborhood-based affinity to rerank the captions.5

The proposed approach is similar in spirit to the non-
parametric K nearest neighbor approach of (Boiman
et al., 2008) in modeling image-to-concept similar-
ity rather than image-to-image similarity, but differs
in that our work is in the context of image descrip-
tion generation rather than classification.

4 Experiments: Association Structure
Improves Image Captioning

Baselines: The proposed approach (to be referred
as ASSOC) requires one-to-many mappings between
captions and images at scale ‚Äî a unique prop-
erty of our dataset. We compare against two base-
lines: instance-based retrieval of (Ordonez et al.,
2011) (INSTANCE) and Kernel Canonical Correla-
tion Analysis (KCCA) (Hardoon et al., 2004; Hodosh
et al., 2013). We implement KCCA with Hardoon‚Äôs
code6. We use a linear kernel since non-linear ker-
nels like RBF showed worse performance.

5We set K = 100 and choose parameter œÉ using a held-out
development set of 300 images. If there are less than œÉ available
images, we use them all.

6http://www.davidroihardoon.com/Professional/
Code_files/kcca_package.tar.gz

method BLEU METEOR
INSTANCE 0.125 0.029
KCCA 0.118 0.024‚àó‚àó

ASSOCgi w/ all 0.130 0.031
ASSOCg+t w/ all 0.133 0.030
ASSOCti w/ all 0.126 0.029
ASSOCgi w/ œÉ 0.172‚àó‚àó 0.033‚àó

ASSOCg+t w/ œÉ 0.159‚àó‚àó 0.033‚àó

ASSOCti w/ œÉ 0.184‚àó‚àó 0.034‚àó‚àó

Table 6: Automatic evaluation for image captioning:
The superscripts denote the image feature for reranking;
gi: GIST; ti: Tinyimage; g+t:= gi + ti. We report the best
setting (gt) for INSTANCE and KCCA. Results statistically
significant compared to INSTANCE with two-tailed t-test
are indicated with * (p < 0.05) and ** (p < 0.005).

Configurations: For image features, we follow
(Ordonez et al., 2011) to experiment with two global
image descriptors and their combination: a) the GIST
feature that represents the dominant spatial struc-
ture of a scene (Oliva and Torralba, 2001); b) the
Tinyimage feature that represents the overall color
of an image (Torralba et al., 2008); c) a combi-
nation of the two. We compute the similarity as
sim(Q, I) = ‚àí‚ÄñQ‚àí I‚Äñ2. The INSTANCE and
the KCCA approaches use the feature combination.
The ASSOC approach also use the combination for
preparing candidate captions, but can use different
features for reranking.

Dataset: We randomly sample 1000 images with
unique captions as test set. The rest of the corpus is
the pool of caption retrieval after discarding: (1) the
original caption c and all of its associated images, to
avoid potential unfair advantage toward ASSOC and
(2) the 10K captions used for training KCCA and all

508



reranking feature INSTANCE ASSOC
gi 42% 58%

g+t 50% 50%
ti 46% 54%

Table 7: Human evaluation for image captioning: the
% of cases judged as visually more relevant, in pairwise
comparisons. gi: GIST; ti: Tinyimage; g+t:= gi+ti.

of their associated images (about 280K).
Evaluation. Automatic evaluation remains to be a
challenge (Elliott and Keller, 2014). We report both
BLEU (Papineni et al., 2002) at 1 without brevity
penalty, and METEOR (Banerjee and Lavie, 2005)
with balanced precision and recall. Table 6 shows
the results: the ASSOC approach (w/ œÉ) significantly
outperforms the two baselines. The largest improve-
ment over INSTANCE is 60% higher in BLEU, and
44% higher in METEOR, demonstrating the bene-
fit of the innate association structure of our corpus.
Using all visual neighborhood (ASSOC w/ all) does
not yield as strong results as selective neighborhood
(ASSOC w/ œÉ), confirming our hypothesis that each
visual concept can have diverse visual renderings.

We also compute crowd-sourced evaluation on a
subset (200 images) randomly sampled out of the
test set. For each query image, we present two cap-
tions generated by two competing methods in a ran-
dom order. Turkers choose the caption that is more
relevant to the visual content of the given image. We
aggregate the choices of three turkers by majority
voting. As shown in Table 7, ASSOC shows overall
improvement over baselines, where the difference is
more pronounced when reranking is based on fea-
ture sets that differ from the one used during the can-
didate retrieval.

5 Image Captioning using Visual
Paraphrases

We present an exploration of visually situated para-
phrase (or visual paraphrase in short hand), and
demonstrate their utility for image captioning. For-
mally, given our corpus G = (T, V,E), a visual
paraphrase relation is a triple (i, c, p), where given
an image i ‚àà V and its original caption c ‚àà T (i.e.,
e(c, i, original) ‚àà E), p ‚àà T is a visual paraphrase
for c situated in a visual context given by the image
i (i.e, e(p, i, paraphrase) ‚àà E). We collect visual
paraphrases using both human annotation and an au-
tomatic algorithm.

(1) Visual Paraphrasing using Crowd-sourcing:
We use Amazon Mechanical Turk to annotate vi-
sual paraphrases for a subset of images in our cor-
pus. Given each image with its original caption,
we showed 10 randomly sampled candidate captions
from our dataset that share at least one physical-
object noun7 with the original caption. Turkers
choose all candidate captions that could also de-
scribe the given image. We collect 7,570 (i, c, p)
paraphrase triples in total.

(2) Visual Paraphrasing using Associative Struc-
ture: We also propose an algorithm for automatic
visual paraphrasing by adapting the ASSOC algo-
rithm for image captioning (¬ß3) as follows: given
an image-caption pair (i, c), it first prepares a set of
candidate captions that share the largest number of
physical-object nouns with c, which are likely to be
semantically close to c; then we rerank the candidate
captions using the same neighborhood-based affinity
as described in ¬ß3.

We apply this algorithm to generate a large set of
visual paraphrases. For each caption in our corpus,
we randomly sample two of its associated images,
and generate one visual paraphrase for each image-
caption pair, which yields 353,560 (i, c, p) triples.
See Figure 2 for example paraphrases.

5.1 Image Captioning using Visual
Paraphrasing

We propose to utilize automatically-generated visual
paraphrases to improve the ASSOC approach (¬ß3) for
image captioning. One potential limitation of the
ASSOC approach is that for some captions, the num-
ber of associated images might be too small for reli-
able estimations of the neighborhood based affinity.
We hypothesize that for a caption with a small visual
neighborhood, merging its neighborhood with those
associated with its visual paraphrases will give a
more reliable estimation of the affinity between a
query image and that caption. Thus we modify the
ASSOC approach as follows.

After preparing a pool of K candidate captions
{c1, c2, . . . , cK}, automatically generate a visual
paraphrase (ii, ci, pi) for each (ii, ci); then rerank
the candidate captions by the following affinity func-
tion that merges the visual neighborhood from the

7under the WordNet ‚Äúphysical entity.n.01‚Äù synset

509



method BLEU METEOR AMT
INSTANCE 0.125 0.029 N/A
ASSOCgi 0.172 0.033 45%
ASSOCgipara 0.187 0.036 55%
ASSOCti 0.184 0.034 45%
ASSOCtipara 0.197 0.036 55%

Table 8: Automatic and human evaluation of exploit-
ing visual paraphrases for image captioning. The super-
scripts represent the image feature used in the rerank-
ing step; gi: GIST; ti: Tinyimage. The AMT column
shows the percentages of captions preferred by human as
of better visual relevance, in pairwise comparisons. The
improvement of ASSOCpara over ASSOC is significant at
p < 0.002 for BLEU, and p < 0.03 for METEOR with two
tailed t-test.

paraphrase,

A(q, Ci) = œÜ(q,Nci ‚à™Npi) (3)

6 Experiments: Visual Paraphrasing
Improves Image Captioning

The experimental configuration basically follows ¬ß4.
We compare ASSOCpara, the visual-paraphrase aug-
mented approach, to the vanilla ASSOC approach.
The image feature setting is the one with which the
ASSOC approach performs best. Both approaches
use the GIST+Tinyimage feature to prepare candi-
date captions, then use either the GIST or Tinyimage
feature for reranking.

Table 8 shows that the ASSOCpara approach sig-
nificantly improves the vanilla ASSOC method un-
der both automatic and human evaluation. As a ref-
erence, the first row shows the performance of the
INSTANCE method (¬ß4). The ASSOC method signif-
icantly improves over the INSTANCE method. On a
similar vein, the ASSOCpara method further improves
over the ASSOC method, as automatic paraphrases
provide a better visual neighborhood. This improve-
ment is remarkable since the paraphrasing associa-
tion is added automatically without any supervised
training. This demonstrates the usefulness of the bi-
partite association structure of our corpus.

7 Image Captioning with Creativity
Naturally existing captions reflect everyday narra-
tives, which in turn reflect figurative language use
such as metaphor, simile, and personification. To
gain better insights, one of the authors manually cat-
egorized a set of 1000 random captions. About 17%

are identified as figurative. Table 3 shows the distri-
bution over different types of figurative captions.

Creative Language Classifier: Using the small
set of labels described above, we train a simple bi-
nary classifier to identify captions with creative lan-
guage.8 Using this classifier, we can control the de-
gree of literalness or creativity in generated captions.
Based on 5-fold cross-validation, the classifier per-
forms with 77% precision and 43% recall.

Importantly, a high-precision and low-recall clas-
sifier suffices our purpose. It is because in the con-
text of creative captioning and creative paraphrasing
presented below, we only need to detect some figu-
rative captions, not all.

7.1 Creative Image Captioning
Given a query image q, we describe it with the most
appropriate figurative caption. We propose the AS-
SOCcreative approach that alters the ASSOC approach
(¬ß3) to return a figurative caption from the candidate
pool, excluding literal captions.

7.2 Creative Visual Paraphrasing
Given a query image q and its original caption c, we
rephrase c to a more creative and inspirational cap-
tion that still describes q. We use the PARAcreative ap-
proach that changes our automatic visual paraphras-
ing algorithm (¬ß5), by retrieving only figurative cap-
tions.

8 Experiments: Creative Image
Captioning and Paraphrasing

8.1 Creative Captioning
We compare the ASSOCcreative approach to the
vanilla ASSOC approach. With the ASSOC ap-
proach, the top-rank caption is usually literal. Both
approaches use the GIST+Tinyimage feature for
preparing candidate captions, and the Tinyimage
feature for reranking, which is the best setting for
the ASSOC approach (¬ß4).

Similarly to ¬ß4, we sample 200 test images from
our corpus, and use AMT to compare two algorithms
in terms of visual relevance and creativity sepa-
rately. For creativity, we ask turkers to choose one

8We use a random forest classifier with features including
words indicating reasoning (but, could, that), generality (never,
always), caption length, abstract nouns (life, and hope), and
whether the caption is a known idiom or quotation.

510



method creativity relevance
ASSOC 33% 41%
ASSOCcreative 67% 59%

Table 9: Human evaluation for creative captioning: % of
captions preferred by judges in pairwise comparisons

of the two captions that is more creative and inspira-
tional than the other to describe each given test im-
age. Results are shown in Table 9.

(1) Creativity. For 2/3 of the query images,
captions produced by the ASSOCcreative method are
judged as more creative than those produced by the
ASSOC method. This result indirectly validates that
the figurativeness classifier has a reasonable preci-
sion to control the literalness of the system caption.

(2) Visual relevance. Interestingly, not only the
captions from the ASSOCcreative method are favored
as creative, they are also judged as visually more rel-
evant than those from the ASSOC method, despite
that each figurative caption has lower neighborhood-
based affinity than the literal counterpart. We con-
jecture that it is easier for human judges to be imag-
inative and draw visual relevance between the query
image and figurative captions than the literal coun-
terparts. This result also suggests that figurative lan-
guage may be of practical use in image caption ap-
plications as a means to smooth the potentially brit-
tle system output. Figure 5 shows example system
output.

8.2 Creative Visual Paraphrasing

We test 200 images that are associated with literal
captions as predicted by the figurativeness classi-
fier. The PARAcreative approach competes against
two baselines: 1) the ORIGINAL captions , and 2)
a text-only variant of the PARAcaption approach sans
visual processing: it randomly chooses a figurative
caption that shares the largest number of physical-
object nouns with the original caption, without look-
ing at the query image. This is for evaluating the
effect of visual context.

In addition to the evaluations as in ¬ß8.1, we also
use a multiple-choice setting that allows a turker to
choose zero to two captions that are visually relevant
to the query image. See Table 10 for results, and
Figure 5 for example outputs.

method creativity relevance
single multiple

ORIGINAL 32% 80% 87%
PARAcreative 68% 20% 60%
PARAcaption 56% 47% 63%
PARAcreative 44% 53% 74%

Table 10: Human eval for creative visual paraphrasing

I. Comparing original captions with creative
paraphrases (ORIGINAL vs. PARAcreative): The
paraphrases are preferred over the original literal
captions as more creative most of the time. As for
the visual relevance, the original captions are fa-
vored over the paraphrases most of the time in the
single-choice competition. However, when we use
a multiple-choice setting, paraphrases has a reason-
able relevance rate (60%), despite the simplicity of
the algorithm. The fact that the original captions has
a high relevance rate (87%) shows that in our cor-
pus the captions have high visual relevance to their
associated images most of the time.

II. Creative paraphrasing with and without the
visual context (PARAcaption vs. PARAcreative): In
terms of creativity, the PARAcaption method is pre-
ferred over the PARAcreative method. We conjec-
ture that without conditioning on the visual con-
tent, PARAcaption method tends to retrieve more un-
expected captions that make turkers think they are
more fun and creative. As for the visual relevance,
by conditioning on the visual context given by query
images, the PARAcreative method significantly im-
proves the visual relevance over the text-only coun-
terpart, PARAcaption method. This result highlights
the pragmatic differences between visually-situated
paraphrasing and text-based paraphrasing.

9 Related work
Image-caption corpus: Our work contributes to
the line of research that makes use of internet web
imagery and text (Ordonez et al., 2011; Berg et al.,
2010) by detecting the visually relevant text (Dodge
et al., 2012) and reducing the noise (Kuznetsova et
al., 2013b; Kuznetsova et al., 2014). Compared to
datasets with crowd-sourced captions (Hodosh et al.,
2013; Lin et al., 2014), in which each image is an-
notated with several captions, our dataset presents
several images for each caption, a subset of which
also includes visually situated paraphrases. The as-

511



-Hood under a 
full moon (*) 
-Mirror, mirror on 
the lake 

-Sky on the way home(*) 
-Red sky at night,  
Shepherd's delight 

-Bee on orange flowers(*) 
-When the flower looms,  
the bees come uninvited 

-Lights in cave(*) 
-There is a light that 
never goes out 

-Sail on by (*) 
-Row, row, row your 
boat gently down 
the stream 

Creative Image Captioning Creative Visual Paraphrasing 

-City of lights (*) 
-Great balls of fire 

-Young roe deer(*) 
-The tree that looks 
like a deer  

  

-The flight of the 
crane(*) 
-That‚Äôs a crane   

-long haired girl(*) 
-Diamonds are a girl's  
best friend   

-Sky on the way home(*) 
-Go home, sky, you‚Äôre 
drunk 

-Falling water(*) 
-Can you see the dogs   

-Red Bean Pastries (*) 
-When life gives you 
lemons 

< Good >  < Bad >   < Bad >   < Good >  

Figure 5: Examples of creative captioning and creative visual paraphrasing. The left column shows good examples
in blue, and the right column shows bad examples in red. The captions marked with * are the original captions of the
corresponding query images.

sociation structure of our dataset is analogous to
that of ImageNet (Deng et al., 2009). Unlike Ima-
geNet that is built for nouns (physical objects) listed
under WordNet (Miller, 1995), our corpus is built
for expressive phrases and full sentences and con-
structed without human curation. Our corpus has
several unique properties to complement existing
corpora. As explored in a very recent work of (Gong
et al., 2014), we expect that it is possible to com-
bine crowd-sourced and web-harvested datasets and
achieve the best of both worlds.

Image captioning: Our work contributes to the
increasing body of research on retrieval-based im-
age captioning (Ordonez et al., 2011; Hodosh et
al., 2013; Hodosh and Hockenmaier, 2013; Socher
et al., 2014), by providing a new large-scale cor-
pus with unique association structure between im-
ages and captions, by proposing an algorithm that
exploits the structure, and by exploring two new di-
mensions: (i) visually situated paraphrasing (and its
utility for retrieval-based image captioning), and (ii)
creative image captioning.

Paraphrasing: Most previous studies in para-
phrasing have focused exclusively on text, and the
primary goal has been learning semantic equiva-
lence of phrases that would be true out of context
(e.g., (Barzilay and McKeown, 2001; Pang et al.,
2003; Dolan et al., 2004; Ganitkevitch et al., 2013)),
rather than targeting situated or pragmatic equiva-
lence given a context. Emerging efforts began ex-
ploring paraphrases that are situated in video con-
tent (Chen and Dolan, 2011), news events (Zhang
and Weld, 2013), and knowledge base (Berant and
Liang, 2014). Our work is the first to introduce vi-

sually situated paraphrasing in which the task is to
find paraphrases that are conditioned on both the in-
put text as well as the visual context. (Chen and
Dolan, 2011) collected situated paraphrases only
through crowd sourcing, while we also explore auto-
matic collection, and further test the quality of auto-
matic paraphrases by using the learned paraphrases
in an extrinsic evaluation setting.

Figurative language: There has been substantial
work for detecting and interpreting figurative lan-
guage (Shutova, 2010; Li et al., 2013; Kuznetsova
et al., 2013a; Tsvetkov et al., 2014), while relatively
less work on generating creative or figurative lan-
guage (Veale, 2011; Ozbal and Strapparava, 2012).
We probe data-driven approaches to creative lan-
guage generation in the context of image captioning.

10 Conclusion
To conclude, we have provided insights into mak-
ing a better use of multimodal web data in the
wild, resulting in a large-scale corpus, Deja Image-
Captions, with several unique properties to comple-
ment datasets with crowdsourced captions. To vali-
date the usefulness of the corpus, we proposed new
image captioning algorithms using the associative
structure, which we extended to several related tasks
ranging from visually situated paraphrasing to en-
hanced image captioning. In the process we have
also explored several new tasks: visually situated
paraphrasing, creative image captioning, and cre-
ative caption paraphrasing.

Acknowledgement The research is supported in
part by NSF Award IIS 1447549 and IIS 1408287.

512



References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:

An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65‚Äì72.

Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, pages 50‚Äì57. Associa-
tion for Computational Linguistics.

Jonathan Berant and Percy Liang. 2014. Semantic Pars-
ing via Paraphrasing. In Association for Computa-
tional Linguistics (ACL).

Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteri-
zation from noisy web data. In ECCV 2010, pages
663‚Äì676. Springer.

Oren Boiman, Eli Shechtman, and Michal Irani. 2008.
In defense of Nearest-Neighbor based image classifi-
cation. In IEEE Conference on Computer Vision and
Pattern Recognition, 2008. CVPR 2008, pages 1‚Äì8,
June.

David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 190‚Äì200. Asso-
ciation for Computational Linguistics.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hier-
archical image database. In Computer Vision and Pat-
tern Recognition, 2009. CVPR 2009. IEEE Conference
on, pages 248‚Äì255. IEEE.

Thomas Deselaers and Vittorio Ferrari. 2011. Visual and
semantic similarity in imagenet. In Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Confer-
ence on, pages 1777‚Äì1784. IEEE.

Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,
Yejin Choi, Hal DaumeÃÅ III, Alexander C. Berg, and
others. 2012. Detecting visual text. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 762‚Äì772. As-
sociation for Computational Linguistics.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics, page 350. Association for
Computational Linguistics.

Desmond Elliott and Frank Keller. 2014. Comparing au-
tomatic evaluation measures for image description. In
Proceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics, volume 2, pages
452‚Äì457.

Song Feng, Sujith Ravi, Ravi Kumar, Polina Kuznetsova,
Wei Liu, Alexander C. Berg, Tamara L. Berg, and
Yejin Choi. 2015. Refer-to-as Relations as Semantic
Knowledge. In AAAI Conference on Artificial Intelli-
gence.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In Proceedings of NAACL-HLT, pages 758‚Äì
764, Atlanta, Georgia, June. Association for Computa-
tional Linguistics.

Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hock-
enmaier, and Svetlana Lazebnik. 2014. Improv-
ing Image-Sentence Embeddings Using Large Weakly
Annotated Photo Collections. In ECCV 2014, pages
529‚Äì545. Springer.

David Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral computation, 16(12):2639‚Äì2664.

Micah Hodosh and Julia Hockenmaier. 2013. Sentence-
based image description with scalable, explicit mod-
els. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops,
pages 294‚Äì300.

Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
data, models and evaluation metrics. Journal of Artifi-
cial Intelligence Research, 47(1):853‚Äì899.

Polina Kuznetsova, Jianfu Chen, and Yejin Choi. 2013a.
Understanding and Quantifying Creativity in Lexical
Composition. In EMNLP, pages 1246‚Äì1258.

Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg, and Yejin Choi. 2013b. General-
izing Image Captions for Image-Text Parallel Corpus.
In ACL (2), pages 790‚Äì796.

Polina Kuznetsova, Vicente Ordonez, Tamara Berg, and
Yejin Choi. 2014. TreeTalk: Composition and Com-
pression of Trees for Image Descriptions. Transac-
tions of the Association for Computational Linguistics.

Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-Driven Metaphor Recognition and Explanation.
TACL, 1:379‚Äì390.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr DollaÃÅr, and
C. Lawrence Zitnick. 2014. Microsoft COCO: Com-
mon Objects in Context. In ECCV, ZuÃàrich.

Rebecca Mason and Eugene Charniak. 2014. Nonpara-
metric Method for Data-driven Image Captioning. In
NAACL.

513



Mary Meeker. 2014. Internet Trends 2014.
George A. Miller. 1995. WordNet: a lexical database for

English. Communications of the ACM, 38(11):39‚Äì41.
Aude Oliva and Antonio Torralba. 2001. Modeling the

shape of the scene: A holistic representation of the
spatial envelope. International journal of computer vi-
sion, 42(3):145‚Äì175.

Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing Images Using 1 Million
Captioned Photographs. In NIPS, volume 1, page 4.

Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C.
Berg, and Tamara L. Berg. 2013. From large scale im-
age categorization to entry-level categories. In Com-
puter Vision (ICCV), 2013 IEEE International Confer-
ence on, pages 2768‚Äì2775. IEEE.

Gozde Ozbal and Carlo Strapparava. 2012. A Computa-
tional Approach to the Automation of Creative Nam-
ing. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 703‚Äì711, Jeju Island, Korea, July.
Association for Computational Linguistics.

Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 102‚Äì109. Association for Computa-
tional Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting on association for computational
linguistics, pages 311‚Äì318. Association for Computa-
tional Linguistics.

Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting Image Annota-
tions Using Amazon‚Äôs Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon‚Äôs Mechan-
ical Turk, CSLDAMT ‚Äô10, pages 139‚Äì147, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Nikhil Rasiwasia, Pedro J. Moreno, and Nuno Vasconce-
los. 2007. Bridging the gap: Query by semantic ex-
ample. Multimedia, IEEE Transactions on, 9(5):923‚Äì
938.

Nikhil Rasiwasia, Jose Costa Pereira, Emanuele
Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger
Levy, and Nuno Vasconcelos. 2010. A New Approach
to Cross-modal Multimedia Retrieval. In Proceedings
of the International Conference on Multimedia, MM
‚Äô10, pages 251‚Äì260, New York, NY, USA. ACM.

Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ‚Äô10,
pages 688‚Äì697, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
the Association for Computational Linguistics, 2:207‚Äì
218.

Antonio Torralba, Robert Fergus, and William T. Free-
man. 2008. 80 million tiny images: A large data set
for nonparametric object and scene recognition. Pat-
tern Analysis and Machine Intelligence, IEEE Trans-
actions on, 30(11):1958‚Äì1970.

Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric
Nyberg, and Chris Dyer. 2014. Metaphor detection
with cross-lingual model transfer. In Proceedings of
ACL.

Tony Veale. 2011. Creative Language Retrieval: A Ro-
bust Hybrid of Information Retrieval and Linguistic
Creativity. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 278‚Äì287, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on human language technology and empirical methods
in natural language processing, pages 347‚Äì354. Asso-
ciation for Computational Linguistics.

Congle Zhang and Daniel S Weld. 2013. Harvesting Par-
allel News Streams to Generate Paraphrases of Event
Relations. In EMNLP, pages 1776‚Äì1786.

514


