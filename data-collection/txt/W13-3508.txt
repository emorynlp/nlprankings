



















































Sentence Compression with Joint Structural Inference


Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65–74,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Sentence Compression with Joint Structural Inference

Kapil Thadani and Kathleen McKeown
Department of Computer Science

Columbia University
New York, NY 10025, USA

{kapil,kathy}@cs.columbia.edu

Abstract

Sentence compression techniques often
assemble output sentences using frag-
ments of lexical sequences such as n-
grams or units of syntactic structure such
as edges from a dependency tree repre-
sentation. We present a novel approach
for discriminative sentence compression
that unifies these notions and jointly pro-
duces sequential and syntactic represen-
tations for output text, leveraging a com-
pact integer linear programming formula-
tion to maintain structural integrity. Our
supervised models permit rich features
over heterogeneous linguistic structures
and generalize over previous state-of-the-
art approaches. Experiments on corpora
featuring human-generated compressions
demonstrate a 13-15% relative gain in 4-
gram accuracy over a well-studied lan-
guage model-based compression system.

1 Introduction

Recent years have seen increasing interest in text-
to-text generation tasks such as paraphrasing and
text simplification, due in large part to their direct
utility in high-level natural language tasks such as
abstractive summarization. The task of sentence
compression in particular has benefited from the
availability of a number of useful resources such
as the the Ziff-Davis compression corpus (Knight
and Marcu, 2000) and the Edinburgh compression
corpus (Clarke and Lapata, 2006b) which make
compression problems highly relevant for data-
driven approaches involving language generation.

The sentence compression task addresses the
problem of minimizing the lexical footprint of a

sentence, i.e., the number of words or characters
in it, while preserving its most salient informa-
tion. This is illustrated in the following example
from the compression corpus of Clarke and Lap-
ata (2006b):

Original: In 1967 Chapman, who had cultivated a
conventional image with his ubiquitous tweed jacket
and pipe, by his own later admission stunned a party
attended by his friends and future Python colleagues
by coming out as a homosexual.

Compressed: In 1967 Chapman, who had cultivated
a conventional image, stunned a party by coming out
as a homosexual.

Compression can therefore be viewed as analo-
gous to text summarization1 defined at the sen-
tence level. Unsurprisingly, independent selec-
tion of tokens for an output sentence does not
lead to fluent or meaningful compressions; thus,
compression systems often assemble output text
from units that are larger than single tokens such
as n-grams (McDonald, 2006; Clarke and Lap-
ata, 2008) or edges in a dependency structure (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010). These systems implicitly rely on
a structural representation of text—as a sequence
of tokens or as a dependency tree respectively—to
to underpin the generation of an output sentence.

In this work, we present structured transduc-
tion: a novel supervised framework for sen-
tence compression which employs a joint infer-
ence strategy to simultaneously recover sentence
compressions under both these structural repre-
sentations of text—a token sequence as well as a
tree of syntactic dependencies. Sentence genera-
tion is treated as a discriminative structured pre-
diction task in which rich linguistically-motivated

1To further the analogy, compression is most often formu-
lated as a word deletion task which parallels the popular view
of summarization as a sentence extraction task.

65



features can be used to predict the informative-
ness of specific tokens within the input text as well
as the fluency of n-grams and dependency rela-
tionships in the output text. We present a novel
constrained integer linear program that optimally
solves the joint inference problem, using the no-
tion of commodity flow (Magnanti and Wolsey,
1994) to ensure the production of valid acyclic se-
quences and trees for an output sentence.

The primary contributions of this work are:

• A supervised sequence-based compression
model which outperforms Clarke & Lapata’s
(2008) state-of-the-art sequence-based com-
pression system without relying on any hard
syntactic constraints.
• A formulation to jointly infer tree structures

alongside sequentially-ordered n-grams,
thereby permitting features that factor over
both phrases and dependency relations.

The structured transduction models offer addi-
tional flexibility when compared to existing mod-
els that compress via n-gram or dependency fac-
torizations. For instance, the use of commodity
flow constraints to ensure well-formed structure
permits arbitrary reorderings of words in the input
and is not restricted to producing text in the same
order as the input like much previous work (Mc-
Donald, 2006; Clarke and Lapata, 2008; Filippova
and Strube, 2008) inter alia.2

We ran compression experiments with the pro-
posed approaches on well-studied corpora from
the domains of written news (Clarke and Lapata,
2006b) and broadcast news (Clarke and Lapata,
2008). Our supervised approaches show signif-
icant gains over the language model-based com-
pression system of Clarke and Lapata (2008) un-
der a variety of performance measures, yielding
13-15% relative F1 improvements for 4-gram re-
trieval over Clarke and Lapata (2008) under iden-
tical compression rate conditions.

2 Joint Structure Transduction

The structured transduction framework is driven
by the fundamental assumption that generating
fluent text involves considerations of diverse struc-
tural relationships between tokens in both input
and output sentences. Models for sentence com-
pression often compose text from units that are

2We do not evaluate token reordering in the current work
as the corpus used for experiments in §3 features human-
generated compressions that preserve token ordering.

larger than individual tokens, such as n-grams
which describe a token sequence or syntactic re-
lations which comprise a dependency tree. How-
ever, our approach is specifically motivated by the
perspective that both these representations of a
sentence—a sequence of tokens and a tree of de-
pendency relations—are equally meaningful when
considering its underlying fluency and integrity. In
other words, models for compressing a token se-
quence must also account for the compression of
its dependency representation and vice versa.

In this section, we discuss the problem of re-
covering an optimal compression from a sen-
tence as a linear optimization problem over het-
erogenous substructures (cf. §2.1) that can be
assembled into valid and consistent representa-
tions of a sentence (cf. §2.2). We then consider
rich linguistically-motivated features over these
substructures (cf. §2.3) for which corresponding
weights can be learned via supervised structured
prediction (cf. §2.4).

2.1 Linear Objective

Consider a single compression instance involving
a source sentence S containing m tokens. The no-
tation Ŝ is used to denote a well-formed compres-
sion of S. In this paper, we follow the standard
assumption from compression research in assum-
ing that candidate compressions Ŝ are assembled
from the tokens in S, thereby treating compression
as a word-deletion task. The inference step aims
to retrieve the output sentence Ŝ∗ that is the most
likely compression of the given input S, i.e., the Ŝ
that maximizes p(Ŝ|S) ∝ p(Ŝ, S) or, in an equiv-
alent discriminative setting, the Ŝ that maximizes
a feature-based score for compression

Ŝ∗ , argmax
Ŝ

w>Φ(S, Ŝ) (1)

where Φ(S, Ŝ) denotes some feature map param-
eterized by a weight vector w.

Let T , {ti : 1 ≤ i ≤ m} represent the set
of tokens in S and let xi ∈ {0, 1} represent a
token indicator variable whose value corresponds
to whether token ti is present in the output sen-
tence Ŝ. The incidence vector x , 〈x1, . . . , xm〉>
therefore represents an entire token configuration
that is equivalent to some subset of T .

If we were to consider a simplistic bag-of-
tokens scenario in which the features factor en-
tirely over the tokens from T , the highest-scoring

66



compression under (1) would simply be the to-
ken configuration that maximizes a linear combi-
nation of per-token scores, i.e.,

∑
ti∈T xi · θtok(i)

where θtok : N→ R denotes a linear scoring func-
tion which measures the relative value of retain-
ing ti in a compression of S based on its features,
i.e., θtok(i) , w>tokφtok(ti). Although this can
be solved efficiently under compression-rate con-
straints, the strong independence assumption used
is clearly unrealistic: a model that cannot consider
any relationship between tokens in the output does
not provide a token ordering or ensure that the re-
sulting sentence is grammatical.

The natural solution is to include higher-order
factorizations of linguistic structures such as n-
grams in the objective function. For clarity of ex-
position, we assume the use of trigrams without
loss of generality. Let U represent the set of all
possible trigrams that can be constructed from the
tokens of S; in other words U , {〈ti, tj , tk〉 : ti ∈
T ∪ {START}, tj ∈ T, tk ∈ T ∪ {END}, i 6= j 6=
k}. Following the notation for token indicators, let
yijk ∈ {0, 1} represent a trigram indicator variable
for whether the contiguous sequence of tokens
〈ti, tj , tk〉 is in the output sentence. The incidence
vector y , 〈yijk〉〈ti,tj ,tk〉∈U hence represents
some subset of the trigrams in U . Similarly, let V
represent the set of all possible dependency edges
that can be established among the tokens of S and
the pseudo-token ROOT, i.e., V , {〈i, j〉 : i ∈
T ∪ {ROOT}, j ∈ T, tj is a dependent of ti in S}.
As before, zij ∈ {0, 1} represents a dependency
arc indicator variable indicating whether tj is a di-
rect dependent of ti in the dependency structure of
the output sentence, and z , 〈zij〉〈ti,tj〉∈V repre-
sents a subset of the arcs from V .

Using this notation, any output sentence Ŝ can
now be expressed as a combination of some to-
ken, trigram and dependency arc configurations
〈x,y, z〉. Defining θngr and θdep analogously to
θtok for trigrams and dependency arcs respectively,
we rewrite (1) as

Ŝ∗ = argmax
x,y,z

∑

ti∈T
xi · θtok(i)

+
∑

〈ti,tj ,tk〉∈U
yijk · θngr(i, j, k)

+
∑

〈ti,tj〉∈V
zij · θdep(i, j)

= argmax
x,y,z

x>θtok + y>θngr + z>θdep (2)

where θtok , 〈θtok(i)〉ti∈T denotes the vector of
token scores for all tokens ti ∈ T and θngr and
θdep represent vectors of scores for all trigrams
and dependency arcs in U and V respectively. The
joint objective in (2) is an appealingly straightfor-
ward and yet general formulation for the compres-
sion task. For instance, the use of standard sub-
structures like n-grams permits scoring of the out-
put sequence configuration y under probabilistic
n-gram language models as in Clarke and Lapata
(2008). Similarly, consideration of dependency
arcs allows the compressed dependency tree z to
be scored using a rich set of indicator features over
dependency labels, part-of-speech tags and even
lexical features as in Filippova and Strube (2008).

However, unlike the bag-of-tokens scenario,
these output structures cannot be constructed effi-
ciently due to their interdependence. Specifically,
we need to maintain the following conditions in
order to obtain an interpretable token sequence y:

• Trigram variables yijk must be non-zero if
and only if their corresponding word vari-
ables xi, xj and xk are non-zero.
• The non-zero yijk must form a sentence-like

linear ordering, avoiding disjoint structures,
cycles and branching.

Similarly, a well-formed dependency tree z will
need to satisfy the following conditions:

• Dependency variables zij must be non-zero if
and only if the corresponding word variables
xi and xj are.
• The non-zero zij must form a directed tree

with one parent per node, a single root node
and no cycles.

2.2 Constrained ILP Formulation
We now discuss an approach to recover exact so-
lutions to (2) under the appropriate structural con-
straints, thereby yielding globally optimal com-
pressions Ŝ ≡ 〈x,y, z〉 given some input sentence
S and model parameters for the scoring functions.
For this purpose, we formulate the inference task
for joint structural transduction as an integer linear
program (ILP)—a type of linear program (LP) in
which some or all of the decision variables are re-
stricted to integer values. A number of highly op-
timized general-purpose solvers exist for solving
ILPs thereby making them tractable for sentence-
level natural language problems in which the num-
ber of variables and constraints is described by a
low-order polynomial over the size of the input.

67



Recent years have seen ILP applied to many
structured NLP applications including depen-
dency parsing (Riedel and Clarke, 2006; Martins
et al., 2009), text alignment (DeNero and Klein,
2008; Chang et al., 2010; Thadani et al., 2012)
and many previous approaches to sentence and
document compression (Clarke and Lapata, 2008;
Filippova and Strube, 2008; Martins and Smith,
2009; Clarke and Lapata, 2010; Berg-Kirkpatrick
et al., 2011; Woodsend and Lapata, 2012).

2.2.1 Basic structural constraints
We start with constraints that define the behavior
of terminal tokens. Let y∗jk, yij∗ and z∗j denote
indicator variables for the sentence-starting tri-
gram 〈START, tj , tk〉, the sentence-ending trigram
〈ti, tj , END〉 and the root dependency 〈ROOT, tj〉
respectively. A valid output sentence will started
and terminate with exactly one trigram (perhaps
the same); similarly, exactly one word should act
as the root of the output dependency tree.

∑

j,k

y∗jk = 1 (3)

∑

i,j

yij∗ = 1 (4)

∑

j

z∗j = 1 (5)

Indicator variables for any substructure, i.e., n-
gram or dependency arc, must be kept consistent
with the token variables that the substructure is de-
fined over. For instance, we require constraints
which specify that tokens can only be active (non-
zero) in the solution when, for 1 ≤ p ≤ n, there
is exactly one active n-gram in the solution which
contains this word in position p.3 Tokens and de-
pendency arcs can similarly be kept consistent by
ensuring that a word can only be active when one
incoming arc is active.

xl −
∑

i,j,k:
l∈{i,j,k}

yijk = 0, ∀tl ∈ T (6)

xj −
∑

i

zij = 0, ∀tj ∈ T (7)

3Note that this does not always hold for n-grams of or-
der n > 2 due to the way terminal n-grams featuring START
and END are defined. Specifically, in a valid linear ordering
of tokens and ∀r ∈ 1 . . . n− 2, there can be no n-grams that
feature the last n−r−1 tokens in the r’th position or the first
n−r−1 tokens in the (n−r+1)’th position. However, this
is easily tackled computationally by assuming that the termi-
nal n-gram replaces these missing n-grams for near-terminal
tokens in constraint (6).

2.2.2 Flow-based structural constraints
A key challenge for structured transduction mod-
els lies in ensuring that output token sequences and
dependency trees are well formed. This requires
that output structures are fully connected and that
cycles are avoided. In order to accomplish this, we
introduce additional variables to establish single-
commodity flow (Magnanti and Wolsey, 1994) be-
tween all pairs of tokens, inspired by recent work
in dependency parsing (Martins et al., 2009). Lin-
ear token ordering is maintained by defining real-
valued adjacency commodity flow variables γadjij
which must be non-zero whenever tj directly fol-
lows ti in an output sentence. Similarly, tree-
structured dependencies are enforced using addi-
tional dependency commodity flow variables γdepij
which must be non-zero whenever tj is the de-
pendent of ti in the output sentence. As with the
structural indicators, flow variables γadj∗j , γ

adj
i∗ , γ

dep
∗j

are also defined for the terminal pseudo-tokens
START, END and ROOT respectively.

Each active token in the solution consumes one
unit of each commodity from the flow variables
connected to it. In conjunction with the consis-
tency constraints from equations (6) and (7), this
ensures that cycles cannot be present in the flow
structure for either commodity.

∑

i

γcij −
∑

k

γcjk = xj , ∀tj ∈ T, (8)
∀c ∈ {adj, dep}

By itself, (8) would simply set all token indica-
tors xi simultaneously to 0. However, since START
and ROOT have no incoming flow variables, the
amount of commodity in the respective outgo-
ing flow variables γadj∗j and γ

dep
∗j remains uncon-

strained. These flow variables therefore provide
a point of origin for their respective commodities.

In order for commodity flow to be meaningful,
it should be confined to mirroring active structural
indicators; for this, we first restrict the amount of
commodity in any γcij to be non-negative.

γcij ≥ 0, ∀ti, tj ∈ T (9)
∀c ∈ {adj, dep}

The adjacency commodity is then linked to the n-
grams that would actually establish an adjacency
relationship between two tokens, while the depen-
dency commodity is linked to its corresponding
dependency arcs. In conjunction with (8–9), these

68



START END

ROOT

Production was closed down at Ford last night for the Christmas period .

8 γadj1,3 = 7 6 5 4 3 2 1

7
γdep3,1 = 1 2 1

2

1

1

Figure 1: An illustration of commodity values for a valid solution of the program. The adjacency com-
modity γadj and dependency commodity γdep are denoted by solid and dashed lines respectively.

constraints also serve to establish connectivity for
their respective structures.

γ
adj
ij − Cmax

∑

k

yijk ≤ 0, ∀ti, tj ∈ T (10)

γ
adj
jk − Cmax

∑

i

yijk ≤ 0, ∀tj , tk ∈ T (11)

γ
dep
ij − Cmaxzij ≤ 0, ∀ti, tj ∈ T (12)

where Cmax is the maximum amount of commod-
ity that the γij variables may carry and serves as an
upper bound on the number of tokens in the output
sentence. Since we use commodity flow to avoid
cyclical structure and not to specify spanning ar-
borescences (Martins et al., 2009), Cmax can sim-
ply be set to an arbitrary large value.

2.2.3 Compression rate constraints
The constraints specified above are adequate to en-
force structural soundness in an output compres-
sion. In addition, compression tasks often involve
a restriction on the size of the output sentence.
When measured in tokens, this can simply be ex-
pressed via constraints over token indicators.

∑

i

xi ≥ Rmin (13)
∑

i

xi ≤ Rmax (14)

where the compression rate is enforced by restrict-
ing the number of output tokens to [Rmin, Rmax].

2.3 Features

The scoring functions θ that guide inference for a
particular compression instance are defined above
as linear functions over structure-specific features.
We employ the following general classes of fea-
tures for tokens, trigrams and dependency arcs.

1. Informativeness: Good compressions might
require specific words or relationships be-
tween words to be preserved, highlighted, or

perhaps explicitly rejected. This can be ex-
pressed through features on token variables
that indicate a priori salience.4 For this pur-
pose, we rely on indicator features for part-
of-speech (POS) sequences of length up to 3
that surround the token and the POS tag of the
token’s syntactic governor conjoined with the
label. Inspired by McDonald (2006), we also
maintain indicator features for stems of verbs
(at or adjacent to the token) as these can be
useful indications of salience in compression.
Finally, we maintain features for whether to-
kens are negation words, whether they appear
within parentheses and if they are part of a
capitalized sequence of tokens (an approxi-
mation of named entities).

2. Fluency: These features are intended to cap-
ture how the presence of a given substructure
contributes to the overall fluency of a sen-
tence. The n-gram variables are scored with a
feature expressing their log-likelihood under
an LM. For n-gram variables, we include fea-
tures that indicate the POS tags and depen-
dency labels corresponding to the tokens it
covers. Dependency variable features involve
indicators for the governor POS tag con-
joined with the dependency direction. In ad-
dition, we also use lexical features for prepo-
sitions in the governor position of depen-
dency variables in order to indicate whether
certain prepositional phrases are likely to be
preserved in compressions.

3. Fidelity: One might reasonably expect that
many substructures in the input sentence will
appear unchanged in the output sentence.
Therefore, we propose boolean features that
indicate that a substructure was seen in the
input. Fidelity scores are included for all
n-gram variables alongside label-specific fi-

4Many compression systems (Clarke and Lapata, 2008;
Filippova and Strube, 2008) use a measure based on tf*idf
which derives from informativeness score of Hori and Furui
(2004), but we found this to be less relevant here.

69



delity scores for dependency arc variables,
which can indicate whether particular labels
are more or less likely to be dropped.

4. Pseudo-normalization: A drawback of us-
ing linear models for generation problems
is an inability to employ output sentence
length normalization in structure scoring. For
this purpose, we use the common machine
translation (MT) strategy of employing word
penalty features. These are essentially word
counts whose parameters are intended to bal-
ance out the biases in output length which are
induced by other features.

Each scale-dependent feature is recorded both ab-
solutely as well as normalized by the length of the
input sentence. This is done in order to permit the
model to acquire some robustness to variation in
input sentence length when learning parameters.

2.4 Learning
In order to leverage a training corpus to recover
weight parameters w∗ for the above features that
encourage good compressions for unseen data,
we rely on the structured perceptron of Collins
(2002). A fixed learning rate is used and param-
eters are averaged to limit overfitting.5 In our ex-
periments, we observed fairly stable convergence
for compression quality over held-out develop-
ment corpora, with peak performance usually en-
countered by 10 training epochs.

3 Experiments

In order to evaluate the performance of the
structured transduction framework, we ran com-
pression experiments over the newswire (NW)
and broadcast news transcription (BN) corpora
collected by Clarke and Lapata (2008). Sen-
tences in these datasets are accompanied by gold
compressions—one per sentence for NW and
three for BN—produced by trained human anno-
tators who were restricted to using word deletion,
so paraphrasing and word reordering do not play
a role. For this reason, we chose to evaluate the
systems using n-gram precision and recall (among
other metrics), following Unno et al. (2006) and
standard MT evaluations.

We filtered the corpora to eliminate instances
with less than 2 and more than 110 tokens and used

5Given an appropriate loss function, large-margin struc-
tured learners such as k-best MIRA (McDonald et al., 2005)
can also be used as shown in Clarke and Lapata (2008).

the same training/development/test splits from
Clarke and Lapata (2008), yielding 953/63/603
sentences respectively for the NW corpus and
880/78/404 for the BN corpus. Dependency parses
were retrieved using the Stanford parser6 and ILPs
were solved using Gurobi.7 As a state-of-the-art
baseline for these experiments, we used a reim-
plementation of the LM-based system of Clarke
and Lapata (2008), which we henceforth refer to
as CL08. This is equivalent to a variant of our pro-
posed model that excludes variables for syntactic
structure, uses LM log-likelihood as a feature for
trigram variables and a tf*idf -based significance
score for token variables, and incorporates several
targeted syntactic constraints based on grammat-
ical relations derived from RASP (Briscoe et al.,
2006) designed to encourage fluent output.

Due to the absence of word reordering in the
gold compressions, trigram variables y that were
considered in the structured transduction approach
were restricted to only those for which tokens
appear in the same order as the input as is the
case with CL08. Furthermore, in order to reduce
computational overhead for potentially-expensive
ILPs, we also excluded dependency arc variables
which inverted an existing governor-dependent re-
lationship from the input sentence parse.

A recent analysis of approaches to evaluating
compression (Napoles et al., 2011b) has shown a
strong correlation between the compression rate
and human judgments of compression quality,
thereby concluding that comparisons of systems
which compress at different rates are unreliable.
Consequently, all comparisons that we carry out
here involve a restriction to a particular compres-
sion rate to ensure that observed differences can
be interpreted meaningfully.

3.1 Results

Table 1 summarizes the results from compression
experiments in which the target compression rate
is set to the average gold compression rate for
each instance. We observe a significant gain for
the joint structured transduction system over the
Clarke and Lapata (2008) approach for n-gram F1.
Since n-gram metrics do not distinguish between
content words and function words, we also in-
clude an evaluation metric that observes the pre-
cision, recall and F-measure of nouns and verbs

6http://nlp.stanford.edu/software/
7http://www.gurobi.com

70



Corpus System
n-grams F1% Content words Syntactic relations F1%

n = 1 2 3 4 P% R% F1% Stanford RASP

NW
CL08 66.65 53.08 40.35 31.02 73.84 66.41 69.38 51.51 50.21
Joint ST 71.91 58.67 45.84 35.62 76.82 76.74 76.33 55.02 50.81

BN
CL08 75.08 61.31 46.76 37.58 80.21 75.32 76.91 60.70 57.27
Joint ST 77.82 66.39 52.81 42.52 80.77 81.93 80.75 61.38 56.47

Table 1: Experimental results under various quality metrics (see text for descriptions). Systems were
restricted to produce compressions that matched their average gold compression rate. Boldfaced entries
indicate significant differences (p < 0.0005) under the paired t-test and Wilcoxon’s signed rank test.

as a proxy for the content in compressed output.
From these, we see that the primary contribution
of the supervised joint approach is in enhancing
the recall of meaning-bearing words.

In addition to the direct measures discussed
above, Napoles et al. (2011b) indicate that various
other metrics over syntactic relations such as those
produced by RASP also correlate significantly
with human judgments of compression quality.
Compressed sentences were therefore parsed with
RASP as well as the Stanford dependency parser
and their resulting dependency graphs were com-
pared to those of the gold compressions. These
metrics show statistically insignificant differences
except in the case of F1 over Stanford dependen-
cies for the NW corpus.8

Comparisons with CL08 do not adequately ad-
dress the question of whether the performance
gain observed is driven by the novel joint infer-
ence approach or the general power of discrimina-
tive learning. To investigate this, we also studied
a variant of the proposed model which eliminates
the dependency variables z and associated com-
modity flow machinery, thereby bridging the gap
between the two systems discussed above. This
system, which we refer to as Seq ST, is other-
wise trained under similar conditions as Joint ST.
Table 2 contains an example of incorrect system
output for the three systems under study and il-
lustrates some specific quirks of each, such as the
tendency of CL08 to preserve deeply nested noun
phrases, the limited ability of Seq ST to identify
heads of constituents and the potential for plausi-
ble but unusual output parses from Joint ST.

Figure 2 examines the variation of content word
F1% when the target compression rate is varied
for the BN corpus, which contains three refer-

8Our RASP F1 results for Clarke and Lapata (2008) in
Table 1 outperform their reported results by about 10% (ab-
solute) which may stem from our Gigaword-trained LM or
improvements in recent versions of RASP.

Input When Los Angeles hosted the Olympics in
1932 , Kurtz competed in high platform diving.

Gold When Los Angeles hosted the Olympics , Kurtz
competed in high diving .

CL08 When Los Angeles hosted Olympics in 1932 ,
in high platform diving .

Seq ST When Los Angeles hosted the Olympics , Kurtz
competed in high platform

Joint ST When Los Angeles hosted the Olympics in
1932 , Kurtz competed diving .

Table 2: Examples of erroneous system compres-
sions for a test instance from the NW corpus.

ence compressions per instance. Although the
gold compressions are often unreachable under
low rates, this provides a view into a model’s abil-
ity to select meaningful words under compression
constraints. We observe that the Joint ST model
consistently identifies content words more accu-
rately than the sequence-only models despite shar-
ing all token and trigram features with Seq ST.

Figure 3 studies the variation of RASP gram-
matical relation F1% with compression rate as an
approximate measure of grammatical robustness.
As all three systems track each other fairly closely,
the plot conveys the absolute difference of the ST
systems from the CL08 baseline, which reveals
that Joint ST largely outperforms Seq ST under
different compression conditions. We also note
that a high compression rate, i.e., minimal com-
pression, is generally favorable to CL08 under the
RASP F1 measure and conjecture that this may be
due to the hard syntactic constraints employed by
CL08, some of which are defined over RASP re-
lations. At higher compression rates, these con-
straints largely serve to prevent the loss of mean-
ingful syntactic relationships, e.g., that between a
preposition and its prepositional phrase; however,
a restrictive compression rate would likely result
in all such mutually-constrained components be-
ing dropped rather than simultaneously preserved.

71



Figure 2: Informativeness of compressions in the
BN test corpus indicated by noun and verb F1%
with respect to gold at different compression rates.

4 Related Work

An early notion of compression was proposed
by Dras (1997) as reluctant sentence paraphras-
ing under length constraints. Jing and McKe-
own (2000) analyzed human-generated summaries
and identified a heavy reliance on sentence re-
duction (Jing, 2000). The extraction by Knight
and Marcu (2000) of a dataset of natural com-
pression instances from the Ziff-Davis corpus
spurred interest in supervised approaches to the
task (Knight and Marcu, 2002; Riezler et al., 2003;
Turner and Charniak, 2005; McDonald, 2006;
Unno et al., 2006; Galley and McKeown, 2007;
Nomoto, 2007). In particular, McDonald (2006)
expanded on Knight & Marcu’s (2002) transition-
based model by using dynamic programming to
recover optimal transition sequences, and Clarke
and Lapata (2006a) used ILP to replace pairwise
transitions with trigrams. Other recent work (Fil-
ippova and Strube, 2008; Galanis and Androut-
sopoulos, 2010) has used dependency trees di-
rectly as sentence representations for compres-
sion. Another line of research has attempted to
broaden the notion of compression beyond mere
word deletion (Cohn and Lapata, 2009; Ganitke-
vitch et al., 2011; Napoles et al., 2011a). Finally,
progress on standalone compression tasks has also
enabled document summarization techniques that
jointly address sentence selection and compres-
sion (Daumé and Marcu, 2002; Clarke and Lapata,
2007; Martins and Smith, 2009; Berg-Kirkpatrick
et al., 2011; Woodsend and Lapata, 2012), a num-
ber of which also rely on ILP-based inference.

Monolingual text-to-text generation research
also faces many obstacles common to MT. Re-

Figure 3: Relative grammaticality of BN test cor-
pus compressions indicated by the absolute differ-
ence of RASP relation F1% from that of CL08.

cent work in MT decoding has proposed more ef-
ficient approaches than ILP to produced text op-
timally under syntactic and sequential models of
language (Rush and Collins, 2011). We are cur-
rently exploring similar ideas for compression and
other text-to-text generation problems.

5 Conclusion
We have presented a supervised discriminative
approach to sentence compression that elegantly
accounts for two complementary aspects of sen-
tence structure—token ordering and dependency
syntax. Our inference formulation permits rich,
linguistically-motivated features that factor over
the tokens, n-grams and dependencies of the out-
put. Structural integrity is maintained by linear
constraints based on commodity flow, resulting in
a flexible integer linear program for the inference
task. We demonstrate that this approach leads to
significant performance gains over a state-of-the-
art baseline compression system without resorting
to hand-picked constraints on output content.

Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.

72



References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.

2011. Jointly learning to extract and compress. In
Proceedings of ACL-HLT, pages 481–490.

Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the ACL-COLING Interactive Presenta-
tion Sessions.

Ming-Wei Chang, Dan Goldwasser, Dan Roth, and
Vivek Srikumar. 2010. Discriminative learning over
constrained latent representations. In Proceedings
of HLT-NAACL, pages 429–437.

James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: an integer program-
ming approach. In Proceedings of ACL-COLING,
pages 144–151.

James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: a comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of ACL-COLING, pages 377–
384.

James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of EMNLP-CoNLL, pages 1–11.

James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: an integer linear
programming approach. Journal for Artificial Intel-
ligence Research, 31:399–429, March.

James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411–441.

Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34(1):637–674, April.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models. In Proceedings of
EMNLP, pages 1–8.

Hal Daumé, III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of ACL, pages 449–456.

John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
HLT, pages 25–28.

Mark Dras. 1997. Reluctant paraphrase: Textual re-
structuring under an optimisation model. In Pro-
ceedings of PacLing, pages 98–104.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of INLG, pages 25–32.

Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of HLT-NAACL, pages
885–893.

Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL, pages 180–
187, April.

Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP, pages 1168–1179.

Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization: an approach through word extraction and a
method for evaluation. IEICE Transactions on In-
formation and Systems, E87-D(1):15–25.

Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceedings
of NAACL, pages 178–185.

Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
Conference on Applied Natural Language Process-
ing, pages 310–315.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI, pages 703–710.

Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107, July.

Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees. In Technical Report 290-94,
Massechusetts Institute of Technology, Operations
Research Center.

André F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, pages 1–9.

André F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL-IJCNLP, pages 342–350.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91–
98.

Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL, pages 297–304.

Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011a. Paraphras-
tic sentence compression with a character-based
metric: tightening without deletion. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 84–90.

73



Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011b. Evaluating sentence com-
pression: pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91–97.

Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Infor-
mation Processing and Management, 43(6):1571–
1587, November.

Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP, pages
129–137.

Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of HLT-NAACL, pages 118–125.

Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72–82.

Kapil Thadani, Scott Martin, and Michael White.
2012. A joint phrasal and dependency model for
paraphrase alignment. In Proceedings of COLING,
pages 1229–1238.

Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL, pages 290–297.

Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun’ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning
approaches. In Proceedings of ACL-COLING, pages
850–857.

Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP, pages 233–
243.

74


