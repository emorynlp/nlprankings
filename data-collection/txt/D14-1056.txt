



















































Resolving Shell Nouns


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 499–510,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Resolving Shell Nouns

Varada Kolhatkar
Department of Computer Science

University of Toronto
Toronto, ON, M5S 3G4, Canada

varada@cs.toronto.edu

Graeme Hirst
Department of Computer Science

University of Toronto
Toronto, ON, M5S 3G4, Canada

gh@cs.toronto.edu

Abstract

Shell nouns, such as fact and problem, oc-
cur frequently in all kinds of texts. These
nouns themselves are unspecific, and can
only be interpreted together with the shell
content. We propose a general approach
to automatically identify shell content of
shell nouns. Our approach exploits lexico-
syntactic knowledge derived from the lin-
guistics literature. We evaluate the ap-
proach on a variety of shell nouns with a
variety of syntactic expectations, achiev-
ing accuracies in the range of 62% (base-
line = 33%) to 83% (baseline = 74%) on
crowd-annotated data.

1 Introduction

Shell nouns are abstract nouns, such as fact, issue,
idea, and problem, which facilitate efficiency by
avoiding repetition of long stretches of text. The
shell metaphor comes from Schmid (2000), and it
captures the various functions of these nouns in a
discourse: containment, signalling, pointing, and
encapsulating. Shell nouns themselves are unspe-
cific, and can only be interpreted together with
their shell content, i.e., the propositional content
they encapsulate in the given context. The process
of identifying this content in the given context is
referred to as shell noun resolution or interpreta-
tion. Examples (1), (2), and (3) show usages of the
shell nouns fact and issue. The shell noun phrases
are resolved to the postnominal that clause, the
complement wh clause, and the immediately pre-

ceding clause, respectively.1,2

(1) The fact that a major label hadn’t been
at liberty to exploit and repackage the
material on CD meant that prices on the
vintage LP market were soaring.

(2) The issue that this country and Congress
must address is how to provide optimal
care for all without limiting access for
the many.

(3) Living expenses are much lower in rural
India than in New York, but this fact is
not fully captured if prices are converted
with currency exchange rates.

Observe that the relation between shell noun
phrases and their shell content is similar to
the relation of abstract anaphora (or cataphora)
(Asher, 1993) with backward- or forward-looking
abstract-object antecedents. For anaphoric shell
noun examples, the shell content precedes the
shell noun phrase, and for cataphoric shell noun
examples the shell content follows the shell noun
phrase.3

Shell nouns as a group occur frequently in argu-
mentative texts (Schmid, 2000; Flowerdew, 2003;
Botley, 2006). They play an important role in or-
ganizing a discourse and maintaining its coher-
ence (Schmid, 2000; Flowerdew, 2003), and re-
solving them is an important component of var-
ious computational linguistics tasks that rely on

1Note that the postnominal that-clause in (1) is not a rela-
tive clause: the fact in question is not an argument of exploit
and repackage.

2All examples in this paper are from the New
York Times corpus (https://catalog.ldc.upenn.edu/
LDC2008T19)

3We use the terms cataphoric shell noun and anaphoric
shell noun for lack of better alternatives.

499



discourse structure. Accordingly, identifying shell
content can be helpful in summarization, informa-
tion retrieval, and ESL learning (Flowerdew, 2003;
Hinkel, 2004).

Despite their importance in discourse, under-
standing of shell nouns from a computational lin-
guistics perspective is only in the preliminary
stage. Recently, we proposed an approach to anno-
tate and resolve anaphoric cases of six typical shell
nouns: fact, reason, issue, decision, question, and
possibility (Kolhatkar et al., 2013b). This work
drew on the observation that shell nouns following
cataphoric constructions are easy to resolve. We
manually developed rules to identify shell content
for such cases. Later, we used these cataphoric ex-
amples and their shell content as training data to
resolve harder anaphoric examples.

In this paper, we propose a general algorithm to
resolve cataphoric shell noun examples. Our long-
term goal is to build an end-to-end shell-noun res-
olution system. If we want to go beyond the six
shell nouns from our previous work, and general-
ize our approach to other shell nouns, first we need
to develop an approach to resolve cataphoric shell
noun examples. A number of challenges are asso-
ciated with this seemingly easy task. The primary
challenges is that this resolution is in many cru-
cial respects a semantic phenomenon. To obtain
the required semantic knowledge, we exploit the
properties of shell nouns and their categorization
described in the linguistics literature. We evalu-
ate our method using crowdsourcing, and demon-
strate how far one can get with simple, determin-
istic shell content extraction.

2 Related work

Shell-nounhood is a well-established concept in
linguistics (Vendler, 1968; Ivanic, 1991; Asher,
1993; Francis, 1994; Schmid, 2000, inter alia).
However, understanding of shell nouns from a
computational linguistics perspective is only in the
preliminary stage.

Shell nouns take a number of semantic argu-
ments. In this respect, they are similar to the gen-
eral class of argument-taking nominals as given
in the NomBank (Meyers et al., 2004). Simi-
larly, there is a small body of literature that ad-
dresses nominal semantic role labelling (Gerber et
al., 2009) and nominal subcategorization frames
(Preiss et al., 2007). That said, the distinguishing
property of shell nouns is that one of their seman-

tic arguments is the shell content, but the literature
in computational linguistics does not provide any
method that is able to identify the shell content.
The focus of our work is to rectify this.

Shell content represents complex and abstract
objects. So traditional linguistic and psycholin-
guistic principles used in pronominal anaphora
resolution (see the survey by Poesio et al. (2011)),
such as gender and number agreement, are not ap-
plicable in resolving shell nouns. That said, there
is a line of literature on annotating and resolving
personal and demonstrative pronouns, which typi-
cally refer to similar kinds of non-nominal abstract
entities (Passonneau, 1989; Eckert and Strube,
2000; Byron, 2003; Müller, 2008; Hedberg et
al., 2007; Poesio and Artstein, 2008; Navarretta,
2011, inter alia). Also, there have been attempts
at annotating the shell content of anaphoric occur-
rences of shell nouns (e.g., Botley (2006), Kol-
hatkar et al. (2013a)). However, none of these
approaches attempt to annotate and resolve cat-
aphoric examples such (1) and (2).

3 Challenges

A number of challenges are associated with the
task of resolving cataphoric shell noun examples,
especially when it comes to developing a holistic
approach for a variety of shell nouns.

First, each shell noun has idiosyncrasies. Dif-
ferent shell nouns have different semantic and syn-
tactic expectations, and hence they take different
types of one or more semantic arguments: one in-
troducing the shell content, and others expressing
circumstantial information about the shell noun.
For instance, fact typically takes a single factual
clause as an argument, which is its shell content,
as we saw in example (1), whereas reason expects
two arguments: the cause and the effect, with the
content introduced in the cause, as shown in exam-
ple (4).4 Similarly, decision takes an agent making
the decision and the shell content is represented as
an action or a proposition, as shown in (5).5

(4) One reason [that 60 percent of New York
City public-school children read below
grade level]effect is [that many elementary
schools don’t have libraries]cause.

4Observe that the postnominal that clause in (4) is not a
relative clause, and still it is not the shell content because it is
not the cause argument of the shell noun reason.

5Observe that this aspect of shell nouns of taking different
numbers and kinds of complement clauses is similar to verbs
having different subcategorization frames.

500



(5) I applaud loudly the decision of
[Greenburgh]agent to ban animal per-
formances.

Second, the relation between a shell noun and
its content is in many crucial respects a seman-
tic phenomenon. For instance, resolving the shell
noun reason to its shell content involves identify-
ing a) that reason generally expects two semantic
arguments: cause and effect, b) that the cause ar-
gument (and not the effect argument) represents
the shell content, and c) that a particular con-
stituent in the given context represents the cause
argument.

Third, at the conceptual level, once we know
which semantic argument represents shell content,
resolving examples such as (4) seems straightfor-
ward using syntactic structure, i.e., by extracting
the complement clause. But at the implementa-
tion level, this is a non-trivial problem for two rea-
sons. The first reason is that examples contain-
ing shell nouns often follow syntactically complex
constructions, including embedded clauses, coor-
dination, and sentential complements. An auto-
matic parser is not always accurate for such ex-
amples. So the challenge is whether the avail-
able tools in computational linguistics such as syn-
tactic parsers and discourse parsers are able to
provide us with the information that is necessary
to resolve these difficult cases. The second rea-
son is that the shell content can occur in many
different constructions, such as apposition (e.g.,
parental ownership of children, a concept that
allows . . . ), postnominal and complement clause
constructions, as we saw in examples (1) and (2),
and modifier constructions (e.g., the liberal trade
policy that . . . ). Moreover, in some constructions,
the content is indefinite (e.g., A bad idea does not
harm until someone acts upon it.) or None be-
cause the example is a non-shell noun usage (e.g.,
this week’s issue of Sports Illustrated), and the
challenge is to identify such cases.

Finally, whether the postnominal clause intro-
duces the shell content or not is dependent on
the context of the shell noun phrase. The reso-
lution can be complicated by complex syntactic
constructions. For instance, when the shell noun
follows verbs such as expect, it becomes difficult
for an automatic system to identify whether the
postnominal or the complement clause is of the
verb or of the shell noun (e.g., they did not expect
the decision to reignite tension in Crown Heights

vs. no one expected the decision to call an elec-
tion). Similarly, shell noun phrases can be ob-
jects of prepositions, and whether the postnomi-
nal clause introduces the shell content or not is de-
pendent on this preposition. For instance, for the
pattern reason that, the postnominal that clause
does not generally introduce the shell content, as
we saw in (4); however, this does not hold when
the shell noun phrase containing reason follows
the preposition for, as shown in (6).

(6) Low tax rates give people an incentive to
work, for the simple reason that they get
to keep more of what they earn.

4 Linguistic framework

Linguists have studied a variety of shell nouns,
their classification, different patterns they follow,
and their semantic and syntactic properties in de-
tail (Vendler, 1968; Ivanic, 1991; Asher, 1993;
Francis, 1994; Schmid, 2000, inter alia). Schmid
points out that being a shell noun is a property of
a specific usage of the noun rather than an inher-
ent property of the word. He provides a list of 670
English nouns that tend to occur as shell nouns. A
few frequently occurring ones are: problem, no-
tion, concept, issue, fact, belief, decision, point,
idea, event, possibility, reason, trouble, question,
plan, theory, aim, and principle.

4.1 Lexico-syntactic patterns
Precisely defining the notion of shell-nounhood
is tricky. A necessary property of shell nouns is
that they are capable of taking clausal arguments,
primarily with two lexico-syntactic constructions:
Noun + postnominal clause and Noun + be + com-
plement clause (Vendler, 1968; Biber et al., 1999;
Schmid, 2000; Huddleston and Pullum, 2002).
Schmid exploits these lexico-syntactic construc-
tions to identify shell noun usages. In particular,
he provides a number of typical lexico-syntactic
patterns that are indicative of either anaphoric or
cataphoric shell noun occurrences. Table 1 shows
these patterns with examples.

Cataphoric These patterns primarily follow two
constructions.

N-be-clause In this construction, the shell
noun phrase occurs as the subject in a subject-
verb-clause construction, with the linking verb be,
and the shell content embedded as a wh clause,
that clause, or to-infinitive clause. The linking

501



Cataphoric

1 N-be-to Our plan is to hire and retain the best managers we can.
2 N-be-that The major reason is that doctors are uncomfortable with uncertainty.
3 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel.
4 N-to The decision to disconnect the ventilator came after doctors found no brain activity.
5 N-that Mr. Shoval left open the possibility that Israel would move into other West Bank cities.
6 N-wh If there ever is any doubt whether a plant is a poppy or not, break off a stem and squeeze it.
7 N-of The concept of having an outsider as Prime Minister is outdated.

Anaphoric

8 th-N Living expenses are much lower in rural India than in New York, but this fact is not fully
captured if prices are converted with currency exchange rates.

9 th-be-N People change. This is a fact.
10 Sub-be-N If the money is available, however, cutting the sales tax is a good idea.

Table 1: Lexico-grammatical patterns of shell nouns (Schmid, 2000). Shell noun phrases are underlined,
the pattern is marked in boldface, and the shell content is marked in italics.

Proportion
Noun N-be-to N-be-that N-be-wh N-to N-that N-wh N-of total

idea 7 2 - 5 23 10 53 91,277
issue - 1 5 7 14 2 71 55,088
concept 1 - - 6 12 - 79 14,301
decision - - - 80 12 1 5 55,088
plan 5 - - 72 17 - 4 67,344
policy 4 1 - 16 25 2 51 24,025

Table 2: Distribution of cataphoric patterns for six shell nouns in the New York Times corpus. Each
column shows the percentage of instances following that pattern. The last column shows the total number
of cataphoric instances of each noun in the corpus.

verb be indicates the semantic identity between the
shell noun and its content in the given context. The
construction follows the patterns in rows 1, 2, and
3 of Table 1.

N-clause This construction includes the cat-
aphoric patterns 4–7 in Table 1. For these patterns
the link between the shell noun and the content
is much less straightforward: whether the post-
nominal clause expresses the shell content or not
is dependent on the shell noun and the syntac-
tic structure under consideration. For instance,
for the shell noun fact, the shell content is em-
bedded in the postnominal that clause, as shown
in (1), but this does not hold for the shell noun
reason in example (4). The N-of pattern is dif-
ferent from other patterns: it follows the con-
struction N-prepositional phrase rather than N-
clause, and since a prepositional phrase can take
different kinds of embedded constituents such as a

noun phrase, a sentential complement, and a verb
phrase, the pattern offers flexibility in the syntactic
type of the shell content.

Anaphoric For these patterns, the link between
the shell noun and the content is created using
linguistic elements such as the, this, that, other,
same, and such. For the patterns 8 and 9 the shell
content does not typically occur in the sentence
containing the shell noun phrase. For the pattern
10, the shell content is the subject in a subject-
verb-N construction.

Pattern preferences Different shell nouns have
different pattern preferences. Table 2 shows the
distribution of cataphoric patterns for six shell
nouns in the New York Times corpus. The shell
nouns idea, issue, and concept prefer N-of pattern,
whereas plan and decision prefer the pattern N-to.
Among all instances of the shell noun decision fol-

502



Idea family
Semantic features: [mental], [conceptual]
Frame: mental; focus on propositional content of IDEA
Nouns: idea, issue, concept, point, notion, theory, . . .
Patterns: N-be-that/of, N-that/of

Plan family
Semantic features: [mental], [volitional], [manner]
Frame: mental; focus on IDEA
Nouns: decision, plan, policy, idea, . . .
Patterns: N-be-to/that, N-to/that

Trouble family
Semantic features: [eventive], [attitudinal], [manner],
[deontic]
Frame: general eventive
Nouns: problem, trouble, difficulty, dilemma, snag
Patterns: N-be-to

Problem family
Semantic features: [factual], [attitudinal], [impeding]
Frame: general factual
Nouns: problem, trouble, difficulty, point, thing, snag,
dilemma , . . .
Patterns: N-be-that/of

Thing family
Semantic features: [factual]
Frame: general factual
Nouns: fact, phenomenon, point, case, thing, business
Patterns: N-that, N-be-that

Reason family
Semantic features: [factual], [causal]
Frame: causal; attentional focus on CAUSE
Nouns: reason, cause, ground, thing
Patterns: N-be-that/why, N-that/why

Table 3: Example families from Schmid (2000). The nouns in boldface are used to evaluate this work.

lowing Schmid’s cataphoric patterns, 80% of the
instances follow the pattern N-to.6

4.2 Categorization of shell nouns
Schmid classifies shell nouns at three levels. At
the most abstract level, he classifies shell nouns
into six semantic classes: factual, linguistic, men-
tal, modal, eventive, and circumstantial. Each se-
mantic class indicates the type of experience the
shell noun is intended to describe. For instance,
the mental class describes ideas and cognitive
states, whereas the linguistic class describes utter-
ances, linguistic acts, and products thereof.

The next level of classification includes more-
detailed semantic features. Each broad semantic
class is sub-categorized into a number of groups.
A group of an abstract class tries to capture
the semantic features associated with the fine-
grained differences between different usages of
shell nouns in that class. For instance, groups
associated with the mental class are: conceptual,
creditive, dubiative, volitional, and emotive.

The third level of classification consists of fam-
ilies. A family groups together shell nouns with
similar semantic features. Schmid provides 79 dis-
tinct families of 670 shell nouns. Each family is
named after the primary noun in that family. Table
3 shows six families: Idea, Plan, Trouble, Prob-
lem, Thing, and Reason. A shell noun can be

6Table 2 does not include anaphoric patterns, as this pa-
per is focused on cataphoric shell noun examples. Anaphoric
patterns are common for all shell nouns: among all instances
of a shell noun, approximately 50 to 80% are anaphoric.

a member of multiple families. The nouns sub-
sumed in a family share semantic features. For
instance, all nouns in the Idea family are mental
and conceptual. They are mental because ideas
are only accessible through thoughts, and concep-
tual because they represent reflection or an appli-
cation of a concept. Each family activates a se-
mantic frame. The idea of these semantic frames is
similar to that of frames in Frame semantics (Fill-
more, 1985) and in semantics of grammar (Talmy,
2000). In particular, Schmid follows Talmy’s con-
ception of frames. A semantic frame describes
conceptual structures, its elements, and their in-
terrelationships. For instance, the Reason family
invokes the causal frame, which has cause and ef-
fect as its elements with the attentional focus on
the cause. According to Schmid, the nouns in a
family also share a number of lexico-syntactic fea-
tures. The patterns attribute in Table 3 shows pro-
totypical lexico-syntactic patterns, which attract
the members of the family. Schmid defines attrac-
tion as the degree to which a lexico-grammatical
pattern attracts a certain noun. For instance, the
patterns N-to and N-that attract the shell nouns in
the Plan family, whereas the N-that pattern attracts
the nouns in the Thing family. The pattern N-of is
restricted to a smaller group of nouns such as con-
cept, problem, and issue.7,8

7Schmid used the British section of COBUILD’S Bank of
English for his classification.

8Schmid’s families could help enrich resources such as
FrameNet (Baker et al., 1998) with the shell content informa-
tion.

503



5 Resolution algorithm

With this exposition, the problem of shell noun
resolution is identifying the appropriate seman-
tic argument of the shell noun representing its
shell content. This section describes our algorithm
to resolve shell nouns following cataphoric pat-
terns. The algorithm addresses the primary chal-
lenge of idiosyncrasies of shell nouns by exploit-
ing Schmid’s semantic families (see Section 4.2).
The input of the algorithm is a shell noun instance
following a cataphoric pattern, and the output is
its shell content or None if the shell content is not
present in the given sentence. The algorithm fol-
lows three steps. First, we parse the given sentence
using the Stanford parser.9 Second, we look for
the noun phrase (NP), where the head of the NP is
the shell noun to be resolved.10 Finally, we extract
the appropriate shell content, if it is present in the
given sentence.

5.1 Identifying potentially anaphoric
shell-noun constructions

Before starting the actual resolution, first we iden-
tify whether the shell content occurs in the given
sentence or not. According to Schmid, the lexico-
syntactic patterns signal the position of the shell
content. For instance, if the pattern is of the form
N-be-clause, the shell content is more likely to
occur in the complement clause in the same sen-
tence. That said, although on the surface level, the
shell noun seems to follow a cataphoric pattern, it
is possible that the shell content is not given in a
postnominal or a complement clause, as shown in
(7).

(7) Just as weekend hackers flock to the golf
ball most used by PGA Tour players,
recreational skiers, and a legion of youth
league racers, gravitate to the skis worn
by Olympic champions. It is the reason
that top racers are so quick flash their skis
for the cameras in the finish area.

Here, the shell noun and its content are linked via
the pronoun it. For such constructions, the shell
noun phrase and shell content do not occur in the
same sentence. Shell content occurs in the preced-
ing discourse, typically in the preceding sentence.

9http://nlp.stanford.edu/software/
lex-parser.shtml

10We extract the head of an NP following the heuristics
proposed by Collins (1999, p. 238).

We identify such cases, and other cases where the
shell content is not likely to occur in the postnom-
inal or complements clauses, by looking for the
patterns below in the given order, returning the
shell content when it occurs in the given sentence.

Sub-be-N This pattern corresponds to the
lexico-grammatical pattern in Figure 1(a). If this
pattern is found, there are three main possibilities
for the subject. First, if an existential there occurs
at the subject position, we move to the next pat-
tern. Second, if the subject is it (example (7)), this
or that, we return None, assuming that the con-
tent is not present in the given sentence. Finally,
if the first two conditions are not satisfied, i.e., if
the subject is neither a pronoun not an existential
there, we assume that subject contains a valid shell
content, and return it. An example is shown in (8).
Note that in such cases, unlike other patterns, the
shell content is expressed as a noun phrase.

(8) Strict liability is the biggest issue when
considering what athletes put in their bod-
ies.

Apposition Another case where shell content
does not typically occur in the postnominal or
complement clause is the case of apposition. In-
definite shell noun phrases often occur in apposi-
tion constructions, as shown in (9).

(9) The LH lineup, according to Gale, will
feature “cab-forward” design, a concept
that particularly pleases him.

In this step, we check for this construction and re-
turn the sentential, verbal, or nominal left sibling
of the shell noun phrase.

Modifier For shell nouns such as issue, phe-
nomenon, and policy, often the shell content is
given in the modifier of the shell noun, as shown
in (10).

(10) But in the 18th century, Leipzig’s central
location in German-speaking Europe and
the liberal trade policy of the Saxon court
fostered publishing.

We deal with such cases as follows. First, we
extract the modifier phrases by concatenating the
modifier words having noun, verb, or adjective
part-of-speech tags. To exclude unlikely modi-
fier phrases as shell content (e.g., good idea, big

504



Parent

NP

Subject

VP

VB*

form of be

NP/NN

head = shell

(a) Sub-be-N pattern

Parent

NP/NN

head = shell

VP

VB*

form of be

SBAR/S

IN

that/wh

S

clause

(b) N-be-clause pattern

Parent

NP/NN

head = shell

SBAR/S

IN

that/wh

S

clause

(c) N-that/wh pattern

Figure 1: Lexico-syntactic patterns for shell nouns

issue), we extract a list of modifiers for a num-
ber of shell nouns and create a stoplist of modi-
fiers. If any of the words in the modifier phrases
is a pronoun or occurs in the stoplist, we move to
the next pattern. If the modifier phrase passes the
stoplist test, to distinguish between non-shell con-
tent and shell content modifiers, we examine the
hypernym paths of the words in the modifier
phrase in WordNet (Fellbaum, 1998). If the synset
abstraction.n.06 occurs in the path, we consider
the modifier phrase to be valid shell content, as-
suming that the shell content of shell nouns most
typically represents an abstract entity.

5.2 Resolving remaining instances
At this stage we are assuming that the shell con-
tent occurs either in the postnominal clause or the
complement clause. So we look for the patterns
below, returning the shell content when found.

N-be-clause The lexico-grammatical pattern
corresponding to the pattern N-be-clause is shown
in Figure 1(b). This is one of the more reliable
patterns for shell content extraction, as the be verb
suggests the semantic identity between the shell
noun and the complement clause. The be-verb
does not necessarily have to immediately follow
the shell noun. For instance, in example (2), the
head of the NP The issue that this country and
Congress must address is the shell noun issue, and
hence it satisfies the construction in Figure 1(b).

N-clause Finally, we look for this pattern. An
example of this pattern is shown in Figure 1(c).
This is the most common (see Table 2) and tricki-
est pattern in terms of resolution, and whether the
shell content is given in the postnominal clause or
not is dependent on the properties of the shell noun
under consideration and the syntactic construction
of the example. For instance, for the shell noun
decision, the postnominal to-infinitive clause typi-

cally represents shell content. But this did not hold
for the shell noun reason, as shown in (11).

(11) The reason to resist becoming a partici-
pant is obvious.

Here, Schmid’s semantic families come in the
picture. We wanted to examine a) the extent to
which the previous steps help in resolution, and b)
whether knowledge extracted from Schmid’s fam-
ilies add value to the resolution. So we employ
two versions of this step.

Include Schmid’s cues (+SC) This version
exploits the knowledge encoded in Schmid’s se-
mantic families (Section 4.2), and extracts post-
nominal clauses only if Schmid’s pattern cues are
satisfied. In particular, given a shell noun, we de-
termine the families in which it occurs and list all
possible patterns of these families as shell content
cues. The postnominal clause is a valid shell con-
tent only if it satisfies these cues. For instance,
the shell noun reason occurs in only one family:
Reason, with the allowed shell content patterns N-
that and N-why. Schmid’s patterns suggest that the
postnominal to-infinitive clauses are not allowed
as shell content for this shell noun, and thus this
step will return None. This version helps correctly
resolving examples such as (11) to None.

Exclude Schmid’s cues (–SC) This version
does not enforce Schmid’s cues in extracting the
postnominal clauses. For instance, the Problem
family does not include N-that/wh/to/of patterns,
but in this condition, we nonetheless allow these
patterns in extracting the shell content of the nouns
from this family.

6 Evaluation data

We claim that our algorithm is able to resolve a
variety of shell nouns. That said, creating eval-
uation data for all of Schmid’s 670 English shell

505



nouns is extremely time-consuming, and is there-
fore not pursued further in the current study. In-
stead we create a sample of representative evalua-
tion data to examine how well the algorithm works
a) on a variety of shell nouns, b) for shell nouns
within a family, c) for shell nouns across families
with completely different semantic and syntactic
expectations, and d) for a variety of shell patterns
from Table 1.

6.1 Selection of nouns
Recall that each shell noun has its idiosyncrasies.
So in order to evaluate whether our algorithm is
able to address these idiosyncrasies, the evalua-
tion data must contain a variety of shell nouns with
different semantic and syntactic expectations. To
examine a), we consider the six families shown in
Table 3. These families span three abstract cat-
egories: mental, eventive, and factual, and five
distinct groups: conceptual, volitional, factual,
causal, and attitudinal. Also, the families have
considerably different syntactic expectations. For
instance, the nouns in the Idea family can have
their content in that or of clauses occurring in N-
clause or N-be-clause constructions, whereas the
Trouble and Problem families do not allow N-
clause pattern. The shell content of the nouns in
the Plan family is generally represented with to-
infinitive clauses. To examine b) and c), we choose
three nouns from each of the first four families
from Table 3. To add diversity, we also include
two shell nouns from the Thing family and a shell
noun from the Reason family. So we selected a
total of 12 shell nouns for evaluation: idea, issue,
concept, decision, plan, policy, problem, trouble,
difficulty, reason, fact, and phenomenon.

6.2 Selection of instances
Recall that the shell content varies based on the
shell noun and the pattern it follows. Moreover,
shell nouns have pattern preferences, as shown in
Table 2. To examine d), we need shell noun exam-
ples following different patterns from Table 1. We
consider the New York Times corpus as our base
corpus, and from this corpus extract all sentences
following the lexico-grammatical patterns in Ta-
ble 1 for the twelve selected shell nouns. Then we
arbitrarily pick 100 examples for each shell noun,
making sure that the selection contains examples
of each cataphoric pattern from Table 1. These
examples consist of 70% examples of each of the
seven cataphoric patterns, and the remaining 30%

of the examples are picked randomly from the dis-
tribution of patterns for that shell noun.

6.3 Crowdsourcing annotation
We designed a crowdsourcing experiment to ob-
tain the annotated data for evaluation. We parse
each sentence using the Stanford parser, and ex-
tract all possible candidates, i.e., arguments of the
shell noun from the parser’s output. Since our ex-
amples include embedding clauses and sentential
complements, the parser is often inaccurate. For
instance, in example (12), the parser attaches only
the first clause of the coordination (that people
were misled) to the shell noun fact.

(12) The fact that people were misled and in-
formation was denied, that’s the reason
that you’d wind up suing.

To deal with such parsing errors, we consider the
30-best parses given by the parser. From these
parses, we extract a list of eligible candidates. This
list includes the arguments of the shell noun given
in the appositional clauses, modifier phrases, post-
nominal that, wh, or to-infinitive clauses, comple-
ment clauses, objects of postnominal prepositions
of the shell noun, and subject if the shell noun fol-
lows subject-be-N construction. On average, there
were three candidates per instance.

After extracting the candidates, we present the
annotators with the sentence, with the shell noun
highlighted, and the extracted candidates. We ask
the annotators to choose the option that provides
the correct interpretation of the highlighted shell
noun. We also provide them the option None of
the above, and ask them to select it if the shell con-
tent is not present in the given sentence or the shell
content is not listed in the list of candidates.

CrowdFlower We used CrowdFlower11 as our
crowdsourcing platform, which in turn uses vari-
ous worker channels such as Amazon Mechanical
Turk12. CrowdFlower offers a number of features.
First, it provides a quiz mode which facilitates
filtering out spammers by requiring an annotator
to pass a certain number of test questions before
starting the real annotation. Second, during an-
notation, it randomly presents test questions with
known answers to the annotators to keep them on
their toes. Based on annotators’ responses to the
test questions, each annotator is assigned a trust

11http://crowdflower.com/
12https://www.mturk.com/mturk/welcome

506



≥ 5 ≥ 4 ≥ 3 < 3
idea 53 67 95 5
issue 44 65 95 5
concept 40 56 96 4
decision 50 72 98 2
plan 41 55 95 5
policy 42 61 94 6
problem 52 70 100 0
trouble 44 69 99 1
difficulty 45 61 96 4
reason 48 60 93 7
fact 52 68 98 2
phenomenon 39 56 95 5

all 46 63 96 4

Table 4: Annotator agreement on shell content.
Each column shows the percentage of instances on
which at least n or fewer than n annotators agree
on a single answer.

score: an annotator performing well on the test
questions gets a high trust score. Finally, Crowd-
Flower allows the user to select the permitted de-
mographic areas and skills required.

Settings We asked for at least 5 annotations per
instance by annotators from the English-speaking
countries. The evaluation task contained a total
of 1200 instances, 100 instances per shell noun.
To maintain the annotation quality, we included
105 test questions, distributed among different an-
swers. We paid 2.5 cents per instance and the an-
notation task was completed in less than 24 hours.

Results Table 4 shows the agreement of the
crowd. In most cases, at least 3 out of 5 anno-
tators agreed on a single answer. We took this an-
swer as the gold standard in our evaluation, and
discard the instances where fewer than three anno-
tators agreed. The option None of the above was
annotated for about 30% of the cases. We include
these cases in the evaluation. In total we had 1,257
instances (1,152 instances where at least 3 annota-
tors agreed + 105 test questions).

7 Evaluation results

Baseline We evaluate our algorithm against
crowd-annotated data using a lexico-syntactic
clause (LSC) baseline. Given a sentence con-
taining a shell instance and its parse tree, this
baseline extracts the postnominal or complement
clause from the parse tree depending only upon
the lexico-syntactic pattern of the shell noun. For
instance, for the N-that and N-be-to patterns, it ex-

Nouns LSC A–SC A+SC

1 idea 74 82 83
2 issue 60 75 77
3 concept 51 67 68
4 decision 70 71 73
5 plan 51 63 62
6 policy 58 70 52
7 problem 66 69 59
8 trouble 63 68 50
9 difficulty 68 75 49
10 reason 43 53 77
11 fact 43 55 68
12 phenomenon 33 62 50

13 all 57 69 64

Table 5: Shell noun resolution results. Each col-
umn shows the percent accuracy of resolution with
the respective method. Boldface is best in row.

tracts the postnominal that clause and the comple-
ment to-infinitive clause, respectively.13

Results Table 5 shows the evaluation results for
the LSC baseline, the algorithm without Schmid’s
cues (A–SC), and the algorithm with Schmid’s
cues (A+SC). The A–SC condition in all cases and
the A+SC condition in some cases outperform the
LSC baseline, which proves to be rather low, espe-
cially for the shell nouns with strict syntactic ex-
pectations (e.g., fact and reason). Thus we see that
our algorithm is adding value.

That said, we observe a wide range of per-
formance for different shell nouns. On the up
side, adding Schmid’s cues helps resolving the
shell nouns with strict syntactic expectations. The
A+SC results for the shell nouns idea, issue, con-
cept, decision, reason, and fact outperform the
baseline and the A–SC results. In particular, the
A+SC results for the shell nouns fact and rea-
son are markedly better than the baseline results.
These nouns have strict syntactic expectations for
the shell content clauses they take: the families
Thing and Certainty of the shell noun fact allow
only a that clause, and the Reason family of the
shell noun reason allows only that and because
clauses for the shell content. These cues help
in correctly resolving examples such as (11) to
None, where the postnominal to-infinitive clause

13Note that we only extract subordinating clauses (e.g.,
(SBAR (IN that) (clause))) and to-infinitive clauses, and not
relative clauses.

507



describes the purpose or the goal for the reason,
but not the shell content itself.

On the down side, adding Schmid’s cues hurts
the performance of more versatile nouns, which
can take a variety of clauses. Although the A–SC
results for the shell nouns plan, policy, problem,
trouble, difficulty, and phenomenon are well above
the baseline, the A+SC results are markedly be-
low it. That is, Schmid’s cues were deleterious.
Our error analysis revealed that these nouns are
versatile in terms of the clauses they take as shell
content, and Schmid’s cues restrict these clauses
to be selected as shell content. For instance, the
shell noun problem occurs in two semantic fami-
lies with N-be-that/of and N-be-to as pattern cues
(Table 3), and postnominal clauses are not allowed
for this noun. Although these cues help in filtering
some unwanted cases, we observed a large number
of cases where the shell content is given in post-
nominal clauses, as shown in (13).

(13) I was trying to address the problem of un-
reliable testimony by experts in capital
cases.

Similarly, the Plan family does not allow the N-
of pattern. This cue works well for the shell noun
decision from the same family because often the
postnominal of clause is the agent for this shell
noun and not the shell content. However, it hurts
the performance of the shell noun policy, as N-
of is a common pattern for this shell noun (e.g.,
. . . officials in Rwanda have established a policy of
refusing to protect refugees. . . ). Other failures of
the algorithm are due to parsing errors and lack of
inclusion of context information.

8 Discussion and conclusion

In this paper, we proposed a general method to re-
solve shell nouns following cataphoric construc-
tions. This is a first step towards end-to-end shell
noun resolution. In particular, this method can
be used to create training data for any given shell
noun, which can later be used to resolve harder
anaphoric cases of that noun using the method that
we proposed earlier (Kolhatkar et al., 2013b).

The first goal of this work was to point out the
difficulties associated with the resolution of cat-
aphoric cases of shell nouns. The low resolution
results of the LSC baseline demonstrate the diffi-
culties of resolving such cases using syntax alone,

suggesting the need for incorporating more lin-
guistic knowledge in the resolution.

The second goal of this work was to examine to
what extent knowledge derived from the linguis-
tics literature helps in resolving shell nouns. We
conclude that Schmid’s pattern and clausal cues
are useful for resolving nouns with strict syntac-
tic expectations (e.g., fact, reason); however, these
cues are defeasible: they miss a number of cases in
our corpus. It is possible to improve on Schmid’s
cues using crowdsourcing annotation and by ex-
ploiting lexico-syntactic patterns associated with
different shell nouns from a variety of corpora.

One limitation of our approach is that in our res-
olution framework, we do not consider the prob-
lem of ambiguity of nouns that might not be used
as shell nouns. The occurrence of nouns with the
lexical patterns in Table 1 does not always guaran-
tee shell noun usage. For instance, in our data, we
observed a number of instances of the noun issue
with the publication sense (e.g., this week’s issue
of Sports Illustrated).

Our algorithm is able to deal with only a re-
stricted number of shell noun usage constructions,
but the shell content can be expressed in a variety
of other constructions. A robust machine learning
approach that incorporates context and deeper se-
mantics of the sentence, along with Schmid’s cues,
could mitigate this limitation.

This work opens a number of new research di-
rections. Our next planned task is clustering dif-
ferent shell nouns based on the kind of comple-
ments they take in different usages similar to verb
clustering (Merlo and Stevenson, 2000; Schulte im
Walde and Brew, 2002).

Acknowledgements

We thank the anonymous reviewers for their com-
ments. We also thank Suzanne Stevenson, Gerald
Penn, Heike Zinsmeister, Kathleen Fraser, Aida
Nematzadeh, and Ryan Kiros for their feedback.
This research was financially supported by the
Natural Sciences and Engineering Research Coun-
cil of Canada and by the University of Toronto.

References
Nicholas Asher. 1993. Reference to Abstract Objects

in Discourse. Kluwer Academic Publishers, Dor-
drecht, Netherlands.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-

508



ings of the 17th International Conference on Com-
putational Linguistics, volume 1 of COLING ’98,
pages 86–90, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, and Edward Finegan. 1999. Longman
Grammar of Spoken and Written English. Pearson
ESL, November.

Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73–112.

Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.

Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.

Miriam Eckert and Michael Strube. 2000. Dialogue
acts, synchronizing units, and anaphora resolution.
Journal of Semantics, 17:51–89.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6(2):222–
254.

John Flowerdew. 2003. Signalling nouns in discourse.
English for Specific Purposes, 22(4):329–346.

Gill Francis. 1994. Labelling discourse: An aspect of
nominal group lexical cohesion. In M. Coulthard,
editor, Advances in written text analysis, pages 83–
101. Routledge, London.

Matthew Gerber, Joyce Chai, and Adam Meyers. 2009.
The role of implicit argumentation in nominal srl.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 146–154, Boulder, Colorado, June.
Association for Computational Linguistics.

Nancy Hedberg, Jeanette K. Gundel, and Ron
Zacharski. 2007. Directly and indirectly anaphoric
demonstrative and personal pronouns in newspaper
articles. In Proceedings of DAARC-2007 8th Dis-
course Anaphora and Anaphora Resolution Collo-
quium, pages 31–36.

Eli Hinkel. 2004. Teaching Academic ESL Writ-
ing: Practical Techniques in Vocabulary and Gram-
mar (ESL and Applied Linguistics Professional).
Lawrence Erlbaum, Mahwah, NJ, London.

Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.

Roz Ivanic. 1991. Nouns in search of a context: A
study of nouns with both open- and closed-system
characteristics. International Review of Applied Lin-
guistics in Language Teaching, 29:93–114.

Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013a. Annotating anaphoric shell nouns
with their antecedents. In Proceedings of the 7th
Linguistic Annotation Workshop and Interoperabil-
ity with Discourse, pages 112–121, Sofia, Bulgaria,
August. Association for Computational Linguistics.

Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013b. Interpreting anaphoric shell nouns us-
ing antecedents of cataphoric shell nouns as training
data. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 300–310, Seattle, Washington, USA, October.
Association for Computational Linguistics.

Paola Merlo and Suzanne Stevenson. 2000. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373–408.

Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation.

Christoph Müller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universität Tübingen.

Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse
phenomena, Göttingen, Germany, Feb.

Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51–59, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.

Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC’08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).

Massimo Poesio, Simone Ponzetto, and Yannick Vers-
ley. 2011. Computational models of anaphora reso-
lution: A survey. Unpublished.

Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 912–919, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.

509



Hans-Jörg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. Mouton de Gruyter,
Berlin.

Sabine Schulte im Walde and Chris Brew. 2002. In-
ducing German semantic verb classes from purely
syntactic subcategorisation information. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 223–230,
Philadelphia, PA.

Leonard Talmy. 2000. The windowing of attention.
In Toward a Cognitive Semantics, volume 1, pages
257–309. The MIT Press.

Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton and Co., The Netherlands.

510


