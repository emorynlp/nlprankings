










































Annotating Participant Reference in English Spoken Conversation


Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256–264,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Annotating Participant Reference in English Spoken Conversation

John Niekrasz and Johanna D. Moore
School of Informatics

University of Edinburgh
Edinburgh, EH8 9AB, UK

{jniekras,jmoore}@inf.ed.ac.uk

Abstract

In conversational language, references to
people (especially to the conversation par-
ticipants, e.g., I, you, and we) are an es-
sential part of many expressed meanings.
In most conversational settings, however,
many such expressions have numerous po-
tential meanings, are frequently vague,
and are highly dependent on social and sit-
uational context. This is a significant chal-
lenge to conversational language under-
standing systems — one which has seen
little attention in annotation studies. In this
paper, we present a method for annotat-
ing verbal reference to people in conver-
sational speech, with a focus on reference
to conversation participants. Our goal is
to provide a resource that tackles the is-
sues of vagueness, ambiguity, and contex-
tual dependency in a nuanced yet reliable
way, with the ultimate aim of supporting
work on summarization and information
extraction for conversation.

1 Introduction

Spoken conversation — the face-to-face verbal in-
teraction we have every day with colleagues, fam-
ily, and friends — is the most natural setting for
language use. It is how we learn to use language
and is universal to the world’s societies. This
makes it an ideal subject for research on the ba-
sic nature of language and an essential subject for
the development of technologies supporting natu-
ral communication. In this paper, we describe our
research on designing and applying an annotation
procedure for a problem of particular relevance to
conversational language — person reference.

The procedure is a coreference annotation of all
references to people, and the focus of our scheme
is on distinguishing different types of participant

reference (references to the conversation’s partic-
ipants), the predominant type of person reference
in face-to-face multi-party conversation. Partici-
pant reference is exemplified by the use of proper
names such as James or most commonly by the
pronouns I, you, and we.

Participant reference plays an essential role in
many of the most important types of expressed
meanings and actions in conversation, including
subjective language, inter-personal agreements,
commitments, narrative story-telling, establishing
social relationships, and meta-discourse. In fact,
some person-referring words are the most frequent
words in conversation.1

Perhaps contrary to intuition, however, in-
terpreting person-referring expressions can be
rather complex. Person-reference interpretation
is strongly dependent on social, situational, and
discourse context. The words you and we
are especially problematic. Either can be used
for generic, plural, or singular reference, as
addressee-inclusive or addressee-exclusive, in ref-
erence to hypothetical individuals or non-human
entities, or even metonymically in reference to ob-
jects connected to individuals (Mühlhäusler and
Harré, 1990; Wales, 1996). In addition, these and
many other issues are not simply occasional prob-
lems but arise regularly.

Consider the following utterance from the AMI
corpus of remote control design meetings, which
is typical of the corpus in terms of complexity of
person-reference.

1The words I and you are the most frequently used nom-
inals in several conversational corpora, including Switch-
board (Godfrey et al., 1992) and the AMI Meeting Cor-
pus (McCowan et al., 2005). In the British National Corpus
they are the two most common of any words in the demo-
graphic (i.e., conversational) subcorpus (Burnard, 2007), and
Google’s Web 1T 5-gram statistics (Brants and Franz, 2006)
list I and you as more frequent even than the word it. The
word we falls within the top 10 most frequent words in all of
these corpora.

256



“Current remote controls do not match well with
the operating behaviour of the user overall. For
example, you can see below there, seventy five
percent of users zap a lot, so you’ve got your
person sunk back in the sofa channel-hopping.”

As this example demonstrates, person-referring
expressions have many potential meanings and are
often vague or non-specific. In this case, “the
user” refers to a non-specific representative of a
hypothetical group, which is referred to itself as
“users.” The first use of “you” refers to the ad-
dressees, but the second use has a more ‘generic’
meaning whilst retaining an addressee-oriented
meaning as well. The phrase “your person” refers
to a specific hypothetical example of the “users”
referred to previously.

1.1 Purpose of the Annotations

The annotation research we describe here aims at
addressing the fact that if conversational language
applications are to be useful and effective (our
interest is primarily with abstractive summariza-
tion), then accurate interpretation of reference to
the conversation’s participants is of critical impor-
tance. Our work looks at language as a means for
action (Clark, 1996), and our focus is on those ac-
tions that the participants themselves consider as
relevant and salient, such as the events occurring
in a meeting that might appear in the minutes of
the meeting. For our system to identify, distin-
guish, or describe such events, it is essential for
it to understand the participants’ roles and rela-
tionships to those events through interpreting their
linguistic expression within the dialogue. This in-
cludes understanding direct reference to partici-
pants and recognizing discourse structure through
evidence of referential coherence.

Another aim of our research is to increase un-
derstanding of the nature of participant reference
through presenting a nuanced yet reliable set of
type and property distinctions. We propose novel
distinctions concerning three main issues. The
first distinction concerns vagueness and indetermi-
nacy, which is often exploited by speakers when
using words such as you, they, and we. Our aim
is to provide a reliable basis for making an ex-
plicit distinction between specific and vague uses,
motivated by usefulness to the aforementioned ap-
plications. The second distinction concerns an
issue faced frequently in informal conversation,
where words typically used to do person-referring
are also commonly used in non-person-referring

ways. A principal goal is thus establishing reliable
person/non-person and referential/non-referential
distinctions for these words. The third issue con-
cerns addressing roles (i.e., speaker, addressee,
and non-addressee), which we propose can be a
useful means for further distinguishing between
different types of underspecified and generic refer-
ences, beyond the specific/underspecified/generic
distinctions made in schemes such as ACE (Lin-
guistic Data Consortium, 2008).

1.2 Summary and Scope of Contributions

The work described in this paper includes the de-
sign of an annotation procedure and a statistical
analysis of a corpus of annotations and their re-
liability. The procedure we propose (Section 3)
is based on a simple non-anaphoric coreference-
like scheme, modest in comparison to much pre-
vious work. The produced dataset (Section 4) in-
cludes annotations of 11,000 occasions of person-
referring in recorded workplace meetings. Our
analysis of the dataset includes a statistical sum-
mary of interesting results (Section 4.1) and an
analysis of inter-coder agreement (with discussion
of specific disagreements) for the introduced dis-
tinctions (Section 4.2).

Though our annotation procedure is designed
primarily for multi-party spoken conversation,
some of the central issues that concern us, such
as addressee inclusion and vagueness, arise in
textual and non-conversational settings as well.
Our scheme therefore has relevance to general
work on reference annotation, though principally
to settings where social relationships between
the participants (i.e., speakers/authors and ad-
dressees/readers) are important.

2 Related Annotation Schemes

Previous work on reference annotation has cov-
ered a wide range of issues surrounding reference
generally. It is useful to categorize this work ac-
cording to the natural language processing tasks
the annotations are designed to support.

2.1 Schemes for anaphora and generation

Several schemes have been designed with the goal
of testing linguistic theoretical models of dis-
course structure or for use in the study of discourse
processing problems like anaphora resolution and
reference generation. These schemes have been
applied to both text and dialogue and label dis-

257



course references with a rich set of syntactic, se-
mantic, and pragmatic properties. For example,
the DRAMA scheme (Passonneau, 1997) and the
GNOME scheme (Poesio, 2000; Poesio, 2004) in-
clude labels for features such as bridging relation
type and NP type in addition to a rich representa-
tion of referent semantics. Other schemes label an-
imacy, prosody, and information structure to study
their relationship to the organization and salience
of discourse reference (Nissim et al., 2004; Cal-
houn et al., 2005). Recent developments include
the explicit handling of anaphoric ambiguity and
discourse deixis (Poesio and Artstein, 2008).

Despite the depth and detail of these schemes,
participant reference has not been their main con-
cern. The annotations by Poesio et al. (2000;
2004) include dialogue source material, but the
rather constrained interactional situations do not
elicit a rich set of references to participants. The
scheme thus employs simple default labels for
words like I and you. The work by Nissim et
al., (2004) is an annotation of the Switchboard cor-
pus (Godfrey et al., 1992), which contains only
two participants who are neither co-present nor
socially connected. Participant reference is thus
rather constrained. Other than labeling corefer-
entiality, the Nissim scheme includes only a sin-
gle distinction between referential and generic in-
stances of the word you.

2.2 Schemes for information extraction

In contrast to the schemes described above, which
are mainly driven toward investigating linguistic
theories of discourse processing, some reference
annotation projects are motivated instead by infor-
mation extraction applications. For these projects
(which includes our own), a priority is placed on
entity semantics and coreference to known entities
in the world. For example, the objective of the Au-
tomatic Content Extraction (ACE) program (Dod-
dington et al., 2004) is to recognize and extract
entities, events, and relations between them, di-
rectly from written and spoken sources, mostly
from broadcast news. The schemes thus focus
on identifying and labeling the properties of en-
tities in the real world, and then marking expres-
sions as referring to these entities. Recent work
in the ACE project has expanded the scope of
this task to include cross-document recognition
and resolution (Strassel et al., 2008). In the ACE
scheme (Linguistic Data Consortium, 2008), per-

son reference is a central component, and in the
broadcast conversation component of the corpus
there is an extensive inventory of participant refer-
ences. The annotation scheme contains a distinc-
tion between specific, underspecified, and general
entities, as well as a distinction between persons
and organizations.

Another closely related set of studies are four
recent investigations of second-person reference
resolution (Gupta et al., 2007a; Gupta et al.,
2007b; Frampton et al., 2009; Purver et al.,
2009). These studies are based upon a common
set of annotations of the word you in source mate-
rial from the Switchboard and ICSI Meeting cor-
pora. The purpose for the annotations was to
support learning of classifiers for two main prob-
lems: disambiguation of the generic/referential
distinction, and reference resolution for referential
cases. In addition to the generic/referential dis-
tinction and an addressing-based reference anno-
tation, the scheme employed special classes for re-
ported speech and fillers and allowed annotators to
indicate vague or difficult cases. Our work builds
directly upon this work by extending the annota-
tion scheme to all person-referring expressions.

3 Annotation Method

Our person-reference annotation method consists
of two main phases: a preliminary phase where
the first names of the conversation participants are
identified, and a subsequent person reference la-
beling process. The first phase is not of central
concern in this paper, though we provide a brief
summary below (Section 3.2). The primary focus
of this paper is the second phase (Section 3.3), dur-
ing which every instance of person-referring oc-
curring in a given meeting is labelled. We pro-
vide more detail concerning the most novel and
challenging aspects of the person-referring label-
ing process in Section 3.4 and present a brief sum-
mary of the annotation tool in Section 3.5.

3.1 Source Material

The source material is drawn from two source
corpora: the AMI corpus (McCowan et al.,
2005), which contains experimentally-controlled
scenario-driven design meetings, and the ICSI cor-
pus (Janin et al., 2003), which contains naturally
occurring workplace meetings. All the meetings
have at least four participants and have an average
duration of about 45 minutes. In the AMI corpus,

258



the participants are experimental subjects who are
assigned institutional roles, e.g. project manager
and industrial designer. This helps to establish
controlled social relationships within the group,
but generally limits the types of person referring.
The ICSI meetings are naturally occurring and ex-
hibit complex pre-existing social relationships be-
tween the participants. Person referring in this cor-
pus is quite complex and often includes other in-
dividuals from the larger institution and beyond.

3.2 Labeling Participant Names
The first phase of annotation consists of identify-
ing the names of the participants. We perform this
task for every participant in every meeting in the
AMI and ICSI source corpora, which totals 275
unique participants in 246 meetings. Despite the
fact that the participants’ are given anonymized
identifiers by the corpus creators, determining par-
ticipants’ names is possible because name men-
tions are not excised from the speech transcript.
This allows identification of the names of any par-
ticipants who are referred to by name in the dia-
logue, as long as the referent is disambiguated by
contextual clues such as addressing.

To extract name information, the list of capi-
talized words in the speech transcript is scanned
manually for likely person names. This was done
manually due to the difficulty of training a suffi-
ciently robust named-entity recognizer for these
corpora. Proceeding through each meeting for
which any participant names are yet unidentified,
and taking each potential name token in order
of frequency of occurrence in that meeting, short
segments of the recording surrounding the occur-
rences were replayed. In most cases, the name was
used in reference to a participant and it was clear
from discourse context which participant was the
intended referent. In the AMI meetings, 158 of
223 (71%) of the participants’ first names were
identified. In the ICSI meetings, 36 of 52 (69%)
were identified. While these numbers may seem
low, failure to determine a name was generally as-
sociated with a low level of participation of the
individual either in terms of amount of speech or
number of meetings attended. As such, the propor-
tion of utterances across both corpora for which
the speaker’s name is identified is actually 91%.

3.3 Person-reference Annotation
The second, principal phase of annotation con-
sists of annotating person-referring — instances

of verbal reference to people. The recognition
of person-referring requires the annotator to si-
multaneously identify whether a referring event
has occurred, and whether the referent is a per-
son. In practice, this is divided into four an-
notation steps: markable identification, referent
identification, functional category labeling, and
co-reference linking. For non-specific references,
there is an additional step of labeling addressing
properties. For each meeting, annotators label ev-
ery instance of person-referring in every utterance
in the meeting, performing the steps in sequence
for each utterance. Section 4 describes the set of
meetings annotated. The UML diagram in Fig-
ure 1 depicts the formal data structure produced
by the procedure.2

The first step is markable identification, which
involves recognizing person-referring expres-
sions in the transcript. Only expressions that are
noun phrases are considered, and only the head
noun is actually labeled by the annotator — the
extent of the expression is not labeled. These iden-
tified head nouns are called markables. Note,
however, that before human annotation begins, an
automatic process identifies occurrences of words
that are likely to be head nouns in person-referring
expressions. The list of words includes all per-
sonal pronouns except it, them, and they (these
are more likely to be non-person-referring in our
dataset) and the wh-pronouns (not labeled in our
scheme). It also includes any occurrences of
the previously identified proper names. Some of
the automatically identified words might not be
person-referring. Also, there may be instances of
person-referring that are not automatically iden-
tified. Annotators do not unmark any of the au-
tomatically identified words, even if they are not
person-referring. The resulting set of manually
and automatically identified words, which may or
may not be person-referring, constitute the com-
plete set of markables.

The second step is the labeling of person refer-
ents. Any people or groups of people that are re-
ferred to specifically and unambiguously (see Sec-
tion 3.4.3 for details) are added by the annotator
to a conversation referent list. The list is auto-
matically populated with each of the conversation
participants.

2The diagram may also be viewed informally as loosely
reflecting a decision tree for the main annotation steps. A
complete coding manual is available from the author’s web
site.

259



ATTR-QUANTIFIED-SUPERSET: Boolean
category: Enum: {
  FUNC-PREF-VOCATIVE,
  FUNC-PREF-INTRODUCTION, 
  FUNC-PREF-TROUBLE, 
  FUNC-PREF-DEFAULT }

Person-Referring Markable

1..*
1

Referent List

*

1
Transcript

* 1

category: Enum: {
  FUNC-FILLER, 
  FUNC-NON-PREF }

Non-Referring Markable

Person Referent

ATTR-SPEAKER-INCL: Boolean
ATTR-ADDRESSEE-INCL: Boolean
ATTR-OTHER-INCL: Boolean

Underspecified Referent (PERSON-OTHER)
id: String
NICKNAME: String

Specific Real Referent

PERSON-SINGLE PERSON-MULTIPLE
2..* 0..*
members

speaker: Participant
word: String
startTime: Double

Word

Conversation

Markable Word Non-Markable Word

Figure 1: A UML diagram depicting the data structure used to represent and store the annotations.

The third step consists of labeling markables
with a functional category (FUNC-*). The func-
tional categories serve two main purposes. They
are used to distinguish person-referring markables
from all others (corresponding to the two main
boxes in the diagram), and they are used to distin-
guish between specific dialogue purposes (the cat-
egories listed within the boxes, see Section 3.4.4).

The final step is to link the markables that were
labeled as person-referring to the appropriate ref-
erent in the referent list. This is only done for
specific and unambiguous referring. Otherwise,
the referent is said to be underspecified, and in-
stead of linking the markable to a referent, it is la-
beled with three binary addressing inclusion at-
tributes. Inclusion attributes label whether the
speaker, addressee, or any other individuals are in-
cluded in the set of people being referred to, given
the social, situational, and discourse context (de-
tails in Section 3.4.5).

3.4 Special Issues

3.4.1 Defining ‘person’ and ‘referring’

To be person-referring, an expression must sat-
isfy two conditions. First, the expression’s pri-
mary contribution to the speaker’s intended mean-

ing or purpose must be either to identify, label,
describe, specify, or address. These are the ba-
sic types of referring. Second, the referent being
identified, labeled, etc., must be a person, which
we define to include any of the following: a dis-
tinct person in the real world; a fictitious or hypo-
thetical person; a human agent, perceiver, or par-
ticipant in a described event, scene, or fact; a class,
type, or kind of person, or representative thereof;
a specification or description of a person or set of
people; a (possibly vaguely defined) group or col-
lection of any of the above; the human race as a
whole, or a representative thereof.

If a noun phrase is used to do person-referring
as defined, the associated markable is labeled with
one of the four person-referring functional cat-
egories (FUNC-PREF-*). If a markable is not
person-referring (either non-referring or referring
to a non-person referent), it is labeled with the
functional category FUNC-NON-PREF. The one
exception to this is the use of a pre-defined list of
common discourse fillers such as you know and I
mean. When used as fillers, these are labeled with
the non-referential FUNC-FILLER category.

260



3.4.2 Joint action and referring ‘trouble’
Annotators are asked to consider occasions of re-
ferring to be joint actions between the speaker and
the addressee(s) of the utterance. The annotator
assumes the role of an overhearer and considers
as referring any case where the speaker’s intended
purpose is to refer. If the instance of referring
is not successfully negotiated between the partic-
ipants (i.e., common ground is not achieved), but
the speaker’s intended purpose is to refer, then the
annotator marks this as FUNC-PREF-TROUBLE.
This is used to identify problematic cases for fu-
ture study.

3.4.3 Specific, Unambiguous Referring
Only the referents of specific, unambiguous re-
ferring to a person in the real world (PERSON-
SINGLE) are included in the conversation referent
list and made the subject of coreference annota-
tion. References to more than one such individual
can qualify (PERSON-MULTIPLE), but only if the
members are precisely enumerable and qualify in-
dividually. The motivation for this distinction is to
distinguish references that would be directly use-
ful to applications. Coreference for underspecified
references is not labeled.

3.4.4 Special Functional Categories
Two functional categories are used to distinguish
special uses of person-referring for subsequent
use in speaker name induction (the task of auto-
matically learning participants’ names). The two
categories are FUNC-PREF-INTRODUCTION and
FUNC-PREF-VOCATIVE, which specify personal
introductions such as “Hi, I’m John,” and vocative
addressing such as “What do you think, Jane?”
These categories are used only for proper names.

3.4.5 Addressing-based Inclusion Attributes
A major novelty in our annotation scheme is the
use of addressing-based distinctions for under-
specified referents. Rather than using the labels
‘generic’ or ‘indeterminate’, we employ three bi-
nary attributes (ATTR-*-INCL) that label whether
the speaker, addressee or any other real individuals
are members of the set of people referred to.

The use of this distinction is informed by the no-
tion that addressing distinctions are of central im-
portance to the recognition of joint activity type,
structure, and participation roles. A generic pro-
noun, for example, will often have all three cat-
egories labeled positively. But as an example

of where this scheme creates a novel distinction,
consider the phrase “You really take a beating
out there on the pitch!”, where the speaker is a
football player describing the nature of play to
someone who has never played the game. This
‘generic’ use of you, used in an activity of autobi-
ographical description, is intuitively interpreted as
not including the addressee (ATTR-ADDRESSEE-
INCL=FALSE) but including the speaker and others
(ATTR-{SPEAKER,OTHER}-INCL=TRUE). These
distinctions are hard to motivate linguistically yet
critical to identifying useful properties relating to
participation in the communicative activity.

3.4.6 Special or Difficult Cases
In some cases, an annotator can determine that a
reference is specific and unambiguous for the par-
ticipants but the annotator himself is unable to de-
termine the identity of the referent. This is gener-
ally due to a lack of contextual awareness such as
not having adequate video. In such cases, the an-
notator assigns a special REF-UNKNOWN referent.

Other difficult aspects of our annotation proce-
dure are covered in the annotation manual, includ-
ing handling of disfluencies, quantification, and
identifying lexical heads.

3.5 Annotation Tool

The annotations were collected using a software
tool we have designed for discrete event-based an-
notation of multi-modal corpora. The tool uses a
simple, low-latency text-based interface that dis-
plays multiple streams of discrete events in tempo-
ral order across the screen. In our case, the events
are time-synchronized words that are distributed
to different rows according to speaker. The inter-
face allows keyboard input only and is synchro-
nized with the MPlayer playback engine.

4 Results and Analysis

4.1 Statistical summary

The dataset consists of approximately 11,000 in-
dividually annotated referring expressions in 16
experimentally-controlled, scenario-driven design
meetings from the AMI corpus (McCowan et al.,
2005) and 3 natural workplace meetings from
the ICSI corpus (Janin et al., 2003). Figure 2
shows, for each grammatical type of referring ex-
pression, the frequency of occurrence of the five
principal markable types, which are defined to
consist of the two non-person-referring functional

261



OTHER

QUANT

3PP

3PS

2P

1PP

1PS

OTHER

QUANT

3PP

3PS

2P

1PP

1PS

FUNC−PREF / PERSON−SINGLE

FUNC−PREF / PERSON−MULTIPLE

FUNC−PREF / PERSON−OTHER

FUNC−NON−PREF

FUNC−FILLER

0 500 1000 2000 3000

Figure 2: Frequency of occurrence of referring
types for the whole corpus, by grammatical type
of the referring expression.

categories (FUNC-NON-PREF and FUNC-FILLER),
and a breakdown of person-referring according
to the type of person referent: a specific indi-
vidual (PERSON-SINGLE), multiple specific indi-
viduals (PERSON-MULTIPLE), or underspecified
(PERSON-OTHER). The grammatical types in-
clude a grouping of the personal pronouns by
grammatical person and number (1PS, 1PP, 2P,
3PS, 3PP), the quantified pronouns (QUANT), and
a group including all other expressions (OTHER).
Table 1 shows the relative frequency for the gram-
matical types and the most frequent expressions.

As is usually found in conversation, first-
person and second-person pronouns are the most
frequent, collectively comprising 82.0% of all
person-referring expressions. Of particular inter-
est, due to their high frequency and multiple possi-
ble referential meanings, are the 1PP and 2P cate-
gories (e.g., we and you), comprising respectively
24.6% and 23.7% of all person-referring expres-

Gram. Freq. Ent. Freq. words
(%) (bits)

1PS 33.7 .57 I, my, me
1PP 24.6 .67 we, our, us
2P 23.7 1.78 you, your, yours
3PS .9 .66 he, his, she
3PP 7.2 1.25 they, them, their
QUANT 1.0 1.14 everyone, everybody
OTHER 8.9 1.57 people, guys, user

Table 1: A statistical summary of all the mark-
ables in the dataset by grammatical type (gram.),
showing their frequency relative to all markables
(freq.), the entropy of the referring type given the
grammatical type (ent.), and a list of the most fre-
quent examples (freq. words).

sions. In Table 1, we show the information en-
tropy of the referring type, given the grammati-
cal category. This measures the uncertainty one
has about the type, given knowledge of only the
grammatical type of the expression. The analysis
reveals that second-person pronouns are a partic-
ularly challenging reference resolution problem,
with a broad and relatively even distribution across
referring types.

4.2 Reliability and Error Analysis

To show that our annotations are credible and suit-
able for empirical testing, we must establish that
the subjective distinctions defined in our scheme
may be applied by individuals other than the
scheme developers. To do this, we assess inter-
coder agreement between two independent anno-
tators on four meetings from the AMI corpus, us-
ing Cohen’s Kappa (Cohen, 1960). Each of the
decisions in the annotation procedure are assessed
separately: markable identification, labeling ref-
erentiality, labeling specificity of person refer-
ents, and labeling addressing inclusion attributes.
Because each decision depends on the previous,
we employ a hierarchical assessment procedure
that considers only instances where the annota-
tors have agreed on previous decisions. This kind
of multi-level assessment corresponds to that de-
scribed and used in Carletta et al., (1997).

Markables The first annotation decision of in-
terest is the identification of markables. Markables
are either automatically identified occurrences of
a pre-defined list of pronouns, or they are identi-

262



fied manually by the annotators. Agreement on
this task, assessed only for manually identified
words, was very good (κ=.94). Error analysis
shows that the main issue with this decision was
not determining lexical heads, but rather deter-
mining whether phrases such as “all age groups,”
“the older generation,” and “the business market”
should be considered as referring to people or not.

Person referentiality The next annotation deci-
sion is between person-referring and non-person-
referring markables. For assessment of this
choice, we measure agreement on a three-way
categorization of the agreed markables as either
FUNC-NON-PREF, FUNC-FILLER, or one of the
FUNC-PREF-* categories. Agreement on this task
was good (κ=.77). The only errors occurred on
first- and second-person pronouns and between the
FUNC-NON-PREF and FUNC-PREF-* categories.
Error analysis suggests confusion tends to occur
when pronouns are used with semantically light
verbs like go, get, and have, for example in phrases
such as “there we go” and “you’ve got the main
things on the front.” As in the latter example,
some of the difficult choices appear to involve de-
scriptions of states, which the speaker can choose
to express either from various participants’ points
of view, as above, or alternatively without ex-
plicit subjectivity, e.g., “the main things are on the
front.”

Specificity and cardinality The next choice we
assess is the decision between referring specif-
ically to a single person (PERSON-SINGLE), to
multiple people (PERSON-MULTIPLE), or as un-
derspecified (also referred to as PERSON-OTHER).
Agreement on this choice was very good (κ=.91),
though considering only the difficult 1PP and 2P
grammatical categories (e.g., we and you), agree-
ment was less strong (κ=.75). Note that due to the
hierarchical nature of the scheme, evaluation con-
sidered only cases where both annotators labeled
a word as person-referring. Errors on this decision
often involved ambiguities in addressing, where
one annotator believed a particular individual was
being addressed by you and the other thought the
whole group was being addressed. Another com-
mon disagreement was on cases such as “we want
it to be original,” where we was interpreted by one
annotator as referring to the present group of par-
ticipants, but by the other as (presumably) refer-
ring to the organization to which the participants

belong.

Addressing inclusion attributes For the three
inclusion attributes for underspecified referents
(ATTR-*-INCL), agreement is calculated three
times, once for each of the binary attributes.
Agreement was good, though slightly problematic
for addressee inclusion (speaker κ=.72; addressee
κ=.50; other κ=.66). Disagreements were mainly
for occurrences of you like the example of autobi-
ography in Section 3.4.5. For example, “it’s your
best friend” was used to explain why a dog is the
speaker’s favorite animal, and the annotators dis-
agreed on whether the addressee was included.

5 Conclusion

We have presented an annotation scheme and a
set of annotations that address participant refer-
ence — a conversational language problem that
has seen little previous annotation work. Our fo-
cus has been on eliciting novel distinctions that we
hypothesize will help us to distinguish, label, and
summarize conversational activities. We also ad-
dress the issues of vagueness, ambiguity, and con-
textual dependency in participant referring.

Based on analysis of inter-annotator agreement,
the major distinctions proposed by the scheme ap-
pear to be reliably codable. In addition, our sta-
tistical analysis shows that our dataset contains a
wide variety of participant references and should
be a useful resource for several reference resolu-
tion problems for conversation. Our novel method
for distinguishing specific reference to real indi-
viduals appears to be very reliably codable. Our
novel addressing-based distinctions for underspec-
ified reference are less reliable but adequate as a
resource for some dialogue structuring tasks.

Further work proposed for this task includes
labeling a variety of conversational and non-
conversation genres. Our immediate concern is to
apply our annotations in the training and/or test-
ing of machine learning approaches to discourse
segmentation and abstractive summarization.

References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-

gram, Version 1. Linguistic Data Consortium. Cat-
alog ID: LDC2006T13.

Lou Burnard, 2007. Reference Guide for the British
National Corpus (XML Edition). Research Tech-
nologies Service at Oxford University Computing
Services.

263



Sasha Calhoun, Malvina Nissim, Mark Steedman, and
Jason Brenier. 2005. A framework for annotating
information structure in discourse. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky.

Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23(1):13–31.

Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.

Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37–46.

George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proc. LREC.

Matthew Frampton, Raquel Fernndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ”you”? Combining linguis-
tic and gaze features to resolve second-person ref-
erences in dialogue. In Proc. EACL.

John J. Godfrey, Edward Holliman, and J. McDaniel.
1992. SWITCHBOARD: Telephone speech corpus
for research and development. In Proc. ICASSP,
pages 517–520, San Francisco, CA.

Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving “you” in multi-
party dialog. In Proc. SIGdial, pages 227–230.

Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential “you” in dialog. In Proc. ACL.

A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. ICASSP, volume 1, pages 364–367.

Linguistic Data Consortium, 2008. ACE (Au-
tomatic Content Extraction) English Annotation
Guidelines for Entities, Version 6.5. Down-
loaded from http://projects.ldc.upenn.
edu/ace/annotation/.

I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Cor-
pus. In Proceedings of Measuring Behavior 2005,
the 5th International Conference on Methods and
Techniques in Behavioral Research, Wageningen,
Netherlands.

Peter Mühlhäusler and Rom Harré. 1990. Pronouns
and People: The Linguistic Construction of Social
and Personal Identity. Blackwell, Oxford.

Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proc. LREC.

R. Passonneau, 1997. Instructions for applying dis-
course reference annotation for multiple applica-
tions (DRAMA).

Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proc. LREC.

Massimo Poesio, 2000. The GNOME Annotation
Scheme Manual, Version 4. University of Edin-
burgh, HCRC and Informatics.

Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the GNOME corpus. In Pro-
ceedings of the ACL 2004 Workshop on Discourse
Annotation, pages 72–79.

Matthew Purver, Raquel Fernndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In Proc. SIGdial, pages 306–309.

Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda1. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proc. LREC.

Katie Wales. 1996. Personal pronouns in present-day
English. Cambridge University Press, Cambridge.

264


