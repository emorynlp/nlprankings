



















































Environment-Driven Lexicon Induction for High-Level Instructions


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 992‚Äì1002,

Beijing, China, July 26-31, 2015. c¬©2015 Association for Computational Linguistics

Environment-Driven Lexicon Induction for High-Level Instructions

Dipendra K. Misra‚àó Kejia Tao‚àó Percy Liang‚àó‚àó Ashutosh Saxena‚àó
dkm@cs.cornell.edu kt454@cornell.edu pliang@cs.stanford.edu asaxena@cs.cornell.edu

‚àó Department of Computer Science, Cornell University
‚àó‚àóDepartment of Computer Science, Stanford University

Abstract

We focus on the task of interpreting com-
plex natural language instructions to a
robot, in which we must ground high-level
commands such as microwave the cup to
low-level actions such as grasping. Pre-
vious approaches that learn a lexicon dur-
ing training have inadequate coverage at
test time, and pure search strategies can-
not handle the exponential search space.
We propose a new hybrid approach that
leverages the environment to induce new
lexical entries at test time, even for new
verbs. Our semantic parsing model jointly
reasons about the text, logical forms, and
environment over multi-stage instruction
sequences. We introduce a new dataset
and show that our approach is able to suc-
cessfully ground new verbs such as dis-
tribute, mix, arrange to complex logical
forms, each containing up to four predi-
cates.

1 Introduction
The task of mapping natural language instructions
to actions for a robot has been gaining momen-
tum in recent years (Artzi and Zettlemoyer, 2013;
Tellex et al., 2011; Misra et al., 2014; Bollini
et al., 2011; Guadarrama et al., 2013; Matuszek
et al., 2012b; Fasola and Mataric, 2013). We
are particularly interested in instructions contain-
ing verbs such as ‚Äúmicrowave‚Äù denoting high-level
concepts, which correspond to more than 10 low-
level symbolic actions such as grasp. In this
setting, it is common to find new verbs requiring
new concepts at test time. For example, in Fig-
ure 1, suppose that we have never seen the verb
‚Äúfill‚Äù. Can we impute the correct interpretation,
and moreover seize the opportunity to learn what
‚Äúfill‚Äù means in a way that generalizes to future in-
structions?

Text: ‚Äúget the cup, fill it with water and then microwave the cup‚Äù

grasping cup3 ‚àß

near(robot1,cup3)

in cup3,microwave ‚àß

state(microwave1,is-on)

state cup3,water ‚àß

on(cup3,sink)

Unseen verb ‚Äú fill ‚Äù is grounded
at test time using environment.

Figure 1: A lexicon learned on the training data
cannot possibly cover all the verb-concept map-
pings needed at test time. Our algorithm learns
the meaning of new verbs (e.g., ‚Äúfill‚Äù) using the
environment context.

Previous work in semantic parsing handles lex-
ical coverage in one of two ways. Kwiatkowski et
al. (2010) induces a highly constrained CCG lex-
icon capable of mapping words to complex log-
ical forms, but it would have to skip new words
(which in Figure 1 would lead to microwaving an
empty cup). Others (Berant and Liang, 2014) take
a freer approach by performing a search over log-
ical forms, which can handle new words, but the
logical forms there are much simpler than the ones
we consider.

In this paper, we present an hybrid approach
that uses a lexicon to represent complex concepts
but also strongly leverages the environment to
guide the search space. The environment can pro-
vide helpful cues in several ways:
‚Ä¢ Only a few environments are likely for a given

scenario‚Äîe.g., the text is unlikely to ask the
robot to microwave an empty cup or put books
on the floor.
‚Ä¢ The logical form of one segment of text con-

strains that of the next segment‚Äîe.g., the text is
unlikely to ask the robot to pick a cup and then
put it back immediately in the same spot.
We show that this environment context provides

992



ùëê1 ùëê2

z1e1 e2

planner

ùëé1

simulator

Shallow Parsing (Section 3.2)

x

z2

planner

ùëé2

ùëêùëò‚ãØ‚ãØ

‚ãØ

Action Sequence: ùëöùëúùë£ùëíùë°ùëú ùë•ùëèùëúùë•1 ; ùëîùëüùëéùë†ùëù ùë•ùëèùëúùë•1 ; ùëùùëüùëíùë†ùë† ùëùùëúùë§ùëíùëü_ùëèùë¢ùë°ùë°ùëúùëõ1 ; ùëöùëúùë£ùëíùë°ùëú ùëêùëë2 ; ùëîùëüùëéùë†ùëù ùëêùëë2 ; ùëñùëõùë†ùëíùëüùë°(ùëêùëë2, ùë•ùëèùëúùë•1) ‚ãØ

Environment: Logical Form z = (‚Ñì, ùúâ)

Frame Node

‚Ñì: ùëùùë¢ùë° ‚áí [ùúÜ  ùë£.state v1,has-cd
‚àß near v1,v2 , ùúâ‚Ä≤]

ùúâ: {ùë£1 ‚Üí ùë•ùëèùëúùë•1; ùë£2 ‚Üí ùëüùëúùëèùëúùë°1}
ùúâ‚Ä≤: old mapping 

ùë•ùëèùëúùë•1
ùë†ùëõùëéùëêùëòùë°ùëéùëèùëôùëí2

On

Power-off

ùúô ùúô ùúô

ek zk

planner

ùëék

Semantic Parsing Model
(Section 5)

zk‚àí1

planner

ùëék‚àí1

simulator

Text: ‚ÄúTurn on xbox. Take Far Cry Game CD and put in xbox.
Throw out beer, coke and sketchy stuff in bowl.‚ãØ ‚Äù

ùúà: throw, 
ùúî: [ beer, coke, sketchy stuff, bowl ]
r: { in: sketchy stuff ‚Üí bowl }

Figure 2: Graphical model overview: we first deterministically shallow parse the text x into a control
flow graph consisting of shallow structures {ci}. Given an initial environment e1, our semantic parsing
model maps these frame nodes to logical forms {zi} representing the postconditions. From this, a planner
and simulator generate the action sequences {ai} and resulting environments {ei}.

a signal for inducing new lexical entries that map
previously unseen verbs to novel concepts. In the
example in Figure 1, the algorithm learns that mi-
crowaving an empty cup is unlikely and this sug-
gests that the verb ‚Äúfill‚Äù must map to actions that
end up making the cup not empty.

Another contribution of this paper is using post-
conditions as logical forms rather than actions, as
in previous work (Artzi and Zettlemoyer, 2013;
Misra et al., 2014). Postconditions not only re-
duce the search space of logical forms, but are also
a more natural representation of verbs. We define
a conditional random field (CRF) model over post-
conditions, and use a planner to convert postcon-
ditions into action sequences and a simulator to
generate new environments.

At test time, we use the lexicon induced from
the training data, but also perform an environment-
guided search over logical forms to induce new
lexical entries on-the-fly. If the predicted action
sequence uses a new lexical entry generated by the
search, it is added to the lexicon, where it can be
reused in subsequent test examples.

We evaluate our algorithm on a new corpus con-
taining text commands for a household robot. The
two key findings of our experiments are: First, the
environment and task context contain enough in-
formation to allow us to learn lexical entries for
new verbs such as ‚Äúdistribute‚Äù and ‚Äúmix‚Äù with
complex semantics. Second, using both lexical
entries generated by a test-time search and those
from the lexicon induced by the training data out-
performs the two individual approaches. This sug-
gests that environment context can help allevi-

ate the problem of having a limited lexicon for
grounded language acquisition.

2 Problem Statement
At training time, we are given a set of examples
D = {(x(m), e(m), a(m), œÄ(m))}Mm=1, where x(m)
is a text containing natural language instructions,
e(m) is an initial environment, a(m) is a human-
annotated sequence of actions, and œÄ(m) specifies
a monotonic alignment between segments of x(m)

and segments of a(m). For example, given words
x(m) = x1x2 and a(m) = a1a2a3, œÄ(m) might
specify that x1 aligns to a1a2 and x2 aligns to a3.

At test time, given a sequence of text-
environment pairs as input {(x(n), e(n))}Nn=1, we
wish to generate a sequence of actions a(n) for
each input pair. Note that our system is allowed to
use information about one test example to improve
performance on subsequent ones. We evaluate a
system on its ability to recover a human-annotated
sequence of actions.

3 Approach Overview
Figure 2 shows our approach for mapping text x
to actions a1:k given the initial environment e1.

3.1 Representation

We use the following representation for the differ-
ent variables in Figure 2.
Environment. An environment ei is represented
by a graph whose nodes are objects and edges
represent spatial relations between these objects.
We consider five basic spatial relations: near,
grasping, on, in and below. Each object has an

993



instance ID (e.g., book9), a category name (e.g.,
chair, xbox), a set of properties such as graspable,
pourable used for planning and a set of boolean
states such as has-water, at-channel3, whose
values can be changed by robot actions. The robot
is also an object in the environment. For example,
the objects xbox1, snacktable2, are two objects
in e1 in Figure 2 with relation on between them.
Postconditions. A postcondition is a conjunction
of atoms or their negations. Each atom consists of
either a spatial relation between two objects (e.g.,
on(book9, shelf3)) or a state and a value (e.g.,
state(cup4, has-water)). Given an environment
e, the postcondition evaluates to true or false.
Actions. Each action in an action sequence ai
consists of an action name with a list of argu-
ments (e.g., grasp(xbox1)). The action name
is one of 15 values (grasp, moveto, wait, etc.),
and each argument is either an object in the envi-
ronment (e.g., xbox1), a spatial relation (e.g., in
for keep(ramen2, in, kettle1), or a postcondi-
tion (e.g., for wait(state(kettle1, boiling))).
Logical Forms. The logical form zi is a pair
(`, Œæ) containing a lexical entry ` and a map-
ping Œæ. The lexical entry ` contains a parameter-
ized postcondition such as Œª~v.grasping(v1, v2)
‚àß¬¨near(v3, v2), and Œæ maps the variables~v to ob-
jects in the environment. Applying the parame-
terized postcondition on Œæ yields a postcondition;
note that a postcondition can be represented by
different logical forms. A lexical entry contains
other information which are used for defining fea-
tures, which is detailed in Section 4.
Control Flow Graphs. Following previous work
(Tellex et al., 2011; Misra et al., 2014), we convert
the text x to a shallow representation. The par-
ticular representation we choose is a control flow
graph, which encodes the sequential relation be-
tween atomic segments in the text. Figure 3 shows
the control flow graph for an example text. In a
control flow graph, each node is either a frame
node or a conditional node. A frame node rep-
resents a single clause (e.g., ‚Äúchange the chan-
nel to a movie‚Äù) and has at most one successor
node. Specifically, a frame node consists of a verb
ŒΩ (e.g., arrange, collect), a set of object descrip-
tions {œâi} which are the arguments of the verb
(e.g., the guinness book, movie channel), and spa-
tial relations r between the arguments (e.g., be-
tween, near). The object description œâ is either an
anaphoric reference (such as ‚Äúit‚Äù) or a tuple con-
taining the main noun, associated modifiers, and
relative clauses.

Text: ‚ÄúIf any of the pots have food in them, then dump them out in the
garbage can and then put them on the sink else keep it on the table.‚Äù

‚àÉe category e,cup ‚àßstate(e,food)

ùúà: dump
ùùé: [them, the garbage can]
ùëü: { in: them ‚Üí garbage can }

ùúà: put
ùùé: [them, the sink]
ùëü: {ùëún: them ‚Üí the sink }

ùúà: keep
ùùé: [it, the table]
ùëü: {ùëún: it ‚Üí the table }

Conditional node (ùëíùë•ùëùùëü)

Frame Node ùúà,ùùé, ùëü
ùúà:  verb
ùùé: set of object description ùúî
ùúî: (main noun or pronoun, modifiers)
ùëü: relationship between descriptions

Figure 3: We deterministically parse text into a
shallow structure called a control flow graph.

A conditional node contains a logical postcon-
dition with at most one existentially quantified
variable (in contrast to a frame node, which con-
tains natural language). For example, in Figure 3
the conditional node contains the expression cor-
responding to the text ‚Äúif any of the pots has food‚Äù
There are two types of conditional nodes: branch-
ing and temporal. A branching conditional node
represents an ‚Äúif ‚Äù statement and has two succes-
sor nodes corresponding to whether the condition
evaluates to true or false in the current environ-
ment. A temporal conditional node represents an
‚Äúuntil‚Äù statement and waits until the condition is
false in the environment.

3.2 Formal Overview

Shallow Parsing. We deterministically convert
the text x into its control flow graph G using a set
of manual rules applied on its constituency parse
tree from the Stanford parser (Klein and Manning,
2003). Conditionals in our dataset are simple and
can be converted into postconditions directly using
a few rules, unlike the action verbs (e.g., ‚Äúfill‚Äù),
which is the focus of this paper. The details of
our shallow parsing procedure is described in the
appendix.

Given an environment e1, G is reduced to a sin-
gle sequence of frame nodes c1, . . . , ck, by evalu-
ating all the branch conditionals on e1.
Semantic Parsing Model. For each frame node ci
and given the current environment ei, the seman-
tic parsing model (Section 5) places a distribution
over logical forms zi. This logical form zi rep-
resents a postcondition on the environment after
executing the instructions in ci.
Planner and Simulator. Since our semantic rep-
resentations involve postconditions but our model
is based on the environment, we need to connect
the two. We use planner and a simulator that to-

994



gether specify a deterministic mapping from the
current environment ei and a logical form zi to
a new environment ei+1. Specifically, the plan-
ner takes the current environment ei and a logical
form zi and computes the action sequence ai =
planner(ei, zi) for achieving the post condition
represented by zi.1 The simulator takes the current
environment ei and an action sequence ai and re-
turns a new environment ei+1 = simulator(ei, ai).

4 Anchored Verb Lexicons
Like many semantic parsers, we use a lexicon to
map words to logical forms. Since the environ-
ment plays an central role in our approach, we pro-
pose an anchored verb lexicon, in which we store
additional information about the environment in
which lexical entries were previously used. We
focus only on verbs since they have the most com-
plex semantics; object references such as ‚Äúcup‚Äù
can be mapped easily, as described in Section 5.

More formally, an anchored verb lexicon Œõ con-
tains lexical entries ` of the following form: [ŒΩ ‚áí
(Œª~v.S, Œæ)] where, ŒΩ is a verb, S is a postcondition
with free variables ~v, and Œæ is a mapping of these
variables to objects. An example lexical entry is:
[ pour‚áí (Œªv1v2v3.S, Œæ)], where:
S = grasping(v1, v2)‚àß near(v1, v3)‚àß¬¨state(v2, milk)

‚àß state(v3, milk)
Œæ = {v1 ‚Üí robot1, v2 ‚Üí cup1, v3 ‚Üí bowl3} (anchoring)

As Table 1 shows, a single verb will in general
have multiple entries due to a combination of pol-
ysemy and the fact that language is higher-level
than postconditions.
Advantages of Postconditions. In contrast to pre-
vious work (Artzi and Zettlemoyer, 2013; Misra et
al., 2014), we use postconditions instead of action
sequence for two main reasons. First, postcondi-
tions generalize better. To illustrate this, consider
the action sequence for the simple task of filling a
cup with water. At the time of learning the lexi-
con, the action sequence might correspond to us-
ing a tap for filling the cup while at test time, the
environment may not have a tap but instead have
a pot with water. Thus, if the lexicon maps to ac-
tion sequence, then it will not be applicable at test
time whereas the postcondition state(z1, water)
is valid in both cases. We thus shift the load of in-
ferring environment-specific actions onto planners

1We use the symbolic planner of Rintanen (2012) which
can perform complex planning. For example, to pick up a
bottle that is blocked by a stack of books, the planner will
first remove the books before grasping the bottle. In contrast,
Artzi and Zettlemoyer (2013) use a simple search over im-
plicit actions.

Table 1: Some lexical entries for the verb ‚Äúturn‚Äù
Sentence Context Lexical entry [turn‚áí (Œª~v.S, Œæ)]
‚Äúturn on the TV‚Äù state(v1, is-on) ‚àß near(v2, v1)

Œæ : v1 ‚Üí tv1, v2 ‚Üí robot1
‚Äúturn on the right state(v1, fire3) ‚àß near(v2, v1)
back burner‚Äù Œæ : v1 ‚Üí stove1, v2 ‚Üí robot1

‚Äúturn off the water‚Äù ¬¨state(v1, tap-on)
Œæ : v1 ‚Üí sink1

‚Äúturn the television state(v1, channel6) ‚àß near(v1, v2)
input to xbox‚Äù Œæ : v1 ‚Üí tv1, v2 ‚Üí xbox1

and use postconditions for representation, which
better captures the semantics of verbs.

Second, because postconditions are higher-
level, the number of atoms needed to repre-
sent a verb is much less than the correspond-
ing number of actions. For example, the
text ‚Äúmicrowave a cup‚Äù, maps to action se-
quence with 10‚Äì15 actions, the postcondition
only has two atoms: in(cup2, microwave1) ‚àß
state(microwave, is-on). This makes search-
ing for new logical forms more tractable.
Advantages of Anchoring. Similar to the VEIL
templates of Misra et al. (2014), the free variables
~v are associated with a mapping Œæ to concrete ob-
jects. This is useful for resolving ellipsis. Suppose
the following lexical entry was created at train-
ing time based on the text ‚Äúthrow the drinks in the
trash bag‚Äù:
[`: throw‚áí Œªxyz.S(x, y, z)], where
S = in(x, y) ‚àß ¬¨grasping(z, x) ‚àß ¬¨state(z, closed)
Œæ = {x‚Üí coke1, y‚Üí garbageBin1, z‚Üí robot1}

Now consider a new text at test time ‚Äúthrow
away the chips‚Äù, which does not explicitly men-
tion where to throw the chips. Our semantic pars-
ing algorithm (Section 5) will use the previous
mapping y ‚Üí garbabeBin1 to choose an object
most similar to a garbage bin.

5 Semantic Parsing Model
Given a sequence of frame nodes c1:k and an ini-
tial environment e1, our semantic parsing model
defines a joint distribution over logical forms z1:k.
Specifically, we define a conditional random field
(CRF) over z1:k, as shown in Figure 2:

pŒ∏(z1:k | c1:k, e1)‚àùexp
(

k‚àë
i=1

œÜ(ci, zi‚àí1, zi, ei) ¬∑ Œ∏
)
, (1)

where œÜ(ci, zi‚àí1, zi, ei) is the feature vector and
Œ∏ is the weight vector. Note that the environ-
ments e1:k are a deterministic function of the log-
ical forms z1:k through the recurrence ei+1 =
simulator(ei, planner(ei, zi)), which couples the
different time steps.

995



Features. The feature vector œÜ(ci, zi‚àí1, zi, ei)
contains 16 features which capture the dependen-
cies between text, logical forms, and environment.
Recall that zi = ([ŒΩ ‚áí (Œª~v.S, Œæ)], Œæi), where Œæ
is the environment in which the lexical entry was
created and Œæi is the current environment. Let
fi = (Œª~v.S)(Œæi) be the current postcondition.
Here we briefly describe the important features
(see the supplemental material for the full list):
‚Ä¢ Language and logical form: The logical form
zi should generally reference objects mentioned
in the text. Assume we have computed a cor-
relation œÅ(œâ, o) between each object description
œâ and object o, whose construction is described
later. We then define two features: precision cor-
relation, which encourages zi to only use objects
referred to in ci; and recall correlation, which
encourages zi to use all the objects referred to in
ci.
‚Ä¢ Logical form: The postcondition fi should be

based on previously seen environments. For ex-
ample, microwaving an empty cup and grasp-
ing a couch are unlikely postconditions. We
define features corresponding to the average
probability (based on the training data) of all
conjunctions of at most two atoms in the
postcondition (e.g., grasping(robot, cup)}).
We do the same with their abstract versions
({grasping(v1, v2)}). In addition, we build the
same set of four probability tables conditioned
on verbs in the training data. For example, the
abstract postcondition state(v1, water) has a
higher probability conditioned on the verb ‚Äúfill‚Äù.
This gives us a total of 8 features of this type.
‚Ä¢ Logical form and environment: Recall that an-

choring helps us in dealing with ellipsis and
noise. We add a feature based on the average
correlation between the objects of the new map-
ping Œæi with the corresponding objects in the an-
chored mapping Œæ.
The other features are based on the relationship

between object descriptions, similarity between Œæ
and Œæi and transition probabilities between logi-
cal forms zi‚àí1 and zi. These probabilities are also
learned from training data.
Mapping Object Descriptions. Our features rely
on a mapping from object descriptions œâ (e.g.,
‚Äúthe red shiny cup‚Äù) to objects o (e.g., cup8),
which has been addressed in many recent works
(Matuszek et al., 2012a; Guadarrama et al., 2014;
Fasola and Matari‚Äôc, 2014).

One key idea is: instead of computing rigid lex-
ical entries such as cup‚Üí cup1, we use a contin-

uous correlation score œÅ(œâ, o) ‚àà [0, 1] that mea-
sures how well œâ describes o. This flexibility al-
lows the algorithm to use objects not explicitly
mentioned in text. Given ‚Äúget me a tank of wa-
ter‚Äù, we might choose an approximate vessel (e.g.,
cup2).

Given an object description œâ, an object o, and a
set of previously seen objects (used for anaphoric
resolution), we define the correlation œÅ(œâ, o) using
the following approach:
‚Ä¢ If œâ is a pronoun, œÅ(œâ, o) is the ratio of the posi-

tion of the last reference of o to the length of the
action sequence computed so far, thus preferring
recent objects.
‚Ä¢ Otherwise, we compute the correlation using

various sources: the object‚Äôs category; the
object‚Äôs state for handling metonymy (e.g.,
the description ‚Äúcoffee‚Äù correlates well with
the object mug1 if mug1 contains coffee‚Äî
state(mug1, has-coffee) is true), WordNet
(Fellbaum, 1998) for dealing synonymy and hy-
ponymy; and word alignments between the ob-
jects and text from Giza++ (Och and Ney, 2003)
to learn domain-specific references (e.g., ‚ÄúGuin-
ness book‚Äù refers to book1, not book2). More
details can be found in the supplemental mate-
rial.

6 Lexicon Induction from Training Data
In order to map text to logical forms, we first in-
duce an initial anchored lexicon Œõ from the train-
ing data {(x(m), e(m), a(m), œÄ(m))}Mm=1. At test
time, we add new lexical entries (Section 7) to Œõ.

Recall that shallow parsing x(m) yields a list of
frame nodes c1:k. For each frame node ci and its
aligned action sequence ai, we take the conjunc-
tion of all the atoms (and their negations) which
are false in the current one ei but true in the
next environment ei+1. We parametrize this con-
junction by replacing each object with a variable,
yielding a postcondition S parametrized by free
variables ~v and the mapping Œæ from ~v to objects
in ei. We then add the lexical entry [verb(ci) ‚áí
(Œª~v.S, Œæ)] to Œõ.
Instantiating Lexical Entries. At test time, for
a given clause ci and environment ei, we generate
set of logical forms zi = (`i, Œæi). To do this, we
consider the lexical entries in Œõ with the same verb
as ci. For each such lexical entry `i, we can map its
free variables ~v to objects in ei in an exponential
number of ways. Therefore, for each `i we only
consider the logical form (`i, Œæi) where the map-
ping Œæi obtains the highest score under the current

996



model: Œæi = arg maxŒæ‚Ä≤ œÜ(ci, zi‚àí1, (`i, Œæ‚Ä≤), ei) ¬∑
Œ∏. For the feature vector œÜ that we consider,
this approximately translates to solving an integer
quadratic program with variables [yij ] ‚àà {0, 1},
where yij = 1 only if vi maps to object j.

7 Environment-Driven Lexicon
Induction at Test Time

Unfortunately, we cannot expect the initial lexicon
Œõ induced from the training set to have full cover-
age of the required postconditions. Even after us-
ing 90% of the data for training, we encountered
17% new postconditions on the remaining 10%.
We therefore propose generating new lexical en-
tries at test time and adding them to Œõ.

Formally, for a given environment ei and frame
node ci, we want to generate likely logical forms.
Although the space of all possible logical forms
is very large, the environment constrains the pos-
sible interpretations. We first compute the set
of atoms that are false in ei and that only con-
tain objects o that are ‚Äúreferred‚Äù to by either
ci or ci‚àí1, where ‚Äúrefers‚Äù means that there ex-
ists some argument œâ in ci for which o ‚àà
arg maxo‚Ä≤ œÅ(œâ, o‚Ä≤). For example, if ci corresponds
to the text ‚Äúdistribute pillows among the couches‚Äù,
we consider the atom on(pillow1, armchair1)
but not on(pillow1, snacktable2) since the ob-
ject armchair1 has the highest correlation to the
description ‚Äúcouches‚Äù.

Next, for each atom, we convert it into a logi-
cal form z = (`, Œæ) by replacing each object with
a variable. While this generalization gives us a
mapping Œæ, we create a lexical entry `i = [ŒΩ ‚áí
(Œª~v.S, ‚àÖ)] without it, where S is the parameter-
ized atom. Note that the anchored mapping is
empty, representing the fact that this lexical en-
try was unseen during training time. For example,
the atom state(tv1, mute) would be converted to
the logical form (`, Œæ), where ` = [verb(ci) ‚áí
(Œªv.state(v, mute), ‚àÖ] and Œæ = {v ‚Üí tv1}. We
do not generalize state names (e.g., mute) because
they generally are part of the meaning of the verb.

The score œÜ(ci, zi‚àí1, zi, ei) ¬∑ Œ∏ is computed
for the logical form zi produced by each post-
condition. We then take the conjunction of ev-
ery pair of postconditions corresponding to the
200 highest-scoring logical forms. This gives us
new set of postconditions on which we repeat
the generalization-scoring-conjunction cycle. We
keep doing this while the scores of the new logi-
cal forms is increasing or while there are logical
forms remaining.

Train Time Anchored Lexicon ùö≤ (Sec 6)

‚Ñì = [ùë£ùëíùëüùëè ‚áí (ùúÜ  ùë£. ùëÜ, ùúâ)]
such that ùë£ùëíùëüùëè = ùë£(ùëêùëñ)

Test Time Search for Logical Forms (Sec 7)

Set of Logical Forms for ùíÑùíä‚àíùüèùíÑùíä, ùíÜùíä, ùíõùíä‚àíùüè

ùëßùëñ = (‚Ñì, ùúâùëñ)
ùúâùëñ is the new assignment

ùëßùëñ = ‚Ñì, ùúâùëñ
where ‚Ñì = [ùë£ùëíùëüùëè ‚áí (ùúÜ  ùë£. ùëÜ , ‚àÖ)]
is a test time lexical entry

Figure 4: Logical forms for a given clause ci, en-
vironment ei, and previous logical form zi‚àí1 are
generated from both a lexicon induced from train-
ing data and a test-time search procedure based on
the environment.

If a logical form z = ([ŒΩ ‚áí (Œª~v.S, ‚àÖ)], Œæ) is
used by the predicted action sequence, we add the
lexical entry [ŒΩ ‚áí (Œª~v.S, Œæ)] to the lexicon Œõ.
This is different to other lexicon induction proce-
dures such as GENLEX (Zettlemoyer and Collins,
2007) which are done at training time only and
require more supervision. Moreover, GENLEX
does not use the environment context in creating
new lexical entries and thus is not appropriate at
test time, since it would vastly overgenerate lexi-
cal entries compared to our approach. For us, the
environment thus provides implicit supervision for
lexicon induction.

8 Inference and Parameter Estimation
Inference. Given a text x (which is converted to
c1:k via Section 3.2) and an initial environment
e1, we wish to predict an action sequence a based
on pŒ∏(a1:k | c1:k, e1), which marginalizes over all
logical forms z1:k (see Figure 2).

To enumerate possible logical forms, semantic
parsers typically lean heavily on a lexicon (Artzi
and Zettlemoyer, 2013), leading to high preci-
sion but lower recall, or search more aggressively
(Berant et al., 2013), leading to higher recall but
lower precision. We adopt the following hybrid
approach: Given ei, ci‚àí1, ci and zi‚àí1, we use both
the lexical entries in Œõ as explained in Section 6
and the search procedure in Section 7 to generate
the set of possible logical forms for zi (see Fig-
ure 4). We use beam search, keeping only the
highest-scoring logical form with satisfiable post-
conditions for each i ‚àà {1, . . . , k} and resulting
action sequence a1:i.
Parameter Estimation. We split 10% of our
training data into a separate tuning set (the 90%
was used to infer the lexicon). On each example
in this set, we extracted the full sequence of logi-
cal forms z1:k from the action sequence a1:k based
on Section 6. For efficiency, we used an objective

997



similar to pseudolikelihood to estimate the param-
eters Œ∏. Specifically, we maximize the average log-
likelihood over each adjacent pair of logical forms
under pÃÉŒ∏:

pÃÉŒ∏(zi | zi‚àí1, ci, ei) ‚àù exp(œÜ(ci, zi‚àí1, zi, ei)>Œ∏). (2)

The weights were initialized to 0. We per-
formed 300 iterations over the validation set with
a learning rate of 0.005N .

9 Dataset and Experiments
9.1 Dataset

We collected a dataset of 500 examples from 62
people using a crowdsourcing system similar to
Misra et al. (2014). We consider two different 3D
scenarios: a kitchen and a living room, each con-
taining an average of 40 objects. Both of these
scenarios have 10 environments consisting of dif-
ferent sets of objects in different configurations.
We define 10 high-level objectives, 5 per scenario,
such as clean the room, make coffee, prepare room
for movie night, etc.

One group of users wrote natural language com-
mands to achieve the high-level objectives. An-
other group controlled a virtual robot to accom-
plish the commands given by the first group. The
dataset contains considerable variety, consisting of
148 different verbs, an average of 48.7 words per
text, and an average of 21.5 actions per action se-
quence. Users make spelling and grammar errors
in addition to occasionally taking random actions
not relevant to the text. The supplementary mate-
rial contains more details.

We filtered out 31 examples containing fewer
than two action sequences. Of the remaining ex-
amples, 378 were used for training and 91 were
used for test. Our algorithm is tested on four new
environments (two from each scenario).

9.2 Experiments and Results

Evaluation Metrics. We consider two metrics,
IED and END, which measure accuracy based on
the action sequence and environment, respectively.
Specifically, the IED metric (Misra et al., 2014) is
the edit distance between predicted and true action
sequence. The END metric is the Jaccard index of
sets A and B, where A is the set of atoms (e.g.,
on(cup1,table1)) whose truth value changed
due to simulating the predicted action sequence,
and B is that of the true action sequence.
Baselines. We compare our algorithm with the
following baselines:

Table 3: Results on the metrics and baselines de-
scribed in section 9.2. The numbers are normal-
ized to 100 with larger values being better.
Algorithm IED END
Chance 0.3 0.5
Manually Defined Templates 2.5 1.8
UBL- Best Parse (Kwiatkowski et al., 2010) 5.3 6.9
VEIL (Misra et al., 2014) 14.8 20.7
Model with only train-time lexicon induction 20.8 26.8
Model with only test-time lexicon induction 21.9 25.9
Full Model 22.3 28.8

1. Chance: Randomly selects a logical form for
every frame node from the set of logical forms
generated by generalizing all possible postcon-
ditions that do not hold in the current environ-
ment. These postconditions could contain up to
93 atoms.
2. Manually Defined Templates: Defines a set
of postcondition templates for verbs similar to
Guadarrama (2013).
3. UBL-Best Parse (Kwiatkowski et al., 2010):
UBL algorithm trained on text aligned with post-
conditions and a noun-phrase seed lexicon. The
planner uses the highest scoring postcondition
given by UBL to infer the action sequence.
4. VEIL (Misra et al., 2014): Uses action se-
quences as logical forms and does not generate
lexical entries at test time.
We also consider two variations of our model: (i)
using only lexical entries induced using the train-
ing data, and (ii) using only the logical forms in-
duced at test-time by the search procedure.

The results are presented in Table 3. We ob-
serve that our full model outperforms the baseline
and the two pure search- and lexicon-based varia-
tions of our model. We further observe that adding
the search procedure (Section 7) improved the ac-
curacy by 1.5% on IED and 2% on END. The log-
ical forms generated by the search were able to
successfully map 48% of the new verbs.

Table 2 shows new verbs and concepts that the
algorithm was able to induce at test time. The
algorithm was able to correctly learn the lexi-
cal entries for the verbs ‚Äúdistribute‚Äù and ‚Äúmix‚Äù,
while the ones for verbs ‚Äúchange‚Äù and ‚Äúboil‚Äù were
only partly correct. The postconditions in Table 2
are not structurally isomorphic to previously-seen
logical forms; hence they could not have been
handled by using synonyms or factored lexicons
(Kwiatkowski et al., 2011). The poor performance
of UBL was because the best logical form often
produced an unsatisfiable postcondition. This can
be remedied by joint modeling with the environ-

998



Table 2: New verbs and concepts induced at test time (Section 7).
Text Postcondition represented by the learned logical form # Log. forms explored
‚Äúmix it with ice cream and syrup‚Äùstate(cup2, ice-cream1) ‚àß state(cup2, vanilla) 15
‚Äúdistribute among the couches‚Äù ‚àßj‚àà{1,3}on(pillowj, loveseat1) ‚àß on(pillowi+1, armchairi+1)386
‚Äúboil it on the stove‚Äù state(stove, stovefire1) ‚àß state(kettle, water) 109
‚Äúchange the channel to a movie‚Äù state(tv1, channel4) ‚àß on(book1, loveseat1) 98

ment. The VEIL baseline used actions for repre-
sentation and does not generalize as well as the
postconditions in our logical forms.

It is also instructive to examine the alternate
postconditions that the search procedure consid-
ers. For the first example in Table 2, the following
postcondition was considered by not selected:
grasping(robot, icecream2)‚àßgrasping(robot, syrup1)

While this postcondition uses all the objects de-
scribed in the text, the environment-based features
suggest it makes little sense for the task to end with
the robot eternally grasping objects. For the sec-
ond example, alternate postconditions considered
included:

1. on(pillow1, pillow2) ‚àß on(pillow3, pillow4)
2. ‚àß4j=1 on(pillowj, loveseat1)
3. ‚àß3j=1 near(robot1, armchairj)
The algorithm did not choose options 1 or 3

since the environment-based features recognizes
these as unlikely configurations. Option 2 was
ruled out since the recall correlation feature real-
izes that not all the couches are mentioned in the
postcondition.

To test how much features on the environment
help, we removed all such features from our full
model. We found that the accuracy fell to 16.0%
on the IED metric and 16.6% on the END metric,
showing that the environment is crucial.

In this work, we relied on a simple deterministic
shallow parsing step. We found that shallow pars-
ing was able to correctly process the text in only
46% of the test examples, suggesting that improv-
ing this initial component or at least modeling the
uncertainty there would be beneficial.

10 Related Work
Our work uses semantic parsing to map nat-
ural language instructions to actions via novel
concepts, which brings together several themes:
actions, semantic parsing, novel concepts, and
robotics.
Mapping Text to Actions. Several works (Brana-
van et al., 2009; Branavan et al., 2010; Vogel and
Jurafsky, 2010) use reinforcement learning to di-
rectly map to text to actions, and do not even re-
quire an explicit model of the environment. How-

ever, they can only handle simple actions, whereas
our planner and simulator allows us to work with
postconditions, and thus tackle high-level instruc-
tions. Branavan et al. (2012) extract precondi-
tion relations from text, learn to map text to sub-
goals (postconditions) for a planner. However,
their postconditions are atomic, whereas ours are
complex conjunctions.

Other works (Chen and Mooney, 2011; Kim
and Mooney, 2012; Kollar et al., 2010; Fasola and
Mataric, 2013) have focused only on navigational
verbs and spatial relations, but do not handle high-
level verbs. Artzi and Zettlemoyer (2013) also fall
into the above category and offer a more composi-
tional treatment. They focus on how words com-
pose; we focus on unraveling single words.

The broader problem of grounded language ac-
quisition, involving connecting words to aspects of
a situated context has been heavily studied (Duval-
let et al., 2014; Yu and Siskind, 2013; Chu et al.,
2013; Chen and Mooney, 2008; Mooney, 2008;
Fleischman and Roy, 2005; Liang et al., 2009).
Semantic Parsing. In semantic parsing, much
work has leveraged CCG (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Kwiatkowski et al., 2010). One challenge behind
lexically-heavy approaches is ensuring adequate
lexical coverage. Kwiatkowski et al. (2011) en-
hanced generalization by factoring a lexical entry
into a template plus a lexeme, but the rigidity of
the template remains. This is satisfactory when
words map to one (or two) predicates, which is the
case in most existing semantic parsing tasks. For
example, in Artzi and Zettlemoyer (2013), verbs
are associated with single predicates (‚Äúmove‚Äù to
move, ‚Äúwalk‚Äù to walk, etc.) In our setting, verbs
contain multi-predicate postconditions, for which
these techniques would not be suitable.

As annotated logical forms for training seman-
tic parsers are expensive to obtain, several works
(Clarke et al., 2010; Liang et al., 2011; Berant et
al., 2013; Kwiatkowski et al., 2013) have devel-
oped methods to learn from weaker supervision,
and as in our work, use the execution of the logi-
cal forms to guide the search. Our supervision is
even weaker in that we are able to learn at test time

999



from partial environment constraints.
Grounding to Novel Concepts. Guadarrama et
al. (2014) map open vocabulary text to objects in
an image using a large database. Matuszek et al.
(2012a) create new predicates for every new ad-
jective at test time. Others (Kirk et al., 2014) ask
users for clarification. In contrast, we neither have
access to large databases for this problem, nor do
we do create new predicates or use explicit super-
vision at test time.
Robotic Applications. Our motivation behind this
work is to build robotic systems capable of taking
commands from users. Other works in this area
have considered mapping text to a variety of ma-
nipulation actions (Sung et al., 2015). Levine et
al. (2015) and Lenz et al. (2015) focus on spe-
cific manipulation actions. In order to build a rep-
resentation of the environment, Ren et al. (2012)
and Wu et al. (2014) present vision algorithms but
only output symbolic labels, which could act as
inputs to our system. In future work, we also plan
to integrate our work with RoboBrain (Saxena et
al., 2014) to leverage these existing systems for
building a robotic system capable of working with
physical world data.

11 Conclusion
We have presented an algorithm for mapping text
to actions that induces lexical entries at test time
using the environment. Our algorithm couples the
lexicon extracted from training data with a test-
time search that uses the environment to reduce
the space of logical forms. Our results suggest that
using the environment to provide lexical coverage
of high-level concepts is a promising avenue for
further research.
Acknowledgements. This research was supported
by the ONR (award N00014-14-1-0156), a Sloan
Research Fellowship to the third author, and a Mi-
crosoft Research Faculty Fellowship and NSF Ca-
reer Award to the fourth author. We thank Aditya
Jami and Jaeyong Sung for useful discussions. We
also thank Jiaqi Su for her help with data collec-
tion and all the people who participated in the user
study.
Reproducibility. Code, data, and experiments for
this paper are available on the CodaLab platform
at https://www.codalab.org/worksheets/
0x7f9151ec074f4f589e4d4786db7bb6de/. De-
mos can be found at http://tellmedave.com.
Appendix: Parsing Text into Control Flow
Graph.

We first decompose the text x into its control

flow graph G using a simple set of rules:
‚Ä¢ The parse tree of x is generated using the Stan-

ford parser (Klein and Manning, 2003) and a
frame node is created for each non-auxiliary
verb node in the tree.
‚Ä¢ Conditional nodes are discovered by look-

ing for the keywords until, if, after, when.
The associated subtree is then parsed deter-
ministically using a set of a rules. For
example, a rule parses ‚Äúfor x minutes‚Äù to
for(digit :x,unit :minutes). We found that
all conditionals can be interpreted against the
initial environment e1, since our world is fully-
observable, deterministic, and the user giving
the command has full view of the world.
‚Ä¢ To find objects, we look for anaphoric terminal

nodes or nominals whose parent is not a nominal
or which have a PP sibling. These are processed
into object descriptions œâ.
‚Ä¢ Object descriptions œâ are attached to the frame

node, whose verb is nearest in the parse tree to
the main noun of œâ.
‚Ä¢ Nodes corresponding to {IN,TO,CC,‚Äú,‚Äù} are

added as the relation between the corresponding
argument objects.
‚Ä¢ If there is a conjunction between two objects in

a frame node and if these objects have the same
relation to other objects, then we split the frame
node into two sequential frame nodes around
these objects. For example, a frame node corre-
sponding to the text segment ‚Äútake the cup and
bowl from table‚Äù is split into two frame nodes
corresponding to ‚Äútake the cup from table‚Äù and
‚Äútake bowl from table‚Äù.
‚Ä¢ A temporal edge is added between successive

frame nodes in the same branch of a condition.
A temporal edge is added between a conditional
node and head of the true and false branches of
the condition. The end of all branches in a sen-
tence are joined to the starting node of the suc-
cessive sentence.

References
Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised

learning of semantic parsers for mapping instruc-
tions to actions. Transactions of the Association for
Computational Linguistics (TACL), 1:49‚Äì62.

J. Berant and P. Liang. 2014. Semantic parsing via
paraphrasing. In Association for Computational
Linguistics (ACL).

J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.
Semantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).

1000



M. Bollini, J. Barry, and D. Rus. 2011. Bakebot: Bak-
ing cookies with the PR2. In The PR2 Workshop,
IROS.

S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping
instructions to actions. In Association for Com-
putational Linguistics and International Joint Con-
ference on Natural Language Processing (ACL-
IJCNLP), pages 82‚Äì90.

S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-
level instructions to commands. In Association
for Computational Linguistics (ACL), pages 1268‚Äì
1277.

S. Branavan, N. Kushman, T. Lei, and R. Barzilay.
2012. Learning high-level planning from text. In
Association for Computational Linguistics (ACL),
pages 126‚Äì135.

D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128‚Äì135.

D. L. Chen and R. J. Mooney. 2011. Learning to in-
terpret natural language navigation instructions from
observations. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 859‚Äì865.

V. Chu, I. McMahon, L. Riano, C. McDonald, Q. He,
J. Perez-Tejada, M. Arrigo, N. Fitter, J. Nappo,
T. Darrell, et al. 2013. Using robotic exploratory
procedures to learn the meaning of haptic adjectives.
In International Conference on Intelligent Robots
and Systems (IROS).

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world‚Äôs re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18‚Äì27.

F. Duvallet, M. R. Walter, T. Howard, S. Hemachan-
dra, J. Oh, S. Teller, N. Roy, and A. Stentz. 2014.
Inferring maps and behaviors from natural language
instructions. In International Symposium on Exper-
imental Robotics (ISER).

J. Fasola and M. Mataric. 2013. Using semantic fields
to model dynamic spatial relations in a robot archi-
tecture for natural language instruction of service
robots. In International Conference on Intelligent
Robots and Systems (IROS).

J. Fasola and M. J. Matari‚Äôc. 2014. Interpreting
instruction sequences in spatial language discourse
with pragmatics towards natural human-robot inter-
action. In International Conference on Robotics and
Automation (ICRA), pages 6667‚Äì6672.

C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.

M. Fleischman and D. Roy. 2005. Intentional context
in situated natural language learning. In Computa-
tional Natural Language Learning (CoNLL), pages
104‚Äì111.

S. Guadarrama, L. Riano, D. Golland, D. Gouhring,
Y. Jia, D. Klein, P. Abbeel, and T. Darrell. 2013.
Grounding spatial relations for human-robot inter-
action. In International Conference on Intelligent
Robots and Systems (IROS).

S. Guadarrama, E. Rodner, K. Saenko, N. Zhang,
R. Farrell, J. Donahue, and T. Darrell. 2014. Open-
vocabulary object retrieval. In Robotics: Science
and Systems (RSS).

J. Kim and R. Mooney. 2012. Unsupervised PCFG in-
duction for grounded language learning with highly
ambiguous supervision. In Computational Natural
Language Learning (CoNLL), pages 433‚Äì444.

N. H. Kirk, D. Nyga, and M. Beetz. 2014. Con-
trolled natural languages for language generation in
artificial cognition. In International Conference on
Robotics and Automation (ICRA), pages 6667‚Äì6672.

D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association for Computational Lin-
guistics (ACL), pages 423‚Äì430.

T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010.
Grounding verbs of motion in natural language com-
mands to robots. In International Symposium on Ex-
perimental Robotics (ISER).

T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order uni-
fication. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223‚Äì1233.

T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in
CCG grammar induction for semantic parsing. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1512‚Äì1523.

T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).

I. Lenz, R. Knepper, and A. Saxena. 2015. Deepmpc:
Learning deep latent features for model predictive
control. In Robotics Science and Systems (RSS).

S. Levine, C. Finn, T. Darrell, and P. Abbeel. 2015.
End-to-end training of deep visuomotor policies.
arXiv preprint arXiv:1504.00702.

P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 91‚Äì99.

1001



P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL),
pages 590‚Äì599.

C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo,
and D. Fox. 2012a. A joint model of language and
perception for grounded attribute learning. In Inter-
national Conference on Machine Learning (ICML),
pages 1671‚Äì1678.

C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox.
2012b. Learning to parse natural language com-
mands to a robot control system. In International
Symposium on Experimental Robotics (ISER).

D. Misra, J. Sung, K. Lee, and A. Saxena. 2014. Tell
Me Dave: Context-sensitive grounding of natural
language to mobile manipulation instructions. In
Robotics: Science and Systems (RSS).

R. Mooney. 2008. Learning to connect language and
perception. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 1598‚Äì1601.

F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29:19‚Äì51.

X. Ren, L. Bo, and D. Fox. 2012. Rgb-(d) scene label-
ing: Features and algorithms. In Computer Vision
and Pattern Recognition (CVPR), pages 2759‚Äì2766.

J. Rintanen. 2012. Planning as satisfiability: Heuris-
tics. Artificial Intelligence, 193.

A. Saxena, A. Jain, O. Sener, A. Jami, D. K. Misra,
and H. S. Koppula. 2014. Robobrain: Large-
scale knowledge engine for robots. arXiv preprint
arXiv:1412.0691.

J. Sung, S. H. Jin, and A. Saxena. 2015. Robobarista:
Object part based transfer of manipulation trajecto-
ries from crowd-sourcing in 3d pointclouds. arXiv
preprint arXiv:1504.03071.

S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Un-
derstanding natural language commands for robotic
navigation and mobile manipulation. In Associa-
tion for the Advancement of Artificial Intelligence
(AAAI).

A. Vogel and D. Jurafsky. 2010. Learning to follow
navigational directions. In Association for Compu-
tational Linguistics (ACL), pages 806‚Äì814.

C. Wu, I. Lenz, and A. Saxena. 2014. Hierarchical se-
mantic labeling for task-relevant RGB-D perception.
In Robotics: Science and Systems (RSS).

H. Yu and J. M. Siskind. 2013. Grounded language
learning from video described with sentences. In
Association for Computational Linguistics (ACL),
pages 53‚Äì63.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI), pages 658‚Äì
666.

L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to log-
ical form. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 678‚Äì687.

1002


