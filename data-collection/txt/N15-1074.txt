



















































Incorporating Word Correlation Knowledge into Topic Modeling


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 725–734,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Incorporating Word Correlation Knowledge into Topic Modeling

Pengtao Xie
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
pengtaox@cs.cmu.edu

Diyi Yang
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
diyiy@cs.cmu.edu

Eric P. Xing
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
epxing@cs.cmu.edu

Abstract

This paper studies how to incorporate the ex-
ternal word correlation knowledge to improve
the coherence of topic modeling. Existing
topic models assume words are generated in-
dependently and lack the mechanism to utilize
the rich similarity relationships among words
to learn coherent topics. To solve this prob-
lem, we build a Markov Random Field (MRF)
regularized Latent Dirichlet Allocation (LDA)
model, which defines a MRF on the latent
topic layer of LDA to encourage words la-
beled as similar to share the same topic label.
Under our model, the topic assignment of each
word is not independent, but rather affected by
the topic labels of its correlated words. Simi-
lar words have better chance to be put into the
same topic due to the regularization of MRF,
hence the coherence of topics can be boosted.
In addition, our model can accommodate the
subtlety that whether two words are similar
depends on which topic they appear in, which
allows word with multiple senses to be put into
different topics properly. We derive a vari-
ational inference method to infer the poste-
rior probabilities and learn model parameters
and present techniques to deal with the hard-
to-compute partition function in MRF. Exper-
iments on two datasets demonstrate the effec-
tiveness of our model.

1 Introduction

Probabilistic topic models (PTM), such as proba-
bilistic latent semantic indexing(PLSI) (Hofmann,
1999) and latent Dirichlet allocation(LDA) (Blei et
al., 2003) have shown great success in documents

modeling and analysis. Topic models posit doc-
ument collection exhibits multiple latent semantic
topics where each topic is represented as a multino-
mial distribution over a given vocabulary and each
document is a mixture of hidden topics. To generate
a document d, PTM first samples a topic proportion
vector, then for each wordw in d, samples a topic in-
dicator z and generatesw from the topic-word multi-
nomial corresponding to topic z.

A key limitation of the existing PTMs is that
words are assumed to be uncorrelated and generated
independently. The topic assignment for each word
is irrelevant to all other words. While this assump-
tion facilitates computational efficiency, it loses the
rich correlations between words. In many applica-
tions, users have external knowledge regarding word
correlation, which can be taken into account to im-
prove the semantic coherence of topic modeling. For
example, WordNet (Miller, 1995a) presents a large
amount of synonym relationships between words,
Wikipedia1 provides a knowledge graph by linking
correlated concepts together and named entity rec-
ognizer identifies the categories of entity mentions.
All of these external knowledge can be leveraged to
learn more coherent topics if we can design a mech-
anism to encourage similar words, correlated con-
cepts, entities of the same category to be assigned to
the same topic.

Many approaches (Andrzejewski et al., 2009; Pet-
terson et al., 2010; Newman et al., 2011) have at-
tempted to solve this problem by enforcing hard and
topic-independent rules that similar words should
have similar probabilities in all topics, which is

1https://www.wikipedia.org/

725



questionable in that two words with similar rep-
resentativeness of one topic are not necessarily of
equal importance for another topic. For example,
in the fruit topic, the words apple and orange have
similar representativeness, while in an IT company
topic, apple has much higher importance than or-
ange. As another example, church and bible are
similarly relevant to a religion topic, whereas their
relevance to an architecture topic are vastly differ-
ent. Exiting approaches are unable to differentiate
the subtleties of word sense across topics and would
falsely put irrelevant words into the same topic. For
instance, since orange and microsoft are both la-
beled as similar to apple and are required to have
similar probabilities in all topics as apple has, in the
end, they will be unreasonably allocated to the same
topic.

The existing approaches fail to properly use the
word correlation knowledge, which is usually a list
of word pairs labeled as similar. The similarity is
computed based on statistics such as co-occurrence
which are unable to accommodate the subtlety that
whether two words labeled as similar are truly sim-
ilar depends on which topic they appear in, as ex-
plained by the aforementioned examples. Ideally,
the knowledge would be word A and B are similar
under topic C. However, in reality, we only know
two words are similar, but not under which topic. In
this paper, we aim to abridge this gap. Gaining in-
sights from (Verbeek and Triggs, 2007; Zhao et al.,
2010; Zhu and Xing, 2010), we design a Markov
Random Field regularized LDA model (MRF-LDA)
which utilizes the external knowledge in a soft and
topic-dependent manner to improve the coherence of
topic modeling. We define a MRF on the latent topic
layer of LDA to encode word correlations. Within a
document, if two words are labeled as similar ac-
cording to the external knowledge, their latent topic
nodes will be connected by an undirected edge and
a binary potential function is defined to encourage
them to share the same topic label. This mecha-
nism gives correlated words a better chance to be
put into the same topic, thereby, improves the co-
herence of the learned topics. Our model provides
a mechanism to automatically decide under which
topic, two words labeled as similar are truly simi-
lar. We encourage words labeled as similar to share
the same topic label, but do not specify which topic

label they should share, and leave this to be de-
cided by data. In the above mentioned apple, or-
ange, microsoft example, we encourage apple and
orange to share the same topic label A and try to
push apple and microsoft to the same topic B. But
A and B are not necessarily the same and they will
be inferred according to the fitness of data. Dif-
ferent from the existing approaches which directly
use the word similarities to control the topic-word
distributions in a hard and topic-independent way,
our method imposes constraints on the latent topic
layer by which the topic-word multinomials are in-
fluenced indirectly and softly and are topic-aware.

The rest of the paper is organized as follows. In
Section 2, we introduce related work. In Section 3,
we propose the MRF-LDA model and present the
variational inference method. Section 4 gives exper-
imental results. Section 5 concludes the paper.

2 Related Work

Different from purely unsupervised topics models
that often result in incoherent topics, knowledge
based topic models enable us to take prior knowl-
edge into account to produce more meaningful top-
ics. Various approaches have been proposed to ex-
ploit the correlations and similarities among words
to improve topic modeling instead of purely rely-
ing on how often words co-occur in different con-
texts (Heinrich, 2009). For instance, Andrzejewski
et al. (2009) imposes Dirichlet Forest prior over the
topic-word multinomials to encode the Must-Links
and Cannot-Links between words. Words with
Must-Links are encouraged to have similar proba-
bilities within all topics while those with Cannot-
Links are disallowed to simultaneously have large
probabilities within any topic. Similarly, Petterson
et al. (2010) adopted word information as features
rather than as explicit constraints and defined a prior
over the topic-word multinomials such that similar
words share similar topic distributions. Newman
et al. (2011) proposed a quadratic regularizer and
a convolved Dirichlet regularizer over topic-word
multinomials to incorporate the correlation between
words. All of these methods directly incorporate
the word correlation knowledge into the topic-word
distributions in a hard and topic-independent way,
which ignore the fact that whether two words are

726



correlated depends on which topic they appear in.
There are several works utilizing knowledge with

more complex structure to improve topic modeling.
Boyd-Graber et al. (2007) incorporate the synset
structure in WordNet (Miller, 1995b) into LDA for
word sense disambiguation, where each topic is a
random process defined over the synsets. Hu et al.
(2011) proposed interactive topic modeling, which
allows users to iteratively refine the discovered top-
ics by adding constraints such as certain set of words
must appear together in the same topic. Andrze-
jewski et al. (2011) proposed a general framework
which uses first order logic to encode various do-
main knowledge regarding documents, topics and
side information into LDA. The vast generality and
expressivity of this model makes its inference to be
very hard. Chen et al. (2013) proposed a topic model
to model multi-domain knowledge, where each doc-
ument is an admixture of latent topics and each topic
is a probability distribution over domain knowledge.
Jagarlamudi et al. (2012) proposed to guide topic
modeling by setting a set of seed words in the begin-
ning that user believes could represent certain topics.
While these knowledge are rich in structure, they are
hard to acquire in the real world applications. In this
paper, we focus on pairwise word correlation knowl-
edge which are widely attainable in many scenarios.

In the domain of computer vision, the idea of
using MRF to enforce topical coherence between
neighboring patches or superpixels has been ex-
ploited by several works. Verbeek and Triggs (2007)
proposed Markov field aspect model where each im-
age patch is modeled using PLSA (Hofmann, 1999)
and a Potts model is imposed on the hidden topic
layer to enforce spatial coherence. Zhao et al. (2010)
proposed topic random field model where each su-
perpixel is modeled using a combination of LDA
and mixture of Gaussian model and a Potts model is
defined on the topic layer to encourage neighboring
superpixels to share the same topic. Similarly, Zhu
and Xing (2010) proposed a conditional topic ran-
dom field to incorporate features about words and
documents into topic modeling. In their model, the
MRF is restricted to be a linear chain, which can
only capture the dependencies between neighboring
words and is unable to incorporate long range word
correlations. Different from these works, the MRF
in our model is not restricted to Potts or chain struc-

ture. Instead, its structure is decided by the word
correlation knowledge and can be arbitrary.

3 Markov Random Field Regularized
Latent Dirichlet Allocation

In this section, we present the MRF-LDA model and
the variational inference technique.

3.1 MRF-LDA

We propose the MRF-LDA model to incorporate
word similarities into topic modeling. As shown
in Figure 1, MRF-LDA extends the standard LDA
model by imposing a Markov Random Field on the
latent topic layer. Similar to LDA, we assume a doc-
ument possesses a topic proportion vector θ sampled
from a Dirichlet distribution. Each topic βk is a
multinomial distribution over words. Each word w
has a topic label z indicating which topic w belongs
to.

In many scenarios, we have access to exter-
nal knowledge regarding the correlations between
words, such as apple and orange are similar, church
and bible are semantically related. These similarity
relationships among words can be leveraged to im-
prove the coherence of learned topics. To do this,
we define a Markov Random Field over the latent
topic layer. Given a document d containingN words
{wi}Ni=1, we examine each word pair (wi, wj). If
they are correlated according to the external knowl-
edge, we create an undirected edge between their
topic labels (zi, zj). In the end, we obtain an undi-
rected graph G where the nodes are latent topic la-
bels {zi}Ni=1 and edges connect topic labels of cor-
related words. In the example shown in Figure 1, G
contains five nodes z1, z2, z3, z4, z5 and four edges
connecting (z1, z3), (z2, z5), (z3, z4), (z3, z5).

Given the undirected graph G, we can turn it into
a Markov Random Field by defining unary poten-
tials over nodes and binary potentials over edges.
We define the unary potential for zi as p(zi|θ),
which is a multinomial distribution parameterized
by θ. In standard LDA, this is how a topic is sampled
from the topic proportion vector. For binary poten-
tial, with the goal to encourage similar words to have
similar topic assignments, we define the edge po-
tential between (zi, zj) as exp{I(zi = zj)}, where
I(·) is the indicator function. This potential func-

727



tion yields a larger value if the two topic labels are
the same and a smaller value if the two topic labels
are different. Hence, it encourages similar words
to be assigned to the same topic. Under the MRF
model, the joint probability of all topic assignments
z = {zi}Ni=1 can be written as

p(z|θ, λ) = 1A(θ,λ)
N∏
i=1

p(zi|θ)
exp{λ ∑

(m,n)∈P
I(zm = zn)}

(1)

where P denotes the edges in G and A(θ, λ) is the
partition function

A(θ) =
∑
z

N∏
i=1

p(zi|θ) exp{λ
∑

(m,n)∈P
I(zm = zn)}

(2)
We introduce λ ≥ 0 as a trade-off parameter be-
tween unary potential and binary potential. In stan-
dard LDA, topic label zi only depends on topic pro-
portion vector θ. In MRF-LDA, zi not only depends
on θ, but also depends on the topic labels of similar
words. If γ is set to zero, the correlation between
words is ignored and MRF-LDA is reduced to LDA.
Given the topic labels, the generation of words is the
same as LDA. wi is generated from the topic-words
multinomial distribution βzi corresponding to zi.

In MRF-LDA, the generative process of a docu-
ment is summarized as follows:

• Draw a topic proportion vector θ ∼ Dir(α)
• Draw topic labels z for all words from the joint

distribution defined in Eq.(1)

• For each word wi, drawn wi ∼ multi(βzi)
Accordingly, the joint distribution of θ, z and w

can be written as

p(θ, z,w|α,β, λ) = p(θ|α)p(z|θ, λ)∏N
i=1 p(wi|zi,β)

(3)

3.2 Variational Inference and Parameter
Learning

The key inference problem we need to solve in
MRF-LDA is to compute the posterior p(θ, z|w) of
latent variables θ, z given observed data w. As in
LDA (Blei et al., 2003), exact computation is in-
tractable. What makes things even challenging in





3z

3w

K



4z

4w

2z

2w

5z

5w

1z

1w

Figure 1: Markov Random Field Regularized Latent
Dirichlet Allocation Model

MRF-LDA is that, an undirected MRF is coupled
with a directed LDA and the hard-to-compute parti-
tion function of MRF makes the posterior inference
and parameter learning very difficult. To solve this
problem, we resort to variational inference (Wain-
wright and Jordan, 2008), which uses a easy-to-
handle variational distribution to approximate the
true posterior of latent variables. To deal with the
partition function in MRF, we seek lower bound of
the variational lower bound to achieve tractability.
We introduce a variational distribution

q(θ, z) = q(θ|η)
N∏
i=1

q(zi|φi) (4)

where Dirichlet parameter η and multinomial pa-
rameters {φi}Ni=1 are free variational parameters.
Using Jensen’s inequality (Wainwright and Jordan,
2008), we can obtain a variational lower bound

L = Eq[log p(θ|α)] + Eq[log p(z|θ, λ)]
+Eq[log

N∏
i=1

p(wi|zi,β)]− Eq[log q(θ|η)]

−Eq[log
N∏
i=1

q(zi|φi)]
(5)

728



in which Eq[log p(z|θ, λ)] can be expanded as

Eq[log p(z|θ, λ)]
= −Eq[logA(θ, λ)] + λ

∑
(m,n)∈P

K∑
k=1

φmkφnk

+
N∑
i=1

K∑
k=1

φik(Ψ(ηk)−Ψ(
K∑
j=1

ηj))

(6)
The item Eq[logA(θ, λ)] involves the hard-to-
compute partition function, which has no analytical
expressions. We discuss how to deal with it in the
sequel. With Taylor expansion, we can obtain an
upper bound of Eq[logA(θ, λ)]

Eq[logA(θ, λ)] ≤ c−1Eq[A(θ, λ)]− 1 + log c (7)

where c ≥ 0 is a new variational parameter.
Eq[A(θ, λ)] can be further upper bounded as

Eq[logA(θ, λ)] ≤ exp{
∑

(m,n)∈P
λ}

∑
n1,n2,··· ,nK

Eq[
K∏
k=1

θnk ]
(8)

where nk denotes the number of words assigned

with topic label k and
K∑
k=1

nk = N . We further

bound
∑

n1,n2,··· ,nK
Eq[

K∏
k=1

θnk ] as follows

∑
n1,n2,··· ,nK

Eq[
K∏
k=1

θnk ]

=
∑

n1,n2,··· ,nK

Γ(
K∑

k=1
ηk)

K∏
k=1

Γ(ηk)

∫ K∏
k=1

θnk+ηk−1dθ

=
∑

n1,n2,··· ,nK

Γ(
K∑

k=1
ηk)

K∏
k=1

Γ(ηk)

K∏
k=1

Γ(nk+ηk)

Γ(
K∑

k=1
nk+ηk)

=
∑

n1,n2,··· ,nK

∏K
k=1(ηk)nk

(
K∑

k=1

ηk)N

≤ ∑
n1,n2,··· ,nK

K∏
k=1

(nk)!

(N)!

(9)

where (a)n denotes the Pochhammer symbol, which
is defined as (a)n = a(a + 1) . . . (a + n − 1) and∑

n1,n2,··· ,nK
∏K

k=1(nk)!
(N)! is a constant. Setting c =

c/
∑

n1,n2,··· ,nK
∏K

k=1(nk)!
(N)! , we get

Eq[logA(θ, λ)] ≤ c−1 exp{
∑

(i,j)∈P
λ} − 1 + log c

(10)
Given this upper bound, we can obtain a lower
bound of the variational lower bound defined in
Eq.(5). Variational parameters and model parame-
ters can be learned by maximizing the lower bound
using iterative EM algorithm. In E-step, we fix
the model parameters and compute the variational
parameters by setting the derivatives of the lower
bound w.r.t the variational parameters to zero

ηk = αk +
N∑
i=1

φik, c = exp{
∑

(m,n)∈P
λ} (11)

φik ∝ exp{Ψ(ηk)−Ψ(
K∑
j=1

ηj) + λ
∑

j∈N (i)
φjk

+
V∑
v=1

wiv log βkv}
(12)

In Eq.(12), N (i) denotes the words that are labeled
to be similar to i. As can be seen from this equa-
tion, the probability φik that word i is assigned to
topic k depends on the probability φjk of i’s cor-
related words j. This explains how our model can
incorporate word correlations in topic assignments.
In M-step, we fix the variational parameters and up-
date the model parameters by maximizing the lower
bound defined on the set of documents {wd}Dd=1

βkv ∝
D∑
d=1

Nd∑
i=1

φd,i,kwd,i,v (13)

λ =
1
|P | log

D∑
d=1

∑
(m,n)∈Pd

K∑
k=1

φd,m,kφd,n,k

|P |
D∑
d=1

1
cd

(14)

4 Experiment

In this section, we corroborate the effectiveness of
our model by comparing it with three baseline meth-
ods on two datasets.

729



dataset 20-Newsgroups NIPS
# documents 18846 1500

# words 40343 12419

Table 1: Dataset Statistics

4.1 Experiment Setup

• Dataset: We use two datasets in the exper-
iments: 20-Newsgroups2 and NIPS3. Their
statistics are summarized in Table 1.

• External Knowledge: We extract word cor-
relation knowledge from Web Eigenwords4,
where each word has a real-valued vector cap-
turing the semantic meaning of this word based
on distributional similarity. Two words are re-
garded as correlated if their representation vec-
tors are similar enough. It is worth mentioning
that, other sources of external word correlation
knowledge, such as Word2Vec (Mikolov et al.,
2013) and Glove (Pennington et al., 2014), can
be readily incorporated into MRF-LDA.

• Baselines: We compare our model with three
baseline methods: LDA (Blei et al., 2003), DF-
LDA (Andrzejewski et al., 2009) and Quad-
LDA (Newman et al., 2011). LDA is the most
widely used topic model, but it is unable to in-
corporate external knowledge. DF-LDA and
Quad-LDA are two models designed to incor-
porate word correlation to improve topic mod-
eling. DF-LDA puts a Dirichlet Forest prior
over the topic-word multinomials to encode the
Must-Links and Cannot-Links between words.
Quad-LDA regularizes the topic-word distri-
butions with a structured prior to incorporate
word relation.

• Parameter Settings: For all methods, we learn
100 topics. LDA parameters are set to their
default settings in (Andrzejewski et al., 2009).
For DF-LDA, we set its parameters as α = 1,
β = 0.01 and η = 100. The Must/Cannot links
between words are generated based on the co-
sine similarity of words’ vector representations

2http://qwone.com/ jason/20Newsgroups/
3http://archive.ics.uci.edu/ml/datasets/Bag+of+Words
4http://www.cis.upenn.edu/ ungar/eigenwords/

in Web Eigenwords. Word pairs with similar-
ity higher than 0.99 are set as Must-Links, and
pairs with similarity lower than 0.1 are put into
Cannot-Link set. For Quad-LDA, β is set as
0.01; α is defined as 0.05·ND·T , where N is the to-
tal occurrences of all words in all documents, D
is the number of documents and T is topic num-
ber. For MRF-LDA, word pairs with similarity
higher than 0.99 are labeled as correlated.

4.2 Results

We compare our model with the baseline methods
both qualitatively and quantitatively.

4.2.1 Qualitative Evaluation
Table 2 shows some exemplar topics learned by

the four methods on the 20-Newsgroups dataset.
Each topic is visualized by the top ten words. Words
that are noisy and lack representativeness are high-
lighted with bold font. Topic 1 is about crime and
guns. Topic 2 is about sex. Topic 3 is about sports
and topic 4 is about health insurance. As can be seen
from the table, our method MRF-LDA can learn
more coherent topics with fewer noisy and meaning-
less words than the baseline methods. LDA lacks the
mechanism to incorporate word correlation knowl-
edge and generates the words independently. The
similarity relationships among words cannot be uti-
lized to imporve the coherence of topic modeling.
Consequently, noise words such as will, year, used
which cannot effectively represent a topic, show up
due to their high frequency. DF-LDA and Quad-
LDA proposed to use word correlations to enhance
the coherence of learned topics. However, they im-
properly enforce words labeled as similar to have
similar probabilities in all topics, which violates the
fact that whether two words are similar depend on
which topic they appear in. As a consequence, the
topics extracted by these two methods are unsatis-
factory. For example, topic 2 learned by DF-LDA
mixed up a sex topic and a reading topic. Less rele-
vant words such as columbia, year, write show up in
the health insurance topic (topic 4) learned by Quad-
LDA. Our method MRF-LDA incorporates the word
correlation knowledge by imposing a MRF over the
latent topic layer to encourage correlated words to
share the same topic label, hence similar words have
better chance to be put into the same topic. Conse-

730



Table 2: Topics Learned from 20-Newsgroups Dataset

LDA DF-LDA
Topic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4
(Crime) (Sex) (Sports) (Health) (Crime) (Sex) (Sports) (Health)
gun sex team government gun book game money
guns men game money police men games pay
weapons homosexuality hockey private carry books players insurance
control homosexual season people kill homosexual hockey policy
firearms gay will will killed homosexuality baseball tax
crime sexual year health weapon reference fan companies
police com play tax cops gay league today
com homosexuals nhl care warrant read played plan
weapon people games insurance deaths male season health
used cramer teams program control homosexuals ball jobs

Quad-LDA MRF-LDA
Topic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4
(Crime) (Sex) (Sports) (Health) (Crime) (Sex) (Sports) (Health)
gun homosexuality game money gun men game care
guns sex team insurance guns sex team insurance
crime homosexual play columbia weapons women hockey private
police sin games pay child homosexual players cost
weapons marriage hockey health police homosexuality play health
firearms context season tax control child player costs
criminal people rom year kill ass fans company
criminals sexual period private deaths sexual teams companies
people gay goal care death gay fan tax
law homosexuals player write people homosexuals best public

quently, the learned topics are of high coherence. As
shown in Table 2, the topics learned by our method
are largely better than those learned by the baseline
methods. The topics are of high coherence and con-
tain fewer noise and irrelevant words.

Our method provides a mechanism to automati-
cally decide under which topic, two words labeled as
similar are truly similar. The decision is made flex-
ibly by data according to their fitness to the model,
rather than by a hard rule adopted by DF-LDA and
Quad-LDA. For instance, according to the external
knowledge, the word child is correlated with gun
and with men simultaneously. Under a crime topic,
child and gun are truly correlated because they co-
occur a lot in youth crime news, whereas, child and
men are less correlated in this topic. Under a sex
topic, child and men are truly correlated whereas
child and gun are not. Our method can differentiate
this subtlety and successfully put child and gun into
the crime topic and put child and men into the sex
topic. This is because our method encourages child

and gun to be put into the same topic A and encour-
ages child and men to be put into the same topic B,
but does not require A and B to be the same. A and
B are freely decided by data.

Table 3 shows some topics learned on NIPS
dataset. The four topics correspond to vision, neural
network, speech recognition and electronic circuits
respectively. From this table, we observe that the
topics learned by our method are better in coherence
than those learned from the baseline methods, which
again demonstrates the effectiveness of our model.

4.2.2 Quantitative Evaluation

We also evaluate our method in a quantitative
manner. Similar to (Xie and Xing, 2013), we use
the coherence measure (CM) to assess how coherent
the learned topics are. For each topic, we pick up the
top 10 candidate words and ask human annotators to
judge whether they are relevant to the topic. First,
annotators needs to judge whether a topic is inter-
pretable or not. If not, the ten candidate words in this

731



Table 3: Topics Learned from NIPS Dataset

LDA DF-LDA
Topic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4
(Vision) (Neural Net) (Speech) (Circuits) (Vision) (Neural Net) (Speech) (Circuits)
image network hmm chip images network speech analog
images neural mlp analog pixel system context chip
pixel feedforward hidden weight view connection speaker vlsi
vision architecture context digital recognition application frame implement
segment research model neural face artificial continuous digital
visual general recognition hardware ica input processing hardware
scene applied probabilities bit vision obtained number voltage
texture vol training neuron system department dependent bit
contour paper markov implement natural fixed frames transistor
edge introduction system vlsi faces techniques spectral design

Quad-LDA MRF-LDA
Topic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4
(Vision) (Neural Net) (Speech) (Circuits) (Vision) (Neural Net) (Speech) (Circuits)
image training speech circuit image network hmm chip
images set hmm analog images model speech synapse
pixel network speaker chip pixel learning acoustic digital
region learning acoustic voltage disparity function context analog
vision net phonetic current color input word board
scene number vocabulary vlsi intensity neural phonetic charge
surface algorithm phone neuron stereo set frames synaptic
texture class utterance gate scene algorithm speaker hardware
local input utterances input camera system phone vlsi
contour examples frames transistor detector data vocabulary programmable

topic are automatically labeled as irrelevant. Other-
wise, annotators are asked to identify words that are
relevant to this topic. Coherence measure (CM) is
defined as the ratio between the number of relevant
words and total number of candidate words. In our
experiments, four graduate students participated the
labeling. For each dataset and each method, 10% of
topics were randomly chosen for labeling.

Table 4 and 5 summarize the coherence mea-
sure of topics learned on 20-Newsgroups dataset and
NIPS dataset respectively. As shown in the table, our
method significantly outperforms the baseline meth-
ods with a large margin. On the 20-Newsgroups
dataset, our method achieves an average coherence
measure of 60.8%, which is two times better than
LDA. On the NIPS dataset, our method is also much
better than the baselines. In summary, we conclude
that MRF-LDA produces much better results on
both datasets compared to baselines, which demon-
strates the effectiveness of our model in exploiting

word correlation knowledge to improve the qual-
ity of topic modeling. To assess the consistency of
the labelings made by different annotators, we com-
puted the intraclass correlation coefficient (ICC).
The ICCs on 20-Newsgroups and NIPS dataset are
0.925 and 0.725 respectively, which indicate good
agreement between different annotators.

5 Conclusion

In this paper, we propose a MRF-LDA model, aim-
ing to incorporate word correlation knowledge to
improve topic modeling. Our model defines a MRF
over the latent topic layer of LDA, to encourage cor-
related words to be put into the same topic. Our
model provides the flexibility to enable a word to
be similar to different words under different top-
ics, which is more plausible and allows a word to
show up in multiple topics properly. We evaluate
our model on two datasets and corroborate its effec-
tiveness both qualitatively and quantitatively.

732



Method Annotator1 Annotator2 Annotator3 Annotator4 Mean Standard Deviation
LDA 30 33 22 29 28.5 4.7

DF-LDA 35 41 35 27 36.8 2.9
Quad-LDA 32 36 33 26 31.8 4.2
MRF-LDA 60 60 63 60 60.8 1.5

Table 4: CM (%) on 20-Newsgroups Dataset

Method Annotator1 Annotator2 Annotator3 Annotator4 Mean Standard Deviation
LDA 75 74 74 69 73 2.7

DF-LDA 65 74 72 47 66 9.5
Quad-LDA 40 40 38 25 35.8 7.2
MRF-LDA 86 85 87 84 85.8 1.0

Table 5: CM (%) on NIPS Dataset

References

David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 25–32. ACM.

David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorporat-
ing general domain knowledge into latent dirichlet al-
location using first-order logic. In Proceedings of the
Twenty-Second international joint conference on Ar-
tificial Intelligence-Volume Volume Two, pages 1171–
1177. AAAI Press.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP-CoNLL, pages 1024–1033.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013.
Leveraging multi-domain prior knowledge in topic
models. In Proceedings of the Twenty-Third Interna-
tional Joint Conference on Artificial Intelligence, IJ-
CAI’13, pages 2071–2077.

Gregor Heinrich. 2009. A generic approach to topic
models. In Machine Learning and Knowledge Discov-
ery in Databases, pages 517–532. Springer.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 50–57. ACM.

Yuening Hu, Jordan L Boyd-Graber, and Brianna Sati-
noff. 2011. Interactive topic modeling. In ACL, pages
248–257.

Jagadeesh Jagarlamudi, Hal Daumé, III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, EACL ’12, pages 204–213.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.

George A Miller. 1995a. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41.

George A. Miller. 1995b. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41, Novem-
ber.

David Newman, Edwin V Bonilla, and Wray Buntine.
2011. Improving topic coherence with regularized
topic models. In Advances in Neural Information Pro-
cessing Systems, pages 496–504.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods in
Natural Language Processing (EMNLP 2014), 12.

James Petterson, Wray Buntine, Shravan M Narayana-
murthy, Tibério S Caetano, and Alex J Smola. 2010.
Word features for latent dirichlet allocation. In Ad-
vances in Neural Information Processing Systems,
pages 1921–1929.

Jakob Verbeek and Bill Triggs. 2007. Region classifi-
cation with markov field aspect models. In Computer
Vision and Pattern Recognition, 2007. CVPR’07. IEEE
Conference on, pages 1–8. IEEE.

Martin J Wainwright and Michael I Jordan. 2008. Graph-
ical models, exponential families, and variational in-
ference. Foundations and Trends R© in Machine Learn-
ing, 1(1-2):1–305.

733



Pengtao Xie and Eric P Xing. 2013. Integrating docu-
ment clustering and topic modeling. Proceedings of
the 30th Conference on Uncertainty in Artificial Intel-
ligence.

Bin Zhao, Li Fei-Fei, and Eric Xing. 2010. Image seg-
mentation with topic random field. Computer Vision–
ECCV 2010, pages 785–798.

Jun Zhu and Eric P Xing. 2010. Conditional topic ran-
dom fields. In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pages
1239–1246.

734


