



















































Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 148–152,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Are Semantically Coherent Topic Models Useful for Ad Hoc Information
Retrieval?

Romain Deveaud Eric SanJuan
University of Avignon - LIA

Avignon, France
romain.deveaud@univ-avignon.fr

eric.sanjuan@univ-avignon.fr

Patrice Bellot
Aix-Marseille University - LSIS

Marseille, France
patrice.bellot@lsis.org

Abstract
The current topic modeling approaches for
Information Retrieval do not allow to ex-
plicitly model query-oriented latent top-
ics. More, the semantic coherence of the
topics has never been considered in this
field. We propose a model-based feedback
approach that learns Latent Dirichlet Al-
location topic models on the top-ranked
pseudo-relevant feedback, and we mea-
sure the semantic coherence of those top-
ics. We perform a first experimental eval-
uation using two major TREC test collec-
tions. Results show that retrieval perfor-
mances tend to be better when using topics
with higher semantic coherence.

1 Introduction

Representing documents as mixtures of “topics”
has always been a challenge and an objective for
researchers working in text-related fields. Based
on the words used within a document, topic mod-
els learn topic level relations by assuming that the
document covers a small set of concepts. Learn-
ing the topics from a document collection can help
to extract high level semantic information, and
help humans to understand the meaning of doc-
uments. Latent Semantic Indexing (Deerwester
et al., 1990) (LSI), probabilistic Latent Seman-
tic Analysis (Hofmann, 2001) (pLSA) and Latent
Dirichlet Allocation (Blei et al., 2003) (LDA) are
the most famous approaches that tried to tackle
this problem throughout the years. Topics pro-
duced by these methods are generally fancy and
appealing, and often correlate well with human
concepts. This is one of the reasons of the inten-
sive use of topic models (and especially LDA) in
current research in Natural Language Processing
(NLP) related areas.

One main problem in ad hoc Information Re-
trieval (IR) is the difficulty for users to translate a

complex information need into a keyword query.
The most popular and effective approach to over-
come this problem is to improve the representa-
tion of the query by adding query-related “con-
cepts”. This approach mostly relies on pseudo-
relevance feedback, where these so-called “con-
cepts” are the most frequent words occurring in the
top-ranked documents retrieved by the retrieval
system (Lavrenko and Croft, 2001). From that
perspective, topic models seem attractive in the
sense that they can provide a descriptive and intu-
itive representation of concepts. But how can we
quantify the usefulness of these topics with respect
to an IR system? Recently, researchers developed
measures which evaluate the semantic coherence
of topic models (Newman et al., 2010; Mimno et
al., 2011; Stevens et al., 2012). We adopt their
view of semantic coherence and apply one of these
measures to query-oriented topics.

Several studies concentrated on improving the
quality of document ranking using topic models,
especially probabilistic ones. The approach by
Wei and Croft (2006) was the first to leverage
LDA topics to improve the estimate of document
language models and achieved good empirical re-
sults. Following this pioneering work, several
studies explored the use of pLSA and LDA un-
der different experimental settings (Park and Ra-
mamohanarao, 2009; Yi and Allan, 2009; Andrze-
jewski and Buttler, 2011; Lu et al., 2011). The re-
ported results suggest that the words and the prob-
ability distributions learned by probabilistic topic
models are effective for query expansion. The
main drawback of these approaches is that topics
are learned on the whole target document collec-
tion prior to retrieval, thus leading to a static top-
ical representation of the collection. Depending
on the query and on its specificity, topics may ei-
ther be too coarse or too fine to accurately rep-
resent the latent concepts of the query. Recently,
Ye et al. (2011) proposed a method which uses

148



LDA and learns topics directly on a limited set
of documents. While this approach is a first step
towards modeling query-oriented topics, it lacks
some theoretic principles and only aims to heuris-
tically construct a “best” topic (from all learned
topics) before expanding the query with its most
probable words. More, none of the aforemen-
tioned works studied the semantic coherence of
those generated topics. We tackle these issues by
making the following contributions:

• we introduce Topic-Driven Relevance Mod-
els, a model-based feedback approach (Zhai
and Lafferty, 2001) for integrating topic mod-
els into relevance models by learning topics
on pseudo-relevant feedback documents (as
opposed to the entire document collection),

• we explore the coherence of those generated
topics using the queries of two major and
well-established TREC test collections,

• we evaluate the effects coherent topics have
on ad hoc IR using the same test collections.

2 Topic-Driven Relevance Models

2.1 Relevance Models
The goal of relevance models is to improve
the representation of a query Q by selecting
terms from a set of initially retrieved docu-
ments (Lavrenko and Croft, 2001). As the concen-
tration of relevant documents is usually higher in
the top ranks of the ranking list, this is constituted
by a number N of top-ranked documents. Rele-
vance models usually perform better when com-
bined with the original query model (or maxi-
mum likelihood estimate). Let θ̃Q be this maxi-
mum likelihood query estimate and θ̂Q a relevance
model, the updated new query model is given by:

P (w|θQ) = λ P (w|θ̃Q) + (1− λ)P (w|θ̂Q) (1)

where λ ∈ [0, 1] is a parameter that controls the
tradeoff between the original query model and the
relevance model. One of the most robust variants
of the relevance models is computed as follows:

P (w|θ̂Q) ∝
∑

θD∈Θ
P (θD)P (w|θD)

∏

t∈Q
P (t|θD)

(2)
where Θ is a set of pseudo-relevant feedback doc-
uments and θD is the language model of document
D. This notion of estimating a query model is

often referred to as model-based feedback (Zhai
and Lafferty, 2001). We assume P (θD) to be uni-
form, resulting in an estimated relevance model
based on a sum of document models weighted
by the query likelihood score. The final, inter-
polated, estimate expressed in equation (1) is of-
ten referred in the literature as RM3. We tackle
the null probabilities problem by smoothing the
document language model using the well-known
Dirichlet smoothing (Zhai and Lafferty, 2004).

2.2 LDA-based Feedback Model
The estimation of the feedback model θ̂Q consti-
tutes the first contribution of this work. We pro-
pose to explicitly model the latent topics (or con-
cepts) that exist behind an information need, and
to use them to improve the query representation.
We consider Θ as the set of pseudo-relevant feed-
back documents from which the latent concepts
would be extracted. The retrieval algorithm used
to obtain these documents can be of any kind, the
important point is that Θ is a reduced collection
that contains the top documents ranked by an au-
tomatic and state-of-the-art retrieval process.

Instead of viewing Θ as a set of document lan-
guage models that are likely to contain topical in-
formation about the query, we take a probabilistic
topic modeling approach. We specifically focus
on Latent Dirichlet Allocation (LDA), since it is
currently one of the most representative. In LDA,
each topic multinomial distribution φk is gener-
ated by a conjugate Dirichlet prior with parame-
ter β, while each document multinomial distribu-
tion θd is generated by a conjugate Dirichlet prior
with parameter α. In other words, θd,k is the prob-
ability of topic k occurring in document D (i.e.
P (k|D)). Respectively, φk,w is the probability of
wordw belonging to topic k (i.e. P (w|k)). We use
variational inference implemented in the LDA-C
software1 to overcome intractability issues (Blei et
al., 2003; Griffiths and Steyvers, 2004). Under this
setting, we compute the topic-driven estimation of
the query model using the following equation:

P (w|θ̂Q) ∝
∑

θD∈Θ

(
P (θD)P (w|θD)

PTM (w|D)
∏

t∈Q
P (t|θD)

)
(3)

where PTM (w|D) is the probability of word w
occurring in document D using the previously

1www.cs.princeton.edu/˜blei/lda-c

149



5 10 20 30 40 50

9.
4

9.
6

9.
8

10
.0

10
.2

C
oh

er
en

ce

Number of feedback documents

Number of topics

3 5 10 15 20

WT10g

5 10 20 30 40 50

9.
4

9.
6

9.
8

10
.0

10
.2

C
oh

er
en

ce

Number of feedback documents

Number of topics

3 5 10 15 20

Robust04

Figure 1: Semantic coherence of the topic models for different values of K, in function of the number
N of feedback documents.

learned multinomial distributions. Let TΘ be a
topic model learned on the Θ set of feedback doc-
uments, this probability is given by:

PTM (w|D) =
∑

k∈TΘ
φk,w · θD,k (4)

High probabilities are thus given to words that are
important in topic k, when k is an important topic
in document D. In the remainder of this paper, we
refer to this general approach as TDRM for Topic-
Driven Relevance Models.

2.3 Measuring the coherence of
query-oriented topics

TDRM relies on two important parameters: the
number of topics K that we want to learn, and
the number of feedback documents N from which
LDA learns the topics. Varying these two param-
eters can help to capture more information and to
model finer topics, but how about their global se-
mantic coherence?

Term similarities measured in restricted do-
mains was the first step for evaluating seman-
tic coherence (Gliozzo et al., 2007), and was a
first basis for the development of several topic
coherence evaluation measures (Newman et al.,
2010). Computing the Pointwise Mutual Informa-
tion (PMI) of all word pairs over Wikipedia was
found to be an effective metric using news and
books corpora. Recently, Stevens et al. (2012)
used (among others) an aggregate version of this
metric to evaluate large amounts of topic models.
We use this method to evaluate the coherence of
query-oriented topics. Specifically, the coherence

of a topic model T KΘ composed of K topics is:

c(T KΘ ) =
1

K

K∑

i=1

∑

(w,w′)∈ki
log

P (w,w′) + �
P (w)P (w′)

(5)

where probabilities of word occurrences and co-
occurrences are estimated using an external refer-
ence corpus. Following Newman et al. (2010), we
use Wikipedia to compute PMI and set � = 1 as
in (Stevens et al., 2012).

3 Evaluation

3.1 Experimental setup
We performed our evaluation using two main
TREC2 collections: Robust04 and WT10g. Ro-
bust04 is composed 528,155 of news articles com-
ing from three newspapers and the FBIS. It sup-
ported the TREC 2004 Robust track, from which
we used the 250 query topics (numbers: 301-450,
601-700). The WT10g collection is composed of
1,692,096 web pages, and supported the TREC
Web track for four years (2001-2004). We focus
on the 2000 and 2001 ad-hoc query topics (num-
bers: 451-550). We used the open-source index-
ing and retrieval system Indri3 to run our exper-
iments. We indexed the two collections with the
exact same parameters: tokens were stemmed with
the well-known light Krovetz stemmer and stop-
words were removed using the standard English
stoplist embedded with Indri (417 words).

3.2 Semantic coherence evaluation
Most coherent topics are composed of rare words
that do not often occur in the reference corpus, but

2trec.nist.gov
3lemurproject.org/indri.php

150



0.
20

0
0.

20
5

0.
21

0
0.

21
5

0.
22

0

5 10 20 30 40 50

M
A

P

Number of feedback documents

●

●

●

●

●

●

●

●

● ●

●

●

●

●

Number of topics

3
5
10
15
20
RM3

WT10g

0.
26

0
0.

26
5

0.
27

0
0.

27
5

0.
28

0
0.

28
5

0.
29

0

5 10 20 30 40 50

M
A

P

Number of feedback documents

●

●

●

●

●

●

●

●

●

●

●

●

●

●

Number of topics

3
5
10
15
20
RM3

Robust04

Figure 2: Retrieval performance in terms of Mean Average Precision (MAP) of the TDRM approach.
Each line represent a different number of topics K, and the performance are reported in function the
number N of feedback documents. The black, plain line represents the RM3 baseline.

co-occur at lot together. We see on Figure 1 that
very coherent topics are identified in the top 5 and
10 feedback documents for the WT10g collection,
suggesting that closely related documents are re-
trieved in the top ranks. Results are quite different
on the Robust04 collection, where topic models
with 20 topics on 5 documents are the least co-
herent. However, when looking at the Robust04
documents, we see that they are on average almost
twice smaller than the WT10g web pages. We hy-
pothesize that the heterogeneous nature of the web
allows to model very different topics covering sev-
eral aspects of the query, while news articles are
contributions focused on a single subject.

Overall, the more coherent topic models contain
a reasonable amount of topics (10-15), thus allow-
ing to fit with variable amounts of documents. The
attentive reader will notice that the topic coher-
ence scores are very high compared to those pre-
viously reported in the literature (Stevens et al.,
2012). The TDRM approach captures topics that
are centered around a specific information need,
often with a limited vocabulary, which favors word
co-occurrence. On the other hand, topics learned
on entire collections are coarser than ours, which
leads to lower coherence scores.

3.3 Document retrieval results

Since TDRM is based on Relevance Mod-
els (Lavrenko and Croft, 2001), we take the RM3
approach presented in Section 2.1 as baseline. The
λ parameter is common between RM3 and TDRM
and is determined for each query using leave-
one-query-out cross-validation (that is: learn the

best parameter setting for all queries but one, and
evaluate the held-out query using the previously
learned parameter).

We report ad hoc document retrieval perfor-
mances in Figure 2. We noticed in the previous
section that the most coherent topic models were
modeled using 5 feedback documents and 20 top-
ics for the WT10g collection, and this parame-
ter combination also achieves the best retrieval re-
sults. Overall, using 10, 15 or 20 topics allow it
to achieve high and similar performance from 5 to
20 documents. We observe than using 20 topics
for the Robust04 collection consistently achieves
the highest results, with the topic model coherence
growing as the number of feedback documents in-
creases. Although topics coming from news ar-
ticles may be limited, they benefit from the rich
vocabulary of professional writers who are trained
to avoid repetition. Their use of synonyms allows
TDRM to model deep topics, with a comprehen-
sive description of query aspects. Since synonyms
are less likely to co-occur in encyclopedic articles
like Wikipedia, we think that, in our case, the se-
mantic coherence measure could be more accurate
using other textual resources. This measure seems
however to be effective when dealing with hetero-
geneously structured documents.

4 Conclusions & Future Work

Overall, modeling query-oriented topic models
and estimating the feedback query model using
these topics greatly improves ad hoc Information
Retrieval, compared to state-of-the-art relevance
models. While semantically coherent topic mod-

151



els do not seem to be effective in the context of a
news articles search task, they are a good indica-
tor of effectiveness in the context of web search.
Measuring the semantic coherence of query top-
ics could help predict query effectiveness or even
choose the best query-representative topic model.

Acknowledgments

This work was supported by the French Agency
for Scientific Research (Agence Nationale de
la Recherche) under CAAS project (ANR 2010
CORD 001 02).

References
David Andrzejewski and David Buttler. 2011. Latent

Topic Feedback for Information Retrieval. In Pro-
ceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ’11, pages 600–608.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Alfio Massimiliano Gliozzo, Marco Pennacchiotti, and
Patrick Pantel. 2007. The Domain Restriction Hy-
pothesis: Relating Term Similarity and Semantic
Consistency. In Human Language Technologies:
The 2007 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 131–138.

Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101 Suppl.

Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177–196.

Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-Based Language Models. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’01, pages 120–127.

Yue Lu, Qiaozhu Mei, and ChengXiang Zhai. 2011.
Investigating task performance of probabilistic topic
models: an empirical study of PLSA and LDA. In-
formation Retrieval, 14:178–203.

David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.

Optimizing Semantic Coherence in Topic Models.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 262–272.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic Evaluation of
Topic Coherence. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 100–108.

Laurence A. Park and Kotagiri Ramamohanarao. 2009.
The Sensitivity of Latent Dirichlet Allocation for In-
formation Retrieval. In Proceedings of the Euro-
pean Conference on Machine Learning and Knowl-
edge Discovery in Databases, ECML PKDD ’09,
pages 176–188.

Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring Topic
Coherence over Many Models and Many Topics.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 952–961.

Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’06, pages 178–185.

Zheng Ye, Jimmy Xiangji Huang, and Hongfei Lin.
2011. Finding a Good Query-Related Topic for
Boosting Pseudo-Relevance Feedback. JASIST,
62(4):748–760.

Xing Yi and James Allan. 2009. A Comparative Study
of Utilizing Topic Models for Information Retrieval.
In Proceedings of the 31th European Conference on
IR Research on Advances in Information Retrieval,
ECIR ’09, pages 29–41. Springer-Verlag.

Chengxiang Zhai and John Lafferty. 2001. Model-
based Feedback in the Language Modeling Ap-
proach to Information Retrieval. In Proceedings
of the Tenth International Conference on Informa-
tion and Knowledge Management, CIKM ’01, pages
403–410.

Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models Applied
to Information Retrieval. ACM Transactions on In-
formation Systems, 22(2):179–214.

152


