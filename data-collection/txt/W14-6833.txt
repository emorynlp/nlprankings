



















































NTOU Chinese Spelling Check System in CLP Bake-off 2014


Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 210–215,
Wuhan, China, 20-21 October 2014

NTOU Chinese Spelling Check System in CLP Bake-off 2014 

 
Wei-Cheng Chu and Chuan-Jie Lin  

Department of Computer Science and Engineering 
National Taiwan Ocean University 

No 2, Pei-Ning Road, Keelung 202, Taiwan R.O.C. 
{wcchu.cse, cjlin}@ntou.edu.tw 

 
  

 

Abstract 

 

This paper describes details of NTOU Chinese 
spelling check system participating in CLP-
2014 Bakeoff.  Confusion sets were expanded 
by using two language resources, Shuowen 
and Four-Corner codes.  A new method to find 
spelling errors in legal multi-character words 
was proposed.  Comparison of sentence gen-
eration probabilities is the main information 
for error detection and correction.  A rule-
based classifier and a SVM-based classifier 
were trained to identify spelling errors.  Two 
formal runs were submitted, and the rule-based 
classifier achieved better performance. 

 

1 Introduction 
Automatic spell checking is a basic and impor-
tant technique in building NLP systems.  It has 
been studied since 1960s as Blair (1960) and 
Damerau (1964) made the first attempt to solve 
the spelling error problem in English.  Spelling 
errors in English can be grouped into two classes: 
non-word spelling errors and real-word spelling 
errors. 

A non-word spelling error occurs when the 
written string cannot be found in a dictionary, 
such as in fly fron* Paris.  The typical approach 
is finding a list of candidates from a large dic-
tionary by edit distance or phonetic similarity 
(Mitten, 1996; Deorowicz and Ciura, 2005; Carl-
son and Fette, 2007; Chen et al., 2007; Mitten 
2008; Whitelaw et al., 2009). 

A real-word spelling error occurs when one 
word is mistakenly used for another word, such 
as in fly form* Paris.  Typical approaches in-
clude using confusion set (Golding and Roth, 
1999; Carlson et al., 2001), contextual informa-

tion (Verberne, 2002; Islam and Inkpen, 2009), 
and others (Pirinen and Linden, 2010; Amorim 
and Zampieri, 2013). 

Spelling error problem in Chinese is quite dif-
ferent.  Because there is no word delimiter in a 
Chinese sentence and almost every Chinese 
character can be considered as a one-character 
word, most of the errors are real-word errors. 

On the other hand, there is also an illegal-
character error where a hand-written symbol is 
not a legal Chinese character (thus not collected 
in a dictionary).  Such an error cannot happen in 
a digital document because all characters in Chi-
nese character sets such as BIG5 or Unicode are 
legal. 

There have been many attempts to solve the 
spelling error problem in Chinese (Chang, 1994; 
Zhang et al., 2000; Cucerzan and Brill, 2004; Li 
et al., 2006; Liu et al., 2008).  Among them, lists 
of visually and phonologically similar characters 
play an important role in Chinese spelling check 
(Liu et al., 2011). 

This bake-off is the second Chinese spell 
checking evaluation project.  It includes two sub-
tasks: error detection and error correction.  The 
task is organized based on some research works 
(Wu et al., 2010; Chen et al., 2011; Liu et al., 
2011). 
 

2 Replacement and Filtering 
Figure 1 shows the architecture of our Chinese 
spelling checking system.  A sentence under 
consideration is first word-segmented.  Candi-
dates of spelling errors are replaced by similar 
characters one by one.  The newly created sen-
tences are word segmented again.  They are 
sorted according to sentence generation prob-
abilities measured by word or POS bigram model.  
If a replacement results in a better sentence, 
spelling error is reported. 

210



In our experience, the confusion sets provided 
by the organizers do not cover all the cases in the 
development set.  Two sources used to expend 
confusion sets are described in Section 2.1. 

There are two kinds of spelling-error candi-
dates in our system: one-character words and 
multi-character words.  Their replacement proce-
dures are different, as described in Section 2.2 
and 2.3. 
 

2.1 Confusion set expansion 
In SIGHAN7 Bake-off 2013 Chinese Spelling 
Check task (Wu et al., 2013), the organizers pro-
vided two kinds of confusion sets, phonologi-
cally similar characters and visually similar char-
acters.  We adopted all these confusion sets ex-
cept the one consisting of characters having the 
same radical and the same number of strokes, 
because we do not think they are similar. 

However, these confusion sets do not cover all 
the spelling error cases in the training data.  We 
used two resources to expand the confusion sets.  
One is Showen and the other is the Four-Corner 
Encoding System. 

Shuowen Jieji1 (說文解字) is a dictionary of 
Chinese characters.  Xu Shen (許慎), author of 
this dictionary, analyzed the characters according 

                                                                                                 
1 說文解字  
http://zh.wikisource.org/wiki/說文解字 

to the six lexicographical categories (六書).  One 
major category is phono-semantic compound 
characters (形聲), which were created by com-
bining a radical (形符) with a phonetic compo-
nent (聲符).  We collect characters with same 
phonetic components to expand confusion sets, 
because they are by definition phonologically 
and visually similar.  For example, the following 
characters share the same phonetic component 
“寺” thus become confusion candidates (their 
actual pronunciation are given in brackets): 

Original sentence 

Segmented org sent 

  Replaced sentences 

  Segmented rpl sent 

Top 1 Result 

Word segmentation

Similar character 
replacement 

Word segmentation

Filtering rules; 
N-gram probabilities 

(words or POS)

Figure 1. Architecture of NTOU Chinese 
Spelling Check System 

侍[si4]持[chi2]恃[shi4]特[te4]時[shi2]... 

The Four-Corner System2 (四角號碼) is an en-
coding system for Chinese characters.  Digits 
0~9 represent some typical shapes in character 
strokes.  A Chinese character is encoded into 4 
digits which represent the shapes found in its 4 
corners.  We collect characters in the same Four-
Corner codes to expand confusion sets, because 
they are by definition visually similar.  For ex-
ample, the following characters are all encoded 
as 6080 in the Four-Corner System: 

只囚貝足炅是員異買圓圚 

 

2.2 One-character word replacement 
After doing word segmentation on the original 
sentence, every one-character word is considered 
as candidate where error occurs.  These candi-
dates are one-by-one replaced by similar charac-
ters in their confusion sets to see if a new sen-
tence is more acceptable. 

Taking C1-1701-2 in the test set as an exam-
ple.  The original sentence is 

...嬰兒個數卻特續下滑... 

and it is segmented as 

...嬰兒 個數 卻 特 續 下滑... 

“卻”, “特” and “續” are one-character words so 
they are candidates of spelling errors.  The con-
fusion set of the character “卻” includes 腳欲叩
卸... and the confusion set of the character “特” 
includes 持時恃峙侍 ...  Replacing these one-
character words with similar characters one-by-
one will produce the following new sentences. 

...嬰兒個數腳特續下滑... 

...嬰兒個數欲特續下滑... 

 
2 四腳號碼列表 
http://code.web.idv.hk/misc/four.php 

211



...嬰兒個數卻持續下滑... 

...嬰兒個數卻時續下滑... 

...... 

2.3 Multi-character word replacement 
Our observation on the training sets finds that 
some errors occur in multi-character words, 
which means that a string containing an incorrect 
character is also a legal word.  Examples are “身
手” (shen1-shou3, skills) versus “生手” (sheng1-
shou3, amateur), and “人員” (ren2-yuan2, mem-
ber) vs. “人緣” (ren2-yuan2, popularity). 

To handle such kinds of spelling errors, we 
created confusion sets for all known words by 
the following method.  The resource for creating 
word-level confusion set is Academia Sinica 
Balanced Corpus (ASBC for short hereafter, cf. 
馬偉雲 et al., 2001). 

For each word appearing in ASBC, each char-
acter in the word is substituted with its similar 
characters one by one.  If a newly created word 
also appears in ASBC, it is collected into the 
confusion set of this word.  Take the word “人
員” as an example. After replacing “人” or “員” 
with their similar characters, new strings 仁員, 
壬員, …, 人緣, and 人韻 are looked up in ASBC.  
Among them, only 人緣, 人猿, 人文, and 人俑
are legal words thus collected in 人員’s confu-
sion set. 

For each multi-character word, if it has a con-
fusion set, similar words in the set one-by-one 
substitute the original word to see if a new sen-
tence is more acceptable. 

Take ID=00058 in the Bakeoff 2013 CSC 
Datasets as an example.  The original sentence is 

... 在教室裡只要人員好... 

and it is segmented as 

... 在 教室 裡 只要 人員 好... 

where “教室”, “只要”, and “人員” are multi-
character words with confusion sets.  By replac-
ing 教室 with 教士, 教師…, replacing 只要 with 
祇要, 只有, and replacing 人員 with 人緣, 人
猿…, the following new sentences will be gener-
ated. 

... 在教士裡只要人員好...  

... 在教師裡只要人員好...  

... 在教室裡祇要人員好...  

... 在教室裡只要人緣好...  

... 在教室裡只要人猿好...  

2.4 Filtering rules 
Two filter rules are applied before error detection 
in order to discard apparently incorrect cases.  
The rules are defined as follows. 
 
Rule 1: No error in person names 

If a replacement results in a person name, dis-
card it.  Our word segmentation system performs 
named entity recognition at the same time.  If the 
replacing similar character can be considered as 
a Chinese family name, the consequent charac-
ters might be merged into a person name.  As 
most of the spelling errors do not occur in per-
sonal names, we simply ignore these replace-
ments.  Take C1-1701-2 as an example: 

...每 位 產 齡 婦女... 

“魏” is phonologically similar to “位” and is a 
Chinese family name.  The newly created sen-
tence is segmented as 

...每 魏產齡(PERSON) 婦女... 

where “魏產齡” is recognized as a person name.  
We will discard such a replacement. 
 
Rule 2: Stopword filtering 

For the one-character replacement, if the re-
placed (original) character is a personal anaphora 
(你 ‘you’ 我 ‘I’ 他 ‘he/she’) or numbers from 1 
to 10 (一二三四五六七八九十), discard the re-
placement.  We assume that a writer seldom mis-
spell such words.  Take B1-0122-2 as an exam-
ple: 

...我 會 在 二 號 出口 等 你... 

Although “二” is a one-character word, it is in 
our stoplist therefore no replacement is per-
formed on this word. 
 

3 Error Detection and Correction 
In our system, error detection and correction 
greatly rely on sentence generation probabilities.  
Therefore, all the newly created sentences should 
also be word segmented.  If a new sentence re-
sults in a better word segmentation, it is very 
likely that the original character is misused and 
this replacement is correct.  But if no replace-
ment is better than the original sentence, it is re-
ported as “no misspelling”. 

Three language models were used to measure 
sentence generation probabilities as described in 
Section 3.1.  Two formal runs were output of two 

212



different classifiers, SVM-based and rule-based 
systems, as described in Section 3.2 and 3.3. 

 

3.1 N-gram probabilities 
The possibility of a sequence of words can be 
measured as sentence generation probability by 
language models.  We used smoothed word-
unigram, word-bigram and POS-bigram models 
in our experiments.  The training corpus used to 
build language models is ASBC.  As usual, we 
use log probabilities instead. 

A basic hypothesis is that a “better” sentence 
often has higher probability than the original one.  
We define preference scores to capture such kind 
of features: 

( )
( ) 1)(Problog

)(Problog
),( −=

newM

orgM
orgnewM S

S
SSpref  (E1) 

where M is the language model (word-unigram 
model, etc.), Sorg is the original sentence, Snew is 
the new sentence, and Prob(s) is the generation 
probability of sentence s.  By this definition, a 
new sentence having higher probability than the 
original one will have a preference score larger 
than 0, and the higher the better. 
 

3.2 SVM-based classifier 
6 features defined in Table 1 were used to train a 
support vector machine classifier (Chang and Lin, 
2011).  Besides the preference scores of word-
unigram, word-bigram, and POS-bigram prob-
abilities, another kind of features reveals whether 
a new sentence has the highest preference score 
among all replacements. 

Unfortunately, the developed classifier tends 
to label all replacements as positive.  So we de-
fine a threshold so that the replacement is ac-
cepted only when SVM thinks the probability of 
assigning “positive” label is larger than 0.95. 

 
# Feature definition 
1 Preference score of word-unigram prob. 
2 Preference score of word-bigram prob. 
3 Preference score of POS-bigram prob. 
4 Is max of word-unigram prob. preference
5 Is max of word-bigram prob. preference 
6 Is max of POS-bigram prob. preference 

3.3 Rule-based classifier 
According to our hypothesis of error detection, a 
correct sentence should have a positive prefer-
ence score since it has higher generation prob-
ability.  Moreover, if many replacements have 
positive preference scores, the correct one should 
have the highest score. 

However, in our observations, sometimes re-
placing with a frequently-seen word may result 
in higher preference score, even if the replace-
ment is incorrect.  Therefore, we define three 
thresholds for each n-gram model, respectively, 
for stricter error detection.  Thresholds were 
trained by using Bakeoff 2013 CSC Datasets 
(Wu et al., 2013). 

The rules of detecting and correcting errors are 
defined as follows. 

1. If no replacement has positive preference 
scores, report “no error” in both error de-
tection and correction subtasks. 

2. Sort the replacements first by their word-
bigram preference scores, and then by their 
word-unigram preference scores, and then 
by the POS-bigram preference scores. 

3. If the top-1 replacement’s preference 
scores are all larger than the thresholds 
(0.004 for word-unigram, 0.03 for word-
bigram, and 0.001 for POS-bigram), report 
“with error” and output the replacing char-
acter and its location in the sentence as 
correction. 

4 Performance 
There are two judging correctness in this bake-
off: detection level and correction level. 

The metrics are evaluated in both levels by the 
following metrics: 

False-Positive Rate = FP / (FP+TN) 
Accuracy = (TP+TN) / (TP+TN+FP+FN) 
Precision = TP / (TP+FP) 
Recall = TP / (TP+FN) 
F1-Score= 2* Precision * Recall)/(Precision + Recall) 

We submitted 2 formal runs based on two differ-
ent classifiers.  The first run was output by the 
rule-based classifier and the second run was out-
put by the SVM-based classifier. 

Table 2 and 3 illustrate the evaluation results 
of formal runs.  As we can see, using the rule-
based classifier performed better than the SVM-
based classifier.  Unfortunately none of them 
could achieve acceptable performance. 

Table 1. Features for training SVM classifier 

 

213



Run FPAlarm Accuracy Precision Recall F1 
Formalrun1_NTOU 0.258 0.4652 0.4219 0.1883 0.2604 
Formalrun2_NTOU 

5 Conclusion 
In this year, we tried to expand confusion sets in 
order to obtain larger coverage of similar charac-
ters.  We also proposed a new method to find 
spelling errors in legal multi-character words.  
We submitted 2 formal runs based on the output 
of a rule-based classifier and a SVM-based clas-
sifier, respectively.  The evaluation results 
showed that the rule-based classifier outper-
formed the SVM-based classifier, but neither of 
them achieved acceptable performance. 

In the future, more features should be investi-
gated and more decision rules should be discov-
ered. 
 

References  
R.C. de Amorim and M. Zampieri. 2013. “Effective 

Spell Checking Methods Using Clustering Algo-
rithms,” Recent Advances in Natural Language 
Processing, 7-13. 

C. Blair. 1960. “A program for correcting spelling 
errors,” Information and Control, 3:60-67. 

A. Carlson, J. Rosen, and D. Roth. 2001. “Scaling up 
context-sensitive text correction,” Proceedings of 
the 13th Innovative Applications of Artificial Intel-
ligence Conference, 45-50. 

A. Carlson and I. Fette. 2007. “Memory-Based Con-
text-Sensitive Spelling Correction at Web Scale,” 
Proceedings of the 6th International Conference on 
Machine Learning and Applications, 166-171. 

C.C. Chang and C.J. Lin. 2011. “LIBSVM: a library 
for support vector machines,” ACM Transactions 
on Intelligent Systems and Technology, 2:27:1-27. 

C.H. Chang. 1994. “A pilot study on automatic chi-
nese spelling error correction,” Journal of Chinese 
Language and Computing, 4:143-149. 

Q. Chen, M. Li, and M. Zhou. 2007. “Improving 
Query Spelling Correction Using Web Search Re-
sults”, Proceedings of the 2007 Conference on 
Empirical Methods in Natural Language (EMNLP-
2007), 181-189. 

Y.Z. Chen, S.H. Wu, P.C. Yang, T. Ku, and G.D. 
Chen. 2011. “Improve the detection of improperly 
used Chinese characters in students’ essays with er-
ror model,” Int. J. Cont. Engineering Education 
and Life-Long Learning, 21(1):103-116. 

S. Cucerzan and E. Brill. 2004. “Spelling correction 
as an iterative process that exploits the collective 
knowledge of web users,” Proceedings of EMNLP, 
293-300. 

F. Damerau. 1964. “A technique for computer detec-
tion and correction of spelling errors.” Communi-
cations of the ACM, 7:171-176. 

S. Deorowicz and M.G. Ciura. 2005. “Correcting 
Spelling Errors by Modelling Their Causes,” Inter-
national Journal of Applied Mathematics and 
Computer Science, 15(2):275-285. 

A. Golding and D. Roth. 1999. “A winnow-based 
approach to context-sensitive spelling correction,” 
Machine Learning, 34(1-3):107-130. 

A. Islam and D. Inkpen. 2009. “Real-word spelling 
correction using googleweb 1t 3-grams,” Proceed-
ings of Empirical Methods in Natural Language 
Processing (EMNLP-2009), 1241-1249. 

M. Li, Y. Zhang, M.H. Zhu, and M. Zhou. 2006. “Ex-
ploring distributional similarity based models for 
query spelling correction,” Proceedings of the 21st 
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the ACL, 
1025-1032. 

W. Liu, B. Allison, and L. Guthrie. 2008. “Professor 
or screaming beast? Detecting words misuse in 
Chinese,” The 6th edition of the Language Re-
sources and Evaluation Conference. 

C.L. Liu, M.H. Lai, K.W. Tien, Y.H. Chuang, S.H. 
Wu, and C.Y. Lee. 2011. “Visually and phonologi-
cally similar characters in incorrect Chinese words: 
Analyses, identification, and applications,” ACM 
Transactions on Asian Language Information 
Processing, 10(2), 10:1-39. 

R. Mitton. 1996. English Spelling and the Computer, 
Harlow, Essex: Longman Group. 

R. Mitton. 2008. “Ordering the Suggestions of a 
Spellchecker Without Using Context,” Natural 
Language Engineering, 15(2):173-192. 

0.9925 0.1045 0.1688 0.2015 0.1837 
Table 2: Formal run performance in detection level. 

Run FPAlarm Accuracy Precision Recall F1 
Formalrun1_NTOU 0.258 0.4557 0.3965 0.1695 0.2375 
Formalrun2_NTOU 0.9925 0.0678 0.1143 0.1281 0.1208 

Table 3: Formal run performance in correction level. 

214



T. Pirinen and K. Linden. 2010. “Creating and 
weighting hunspell dictionaries as finite-state 
automata,” Investigationes Linguisticae, 21. 

S. Verberne. 2002. Context-sensitive spell checking 
based on word trigram probabilities, Master thesis, 
University of Nijmegen. 

C. Whitelaw, B. Hutchinson, G.Y. Chung, and G. 
Ellis. 2009. “Using the Web for Language Inde-
pendent Spellchecking and Autocorrection,” Pro-
ceedings Of Conference On Empirical Methods In 
Natural Language Processing (EMNLP-2009), 
890-899. 

S.H. Wu, Y.Z. Chen, P.C. Yang, T. Ku, and C.L. Liu. 
2010. “Reducing the False Alarm Rate of Chinese 
Character Error Detection and Correction,” Pro-
ceedings of CIPS-SIGHAN Joint Conference on 
Chinese Language Processing (CLP 2010), 54-61. 

S.H. Wu, C.L. Liu, and L.H. Lee. 2013. “Chinese 
Spelling Check Evaluation at SIGHAN Bake-off 
2013,” Proceedings of the 7th SIGHAN Workshop 
on Chinese Language Processing (SIGHAN'13), 
35-42. 

L. Zhang, M. Zhou, C.N. Huang, and H.H. Pan. 2000. 
“Automatic detecting/correcting errors in Chinese 
text by an approximate word-matching algorithm,” 
Proceedings of the 38th Annual Meeting on Asso-
ciation for Computational Linguistics, 248-254. 

馬偉雲, 謝佑明, 楊昌樺, 陳克健. 2001. “中文語料
庫構建及管理系統設計,” Proceedings of the 14th 
Conference on Computational Linguistics and 
Speech Processing (ROCLING 14), 1-17. 

215


