



















































An Infinite Hierarchical Bayesian Model of Phrasal Translation


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

An Infinite Hierarchical Bayesian Model of Phrasal Translation

Trevor Cohn
Department of Computer Science

The University of Sheffield
Sheffield, United Kingdom

t.cohn@sheffield.ac.uk

Gholamreza Haffari
Faculty of Information Technology

Monash University
Clayton, Australia

reza@monash.edu

Abstract

Modern phrase-based machine translation
systems make extensive use of word-
based translation models for inducing
alignments from parallel corpora. This
is problematic, as the systems are inca-
pable of accurately modelling many trans-
lation phenomena that do not decompose
into word-for-word translation. This pa-
per presents a novel method for induc-
ing phrase-based translation units directly
from parallel data, which we frame as
learning an inverse transduction grammar
(ITG) using a recursive Bayesian prior.
Overall this leads to a model which learns
translations of entire sentences, while also
learning their decomposition into smaller
units (phrase-pairs) recursively, terminat-
ing at word translations. Our experiments
on Arabic, Urdu and Farsi to English
demonstrate improvements over competi-
tive baseline systems.

1 Introduction

The phrase-based approach (Koehn et al., 2003)
to machine translation (MT) has transformed MT
from a narrow research topic into a truly useful
technology to end users. Leading translation sys-
tems (Chiang, 2007; Koehn et al., 2007; Marcu et
al., 2006) all use some kind of multi-word transla-
tion unit, which allows translations to be produced
from large canned units of text from the training
corpus. Larger phrases allow for the lexical con-
text to be considered in choosing the translation,
and also limit the number of reordering decisions
required to produce a full translation.

Word-based translation models (Brown et al.,
1993) remain central to phrase-based model train-
ing, where they are used to infer word-level align-
ments from sentence aligned parallel data, from

which phrasal translation units are extracted us-
ing a heuristic. Although this approach demon-
strably works, it suffers from a number of short-
comings. Firstly, many phrase-based phenomena
which do not decompose into word translations
(e.g., idioms) will be missed, as the underlying
word-based alignment model is unlikely to pro-
pose the correct alignments. Secondly, the rela-
tionship between different phrase-pairs is not con-
sidered, such as between single word translations
and larger multi-word phrase-pairs or where one
large phrase-pair subsumes another.

This paper develops a phrase-based translation
model which aims to address the above short-
comings of the phrase-based translation pipeline.
Specifically, we formulate translation using in-
verse transduction grammar (ITG), and seek to
learn an ITG from parallel corpora. The novelty
of our approach is that we develop a Bayesian
prior over the grammar, such that a nontermi-
nal becomes a ‘cache’ learning each production
and its complete yield, which in turn is recur-
sively composed of its child constituents. This is
closely related to adaptor grammars (Johnson et
al., 2007a), which also generate full tree rewrites
in a monolingual setting. Our model learns trans-
lations of entire sentences while also learning their
decomposition into smaller units (phrase-pairs) re-
cursively, terminating at word translations. The
model is richly parameterised, such that it can de-
scribe phrase-based phenomena while also explic-
itly modelling the relationships between phrase-
pairs and their component expansions, thus ame-
liorating the disconnect between the treatment of
words versus phrases in the current MT pipeline.
We develop a Bayesian approach using a Pitman-
Yor process prior, which is capable of modelling
a diverse range of geometrically decaying distri-
butions over infinite event spaces (here translation
phrase-pairs), an approach shown to be state of the
art for language modelling (Teh, 2006).

780



We are not the first to consider this idea; Neu-
big et al. (2011) developed a similar approach for
learning an ITG using a form of Pitman-Yor adap-
tor grammar. However Neubig et al.’s work was
flawed in a number of respects, most notably in
terms of their heuristic beam sampling algorithm
which does not meet either of the Markov Chain
Monte Carlo criteria of ergodicity or detailed bal-
ance. Consequently their approach does not con-
stitute a valid Bayesian model. In contrast, this
paper provides a more rigorous and theoretically
sound method. Moreover our approach results in
consistent translation improvements across a num-
ber of translation tasks compared to Neubig et al.’s
method, and a competitive phrase-based baseline.

2 Related Work

Inversion transduction grammar (or ITG) (Wu,
1997) is a well studied synchronous grammar for-
malism. Terminal productions of the form X →
e/f generate a word in two languages, and non-
terminal productions allow phrasal movement in
the translation process. Straight productions, de-
noted by their non-terminals inside square brack-
ets [...], generate their symbols in the given or-
der in both languages, while inverted productions,
indicated by angled brackets 〈...〉, generate their
symbols in the reverse order in the target language.

In the context of machine translation, ITG
has been explored for statistical word alignment
in both unsupervised (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Zhang et al., 2008; Pauls et
al., 2010) and supervised (Haghighi et al., 2009;
Cherry and Lin, 2006) settings, and for decoding
(Petrov et al., 2008). Our paper fits into the re-
cent line of work for jointly inducing the phrase ta-
ble and word alignment (DeNero and Klein, 2010;
Neubig et al., 2011). The work of DeNero and
Klein (2010) presents a supervised approach to
this problem, whereas our work is unsupervised
hence more closely related to Neubig et al. (2011)
which we describe in detail below.

A number of other approaches have been de-
veloped for learning phrase-based models from
bilingual data, starting with Marcu and Wong
(2002) who developed an extension to IBM model
1 to handle multi-word units. This pioneer-
ing approach suffered from intractable inference
and moreover, suffers from degenerate solutions
(DeNero and Klein, 2010). Our approach is simi-
lar to these previous works, except that we impose

additional constraints on how phrase-pairs can be
tiled to produce a sentence pair, and moreover,
we seek to model the embedding of phrase-pairs
in one another, something not considered by this
prior work. Another strand of related research is
in estimating a broader class of synchronous gram-
mars than ITGs, such as SCFGs (Blunsom et al.,
2009b; Levenberg et al., 2012). Conceptually, our
work could be readily adapted to general SCFGs
using similar techniques.

This work was inspired by adaptor grammars
(Johnson et al., 2007a), a monolingual grammar
formalism whereby a non-terminal rewrites in a
single step as a complete subtree. The model prior
allows for trees to be generated as a mixture of a
cache and a base adaptor grammar. In our case,
we have generalised to a bilingual setting using an
ITG. Additionally, we have extended the model to
allow recursive nesting of adapted non-terminals,
such that we end up with an infinitely recursive
formulation where the top-level and base distribu-
tions are explicitly linked together.

As mentioned above, ours is not the first work
attempting to generalise adaptor grammars for ma-
chine translation; (Neubig et al., 2011) also devel-
oped a similar approach based around ITG using a
Pitman-Yor Process prior. Our approach improves
upon theirs in terms of the model and inference,
and critically, this is borne out in our experiments
where we show uniform improvements in transla-
tion quality over a baseline system, as compared
to their almost entirely negative results. We be-
lieve that their approach had a number of flaws:
For inference they use a beam-search, which may
speed up processing but means that they are no
longer sampling from the true distribution, nor a
distribution with the same support as the posterior.
Moreover they include a Metropolis-Hastings cor-
rection step, which is required to correct the sam-
ples to account for repeated substructures which
will be otherwise underrepresented. Consequently
their approach does not constitute a Markov Chain
Monte Carlo sampler, but rather a complex heuris-
tic.

The other respect in which this work differs
from Neubig et al. (2011) is in terms of model for-
mulation. They develop an ITG which generates
phrase-pairs as terminals, while we employ a more
restrictive word-based model which forces the de-
composition of every phrase-pair. This is an im-
portant restriction as it means that we jointly learn

781



a word and phrase based model, such that word
based phenomena can affect the phrasal struc-
tures. Finally our approach models separately the
three different types of ITG production (mono-
tone, swap and lexical emission), allowing for a
richer parameterisation which the model exploits
by learning different hyper-parameter values.

3 Model

The generative process of the model follows that
of ITG with the following simple grammar

X → [X X] | 〈X X〉
X → e/f | e/⊥ | ⊥/f ,

where [·] denotes monotone ordering and 〈·〉 de-
notes a swap in one language. The symbol ⊥ de-
notes the empty string. This corresponds to a sim-
ple generative story, with each stage being a non-
terminal rewrite starting with X and terminating
when there are no frontier non-terminals.

A popular variant is a phrasal ITG, where the
leaves of the ITG tree are phrase-pairs and the
training seeks to learn a segmentation of the source
and target which yields good phrases. We would
not expect this model to do very well as it cannot
consider overlapping phrases, but instead is forced
into selecting between many competing – and of-
ten equally viable – options. Our approach im-
proves over the phrasal model by recursively gen-
erating complete phrases. This way we don’t insist
on a single tiling of phrases for a sentence pair, but
explicitly model the set of hierarchically nested
phrases as defined by an ITG derivation. This ap-
proach is closer in spirit to the phrase-extraction
heuristic, which defines a set of ‘atomic’ terminal
phrase-pairs and then extracts every combination
of these atomic phase-pairs which is contiguous in
the source and target.1

The generative process is that we draw a com-
plete ITG tree, t ∼ P2(·), as follows:

1. choose the rule type, r ∼ R, where r ∈
{mono, swap, emit}

2. for r = mono
(a) draw the complete subtree expansion,

t = X → [. . .] ∼ TM
3. for r = swap

(a) draw the complete subtree expansion,
t = X → 〈. . .〉 ∼ TS

1Our technique considers the subset of phrase-pairs which
are consistent with the ITG tree.

4. for r = emit
(a) draw a pair of strings, (e, f) ∼ E
(b) set t = X → e/f

Note that we split the problem of drawing a tree
into two steps: first choosing the top-level rule
type and then drawing a rule of that type. This
gives us greater control than simply drawing a tree
of any type from one distribution, due to our pa-
rameterisation of the priors over the model param-
eters TM , TS and E.

To complete the generative story, we need to
specify the prior distributions for TM , TS and
E. First, we deal with the emission distribu-
tion, E which we drawn from a Dirichlet Pro-
cess prior E ∼ DP(bE , P0). We restrict the emis-
sion rules to generate word pairs rather than phrase
pairs.2 For the base distribution, P0, we use a sim-
ple uniform distribution over word pairs,

P0(e, f) =





η2 1VEVF e 6= ⊥, f 6= ⊥
η(1− η) 1VF e = ⊥, f 6= ⊥
η(1− η) 1VE e 6= ⊥, f = ⊥

,

where the constant η denotes the binomial proba-
bility of a word being aligned.3

We use Pitman-Yor Process priors for the TM
and TS parameters

TM ∼ PYP(aM , bM , P1(·|r = mono))
TS ∼ PYP(aS , bS , P1(·|r = swap))

where P1(t1, t2|r) is a distribution over a pair of
trees (the left and right children of a monotone or
swap production). P1 is defined as follows:

1. choose the complete left subtree t1 ∼ P2,
2. choose the complete right subtree t2 ∼ P2,
3. set t = X → [t1 t2] or t = X → 〈t1 t2〉

depending on r
This generative process is mutually recursive: P2
makes draws from P1 and P1 makes draws from
P2. The recursion is terminated when the rule type
r = emit is drawn.

Following standard practice in Bayesian mod-
els, we integrate out R, TM , TS and E. This
means draws from P2 (or P1) are no longer iid:
for any non-trivial tree, computing its probabil-
ity under this model is complicated by the fact

2Note that we could allow phrases here, but given the
model can already reason over phrases by way of its hier-
archical formulation, this is an unnecessary complication.

3We also experimented with using word translation prob-
abilities from IBM model 1, based on the prior used by Lev-
enberg et al. (2012), however we found little empirical differ-
ence compared with this simpler uniform model.

782



that the probability of its two subtrees are inter-
dependent. This is best understood in terms of
the Chinese Restaurant Franchise (CRF; Teh et al.
(2006)), which describes the posterior distribution
after integrating out the model parameters. In our
case we can consider the process of drawing a tree
from P2 as a customer entering a restaurant and
choosing where to sit, from an infinite set of ta-
bles. The seating decision is based on the number
of other customers at each table, such that popular
tables are more likely to be joined than unpopular
or empty ones. If the customer chooses an occu-
pied table, the identity of the tree is then set to
be the same as for the other customers also seated
there. For empty tables the tree must be sampled
from the base distribution P1. In the standard CRF
analogy, this leads to another customer entering
the restaurant one step up in the hierarchy, and
this process can be chained many times. In our
case, however, every new table leads to new cus-
tomers reentering the original restaurant – these
correspond to the left and right child trees of a
monotone or swap rule. The recursion terminates
when a table is shared, or a new table is labelled
with a emit rule.

3.1 Inference

The probability of a tree (i.e., a draw from P2) un-
der the model is

P2(t) = P (r)P2(t|r) (1)

where r is the rule type, one of mono, swap or
emit. The distribution over types, P (r), is de-
fined as

P (r) =
nT,−r + bT 13
nT,− + bT

where nT,− are the counts over rules of types.4

The second component in (1), P2(t|r), is de-
fined separately for each rule type. For r = mono
or r = swap rules, it is defined as

P2(t|r) =
n−t,r −K−t,rar
n−r + br

+
K−r ar + br
n−r + br

P1(t1, t2|r) ,
(2)

where n−t,r is the count for tree t in the other train-
ing sentences, K−t,r is the table count for t and n

−
r

4The conditioning on event and table counts, n−,K− is
omitted for clarity.

and K−r are the total count of trees and tables, re-
spectively. Finally, the probability for r = emit
is given by

P2(t|r = emit) =
n−t,E + bEP0(e, f)

n−r + br
,

where t = X → e/f .
To complete the derivation we still need to de-

fine P1, which is formulated as

P1(t1, t2) = P2(t1)P2(t2|t1) ,

where the conditioning of the second recursive call
to P2 reflects that the counts n− and K− may
be affected by the first draw from P2. Although
these two draws are assumed iid in the prior, after
marginalising out T they are no longer indepen-
dent. For this reason, evaluating P2(t) is computa-
tionally expensive, requiring tracking of repeated
substructures in descendent sub-trees of t, which
may affect other descendants. This results in an
asymptotic complexity exponential in the number
of nodes in the tree. For this reason we consider
trees annotated with binary values denoting their
table assignment, namely whether they share a ta-
ble or are seated alone. Given this, the calculation
is greatly simplified, and has linear complexity.5

We construct an approximating ITG following
the technique used for sampling trees from mono-
lingual tree-substitution grammars (Cohn et al.,
2010). To do so we encode the first term from
(2) separately from the second term (correspond-
ing to draws from P1). Summing together these
two alternate paths – i.e., during inside inference –
we recover P2 as shown in (2). The full grammar
transform for inside inference is shown in Table 1.

The sampling algorithm closely follows the
process for sampling derivations from Bayesian
PCFGs (Johnson et al., 2007b). For each sentence-
pair, we first decrement the counts associated with
its current tree, and then sample a new deriva-
tion. This involves first constructing the inside
lattice using the productions in Table 1, and then
performing a top-down sampling pass. After
sampling each derivation from the approximating
grammar, we then convert this into its correspond-
ing ITG tree, which we then score with the full
model and accept or reject the sample using the

5To support this computation, we track explicit table as-
signments for every training tree and their component sub-
trees. We also sample trees labelled with seating indicator
variables.

783



Ty
pe

X →M P (r = mono)
X → S P (r = swap)
X → E P (r = emit)

B
as

e M → [XX]
K−MaM+bM
n−M+bM

S → 〈XX〉 K
−
S aS+bS

n−S+bS

C
ou

nt

For every tree, t, of type r = mono, with nt,M > 0:

M → sig(t) n
−
t,M−K

−
t,Mar

n−M+bM

sig(t)→ yield(t) 1
For every tree, t, of type r = swap, with nt,S > 0:

S → sig(t) n
−
t,S−K

−
t,SaS

n−S+bS

sig(t)→ yield(t) 1

E
m

it For every word pair, e/f in sentence pair,
where one of e, f can be ⊥:

E → e/f P2(t)

Table 1: Grammar transformation rules for MAP
inside inference. The function sig(t) returns a
unique identifier for the complete tree t, and
the function yield(t) returns the pair of terminal
strings from the yield of t.

Metropolis-Hastings algorithm.6 Accepted sam-
ples then replace the old tree (otherwise the old
tree is retained) and the model counts are incre-
mented. This process is then repeated for each
sentence pair in the corpus in a random order.

4 Experiments

Datasets We train our model across three
language pairs: Urdu→English (UR-EN),
Farsi→English (FA-EN), and Arabic→English
(AR-EN). The corpora statistics of these trans-
lation tasks are summarised in Table 2. The
UR-EN corpus comes from NIST 2009 translation
evaluation.7 The AR-EN training data consists
of the eTIRR corpus (LDC2004E72), the Ara-
bic news corpus (LDC2004T17), the Ummah
corpus (LDC2004T18), and the sentences with
confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For FA-EN, we use TEP8 Tehran English-Persian
Parallel corpus (Pilevar and Faili, 2011), which
consists of conversational/informal text extracted

6The full model differs from the approximating grammar
in that it accounts for inter-dependencies between subtrees
by recursively tracking the changes in the customer and table
counts while scoring the tree. Around 98% of samples were
accepted in our experiments.

7http://www.itl.nist.gov/iad/mig/tests/mt/2009
8http://ece.ut.ac.ir/NLP/resources.htm

source target sentences
UR-EN 745K 575K 148K
FA-EN 4.7M 4.4M 498K
AR-EN 1.94M 2.08M 113K

Table 2: Corpora statistics showing numbers of
parallel sentences and source and target words for
the training sets.

from 1600 movie subtitles. We tokenized this
corpus, removed noisy single-word sentences,
randomly selected the development and test sets,
and used the rest of the corpus as the training set.
We discard sentences with length above 30 from
the datasets for all experiments.9

Sampler configuration Samplers are initialised
with trees created from GIZA++ alignments
constructed using a SCFG factorisation method
(Blunsom et al., 2009a). This algorithm repre-
sents the translation of a sentence as a large SCFG
rule, which it then factorises into lower rank SCFG
rules, a process akin to rule binarisation com-
monly used in SCFG decoding. Rules that can-
not be reduced to a rank-2 SCFG are simplified
by dropping alignment edges until they can be
factorised, the net result being an ITG derivation
largely respecting the alignments.10

The blocked sampler was run 1000 iterations
for UR-EN, 100 iterations for FA-EN and AR-
EN. After each full sampling iteration, we resam-
ple all the hyper-parameters using slice-sampling,
with the following priors: a ∼ Beta(1, 1),
b ∼ Gamma(10, 0.1). Figure 1 shows the poste-
rior probability improves with each full sampling
iterations. The alignment probability was set to
η = 0.99. The sampling was repeated for 5 in-
dependent runs, and we present results where we
combine the outputs of these runs. This is a form
of Monte Carlo integration which allows us to rep-
resent the uncertainty in the posterior, while also
representing multiple modes, if present.

The time complexity of our inference algorithm
is O(n6), which can be prohibitive for large scale
machine translation tasks. We reduce the com-
plexity by constraining the inside inference to
consider only derivations which are compatible

9Hence the BLEU scores we get for the baselines may
appear lower than what reported in the literature.

10Using the factorised alignments directly in a translation
system resulted in a slight loss in BLEU versus using the un-
factorised alignments. Our baseline system uses the latter.

784



0 100 200 300 400 500

−
91

00
00

0
−

90
50

00
0

−
90

00
00

0
−

89
50

00
0

iteration

lo
g 

po
st

er
io

r

Figure 1: Training progress on the UR-EN corpus,
showing the posterior probability improving with
each full sampling iteration. Different colours de-
note independent sampling runs.

●

●

●

●

●

●

●

●

●

●
●●

●

●

●
●

●

●
●●

●

●

●

●
●

●
●

●

●

●
●

●
●

●

●

●

●
●

●
●

●
●

●

●

●
●

●
●

●
●●

●

●

●●

●●
●

●
●

●

●
●

●

●
●

●
●

●
●●

●

●
●

●
●

●
●

●
●

●
●

●

●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●
●

●●
●●

●

●
●

●
●

●
●

●
●

●
●

●●
●

●

●

●
●

●
●

●
●

●
●

●
●●

●●
●

●
●

●
●

●
●

●
●

●
●●

●●
●●

●

●
●

●
●

●
●

●
●

●
●

●●
●●

●●
●

●
●

●
●

●
●

●
●

●
●

●●
●●

●●
●●

●●
●

●
●

●
●

●
●

●●
●●

●●
●●

●●
●

●

●
●

●
●

●
●

●
●

●●
●●

●●
●●

●●

●●
●

●
●

●
●

●
●

●●
●●

●●
●●

●●
●●

●

●

●
●

●
●

●
●

●
●●

●●
●●

●●
●●

●●
●

●
●

●
●

●
●

●
●

●●
●

●●
●●

●●
●●

●
●

●

●
●

●
●

●
●

●●
●●

●
●●

●●
●●

●●

●
●

●
●

●
●

●
●

●
●

●
●

●●
●●

●●
●●

●

●

●
●

●
●

●
●

●
●

●
●

●
●●

●●
●●

●●
●

●
●

●
●

●
●

●
●

●
●

●●
●●

●●
●●

●

●
●

●
●

●
●

●
●

●
●

●●
●●

●●
●●

●

●
●

●
●

●
●

●
●

●
●●

●●
●●

●●
●

●

●
●

●
●

●
●

●
●

●
●●

●●
●●

●●
●●

0 5 10 15 20 25 30

1e
−

05
1e

−
03

1e
−

01

average sentence length

tim
e 

(s
)

Figure 2: The runtime cost of bottom-up inside in-
ference and top-down sampling as a function of
sentence length (UR-EN), with time shown on a
logarithmic scale. Full ITG inference is shown
with red circles, and restricted inference using the
intersection constraints with blue triangles. The
average time complexity for the latter is roughly
O(l4), as plotted in green t = 2× 10−7l4.

with high confidence alignments from GIZA++.11

Figure 2 shows the sampling time with respect
to the average sentence length, showing that our
alignment-constrained sampling algorithm is bet-
ter than the unconstrained algorithm with empir-
ical complexity of n4. However, the time com-
plexity is still high, so we set the maximum sen-
tence length to 30 to keep our experiments practi-
cable. Presumably other means of inference may
be more efficient, such as Gibbs sampling (Lev-
enberg et al., 2012) or auxiliary variable sampling
(Blunsom and Cohn, 2010); we leave these exten-
sions to future work.

Baselines. Following (Levenberg et al., 2012;
Neubig et al., 2011), we evaluate our model by
using its output word alignments to construct a
phrase table. As a baseline, we train a phrase-
based model using the moses toolkit12 based on
the word alignments obtained using GIZA++ in
both directions and symmetrized using the grow-
diag-final-and heuristic13 (Koehn et al., 2003).
This alignment is used as input to the rule fac-
torisation algorithm, producing the ITG trees with
which we initialise our sampler. To put our results
in the context of the previous work, we also com-
pare against pialign (Neubig et al., 2011), an ITG
algorithm using a Pitman-Yor process prior, as de-
scribed in Section 2.14

In the end-to-end MT pipeline we use a stan-
dard set of features: relative-frequency and lexical
translation model probabilities in both directions;
distance-based distortion model; language model
and word count. We set the distortion limit to
6 and max-phrase-length to 7 in all experiments.
We train 3-gram language models using modified
Kneser-Ney smoothing. For AR-EN experiments
the language model is trained on English data as
(Blunsom et al., 2009a), and for FA-EN and UR-
EN the English data are the target sides of the
bilingual training data. We use minimum error
rate training (Och, 2003) with nbest list size 100
to optimize the feature weights for maximum de-
velopment BLEU.

11These are taken from the final model 4 word alignments,
using the intersection of the source-target and target-source
models. These alignments are very high precision (but have
low recall), and therefore are unlikely to harm the model.

12http://www.statmt.org/moses
13We use the default parameter settings in both moses and

GIZA++.
14http://www.phontron.com/pialign

785



Baselines This paper
GIZA++ pialign individual combination

UR-EN 16.95 15.65 16.68 ± .12 16.97
FA-EN 20.69 21.41 21.36 ± .17 21.50

AR-EN

MT03 44.05 43.30 44.8 ± .28 45.10
MT04 38.15 37.78 38.4 ± .08 38.4
MT05 42.81 42.18 43.13 ± .23 43.45
MT08 32.43 33.00 32.7 ± .15 32.80

Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show
the average and 95% confidence intervals for 5 independent runs, whereas the combination column show
the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and
those generated by the pialign (Neubig et al., 2011) bold: the best result.

1 2 5 10 20 50 100

1e
−

05
1e

−
03

1e
−

01

rule frequency

fr
ac

tio
n 

of
 g

ra
m

m
ar

monotone
swap
emit

Figure 3: Fraction of rules with a given frequency,
using a single sample grammar (UR-EN).

4.1 Results

Table 3 shows the BLEU scores for the three trans-
lation tasks UR/AR/FA→EN based on our method
against the baselines. For our models, we report
the average BLEU score of the 5 independent runs
as well as that of the aggregate phrase table gen-
erated by these 5 independent runs. There are
a number of interesting observations in Table 3.
Firstly, combining the phrase tables from indepen-
dent runs results in increased BLEU scores, possi-
bly due to the representation of uncertainty in the
outputs, and the representation of different modes
captured by the individual models. We believe this
type of Monte Carlo model averaging should be
considered in general when sampling techniques
are employed for grammatical inference, e.g. in
parsing and translation. Secondly, our approach
consistently improves over the Giza++ baseline
often by a large margin, whereas pialign under-

performs the GIZA++ baseline in many cases.
Thirdly, our model consistently outperforms pi-
align (except in AR-EN MT08 which is very
close). This highlights the modeling and inference
differences between our method and the pialign.

5 Analysis

In this section, we present some insights about the
learned grammar and the model hyper-parameters.
Firstly, we start by presenting various statistics
about different learned grammars. Figure 3 shows
the fraction of rules with a given frequency for
each of the three rule types. The three types of rule
exhibit differing amounts of high versus low fre-
quency rules, and all roughly follow power laws.
As expected, there is a higher tendency to reuse
high-frequency emissions (or single-word transla-
tion) compared to other rule types, which are the
basic building blocks to compose larger rules (or
phrases). Table 4 lists the high frequency mono-
tone and swap rules in the learned grammar. We
observe the high frequency swap rules capture re-
ordering in verb clusters, preposition-noun inver-
sions and adjective-noun reordering. Similar pat-
terns are seen in the monotone rules, along with
some common canned phrases. Note that “in Iraq”
appears twice, once as an inversion in UR-EN and
another time in monotone order for AR-EN.

Secondly, we analyse the values learned for
the model hyper-parameters; Figure 4.(a) shows
the posterior distribution over the hyper-parameter
values. There is very little spread in the inferred
values, suggesting the sampling chains may have
converged. Furthermore, there is a large differ-
ence between the learned hyper-parameters for the
monotone rules versus the swap rules. For the
Pitman-Yor Process prior, the values of the hyper-

786



} $ $ $ $
}
$ $ $

$ $ } $
} } $

Table 5: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables
coming from our method vs that of pialign for FA-EN and AR-EN translation tasks.

parameters affects the rate at which the number of
types grows compared to the number of tokens.
Specifically, as the discount a or the concentra-
tion b parameters increases we expect for a rela-
tive increase in the number of types. If the number
of observed monotone and swap rules were equal,
then there would be a higher chance in reusing the
monotone rules. However, the number of observed
monotone and swap rules are not equal, as plotted
in Figure 4.(b). Similar results were observed for
the other language pairs (figures omitted for space
reasons).

Thirdly, we performed a manual evaluation for
the quality of the phrase-pairs learned exclusively
by our method vs pialign. For each method,
we considered the top-100 high frequency phrase-
pairs which are specific to that method. Then we
asked a bilingual human expert to identify rea-
sonably well phrase-pairs among these top-100
phrase-pairs. The results are summarized in Ta-
ble 5, and show that we learn roughly twice as
many reasonably good phrase-pairs for AR-EN
and FA-EN compared to pialign.

Conclusions

We have presented a novel method for learn-
ing a phrase-based model of translation directly
from parallel data which we have framed as learn-
ing an inverse transduction grammar (ITG) us-
ing a recursive Bayesian prior. This has led
to a model which learns translations of en-
tire sentences, while also learning their decom-
position into smaller units (phrase-pairs) recur-
sively, terminating at word translations. We have
presented a Metropolis-Hastings sampling algo-
rithm for blocked inference in our non-parametric
ITG. Our experiments on Urdu-English, Arabic-
English, and Farsi-English translation tasks all
demonstrate improvements over competitive base-
line systems.

Acknowledgements

The first author was supported by the EPSRC
(grant EP/I034750/1) and an Erasmus-Mundus
scholarship funding a research visit to Melbourne.
The second author was supported by an early ca-
reer research award from Monash University.

787



0.905 0.910 0.915 0.920 0.925

0
20

0
40

0
60

0
80

0
10

00

am and as

D
en

si
ty

1000 2000 3000 4000

0.
00

00
0.

00
10

0.
00

20

bm and bs

D
en

si
ty

0 5 10 15 20 25 30

0.
00

0.
01

0.
02

0.
03

0.
04

0.
05

0.
06

be

D
en

si
ty

65000 65500 66000

0.
00

00
0.

00
05

0.
00

10
0.

00
15

bt

D
en

si
ty

(a)

291000 292000 293000

0.
00

00
0.

00
04

0.
00

08
0.

00
12

monotone

176000 177000

0.
00

00
0.

00
04

0.
00

08
0.

00
12

swap

D
en

si
ty

(b)

Figure 4: (a) Posterior over the hyper-parameters,
aM , aS , bM , bS , bE , bT , measured for UR-EN us-
ing samples 400–500 for 3 independent sampling
chains, and the intersection constraints. (b) Poste-
rior over the number of monotone and swap rules
in the resultant grammars. The distribution for
emission rules was also peaked about 147k rules.

⊥
〈 ⊥ 〉

⊥

⊥
〈 ⊥ 〉

〈 〉
〈 〉
〈 〉
〈 〉
〈 〉

〈 ⊥ 〉

⊥ $ ⊥

〈 $ 〉
〈 〉
〈 ⊥ 〉
〈 〉
〈 〉

〈 〉
〈 〉

〈 〉
〈 〉
〈 〉

Table 4: Top 5 monotone and swap productions
and their counts. Rules with mostly punctuation
or encoding 1:many or many:1 alignments were
omitted.

788



References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-

chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238–241,
Los Angeles, California, June. Association for Com-
putational Linguistics.

Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009a. A Gibbs sampler for phrasal syn-
chronous grammar induction. In ACL2009, Singa-
pore, August.

Phil Blunsom, Trevor Cohn, and Miles Osborne.
2009b. Bayesian synchronous grammar induction.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 161–168. MIT Press.

P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311.

Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of COLING/ACL. As-
sociation for Computational Linguistics.

Colin Cherry and Dekany Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proc. of the HLT-NAACL Workshop on
Syntax and Structure in Statistical Translation (SSST
2007), Rochester, USA.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053–3096.

John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In The 48th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL).

Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, Suntec, Singapore. Association for
Computational Linguistics.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007a. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In B. Schölkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.

Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007b. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139–146.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003), pages 81–88,
Edmonton, Canada, May.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-
2007), Prague.

Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223–232, Jeju Island, Korea, July.
Association for Computational Linguistics.

Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proc. of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), pages 133–139, Philadelphia, July.
Association for Computational Linguistics.

Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 44–52, Sydney, Australia, July.

Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In The 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 632–641,
Portland, Oregon, USA, 6.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160–
167, Sapporo, Japan.

Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment
with inversion transduction grammars. In Proceed-
ings of the North American Conference of the Asso-
ciation for Computational Linguistics (NAACL). As-
sociation for Computational Linguistics.

789



Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Association for Computational Linguistics.

M. T. Pilevar and H. Faili. 2011. Tep: Tehran english-
persian parallel corpus. In Proc. International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing).

Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566–1581.

Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
985–992.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). Association for Computational Linguis-
tics.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. of the 46th Annual Conference of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), pages 97–105,
Columbus, Ohio, June.

790


