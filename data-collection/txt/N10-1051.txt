










































Crowdsourcing the evaluation of a domain-adapted named entity recognition system


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345–348,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Crowdsourcing the evaluation of a domain-adapted named entity
recognition system

Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek

Department of Computer Science
University of Maryland

College Park, MD 20742
asayeed@cs.umd.edu,
tmeyer1@umd.edu,

{hcnguyen88,olivia.buzek}
@gmail.com

Amy Weinberg
Department of Linguistics

University of Maryland
College Park, MD 20742

weinberg@umiacs.umd.edu

Abstract

Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.

1 Introduction

1.1 Named entities and errors
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.

Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social

network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.

Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.

Machine learning is currently used extensively in
building NER systems. One such system is BBN’s
Identifinder (Bikel et al., 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.

The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system’s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-

345



cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.

1.2 Crowdsourcing

Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al., 2008; Callison-Burch, 2009).

The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.

Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.

2 Methodology

2.1 Process overview

The overall process for running this experiment was
as follows (figure 1).

Figure 1: Diagram of data pipeline.

First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-

neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.

Next, we constructed an Amazon Mechanical
Turk-based interface for naı̈ve web users or “Turk-
ers” to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.

We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.

2.2 Performance evaluation
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.

As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.

3 Experiments and results

Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.

3.1 Baseline performance assessment
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.

After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:

346



Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85

Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.

• IdentiFinder tags words that are simply not
named entities.

• IdentiFinder assigns the wrong category (per-
son or organization) to an entity.

• IdentiFinder includes extraneous words in an
otherwise correct entity.

The second and third types of error are particu-
larly challenging. An example of the second type is
the following:

Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .

Google is marked twice incorrectly as being a person
rather than an organization.

Finally, here is an example of the third error type:

A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.

IdentiFinder incorrectly marks “danced” as part of a
person tag.

We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al., 1999).

3.2 Domain-specific error reduction
We wrote a series of rule-based filters to remove
instances of the error types—of which there were
many subtypes—described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
“danced” was labelled as a verb, and entities with

tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder’s majority labelling of Google throughout the
corpus—as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.

This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.

3.3 Mechanical Turk tasks

The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.

We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: “Person,” “Organization,”
“Neither,” and “Don’t Know.” Then for each entity,
the user selects exactly one of the four options.

Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.

We calculated the agreement between our annota-
tions and those developed from the Turker majority

347



vote scheme. This yields a Cohen’s κ of 0.68. We
considered this to be substantial agreement.

After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).

We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.

4 Discussion

4.1 Benefits

It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.

However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.

One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.

This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.

4.2 Costs

It took not more than an estimated two person weeks
to complete this work. This includes doing the

expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.

For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).

5 Conclusions and Future Work

This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.

Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al., 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.

Acknowledgements

This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.

References
Daniel M. Bikel, Richard Schwartz, and Ralph M.

Weischedel. 1999. An algorithm that learns what‘s
in a name. Mach. Learn., 34(1-3).

Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon’s Me-
chanical Turk. In EMNLP 2009, Singapore, August.

Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.

348


