










































Shift-Reduce Word Reordering for Machine Translation


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1382–1386,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Shift-Reduce Word Reordering for Machine Translation

Katsuhiko Hayashi†, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan

†hayashi.katsuhiko@lab.ntt.co.jp

Abstract

This paper presents a novel word reordering
model that employs a shift-reduce parser for
inversion transduction grammars. Our model
uses rich syntax parsing features for word re-
ordering and runs in linear time. We apply it to
postordering of phrase-based machine trans-
lation (PBMT) for Japanese-to-English patent
tasks. Our experimental results show that our
method achieves a significant improvement
of +3.1 BLEU scores against 30.15 BLEU
scores of the baseline PBMT system.

1 Introduction

Even though phrase-based machine translation
(PBMT) (Koehn et al., 2007) and tree-based MT
(Graehl and Knight, 2004; Chiang, 2005; Galley
et al., 2006) systems have achieved great success,
many problems remain for distinct language pairs,
including long-distant word reordering.

To improve such word reordering, one promis-
ing way is to separate it from the translation pro-
cess as preordering (Collins et al., 2005; DeNero
and Uszkoreit, 2011) or postordering (Sudoh et al.,
2011; Goto et al., 2012). Many studies utilize a rule-
based or a probabilistic model to perform a reorder-
ing decision at each node of a syntactic parse tree.

This paper presents a parser-based word reorder-
ing model that employs a shift-reduce parser for in-
version transduction grammars (ITG) (Wu, 1997).
To the best of our knowledge, this is the first study
on a shift-reduce parser for word reordering.

The parser-based reordering approach uses rich
syntax parsing features for reordering decisions.
Our propoesd method can also easily define such

....
Source-

ordered Target
Sentence (HFE)

.Source Sen-
tence (J)

. Target Sen-
tence (E)

...

reordering

Figure 1: A description of the postordering MT system.

non-local features as the N -gram words of reordered
strings. Even when using these non-local features,
the complexity of the shift-reduce parser does not
increase at all due to give up achieving an optimal
solution. Therefore, it works much more efficient.

In our experiments, we apply our proposed
method to postordering for J-to-E patent tasks be-
cause their training data for reordering have little
noise and they are ideal for evaluating reordering
methods. Although our used J-to-E setups need
a language-dependent scheme and we describe our
proposed method as a J-to-E postordering method,
the key algorithm is language-independent and it can
be applicable to preordering as well as postordering
if the training data for reordering are available.

2 Postordering by Parsing

As shown in Fig.1, postordering (Sudoh et al., 2011)
has two steps; the first is a translation step that trans-
lates an input sentence into source-ordered transla-
tions. The second is a reordering step in which the
translations are reordered in the target language or-
der. The key to postordering is the second step.

Goto et al. (2012) modeled the second step by
parsing and created training data for a postordering
parser using a language-dependent rule called head-
finalization. The rule moves syntactic heads of a
lexicalized parse tree of an English sentence to the

1382



....S(saw).

.... ..VP(saw).

.... ..PP(with).

.... ..NP(telescope).

.... ..N(telescope).

....telescope.

....D(a).

....a

.

....PR(with).

....with

.

....VP(saw).

.... ..NP(girl).

.... ..N(girl).

....girl.

....D(a).

....a

.

....V(saw).

....saw

.

....NP(I).

....N(I).

....I

.

..

.... ..mita. .... ..wo. .... ..shoujyo. .... ..de. .... ..bouenkyo. .... ..wa. ....watashi

. ..S(saw).

.... ..VP#(saw).

.... ..VP#(saw).

.... ..V(saw).

....saw

.

....NP(wo)“a/an”.

.... ..WO(wo).

....wo.

....N(girl).

....girl.

....PP#(with).

.... ..PR(with).

....with

.

....NP(telescope)“a/an”.

....N(telescope).

....telescope

.

....NP(wa)“no articles”.

.... ..WA(wa).

....wa.

....N(I).

....I

.

..

.... ..mita. .... ..wo. .... ..shoujyo. .... ..de. .... ..bouenkyo. .... ..wa. ....watashi

Figure 2: An example of the head-finzalizaton process for an English-Japanese sentence pair: the left-hand side tree
is the original English tree, and the right-hand side tree is its head-final English tree.

end of the corresponding syntactic constituents. As
a result, the terminal symbols of the English tree are
sorted in a Japanese-like order. In Fig.2, we show an
example of head-finalization and a tree on the right-
hand side is a head-finalized English (HFE) tree of
an English tree on the left-hand side. We annotate
each parent node of the swapped edge with # sym-
bol. For example, a nonterminal symbol PP#(with)
shows that a noun phrase “a/an telescope” and a
word “with” are inverted.

For better word alignments, Isozaki et al. (2012)
also deleted articles “the” “a” “an” from English be-
cause Japanese has no articles, and inserted Japanese
particles “ga” “wo” “wa” into English sentences.
We privilege the nonterminals of a phrase modified
by a deleted article to determine which “the” “a/an”
or “no articles” should be inserted at the front of the
phrase. Note that an original English sentence can
be recovered from its HFE tree by using # symbols
and annotated articles and deleting Japanese parti-
cles.

As well as Goto et al. (2012), we solve postorder-
ing by a parser whose model is trained with a set
of HFE trees. The main difference between Goto et
al. (2012)’s model and ours is that while the former
simply used the Berkeley parser (Petrov and Klein,
2007), our shift-reduce parsing model can use such
non-local task specific features as the N -gram words
of reordered strings without sacrificing efficiency.

Our method integrates postediting (Knight and
Chander, 1994) with reordering and inserts articles
into English translations by learning an additional
“insert” action of the parser. Goto et al. (2012)
solved the article generation problem by using an

N -gram language model, but this somewhat compli-
cates their approach. Compared with other parsers,
one advantage of the shift-reduce parser is to easily
define such additional operations as “insert”.

HFE trees can be defined as monolingual ITG
trees (DeNero and Uszkoreit, 2011). Our monolin-
gual ITG G is a tuple G = (V, T, P, I, S) where V
is a set of nonterminals, T is a set of terminals, P
is a set of production rules, I is a set of nontermi-
nals on which “the” “a/an” or “no articles” must be
determined, and S is the start symbol.

Set P consists of terminal production rules that
are responsible for generating word w(∈ T ):

X → w

and binary production rules in two forms:

X → Y Z
X# → Y Z

where X, X#, Y and Z are nonterminals. On
the right-hand side, the second rule generates two
phrases Y and Z in the reverse order. In our experi-
ments, we removed all unary production rules.

3 Shift-Reduce Parsing

Given an input sentence w1 . . . wn, the shift-reduce
parser uses a stack of partial derivations, a buffer of
input words, and a set of actions to build a parse tree.

The following is the parser’s configuration:

ℓ : ⟨i, j, S⟩ : π

where ℓ is the step size, S is a stack of elements
s0, s1, . . . , i is the leftmost span index of the stack

1383



top element s0, j is an index of the next input word
of the buffer, and π is a set of predictor states1.

Each stack element has at least the following com-
ponents of its partial derivation tree:

s = {H, h, wleft, wright, a}

where H is a root nonterminal or a part-of-speech tag
of the subtree, h is a head index of H, a is a variable
to which “the” “a/an” “no articles” or null are as-
signed, and wleft, wright are the leftmost and right-
most words of phrase H. When referring to compo-
nent ∗, we use a s.∗ notation.

Our proposed system has 4 actions shift-X, insert-
x, reduce-MR-X and reduce-SR-X.

The shift-X action pushes the next input word
onto the stack and assigns a part-of-speech tag X to
the word. The deduction step is as follows:

X → wj ∈ P
p︷ ︸︸ ︷

ℓ : ⟨i, j, S|s′0⟩ : π
ℓ + 1 : ⟨j, j + 1, S|s′0|s0)⟩ : {p}

where s0 is {X, j, wj , wj , null}.
The insert-x action determines whether to gener-

ate “the” “a/an” or “no articles” (= x):

s′0.X ∈ I ∧ (s′0.a ̸= “the” ∧ s′0.a ̸= “a/an”)
ℓ : ⟨i, j, S|s′0)⟩ : π

ℓ + 1 : ⟨i, j, S|s0⟩ : π

where s′0 is {X, h, wleft, wright, a} and s0 is
{X, h, wleft, wright, x} (i ≤ h, left, right < j).
The side condition prevents the parser from inserting
articles into phrase X more than twice. During pars-
ing, articles are not explicitly inserted into the input
string: they are inserted into it when backtracking to
generate a reordered string after parsing.

The reduce-MR-X action has a deduction rule:

X → Y Z ∈ P ∧ q ∈ π
q︷ ︸︸ ︷

: ⟨k, i, S|s′2|s′1⟩ : π′ ℓ : ⟨i, j, S|s′1|s′0⟩ : π
ℓ + 1 : ⟨k, j, S|s′2|s0⟩ : π′

1Since our notion of predictor states is identical to that in
(Huang and Sagae, 2010), we omit the details here.

s0.wh ◦ s0.th s0.H s0.H ◦ s0.th s0.wh ◦ s0.H
s1.wh ◦ s1.th s1.H s1.H ◦ s1.th s1.wh ◦ s1.H
s2.th ◦ s2.H s2.wh ◦ s2.H q0.w q1.w q2.w
s0.tl ◦ s0.L s0.wl ◦ s0.L s1.tl ◦ s1.L s1.wl ◦ s1.L
s0.wh ◦ s0.H ◦ s1.wh ◦ s1.H s0.H ◦ s1.wh s0.wh ◦ s1.H
s0.H ◦ s1.H s0.wh ◦ s0.H ◦ q0.w s0.H ◦ q0.w
s1.wh ◦ s1.H ◦ q0.w s1.H ◦ q0.w s1.th ◦ q0.w ◦ q1.w
s0.wh ◦ s0.H ◦ s1.H ◦ q0.w s0.H ◦ s1.wh ◦ s1.H ◦ q0.w
s0.H ◦ s1.H ◦ q0.w s0.th ◦ s1.th ◦ q0.w
s0.wh ◦ s1.H ◦ q0.w ◦ q1.w s0.H ◦ q0.w ◦ q1.w
s0.th ◦ q0.w ◦ q1.w s0.wh ◦ s0.H ◦ s1.H ◦ s2.H
s0.H ◦ s1.wh ◦ s1.H ◦ s2.H s0.H ◦ s1.H ◦ s2.wh ◦ s2.H
s0.H ◦ s1.H ◦ s2.H s0.th ◦ s1.th ◦ s2.th
s0.H ◦ s0.R ◦ s0.L s1.H ◦ s1.R ◦ s1.L s0.H ◦ s0.R ◦ q0.w
s0.H ◦ s0.L ◦ s1.H s0.H ◦ s0.L ◦ s1.wh s0.H ◦ s1.H ◦ s1.L
s0.wh ◦ s1.H ◦ s1.R
s0.wleft ◦ s1.wright s0.tleft ◦ s1.tright
s0.wright ◦ s1.wleft s0.tright ◦ s1.tleft
s0.a ◦ s0.wleft s0.a ◦ s0.tleft s0.a ◦ s0.wleft ◦ s1.wright
s0.a ◦ s0.tleft ◦ s1.tright s0.a ◦ s0.wh s0.a ◦ s0.th

Table 1: Feature templates: s.L and s.R denote the left
and right subnodes of s. l and r are head indices of L and
R. q denotes a buffer element. t is a part-of-speech tag.

where s′0 is {Z, h0, wleft0, wright0, a0} and s′1 is
{Y, h1, wleft1, wright1, a1}. The action generates s0
by combining s′0 and s

′
1 with binary rule X→Y Z:

s0 = {X, h0, wleft1, wright0, a1}.

New nonterminal X is lexicalized with head word
wh0 of right nonterminal Z. This action expands Y
and Z in a straight order. The leftmost word of
phrase X is set to leftmost word wleft1 of Y, and the
rightmost word of phrase X is set to rightmost word
wright0 of Z. Variable a is set to a1 of Y.

The difference between reduce-MR-X and
reduce-SR-X actions is new stack element s0. The
reduce-SR-X action generates s0 by combining s′0
and s′1 with binary rule X

# →Y Z:

s0 = {X#, h0, wleft0, wright1, a0}.

This action expands Y and Z in a reverse order, and
the leftmost word of X# is set to wleft0 of Z, and the
rightmost word of X# is set to wright1 of Y. Variable
a is set to a0 of Z.

We use a linear model that is discriminatively
trained with the averaged perceptron (Collins and
Roark, 2004). Table 1 shows the feature templates
used in our experiments and we call the features in
the bottom two rows “non-local” features.

1384



train dev test9 test10
# of sent. 3,191,228 2,000 2,000 2,300
ave. leng. (J) 36.4 36.6 37.0 43.1
ave. leng. (E) 33.3 33.3 33.7 39.6

Table 2: NTCIR-9 and 10 data statistics.

4 Experiments

4.1 Experimental Setups

We conducted experiments for NTCIR-9 and 10
patent data using a Japanese-English language pair.
Mecab2 was used for the Japanese morphological
analysis. The data are summarized in Table 2.

We used Enju (Miyao and Tsujii, 2008) for pars-
ing the English training data and converted parse
trees into HFE trees by a head-finalization scheme.
We extracted grammar rules from all the HFE trees
and randomly selected 500,000 HFE trees to train
the shift-reduce parser.

We used Moses (Koehn et al., 2007) with lexical-
ized reordering and a 6-gram language model (LM)
trained using SRILM (Stolcke et al., 2011) to trans-
late the Japanese sentences into HFE sentences.

To recover the English sentences, our shift-reduce
parser reordered only the 1-best HFE sentence. Our
strategy is much simpler than Goto et al. (2012)’s
because they used a linear inteporation of MT cost,
parser cost and N -gram LM cost to generate the best
English sentence from the n-best HFE sentences.

4.2 Main Results

The main results in Table 3 indicate our method was
significantly better and faster than the conventional
PBMT system. Our method also ourperformed Goto
et al. (2012)’s reported systems as well as a tree-
based (moses-chart) system3. Our proposed model
with “non-local” features (w/ nf.) achieved gains
against that without the features (w/o nf.). Further
feature engineering may improve the accuracy more.

4.3 Analysis

We show N -gram precisions of PBMT (dist=6,
dist=20) and proposed systems in Table 5. The re-
sults clearly show that improvements of 1-gram pre-

2https://code.google.com/p/mecab/
3All the data and the MT toolkits used in our experiments

are the same as theirs.

test9 test10
BLEU RIBES BLEU RIBES

HFE w/ art. 28.86 73.45 29.9 73.52
proposed 32.93 76.68 33.25 76.74
w/o art. 19.86 75.62 20.17 75.63
N -gram 32.15 76.52 32.28 76.46

Table 4: The effects of article generation: “w/o art.” de-
notes evaluation scores for translations of the best system
(“proposed”) in Table 3 from which articles are removed.
“HFE w/ art.” system used HFE data with articles and
generated them by MT system and the shift-reduce parser
performed only reordering. “N -gram” system inserted
articles into the translations of “w/o art.” by Goto et al.
(2012)’s article generation method.

(1–4)-gram precision

moses (dist=6) 67.1 / 36.9 / 20.7 / 11.5
moses (dist=20) 67.7 / 38.9 / 23.0 / 13.7

proposed 68.9 / 40.6 / 25.7 / 16.7

Table 5: N -gram precisions of moses (dist=6, dist=20)
and proposed systems for test9 data.

cisions are the main factors that contribute to bet-
ter performance of our proposed system than PBMT
systems. It seems that the gains of 1-gram presicions
come from postediting (article generation).

In table 4, we show the effectiveness of our joint
reordering and postediting approach (“proposed”).
The “w/o art.” results clearly show that generating
articles has great effects on MT evaluations espe-
cially for BLEU metric. Comparing “proposed” and
“HFE w/ art.” systems, these results show that poste-
diting is much more effective than generating arti-
cles by MT. Our joint approach also outperformed
“N -gram” postediting system.

5 Conclusion

We proposed a shift-reduce word ordering model
and applied it to J-to-E postordering. Our experi-
mental results indicate our method can significantly
improve the performance of a PBMT system.

Future work will investigate our method’s use-
fulness on various language datasets. We plan to
study more general methods that use word align-
ments to embed swap information in trees (Galley
et al., 2006).

1385



test9 test10
BLEU RIBES time (sec.) BLEU RIBES time (sec.)

PBMT (dist=6) 27.1 67.76 2.66 27.92 68.13 3.18
PBMT (dist=12) 29.55 69.84 4.15 30.03 69.88 4.93
PBMT (dist=20) 29.98 69.87 6.22 30.15 69.43 7.19

Tree-based MT** (Goto et al., 2012) 29.53 69.22 – – – –
PBMT (dist=20)** (Goto et al., 2012) 30.13 68.86 – – – –

Goto et al. (2012)** 31.75 72.57 – – – –
PBMT (dist=0) + proposed w/o nf. (beam=12) 32.59 76.35 1.46 + 0.01 32.83 76.44 1.7 + 0.01
PBMT (dist=0) + proposed w/o nf. (beam=48) 32.61 76.58 1.46 + 0.06 32.86 76.6 1.7 + 0.06

PBMT (dist=0) + proposed w/ nf. (beam=12) 32.91 76.38 1.46 + 0.01 33.15 76.53 1.7 + 0.02
PBMT (dist=0) + proposed w/ nf. (beam=48) 32.93 76.68 1.46 + 0.07 33.25 76.74 1.7 + 0.07

Table 3: System comparison: time represents the average second per sentence. ** denotes “not our experiments”.

References
David Chiang. 2005. A hierarchical phrase-based model

for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 111.

Michael Collins, Philipp Koehn, and Ivona Kučerová.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531–540.

John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 193–203.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics, pages
961–968.

Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statisti-
cal machine translation. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 311–316.

Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105–112.

Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-

ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1077–1086.

Hideki Isozaki, Jun Suzuki, Hajime Tsukada, Masaaki
Nagata, Sho Hoshino, and Yusuke Miyao. 2012.
HPSG-based preprocessing for English-to-Japanese
translation. ACM Transactions on Asian Language In-
formation Processing (TALIP), 11(3).

Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of the
National Conference on Artificial Intelligence, pages
779–779.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177–180.

Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35–80.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North American
chapter of the Association for Computational Linguis-
tics, pages 404–411.

Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and outlook.
In Proceedings of IEEE Automatic Speech Recognition
and Understanding Workshop.

Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering in
statistical machine translation. In Proc. MT Summit.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

1386


