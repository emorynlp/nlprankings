Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 403–411,

Beijing, August 2010

403

Paraphrase Alignment for Synonym Evidence Discovery

Jo˜ao Cordeiro, Ga¨el Dias,

Rumen Moraliyski

HULTIG

University of Beira Interior

{jpaulo,ddg,rumen}@di.ubi.pt

Pavel Brazdil
LIAAD/FEP

University of Porto

pbrazdil@liaad.up.pt

Gintar˙e Grigonyt˙e
Saarland University

g.grigonyte@hmf.vdu.lt

Abstract

We describe a new unsupervised approach
for synonymy discovery by aligning para-
phrases in monolingual domain corpora.
For that purpose, we identify phrasal
terms that convey most of the concepts
within domains and adapt a methodol-
ogy for the automatic extraction and align-
ment of paraphrases to identify para-
phrase casts from which valid synonyms
are discovered. Results performed on two
different domain corpora show that gen-
eral synonyms as well as synonymic ex-
pressions can be identiﬁed with a 67.27%
precision.
Introduction

1
Synonymy is a speciﬁc type of a semantic re-
lationship. According to (Sowa and Siekmann,
1994), a synonym is a word (or concept) that
means the same or nearly the same as another
word (or concept).
It has been observed that
words are similar if their contexts are similar (Fre-
itag et al., 2005) and so synonymy detection has
received a lot of attention during the last decades.
However, words used in the same context are
not necessarily synonyms and can embody dif-
ferent semantic relationships such as hyponyms,
meronyms or co-hyponyms (Heylen et al., 2008).
In this paper, we introduce a new unsupervised
methodology for synonym detection by extract-
ing and aligning paraphrases on normalized do-
main corpora1. In particular, we study a speciﬁc
structure within aligned paraphrases, paraphrase
1By normalized, we intend that phrasal terms have been

previously identiﬁed.

casts, from which valid synonyms are discovered.
In fact, we propose a new approach based on the
idea that synonyms are substitutable words within
a given domain corpus. Results performed on two
different domain corpora, the Corpus of Computer
Security (COCS) and the Corpus of Cancer Re-
search (COCR), show that general synonyms as
well as synonymic expressions can be identiﬁed
with a 67.27% precision performance.

2 Related Work

Automatic synonymy detection has been tackled
in a variety of ways which we explain as follows.

2.1 Pattern-based Approaches
This approach to information extraction is based
on a technique called selective concept extraction
as deﬁned by (Riloff, 1993). Selective concept
extraction is a form of text skimming that selec-
tively processes relevant text while effectively ig-
noring surrounding text that is thought to be ir-
relevant to the domain. The pioneer of pattern-
based approaches (Hearst, 1992) has introduced
lexico-syntactic patterns to automatically acquire
given word semantic relationships. Speciﬁc pat-
terns like ”X and other Y” or ”X such as Y” were
used for hypernym-hyponym detection. Later, the
idea was extended and adapted for synonymy by
other researchers such as (Roark and Charniak,
1998), (Caraballo, 1999) and (Maynard and Pe-
ters, 2009). In general, manual pattern deﬁnition
is time consuming and requires linguistic skills.
Usually, systems based on lexico-syntactic pat-
terns perform with very high precision, but low
recall due to the fact that these patterns are rare.
However, recent work by (Ohshima and Tanaka,

404

2009) on Web data reported high recall ﬁgures.
To avoid manual encoding of patterns, many su-
pervised approaches have been proposed as sum-
marized in (Stevenson and Greenwood, 2006).

2.2 Distributional Similarity
Distributional similarity for capturing semantic
relatedness is relying on the hypothesis that se-
mantically similar words share similar contexts.
These methods vary in the level of supervision
from unsupervised to semi-supervised or to su-
pervised. The ﬁrst type of methods includes the
work of (Hindle, 1990), (Lin, 1998) and (Heylen
et al., 2008) who used unsupervised methods
for detecting word similarities based on shallow-
parsed corpora. Others have proposed unsuper-
vised methodologies to solve TOEFL-like tests,
instead of discovering synonyms (Turney, 2001),
(Terra and Clarke, 2003) and (Freitag et al., 2005).
Other researchers, such as (Girju et al., 2004),
(Muller et al., 2006), (Wu and Zhou, 2003) and
(Wei et al., 2009), have used language or knowl-
edge resources to enhance the representation of
the vector space model. Unlike the pattern-based
approach, the distributional similarity-based ap-
proach shows low precision compared to high re-
call.

One obvious way to verify all the possible con-
nections between words of the vocabulary em-
ploys an exhaustive search. However, compari-
son based on word usage can only highlight those
terms that are highly similar in meaning. This
method of representation is usually unable to dis-
tinguish between middle strength and weak se-
mantic relations, or detect the relationship be-
tween hapax-legomena.

2.3 Hybrid Approaches
More recently, approaches combining patterns
and distributional similarity appeared to bring the
best of the two methodologies.
(Hagiwara et
al., 2009) describe experiments that involve train-
ing various synonym classiﬁers. (Giovannetti et
al., 2008) use syntactically parsed text and man-
ually composed patterns together with distribu-
tional similarity for detecting semantically related
words. Finally, (Turney, 2008) proposes a super-
vised machine learning approach for discovering

synonyms, antonyms, analogies and associations.
For that purpose, feature vectors are based on fre-
quencies of patterns and classiﬁed by a SVM.

2.4 Our Approach
(Van der Plas and Tiedemann, 2006) state that
”People use multiple ways to express the same
idea. These alternative ways of conveying the
same information in different ways are referred
to by the term paraphrase and in the case of
single words or phrasal terms sharing the same
meaning, we speak of synonyms”. Based on this,
we propose that in order to discover pairs of se-
mantically related words (in the best case syn-
onyms) that may be used in ﬁgurative or rare
sense, and as consequence impossible to be iden-
tiﬁed by the distributional similarity approach,
we need to have them highlighted by their lo-
cal speciﬁc environment. Here we differ from
the pattern-based approach that use local general
environment. We propose to align paraphrases
from domain corpora and discover words that are
possibly substitutable for one another in a given
context (paraphrase casts), and as such are syn-
onyms or near-synonyms. Comparatively to exist-
ing approaches, we propose an unsupervised and
language-independent methodology which does
not depend on linguistic processing2, nor manual
deﬁnition of patterns or training sets and leads to
higher precision when compared to distributional
similarity-based approaches.

3 Normalization of the Corpora
The main goal of our research is to build knowl-
edge resources in different domains that can ef-
fectively be used in different NLP applications.
As such, precision takes an important part in the
overall process of our methodology. For that pur-
pose, we ﬁrst identify the phrasal terms (or multi-
word units) present in the corpora. Indeed, it has
been shown in many works that phrasal terms con-
vey most of the speciﬁc contents of a given do-
main. Our approach to term extraction is based
on linguistic pattern matching and Inverse Doc-
ument Frequency (IDF) measurements for term

2We will see in the next section that we will use linguistic
resources to normalize our corpora, but the methodology can
be applied to any raw text.

405

quality assurance as explained in (Avizienis et al.,
2009). For that purpose, we present a domain in-
dependent hybrid term extraction framework that
includes the following steps. First, the text is
morphologically annotated with the MPRO sys-
tem (Maas et al., 2009). Then grammar rules for
morphological disambiguation, syntactic parsing
and noun phrase detection are applied based on
ﬁnite-state automata technology, KURD (Carl and
Schmidt-Wigger, 1998). Following this, a vari-
ant and non-basic term form detection is applied,
as well as stop words removal. Then, combining
rich morphological and shallow syntactical analy-
sis with pattern matching techniques allows us to
extract a wide span of candidate terms which are
ﬁnally ﬁltered with the well-known IDF measure.

4 Paraphrase Identiﬁcation

A few unsupervised metrics have been applied to
automatic paraphrase identiﬁcation and extraction
(Barzilay and McKeown, 2001) and (Dolan et al.,
2004). However, these unsupervised methodolo-
gies show a major drawback by extracting quasi-
exact or even exact match pairs of sentences as
they rely on classical string similarity measures.
Such pairs are useless for our research purpose.
More recently, (Cordeiro et al., 2007a) proposed
the sumo metric specially designed for asymmet-
rical entailed pair identiﬁcation in corpora which
obtained better performance than previously es-
tablished metrics, even in corpora with exclu-
sively symmetrical entailed paraphrases as in the
Microsoft Paraphrase Research Corpus (Dolan et
al., 2004). This function states that for a given
sentence pair hSa, Sbi, having m and n words in
each sentence and λ lexical exclusive links (word
overlaps, see ﬁgure 1) between them, its lexi-
cal connection strength is computed as deﬁned in
Equations 1 and 2.

Sumo(Sa, Sb) =

where

S(m, n, λ)

if S(m, n, λ) < 1

0

if λ = 0

e−kS(m,n,λ)

otherwise

S(m, n, λ) = α log2( m

λ ) + β log2( n
λ )

α, β ∈ [0, 1], α + β = 1

(1)

(2)

Figure 1: 4 exclusive links between Sa and Sb.

To obtain a paraphrase corpus, we compute all
sentence pairs similarities Sumo(Sa, Sb) and se-
lect only those pairs exceeding a given threshold,
in our case threshold = 0.85, which is quite re-
strictive, ensuring the selection of pairs strongly
connected3.

However, to take into account the normalization
of the corpus, little adjustments had to be inte-
grated in the methodology proposed in (Cordeiro
et al., 2007a).
Indeed, the original Sumo(., .)
function was under-weighting links that occurred
between phrasal terms such as “molecular labo-
ratory” or “renal cancer”. So, instead of counting
the number of lexical links among sentences, each
link weights differently according to the word
length in the connection, hence connections of
longer words will result in a larger value. For ex-
ample, in ﬁgure 1, instead of having λ = 4, we
count λ = 3 + 8 + 7 + 4 = 22. This adjust-
ment is important as multi-word units are treated
as longer words in the corpus. This modiﬁcation
has also, as a side effect, under-evaluation of func-
tional words which usually follow the Zipf’s Law
and give more importance to meaningful words in
the paraphrase extraction process.

5 Paraphrase Alignment
In order to usefully explore the evidence syn-
onymy from paraphrases, sentence alignment
techniques must be applied to paraphrases in or-
der to identify paraphrase casts, i.e., substitutable
word pairs as shown in ﬁgure 2. As we can see,
the paraphrase cast includes three parts: the left
segment (context), a middle segment and the right
segment (context). In our ﬁgure the left and right
segments (contexts) are identical.

In the context of DNA sequence alignment,
two main algorithms have been proposed: (1) the
Needleman-Wunsch algorithm (Needleman and

3Further details about the sumo metric are available in

(Cordeiro et al., 2007a).

406

function gs(., .) in Equations 4 and 5 which prior-
itize the alignment of speciﬁc domain key terms
i.e., single match, or key expressions i.e., phrasal
match, (reward 50), as well as lexically similar5
words such as ”programme” and ”programming”
for example. In particular, for these similar words
an adaptation of the well known Edit Distance is
used, the c(., .) function (5), which is explained in
more detail in (Cordeiro et al., 2007b).



gs(Sai , Sbj ) =

where

−1
−1
10

50

if (Sai = −) and (Sbj 6= −)
if (Sbj = −) and (Sai 6= −)
Single Match

Phrasal Match

c(Sai , Sbj ) Mismatch

(4)

(5)

c(Sai , Sbj ) = −

edist(Sai , Sbj )

 + maxseq(Sai , Sbj )

To obtain local alignments, the SW algorithm is
applied, using the alignment function deﬁned with
H(., .) in 3. The alignment of the paraphrase in
ﬁgure 2 would give the result in ﬁgure 3.

Figure 3: An alignment.

6 Paraphrase Casts
In order to discover synonyms, we are looking for
special patterns from the aligned paraphrase sen-
tences, which naturally give us more evidence for
the existence of equivalent terms or expressions.
Due to the topological aspect of such patterns, we
decided to name them paraphrase casts, or just
casts as shown in ﬁgure 2. As we have mentioned
earlier, the paraphrase cast includes three parts:
the left segment (contextL), a middle segment and
the right segment (contextR). In the following ex-
ample the left and right segments (contexts) are
identical, but the middle segment includes differ-
ent misaligned sequences of words, represented
by wordSa and wordSb.

contextL wordSa ----- contextR
contextL ----- wordSb contextR

Figure 2: A paraphrase cast.

Wunsch, 1970) based on dynamic programming
which outputs a unique global alignment and (2)
the Smith-Waterman (SW) algorithm (Smith and
Waterman, 1981) which is an adaptation of the
previous algorithm and outputs local alignments.
In the context of NLP, (Cordeiro et al., 2007a)
proposed a combination of both algorithms de-
pending on the structure of paraphrase. How-
ever, since any local alignment is a candidate for
paraphrase casts, the SW algorithm revealed it-
self more appropriate and was always chosen. The
SW alignment algorithm uses dynamic program-
ming to compute the optimal local alignments be-
tween two sequences4. This process requires ﬁrst
the deﬁnition of an alignment matrix (function),
which governs the likelihood of alignment of two
symbols. Thus we ﬁrst build a matrix H such that
H(i, 0) = 0 and H(0, j) = 0, for 0 ≤ i ≤ m,
and 0 ≤ j ≤ n, where m and n are the number of
words in the paraphrase sentences. The rest of the
H elements are recursively calculated as in Equa-
tion 3 where gs(., .) is the gap-scoring function
and Sai (resp. Sbj ) represents the ith (resp. jth)
word of sentence Sa (resp. Sb).

H(i, j) = max

0

H(i − 1, j − 1) + gs(Sai , Sbj ) MMisatch
Deletion
H(i − 1, j) + gs(Sai ,
H(i, j − 1) + gs( , Sbj )

Insertion

)

(3)

Obviously, this algorithm is based on an align-
ment function which exploits the alignment like-
lihood between two alphabet symbols. For DNA
sequence alignments, this function is deﬁned as
a mutation matrix, scoring gene mutation and gap
alignments. In our case, we deﬁne the gap-scoring

4In our case, the two sequences are the two sentences of

a paraphrase

5This is why we have in equation 3 the label “Mismatch”,
where “mismatch” means different yet lexically near words.

407

We can attribute different levels of conﬁdence
to different paraphrase casts.
Indeed, the larger
the contexts and the smaller the misaligned se-
quences are, the more likely it is for single or
phrasal terms to be synonyms or near-synonyms.
Note that in the cast shown in ﬁgure 3, each con-
text has a signiﬁcant size, with four words on
each side, and the misaligned segments are in fact
equivalent expressions i.e. ”paper” is a synonym
of ”research article”.
In the analyzed domain
these expressions are equivalent and interchange-
able and appear to be interchangeable in other do-
mains. For the purpose of this paper, we only
take into account the casts where the misaligned
sequences of words contain only one word or one
multi-word unit in each sentence. That is, we have
a one-to-one match. However, no constraints are
imposed on the contexts6. So, all casts are com-
puted and analyzed for synonym discovery and re-
sults are provided in the next section.

7 Experiments

To evaluate our methodology we have used
two different corpora, both from scientiﬁc do-
mains built from abstracts of publications (see
Table 1).
The corpus of computer secu-
rity (COCS) is a collection of 4854 abstracts
on computer security extracted from the IEEE
(http://ieee.rkbexplorer.com/) repository7.
The
corpus of cancer research (COCR) contains 3334
domain speciﬁc abstracts of scientiﬁc publica-
tions extracted from the PubMed8 on three types
of cancer: (1) the mammary carcinoma register
(COCR1) consisting of 1500 abstracts, (2) the
nephroblastoma register (COCR2) consisting of
1500 abstracts, and (3) the rhabdoid tumor regis-
ter (COCR3) consisting of 334 abstracts. From
the paraphrase casts, we were able to automat-
ically extract, without further processing, single
synonymous word pairs, as well as synonymic
multi-word units, as can be seen in Table 2. For
that purpose we have used speciﬁc paraphrase
casts, whose aim was to privilege precision to

6This issue will be discussed in the next section.
7An example of an abstract can be viewed at:

http://ieee.rkbexplorer.com/description/publication-
00534618

8http://www.ncbi.nlm.nih.gov/pubmed

Corpus
Tokens
Sentences

Aligned Pairs

Casts without ﬁlter

Casts with ﬁlter

COCS
412.265
18.974

589
320
202

COCR1
336.745
15.195

994

10.217

361

COCR2
227.477
10.575

511
2.520
292

COCR3
46.215
2.321
125
48
16

Table 1: Corpora

In particular, we have removed all casts
recall.
which contained numbers or special characters i.e.
casts with ﬁlter in Table 1. However, no con-
straints were imposed on the frequency of the
casts.
Indeed, all casts were included even if
their overall frequency was just one. Although

Synonym (COCS)
frequency tuning

attack consequences
error-free operation

pseudo code

tolerance

package loss

adjustable algorithm

helpful comment
Synonym (COCR)
childhood renal tumor

hypertrophy
doxorubicin

metastasis
renal tumor

neoplastic thrombus

vincristine

Complementary
frequency control

attack impact

error free operation
pseudo algorithm

resilience

message loss

context-aware algorithm

valuable comment
Complementary

childhood kidney tumor

growth

vincristine

neoplasm

renal malignancy
tumor thrombus

adriamycin

carcinomas of the kidney

sarcoma of the kidney

Table 2: Synonyms for COCS

most of the word relationships were concerned
with synonymy, the other ones were not just er-
rors, but lexically related words, namely examples
of antonymy, hyperonymy/hyponymy and associ-
ations as shown in Table 3. In the evaluation, we

Antonym

Complementary

positive sentinel nodes

higher bits
older version
Hypernym

negative sentinel nodes

lower bits

newer version

Hyponym

Multi-Tasking Virtual Machine

Java Virtual Machine

therapy

hormone breast cancer

Association
performance

statistical difference

relationship

chemotherapy

estrogen breast cancer

Complementary

reliability

signiﬁcant difference

correlation

Table 3: Other Word Semantic Relationships.

have focused on the precision of the method. The
evaluation of the extracted pairs was performed
manually by two domain experts.
In fact, four

408

different evaluations were carried out depending
on whether the adapted S(., .) measure was used
(or not) and whether the normalization of the cor-
pora was used (or not). The best results were ob-
tained in all cases for the adapted S(., .) measure
with the multi-word units. Table 4 shows also the
worst results for the COCS as a baseline (COCS
(1)), i.e. non-adapted S(., .) and non-normalized
corpus. For the rest of the experiments we always
present the results with the adapted S(., .) mea-
sure and normalized corpus.

the target word. In our case, no frequency thresh-
old is imposed to enable extraction of synonyms
with low frequency, such as hapax legomena. This
situation is illustrated in ﬁgure 4. We note that
most of the candidate pairs appear only once in
the corpus.

Corpus
Precision

Errors
Corpus
Precision

Errors

Extracted Synonyms

Extracted Synonyms

COCS (1)
54.62%

130
108

COCR1
69.80%

252
109

COCS (2)
71.29%

144
58

COCR2
61.30%

178
111

COCR3
75.00%

12
4

Table 4: Overall Results

7.1 Discussion
Many results have been published in the literature,
especially tackling the TOEFL synonym detection
problem which aims at identifying the correct syn-
onym among a small set of alternatives (usually
four). For that purpose, the best precision rate has
been reached by (Turney et al., 2003) with 97.50%
who have exploited many resources, both statis-
tical and linguistic. However, our methodology
tackles a different problem.
Indeed, our goal
is to automatically extract synonyms from texts.
The published works toward this direction have
not reached so good results. One of the latest stud-
ies was conducted by (Heylen et al., 2008) who
used distributional similarity measures to extract
synonyms from shallow parsed corpora with the
help of unsupervised methods. They report that
”the dependency-based model ﬁnds a tightly re-
lated neighbor for 50% of the target words and a
true synonym for 14%”. So, it means that by com-
paring all words in a corpus with all other words,
one can expect to ﬁnd a correct semantic relation-
ship in 50% of the cases and a correct synonym
in just 14%.
In that perspective, our approach
reaches higher results. Moreover, (Heylen et al.,
2008) use a frequency cut-off which only selects
features that occur at least ﬁve times together with

Figure 4: Synonyms Frequency Distribution.

In order to assess the quality of our results,
we calculated the similarity between all extracted
pairs of synonyms following the distributional
analysis paradigm as in (Moraliyski and Dias,
2007) who build context9 feature vectors for noun
synonyms. In particular, we used the cosine sim-
ilarity measure and the Loglike association mea-
sure (Dunning, 1993) as the weighting scheme of
the context features. The distribution of the simi-
larity measure for all noun synonyms (62 pairs) is
shown in ﬁgure 5.

Figure 5: Synonym Pairs Similarity Distribution.

The results clearly show that all extracted syn-
onyms are highly correlated in terms of context.
9In this case, the contexts are the surrounding nouns,
verbs and adjectives in the closest chunks of a shallow parsed
corpus.

i

s
r
i
a
p
 
e
t
a
d
d
n
a
c
 
f
o
 
r
e
b
m
u
n
 
g
o
L

6

5

4

3

2

1

0

Synonymous
Non−synonymous

1

2

3

4

5

Frequency of candidate pairs

y
c
n
e
u
q
e
r
F

30

25

20

15

10

5

0

0.0

0.2

0.4
0.6
Similarity

0.8

1.0

409

Nearly half of the cases have similarities higher
than 0.5.
It is important to notice that a spe-
ciﬁc corpus10 was built to calculate as sharply as
possible the similarity measures as it is done in
(Moraliyski and Dias, 2007).
Indeed, based on
the COCS and the COCR most statistics were in-
signiﬁcant leading to zero-valued features. This
situation is well-known as it is one of the major
drawbacks of the distributional analysis approach
which needs huge quantities of texts to make se-
cure decisions. So we note that applying the distri-
butional analysis approach to such small corpora
would have led to rather poor results. Even with
an adapted corpus, ﬁgure 5 (left-most bar) shows
that there are no sufﬁcient statistics for 30 pairs of
synonyms. Although the quality of the extracted
pairs of synonyms is high, the precision remains
relatively low with 67.27% precision on average.
As we pointed out in the previous section, we did
not make any restrictions to the left and right con-
texts of the casts. However, the longer these con-
texts are, compared to the misaligned sequence of
words, the higher the chance is that we ﬁnd a cor-
rect synonym. Table 5 shows the average lengths
of both cast contexts for synonyms and erroneous
pairings, in terms of words (WCL) and charac-
ters (CCL). We also provide the ratio (R) between
the character lengths of the middle segment (i.e.
misaligned character sequences) and the charac-
ter lengths of the cast contexts (i.e. right and left
sizes of equally aligned character sequences). It is

Type

Synonyms

Errors

WCL
2.70
2.45

CCL
11.67
8.05

R
0.70
0.55

Table 5: Average Casts Contexts Lengths

clear that a more thorough study of the effects of
the left and right contexts should be carried out to
improve precision of our approach, although this
may be to the detriment of recall. Based on the
results of the ratio R11, we note that the larger the
cast context is compared to the cast content, the
more likely it is that the misaligned words are syn-
onyms.

10This corpus contains 125.888.439 words.
11These results are statistically relevant with p − value <

0.001 using the Wilcoxon Rank-Sum Test.

8 Conclusions

In this paper we have introduced a new unsu-
pervised methodology for synonym detection that
involves extracting and aligning paraphrases on
normalized domain corpora.
In particular, we
have studied a speciﬁc structure within aligned
paraphrases, paraphrase casts, from which valid
synonyms were discovered. The overall preci-
sion was 71.29% for the computer security do-
main and 66.06% for the cancer research domain.
This approach proved to be promising for ex-
tracting synonymous words and synonymic multi-
word units. Its strength is the ability to effectively
work with small domain corpora, without super-
vised training, nor deﬁnition of speciﬁc language-
dependent patterns. Moreover, it is capable to
extract synonymous pairs with ﬁgurative or rare
senses which would be impossible to identify us-
ing the distributional similarity approach. Finally,
our approach is completely language-independent
as it can be applied to any raw text, not obli-
gatorily normalized corpora, although the results
for domain terminology may be improved by the
identiﬁcation of phrasal terms.

However, further improvements of the method
should be considered. A measure of quality of the
paraphrase casts is necessary to provide a mea-
sure of conﬁdence to the kind of extracted word
semantic relationship. Indeed, the larger the con-
texts and the smaller the misaligned sequences
are, the more likely it is for single or phrasal terms
to be synonyms or near-synonyms. Further work
should also be carried out to differentiate the ac-
quired types of semantically related pairs. As it
is shown in Table 3, some of the extracted pairs
were not synonymic, but lexically related words
such as antonyms, hypernyms/hyponyms and as-
sociations. A natural follow-up solution for dis-
criminating between semantic types of extracted
pairs could involve context-based classiﬁcation of
acquired casts pairs. In particular, (Turney, 2008)
tackled the problem of classifying different lexi-
cal information such as synonymy, antonymy, hy-
pernymy and association by using context words.
In order to propose a completely unsupervised
methodology, we could also follow the idea of
(Dias et al., 2010) to automatically construct small

410

TOEFL-like tests based on sets of casts which
could be handled with the help of different dis-
tributional similarities.

Acknowledgments
We thank anonymous reviewers whose comments
helped to improve this paper. We also thank
IFOMIS institute (Saarbrucken) and the ReSIST
project for allowing us to use the COCR and
COCS corpora.

References
Avizienis, A., Grigonyte, G., Haller, J., von Henke,
F., Liebig, T., and Noppens, O. 2009. Organiz-
ing Knowledge as an Ontology of the Domain of
Resilient Computing by Means of NLP - An Experi-
ence Report. In Proc. of the 22th Int. FLAIRS Conf.
AAAI Press, pp. 474-479.

Barzilay, R. and McKeown, K. R. 2001. Extracting
Paraphrases from a Parallel Corpus. In Proc. of the
39th meeting of ACL, pp. 50-57.

Caraballo, S. A. 1999. Automatic Construction of a
In

Hypernym-labeled Noun Hierarchy from Text.
Proc. of 37th meeting of ACL 1999, pp 120-126.

Carl, M., and Schmidt-Wigger, A. 1998. Shallow Post
Morphological Processing with KURD. In Proc. of
the Conf. on New Methods in Language Processing.

Cordeiro, J.P., Dias, G. and Brazdil, P. 2007. Learning
Paraphrases from WNS Corpora.
In Proc. of the
20th Int. FLAIRS Conf. AAAI Press, pp. 193-198.

Cordeiro, J.P., Dias, G. and Cleuziou, G. 2007. Biol-
ogy Based Alignments of Paraphrases for Sentence
Compression. In Proc. of the 20th meeting of ACL,
workshop PASCAL, pp. 177-184.

Dias, G., Moraliyski, R., Cordeiro, J.P., Doucet, A. and
Ahonen-Myka, H. 2010. Automatic Discovery of
Word Semantic Relations using Paraphrase Align-
ment and Distributional Lexical Semantics Analy-
sis.
In Journal of Natural Language Engineering,
Cambridge University Press. ISSN 1351-3249, pp.
1-26.

Dolan, B., Quirk, C. and Brockett, C. 2004. Un-
supervised Construction of Large Paraphrase Cor-
pora: Exploiting Massively Parallel News Sources.
In Proc. of the 20th int. Conf. on Computational
Linguistics.

Dunning T. D 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. In Computational
Linguistics, 19(1), pp. 61-74.

Freitag, D., Blume, M., Byrnes, J., Chow, E., Kapa-
dia, S., Rohwer, R. and Wang, Z. 2005. New Ex-
periments in Distributional Representations of Syn-
onymy. In Proc. of 9th conf. on Computational Nat-
ural Language Learning, pp. 25-32.

Giovannetti, E., Marchi, S. and Montemagni, S.
2008.
Combining Statistical Techniques and
Lexico-Syntactic Patterns for Semantic Relations
Extraction from Text. In Proc. of the 5th Workshop
on Semantic Web Applications and Perspectives.

Girju, R., Giuglea, A. M., Olteanu, M., Fortu, O.,
Bolohan, O. and Moldovan, D. 2004. Support Vec-
tor Machines Applied to the Classiﬁcation of Se-
mantic Relations in Nominalized Noun Phrases. In
Proc. of the HLT-NAACL Workshop on Computa-
tional Lexical Semantics, pp. 68-75.

Hagiwara, M. O. Y. and Katsuhiko, T. 2009. Su-
pervised Synonym Acquisition using Distributional
Features and Syntactic Patterns. In Information and
Media Technologies 4(2), pp. 558-582.

Hearst, M. A. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora.
In Proc. of the
14th conf. on Computational Linguistics, pp. 539-
545.

Heylen, K., Peirsman, Y., Geeraerts, D. and Speelman,
D. 2008. Modelling Word Similarity: an Evalua-
tion of Automatic Synonymy Extraction Algorithms.
In Proc. of the 6th LREC.

Hindle, D. 1990. Noun Classiﬁcation from Predicate-
Argument Structures. In Proc. of the 28th meeting
of ACL, pp. 268-275.

Inkpen, D.

2007. A Statistical Model for Near-
Synonym choice. In ACM Trans. Speech Lang. Pro-
cess. 4(1), 1-17.

Kiefer, J. 1953. Sequential Minimax Search for a
Maximum. In Proc. of the American Mathematical
Society 4, pp. 502-506.

Lin, D. 1998. Automatic Retrieval and Clustering of
In Proc. of the 17th Int. Conf. on

Similar Words.
Computational Linguistics, pp. 768-774.

Maas, D., Rosener, Ch., and Theoﬁlidis, A. 2009.
Morphosyntactic and Semantic Analysis of Text:
The MPRO Tagging Procedure. Workshop on sys-
tems and frameworks for computational morphol-
ogy. Springer., pp. 76-87.

Maynard, D. F. A. and Peters, W. 2009. Using lexico-
syntactic Ontology Design Patterns for Ontology
Creation and Population. In Proc. of the Workshop
on Ontology Patterns.

411

Van der Plas, L. and Tiedemann, J. 2006. Finding Syn-
onyms Using Automatic Word Alignment and Mea-
sures of Distributional Similarity.
In Proc. of the
21st Int. Conf. on Computational Linguistics, pp.
866-873.

Wei, X., Peng, F., Tseng, H., Lu, Y. and Dumoulin,
B. 2009. Context Sensitive Synonym Discovery for
Web Search Queries.
In Proc. of the 18th ACM
conference on Information and Knowledge Man-
agement, pp. 1585-1588.

Wu, H. and Zhou, M. 2003. Optimizing Synonym
Extraction Using Monolingual and Bilingual Re-
sources. In Proc. of the 2nd Int. Workshop on Para-
phrasing, pp. 72-79.

Moraliyski, R., and Dias, G. 2007. One Sense per
Discourse for Synonym Detection.
In Proc. of the
Int. Conf. On Recent Advances in NLP, Bulgaria,
pp. 383-387.

Muller, P., Hathout, N. and Gaume, B. 2006. Synonym
Extraction Using a Semantic Distance on a Dictio-
nary. In Proc. of the 1st Workshop on Graph-Based
Methods for NLP, pp. 65-72.

Needleman, S. B. and Wunsch, C. D. 1970. A General
Method Applicable to the Search for Similarities in
the Amino Acid Sequence of two Proteins. In Jour-
nal of Molecular Biology 48(3), pp. 443-453.

Ohshima, H. and Tanaka, K. 2009. Real Time Ex-
traction of Related Terms by Bi-directional lexico-
syntactic Patterns from the Web. In Proc. of the 3rd
Int. Conf. on Ubiquitous Information Management
and Communication, pp. 441-449.

Riloff, E. 1993. Automatically Constructing a Dic-
tionary for Information Extraction Tasks. In Proc.
of the 11th Nat. Conf. on Artiﬁcial Intelligence, pp.
811-816.

Roark, B. and Charniak, E.

1998. Noun-phrase
Co-occurrence Statistics for Semiautomatic Seman-
tic Lexicon Construction.
In Proc. of the 17th
Int. Conf. on Computational Linguistics, pp. 1110-
1116.

Smith, T. and Waterman, M. 1981. Identiﬁcation of
In Journal of

Common Molecular Subsequences.
Molecular Biology 147, pp. 195-197.

Sowa, J. F. and Siekmann, J. H.

1994. Concep-
tual Structures: Current Practices. Springer-Verlag
New York, Inc., Secaucus, NJ, USA.

Stevenson, M. and Greenwood, M. 2006. Compar-
ing Information Extraction Pattern Models. In Proc.
of the Workshop on Information Extraction Beyond
the Document, ACL, pp. 29-35.

Terra, E. and Clarke, C. 2003. Frequency Estimates
In Proc.

for Statistical Word Similarity Measures.
of HTL/NAACL 2003, pp. 165-172.

Turney, P. D. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167, pp. 491-502.

Turney, P. D., Littman, M. L., Bigham, J. and Shnay-
der, V. 2003. Combining Independent Modules in
Lexical Multiple-choice Problems.
In Recent Ad-
vances in NLP III: Selected Papers, pp. 101-110.

Turney, P. D. 2008. A Uniform Approach to Analogies,
Synonyms, Antonyms and Associations. In Proc .of
the 22nd Int. Conf. on Computational Linguistics,
pp. 905-912.

