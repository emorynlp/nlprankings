



















































Knowledge Base Unification via Sense Embeddings and Disambiguation


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 726–736,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Knowledge Base Unification via Sense Embeddings and Disambiguation

Claudio Delli Bovi
Department of

Computer Science
Sapienza University of Rome
dellibovi@di.uniroma1.it

Luis Espinosa-Anke
Department of Information and
Communication Technologies

Universitat Pompeu Fabra
luis.espinosa@upf.edu

Roberto Navigli
Department of

Computer Science
Sapienza University of Rome
navigli@di.uniroma1.it

Abstract

We present KB-UNIFY, a novel approach
for integrating the output of different
Open Information Extraction systems into
a single unified and fully disambiguated
knowledge repository. KB-UNIFY con-
sists of three main steps: (1) disambigua-
tion of relation argument pairs via a sense-
based vector representation and a large
unified sense inventory; (2) ranking of se-
mantic relations according to their degree
of specificity; (3) cross-resource relation
alignment and merging based on the se-
mantic similarity of domains and ranges.
We tested KB-UNIFY on a set of four
heterogeneous knowledge bases, obtain-
ing high-quality results. We discuss and
provide evaluations at each stage, and re-
lease output and evaluation data for the use
and scrutiny of the community1.

1 Introduction

The breakthrough of the Open Information Ex-
traction (OIE) paradigm opened up a research
area where Web-scale unconstrained Information
Extraction systems are developed to acquire and
formalize large quantities of knowledge. How-
ever, while successful, to date most state-of-the-
art OIE systems have been developed with their
own type inventories, and no portable ontologi-
cal structure. In fact, OIE systems can be very
different in nature. Early approaches (Etzioni et
al., 2008; Wu and Weld, 2010; Fader et al., 2011)
focused on extracting a large number of relations
from massive unstructured corpora, mostly rely-
ing on dependencies at the level of surface text.
Systems like NELL (Carlson et al., 2010) com-
bine a hand-crafted taxonomy of entities and re-
lations with self-supervised large-scale extraction

1http://lcl.uniroma1.it/kb-unify

from the Web, but they require additional process-
ing for linking and integration (Dutta et al., 2014).

More recent work has focused, instead, on
deeper language understanding, especially at the
level of syntax and semantics (Nakashole et al.,
2012; Moro and Navigli, 2013). By leveraging
semantic analysis, knowledge gathered from un-
structured text can be adequately integrated and
used to enrich existing knowledge bases, such
as YAGO (Mahdisoltani et al., 2015), FREEBASE
(Bollacker et al., 2008) and DBPEDIA (Lehmann
et al., 2014). A large amount of reliable struc-
tured knowledge is crucial for OIE approaches
based on distant supervision (Mintz et al., 2009;
Riedel et al., 2010), even when multi-instance
multi-learning algorithms (Surdeanu et al., 2012)
or matrix factorization techniques (Riedel et al.,
2013; Fan et al., 2014) come into play to deal
with noisy extractions. For this reason a recent
trend of research has focused on Knowledge Base
(KB) completion (Nickel et al., 2012; Bordes et
al., 2013), exploiting the fact that distantly super-
vised OIE and structured knowledge can comple-
ment each other. However, the majority of integra-
tion approaches nowadays are not designed to deal
with many different resources at the same time.

We propose an approach where the key idea is to
bring together knowledge drawn from an arbitrary
number of OIE systems, regardless of whether
these systems provide links to some general-
purpose inventory, come with their own ad-hoc
structure, or have no structure at all. Knowledge
from each source, in the form of 〈subject, predi-
cate, object〉 triples, is disambiguated and linked
to a single large sense inventory. This enables us
to discover alignments at a semantic level between
relations from different KBs, and to generate a
unified, fully disambiguated KB of entities and
semantic relations. KB-UNIFY achieves state-
of-the-art disambiguation and provides a general,
resource-independent representation of semantic
relations, suitable for any kind of KB.

726



The remainder of this paper is structured as fol-
lows: Section 2 reviews relevant related work;
Sections 3, 4, 5 and 6 describe in detail each stage
of the approach; Sections 7 and 8 describe the ex-
periments carried out and the results obtained; and
finally Section 9 summarizes our findings and dis-
cusses potential directions for future work.

2 Related Work

The integration of knowledge drawn from dif-
ferent sources has received much attention over
the last decade. Among the most notable examples
are resources like BabelNet (Navigli and Ponzetto,
2012), UBY (Gurevych et al., 2012) and YAGO
(Mahdisoltani et al., 2015). While great effort
has been put into aligning knowledge at the con-
cept level, most approaches do not tackle the prob-
lem of integrating heterogeneous knowledge at the
relation level, nor do they exploit effectively the
huge amount of information harvested with OIE
systems, even when this information is unambigu-
ously linked to a structured resource, as in (Nakas-
hole et al., 2012), or (Moro and Navigli, 2013).
In fact, as the number of resources increases, KB
alignment is already becoming an emergent re-
search field: Dutta et al. (2014) describe a method
for linking arguments in NELL triples to DBPE-
DIA by combining First Order Logic and Markov
Networks; Grycner and Weikum (2014) seman-
tify PATTY’s pattern synsets and connect them
to WordNet verbs; Lin et al. (2012) propose a
method to propagate FREEBASE types across RE-
VERB and deal with the problem of unlinkable
entities. All these approaches achieve very com-
petitive results in their respective settings, but un-
like the approach being proposed here, they limit
the task to 1-to-1 alignments. A few contributions
have tried to broaden the scope and include dif-
ferent resources at the same time, but with rather
different goals from ours. For example, Riedel et
al. (2013) propose a universal schema that inte-
grates structured data with OIE data by learning
latent feature vectors for entities and relations; the
KNOWLEDGE VAULT (Dong et al., 2014) uses a
graph-based probabilistic framework where prior
knowledge from existing resources (e.g. FREE-
BASE) improves Web extractions by predicting
their reliability. However, in both cases the main
objective is distantly supervised extraction from
unstructured text, rather than KB unification. A re-
cent trend of research focuses on learning embed-
ding models for structured knowledge and their

application to tasks like relation extraction and KB
completion (Socher et al., 2013; Weston et al.,
2013; Bordes et al., 2013). These approaches,
however, leverage embeddings at surface level,
which are suboptimal for our task, as will be dis-
cussed in Section 3. Since we require a com-
mon semantic framework for KB unification, we
use vector representations based on word senses,
which are mapped to a very large sense inventory.
This shared sense inventory, then, constitutes the
common ground in which disambiguation, align-
ment and final unification occurs.

3 Knowledge Base Unification: Overview

KB-UNIFY takes as input a set of KBs K =
{KB1, ...,KBn} and outputs a single, unified and
fully disambiguated KB, denoted as KB∗. For
our purposes we can define a KB KBi as a triple
〈Ei, Ri, Ti〉, where Ei is a set of entities, Ri is
a set of semantic relations, and Ti is a set of
triples (facts) 〈ed, r, eg〉 with subject and object
ed, eg ∈ Ei and predicate r ∈ Ri. Depending
on the nature of each KBi, entities in Ei might
be disambiguated and linked to an external in-
ventory (e.g. the entity Washington linked to the
Wikipedia page GEORGE WASHINGTON), or un-
linked and only available as ambiguous mentions
(e.g. the bare word washington might refer to the
president, the city or the state). We can thus par-
tition K into a subset of linked resources KD, and
one of unlinked resources KU . In order to align
very different and heterogeneous KBs at the se-
mantic level, KB-UNIFY exploits:

• A unified sense inventory S, which acts as
a superset for the inventories of individual
KBs. We choose BabelNet (Navigli and
Ponzetto, 2012) for this purpose: by merg-
ing complementary knowledge from different
resources (e.g. Wikipedia, WordNet, Wiki-
data and Wiktionary, among others), Babel-
Net provides a wide coverage of entities and
concepts whilst at the same time enabling
convenient inter-resource mappings for KBi
in KD. For instance, each Wikipedia page (or
Wikidata item) has a corresponding synset in
BabelNet, which enables a one-to-one map-
ping between BabelNet’s synsets and entries
in, e.g., DBPEDIA or FREEBASE;

• A vector space model VS that enables a se-
mantic representation for every item in S.
Current distributional models, like word em-

727



Figure 1: Unification algorithm workflow

beddings (Mikolov et al., 2013), are not suit-
able to our setting: they are constrained to
surface word forms, and hence they inher-
ently retain ambiguity of polysemous words
and entity mentions. We thus leverage
SENSEMBED (Iacobacci et al., 2015), a novel
semantically-enhanced approach to embed-
dings. SENSEMBED is trained on a large
annotated corpus and produces continuous
representations for individual word senses
(sense embeddings), according to an under-
lying sense inventory.

Figure 1 illustrates the workflow of our KB uni-
fication approach. Entities coming from any
KBi ∈ KD can be directly (and unambiguously)
mapped to the corresponding entries in S via Ba-
belNet inter-resource linking (Figure 1(a)): in the
above example, the entity Washington linked to
the Wikipedia page GEORGE WASHINGTON is in-
cluded in the BabelNet synset Washington4bn.
In contrast, unlinked (and potentially ambiguous)
entities need an explicit disambiguation step (Fig-
ure 1(b)) connecting them to appropriate entries,
i.e. synsets, in S: this is the case, in the above
example, for the ambiguous mention washington
that has to be linked to either the president, the
city or the state. Therefore, our approach com-
prises two successive stages:

• A disambiguation stage (Section 5) where
all KBi ∈ K are linked to S, either by
inter-resource mapping (Figure 1(a)) or dis-
ambiguation (Figure 1(b)), and all Ei are
merged into a unified set of entities E∗. As
a result of this process we obtain a set KS
comprising all the KBs in K redefined using
the common sense inventory S;

• An alignment stage (Section 6, Figure 1(c))
where, for each pair of KBs KBSi ,KB

S
j ∈

KS , we compare any relation pair 〈ri, rj〉,

ri ∈ RSi and rj ∈ RSj , in order to identify
cross-resource alignments and merge rela-
tions sharing equivalent semantics into rela-
tion clusters (relation synsets). This process
yields a unified set of relation synsets R∗.
The overall result is KB∗ = 〈E∗, R∗, T ∗〉,
where T ∗ is the set of all disambiguated
triples redefined over E∗ and R∗.

4 Background

The disambiguation stage of our approach is
based on the interplay between two core compo-
nents: a vector space model VS , as introduced
in Section 3, which provides an unambiguous se-
mantic representation for each item in S; and a
Word Sense Disambiguation/Entity Linking sys-
tem, working on the same sense inventory S,
which discovers and disambiguates concepts and
entity mentions within a given input text. In this
section we briefly describe our choice for these
two components: SENSEMBED (Iacobacci et al.,
2015) and BABELFY (Moro et al., 2014).

SENSEMBED is a knowledge-based approach
for obtaining latent continuous representations of
individual word senses. Unlike other sense-based
embeddings approaches, like (Huang et al., 2012),
which address the inherent polysemy of word-
level representations relying solely on text cor-
pora, SENSEMBED exploits the structured knowl-
edge of a large sense inventory along with the dis-
tributional information gathered from text corpora.
In order to do this, SENSEMBED requires a sense-
annotated corpus; for each target word sense, then,
a representation is computed by maximizing the
log likelihood of the word sense with respect to
its context within the annotated text, similarly to
the word-based embeddings model. Following Ia-
cobacci et al. (2015), we trained SENSEMBED us-
ing the English Wikipedia and, as sense inventory,
BabelNet.

BABELFY2 is a joint state-of-the-art approach
to multilingual Entity Linking and Word Sense
Disambiguation. Given the BabelNet lexical-
ized semantic network as underlying structure,
BABELFY first models each concept in the net-
work through its corresponding semantic signa-
ture by leveraging a graph random walk algorithm.
Then, given an input text, the generated seman-
tic signatures are used to construct a subgraph

2http://babelfy.org

728



of the semantic network representing the mean-
ing of the content words in that text. BABELFY
then searches this subgraph for the intended sense
of each content word, by means of a densest-
subgraph heuristic that identifies high-coherence
interpretations. Given its unified approach that
covers concepts and named entities alike, and its
flexibility in disambiguating both bag-of-words
and proper text, BABELFY constitutes the most
convenient choice for linking relation triples to a
high-coverage sense inventory like BabelNet.

5 Disambiguation

In the disambiguation phase (Figure 1(b)), all
KBi ∈ KU are linked to the unified sense in-
ventory S and added to the set of redefined KBs
KS . As explained in Section 3, while each KB
in KD can be unambiguously redefined via Babel-
Net inter-resource links and added to KS , KBs in
KU require an explicit disambiguation step. Given
KBi ∈ KU , our disambiguation module (Figure
2) takes as input its set of unlinked triples Ti and
outputs a set TSi ⊆ Ti of disambiguated triples
with subject-object pairs linked to S. The triples in
TSi , together with their corresponding entity sets
and relation sets, constitute the redefined KBSi
which is then added to KS . However, applying
a straightforward approach that disambiguates all
triples in isolation might lead to very imprecise re-
sults, due to the lack of available context for each
individual triple. We thus devised a disambigua-
tion strategy that comprises three successive steps:

1. We identify a set of high-confidence seeds
from Ti (Section 5.1), i.e. triples 〈ed, r, eg〉
where subject ed and object eg are highly
semantically related, and disambiguate them
using the senses that maximize their similar-
ity in our vector space VS ;

2. We use the seeds to generate a ranking of

Figure 2: Disambiguation algorithm workflow

the relations in Ri according to their degree
of specificity (Section 5.2). We represent
each r ∈ Ri in our vector space VS and as-
sign higher specificity to relations whose ar-
guments are closer in VS ;

3. We finally disambiguate the remaining non-
seed triples in Ti (Section 5.3) starting from
the most specific relations, and jointly using
all participating argument pairs as context.

5.1 Identifying Seed Argument Pairs
The first stage of our disambiguation approach

aims at extracting reliable seeds from Ti, i.e.
triples 〈ed, r, eg〉 where subject ed and object eg
can be confidently disambiguated without addi-
tional context. In order to do this we leverage
the sense embeddings associated with each can-
didate disambiguation for ed and eg. We consider
all the available senses for both ed and eg in S,
namely sd = {s1d, ..., smd } and sg = {s1g, ..., sm

′
g },

and the corresponding sets of sense embeddings
vd = {v1d, ..., vmd } and vg = {v1g , ..., vm

′
g }. We

then select, among all possible pairs of senses, the
pair 〈s∗d, s∗g〉 that maximizes the cosine similarity
between the corresponding embeddings 〈v∗d, v∗g〉:

〈v∗d, v∗g〉 = argmaxvd∈vd, vg∈vg
vd · vg
‖vd‖ ‖vg‖ (1)

For each disambiguated triple 〈s∗d, r, s∗g〉, the co-
sine similarity value associated with 〈v∗d, v∗g〉 rep-
resents the disambiguation confidence ζdis. We
rank all such triples according to their confidence,
and select those above a given threshold δdis. The
underlying assumption is that, for high-confidence
subject-object pairs, the embeddings associated
with the correct senses s∗d and s

∗
g will be closest

in VS compared to any other candidate pair. In-
tuitively, the more the relation r between ed and
eg is semantically well defined, the more this as-
sumption is justified. As an example, consider the
triple 〈Armstrong, worked for, NASA〉: among all
the possible senses for Armstrong (the astronaut,
the jazz musician the cyclist, etc.) and NASA (the
space agency, the racing organization, a Swedish
band, etc.) we expect the vectors corresponding to
the astronaut and the space agency to be closest in
the vector space model VS .

5.2 Relation Specificity Ranking
The assumption that, given an ambiguous

subject-object pair, correct argument senses are

729



the closest pair in the vector space (Section 5.1) is
easily verifiable for general relations (e.g. is a, is
part of ). However, as a semantic relation becomes
specific, its arguments are less guaranteed to be
semantically related (e.g. is a professor in the uni-
versity of ) and a disambiguation approach based
exclusively on similarity is prone to errors. On
the other hand, specific relations tend to narrow
down the scope of possible entity types occurring
as subject and object. In the above example, is a
professor in the university of requires entity pairs
with professors as subjects and cities as objects.
Our disambiguation strategy should thus vary ac-
cording to the specificity of the relations taken into
account. In order to consider this observation in
our disambiguation pipeline, we first need to es-
timate the degree of specificity for each relation
in the relation set Ri of the target KB to be dis-
ambiguated. Given Ri and a set of seeds from the
previous stage (Section 5.1), we apply a specificity
ranking policy and sort relations in Ri from the
most general to the most specific. We compute the
generality Gen(r) of a given relation r by looking
at the spatial dispersion of the sense embeddings
associated with its seed subjects and objects. Let
vD (vG) be the set of sense embeddings associated
with the domain (range) seed arguments of r. For
both vD and vG, we compute the corresponding
centroid vectors µD and µG as:

µk =
1
|vk|

∑
v∈vk

v

‖v‖ , k ∈ {D,G} (2)

Then, the variances σ2D and σ
2
G are given by:

σ2k =
1
|vk|

∑
v∈vk

(1− cos (v, µk))2 (3)

with k ∈ {D,G} as before. We finally compute
Gen(r) as the average of σ2D and σ

2
G. The result

of this procedure is a relation specificity ranking
that associates each relation r with its general-
ity Gen(r). Intuitively, we expect more general
relations to show higher variance (hence higher
Gen(r)), as their subjects and objects are likely
to be rather disperse throughout the vector space;
instead, arguments of very specific relations are
more likely to be clustered together in compact re-
gions, yielding lower values of Gen(r).

5.3 Disambiguation with Relation Context

In the third step, both the specificity ranking
and the seeds are exploited to disambiguate the

remaining triples in Ti. To do this we leverage
BABELFY (Moro et al., 2014) (introduced in Sec-
tion 4). As we observed in Section 5.2, spe-
cific relations impose constraints on their subject-
object types and tend to show compact domains
and ranges in the vector space. Therefore, given
a triple 〈ed, r, eg〉, knowing that r is specific en-
ables us to put together all the triples in Ti where
r occurs, and use them to provide meaningful con-
text for disambiguation. If r is general, instead, its
subject-object types are less constrained and addi-
tional triples do not guarantee to provide semanti-
cally related context.

At this stage, our algorithm takes as input the
set of triples Ti, along with the associated disam-
biguation seeds (Section 5.1), the specificity rank-
ing (Section 5.2) and a specificity threshold δspec.
Ti is first partitioned into two subsets: T

spec
i , com-

prising all the triples for which Gen(r) < δspec,
and T geni = Ti \ T speci . We then employ two dif-
ferent disambiguation strategies:

• For each distinct relation r occurring in
T speci , we first retrieve the subset T

spec
i,r ⊂

T speci of triples where r occurs, and then dis-
ambiguate T speci,r as a whole with BABELFY.
For each triple in T speci,r , context is provided
by all the remaining triples along with the
disambiguated seeds extracted for r.

• We disambiguate the remaining triples in
T geni one by one in isolation with BABELFY,
providing for each triple only the predicate
string r as additional context.

6 Cross-Resource Relation Alignment

After disambiguation (Section 5) each KB in
K is linked to the unified sense inventory S and
added to KS . However, eachKBSi ∈ KS still pro-
vides its own relation set RSi ⊆ Ri. Instead, in the
unified KB∗, relations with equivalent semantics
should be considered as part of a single relation
synset even when they come from different KBs.
Therefore, at this stage, we apply an alignment al-
gorithm to identify pairs of relations from different
KBs having equivalent semantics. We exploit the
fact that each relation r is now defined over entity
pairs linked to S, and we generate a semantic rep-
resentation of r in the vector space VS based on
the centroid vectors of its domain and range. Due
to representing the semantics of relations on this
common ground, we can compare them by com-
puting their domain and range similarity in VS . We

730



first consider each KBSi ∈ KS and, for each rela-
tion ri in RSi , we compute the corresponding cen-
troid vectors µrid and µ

ri
g using formula (2). Then,

for each pair of KBs 〈KBSi ,KBSj 〉 ∈ KS × KS ,
we compare all relation pairs 〈ri, rj〉 ∈ RSi × RSj
by computing the cosine similarity between do-
main centroids sD and between range centroids
sG:

sk =
µrik · µ

rj
k

‖µrik ‖ ‖µ
rj
k ‖

(4)

where µrk denotes the centroid associated with re-
lation r and k ∈ {D,G}. The average of sD and
sG gives us an alignment confidence ζalign for the
pair 〈ri, rj〉. If confidence is above a given thresh-
old δalign then ri and rj are merged into the same
relation synset. Relations for which no alignment
is found are turned into singleton relation synsets.
As a result of this alignment procedure we obtain
the unified set of relations R∗.

7 Experimental Setup

The setting for our experimental evaluation was
the following:

• We used BabelNet 3.03 as our unified sense
inventory for the unification procedure as
well as the underlying inventory for both BA-
BELFY and SENSEMBED. Currently, Babel-
Net contains around 14M synsets and repre-
sents the largest single multilingual reposi-
tory of entities and concepts;

• We selected PATTY (Nakashole et al., 2012)
and WISENET (Moro and Navigli, 2013)
as linked resources. We used PATTY with
FREEBASE types and pattern synsets derived
from Wikipedia, and WISENET 2.0 with
Wikipedia relational phrases;

• We selected NELL (Carlson et al., 2010) and
REVERB (Fader et al., 2011) as unlinked
resources. We used KB beliefs updated to
November 2014 for the former, and the set
of relation instances from ClueWeb09 for the
latter.

Comparative statistics in Table 1 show that the
input KBs are rather different in nature: NELL
is based on 298 predefined relations and contains
beliefs for about 2 million entities. The distri-
bution of entities over relations is however very

3http://babelnet.org

KU KD
NELL REVERB PATTY WISENET

# relations 298 1 299 844 1 631 531 245 935
# triples 2 245 050 14 728 268 15 802 946 2 271 807
# entities 1 996 021 3 327 425 1 087 907 1 636 307

Table 1: Statistics on the input KBs

(a)

(b)

Figure 3: Precision (left) and coverage (right) of disam-
biguated seeds at different values of δdis for (a) the whole set
of triples in PATTY and (b) the subset of ambiguous triples

skewed, with 80.33% of the triples being instances
of the generalizations relationship. In con-
trast, REVERB contains a highly sparse relation
set (1,299,844 distinct relations) and more than
3 million distinct entities. PATTY features the
largest (and, together with WISENET, sparsest)
set of triples, with 1,631,531 distinct relations and
less than 10 triples per relation on average.

8 Experiments

8.1 Disambiguation
We tested our disambiguation approach exper-

imentally in terms of both disambiguated seed
quality (Section 8.1.1) and overall disambiguation
performance (Section 8.1.2). We created a de-
velopment set by extracting a subset of 6 million
triples from the largest linked KB in our experi-
mental setup, i.e. PATTY. Triples in PATTY are
automatically linked to YAGO, which is in turn
linked to WordNet and DBPEDIA. Since both re-
sources are also linked by BabelNet, we mapped
the original triples to the BabelNet sense inventory
and used them to tune our disambiguation module.
We also provide two baseline approaches: (1) di-

731



SENSEMBED Baseline
ζdis 0.5-0.7 0.7-0.9 0.9-1.0 0.5-0.7 0.7-0.9 0.9-1.0

PATTY .980 .980 1.000 .793 .780 1.000
WISENET .958 .960 .973 .726 .786 .791
NELL .955 .995 1.000 .800 .770 .885
REVERB .930 .940 .950 .775 .725 .920

Table 2: Disambiguation precision for all KBs

δspec = 0.8 δspec = 0.5 δspec = 0.3
all only seeds all only seeds all only seeds

PATTY 62.15 26.60 52.49 24.06 40.75 21.41
WISENET 60.00 37.46 54.44 22.26 53.58 16.62
NELL 76.97 62.98 50.95 20.71 44.70 4.36
REVERB 41.20 38.57 25.14 23.70 13.37 12.75

Table 3: Coverage results (%) for all KBs

rect disambiguation on individual triples with BA-
BELFY alone (without the seeds) and (2) direct
disambiguation of the seeds only (without BA-
BELFY).

8.1.1 Results: Disambiguated Seeds
We tuned our disambiguation algorithm by

studying the quality of the disambiguated seeds
(Section 5.1) extracted from the surface text triples
of PATTY. Figure 3 shows precision and cover-
age for increasing values of the confidence thresh-
old δdis. We computed precision by checking
each disambiguated seed against the correspond-
ing linked triple in the development set, and cov-
erage as the ratio of covered triples. We analyzed
results for both the whole set of triples in PATTY
(Fig. 3a) and the subset of ambiguous triples (Fig.
3b), i.e. those triples whose subjects and objects
have at least two candidate senses each in the Ba-
belNet inventory. In both cases, precision of dis-
ambiguated seeds increases rapidly with δdis, sta-
bilizing above 90% with δdis > 0.25. Coverage
displays the opposite behavior, decreasing expo-
nentially with more confident outcomes, from 6
million triples to less than a thousand (for seeds
with confidence δdis > 0.95). As a result, we
chose δdis = 0.25 as optimal threshold value
throughout the rest of the evaluations.

In addition, we manually evaluated the disam-
biguated seeds extracted from both linked KBs
(PATTY and WISENET) and unlinked KBs (NELL
and REVERB). For each KB, we extracted up to
three random samples of 150 triples according to
different levels of confidence ζdis: the first sam-
ple included extraction with 0.5 ≤ ζdis < 0.7,
the second with 0.7 ≤ ζdis < 0.9, and the third
with ζdis ≥ 0.9. Each sample was evaluated by
two human judges: for each disambiguated triple

KB-UNIFY Dutta et al. Baseline
all only seeds (α = 0.5)

Precision .852 .957 .931 .749
Recall .875 .117 .799 .608
F-score .864 .197 .857 .671

Table 4: Disambiguation results over NELL gold standard

〈ed, r, eg〉, we presented our judges with the sur-
face text arguments ed, eg and the relation string r,
along with the two BabelNet synsets correspond-
ing to the disambiguated arguments s∗d, s

∗
g, and we

asked whether the association of each subject and
object with the proposed BabelNet synset was cor-
rect. We then estimated precision as the average
proportion of correctly disambiguated triples. For
each sample we compared disambiguation preci-
sion using SENSEMBED, as in Section 5.1, against
the first baseline with BABELFY alone. Results,
reported in Table 2, show that our approach consis-
tently outperforms the baseline and provides high
precision over all samples and KBs.

8.1.2 Results: Disambiguation with Relation
Context

We then evaluated the overall disambiguation
output after specificity ranking (Section 5.2) and
disambiguation with relation context using BA-
BELFY (Section 5.3). We analyzed three config-
urations of the disambiguation pipeline, namely
δspec ∈ {0.8, 0.5, 0.3}. We ran the algorithm over
both linked and unlinked KBs of our experimen-
tal setup, and computed the coverage for each KB
as the overall ratio of disambiguated triples. Re-
sults are reported in Table 3 and compared to the
coverage obtained from the disambiguated seeds
only: context-aware disambiguation substantially
increases coverage over all KBs. Table 3 also
shows that a restrictive δspec results in lower cover-
age values, due to the increased number of triples
disambiguated without context.

Finally, we evaluated the quality of disam-
biguation on a publicly available dataset (Dutta
et al., 2014) comprising manual annotations for
NELL. This dataset provides a gold standard
of 1200 triples whose subjects and objects are
manually assigned a proper DBpedia URI. We
again used BabelNet’s inter-resource links to ex-
press DBpedia annotations with our sense inven-
tory and then sought, for each annotated triple in
the dataset, the corresponding triple in our disam-
biguated version of NELL with δdis = 0.25 and
δspec = 0.8. We then repeated this process con-

732



Figure 4: Average argument similarity against Gen(r)

NELL REVERB PATTY WISENET
Precision .660 .715 .625 .750
Cohen’s kappa - .430 .620 .600

Table 5: Specificity ranking evaluation

sidering only the disambiguated seeds instead of
the whole disambiguation pipeline. In line with
(Dutta et al., 2014), we computed precision, recall
and F-score for each setting. Results are reported
in Table 4 and compared against those of Dutta et
al. (2014) and against our first baseline with BA-
BELFY alone. KB-UNIFY achieves the best result,
showing that a baseline based on state-of-the-art
disambiguation is negatively affected by the lack
of context for each individual triple. In contrast,
an approach that relies only on the disambiguated
seeds affords very high precision, but suffers from
dramatically lower coverage.

8.2 Specificity Ranking

We evaluated the specificity ranking (Section
5.2) generated by KB-UNIFY for all KBs of our
experimental setup. First of all, we empirically
validated our scoring function Gen(r) over each
resource: for each relation we computed the aver-
age cosine similarity among all its domain argu-
ments s̄D and among all its range arguments s̄G.
We then plotted the average s̄ of s̄D and s̄G against
Gen(r) for each relation r (Figure 4). As observed
in Section 5.2, the average similarity among do-
main and range arguments decreases for increas-
ing values of Gen(r), indicating that more gen-
eral relations allow less semantically constrained
subject-object types. We then used human judge-
ment to assess the quality of our specificity rank-
ings. First, each ranking was split into four quar-

NELL

High Gen(r) agent created
at location

Low Gen(r) person in economic sector
restaurant in city

REVERB

High Gen(r) is for
is in

Low Gen(r) enter Taurus in
carry oxygen to

PATTY

High Gen(r) located in
later served to

Low Gen(r) starting pitcher who played
league coach for

WISENET

High Gen(r) include
is a type of

Low Gen(r) lobe-finned fish lived during
took part in the Eurovision contest

Table 6: Examples of general and specific relations for all
KBs

tiles, and two human evaluators were presented
with a sample from the top quartile (i.e. a relation
falling into the most general category) and a sam-
ple from the bottom quartile (i.e. a relation falling
into the most specific category). We shuffled each
relation pair, showed it to our human judges, and
then asked which of the two relations they consid-
ered to be the more specific. Ranking precision
was computed by considering those pairs where
human choice agreed with the ranking. Finally,
we computed inter-annotator agreement on each
specificity ranking (except for NELL, due to the
small sample size) with Cohen’s kappa coefficient
(Cohen, 1968). Results for each ranking are re-
ported in Table 5, while some examples of general
and specific relations for each KB are shown in
Table 6. Disagreement between human choice and
ranking is higher in NELL (where the set of rela-
tions is quite small compared to other KBs) and in
PATTY (due to a sparser set of relations, biased
towards very specific patterns). Inter-annotator
agreement is instead lower for REVERB, where
unconstrained Web harvesting often results in am-
biguous relation strings.

8.3 Alignment

Due to the novelty of our approach, and hence
the lack of widely accepted gold standards and
testbeds, we evaluated our cross-resource relation
alignment algorithm (Section 6) by exploiting hu-
man judgement once again. Given the results of

733



PATTY-WISENET PATTY-REVERB NELL-REVERB
δalign 0.7 0.9 0.7 0.9 0.7 0.9

Prec. .68 .80 .58 .74 .61 .75
# Align. 128k 1.2k 47k 643 2.6k 88

PATTY-NELL WISENET-NELL WISENET-REVERB
δalign 0.7 0.9 0.7 0.9 0.7 0.9

Prec. .66 1.00 .70 .84 .59 .87
# Align. 2.6k 57 381 34 9.9k 169

Table 7: Cross-resource alignment evaluation

PATTY-WISENET ζalign
portrayed ’s character 0.84
debuted in first appeared in 0.86

PATTY-REVERB ζalign
language in is spoken in 0.81
mostly known for plays the role of 0.70

NELL-REVERB ζalign
bookwriter is a novel by 0.88
personleadscity is the mayor of 0.60

NELL-PATTY ζalign
worksfor was hired by 0.72
riveremptiesintoriver tributary of 0.89

NELL-WISENET ζalign
animaleatfood feeds on 0.72
teamhomestadium play their home games at 0.88

REVERB-WISENET ζalign
has a selection of offers 0.82
had grown up in was born and raised in 0.85

Table 8: Examples of cross resource relation alignments

Section 8.1, we considered the top 10k frequent
relations for each KB and ran the algorithm over
each possible pair of KBs with two different con-
figurations: δalign = 0.7 and δalign = 0.9. From
each pair of KBs 〈KBi,KBj〉 we obtained a list
of candidate alignments, i.e. pairs of relations
〈ri, rj〉 where ri ∈ KBi and rj ∈ KBj . From
each list we then extracted a random sample of
150 candidate alignments. We showed each align-
ment4 〈ri, rj〉 to two human judges, and asked
whether ri and rj represented the same relation.
The problem was presented in terms of paraphras-
ing: for each pair, we asked whether exchanging
ri and rj within a sentence would have changed
that sentence’s meaning. In line with Section 8.2
we computed precision based on the agreement
between human choice and automatic alignments.
Results are reported in Table 7. Our alignment al-
gorithm shows high precision in all pairings where
δalign = 0.9. Alignment reliability decreases for
lower δalign, as relation pairs where ri is a gener-
alization of rj (or vice versa) tend to have similar
centroids in VS . The same holds for pairs where ri
is the negation of rj (or vice versa). Even though
we could have utilized measures based on rela-

4In the case of relation synsets, such as PATTY and
WISENET, we selected up to three random relation strings
from each synset.

tion string similarity (Dutta et al., 2015) to reduce
wrong alignments in these cases, by relying on a
purely semantic criterion we removed any prior as-
sumption on the format of input KBs. Some exam-
ples of alignments are shown in Table 8.

To conclude, we report statistics regarding the
unified KB∗ produced from the initial set of re-
sources in our experimental setup (cf. Section
7). We validated our thresholds for high-precision,
and selected δdis = 0.25, δspec = 0.8 and δalign
= 0.8. Our alignment algorithm produced 56,673
confident alignments, out of which 2,207 relation
synsets were derived, with an average size of 16.82
individual relations per synset. As a result, we ob-
tained a unified KB∗ comprising 24,221,856 dis-
ambiguated triples defined over 1,952,716 distinct
entities and 2,675,296 distinct relations.

9 Conclusion and Future Work

We have presented KB-UNIFY, a novel, gen-
eral approach for disambiguating and seamlessly
unifying KBs produced by different OIE sys-
tems. KB-UNIFY represents entities and relations
using a shared semantic representation, leverag-
ing a unified sense inventory together with a
semantically-enhanced vector space model and a
disambiguation algorithm. This enables us to dis-
ambiguate unlinked resources (like NELL and RE-
VERB) with high precision and coverage, and to
discover relation-level cross-resource alignments
effectively. One of the key features of our strat-
egy is its generality: by representing each KB on
a common ground, we need no prior assumption
on the nature and format of the knowledge it en-
codes. We tested our approach experimentally on
a set of four very different KBs, both linked and
unlinked, and we evaluated disambiguation and
alignment results extensively at every stage, ex-
ploiting both human evaluations and public gold
standard datasets (when available). This work
opens compelling avenues for future work. We
plan to further exploit sense-enhanced unified rep-
resentations of relations in various ways: provid-
ing an ontological structure for the unified KB,
exploring complementary approaches for captur-
ing semantic relation alignments, and incorporat-
ing multilinguality.

Acknowledgments

The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.

734



References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim

Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database For Structur-
ing Human Knowledge. In Proceedings of SIG-
MOD, pages 1247–1250.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating Embeddings for Modeling Multi-
relational Data. In Advances in NIPS, volume 26,
pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. In Proceedings of
AAAI, pages 1306–1313.

Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or
Partial Credit. Psychological Bulletin, 70(4):213–
220.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge Vault: A Web-scale Approach to Probabilistic
Knowledge Fusion. In Proceedings of the SIGKDD,
pages 601–610.

Arnab Dutta, Christian Meilicke, and Simone Paolo
Ponzetto. 2014. A Probabilistic Approach for Inte-
grating Heterogeneous Knowledge Sources. In Pro-
ceedings of ESWC, pages 286–301.

Arnab Dutta, Christian Meilicke, and Heiner Stucken-
schmidt. 2015. Enriching Structured Knowledge
with Open Information. In Proceedings of WWW,
pages 267–277.

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open Information Extraction
from the Web. Commun. ACM, 51(12):68–74.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of EMNLP, pages 1535–
1545.

Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,
Thomas Fang Zheng, and Edward Y. Chang. 2014.
Distant Supervision for Relation Extraction with
Matrix Completion. In Proceedings of ACL, pages
839–849.

Adam Grycner and Gerhard Weikum. 2014. HARPY:
Hypernyms and Alignment of Relational Para-
phrases. In Proceedings of ACL, pages 2195–2204.

Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby: A large-scale unified
lexical-semantic resource based on LMF. In Pro-
ceedings of ACL, pages 580–590.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873–882.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Simi-
larity. In Proceedings of ACL, pages 95–105.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, Sören Auer, and Christian Bizer. 2014. DB-
pedia - A Large-scale, Multilingual Knowledge Base
Extracted from Wikipedia. Semantic Web Journal,
pages 1–29.

Thomas Lin, Mausam, and Oren Etzioni. 2012. No
Noun Phrase Left Behind: Detecting and Typing
Unlinkable Entities. In Proceedings of EMNLP-
CoNLL, pages 893–903.

Farzaneh Mahdisoltani, Joanna Biega, and Fabian M.
Suchanek. 2015. YAGO3: A Knowledge Base from
Multilingual Wikipedias. In CIDR.

Tomas Mikolov, Kal Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant Supervision for Relation Extrac-
tion Without Labeled Data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.

Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proceedings of
IJCAI, pages 2148–2154.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. TACL, 2:231–
244.

Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A Taxonomy
of Relational Patterns with Semantic Types. In
Proceedings of EMNLP-CoNLL, pages 1135–1145.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The Automatic Construction, Evaluation
and Application of a Wide-Coverage Multilingual
Semantic Network. Artificial Intelligence, 193:217–
250.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: Scalable Ma-
chine Learning for Linked Data. In Proceedings of
WWW, pages 271–280.

735



Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling Relations and Their Mentions
Without Labeled Text. In Proceedings of ECML-
PKDD, pages 148–163.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In Proceedings of NAACL, pages 74–84.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with Neural
Tensor Networks for Knowledge Base Completion.
In Advances in NIPS, pages 926–934.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
Multi-label Learning for Relation Extraction. In
Proceedings of EMNLP-CoNLL, pages 455–465.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting Language
and Knowledge Bases with Embedding Models for
Relation Extraction. In Proceedings of EMNLP,
pages 1366–1371.

Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction using Wikipedia. In Proceedings of ACL,
pages 118–127.

736


