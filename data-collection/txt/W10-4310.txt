










































Using entity features to classify implicit discourse relations


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59–62,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

Using entity features to classify implicit discourse relations

Annie Louis, Aravind Joshi, Rashmi Prasad, Ani Nenkova

University of Pennsylvania
Philadelphia, PA 19104, USA

{lannie,joshi,rjprasad,nenkova}@seas.upenn.edu

Abstract

We report results on predicting the sense

of implicit discourse relations between ad-

jacent sentences in text. Our investigation

concentrates on the association between

discourse relations and properties of the

referring expressions that appear in the re-

lated sentences. The properties of inter-

est include coreference information, gram-

matical role, information status and syn-

tactic form of referring expressions. Pre-

dicting the sense of implicit discourse re-

lations based on these features is consid-

erably better than a random baseline and

several of the most discriminative features

conform with linguistic intuitions. How-

ever, these features do not perform as well

as lexical features traditionally used for

sense prediction.

1 Introduction

Coherent text is described in terms of discourse re-

lations such as “cause” and “contrast” between its

constituent clauses. It is also characterized by en-

tity coherence, where the connectedness of the text

is created by virtue of the mentioned entities and

the properties of referring expressions. We aim to

investigate the association between discourse rela-

tions and the way in which references to entities

are realized. In our work, we employ features re-

lated to entity realization to automatically identify

discourse relations in text.

We focus on implicit relations that hold be-

tween adjacent sentences in the absence of dis-

course connectives such as “because” or “but”.

Previous studies on this task have zeroed in on

lexical indicators of relation sense: dependencies

between words (Marcu and Echihabi, 2001; Blair-

Goldensohn et al., 2007) and the semantic orien-

tation of words (Pitler et al., 2009), or on general

syntactic regularities (Lin et al., 2009).

The role of entities has also been hypothesized

as important for this task and entity-related fea-

tures have been used alongside others (Corston-

Oliver, 1998; Sporleder and Lascarides, 2008).

Corpus studies and reading time experiments per-

formed by Wolf and Gibson (2006) have in fact

demonstrated that the type of discourse relation

linking two clauses influences the resolution of

pronouns in them. However, the predictive power

of entity-related features has not been studied in-

dependently of other factors. Further motivation

for studying this type of features comes from new

corpus evidence (Prasad et al., 2008), that about a

quarter of all adjacent sentences are linked purely

by entity coherence, solely because they talk about

the same entity. Entity-related features would be

expected to better separate out such relations.

We present the first comprehensive study of the

connection between entity features and discourse

relations. We show that there are notable differ-

ences in properties of referring expressions across

the different relations. Sense prediction can be

done with results better than random baseline us-

ing only entity realization information. Their per-

formance, however, is lower than a knowledge-

poor approach using only the words in the sen-

tences as features. The addition of entity features

to these basic word features is also not beneficial.

2 Data

We use 590 Wall Street Journal (WSJ) articles

with overlapping annotations for discourse, coref-

erence and syntax from three corpora.

The Penn Discourse Treebank (PDTB) (Prasad

et al., 2008) is the largest available resource of

discourse relation annotations. In the PDTB, im-

plicit relations are annotated between adjacent

sentences in the same paragraph. They are as-

signed senses from a hierarchy containing four top

level categories–Comparison, Contingency, Tem-

poral and Expansion.

59



An example “Contingency” relation is shown

below. Here, the second sentence provides the

cause for the belief expressed in the first.

Ex 1. These rate indications aren’t directly comparable.

Lending practices vary widely by location.

Adjacent sentences can also become related

solely by talking about a common entity without

any of the above discourse relation links between

their propositions. Such pairs are annotated as En-

tity Relations (EntRels) in the PDTB, for example:

Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S

sales to remain steady at about 1,200 cars in 1990. The luxury

auto maker last year sold 1,214 cars in the U.S.

We use the coreference annotations from the

Ontonotes corpus (version 2.9) (Hovy et al., 2006)

to compute our gold-standard entity features. The

WSJ portion of this corpus contains 590 articles.

Here, nominalizations and temporal expressions

are also annotated for coreference but we use the

links between noun phrases only. We expect these

features computed on the gold-standard annota-

tions to represent an upper bound on the perfor-

mance of entity features.

Finally, the Penn Treebank corpus (Marcus et

al., 1994) is used to obtain gold-standard parse and

grammatical role information.

Only adjacent sentences within the same para-

graph are used in our experiments.

3 Entity-related features

We associate each referring expression in a sen-

tence with a set of attributes as described below.

In Section 3.2, we detail how we combine these

attributes to compute features for a sentence pair.

3.1 Referring expression attributes

Grammatical role. In exploratory analysis of

Comparison relations, we often observed parallel

syntactic realizations for entities in the subject po-

sition of the two sentences:

Ex 3. {Longer maturities}E1 are thought to indicate de-

clining interest rates. {Shorter maturities}E2 are considered

a sign of rising rates because portfolio managers can capture

higher rates sooner.

So, for each noun phrase, we record whether

it is the subject of a main clause (msubj), subject

of other clauses in the sentence (esubj) or a noun

phrase not in subject position (other).

Given vs. New. When an entity is first intro-

duced in the text, it is considered a new entity.

Subsequent mentions of the same entity are given

(Prince, 1992). New-given distinction could help

to identify some of the Expansion and Entity re-

lations. When a sentence elaborates on another, it

might contain a greater number of new entities.

We use the Ontonotes coreference annotations

to mark the information status for entities. For

an entity, if an antecedent is found in the previ-

ous sentences, it is marked as given, otherwise it

is a new entity.

Syntactic realization. In Entity relations, the sec-

ond sentence provides more information about a

specific entity in the first and a definite description

for this second mention seems likely. Also, given

the importance of named entities in news, entities

with proper names might be the ones frequently

described using Entity relations.

We use the part of speech (POS) tag associated

with the head of the noun phrase to assign one of

the following categories: pronoun, nominal, name

or expletive. When the head does not belong to

the above classes, we simply record its POS tag.

We also mark whether the noun phrase is a definite

description using the presence of the article ‘the’.

Modification. We expected modification proper-

ties to be most useful for predicting Comparison

relations. Also, named or new entities in Entity

relations are very likely to have post modification.

We record whether there are premodifiers or

postmodifiers in a given referring expression. In

the absence of pre- and postmodifiers, we indicate

bare head realization.

Topicalization. Preposed prepositional or ad-

verbial phrases before the subject of a sentence

indicate the topic under which the sentence is

framed. We observed that this property is frequent

in Comparison and Temporal relations. An exam-

ple Comparison is shown below.

Ex 4. {Under British rules}T1, Blue Arrow was able to

write off at once $1.15 billion in goodwill arising from the

purchase. {As a US-based company}T2, Blue Arrow would

have to amortize the good will over as many as 40 years, cre-

ating a continuing drag on reported earnings.

When the left sibling of a referring expression is

a topicalized phrase, we mark the topic attribute.

Number. Using the POS tag of the head word, we

note whether the entity is singular or plural.

3.2 Features for classification

Next, for each sentence pair, we associate two sets

of features using the attributes described above.

60



Let S1 and S2 denote the two adjacent sentences

in a relation, where S1 occurs first in the text.

Sentence level. These features characterize S1

and S2 individually. For each sentence, we add a

feature for each of the attributes described above.

The value of the feature is the number of times that

attribute is observed in the sentence; i.e., the fea-

ture S1given would have a value of 3 if there are 3

given entities in the first sentence.

Sentence pair. These features capture the interac-

tions between the entities present in S1 and S2.

Firstly, for each pair of entities (a, b), such that

a appears in S1 and b appears in S2, we assign

one of the following classes: (i) SAME: a and b

are coreferent, (ii) RELATED: their head words are

identical, (iii) DIFFERENT: neither coreferent nor

related. The RELATED category was introduced to

capture the parallelism often present in Compari-

son relations. Even though the entities themselves

are not coreferent, they share the same head word

(i.e. longer maturities and shorter maturities).

For features, we use the combination of the

class ((i), (ii) or (iii)) with the cross product of

the attributes for a and b. For example if a has

attributes {msubj, noun, ...} and b has attributes
{esubj, defdesc, ...} and a and b are corefer-
ent, we would increment the count for features–

{sameS1msubjS2esubj, sameS1msubjS2defdesc,
sameS1nounS2esubj, sameS1nounS2defdesc ...}.
Our total set of features observed for instances

in the training data is about 2000.

We experimented with two variants of fea-

tures: one using coreference annotations from

the Ontonotes corpus (gold-standard) and an-

other based on approximate coreference informa-

tion where entities with identical head words are

marked as coreferent.

4 Experimental setup

We define five classification tasks which disam-

biguate if a specific PDTB relation holds between

adjacent sentences. In each task, we classify the

relation of interest (positive) versus a category

with a naturally occurring distribution of all of the

other relations (negative).

Sentence pairs from sections 0 to 22 of WSJ are

used as training data and we test on sections 23

and 24. Given the skewed distribution of positive

and negative examples for each task, we randomly

downsample the negative instances in the training

set to be equal to the positive examples. The sizes

of training sets for the tasks are

Expansion vs other (4716)

Contingency vs other (2466)

Comparison vs other (1138)

Temporal vs other (474)

EntRel vs other (2378)

Half of these examples are positive and the

other negative in each case.

The test set contains 1002 sentence pairs:

Comp. (133), Cont. (230), Temp. (34), Expn.

(369), EntRel (229), NoRel1 (7). We do not down-

sample our test set. Instead, we evaluate our pre-

dictions on the natural distribution present in the

data to get a realistic estimate of performance.

We train a linear SVM classifier (LIBLIN-

EAR2) for each task.3 The optimum regulariza-

tion parameter was chosen using cross validation

on the training data.

5 Results

5.1 Feature analysis

We ranked the features (based on gold-standard

coreference information) in the training sets by

their information gain. We then checked which

attributes are common among the top five features

for different classification tasks.

As we had expected, the topicalization attribute

and RELATED entities frequently appear among

the top features for Comparison.

Features with the name attribute were highly

predictive of Entity relations as hypothesized.

However, while we had expected Entity relations

to have a high rate of coreference, we found coref-

erent mentions to be very indicative of Temporal

relations: all the top features involve the SAME at-

tribute. A post-analysis showed that close to 70%

of Temporal relations involve coreferent entities

compared to around 50% for the other classes.

The number of pronouns in the second sentence

was most characteristic of the Contingency rela-

tion. In the training set for Contingency task,

about 45% of sentences pairs belonging to Contin-

gency relation have a pronoun in the second sen-

tence. This is considerably larger than 32%, which

is the percentage of sentence pairs in the negative

examples with a pronoun in second sentence.

1PDTB relation for sentence pair when both entity and
discourse relations are absent, very rare about 1% of our data.

2http://www.csie.ntu.edu.tw/˜cjlin/liblinear/
3SVMs with linear kernel gave the best performance. We

also experimented with SVMs with radial basis kernel, Naive
Bayes and MaxEnt classifiers.

61



5.2 Performance on sense prediction

The classification results (fscores) are shown in

Table 1. The random baseline (Base.) represents

the results if we predicted positive and negative re-

lations according to their proportion in the test set.

Entity features based on both gold-standard

(EntGS) and approximate coreference (EntApp)

outperform the random baseline for all the tasks.

The drop in performance without gold-standard

coreference information is strongly noticable only

for Expansion relations.

The best improvement from the baseline is seen

for predicting Contingency and Entity relations,

with around 15% absolute improvement in fscore

with both EntGS and EntApp features. The im-

provements for Comparisons and Expansions are

around 11% in the approximate case. Temporal

relations benefit least from these features. These

relations are rare, comprising 3% of the test set

and harder to isolate from other relations. Overall,

our results indicate that discourse relations and en-

tity realization have a strong association.

5.3 Comparison with lexical features

In the context of using entity features for sense

prediction, one would also like to test how these

linguistically rich features compare with simpler

knowledge-lean approaches used in prior work.

Specifically, we compare with word pairs, a

simple yet powerful set of features introduced by

Marcu and Echihabi (2001). These features are the

cross product of words in the first sentence with

those in the second.

We trained classifiers on the word pairs from the

sentences in the PDTB training sets. In Table 1,

we report the performance of word pairs (WP) as

well as their combination with gold-standard en-

tity features (WP+EntGS). Word pairs turn out as

stronger predictors for all discourse relations com-

pared to our entity features (except for Expansion

prediction with EntGS features). Further, no ben-

efits over word pair results are obtained by com-

bining entity realization information.

6 Conclusion

In this work, we used a task-based approach to

show that the two components of coherence—

discourse relations and entities—are related and

interact with each other. Coreference, givenness,

syntactic form and grammatical role of entities can

predict the implicit discourse relation between ad-

Task Base. EntGS EntApp WP WP+EntGS
Comp vs Oth. 13.27 24.18 24.14 27.30 26.19
Cont vs Oth. 22.95 37.57 38.16 38.17 38.99
Temp vs Oth. 3.39 7.58 5.61 11.09 10.04
Expn vs Oth. 36.82 52.42 47.82 48.54 49.06
Ent vs Oth. 22.85 38.03 36.73 38.48 38.14

Table 1: Fscore results

jacent sentences with results better than random

baseline. However, with respect to developing au-

tomatic discourse parsers, these entity features are

less likely to be useful. They do not outperform

or complement simpler lexical features. It would

be interesting to explore whether other aspects of

entity reference might be useful for this task, such

as bridging anaphora. But currently, annotations

and tools for these phenomena are not available.

References

S. Blair-Goldensohn, K. McKeown, and O. Rambow.
2007. Building and refining rhetorical-semantic re-
lation models. In HLT-NAACL.

S.H. Corston-Oliver. 1998. Beyond string matching
and cue phrases: Improving efficiency and coverage
in discourse analysis. In The AAAI Spring Sympo-
sium on Intelligent Text Summarization.

E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: the 90% solution.
In NAACL-HLT.

Z. Lin, M. Kan, and H.T. Ng. 2009. Recognizing im-
plicit discourse relations in the Penn Discourse Tree-
bank. In EMNLP.

D. Marcu and A. Echihabi. 2001. An unsupervised ap-
proach to recognizing discourse relations. In ACL.

M. Marcus, B. Santorini, and M. Marcinkiewicz. 1994.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics.

E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In ACL-IJCNLP.

R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
penn discourse treebank 2.0. In LREC.

E. Prince. 1992. The zpg letter: subject, definiteness,
and information status. In Discourse description:
diverse analyses of a fund raising text, pages 295–
325. John Benjamins.

C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369–416.

F. Wolf and E. Gibson. 2006. Coherence in natural
language: data structures and applications. MIT
Press.

62


