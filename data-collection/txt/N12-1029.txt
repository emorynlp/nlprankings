










































Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 284–294,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Correcting Comma Errors in Learner Essays, and Restoring Commas in
Newswire Text

Ross Israel
Indiana University
Memorial Hall 322

Bloomington, IN 47405, USA
raisrael@indiana.edu

Joel Tetreault
Educational Testing Service

660 Rosedale Road
Princeton, NJ 08541, USA
jtetreault@ets.org

Martin Chodorow
Hunter College of CUNY

695 Park Avenue
New York, NY 10065, USA

mchodoro@hunter.cuny.edu

Abstract

While the field of grammatical error detection
has progressed over the past few years, one
area of particular difficulty for both native and
non-native learners of English, comma place-
ment, has been largely ignored. We present a
system for comma error correction in English
that achieves an average of 89% precision and
25% recall on two corpora of unedited student
essays. This system also achieves state-of-the-
art performance in the sister task of restor-
ing commas in well-formed text. For both
tasks, we show that the use of novel features
which encode long-distance information im-
proves upon the more lexically-driven features
used in prior work.

1 Introduction

Automatically detecting and correcting grammati-
cal errors in learner language is a growing sub-field
of Natural Language Processing. As the field has
progressed, we have seen research focusing on a
range of grammatical phenomena including English
articles and prepositions (c.f. Tetreault et al., 2010;
De Felice and Pulman, 2008), particles in Korean
and Japanese (c.f. Dickinson et al., 2011; Oyama,
2010), and broad approaches that aim to find mul-
tiple error types (c.f Rozovskaya et al., 2011; Ga-
mon, 2011). However, to the best of our knowledge,
there has not been any research published specifi-
cally on correcting erroneous comma usage in En-
glish (though there have been efforts such as the MS
Word grammar checker, and products like Gram-
marly and White Smoke that include comma check-
ing).

There are a variety of reasons that motivate our
interest in attempting to correct comma errors. First
of all, a review of error typologies in Leacock et al.
(2010) reveals that comma usage errors are the
fourth most common error type among non-native
writers in the Cambridge Learner Corpus (Nicholls,
1999), which is composed of millions of words of
text from essays written by learners of English. The
problem of comma usage is not limited to non-
native writers; six of the top twenty error types for
native writers involve misuse of commas (Connors
and Lunsford, 1988). Given these apparent deficits
among both non-native and native speakers, devel-
oping a sound methodology for automatically iden-
tifying comma errors will prove useful in both learn-
ing and automatic assessment environments.

A quick examination of English learner essays re-
veals a variety of errors, with writers both overusing
and underusing commas in certain contexts. Con-
sider examples (1) and (2):

(1) erroneous: If you want to be a master you
should know your subject well.
corrected: If you want to be a master , you
should know your subject well.

(2) erroneous: I suppose , that it is better to spe-
cialize in one specific subject.
corrected: I suppose that it is better to special-
ize in one specific subject.

In example (1), an introductory conditional phrase
begins the sentence, but the learner has not used the
appropriate comma to separate the dependent clause
from the independent clause. The comma in this
case helps the reader to see where one clause ends

284



and another begins. In example (2), the comma after
suppose is unnecessary in American English, and al-
though this error is related more to style than to read-
ability, most native writers would omit the comma
in this context, so it should be avoided by learners as
well.

Another motivating factor for this work is the fact
that sentence internal punctuation contributes to the
overall readability of a sentence (Hill and Murray,
1998). Proper comma placement can lead to faster
reading times and reduce the need to re-read en-
tire sentences. Commas also help remove or reduce
problems arising from difficult ambiguities; the gar-
den path effect can be greatly reduced if commas are
correctly inserted after introductory phrases and re-
duced relative clauses.

This paper makes the following contributions:

• We present the first published comma error cor-
rection system for English, evaluated on essays
written by both native and non-native speakers
of English.

• The same system also achieves state-of-the-art
performance in the task of restoring commas in
well-edited text.

• We describe a novel annotation scheme that al-
lows for robust mark up of comma errors and
use it to annotate two corpora of student essays.

• We show that distance and combination fea-
tures can improve performance for both the er-
ror correction and restoration tasks.

The rest of this paper is organized as follows.
In section 2, we review prior work. Section 3 de-
tails our typology of comma usage. We discuss our
choice of classifier and selection of features in sec-
tion 4. In section 5, we apply our system to the task
of comma restoration. We describe our annotation
scheme and error correction system and evaluation
in sections 6 and 7. Finally, we summarize and out-
line plans for future research in section 8.

2 Previous Work

The only reported research that we are aware
of which specifically deals with comma errors in

learner writing is reported in Hardt (2001) and Ale-
gria et al. (2006), two studies that deal with Dan-
ish and Basque, respectively. Hardt (2001) employs
an error driven approach featuring the Brill tagger
(Brill, 1993). The Brill tagger works as it would
for the part-of-speech tagging task for which it was
designed, i.e. it learns rules based on templates by
iterating over a large corpus. This work is also eval-
uated on native text where all existing commas are
considered correct, and additional “erroneous” com-
mas are added randomly to a sub-corpus, so that the
tagger can learn from the errors. The system is tested
on a distinct subset for the task of correcting exist-
ing comma errors and achieves 91.4% precision and
76.9% recall.

Alegria et al. (2006) compare implementations
of Naive Bayes, decision-tree, and support vector
machine (SVM) classifiers and utilize a feature set
based on word-forms, categories, and syntactic in-
formation about each decision point. While the sys-
tem is designed as a possible means for correcting
errors, it is only evaluated on the task of restor-
ing commas in well-formed text produced by native
writers. The system obtains good precision (96%)
and recall (98.3%) for correctly not inserting com-
mas, but performs less well at actually inserting
commas (69.6% precision, 48.6% recall).

It is important to note that the results in both of the
projects are based on constructed errors in an other-
wise native corpus which is free of any other con-
textual errors that might be present in actual learner
data. Moreover, as we will show in section 6, er-
rors of omission (failing to use needed commas) are
much more common than errors of commission (in-
serting commas inappropriately) in the English as
a Foreign Language (EFL) data that we use. Cru-
cially, our error correction efforts described in sec-
tion 7 must be able to account for noise and be able
to insert new commas as well as remove erroneous
ones, as we do evaluate on a set of English learner
essays.

Although we have not found any work published
specifically on correcting comma errors in English,
for language learners or otherwise, there is a fairly
large amount of work that focuses on the task of
comma restoration. Comma restoration refers to
placing commas in a sentence which is presented
with no sentence internal punctuation. This task is

285



mostly attempted in the larger context of Automatic
Speech Recognition (ASR), since there are no ab-
solute cues of where commas should be placed in a
stream of speech. Many of these systems use feature
sets that include prosodic elements that are clearly
not available for text based work (see e.g., Favre
et al., 2009; Huang and Zweig, 2002; Moniz et al.,
2009).

There are, however, a few punctuation restora-
tion projects that have used well-formed text-only
data. Shieber and Tao (2003) explore restoring com-
mas to the Wall Street Journal (WSJ) section of
the Penn Treebank (PTB). The authors augment a
HMM trigram-based system with constituency parse
information at each insertion point. Using fully
correct parses directly from the PTB, the authors
achieve an F-score of 74.8% and sentence accuracy
of 57.9%1. However, a shortcoming of this method-
ology is that it dictates that all commas are missing,
but these parses were generated with comma infor-
mation present in the sentence and moreover hand-
corrected by human annotators. Using parses auto-
matically generated with commas removed from the
data, they achieve an F-score of 70.1% and sentence
accuracy of 54.9%.

More recently, Gravano et al. (2009), who work
with newswire text, including WSJ, pursue the task
of inserting all punctuation and correcting capital-
ization in a string of text in a single pass, rather
than just comma restoration, but do provide results
based solely on comma insertion. The authors em-
ploy an n-gram language model and experiment with
n-grams from size n = 3 to n = 6, and with different
training data sizes. The result relevant to our work is
their comma F-score on WSJ test data, which is just
over 60% when using 5-grams and 55 billion train-
ing tokens. Baldwin and Joseph (2009) also restore
punctuation and capitalization to newswire texts, us-
ing machine based learning with retagging. Their
results are difficult to compare with our work be-
cause they use a different data set and do not focus
on commas in their evaluation.

Lu and Ng (2010) take an approach that inserts all

1Sentence accuracy is a measure used by some in the field
that counts sentences with 100% correct comma decisions as
correct, and any sentence where a comma is missing or mis-
takenly placed as incorrect. It is motivated by the idea that all
commas are essential to understanding a sentence.

punctuation symbols into text. They use transcribed
English and Chinese speech data and do not provide
specific evaluation for commas, however one im-
portant contribution of their research to our current
task is the finding that Conditional Random Fields
(CRFs) perform better at this task than Hidden Event
Language Models, another algorithm that has been
used for restoration. One reason for this could be
CRFs’ better handling of long range dependencies
because they model the entire sequence, rather than
making a singular decision based on information at
each point in the sequence (Liu et al., 2005). CRFs
also do not suffer from the label bias problem that
affects Maximum Entropy classifiers (Lafferty et al.,
2001).

3 Comma Usage

One of the challenges present in this research is the
ambiguity as to what constitutes “correct” comma
usage in American English. For one thing, not
all commas contribute to grammaticality; some are
more tied to stylistic rules and preferences. While
there are certainly rule-based decision points for
comma insertion (Doran, 1998), particularly in the
case of commas that set off significant chunks or
phrases within sentences, there are also some com-
mas that appear to be more prescriptive, as they have
less of an effect on sentence processing (such as in
example (2) in the introduction), and opposing us-
age rules for the same contexts are attested in differ-
ent style manuals. A common example of opposing
rules is the notorious serial or Oxford comma that
refers to the final comma found in a series, which
is required by the Chicago Manual of Style (Univer-
sity of Chicago, 1993), but is considered incorrect
by the New York Times Manual of Style (Siegal and
Connolly, 1999).

As a starting point, we needed to know what kinds
of commas are taught by English language teachers,
as well as what style manuals recommend and/or re-
quire. However, creating a list of comma uses was
a non-trivial part of the process. After consulting
style manuals (University of Chicago, 1993; Siegal
and Connolly, 1999; Strunk and White, 1999) and
popular ESL websites, we compiled a list of over 30
rules for use of commas in English. We took the
most commonly mentioned rules and created a final

286



Rule Example
Elements in a List Paul put the kettle on, Don fetched the teapot, and I made tea.
Initial Word/Phrase Hopefully, this car will last for a while.
Dependent Clause After I brushed the cat, I lint-rollered my clothes.
Independent Clause I have finished painting, but he is still sanding the doors.
Parentheticals My father, a jaded and bitter man, ate the muffin.
Quotations “Why,” I asked, “do you always forget to do it?”
Adjectives She is a strong, healthy woman.
Conjunctive Adverbs I would be happy, however, to volunteer for the Red Cross.
Contrasting Elements He was merely ignorant, not stupid.
Numbers 345,280,000
Dates She met her husband on December 5, 2003.
Geographical Names I lived in San Francisco, California, for 20 years.
Titles Al Mooney, M.D., is a good doctor
Introducing Words You may be required to bring many items, e.g., spoons, pans, and flashlights.
Other Catch-all rule for any other comma use

Table 1: Common Comma Uses

list of 15 usage rules (the 14 most common plus one
miscellaneous category) for our annotation scheme,
which is discussed in section 6. These rules are
given in Table 1. The 16 rules that were removed
from the list occurred in only one source or were
similar enough to other rules to be conflated. It is
worth noting here that while many of the comma
uses in this table might be best served by some sta-
tistical methodology like the one we describe in sec-
tion 4, one can envision fairly simple heuristic rules
to insert commas and find errors in numbers, dates,
geographical names, titles, and introducing words.

4 Classifier and Features

We use CRFs2 as the basis for our system and treat
the task of comma insertion as a sequence label-
ing task; each space between words is considered
by the classifier, and a comma is either inserted or
not. The feature set incorporates features that have
proven useful in comma restoration and other error
correction tasks, as well as a handful of new features
devised for this specific task (combination and dis-
tance features). The full set of features used in our
final system is given in Figure 1 along with exam-
ples of each feature for the sentence If the teacher
easily gets mad , then the child will always fear go-
ing to school and class. The target insertion point is
after the word mad.

2http://crfpp.sourceforge.net/

Feature Example(s)
Lexical and Syntactic Features

unigram easily, gets, mad, then, the
bigram easily gets, gets mad, mad then, ...
trigram easily gets mad, gets mad then, ...
pos uni RB, VBZ, JJ, RB, DT
pos bi RB VBZ, VBZ JJ, JJ RB, ...
pos tri RB VBZ JJ, VBZ JJ RB, ...
combo easily+RB, gets+VBZ,mad+JJ, ...
first combo If+RB

Distance Features
bos dist 5
eos dist 10
prevCC dist -
nextCC dist 9

Figure 1: CRF Features with examples for:
If the teacher easily gets mad , then the child will always

fear going to school and class.

4.1 Lexical and Syntactic Features

The first six features in Figure 1 refer to simple uni-
grams, bigrams, and trigrams of the words and POS
tags in a sliding 5 word window (target word, +/- 2
words). The lexical items help to encode any id-
iosyncratic relationships between words and com-
mas that might not be exploited through the exami-
nation of more in-depth linguistic features. For ex-
ample, then is a special case of an adverb (RB) that
is often preceded by a comma, even if other adverbs
are not, so POS tags might not capture this relation-

287



ship. The lexical items also provide an approxima-
tion of a language model or hidden event language
model approach, which has proven to be useful in
comma restoration tasks (see e.g. Lu and Ng, 2010).

The POS features abstract away from the words
and avoid the problem of data sparseness by allow-
ing the classifier to focus on the categories of the
words, rather than the lexical items themselves. The
combination (combo) feature is a unigram of the
word+pos for every word in the sliding window. It
reinforces the relationship between the lexical items
and their POS tags, further strengthening the evi-
dence of entries like then RB. All of these features
have been used in previous grammatical error detec-
tion tasks which target particle, article, and prepo-
sition errors (c.f., Dickinson et al., 2011; Gamon,
2010; Tetreault and Chodorow, 2008).

The first combo feature keeps track of the first
combination feature of the sentence so that it can
be referred to by the classifier throughout process-
ing the entire sentence. This feature is helpful when
an introductory phrase is longer than the classifier’s
five word window. Figure 1 provides a good exam-
ple of the utility of this feature, as If the teacher eas-
ily gets mad is so long that by the time the window
has moved to the target position of the space follow-
ing mad, the first word and POS, If RB, which can
often indicate an introductory phrase, is beyond the
scope of the sliding window.

4.2 Distance Features

Next, we encode four distance features. We keep
track of the following distances: from the beginning
of the sentence (bos dist), to the end of the sentence
(eos dist), from the previous coordinating conjunc-
tion (prevCC dist), and to the next coordinating con-
junction (nextCC dist). All of these distance fea-
tures help the classifier by encoding measures for
components of the sentence that can affect the deci-
sion to insert a comma. These features are especially
helpful over long range dependencies, when the in-
formation encoded by the feature is far outside the
scope of the 5-word window the CRF uses. The dis-
tance to the beginning of the sentence helps to en-
code introductory words and phrases, which make
up the bulk of the commas used in essays by learners
of English. The distance to the end of the sentence
is less obviously useful, but it can let the classifier

know the likelihood of a phrase beginning or ending
at a certain point in the sentence. The distances to
and from the nearest CC are useful because many
commas are collocated with coordinating conjunc-
tions. The distance features, as well as first combo,
were designed specifically for the task of comma er-
ror correction, and have not, as far as we know, been
utilized in previous research.

5 Comma Restoration

Before applying our system to the task of error cor-
rection, we tested its utility in restoring commas in
newswire texts. Specifically, we evaluate on section
23 of the WSJ, training on sections 02-22. Here,
the task is straightforward: we remove all commas
from the test data and performance is measured on
the system’s ability to put the commas back in the
right places. After stripping all commas from our
test data, the text is tokenized and POS tagged using
a maximum entropy tagger (Ratnaparkhi, 1996) and
every token is considered by the classifier as either
requiring a following comma or not. Out of 53,640
tokens, 3062 should be followed by a comma. We
provide accuracy, precision, recall, F1-score, and
sentence accuracy (S Acc.) for these tests, along
with results from Gravano et al. (2009) and Shieber
and Tao (2003) in Table 2. The first system (LexSyn)
includes only the lexical and syntactic features from
Figure 1; the second (LexSyn+Dist) includes all of
the features.

System Acc. P R F S Acc.
LexSyn 97.4 85.8 64.9 73.9 60.5
LexSyn+Dist 97.5 85.8 66.3 74.8 61.4
Shieber & Tao 97.0 79.7 62.6 70.1 54.9
Gravano et al. N.A. 57 67 ≈61 N.A.

Table 2: Comma Restoration System Results (%)

As can be seen in Table 2, the full system
(LexSyn+Dist) performs significantly better than
WSJ LexSyn (p < .02, two-tailed), achieving an
F-score of 74.8 on WSJ. This F-score outperforms
Shieber and Tao’s system, which was also tested on
section 23 of the WSJ, by about 4% and our sentence
accuracy of 61.5% is about 7% higher than theirs.
Our F-score is also about 13% higher than that of
Gravano et al. (2009), however, they evaluate on the

288



entire WSJ section of the Penn Treebank, so it is not
totally fair to compare results.

6 Annotation

For the comma restoration task, we needed only to
obtain well-formed text and remove the commas to
produce a test set. However, this is not so in the case
of error correction. In order to test a system that
corrects errors in learner essays, we need an anno-
tated test corpus that tells us where the errors are.
Although there are a handful of corpora that include
punctuation errors in their annotation scheme, such
as NUCLE (Dahlmeier and Ng, 2011) and HOO
(Dale and Kilgarriff, 2010), there are none to our
knowledge that focus specifically on commas. Thus,
we designed and implemented our own annotation
scheme on a set of essays to allow us the freedom to
identify the most important aspects of comma usage
for our work.

Our annotation scheme allows the mark-up of a
number of aspects of comma usage. First, each
comma in a text is marked as rejected or accepted
by the annotator. Additionally, any space between
words can be treated as an insertion point for a miss-
ing comma. The annotators also marked all accepted
and inserted commas as either required or optional.
Finally, the annotation also includes the appropriate
usage rule from the set in Table 1.3 In contrast, the
NUCLE and HOO data sets do not have this gran-
ularity of information (the annotation only indicates
whether a comma should be inserted or removed)
and are not exhaustively annotated.

After a one-hour training session on comma us-
age rules, three native English speakers were given a
set of ten learner essays comprising 3,665 tokens to
annotate for comma errors. To assess the difficulty
of the annotation task, we calculated agreement and
kappa. Agreement is a simple measure of how often
the annotators agree, and kappa provides a more ro-
bust measure of agreement since it takes chance into
account (Cohen, 1960). Table 3 provides the results
of these measurements. As can be seen in the table,
the agreement is quite high at either 97 or 98%, and
kappa is a bit lower, ranging from 72 to 81%. The

3The full annotation manual is available at
http://www.cs.rochester.edu/˜tetreaul/
comma-manual.pdf

agreement is likely so high due to the great number
of decision points where it is obvious to any native
writer that no comma is needed. To account for this
imbalance, we also provide an adjusted agreement
in the final column of the table that excludes all de-
cisions where both annotators agree that no comma
is necessary.

Annotators Agreement Kappa Adj. agr.
1 & 2 97 74 61
1 & 3 98 72 61
2 & 3 98 81 76

Table 3: Agreement over Annotation Training Set (%)

After completing the training phase, we assigned
one annotator the task of annotating our develop-
ment and test data from two different corpora: es-
says written by English as a foreign language learn-
ers (EFL) and essays written by native speakers of
English (Native). For both data sets we selected 60
essays for development and 60 essays for test. The
annotation was carried out using an annotation tool
developed in-house that gives the annotator an easy
to use interface and outputs standoff annotations in
xml format. (3) is an example of an annotated sen-
tence from an EFL essay, where “ ×” marks a span
for annotation.

(3) The new millenium , 1 the 21st century 2
has dawned upon us 3 and this new century
has brought many positive advancements in our
daily lives .
1) Accept, required, parenthetical
2) Insert, required, parenthetical
3) Insert, required, independent clause

Table 4 provides the comma usage information for
the essays in both sets used in development and test-
ing. The table shows the total number of sentences,
commas in the original text that were accepted by
the annotator, and errors (rejected and missing com-
mas) for the 60 essays in each set.

As can be seen in Table 4, the majority of exist-
ing commas (columns Accept plus Rej) in the texts
were accepted by the annotator; about 84% in the
EFL development set, 87% in the EFL test set, 85%
in the Native development set, and 88% in the Na-
tive test set. The important fact uncovered by these
numbers is that most of the commas that learners do

289



Data Set Sent
Commas

Accept ErrorsRej Miss
EFL Dev 717 474 49 233
EFL Test 683 427 65 232

Native Dev 970 506 86 363
Native Test 839 377 50 314

Table 4: Comma Usage Statistics

use are correct. However, there are a great number
of commas that the annotator inserted (over 80% of
all errors are missing commas) meaning that these
learners are more prone to underusing than overus-
ing commas. Another interesting fact that can be
gleaned from our annotation is that the top five
comma uses, those listed in the first five rows of Ta-
ble 1, account for more than 80% of all commas in
these essays.

7 Error Correction

With a competitive comma restoration system in
place, we turn to the primary task of correcting er-
rors in learner essays. While the task remains simi-
lar to comma restoration, error correction in student
writing brings a new set of challenges, especially
when the writers are non-native. Newswire texts are
most often well-formed, so the system should not
experience interference from other contextual errors
around the missing commas. Sentences taken from
learner texts, though, often contain multiple errors
that can make it difficult to focus on a single problem
at a time. Spelling errors, for example, can exacer-
bate error correction efforts that use contextual lex-
ical features because well-formed text that is often
used for training data is usually free of such noise.

In these experiments, we use the annotated es-
says described in section 6 for evaluation and train
on 40,000 sentences taken from essays written by
both native and non-native high level college stu-
dents. All of the essays are run through automatic
spelling correction to reduce the noise in the test set
before being tagged with the same tagger used in the
comma restoration experiments.

Because we approach comma error correction as
essentially a comma restoration task, we can we use
largely the same system for error correction as we
did for comma restoration. We still employ CRFs

and label each space between words as requiring
a comma or not, however, there is one significant
change to our methodology for this task. Namely,
we can leave the commas that were present in the
text as provided by the writer as we pre-process
the data for error correction, whereas they were re-
moved in the comma restoration task. For error cor-
rection, the task is really comparing the system’s an-
swer to the annotator’s and the learner’s, as opposed
to simply inserting commas into raw text. Leaving
the learners’ commas in the text does introduce some
errors to the POS tagging phase. However, since
over 85% of the existing commas in the development
set were judged as acceptable by our annotator (cf.
section 6) , the number of erroneous commas is not
so great as to contaminate the system. Removing all
of the commas would introduce unnecessary errors
in the pre-processing phase.

We also augment the system with three post-
processing filters that we tuned on the development
set. One requires that the classifier be completely
confident before a change is made to an existing
comma; crf++ will give 100% confidence to a single
class in some cases. This filter is based on the fact
that 85% of the existing commas can be expected
to be correct. A similar filter requires that the clas-
sifier be at least 90% confident in a decision to in-
sert a new comma. The final filter, which overrides
any other information provided by the system, does
not allow commas to be inserted before the word be-
cause. These ensure high precision even though they
may reduce recall.

Table 5 provides the accuracy, precision, recall,
F-score, and number of errors in each set for tests on
our 60 annotated EFL and Native essays, and the re-
sult for the combined corpus. The system performs
quite well on the EFL test set, with scores of 94%
precision, 31.7% recall, and 47.4% F-score for the
LexSyn+Dist system. The results for the Native set
are a bit lower, with 84.9% precision, 20% recall,
and 32.4% F-score for the LexSyn+Dist system.

For both data sets, when the distance features are
added to the model, precision increases by 1%, and
in the EFL set, recall also increases. In keeping with
practices established within the field of grammatical
error correction, the system has been optimized for
high precision even at the cost of recall, to ensure
that feedback systems avoid confusing learners by

290



Data System Acc. P R F n

EFL LexSyn 98.2 92.9 30.9 46.5 297LexSyn+Dist 98.3 94.0 31.7 47.4 297

Native LexSyn 97.8 83.9 20.0 32.3 365LexSyn+Dist 97.8 84.9 20.0 32.4 365

Combined LexSyn 98.1 88.7 24.9 38.9 662LexSyn+Dist 98.1 89.8 25.2 39.4 662

Table 5: Comma Error Correction Results (%)

marking correct comma usage as erroneous. Con-
sidering performance over all of the test data, the
system achieves over 89% precision and 25% recall,
results which are comparable to those in other er-
ror correction tasks. For example, the preposition
error detection system described in Tetreault and
Chodorow (2008) achieved 84% precision, 19% re-
call for prepositions.

It is worth noting that the results in Table 5 in-
clude commas that the annotator had marked as
optional. For these, whatever decision the system
makes is scored as correct. Since the grammatical-
ity/readability of the sentence will not be affected by
the presence or absence of a comma in these cases,
we feel this is the fairest assessment of the system.

7.1 Error Analysis

In order to get a sense of what kinds of construc-
tions are difficult for our system, we randomly ex-
tracted 50 sentences from the output that exhibited
at least one wrong comma decision made by the sys-
tem. The 50 sentences contained a combined total of
62 system errors. Among these cases, the most com-
mon context where the system makes the wrong de-
cision is in introductory words and phrases, which is
not surprising given the frequency with which com-
mas occur in these environments in our development
set (about 40% of all commas in the essays). In (4),
for example, the first word, Here, should be followed
by a comma. Since Here is not a common introduc-
tory word in this type of sentence structure in the
training data, this is a difficult case for the system to
correct.

(4) Here we can get specific knowledge in the sci-
ence that we like the most .

The next most common misclassification involves
comma splices, i.e. conjoining complete sentences

with a comma rather than separating them with a full
stop. In (5), for example, there should be a full stop
between college and I, rather than a comma. This
result is not surprising because the system is not
yet equipped to deal with comma splices. Comma
splices are a different type of phenomenon because
correcting them requires removing the comma and
inserting a full stop, essentially two separate steps
rather than the single reject/accept step that the sys-
tem currently handles.

(5) I entered college, I could learn it and make an
effort to achieve my goal.

The next most common context for system errors
was between clauses that are conjoined with a co-
ordinating conjunction as in (6), where there should
not be a comma. In (6), the second clause is actually
a dependent clause, so no comma should precede
the coordinating conjunction. There are a number
of system errors dealing with commas between two
independent clauses. For example in (7), our annota-
tor recommended a comma between things and but,
however the system did not make the insertion. The
problem with these examples likely stems from the
fact that the rule for comma usage in these contexts
is not clearly stated, even in well-respected manu-
als, and therefore likely not clearly understood, even
by high-level native writers. For example, the NYT
style manual (Siegal and Connolly, 1999) states that
“Commas should be used in compound sentences
before conjunctions... When the clauses are excep-
tionally short, however, the comma may be omit-
ted.” Adding a feature that measures clause length
might help, but even then the classifier must rely on
training data that may have considerable variation
as to what length of clauses requires an intervening
comma.

291



(6) They wants to see their portfolio, and what kind
of skill do they have for company.

(7) I have many things but the best is my parents.

Another facet of the data that consistently chal-
lenges the system is the existence of errors other
than the commas in the sentences. Consider the sen-
tence in (8), where erroneous is the original text
from the essay and corrected is a well-formed in-
terpretation.

(8) erroneous: In the other hand , having just
one specific subject , which represents a great
downfall for many students
corrected: On the other hand, knowing only
one subject is a downfall for many students.

The comma after subject is unnecessary, but so is
the word which. In fact, which would normally sig-
nify the beginning of a non-restrictive clause in this
context, which should be set off with a comma. It
is no surprise then, that the system has trouble re-
moving commas in these types of contexts. At least
11 of the 62 system mistakes that we examined have
grammatical errors in the immediate context of the
comma in question, which makes the classification
more difficult.

8 Summary and Conclusion

We presented a novel comma error correction sys-
tem for English that achieves an average of 89% pre-
cision and 25% recall on essays written by learn-
ers of different levels and language backgrounds,
including native English speakers. The system
achieves state-of-the-art performance on the task of
comma restoration, beating previous systems’ F-
score and sentence accuracy by 4% and 7%, respec-
tively. We discovered that augmenting lexical fea-
tures, which have been commonly used in previous
work, with the combination and distance features
can improve F-score by as much as 1% in both the
error correction and comma restoration tasks. We
also developed and implemented a novel comma er-
ror annotation scheme.

Additionally, we are interested in the effect of
correct comma placement on other NLP processes.
Jones (1994) and Briscoe and Carroll (1995) show
that adding punctuation to grammars that utilize

part-of-speech (POS) tags, rather than lexical items,
adds more structure and reduces ambiguity as well
as the number of parses for each sentence. Simi-
larly, Doran (1998) and White and Rajkumar (2008)
found that adding punctuation improved parsing re-
sults in tree-adjoining grammar (TAG) and combi-
natorial categorial grammar (CCG) parsing, respec-
tively. These studies all highlight the importance of
correctly inserted punctuation, especially commas,
for parsing. Given these results, we believe that by
enhancing the quality of the text, comma error cor-
rection will improve not only tagging and parsing,
but also the ability of systems to correct many other
forms of grammatical errors, such as those involv-
ing incorrect word order, number disagreement, and
misuse of prepositions, articles, and collocations.

Acknowledgments

We would like to thank Melissa Lopez for help with
annotating our corpora and Michael Flor for kindly
developing an annotation tool for our purposes. We
also thank Aoife Cahill, Robbie Kantor, Markus
Dickinson, Michael Heilman, Nitin Madnani, and
our anonymous reviewers for insightful comments
and discussion.

References

Iñaki Alegria, Bertol Arrieta, Arantza Diaz de Ilar-
raza, Eli Izagirre, and Montse Maritxalar. 2006.
Using machine learning techniques to build a
comma checker for Basque. In Proceedings of the
COLING/ACL main conference poster sessions.

Timothy Baldwin and Manuel Paul Anil Kumar
Joseph. 2009. Restoring punctuation and casing
in English text. In Australasian Conference on
Artificial Intelligence’09.

E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. Ph.D. thesis, The University of
Pennsylvania, Philadelpha, PA.

Ted Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic LR parser of part-of-
speech and punctuation labels. In Proceedings of
the ACL/SIGPARSE 4th International Workshop
on Parsing Technologies.

Jacob Cohen. 1960. A coefficient of agreement for

292



nominal scales. Educational and Psychological
Measurement, 20(1):37–46.

Robert J. Connors and Andrea A. Lunsford. 1988.
Frequency of formal errors in current college
writing, or Ma and Pa Kettle do research. Col-
lege Composition and Communication, 39(4).

Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure
optimization. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies - Volume 1. Association for Computational
Linguistics.

Robert Dale and Adam Kilgarriff. 2010. Helping
our own: Text massaging for computational lin-
guistics as a new shared task. In International
Conference on Natural Language Generation.

Rachele De Felice and Stephen Pulman. 2008. A
classifier-based approach to preposition and de-
terminer error correction in L2 English. In Pro-
ceedings of COLING-08. Manchester.

Markus Dickinson, Ross Israel, and Sun-Hee Lee.
2011. Developing methodology for Korean par-
ticle error detection. In Proceedings of the 6th
Workshop on Innovative Use of NLP for Building
Educational Applications. Portland, Oregon.

Christine Doran. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree-
Adjoining Grammar Perspective. Ph.D. thesis,
University of Pennsylvania.

Benoit Favre, Dilek Hakkani-Tur, and Elizabeth
Shriberg. 2009. Syntactically-informed models
for comma prediction. In Proceedings of the
2009 IEEE International Conference on Acous-
tics, Speech and Signal Processing.

Michael Gamon. 2010. Using mostly native data
to correct errors in learners’ writing: A meta-
classifier approach. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics.

Michael Gamon. 2011. High-order sequence model-
ing for language learner detection high-order se-
quence modeling for language learner error de-
tection. In Proceedings of the 6th Workshop on

Innovative Use of NLP for Building Educational
Applications.

Agustin Gravano, Martin Jansche, and Michiel Bac-
chiani. 2009. Restoring punctuation and capi-
talization in transcribed speech. In Proceedings
of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing.

Daniel Hardt. 2001. Comma Checking in Danish. In
Corpus Linguistics.

Robin L. Hill and Wayne S. Murray. 1998. Commas
and spaces: The point of punctuation. In 11th An-
nual CUNY Conference on Human Sentence Pro-
cessing.

Jing Huang and Geoffrey Zweig. 2002. Maximum
entropy model for punctuation annotation from
speech. In Proceedings of ICSLP 2002.

Bernard E. M. Jones. 1994. Exploring the role of
punctuation in parsing natural text. In Proceed-
ings of the 15th conference on Computational lin-
guistics - Volume 1.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning.

Claudia Leacock, Martin Chodorow, Michael Ga-
mon, and Joel R. Tetreault. 2010. Auto-
mated Grammatical Error Detection for Lan-
guage Learners. Synthesis Lectures on Hu-
man Language Technologies. Morgan & Claypool
Publishers.

Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and
Mary Harper. 2005. Comparing hmm, maximum
entropy, and conditional random fields for disflu-
ency detection. In In Proceeedings of the Euro-
pean Conference on Speech Communication and
Technology.

Wei Lu and Hwee T. Ng. 2010. Better punctua-
tion prediction with dynamic conditional random
fields. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing.

Helena Moniz, Fernando Batista, Hugo Meinedo,
and Alberto Abad. 2009. Prosodically-based au-
tomatic segmentation and punctuation. In Pro-

293



ceedings of the 5th International Conference on
Speech Prosody.

Diane Nicholls. 1999. The cambridge learner corpus
- error coding and analysis for writing dictionaries
and other books for english learners. In Summer
Workshop on Learner Corpora. Showa Woman’s
University.

Hiromi Oyama. 2010. Automatic error detection
method for japanese particles. Polyglossia, 18.

Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of the
Empirical Methods in Natural Language Process-
ing.

Alla Rozovskaya, Mark Sammons, Joshua Gioja,
and Dan Roth. 2011. University of Illinois sys-
tem in HOO text correction shared task. In Pro-
ceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 263–266. Association
for Computational Linguistics, Nancy, France.

Stuart M. Shieber and Xiaopeng Tao. 2003. Comma
restoration using constituency information. In
Proceedings of the 2003 Human Language Tech-
nology Conference and Conference of the North
American Chapter of the Association for Compu-
tational Linguistics.

Allan M. Siegal and William G. Connolly. 1999. The
New York Times Manual of Style and Usage : The
Official Style Guide Used by the Writers and Edi-
tors of the World’s Most Authoritative Newspaper.
Crown, rev sub edition.

William Strunk and E. B. White. 1999. The Ele-
ments of Style, Fourth Edition. Longman, fourth
edition.

Joel Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL
writing. In Proceedings of COLING-08. Manch-
ester.

Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers.

University of Chicago. 1993. The Chicago Manual
of Style. University Of Chicago Press, Chicago,
fourteenth edition.

Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Pro-
ceedings of the Workshop on Grammar Engineer-
ing Across Frameworks.

294


