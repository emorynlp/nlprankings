










































Identifying Event-Sentiment Association using Lexical Equivalence and Co-reference Approaches


Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 19–27,
Portland, Oregon, USA, June 23, 2011. c©2011 Association for Computational Linguistics

Identifying Event – Sentiment Association using Lexical Equivalence and 
Co-reference Approaches 

 
 

Anup Kumar Kolya1       Dipankar Das1      Asif Ekbal2      Sivaji Bandyopadhyay1 
1 Computer Science and Engineering Department, Jadavpur University, India  

2 Indian Institute of Technology, Patna (IITP), India 
anup.kolya@gmail.com, dipankar.dipnil2005@gmail.com 

asif.ekbal@gmail.com, sivaji_cse_ju@yahoo.com 

 
 
 

Abstract 

In this paper, we have identified event and sen-
timent expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. A hybrid approach that con-
sists of Conditional Random Field (CRF) based 
machine learning framework in conjunction 
with several rule based strategies has been 
adopted for event identification within the 
TimeML framework. The strategies are based 
on semantic role labeling, WordNet relations 
and some handcrafted rules. The sentiment ex-
pressions are identified simply based on the 
cues that are available in the sentiment lexicons 
such as Subjectivity Wordlist, SentiWordNet 
and WordNet Affect. The identification of lexi-
cal equivalence between event and sentiment 
expressions based on the part-of-speech (POS) 
categories is straightforward. The emotional 
verbs from VerbNet have also been employed 
to improve the coverage of lexical equivalence. 
On the other hand, the association of sentiment 
and event has been analyzed using the notion of 
co-reference. The parsed dependency relations 
along with basic rhetoric knowledge help to 
identify the co-reference between event and 
sentiment expressions. Manual evaluation on 
the 171 sentences of TempEval-2010 dataset 
yields the precision, recall and F-Score values 
of 61.25%, 70.29% and 65.23% respectively.  

1 Introduction 

Event and Sentiment are two abstract entities 
closely coupled with each other from social, psy-

chological and commercial perspectives. Some 
kind of action that is going on or something that is 
being happened are addressed as events in general 
by the Natural Language (NL) researchers. The 
events are described in texts where the time, tem-
poral location and ordering of the events are speci-
fied. Event entities are represented by finite 
clauses, nonfinite clauses, nominalizations, event-
referring nouns, adjectives and even some kinds of 
adverbial clauses.  

On the other hand, text not only contains the in-
formative contents, but also some attitudinal pri-
vate information that includes sentiments. 
Nowadays, in the NLP communities, research ac-
tivities on sentiment analysis are in full swing. But, 
the identification of sentiment from texts is not an 
easy task as it is not open to any objective observa-
tion or verification (Quirk et al., 1985).  

Sometimes, similar or different types of senti-
ments are expressed on a single or multiple events. 
Sentiment of people over different events is impor-
tant as it has great influence on our society. Track-
ing users’ sentiments about products or events or 
about political candidates as expressed in online 
forums, customer relationship management, stock 
market prediction, social networking etc., temporal 
question answering, document summarization, in-
formation retrieval systems are some of the impor-
tant applications of sentiment analysis.  

The identification of the association between 
event and sentiment is becoming more popular and 
interesting research challenge in the area of Natu-
ral Language Processing (NLP). Our present task is 
to identify the event and sentiment expressions 
from the text, analyze their associative relationship 

19



and investigate the insides of event-sentiment rela-
tions.  

For example, in the following sentence, the an-
notated events are, talked, sent and hijacked .But, 
it also shows the presence of underlying sentiments 
(as shown in underlined script) inscribed in the 
sentence. Here, sentiment helps to evoke the event 
property at lexical entity level (e.g. negative (-ve) 
sentiment for only the event word hijacked) as well 
as at context level (e.g. positive (+ve) sentiment 
associated with the event hijacked as the event 
word appears with the evaluative expression, re-
cover that gives the +ve polarity).  

 
“The prime minister of India told Friday that he 

has talked with top commander of Indian military 
force and sent a team to recover the host of Taj 
Hotel hijacked.”  
 

 Hence, we have organized the entire task into 
three different steps i) event identification, ii) sen-
timent expression identification and iii) identifica-
tion of event sentiment relationships at context 
level using lexical equivalence and co-reference 
approaches.  

In the first step, we propose a hybrid approach 
for event extraction from the text under the Tem-
pEval-2010 framework. Initially, we have used a 
Conditional Random Field (CRF) (Lafferty et al., 
2001) machine learning framework but we observe 
that it often makes the errors in extracting the 
events denoted by deverbial entities. This observa-
tion prompts us to employ several strategies in 
conjunction with machine learning. These strate-
gies are implemented based on semantic role labe-
ling, WordNet (Miller, 1990) and some 
handcrafted rules. We have experimented with the 
TempEval-2010 evaluation challenge setup (Kolya 
et al., 2010).  Evaluation results yield the preci-
sion, recall and F-measure values of approximate-
ly 93.00%, 96.00% and 94.47% respectively. This 
is approximately 12% higher F-measure in com-
parison to the best system (Llorens et al., 2010) of 
TempEval-2010. 
    On the other hand, the identification of the sen-
timent expressions is carried out based on the sen-
timent word. The words are searched in three 
different sentiment lexicons, the Subjectivity Word 
lists (Banea et al., 2008), SentiWordNet (Baccia-
nella et al., 2010) and WordNet Affect (Strapparava 
and Valitutti, 2004). The coarse-grained (positive 

and negative) as well as Ekman’s (1993) six fine- 
grained sentiment or emotion expressions (happy, 
sadness, anger, disgust, fear and surprise) are 
tagged in the corpus. As there is no annotation in 
the TemEval-2010 corpus for sentiment expres-
sions, the evaluation has been carried out by the 
authors and it achieves the precision, recall and F-
measure values of approximately 73.54%, 86.04% 
and 79.30% respectively 

Determining the lexical equivalence of event 
and sentiment expressions based on the POS prop-
erty at the lexical entity level is straightforward. If 
an event word also expresses the sentiment word, 
we have associated the corresponding sentiment 
type with the event word directly. In addition to the 
sentiment lexicons, the emotional verbs extracted 
from the VerbNet (Kipper-Schuler, 2005) are used 
in this phase. It improves the coverage of lexical 
equivalence by 12.76%. 

But, if the event and sentiment expressions oc-
cupy separate text spans in a sentence, we have 
adopted a co-reference approach for identifying 
their association. The parsed dependency relations 
along with some basic rhetoric components, such 
as nucleus, satellite and locus help in identifying 
the co-reference between the event and sentiment 
expressions. The text span containing sentiment 
word is hypothesized as the locus, the main effec-
tive part of the nucleus or satellite. The text span 
that reflects the primary goal of the writer is 
termed as nucleus (marked as “{ }”) whereas the 
span that provides supplementary material is 
termed as satellite (marked as “[ ]”). The distin-
guished identification of nucleus and satellite as 
well as their separation from each other is carried 
out based on the direct and transitive dependency 
relations, causal verbs, relaters or discourse mark-
ers. If both the locus and event are identified to-
gether in either nucleus or satellite, we term their 
association as co-referenced. If they occur sepa-
rately in nucleus and satellite and share at least one 
direct dependency relation, we consider their asso-
ciation as co-referenced.  

The evaluation of the lexical equivalence as 
well as co-reference systems has been performed 
by the authors. Primarily, the evaluation of both 
systems has been conducted on the random sam-
ples of 200 sentences of the TempEval-2010 train-
ing dataset.  Finally, the co-reference system 
achieves the precision, recall and F-Scores of 

20



61.25%, 70.29% and 65.23% respectively on 171 
sentences of the TempEval-2010 test corpus.  

The rest of the paper is organized as follows. 
Section 2 describes the related work. The event 
identification is discussed in Section 3. The identi-
fication of sentiment expressions is described in 
Section 4. Determination of lexical equivalence 
between event and sentiment expressions is speci-
fied in Section 5. The co-reference approach for 
identifying the association between event and sen-
timent is described in Section 6. Finally Section 7 
concludes the paper. 

2 Related Work 

The existing works on event extraction are based 
either on pattern-matching rules (Mani and Wilson 
2000), or on the machine learning approach (Bo-
guraev and Ando, 2005). But, still the problems 
persist with the high complexities involved in the 
proper extractions of events. The events expres-
sions were annotated in the TempEval 2007 
source in accordance with the TimeML standard 
(Pustejovsky et al., 2003). On the other hand, the 
Task B of TempEval-2010 evaluation challenge 
setup (Verhagen et al., 2010) was aimed at identi-
fying events from text. The best achieved result 
was obtained by (Llorens et al., 2010). 

The majority of subjective analysis methods 
that are related to emotion is based on textual key-
words spotting that use specific lexical resources. 
A lexicon that provides appraisal attributes for 
terms was constructed and the features were used 
for emotion classification (Whitelaw et al., 2005). 
The features along with the bag-of-words model 
give 90.2% accuracy. UPAR7 (Chaumartin, 2007), 
a rule-based system uses a combination of Word-
Net Affect and SentiWordNet. The system was 
semi-automatically enriched with the original trial 
data provided during the SemEval task (Strappara-
va and Mihalcea, 2007). SWAT (Katz et al., 2007) 
is another supervised system that uses a unigram 
model trained to annotate emotional content. 

Our motivation is that though events and senti-
ments are closely coupled with each other from 
social, psychological and commercial perspectives, 
very little attention has been given about their de-
tection and analysis. To the best of our knowledge, 
only a few tasks have been attempted (Fukuhara et 
al., 2007) (Das et al., 2010).  

Sometimes, the opinion topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics, as 
well as opinions that are on the same topic sepa-
rated by opinions that do not share that topic 
(Stoyanov and Cardie 2008). The authors have es-
tablished their hypothesis by applying the co-
reference technique. Similarly, we have adopted 
the co-reference technique based on basic rhetoric 
components for identifying the association be-
tween event and sentiment expressions.  In addi-
tion to that, we have also employed the lexical 
equivalence approach for identifying their associa-
tion.  

3 Event Identification 

In this work, we propose a hybrid approach for 
event identification from the text under the Tem-
pEval-2010 framework. We use Conditional Ran-
dom Field (CRF) as the underlying machine 
learning algorithm. We observe that this machine 
learning based system often makes the errors in 
identifying the events denoted by deverbial enti-
ties. This observation prompts us to employ several 
strategies in conjunction with machine learning 
techniques. These strategies have been imple-
mented based on semantic role labeling, WordNet 
senses and some handcrafted rules.  

We have experiment with the TempEval-2010 
evaluation challenge setup (Kolya et al., 2010).  
Evaluation results yield the precision, recall and F-
measure values of approximately 93.00%, 96.00% 
and 94.47% respectively. This is approximately 
12% higher F-measure in comparison to the best 
system (Llorens et al., 2010) of TempEval-2010. 

3.1 CRF based Approach for Event Identifi-
cation 

We extract the gold-standard TimeBank features 
for events in order to train/test the CRF model. In 
the present work, we mainly use the various com-
binations of the following features:  

Part of Speech (POS) of event terms (e.g. Ad-
jective, Noun and Verb), Tense (Present, Past, Fu-
ture, Infinitive, Present part, Past part, or NONE), 
Aspect (Progressive, Perfective and Perfective 
Progressive or NONE), Class (Reporting, Percep-
tion, Aspectual, I_action, I_state, State, Occur-
rence), Stem (e.g., discount /s/).  

21



3.2 Use of Semantic Roles for Event Identifi-
cation 

We use an open source Semantic Role Labeler 
1(SRL) (Gildea et al., 2002) (Pradhan et al., 2004) 
to identify different features of the sentences. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents, determining 
their arguments (agent, patient etc.) and adjuncts 
(locative, temporal etc.). Semantic roles can be 
used to detect the events that are the nominaliza-
tions of verbs such as agreement for agree or con-
struction for construct. Nominalizations (or, 
deverbal nouns) are commonly defined as nouns 
that are morphologically derived from verbs, 
usually by suffixation (Quirk et al., 1985). Event 
nominalizations often afford the same semantic 
roles as verbs and often replace them in written 
language (Gurevich et al., 2006).  Event nominali-
zations constitute the bulk of deverbal nouns.  The 
following example sentence shows how semantic 
roles can be used for event identification.  
 
[ARG1 All sites] were [TARGET inspected] to the satis-
faction of the inspection team and with full coope-
ration of Iraqi authorities, [ARG0 Dacey] [TARGET 
said]. 
 
   The extracted target words are treated as the 
event words. It has been observed that many of 
these target words are identified as the event ex-
pressions by the CRF model. But, there exists ma-
ny nominalised event expressions (i.e., deverbal 
nouns) that are not identified as events by the su-
pervised CRF. These nominalised expressions are 
correctly identified as events by SRL.  

3.3 Use of WordNet for Event Identification 

WordNet is mainly used to identify non-deverbal 
event nouns. We observed that the event entities 
like ‘war’, ‘attempt’, ‘tour’ are not properly identi-
fied. These words have noun (NN) POS informa-
tion as the previous approaches, i.e., CRF and SRL 
can only identify those event words that have verb 
(VB) POS information. We know from the lexical 
information of WordNet that the words like ‘war’ 
and ‘tour’ are generally used as both noun and 
verb forms in the sentence. Therefore, we have 

                                                        
1 http://cemantix.org/assert.html 

designed the following two rules based on the 
WordNet: 
 
Rule 1: The word tokens having Noun (NN) POS 
categories are looked into the WordNet. If it ap-
pears in the WordNet with noun and verb senses, 
then that word token is considered as an event.  For 
example, war has both noun and verb senses in the 
WordNet, and hence war is considered as an event.  
 
Rule 2: The stems of the noun word tokens are 
looked into the WordNet. If one of the WordNet 
senses is verb then the token is considered as verb. 
For example, the stem of proposal, i.e., propose 
has two different senses, noun and verb in the 
WordNet, and thus it is considered as an event.  

3.4    Use of Rules for Event Identification 

Here, we mainly concentrate on the identification 
of specific lexical classes like ‘inspection’ and 
‘resignation’. These can be identified by the suf-
fixes such as (‘-ción’), (‘-tion’) or (‘-ion’), i.e., the 
morphological markers of deverbal derivations. 
  Initially, we have employed the CRF based Stan-
ford Named Entity (NE) tagger2 on the TempEval-
2 test dataset. The output of the system is tagged 
with Person, Location, Organization and Other 
classes. The words starting with the capital letters 
are also considered as NEs. Thereafter, we came 
up with the following rules for event identification: 
  
Cue-1: The deverbal nouns are usually identified 
by the suffixes like ‘-tion’, ’-ion’, ’-ing’ and ’-ed’ 
etc. The nouns that are not NEs, but end with these 
suffixes are considered as the event words. 
  
Cue 2: The verb-noun combinations are searched 
in the sentences of the test set. The non-NE noun 
word tokens are considered as the events.  
 
Cue 3: Nominals and non-deverbal event nouns 
can be identified by the complements of aspectual 
PPs headed by prepositions like during, after and 
before, and complex prepositions such as at the 
end of and at the beginning of etc.  The next word 
token(s) appearing after these clue word(s) or 
phrase(s) are considered as events.  

                                                        
2 http://nlp.stanford.edu/software/CRF-NER.shtml 

22



Cue 4: The non-NE nouns occurring after the ex-
pressions such as frequency of, occurrence of and 
period of are most probably the event nouns. 
 
Cue 5: Event nouns can also appear as objects of 
aspectual and time-related verbs, such as have be-
gun a campaign or have carried out a campaign 
etc. The non-NEs that appear after the expressions 
like “have begun a”, “have carried out a” etc.  are 
also denoted as the events.   

4 Sentiment Expression Identification 

Sentiment is an important cue that effectively de-
scribes the events associated with it. The binary 
classification of the sentiments (positive and nega-
tive) as well as the fine-grained categorization into 
Ekman’s (1993) six emotions is therefore em-
ployed for identifying the sentiment expressions. 
200 sentences are randomly selected from the 
training dataset of the TempEval-2010 corpus. 
These sentences have been considered as our de-
velopment set. On the other hand, 171 sentences 
were already provided as the test sentences in the 
TempEval-2010 evaluation challenge.   

The events are already annotated in the Tem-
pEval-2010 corpus. But, no sentiment or emotion 
related annotation is available in the corpus. 
Hence, we have annotated the sentiment expres-
sions at word level in a semi-supervised way. The 
word level entities are tagged by their coarse and 
fine grained sentiment tags using the available sen-
timent related lexical resources. Then the automat-
ic annotation has been evaluated manually by the 
authors. The semi-supervised sentiment annotation 
agreements were 90.23% for the development set 
and 92.45% for the test sets respectively.  

4.1 Lexicon based Approach 

The tagging of the evaluative expressions or more 
specifically the sentiment expressions on the Tem-
pEval-2010 corpus has been carried out using the 
available sentiment lexicons. We passed the sen-
tences through three sentiment lexicons, Subjectivi-
ty Wordlists (Banea et al., 2008), SentiWordNet 
(Baccianella et al., 2010) and WordNet Affect 
(Strapparava and Valitutti, 2004). Subjectivity 
Wordlist assigns words with the strong or weak 
subjectivity and prior polarities of types positive, 
negative and neutral. SentiWordNet, used in opi-

nion mining and sentiment analysis, assigns three 
sentiment scores such as positive, negative and 
objective to each synset of WordNet. WordNet Af-
fect, a small well-used lexical resource but valua-
ble for its affective annotation contains the words 
that convey emotion.  

The algorithm is that, if a word in a sentence is 
present in any of these resources; the word is 
tagged as the sentiment expression. But, if any 
word is not found in any of them, each word of the 
sentence is passed through the WordNet Morpho-
logical analyzer (Miller, 1990) to identify its root 
form and the root form is searched through the re-
sources again. If the root form is found, the corres-
ponding word is tagged as sentiment expression 
accordingly.  

The identified sentiment expressions have been 
evaluated by the authors and it achieves the preci-
sion, recall and F-Score of 73.54%, 86.04% and 
79.30%, respectively on a total of 171 test sen-
tences of the TempEval-2010 corpus.   

The identification of event words that also ex-
press sentiment is straightforward. But, the prob-
lem arises when the event and sentiment 
expressions are present separately in a sentence 
and the sentiment is either closely associated with 
the event or affects it. In case of the former, we 
have adopted the approach of lexical equivalence 
between the event and sentiment entities whereas 
the co-reference technique has been introduced for 
resolving the latter case.  

5 Lexical Equivalence between Event and 
Sentiment Expressions  

It is observed that in general the verbs, nouns and 
adjectives represent events. The sentences are 
passed through an open source Stanford Maximum 
Entropy based POS tagger (Manning and Toutano-
va, 2000). The best reported accuracy for the POS 
tagger on the Penn Treebank is 96.86% overall and 
86.91% on previously unseen words. Our objective 
was to identify the event words that also express 
sentiments. Hence, we have identified the event 
words that have also been tagged as the sentiment 
expressions. The coverage of these lexical re-
sources in identifying the event sentiment associa-
tion is shown in Table 1. 

On the other hand, not only the adjectives or 
nouns, the sentiment or emotional verbs play an 
important role in identifying the sentiment expres-

23



sions. Hence, in addition to the above mentioned 
sentiment resources, we have also incorporated 
English VerbNet (Kipper-Schuler, 2005) for the 
automatic annotation process. VerbNet associates 
the semantics of a verb with its syntactic frames 
and combines traditional lexical semantic informa-
tion such as thematic roles and semantic predi-
cates, with syntactic frames and selectional 
restrictions. Verb entries in the same VerbNet class 
share common syntactic frames and thus they are 
believed to have the same syntactic behavior. For 
example, the emotional verbs “love” and “enjoy” 
are members of the admire-31.2-1 class and “en-
joy” also belongs to the class want-32.1-1.  

The XML files of VerbNet are preprocessed to 
build up a general list that contains all member 
verbs and their available syntax information re-
trieved from VerbNet. The main criterion for se-
lecting the member verbs as sentiment expressions 
is the presence of “emotional_state” type predicate 
in their frame semantics. The frequencies of the 
event words matched against the above said four 
resources are shown in Table 1.  It has been ob-
served that the adjective events are not identified 
by the lexical resources as their frequency in the 
test corpus was very low. But, the lexical coverage 
has been improved by 12.76% by incorporating 
VerbNet. 

 
Resources Noun   Adjective  Verb 

#114    #4              #380 
Subjectivity Wordlists 
SentiWordNet 
WordNet Affect List 
VerbNet (emotional 
verbs) 

24            --             35 
32            --             59  
12            --             25 
 --            --             79 

Accuracy (in %) 59.64                    52.57 
 

Table 1: Results of Lexical Equivalence between 
Event and Sentiment based on different resources  

6 Co-reference between Event and Senti-
ment Expressions  

The opinion and/or sentiment topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics. 
Sometimes, the opinions that are on the same topic 
are separated by opinions that do not share that 
topic (Stoyanov and Cardie, 2008). We observe the 
similar situation in case of associating sentiments 

with events. Hence, the hypothesis for opinion top-
ic is established for sentiment events by applying 
the co-reference technique along with the rhetori-
cal structure. We have proposed two different sys-
tems for identifying the association of sentiments 
with the events at context level. 

6.1 Baseline Co-reference System 

The baseline system has been developed based on 
the object information present in the dependency 
relations of the parsed sentences. Stanford Parser 
(Marneffe et al., 2006), a probabilistic lexicalized 
parser containing 45 different part of speech (POS) 
tags of Pen Treebank tagset  has been used to get 
the parsed sentences and dependency relations. 
The dependency relations are checked for the pre-
dicates “dobj” so that the related components 
present in the predicate are considered as the prob-
able candidates for the events.  

If a dependency relation contains both the event 
and sentiment words, we have considered the pres-
ence of co-reference between them. But, it has 
been observed that the event and sentiment expres-
sions are also present in two different relations that 
share a common word element. Hence, if the event 
and sentiment words appear in two different rela-
tions but both of the relations contain at least one 
common element, the event and sentiment words 
are termed as co-referenced.    

Overall, the baseline co-reference system 
achieves the precision, recall and F-Scores of 
40.03%, 46.10% and 42.33% for event-sentiment 
co-reference identification. For example in the fol-
lowing sentence, the writer’s direct as well as indi-
rect emotional intentions are reflected by 
mentioning one or more topics or events (spent, 
thought) and their associated sentiments (great).  

 
“When Wong Kwan spent seventy million dol-

lars for this house, he thought it was a great deal.” 
 
The baseline co-reference system fails to asso-

ciate the sentiment expressions with their corres-
ponding event expressions. Hence, we aimed for 
the rhetoric structure based co-reference system to 
identify their association. 

6.2  Rhetoric Co-reference System 

The distribution of events and sentiment expres-
sions in different text spans of a sentence needs the 

24



analysis of sentential structure. We have incorpo-
rated the knowledge of Rhetorical Structure 
Theory (RST) (Mann and Thompson 1987) for 
identifying the events that are co-referred by their 
corresponding sentiment expressions.  

The theory maintains that consecutive discourse 
elements, termed text spans, are related by a rela-
tively small set (20–25) of rhetorical relations. 
But, instead of identifying the rhetorical relations, 
the present task acquires the basic and coarse rhe-
torical components such as locus, nucleus and sa-
tellite from a sentence.  These rhetoric clues help 
in identifying the individual event span associated 
with the span denoting the corresponding senti-
ment expression in a sentence. The text span that 
reflects the primary goal of the writer is termed as 
nucleus (marked as “{ }”) whereas the span that 
provides supplementary material is termed as satel-
lite (marked as “[ ]”). For example, the nucleus and 
satellite textual spans are shown in the following 
sentence as, 

 
{Traders said the market remains extremely 

nervous} because [the wild swings seen on the 
New York Stock Exchange last week]. 

 
The event or topic of an opinion or sentiment 

depends on the context in which the associated 
opinion or sentiment expression occurs (Stoyanov 
and Cardie 2008). Considering the similar hypo-
thesis in case of events instead of topics, the co-
reference between an event and a sentiment ex-
pression is identified from the nucleus and/or satel-
lite by positioning the sentiment expression as 
locus. We have also incorporated the WordNet’s 
(Miller 1990) morphological analyzer to identify 
the stemmed forms of the sentiment words.  

The preliminary separation of nucleus from sa-
tellite was carried out based on the list of frequent-
ly used causal keywords (e.g., as, because, that, 
while, whether etc) and punctuation markers (,) (!) 
(?).The discourse markers and causal verbs are 
also the useful clues if they are explicitly specified 
in the text. The identification of discourse markers 
from written text itself is a research area (Azar 
1999). Hence, our task was restricted to identify 
only the explicit discourse markers that are tagged 
by conjunctive_() or mark_() type dependency re-
lations of the parsed constituents. The dependency 
relations containing conjunctive markers (e.g., 
conj_and(), conj_or(), conj_but()) were considered 

for separating nucleus from satellite if the markers 
are present in between two successive clauses. 
Otherwise, the word token contained in the 
mark_() type dependency relation was considered 
as a discourse marker. 

The list of causal verbs is prepared by 
processing the XML files of VerbNet. If any Verb-
Net class file contains any frame with semantic 
type as Cause, we collect the member verbs of that 
XML class file and term the member verbs as 
causal verbs. We used a list that contains a total 
number of 253 causal verbs.  

If any clause tagged as S or SBAR in the parse 
tree contains any causal verb, that clause is consi-
dered as the nucleus and the rest of the clauses de-
note the satellites. Considering the basic theory of 
rhetorical structure (Mann and Thompson 1987), 
the clauses were separated into nucleus and satel-
lite to identify the event and sentiment expressions. 

The direct dependency is identified based on the 
simultaneous presence of locus and the event word 
in the same dependency relation whereas the tran-
sitive dependency is verified if the word is con-
nected to locus and event via one or more 
intermediate dependency relations.  

If the event and sentiment words are together 
present in either nucleus or satellite, the associa-
tion between the two expressions is considered as 
co-referenced. If they occur in nucleus and satellite 
separately, but the event and sentiment words are 
present in at least one direct dependency relation, 
the expressions are termed as co-referenced.  

In the previous example, the event expressions, 
“said” and “remains” are associated with the sen-
timent expression “nervous” as both the event ex-
pressions share the direct dependency relations 
“cop(nervous-7, remains-5)” and “ccomp(said-2, 
nervous-7)” in the nucleus segment. Similarly, the 
event word, “seen” and sentiment word “wild” are 
present in the satellite part and they share a direct 
dependency relation “partmod(swings-12, seen-
13)”. But, no direct dependency relation is present 
between the “nervous” and “seen” or “said” and 
“wild” or “remains” and “wild”.  

6.3 Results 

Though the event annotation is specified in the 
TempEval-2010 corpus, the association between 
the event and sentiment expressions was not speci-
fied in the corpus. Hence, we have carried out the 

25



evaluation manually. The 200 random samples of 
the training set that were used in sentiment expres-
sion identification task have been considered as 
our development set. The Evaluation Vectors 
(EvalV) are prepared manually from each sentence 
of the development and test sets. The vectors 
<EvExp, SentiExp> are filled with the annotated 
events and sentiment expressions by considering 
their association. The annotation of sentiment ex-
pressions using the semi-supervised process has 
been described in Section 4. 
    The rule based baseline and rhetoric based co-
reference systems identify the event and sentiment 
expressions from each sentence and stores them in 
a Co-reference Vector (CorefV). The evaluation is 
carried out by comparing the system generated Co-
reference Vectors (CorefV) with their correspond-
ing Evaluation Vectors (EvalV). The evaluation 
results on 171 test sentences are shown in Table 2. 

 
Co-reference  
Approaches 

Prec.     Rec.    F-Score 
(in %) 

Baseline System 40.03    46.10       42.33 
Rhetoric System 61.25    70.29       65.23 

 
Table 2: Precision (Prec.), Recall (Rec.) and F-

Scores (in %) of the event-sentiment co-reference 
systems  

 
Overall, the precision, recall and F-Scores are 

61.25%, 70.29% and 65.23% for event-sentiment 
co-reference identification using rhetoric clues. 
Though the co-reference technique performs satis-
factorily for identifying the event-sentiment co-
reference, the problem arises in distinguishing the 
corresponding spans of events from an overlapped 
text span of multi-word tokens.  

7 Conclusion  

In this present work, we have identified event and 
sentiment expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. It has been observed that the lex-
ical equivalence based on lexicons performs satis-
factorily but overall, the co-reference entails that 
the presence of indirect affective clues can also be 
traced with the help of rhetoric knowledge and de-
pendency relations. The association of the senti-
ments with their corresponding events can be used 

in future concerning the time based sentiment 
change over events.  

Acknowledgments 
The work is supported by a grant from the India-
Japan Cooperative Programme (DST-JST) 2009 
Research project entitled “Sentiment Analysis 
where AI meets Psychology” funded by Depart-
ment of Science and Technology (DST), Govern-
ment of India. 

References  
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-

brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 

Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation. 

Boguraev, B., Ando, R. K. 2005. TimeBank-
DrivenTimeML Analysis. Annotating, Extracting and 
Reasoning about Time and Events 2005. 

Chaumartin, F. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. SemEval-200,  
Czech Republic. 

Ekman Paul. 1993. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 

Fukuhara T., Nakagawa, H. and Nishida, T. 2007. Un-
derstanding Sentiment of People from News Articles: 
Temporal Sentiment Analysis of Social Events. 
ICWSM’2007, Boulder, Colorado. 

Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245–288. 

Gurevich, O., R. Crouch, T. King, and V. de Paiva. 
2006. Deverbal Nouns in Knowledge Representation. 
Proceedings of FLAIRS, pages 670–675, Melbourne 
Beach, FL. 

Katz, P., Singleton, M. and Wicentowski, R. 2007. 
Swat-mp: the semeval-2007 systems for task 5 and 
task SemEval-2007.  

Kipper-Schuler, K. 2005.  VerbNet: A broad-coverage, 
comprehensive verb lexicon. Ph.D. thesis, Computer 
and Information Science Dept., University of Penn-
sylvania, Philadelphia, PA. 

26



Kolya, A., Ekbal, A. and Bandyopadhyay, S. 2010. 
JU_CSE_TEMP: A First Step towards Evaluating 
Events, Time Expressions and Temporal Relations. 
In Proceedings of the 5th International Workshop on 
Semantic Evaluation, ACL 2010, July 15-16, Swe-
den, pp. 345–350. 

Lafferty, J., McCallum, A.K., Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. International 
Conference on Machine Learning. 

Llorens Hector, Estela Saquete, Borja Navarro. 2010. 
TIPSem (English and Spanish): Evaluating CRFs and 
Semantic Roles. Proceedings of the 5th International 
Workshop on Semantic Evaluation, ACL 2010, pages 
284–291, Uppsala, Sweden, 15-16 July 2010. 

Mani, I., and Wilson G. 2000. Processing of News. In 
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 69-76. 

Mann, W. and S. Thompson. 1987. Rhetorical Structure 
Theory: Description and Construction of Text Struc-
ture. In G. Kempen (ed.), Natural Language Genera-
tion, Martinus Nijhoff, The Hague, pp. 85–96. 

Manning Christopher and Toutanova, Kristina. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC)  

Marneffe, Marie-Catherine de, Bill MacCartney, and 
Christopher D.Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 5th 
International Conference on Language Resources 
and Evaluation.  

Miller George A. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235–312 

Pradhan S., Wayne W., Hacioglu, K., Martin, J.H. and 
Jurafsky, D. 2004. Shallow Semantic Parsing using 
Support Vector Machines. Proceedings of the Human 
Language Technology Conference/North American 
chapter of the Association for Computational Lin-
guistics annual meeting Boston, MA, May 2-7. 

Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gai-
zauskas, R., Setzer, A., Katz, G. and Radev, D. 
TimeML: Robust specification of event and temporal 
expressions in text. In AAAI Spring Symposium on 
New Directions in Question-Answering, pp. 28-34, 
CA, 2003. 

Quirk, R., Greenbaum, S. Leech, G. and Svartvik, J. 
1985. A Comprehensive Grammar of the English 
Language. Longman.  

Strapparava C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In 4th Internation-
al Conference on Language Resources and Evalua-
tion, pp. 1083-1086. 

Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 

Stoyanov, V., and Cardie, C. 2008. Annotating topics of 
opinions. In Proceedings of LREC.  

 

27


