




































Event Causality Recognition Exploiting Multiple Annotators' Judgments and Background Knowledge


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5816â€“5822,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

5816

Event Causality Recognition Exploiting Multiple Annotatorsâ€™ Judgments
and Background Knowledge

Kazuma KadowakiÂ§â€¡, Ryu IidaÂ§Â¶, Kentaro TorisawaÂ§Â¶,
Jong-Hoon OhÂ§, and Julien KloetzerÂ§

Â§Data-driven Intelligent System Research Center (DIRECT),
National Institute of Information and Communications Technology (NICT)
â€¡Advanced Technology Laboratory, The Japan Research Institute, Limited

Â¶Graduate School of Science and Technology, Nara Institute of Science and Technology
{kadowaki,ryu.iida,torisawa,rovellia,julien}@nict.go.jp

Abstract
We propose new BERT-based methods for
recognizing event causality such as â€œsmoke
cigarettesâ€ â†’ â€œdie of lung cancerâ€ written in
web texts. In our methods, we grasp each
annotatorâ€™s policy by training multiple classi-
fiers, each of which predicts the labels given
by a single annotator, and combine the re-
sulting classifiersâ€™ outputs to predict the fi-
nal labels determined by majority vote. Fur-
thermore, we investigate the effect of sup-
plying background knowledge to our classi-
fiers. Since BERT models are pre-trained
with a large corpus, some sort of background
knowledge for event causality may be learned
during pre-training. Our experiments with a
Japanese dataset suggest that this is actually
the case: Performance improved when we pre-
trained the BERT models with web texts con-
taining a large number of event causalities in-
stead of Wikipedia articles or randomly sam-
pled web texts. However, this effect was lim-
ited. Therefore, we further improved perfor-
mance by simply adding texts related to an in-
put causality candidate as background knowl-
edge to the input of the BERT models. We be-
lieve these findings indicate a promising future
research direction.

1 Introduction

Event causality, such as â€œsmoke cigarettesâ€ â†’
â€œdie of lung cancer,â€ is critical knowledge for NLP
applications such as machine reading (Rajpurkar
et al., 2016). For the task of recognizing event
causality written in web texts, we propose new
BERT-based methods that exploit independent la-
bels in a gold dataset provided by multiple annota-
tors. In the creation of the dataset we used (Hashi-
moto et al., 2014), three annotators independently
labeled the data and the final labels were deter-
mined by majority vote. In the previous work,
each annotatorâ€™s independent judgments were ig-
nored, but in our proposed method, we exploit

each annotatorâ€™s judgments in predicting the ma-
jority vote labels.

The dataset we used had a reasonable degree
of inter-annotator agreement (Fleissâ€™ Kappa value
was 0.67), but a discrepancy remained among the
annotators. Despite this discrepancy, we assume
that their judgments are more or less consistent
and that we can improve performance by training
multiple classifiers, each from the labels provided
by an individual annotator to grasp her/his policy,
and by combining the resulting outputs of these
classifiers.

Researchers have studied how to exploit the
differences between the behaviors of annotators
and crowd workers to improve the quality of gold
datasets (Snow et al., 2008; Zaidan and Callison-
Burch, 2011; Zhou et al., 2012; Jurgens, 2013;
Plank et al., 2014; Jamison and Gurevych, 2015;
Felt et al., 2016; Li et al., 2017). In an attempt that
resembles ours, one study (Jamison and Gurevych,
2015) successfully improved the performance of
several NLP tasks by computing the agreement ra-
tio of each training instance and using only those
instances with high agreement. Another work
(Plank et al., 2014) improved part-of-speech tag-
ging by measuring the inter-annotator agreement
on a small number of sampled data and incorpo-
rating this value during training via a modified
loss function. However, neither of them directly
used each annotatorâ€™s judgments, as we did in this
work.

As another research direction, we also inves-
tigate how to appropriately exploit background
knowledge. In previous work, text fragments such
as binary patterns (e.g., â€œA causes Bâ€) and texts
expressing causalities (e.g., â€œHe died due to lung
cancerâ€) retrieved from large corpora were given
to causality recognizers as background knowledge
(Hashimoto et al., 2014; Kruengkrai et al., 2017),
as well as association among words (Torisawa,



5817

2006; Riaz and Girju, 2010; Do et al., 2011), se-
mantic polarities (Hashimoto et al., 2012), an-
swers obtained from a web-based open-domain
why-QA system and other causality related texts
(Kruengkrai et al., 2017), and causality-related
word embeddings (Xie and Mu, 2019).

In this work, we investigate whether BERT (De-
vlin et al., 2019) (especially its pre-training) en-
ables novel ways to exploit background knowl-
edge. Our assumption is that if a BERT model
is pre-trained using a large amount of causality-
rich texts, it can learn some sort of background
knowledge from the text. If the pre-training is ad-
equately performed, background knowledge in the
form of text fragments and a special mechanism
for dealing with them might become obsolete.
Our experimental results show that a BERT model
pre-trained with causality-rich texts achieved sig-
nificantly better performance than models using
Wikipedia articles or randomly sampled web texts,
both of which can be viewed as texts that do not
specifically focus on causality. But the BERT
model does not seem to sufficiently capture back-
ground knowledge, at least in our task setting. Fur-
ther improvement is possible by simply concate-
nating, to an input causality candidate, text frag-
ments related to it as background knowledge.

In our experiments, we show that our best
method significantly outperformed a state-of-the-
art method (Kruengkrai et al., 2017) by about 5%
in average precision.

2 Proposed Method

We propose three BERT-based methods for event-
causality recognition, and show an overview of
them in Figure 1. All of the methods take input
matrix x, which represents an input causality can-
didate such as â€œsmoke cigarettesâ€ â†’ â€œdie of lung
cancer,â€ and obtain each annotatorâ€™s labels (either
ProperCausality or NonProperCausality), which
are denoted by yA, yB , and yC for three annota-
tors A, B, and C, respectively. These are used
for predicting final labels yMV, which are deter-
mined by majority vote. Here, we assume that the
dataset was labeled by three annotators, A, B and
C, but extending the methods to deal with an arbi-
trary number of annotators is straightforward.

The proposed methods compute the probability
P (yMV|x) that a causality candidate represented
by x expresses a proper event causality. We regard
the candidate as proper if and only if P (yMV|x) >

(a) ProposedMulti (b) ProposedSingle

ğ‘ƒ"#$#(ğ‘¦"|ğ±) ğ‘ƒ*#$#(ğ‘¦*|ğ±) ğ‘ƒ+#$#(ğ‘¦+|ğ±)

BERT#$#*BERT#$#"

Avg

BERT#$#+

ğ‘ƒ(ğ‘¦,-|ğ±)

ğ±

ğ‘¦" ğ‘¦* ğ‘¦+

ğ‘ƒ"#$#(ğ‘¦"|ğ±) ğ‘ƒ*#$#(ğ‘¦*|ğ±)ğ‘ƒ+#$#(ğ‘¦+|ğ±)

BERT#$#

Avg

ğ‘ƒ(ğ‘¦,-|ğ±)

ğ±

ğ‘¦" ğ‘¦* ğ‘¦+

(c) ProposedRU

BERT!"

âŠ—
ğ‘ƒ(ğ‘¦'(|ğ±)

ğ‘¦!"
1 âˆ’ ğ‘ƒ!"(ğ‘ğ‘›ğ‘›|ğ±)

{ğ‘¤233}

ğ‘ƒ!"(ğ‘ğ‘›ğ‘›|ğ±)

ğ‘¦'(

ğ±

ğ‘ƒ5676(ğ‘¦5|ğ±)ğ‘ƒ8676(ğ‘¦8|ğ±)ğ‘ƒ9676(ğ‘¦9|ğ±)

BERT676

ğ‘¦5 ğ‘¦8 ğ‘¦9

Figure 1: Proposed architectures, where red symbols
(e.g., yA) stand for gold labels used in training.

P (yMV|x) =
âˆ‘
ann

wannP
lbl
ann(y

ann|x) (1)

P lblann(y
ann|x) = softmax(WannBERTlbl(x)) (2)

P ru(ann|x) = softmax(WDBERTru(x)) (3)
wann = softmaxann(1âˆ’ P ru(ann|x)) (4)

WD âˆˆ R3Ã—d and Wann âˆˆ R2Ã—d are trainable matrices,
where d is size of hidden state in BERT models.

Table 1: Equations used in ProposedRU

Î¸ (Î¸ = 0.5).

ProposedMulti: The outputs of this method are
achieved by an ensemble of three classifiers,
each of which is independently trained (or,
more precisely, fine-tuned) with the judg-
ments of one of three annotators to mimic
her/his judgments. The architecture of this
method is shown in Figure 1(a) and consists
of three pairs of a BERT model and a sub-
sequent softmax layer, where each pair com-
putes the probability P lblann(y

ann|x) of labels
yann of each annotator ann âˆˆ {A,B,C}, in
the same manner as Equation (2) in Table 1.
Probability P (yMV|x) of final label yMV is
the average of P lblann(y

ann|x).

ProposedSingle: This method uses multi-task
learning in which each task corresponds to
predicting labels given by one of the three
annotators. The architecture of this method
is shown in Figure 1(b) and consists of a sin-
gle BERT model with three softmax layers,
where the output of each softmax layer cor-
responds to an annotatorâ€™s label. P (yMV|x)



5818

Probability distribution(s)

[CLS]

Token embeddings

Segment embeddings

Position embeddings

ğ‘¤" [SEP]

+ + +

Feature embeddings

+ + +

+ + +

E[CLS] E1 EN

E[CLS] Eğ‘¤" E[SEP]

EA EA EA

E0 E1 EN

Eother Earg Eother

Feed
forward

Add & norm

Add & norm

ğ‘¤#

+

+

+

E2

Eğ‘¤#

EA

E2

Epredi

ğ‘¤$

+

+

+

Ei

Eğ‘¤$

EA

Ei

Epred

...

...

...

...

...

...cause cause effect

C T1 TNT2 Ti...

...

...

... ... ..
. ...

Trm Trm TrmTrm Trm

...

...

...

...

...

...

...

...

Multi-head
attention

ğ±

Figure 2: Proposed BERT architecture and its input x

is computed in the same way as in Proposed-
Multi.

ProposedRU: This method uses the Proposed-
Single architecture, which consists of a
BERT model (BERTlbl in Figure 1(c)) and
three softmax layers, to compute probabil-
ity P lblann(y

ann|x) for annotator ann. We
also add another BERT model (BERTru) and
its subsequent softmax layer to the architec-
ture to assign a lower weight to the predic-
tions of an annotator who is likely to dis-
agree with the majority vote. The entire
computation is done using the equations in
Table 1. Since each causality candidate is in-
dependently labeled by three annotators, at
most one annotator disagrees with the ma-
jority vote label. To identify that annotator,
BERTru, along with the softmax layer, esti-
mates the probability P ru(ann|x) that anno-
tator ann disagrees with the majority vote.
We call this probability the relative unreli-
ability of ann (Equation (3)). Instead of
averaging P lblann(y

ann|x) as in ProposedSin-
gle, ProposedRU uses the weighted sum of
P lblann(y

ann|x) to predict the final label (Equa-
tion (1)). Weight wann is computed from
P ru(ann|x) to consider the relative unreli-
ability of ann (Equation (4))1.

Representation x of the input causality candi-
date is computed from the entire sentence (e.g.,
â€œHe smoked cigarettes and died of lung cancer
caused by themâ€) that contains a pair of cause

1 Note that this method is applicable only for the problem
setting in which the final binary-class label is determined by
using three annotatorsâ€™ labels. Some extension is needed for
computing relative unreliability P ru(ann|x) for an arbitrary
number of annotators by considering the probability that each
annotator agrees with the majority.

L = Î±LMV + Î²
âˆ‘
ann

Llblann + Î³L
ru (5)

LMV = âˆ’ logP (yMV|x) (6)

Llblann = âˆ’ logP lblann(yann|x) (7)

Lru =

{
0 if yA = yB = yC

âˆ’ logP ru(ann|x) otherwise
(8)

Table 2: Loss functions used in ProposedRU

phrase (â€œsmoke cigarettesâ€) and effect phrase
(â€œdied of lung cancerâ€). x consists of the token,
position, and segment embeddings of each word
(Devlin et al., 2019). We distinguish the words
in the cause and effect candidates from the other
words by giving them feature embeddings; the
cause argument, the cause predicate, the effect ar-
gument, the effect predicate, and the other words
are all given a different feature embedding vector
that is randomly initialized (see Figure 2).

Note that we followed the fine-tuning scheme
presented in Devlin et al. (2019) in the training of
ProposedMulti and ProposedSingle. For train-
ing ProposedRU, we designed a special loss func-
tion L in Equation (5) in Table 2 as the weighted
sum of LMV, Llblann, and L

ru, each of which is a
loss function for P (yMV|x), P lblann(yann|x), and
P ru(ann|x), respectively. Here, Î±, Î², and Î³ are
hyper-parameters (Î±+ Î² + Î³ = 1.0).

The BERT model in our methods is pre-trained
from scratch using causality-rich texts to investi-
gate whether such BERT models can learn back-
ground knowledge during their pre-training. We
used 19,567,386 sentences from 2,799,079 pas-
sages extracted from four billion web pages,
where each passage consists of seven sentences
and includes at least one event causality detected
by a CRF-based causality recognizer2 (Oh et al.,
2013) (3,046,619 event causalities were detected
in the passages).

We also introduce ProposedRU+BK, which in-
tegrates the background knowledge used in the
previous work into ProposedRU. As input, Pro-
posedRU+BK is given a pseudo sentence, which
is the concatenation of the original input sen-
tence and Kruengkrai et al. (2017)â€™s text fragments
embodying background knowledge3 along with a

2The recognizer was applied only to passages that were
extracted by specifically focusing on clue terms such as â€œbe-
cause.â€ Event causalities recognized this way are a different
type than those under focus in this work.

3We used all three types of text fragments from Krueng-



5819

separator to compute input representation x.
Here, we hypothesized that BERT models could

learn some sort of background knowledge by us-
ing causality-rich texts in pre-training. To inves-
tigate whether this is true, we pre-trained sev-
eral BERT models, each using either causality-rich
texts or general texts, and then fine-tuned each of
the models using the same architecture as used in
ProposedRU. We introduce ProposedRUwiki and
ProposedRUweb, in which the pre-training is per-
formed from scratch using a single corpus, that
is, either Wikipedia articles (19,567,381 sentences
retrieved in August 2018) or randomly sampled
web texts (19,567,396 sentences in 1,990,472 web
pages randomly sampled from the four billion web
pages), respectively. The fine-tuning is then done
as in ProposedRU.

In contrast to the above single-step pre-training,
we also attempted to start from the pre-trained
model for ProposedRUweb, which was pre-trained
using general web text, and then additionally pre-
train it with causality-rich texts to make the model
suitable for causality-event recognition4. More
precisely, to focus on the event causality part de-
scribed in each passage, we extracted the pairs
of cause and effect parts detected by Oh et al.
(2013)â€™s CRF-based causality recognizer and used
them as a causality-rich corpus while assuming
that the cause and effect are sentence pairs for
the next-sentence prediction task (9,783,691 pairs
or 19,567,382 sentences). This model is called
ProposedRUweb+pair hereafter.

For comparison, we also introduce Proposed-
RUweb+web, which uses another set of web sen-
tences randomly sampled from four billion web
pages, as a corpus of the general but not causality-
rich texts. The size is the same as that of the cause-
effect pairs used for ProposedRUweb+pair.

3 Experiments

3.1 Settings

We used the datasets for event-causality recogni-
tion in Japanese of Hashimoto et al. (2014). They
regard causality candidate A â†’ B proper if and
only if â€œif A happens, the probability of B in-
creases.â€ To annotate this dataset, three annota-

krai et al. (2017) (i.e., short binary patterns, why-QA sys-
temâ€™s answers, and sentences with clue terms). Every text
fragment was extracted from four billion Japanese web pages.

4 This approach is recommended by Googleâ€™s BERT
implementation (https://github.com/google-
research/bert) for computational efficiency.

Data #Instances #True causalities
Training 107,068 8,986
Development 23,602 3,759
Test 23,650 3,647

Table 3: Statistics of datasets

(d) VanillaBERT (e) MajorityMulti

ğ‘ƒ"#"(ğ‘¦&'|ğ±)

BERT"#"

ğ‘ƒ(ğ‘¦&'|ğ±)

ğ‘¦&'

ğ±

ğ‘ƒ"#$#(ğ‘¦'(|ğ±)ğ‘ƒ,#$#(ğ‘¦'(|ğ±)ğ‘ƒ-#$#(ğ‘¦'(|ğ±)

BERT#$#,BERT#$#"

Avg

BERT#$#-

ğ‘ƒ(ğ‘¦'(|ğ±)

ğ‘¦'(

ğ±
We trained a single
BERT model and its
softmax layer with
majority vote labels.

The architecture is identical to
ProposedMulti, but the train-
ing was performed with majority
vote labels.

(f) MajoritySingle

BERT!"!

ğ‘ƒ$!"!(ğ‘¦'(|ğ±)ğ‘ƒ,!"!(ğ‘¦'(|ğ±)ğ‘ƒ-!"!(ğ‘¦'(|ğ±)

Avg

ğ‘ƒ(ğ‘¦'(|ğ±)

ğ‘¦'(

ğ±
The architecture is identical to ProposedSingle, but
the training was performed with majority vote labels.

Figure 3: Baseline architectures

tors used their own judgment independently, and
each annotatorâ€™s individual labels are included in
the datasets. Table 3 shows their statistics5.

As baselines, we used the state-of-the-art
method of Kruengkrai et al. (2017), MCNN,
which uses a CNN and exploits text fragments
retrieved from four billion web pages as back-
ground knowledge, and three BERT-based meth-
ods, VanillaBERT, MajorityMulti, and Major-
itySingle, which use only majority vote labels
(Figure 3). These BERT-based baselines used the
same pre-trained BERT model (pre-trained with
causality-rich texts) as in our proposed methods.

As the parameter settings of the pre-training
for all of the BERT models, we followed the
BERTBASE settings6 (Devlin et al., 2019) except
for a batch size of 50. For ProposedRUweb+pair
and ProposedRUweb+web, we additionally pre-

5The number of training instances in Hashimoto et al.
(2014)â€™s dataset was 112,098, but we excluded the duplicates
in it and used the resulting 107,068 instances for training each
method.

612-layers, 768 hidden states, 12 heads, and training for
one million steps with a warmup rate of 1% using an Adam
optimizer with a learning rate of 1e-4.

https://github.com/google-research/bert
https://github.com/google-research/bert


5820

Model R P F Avg.P
MCNN 40.2 61.1 48.5 55.1
VanillaBERT 63.6 49.7 55.8* 54.2
MajorityMulti 61.5 51.8 56.2* 55.7
MajoritySingle 48.3 56.8 52.2* 54.4
ProposedMulti 63.9 51.3 56.9* 56.7
ProposeSingle 62.8 52.7 57.3*â€  57.1
ProposedRU 64.0 52.0 57.4* 57.4

â€˜*â€™ stands for significant improvement over MCNN and â€˜â€ â€™
means that over MajorityMulti (McNemar test, p = 0.05).

Table 4: Results of event-causality recognition

Model R P F Avg.P
ProposedRUwiki 57.4 49.6 53.2â‹† 53.3
ProposedRUweb 59.0 50.9 54.6â‹† 54.5
ProposedRU 64.0 52.0 57.4 57.4
ProposedRUweb+web 62.5 49.0 54.9â‹† 54.8
ProposedRUweb+pair 64.3 48.2 55.1â‹† 55.3
ProposedRU+BK 67.4 52.3 58.9 59.9

â€˜â‹†â€™ stands for significant difference from ProposedRU.

Table 5: Results of ProposedRU and its variants

trained the models for 0.3 million steps with a
learning rate of 2e-5. As for the fine-tuning, we
chose the combination of epochs {1, 2, 3} and
learning rates {5e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5}
that achieved the best average precision on 50%
of the development dataset, as done in Kruengkrai
et al. (2017). For ProposedRU and its variants,
we additionally optimized the Î±, Î², and Î³ hyper-
parameters in the same way.

3.2 Results

The recall, precision, F1-score, and average pre-
cision of each method on the majority vote la-
bels of the test dataset are presented in Table 4.
ProposedRU achieved the best F1-score and av-
erage precision. All of the proposed meth-
ods (ProposedMulti, ProposedSingle, and Pro-
posedRU) outperformed all of the baseline meth-
ods (MCNN, VanillaBERT, MajorityMulti, and
MajoritySingle) in average precision. This sug-
gests that using each annotatorâ€™s labels produced
a positive effect. The F1-scores of ProposedRU
and ProposedSingle were not significantly differ-
ent (McNemar test, p = 0.05).

Table 5 shows the results of our investiga-
tion into the pre-training of some sort of back-
ground knowledge. Among the methods to be
compared in Table 5, the first three methods
(i.e., ProposedRUwiki, ProposedRUweb, and Pro-
posedRU) utilized a single-step pre-training from
scratch, whereas the next two methods (Proposed-
RUweb+web and ProposedRUweb+pair) performed

additional pre-training before the fine-tuning. The
results show that pre-training using causality-
rich texts contributes to further performance im-
provement than that using general texts, such as
Wikipedia articles and random web texts (see
ProposedRU vs. ProposedRUwiki, ProposedRU
vs. ProposedRUweb, and ProposedRUweb+pair
vs. ProposedRUweb+web). In short, we can
say that BERT might learn some sort of back-
ground knowledge from causality-rich texts. An
interesting point is that, although the amount of
texts used for ProposedRU is almost the same as
that of the second-step causality-rich pre-training
for ProposedRUweb+pair, the performance differs
considerably (about 2% difference). This sug-
gests that the design of the pre-training steps is not
straightforward and thus merits further research.

We further evaluated ProposedRU+BK, in
which text fragments embodying background
knowledge are concatenated to the input sen-
tence as explained in Section 2. Table 5 shows
that ProposedRU+BK improved the average pre-
cision over ProposedRU by about 2.5% (i.e.,
ProposedRU+BK significantly outperformed the
state-of-the-art method, MCNN, by about 5%),
suggesting that background knowledge in the form
of text fragments is still useful, at least in our cur-
rent experimental setting. However, the usefulness
might be lost when a model is appropriately pre-
trained with a larger amount of texts that covers
even more background knowledge.

4 Conclusion

This paper proposed BERT-based methods for rec-
ognizing event causality that exploit each annota-
torâ€™s independent judgments. By using each an-
notatorâ€™s judgments, we showed that even a sim-
ple multi-task learning approach or an ensem-
ble method improved performance in our experi-
ments. Our best-performing method significantly
outperformed the state-of-the-art method by about
5% in average precision. We also confirmed that
the pre-trained BERT model learns some sort of
background knowledge for event causality from
causality-rich texts, although text fragments em-
bodying background knowledge remain useful, at
least in our current setting. As future work, we
are examining the possibility of using an adversar-
ial learning framework (Goodfellow et al., 2014),
which was recently used in the why-QA task (Oh
et al., 2019) for causality recognition.



5821

References
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171â€“4186.

Quang Do, Yee Seng Chan, and Dan Roth. 2011. Min-
imally supervised event causality identification. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
294â€“303.

Paul Felt, Eric Ringger, and Kevin Seppi. 2016.
Semantic annotation aggregation with conditional
crowdsourcing models and word embeddings. In
Proceedings of the 26th International Conference on
Computational Linguistics, pages 1787â€“1796.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of the 27th Interna-
tional Conference on Neural Information Processing
Systems - Volume 2, pages 2672â€“2680.

Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Junâ€™ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 619â€“630.

Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer,
Motoki Sano, IstvaÌn Varga, Jong-Hoon Oh, and Yu-
taka Kidawara. 2014. Toward future scenario gener-
ation: Extracting event causality exploiting semantic
relation, context, and association features. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 987â€“997.

Emily Jamison and Iryna Gurevych. 2015. Noise or ad-
ditional information? Leveraging crowdsource an-
notation item agreement for natural language tasks.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
291â€“297.

David Jurgens. 2013. Embracing ambiguity: A com-
parison of annotation methodologies for crowd-
sourcing word sense labels. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 556â€“562.

Canasai Kruengkrai, Kentaro Torisawa, Chikara Hashi-
moto, Julien Kloetzer, Jong-Hoon Oh, and Masahiro
Tanaka. 2017. Improving event causality recogni-
tion with multiple background knowledge sources
using multi-column convolutional neural networks.

In Proceedings of the 31st AAAI Conference on Ar-
tificial Intelligence, pages 3466â€“3473.

Jiyi Li, Yukino Baba, and Hisashi Kashima. 2017. Hy-
per questions: Unsupervised targeting of a few ex-
perts in crowdsourcing. In Proceedings of the 2017
ACM on Conference on Information and Knowledge
Management, pages 1069â€“1078.

Jong-Hoon Oh, Kazuma Kadowaki, Julien Kloetzer,
Ryu Iida, and Kentaro Torisawa. 2019. Open-
domain why-question answering with adversarial
learning to encode answer texts. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 4227â€“4237.

Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 1733â€“1743.

Barbara Plank, Dirk Hovy, and Anders SÃ¸gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 742â€“751.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383â€“2392.

Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. 2010 IEEE
Fourth International Conference on Semantic Com-
puting, pages 361â€“368.

Rion Snow, Brendan Oâ€™Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast â€“ but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 254â€“263.

Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using Japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
57â€“64.

Zhipeng Xie and Feiteng Mu. 2019. Distributed rep-
resentation of words in cause and effect spaces. In
Proceedings of the AAAI Conference on Artificial In-
telligence, volume 33, pages 7330â€“7337.

Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1220â€“1229.



5822

Dengyong Zhou, Sumit Basu, Yi Mao, and John C.
Platt. 2012. Learning from the wisdom of crowds
by minimax entropy. In Advances in Neural Infor-
mation Processing Systems 25, pages 2195â€“2203.


