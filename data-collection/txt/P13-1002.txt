



















































Integrating Translation Memory into Phrase-Based Machine Translation during Decoding


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11–21,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Integrating Translation Memory into Phrase-Based 

Machine Translation during Decoding 

 

 

Kun Wang
†
        Chengqing Zong

†
        Keh-Yih Su

‡
 

†
National Laboratory of Pattern Recognition, Institute of Automation, 

Chinese Academy of Sciences, Beijing, China 
‡
Behavior Design Corporation, Taiwan 

†
{kunwang, cqzong}@nlpr.ia.ac.cn, 

‡
kysu@bdc.com.tw 

 

  

 

Abstract 

Since statistical machine translation (SMT) 

and translation memory (TM) complement 

each other in matched and unmatched regions, 

integrated models are proposed in this paper to 

incorporate TM information into phrase-based 

SMT. Unlike previous multi-stage pipeline 

approaches, which directly merge TM result 

into the final output, the proposed models refer 

to the corresponding TM information associat-

ed with each phrase at SMT decoding. On a 

Chinese–English TM database, our experi-

ments show that the proposed integrated Mod-

el-III is significantly better than either the 

SMT or the TM systems when the fuzzy match 

score is above 0.4. Furthermore, integrated 

Model-III achieves overall 3.48 BLEU points 

improvement and 2.62 TER points reduction 

in comparison with the pure SMT system. Be-

sides, the proposed models also outperform 

previous approaches significantly.  

1 Introduction 

Statistical machine translation (SMT), especially 

the phrase-based model (Koehn et al., 2003), has 

developed very fast in the last decade. For cer-

tain language pairs and special applications, 

SMT output has reached an acceptable level, es-

pecially in the domains where abundant parallel 

corpora are available (He et al., 2010). However, 

SMT is rarely applied to professional translation 

because its output quality is still far from satis-

factory. Especially, there is no guarantee that a 

SMT system can produce translations in a con-

sistent manner (Ma et al., 2011). 

In contrast, translation memory (TM), which 

uses the most similar translation sentence (usual-

ly above a certain fuzzy match threshold) in the 

database as the reference for post-editing, has 

been widely adopted in professional translation 

field for many years (Lagoudaki, 2006). TM is 

very useful for repetitive material such as updat-

ed product manuals, and can give high quality 

and consistent translations when the similarity of 

fuzzy match is high. Therefore, professional 

translators trust TM much more than SMT. 

However, high-similarity fuzzy matches are 

available unless the material is very repetitive. 

In general, for those matched segments
1
, TM 

provides more reliable results than SMT does. 

One reason is that the results of TM have been 

revised by human according to the global context, 

but SMT only utilizes local context. However, 

for those unmatched segments, SMT is more re-

liable. Since TM and SMT complement each 

other in those matched and unmatched segments, 

the output quality is expected to be raised signif-

icantly if they can be combined to supplement 

each other. 

In recent years, some previous works have in-

corporated TM matched segments into SMT in a 

pipelined manner (Koehn and Senellart, 2010; 

Zhechev and van Genabith, 2010; He et al., 2011; 

Ma et al., 2011). All these pipeline approaches 

translate the sentence in two stages. They first 

determine whether the extracted TM sentence 

pair should be adopted or not. Most of them use 

fuzzy match score as the threshold, but He et al. 

(2011) and Ma et al. (2011) use a classifier to 

make the judgment. Afterwards, they merge the 

relevant translations of matched segments into 

the source sentence, and then force the SMT sys-

tem to only translate those unmatched segments 

at decoding. 

There are three obvious drawbacks for the 

above pipeline approaches. Firstly, all of them 

determine whether those matched segments 

                                                 
1 We mean “sub-sentential segments” in this work. 

11



should be adopted or not at sentence level. That 

is, they are either all adopted or all abandoned 

regardless of their individual quality. Secondly, 

as several TM target phrases might be available 

for one given TM source phrase due to insertions, 

the incorrect selection made in the merging stage 

cannot be remedied in the following translation 

stage. For example, there are six possible corre-

sponding TM target phrases for the given TM 

source phrase “关联4 的5 对象6” (as shown in 
Figure 1) such as “object2 that3 is4 associated5”, 

and “an1 object2 that3 is4 associated5  with6”, etc. 

And it is hard to tell which one should be adopt-

ed in the merging stage. Thirdly, the pipeline 

approach does not utilize the SMT probabilistic 

information in deciding whether a matched TM 

phrase should be adopted or not, and which tar-

get phrase should be selected when we have mul-

tiple candidates. Therefore, the possible im-

provements resulted from those pipeline ap-

proaches are quite limited. 

On the other hand, instead of directly merging 

TM matched phrases into the source sentence, 

some approaches (Biçici and Dymetman, 2008; 

Simard and Isabelle, 2009) simply add the long-

est matched pairs into SMT phrase table, and 

then associate them with a fixed large probability 

value to favor the corresponding TM target 

phrase at SMT decoding. However, since only 

one aligned target phrase will be added for each 

matched source phrase, they share most draw-

backs with the pipeline approaches mentioned 

above and merely achieve similar performance. 

To avoid the drawbacks of the pipeline ap-

proach (mainly due to making a hard decision 

before decoding), we propose several integrated 

models to completely make use of TM infor-

mation during decoding. For each TM source 

phrase, we keep all its possible corresponding 

target phrases (instead of keeping only one of 

them). The integrated models then consider all 

corresponding TM target phrases and SMT pref-

erence during decoding. Therefore, the proposed 

integrated models combine SMT and TM at a 

deep level (versus the surface level at which TM 

result is directly plugged in under previous pipe-

line approaches). 

On a Chinese–English computer technical 

documents TM database, our experiments have 

shown that the proposed Model-III improves the 

translation quality significantly over either the 

pure phrase-based SMT or the TM systems when 

the fuzzy match score is above 0.4. Compared 

with the pure SMT system, the proposed inte-

grated Model-III achieves 3.48 BLEU points im-

provement and 2.62 TER points reduction over-

all. Furthermore, the proposed models signifi-

cantly outperform previous pipeline approaches. 

2 Problem Formulation 

Compared with the standard phrase-based ma-

chine translation model, the translation problem 

is reformulated as follows (only based on the 

best TM, however, it is similar for multiple TM 

sentences): 

  (1) 

Where  is the given source sentence to be trans-

lated,  is the corresponding target sentence and  

is the final translation;  

are the associated information of the best TM 

sentence-pair;  and  denote the corre-

sponding TM sentence pair;  denotes its 

associated fuzzy match score (from 0.0 to 1.0); 

 is the editing operations between  and ; 

and  denotes the word alignment between 

 and . 

Let  and  denote the k-th associated 

source phrase and target phrase, respectively. 

Also,  and  denote the associated source 

phrase sequence and the target phrase sequence, 

respectively (total  phrases without insertion). 

Then the above formula (1) can be decomposed 

as below: 

 

(2) 

Afterwards, for any given source phrase , 

we can find its corresponding TM source phrase 

 and all possible TM target phrases (each 

of them is denoted by ) with the help of 

corresponding editing operations  and word 

alignment . As mentioned above, we can 

have six different possible TM target phrases for 

the TM source phrase “关联 4 的 5 对象 6”. This 

获取0                    与1  批注2  标签3  关联4  的5  对象6  。7

获取0  或1  设置2  与3  批注4             关联5  的6  对象7  。8

gets0  an1  object2  that3  is4  associated5  with6  the7  annotation8  label9  .10

Source

TM Source

TM Target
 

Figure 1: Phrase Mapping Example 

12



is because there are insertions around the directly 

aligned TM target phrase. 

In the above Equation (2), we first segment the 

given source sentence into various phrases, and 

then translate the sentence based on those source 

phrases. Also,  is replaced by , as they 

are actually the same segmentation sequence. 

Assume that the segmentation probability 

 is a uniform distribution, with the corre-

sponding TM source and target phrases obtained 

above, this problem can be further simplified as 

follows: 

 

(3) 

Where  is the corresponding TM phrase 

matching status for , which is a vector consist-

ing of various indicators (e.g., Target Phrase 

Content Matching Status, etc., to be defined lat-

er), and reflects the quality of the given candi-

date;  is the linking status vector of  (the 

aligned source phrase of  within ), and indi-

cates the matching and linking status in the 

source side (which is closely related to the status 

in the target side); also,  indicates the corre-

sponding TM fuzzy match interval specified later.  

In the second line of Equation (3), we convert 

the fuzzy match score  into its correspond-

ing interval , and incorporate all possible com-

binations of TM target phrases. Afterwards, we 

select the best one in the third line. Last, in the 

fourth line, we introduce the source matching 

status and the target linking status (detailed fea-

tures would be defined later). Since we might 

have several possible TM target phrases , 

the one with the maximum score will be adopted 

during decoding. 

The first factor  in the above for-

mula (3) is just the typical phrase-based SMT 

model, and the second factor  (to be 

specified in the Section 3) is the information de-

rived from the TM sentence pair. Therefore, we 

can still keep the original phrase-based SMT 

model and only pay attention to how to extract 

useful information from the best TM sentence 

pair to guide SMT decoding. 

3 Proposed Models 

Three integrated models are proposed to incorpo-

rate different features as follows: 

3.1 Model-I 

In this simplest model, we only consider Target 

Phrase Content Matching Status (TCM) for . 

For , we consider four different features at the 

same time: Source Phrase Content Matching 

Status (SCM), Number of Linking Neighbors 

(NLN), Source Phrase Length (SPL), and Sen-

tence End Punctuation Indicator (SEP). Those 

features will be defined below.  is 

then specified as: 

 

All features incorporated in this model are speci-

fied as follows: 

TM Fuzzy Match Interval (z): The fuzzy match 

score (FMS) between source sentence  and TM 

source sentence  indicates the reliability of 

the given TM sentence, and is defined as (Sikes, 

2007): 

 

Where  is the word-based 

Levenshtein Distance (Levenshtein, 1966) be-

tween  and . We equally divide FMS into 

ten fuzzy match intervals such as: [0.9, 1.0), [0.8, 

0.9) etc., and the index  specifies the corre-

sponding interval. For example, since the fuzzy 

match score between  and  in Figure 1 is 

0.667, then . 

Target Phrase Content Matching Status 
(TCM): It indicates the content matching status 

between   and , and reflects the quality 

of . Because  is nearly perfect when FMS 

is high, if the similarity between    and  

is high, it implies that the given  is possibly a 

good candidate. It is a member of {Same, High, 

Low, NA (Not-Applicable)}, and is specified as: 

(1) If  is not null: 

(a) if , ; 

(b) else if , ; 

(c) else, ; 

(2) If  is null, ; 

Here  is null means that either there is no 

corresponding TM source phrase  or 

there is no corresponding TM target phrase 

13



 aligned with . In the example of 

Figure 1, assume that the given  is “关联 5  

的 6  对象 7” and  is “object that is associated”. 
If  is “object2 that3 is4 associated5”, 

; if  is “an1 object2 that3 

is4 associated5”, . 

Source Phrase Content Matching Status 
(SCM): Which indicates the content matching 

status between  and , and it affects 

the matching status of  and  greatly. 

The more similar  is to , the more 

similar   is to . It is a member of {Same, 

High, Low, NA} and is defined as: 

(1) If  is not null: 

(a) if , ; 

(b) else if , 

; 

(c) else, ; 

(2) If  is null, ; 

Here  is null means that there is no corre-

sponding TM source phrase  for the giv-

en source phrase . Take the source phrase  

 “关联 5 的 6 对象 7” in Figure 1 for an ex-

ample, since its corresponding  is “关联 4 

的 5 对象 6”, then . 

Number of Linking Neighbors (NLN): Usually, 

the context of a source phrase would affect its 

target translation. The more similar the context 

are, the more likely that the translations are the 

same. Therefore, this NLN feature reflects the 

number of matched neighbors (words) and it is a 

vector of <x, y>. Where “x” denotes the number 

of matched source neighbors; and “y” denotes 

how many those neighbors are also linked to tar-

get words (not null), which also affects the TM 

target phrase selection. This feature is a member 

of {<x, y>: <2, 2>, <2, 1>, <2, 0>, <1, 1>, <1, 0>, 

<0, 0>}. For the source phrase “关联 5 的 6 对象

7” in Figure 1, the corresponding TM source 

phrase is “关联 4 的 5 对象 6” . As only their 

right neighbors “。8” and “。7” are matched, and 

“。7” is aligned with “.10”, NLN will be <1, 1>. 

Source Phrase Length (SPL): Usually the long-

er the source phrase is, the more reliable the TM 

target phrase is. For example, the corresponding 

 for the source phrase with 5 words 

would be more reliable than that with only one 

word. This feature denotes the number of words 

included in , and is a member of {1, 2, 3, 4, 

≥5}. For the case “关联 5 的 6 对象 7”, SPL will 
be 3.  

Sentence End Punctuation Indicator (SEP): 

Which indicates whether the current phrase is a 

punctuation at the end of the sentence, and is a 

member of {Yes, No}. For example, the SEP for 

“关联 5 的 6 对象 7” will be “No”. It is intro-
duced because the SCM and TCM for a sen-

tence-end-punctuation are always “Same” re-

gardless of other features. Therefore, it is used to 

distinguish this special case from other cases. 

3.2 Model-II 

As Model-I ignores the relationship among vari-

ous possible TM target phrases, we add two fea-

tures TM Candidate Set Status (CSS) and Long-

est TM Candidate Indicator (LTC) to incorporate 

this relationship among them.  Since CSS is re-

dundant after LTC is known, we thus ignore it 

for evaluating TCM probability in the following 

derivation: 

 

The two new features CSS and LTC adopted in 

Model-II are defined as follows: 

TM Candidate Set Status (CSS): Which re-

stricts the possible status of , and is a 

member of {Single, Left-Ext, Right-Ext, Both-Ext, 

NA}. Where “Single” means that there is only 

one  candidate for the given source 

phrase ; “Left-Ext” means that there are 

multiple  candidates, and all the candi-

dates are generated by extending only the left 

boundary; “Right-Ext” means that there are mul-

tiple  candidates, and all the candidates 

are generated by only extending to the right; 

“Both-Ext” means that there are multiple  

candidates, and the candidates are generated by 

extending to both sides; “NA” means that 

 is null. 

For “关联 4 的 5 对象 6” in Figure 1, the 
linked TM target phrase is “object2 that3 is4 asso-

ciated5”, and there are 5 other candidates by ex-

tending to both sides. Therefore, 

. 

Longest TM Candidate Indicator (LTC): 

Which indicates whether the given  is the 

longest candidate or not, and is a member of 

{Original, Left-Longest, Right-Longest, Both-

Longest, Medium, NA}. Where “Original” means 

that the given  is the one without exten-

sion; “Left-Longest” means that the given 

14



 is only extended to the left and is the 

longest one; “Right-Longest” means that the giv-

en  is only extended to the right and is 

the longest one; “Both-Longest” means that the 

given  is extended to both sides and is the 

longest one; “Medium” means that the given 

 has been extended but not the longest 

one; “NA” means that  is null. 

For  “object2 that3 is4 associated5” in 

Figure 1, ; for  “an1 ob-

ject2 that3 is4 associated5”, ; 

for the longest  “an1 object2 that3 is4 as-

sociated5 with6 the7”, . 

3.3 Model-III 

The abovementioned integrated models ignore 

the reordering information implied by TM. 

Therefore, we add a new feature Target Phrase 

Adjacent Candidate    Relative   Position   

Matching    Status (CPM) into Model-II and 

Model-III is given as: 

 

We assume that CPM is independent with SPL 

and SEP, because the length of source phrase 

would not affect reordering too much and SEP is 

used to distinguish the sentence end punctuation 

with other phrases.  

The new feature CPM adopted in Model-III is 

defined as: 

Target Phrase Adjacent Candidate Relative 

Position Matching Status (CPM): Which indi-

cates the matching status between the relative 

position of 
 
and the relative position of  

. It checks if  are 

positioned in the same order with 

, and reflects the quality of 

ordering the given target candidate . It is a 

member of {Adjacent-Same, Adjacent-Substitute, 

Linked-Interleaved, Linked-Cross, Linked-

Reversed, Skip-Forward, Skip-Cross, Skip-

Reversed, NA}. Recall that 
 
is always right ad-

jacent to , then various cases are defined as 

follows: 

(1) If both  and  are not null: 

(a) If  is on the right of  

and they are also adjacent to each other: 

i. If the right boundary words of  and 

 are the same, and the left 

boundary words of  and  are 

the same, ; 

ii. Otherwise, ; 

(b) If  is on the right of  

but they are not adjacent to each other, 

; 

(c) If  is not on the right of 

: 

i. If there are cross parts between  

and , ; 

ii. Otherwise, ; 

(2) If   is null but  is not null, 

then find the first which is 

not null (  starts from 2)
2
: 

(a) If  is on the right of , 

; 

(b) If  is not on the right of 

: 

i. If there are cross parts between  

and , ; 

ii. Otherwise, . 

(3) If  is null, . 

In Figure 1, assume that ,  and 

 are “gets an”, “object that is associat-

ed with” and “gets0 an1”, respectively. For 

 “object2 that3 is4 associated5”, because 

 is on the right of  and they are 

adjacent pair, and both boundary words (“an” 

and “an1”; “object” and “object2”) are matched, 

; for  “an1 object2 

that3 is4 associated5”, because there are cross 

parts “an1” between  and , 

. On the other hand, as-

sume that ,  and  are “gets”, “ob-

ject that is associated with” and “gets0”, respec-

tively. For  “an1 object2 that3 is4 associ-

ated5”, because  and  are adja-

cent pair, but the left boundary words of  and 

 (“object” and “an1”) are not matched, 

; for  “object2 

that3 is4 associated5”, because  is on the 

right of  but they are not adjacent pair, 

therefore, . One more 

example, assume that ,  and  are 

“the annotation label”, “object that is associated 

with” and “the7 annotation8 label9”, respectively. 

For  “an1 object2 that3 is4 associated5”, 

because  is on the left of , and 

there are no cross parts, .  

                                                 
2 It can be identified by simply memorizing the index of 

nearest non-null  during search. 

15



4 Experiments 

4.1 Experimental Setup 

Our TM database consists of computer domain 

Chinese-English translation sentence-pairs, 

which contains about 267k sentence-pairs. The 

average length of Chinese sentences is 13.85 

words and that of English sentences is 13.86 

words. We randomly selected a development set 

and a test set, and then the remaining sentence 

pairs are for training set. The detailed corpus sta-

tistics are shown in Table 1. Furthermore, devel-

opment set and test set are divided into various 

intervals according to their best fuzzy match 

scores. Corpus statistics for each interval in the 

test set are shown in Table 2.  

For the phrase-based SMT system, we adopted 

the Moses toolkit (Koehn et al., 2007). The sys-

tem configurations are as follows: GIZA++ (Och 

and Ney, 2003) is used to obtain the bidirectional 

word alignments. Afterwards, “intersection”
3
 

refinement (Koehn et al., 2003) is adopted to ex-

tract phrase-pairs. We use the SRI Language 

Model toolkit (Stolcke, 2002) to train a 5-gram 

model with modified Kneser-Ney smoothing 

(Kneser and Ney, 1995; Chen and Goodman, 

1998) on the target-side (English) training corpus. 

All the feature weights and the weight for each 

probability factor (3 factors for Model-III) are 

tuned on the development set with minimum-

error-rate training (MERT) (Och, 2003). The 

maximum phrase length is set to 7 in our exper-

iments. 

In this work, the translation performance is 

measured with case-insensitive BLEU-4 score 

(Papineni et al., 2002) and TER score (Snover et 

al., 2006). Statistical significance test is conduct-

ed with re-sampling (1,000 times) approach 

(Koehn, 2004) in 95% confidence level. 

4.2 Cross-Fold Translation 

To estimate the probabilities of proposed models, 

the corresponding phrase segmentations for bi-

lingual sentences are required. As we want to 

check what actually happened during decoding in 

the real situation, cross-fold translation is used to 

obtain the corresponding phrase segmentations. 

We first extract 95% of the bilingual sentences as 

a new training corpus to train a SMT system. 

Afterwards, we generate the corresponding 

phrase segmentations for the remaining 5% bi-

                                                 
3 “grow-diag-final” and “grow-diag-final-and” are also test-

ed. However, “intersection” is the best option in our exper-

iments, especially for those high fuzzy match intervals.  

lingual sentences with Forced Decoding (Li et 

al., 2000; Zollmann et al., 2008; Auli et al., 2009; 

Wisniewski et al., 2010), which searches the best 

phrase segmentation for the specified output. 

Having repeated the above steps 20 times
4
, we 

obtain the corresponding phrase segmentations 

for the SMT training data (which will then be 

used to train the integrated models). 

Due to OOV words and insertion words, not 

all given source sentences can generate the de-

sired results through forced decoding. Fortunate-

ly, in our work, 71.7% of the training bilingual 

sentences can generate the corresponding target 

results. The remaining 28.3% of the sentence 

pairs are thus not adopted for generating training 

samples. Furthermore, more than 90% obtained 

source phrases are observed to be less than 5 

words, which explains why five different quanti-

zation levels are adopted for Source Phrase 

Length (SPL) in section 3.1. 

4.3 Translation Results 

After obtaining all the training samples via cross-

fold translation, we use Factored Language 

Model toolkit (Kirchhoff et al., 2007) to estimate 

the probabilities of integrated models with Wit-

ten-Bell smoothing (Bell et al., 1990; Witten et 

al., 1991) and Back-off method. Afterwards, we 

incorporate the TM information  for 

each  phrase  at  decoding.   All  experiments  are 

                                                 
4  This training process only took about 10 hours on our 

Ubuntu server (Intel 4-core Xeon 3.47GHz, 132 GB of 

RAM).  

  Train Develop Test 
#Sentences 261,906 2,569 2,576 

#Chn. Words 3,623,516 38,585 38,648 

#Chn. VOC. 43,112 3,287 3,460 

#Eng. Words 3,627,028 38,329 38,510 

#Eng. VOC. 44,221 3,993 4,046 

Table 1: Corpus Statistics 

Intervals #Sentences #Words W/S 

[0.9, 1.0) 269 4,468 16.6 

[0.8, 0.9) 362 5,004 13.8 

[0.7, 0.8) 290 4,046 14.0 

[0.6, 0.7) 379 4,998 13.2 

[0.5, 0.6) 472 6,073 12.9 

[0.4, 0.5) 401 5,921 14.8 

[0.3, 0.4) 305 5,499 18.0 

(0.0, 0.3) 98 2,639 26.9 

(0.0, 1.0) 2,576 38,648 15.0 

Table 2: Corpus Statistics for Test-Set 

16



Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 

[0.9, 1.0) 81.31 81.38 85.44  * 86.47  *# 89.41  *# 82.79 77.72 82.78 

[0.8, 0.9) 73.25 76.16 79.97  * 80.89  * 84.04  *# 79.74  * 73.00 77.66 

[0.7, 0.8) 63.62 67.71 71.65  * 72.39  * 74.73  *# 71.02  * 66.54 69.78 

[0.6, 0.7) 43.64 54.56 54.88    # 55.88  *# 57.53  *# 53.06 54.00 56.37 

[0.5, 0.6) 27.37 46.32 47.32  *# 47.45  *# 47.54  *# 39.31 46.06 47.73 

[0.4, 0.5) 15.43 37.18 37.25    # 37.60    # 38.18  *# 28.99 36.23 37.93 

[0.3, 0.4) 8.24 29.27 29.52    # 29.38    # 29.15    # 23.58 29.40 30.20 

(0.0, 0.3) 4.13 26.38 25.61    # 25.32    # 25.57    # 18.56 26.30 26.92 

(0.0, 1.0) 40.17 53.03 54.57  *# 55.10  *# 56.51  *# 50.31 51.98 54.32 

Table 3: Translation Results (BLEU%). Scores marked by “*” are significantly better (p < 0.05) than both TM 

and SMT systems, and those marked by “#” are significantly better (p < 0.05) than Koehn-10. 

Intervals TM SMT Model-I Model-II Model-III Koehn-10 Ma-11 Ma-11-U 

[0.9, 1.0) 9.79 13.01 9.22      # 8.52    *# 6.77    *# 13.01 18.80 11.90 

[0.8, 0.9) 16.21 16.07 13.12  *# 12.74  *# 10.75  *# 15.27 20.60 14.74 

[0.7, 0.8) 27.79 22.80 19.10  *# 18.58  *# 17.11  *# 21.85 25.33 21.11 

[0.6, 0.7) 46.40 33.38 32.63    # 32.27  *# 29.96  *# 35.93 35.24 31.76 

[0.5, 0.6) 62.59 39.56 38.24  *# 38.77  *# 38.74  *# 47.37 40.24 38.01 

[0.4, 0.5) 73.93 47.19 47.03    # 46.34  *# 46.00  *# 56.84 48.74 46.10 

[0.3, 0.4) 79.86 55.71 55.38    # 55.44    # 55.87    # 64.55 55.93 54.15 

(0.0, 0.3) 85.31 61.76 62.38    # 63.66    # 63.51    # 73.30 63.00 60.67 

(0.0, 1.0) 50.51 35.88 34.34  *# 34.18  *# 33.26  *# 40.75 38.10 34.49 

Table 4: Translation Results (TER%). Scores marked by “*” are significantly better (p < 0.05) than both TM and 

SMT systems, and those marked by “#” are significantly better (p < 0.05) than Koehn-10. 

conducted using the Moses phrase-based decoder 

(Koehn et al., 2007). 

Table 3 and 4 give the translation results of 

TM, SMT, and three integrated models in the test 

set. In the tables, the best translation results (ei-

ther in BLEU or TER) at each interval have been 

marked in bold. Scores marked by “*” are signif-

icantly better (p < 0.05) than both the TM and 

the SMT systems. 

It can be seen that TM significantly exceeds 

SMT at the interval [0.9, 1.0) in TER score, 

which illustrates why professional translators 

prefer TM rather than SMT as their assistant tool. 

Compared with TM and SMT, Model-I is signif-

icantly better than the SMT system in either 

BLEU or TER when the fuzzy match score is 

above 0.7; Model-II significantly outperforms 

both the TM and the SMT systems in either 

BLEU or TER when the fuzzy match score is 

above 0.5; Model-III significantly exceeds both 

the TM and the SMT systems in either BLEU or 

TER when the fuzzy match score is above 0.4. 

All these improvements show that our integrated 

models have combined the strength of both TM 

and SMT.  

However, the improvements from integrated 

models get less when the fuzzy match score de-

creases. For example, Model-III outperforms 

SMT 8.03 BLEU points at interval [0.9, 1.0), 

while the advantage is only 2.97 BLEU points at 

interval [0.6, 0.7). This is because lower fuzzy 

match score means that there are more un-

matched parts between  and ; the output of 

TM is thus less reliable. 

Across all intervals (the last row in the table), 

Model-III not only achieves the best BLEU score 

(56.51), but also gets the best TER score (33.26). 

If intervals are evaluated separately, when the 

fuzzy match score is above 0.4, Model-III out-

performs both Model-II and Model-I in either 

BLEU or TER. Model-II also exceeds Model-I in 

either BLEU or TER. The only exception is at 

interval [0.5, 0.6), in which Model-I achieves the 

best TER score. This might be due to that the 

optimization criterion for MERT is BLEU rather 

than TER in our work. 

4.4 Comparison with Previous Work 

In order to compare our proposed models with 

previous work, we re-implement two XML-

Markup approaches: (Koehn and Senellart, 2010) 

and (Ma et al, 2011), which are denoted as 

Koehn-10 and Ma-11, respectively. They are 

selected because they report superior perfor-

mances in the literature. A brief description of 

them is as follows: 

17



Source 
如果 0 禁用 1 此 2 策略 3 设置 4 ，5 internet6 explorer7 不 8 搜索 9 internet10 查找 11 浏览器 12 

的 13 新 14 版本 15 ，16 因此 17 不 18 会 19 提示 20 用户 21 安装 22 。23 

Reference 

if0 you1 disable2 this3 policy4 setting5 ,6 internet7 explorer8 does9 not10 check11 the12 internet13 

for14 new15 versions16 of17 the18 browser19 ,20 so21 does22 not23 prompt24 users25 to26 install27 

them28 .29 

TM 

Source 

如果 0 不 1 配置 2 此 3 策略 4 设置 5 ，6 internet7 explorer8 不 9 搜索 10 internet11 查找 12 浏览

器 13 的 14 新 15 版本 16 ，17 因此 18 不 19 会 20 提示 21 用户 22 安装 23 。24 

TM 

Target 

if0 you1 do2 not3 configure4 this5 policy6 setting7 ,8 internet9 explorer10 does11 not12 check13 the14 

internet15 for16 new17 versions18 of19 the20 browser21 ,22 so23 does24 not25 prompt26 users27 to28 

install29 them30 .31 

TM 

Alignment 

0-0 1-3 2-4 3-5 4-6 5-7 6-8 7-9 8-10 9-11 11-15 13-21 14-19 15-17 16-18 17-22 18-23 19-24 

21-26 22-27 23-29 24-31 

SMT 

if you disable this policy setting , internet explorer does not prompt users to install internet for 

new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-

tion] 

Koehn-10 

if you do you disable this policy setting , internet explorer does not check the internet for new 

versions of the browser , so does not prompt users to install them .    [Insert two spurious target 

words] 

Ma-11 

if you disable this policy setting , internet explorer does not prompt users to install internet for 

new versions of the browser .    [Miss 7 target words: 9~12, 20~21, 28; Has one wrong permuta-

tion] 

Model-I 

if you disable this policy setting , internet explorer does not prompt users to install new ver-

sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 

wrong permutation] 

Model-II 

if you disable this policy setting , internet explorer does not prompt users to install new ver-

sions of the browser , so does not check the internet .    [Miss 2 target words: 14, 28; Has one 

wrong permutation] 

Model-III 
if you disable this policy setting , internet explorer does not check the internet for new versions 

of the browser , so does not prompt users to install them .    [Exactly the same as the reference] 

Figure 2: A Translation Example at Interval [0.9, 1.0] (with FMS=0.920) 

Koehn et al. (2010) first find out the un-

matched parts between the given source sentence 

and TM source sentence. Afterwards, for each 

unmatched phrase in the TM source sentence, 

they replace its corresponding translation in the 

TM target sentence by the corresponding source 

phrase in the input sentence, and then mark the 

substitution part. After replacing the correspond-

ing translations of all unmatched source phrases 

in the TM target sentence, an XML input sen-

tence (with mixed TM target phrases and marked 

input source phrases) is thus obtained. The SMT 

decoder then only translates the un-

matched/marked source phrases and gets the de-

sired results. Therefore, the inserted parts in the 

TM target sentence are automatically included. 

They use fuzzy match score to determine wheth-

er the current sentence should be marked or not; 

and their experiments show that this method is 

only effective when the fuzzy match score is 

above 0.8. 

Ma et al. (2011) think fuzzy match score is not 

reliable and use a discriminative learning method 

to decide whether the current sentence should be 

marked or not. Another difference between Ma-

11 and Koehn-10 is how the XML input is con-

structed. In constructing the XML input sentence, 

Ma-11 replaces each matched source phrase in 

the given source sentence with the corresponding 

TM target phrase. Therefore, the inserted parts in 

the TM target sentence are not included. In Ma’s 

another paper (He et al., 2011), more linguistic 

features for discriminative learning are also add-

ed. In our work, we only re-implement the XML-

Markup method used in (He et al., 2011; Ma et al, 

2011), but do not implement the discriminative 

learning method. This is because the features 

adopted in their discriminative learning are com-

plicated and difficult to re-implement. However, 

the proposed Model-III even outperforms the 

upper bound of their methods, which will be dis-

cussed later.  

Table 3 and 4 give the translation results of 

Koehn-10 and Ma-11 (without the discriminator). 

Scores marked by “#” are significantly better (p 

< 0.05) than Koehn-10. Besides, the upper bound 

of (Ma et al, 2011) is also given in the tables, 

which is denoted as Ma-11-U. We calculate this 

18



upper bound according to the method described 

in (Ma et al., 2011). Since He et al., (2011) only 

add more linguistic features to the discriminative 

learning method, the upper bound of (He et al., 

2011) is still the same with (Ma et al., 2011); 

therefore, Ma-11-U applies for both cases. 

It is observed that Model-III significantly ex-

ceeds Koehn-10 at all intervals. More important-

ly, the proposed models achieve much better 

TER score than the TM system does at interval 

[0.9, 1.0), but Koehn-10 does not even exceed 

the TM system at this interval. Furthermore, 

Model-III is much better than Ma-11-U at most 

intervals. Therefore, it can be concluded that the 

proposed models outperform the pipeline ap-

proaches significantly.  

Figure 2 gives an example at interval [0.9, 1.0), 

which shows the difference among different sys-

tem outputs. It can be seen that “you do” is re-

dundant for Koehn-10, because they are inser-

tions and thus are kept in the XML input. How-

ever, SMT system still inserts another “you”, 

regardless of “you do” has already existed. This 

problem does not occur at Ma-11, but it misses 

some words and adopts one wrong permutation. 

Besides, Model-I selects more right words than 

SMT does but still puts them in wrong positions 

due to ignoring TM reordering information. In 

this example, Model-II obtains the same results 

with Model-I because it also lacks reordering 

information. Last, since Model-III considers both 

TM content and TM position information, it 

gives a perfect translation. 

5 Conclusion and Future Work 

Unlike the previous pipeline approaches, which 

directly merge TM phrases into the final transla-

tion result, we integrate TM information of each 

source phrase into the phrase-based SMT at de-

coding. In addition, all possible TM target 

phrases are kept and the proposed models select 

the best one during decoding via referring SMT 

information. Besides, the integrated model con-

siders the probability information of both SMT 

and TM factors. 

The experiments show that the proposed 

Model-III outperforms both the TM and the SMT 

systems significantly (p < 0.05) in either BLEU 

or TER when fuzzy match score is above 0.4. 

Compared with the pure SMT system, Model-III 

achieves overall 3.48 BLEU points improvement 

and 2.62 TER points reduction on a Chinese–

English TM database. Furthermore, Model-III 

significantly exceeds all previous pipeline ap-

proaches. Similar improvements are also ob-

served on the Hansards parts of LDC2004T08 

(not shown in this paper due to space limitation). 

Since no language-dependent feature is adopted, 

the proposed approaches can be easily adapted 

for other language pairs. 

Moreover, following the approaches of 

Koehn-10 and Ma-11 (to give a fair comparison), 

training data for SMT and TM are the same in 

the current experiments. However, the TM is 

expected to play an even more important role 

when the SMT training-set differs from the TM 

database, as additional phrase-pairs that are un-

seen in the SMT phrase table can be extracted 

from TM (which can then be dynamically added 

into the SMT phrase table at decoding time). Our 

another study has shown that the integrated mod-

el would be even more effective when the TM 

database and the SMT training data-set are from 

different corpora in the same domain (not shown 

in this paper). In addition, more source phrases 

can be matched if a set of high-FMS sentences, 

instead of only the sentence with the highest 

FMS, can be extracted and referred at the same 

time. And it could further raise the performance. 

Last, some related approaches (Smith and 

Clark, 2009; Phillips, 2011) combine SMT and 

example-based machine translation (EBMT) 

(Nagao, 1984). It would be also interesting to 

compare our integrated approach with that of 

theirs. 
 

Acknowledgments 

 

The research work has been funded by the Hi-

Tech Research and Development Program 

(“863” Program) of China under Grant No. 

2011AA01A207, 2012AA011101, and 

2012AA011102 and also supported by the Key 

Project of Knowledge Innovation Program of 

Chinese Academy of Sciences under Grant 

No.KGZD-EW-501.  

The authors would like to thank the anony-

mous reviewers for their insightful comments 

and suggestions. Our sincere thanks are also ex-

tended to Dr. Yanjun Ma and Dr. Yifan He for 

their valuable discussions during this study.  

References  

Michael Auli, Adam Lopez, Hieu Hoang and Philipp 

Koehn, 2009. A systematic analysis of translation 

model search spaces. In Proceedings of the Fourth 

Workshop on Statistical Machine Translation, pag-

es 224–232. 

19



Timothy C. Bell, J.G. Cleary and Ian H. Witten, 1990. 

Text compression: Prentice Hall, Englewood Cliffs, 

NJ. 

Ergun Biçici and Marc Dymetman. 2008. Dynamic 

translation memory: using statistical machine trans-

lation to improve translation memory fuzzy match-

es. In Proceedings of the 9th International Confer-

ence on Intelligent Text Processing and Computa-

tional Linguistics (CICLing 2008), pages 454–465. 

Stanley F. Chen and Joshua Goodman. 1998. An em-

pirical study of smoothing techniques for language 

modeling. Technical Report TR-10-98, Harvard 

University Center for Research in Computing 

Technology. 

Yifan He, Yanjun Ma, Josef van Genabith and Andy 

Way, 2010. Bridging SMT and TM with transla-

tion recommendation. In Proceedings of the 48th 

Annual Meeting of the Association for Computa-

tional Linguistics (ACL), pages 622–630. 

Yifan He, Yanjun Ma, Andy Way and Josef van 

Genabith. 2011. Rich linguistic features for transla-

tion memory-inspired consistent translation. In 

Proceedings of the Thirteenth Machine Translation 

Summit, pages 456–463. 

Reinhard Kneser and Hermann Ney. 1995. Improved 

backing-off for m-gram language modeling. In 

Proceedings of the IEEE International Conference 

on Acoustics, Speech and Signal Processing, pages 

181–184. 

Katrin Kirchhoff, Jeff A. Bilmes and Kevin Duh. 

2007. Factored language models tutorial. Technical 

report, Department of Electrical Engineering, Uni-

versity of Washington, Seattle, Washington, USA.  

Philipp Koehn. 2004. Statistical significance tests for 

machine translation evaluation. In Proceedings of 

the 2004 Conference on Empirical Methods in 

Natural Language Processing (EMNLP), pages 

388–395, Barcelona, Spain. 

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 

Callison-Burch, Marcello Federico, Nicola Bertoldi, 

Brooke Cowan, Wade Shen, Christine Moran, 

Richard Zens, Chris Dyer and Ondřej Bojar. 2007. 

Moses: Open source toolkit for statistical machine 

translation. In Proceedings of the ACL 2007 Demo 

and Poster Sessions, pages 177–180. 

Philipp Koehn, Franz Josef Och and Daniel Marcu. 

2003. Statistical phrase-based translation. In Pro-

ceedings of the 2003 Conference of the North 

American Chapter of the Association for Computa-

tional Linguistics on Human Language Technology, 

pages 48–54. 

Philipp Koehn and Jean Senellart. 2010. Convergence 

of translation memory and statistical machine 

translation. In AMTA Workshop on MT Research 

and the Translation Industry, pages 21–31. 

Elina Lagoudaki. 2006. Translation memories survey 

2006: Users’ perceptions around tm use. In Pro-

ceedings of the ASLIB International Conference 

Translating and the Computer 28, pages 1–29. 

Qi Li, Biing-Hwang Juang, Qiru Zhou, and Chin-Hui 

Lee. 2000. Automatic verbal information verifica-

tion for user authentication. IEEE transactions on 

speech and audio processing, Vol. 8, No. 5, pages 

1063–6676. 

Vladimir Iosifovich Levenshtein. 1966. Binary codes 

capable of correcting deletions, insertions, and re-

versals. Soviet Physics Doklady, 10 (8). pages 707–
710. 

Yanjun Ma, Yifan He, Andy Way and Josef van 

Genabith. 2011. Consistent translation using dis-

criminative learning: a translation memory-inspired 

approach. In Proceedings of the 49th Annual Meet-

ing of the Association for Computational Linguis-

tics, pages 1239–1248, Portland, Oregon. 

Makoto Nagao, 1984. A framework of a mechanical 

translation between Japanese and English by anal-

ogy principle. In: Banerji, Alick Elithorn and  Ran-

an (ed). Artifiical and Human Intelligence: Edited 

Review Papers Presented at the International 

NATO Symposium on Artificial and Human Intelli-

gence. North-Holland, Amsterdam, 173–180. 

Franz Josef Och. 2003. Minimum error rate training 

in statistical machine translation. In Proceedings of 

the 41st Annual Meeting of the Association for 

Computational Linguistics, pages 160–167. 

Franz Josef Och and Hermann Ney. 2003. A system-

atic comparison of various statistical alignment 

models. Computational Linguistics, 29 (1). pages 

19–51. 

Kishore Papineni, Salim Roukos, Todd Ward and 

Wei-Jing Zhu. 2002. BLEU: a method for automat-

ic evaluation of machine translation. In Proceed-

ings of the 40th Annual Meeting of the Association 

for Computational Linguistics (ACL), pages 311–
318. 

Aaron B. Phillips, 2011. Cunei: open-source machine 

translation with relevance-based models of each 

translation instance. Machine Translation, 25 (2). 

pages 166-177. 

Richard Sikes. 2007, Fuzzy matching in theory and 

practice. Multilingual, 18(6):39–43. 

Michel Simard and Pierre Isabelle. 2009. Phrase-

based machine translation in a computer-assisted 

translation environment. In Proceedings of the 

Twelfth Machine Translation Summit (MT Summit 

XII), pages 120–127. 

James Smith and Stephen Clark. 2009. EBMT for 

SMT: a new EBMT-SMT hybrid. In Proceedings 

of the 3rd International Workshop on Example-

20



Based Machine Translation (EBMT'09), pages 3–
10, Dublin, Ireland. 

Matthew Snover, Bonnie Dorr, Richard Schwartz, 

Linnea Micciulla and John Makhoul. 2006. A 

study of translation edit rate with targeted human 

annotation. In Proceedings of Association for Ma-

chine Translation in the Americas (AMTA-2006), 

pages 223–231. 

Andreas Stolcke. 2002. SRILM-an extensible lan-

guage modeling toolkit. In Proceedings of the In-

ternational Conference on Spoken Language Pro-

cessing, pages 311–318. 

Guillaume Wisniewski, Alexandre Allauzen and 

François Yvon, 2010. Assessing phrase-based 

translation models with oracle decoding. In Pro-

ceedings of the 2010 Conference on Empirical 

Methods in Natural Language Processing, pages 

933–943. 

Ian H. Witten and Timothy C. Bell. 1991. The zero-

frequency problem: estimating the probabilities of 

novel events in adaptive test compression. IEEE 

Transactions on Information Theory, 37(4): 1085–
1094, July. 

Ventsislav Zhechev and Josef van Genabith. 2010. 

Seeding statistical machine translation with transla-

tion memory output through tree-based structural 

alignment. In Proceedings of the 4th Workshop on 

Syntax and Structure in Statistical Translation, 

pages 43–51. 

Andreas Zollmann, Ashish Venugopal, Franz Josef 

Och and Jay Ponte, 2008. A systematic comparison 

of phrase-based, hierarchical and syntax-

augmented statistical MT. In Proceedings of the 

22nd International Conference on Computational 

Linguistics (Coling 2008), pages 1145–1152. 

 

 

21


