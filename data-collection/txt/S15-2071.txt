



















































ezDI: A Supervised NLP System for Clinical Narrative Analysis


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 412–416,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

ezDI: A Supervised NLP System for Clinical Narrative Analysis

Parth Pathak, Pinal Patel, Vishal Panchal, Sagar Soni,
Kinjal Dani, Narayan Choudhary, Amrish Patel

ezDI, LLC.
{parth.p, pinal.p, vishal.p, sagar.s,
kinjal.d, narayan.c, amrish.p} @ezdi.us

Abstract

This paper describes the approach used by
ezDI at the SemEval 2015 Task-14: ”Anal-
ysis of Clinical Text”. The task was di-
vided into two embedded tasks. Task-1 re-
quired determining disorder boundaries (in-
cluding the discontiguous ones) from a given
set of clinical notes and normalizing the dis-
orders by assigning a unique CUI from the
UMLS/SNOMEDCT1. Task-2 was about find-
ing different type of modifiers for given disor-
der mention. Task-2 was divided further into
two subtasks. In subtask-2a, gold set of disor-
der was already provided and system needed
to just fill modifier types into the pre-specified
slots. Subtask 2b did not provide any gold set
of disorders and both the disorders and its re-
lated modifiers are to be identified by the sys-
tem itself. In Task-1 our system was ranked
first with F-score of 0.757 for strict evalua-
tion and 0.788 for relaxed evaluation. In both
Task-2a and 2b our system was placed second
with weighted F-score of 0.88 and 0.795 re-
spectively.

1 Introduction

Extracting medical information from clinical natural
text has gained a lot of attraction over the past few
years. Approximately 80% of patient related infor-
mation resides under unstructured transcribed text.
Amount of this unstructured text is increasing con-
stantly and automated methods of extracting crucial
information is of paramount interest to health care
informatics industry. Task-14 of SemEval 2015 on

1http://www.nlm.nih.gov/research/umls/

”analysis of clinical text” addresses the same con-
cern.

Task-14 of SemEval-2015 was in continuation of
the 2013 ShaRe/CLEF Task-1 (Suominen, H. et al.,
2013) and task-7 of SemEval 2014. The task was
divided into two parts. In continuation of last year,
task-1 was about finding disorder mentions from the
clinical text and associating them with their related
CUIs (concept unique identifiers) as given in the
UMLS (Unified Medical Language System). This
year one additional task (Task-2) of disorder modi-
fier slot filing was added. Task-2 was further sub-
divided into two parts. In subtask-2a, a gold set of
disorder mentions was provided and the participants
had to only fill the pre-specified slots with the nor-
malized modifiers. In task 2b, no gold set of disorder
mentions was provided. Figure1 provides detailed
overview about task 1 and 2.

Clinical NLP has evolved a lot in the tasks re-
lated to medical entity detection. NLP systems
like cTAKES (Savova, Guergana K., et al., 2010),
MetaMap (A. Aronson, 2001) and MedLEE (C.
Friedman et al., 1994) have focused on rule based
and dictionary look-up approaches for thid task. Re-
cently a few attempts have been made to use su-
pervised and semi-supervised learning models. In
2009, Yefang Wang (Wang et al., 2009) used cas-
cading classifiers on manually annotated data and
achieved around 83.2% accuracy. In 2010, i2b2
shared task challenge focused on finding test, treat-
ment and problem mentions from clinical document.
From 2013 on-words, entity detection task is regu-
larly featuring in Share/CLEF and SemEval tasks.

Tasks related to modifier slot filling are relatively

412



new and no extensive research has been done yet.
However for negation modifier, negEx (Chapman
et al., 2011) or various other variants of negEx have
been used in the last 10 years. These are keyword
based dictionary look-up algorithms, but still gives
around 92% of accuracy. However, these algorithms
are not scalable because there is no proper mecha-
nism defined to detect boundary for given negated
keyword. In 2010 i2b2 challenge, there was a sepa-
rate task for detecting 5 categories of negation. Sys-
tems used in this task showcase various statistical
approaches and the accuracy numbers were in the
range of 90 to 93%.

In this paper we have proposed a hybrid super-
vised learning approach based on CRF and SVM
to find out disorder mentions from clinical doc-
uments, a dictionary look-up approach on a cus-
tomized UMLS meta-thesaurus to find correspond-
ing CUIs and a SVM based generic approach to find
out all different disorder modifiers.

Figure 1: Task-2 with Examples.

2 Data Set

The SemEval-2015 corpus comprises of de-
identified plain text from MIMIC2 version 2.5
database. A disorder mention was defined as any
span of text which can be mapped to a concept in
UMLS and which belongs to the disorder semantic
group. Some other disorders which were not present
in the UMLS were marked as CUI-less. The train-
ing and development data sets of the previous year’s

2http://mimic.physionet.org/database/
releases/70-version-25.html

task were combined to be used as training set (298
documents) while the test data set of the previous
year was used as development set. There were 100
documents used as test data set. Same set of 4 hun-
dred thousand unlabelled documents were added to
encourage use of unsupervised learning methods.

3 Disorder Detection and Normalization

For Task-1 our system was very similar to the sys-
tem we developed last year (Pathak, et al, 2014).
Entity detection task was converted into sequence
labelling task using BIO format. A Conditional Ran-
dom Fields (CRF) was used to detect continuous en-
tity using CRF++3 toolkit. To detect discontiguous
entities, a binary SVM classifier was used to detect
whether relationship existed between two disorder
mentions or not. For contiguous entity detection
task, our feature set was very similar to the one we
used last year:

• Standard features like bag of words (for win-
dow +2 to -2), word stemmer (snowball stem-
mer) 4, prefix and suffix of length 1 to 5.

• Orthographic features like word contains digit,
contains slash, contains special character and
word shape (ezDI becomes aA).

• Grammatical features like parts of speech
(PoS) tags for which we used an internally de-
veloped PoS tagger (Choudhary et al. , 2014),
chunk (using Charniak’s parser (Charniak and
Johnson , 2005)) and head of noun and verb
phrases.

• Dictionary look-up matches for window +2 to
-2, stop words

• Section header and document type information
and sentence cluster id

Support Vector Machine (LibSVM5) was used to
identify disjoint entities. For all the possible combi-
nation of entities within a sentence, we ran a binary
SVM classifier to find whether a relationship existed
between those two entities or not. Feature set con-
sisted of following features:

3http://crfpp.googlecode.com/
4http://snowball.tartarus.org/
5http://www.csie.ntu.edu.tw/\˜cjlin/

libsvm/

413



• Bag of words, PoS tags and chunk labels for all
the tokens appearing in between two entities.

• Few simple rules were implemented on Char-
niak parse output to find relationship between
two entities. A binary feature was used stating
whether relationship was found using rules or
not.

• Position of preposition, conjunction, main verb
and special characters like colon (:), hyphen (-
) and semi colon (;) in the context of the first
entity.

• Binary feature stating whether any of the de-
tected entity contained head of a noun phrase.

This hybrid approach was very helpful in detect-
ing disjoint entities. We got around 70% accuracy in
detecting disjoint entities using this approach.

3.1 CUI Detection

CUI detection task was divided into three separate
steps:

1) Direct dictionary search: In the first step, for
each word found in an entity we found all of its lexi-
cal variants using LVG 6. After that, for all the possi-
ble permutations we tried searching the string in the
UMLS. If the string matched any UMLS entry, we
associated the corresponding CUI with that entity.

2) Dictionary search on modified entities: For a
better mapping of the entities detected by NLP in-
side the given input text, we found it to be a bet-
ter approach to divide the UMLS entities into vari-
ous phrases. This was done semi-automatically by
splitting the strings based on function words such
as prepositions, particles and non-nominal word
classes such as verbs, adjectives and adverbs. While
most of the disorder entities in UMLS can be con-
tained into a single noun phrase (NP) there are also
quite a few that contain multiple NPs related with
prepositional phrases (PPs), verb phrases (VPs) and
adjectival phrases (ADJPs).

This exercise gave us a modified version of the
UMLS disorder entities along with their CUIs. Table
4 gives a snapshot of what this customized UMLS
dictionary looked like.

6http://lexsrv2.nlm.nih.gov/

CUI Text P1 P2 P3

C001
3132

Dribbling
from
mouth

Dribbling from mouth

C001
4591

Bleeding
from nose

Bleeding from nose

C002
9163

Hemorr-
hage from
mouth

Hemo-
rrhage

from mouth

C039
2685

Chest pain
at rest

Chest pain at rest

C026
9678

Fatigue
during
pregnancy

Fatigue during
pregn
ancy

Table 1: An example of the modified UMLS disorder en-
tities split as per their linguistic phrase types.

3) String similarity algorithm: If an entity was not
found even after the first two steps, then we gener-
ated a list of possible text span from UMLS which
can possibly match with the given entity. After
that, Levenshtein edit distance algorithm was used
to find best string match. If the best string match
was greater than a certain threshold value, the corre-
sponding CUI was associated with the entity other-
wise the entity was marked as ”CUI-less”.

4 Modifier Detection:

For this task we tried to develop a generic approach
so that it can be applied to any type of modifier. We
divided the task of modifier detection into two parts:
1) Modifier keywords detection 2) Relating detected
keywords with entity.

1) Modifier keywords detection: For each modi-
fier type, an extensive dictionary was prepared hav-
ing different possible keywords with its normalized
values. A simple dictionary look-up algorithm was
used to calculate a baseline accuracy. On train-
ing data set, accuracy ranged from 60% to 85% for
different modifier types. This baseline algorithm
achieved great recall but much less precision. To
counter this, we used CRF algorithm with common
features like bag of words, stem value and other or-
thographic features. CRF helped significantly in im-
proving precision for modifier keyword detection.

2) Relating detected modifier with entities: We

414



treated this task similar to the task of finding re-
lationship between two entities. So a binary clas-
sifier was used to check if relationship existed be-
tween a modifier keyword and an entity or not. Fea-
ture set consisted of: Bag of Words between entity
and modifier keyword, PoS tags, a binary flag stat-
ing whether the modifier keyword and the entity ap-
peared in the same chunk or not, relative position
of entity and modifier, special characters appearing
in the sentence, section header (for subject modifier
type).

5 System Accuracy

For Task-1, the accuracy was defined as the number
of pre-annotated spans with correctly generated
code divided by the total number of pre-annotated
spans.

Strict Accuracy = #of CUIs with Exact span matchTotal annotation in gold standard

Relaxed Accuracy = #of CUIswithpartialspanmatchTotal annotation in gold standard
Both training and development data sets were

used for training purpose. We used only 1 run with
above mentioned system set up. We were ranked
first for this task with results shown in Table 3.

Precision Recall Accuracy
Strict 0.783 0.732 0.757
Relaxed 0.815 0.761 0.787

Table 2: Task-1 Results.

For Task 2, weighted and unweighted accura-
cies were calculated. The unweighted accuracy is
the average of the per-disorder unweighted accuracy
over all the disorders in the test set. Each gold-
standard slot value is pre-assigned a weight based
on its prevalence in the training set. The weighted
accuracy is the average of the per-disorder weighted
accuracy over all the disorders in the test set.

Ranks for task-2 were given based on weighted
accuracy. ezDI was ranked second in both Task-2a
and Task-2b. The results were as given below:

6 Error Analysis

Abbreviations and disjoint entities still cause a lot
of error in CUI normalization task. Dictionary re-

F A F*A WA F*WA
Task-2A 1 0.934 0.934 0.880 0.880
Task-2B 0.915 0.935 0.856 0.868 0.795

Table 3: Task-2 Results.

lated features are still not very helpful. Accuracy de-
creases significantly if medical domain is changed.
Probably more sophisticated approach will be re-
quired to fully utilize UMLS dictionary. There is
still a lot to explore in modifier detection. Statisti-
cal approaches are still not out-performing baseline
dictionary based approaches. Modifier boundary de-
tection is still a bigger challenge to be solved.

7 Conclusion

In this paper we have proposed a CRF and SVM
based hybrid approach to find disorder mentions
from a given clinical text, a novel dictionary look-up
approach for discovering CUIs from UMLS meta-
thesaurus and a generic statistical approach for mod-
ifier slot filling. Our system did produce competitive
results and was best among all the participants for
task 1. In future, we would like to explore semi-
supervised learning approaches to take advantage of
large amount of available un-annotated free clinical
text.

References

Aronson, Alan R. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program.

Chapman W, Bridewell W, Hanbury P, Cooper G,
Buchanan B. 2001. A simple algorithm for identifying
negated findings and diseases in discharge summaries.

Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-best Parsing and MaxEnt Discriminative
Reranking.

Choudhary, Narayan, Parth Pathak, Pinal Patel, Vishal
Panchal. 2014. Annotating a Large Representative
Corpus of Clinical Notes for Parts of Speech.

Friedman C, Alderson PO, Austin JH, Cimino JJ, John-
son SB. 1994. A general natural-language text pro-
cessor for clinical radiology.

Pathak, Parth, Pinal Patel, Vishal Panchal, Narayan
Choudhary, Amrish Patel, Gautam Joshi. 2014. ezDI:
A Hybrid CRF and SVM based Model for Detecting
and Encoding Disorder Mentions in Clinical Notes.

415



Savova, Guergana K., James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction System
(cTAKES): architecture, component evaluation and
applications.

Suominen, Hanna, Sanna Salanter, Sumithra Velupillai,
Wendy W. Chapman, Guergana Savova, Noemie El-
hadad, Sameer Pradhan. 2013. Overview of the
ShARe/CLEF eHealth evaluation lab 2013.

Wang, Yefeng and Jon Patrick. 2009. Cascading classi-
fiers for named entity recognition in clinical notes.

416


