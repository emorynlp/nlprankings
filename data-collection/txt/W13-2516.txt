










































Learning Comparable Corpora from Latent Semantic Analysis Simplified Document Space


Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 129–137,
Sofia, Bulgaria, August 8, 2013. c©2013 Association for Computational Linguistics

 

 

Learning Comparable Corpora from Latent Semantic Analysis  

Simplified Document Space 

 

 

Ekaterina Stambolieva 

euroscript Luxembourg S.à. r.l. 

55, rue de Luxembourg, L-8077 

Luxembourg 

ekaterina.stambolieva@euroscript.lu 

 

  

 

Abstract 

Focusing on a systematic Latent Semantic 

Analysis (LSA) and Machine Learning (ML) 

approach, this research contributes to the de-

velopment of a methodology for the automatic 

compilation of comparable collections of doc-

uments. Its originality lies within the delinea-

tion of relevant comparability characteristics 

of similar documents in line with an estab-

lished definition of comparable corpora. These 

innovative characteristics are used to build a 

LSA vector-based representation of the texts. 

In accordance with this new reduced in dimen-

sionality document space, an unsupervised 

machine learning algorithm gathers similar 

texts into comparable clusters. On a monolin-

gual collection of less than 100 documents, the 

proposed approach assigns comparable docu-

ments to different comparable corpora with 

high confidence. 

 

1 Introduction 

The problem of collecting comparable corpora is 

challenging and yet enchanting. Many can bene-

fit from the availability of such corpora as trans-

lation professionals, machine learning research-

ers and computational linguistics specialists. Yet 

there is not an even consent about the notion 

covered by the term comparable corpora. The 

degree of similarity between comparable corpora 

documents has not been formalized strictly and 

leaves space for different interpretations of simi-

larity, contributing to abundant text collections 

of similar and semi-similar documents. The cur-

rent research endeavors to contribute to an ap-

proach, which assembles a collection of compa-

rable documents that are closely related to each 

other on the basis of a strict definition of compa-

rable corpora. The proposed approach incorpo-

rates originally a Latent Semantic Analysis tech-

nique in order to match similar concepts instead 

of words thus contributing to better automatic 

learning of comparability between documents.  

2 Comparable Corpora Definition 

Maia (2003) discusses the characteristics of 

comparable corpora. Nevertheless, the adopted 

definition of comparable corpora in this study is 

given by McEnery (2003): 

“Comparable corpora are corpora where series 

of monolingual corpora are collected for a range 

of languages, preferably using the same sampling 

and frame and with similar balance and repre-

sentativeness, to enable the study of those lan-

guages in contrast.” 

 

McEnery (2003) characterizes comparable 

corpora as “corpora where series of monolingual 

corpora are collected for the range of languages”. 

In the views of McEnery (2003), a monolingual 

corpus is a corpus that is not collected for a range 

of languages, but instead the documents selected 

are written in one language. In the context of the 

current research, a comparable corpus, a sub-

language corpus, can be constructed from docu-

ments in one language under the condition they 

are compliant with the preferred guidelines pro-

vided by McEnery (2003). These preferred 

guidelines are similar sampling frame, balance 

and representativeness. 

A document feature corresponding to text 

sampling is explicated taking into consideration 

the domain and genre of the documents. Addi-

129



 

 

tionally, similar terminology vocabulary insures 

genre correspondence. Therefore, the same sam-

pling scheme in collecting documents is evaluat-

ed considering domain and genre and viewed as 

document features.  

Language is rapidly changing and evolving 

throughout the years (Crystal 2001). As a result, 

restricting the time period a document has been 

published increases the chances of it being com-

parable to another one written during the same 

time frame. When events are reported in the 

newspaper domain, their date of publication is 

strong similarity evidence and is used as a filter 

between weakly comparable and non-comparable 

text articles (Skadiņa et al. 2010a). 

The question of how representativeness of a 

corpus is decided upon is answered in different 

ways depending on the specific corpus purpose. 

For the purposes of this research, a corpus is 

considered representative when corresponding 

texts are similar in size. As reported by Manning 

and Schűtze (1999), a balanced corpus is one, 

which is assembled “as to give each subtype of 

text a share of the corpus that is proportional to 

some predetermined criterion of importance”. 

Skadina et al. (2010b) present a good summary 

of the advantages of exploiting comparable cor-

pora. It is discussed that “they can draw on much 

richer, more available and more diverse sources 

which are produced every day (e.g. multilingual 

news feeds) and are available on the Web in 

large quantities for many languages and do-

mains.” (Skadina et al. 2010b). 

3 Related Work 

The most closely-related to machine learning 

work that mines comparable corpora is that by 

Sharoff (2010). His research incorporates intelli-

gent self-learning techniques to the compilation 

of comparable documents. Unlike other re-

searchers that experiment with Cross-Lingual 

Information Retrieval (CLIR) techniques as in 

Tao and Zhai (2005), Sharoff (2010) estimates 

the document collection’s internal subgroup sys-

tem in search for structure. The possible structure 

and grouping of a set of documents is most easily 

defined by ranked words that are representative 

for the subsets in the collection. Sharoff's ap-

proach relies heavily on keywords and keyword 

estimation. One thing Sharoff (2010) does not 

elaborate on in details is the definition of a com-

parable corpus. A possible reason for that is that 

unsupervised machine learning approaches pro-

duce related sets of documents in an environment 

where the selection process is automated and not 

supervised by any linguistically-dependent rules. 

What is written by Goeuriot et al. (2009) is al-

so an influential and relevant material to the cur-

rent research. Their paper is on the compilation 

of comparable corpora in a specialized domain 

with a focus on English and Japanese. The article 

is significant for the reason the authors investi-

gate ways of building comparable corpora using 

machine learning classification algorithms, 

namely Support Vector Machine and C4.5. The 

experimental setup in the work of Goeuriot et al. 

(2009) relies on manually labeled data, which is 

then fed to the machine learning algorithm core. 

The paper by Goeuriot et al. (2009) is directed 

towards building a tool to automatically compile 

comparable corpora in a predefined set of docu-

ments and languages. The text comparability 

characteristics extracted, which allow compari-

son between the documents, are external and in-

ternal to the textual data. Goeuriot et al. (2009) 

emphasize on selecting ways to automatic recog-

nition of useful features similar texts have and 

experiment with these features to test and predict 

their reliability. The comparability of the docu-

ments defined by them is on three levels - type of 

discourse, topic and domain, focusing on locu-

tive, ellocutive and allocutive act labels. 

Bekavac et al. (2004) discuss the grounds of a 

methodology describing similarity comparison of 

under-resourced monolingual corpora. Contrary 

to other methodologies that exploit seed words or 

seed texts as a basis for search, the researchers 

have at their disposal two monolingual docu-

ments sets from which they aim to mine compa-

rable documents. The advantage of their ap-

proach is that it is applicable to texts collection 

written in one language for the reason that they 

are easily mined and compiled from the available 

textual resources nowadays. The concept behind 

their research is to align comparable documents 

that are found in pre-collected different monolin-

gual corpora. Content features are used to test the 

degree to which two texts are similar to each 

other in the sense of sharing the same infor-

mation and common words. These features, 

composition features, need to be representative 

for the texts. The composition features, extracted 

from the data, monitor the size, the format and 

the time span of the documents. 

Clustering based on semantic keyword extrac-

tion is performed by Finkelstein et al. (2001). 

This approach is relevant to the current research 

as it suggests a different methodology of feeding 

texts to machine learning algorithms. The re-

130



 

 

searchers aim to generate new content based on 

input user queries by using context – “a body of 

words surrounding a user-selected phrase” 

(Finkelstein et al. 2001). They emphasise on the 

significance of using context when developing 

Natural Language Processing (NLP) applica-

tions. The keyword extraction algorithm present-

ed relies on a precisely-designed clustering algo-

rithm, different than k-means, to recursively 

clean clustering results and present refined statis-

tical output. 

With regards to evaluation metrics of compa-

rable corpora, one of the main focuses of the 

ACCURAT Project (Skadina et al. 2010b) is to 

design metrics of comparability estimation be-

tween texts. The ACCURAT researchers (Skadi-

na et al. 2010b) concentrate on the development 

of comparable corpora criteria for different texts 

and different types of parallelism between the 

texts. Saralegi et al. (2008) suggest measures 

based on distribution of topics or time with re-

gards to publication dates. Kilgariff (2001) aims 

to measure the level of comparability between 

two collections of documents. He focuses addi-

tionally on the shortcoming of known corpus 

similarity metrics. He discusses evaluation meth-

ods for corpus comparability measures, which 

are based on Spearman rank correlation co-

efficient, perplexity and cross-entropy, χ
2
 and 

others. To his knowledge, the χ
2 
test performs the 

best when comparing two sets of documents. It is 

important to note that the approach adopted by 

Kilgariff (2001) relies on words and n-gram se-

quence features. Not only does he regard the 

texts as bag-of-words, but also he incorporates n-

gram characteristics in his evaluation metric 

analysis.  

Mining word similarity techniques are dis-

cussed in the work of Deerwester et al. (1990); 

Baeza-Yates and Ribeiro-Netto (1999); and Da-

gan, Lee and Pereira (1999). Deerwester et al. 

(1990) incorporate LSA as a technique to identi-

fy word relatedness. LSA “identifies a number of 

most prominent dimensions in the data, which 

are assumed to correspond to ‘latent concepts’.” 

(Radinsky et al. 2011). Radinsky et al. (2011) 

indicate that LSA vector space models are “diffi-

cult to interpret”. Consequently, the current re-

search focuses not only on the incorporation of 

LSA to mapping content, but also of the em-

ployment of a machine learning technique to 

group projected into the two-dimensional space 

documents into similar clusters. Baeza-Yates and 

Ribeiro-Netto (1999), as Sharoff (2009) and 

Goeuriot et al. (2010), consider texts as bag-of-

words as the least complex word similarity ap-

proaches can be incorporated. Mapping distri-

butional similarity, Lee (1999) opts for similar 

word co-occurrence probability estimation im-

provement. Dagan et al. (1999) also aim for 

better estimation of word co-occurrence likeli-

hood not based on empirical methods, but in-

stead relying on distributional similarity for the 

generation of language models. WordNet-

based and distributional-similarity compari-

sons of word similarity are presented in Agirre 

et al. (2009). They suggest different views of 

word relatedness comparison – bag-of-words, 

context windows and syntactic dependency 

approaches. They describe their findings as 

yielding best results on known test sets. What 

is important to be remarked is that their meth-

odology requires minor fine-tuning in order to 

give good results on cross-lingual word simi-

larity. 

4 Methodology 

The novelty of our approach is the incorpora-

tion of the Latent Semantic Analysis tech-

nique, which matches concepts, or information 

units, from one document to another instead of 

approximating word similarity. LSA expects 

and constructs a new vector-based representa-

tion of the documents to be compared. A con-

cept holds not only textual, but also morpho-

logical information about each word present in 

the texts. By employing LSA, the document 

space is projected into the two-dimensional 

space in correspondence with the latent rela-

tionships between the words in the texts. In the 

two-dimensional space, clusters of similar 

documents are compiled together using a sim-

ple, but powerful unsupervised machine learn-

ing algorithm, k-means clustering. Clustering 

evaluation metrics such as precision, recall and 

purity are employed towards automatic evalua-

tion and analysis of the resulting comparable 

corpora. 

In order to compile comparable corpora with 

the current settings, a set of pre-collected doc-

uments is needed. From this set of documents, 

two to five comparable corpora are identified 

and texts with similar topics, domains and fea-

tures are assigned to relevant comparable cor-

pora. 

LSA has its known limitations. It acknowl-

edges documents as bags-of-words and mines 

131



 

 

the latent relationships between the words in 

the bags-of-words. Working with information 

units overcomes this limitation of LSA. The 

information units contain additional linguistic 

information about the syntactic and morpho-

logical relationships between words, therefore 

forming concepts of these words. The order of 

the words, or the information units, is not im-

perative, therefore it is not controlled by the 

methodology. 

LSA allows words to have only one mean-

ing thus restricting the robustness of the natu-

ral languages. This limitation is tackled by 

suggesting different word sense candidates for 

words and constructing a separate information 

unit for each promoted word sense.  

5 Data Feature Selection 

The innovation of the discussed research ap-

proach lays in its basic concept of perceiving 

texts as bags of interrelated concepts. The sur-

face-form words found in the texts are en-

riched with linguistic information that furnish-

es better matching procedure of the concepts 

lying within the texts for comparison. 

Unlike previous work, which regards docu-

ments as bags-of-words (Sharoff 2009, 

Goeuriot et al. 2010) the methodology treats 

documents as collections of concepts, each 

concept containing comparable textual infor-

mation. The concepts are represented by in-

formation units. The process of recognizing 

such units happens at document level, where 

each document is viewed as a separate text 

with its own context. Each information unit is 

defined as the inseparable pair of lemma and 

its context-dependent part-of-speech (POS) 

tag. A lemmatization technique is applied to 

transform the texts into linguistically-

simplified versions of the originals, where each 

word (infected or not) is substituted by its cor-

responding lexeme. 

As stated before, the information units in-

corporate POS output. A POS tagger is used to 

process the texts before linguistically-

simplifying it using lemmatization techniques. 
The idea of enriching the words by POS infor-

mation is not new to the research of Natural 

Language Processing, but it is new for the re-

search of compiling comparable corpora. By 

identifying the POS information of a sentence, 

lexical ambiguity is reduced. The accompany-

ing POS tag to each lemma assists the disam-

biguation of the information units. For exam-

ple, run as being the action of walking fast has 

a verb POS tag opposed to run as the period of 

some event happening has a noun POS tag. In 

this example, the POS tag provides the needed 

information for disambiguating the two differ-

ent meanings of a word. In the current research 

scenario, the POS tagging module 1  emulates 

the results of a basic Word Sense Disambigua-

tion technique. 

Furthermore, the input set of documents is 

transformed into a set of lists of information 

units as described, where a single list of units 

corresponds to a single document. When com-

pared, the units are matched for correspond-

ence both based on the lemma's lexical catego-

ry in the sentence and its base form. 
Another feature, which helps build context re-

lated concepts, is the identification of Noun 

Phrases (NP) in the texts. Noun Phrase recogni-

tion is imperative since it further develops the 

simple word sense disambiguation method. Some 

words to have a different meaning when occur-

ring in a chain of words such as a noun phrase. 

Unlike the proposed by Su and Babych (2012) 
approach to NP recognition, NPs are identified 

following linguistically-derived rules, which rep-

resent common constructions of the language 

under consideration. When a NP is identified, it 

is listed as a new information unit with a corre-

sponding NP POS tag. All POS annotations as 

well as lemma information of its constituent 

words are removed from the documents' list of 

information units.  

6 Experiments  

6.1 Experimental Corpus 

A pre-collected corpus of documents, part of the 

NPs for Events (NP4E) corpus (Hasler et al. 

2006), is used for experimenting. The NP4E cor-

pus is collected for the special purpose of ex-

tracting coreference resolution in English. Never-

theless, the structure and the organization of the 

corpus are suitable for the needs of acquisition of 

a test corpus for the current study. The NP4E 

corpus contains five different groups of news 

articles based on topic gathered from the Reuters. 

The news articles are collected in the time frame 

                                                 
1
 TreeTagger http://www.ims.uni-

stuttgart.de/projekte/corplex/TreeTagger/ 

 

132



 

 

of two years – 1996 and 1997 (Rose, Stevenson 

and Whitehead 2002). Four of the five NP4E 

news article groups are used to compile an exper-

imental corpus containing roughly 40000 words 

or 520 words per text. The chosen experimental 

collection consists of sub-corpora that 

have documents comparable to the others in their 

sub-corpora based on domain. The domain of 

these comparable corpora is terrorism, and the 

four distinct topics are connected with terrorism, 

bombing and suicide respectively for events in 

Israel, Tajikistan, China and Peru. In total, the 

experimental corpus consists of 77 newswire ar-

ticles. The distribution of the documents in this 

selected corpus is 20 on Israel topic, 19 on Tajik-

istan topic, 19 for China topic and 19 on Peru 

topic. These sub-corpora are referred to as Israel 

(I), Tajikistan (T), China (C) and Peru (P) on-

wards.  

6.2 Experimental Set-up 

The experimental set-up is structured as a chain 

of two simple procedures. They are respectively 

an experimental setup data selection 

and experimental setup clustering distribution. 

6.2.1 Data Selection Frame 

The data selection frame describes how docu-

ment features are selected. The documents are 

afterwards preprocessed in order to extract all 

underlying text features and binary vectors are 

constructed to represent each separate document. 

The document features on focus consist of all 

identified information units enriched with the 

noun phrases that were recognized in the texts. 

The binary vectors then are used as an input to 

the LSA algorithm. 

6.2.2 Cluster Distribution 

The number of resulting clusters, or comparable 

corpora, should be set in advance for unsuper-

vised machine learning algorithms. An experi-

ments with k, k is in the range of 2 to 5, are con-

ducted. Testing with number of clusters greater 

or equal to two comes logical. In the case of ex-

pecting two resulting clusters, the methodology 

groups all similar documents in one comparable 

corpus, and withdraws the non-similar docu-

ments to the second collection. When k is chosen 

to be 2 or 3, the resulting comparable corpora 

tend to be weakly-comparable (Skadiņa et al. 

2010a) for the reason the algorithms are forced to 

gather documents with four distinct topics into 

only two or three comparable collections. It is 

interesting to analyze the research methodolo-

gy’s performance in the case four output compa-

rable corpora are expected, meaning when the 

learning algorithm is asked to suggest four com-

parable sets of documents.  

To evaluate clustering performance in terms of 

forcing the system to split the document collec-

tion into more comparable corpora than present, 

k equals to 5 is also used in the experiments. 

Consequently, the number of clusters varies be-

tween 2 and 5.  

6.2.3 Evaluation Metrics 

Three metrics are chosen to evaluate results - the 

standard precision and recall, and additionally - 

purity. Precision shows how many documents in 

the resulting collections are identified correctly 

as comparable to the majority of documents on a 

specific topic in the cluster. For example, when 

16 out of 19 documents are recognized to be 

comparable to each other, the precision of this 

clustering result is 0.84. Recall shows how many 

false negatives are identified as comparable to a 

certain topic-related collection of texts. The false 

negatives are the documents on a different topic, 

which the machine learning algorithm falsely 

lists to be comparable to documents on another 

topic. When 21 documents are grouped in one 

similarity cluster, 19 of them being on a related 

topic, 3 of them being on another topic, the recall 

of the learning performance is 0.86. 

Purity is an evaluation metric used to estimate 

the purity of the resulting clusters (Figure 1.). A 

cluster is recognized as pure when it contains a 

number of documents with the same label (mean-

ing they are listed to be comparable to each other 

by a human evaluator) and as less as possible 

documents that have a different label from the 

dominant label (Manning et al. 2008): 

 
Figure 1. Purity score formula 

 

where nomcluster i is the number of the majority 

class members in each resulting cluster i, and 

noclustrers is the number of resulting clusters, or k. 

As Manning et al. (2008) warn “High purity is 

easy to achieve when the number of clusters is 

large - in particular, purity is 1 if each document 

gets its own cluster”. The number of clusters for 

the current research is not big. Nonetheless, the 

results are evaluated based on two other metrics. 

133



 

 

The other metrics for measuring the 

comparability between documents that are 

chosen for exploitation in the current research, 

are Mutual Infromation (MI) and Normalized 

Mutual Infroamtion (NMI). The formula for 

NMI is as follows and shown in Figure 2.:  

 

 
Figure 2. NMI score formula 

 

MI is explained in details in Kalgariff (2001) 

and (Manning et al. 2008). Manning et al. (2008) 

discuss additionally the formula for the entropy 

H, and NMI. Ω is the group of clusters addressed 

in the experiments, and C is the group of labels – 

namely the different characteristics of the com-

parable corpora.  

In the current scenario, no human evaluation is 

performed. Rather than that the corpus is pre-

designed in a way to contain four different com-

parable corpora that need not to be manually la-

beled 

6.3 Evaluation 

Results are obtained after conducting different 

set-up experiments. One set-up focuses on evalu-

ating comparable corpus collection having as an 

input part of the experimental corpus. This part 

contains documents on two out of the four differ-

ent topics. The two-topic collections are com-

piled by combining all combinations possible of 

two topic-based sets together from the four dis-

tinct topic sub-corpora. In this experimental sce-

nario, the total of different corpora for evaluation 

is 6 (according to the combination’s formula ) 

- Peru and China, Peru and Tajikistan, Peru and 

Israel, Tajikistan and China, Tajikistan and Isra-

el, China and Israel. Table 1 shows the results of 

running LSA with k-means clustering on the dis- 

 

 

cussed sub-groups. As seen on Table 1. the learn-

ing algorithm performance is excellent when the 

number of comparable corpora that are expected 

is greater than two. When three or more compa-

rable clusters are elected, each similar by topic 

document is grouped with all other documents 

that are comparable to it in the same resulting 

comparable corpus. In the case of expecting three 

comparable corpora with Precision and Recall 

equal to 1.0, one of these corpora contains all 

documents of two different sub-corpora and the 

rest contain all documents of one of the pre-

defined experimental sub-corpora. In the case of 

expecting five comparable corpora with Preci-

sion and Recall equal to 1.0, one sub-corpus is 

split into two comparable clusters, these clusters 

containing documents on the same topic. What is 

interesting in this experimental set-up are the 

results the learning algorithm obtains when it 

aims to produce only two comparable clusters. 

For three of the test sets - China and Israel, Peru 

and China and Tajikistan and Israel, grouping of 

documents on different topics into the same simi-

lar collection is seen. The lowest results obtained 

are for the test set Tajikistan and Israel, where 3 

of the 19 documents on an Israel topic are 

grouped together with the texts on the Tajikistan 

topic. The reason behind this automatic learning 

confusion originates from the fact the Tajikistan 

and Israel topic documents contain many similar 

concepts, which make good clustering harder to 

achieve.  

The purity of the resulting corpora is very 

high, above 0.9, indicating that comparable doc-

uments are identified correctly with high rele-

vance. The only exception is the results on the 

Tajikistan and Israel test set with purity 0.56. 

This exception occurs because of poor clustering 

results, which have been discussed.  

 

 

 

Sub-

corpus 

Topic Precision    Recall    Purity 

  2Cl 3Cl 4Cl 5Cl 2Cl 3Cl 4Cl 5Cl  

P Peru 0.84 1 1 1 1 1 1 1 0.921 

C China 1 1 1 1 0.86 1 1 1  

P Peru 0.84 1 1 1 1 1 1 1 0.921 

T Tajikistan 1 1 1 1 0.86 1 1 1  

P Peru 1 1 1 1 1 1 1 1 1.00 

I Israel 1 1 1 1 1 1 1 1  

T Tajikistan 1 1 1 1 1 1 1 1 1.00 

C China 1 1 1 1 1 1 1 1  

T Tajikistan 1 1 1 1 0.52 1 1 1 0.56 

I Israel 0.15 1 1 1 1 1 1 1  

C China 0.86 1 1 1 1 1 1 1 0.923 

I Israel 1 1 1 1 0.85 1 1 1  

Table 1. Clustering results for test sets of combinations of two topic sub-corpora 

(nCl pointing to the numbers of clusters identified ) 
134



 

 

Another set-up focuses on the analysis and 

evaluation of the results on clusters containing 

documents on three of the four different topics. 

The same way as the two-topic collections are 

constructed, combining three topic sub-corpora 

into one results in the development of the input 

for the LSA and k-means clustering algorithms.  

In this experimental scenario, a total of 4 distinct 

input collections are compiled -Tajikistan, Israel 

and China; Tajikistan, Israel and Peru; Peru, 

China and Israel; and Tajikistan, China and Peru.   

The results of the learning comparable corpora 

from them are listed in Table 2. As it can be easi-

ly seen, the clustering performance is impecca-

ble. Therefore, providing more documents, more 

data features, helps identifying better similar 

documents applying the proposed research ap-

proach. 

 

 

 

 
 Precision Recall Purity 

 2cl 3cl 4cL 5cl 2cl 3cl 4cl 5cl  

T 1 1 1 1 1 1 1 1  

C 1 1 1 1 1 1 1 1 1.00 

I 1 1 1 1 1 1 1 1  

P 1 1 1 1 1 1 1 1  

Table 3.  Clustering results on the whole experi-
mental corpus 

 
 Mutual 

Information 

H(Ω) H(C) NMI 

2CL 2CL 2CL 2Cl 

Peru 

China 

0.6866 0.9927 1 0.6916 

Peru 
Tajikistan 

0.6866 0.9927 1 0.6916 

Peru 

Israel 

1.0230 1.0074 1.0074 0.9522 

Tajikistan 

China 

1 1 1 1 

Tajikistan 
Israel 

0.0844 0.3912 1.0074 0.1262 

China 

Israel 

0.6855 0.9744 1.0074 0.6917 

Table 4.  MI and NMI scores results for test sets of 
combinations of two topic sub-corpora 

Table 3. Shows the clustering results when all 

texts of the experimental corpus are suggested as 

an input. The algorithms once more do not have 

problems collecting the similar documents into 

comparable corpora with high precision and re-

call. 

MI and NMI are computed only for the results 

presented in Table 1. The reasoning behind is 

that Table 2. And Table 3. show perfect cluster-

ing results of comparable corpora obtained on 

the whole set of input documents described in 

Section 6.1.   

The results of the comparable texts grouping 

are estimated using a clustering quality trade-off 

metric, NMI. Table 4. shows the NMI results of 

the clustering performance on the two-topic col-

lections described in the first experimental set-up 

at the beginning of  Section 6.3.  

 

 

 

Consequently, the results shown on Table 4. 

are obtained with respects to the precision, recall 

and purity scores presented in Table 1. The NMI 

score is evidence of the identified comparable 

corpora quality. As seen on Table 4., the lowest 

NMI score correspond to the clustering results on 

the Peru- and China- topic texts. As shown on 

Table 1., the proposed approach is not confident 

when grouping the Peru- and China- topic texts  

into comparable collections. The results of the 

NMI metric shown on Table 4. only confirm this 

conclusion. The best results obtained according 

to the NMI score are NMI is dependent on the 

mutual information and the entropy the texts to 

be clustered share. MI is a metric, which esti-

mates how the amount of information presented 

in the documents affect the clustering output. 

When the MI score is low, as in the example of 

grouping the Tajikistan- and Israel- topic texts, 

the information contained in the documents does 

not contribute to highly-comparable clusters of 

corpora. When the MI score obtained is high, as 

Sub-

corpus 

Topic Precision    Recall    Purity 

  2Cl 3Cl 4CL 5Cl 2Cl 3Cl 4Cl 5Cl  

T Tajikistan 1 1 1 1 1 1 1 1  

I Israel 1 1 1 1 1 1 1 1 1.00 

C China 1 1 1 1 1 1 1 1  

T Tajikistan 1 1 1 1 1 1 1 1  

I Israel 1 1 1 1 1 1 1 1 1.00 

P Peru 1 1 1 1 1 1 1 1  

P Peru 1 1 1 1 1 1 1 1  

C China 1 1 1 1 1 1 1 1 1.00 

I Israel 1 1 1 1 1 1 1 1  

T Tajikistan 1 1 1 1 1 1 1 1  

C China 1 1 1 1 1 1 1 1 1.00 

P Peru 1 1 1 1 1 1 1 1  

Table 2. Clustering results for test sets of combinations of three topic sub-corpora 

 

135



 

 

in the Tajikistan- and China- topic documents 

experiment, the information in these documents 

is a strong evidence of the text relatedness. Table 

4. lists the intermediate calculations of the entro-

py based on the available labels H(C) and the 

resulting clusters H(Ω). 

7 Remarks 

The problems identified in the current methodol-

ogy are classified into two different groups: text 

processing resources errors and clustering output 

errors. The processing resources are taken as off-

the-shelf modules and the development focus of 

the study in not concentrating on improving their 

performance. The second type of errors is the 

clustering errors. Their size can be reduced by 

improving the performance of the text prepro-

cessing resources. Additionally, enhanced clus-

tering output evaluation metrics can reveal learn-

ing algorithm’s weaknesses and suggest ways for 

improvement. 

8 Future Work 

More can be done in the future to improve the 

proposed methodology. One idea for further in-

vestigation is experimenting with larger collec-

tions of data. The results on the experimental 

corpus are promising, but the document collec-

tion is not big and contains less than 80 texts. It 

would be interesting to experiment with corpora 

that consist of hundreds of documents to test 

clustering performance. Additionally, a new ex-

perimental collection of documents is being 

compiled. It contains psycholinguistics texts both 

in Spanish and English. As the collection of this 

document set is still in progress, the results ob-

tained on it are not presented in the current pa-

per. These results will be reported in future work 

publications.  

Furthermore, a new translation equivalent 

source can be added. In the case of compiling 

specialized collections of comparable docu-

ments, a specialized bilingual or multilingual 

dictionary can prove to be a valuable resource. 

An untested interesting experimental setup can 

be investigating the resulting clustering perfor-

mance when more than 50% or more of the most 

relevant lemmas (with noun phrases) are selected 

as document features. A Named Entity Recog-

nizer (NER) and a synonymy suggestion module 

have the possibility to serve as good text pro-

cessing resources and further improve grouping 

outcomes. In connection with NER, it is interest-

ing additionally to investigate if the test corpus 

contains local names, which make clustering bet-

ter easier. Lastly, potential source for further de-

velopment is the automatic recognition of diasys-

tematic text features, such as diachronic, diatopic 

or diatechnic information. 

Clustering results of comparable corpora are 

obtained when the document characteristics are 

filtered by best keyword estimation metric - 

TF.BM25, explained in Pérez-Iglesias et al. 

(2009). The results show decrease in good clus-

tering performance. A future work aspect is to 

investigate the cause this lower performance. 

9 Conclusion 

An innovative approach to the problem of 

compilation of comparable corpora is described. 

The approach suggests guidelines to textual 

characteristics selection scheme. Additionally, 

the approach incorporates LSA and unsupervised 

ML techniques. Different evaluation metrics, 

such as precision, purity and normalized mutual 

information, are employed to estimate compara-

ble corpus clustering results. These metrics show 

good results when evaluating comparable clus-

ters from a predefined set of less than 100 docu-

ments. The methodology suggested is applied for 

monolingual selection of documents; nonetheless 

it is readily extendable to more languages.  

References  

Agirre, Eneko, Alfonseca, Enrique, Hall, Keith, 

Kravalova, Jana, Paşca, Marius and Soroa, Ai-

tor.2009. A study of Similarity and Relatedness 

Using Distributional and WordNet-based ap-

proaches. In  NAACL ’09, pages 19-27. 

Baeza-Yates, Ricardo and Ribeiro-Neto, Betrhier. 

1999. Modern Infromation Retieval, Addison 
Wesley. 

Bekavac, Božo, Osenova, Petya, Simov, Kiril and 

Tadic, Marco. 2004. Making Monolingual Corpora 

Comparable: a Case Study of Bulgarian and Croa-

tian. In Proceedings of LREC2004, pages 
1187-1190, Lisbon. 

Crystal, David. 2001. Language and the Internet. 
Cambidge University, Press. Cambidge.UK, pages 

91-93. 

Dagan, Igo, Lee, Lillian and Pereira, Fernando. 1999. 

Similarity-based models of word co-occurrence 

probabilities. Machine Learning. 34(1-3), pages 
43-69. 

Deerwester, Scott, Dumais, Susan, Furnas, George, 

Landauer, Thomas and Harshman, Richard. 1990. 

Indexing by latent semantic analysis. Journal of 

136



 

 

the Americal Society for Information Science. 
41(6), pages 391-407. 

Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yos-

si, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi and 

Ruppin, Eytan. 2001. Placing Search in Context: 

The Concept Revisited. In WWW’01, pages 406-
414. 

Goeuriot, Lorraine, Emmanuel Morin and Béatrice 

Daille. 2009. Compilation of specialized compara-

ble corpora in French and Japanese. In Proceed-

ings of the 2
nd

 workshop on Building and Us-

ing Comparable Corpora: from Parallel to 

Non-parallel Corpora, August 06, 2009, Suntec, 
Singapore.  

Hasler, Laura, Constantin Orasan and Karin Nau-

mann. 2006. NPs for Events: Experiments in Con-

ference Annotation. In Proceedings of the 5
th
 

edition of the International Conference on 

Language Resources and Evaluation 

(LREC2006),pages 1167-1172, 24-26 May 2006, 
Genoa, Italy. 

Ion, Radu, Dan Tufiş, Tiberiu Boroş, Ru Ceauşu and 

Dan Ştefănescu. 2010. On-line Compilation of 

Comparable Corpora and Their Evaluation. In 

Proceedingds of the 7
th
 International Confer-

ence of Formal Approaches to South Slavic 

and Balkan Languages (FASSBL7), pages 29-

34, Dubrovnic, Croatia. 

Kilgarriff, Adam. 2001. Comparing corpora. Interna-

tional Journal of Corpus Lingusitics, 6(1), pag-
es 97-133. 

Lee, Lillian. 1999. Measures of distributional similari-

ty. Proceedings of ACL 1999, pages 25-32. 

Maia, Belinda. 2003. What are Comparable Corpora?    

Electronic resource: 
http://web.letras.up.pt/bhsmaia/ 

belinda/pubs/CL2003%20workshop.doc. 

Manning, Christopher D. and Hinrich Schűtze. 1999. 

Introduction to Information Retrieval. Cam-
bridge University Press, Cambridge, UK. 

Manning, Christopher D., Prabhakan Raghavan, and 

Hinrich Schűtze. 2008. Introduction to Infor-

mation Retrieval, Cambridge University Press, 
pages 356-358.  

McEnery, Tony. 2003. Corpus Linguistics. In Ruslan 

Mitkov, editor, The Handbook of Computation-

al Lingustics. Oxford University Press, Oxford, 
UK, pages 448-464. 

Radinsky, Kira, Agichtein, Eugene, Gabrilovich, 

Evgeniy and Markovitch, Shaul. 2011. A word at a 

time: Computing Word Relatedness using Tem-

poral Semantic Analysis. In  WWW’11, pages 337-
346. 

Pérez-Iglesias, Joaquín, Pérez-Agüera, José, Fresno, 

Víctor and Feinstein, Yuval. 2009. Integrating the 

probabilistic model BM25/BM25F into Lucene. In  

CoRR, abs/0911.5046. 

Rose, Tony, Mark Stevenson and Miles Whitehead. 

2002. The Reuters Corpus Volume 1 – from Yes-

terday’s News to Tomorrow’s Language Resource. 

In  Proceedings of  LREC2002, pages 827-833. 

Sarageli, Xabier., San Vincente, Inaki, Gurrutxaga. 

Antton 2002.Automatic Extraction of bilingual 

terms from comparable corpora in a popular sci-

ence domain. In  Proceedings of  the workshop 

on Comparable Corpora, LREC’08. 

Sharoff, Serge. 2010. Analysing similarities and dif-

ferences between corpora. In Proceedings of the 

7
th
 Conference of Language Technologies 

(Jezikovne Tehnologije), pages 5-11, 

Ljubljiana. Slovenia. 

Skadiņa, Inguna, Ahmet Aker, Voula Giouli, Dan 

Tufis, Robert Gaizauskas, Madara Mieriņa and Ni-

kos Mastropavlos. 2010a. A Collection of Compa-

rable Corpora for Under-Resourced Languages. In 

Inguna Skadiņa and Dan Tufis, editors, Human 

Language Technologies. The Baltic Perspec-

tive. Proceedings of the 4th International Con-

ference Baltic HLT 2010, pages 161-168. 

Skadiņa, Inguna, Vasiljeiv, Andrejs, Skadiņš, Raivis, 

Gaizauskas, Robert, Tufiş, Dan and Gornostay, 

Tatiana. 2010b. Analysis and Evaluation of Com-

poarable Corpora for Under Resourced Areas of 

Machine Translation. In Proceedings of the 3
rd

 

Workshop on Building and Using Comparable 

Corpora. Applications of Parallel and Com-

parable Corpora in Natural Language PEngi-

neering and the Humanities, pages 6-14. 

Su, Fangzhoung and Bogdan Babych. 2012. Measur-

ing Comparability of Documents in Non-Parallel 

Corpora for Efficient Extraction of (Semi-)Parallel 

Translation Equivalents. In Proceedings of the 

Joint Workshop on Exploiting Synergies be-

tween Information Retrieval and Machine 

Translation (ESIRMT) and Hybrid Approach-

es to Machine Translation (HyTra), pages 10-
19, Avignon, France. 

Tao, Tao and Cheng Xiang Zhai. 2005. Mining Com-

parable Bilingual Text Corpora for Cross-

Language Information Integration. In Proceedings 

of the eleventh ACM SIGKDD international 

conference on Knowledge discovery in data 

mining, pages 691-696. 
 

137


