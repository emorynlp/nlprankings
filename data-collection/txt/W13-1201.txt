










































Coping With Implicit Arguments And Events Coreference


Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1–10,
Atlanta, Georgia, 14 June 2013. c©2013 Association for Computational Linguistics

Coping With Implicit Arguments And Events Coreference 

Rodolfo Delmonte 
Department of Language Studies & Department of Computer Science 

Ca’ Foscari University - 30123, Venezia, Italy 
delmont@unive.it

 
 

Abstract 

In this paper we present ongoing work for 
the creation of a linguistically-based system 
for event coreference. We assume that this 
task requires deep understanding of text and 
that statistically-based methods, both 
supervised and unsupervised are inadequate. 
The reason for this choice is due to the fact 
that event coreference can only take place 
whenever argumenthood is properly 
computed. It is a fact that in many cases, 
arguments of predicates are implicit and 
thus linguistically unexpressed. This 
prevents training to produce sensible results. 
We also assume that spatiotemporal 
locations need to be taken into account and 
this is also very often left implicit. We used 
GETARUNS system to develop the 
coreference system which works on the 
basis of the discourse model and the 
automatically annotated markables. We 
present data from the analysis, both on 
unexpressed implicit arguments and the 
description of the coreference algorithm. 

1 Introduction 

NLP processing is more and more oriented towards 
semantic processing which in turn requires deep 
understanding of texts. We assume that this is only 
possible if unexpressed implicit linguistic elements 
and semantically deficient items are taken into 
consideration (Delmonte 2009a, 2009b). One of 
the first problem in the analysis of any text is 
accounting for implicit or linguistically 
unexpressed information. This kind of information 
is not available in dependecy-based current 
annotated corpora or is only partially available – as 
in Penn Treebank – but it cannot possibly be learnt. 
The problem of null and pronominal elements is 
paramount in the recovery of Predicate-Argument 
Structures which constitutes the fundamental 
element onto which propositional semantics is 
made to work. However, applying machine 

learning techniques on available treebanks is of no 
help. State of the art systems are using more and 
more dependency representations which have 
lately shown great resiliency, robustness, 
scalability and great adaptability for semantic 
enrichment and processing. However, by far the 
majority of systems available off the shelf don’t 
support a fully semantically consistent 
representation and lack Null Elements or 
Antecedents for pronominal ones. 
If we limit ourselves to Null Elements, and to 
PennTreebank (hence PT), we may note that 
Marcus (’94) referred explicitly to Predicate-
Argument Structures (hence PASs) and to the need 
to address this level of annotation. He mentions 
explicitly that “we intend to automatically extract a 
bank of PASs intended at the very least for parser 
evaluation from the resulting annotated corpus” 
and further on “the notation should make it easy to 
automatically recover PAS” (ibid. 121). He also 
mentions the need to allow for a clear and concise 
distinction between verb ARGUMENTs and 
ADJUNCTs, which he asserts to be very difficult 
to make, consistently. This happens to be true: the 
final version of PT II does not include coindexing 
in controversial cases and has coindexing for null 
SBJ only in a percentage of the cases. PT contains 
36862 cases of null elements (including traces, 
expletives, gapping and ambiguity) as listed in 
Johansson(2007), over 93532 simple clauses and 
55600 utterances, for a percentage of 66.3%. Of 
course this number does not include pronominal 
arguments which need to be bound – and are not 
bound in PT - to an antecedent in order to become 
semantically consistent. 
As to PT, the difficulty of the task is testified by 
the presence of non coindexed Null Elements: in 
particular we see that they are 8416, that is 22.83%. 
If we exclude all traces of WH and topicalization 
and limit ourselves to the category OTHER 
TRACES which includes all unexpressed 
SUBJects of infinitivals and gerundives, we come 
up with 12172 cases of Null non-coindexed 

1



elements, 33% of all cases. We should note that for 
how much large this number may seem, this still 
represents a small percentage when compared to 
the number of null elements in languages like 
Chinese or Romance languages like Italian, which 
allow for free null subjects insertion in tensed 
clauses.  
Current statistically dependency parsers have made 
improvements in enriching their structural output 
representation (Gabbard et al. 2006; Sagae and 
Tsujii, 2008;	  Choi & Palmer, 2010;	  Cai et al. 2011). 
However, coindexation is not always performed: 
when it is, its performance is computed separately 
because it is lower than accuracy for 
labeled/unlabeled tasks. In particular, Schmid 
reports 84% F-score for empty elements prediction 
and 77% for coindexation on PT. However, other 
parsers have much worse results, with 
Johnson(2001) being the worst, with 68% F-score. 
The presence of additional difficulties to predict 
empty categories is the cause of a bad drop in 
performance in Chinese - no more than 50% 
accuracy reported by Cai et al. (2011) compared to 
74/77% of the labeled/unlabeled task. Results 
reported by Yang & Xue (2010) on recovering 
labeled empty elements in an experiment carried 
on a small subset of the Penn Chinese Treebank 
6.0 reach an average of 60.5% of F-measure. As to 
recovery of specific items, we note that over a total 
number of 290 little_pro items recall fares around 
50%. 
Of course the phenomenon is very much language 
dependent, as discussed above. If we consider a 
language like Italian – which we described fully 
from structures annotated in the treebank called 
VIT (Delmonte 2004) – we can see that in addition 
to untensed sentences also simple clauses with 
tensed verbs show the same problem. In fact, over 
66.5% (9634 over 15874) of all simple clauses are 
subjectless, they have an omitted or unexpressed 
subject which is marked in linguistics with a 
little_pro and the agreement coming from 
morphology of the main verb. Of the remaining 
lexically expressed subjects, only 64% (6166 over 
9634) are in canonical position, that is in preverbal 
position and adjacent to the inflected verb. The 
remaining 36% of lexically expresses subjects are 
positioned to the right or are separated from the 
verb by other constituents. 

2 Events and Null Elements 

We will now try to describe events in terms of the 
contribution of Null Elements. Events are mainly 
characterized by their meaning which is defined in 
a gloss or by one or more semantic categories, or 
even by a synset of synonym concepts. In addition 
to that, events may be regarded as being composed 
of two other elements: 

- the participants to the event, which are arguments 
and adjuncts or circumstantials 

- the spatiotemporal location of the event 

Both components may be linguistically expressed 
or be left implicit and thus should be inferred from 
previous discourse. In fact the spatiotemporal 
location of the event is usually indicated explicitly 
only if needed and is mostly left unexpressed. 
Participants on the contrary are mostly explicitly 
expressed before they can be left implicit. 
However, in some case, participants are 
linguistically unexpressed for structural reasons or 
else expressed by a pronoun. Both cases require a 
deep system or a deep parser together with a 
pronominal binding algorithm to be in place, in 
order to find the appropriate antecedent and bind 
the empty arguments. There are exceptions to these 
rules and they are constituted by utterances of 
generic or arbitrary reference, something intended 
in utterances such as, 

(1) Doing regular physical exercise is strongly 
recommended at a certain age. 

where no participant is explicitly indicated, but it is 
clearly understood by inferences determined by 
knowledge of the world. 
Events may be coreferred or may be queried: in 
both cases, we are also dealing with semantic 
relations at discourse structure level. The need to 
corefer to a previous event derives from 
conversational or argumentative strategies. 
Generally speaking, it is due to the need of 
expanding concepts and facts reported in the 
previous mention. At a discourse level, this is 
usually called ELABORATION or 
EXPLANATION. Other possible cases of event 
coreference at discourse structure level can be due 
to the need of enriching the previous description of 
additional facts cooccuring with the previously 
mentioned event: in this case we may have an 
hypernymic or an hyponymic relation intervening 

2



between the two facts or concepts. Let’s look at 
some examples taken from the demo text made 
available by the organizers. 
After the title, we have a first event description, 
reported by a newspaper, which is a violent event 
followed by an adjunct clause describing the 
effects or caused consequences: we capitalize 
event naming words and then indicate the semantic 
discourse relation: 

 “A Kurdish newspaper said Wednesday that Iraqi 
members of an Al Qaeda-linked group, a Kurd and 
an Arab, BLEW themselves up in northern Iraq on 
February 1, KILLING at least 105 people.”  

CAUSE è BLOW, KILL 

The idea in this case is that the two events are 
linked by a semantic relation rather than simply the 
first event being coreferred by the second. The text 
continues by expanding the event introducing some 
comment that elaborates on the previous sentence: 

 “The twin suicide bombing WAS the deadliest 
attack in post-war Iraq and WAS SUSPECTED 
TO HAVE BEEN CARRIED OUT by foreign 
fighters, possibly linked to Osama bin Laden's Al-
Qaeda network.” 

ELABORATION è BOMBING, ATTACK 

CAUSE è BOMBING, BLOW 

EXPLANATION è SUSPECT(CARRY_OUT), 
BLOW 

Discourse relations are triggered by event 
coreference which in this sentence is achieved by 
two nominalizations: in fact only definite 
expressions are taken into consideration, in 
particular if singular in number. The first one is 
TWIN SUICIDE BOMBING which we understand 
to be a new enriched mention of BLOW at first by 
a causal relation intervening between BOMB and 
BLOW. This semantic relation is not available 
from WordNet but from Sumo-Milo, where the 
verbs BOMB, BLAST, ATTACK, KILL, and 
FIGHT all share one semantic class, 
VIOLENT_CONTEST and/or DESTRUCTION 
with BLOW. The causal relation is derived from 
commonsense knowledge available in 
"ConceptNet" by the AI Laboratory of MIT. 

Searching for relations intervening between 
BOMB and BLOW_UP, this is what you can find - 
represented in an appropriate Prolog-like format: 

cpn(udf,bomb,[blow,something,up]). 
cpn(udf,bomb,[blow,things,up]). 
cpn(udf,bomb,[blow,up,buildings]). 
cpn(udf,bomb,[blow,up,stuff]). 
cpn(udf,bomb,blow). 
cpn(do,person,[don_t,want,be,blow,up,by,bomb]). 
cpn(dof,person,[not,be,blow,up,by,bomb]). 
 

They are also all classified as NEGATIVE polarity 
items and are part of the same Lexical Field in 
Roget's Thesaurus. 
Then the additional contribution of its arguments, 
where “blowing themselves up” implies a 
SUICIDE took place. At the same time, the use of 
“twin” is coreferring with “members” a plural 
noun, better specified as being composed of two 
individuals “a Kurd and an Arab” in an apposition 
to it. Thus, the nominalization does not add any 
new information that could not be understood from 
previous mention, but certainly clarifies previous 
information thus respecting Grice’s maxims.  
The copulative structure headed by WAS, is used 
to assign a property to the coreferred event thus 
contributing new information. We now know that 
the newspaper reports the event as being “the 
deadliest attack in post-war Iraq”. We also learn 
that the two fighters identity was suspected to be 
not Iraqi but possibly “foreign”, deemed to belong 
to bin Laden’s network. All of this new 
information can be labeled as “Explanation”. 
The news story continues by elaborating on the 
two fighters by expanding on their identity, and 
then explaining the way in which the bombing was 
organized in the following two sentences. 

 “The pair were named respectively as Abu Bakr 
Hawleri and Kazem Al-Juburi, alias Abu Turab, by 
independent newspaper Hawlani, which said they 
belonged to the Army of Ansar al-Sunna. 

The Kurd blew himself up in the offices of the 
Patriotic Union of Kurdistan (PUK) and the Arab 
in the offices of the Kurdistan Democratic Party 
(KDP), both in the Kurdish city of Arbil, said the 
newspaper. 

3



Each one carried a belt packed with four kilograms 
(8.8 pounds) of TNT mixed with phosphorus, a 
highly flammable material, the newspaper said.” 

The use of a definite singular expression is highly 
indicative of the coreference mechanism being 
activated. This applies to THE PAIR, coreferring 
with "Iraqi members" and also with "twin". The 
same can be said of "The Kurd" coreferring with 
the previous mention and also the use of the same 
predicate BLOW UP. In the following stretch of 
discourse, the story corefers to the “Army of Ansar 
al-Sunna”, to explain the role that the organization 
had in the bombing: 

“Ansar al-Sunna last week claimed the twin 
bombings in a statement posted on an Islamist 
website. The newspaper said the motive of the 
attack was to "punish" the two Kurdish secular 
groups, which control Iraqi Kurdistan, for their 
alliance with the US-led coalition. 

The newspaper said Ansar al-Sunna broke away 
from the Ansar al-Islam group last October and 
was led by an Arab whose alias is Abu Abdullah 
Hasan bin Mahmud. Ansar al-Sunna is more 
extreme, said the newspaper.” 

The first coreference link is expressed by the 
sentence “Ansar claimed the TWIN BOMBINGs” 
which is used to expand on the role of the 
organization of the original event. Additional 
events are the STATEMENT, a nominal event, and 
the MOTIVE OF THE ATTACK which introduces 
the MOTIVATION for the event. This causal link 
is connected to actual causal event: PUNISHing 
the Kurdish group controlling Iraqi Kurdistan. In 
turn, the action of PUNISHing is explained by 
another eventive nominalization, the ALLIANCE 
of the group (the possessive THEIR corefers with 
it), with the US-led coalition. 
Additional explanation is reported in the final 
sentence (longer than 40 tokens!!) where the 
relation intervening between the motive the attack 
as contained in the statement and previously 
occurring facts is further clarified: 

 “The newspaper added that bin Mahmud is the 
brother of man whose alias is Abdullah Al-Shami, 
an Ansar al-Islam leader who was killed last year 
while fighting a US-backed onslaught by the PUK 

that forced the group out of its enclave near the 
Iranian border at the end of March last year.” 

The sentence contains additional coreferring 
nominalizations like ONSLAUGHT, which 
reminds of the bombing and of the previous attack. 
The overall events description is rich in temporal 
and spatial locations which contribute to the 
understanding and the overall discourse structure. 
In particular we start out by a spatial location, 
NORTHERN IRAQ, and a temporal location, 
FEBRUARY 1st. Both locations remain the same 
in the following sentences until we reach a change 
in topics and locations. This happens when “Ansar 
al-Sunna” is introduced as SUBJect of CLAIM, an 
event location in time, LAST WEEK.  Additional 
information the Ansar al-Islam group takes us back 
to LAST OCTOBER. Eventually, in the final 
sentence, we have been told that the current 
bombing event may have relation with the killing 
of another Ansar al-Islam leader, during an 
ONSLAUGHT that took place LAST YEAR, in a 
different location, NEAR THE IRANIAN 
BORDER. The generic location LAST_YEAR is 
further specified as being END OF MARCH. 
 
3. GETARUNS : a system for text 
understanding 
 

GETARUNS1, the system for text understanding 
developed at the University of Venice, is organized 
as a pipeline which includes two versions of the 
system: what we call the Partial and the Deep 
GETARUNS and they work in a backoff policy. 
There are in fact three parsers interconnected and 
they are activated in order to prevent failure to take 
place. The system has a middle module for 
semantic interpretation and discourse model 
construction which is cast into Situation 
Semantics; and a higher module where reasoning 
and generation takes place.  

The system is based on LFG theoretical 

	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
1 The system has been tested in STEP competition (see 
Delmonte 2008), and can be downloaded in two separate 
places. The partial system called VENSES in its stand-alone 
version is available at http://www.aclweb.org/aclwiki/ 
index.php?title=Textual_Entailment_Resource_Pool. The 
complete deep system is available both at 
http://www.sigsem.org/wiki/ 
STEP_2008_shared_task:_comparing_semantic_representatio
ns, and at, http://project.cgm.unive.it/html/sharedtask/. 

4



framework and has a highly interconnected 
modular structure.  

The output of grammatical modules is fed then 
onto the Binding Module which activates an 
algorithm for anaphoric binding. Antecedents for 
pronouns are ranked according to grammatical 
function, semantic role, inherent features and their 
position at f-structure. Eventually, this information 
is added into the original f-structure graph and then 
passed on to the Discourse Module (hence DM). 

GETARUNS, has a linguistically based semantic 
module which is used to build up the DM. 
Semantic processing is strongly modularized and 
distributed amongst a number of different 
submodules which take care of Spatio-Temporal 
Reasoning, Discourse Level Anaphora Resolution, 
and other subsidiary processes like Topic 
Hierarchy which cooperate to find the most 
probable antecedent of coreferring and 
cospecifying referential expressions when creating 
semantic individuals. These are then asserted in the 
DM, which is then the sole knowledge 
representation used to solve nominal coreference. 
The system uses two resolution submodules which 
work in sequence: the first one is fired whenever a 
free sentence external pronoun is spotted; the 
second one takes the results of the first submodule 
and checks for nominal anaphora. They have 
access to all data structures contemporarily and 
pass the resolved pair, anaphor-antecedent to the 
following modules. Semantic Mapping is 
performed in two steps: at first a Logical Form is 
produced which is a structural mapping from 
DAGs onto unscoped well-formed formulas. These 
are then turned into situational semantics 
informational units, infons which may become 
facts or sits. Each unit has a relation, a list of 
arguments which in our case receive their semantic 
roles from lower processing – a polarity, a 
temporal and a spatial location index. 
All entities and their properties are asserted in the 
DM with the relations in which they are involved; 
in turn the relations may have modifiers - sentence 
level adjuncts, and entities may also have 
modifiers and attributes. Each entity has a polarity 
and a couple of spatiotemporal indices which are 
linked to main temporal and spatial locations if any 
exists; else they are linked to presumed time 
reference derived from tense and aspect 
computation. On second occurrence of the same 
nominal head the semantic index is recovered from 

the  history list and the system checks whether it is 
the same referring expression and has non-
conflicting attributes or properties. In all other 
cases a new entity is asserted in the DM which 
however is also computed as being included in (a 
superset of) or by (a subset of) the previous entity. 

4. A System For Event Marking And Event 
Coreference 

I will now go through the text above indicating 
places where the system has been able to locate 
and identify missing arguments. In order to clarify 
the working of the system I will use the output of 
the discourse model, which contains fully 
coreferred empty or linguistically unexpressed 
elements which have gone through pronominal 
binding process as well as coreference analysis.  
The first unexpressed element is the subject of the 
adjunct gerundive headed by KILL in the first 
sentence: 
 
(1) A Kurdish newspaper said Wednesday that 
Iraqi members of an Al Qaeda-linked group, a 
Kurd and an Arab, blew themselves up in northern 
Iraq on February 1, killing at least 105 people. 
 
The second unexpressed argument is contained in 
sentence 2, in the infinitival governed by 
SUSPECT and headed by CARRY_OUT, which is 
contained in the coordinate structure headed by 
SUSPECT, 
 
(2) The twin suicide bombing was the deadliest 
attack in post-war Iraq and was suspected to have 
been carried out by foreign fighters, possibly 
linked to Osama bin Laden's Al-Qaeda network. 
 
where the suicide_bombing is predicated as the 
"deadliest attack" in the previous main sentence. 
The main spatial location now becomes Iraq. 
Another unexpressed argument is the subject of 
POSTED, a participial modifying STATEMENT, 
 
(3) Ansar al-Sunna last week claimed the twin 
bombings in a statement posted on an Islamist 
website. 
 
where we still want to know who posted the 
“statement”, and the information is passed by the 

5



main clause as the subject of CLAIM, i.e. Ansar al-
Sunna.  
Then we have another infinitival lacking subject 
argument information, in the following sentence, 
 
(4) The newspaper said the motive of the attack 
was to "punish" the two Kurdish secular groups, 
which control Iraqi Kurdistan, for their alliance 
with the US-led coalition. 
 
This is a copulative structure where the subject 
MOTIVE is predicated by the infinitival headed by 
PUNISH. In fact, this verb is lacking a referential 
subject simply because the predication prevents it 
from having a specific one. 
The same GROUP is coreferred in the final 
sentence that contains the most important sequence 
of unexpressed but yet essential arguments: 
 
(5) The newspaper added that bin Mahmud is the 
brother of man whose alias is Abdullah Al-Shami, 
an Ansar al-Islam leader who was killed last year 
while fighting a US-backed onslaught by the PUK 
that forced the group out of its enclave near the 
Iranian border at the end of March last year. 
 
The gerundive headed by FIGHT has LEADER as 
SUBJect and as OBJect the GROUP we found in 
the previous sentence. It is important to notice that 
this mention of GROUP is NOT coreferent with 
the one appearing at the beginning of the text. This 
non coreference is clearly apparent from attributes 
accompanying the head: the first one is expressed 
as "an Al Qaeda-linked group", whereas the second 
as "the two Kurdish secular groups". 
In the final computation, the system produces a set 
of entity pools, that is a set of all referents to a 
given semantic index - be they properties, entities 
or relations. In particular, the referent to the 
LEADER coincides by virtue of a predication, with 
Abdullah Al-Shami and has the property of being 
associated to Ansar al-Islam: from the pool, we 
now know that he was KILLED, being associated 
to the THEME_AFFected role. 
 
4.1 The Experiment and an Evaluation 

We tested the coreference module with the sample 
text and produced the following output that we 
comment in this section. For each event we have 
two vectors of information that we then use to 

evaluate its relevance and its possible coreference 
in the previous text. The categories used are fully 
explained in Delmonte (2007; 2009) and here we 
limit ourselves to a short description. 
The event may be a verb and be related to a 
propositional analysis or be a noun. Nouns 
classified as activity or events are selected as 
markables: this classification is partially derived 
from NomBank associated information about 
eventive nominals. Coreference links are activated 
by synonymity or just similarity, measured by 
WordNet synset, a Thesaurus or sharing identical 
semantic classes as indicated in SUMO-MILO or 
other similar computational lexica. The certainty 
value varies accordingly: from more certain, say .9, 
to less certain .4. Obviously, copulative 
predications are marked with certainty equal to 1 
being properties predicated in the syntax of the 
subject. 
 
4.1.1 Coreference links 

We present here briefly the addition to the system 
GETARUNS that have been produced for this task. 
The annotation of each text is shown in an xml file 
which has been obtained in the following steps: 
a. the system GETARUNS produces a deep 
analysis of each text on a sentence by sentence 
basis. At the end of the analysis of each sentence, 
markables are collected and all semantic 
information is attached to each word of the 
sentence. We collected all verbs and also eventive 
nominals and possible eventive modifiers. This is 
done in two steps.  
b. at the end of parsing each word of the sentence 
is associated to its lemma and general semantic 
categories are also collected from the analysis.  
c. The system produces then the steps required for 
the Discourse Model which is where entities, 
relations and properties are asserted with their 
attributes. Semantic indices are assigned to each 
new entity and previous mentions receive 
previously assigned indices. At this point the 
contents of the discourse model are associated to 
each word of the sentence.  
d. At the end of the analysis of the text the system 
collects all markables, which are internally made 
of four elements: an markable index, a word, a 
lemma, a semantic index (from the discourse 
model) or a generic indicator of eventuality for all 
verbs.  

6



e. Then the complete discourse model is searched 
to produce a list of all entities, relations and 
properties with their spatiotemporal relations and 
polarity, as documented in situation semantics. 
Additional information is derived in this phase 
from WordNet, FrameNet or SumoMilo ontology 
and is made available to the coreference algorithm. 
Another component that is activated at the end of 
the analysis is sentiment analysis that computes an 
affective label associated to each markable - if 
possible - and classifies each markable into three 
different classes: positive, negative and neutral. 
d. The coreference algorithm works as follows: for 
each markable it check all possible coreference 
links, at first on the basis only of inherent semantic 
features, which are: wordform and lemma identity; 
then semantic similarity measured on the basis of a 
number of similarity criteria which are lexically 
based (no statistical measure is used). We search 
WordNet synsets and assign a score according to 
whether the markables are directly contained in the 
same synset or not. A different score is assigned if 
their relation can be inferred from the hierarchy. 
Other computational lexical resources we use are 
those documented in our work on Text Entailment 
Recognition (Delmonte et al. 2005; 2006; 2007; 
2008), and include FrameNet and Frames 
hierarchy; SumoMilo and its semantic 
classification.  
f. After collecting all possible coreferential 
relations between semantically validated markables, 
we then proceed to filter out those links that are 
inconsistent or incompatible according to three 
criteria: 
- first criterion: diverse sentiment polarity 
- second criterion: different argument structure  
- third criterion: non related spatiotemporal 
relations 
Both argument structure and spatiotemporal 
relations are collected in the discourse structure 
which also contains dependence relations 
expressed by discourse relations in discourse 
structures; temporal logical relations as computed 
from an adaptation of Allen's algorithm; and a 
point of view computed on the basis of presence of 
“reportive” verbs, or direct speech, reported speech, 
reported indirect speech.  
Another criterion we adopt is the nature of 
semantic similarity computed by the system. 
Values below a certain threshold indicate the 
coreference has been chosen on the basis of weak 

similarity, as may apply to semantic lexical fields. 
These are based on thesauri classification. Some 
examples below. 
As said above, event coreference links require 
sentiment match, argument identity or semantic 
similarity. In particular consider such cases as  
 
<MARK  ID=m34> claimed  </MARK>. 
 
is semantically computed as a communication verb 
on a par with SAY, but coreference is prevented by 
the fact that arguments don't coincide. SAY in all 
its various forms is used to report what the 
newspaper Hawlani said. Here CLAIM is related to 
different arguments as shown in the discourse 
structure entry, 
ds(to(7-17),7-
18,claim([id86:[ansar,sunna,al],id4:suspect,id87:st
atement],1,id71),during(tes(sn19evs7),tes(sn31evs
6)), narration,'ansar_al-sunna') 

The same applies to the use of KILL in the last 
sentence (11) whose argument structure prevents a 
coreference link with the previous occurrence of an 
identical verb form in sentence (2). Here below are 
the two discourse structures containing argument 
structures for the verb KILL in the two sentences: 
 
ds(down(11-28),11-
29,kill([id140:[[abdullah,shami,al],leader],id145:exist],1
,id71), after(tes(f562evs11),tes(f772evs11)), 
narration,narrator), 

ds(to(2-3),2-
4,kill([id16:member,id18:people],1,univ),after(tes(f4_ev
s_2),tes(f2_evs_1)),result,narrator), 

Discourse Structures also contain temporal logical 
relations, Discourse relation and Point of View. If 
we consider all computed markables, which are in 
our system 67, we come up with 47 possible 
coreference links. However only 17 have been 
regarded admissible and consistent and are listed 
here below.  
 
1.coref-ident m1 m7 hypothetical_certainty 1 
2.coref-ident m3 m17 hypothetical_certainty 1 
3.coref-simil m2 m20 hypothetical_certainty 0.9 
4.coref-simil m4 m14 hypothetical_certainty 0.9 
5.coref-ident m7 m25 hypothetical_certainty 1 
6.coref-simil m10 m21 hypothetical_certainty 0.9 
7.coref-ident m6 m28 hypothetical_certainty 1 

7



8.coref-ident m7 m29 hypothetical_certainty 1 
9.coref-ident m7 m33 hypothetical_certainty 1 
10.coref-simil m1 m36 hypothetical_certainty 0.9 
11.coref-simil m11 m35 hypothetical_certainty 0.9 
12.coref-simil m15 m37 hypothetical_certainty 0.9 
13.coref-ident m6 m43 hypothetical_certainty 1 
14.coref-ident m7 m38 hypothetical_certainty 1 
15.coref-ident m14 m40 hypothetical_certainty 1 
16.coref-simil m32 m47 hypothetical_certainty 0.9 
17.coref-ident m7 m48 hypothetical_certainty 1 
 
Markables M1, M7, M25, M33, M38, M48 all 
refer to verb SAY and have as SUBJect the 
newspaper; in one case M1 is wrongly coreferred 
to M36, STATEMENT. M3 is attached to the noun 
SUSPECT and is made to corefer to M17, the verb 
SUSPECT which share arguments with the noun. 
M2 is “Al_Qaeda-linked” and is coreferred to M20, 
“linked”. M4 is BLASTS and is coreferred to M14, 
ATTACK. M10 is TWIN and is coreferred to M21, 
PAIR. M6 is “Kurdish” coreferred to M28 again 
Kurdish, but also M43. M11 is “suicide_bombing” 
which corefers to M35, “bombings”. M15 is “post-
war” and is wrongly coreferred to M37 “posted”. 
M14 ATTACK is coreferred to M40 again 
ATTACK. M32 MIXED is wrongly coreferred to 
M47 COALITION. There are three errors over 17. 
Omitted links include the following one 
coref- (m4- (blasts-blast-id5))- (m8- (blew-
blow_up-id26))-5 
where coreference between BLAST and 
BLOW_UP is established and the score assigned is 
0.5. This score is regarded too low and is filtered 
out, even though a causal link was clearly 
inferrable. 
 
5. Conclusion and Future Work 
 
We show here below in Table 1. total counts for 
the 13 texts distributed with the Event Coreference 
Task. The system computed automatically 
Controllers and Antecedents: the first are referred 
to syntactically controlled Null Elements of 
Relative and Interrogative Clauses. The second are 
referred to SUBJects of infinitivals, and other  
predicative structures both argumental and non-
argumental. The table also includes counts of 
Markables and Coreferent Links, again computed 
automatically. There is no evaluation yet available. 
What we wanted to show is the proportion of NEs 

with respect to sentences, which is 1.6 per sentence, 
that is there are three NEs every two sentences.  
 

LI/Rounds Round1 Round2 Round3 Total 
Markables 334 372 325 1031 
Corefs 72 79 37 188 
Controllers 69 57 55 181 
Antecedents 60 57 53 170 
Sentences 69 78 72 219 
Total Null  
Elementss 

 
129 

 
114 

 
108 

 
351 

Table 1. Null Elements, Markables and Coreferents 
automatically computed by Getaruns on the 13 
texts of the Task 
 
In this paper we presented ongoing work to 
produce a system for event coreference that uses a 
linguistically-based approach and the output of a 
deep system for the representation of a text in a 
situation semantics framework. The output of the 
system on the sample text has been fairly 
consistent in particular for what concerns the 
computation of implicit information which we 
regard paramount for a successful performance in 
the task at hand.  
Semantic relations have been built taking into due 
account all attributes and modifiers of the semantic 
head. This process has allowed preventing 
coreference to take place on the basis of simple 
concept matching procedures. Some inferential 
processes have been fired using commonsense 
knowledge stored in the publicly available resource, 
ConceptNet.  
Besides, the computation of temporal relations 
based on a revised version of Allen's algorithm has 
allowed to control inclusion relations intervening 
between event structures. The output of the system 
includes a discourse structure which shows 
coordination and subordination links between 
discourse stretches defined by propositional level 
analysis. Structural inclusion is allowed again only 
in presence of same TOPIC and same 
spatiotemporal relation checking. Both NEW topic 
and NEW spatiotemporal relation will cause the 
structure to jump up to any possible previous node 
that may be used to provide a cohesion link in the 
text. This notion of coreference has not been 
explored yet and will be the topic of further study. 
 

  

8



REFERENCES 

Alshawi, H., Pi-Chuan Chang, M. Ringgaard. (2011). 
Deterministic Statistical Mapping of Sentences to 
Underspecified Semantics, in Johan Bos and Stephen 
Pulman (editors), Proceedings of the 9th International 
Conference on Computational Semantics, IWCS,15-
24. 

Bender, E.M. and D.Flickinger (2005). Rapid 
prototyping of scalable grammars: Towards 
modularity in extensions to a language-independent 
core. in Proc. 2nd IJCNLP-05, Jeju Island, Korea. 

Bender, E.M., D.Flickinger, and S.Oepen (2002). The 
Grammar Matrix: An open-source starter-kit for the 
rapid development of cross-linguistically consistent 
broad-coverage precision grammars. In J.Carroll et 
al.(Eds.), Proc. Workshop Grammar Engineering 
and Evaluation at COLING19, Taipei, Taiwan, 8-14. 

Bresnan, J., 2000. Lexical-Functional Syntax, Blackwell. 
Cai, Shu, David Chiang, Yoav Goldberg, 2011. 

Language-Independent Parsing with Empty 
Elements, in Proceedings of the 49th Annual 
Meeting of the ACL, 212–216.  

Choi, Jinho D., Martha Palmer, 2010. Robust 
Constituent-to-Dependency Conversion for English, 
in Proceedings of the 9th International Workshop on 
Treebanks and Linguistic Theories (TLT'9), 55-66, 
Tartu, Estonia.  

Clark P., C. Fellbaum, J. Hobbs, P. Harrison, 
W.R.Murray, J. Thompson, 2008. Augmenting 
WordNet for Deep Understanding of Text, in J.Bos 
& R.Delmonte(eds.), 2008. ACL-SigSem, STEP 
(Semantics in Text Processing), College 
Publications, London, p.45-58. 

Copestake, Ann. 2004/2006. Robust Minimal Recursion 
Semantics, Unpublished draft (downloadable from 
http://www.cl.cam.ac.uk/~aac10/papers.html). 

Copestake, Ann, (2009). Invited Talk: Slacker 
Semantics: Why Superficiality, Dependency and 
Avoidance of Commitment can be the Right Way to 
Go. In: Proceedings of the 12th Conference of the 
European Chapter of the ACL (EACL 2009), pages 
1-9. Athens, Greece, 2009. 

Copestake, A., D.Flickinger, C.Pollard, and I.Sag 
(2005). Minimal recursion semantics: An 
introduction. Research on Language and 
Computations 3(4), 281-332. 

CoreLex:- http://www.cs.brandeis.edu/~paulb/CoreLex/ 
corelex.html 
EuroWordNet:- http://www.illc.uva.nl/EuroWordNet/ 
Delmonte R.(1990), Semantic Parsing with an LFG-

based Lexicon and Conceptual Representations, 
Computers & the Humanities, 5-6, pp.461-488. 

Delmonte R., D.Bianchi(1991), Binding Pronominals 
with an LFG Parser, Proceeding of the Second 
International Workshop on Parsing Technologies, 

Cancun(Messico), ACL 1991, pp.59-72. 
Delmonte R.(1995), Lexical Representations: Syntax-

Semantics interface and World Knowledge, in 
Rivista dell'AI*IA (Associazione Italiana di 
Intelligenza Artificiale), Roma, pp.11-16. 

Delmonte R.(1996),  Lexical Representations, Event 
Structure and Quantification, Quaderni Patavini di 
Linguistica 15, 39-93. 

Bianchi D., Delmonte R. (1996),  Temporal Logic in 
Sentence and Discourse, in Atti SIMAI'96, pp.226-
228. 

Dario Bianchi, Rodolfo Delmonte(1999), Reasoning 
with A Discourse Model and Conceptual 
Representations, Proc. VEXTAL, Unipress, pp. 401-
411. 

Delmonte R.(2002),  From Deep to Shallow Anaphora 
Resolution:, in Proc. DAARC2002 , 4th Discourse 
Anaphora and Anaphora Resolution Colloquium, 
Lisbona, pp.57-62. 

Delmonte R.(2002), GETARUN PARSER - A parser 
equipped with Quantifier Raising and Anaphoric 
Binding based on LFG, Proc. LFG2002 Conference, 
Athens, pp.130-153, at http://cslipublications. 
stanford.edu/hand/miscpubsonline.html.  

Delmonte R., Sara Tonelli, Marco Aldo Piccolino 
Boniforti, Antonella Bristot, Emanuele Pianta (2005), 
VENSES – a Linguistically-Based System for 
Semantic Evaluation, in Joaquin Quiñonero-Candela, 
Ido Dagan, Bernardo Magnini, Florence d’Alché-
Buc, 2005, Machine Learning Challenges. 
Evaluating Predictive Uncertainty, Visual Object 
Classification, and Recognising Textual Entailment.: 
First PASCAL Machine Learning Challenges 
Workshop, MLCW 2005, Southampton, UK, April 
11-13, 2005, Revised Selected Papers, 344-371. 

Delmonte, R., Antonella Bristot, Marco Aldo Piccolino 
Boniforti and Sara Tonelli, 2006. Another 
Evaluation of Anaphora Resolution Algorithms and 
a Comparison with GETARUNS' Knowledge Rich 
Approach, ROMAND 2006, 11th EACL, Trento, 
Association for Computational Linguistics, 3-10. 

Delmonte, R., A. Bristot, M.A.Piccolino Boniforti and S. 
Tonelli, 2006. Coping with semantic uncertainty 
with VENSES, in Bernardo Magnini, Ido 
Dagan(eds.), Proceedings of the Challenges 
Workshop - The 2nd PASCAL Recognizing Textual 
Entailment Challenge, 86-91, Università Ca' Foscari, 
Venezia. 

Delmonte R., (2007), Computational Linguistic Text 
Processing – Logical Form, Semantic Interpretation, 
Discourse Relations and Question Answering, Nova 
Science Publishers, New York. 

Delmonte R., A. Bristot, M.A.Piccolino Boniforti, 
S.Tonelli (2007), Entailment and Anaphora 
Resolution in RTE3, in Proc. ACL Workshop on 
Text Entailment and Paraphrasing, Prague, ACL 

9



Madison, USA, pp. 48-53. 
Bos Johan & Rodolfo Delmonte (eds.), 2008. Semantics 

in Text Processing (STEP), Research in 
Computational Semantics, Vol.1, College 
Publications, London. 

Delmonte R., 2009. Computational Linguistic Text 
Processing – Lexicon, Grammar, Parsing and 
Anaphora Resolution, Nova Science Publishers, 
New York. 

Delmonte R., G. Nicolae, S. Harabagiu, C.Nicolae 
(2007), A Linguistically-based Approach to 
Discourse Relations Recognition, in B.Sharp & 
M.Zock(eds.), Natural Language Processing and 
Cognitive Science, Proc. 4th NLPCS, Funchal, 
Portugal, INSTICC PRESS, pp. 81-91. 

Delmonte R., G. Nicolae, S. Harabagiu (2007), A 
Linguistically-based Approach to Detect Causality 
Relations in Unrestricted Text, in Proc. MICAI-2007, 
IEEE Publications, 173-185. 

Delmonte R., 2008. Semantic and Pragmatic Computing 
with GETARUNS, in Bos & Delmonte (eds.), 
Semantics in Text Processing (STEP), Research in 
Computational Semantics, Vol.1, College 
Publications, London, pp. 287-298. 

Delmonte R., E. Pianta, (2009), Computing Implicit 
Entities and Events for Story Understanding, in 
H.Bunt, V.Petukhova and S.Wubben(eds.), Proc. 
Eighth International Conference on Computational 
Semantics IWCS-8, Tilburg University Press, pp. 
277-281.  

Delmonte R., (2009), A computational approach to 
implicit entities and events in text and discourse, in 
International Journal of Speech Technology (IJST), 
Springer, pp. 1-14. 

Gabbard, Ryan, Mitchell Marcus, Seth Kulick, 2006. 
Fully Parsing the Penn Treebank, in Proceedings of 
the HLT Conference of the North American Chapter 
of the ACL, 184–191. 

Grice, H. P., 1975. Logic and Conversation. in P. Cole 
& J. L. Morgan, Syntax and Semantics, Vol. 3: 
Speech Acts. New York : Academic Press, 41-58. 

Harabagiu, S.M., Miller, G.A., Moldovan, D.I.: 
eXtended WordNet - A Morphologically and 

Semantically Enhanced Resource (2003), 
http://xwn.hlt.utdallas.edu, 1-8. 

Hobbs, J. (2005). Toward a useful notion of causality 
for lexical semantics. Journal of Semantics 22, 181–
209. 

Hobbs, J. (2008). Encoding commonsense knowledge. 
Technical report, USC/ISI. 
http://www.isi.edu/∼hobbs/csk.html. 

Johansson, R. and P. Nugues. 2007. Extended 
Constituent-to-dependency Conversion for English. 
In Proceedings of NODALIDA 2007, Tartu, Estonia. 

Johnson. M., 2001. Joint and conditional estimation of 
tagging and parsing models. In ACL 2001, pages 
322–329. 

Liu, H., Singh, P. (2004). ConceptNet: A Practical 
Commonsense Reasoning Toolkit. 

Marcus, M., G. Kim, M. Ann Marcinkiewicz, R. 
Macintyre, A. Bies, M. Ferguson, K. Katz, B. 
Schasberger, (1994). The Penn Treebank: 
Annotating Predicate Argument Structure, In ARPA 
Human Language Technology Workshop, 114-119. 

Sagae, K. and Tsujii, J. 2008. Shift-Reduce Dependency 
DAG Parsing. Proceedings of the 22nd International 
Conference on Computational Linguistics (Coling 
2008). Manchester, UK. 

Yang, Yaqin and Nianwen Xue. 2010. Chasing the 
ghost: recovering empty categories in the Chinese 
Treebank. In Proc. COLING.Schubert, L. and C. 
Hwang (1993). Episodic logic: A situational logic 
for NLP. In Situation Theory and Its Applications, 
pp. 303–337. 

Schubert, L. and C.Hwang (1993). Episodic logic: A 
situational logic for NLP. In Peter Aczel, David 
Israel, Yasuhiro Katagiri, and Stanley Peters, (Eds.), 
Situation Theory and its Applications, vol.3, 303-
337. 

Tonelli, S. & R. Delmonte, 2011. "Desperately seeking 
Implicit arguments in text", in RELMS'2011, 
Workshop on Relational Models of Semantics at 
ACL 2011 Portland, USA. pp.54-62. 

At:http://web.media.mit.edu/~push/ConceptNet.pdf. 

 

10


