757

Coling 2010: Poster Volume, pages 757–765,

Beijing, August 2010

Dependency-Driven Feature-based Learning for Extracting 

Protein-Protein Interactions from Biomedical Text 

Bing Liu      Longhua Qian(cid:13)      Hongling Wang      Guodong Zhou 

Jiangsu Provincial Key Lab for Computer Information Processing Technology 

School of Computer Science and Technology 
Email: liubingnlp@gmail.com 

Soochow University 

{qianlonghua,redleaf,gdzhou}@suda.edu.cn

Abstract

Recent  kernel-based  PPI  extraction 
systems  achieve  promising  perform-
ance  because  of  their  capability  to 
capture  structural  syntactic  informa-
tion,  but  at  the  expense  of  computa-
tional complexity. This paper incorpo-
rates  dependency  information  as  well 
as  other  lexical  and  syntactic  knowl-
edge  in  a  feature-based  framework. 
Our motivation is that, considering the 
large  amount  of  biomedical  literature 
being  archived  daily,  feature-based 
methods with comparable performance 
are more suitable for practical applica-
tions.  Additionally,  we  explore  the 
difference of lexical characteristics be-
tween  biomedical  and  newswire  do-
mains. Experimental evaluation on the 
AIMed  corpus  shows  that  our  system 
achieves  comparable  performance  of 
54.7 
other 
state-of-the-art PPI extraction systems, 
yet the best performance among all the 
feature-based ones.   

F1-Score  with 

in 

Introduction 

1
In  recent  years,  automatically  extracting 
biomedical information has been the subject of 
significant  research  efforts  due  to  the  rapid 
in  biomedical  development  and 
growth 
discovery.  A  wide  concern 
to 
characterize protein interaction partners since 
it 
is  crucial  to  understand  not  only  the 
functional role of individual proteins but also 
                                                          
(cid:13) Corresponding author 

is  how 

the  organization  of 
the  entire  biological 
process.  However,  manual  collection  of 
relevant  Protein-Protein 
(PPI) 
information from thousands of research papers 
published every day is so time-consuming that 
automatic extraction approaches with the help 
of  Natural  Language  Processing 
(NLP) 
techniques become necessary.   

Interaction 

Various  machine  learning  approaches for 
relation  extraction  have  been  applied  to  the 
biomedical domain,  which  can  be  classified 
into  two  categories:  feature-based  methods 
(Mitsumori et al., 2006; Giuliano et al., 2006; 
Sætre et al., 2007) and kernel-based methods 
(Bunescu  et  al.,  2005;  Erkan  et  al.,  2007; 
Airola et al., 2008; Kim et al., 2010). 

Provided  a  large-scale  manually  annotated 
corpus,  the  task  of  PPI  extraction  can  be 
formulated  as  a  classification  problem. 
Typically,  for  featured-based  learning  each 
protein pair is represented as a vector  whose 
features  are  extracted  from 
the  sentence 
involving  two  protein  names.  Early  studies 
identify  the  existence  of  protein  interactions 
by  using  “bag-of-words”  features  (usually 
uni-gram  or  bi-gram)  around  the  protein 
names  as  well  as  various  kinds  of  shallow 
linguistic  information,  such  as  POS  tag, 
lemma and orthographical features. However, 
these systems do not achieve promising results 
since they disregard any syntactic or semantic 
information  altogether, which are very useful 
for  the  task  of  relation  extraction  in  the 
newswire domain (Zhao and Grishman, 2005; 
Zhou et al., 2005). Furthermore, feature-based 
methods 
the 
structural  information,  which  is  essential  to 

to  effectively  capture 

fail 

758

identify the relationship between two proteins 
in a syntactic representation. 

With  the  wide  application  of  kernel-based 
methods  to  many  NLP  tasks,  various kernels 
such  as  subsequence  kernels  (Bunescu  and 
Mooney,  2005)  and  tree  kernels  (Li  et  al., 
2008),  are  also  applied  to  PPI  detection.. 
Particularly,  dependency-based  kernels  such 
as  edit  distance  kernels  (Erkan  et  al.,  2007) 
and graph kernels (Airola et al., 2008; Kim et 
al., 2010) show some promising results for PPI 
extraction.  This  suggests  that  dependency 
information  play  a  critical  role 
in  PPI 
extraction  as  well  as  in  relation  extraction 
from newswire stories (Culotta and Sorensen, 
2004). In order to appreciate the advantages of 
both  feature-based  methods  and  kernel-based 
methods,  composite  kernels  (Miyao  et  al., 
2008; Miwa et al., 2009a; Miwa et al., 2009b) 
are  further  employed  to  combine  structural 
syntactic  information  with  flat  word  features 
and significantly improve the performance of 
PPI 
critical 
challenge  for  kernel-based  methods  is  their 
computation complexity, which prevents them 
from  being  widely  deployed  in  real-world 
applications  regarding  the  large  amount  of 
biomedical literature being archived everyday.   
Considering the potential of dependency in-
formation for PPI extraction and the challenge 
of  computation  complexity  of  kernel-based 
methods, one may naturally ask the question: 
“Can the essential dependency information be 
maximally  exploited  in  featured-based  PPI 
extraction  so  as  to  enhance  the  performance 
without loss of efficiency?” “If the answer is 
Yes, then How?” 

extraction.  However,  one 

This paper addresses these problems, focus-
ing on the application of dependency informa-
tion  to  feature-based  PPI  extraction.  Starting 
from  a  baseline  system  in  which  common 
lexical and syntactic features are incorporated 
using  Support  Vector  Machines  (SVM),  we 
further augment the baseline with various fea-
tures  related 
information, 
including  predicates  in  the  dependency  tree. 
Moreover,  in  order  to  reveal  the  linguistic 
difference  between  distinct  domains  we also 
compare the effects of various features on PPI 
extraction from biomedical texts with those on 
relation  extraction  from  newswire  narratives. 
Evaluation on the AIMed and  other  PPI  cor-

to  dependency 

pora shows that our method achieves the  best 
performance among all feature-based systems. 
The rest of the paper is organized as follows. 
A feature-based PPI extraction baseline system 
is given in Section 2 while Section 3 describes 
our dependency-driven method. We report our 
experiments  in  Section  4,  and  compare  our 
work  with  the  related  ones  in  Section  5.   
Section 6 concludes this paper and gives some 
future directions. 

2

Feature-based  PPI 
Baseline

extraction: 

PPI 

instances 

For feature-based methods, PPI extraction task 
is  re-cast  as  a  classification  problem  by  first 
transforming 
into 
multi-dimensional  vectors  with  various  fea-
tures, and then applying machine learning ap-
proaches  to  detect  whether  the  potential 
relationship exists for a particular protein pair. 
In training, a  feature-based classifier learning 
algorithm, such as SVM or MaxEnt, uses the 
annotated  PPI  instances  to  learn  a  classifier 
while, in testing, the learnt classifier is in  turn 
applied to new instances to determine their PPI 
binary classes and thus candidate PPI instances 
are extracted. 

As  a  baseline,  various  linguistic  features, 
such as words, overlap, chunks, parse tree fea-
tures as  well  as  their  combined  ones  are  ex-
tracted from a sentence and formed as a vector 
into the feature-based learner. 
1) Words 
Four sets of word features are used in our sys-
tem: 1) the words of both the proteins; 2) the 
words between the two proteins; 3) the words 
before M1 (the 1st protein); and 4) the words 
after M2 (the 2nd protein). Both the words be-
fore M1 and after M2 are classified into two 
bins: the first word next to the proteins and the 
second word next to the proteins. This means 
that we only consider the two words before M1 
and after M2. Words features include: 

(cid:120) MW1: bag-of-words in M1 
(cid:120) MW2: bag-of-words in M2 
(cid:120) BWNULL: when no word in between 
(cid:120) BWO:  other  words  in  between  except 
first and last words when at least three 
words in between 

(cid:120) BWM1FL: the only word before M1 

759

(cid:120) BWM1F: first word before M1 
(cid:120) BWM1L: second word before M1 
(cid:120) BWM1:  first  and  second  word  before 

M1

(cid:120) BWM2FL: the only word after M2 
(cid:120) BWM2F: first word after M2 
(cid:120) BWM2L: second word after M2 
(cid:120) BWM2: first and second word after M2 

2) Overlap 
The numbers of other protein names as well as 
the  words  that  appear  between  two  protein 
names  are  included  in  the  overlap  features. 
This category of features includes: 

(cid:120) #MB:  number  of  other  proteins  in  be-

tween

(cid:120) #WB: number of words in between 
(cid:120) E-Flag: flag indicating whether the two 

proteins are embedded or not 

3) Chunks
It  is  well  known  that  chunking  plays  an 
important role in the task of relation extraction 
in the ACE program (Zhou et al., 2005). How-
ever, its significance in PPI extraction has not 
fully  investigated.  Here, the Stanford  Parser1
is  first  employed  for  full  parsing,  and  then 
base phrase chunks are derived from full parse 
trees using the Perl script2. The chunking fea-
tures usually concern about the head words of 
the  phrases  between  the  two  proteins,  which 
are further  classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between. In 
addition, the path of phrasal labels connecting 
two  proteins  is  also  a  common  syntactic 
indicator  of  the  polarity  of  the  PPI  instance, 
just  as  the  path  NP_VP_PP_NP  in  the  sen-
tence  “The ability of PROT1 to interact with 
the PROT2 was investigated.” is likely to sug-
gest the positive interaction between two pro-
teins.  These  base  phrase  chunking  features 
contain:

(cid:120) CPHBNULL:  when  no  phrase  in  be-

tween.

(cid:120) CPHBFL:  the  only  phrase  head  when 

only one phrase in between 

(cid:120) CPHBF: the first phrase head in between 

when at least two phrases in between. 

                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
2 http://ilk.kub.nl/~sabine/chunklink/ 

(cid:120) CPHBL: the last phrase head in between 
when  at  least  two  phrase  heads  in  be-
tween.

(cid:120) CPHBO: other phrase heads in between 
except first and last phrase heads when 
at least three phrases in between. 

(cid:120) CPP:  path  of  phrase  labels  connecting 

the two entities in the chunking 

Furthermore,  we  also  generate  a  set  of 
bi-gram  features  which  combine  the  above 
chunk  features  except  CPP  with  their  corre-
sponding chunk types.   
4) Parse Tree 
It  is  obvious  that  full  pares  trees  encompass 
rich  structural  information  of  a  sentence. 
Nevertheless,  it  is  much  harder  to  explore 
such  information  in  featured-based  methods 
than  in  kernel-based  ones.  Thus  so  far  only 
the path connecting two protein names in the 
full-parse  tree  is  considered  as  a  parse  tree 
feature.

(cid:120) PTP:  the  path  connecting  two  protein 

names in the full-parse tree. 

and 

PROT2 

PROT1 

  Again,  take  the  sentence  “The  ability  of 
PROT1  to  interact  with  the  PROT2  was 
investigated.”  as  an  example,  the  parse  path 
between 
is 
NP_S_VP_PP_NP, which is slightly different 
from the CPP feature in the chunking feature 
set.
3 Dependency-Driven PPI Extraction 
The  potential of  dependency  information  for 
PPI extraction lies in the fact that the depend-
ency 
tree  may  well  reveal  non-local  or 
long-range  dependencies  between  the  words 
within a  sentence.  In  order  to  capture  the 
necessary 
the 
depedency 
their 
relationship,  various  kernels,  such  as  edit 
distance  kernel  based  on  dependency  path 
(Erkan  et  al.,  2007),  all-dependency-path 
graph  kernel  (Airola  et  al.,  2008),  and 
walk-weighted  subsequence  kernels  (Kim  et 
al.,  2010)  as well  as  other  composite  kernels 
(Miyao et al., 2008; Miwa et al., 2009a; Miwa 
et al., 2009b), have been proposed to address 
this  problem.  It’s  true  that  these  methods 
achieve encouraging results, neverthless, they 
suffer  from  prohibitive  computation  burden. 

information 
for 

inherent 
identifying 

tree 

in 

760

into 

information  back 

Thus,  our  solution  is  to  fold  the  structural 
dependency 
flat 
features in a feature-based framework so as to 
speed up the learning process while retaining 
comparable  performance.  This  is  what  we 
refer to as dependency-driven PPI extraction. 
  First,  we construct dependency trees from 
grammatical  relations  generated  by  the  Stan-
ford Parser. Every grammatical relation has the 
form  of  dependent-type (word1, word2),
Where word1 is the head word, word2 is de-
pendent on word1, and dependent-type denotes 
the  pre-defined  type  of  dependency.  Then, 
from these grammatical relations the following 
features  called  DependenecySet1  are  taken 
into consideration as illustrated in Figure 1: 

(cid:120) DP1TR:  a  list  of  words  connecting 

PROT1 and the dependency tree root. 

(cid:120) DP2TR:  a  list  of  words  connecting 

PROT2 and the dependency tree root. 

(cid:120) DP12DT:  a  list  of  dependency  types 
connecting  the  two  proteins  in  the 
dependency tree. 

(cid:120) DP12:  a  list  of  dependent  words  com-
bined with their dependency types con-
necting the two proteins in the depend-
ency tree. 

(cid:120) DP12S:  the  tuple  of  every  word  com-
bined with its dependent type in DP12. 
(cid:120) DPFLAG:  a  boolean  value  indicating 
whether  the  two  proteins  are  directly 
dependent on each other. 

The  typed  dependencies  produced  by  the 
Stanford  Parser  for  the  sentence  “PROT1 
contains  a  sequence  motif  binds  to  PROT2.” 
are listed as follows: 

nsubj(contains-2,PROT1-1)
det(motif-5, a-3) 
nn(motif-5, sequence-4) 
nsubj(binds-6, motif-5) 
ccomp(contains-2, binds-6) 
prep_to(binds-6, PROT2-8) 
Each  word  in  a  dependency  tuple  is  fol-
lowed  by  its  index  in  the  original  sentence, 
ensuring  accurate  positioning  of  the  head 
word and dependent word. Figure 1 shows the 
dependency tree we construct from the above 
grammatical relations.   

contains 

nsubj 

PROT1 

ccomp 

binds 

nsubj 

motif 

prep_to

PROT2

det 

nn 

a

sequence 

Figure  1:  Dependency  tree  for  the  sentence 
“PROT1  contains  a  sequence  motif  binds  to 
PROT2.” 

report  promising 

Erkan  et  al.  (2007)  extract 

the  path 
information  between  PROT1  and  PROT2  in 
the  dependency  tree  for  kernel-based  PPI 
extraction  and 
results, 
neverthless,  such  path  is  so  specific  for 
feature-based  methods  that  it  may  incure 
higher  precision  but  lower  recall.  Thus  we 
alleviate this problem by collapsing the feature 
into  multiple  ones  with  finer  granularity, 
leading to the features such as DP12S. 

It  is  widely  acknowledged  that  predicates 
play  an  important  role  in  PPI  extraction.  For 
example,  the  change  of  a  pivot  predicate 
between  two  proteins  may  easily  lead  to  the 
polarity reversal of a PPI instance. Therefore, 
we extract the predicates and their positions in 
the  dependency  tree  as  predicate  features 
called DependencySet2:   

(cid:120) FVW: the predicates in the DP12 feature 

occurring prior to the first protein. 

(cid:120) LVW: the predicates in the DP12 feature 

occurring next to the second entity. 

(cid:120) MVW:  other  predicates  in  the  DP12 

features. 

(cid:120) #FVW: the number of FVW 
(cid:120) #LVW: the number of LVW 
(cid:120) #MVW: the number of MVW 

4 Experimentation
This  section systematically evaluates our fea-
ture-based  method  on  the  AIMed  corpus  as 
well  as  other  commonly  used  corpus  and re-
ports our experimental results. 

761

4.1 Data Sets 
We use five corpora3 with the AIMed corpus 
as the main experimental data, which contains 
177  Medline  abstracts  with  interactions  be-
tween two interactions, and 48 abstracts with-
out any PPI within single sentences. There are 
4,084  protein  references  and  around  1,000 
annotated interactions in this data set.   

For corpus pre-procession, we first rename 
two proteins of a pair as PROT1 and PROT2 
respectively in  order  to  blind  the  learner  for 
fair  comparison  with  other  work.    Then,  all 
the instances are generated from the sentences 
which contain at least two proteins,    that is, if 
a sentence contains n different proteins, there 
are (cid:11)n
2 (cid:12)different  pairs  of  proteins  and  these 
pairs  are  considered  untyped  and  undirected. 
For the purpose of comparison with previous 
work,  all  the  self-interactions  (59  instances) 
are removed,  while  all the PPI instances with 
nested  protein  names  are  retained  (154  in-
stances).  Finally,  1002 positive instances and 
4794  negative  instances  are  generated  and 
their corresponding features are extracted.   

We select Support Vector Machines (SVM) 
as  the  classifier  since  SVM  represents  the 
state-of-the-art  in  the  machine  learning  re-
search  community.  In  particular,  we  use  the 
binary-class  SVMLigh 4
developed  by 
Joachims (1998) since it satisfies our require-
ment of detecting potential PPI instances. 

Evaluation  is  done  using  10-fold  docu-
ment-level  cross-validation.  Particularly,  we 
apply  the  extract  same  10-fold  split  that  was 
used by Bunescu et al. (2005) and Giuliano et 
al. (2006). Furthermore,  OAOD (One Answer 
per  Occurrence  in  the  Document)  strategy  is 
adopted, which means that the correct interac-
tion  must  be  extracted  for  each  occurrence. 
This guarantees the maximal use of the avail-
able  data,  and  more  important,  allows  fair 
comparison with earlier relevant work.   

The evaluation metrics are commonly used 
Precision  (P),  Recall  (R)  and  harmonic 
F1-score  (F1).  As  an  alternative  to  F1-score, 
the  AUC  (area  under  the  receiver  operating 
characteristics curve) measure is proved to be 
invariant to the class distribution of the train-
ing dataset. Thus we also provide AUC scores 
                                                          
3  http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 
4  http://svmlight.joachims.org/

F1 

P(%) R(%) 

for  our  system  as  Airola  et  al.  (2008)  and 
Miwa et al. (2009a). 
4.2 Results and Discussion 
Features
Baseline features 
59.4
Words
60.4
+Overlap
59.2
+Chunk
+Parse
60.9
Dependency-driven features 
62.9
+Dependency Set1 
53.9
63.4
54.7
+Dependency Set2 
Table 1: Performance of PPI extraction with vari-
ous features in the AIMed corpus 

40.6
39.9
44.5
44.8

47.6
47.4
50.6
51.4

48.0
48.8

We present in Table 1 the performance of our 
system  using  document-wise 
evaluation 
strategies  and  10-fold  cross-validation  with 
different features in the AIMed corpus, where 
the  plus  sign  before  a  feature  means  it  is 
incrementally added to the feature set. Table 1 
reports that  our system achieves the best per-
formance of 63.4/48.8/54.7 in P/R/F scores. It 
also shows that: 
(cid:120) Words features alone achieve a relatively 
low  performance  of  59.4/40.9/47.6  in 
P/R/F,  particularly  with  fairly  low  recall 
score.  This  suggests  the  difficulty  of  PPI 
extraction and words features alone can’t 
effectively  capture  the  nature  of  protein 
interactions.

(cid:120) Overlap features slightly decrease the per-
formance.  Statistics  show  that  both  the 
distributions  of  #MB  and  #WB  between 
positives and negatives are so similar that 
they are by no means the discriminators for 
PPI  extraction.  Hence,  we  exclude  the 
overlap features in the succeeding experi-
ments.

(cid:120) Chunk features significantly improves the 
F-measure by 3 units largely due to the in-
crease  of  recall  by  3.9%,  though  at  the 
slight expense of precision. This suggests 
the effectiveness of shallow parsing infor-
mation in the form of headwords captured 
by chunking on PPI extraction.   

(cid:120) The usefulness of the parse tree features is 
quite 
the 
F-measure by 0.8 units. The main reason 
may  be  that  these  paths  are  usually  long 

limited.  It  only 

improves 

762

and  specific,  thus  they  suffer  from  the 
problem  of  data  sparsity.  Furthermore, 
some of the parse tree features are already 
involved in the chunk features.   

(cid:120) The  DependencySet1  features  are  very 
effective in that it can increase the preci-
sion  and  recall  by  2.0  and  3.2  units 
respectively, leading to the increase of F1 
score by 2.5 units. This means that the de-
pendency-related  features  can  effectively 
retrieve more PPI instances without intro-
ducing  noise  that  will  severely  harm  the 
precision. According to our statistics, there 
are over 60% sentences with more than 5 
words between their protein entities in the 
AIMed corpus. Therefore, dependency in-
formation  exhibit  great  potential  to  PPI 
extraction 
capture 
long-range dependencies within sentences. 
Take 
sentence 
“PROT1 contains a sequence motif binds 
to  PROT2.”  as  an  example,  although  the 
two  proteins  step  over  a  relatively  long 
distance,  the  dependency  path  between 
them is concise and accurate, reflecting the 
essence of the interaction. 

aforementioned 

since 

they 

can 

the 

(cid:120) The  predicate  features  also  contribute  to 
the  F1-score  gain  of  0.8  units.  It  is  not 
surprising  since  some  predicates,  such  as 
“interact”, “activate” and “inhibit” etc, are 
strongly  suggestive  of  the  interaction 
polarity between two proteins. 
We compare in Table 2 the performance of 
our  system  with  other  systems  in  the  AIMed 
corpus using the same 10-fold cross validation 
strategy. These systems are grouped into three 
distinct  classes:  feature-based,  kernel-based 
and composite kernels. Except for Airola et al. 
(2008)  Miwa  et  al.  (2009a)  and  Kim  et  al. 
(2010), which adopt graph kernels, our system 
performs  comparably  with  other  systems.  In 
particular,  our  dependency-driven  system 
achieves the best F1-score of 54.7 among all 
feature-based systems. 

In order to measure the generalization abil-
ity  of  our  dependency-driven  PPI  extraction 
system  across  different  corpora,  we  further 
apply our method to other four publicly avail-
able  PPI  corpora:  BioInfer,  HPRD50,  IEPA 
and LLL.   

F1

P(%) R(%)

54.7
59.0
52.0
47.7
33.4

48.8
57.2
44.1
42.6
33.1

63.4
60.9
64.3
54.2
33.7

Systems 
Feature-based methods 
Our system 
Giuliano et al., 20065
Sætre et al., 2007 
Mitsumori et al., 2006 
Yakushiji et al., 2005 
Kernel-based methods 
Kim et al., 2010 
Airola et al., 2008 
Bunescu et al., 2006   
Composite kernels 
62.0
Miwa et al., 2009a 
Miyao et al., 20086
54.5
Table  2:  Comparison  with  other  PPI  extraction 
systems in the AIMed corpus 

61.4
52.9
65.0

53.3
61.8
46.4

56.7
56.4
54.2

51.8

58.1

-

-

The 

performance 

corresponding 

of 
F1-score  and  AUC  metrics  as  well  as  their 
standard  deviations  is  present  in  Table  3.   
Comparative  available  results  from  Airola  et 
al.  (2008)  and  Miwa  et  al.  (2009a)  are  also 
included in Table 3 for comparison. This table 
shows 
that  our  system  performs  almost 
consistently with the other two systems, that is, 
the LLL corpus gets the best performance yet 
with  the  greatest  variation,  while  the  AIMed 
corpus  achieves  the  lowest  performance  with 
reasonable variation. 

It  is  well  known  that  biomedical  texts  ex-
hibit  distinct  linguistic  characteristics  from 
newswire narratives, leading to dramatic per-
formance  gap  between  PPI  extraction  and 
relation  detection  in  the  ACE  corpora.  How-
ever, no previous work has ever addressed this 
problem  and  empirically  characterized  this 
difference. In this paper, we devise a series of 
experiments over the ACE RDC corpora using 
our  dependency-driven  feature-based  method 
as a touchstone task. In order to do that, a sub-
                                                          
5  Airola et al. (2008) repeat the method published by 
Giuliano et al. (2006) with a correctly preprocessed 
AIMed and reported an F1-score of 52.4%. 
6  The results from Table 1 (Miyao et al., 2009) with the 
most similar settings to ours (Stanford Parser with SD 
representation) are reported. 

763

set  of  5796  relation  instances  is  randomly 
sampled  from  the  ACE  2003  and  2004  cor-
pora respectively.    The same cross-validation 

and  evaluation  metrics  are  applied  to  these 
two sets as PPI extraction in the AIMed cor-
pus.

Corpus
AIMed
BioInfer
HPRD50
IEPA
LLL

Our system 
(cid:305)F1 AUC (cid:305)AUC
3.5
3.3
8.5
6.6
8.3

F1
82.4
54.7  4.5
59.8  3.5
80.9
64.9  13.4  79.8
62.1  6.2
74.8
85.1
78.1 15.8

Airola et al. (2008) 7

F1
(cid:305)F1
5.0
56.4
61.3
5.3
63.4 11.4
75.1
7.0
76.8 17.8

AUC (cid:305)AUC
2.3
84.8
6.5
81.9
6.3
79.7
85.1
5.1
12.2
83.4

Miwa et al. (2009a) 

F1
86.8
60.8
68.1
85.9
70.9 10.3  82.2
71.7
84.4
7.8 
86.3
80.1 14.1

(cid:305)F1 AUC (cid:305)AUC
3.3
6.6 
4.4
3.2 
6.3
4.2
10.8

Table 3: Comparison of performance across the five PPI corpora 

ACE2003 

Features
Words
+Overlap
+Chunk
+Parse
+Dependency Set1 
+Dependency Set2 

F1 
63.4
+2.7
+1.7
+0.5
+0.7
+0.3
Table 4: Comparison of contributions of different features to relation detection across multiple domains 

P(%)  R(%)
59.6
68.1
+1.2
+4.6
+1.5 
+1.9 
+0.4
+0.6
+0.9
+0.5
+0.2
+0.4

R(%)
51.6
+1.8
+5.1 
+0.6
+0.7
+0.2

P(%)
59.4
+1.0
-1.7
+1.7
+2.0
+0.5

P(%)
66.5
+5.4
+2.3
+0.3
+0.8
+0.3

F1 
47.6
-0.2
+3.2
+0.8
+2.5
+0.8

F1 
57.9
+3.2
+4.0
+0.5
+0.7
+0.3

AIMed 
R(%)
40.6
-0.7
+4.6
+0.3
+3.2
+0.8

ACE2004 

Table  4  compares  the  performance  of  our 
method  over  different  domains.  The  table  re-
ports that the words features alone achieve the 
best F1-score of 63.4 in ACE2004 but the low-
est  F1-score  of  47.6  in  AIMed.  This  suggests 
the  wide  difference  of  lexical  distribution  be-
tween  these  domains.  We  extract  the  words 
appearing before the 1st mention, between the 
two mentions and after the 2nd mention from 
the training sets of these corpora respectively, 
and  summarize  the  statistics  (the  number  of 
tokens, the number of occurrences) in Table 5, 
where  the  KL  divergence  between  positives 
and negatives is summed over the distribution 
of the 500 most frequently occurring words. 
Statistics 
# of tokens 
# of occurrences 
KL divergence   

AIMed  ACE2003 ACE2004
2,340 
2,099(cid:3)
49,570
69,976 
0.22 
0.33 

2,064 
53,744 
0.28 

Table 5: Lexical statistics on three corpora 

In 

alone 

by  words 

and  negatives,  as  indicated  by  its  least  KL 
divergence.  Therefore,  the  lexical  words  in 
AIMed  are  less  discriminative  for  relation 
detection  than  they  do  in  the  other  two.  This 
naturally explains the reason why the perform-
ance 
feature 
is 
addition, 
AIMed<ACE2003<ACE2004. 
Table 4 also shows that: 
(cid:120) The overlap features significantly improve 
the  performance  in  ACE  while  slightly 
deteriorating that in AIMed. The reason is 
that, as indicated in Zhou et al. (2005), most 
of  the  positive  relation  instances  in  ACE 
exist  in  local  contexts,  while  the  positive 
interactions  in  AIMed  occur  in  relative 
long-range just as the negatives, therefore 
these  features  are  not  discriminative  for 
AIMed.

(cid:120) The  chunk  features  consistently  greatly 
boost the performance across multiple cor-
pora.  This  implies  that  the  headwords  in 
chunk phrases can well capture the partial 
nature  of  relation  instances  regardless  of 
their genre. 
It’s  not  surprising  that  the  parse  feature 
attain moderate performance gain in all do-
mains  since  these  parse  paths  are  usually 

The  table  shows  that  AIMed  uses  the  most 
kinds of words and the most words around the 
two mentions than the other two. More impor-
tant,  AIMed  has  the  least  distribution  differ-
ence between the words appearing in positives 
                                                          
7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et 
al. (2009b) 

(cid:120)

764

(cid:120)

leading 

to  data 

long  and  specificity, 
sparseness problem. 
It  is  interesting  to  note  that  the  depend-
ency-related features  exhibit  more  signifi-
cant  improvement  in  AIMed  than  that  in 
ACE.  The  reason  may  be  that,  these 
dependency  features  can  effectively  cap-
ture  long-range  relationships  prevailing  in 
AIMed,  while  in  ACE  a  large  number  of 
local relationships dominate the corpora. 

5 Related Work 
Among  feature-based  methods,  the  PreBIND 
system (Donaldson et al., 2003) uses words and 
word bi-grams features to identify the existence 
of  protein  interactions  in  abstracts  and  such 
information is used to enhance manual expert 
reviewing for the BIND database. Mitsumori et 
al.  (2006)  use  SVM  to  extract  protein-protein 
interactions, where bag-of-words features, spe-
cifically  the  words  around  the  protein  names, 
are  employed.  Sugiyama  et  al.  (2003)  extract 
various  features  from  the  sentences  based  on 
the verbs and nouns in the sentences such as the 
verbal forms, and the part-of-speech tags of the 
20 words surrounding the verb. In addition to 
word  features,  Giuliano  et  al.  (2006)  extract 
shallow linguistic information such as POS tag, 
lemma, and orthographic features of tokens for 
PPI extraction.  Unlike  our  dependency-driven 
method,  these  systems  do  not  consider  any 
syntactic information.   

For kernel-based methods, there are several 
systems which utilize dependency information. 
Erkan et al. (2007) defines similarity functions 
based  on  cosine  similarity  and  edit  distance 
between  dependency  paths,  and  then  incorpo-
rate  them  in  SVM  and  KNN  learning  for  PPI 
extraction.  Airola  et  al.  (2008) 
introduce 
all-dependency-paths  graph  kernel  to  capture 
the complex dependency relationships between 
lexical  words  and  attain  significant  perform-
ance  boost  at  the  expense  of  computational 
complexity.  Kim 
adopt 
walk-weighted  subsequence  kernel  based  on 
dependency paths to explore various substruc-
tures  such  as  e-walks,  partial  match,  and 
non-contiguous paths. Essentially, their kernel 
is also a graph-based one. 

(2010) 

For  composite  kernel  methods, Sætre et al. 
(2007) combine a “bag-of-words” kernel with 

al. 

et 

representations 

dependency  and  PAS  (Predicate  Argument 
Structure) tree kernels to exploit both the words 
features and the structural syntactic information. 
Hereafter, Miyao  et  al.  (2008)  investigate  the 
contribution of various syntactic features using 
different 
from  dependency 
parsing,  phrase  structure  parsing  and  deep 
parsing  by  different  parsers.  Miwa  et  al. 
(2009a)  integrate  “bag-of-words”  kernel,  PAS 
tree  kernel  and  all-dependency-paths  graph 
kernel to achieve the higher performance. They 
(Miwa et al., 2009b) also use similar compos-
ite  kernels  for  corpus  weighting  learning 
across multiple PPI corpora.   
6 Conclusion and Future Work 
In this paper, we have combined various lexical 
and syntactic features, particularly dependency 
information, into a feature-based PPI extraction 
system. We find that the  dependency informa-
tion as  well  as the  chunk features contributes 
most  to  the  performance  improvement.    The 
predicate features involved in the dependency 
tree can also moderately enhance the perform-
ance. Furthermore, comparative study between 
biomedical  domain  and  the  ACE  newswire 
domain  shows  that  these  domains  exhibit 
different  lexical  characteristics,  rendering  the 
task of PPI extraction much more difficult than 
that  of  relation  detection  from  the  ACE  cor-
pora.

In future work, we will explore more syntac-
tic  features  such  as  PAS  information  for fea-
ture-based  PPI extraction to  further  boost  the 
performance. 

research 

Acknowledgment 
This 
is  supported  by  Projects 
60873150  and  60970056  under  the  National 
Natural Science Foundation of China and Pro-
ject  BK2008160  under  the  Natural  Science 
Foundation of Jiangsu, China. We are also very 
grateful  to  Dr.  Antti  Airola  from  Truku 
University  for  providing  partial  experimental 
materials. 

References 
A.  Airola,  S.  Pyysalo,  J.  Björne,  T.  Pahikkala,  F. 
Ginter, and T. Salakoski. 2008. All-paths graph 
kernel  for  protein-protein  interaction  extraction 

765

with  evaluation  of  cross  corpus  learning.  BMC
Bioinformatics.

R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, 
A.  Ramani,  and  Y.  Wong.  2005.  Comparative 
Experiments  on  learning  information  extractors 
for  Proteins  and  their  interactions.  Journal  of 
Artiﬁcial Intelligence In Medicine, 33(2).   

R.  Bunescu  and  R.  Mooney.  2005.  Subsequence 
kernels for relation extraction. In Proceedings of   
NIPS’05, pages 171–178. 

A.  Culotta  and  J.  Sorensen.  2004.    Dependency 
In 

for  Relation  Extraction. 

Tree  Kernels 
Proceedings of ACL’04.

I. Donaldson, J. Martin, B. de Bruijn, C. Wolting, V. 
Lay,  B.  Tuekam,  S.  Zhang,  B.  Baskin,  G.  D. 
Bader, K. Michalockova, T. Pawson, and C. W. V. 
Hogue. 2003. Prebind and textomy - mining the 
biomedical 
protein-protein 
interactions  using  a  support  vector  machine. 
Journal of BMC Bioinformatics, 4(11). 

literature 

for 

G.  Erkan,  A.  Özgür,  and  D.R.  Radev.  2007. 
Semi-Supervised  Classification  for  Extracting 
Protein Interaction Sentences using Dependency 
Parsing,  In  Proceedings  of  EMNLP-CoNLL’07,
pages 228–237. 

Y.  Miyao,  R.  Sætre,  K.  Sagae,  T.  Matsuzaki,  and 
J.Tsujii.  2008.  Task-oriented  evaluation  of 
syntactic  parsers  and  their  representations.  In 
Proceedings of ACL’08, pages 46–54. 

T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi. 
2001.  Automated  extraction  of  information  on 
protein-protein  interactions  from  the  biological 
literature. Journal of Bioinformatics, 17(2). 

  K.  Sugiyama,  K.  Hatano,  M.  Yoshikawa,  and  S. 
information  on 
Uemura.  2003.  Extracting 
protein-protein 
from  biological 
interactions 
literature based on machine learning approaches. 
Journal of Genome Informatics, (14): 699–700. 

R.  Sætre,  K.  Sagae,  and  J.  Tsujii.  2007.  Syntactic 
features for protein-protein interaction extraction. 
In Proceedings of LBM’07, pages 6.1–6.14. 

A.  Yakushiji,  M.  Yusuke,  T.  Ohta,  Y.  Tateishi,  J. 
construction  of 
for 
In 

Tsujii.  2006.  Automatic 
patterns 
predicate-argument 
biomedical 
extraction. 
Proceedings of EMNLP’06, pages 284–292. 

information 

structure 

S.B.  Zhao  and  R.  Grishman.  2005.  Extracting 
Relations  with  Integrated  Information  Using 
Kernel  Methods.  In  Proceedings  of  ACL’05,
pages 419-426.   

C.  Giuliano,  A.  Lavelli,  and  L.  Romano.  2006. 
Exploiting  Shallow  Linguistic  Information  for 
Relation Extraction from Biomedical Literature. 
In Proceedings of EACL’06, pages 401–408. 

G.D.  Zhou,  J.  Su,  J.  Zhang,  and  M.  Zhang.  2005. 
Exploring  various  knowledge 
relation 
extraction.    In  Proceedings  of  ACL’05,  pages 
427-434.   

in 

  S.  Kim,  J.  Yoon,  J.  Yang,  and  S.  Park.  2010. 
Walk-weighted 
for 
protein-protein interaction extraction. Journal of 
BMC Bioinformatics, 11(107). 

subsequence 

kernels 

J.  Li,  Z.  Zhang,  X.  Li,  and  H.  Chen.  2008. 
Kernel-Based Learning for Biomedical Relation 
extraction.  Journal  of  the  American  Society  for 
Information Science and Technology, 59(5). 

T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. 
Doi. 2006. Extracting protein-protein interaction 
information  from  biomedical  text  with  SVM. 
IEICE Transactions on Information and System, 
E89-D (8).

M. Miwa, R. Sætre, Y. Miyao, and J. Tsujii. 2009a. 
Protein-Protein 
by 
Leveraging  Multiple  Kernels  and  Parsers. 
Journal of Medical Informatics, 78(2009). 

Interaction  Extraction 

M. Miwa, R. Sætre, Y. Miyao, and J. Tsujii. 2009b.   
A  Rich  Feature  Vector  for  Protein-Protein 
Interaction Extraction from Multiple Corpora. In 
Proceedings of EMNLP’09, pages 121–130.   

