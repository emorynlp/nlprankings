



















































Computational Exploration to Linguistic Structures of Future: Classification and Categorization


Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 168–173,
Denver, Colorado, June 1, 2015. c©2015 Association for Computational Linguistics

Computational Exploration of the Linguistic Structures of Future-Oriented
Expression: Classification and Categorization

Aiming Nie1,2, Jason Shepard2, Jinho Choi1, Bridget Copley3, Phillip Wolff2
1Dept. of Computer Science, Emory University, 2Dept. of Psychology, Emory University

3Structures Formelles du Language, 4CNRS / Universite Paris 8, Paris, France 30322
{anie, jason.s.shepard, jinho.choi, pwolff}@emory.edu, bridget.copley@sfl.cnrs.fr

Abstract

English, like many languages, uses a wide va-
riety of ways to talk about the future, which
makes the automatic identification of future
reference a challenge. In this research we ex-
tend Latent Dirichlet allocation (LDA) for use
in the identification of future-referring sen-
tences. Building off a set of hand-designed
rules, we trained a ADAGRAD classifier to be
able to automatically detect sentences refer-
ring to the future. Uni-bi-trigram and syntac-
tic rule mixed feature was found to provide the
highest accuracy. Latent Dirichlet Allocation
(LDA) indicated the existence of four major
categories of future orientation. Lastly, the re-
sults of these analyses were found to correlate
with a range of behavioral measures, offering
evidence in support of the psychological real-
ity of the categories.

1 Introduction

Early formal work on tense such as (Prior, 1967)
treated tenses as logical operators; this approach,
however, could not correctly account for com-
plex tenses, and was superseded by relational ac-
counts (Reichenbach, 1947; Hornstein, 1990; Klein,
1997). However, these frameworks too fall short to
the extent that they only posit three times (corre-
sponding to the speech time, a reference time, and
a time at which an event happens (Reichenbach’s S,
R, and T respectively). Natural language, however,
can accommodate more than three times, as in Be-
fore yesterday, Mary had been going to go to Paris
on Friday. In a Reichenbachian system, the refer-
ence time referred to by this sentence, would be yes-

terday, but then not only is there the event time of
her going to Paris, but a time before yesterday is
needed for Mary’s plan as well. The future orien-
tation (that is, the future relationship between refer-
ence time and event time) of such a sentence cannot
be modeled in Reichenbach’s system. Such exam-
ples indicate that a analysis with greater sensitivity
to linguistic structure is needed if reference to the
future is to be identified and modeled.

In this paper we use the syntactic properties of a
sentence to identify references to the future. We also
examine how references to the future might be diag-
nostic of a person’s psychological wellbeing. In par-
ticular, we hypothesize that references to the future
reflect, in part, a person’s future-orientation, that is
the proportion of time a person’s thoughts concern
the future.

Apparently, reference to future has sparked the in-
terests of many Psychologists. Recent researches
suggest that future-oriented thinking is linked to
physical and mental health, academic achieve-
ment, increased social involvement, and lower dis-
tress (Kahana et al., 2005; Aspinwall, 2005; Simons
et al., 2004).

While future-oriented thought appears to play a
central role in cognition, it’s identification in lan-
guages such as English is not easily accomplished.
As pointed out earlier, the absence of explicit and
necessary morphology for the encoding of future
reference often makes distinguish references to the
future or present difficult to determine.

The goal of this research is to develop procedures
for the automated detection of references to the fu-
ture, even in the context of a mix of verbs with differ-

168



ent tenses. Such procedures will allow linguists and
psychologists to more effectively mine text from so-
cial media to better extract chains and causation, as
well as, potentially determine a person’s or group’s
state of wellbeing. To the best of our knowledge,
this is the first time that a project of this kind has
been done in English, though similar research has
been conducted in Japanese (Nakajima et al., 2014).

2 Related work

Document classification has been a long researched
topic. Tools and algorithms have been developed
to enable people to classify pre-labeled documents.
The approach in this paper is single-label text clas-
sification using ADAGRAD (Duchi et al., 2011a).

Later on, we explored Latent Dirichlet Model-
ing (Blei et al., 2003) on the basis of induced sub-
trees, which are commonly used in data mining, but
not frequently seen in Natural Language Processing.
Frequent Subtree Mining is a common data min-
ing topic. Related algorithms such as TreeMiner,
FreeQT have been developed to find most frequent
structure in a given tree bank (Chi et al., 2005).

Similar approaches have been explored in Mos-
chitti (2006)’s work on using subtrees as features
for Support Vector Machine. We did not use his ap-
proach because were were not interested in the sim-
ilarity between tree structures, but rather in the lin-
guistic regularities implicit in the text. For this rea-
son, we chose to use Varro algorithm developed by
Martens (2010), to exhaustively generate subtrees.

3 Data

We used data collected through Amazon Mechani-
cal Turk (MTurk). Participants were asked to write
down their mind wanderings as follows:

Please think back to the last time you were
thinking about something other than what
you were currently doing. Please share
with us what you were thinking about. If
you found yourself thinking about many
different things, please share with us as
many of these things that you can remem-
ber.

In addition to writing down their mind wanderings,
participants (N = 795) also answered a series of be-
havioral survey questions related to anxiety, health,

happiness, life and financial satisfaction. The task
resulted in a total of 2007 sentences. Table 1 de-
scribes the distribution of our data.

The sentences were rated by three human raters.
For each sentence, raters indicated whether the ex-
pression referred to the future and their level of con-
fidence of their decision.

Sentence Subtree Token
Future 867 164,772 11,910

Not Future 1140 196,049 15,228

Table 1: Total number of sentences, subtrees and tokens

We used the Stanford factored parser (Klein and
Manning, 2002) to parse sentences into constituency
grammar tree representations. Tokens were gen-
erated by a uni-bi-trigram mixed model. Subtree
structures were generated using the Varro algo-
rithm (Martens, 2010) with threshold k = 1 to in-
clude lexicons. For the future corpus, 2,529,040
subtrees were processed while for the non-future
corpus 2,792,875 were processed. A subset of the
subtrees were selected as words for the LDA analy-
sis, as described in Martens (2009).

4 Examples

While there are many cases of grammatical future
marking (i.e., will, be going to) and lexical future
meaning (e.g., plan, want, need, tomorrow, goal,
ambition), many of the ways people use to refer to
the future do not fall into one of these two types of
linguistic categories.

For example, as we have seen, it’s possible to have
future reference without an obvious grammatical or
lexical way of referring to the future. One way of
doing this is with so-called futurate sentences (Cop-
ley, 2009; Kaufmann, 2005), such as Mary is going
to Paris, which can refer to a contextually-provided
future time (e.g., tomorrow). Another way to refer
to the future without grammatical or lexial means is
to use a wh-question word with an infinitive, such as
in I’m thinking about what to eat. Such cases will be
missed by ngram approaches.

Secondly, relying purely on lexical targets will
not work well when sense disambiguation is re-
quired. Modals in English can have multiple mean-
ings (Palmer, 1986):

169



I was thinking about the local news be-
cause they were showing what the weather
would be like.

I was thinking about my life and marriage
and how much money or lack of plays a
role in my obligations, and what my hus-
band would do if I died.

Both sentences have the modal word would. Many
cases of would are “sequence-of-tense” woulds, as in
the first sentence above. That is, they should really
be seen as will in the past; the past-tense marking
inherent to would is functioning as a kind of tense
agreement with the main clause past. The future
orientation provided by would is future with respect
to the past reference time. However, the would in
the second sentence is not a will of a past reference
time, but picks out a “less-vivid” future relative to
the present reference time (Iatridou, 2000).

5 Classification

5.1 Syntactic structural rules

We used the constituency grammar rules generated
by Wolff and Copley. Rules were generated on
the basis of linguistic theory, and then later refined
on the basis of analyses of the false positives and
misses.

The rules were instantiated in the Tregex pattern
language (Levy and Andrew, 2006), which could
then be used to find matching structures in the
parsed sentences. There were 39 future-related
rules, 16 past-related rules, and 3 present-related
rules. The rules varied from the purely syntactic
to the lexical, with a number of rules containing
of mix of both. Syntactic information helped
to disambiguate the senses of the modal verbs.
Fourteen of the future-related rules empha-
sized the modal verbs. Rules are released online at
https://github.com/clir/time-percep
tion.

5.2 Adaptive sub-gradient descent

To build statistical models, we used a stochastic
adaptive subgradient algorithm called ADAGRAD
that uses per-coordinate learning rates to exploit
rarely seen features while remaining scalable (Duchi
et al., 2011b). This is suitable for NLP tasks where

rarely seen features often play an important role
and training data consists of a large number of in-
stances with high dimensional features. We use the
implementation of ADAGRAD in ClearNLP (Choi,
2013) using the hinge-loss, and the default hyper-
parameters (learning rate: a = 0.01, termination
criterion: r = 0.1).

5.3 Experiments
Our experiment consists of four parts. First, we used
the Tregex-based rule discussed in section 5.1 to de-
termine whether the sentences referred to the future.
Each sentence was matched against all rules, and an
odd ratio score was calculated on the basis of the
equation in (1).

Future

Future + Past + Present
(1)

We used this as our baseline classifier. In the sec-
ond part of the experiment, we converted the rule
matches into vector: matches were coded as 1’s, ab-
sences as 0’s.

In the third part of the experiment, we used a more
traditional uni-bi-trigram mixed model as features
for ADAGRAD. The extracted number of tokens
from the corpus are represented in Table 1. Finally,
we mixed the ngram features with rule-based fea-
tures to train the final classifier. All classifiers were
trained through a 5-fold cross-validation process. In
the case of the human raters, we selected the label
that was selected by 2 of the 3 raters. Table 3 shows
the results of our classification.

odd-ratio human
accuracy 70.75 87.381

Table 2: Simple Rule and Human Performance

6 Categorization

6.1 Induced subtree
Three types of subtrees are generally researched in
subtree mining: bottom-up subtrees, induced sub-
trees, and embedded subtrees. They are ranked
in order from the most restrictive to the most free

1Due to the fact that the corpus was slowly built over a year,
and confidence rating task was later added to the rating task,
thus only tested over 1034 sentences.

170



rules ngram ngram + rules
75.12 77.61 83.33
71.14 81.09 78.86
75.56 83.54 83.29
74.81 79.30 82.04
74.81 80.55 84.79
74.29 80.42 82.46

Table 3: 5-fold Cross-Validation: ADAGRAD Classifier
Performance in Accuracy

form. Bottom-up subtree mining does not capture
the transformations of a sentence, while embedded
tree mining breaks a sentence structure down into
units that are often unhelpful. Given these lim-
itations, we used induced subtree mining, as re-
comended in (Martens, 2009).

After the initial extraction, we combined subtrees
from the future, past, and present corpora to produce
322,691 subtrees. Each subtree’s weights were cal-
culated using the frequency of the subtree appearing
in the future corpus divided by total number of sen-
tence in future corpus minus the same subtree ap-
pearing in non-future corpora divided by total num-
ber of sentences in non-future corpus.

Linguists have long argued that syntactic con-
structions encode meaning (Grimshaw, 1990; Levin
and Hovav, 1995). We argue that by using the sub-
tree structures to represent a sentence, the compo-
nents of meaning associated with a syntactic con-
struction can be teased apart. The components of
meaning associated with these subtrees can then be
inferred using procedures such as latent dirichlet al-
location (LDA).

6.2 Recursive LDA

We implemented a procedure called recursive LDA
in which LDA was performed iteratively within new
topics. One of the obstacles of modelling data using
LDA is that the number of topics must be chosen
in advance. Therefore it is very necessary to under-
stand the properties of the data being modelled and
choose a number of categories appropriately. Vari-
ations and extensions of LDA should also be mod-
elled to reflect the characteristics of the space and
the categories being modelled. With this in mind, we
hypothesize that the total future-oriented reference

space could be divided into a small number of cat-
egories and within each semantic category, future-
oriented reference relate to each other will form
more specific categories. In comparison to a similar
extension: hLDA (Griffiths and Tenenbaum, 2004),
rLDA provides better control to researchers, and is
more suitable to discover categories on well-studied
problems.

To run rLDA, we selected subtrees with weights
larger than 0 (N = 21,156; 6.56% of the total gen-
erated subtree structures) as our features (words)
and sentences identified as referring to the future as
our collections (N = 867)(documents). Specifically,
LDA was run on all of the subtrees with the goal of
discovering 2 topics. The solution from this analysis
was then used to divide the subtrees into two groups,
and LDA was subsequently run again on each set of
subtrees.

6.3 Experiments
We obtained 4 topics through two recursive run with
LDA. All of which have significant statistical corre-
lations with behavioral data. Two topics on the first
level are labeled as topic A and topic B.

Figure 1: Recursive LDA Topic Hierarchy

The main semantic difference between A and B
seemed to concern the distinction between open and
fixed futures. Sentences in topic A indicate far fewer
or more fixed choices, normally between just two
choices. Sentences in topic B tend to include open-
ended questions. Example sentences from these two
sub-types are shown below:

Topic A - Fixed future:

I was thinking that I should not be playing
Hay Day and I should do my work.

Last night I decided that I should travel to
meet my aunt in Rhode Island as I haven’t

171



Topic AA Topic AB Topic BA Topic BB
Age .055 .397** -.286** -.167

Vividness .157 .199 -.266* -.100
Anxiety-State .105 -.383** .260 -.041
Anxiety-Trait .050 -.342* .247 -.008

Financial Satisfaction .114 .326* -.364** -.032
Control over Life .107 -.299** .149 .039

Table 4: Correlation Table Between LDA Topics and Behavioral Data. Due to the iterative design of our survey, we did
not have a complete behavioral question section till the end of our data collection. 146 people accounting for 18.36%
of the total sample participated in the behavioral question research, and a subset of 81 people had future sentences in
their response. Only content items that correlated with at least one category reported. *p < .01, **p < .002

seen her in a long time.

Topic B - Open Future:

At the same time I was thinking about
what I was going to have for breakfast.

I was thinking about what I would cook for
dinner tonight.

From the second level, more fine-grained topics
emerged. Descending from topic A (fixed future),
the two sub-types seemed to differ with respect to
level of certainty: Topic AA tended to involve sen-
tences conveying the notion of uncertainty, while
Topic AB tended to involve sentences implying cer-
tainty. From Table 4 People, who construct future
sentences with high certainty, have less control over
life, scored lower on the trait and state anxiety in-
ventory (Spielberger, 2010).

Topic AA - Uncertainty:

I was thinking about a trip that I may take
at the end of the summer.

I was wondering if we would end up to-
gether and thinking about the fact that
something that can seem so certain now
may not be in the future.

Topic AB - Certainty:

I was making my wife ’s lunch to take to
work , and I was thinking about playing
golf this weekend .

I am getting married in April , and there is
a bunch of stuff left to be done .

Topic B appeared to be mostly about an open future.
Its sub-types seemed to differ with respect to the no-
tion of constraint: Topic BA seemed to consist of
sentences about an unconstrained future while Topic
BB seemed to concern sentences implying a con-
strained future. Our categorization matches with be-
havioral data in Table 4. People using unconstrained
future sentence constructs rated their future as less
vivid. They also were younger and had lower finan-
cial satisfaction.

Topic BA - Unconstrained:

I was thinking about what I should do for
the rest of the day.

I was thinking about what I should ani-
mate for my next cartoon.

Topic BB - Constrained:

Two hours ago I was debating what I
should have for lunch and what I should
watch while I was eating.

I was thinking about a girl I would like to
meet , what we would do , and how long
we would do it.

7 Conclusion

In this research we leveraged recent developments in
linguistic theory (Iatridou, 2000; Condoravdi, 2002;
Copley and Martin, 2014) to build an automated
system capable of discovering different ways of ex-
pressing the future. Specifically, we trained a ADA-
GRAD classifier to a relatively high level of accuracy
and examined the number of topics associated with
references to the future through the use of recursive

172



LDA. Finally, we established the psychological real-
ity of our topics via comparisons to behavioral mea-
sures.

8 Acknowledgements
This research was supported by a grant from the U
Penn / John Templeton Foundation to B. Copley and
P. Wolff.

References
L. G. Aspinwall. 2005. The psychology of future-

oriented thinking: From achievement to proactive cop-
ing, adaptation, and aging. Motivation and Emotion,
29(4):203–235.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Yun Chi, Richard R Muntz, Siegfried Nijssen, and
Joost N Kok. 2005. Frequent subtree mining-an
overview. Fundamenta Informaticae, 66(1):161–198.

Jinho D Choi. 2013. Clearnlp.
Cleo Condoravdi. 2002. Temporal interpretation of

modals. In David Beaver, Stefan Kaufmann, Brady
Clark, and Luis Casillas, editors, Stanford Papers on
Semantics. CSLI Publications, Palo Alto.

Bridget Copley and Fabienne Martin, editors. 2014.
Causation in Grammatical Structures. Oxford Uni-
versity Press.

Bridget Copley. 2009. The semantics of the future. Rout-
ledge.

John Duchi, Elad Hazan, and Yoram Singer. 2011a.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

John Duchi, Elad Hazan, and Yoram Singer. 2011b.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Machine
Learning Research, 12(39):2121–2159.

DMBTL Griffiths and MIJJB Tenenbaum. 2004. Hierar-
chical topic models and the nested chinese restaurant
process. Advances in neural information processing
systems, 16:17.

Jane Grimshaw. 1990. Argument structure. the MIT
Press.

Norbert Hornstein. 1990. As Time Goes By. MIT Press.
Sabine Iatridou. 2000. The grammatical ingredients of

counterfactuality. LI, 31:231–270.
E. Kahana, B. Kahana, and J. Zhang. 2005. Motivational

antecedents of preventive proactivity in late life: Link-
ing future orientation and exercise. Motivation and
emotion, 29(4):438–459.

Stefan Kaufmann. 2005. Conditional truth and future
reference. Journal of Semantics, 22(3):231–280, Au-
gust.

Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, pages 3–10.

Wolfgang Klein. 1997. Time in Language. Routledge,
New York.

Beth Levin and Malka Rappaport Hovav. 1995. Unac-
cusativity: At the syntax-lexical semantics interface,
volume 26. MIT press.

Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In Proceedings of the fifth international
conference on Language Resources and Evaluation,
pages 2231–2234. Citeseer.

Scott Martens. 2009. Quantitative analysis of treebanks
using frequent subtree mining methods. In Proceed-
ings of the 2009 Workshop on Graph-based Methods
for Natural Language Processing, pages 84–92. Asso-
ciation for Computational Linguistics.

Scott Martens. 2010. Varro: an algorithm and toolkit for
regular structure discovery in treebanks. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 810–818. Associ-
ation for Computational Linguistics.

Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In EACL, volume
113, page 24.

Yoko Nakajima, Michal Ptaszynski, Hirotoshi Honma,
and Fumito Masui. 2014. Investigation of future refer-
ence expressions in trend information. In Proceedings
of the 2014 AAAI Spring Symposium Series,Big data
becomes personal: knowledge into meaning–For bet-
ter health, wellness and well-being, pages 31–38.

F. R. Palmer. 1986. Mood and modality. Cambridge
University Press, Cambridge.

Arthur Prior. 1967. Past, Present, and Future. Oxford
University Press, Oxford.

Hans Reichenbach. 1947. The tenses of verbs. na.
J. Simons, M. Vansteenkiste, W. Lens, and M. Lacante.

2004. Placing motivation and future time perspective
theory in a temporal perspective. Educational Psy-
chology Review, 16(2):121–139.

Charles D Spielberger. 2010. State-Trait Anxiety Inven-
tory. Wiley Online Library.

173


