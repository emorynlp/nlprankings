



















































Finding middle ground? Multi-objective Natural Language Generation from time-series data


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Finding middle ground? Multi-objective Natural Language Generation
from time-series data

Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh

{dg106, h.hastie, o.lemon}@hw.ac.uk

Abstract

A Natural Language Generation (NLG)
system is able to generate text from non-
linguistic data, ideally personalising the
content to a user’s specific needs. In some
cases, however, there are multiple stake-
holders with their own individual goals,
needs and preferences. In this paper, we
explore the feasibility of combining the
preferences of two different user groups,
lecturers and students, when generating
summaries in the context of student feed-
back generation. The preferences of each
user group are modelled as a multivariate
optimisation function, therefore the task
of generation is seen as a multi-objective
(MO) optimisation task, where the two
functions are combined into one. This ini-
tial study shows that treating the prefer-
ences of each user group equally smooths
the weights of the MO function, in a way
that preferred content of the user groups is
not presented in the generated summary.

1 Introduction
Summarisation of time-series data refers to the
task of automatically generating summaries from
attributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information to be included in a report (Re-
iter and Dale, 2000). Here, we consider the task
of automatically generating feedback summaries
for students describing their performance during
the lab of a computer science module over the
semester. This work is motivated by the fact that
different user groups have different preferences of
the content that should be conveyed in a summary,
as shown by Gkatzia et al. (2013).

Various factors can influence students’ learning,
such as difficulty of the material (Person et al.,
1995), workload (Craig et al., 2004), attendance

in lectures (Ames, 1992) etc. These factors change
over time and can be interdependent. The different
stakeholders (i.e. lecturers and students) have dif-
ferent perceptions regarding what constitutes good
feedback. Therefore, when generating feedback,
we should take into account all preferences in or-
der to be able to produce feedback summaries that
are acceptable by both user groups.

Stakeholders often have conflicting goals, needs
and preferences, for example managers with em-
ployees or doctors with patients and relatives. In
our data, for instance, lecturers tend to comment
on the hours that a student studied, whereas the
students disprefer this content. Generating the
same summary for both groups allows for mean-
ingful further discussion with common ground.

Previous work on NLG systems that address
more than one user group use different versions of
a system for each different user group (Gatt et al.,
2009) or make use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuk-
erman and Litman, 2001). Here, we explore a
method that adapts to both expert preferences and
users simultaneously (i.e. lecturer and students
preferences), by applying Multi-Objective opti-
misation (MOO). MOO can be applied to situa-
tions where optimal decisions are sought in the
presence of trade-offs between conflicting objec-
tives (Chankong and Haimes, 1983). We explore
whether balancing the preferences of two user
groups can result in an adaptive system that is ac-
ceptable by all users. At the same time, the pro-
gramming effort is reduced as only one system
needs to be developed. Moreover, by pooling all
available data together, there is less need for an
extensive data collection.

In the next section, we present three systems:
one tuned for lecturers, one for students, and one
that attempts to find middle ground. In Section 3,
we describe an evaluation of these three systems
and in Section 4 we discuss the results. Finally, in

210



Section 5, directions for future work are discussed.

2 Methodology
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns
to take optimal sequences of actions so as to
maximize a cumulative reward (Sutton and Barto,
1998). Here we extend the framework proposed
by Gkatzia et al. (2013) whereby the content selec-
tion is seen as a Markov Decision problem and the
goal of the agent is to learn to take the sequence
of actions that leads to optimal content selection.
A Temporal Difference learning method (Sutton
and Barto, 1998) was used to train an agent for
content selection. Firstly, we will describe the
data in general. Secondly, we refer to the RL
system that adapts to lecturers’ preferences as de-
scribed by Gkatzia et al. (2013). Thirdly, we will
describe how we collected data and developed a
methodology that adapts to students’ preferences
and finally how we combined the knowledge of
both steps to develop an MO system. The three
systems (Lecturer-adapted, Student-adapted, MO)
share the same architecture but the difference lies
in the reward functions used for training.

2.1 The Data
For this study, the dataset described by Gkatzia
et al. (2013) was used. Table 1 shows an exam-
ple of this dataset that describes a student’s learn-
ing habits and a corresponding feedback summary
provided by a lecturer. The dataset is composed
of 37 similar instances. Each instance consists of
time-series information about the student’s learn-
ing routine and the selected templates that lectur-
ers used to provide feedback to this student. A
template is a quadruple consisting of an id, a fac-
tor (Table 1), a reference type (trend, weeks, aver-
age, other) and surface text. For instance, a tem-
plate can be (1, marks, trend, ‘Your marks were
<trend>over the semester’). The lexical choice
for <trend>(i.e. increasing or decreasing) de-
pends on the values of time-series data. There
is a direct mapping between the values of factor
and reference type and the surface text. The time-
series attributes are listed in Table 1 (bottom left).

2.2 Time-series summarisation systems
Actions and states: The state consists of the time-
series data and the selected templates. In order to
explore the state space the agent selects a time-
series attribute (e.g. marks, deadlines etc.) and

then decides whether to talk about it or not. The
states and actions are similar for all systems.

Lecturer-adapted reward function
The reward function is derived from analysis with
linear regression of the provided dataset and is the
following cumulative multivariate function:

RewardLECT = a +
n∑

i=1

bi ∗ xi + c ∗ length
where X = {x1, x2, ..., xn} is the vector of

combinations of the data trends observed in the
time-series data and a particular reference type of
the factor. The value of xi is given by the function:

xi =


1, if the combination of a factor trend

and a particular reference type is
included in the feedback

0, if not.

The coefficients represent the preference level of
a factor to be selected and how to be conveyed
in the summary. Important factors are associated
with high positive coefficients and the unimpor-
tant ones with negative coefficients. In the train-
ing phase, the agent selects a factor and then de-
cides whether to talk about it or not. If it decides
to refer to a factor, the selection of the template is
performed deterministically, i.e. it selects the tem-
plate that results in higher reward. Length rep-
resents the number of factors selected for gener-
ation.

Student-adapted reward function
The Student-adapted system uses the same RL al-
gorithm as the Lecturer-adapted one. The differ-
ence lies in the reward function. The reward func-
tion used for training is of a similar style as the
Lecturer-adapted reward function. This function
was derived by manipulating the student ratings in
a previous experiment and estimating the weights
using linear regression in a similar way as Walker
et al. (1997) and Rieser et al. (2010).

Multi-objective function
The function used for the multi-objective method
is derived by weighting the sum of the individual
reward functions.

RMO = 0.5 ∗ RLECT + 0.5 ∗ RSTUDENT
To reduce the confounding variables, we kept

the ordering of content in all systems the same.

3 Evaluation
The output of the above-mentioned three systems
were evaluated both in simulation and with real

211



Raw Data

factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...

Trends from Data

factors factor trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing

Summary

Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think about whether
revising has improved your performance.

Table 1: Top left: example of the time-series raw data for feedback generation. Bottom left: example of
described trends. Right box: a target summary generated by an expert (bold signifies the chosen content).

users. Example summaries of all systems are pre-
sented in Table 2.

3.1 Evaluation in Simulation
26 summaries were produced by each system. The
output of each system was evaluated with the three
reward functions. Table 3 shows the results.

As expected, all systems score highly when
evaluated with the reward function for which
they were trained, with the second highest reward
scored from the MO function. Table 2 illustrates
this with the MO Policy clearly between the other
two policies. Moreover, the MO function reduces
the variability between summaries as is also re-
flected in the standard deviation given in Table 3.

We used BLEU (4-grams) (Papineni et al.,
2002) to measure the similarities between the
feedback summaries generated by the three sys-
tems. BLEU score is between 0-1 with values
closer to 1 indicating texts are more similar. Our
results demonstrate that the summaries generated
by the three systems are quite different (BLEU
score between 0.33 and 0.36). This shows that the
framework presented here is capable of producing
quite different summaries based on the various re-
ward functions.

3.2 Evaluation with real users
The goal of the evaluation is to determine whether
the end-user can pick up on the above-mentioned

differences in the feedback and rank them accord-
ing to their preferences. The output of the three
systems was ranked by 19 lecturers and 48 first-
year Computer Science students. Time-series data
of three students were presented on graphs to each
participant. They were also shown 3 feedback
summaries and they were asked to rank them in
terms of preference.

As we can see from Table 4, the two user groups
significantly preferred the output of the system
which was trained for their preferences (Mann-
Whitney U test, p < 0.05). Interestingly, lecturers
found both the outputs produced by the Lecturer-
adapted system and the Student-adapted system
significantly preferable (p < 0.05) to the output
produced by the MO system. In contrast, students
significantly preferred the output generated by the
Student-adapted system over the other two. Fi-
nally, both user groups rated the MO system 3rd,
but there is not a significant difference between
the student ratings for the MO system and the
Lecturer-adapted system.

4 Discussion
It is interesting to examine the weights derived
from the multiple-linear regression to determine
the preferences of the different user groups. For
instance, lecturers’ most preferred content is
hours studied, therefore the reward function gives
high scores to summaries that mention the hours

212



Lecturer-adapted Student-adapted Multi-objective

Make sure you revise the learning
material and try to do the lab ex-
ercises again. You dedicated more
time studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your under-
standing of the material could be
improved. Try going over the teach-
ing material again. You have had
other deadlines during weeks 5, 6,
8, 9 and 10. You may want to plan
your studying and work ahead. You
did not face any health problems
during the semester.

You found the lab exercises very
challenging. Make sure that you
have understood the taught material
and don’t hesitate to ask for clari-
fication. You dedicated more time
studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your un-
derstanding of the material could
be improved. Try going over the
teaching material again. Revising
material during the semester will
improve your performance in the
lab.

Your attendance was varying over the
semester. Have a think about how to
use time in lectures to improve your un-
derstanding of the material. You found
the lab exercises very challenging. Make
sure that you have understood the taught
material and don’t hesitate to ask for
clarification. You dedicated more time
studying the lecture material in the be-
ginning of the semester compared to the
end of the semester. Have a think about
what is preventing you from studying.
You did not face any health problems
during the semester. You revised part
of the learning material. Have a think
whether revising has improved your per-
formance.

Table 2: Example outputs from the three different systems (bold signifies the chosen content).

Time-Series Summarisation Systems Lecturer Function Student Function MO Function
Lecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)
Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)
MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)

Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.
Bold signifies higher reward.

Summarisation
Systems

Lecturer’s Rat-
ing

Student’s
Rating

Lecturer-adapted 1st (2.15)* 3rd (1.97)
Student-adapted 1st (2.01)* 1st* (2.22)
MO 2nd, 3rd (1.81) 3rd (1.79)

Table 4: Mode of the ratings for each user group
(*Mann-Whitney U test, p < 0.05, when compar-
ing each system to the MO system).

that a student studied in all cases (i.e. when the
hours studied increased, decreased, or remained
stable). This, however, does not factor heavily into
the student’s reward function.

Secondly, lecturers find it useful to give some
advice to students who faced personal issues dur-
ing the semester, such as advising them to talk to
their mentor. Students, on the other hand, like
reading about personal issues only when the num-
ber of issues they faced was increasing over the
semester, perhaps as this is the only trend that may
affect their performance. Students seem to mostly
prefer a feedback summary that mentions the un-
derstandability of the material when it increases
which is positive feedback. Finally, the only factor
that both groups agree on is that health issues is

negatively weighted and therefore not mentioned.
The MO reward function attempts to balance

the preferences of the two user groups. Therefore,
for this function, the coefficient for mentioning
health issues is also negative, however the other
coefficients are smoothed providing neither strong
negative or positive coefficients. This means that
there is less variability (see Table 3) but that per-
haps this function meets neither group’s criteria.

5 Conclusion and Future Work
In conclusion, we presented a framework for de-
veloping and evaluating various reward functions
for time-series summarisation of feedback. This
framework has been validated in that both simula-
tion and subjective studies show that each group
does indeed prefer feedback generated using a
highly tuned reward function, with lecturers being
slightly more open to variation. Further investiga-
tion is required as to whether it is indeed possible
to find middle ground between these two groups.
Choices for one group may be negatively rated
by the other and it might not be possible to find
middle ground but it is worth investigating further
other methods of reward function derivation using
stronger feature selection methods, such as Princi-
pal Component Analysis.

213



References
Carole Ames. 1992. Classrooms: Goals, structures,

and student motivation. Journal of Educational Psy-
chology, 84(3):p261–71.

Chankong and Haimes. 1983. Multiobjective decision
making theory and methodology. In New York: El-
sevier Science Publishing.

Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor. In Journal of Educational Media,
29:241-250.

Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
In Journal of AI Communications, 22:153-186.

Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation.

Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue.

K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual meeting of the As-
sociation for Computational Linguistics.

Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. In Journal of Cogni-
tion and Instruction, 13(2):161-188.

Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. In Cambridge Univer-
sity Press.

Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.

Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. In MIT Press.

Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. In Journal of Artificial In-
telligence Research 21, 333-428.

Marilyn Walker, Diane Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In 35th Annual
meeting of the Association for Computational Lin-
guistics.

Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2), 129-158.

214


