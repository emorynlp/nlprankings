

















































book


Workshop on Semantic Interpretation in an Actionable Context, NAACL-HLT 2012, pages 7–14,
Montréal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics

Toward Learning Perceptually Grounded Word Meanings from Unaligned
Parallel Data

Stefanie Tellex and Pratiksha Thaker and Josh Joseph and Matthew R. Walter and Nicholas Roy
MIT Computer Science and Artificial Intelligence Laboratory

Abstract

In order for robots to effectively understand
natural language commands, they must be able
to acquire a large vocabulary of meaning rep-
resentations that can be mapped to perceptual
features in the external world. Previous ap-
proaches to learning these grounded meaning
representations require detailed annotations at
training time. In this paper, we present an
approach which is capable of jointly learning
a policy for following natural language com-
mands such as “Pick up the tire pallet,” as well
as a mapping between specific phrases in the
language and aspects of the external world;
for example the mapping between the words
“the tire pallet” and a specific object in the
environment. We assume the action policy
takes a parametric form that factors based on
the structure of the language, based on the G3
framework and use stochastic gradient ascent
to optimize policy parameters. Our prelimi-
nary evaluation demonstrates the effectiveness
of the model on a corpus of “pick up” com-
mands given to a robotic forklift by untrained
users.

1 Introduction

In order for robots to robustly understand human
language, they must have access to meaning rep-
resentations capable of mapping between symbols
in the language and aspects of the external world
which are accessible via the robot’s perception sys-
tem. Previous approaches have represented word
meanings as symbols in some specific symbolic
language, either programmed by hand [Winograd,

1971, MacMahon et al., 2006] or learned [Matuszek
et al., 2010, Chen and Mooney, 2011, Liang et al.,
2011, Branavan et al., 2009]. Because word mean-
ings are represented as symbols, rather than percep-
tually grounded features, the mapping between these
symbols and the external world must still be de-
fined. Furthermore, the uncertainty of the mapping
between constituents in the language and aspects of
the external world cannot be explicitly represented
by the model.

Language grounding approaches, in contrast, map
words in the language to groundings in the external
world [Mavridis and Roy, 2006, Hsiao et al., 2008,
Kollar et al., 2010, Tellex et al., 2011]. Groundings
are the specific physical concept that is referred to
by the language and can be objects (e.g., a truck
or a door), places (e.g., a particular location in the
world), paths (e.g., a trajectory through the envi-
ronment), or events (e.g., a sequence of robot ac-
tions). This symbol grounding approach [Harnad,
1990] represents word meanings as functions which
take as input a perceptual representation of a ground-
ing and return whether it matches words in the lan-
guage. Recent work has demonstrated how to learn
grounded word meanings from a parallel corpus of
natural language commands paired with groundings
in the external world [Tellex et al., 2011]. How-
ever, learning model parameters required that the
parallel corpus be augmented with additional an-
notations specifying the alignment between specific
phrases in the language and corresponding ground-
ings in the external world. Figure 1 shows an ex-
ample command from the training set paired with
these alignment annotations, represented as arrows

7



pointing from each linguistic constituent to a corre-
sponding grounding.
Our approach in this paper relaxes these annota-

tion requirements and learns perceptually grounded
word meanings from an unaligned parallel corpus
that only provides supervision for the top-level ac-
tion that corresponds to a natural language com-
mand. Our system takes as input a state/action space
for the robot defining a space of possible groundings
and available actions in the external world. In addi-
tion it requires a corpus of natural language com-
mands paired with the correct action executed in
the environment. For example, an entry in the cor-
pus consists of a natural language command such as
“Pick up the tire pallet” given to a robotic forklift,
paired with an action sequence of the robot as drives
to the tire pallet, inserts its forks, and raises it off the
ground, drives to the truck, and sets it down.
To learn from an unaligned corpus, we derive

a new training algorithm that combines the Gen-
eralized Grounding Graph (G3) framework intro-
duced by Tellex et al. [2011] with the policy gra-
dient method described by Branavan et al. [2009].
We assume a specific parametric form for the action
policy that is defined by the linguistic structure of
the natural language command. The system learns
a policy parameters that maximize expected reward
using stochastic gradient ascent. By factoring the
policy according to the structure of language, we can
propagate the error signal to each term, allowing the
system to infer groundings for each linguistic con-
stituent even without direct supervision. We eval-
uate our model using a corpus of natural language
commands collected from untrained users on the in-
ternet, commanding the robot to pick up objects or
drive to locations in the environment. The evalua-
tion demonstrates that the model is able to predict
both robot actions and noun phrase groundings with
high accuracy, despite having no direct supervision
for noun phrase groundings.

2 Background

We briefly review the G3 framework, introduced by
Tellex et al. [2011]. In order for a robot to un-
derstand natural language, it must be able to map
between words in the language and corresponding
groundings in the external world. The aim is to find

Figure 1: Sample entry from an aligned corpus,
where mappings between phrases in the language
and groundings in the external world are explicitly
specified as arrows. Learning the meaning of “the
truck” and “the pallet” is challenging when align-
ment annotations are not known.

λr1
“Pick up”

γ1 =

φ1

λf2
“the pallet.”

φ2

γ2 =

Figure 2: Grounding graph for “Pick up the tire pal-
let.

8



the most probable groundings γ1 . . . γN given the
language Λ and the robot’s model of the environ-
mentM :

argmax
γ1...γN

p(γ1 . . . γN |Λ,M) (1)

M consists of the robot’s location, the loca-
tions, geometries, and perceptual tags of objects, and
available actions the robot can take. For brevity, we
omitM from future equations in this section.
To learn this distribution, one standard approach

is to factor it based on certain independence assump-
tions, then train models for each factor. Natural
language has a well-known compositional, hierar-
chical argument structure [Jackendoff, 1983], and a
promising approach is to exploit this structure in or-
der to factor the model. However, if we define a di-
rected model over these variables, we must assume
a possibly arbitrary order to the conditional γi fac-
tors. For example, for a phrase such as “the tire pal-
let near the other skid,” we could factorize in either
of the following ways:

p(γtires, γskid|Λ) = p(γskid|γtires,Λ)× p(γtires|Λ) (2)
p(γtires, γskid|Λ) = p(γtires|γskid,Λ)× p(γskid|Λ) (3)

Depending on the order of factorization, we will
need different conditional probability tables that cor-
respond to the meanings of words in the language.
To resolve this issue, another approach is to use
Bayes’ Rule to estimate the p(Λ|γ1 . . . γN ), but this
approach would require normalizing over all possi-
ble words in the language Λ. Another alternative
is to use an undirected model, but this approach is
intractable because it requires normalizing over all
possible values of all γi variables in the model, in-
cluding continuous attributes such as location and
size.
To address these problems, the G3 framework in-

troduced a correspondence vector Φ to capture the
dependency between γ1 . . . γN and Λ. Each entry
in φi ∈ Φ corresponds to whether linguistic con-
stituent λi ∈ Λ corresponds to grounding γi. We
assume that γ1 . . . γN are independent of Λ unless
Φ is known. Introducing Φ enables factorization ac-
cording to the structure of language with local nor-
malization at each factor over a space of just the two
possible values for φi.

2.1 Inference
In order to use the G3 framework for inference, we
want to infer the groundings γ1 . . . γN that maxi-
mize the distribution

argmax
γ1...γN

p(γ1 . . . γN |Φ,Λ) (4)

which is equivalent to maximizing the joint distribu-
tion of all groundings γ1 . . . γN , Φ and Λ,

argmax
γ1...γN

p(γ1 . . . γN ,Φ,Λ). (5)

We assume that Λ and γ1 . . . γN are independent
when Φ is not known, yielding:

argmax
γ1...γN

p(Φ|Λ, γ1 . . . γN )p(Λ)p(γ1 . . . γN ) (6)

This independence assumption may seem unintu-
itive, but it is justified because the correspondence
variable Φ breaks the dependency between Λ and
γ1 . . . γN . If we do not know whether γ1 . . . γN cor-
respond to Λ, we assume that the language does not
tell us anything about the groundings.
Finally, for simplicity, we assume that any object

in the environment is equally likely to be referenced
by the language, which amounts to a constant prior
on γ1 . . . γN . In the future, we plan to incorporate
models of attention and salience into this prior. We
ignore p(Λ) since it does not depend on γ1 . . . γN ,
leading to:

argmax
γ1...γN

p(Φ|Λ, γ1 . . . γN ) (7)

To compute the maximum value of the objective
in Equation 7, the system performs beam search
over γ1 . . . γN , computing the probability of each
assignment from Equation 7 to find the maximum
probability assignment. Although we are using
p(Φ|Λ, γ1 . . . γN ) as the objective function, Φ is
fixed, and the γ1 . . . γN are unknown. This approach
is valid because, given our independence assump-
tions, p(Φ|Λ, γ1 . . . γN ) corresponds to the joint dis-
tribution over all the variables given in Equation 5.
In order to perform beam search, we factor the

model according to the hierarchical, compositional
linguistic structure of the command:

p(Φ|Λ, γ1 . . . γN ) =
�

i

p(φi|λi, γi1 . . . γik) (8)

9



This factorization can be represented graphically;
we call the resulting graphical model the grounding
graph for a natural language command. The directed
model for the command “Pick up the pallet” appears
in Figure 2. The λ variables correspond to language;
the γ variables correspond to groundings in the ex-
ternal world, and the φ variables are True if the
groundings correspond to the language, and False
otherwise.
In the fully supervised case, we fit model param-

eters Θ using an aligned parallel corpus of labeled
positive and negative examples for each linguistic
constituent. The G3 framework assumes a log-linear
parametrization with feature functions fj and fea-
ture weights θj :

p(Φ|Λ, γ1 . . . γN ) = (9)
�

i

1

Z
exp(

�

j

θjfj(φi,λi, γi1 . . . γik)) (10)

This function is convex and can be optimized with
gradient-based methods [McCallum, 2002].
Features correspond to the degree to which each

Γ correctly grounds λi. For a relation such as “on,”
a natural feature is whether the grounding corre-
sponding to the head noun phrase is supported by
the grounding corresponding to the argument noun
phrases. However, the feature supports(γi, γj)
alone is not enough to enable the model to learn that
“on” corresponds to supports(γi, γj). Instead we
need a feature that also takes into account the word
“on:”

supports(γi, γj) ∧ (“on” ∈ λi) (11)

3 Approach

Our goal is to learn model parameters for the G3
framework from an unaligned corpus of natural lan-
guage commands paired with robot actions. Previ-
ously, the system learned model parameters Θ using
an aligned corpus in which values for all ground-
ing variables are known at training time, and anno-
tators provided both positive and negative examples
for each factor. In this paper we describe how to
relax this annotation requirement so that only the
top-level action needs to be observed in order to
train the model. It is easy and fast to collect data

with these annotations, whereas annotating the val-
ues of all the variables, including negative examples
is time-consuming and error prone. Once we know
the model parameters we can use existing inference
to find groundings corresponding to word meanings,
as in Equation 4.
We are given a corpus of D training examples.

Each example d consists of a natural language com-
mand Λd with an associated grounding graph with
grounding variables Γd. Values for the grounding
variables are not known, except for an observed
value gda for the top-level action random variable, γda .
Finally, an example has an associated environmental
context or semantic map Md. Each environmental
context will have a different set of objects and avail-
able actions for the robot. For example, one training
example might contain a command given in an envi-
ronment with a single tire pallet; another might con-
tain a command given in an environment with two
box pallets and a truck.
We define a sampling distribution to choose val-

ues for the Γ variables in the model using the G3
framework with parameters Θ:

p(Γd|Φd,Λd,Md,Θ) (12)

Next, we define a reward function for choosing the
correct grounding ga for a training example:

r(Γd, gda) =

�
1 if γda = gda
−1 otherwise (13)

Here γa is a grounding variable for the top-level ac-
tion corresponding to the command; it is one of the
variables in the vector Γ. Our aim is to find model
parameters that maximize expected reward when
drawing values for Γd from p(Γd|Φd,Λd,Md,Θ)
over the training set:

argmax
Θ

�

d

Ep(Γd|Φd,Λd,Md,Θ)r(Γ
d, gda) (14)

Expanding the expectation we have:

argmax
Θ

�

d

d�

Γ

r(Γd, gda)p(Γ
d|Φd,Λd,Md,Θ)

(15)

We use stochastic gradient descent to find model pa-
rameters that maximize reward. First, we take the

10



derivative of the expectation with respect to Θ. (We
drop the d subscripts for brevity.)

∂

∂θk
Ep(Γ|Φ,Λ,M,Θ)r(Γ, ga) =

�

Γ

r(Γ, ga)
∂

∂θk
p(Γ|Φ,Λ,M,Θ) (16)

Focusing on the inner term, we expand it with
Bayes’ rule:

∂

∂θk
p(Γ|Φ,Λ,M,Θ) =

∂

∂θk

p(Φ|Γ,Λ,M,Θ)p(Γ|Λ,M,Θ)
p(Φ|Λ,M,Θ) (17)

We assume the priors do not depend on Θ:

p(Γ|M)
p(Φ|Λ)

∂

∂θk
p(Φ|Γ,Λ,M,Θ) (18)

For brevity, we compress Γ, Λ, and M in the vari-
able X . Next, we take the partial derivative of the
likelihood of Φ. First we assume each factor is inde-
pendent.

∂

∂θk
p(Φ|X,Θ) = ∂

∂θk

�

i

p(φi|X,Θ) (19)

=
�

i

p(φi|X,Θ)×




�

j

∂
∂θk

p(φj |X,Θ)
p(φj |X,Θ)





(20)

Finally, we assume the distribution over φj takes a
log-linear form with feature functions fk and param-
eters θk, as in the G3 framework.

∂

∂θk
p(φj |X,Θ) = (21)

p(φ|X,Θ)×
�
fk(φ, X)− Ep(φ�|X,Θ)

�
fk(φ

�, X)
�
�

We substitute back into the overall expression for the
partial derivative of the expectation:

∂

∂θk
Ep(Γ|X,Θ)r(Γ, ga) =

Ep(Γ|X,Θ)r(Γ, ga)×


�

j

fk(φj , X)− Ep(φ�|X,Θ)
�
fk(φ

�, X)
�


 (22)

Input:
1: Initial values for parameters, Θ0.
2: Training dataset, D.
3: Number of iterations, T .
4: Step size, α.
5:
6: Θ ← Θ0
7: for t ∈ T do
8: for d ∈ D do
9: ∇Θ ← ∂∂θkEp(Γd|Φd,Λd,Md,Θ)r(Γ

d, gda)
10: end for
11: Θ ← Θ+ α∇Θ
12: end for
Output: Estimate of parameters Θ

Figure 3: Training algorithm.

We approximate the expectation over Γ with
highly probable bindings for the Γ and update the
gradient incrementally for each example. The train-
ing algorithm is given in Figure 3.

4 Results

We present preliminary results for the learning algo-
rithm using a corpus of natural language commands
given to a robotic forklift. We collected a corpus
of natural language commands paired with robot ac-
tions by showing annotators on Amazon Mechani-
cal Turk a video of the robot executing an action and
asking them to describe in words they would use to
command an expert human operator to carry out the
commands in the video. Frames from a video in our
corpus, together with commands for that video ap-
pear in Figure 4. Since commands often spanned
multiple parts of the video, we annotated the align-
ment between each top-level clause in the command
and the robot’s motion in the video. Our initial eval-
uation uses only commands from the corpus that
contain the words “pick up” due to scaling issues
when running on the entire corpus.
We report results using a random cost function as

a baseline as well as the learned parameters on a
training set and a held-out test set. Table 1 shows
performance on a training set using small environ-
ments (with one or two other objects) and a test set
of small and large environments (with up to six other
objects).

11



(a) t=0 (b) t=20 (c) t=30

Pick up pallet with refridgerator [sic] and place on truck to
the left.

A distance away you should see a rectangular box. Approach it
slowly and load it up onto your forklift. Slowly proceed to
back out and then make a sharp turn and approach the truck.
Raise your forklift and drop the rectangular box on the back of
the truck.

Go to the pallet with the refrigerator on it and pick it
up. Move the pallet to the truck trailer. Place the pallet on
the trailer.

Pick up the pallet with the refrigerator and place it on
the trailer.

(d) Commands

Figure 4: Frames from a video in our dataset, paired
with natural language commands.

As expected, on the training set the system learns
a good policy, since it is directly rewarded for act-
ing correctly. Because the environments are small,
the chance of correctly grounding concrete noun
phrases with a random cost function is high. How-
ever after training performance at grounding noun
phrases increases to 92% even though the system
had no access to alignment annotations for noun
phrases at training time; it only observes reward
based on whether it has acted correctly.
Next, we report performance on a test set to assess

generalization to novel commands given in novel en-
vironments. Since the test set includes larger en-
vironments with up to six objects, baseline perfor-
mance is lower. However the trained system is able
to achieve high performance at both inferring cor-
rect actions as well as correct object groundings, de-
spite having no access to a reward signal of any kind
during inference. This result shows the system has
learned general word meanings that apply in novel
contexts not seen at training time.

5 Related Work

Beginning with SHRDLU [Winograd, 1971], many
systems have exploited the compositional structure
of language to statically generate a plan correspond-

% Correct
Actions Concrete Noun Phrases

Before Training 31% 61%
After Training 100% 92%

(a) Training (small environments)

% Correct
Actions Concrete Noun Phrases

Before Training 3% 26%
After Training 84% 77%

(b) Testing (small and large environments)

Table 1: Results on the training set and test set.

ing to a natural language command [Hsiao et al.,
2008, MacMahon et al., 2006, Skubic et al., 2004,
Dzifcak et al., 2009]. Our work moves beyond
this framework by defining a probabilistic graphical
model according to the structure of the natural lan-
guage command, inducing a distribution over plans
and groundings.
Models that learned word meanings [Tellex et al.,

2011, Kollar et al., 2010] require detailed align-
ment annotations between constituents in the lan-
guage and objects, places, paths, or events in the ex-
ternal world. Previous approaches capable of learn-
ing from unaligned data [Vogel and Jurafsky, 2010,
Branavan et al., 2009] used sequential models that
could not capture the hierarchical structure of lan-
guage. Matuszek et al. [2010], Liang et al. [2011]
and Chen and Mooney [2011] describe models that
learn compositional semantics, but word meanings
are symbolic structures rather than patterns of fea-
tures in the external world.
There has been a variety of work in transferring

action policies between a human and a robot. In imi-
tation learning, the goal is to create a system that can
watch a teacher perform an action, and then repro-
duce that action [Kruger et al., 2007, Chernova and
Veloso, 2009, Schaal et al., 2003, Ekvall and Kragic,
2008]. Rybski et al. [2007] developed an imitation
learning system that learns from a combination of
imitation of the human teacher, as well as natural
language input. Our work differs in that the system
must infer an action from the natural language com-
mands, rather than from watching the teacher per-

12



form an action. The system is trained off-line, and
the task of the robot is to respond on-line to the nat-
ural language command.

6 Conclusion

In this paper we described an approach for learning
perceptually grounded word meanings from an un-
aligned parallel corpus of language paired with robot
actions. The training algorithm jointly infers poli-
cies that correspond to natural language commands
as well as alignments between noun phrases in the
command and groundings in the external world. In
addition, our approach learns grounded word mean-
ings or distributions corresponding to words in the
language, that the system can use to follow novel
commands that it may have never encountered dur-
ing training. We presented a preliminary evaluation
on a small corpus, demonstrating that the system is
able to infer meanings for concrete noun phrases de-
spite having no direct supervision for these values.
There are many directions for improvement. We

plan to train our system using a large dataset of lan-
guage paired with robot actions in more complex en-
vironments, and on more than one robotic platform.
Our approach points the way towards a framework
that can learn a large vocabulary of general grounded
word meanings, enabling systems that flexibly re-
spond to a wide variety of natural language com-
mands given by untrained users.

References

S. R. K. Branavan, H. Chen, L. S. Zettlemoyer, and
R. Barzilay. Reinforcement learning for mapping
instructions to actions. In Proceedings of ACL,
page 82–90, 2009.

D. L. Chen and R. J. Mooney. Learning to interpret
natural language navigation instructions from ob-
servations. In Proc. AAAI, 2011.

S. Chernova and M. Veloso. Interactive policy learn-
ing through confidence-based autonomy. JAIR, 34
(1):1–25, 2009.

J. Dzifcak, M. Scheutz, C. Baral, and P. Schermer-
horn. What to do and how to do it: Translating
natural language directives into temporal and dy-
namic logic representation for goal management
and action execution. In Proc. IEEE Int’l Conf.

on Robotics and Automation (ICRA), pages 4163–
4168, 2009.

S. Ekvall and D. Kragic. Robot learning from
demonstration: a task-level planning approach.
International Journal of Advanced Robotic Sys-
tems, 5(3), 2008.

S. Harnad. The symbol grounding problem. Physica
D, 43:335–346, 1990.

K. Hsiao, S. Tellex, S. Vosoughi, R. Kubat, and
D. Roy. Object schemas for grounding language
in a responsive robot. Connection Science, 20(4):
253–276, 2008.

R. S. Jackendoff. Semantics and Cognition, pages
161–187. MIT Press, 1983.

T. Kollar, S. Tellex, D. Roy, and N. Roy. Toward un-
derstanding natural language directions. In Proc.
ACM/IEEE Int’l Conf. on Human-Robot Interac-
tion (HRI), pages 259–266, 2010.

V. Kruger, D. Kragic, A. Ude, and C. Geib. The
meaning of action: A review on action recogni-
tion and mapping. Advanced Robotics, 21(13),
2007.

P. Liang, M. I. Jordan, and D. Klein. Learning
dependency-based compositional semantics. In
Proc. Association for Computational Linguistics
(ACL), 2011.

M. MacMahon, B. Stankiewicz, and B. Kuipers.
Walk the talk: Connecting language, knowledge,
and action in route instructions. In Proc. Nat’l
Conf. on Artificial Intelligence (AAAI), pages
1475–1482, 2006.

C. Matuszek, D. Fox, and K. Koscher. Following
directions using statistical machine translation. In
Proc. ACM/IEEE Int’l Conf. on Human-Robot In-
teraction (HRI), pages 251–258, 2010.

N. Mavridis and D. Roy. Grounded situation models
for robots: Where words and percepts meet. In
2006 IEEE/RSJ International Conference on In-
telligent Robots and Systems, pages 4690–4697.
IEEE, Oct. 2006. ISBN 1-4244-0258-1.

A. K. McCallum. MALLET: A machine learning
for language toolkit. http://mallet.cs.umass.edu,
2002.

P. Rybski, K. Yoon, J. Stolarz, and M. Veloso. In-
teractive robot task training through dialog and

13



demonstration. In Proceedings of HRI, page 56.
ACM, 2007.

S. Schaal, A. Ijspeert, and A. Billard. Computational
approaches to motor learning by imitation. Phil.
Trans. R. Soc. Lond. B, (358), 2003.

M. Skubic, D. Perzanowski, S. Blisard, A. Schultz,
W. Adams, M. Bugajska, and D. Brock. Spatial
language for human-robot dialogs. IEEE Trans.
on Systems, Man, and Cybernetics, Part C: Appli-
cations and Reviews, 34(2):154–167, 2004. ISSN
1094-6977.

S. Tellex, T. Kollar, S. Dickerson, M. Walter,
A. Banerjee, S. Teller, and N. Roy. Understand-
ing natural language commands for robotic navi-
gation and mobile manipulation. In Proc. AAAI,
2011.

A. Vogel and D. Jurafsky. Learning to follow naviga-
tional directions. In Proc. Association for Compu-
tational Linguistics (ACL), pages 806–814, 2010.

T. Winograd. Procedures as a Representation for
Data in a Computer Program for Understanding
Natural Language. PhD thesis, Massachusetts In-
stitute of Technology, 1971. Ph.D. thesis.

14


