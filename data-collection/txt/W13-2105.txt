










































Generating Elliptic Coordination


Proceedings of the 14th European Workshop on Natural Language Generation, pages 40–50,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Generating Elliptic Coordination

Claire Gardent
CNRS, LORIA, UMR 7503

Vandoeuvre-lès-Nancy, F-54500, France
claire.gardent@loria.fr

Shashi Narayan
Université de Lorraine, LORIA, UMR 7503

Villers-lès-Nancy, F-54600, France
shashi.narayan@loria.fr

Abstract

In this paper, we focus on the task of gen-
erating elliptic sentences. We extract from
the data provided by the Surface Realisa-
tion (SR) Task (Belz et al., 2011) 2398 in-
put whose corresponding output sentence
contain an ellipsis. We show that 9% of the
data contains an ellipsis and that both cov-
erage and BLEU score markedly decrease
for elliptic input (from 82.3% coverage for
non-elliptic sentences to 65.3% for ellip-
tic sentences and from 0.60 BLEU score
to 0.47). We argue that elided material
should be represented using phonetically
empty nodes and we introduce a set of
rewrite rules which permits adding these
empty categories to the SR data. Finally,
we evaluate an existing surface realiser on
the resulting dataset. We show that, after
rewriting, the generator achieves a cover-
age of 76% and a BLEU score of 0.74 on
the elliptical data.

1 Introduction

To a large extent, previous work on generating el-
lipsis has assumed a semantically fully specified
input (Shaw, 1998; Harbusch and Kempen, 2009;
Theune et al., 2006). Given such input, elliptic
sentences are then generated by first producing
full sentences and second, deleting from these sen-
tences substrings that were identified to obey dele-
tion constraints.

In contrast, recent work on generation often as-
sumes input where repeated material has already
been elided. This includes work on sentence com-
pression which regenerates sentences from surface
dependency trees derived from parsing the initial
text (Filippova and Strube, 2008); Surface realisa-
tion approaches which have produced results for
regenerating from the Penn Treebank (Langkilde-
Geary, 2002; Callaway, 2003; Zhong and Stent,

2005; Cahill and Van Genabith, 2006; White and
Rajkumar, 2009); and more recently, the Surface
Realisation (SR) Task (Belz et al., 2011) which
has proposed dependency trees and graphs de-
rived from the Penn Treebank (PTB) as a com-
mon ground input representation for testing and
comparing existing surface realisers. In all these
approaches, repeated material is omitted from the
representation that is input to surface realisation.

As shown in the literature, modelling the inter-
face between the empty phonology and the syn-
tactic structure of ellipses is a difficult task. For
parsing, Sarkar and Joshi (1996), Banik (2004)
and Seddah (2008) propose either to modify the
derivation process of Tree Adjoining Grammar
or to introduce elementary trees anchored with
empty category in a synchronous TAG to accom-
modate elliptic coordinations. In HPSG (Head-
Driven Phrase Structure Grammar), Levy and Pol-
lard (2001) introduce a neutralisation mechanism
to account for unlike constituent coordination ;
in LFG (Lexical Functional Grammar), Dalrym-
ple and Kaplan (2000) employ set values to model
coordination; in CCG (Combinatory Categorial
Grammar, (Steedman, 1996)), it is the non stan-
dard notion of constituency assumed by the ap-
proach which permits accounting for coordinated
structures; finally, in TLCG (Type-Logical Cat-
egorial Grammar), gapping is treated as like-
category constituent coordinations (Kubota and
Levine, 2012).

In this paper, we focus on how surface reali-
sation handles elliptical sentences given an input
where repeated material is omitted. We extract
from the SR data 2398 input whose correspond-
ing output sentence contain an ellipsis. Based on
previous work on how to annotate and to represent
ellipsis, we argue that elided material should be
represented using phonetically empty nodes (Sec-
tion 3) and we introduce a set of rewrite rules
which permits adding these empty categories to

40



the SR data (Section 4). We then evaluate our sur-
face realiser (Narayan and Gardent, 2012b) on the
resulting dataset (Section 5) and we show that, on
this data, the generator achieves a coverage of 76%
and a BLEU score, for the generated sentences, of
0.74. Section 6 discusses related work on generat-
ing elliptic coordination. Section 7 concludes.

2 Elliptic Sentences

Elliptic coordination involves a wide range of phe-
nomena including in particular non-constituent
coordination (1, NCC) i.e., cases where sequences
of constituents are coordinated; gapping (2, G)
i.e., cases where the verb and possibly some
additional material is elided; shared subjects (3,
SS) and right node raising (4, RNR) i.e., cases
where a right most constituent is shared by two or
more clauses1.

(1) [It rose]i 4.8 % in June 1998 and ǫi 4.7% in
June 1999. NCC
(2) Sumitomo bank [donated]i $500, 000, Tokyo
prefecture ǫi $15, 000 and the city of Osaka ǫi
$10, 000 . Gapping
(3) [the state agency ’s figures]i ǫi confirm pre-
vious estimates and ǫi leave the index at 178.9 .

Shared Subject
(4) He commissions ǫi and splendidly interprets
ǫi [fearsome contemporary scores]i . RNR

We refer to the non elliptic clause as the source
and to the elliptic clause as the target. In the
source, the brackets indicate the element shared
with the target while in the target, the ǫi sign in-
dicate the elided material with co-indexing indi-
cating the antecedent/ellipsis relation. In gapping
clauses, we refer to the constituents in the gapped
clause, as remnants.

3 Representing and Annotating Elided
Material

We now briefly review how elided material is rep-
resented in the literature.

Linguistic Approaches. While Sag (1976),
Williams (1977), Kehler (2002), Merchant (2001)

1Other types of elliptic coordination include sluicing and
Verb-Phrase ellipsis. These will not be discussed here be-
cause they can be handled by the generator by having the
appropriate categories in the grammar and the lexicon e.g.,
in a Tree Adjoining Grammar, an auxiliary anchoring a verb
phrase for VP ellipsis and question words anchoring a sen-
tence for sluicing.

and van Craenenbroeck (2010) have argued for
a structural approach i.e., one which posits syn-
tactic structure for the elided material, Keenan
(1971), Hardt (1993), Dalrymple et al. (1991),
Ginzburg and Sag (2000) and Culicover and Jack-
endoff (2005) all defend a non structural approach.
Although no consensus has yet been reached on
these questions, many of these approaches do pos-
tulate an abstract syntax for ellipsis. That is they
posit that elided material licenses the introduction
of phonetically empty categories in the syntax or
at some more abstract level (e.g., the logical form
of generative linguistics).

Treebanks. Similarly, in computational linguis-
tics, the treebanks used to train and evaluate
parsers propose different means of representing el-
lipsis.

For phrase structure syntax, the Penn Treebank
Bracketing Guidelines extensively describe how to
annotate coordination and missing material in En-
glish (Bies et al., 1995). For shared complements
(e.g., shared subject and right node raising con-
structions), these guidelines state that the elided
material licenses the introduction of an empty
*RNR* category co-indexed with the shared com-
plement (cf. Figure 1) while gapping construc-
tions are handled by labelling the gapping rem-
nants (i.e., the constituents present in the gapping
clause) with the index of their parallel element in
the source (cf. Figure 2).

(S
(VP (VB Do)(VP (VB avoid)

(S (VP (VPG puncturing(NP *RNR*-5))
(CC or)
(VP (VBG cutting)(PP (IN into)

(NP *RNR*-5)))
(NP-5 meats)))))

Figure 1: Penn Treebank annotation for Right
Node Raising “Do avoid puncturing ǫi or cutting into ǫi
[meats]i.”

(S
(S (NP-SBJ-10 Mary)

(VP (VBZ likes) (NP-11 potatoes)))
(CC and)
(S (NP-SBJ=10 Bill)

(, ,) (NP=11 ostriches)))

Figure 2: Penn Treebank annotation for gapping
“Mary [likes]i potatoes and Bill ǫi ostriches.”

In dependency treebanks, headless elliptic con-
structs such as gapping additionally raise the is-

41



sue of how to represent the daughters of an empty
head. Three main types of approaches have been
proposed. In dependency treebanks for German
(Daum et al., 2004; Hajič et al., 2009) and in
the Czech treebank (Čmejrek et al., 2004; Hajič
et al., 2009), one of the dependents of the head-
less phrase is declared to be the head. This is
a rather undesirable solution because it hides the
fact that there the clause lacks a head. In contrast,
the Hungarian dependency treebank (Vincze et al.,
2010) explicitly represents the elided elements in
the trees by introducing phonetically empty ele-
ments that serve as attachment points to other to-
kens. This is the cleanest solution from a linguistic
point of view. Similarly, Seeker and Kuhn (2012)
present a conversion of the German Tiger treebank
which introduces empty nodes for verb ellipses if
a phrase normally headed by a verb is lacking a
head. They compare the performance of two sta-
tistical dependency parsers on the canonical ver-
sion and the CoNLL 2009 Shared Task data and
show that the converted dependency treebank they
propose yields better parsing results than the tree-
bank not containing empty heads.

In sum, while some linguists have argued for an
approach where ellipsis has no syntactic represen-
tation, many have provided strong empirical evi-
dence for positing empty nodes as place-holders
for elliptic material. Similarly, in devising tree-
banks, computational linguists have oscillated be-
tween representations with and without empty cat-
egories. In the following section, we present the
way in which elided material is represented in the
SR data; we show that it underspecifies the sen-
tences to be generated; and we propose to mod-
ify the SR representations by making the relation-
ship between ellipsis and antecedent explicit using
phonetically empty categories and co-indexing.

4 Rewriting the SR Data

The SR Task 2011 made available two types of
data for surface realisers to be tested on: shallow
dependency trees and deep dependency graphs.
Here we focus on the shallow dependency trees
i.e., on syntactic structures.

The input data provided by the SR Task were
obtained from the Penn Treebank. They were
derived indirectly from the LTH Constituent-to-
Dependency Conversion Tool for Penn-style Tree-
banks (Pennconverter, (Johansson and Nugues,
2007)) by post-processing the CoNLL data to re-

d o n a t e

Sumitomo bank

SBJ

a n d

COORD

$ 5 0 0 , 0 0 0

OBJ

Tokyo prefecture

GAP-SBJ

$ 1 5 , 0 0 0

GAP-OBJ

Figure 3: Gapping in the SR data. “Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture ǫi $15, 000.”

move word order, inflections etc. It consists of
a set of unordered labelled syntactic dependency
trees whose nodes are labelled with word forms,
part of speech categories, partial morphosyntac-
tic information such as tense and number and, in
some cases, a sense tag identifier. The edges are
labelled with the syntactic labels provided by the
Pennconverter. All words (including punctuation)
of the original sentence are represented by a node
in the tree. Figures 3, 4, 5 and 6 show (simplified)
input trees from the SR data.

In the SR data, the representation of ellipsis
adopted in the Penn Treebank is preserved modulo
some important differences regarding co-indexing.

Gapping is represented as in the PTB by la-
belling the remnants with a marker indicating the
source element parallel to each remnant. However
while in the PTB, this parallelism is made explicit
by co-indexing (the source element is marked with
an index i and its parallel target element with the
marker = i), in the SR data this parallelism is ap-
proximated using functions. For instance, if the
remnant is parallel to the source subject, it will be
labelled GAP-SBJ (cf. Figure 3).

commission

He

SBJ

a n d

COORD

fearsome contemporary  score

OBJ

splendidly interpret

CONJ

Figure 4: Subject Sharing and RNR in the SR
data. “[He]j ǫj commissions ǫi and ǫj splendidly interprets
ǫi [fearsome contemporary scores]i .”

For right-node raising and shared subjects, the
coindexation present in the PTB is dropped in the
SR data. As a result, the SR representation under-

42



b e

They

SBJ

s h o w

VC

n o t

ADV

t a k e

OPRD

James Madison

OBJ

or

COORD

a puff

OBJ

light up

CONJ

Figure 5: Non shared Object “They aren’t showing
James Madison taking a puff or lighting up”

rise

It

SBJ

a n d

COORD

4.8  %

EXT

in June 1998

TMP

4.7  %

GAP-EXT

in June 1999

GAP-TMP

Figure 6: NCC in the SR data. “It rose 4.8 % in June
1998 and 4.7% in June 1999.”

V V

FUNC COORD FUNC COORD

X CONJ W X CONJ W

GAP-FUNC ⇒ CONJ

Y Vǫ

FUNC

Y Wǫ

Figure 7: Gapping and Non Constituent Coordina-
tion structures rewriting (V: a verb, CONJ: a con-
junctive coordination, X, Y and W three sets of de-
pendents). The antecedent verb (V) and the source
material without counterpart in the gapping clause
(W) are copied over to the gapping clause and
marked as phonetically empty.

specifies the relation between the object and the
coordinated verbs in RNR constructions: the ob-
ject could be shared as in He commissions ǫi and splen-
didly interprets ǫi [fearsome contemporary scores]i . (Fig-
ure 4) or not as in They aren’t showing James Madison
taking a puff or lighting up (Figure 5). In both cases,
the representation is the same i.e., the shared ob-
ject (fearsome contemporary scores) and the unshared
object (a puff ) are both attached to the first verb.

Finally, NCC structures are handled in the same
way as gapping by having the gapping remnants
labelled with a GAP prefixed function (e.g., GAP-
SBJ) indicating which element in the source the
gapping remnant is parallel to (cf. Figure 6).

Summing up, the SR representation schema un-
derspecifies ellipsis in two ways. For gapping and
non-constituent coordination, it describes paral-
lelism between source and target elements rather
than specifying the syntax of the elided material.
For subject sharing and right node raising, it fails
to explicitly specify argument sharing.

V1 V1

SUBJ COORD SUBJ COORD

X CONJ Y1 X CONJ Y1

CONJ ⇒ CONJ

V2 V2

SUBJ

Y2 Xǫ Y2

Figure 8: Subject sharing: the subject dependent
is copied over to the target clause and marked as
phonetically empty.

To resolve this underspecification, we rewrite
the SR data using tree rewrite rules as follows.

In Gapping and NCC structures, we copy the
source material that has no (GAP- marked) coun-
terpart in the target clause to the target clause
marking it to indicate a phonetically empty cate-
gory (cf. Figure 7).

For Subject sharing, we copy the shared subject
of the source clause in the target clause and mark it
to be a phonetically empty category (cf. Figure 8).

For Right-Node-Raising, we unfold the am-
biguity producing structures where arguments
present in the source but not in the target are op-
tionally copied over to the target (cf. Figure 9).

These rewrite rules are implemented efficiently

43



V1 V1 V1
COORD OBJ COORD OBJ COORD OBJ

X1 CONJ Y X1 CONJ Y X1 CONJ Yǫ

CONJ ⇒

{

CONJ , CONJ

}

V2 V2 V2
OBJ

X2 X2 X2 Y

Figure 9: Right-Node-Raising: the object dependent is optionally copied over to the target clause and
marked as phonetically empty in the source clause.

using GrGen, an efficient graph rewriting sys-
tem (Geißet al., 2006).

5 Generating Elliptic Coordination

5.1 The Surface Realiser

To generate sentences from the SR data, we
use our surface realiser (Narayan and Gardent,
2012b), a grammar-based generator based on a
Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English. This generator first
selects the elementary FB-LTAG trees associated
in the lexicon with the lemmas and part of speech
tags associated with each node in the input de-
pendency tree. It then attempts to combine the
selected trees bottom-up taking into account the
structure of the input tree (only trees that are se-
lected by nodes belonging to the same local input
tree are tried for combination). A language model
is used to implement a beam search letting through
only the n most likely phrases at each bottom up
combination step. In this experiment, we set n to
5. The generator thus outputs at most 5 sentences
for each input.

Figure 10: Gapping after rewriting “Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture ǫi $15, 000.”

As mentioned in the introduction, most compu-
tational grammars have difficulty accounting for
ellipses and FB-LTAG is no exception.

The difficulty stems from the fact that in ellip-
tical sentences, there is meaning without sound.
As a result, the usual form/meaning mappings that
in non-elliptic sentences allow us to map sounds
onto their corresponding meanings, break down.
For instance, in the sentence John eats apples
and Mary pear, the Subject-Verb-Object structure
which can be used in English to express a binary
relation is present in the source clause but not in
the elided one. In practice, the syntax of ellipti-
cal sentences often leads to a duplication of the
grammatical system, one system allowing for non
elliptical sentences and the other for their elided
counterpart.

For parsing with TAG, two main methods have
been proposed for processing elliptical sentences.
(Sarkar and Joshi, 1996) introduces an additional
operation for combining TAG trees which yields
derivation graphs rather than trees. (Seddah, 2008)
uses Multi-Component TAG and proposes to asso-
ciate each elementary verb tree with an elliptic tree
with different pairs representing different types of
ellipses.

We could use either of these approaches for
generation. The first approach however has the
drawback that it leads to a non standard notion of
derivation (the derivation trees become derivation
graphs). The second on the other hand, induces a
proliferation of trees in the grammar and impacts
efficiency.

Instead, we show that, given an input enriched
with empty categories as proposed in the previous
section, neither the grammar nor the tree combi-
nation operation need changing. Indeed, our FB-
LTAG surface realiser directly supports the gen-
eration of elliptic sentences. It suffices to as-
sume that an FB-LTAG elementary tree may be an-
chored by the empty string. Given an input node
marked as phonetically empty, the generator will

44



then select all FB-LTAG rules that are compatible
with the lexical and the morpho-syntactic features
labelling that node. Generation will then proceed
as usual by composing the trees selected on the ba-
sis of the input using substitution and adjunction;
and by retrieving from the generation forest those
sentences whose phrase structure tree covers the
input.

For instance, given the rewritten input shown in
Figure 10, the TAG trees associated in the lexi-
con with donate will be selected; anchored with
the empty string and combined with the TAG trees
built for Tokyo Prefecture and $15,000 thus yield-
ing the derivation shown in Figure 11.

NP

Tokyo prefecture

S

NP↓ VP

V NP↓

ǫ

NP

$15,000

Figure 11: Derivation for “Tokyo prefecture ǫ $15,000”

5.2 The Data

We use both the SR test data (2398 sentences) and
the SR training data (26604 sentences) to evaluate
the performance of the surface realiser on elliptic
coordination. Since the realiser we are using is
not trained on this data (the grammar was written
manually), this does not bias evaluation. Using
the training data allows us to gather a larger set of
elliptic sentences for evaluation while evaluating
also on the test data allows comparison with other
realisers.

To focus on ellipses, we retrieve those sentences
which were identified by our rewrite rules as po-
tentially containing an elliptic coordination. In
essence, these rewrite rules will identify all cases
of non-constituent coordination and gapping (be-
cause these involve GAP-X dependencies with “X”
a dependency relation and are therefore easily de-
tected) and of shared-subjects (because the tree
patterns used to detect are unambiguous i.e., only
apply if there is indeed a shared subject). For
RNR, as discussed in the previous section, the SR
format is ambiguous and consequently, the rewrite
rules might identify as object sharing cases where
in fact the object is not shared. As noted by one
of our reviewers, the false interpretation could be

Elliptic Coordination Data

Elliptic Coordination Pass
BLEU Scores
COV ALL

RNR (384)
Before 66% 0.68 0.45
After 81% 0.70 0.57
Delta +15 +0.02 +0.12

SS (1462)
Before 70% 0.74 0.52
After 75% 0.75 0.56
Delta +5 +0.01 +0.04

SS + RNR
(456)

Before 61% 0.71 0.43
After 74% 0.73 0.54
Delta +13 +0.02 +0.11

Gapping (36)
Before 3% 0.53 0.01
After 67% 0.74 0.49
Delta +64 +0.21 +0.48

NCC (60)
Before 5% 0.68 0.03
After 73% 0.74 0.54
Delta +68 +0.06 +0.51

Total (2398)
Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09

Table 1: Generation results on elliptical data be-
fore and after input rewriting (SS: Shared Subject,
NCC: Non Constituent Coordination, RNR: Right
Node Raising). The number in brackets in the first
column is the number of cases. Pass stands for
the coverage of the generator. COV and ALL in
BLEU scores column stand for BLEU scores for
the covered and the total input data.

dropped out by consulting the Penn Treebank2.
The approach would not generalise to other data
however.

In total, we retrieve 2398 sentences3 potentially
containing an elliptic coordination from the SR
training data. The number and distribution of these
sentences in terms of ellipsis types are given in Ta-
ble 1. From the test data, we retrieve an additional
182 elliptic sentences.

5.3 Evaluation

We ran the surface realiser on the SR input data
both before and after rewriting elliptic coordina-
tions; on the sentences estimated to contain ellip-
sis; on sentences devoid of ellipsis; and on all sen-
tences. The results are shown in Table 2. They
indicate coverage and BLEU score before and af-
ter rewriting. BLEU score is given both with re-
spect to covered sentences (COV) i.e., the set of
input for which generation succeeds; and for all
sentences (ALL). We evaluate both with respect to
the SR test data and with respect to the SR training

2The Penn Treebank makes the RNR interpretations ex-
plicit (refer to Figure 1).

3It is just a coincidence that the size of the SR test data
and the number of extracted elliptic sentences are the same.

45



SR Data Pass
BLEU Scores
COV ALL

Test

+E (182)
Before 58% 0.59 0.34
After 67% 0.59 0.40
Delta +9 +0.00 +0.06

-E (2216)
Before 80% 0.59 0.47
After 80% 0.59 0.48
Delta +0 +0.00 +0.01

T (2398)
Before 78% 0.58 0.46
After 79% 0.59 0.47
Delta +1 +0.01 +0.01

Training

+E (2398)
(Table 1)

Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09

-E (24206)
Before 82% 0.73 0.60
After 82% 0.73 0.60
Delta +0 +0.00 +0.00

T (26604)
Before 81% 0.72 0.58
After 82% 0.73 0.60
Delta +1 +0.01 +0.02

Table 2: Generation results on SR test and SR
training data before and after input rewriting (+E
stands for elliptical data, -E for non elliptical data
and T for total.)

data. We use the SR Task scripts for the computa-
tion of the BLEU score.

The impact of ellipsis on coverage and preci-
sion. Previous work on parsing showed that co-
ordination was a main source of parsing failure
(Collins, 1999). Similarly, ellipses is an important
source of failure for the TAG generator. Ellipses
are relatively frequent with 9% of the sentences
in the training data containing an elliptic struc-
ture and performance markedly decreases in the
presence of ellipsis. Thus, before rewriting, cov-
erage decreases from 82.3% for non-elliptic sen-
tences to 80.75% on all sentences (elliptic and non
elliptic sentences) and to 65.3% on the set of el-
liptic sentences. Similarly, BLEU score decreases
from 0.60 for non elliptical sentences to 0.58 for
all sentences and to 0.47 for elliptic sentences. In
sum, both coverage and BLEU score decrease as
the number of elliptic input increases.

The impact of the input representation on cov-
erage and precision. Recent work on treebank
annotation has shown that the annotation schema
adopted for coordination impacts parsing. In par-
ticular, Maier et al. (2012) propose revised annota-
tion guidelines for coordinations in the Penn Tree-
bank whose aim is to facilitate the detection of co-
ordinations. And Dukes and Habash (2011) show
that treebank annotations which include phoneti-
cally empty material for representing elided mate-

rial allows for better parsing results.

Similarly, Table 2 shows that the way in which
ellipsis is represented in the input data has a strong
impact on generation. Thus rewriting the input
data markedly extends coverage with an overall
improvement of 11 points (from 65% to 76%) for
elliptic sentences and of almost 1 point for all sen-
tences.

As detailed in Table 1 though, there are impor-
tant differences between the different types of el-
liptic constructs: coverage increases by 68 points
for NCC and 64 points for gapping against only
15, 13 and 5 points for RNR, mixed RNR-Shared
Subject and Shared Subject respectively. The rea-
son for this is that sentences are generated for
many input containing the latter types of con-
structions (RNR and Shared Subject) even with-
out rewriting. In fact, generation succeeds on the
non rewritten input for a majority of RNR (66%
PASS), Shared Subject (70% PASS) and mixed
RNR-Shared Subject (61% PASS) constructions
whereas it fails for almost all cases of gapping
(3% PASS) and of NCC (5% PASS). The reason
for this difference is that, while the grammar can-
not cope with headless constructions such as gap-
ping and NCC constructions, it can often provide
a derivation for shared subject sentences by using
the finite verb form in the source sentence and the
corresponding infinitival form in the target. Since
the infinitival does not require a subject, the tar-
get sentence is generated. Similarly, RNR con-
structions can be generated when the verb in the
source clause has both a transitive and an intran-
sitive form: the transitive form is used to gener-
ate the source clause and the intransitive for the
target clause. In short, many sentences contain-
ing a RNR or a shared subject construction can be
generated without rewriting because the grammar
overgenerates i.e., it produces sentences which are
valid sentences of English but whose phrase struc-
ture tree is incorrect.

Nevertheless, as the results show, rewriting
consistently helps increasing coverage even for
RNR (+15 points), Shared Subject (+5 points) and
mixed RNR-Shared Subject (+13 points) construc-
tions because (i) not all verbs have both a transi-
tive and an intransitive verb form and (ii) the input
for the elliptic clause may require a finite form for
the target verb (e.g., in sentences such as “[they]i
weren’t fired but instead ǫi were neglected” where the tar-
get clause includes an auxiliary requiring a past

46



participial which in this context requires a sub-
ject).

Precision is measured using the BLEU score.
For each input, we take the best score obtained
within the 5 derivations4 produced by the gener-
ator. Since the BLEU score reflects the degree to
which a sentence generated by the system matches
the corresponding Penn Treebank sentence, it is
impacted not just by elliptic coordination but also
by all linguistic constructions present in the sen-
tence. Nonetheless, the results show that rewrit-
ing consistently improves the BLEU score with an
overall increase of 0.09 points on the set of ellip-
tic sentences. Moreover, the consistent improve-
ment in terms of BLEU score for generated sen-
tences (COV column) shows that rewriting simul-
taneously improves both coverage and precision
that is, that for those sentences that are generated,
rewriting consistently improves precision.

Analysing the remaining failure cases. To bet-
ter assess the extent to which rewriting and the FB-
LTAG generation system succeed in generating el-
liptic coordinations, we performed error mining
on the elliptic data using our error miner described
in (Narayan and Gardent, 2012a). This method
permits highlighting the most likely sources of er-
ror given two datasets: a set of successful cases
and a set of failure cases. In this case, the suc-
cessful cases is the subset of rewritten input data
for elliptic coordination cases for which genera-
tion succeeds . The failure cases is the subset for
which generation fails. If elliptic coordination was
still a major source of errors, input nodes or edges
labelled with labels related to elliptic coordination
(e.g., the COORD and the GAP-X dependency rela-
tions or the CONJ part of speech tag) would sur-
face as most suspicious forms. In practice how-
ever, we found that the 5 top sources of errors
highlighted by error mining all include the DEP re-
lation, an unknown dependency relation used by
the Pennconverter when it fails to assign a label
to a dependency edge. In other words, most of the
remaining elliptic cases for which generation fails,
fails for reasons unrelated to ellipsis.

Comparison with other surface realisers
There is no data available on the performance
of surface realisers on elliptic input. However,
the performance of the surface realiser can be

4The language model used in the generator allows only 5
likely derivations (refer to section 5.1).

compared with those participating in the shallow
track of the SR challenge. On the SR training
data, the TAG surface realiser has an average
run time of 2.78 seconds per sentence (with an
average of 20 words per sentence), a coverage
of 82% and BLEU scores of 0.73 for covered
and 0.60 for all. On the SR test data, the realiser
achieves a coverage of 79% and BLEU scores of
0.59 for covered and 0.47 for all. In comparison,
the statistical systems in the SR Tasks achieved
0.88, 0.85 and 0.67 BLEU score on the SR test
set and the best symbolic system 0.25 (Belz et al.,
2011).

6 Related work

Previous work on generating elliptic sentences has
mostly focused on identifying material that could
be elided and on defining procedures capable of
producing input structures for surface realisation
that support the generation of elliptic sentences.

Shaw (1998) developed a sentence planner
which generates elliptic sentences in 3 steps. First,
input data are grouped according to their simi-
larities. Second, repeated elements are marked.
Third, constraints are used to determine which oc-
currences of a marked element should be deleted.
The approach is integrated in the PLANDoc sys-
tem (McKeown et al., 1994) and shown to gen-
erate a wide range of elliptic constructs includ-
ing RNR, VPE and NCC using FUF/SURGE (El-
hadad, 1993), a realisation component based on
Functional Unification Grammar.

Theune et al. (2006) describe how elliptic sen-
tences are generated in a story generation system.
The approach covers conjunction reduction, right
node raising, gapping and stripping and uses de-
pendency trees connected by rhetorical relations
as input. Before these trees are mapped to sen-
tences, repeated elements are deleted and their an-
tecedent (thesource element) is related by a SUB-
ORROWED relation to their governor in the ellip-
tic clause and a SUIDENTICAL relation to their
governor in the antecedent clause. This is then in-
terpreted by the surface realiser to mean that the
repeated element should be realised in the source
clause, elided in the target clause and that it li-
censes the same syntactic structure in both clauses.

Harbusch and Kempen (2009) have proposed a
module called Elleipo which takes as input unre-
duced, non-elliptic, syntactic structures annotated
with lexical identity and coreference relationships

47



between words and word groups in the conjuncts;
and returns as output structures annotated with
elision marks indicating which elements can be
elided and how (i.e., using which type of ellip-
sis). The focus is on developing a language in-
dependent module which can mediate between the
unreduced input syntactic structures produced by a
generator and syntactic structures that are enriched
with elision marks rich enough to determine the
range of possible elliptic and non elliptic output
sentences.

In CCG, grammar rules (type-raising and com-
position) permit combining non constituents into a
functor category which takes the shared element as
argument; and gapping remnants into a clause tak-
ing as argument its left-hand coordinated source
clause. White (2006) describes a chart based algo-
rithm for generating with CCG and shows that it
can efficiently realise NCC and gapping construc-
tions.

Our proposal differs from these approaches in
that it focuses on the surface realisation stage (as-
suming that the repeated elements have already
been identified) and is tested on a large corpus
of newspaper sentences rather than on hand-made
document plans and relatively short sentences.

7 Conclusion

In this paper, we showed that elliptic structures
are frequent and can impact the performance of
a surface realiser. In line with linguistic theory
and with some recent results on treebank annota-
tion, we argued that the representation of ellipsis
should involve empty categories and we provided
a set of tree rewrite rules to modify the SR data ac-
cordingly. We then evaluated the performance of a
TAG based surface realiser on 2398 elliptic input
derived by the SR task from the Penn Treebank
and showed that it achieved a coverage of 76% and
a BLEU score of 0.74 on generated sentences. Our
approach relies both on the fact that the grammar
is lexicalised (each rule is associated with a word
from the input) and on TAG extended domain of
locality (which permits using a rule anchored with
the empty string to reconstruct the missing syntax
in the elided clause thereby making it grammati-
cal).

We will release the 2398 input representations
we gathered for evaluating the generation of el-
liptic coordination so as to make it possible for
other surface realisers to be evaluated on their abil-

ity to generate ellipsis. In particular, its would
be interesting to examine how other grammar
based generators perform on this dataset such
as White’s CCG based generator (2006) (which
eschews empty categories by adopting a more
flexible notion of constituency) and Carroll and
Oepen’s HPSG based generator (2005) (whose do-
main of locality differs from that of TAG).

Acknowledgments

We would like to thank Anja Belz and Mike White
for providing us with the evaluation data and the
evaluation scripts. The research presented in this
paper was partially supported by the European
Fund for Regional Development within the frame-
work of the INTERREG IV A Allegro Project.

References

Eva Banik. 2004. Semantics of VP coordination
in LTAG. In Proceedings of the 7th International
Workshop on Tree Adjoining Grammars and Re-
lated Formalisms (TAG+), volume 7, pages 118–
125, Vancouver, Canada.

Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.

Ann Bies, Mark Ferguson, Katz Katz, Robert MacIn-
tyre, Victoria Tredinnick, Grace Kim, Marry Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for treebank II style penn tree-
bank project. University of Pennsylvania.

Aoife Cahill and Josef Van Genabith. 2006. Robust
pcfg-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
(COLING) and the 44th annual meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1033–1040, Sydney, Australia.

Charles B Callaway. 2003. Evaluating coverage for
large symbolic nlg grammars. In Proceedings of the
18th International joint conference on Artificial In-
telligence (IJCAI), volume 18, pages 811–816, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Process-
ing (IJCNLP), pages 165–176, Jeju Island, Korea.
Springer.

48



M. Čmejrek, J. Hajič, and V. Kuboň. 2004. Prague
czech-english dependency treebank: Syntactically
annotated resources for machine translation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC), Lis-
bon, Portugal.

Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.

Peter W. Culicover and Ray Jackendoff. 2005. Simpler
Syntax. Oxford University Press.

Mary Dalrymple and Ronald M. Kaplan. 2000. Fea-
ture indeterminacy and feature resolution. Lan-
guage, pages 759–798.

Mary Dalrymple, Stuart M. Sheiber, and Fernando
C. N. Pereira. 1991. Ellipsis and higher-order unifi-
cation. Linguistics and Philosophy.

Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks
to dependency trees. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.

Kais Dukes and Nizar Habash. 2011. One-step sta-
tistical parsing of hybrid dependency-constituency
syntactic representations. In Proceedings of the
12th International Conference on Parsing Technolo-
gies, pages 92–103, Dublin, Ireland. Association for
Computational Linguistics.

Michael Elhadad. 1993. Using argumentation to con-
trol lexical choice: a functional unification imple-
mentation. Ph.D. thesis, Columbia University.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25–32, Salt
Fork, Ohio, USA. Association for Computational
Linguistics.

Rubino Geiß, Gernot Veit Batz, Daniel Grund, Sebas-
tian Hack, and Adam M. Szalkowski. 2006. Grgen:
A fast spo-based graph rewriting tool. In Proceed-
ings of the 3rd International Conference on Graph
Transformation, pages 383–397. Springer. Natal,
Brasil.

Jonathan Ginzburg and Ivan Sag. 2000. Interrogative
investigations. CSLI Publications.

J. Hajič, M. Ciaramita, R. Johansson, D. Kawahara,
M.A. Martı́, L. Màrquez, A. Meyers, J. Nivre,
S. Padó, J. Štěpánek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 1–18.

Karin Harbusch and Gerard Kempen. 2009. Gener-
ating clausal coordinate ellipsis multilingually: A
uniform approach based on postediting. In Proceed-
ings of the 12th European Workshop on Natural Lan-
guage Generation, pages 138–145, Athens, Greece.
Association for Computational Linguistics.

Daniel Hardt. 1993. Verb phrase ellipsis: Form,
meaning and processing. Ph.D. thesis, University
of Pennsylvania.

Richert Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA), pages
105–112, Tartu, Estonia.

Edward Keenan. 1971. Names, quantifiers, and the
sloppy identity problem. Papers in Linguistics,
4:211–232.

Andrew Kehler. 2002. Coherence in discourse. CSLI
Publications.

Yusuke Kubota and Robert Levine. 2012. Gapping
as like-category coordination. In Proceedings of the
7th international conference on Logical Aspects of
Computational Linguistics (LACL), pages 135–150,
Nantes, France. Springer-Verlag.

Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
12th International Natural Language Generation
Workshop, pages 17–24.

Roger Levy and Carl Pollard. 2001. Coordination and
neutralization in HPSG. Technology, 3:5.

Wolfgang Maier, Erhard Hinrichs, Julia Krivanek, and
Sandra Kübler. 2012. Annotating coordination
in the Penn Treebank. In Proceedings of the 6th
Linguistic Annotation Workshop (LAW), pages 166–
174, Jeju, Republic of Korea. Association for Com-
putational Linguistics.

Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic documentation
generation. In Proceedings of the fourth conference
on Applied natural language processing (ANLC),
pages 7–14, Stuttgart, Germany. Association for
Computational Linguistics.

Jason Merchant. 2001. The syntax of silence: Sluicing,
islands, and the theory of ellipsis. Oxford Univer-
sity Press.

Shashi Narayan and Claire Gardent. 2012a. Error min-
ing with suspicion trees: Seeing the forest for the
trees. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
Mumbai, India.

Shashi Narayan and Claire Gardent. 2012b. Structure-
driven lexicalist generation. In Proceedings of the
24th International Conference on Computational
Linguistics (COLING), Mumbai, India.

49



Ivan Sag. 1976. Deletion and logical form. Ph.D.
thesis, Massachusetts Institute of Technology, Cam-
bridge, Massachusetts.

Anoop Sarkar and Arvind Joshi. 1996. Coordination
in tree adjoining grammars: Formalization and im-
plementation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 610–
615, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Djamé Seddah. 2008. The use of mctag to process
elliptic coordination. In Proceedings of The Ninth
International Workshop on Tree Adjoining Gram-
mars and Related Formalisms (TAG+ 9), volume 1,
page 2, Tübingen, Germany.

Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey.

James Shaw. 1998. Segregatory coordination and
ellipsis in text generation. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, pages 1220–
1226, Montreal, Quebec, Canada.

Mark Steedman. 1996. Surface Structure and Inter-
pretation, volume 30. MIT press Cambridge, MA.

Mariët Theune, Feikje Hielkema, and Petra Hendriks.
2006. Performing aggregation and ellipsis using dis-
course structures. Research on Language & Compu-
tation, 4(4):353–375.

Jeoren van Craenenbroeck. 2010. The syntax of ellip-
sis: Evidence from Dutch dialects. Oxford Univer-
sity Press.

V. Vincze, D. Szauter, A. Almási, G. Móra, Z. Alexin,
and J. Csirik. 2010. Hungarian dependency tree-
bank. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.

Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 410–419, Singapore. Association for Compu-
tational Linguistics.

Michael White. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language & Computation, 4(1):39–75.

Edwin Williams. 1977. Discourse and logical form.
Linguistic Inquiry.

Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation (UCNLG), volume 5,
pages 49–54.

50


