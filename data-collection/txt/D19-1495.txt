



















































Event Representation Learning Enhanced with External Commonsense Knowledge


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4894â€“4903,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

4894

Event Representation Learning Enhanced with External Commonsense
Knowledge

Xiao Ding, Kuo Liao, Ting Liuâˆ—, Zhongyang Li, Junwen Duan
Research Center for Social Computing and Information Retrieval

Harbin Institute of Technology, China
{xding, kliao, tliu, zyli, jwduan}@ir.hit.edu.cn

Abstract

Prior work has proposed effective methods to
learn event representations that can capture
syntactic and semantic information over text
corpus, demonstrating their effectiveness for
downstream tasks such as script event predic-
tion. On the other hand, events extracted from
raw texts lacks of commonsense knowledge,
such as the intents and emotions of the event
participants, which are useful for distinguish-
ing event pairs when there are only subtle dif-
ferences in their surface realizations. To ad-
dress this issue, this paper proposes to leverage
external commonsense knowledge about the
intent and sentiment of the event. Experiments
on three event-related tasks, i.e., event simi-
larity, script event prediction and stock mar-
ket prediction, show that our model obtains
much better event embeddings for the tasks,
achieving 78% improvements on hard similar-
ity task, yielding more precise inferences on
subsequent events under given contexts, and
better accuracies in predicting the volatilities
of the stock market1.

1 Introduction

Events are a kind of important objective informa-
tion of the world. Structuralizing and represent-
ing such information as machine-readable knowl-
edge are crucial to artificial intelligence (Li et al.,
2018b, 2019). The main idea is to learn distributed
representations for structured events (i.e. event
embeddings) from text, and use them as the basis
to induce textual features for downstream appli-
cations, such as script event prediction and stock
market prediction.

Parameterized additive models are among the
most widely used for learning distributed event
representations in prior work (Granroth-Wilding

âˆ—Corresponding author
1The code and data are available on

https://github.com/MagiaSN/CommonsenseERL EMNLP 2019.

PersonX threw basketball

PersonZ attacked embassy

Intent
PersonY threw bomb

PersonZ attacked embassy

PersonX broke record
PersonY broke vase

Sentiment

PersonX broke record

PersonY broke vase

(a)

(b)

PersonY threw bomb

PersonX threw basketball

PersonZ achieved goal PersonZ achieved goal 

Figure 1: Intent and sentiment enhanced event embed-
dings can distinguish distinct events even with high lex-
ical overlap, and find similar events even with low lex-
ical overlap.

and Clark, 2016; Modi, 2016), which passes the
concatenation or addition of event argumentsâ€™
word embeddings to a parameterized function.
The function maps the summed vectors into an
event embedding space. Furthermore, Ding et al.
(2015) and Weber et al. (2018) propose using neu-
ral tensor networks to perform semantic composi-
tion of event arguments, which can better capture
the interactions between event arguments.

This line of work only captures shallow event
semantics, which is not capable of distinguishing
events with subtle differences. On the one hand,
the obtained event embeddings cannot capture the
relationship between events that are syntactically
or semantically similar, if they do not share sim-
ilar word vectors. For example, as shown in Fig-
ure 1 (a), â€œPersonX threw bombâ€ and â€œPersonZ at-
tacked embassyâ€. On the other hand, two events
with similar word embeddings may have similar
embeddings despite that they are quite unrelated,
for example, as shown in Figure 1 (b), â€œPersonX
broke recordâ€ and â€œPersonY broke vaseâ€. Note
that in this paper, similar events generally refer
to events with strong semantic relationships rather
than just the same events.

One important reason for the problem is the lack



4895

Share weights

Share weights

BiLSTM BiLSTM
Share weights

ð‘¦ ð‘“ ð‘Šð‘¥ ð‘

actor predicate object objectneg intent intentneg

event eventneg intentnegintent

sentipred sentigold

score scoreneg

lossevent
lossintent

sim simneg

losssenti

ð‘¦ ð‘“ ð‘Šð‘¥ ð‘ ð‘¦ ð‘“ ð‘Šð‘¥ ð‘ cosine cosine

loss

+

NTN
Tensor

Tensor Tensor

NTN
Tensor

Tensor Tensor

Figure 2: Architecture of the joint embedding model. eventneg refers to the corrupted event tuple, which is derived
by replacing each word of the event object with a random word in our dictionary. intentneg is the incorrect intent
for the given event, which is randomly selected from the annotated dataset.

of the external commonsense knowledge about the
mental state of event participants when learning
the objective event representations. In Figure 1 (a),
two event participants â€œPersonYâ€ and â€œPersonZâ€
may carry out a terrorist attack, and hence, they
have the same intent: â€œto bloodshedâ€, which
can help representation learning model maps two
events into the neighbor vector space. In Fig-
ure 1 (b), a change to a single argument leads to
a large semantic shift in the event representations,
as the change of an argument can result in different
emotions of event participants. Who â€œbroke the
recordâ€ is likely to be happy, while, who â€œbroke
a vaseâ€ may be sad. Hence, intent and sentiment
can be used to learn more fine-grained semantic
features for event embeddings.

Such commonsense knowledge is not explic-
itly expressed but can be found in a knowledge
base such as Event2Mind (Rashkin et al., 2018)
and ATOMIC (Sap et al., 2019). Thus, we aim
to incorporate the external commonsense knowl-
edge, i.e., intent and sentiment, into the learn-
ing process to generate better event representa-
tions. Specifically, we propose a simple and ef-
fective model to jointly embed events, intents and
emotions into the same vector space. A neural ten-
sor network is used to learn baseline event embed-
dings, and we define a corresponding loss function
to incorporate intent and sentiment information.

Extensive experiments show that incorporating
external commonsense knowledge brings promis-
ing improvements to event embeddings, achiev-
ing 78% and 200% improvements on hard similar-
ity small and big dataset, respectively. With bet-
ter embeddings, we can achieve superior perfor-
mances on script event prediction and stock mar-
ket prediction compared to state-of-the-art base-
line methods.

2 Commonsense Knowledge Enhanced
Event Representations

The joint embedding framework is shown in Fig-
ure 2. We begin by introducing the baseline
event embedding learning model, which serves as
the basis of the proposed framework. Then, we
show how to model intent and sentiment infor-
mation. Subsequently, we describe the proposed
joint model by integrating intent and sentiment
into the original objective function to help learn
high-quality event representations, and introduce
the training details.

2.1 Low-Rank Tensors for Event Embedding
The goal of event embedding is to learn low-
dimension dense vector representations for event
tuples E = (A,P,O), where P is the action or
predicate, A is the actor or subject and O is the
object on which the action is performed. Event



4896

+ +

Tensor Composition

AT

P P

A

T

b

W

S1 = f

A P O

C

S1 S2

Tensor 
Composition

Tensor 
Composition

Tensor 
Composition

Figure 3: Baseline event-embedding model.

embedding models compound vector representa-
tions over its predicate and arguments representa-
tions. The challenge is that the composition mod-
els should be effective for learning the interactions
between the predicate and the argument. Simple
additive transformations are incompetent.

We follow Ding et al. (2015) modelling such
informative interactions through tensor composi-
tion. The architecture of neural tensor network
(NTN) for learning event embeddings is shown in
Figure 3, where the bilinear tensors are used to ex-
plicitly model the relationship between the actor
and the action, and that between the object and the
action.

The inputs of NTN are the word embeddings
of A, P and O, and the outputs are event embed-
dings. We initialized our word representations us-
ing publicly available d-dimensional (d = 100)
GloVe vectors (Pennington et al., 2014). As most
event arguments consist of several words, we rep-
resent the actor, action and object as the average
of their word embeddings, respectively.

From Figure 3, S1 âˆˆ Rd is computed by:

S1 = f

(
ATT

[1:k]
1 P +W

[
A
P

]
+ b

)
g(S1) = g(A,P ) = U

TS1

(1)

where T [1:k]1 âˆˆ RdÃ—dÃ—k is a tensor, which is a
set of k matrices, each with d Ã— d dimensions.
The bilinear tensor product ATT [1:k]1 P is a vec-
tor r âˆˆ Rk, where each entry is computed by one
slice of the tensor (ri = ATT

[i]
1 P, i = 1, Â· Â· Â· , k).

The other parameters are a standard feed-forward
neural network, where W âˆˆ RkÃ—2d is the weight
matrix, b âˆˆ Rk is the bias vector, U âˆˆ Rk is a

â‰ˆ +slice=1

â€¦ â€¦ â€¦

â‰ˆ +slice=k

Figure 4: An illustration of low-rank neural tensor net-
work for learning event embeddings.

hyper-parameter and f = tanh is a standard non-
linearity applied element-wise. S2 and C in Fig-
ure 3 are computed in the same way as S1.

One problem with tensors is curse of dimen-
sionality, which limits the wide application of ten-
sors in many areas. It is therefore essential to ap-
proximate tensors of higher order in a compressed
scheme, for example, a low-rank tensor decom-
position. To decrease the number of parameters
in standard neural tensor network, we make low-
rank approximation that represents each matrix by
two low-rank matrices plus diagonal, as illustrated
in Figure 4. Formally, the parameter of the i-th
slice is T [i]appr = T [i1] Ã— T [i2] + diag(t[i]), where
T [i1] âˆˆ RdÃ—n, T [i2] âˆˆ RnÃ—d, t[i] âˆˆ Rd, n is a
hyper-parameter, which is used for adjusting the
degree of tensor decomposition. The output of
neural tensor layer is formalized as follows.

S1 = f

(
AT [Tappr]

[1:k]
1 P +W

[
A
P

]
+ b

)
, (2)

where [Tappr]
[1:k]
1 is the low-rank tensor that de-

fines multiple low-rank bilinear layers. k is the
slice number of neural tensor network which is
also equal to the output length of S1.

We assume that event tuples in the training data
should be scored higher than corrupted tuples, in
which one of the event arguments is replaced with
a random argument. Formally, the corrupted event
tuple is Er = (Ar, P,O), which is derived by re-
placing each word in A with a random word wr in
our dictionary D (which contains all the words in
the training data) to obtain a corrupted counterpart
Ar. We calculate the margin loss of the two event
tuples as:

LE = loss(E,Er) = max(0, 1âˆ’g(E)+g(Er))+Î»â€–Î¦â€–22,
(3)

where Î¦ = (T1, T2, T3,W, b) is the set of model
parameters. The standard L2 regularization is
used, for which the weight Î» is set as 0.0001. The



4897

algorithm goes over the training set for multiple
iterations. For each training instance, if the loss
loss(E,Er) = max(0, 1âˆ’g(E)+g(Er)) is equal
to zero, the online training algorithm continues to
process the next event tuple. Otherwise, the pa-
rameters are updated to minimize the loss using
back-propagation (Rumelhart et al., 1985).

2.2 Intent Embedding

Intent embedding refers to encoding the event
participantsâ€™ intents into event vectors, which
is mainly used to explain why the actor per-
formed the action. For example, given two events
â€œPersonX threw basketballâ€ and â€œPersonX threw
bombâ€, there are only subtle differences in their
surface realizations, however, the intents are to-
tally different. â€œPersonX threw basketballâ€ is just
for fun, while â€œPersonX threw bombâ€ could be a
terrorist attack. With the intents, we can easily dis-
tinguish these superficial similar events.

One challenge for incorporating intents into
event embeddings is that we should have a large-
scale labeled dataset, which annotated the event
and its actorâ€™s intents. Recently, Rashkin et al.
(2018) and Sap et al. (2019) released such valu-
able commonsense knowledge dataset (ATOMIC),
which consists of 25,000 event phrases covering a
diverse range of daily-life events and situations.
For example, given an event â€œPersonX drinks cof-
fee in the morningâ€, the dataset labels PersonXâ€™s
likely intent is â€œPersonX wants to stay awakeâ€.

We notice that the intents labeled in ATOMIC
is a sentence. Hence, intent embedding is ac-
tually a sentence representation learning task.
Among various neural networks for encoding sen-
tences, bi-directional LSTMs (BiLSTM) (Hochre-
iter and Schmidhuber, 1997) have been a dominant
method, giving state-of-the-art results in language
modelling (Peters et al., 2018) and syntactic pars-
ing (Dozat and Manning, 2016).

We use BiLSTM model to learn intent represen-
tations. BiLSTM consists of two LSTM compo-
nents, which process the input in the forward left-
to-right and the backward right-to-left directions,
respectively. In each direction, the reading of in-
put words is modelled as a recurrent process with
a single hidden state. Given an initial value, the
state changes its value recurrently, each time con-
suming an incoming word.

Take the forward LSTM component for exam-
ple. Denoting the initial state as

âˆ’â†’
h 0, which is a

model parameter, it reads the input word repre-
sentations x0,x1, . . . ,xn, and the recurrent state
transition step for calculating

âˆ’â†’
h 1, . . . ,

âˆ’â†’
h n+1 is

defined as Graves and Schmidhuber (2005).
The backward LSTM component follows the

same recurrent state transition process as the for-
ward LSTM component. Starting from an initial
state

â†âˆ’
h n+1, which is a model parameter, it reads

the input xn,xnâˆ’1, . . . ,x0, changing its value toâ†âˆ’
h n,
â†âˆ’
h nâˆ’1, . . . ,

â†âˆ’
h 0, respectively.

The BiLSTM model uses the concatenated
value of

âˆ’â†’
h t and

â†âˆ’
h t as the hidden vector for wt:

ht = [
âˆ’â†’
h t;
â†âˆ’
h t] (4)

A single hidden vector representation vi of the in-
put intent can be obtained by concatenating the
last hidden states of the two LSTMs:

vi = [
âˆ’â†’
h n+1;

â†âˆ’
h 0] (5)

In the training process, we calculate the similar-
ity between a given event vector ve and its related
intent vector vi. For effectively training the model,
we devise a ranking type loss function as follows:

LI = max(0, 1âˆ’ cosine(ve,vi) + cosine(ve,vâ€²i)) (6)

where vâ€²i is the incorrect intent for ve, which is
randomly selected from the annotated dataset.

2.3 Sentiment Embedding

Sentiment embedding refers to encoding the event
participantsâ€™ emotions into event vectors, which is
mainly used to explain how does the actor feel af-
ter the event. For example, given two events â€œPer-
sonX broke recordâ€ and â€œPersonX broke vaseâ€,
there are only subtle differences in their surface re-
alizations, however, the emotions of PersonX are
totally different. After â€œPersonX broke recordâ€,
PersonX may be feel happy, while after â€œPersonX
broke vaseâ€, PersonX could be feel sad. With the
emotions, we can also effectively distinguish these
superficial similar events.

We also use ATOMIC (Sap et al., 2019) as the
event sentiment labeled dataset. In this dataset,
the sentiment of the event is labeled as words.
For example, the sentiment of â€œPersonX broke
vaseâ€ is labeled as â€œ(sad, be regretful, feel sorry,
afraid)â€. We use SenticNet (Cambria et al.,
2018) to normalize these emotion words (W =
{w1, w2, . . . , wn}) as the positive (labeled as 1) or



4898

the negative (labeled as -1) sentiment. The senti-
ment polarity of the event Pe is dependent on the
polarity of the labeled emotion words PW : Pe =
1, if

âˆ‘
i Pwi > 0, or Pe = âˆ’1, if

âˆ‘
i Pwi < 0.

We use the softmax binary classifier to learn sen-
timent enhanced event embeddings. The input of
the classifier is event embeddings, and the output
is its sentiment polarity (positive or negative). The
model is trained in a supervised manner by min-
imizing the cross entropy error of the sentiment
classification, whose loss function is given below.

LS = âˆ’
âˆ‘
xeâˆˆC

âˆ‘
lâˆˆL

pgl (xe) Â· log(pl(xe)) (7)

where C means all training instances, L is the col-
lection of sentiment categories, xe means an event
vector, pl(xe) is the probability of predicting xe
as class l, pgl (xe) indicates whether class l is the
correct sentiment category, whose value is 1 or -1.

2.4 Joint Event, Intent and Sentiment
Embedding

Given a training event corpus with annotated in-
tents and emotions, our model jointly minimizes a
linear combination of the loss functions on events,
intents and sentiment:

L = Î±LE + Î²LI + Î³LS (8)

where Î±, Î², Î³ âˆˆ [0, 1] are model parameters to
weight the three loss functions.

We use the New York Times Gigaword Cor-
pus (LDC2007T07) for pre-training event embed-
dings. Event triples are extracted based on the
Open Information Extraction technology (Schmitz
et al., 2012). We initialize the word embedding
layer with 100 dimensional pre-trained GloVe vec-
tors (Pennington et al., 2014), and fine-tune initial-
ized word vectors during our model training. We
use Adagrad (Duchi et al., 2011) for optimizing
the parameters with initial learning rate 0.001 and
batch size 128.

3 Experiments

We compare the performance of intent and senti-
ment powered event embedding model with state-
of-the-art baselines on three tasks: event similar-
ity, script event prediction and stock prediction.

3.1 Baselines

We compare the performance of our approach
against a variety of event embedding models de-

veloped in recent years. These models can be cat-
egorized into three groups:

â€¢ Averaging Baseline (Avg) This represents
each event as the average of the constituent
word vectors using pre-trained GloVe embed-
dings (Pennington et al., 2014).

â€¢ Compositional Neural Network (Comp.
NN) The event representation in this model
is computed by feeding the concatenation of
the subject, predicate, and object embedding
into a two layer neural network (Modi and
Titov, 2013; Modi, 2016; Granroth-Wilding
and Clark, 2016).

â€¢ Element-wise Multiplicative Composition
(EM Comp.) This method simply concate-
nates the element-wise multiplications be-
tween the verb and its subject/object.

â€¢ Neural Tensor Network This line of work
use tensors to learn the interactions between
the predicate and its subject/object (Ding
et al., 2015; Weber et al., 2018). Accord-
ing to the different usage of tensors, we have
three baseline methods: Role Factor Ten-
sor (Weber et al., 2018) which represents
the predicate as a tensor, Predicate Tensor
(Weber et al., 2018) which uses two tensors
learning the interactions between the predi-
cate and its subject, and the predicate and its
object, respectively, NTN (Ding et al., 2015),
which we used as the baseline event embed-
ding model in this paper, and KGEB (Ding
et al., 2016), which incorporates knowledge
graph information in NTN.

3.2 Event Similarity Evaluation

3.2.1 Hard Similarity Task
We first follow Weber et al. (2018) evaluating
our proposed approach on the hard similarity task.
The goal of this task is that similar events should
be close to each other in the same vector space,
while dissimilar events should be far away with
each other. To this end, Weber et al. (2018) cre-
ated two types of event pairs, one with events that
should be close to each other but have very little
lexical overlap (e.g., police catch robber / author-
ities apprehend suspect), and another with events
that should be farther apart but have high overlap
(e.g., police catch robber / police catch disease).



4899

Method Hard Similarity (Accuracy %) Transitive Sentence Similarity (Ï)Small Dataset Big Dataset
Avg 5.2 13.7 0.67
Comp. NN 33.0 18.9 0.63
EM Comp. 33.9 18.7 0.57
Role Factor Tensor 43.5 20.7 0.64
Predicate Tensor 41.0 25.6 0.63
KGEB 52.6 49.8 0.61
NTN 40.0 37.0 0.60
NTN+Int 65.2 58.1 0.67
NTN+Senti 54.8 52.2 0.61
NTN+Int+Senti 77.4 62.8 0.74

Table 1: Experimental results on hard similarity dataset and transitive sentence similarity dataset. The small
dataset (230 event pairs) of hard similarity task from Weber et al. (2018), and the big dataset (2,000 event pairs) is
annotated by us. The best results are in bold.

The labeled dataset contains 230 event pairs
(115 pairs each of similar and dissimilar types).
Three different annotators were asked to give the
similarity/dissimilarity rankings, of which only
those the annotators agreed upon completely were
kept. For each event representation learning
method, we obtain the cosine similarity score of
the pairs, and report the fraction of cases where the
similar pair receives a higher cosine value than the
dissimilar pair (we use Accuracy âˆˆ [0, 1] denoting
it). To evaluate the robustness of our approach,
we extend this dataset to 1,000 event pairs (similar
and dissimilar events each account for 50%), and
we will release this dataset to the public.

3.2.2 Transitive Sentence Similarity

Except for the hard similarity task, we also evalu-
ate our approach on the transitive sentence sim-
ilarity dataset (Kartsaklis and Sadrzadeh, 2014),
which contains 108 pairs of transitive sentences:
short phrases containing a single subject, object
and verb (e.g., agent sell property). It also has an-
other dataset which consists of 200 sentence pairs.
In this dataset, the sentences to be compared are
constructed using the same subject and object and
semantically correlated verbs, such as â€˜spellâ€™ and
â€˜writeâ€™; for example, â€˜pupils write lettersâ€™ is com-
pared with â€˜pupils spell lettersâ€™. As this dataset
is not suitable for our task, we only evaluate our
approach and baselines on 108 sentence pairs.

Every pair is annotated by a human with a simi-
larity score from 1 to 7. For example, pairs such as
(design, reduce, amount) and (company, cut, cost)
are annotated with a high similarity score, while
pairs such as (wife, pour, tea) and (worker, join,
party) are given low similarity scores. Since each
pair has several annotations, we use the average

annotator score as the gold score2. To evaluate
the cosine similarity given by each model and the
annotated similarity score, we use the Spearmanâ€™s
correlation (Ï âˆˆ [âˆ’1, 1]).

3.2.3 Results
Experimental results of hard similarity and transi-
tive sentence similarity are shown in Table 1. We
find that:

(1) Simple averaging achieved competitive per-
formance in the task of transitive sentence similar-
ity, while performed very badly in the task of hard
similarity. This is mainly because hard similarity
dataset is specially created for evaluating the event
pairs that should be close to each other but have lit-
tle lexical overlap and that should be farther apart
but have high lexical overlap. Obviously, on such
dataset, simply averaging word vectors which is
incapable of capturing the semantic interactions
between event arguments, cannot achieve a sound
performance.

(2) Tensor-based compositional methods (NTN,
KGEB, Role Factor Tensor and Predicate Ten-
sor) outperformed parameterized additive models
(Comp. NN and EM Comp.), which shows that
tensor is capable of learning the semantic compo-
sition of event arguments.

(3) Our commonsense knowledge enhanced
event representation learning approach outper-
formed all baseline methods across all datasets
(achieving 78% and 200% improvements on hard
similarity small and big dataset, respectively, com-
pared to previous SOTA method), which indicates
that commonsense knowledge is useful for distin-
guishing distinct events.

2To directly compare with baseline methods (Weber et al.,
2018), this paper compares with averaged annotator scores,
other than comparing with every annotator scores.



4900

Event1 Event 2 oScore mScore Event1 Event 2 oScore mScore
man clears test he passed exam -0.08 0.40 man passed car man passed exam 0.81 0.12
he grind corn cook chops beans 0.31 0.81 he grind corn he grind teeth 0.89 0.36
he made meal chef cooked pasta 0.51 0.85 chef cooked pasta chef cooked books 0.89 0.45

farmer load truck person packs car 0.58 0.83 farmer load truck farmer load gun 0.93 0.55
player scored goal she carried team 0.19 0.44 she carried bread she carried team 0.59 0.09

Table 2: Case study of the cosine similarity score changes with incorporating the intent and sentiment. oScore
is the original cosine similarity score without intent and sentiment, and mScore is the modified cosine similarity
score with intent and sentiment.

Methods Acc (%)
SGNN 52.45
SGNN+Int 53.93
SGNN+Senti 53.57
SGNN+Int+Senti 53.88
SGNN+PairLSTM 52.71
SGNN+EventComp 54.15
SGNN+EventComp+PairLSTM 54.93
SGNN+PairLSTM+Int+Senti 54.14
SGNN+EventComp+Int+Senti 55.08
SGNN+EventComp+PairLSTM+Int+Senti 56.03

Table 3: Results of script event prediction on the test
set. The improvement is significant at p < 0.05. Acc is
short for Accuracy.

3.2.4 Case Study
To further analyse the effects of intents and emo-
tions on the event representation learning, we
present case studies in Table 2, which directly
shows the changes of similarity scores before and
after incorporating intent and sentiment. For ex-
ample, the original similarity score of two events
â€œchef cooked pastaâ€ and â€œchef cooked booksâ€ is
very high (0.89) as they have high lexical overlap.
However, their intents differ greatly. The intent
of â€œchef cooked pastaâ€ is â€œto hope his customer
enjoying the delicious foodâ€, while the intent of
â€œchef cooked booksâ€ is â€œto falsify their financial
statementsâ€. Enhanced with the intents, the sim-
ilarity score of the above two events dramatically
drops to 0.45. For another example, as the event
pair â€œman clears testâ€ and â€œhe passed examâ€ share
the same sentiment polarity, their similarity score
is boosted from -0.08 to 0.40.

3.3 Script Event Prediction

Event is a kind of important real-world knowl-
edge. Learning effective event representations can
be benefit for numerous applications. Script event
prediction (Chambers and Jurafsky, 2008) is a
challenging event-based commonsense reasoning
task, which is defined as giving an existing event
context, one needs to choose the most reasonable
subsequent event from a candidate list.

50.00%
52.00%
54.00%
56.00%
58.00%
60.00%
62.00%
64.00%
66.00%
68.00%
70.00%

Word Event NTN KGEB +Int +Senti +Int+Senti

A
cc
ur
ac
y

Methods

Figure 5: Experimental results on S&P 500 index pre-
diction. â€œ+Intâ€ means that we encode the intent infor-
mation into the original event embeddings.

Following Li et al. (2018a), we evaluate on the
standard multiple choice narrative cloze (MCNC)
dataset (Granroth-Wilding and Clark, 2016). As
SGNN proposed by Li et al. (2018a) achieved
state-of-the-art performances for this task, we
use the framework of SGNN, and only replace
their input event embeddings with our intent and
sentiment-enhanced event embeddings.

Wang et al. (2017) and Li et al. (2018a) showed
that script event prediction is a challenging prob-
lem, and even 1% of accuracy improvement is
very difficult. Experimental results shown in Ta-
ble 3 demonstrate that we can achieve more than
1.5% improvements in single model comparison
and more than 1.4% improvements in multi-model
integration comparison, just by replacing the in-
put embeddings, which confirms that better event
understanding can lead to better inference results.
An interesting result is that the event embeddings
only incorporated with intents achieved the best
result against other baselines. This confirms that
capturing peopleâ€™s intents is helpful to infer their
next plan. In addition, we notice that the event
embeddings only incorporated with sentiment also
achieve better performance than SGNN. This is
mainly because the emotional consistency does
also contribute to predicate the subsequent event.



4901

3.4 Stock Market Prediction

It has been shown that news events influence
the trends of stock price movements (Luss and
dâ€™Aspremont, 2012). As news events affect hu-
man decisions and the volatility of stock prices is
influenced by human trading, it is reasonable to
say that events can influence the stock market.

In this section, we compare with several event-
driven stock market prediction baseline methods:
(1) Word, Luss and dâ€™Aspremont (2012) use bag-
of-words represent news events for stock predic-
tion; (2) Event, Ding et al. (2014) represent events
by subject-predicate-object triples for stock pre-
diction; (3) NTN, Ding et al. (2015) learn contin-
ues event vectors for stock prediction; (4) KGEB,
Ding et al. (2016) incorporate knowledge graph
into event vectors for stock prediction.

Experimental results are shown in Figure 5. We
find that knowledge-driven event embedding is a
competitive baseline method, which incorporates
world knowledge to improve the performances of
event embeddings on the stock prediction. Sen-
timent is often discussed in predicting stock mar-
ket, as positive or negative news can affect peo-
pleâ€™s trading decision, which in turn influences the
movement of stock market. In this study, we em-
pirically show that event emotions are effective
for improving the performance of stock prediction
(+2.4%).

4 Related Work

Recent advances in computing power and NLP
technology enables more accurate models of
events with structures. Using open information
extraction to obtain structured events representa-
tions, we find that the actor and object of events
can be better captured (Ding et al., 2014). For
example, a structured representation of the event
above can be (Actor = Microsoft, Action = sues,
Object = Barnes & Noble). They report improve-
ments on stock market prediction using their struc-
tured representation instead of words as features.

One disadvantage of structured representations
of events is that they lead to increased spar-
sity, which potentially limits the predictive power.
Ding et al. (2015) propose to address this issue
by representing structured events using event em-
beddings, which are dense vectors. The goal of
event representation learning is that similar events
should be embedded close to each other in the
same vector space, and distinct events should be

farther from each other.
Previous work investigated compositional mod-

els for event embeddings. Granroth-Wilding and
Clark (2016) concatenate predicate and argument
embeddings and feed them to a neural network to
generate an event embedding. Event embeddings
are further concatenated and fed through another
neural network to predict the coherence between
the events. Modi (2016) encodes a set of events in
a similar way and use that to incrementally predict
the next event â€“ first the argument, then the predi-
cate and then next argument. Pichotta and Mooney
(2016) treat event prediction as a sequence to se-
quence problem and use RNN based models con-
ditioned on event sequences in order to predict the
next event. These three works all model narrative
chains, that is, event sequences in which a single
entity (the protagonist) participates in every event.
Hu et al. (2017) also apply an RNN approach, ap-
plying a new hierarchical LSTM model in order to
predict events by generating descriptive word se-
quences. This line of work combines the words in
these phrases by the passing the concatenation or
addition of their word embeddings to a parameter-
ized function that maps the summed vector into
event embedding space. The additive nature of
these models makes it difficult to model subtle dif-
ferences in an eventâ€™s surface form.

To address this issue, Ding et al. (2015), and
Weber et al. (2018) propose tensor-based compo-
sition models, which combine the subject, predi-
cate and object to produce the final event repre-
sentation. The models capture multiplicative inter-
actions between these elements and are thus able
to make large shifts in event semantics with only
small changes to the arguments.

However, previous work mainly focuses on the
nature of the event and lose sight of external com-
monsense knowledge, such as the intent and sen-
timent of event participants. This paper proposes
to encode intent and sentiment into event embed-
dings, such that we can obtain a kind of more pow-
erful event representations.

5 Conclusion

Understanding events requires effective repre-
sentations that contain commonsense knowledge.
High-quality event representations are valuable for
many NLP downstream applications. This paper
proposed a simple and effective framework to in-
corporate commonsense knowledge into the learn-



4902

ing process of event embeddings. Experimental
results on event similarity, script event prediction
and stock prediction showed that commonsense
knowledge enhanced event embeddings can im-
prove the quality of event representations and ben-
efit the downstream applications.

Acknowledgments

We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Key Re-
search and Development Program of China
(SQ2018AAA010010), the National Key Re-
search and Development Program of China
(2018YFB1005103), the National Natural Science
Foundation of China (NSFC) via Grant 61702137.

References
Erik Cambria, Soujanya Poria, Devamanyu Hazarika,

and Kenneth Kwok. 2018. Senticnet 5: discover-
ing conceptual primitives for sentiment analysis by
means of context embeddings. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelli-
gence, (AAAI-18), the 30th innovative Applications
of Artificial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artificial
Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789â€“797. Associa-
tion for Computational Linguistics.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2014. Using structured events to predict stock price
movement: An empirical investigation. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2014, Oc-
tober 25-29, 2014, Doha, Qatar, A meeting of SIG-
DAT, a Special Interest Group of the ACL, pages
1415â€“1425, Doha, Qatar. Association for Computa-
tional Linguistics.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2015. Deep learning for event-driven stock predic-
tion. In Proceedings of the Twenty-Fourth Inter-
national Joint Conference on Artificial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 25-31,
2015, pages 2327â€“2333.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2016. Knowledge-driven event embedding for stock
prediction. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 2133â€“2142.

Timothy Dozat and Christopher D Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. arXiv preprint arXiv:1611.01734.

John C Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121â€“2159.

Mark Granroth-Wilding and Stephen Christopher
Clark. 2016. What happens next? event predic-
tion using a compositional neural network model.
In Proceedings of the Thirtieth AAAI Conference
on Artificial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA., pages 2727â€“2733.

Alex Graves and JuÌˆrgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602â€“610.

Sepp Hochreiter and JuÌˆrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735â€“1780.

Linmei Hu, Juanzi Li, Liqiang Nie, Xiao-Li Li, and
Chao Shao. 2017. What happens next? future
subevent prediction using contextual hierarchical
lstm. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, February 4-9,
2017, San Francisco, California, USA., pages 3450â€“
3456.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. Electronic Proceedings in Theo-
retical Computer Science, 172.

Zhongyang Li, Xiao Ding, and Ting Liu. 2018a.
Constructing narrative event evolutionary graph for
script event prediction. In Proceedings of the
Twenty-Seventh International Joint Conference on
Artificial Intelligence, IJCAI 2018, July 13-19, 2018,
Stockholm, Sweden., pages 4201â€“4207.

Zhongyang Li, Xiao Ding, and Ting Liu. 2018b. Gen-
erating reasonable and diversified story ending using
sequence to sequence model with adversarial train-
ing. In Proceedings of the 27th International Con-
ference on Computational Linguistics, pages 1033â€“
1043, Santa Fe, New Mexico, USA. Association for
Computational Linguistics.

Zhongyang Li, Xiao Ding, and Ting Liu. 2019. Story
ending prediction by transferable bert. In Pro-
ceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI 2019,
Macao, China, August 10-16, 2019, pages 1800â€“
1806.

Ronny Luss and Alexandre dâ€™Aspremont. 2012.
Predicting abnormal returns from news us-
ing text classification. Quantitative Finance,
(doi:10.1080/14697688.2012.672762):1â€“14.

Ashutosh Modi. 2016. Event embeddings for se-
mantic script modeling. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning, pages 75â€“83.

http://aclweb.org/anthology/P08-1090
http://aclweb.org/anthology/P08-1090
http://www.aclweb.org/anthology/D14-1148
http://www.aclweb.org/anthology/D14-1148
https://doi.org/10.4204/EPTCS.172.17
https://doi.org/10.4204/EPTCS.172.17
https://doi.org/10.4204/EPTCS.172.17
https://www.aclweb.org/anthology/C18-1088
https://www.aclweb.org/anthology/C18-1088
https://www.aclweb.org/anthology/C18-1088
https://www.aclweb.org/anthology/C18-1088


4903

Ashutosh Modi and Ivan Titov. 2013. Learning seman-
tic script knowledge with event embeddings. In 2nd
International Conference on Learning Representa-
tions, ICLR 2014, Banff, AB, Canada, April 14-16,
2014, Workshop Track Proceedings.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532â€“1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227â€“2237.

Karl Pichotta and Raymond J Mooney. 2016. Learn-
ing statistical scripts with lstm recurrent neural net-
works. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, February 12-17,
2016, Phoenix, Arizona, USA., pages 2800â€“2806.

Hannah Rashkin, Maarten Sap, Emily Allaway,
Noah A. Smith, and Yejin Choi. 2018. Event2mind:
Commonsense inference on events, intents, and re-
actions. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 463â€“473. Associa-
tion for Computational Linguistics.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.

Maarten Sap, Ronan LeBras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2019.
Atomic: An atlas of machine commonsense for if-
then reasoning. pages 3027â€“3035.

Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523â€“534. Asso-
ciation for Computational Linguistics.

Zhongqing Wang, Yue Zhang, and Ching-Yun Chang.
2017. Integrating order information and event re-
lation for script event prediction. In Proceedings of
the 2017 Conference on Empirical Methods in Natu-
ral Language Processing, pages 57â€“67. Association
for Computational Linguistics.

Noah Weber, Niranjan Balasubramanian, and
Nathanael Chambers. 2018. Event representations
with tensor-based compositions. In Proceedings of
the Thirty-Second AAAI Conference on Artificial

Intelligence, (AAAI-18), the 30th innovative Ap-
plications of Artificial Intelligence (IAAI-18), and
the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018.

http://aclweb.org/anthology/P18-1043
http://aclweb.org/anthology/P18-1043
http://aclweb.org/anthology/P18-1043
https://doi.org/10.18653/v1/D17-1006
https://doi.org/10.18653/v1/D17-1006

