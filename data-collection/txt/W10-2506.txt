










































n-Best Parsing Revisited


Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46–54,
Uppsala, Sweden, 16 July 2010. c©2010 Association for Computational Linguistics

n-Best Parsing Revisited∗

Matthias Büchse and Daniel Geisler and Torsten Stüber and Heiko Vogler
Faculty of Computer Science

Technische Universität Dresden
01062 Dresden

{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de

Abstract

We derive and implement an algorithm
similar to (Huang and Chiang, 2005) for
finding the n best derivations in a weighted
hypergraph. We prove the correctness and
termination of the algorithm and we show
experimental results concerning its run-
time. Our work is different from the afore-
mentioned one in the following respects:
we consider labeled hypergraphs, allowing
for tree-based language models (Maletti
and Satta, 2009); we specifically handle
the case of cyclic hypergraphs; we admit
structured weight domains, allowing for
multiple features to be processed; we use
the paradigm of functional programming
together with lazy evaluation, achieving
concise algorithmic descriptions.

1 Introduction

In statistical natural language processing, proba-
bilistic models play an important role which can
be used to assign to some input sentence a set of
analyses, each carrying a probability. For instance,
an analysis can be a parse tree or a possible trans-
lation. Due to the ambiguity of natural language,
the number of analyses for one input sentence can
be very large. Some models even assign an infinite
number of analyses to an input sentence.

In many cases however, the set of analyses can
in fact be represented in a finite and compact way.
While such a representation is space-efficient, it
may be incompatible with subsequent operations.
In these cases a finite subset is used as an approx-
imation, consisting of n best analyses, i. e. n anal-
yses with highest probability. For example, this
approach has the following two applications.

(1) Reranking: when log-linear models (Och
and Ney, 2002) are employed, some features may

∗ This research was financially supported by DFG VO
1101/5-1.

not permit an efficient evaluation during the com-
putation of the analyses. These features are com-
puted using individual analyses from said approx-
imation, leading to a reranking amongst them.

(2) Spurious ambiguity: many models produce
analyses which may be too fine-grained for further
processing (Li et al., 2009). As an example, con-
sider context-free grammars, where several left-
most derivations may exist for the same terminal
string. The weight of the terminal string is ob-
tained by summing over these derivations. The
n best leftmost derivations may be used to approx-
imate this sum.

In this paper, we consider the case where the
finite, compact representation has the form of a
weighted hypergraph (with labeled hyperedges)
and the analyses are derivations of the hypergraph.
This covers many parsing applications (Klein and
Manning, 2001), including weighted deductive
systems (Goodman, 1999; Nederhof, 2003), and
also applications in machine translation (May and
Knight, 2006).

In the nomenclature of (Huang and Chiang,
2005), which we adopt here, a derivation of a hy-
pergraph is a tree which is obtained in the follow-
ing way. Starting from some node, an ingoing hy-
peredge is picked and recorded as the label of the
root of the tree. Then, for the subtrees, one con-
tinues with the source nodes of said hyperedge in
the same way. In other words, a derivation can be
understood as an unfolding of the hypergraph.

The n-best-derivations problem then amounts
to finding n derivations which are best with re-
spect to the weights induced by the weighted hy-
pergraph.1 Among others, weighted hypergraphs
with labeled hyperedges subsume the following
two concepts.

(I) probabilistic context-free grammars (pcfgs).

1Note that this problem is different from the n-best-
hyperpaths problem described by Nielsen et al. (2005), as
already argued in (Huang and Chiang, 2005, Section 2).

46



In this case, nodes correspond to nonterminals,
hyperedges are labeled with productions, and the
derivations are exactly the abstract syntax trees
(ASTs) of the grammar (which are closely related
the parse trees). Note that, unless the pcfg is un-
ambiguous, a given word may have several cor-
responding ASTs, and its weight is obtained by
summing over the weights of the ASTs. Hence,
the n best derivations need not coincide with the
n best words (cf. application (2) above).

(II) weighted tree automata (wta) (Alexandrakis
and Bozapalidis, 1987; Berstel and Reutenauer,
1982; Ésik and Kuich, 2003; Fülöp and Vogler,
2009). These automata serve both as a tree-based
language model and as a data structure for the
parse forests obtained from that language model
by applying the Bar-Hillel construction (Maletti
and Satta, 2009). It is well known that context-free
grammars and tree automata are weakly equiv-
alent (Thatcher, 1967; Ésik and Kuich, 2003).
However, unlike the former formalism, the latter
one has the ability to model non-local dependen-
cies in parse trees.

In the case of wta, nodes correspond to states,
hyperedges are labeled with input symbols, and
the derivations are exactly the runs of the automa-
ton. Since, due to ambiguity, a given tree may
have several accepting runs, the n best derivations
need not coincide with the n best trees. As for
the pcfgs, this is an example of spurious ambigu-
ity, which can be tackled as indicated by appli-
cation (2) above. Alternatively, one can attempt
to find an equivalent deterministic wta (May and
Knight, 2006; Büchse et al., 2009).

Next, we briefly discuss four known algorithms
which solve the n-best-derivations problem or
subproblems thereof.

• The Viterbi algorithm solves the 1-best-
derivation problem for acyclic hypergraphs. It is
based on a topological sort of the hypergraph.

• Knuth (1977) generalizes Dijkstra’s algorithm
(for finding the single-source shortest paths in a
graph) to hypergraphs, thus solving the case n = 1
even if the hypergraph contains cycles. Knuth as-
sumes the weights to be real numbers, and he re-
quires weight functions to be monotone and supe-
rior in order to guarantee that a best derivation ex-
ists. (The superiority property corresponds to Di-
jkstra’s requirement that edge weights—or, more
generally, cycle weights—are nonnegative.)

• Huang and Chiang (2005) show that the n-

best-derivations problem can be solved efficiently
by first solving the 1-best-derivation problem and
then extending that solution in a lazy manner.
Huang and Chiang assume weighted unlabeled hy-
pergraphs with weights computed in the reals, and
they require the weight functions to be monotone.

Moreover they assume that the 1-best-
derivation problem be solved using the Viterbi
algorithm, which implies that the hypergraph must
be acyclic. However they conjecture that their
second phase also works for cyclic hypergraphs.
• Pauls and Klein (2009) propose a variation

of the algorithm of Huang and Chiang (2005) in
which the 1-best-derivation problem is computed
via an A∗-based exploration of the 1-best charts.

In this paper, we also present an algorithm
for solving the n-best-derivations problem. Ulti-
mately it uses the same algorithmic ideas as the
one of Huang and Chiang (2005); however, it is
different in the following sense:

1. we consider labeled hypergraphs, allowing
for wta to be used in parsing;

2. we specifically handle the case of cyclic
hypergraphs, thus supporting the conjecture of
Huang and Chiang; for this we impose on the
weight functions the same requirements as Knuth
and use his algorithm;

3. by using the concept of linear pre-orders (and
not only linear orders on the set of reals) our ap-
proach can handle structured weights such as vec-
tors over frequencies, probabilities, and reals;

4. we present our algorithm in the framework
of functional programming (and not in that of im-
perative programming); this framework allows to
decribe algorithms in a more abstract and concise,
yet natural way;

5. due to the lazy evaluation paradigm often
found in functional programming, we obtain the
laziness on which the algorithm of Huang and Chi-
ang (2005) is based for free;

6. exploiting the abstract level of description
(see point 4) we are able to prove the correctness
and termination of our algorithm.

At the end of this paper, we will discuss experi-
ments which have been performed with an imple-
mentation of our algorithm in the functional pro-
gramming language HASKELL.

2 The n-best-derivations problem

In this section, we state the n-best-derivations
problem formally, and we give a comprehensive

47



example. First, we introduce some basic notions.

Trees and hypergraphs The definition of
ranked trees commonly used in formal tree lan-
guage theory will serve us as the basis for defining
derivations.

A ranked alphabet is a finite set Σ (of symbols)
where every symbol carries a rank (a nonnegative
integer). By Σ(k) we denote the set of those sym-
bols having rank k. The set of trees over Σ, de-
noted by TΣ , is the smallest set T such that for
every k ∈ N, σ ∈ Σ(k), and ξ1, . . . , ξk ∈ T ,
also σ(ξ1, . . . , ξk) ∈ T ;2 for σ ∈ Σ(0) we ab-
breviate σ() by σ. For every k ∈ N, σ ∈
Σ(k) and subsets T1, . . . , Tk ⊆ TΣ we define
the top-concatenation (with σ) σ(T1, . . . , Tk) =
{σ(ξ1, . . . , ξk) | ξ1 ∈ T1, . . . , ξk ∈ Tk}.

A Σ-hypergraph is a pair H = (V,E) where
V is a finite set (of vertices or nodes) and E ⊆
V ∗×Σ×V is a finite set (of hyperedges) such that
for every (v1 . . . vk, σ, v) ∈ E we have that σ ∈
Σ(k).3 We interpret E as a ranked alphabet where
the rank of each edge is carried over from its label
in Σ. The family (Hv | v ∈ V ) of derivations of H
is the smallest family (Pv | v ∈ V ) of subsets
of TE such that e(Pv1 , . . . , Pvk) ⊆ Pv for every
e = (v1 . . . vk, σ, v) ∈ E.

A Σ-hypergraph (V,E) is cyclic if there
are hyperedges (v11 . . . v

1
k1

, σ1, v
1), . . . ,

(vl1 . . . v
l
kl

, σl, v
l) ∈ E such that vj−1 occurs

in vj1 . . . v
j
kj

for every j ∈ {2, . . . , l} and vl occurs

in v11 . . . v
1
k1

. It is called acyclic if it is not cyclic.

Example 1 Consider the ranked alphabet Σ =
Σ(0)∪Σ(1)∪Σ(2) with Σ(0) = {α, β}, Σ(1) = {γ},
and Σ(2) = {σ}, and the Σ-hypergraph H =
(V,E) where

• V = {0, 1} and

• E = {(ε, α, 1), (ε, β, 1), (1, γ, 1), (11, σ, 0),
(1, γ, 0)}.

A graphical representation of this hypergraph is
shown in Fig. 1. Note that this hypergraph is cyclic
because of the edge (1, γ, 1).

We indicate the derivations of H , assuming that
e1, . . . , e5 are the edges in E in the order given
above:

2The term σ(ξ1, . . . , ξk) is usually understood as a string
composed of the symbol σ, an opening parenthesis, the
string ξ1, a comma, and so on.

3The hypergraphs defined here are essentially nondeter-
ministic tree automata, where V is the set of states and E is
the set of transitions.

γ α

0 1 γ

σ β

e5

e3

e4

e1

e2

Figure 1: Hypergraph of Example 1.

• H1 = {e1, e2, e3(e1), e3(e2), e3(e3(e1)), . . . }
and

• H0 = e4(H1,H1) ∪ e5(H1) where, e. g.,
e4(H1,H1) is the top-concatenation of H1,
H1 with e4, and thus

e4(H1,H1) = {e4(e1, e1), e4(e1, e2),

e4(e1, e3(e1)), e4(e3(e1), e1), . . . } .

Next we give an example of ambiguity in hyper-
graphs with labeled hyperedges. Suppose that E
contains an additional hyperedge e6 = (0, γ, 0).
Then H0 would contain the derivations e6(e5(e1))
and e5(e3(e1)), which describe the same Σ-tree,
viz. γ(γ(α)) (obtained by the node-wise projec-
tion to the second component). �

In the sequel, let H = (V,E) be a Σ-hypergraph.

Ordering Usually an ordering is induced on the
set of derivations by means of probabilities or,
more generally, weights. In the following, we will
abstract from the weights by using a binary rela-
tion - directly on derivations, where we will in-
terpret the fact ξ1 - ξ2 as “ξ1 is better than or
equal to ξ2”.

Example 2 (Ex. 1 contd.) First we show how an
ordering is induced on derivations by means of
weights. To this end, we associate an operation
over the set R of reals with every hyperedge (re-
specting its arity) by means of a mapping θ:

θ(e1)() = 4 θ(e2)() = 3

θ(e3)(x1) = x1 + 1 θ(e4)(x1, x2) = x1 + x2

θ(e5)(x1) = x1 + 0.5

The weight h(ξ) of a tree ξ ∈ TE is obtained by
interpreting the symbols at each node using θ, e. g.
h(e3(e2)) = θ(e3)(θ(e2)()) = θ(e2)() + 1 = 4.

Then the natural order ≤ on R induces the bi-
nary relation - over TE as follows: for every
ξ1, ξ2 ∈ TE we let ξ1 - ξ2 iff h(ξ1) ≤ h(ξ2),
meaning that trees with smaller weights are con-
sidered better. (This is, e. g., the case when calcu-
lating probabilites in the image of − log x.) Note

48



that we could just as well have defined - with the
inverted order.

Since addition is commutative, we obtain
for every ξ1, ξ2 ∈ TE that h(e4(ξ1, ξ2)) =
h(e4(ξ2, ξ1)) and thus e4(ξ1, ξ2) - e4(ξ2, ξ1) and
vice versa. Thus, for two different trees (e4(ξ1, ξ2)
and e4(ξ2, ξ1)) having the same weight, - should
not prefer any of them. That is, - need not be
antisymmetric.

As another example, the mapping θ could as-
sign to each symbol an operation over real-valued
vectors, where each component represents one
feature of a log-linear model such as frequencies,
probabilities, reals, etc. Then the ordering could
be defined by means of a linear combination of the
feature weights. �

We use the concept of a linear pre-order to cap-
ture the orderings which are obtained this way.

Let S be a set. A pre-order (on S) is a binary
relation - ⊆ S × S such that (i) s - s for ev-
ery s ∈ S (reflexivity) and (ii) s1 - s2 and s2 - s3
implies s1 - s3 for every s1, s2, s3 ∈ S (transi-
tivity). A pre-order - is called linear if s1 - s2
or s2 - s1 for every s1, s2 ∈ S. For instance, the
binary relation - on TE as defined in Ex. 2 is a
linear pre-order.

We will restrict our considerations to a class
of linear pre-orders which admit efficient algo-
rithms. For this, we will always assume a lin-
ear pre-order - with the following two properties
(cf. Knuth (1977)).4

SP (subtree property) For every e(ξ1, . . . , ξk) ∈
TE and i ∈ {1, . . . , k} we have ξi -
e(ξ1, . . . , ξk).5

CP (compatibility) For every pair e(ξ1, . . . , ξk),
e(ξ′1, . . . , ξ

′
k) ∈ TE with ξ1 - ξ

′
1, . . . ,

ξk - ξ
′
k we have that e(ξ1, . . . , ξk) -

e(ξ′1, . . . , ξ
′
k).

It is easy to verify that the linear pre-order - of
Ex. 2 has the aforementioned properties.

In the sequel, let - be a linear pre-order
on TE fulfilling SP and CP.

4Originally, these properties were called “superiority” and
“monotonicity” because they were viewed as properties of
the weight functions. We use the terms “subtree property”
and “compatibility” respectively, because we view them as
properties of the linear pre-order.

5This strong property is used here for simplicity. It suf-
fices to require that for every v ∈ V and pair ξ, ξ′ ∈ Hv we
have ξ - ξ′ if ξ is a subtree of ξ′.

Before we state the n-best-derivations problem
formally, we define the operation minn, which
maps every subset T of TE to the set of all se-
quences of n best elements of T . To this end, let
T ⊆ TE and n ≤ |T |. We define minn(T ) to be
the set of all sequences (ξ1, . . . , ξn) ∈ T n of pair-
wise distinct elements such that ξ1 - . . . - ξn and
for every ξ ∈ T \ {ξ1, . . . , ξk} we have ξn - ξ.
For every n > |T | we set minn(T ) = min|T |(T ).
In addition, we set min≤n(T ) =

⋃n
i=0 mini(T ).

n-best-derivations problem The n-best-
derivations problem amounts to the following.

Given a Σ-hypergraph H = (V,E), a vertex v ∈
V , and a linear pre-order - on TE fulfilling
SP and CP,

compute an element of minn(Hv).

3 Functional Programming

We will describe our main algorithm as a func-
tional program. In essence, such a program is a
system of (recursive) equations that defines sev-
eral functions (as shown in Fig. 2). As a conse-
quence the main computational paradigm for eval-
uating the application (f a) of a function f to an
argument a is to choose an appropriate defining
equation f x = r and then evaluate (f a) to r’
which is obtained from r by substituting every oc-
currence of x by a.

We assume a lazy (and in particular, call-by-
need) evaluation strategy, as in the functional pro-
gramming language HASKELL. Roughly speak-
ing, this amounts to evaluating the arguments of
a function only as needed to evaluate the its body
(i. e. for branching). If an argument occurs multi-
ple times in the body, it is evaluated only once.

We use HASKELL notation and functions for
dealing with lists, i. e. we denote the empty list by
[] and list construction by x:xs (where an ele-
ment x is prepended to a list xs), and we use the
functions head (line 01), tail (line 02), and take
(lines 03 and 04), which return the first element in
a list, a list without its first element, and a prefix
of a list, respectively.

In fact, the functions shown in Fig. 2 will be
used in our main algorithm (cf. Fig. 4). Thus,
we explain the functions merge (lines 05–07) and
e(l1, . . . ,lk) (lines 08–10) a bit more in detail.

The merge function takes a set L of pairwise
disjoint lists of derivations, each one in ascend-
ing order with respect to -, and merges them into

49



-- standard Haskell functions: list deconstructors, take operation
01 head (x:xs) = x
02 tail (x:xs) = xs
03 take n xs = [] if n == 0 or xs == []
04 take n xs = (head xs):take (n-1) (tail xs)

-- merge operation (lists in L should be disjoint)
05 merge L = [] if L \ {[]} = ∅
06 merge L = m:merge ({tail l | l ∈ L, l != [], head l == m} ∪

{l | l ∈ L, l != [], head l != m})

07 where m = min{head l | l ∈ L, l != []}

-- top concatenation
08 e(l1, . . . ,lk) = [] if li == [] for some i ∈ {1, . . . , k}
09 e(l1, . . . ,lk) = e(head l1, . . . , head lk):merge {e(l

i
1, . . . ,l

i
k) | i ∈ {1, . . . , k}}

10 where lij =











lj if j < i

tail lj if j = i

[head lj] if j > i

Figure 2: Some useful functions specified in a functional programming style.

one list with the same property (as known from the
merge sort algorithm).

Note that the minimum used in line 07 is based
on the linear pre-order -. For this reason, it
need not be uniquely determined. However, in an
implementation this function is deterministic, de-
pending on the the data structures.

The function e(l1, . . . ,lk) implements the top-
concatenation with e on lists of derivations. It is
defined for every e = (v1 . . . vk, σ, v) ∈ E and
takes lists l1, . . . , lk of derivations, each in as-
cending order as for merge. The resulting list is
also in ascending order.

4 Algorithm

In this section, we develop our algorithm for solv-
ing the n-best-derivations problem. We begin by
motivating our general approach, which amounts
to solving the 1-best-derivation problem first and
then extending that solution to a solution of the n-
best-derivations problem.

It can be shown that for every m ≥ n, the
set minn(Hv) is equal to the set of all prefixes of
length n of elements of minm(Hv). According
to this observation, we will develop a function p
mapping every v ∈ V to a (possibly infinite) list
such that the prefix of length n is in minn(Hv)
for every n. Then, by virtue of lazy evaluation, a
solution to the n-best-derivations problem can be

obtained by evaluating the term

take n (p v)

where take is specified in lines 03–04 of Fig. 2.
Thus, what is left to be done is to specify p appro-
priately.

4.1 A provisional specification of p

Consider the following provisional specification
of p:

p v = merge {e(p v1, . . . ,p vk) |
e = (v1 . . . vk, σ, v) ∈ E} (†)

where the functions merge and e(l1, . . . ,lk) are
specified in lines 05–07 and lines 08–10 of Fig. 2,
respectively. This specification models exactly the
trivial equation

Hv =
⋃

e=(v1...vk ,σ,v)∈E

e(Hv1 , . . . ,Hvk)

for every v ∈ V , where the union and the top-
concatenation have been implemented for lists via
the functions merge and e(l1, . . . ,lk).

This specification is adequate if H is acyclic.
For cyclic hypergraphs however, it can not even
solve the 1-best-derivation problem. To illustrate
this, we consider the hypergraph of Ex. 2 and cal-

50



culate6

take 1 (p 1)

= (head (p 1)):take 0 (tail (p 1)) (04)

= head (p 1) (03)

= head (merge {e1(), e2(), e3(p 1)}) (†)

= min{head e1(), head e2(), head e3(p 1)}

(01, 06, 07)

= min{head e1(), head e2(), e3(head (p 1))}.

(09)

Note that the infinite regress occurs because the
computation of the head element head (p 1) de-
pends on itself. This leads us to the idea of
“pulling” this head element (which is the solu-
tion to the 1-best-derivation problem) “out” of the
merge in (†). Applying this idea to our particular
example, we reach the following equation for p 1:

p 1 = e2: merge {e1(), e3(p 1)}

because e2 is the best derivation in H1. Then, in
order to evaluate merge we have to compute

min{head e1(), head e3(p 1)}

= min{e1, e3(head (p 1))}

= min{e1, e3(e2)}.

Since h(e1) = h(e3(e2)) = 4, we can choose any
of them, say e1, and continue:

e2: merge {e1(), e3(p 1)}

= e2: e1: merge {tail e1(), e3(p 1)}

= e2: e1: e3(e2): merge {tail e3(p 1)}

= ...

Generalizing this example, the function p could
be specified as follows:

p 1 = (b 1) : merge {exp} (††)

where b 1 evaluates the 1-best derivation in H1
and exp “somehow” calculates the next best
derivations. In the following subsection, we elabo-
rate this approach. First, we develop an algorithm
for solving the 1-best-derivation problem.

4.2 Solving the 1-best-derivation problem

Using SP and CP, it can be shown that for ev-
ery v ∈ V such that Hv 6= ∅ there is a mini-
mal derivation in Hv which does not contain any
subderivation in Hv (apart from itself). In other
words, it is not necessary to consider cycles when
solving the 1-best-derivation problem.

6Please note that e1() is an application of the function in
lines 08–10 of Fig. 2 while e1 is a derivation.

We can exploit this knowledge in a program by
keeping a set U of visited nodes, taking care not to
consider edges which lead us back to those nodes.
Consider the following function:

b v U = min{e(b v1 U’, . . . , b vk U’) |
e = (v1 . . . vk, σ, v) ∈ E,
{v1, . . . , vk} ∩ U’ = ∅}

where U’ = U ∪ {v}

The argument U is the set of visited nodes. The
term b v ∅ evaluates to a minimal element of Hv,
or to min ∅ if Hv = ∅. The problem of this divide-
and-conquer (or top-down) approach is that man-
aging a separate set U for every recursive call in-
curs a big overhead in the computation.

This overhead can be avoided by using a
dynamic programming (or bottom-up) approach
where each node is visited only once, and nodes
are visited in the order of their respective best
derivations.

To be more precise, we maintain a family (Pv |
v ∈ V ) of already found best derivations (where
Pv ∈ min≤1(Hv) and initially empty) and a set C
of candidate derivations, where candidates for all
vertices are considered at the same time. In each
iteration, a minimal candidate with respect to - is
selected. This candidate is then declared the best
derivation of its respective node.

The following lemma shows that the bottom-up
approach is sound.

Lemma 3 Let (Pv | v ∈ V ) be a family such that
Pv ∈ min≤1(Hv). We define

C =
⋃

e=(v1...vk ,σ,v)∈E,
Pv=∅

e(Pv1 , . . . , Pvk) .

Then (i) for every ξ ∈
⋃

v∈V,Pv=∅
Hv there is a

ξ′ ∈ C such that ξ′ � ξ, and (ii) for every v ∈ V
and ξ ∈ C ∩ Hv the following implication holds:
if ξ ≤ ξ′ for every ξ′ ∈ C , then ξ ∈ min1(Hv).

An algorithm based on this lemma is shown in
Fig. 3. Its key function iter uses the notion of ac-
cumulating parameters. The parameter q is a map-
ping corresponding to the family (Pv | v ∈ V ) of
the lemma, i. e., q v = Pv; the parameter c is a
set corresponding to C . We begin in line 01 with
the function q0 mapping every vertex to the empty
list. According to the lemma, the candidates then
consist of the nullary edges.

As long as there are candidates left (line 04),
in a recursive call of iter the parameter q is up-
dated with the newly found pair (v, [ξ]) of ver-
tex v and (list of) best derivation ξ (expressed by

51



Require Σ-hypergraph H = (V,E), linear pre-
order - fulfilling SP and CP.
Ensure b v ∈ min1(Hv) for every v ∈ V
such that if b v == [e(ξ1, . . . , ξk)] for some
e = (v1 . . . vk, σ, v) ∈ E, then b vi == [ξi] for
every i ∈ {1, . . . , k}.

01 b = iter q0 {(ε, α, v) ∈ E | α ∈ Σ(0)}

02 q0 v = []

03 iter q ∅ = q
04 iter q c = iter (q//(v,[ξ])) c’
05 where
06 ξ = min c and ξ ∈ Hv
07 c’ =

⋃

e=(v1...vk,σ,v)∈E
q v == []

e(q v1, . . . ,q vk)

Figure 3: Algorithm solving the 1-best-derivation
problem.

q//(v,[ξ])) and the candidate set is recomputed
accordingly. When the candidate set is exhausted
(line 03), then q is returned.

Correctness and completeness of the algorithm
follow from Statements (ii) and (i) of Lemma 3,
respectively. Now we show termination. In every
iteration a new next best derivation is determined
and the candidate set is recomputed. This set only
contains candidates for vertices v ∈ V such that
q v == []. Hence, after at most |V | iterations
the candidates must be depleted, and the algorithm
terminates.

We note that the algorithm is very similar to that
of Knuth (1977). However, in contrast to the latter,
(i) it admits Hv = ∅ for some v ∈ V and (ii) it
computes some minimal derivation instead of the
weight of some minimal derivation.

Runtime According to the literature, the run-
time of Knuth’s algorithm is in O(|E| · log|V |)
(Knuth, 1977). This statement relies on a number
of optimizations which are beyond our scope. We
just sketch two optimizations: (i) the candidate set
can be implemented in a way which admits ob-
taining its minimum in O(log|C|), and (ii) for the
computation of candidates, each edge needs to be
considered only once during the whole run of the
algorithm.

4.3 Solving the n-best-derivations problem

Being able to solve the 1-best-derivation problem,
we can now refine our specification of p. The re-
fined algorithm is given in Fig. 4; for the func-

tions not given there, please refer to Fig. 3 (func-
tion b) and to Fig. 2 (functions merge, tail, and
the top-concatenation). In particular, line 02 of
Fig. 4 shows the general way of “pulling out” the
head element as it was indicated in Section 4.1 via
an example. We also remark that the definition of
the top-concatenation (lines 08–09 of Fig. 2) cor-
responds to the way in which mult�k was sped up
in Fig. 4 of (Huang and Chiang, 2005).

Theorem 4 The algorithm in Fig. 4 is correct with
respect to its require/ensure specification and it
terminates for every input.

PROOF (SKETCH). We indicate how induction on n
can be used for the proof. If n = 0, then the statement
is trivially true. Let n > 0. If b v == [], then the
statement is trivially true as well. Now we consider the
converse case. To this end, we use the following three
auxiliary statements.

(1) take n (merge {l1, . . . ,lk}) =
take n (merge {take n l1, . . . ,take n lk}),

(2) take n e(l1, . . . ,lk) =
take n e(take n l1, . . . ,take n lk),

(3) take n (tail l) = tail (take (n+1) l).

Using these statements, line 04 of Fig. 2, and line 02
of Fig. 4, we are able to “pull” the take of take n (p
v) “into” the right-hand side of p v, ultimately yield-
ing terms of the form take n (p vj) in the first line
of the merge application and take (n-1) (p v′j) in
the second one.

Then we can show the following statement by induc-
tion on m (note that the n is still fixed from the outer
induction): for every m ∈ N we have that if the tree
in b v has at most height m, then take n (p v) ∈
minn(Hv). To this end, we use the following two aux-
iliary statements.

(4) For every sequence of pairwise disjoint sub-
sets P1, . . . , Pk ⊆

⋃

v∈V Hv, sequence of nat-
ural numbers n1, . . . , nk ∈ N, and lists l1 ∈
minn1(P1), . . . , lk ∈ minnk(Pk) such that
nj ≥ n for every j ∈ {1, . . . , k} we have that
take n (merge {l1, . . . , lk}) ∈ minn(P1∪. . .∪Pk).

(5) For every edge e = (v1 . . . vk, σ, v) ∈ E, subsets
P1, . . . , Pk ⊆

⋃

v∈V Hv , and lists l1 ∈ minn(P1), . . . ,
lk ∈ minn(Pk) we have that take n e(l1, . . . , lk) ∈
minn(e(P1, . . . , Pk)).

Using these statements, it remains to show that
{e(ξ1, . . . , ξk)} ◦ minn−1

(

(e(Hv1 , . . . , Hvk) \

{e(ξ1, . . . , ξk)}) ∪
⋃

e′ 6=e e
′(Hv′

1
, . . . , Hv′

k
)
)

⊆
minn(Hv) where b v = [e(ξ1, . . . , ξk)] and ◦
denotes language concatenation. This can be shown by
using the definition of minn.

Termination of the algorithm now follows from the
fact that every finite prefix of p v is well defined. �

52



Require Σ-hypergraph H = (V,E), linear pre-order - fulfilling SP and CP.
Ensure

(

take n (p v)
)

∈ minn(Hv) for every v ∈ V and n ∈ N.

01 p v = [] if b v == []
02 p v = e(ξ1, . . . , ξk):merge ({tail e(p v1, . . . , p vk) | e = (v1 . . . vk, σ, v) ∈ E} ∪

{e′(p v′1, . . . , p v
′
k) | e

′ = (v′1 . . . v
′
k, σ

′, v) ∈ E, e′ 6= e})

if b v == [e(ξ1, . . . , ξk)]

Figure 4: Algorithm solving the n-best-derivations problem.

4.4 Implementation, Complexity, and
Experiments

We have implemented the algorithm (consisting
of Figs. 3 and 4 and the auxiliary functions of
Fig. 2) in HASKELL. The implementation is
rather straightforward except for the following
three points.

(1) Weights: we assume that - is defined by
means of weights (cf. Ex. 2), and that comparing
these weights is in O(1) (which often holds be-
cause of limited precision). Hence, we store with
each derivation its weight so that comparison ac-
cording to - is in O(1) as well.

(2) Memoization: we use a memoization tech-
nique to ensure that no derivation occurring in p v
is computed twice.

(3) Merge: the merge operation deserves some
consideration because it is used in a nested fash-
ion, yielding trees of merge applications. This
leads to an undesirable runtime complexity be-
cause these trees need not be balanced. Thus, in-
stead of actually computing the merge in p and in
the top-concatenation, we just return a data struc-
ture describing what should be merged. That data
structure consists of a best element and a list of
lists of derivations to be merged (cf. lines 06 and
09 in Fig. 2). We use a higher-order function to
manage these data structures on a heap, perform-
ing the merge in a nonnested way.

Runtime Here we consider the n-best part of the
algorithm, i. e. we assume the computation of the
mapping b to take constant time. Note however
that due to memoization, b is only computed once.
Then the runtime complexity of our implementa-
tion is in O

(

|E|+ |V | ·n · log(|E|+n)
)

. This can
be seen as follows.

By line 02 in Fig. 4, the initial heaps in the
higher-order merge described under (3) have a to-
tal of |E| elements. Building these heaps is thus
in O(|E|). By line 09 in Fig. 2, each newly found
derivation spawns at most as many new candidates

n total time [s] time for n-best part [s]

1 8.713 —
25 000 10.832 2.119
50 000 12.815 4.102

100 000 16.542 7.739
200 000 24.216 15.503

Table 1: Experimental results

on the heap as the maximum rank in Σ. We assume
this to be constant. Moreover, at most n deriva-
tions are computed for each node, that is, at most
|V |·n in total. Hence, the size of the heap of a node
is in O(|E|+n). For each derivation we compute,
we have to pop the minimal element off the heap
(cf. line 07 in Fig. 2), which is in O

(

log(|E|+n)
)

,
and we have to compute the union of the remaining
heap with the newly spawned candidates, which
has the same complexity.

We give another estimate for the total number
of derivations computed by the algorithm, which
is based on the following observation. When pop-
ping a new derivation ξ off the heap, new next best
candidates are computed. This involves comput-
ing at most as many new derivations as the number
of nodes of ξ, because for each hyperedge occur-
ring in ξ we have to consider the next best alter-
native. Since we pop off at most n elements from
the heap belonging to the target node, we arrive at
the estimate d ·n, where d is the size of the biggest
derivation of said node.

A slight improvement of the runtime complex-
ity can be obtained by restricting the heap size to
n best elements, as argued by Huang and Chiang
(2005). This way, they are able to obtain the com-
plexity O(|E|+ d · n · log n).

We have conducted experiments on an Intel
Core Duo 1200 MHz with 2 GB of RAM using
a cyclic hypergraph containing 671 vertices and
12136 edges. The results are shown in Table 1.
This table indicates that the runtime of the n-best
part is roughly linear in n.

53



References

Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene’s theorem.
Inform. Process. Lett., 24(1):1–4.

Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115–148.

Matthias Büchse, Jonathan May, and Heiko Vogler.
2009. Determinization of weighted tree automata
using factorizations. Talk presented at FSMNLP 09
in Pretoria, South Africa.

Zoltán Ésik and Werner Kuich. 2003. Formal tree se-
ries. J. Autom. Lang. Comb., 8(2):219–285.

Zoltán Fülöp and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9. Springer.

Joshua Goodman. 1999. Semiring parsing. Comp.
Ling., 25(4):573–605.

Liang Huang and David Chiang. 2005. Better k-
best parsing. In Parsing ’05: Proceedings of the
Ninth International Workshop on Parsing Technol-
ogy, pages 53–64. ACL.

Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT, pages
123–134.

Donald E. Knuth. 1977. A Generalization of Dijkstra’s
Algorithm. Inform. Process. Lett., 6(1):1–5, Febru-
ary.

Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL-IJCNLP ’09, pages 593–601.
ACL.

Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Conf. Parsing Technologies, pages 1–12. ACL.

Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In Proc. HLT, pages 351–358. ACL.

Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Comp. Ling., 29(1):135–
143.

Lars Relund Nielsen, Kim Allan Andersen, and
Daniele Pretolani. 2005. Finding the k shortest hy-
perpaths. Comput. Oper. Res., 32(6):1477–1497.

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295–302.

Adam Pauls and Dan Klein. 2009. k-best a* parsing.
In Proc. ACL-IJCNLP ’09, pages 958–966, Morris-
town, NJ, USA. ACL.

J. W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization
of finite automata theory. J. Comput. Syst. Sci.,
1(4):317–322.

54


