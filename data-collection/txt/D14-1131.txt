



















































Exact Decoding for Phrase-Based Statistical Machine Translation


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Exact Decoding for Phrase-Based Statistical Machine Translation

Wilker Aziz† Marc Dymetman‡ Lucia Specia†

†Department of Computer Science, University of Sheffield, UK
W.Aziz@sheffield.ac.uk

L.Specia@sheffield.ac.uk
‡Xerox Research Centre Europe, Grenoble, France

Marc.Dymetman@xrce.xerox.com

Abstract

The combinatorial space of translation
derivations in phrase-based statistical ma-
chine translation is given by the intersec-
tion between a translation lattice and a tar-
get language model. We replace this in-
tractable intersection by a tractable relax-
ation which incorporates a low-order up-
perbound on the language model. Exact
optimisation is achieved through a coarse-
to-fine strategy with connections to adap-
tive rejection sampling. We perform ex-
act optimisation with unpruned language
models of order 3 to 5 and show search-
error curves for beam search and cube
pruning on standard test sets. This is the
first work to tractably tackle exact opti-
misation with language models of orders
higher than 3.

1 Introduction

In Statistical Machine Translation (SMT), the task
of producing a translation for an input string x =
〈x1, x2, . . . , xI〉 is typically associated with find-
ing the best derivation d∗ compatible with the in-
put under a linear model. In this view, a derivation
is a structured output that represents a sequence of
steps that covers the input producing a translation.
Equation 1 illustrates this decoding process.

d∗ = argmax
d∈D(x)

f(d) (1)

The set D(x) is the space of all derivations com-
patible with x and supported by a model of trans-
lational equivalences (Lopez, 2008). The func-
tion f(d) = Λ · H(d) is a linear parameteri-
sation of the model (Och, 2003). It assigns a
real-valued score (or weight) to every derivation
d ∈ D(x), where Λ ∈ Rm assigns a relative
importance to different aspects of the derivation

independently captured by m feature functions
H(d) = 〈H1(d), . . . ,Hm(d)〉 ∈ Rm.

The fully parameterised model can be seen as
a discrete weighted set such that feature func-
tions factorise over the steps in a derivation. That
is, Hk(d) =

∑
e∈d hk(e), where hk is a (local)

feature function that assesses steps independently
and d = 〈e1, e2, . . . , el〉 is a sequence of l steps.
Under this assumption, each step is assigned the
weightw(e) = Λ ·〈h1(e), h2(e), . . . , hm(e)〉. The
setD is typically finite, however, it contains a very
large number of structures — exponential (or even
factorial, see §2) with the size of x — making
exhaustive enumeration prohibitively slow. Only
in very restricted cases combinatorial optimisation
techniques are directly applicable (Tillmann et al.,
1997; Och et al., 2001), thus it is common to resort
to heuristic techniques in order to find an approxi-
mation to d∗ (Koehn et al., 2003; Chiang, 2007).

Evaluation exercises indicate that approximate
search algorithms work well in practice (Bojar
et al., 2013). The most popular algorithms pro-
vide solutions with unbounded error, thus pre-
cisely quantifying their performance requires the
development of a tractable exact decoder. To
date, most attempts were limited to short sentences
and/or somewhat toy models trained with artifi-
cially small datasets (Germann et al., 2001; Igle-
sias et al., 2009; Aziz et al., 2013). Other work
has employed less common approximations to the
model reducing its search space complexity (Ku-
mar et al., 2006; Chang and Collins, 2011; Rush
and Collins, 2011). These do not answer whether
or not current decoding algorithms perform well at
real translation tasks with state-of-the-art models.

We propose an exact decoder for phrase-based
SMT based on a coarse-to-fine search strategy
(Dymetman et al., 2012). In a nutshell, we re-
lax the decoding problem with respect to the Lan-
guage Model (LM) component. This coarse view
is incrementally refined based on evidence col-

1237



lected via maximisation. A refinement increases
the complexity of the model only slightly, hence
dynamic programming remains feasible through-
out the search until convergence. We test our de-
coding strategy with realistic models using stan-
dard data sets. We also contribute with optimum
derivations which can be used to assess future im-
provements to approximate decoders. In the re-
maining sections we present the general model
(§2), survey contributions to exact optimisation
(§3), formalise our novel approach (§4), present
experiments (§5) and conclude (§6).

2 Phrase-based SMT

In phrase-based SMT (Koehn et al., 2003), the
building blocks of translation are pairs of phrases
(or biphrases). A translation derivation d is an
ordered sequence of non-overlapping biphrases
which covers the input text in arbitrary order gen-
erating the output from left to right.1

f(d) = ψ(y) +
l∑

i=1

φ(ei) +
l−1∑
i=1

δ(ei, ei−1) (2)

Equation 2 illustrates a standard phrase-based
model (Koehn et al., 2003): ψ is a weighted tar-
get n-gram LM component, where y is the yield
of d; φ is a linear combination of features that
decompose over phrase pairs directly (e.g. back-
ward and forward translation probabilities, lexi-
cal smoothing, and word and phrase penalties);
and δ is an unlexicalised penalty on the num-
ber of skipped input words between two adjacent
biphrases. The weighted logic program in Figure
1 specifies the fully parameterised weighted set of
solutions, which we denote 〈D(x), f(d)〉.2

A weighted logic program starts from its ax-
ioms and follows exhaustively deducing new items
by combination of existing ones and no deduction
happens twice. In Figure 1, a nonteminal item
summarises partial derivation (or hypotheses). It is
denoted by [C, r, γ] (also known as carry), where:
C is a coverage vector, necessary to impose the
non-overlapping constraint; r is the rightmost po-
sition most recently covered, necessary for the
computation of δ; and γ is the last n − 1 words

1Preventing phrases from overlapping requires an expo-
nential number of constraints (the powerset of x) rendering
the problem NP-complete (Knight, 1999).

2Weighted logics have been extensively used to describe
weighted sets (Lopez, 2009), operations over weighted sets
(Chiang, 2007; Dyer and Resnik, 2010), and a variety of dy-
namic programming algorithms (Cohen et al., 2008).

ITEM
[{0, 1}I , [0, I + 1],∆n−1]

GOAL
[
1I , I + 1, EOS

]
AXIOM
〈BOS→ BOS〉

[0I , 0, BOS] : ψ(BOS)
EXPAND[
C, r, yj−1j−n+1

] 〈
xi

′
i

φr−−→ yj′j
〉

[
C′, i′, yj

′
j′−n+2

]
: w

⊕i′
k=i ck = 0̄

where c′k = ck if k < i or k > i
′ else 1̄

w = φr ⊗ δ(r, i)⊗ ψ(yj′j |yj−1j−n+1)
ACCEPT [

1I , r, γ
]

[1I , I + 1, EOS] : δ(r, I + 1)⊗ ψ(EOS|γ) r ≤ I

Figure 1: Specification for the weighted set of
translation derivations in phrase-based SMT with
unconstrained reordering.

in the yield, necessary for the LM component. The
program expands partial derivations by concatena-
tion with a translation rule

〈
xi

′
i

φr−−→ yj′j
〉

, that is, an

instantiated biphrase which covers the span xi
′
i and

yields yj
′
j with weight φr. The side condition im-

poses the non-overlapping constraint (ck is the kth
bit in C). The antecedents are used to compute the
weight of the deduction, and the carry is updated
in the consequent (item below the horizontal line).
Finally, the rule ACCEPT incorporates the end-of-
sentence boundary to complete items.3

It is perhaps illustrative to understand the set of
weighted translation derivations as the intersection
between two components. One that is only locally
parameterised and contains all translation deriva-
tions (a translation lattice or forest), and one that
re-ranks the first as a function of the interactions
between translation steps. The model of transla-
tional equivalences parameterised only with φ is
an instance of the former. An n-gram LM compo-
nent is an instance of the latter.

2.1 Hypergraphs

A backward-hypergraph, or simply hypergraph,
is a generalisation of a graph where edges have
multiple origins and one destination (Gallo et al.,
1993). They can represent both finite-state and
context-free weighted sets and they have been
widely used in SMT (Huang and Chiang, 2007).
A hypergraph is defined by a set of nodes (or ver-

3Figure 1 can be seen as a specification for a weighted
acyclic finite-state automaton whose states are indexed by
[l, C, r] and transitions are labelled with biphrases. However,
for generality of representation, we opt for using acyclic hy-
pergraphs instead of automata (see §2.1).

1238



tices) V and a weighted set of edges 〈E,w〉. An
edge e connects a sequence of nodes in its tail
t[e] ∈ V ∗ under a head node h[e] ∈ V and has
weight w(e). A node v is a terminal node if it
has no incoming edges, otherwise it is a nontermi-
nal node. The node that has no outgoing edges,
is called root, with no loss of generality we can
assume hypergraphs to have a single root node.

Hypergraphs can be seen as instantiated logic
programs. In this view, an item is a template
for the creation of nodes, and a weighted deduc-
tion rule is a template for edges. The tail of
an edge is the sequence of nodes associated with
the antecedents, and the head is the node associ-
ated with the consequent. Even though the space
of weighted derivations in phrase-based SMT is
finite-state, using a hypergraph as opposed to a
finite-state automaton makes it natural to encode
multi-word phrases using tails. We opt for rep-
resenting the target side of the biphrase as a se-
quence of terminals nodes, each of which repre-
sents a target word.

3 Related Work

3.1 Beam filling algorithms

Beam search (Koehn et al., 2003) and cube prun-
ing (Chiang, 2007) are examples of state-of-the-art
approximate search algorithms. They approximate
the intersection between the translation forest and
the language model by expanding a limited beam
of hypotheses from each nonterminal node. Hy-
potheses are organised in priority queues accord-
ing to common traits and a fast-to-compute heuris-
tic view of outside weights (cheapest way to com-
plete a hypothesis) puts them to compete at a fairer
level. Beam search exhausts a node’s possible ex-
pansions, scores them, and discards all but the k
highest-scoring ones. This process is wasteful in
that k is typically much smaller than the number of
possible expansions. Cube pruning employs a pri-
ority queue at beam filling and computes k high-
scoring expansions directly in near best-first order.
The parameter k is known as beam size and it con-
trols the time-accuracy trade-off of the algorithm.

Heafield et al. (2013a) move away from us-
ing the language model as a black-box and build
a more involved beam filling algorithm. Even
though they target approximate search, some of
their ideas have interesting connections to ours
(see §4). They group hypotheses that share partial
language model state (Li and Khudanpur, 2008)

reasoning over multiple hypotheses at once. They
fill a beam in best-first order by iteratively vis-
iting groups using a priority queue: if the top
group contains a single hypothesis, the hypothesis
is added to the beam, otherwise the group is parti-
tioned and the parts are pushed back to the queue.
More recently, Heafield et al. (2014) applied their
beam filling algorithm to phrase-based decoding.

3.2 Exact optimisation
Exact optimisation for monotone translation has
been done using A∗ search (Tillmann et al., 1997)
and finite-state operations (Kumar et al., 2006).
Och et al. (2001) design near-admissible heuris-
tics for A∗ and decode very short sentences (6-
14 words) for a word-based model (Brown et al.,
1993) with a maximum distortion strategy (d = 3).

Zaslavskiy et al. (2009) frame phrase-based de-
coding as an instance of a generalised Travel-
ling Salesman Problem (TSP) and rely on ro-
bust solvers to perform decoding. In this view,
a salesman graph encodes the translation options,
with each node representing a biphrase. Non-
overlapping constraints are imposed by the TSP
solver, rather than encoded directly in the sales-
man graph. They decode only short sentences
(17 words on average) using a 2-gram LM due to
salesman graphs growing too large.4

Chang and Collins (2011) relax phrase-based
models w.r.t. the non-overlapping constraints,
which are replaced by soft penalties through La-
grangian multipliers, and intersect the LM com-
ponent exhaustively. They do employ a maximum
distortion limit (d = 4), thus the problem they
tackle is no longer NP-complete. Rush and Collins
(2011) relax a hierarchical phrase-based model
(Chiang, 2005)5 w.r.t. the LM component. The
translation forest and the language model trade
their weights (through Lagrangian multipliers) so
as to ensure agreement on what each component
believes to be the maximum. In both approaches,
when the dual converges to a compliant solution,
the solution is guaranteed to be optimal. Other-

4Exact decoding had been similarly addressed with Inte-
ger Linear Programming (ILP) in the context of word-based
models for very short sentences using a 2-gram LM (Ger-
mann et al., 2001). Riedel and Clarke (2009) revisit that for-
mulation and employ a cutting-plane algorithm (Dantzig et
al., 1954) reaching 30 words.

5In hierarchical translation, reordering is governed by a
synchronous context-free grammar and the underlying prob-
lem is no longer NP-complete. Exact decoding remains in-
feasible because the intersection between the translation for-
est and the target LM is prohibitively slow.

1239



wise, a subset of the constraints is explicitly added
and the dual optimisation is repeated. They handle
sentences above average length, however, resort-
ing to compact rulesets (10 translation options per
input segment) and using only 3-gram LMs.

In the context of hierarchical models, Aziz et
al. (2013) work with unpruned forests using up-
perbounds. Their approach is the closest to ours.
They also employ a coarse-to-fine strategy with
the OS∗ framework (Dymetman et al., 2012), and
investigate unbiased sampling in addition to op-
timisation. However, they start from a coarser
upperbound with unigram probabilities, and their
refinement strategies are based on exhaustive in-
tersections with small n-gram matching automata.
These refinements make forests grow unmanage-
able too quickly. Because of that, they only deal
with very short sentences (up to 10 words) and
even then decoding is very slow. We design bet-
ter upperbounds and a more efficient refinement
strategy. Moreover, we decode long sentences us-
ing language models of order 3 to 5.6

4 Approach

4.1 Exact optimisation with OS∗

Dymetman et al. (2012) introduced OS∗, a unified
view of optimisation and sampling which can be
seen as a cross between adaptive rejection sam-
pling (Robert and Casella, 2004) and A∗ optimisa-
tion (Hart et al., 1968). In this framework, a com-
plex goal distribution is upperbounded by a sim-
pler proposal distribution for which optimisation
(and sampling) is feasible. This proposal is incre-
mentally refined to be closer to the goal until the
maximum is found (or until the sampling perfor-
mance exceeds a certain level).

Figure 2 illustrates exact optimisation with OS∗.
Suppose f is a complex target goal distribution,
such that we cannot optimise f , but we can as-
sess f(d) for a given d. Let g(0) be an upper-
bound to f , i.e., g(0)(d) ≥ f(d) for all d ∈ D(x).
Moreover, suppose that g(0) is simple enough to
be optimised efficiently. The algorithm proceeds
by solving d0 = argmaxd g(0)(d) and comput-

6The intuition that a full intersection is wasteful is also
present in (Petrov et al., 2008) in the context of approximate
search. They start from a coarse distribution based on au-
tomatic word clustering which is refined in multiple passes.
At each pass, hypotheses are pruned a posteriori on the basis
of their marginal probabilities, and word clusters are further
split. We work with upperbounds, rather than word clusters,
with unpruned distributions, and perform exact optimisation.

f

g(0)

d0
D(x)

g(1)

d1d
*

f1

f0

f*

Figure 2: Sequence of incrementally refined up-
perbound proposals.

ing the quantity r0 = f(d0)/g(0)(d0). If r0 were
sufficiently close to 1, then g(0)(d0) would be
sufficiently close to f(d0) and we would have
found the optimum. However, in the illustration
g(0)(d0) � f(d0), thus r0 � 1. At this point
the algorithm has concrete evidence to motivate
a refinement of g(0) that can lower its maximum,
bringing it closer to f∗ = maxd f(d) at the cost
of some small increase in complexity. The re-
fined proposal must remain an upperbound to f .
To continue with the illustration, suppose g(1) is
obtained. The process is repeated until eventually
g(t)(dt) = f(dt), where dt = argmaxd g(t)(d),
for some finite t. At which point dt is the opti-
mum derivation d∗ from f and the sequence of
upperbounds provides a proof of optimality.7

4.2 Model

We work with phrase-based models in a standard
parameterisation (Equation 2). However, to avoid
having to deal with NP-completeness, we con-
strain reordering to happen only within a limited
window given by a notion of distortion limit. We
require that the last source word covered by any
biphrase must be within d words from the leftmost
uncovered source position (Lopez, 2009). This is
a widely used strategy and it is in use in the Moses
toolkit (Koehn et al., 2007).8

Nevertheless, the problem of finding the best

7If d is a maximum from g and g(d) = f(d), then it is
easy to show by contradiction that d is the actual maximum
from f : if there existed d′ such that f(d′) > f(d), then it
follows that g(d′) ≥ f(d′) > f(d) = g(d), and hence d
would not be a maximum for g.

8A distortion limit characterises a form of pruning that
acts directly in the generative capacity of the model leading
to induction errors (Auli et al., 2009). Limiting reordering
like that lowers complexity to a polynomial function of I and
an exponential function of the distortion limit.

1240



derivation under the model remains impractica-
ble due to nonlocal parameterisation (namely,
the n-gram LM component). The weighted set
〈D(x), f(d)〉, which represents the objective, is
a complex hypergraph which we cannot afford
to construct. We propose to construct instead a
simpler hypergraph for which optimisation by dy-
namic programming is feasible. This proxy rep-
resents the weighted set

〈D(x), g(0)(d)〉, where
g(0)(d) ≥ f(d) for every d ∈ D(x). Note that
this proposal contains exactly the same translation
options as in the original decoding problem. The
simplification happens only with respect to the pa-
rameterisation. Instead of intersecting the com-
plete n-gram LM distribution explicitly, we im-
plicitly intersect a simpler upperbound view of it,
where by simpler we mean lower-order.

g(0)(d) =

l∑
i=1

ω(y[ei]) +
l∑
i=1

φ(ei) +

l−1∑
i=1

δ(ei, ei−1) (3)

Equation 3 shows the model we use as a proxy
to perform exact optimisation over f . In compar-
ison to Equation 2, the term

∑l
i=1 ω(y[ei]) replaces

ψ(y) = λψpLM(y). While ψ weights the yield y
taking into account all n-grams (including those
crossing the boundaries of phrases), ω weights
edges in isolation. Particularly, ω(y[ei]) =
λψqLM(y[ei]), where y[ei] returns the sequence of
target words (a target phrase) associated with the
edge, and qLM(·) is an upperbound on the true LM
probability pLM(·) (see §4.3). It is obvious from
Equation 3 that our proxy model is much simpler
than the original — the only form of nonlocal pa-
rameterisation left is the distortion penalty, which
is simple enough to represent exactly.

The program in Figure 3 illustrates the con-
struction of

〈D(x), g(0)(d)〉. A nonterminal item
[l, C, r] stores: the leftmost uncovered position l
and a truncated coverage vector C (together they
track d input positions); and the rightmost position
r most recently translated (necessary for the com-
putation of the distortion penalty). Observe how
nonterminal items do not store the LM state.9 The
rule ADJACENT expands derivations by concate-
nation with a biphrase

〈
xi

′
i → yj

′
j

〉
starting at the

leftmost uncovered position i = l. That causes
the coverage window to move ahead to the next
leftmost uncovered position: l′ = l + α1(C) + 1,

9Drawing a parallel to (Heafield et al., 2013a), a nontermi-
nal node in our hypergraph groups derivations while exposing
only an empty LM state.

ITEM
[
[1, I + 1], {0, 1}d−1, [0, I + 1]]

GOAL [I, ∅, I + 1]
AXIOMS
〈BOS→ BOS〉

[1, 0d−1, 0] : ω(BOS)
ADJACENT

[l, C, r]
〈
xi

′
i

φr−−→ yj′j
〉

[l′, C′, i′] : φr ⊗ δ(r, i′)⊗ ω(yj′j )
i = l⊕i′−l

k=i−l ck = 0̄
where l′ = l + α1(C) + 1

C′ � α1(C) + 1
NON-ADJACENT

[l, C, r]
〈
xi

′
i

φr−−→ yj′j
〉

[l, C′, i′] : φr ⊗ δ(r, i′)⊗ ω(yj′j )

i > l⊕i′−l
k=i−l ck = 0̄

|r − i+ 1| ≤ d
|i′ − l + 1| ≤ d

where c′k = ck if k < i− l or k > i′ − l else 1̄
ACCEPT

[I + 1, C, r]

[I + 1, ∅, I + 1] : δ(r, I + 1)⊗ ω(EOS) r ≤ I

Figure 3: Specification of the initial proposal hy-
pergraph. This program allows the same reorder-
ings as (Lopez, 2009) (see logic WLd), however,
it does not store LM state information and it uses
the upperbound LM distribution ω(·).

where α1(C) returns the number of leading 1s in
C, and C ′ � α1(C) + 1 represents a left-shift.
The rule NON-ADJACENT handles the remaining
cases i > l provided that the expansion skips at
most d input words |r − i+ 1| ≤ d. In the conse-
quent, the window C is simply updated to record
the translation of the input span i..i′. In the non-
adjacent case, a gap constraint imposes that the
resulting item will require skipping no more than
d positions before the leftmost uncovered word is
translated |i′ − l + 1| ≤ d.10 Finally, note that
deductions incorporate the weighted upperbound
ω(·), rather than the true LM component ψ(·).11

4.3 LM upperbound and Max-ARPA

Following Carter et al. (2012) we compute an
upperbound on n-gram conditional probabilities
by precomputing max-backoff weights stored in
a “Max-ARPA” table, an extension of the ARPA
format (Jurafsky and Martin, 2000).

A standard ARPA table T stores entries

10This constraint prevents items from becoming dead-ends
where incomplete derivations require a reordering step larger
than d. This is known to prevent many search errors in beam
search (Chang and Collins, 2011).

11Unlike Aziz et al. (2013), rather than unigrams only, we
score all n-grams within a translation rule (including incom-
plete ones).

1241



〈Z,Z.p,Z.b〉, where Z is an n-gram equal to the
concatenation Pz of a prefix P with a word z, Z.p
is the conditional probability p(z|P), and Z.b is
a so-called “backoff” weight associated with Z.
The conditional probability of an arbitrary n-gram
p(z|P), whether listed or not, can then be recov-
ered from T by the simple recursive procedure
shown in Equation 4, where tail deletes the first
word of the string P.

p(z|P) =
 p(z| tail(P)) Pz 6∈ T and P 6∈ Tp(z| tail(P))× P.b Pz 6∈ T and P ∈ TPz.p Pz ∈ T

(4)

The optimistic version (or “max-backoff”) q of
p is defined as q(z|P) ≡ maxH p(z|HP), where
H varies over all possible contexts extending the
prefix P to the left. The Max-ARPA table allows to
compute q(z|P) for arbitrary values of z and P. It
is constructed on the basis of the ARPA table T by
adding two columns to T : a column Z.q that stores
the value q(z|P) and a column Z.m that stores an
optimistic version of the backoff weight.

These columns are computed offline in two
passes by first sorting T in descending order of
n-gram length.12 In the first pass (Algorithm 1),
we compute for every entry in the table an opti-
mistic backoff weight m. In the second pass (Algo-
rithm 2), we compute for every entry an optimistic
conditional probability q by maximising over 1-
word history extensions (whose .q fields are al-
ready known due to the sorting of T ).

The following Theorem holds (see proof be-
low): For an arbitrary n-gram Z = Pz, the prob-
ability q(z|P) can be recovered through the proce-
dure shown in Equation 5.

q(z|P) =
 p(z|P) Pz 6∈ T and P 6∈ Tp(z|P)× P.m Pz 6∈ T and P ∈ TPz.q Pz ∈ T (5)

Note that, if Z is listed in the table, we return its
upperbound probability q directly. When the n-
gram is unknown, but its prefix is known, we take
into account the optimistic backoff weight m of the
prefix. On the other hand, if both the n-gram and
its prefix are unknown, then no additional context
could change the score of the n-gram, in which
case q(z|P) = p(z|P).

In the sequel, we will need the following defini-
tions. Suppose α = yJI is a substring of y = y

M
1 .

12If an n-gram is listed in T , then all its substrings must
also be listed. Certain pruning strategies may corrupt this
property, in which case we make missing substrings explicit.

Then pLM(α) ≡
∏J
k=I p(yk|yk−11 ) is the contribu-

tion of α to the true LM score of y. We then ob-
tain an upperbound qLM(α) to this contribution by
defining qLM(α) ≡ q(yI |�)

∏J
k=I+1 q(yk|yk−1I ).

Proof of Theorem. Let us first suppose that the length
of P is strictly larger than the order n of the language
model. Then for any H, p(z|HP) = p(z|P); this is be-
cause HP /∈ T and P /∈ T , along with all intermedi-
ary strings, hence, by (4), p(z|HP) = p(z| tail(HP)) =
p(z| tail(tail(HP))) = . . . = p(z|P). Hence q(z|P) =
p(z|P), and, because Pz /∈ T and P /∈ T , the theorem
is satisfied in this case.

Having established the theorem for |P| > n, we
now assume that it is true for |P| > m and prove by
induction that it is true for |P| = m. We use the
fact that, by the definition of q, we have q(z|P) =
maxx∈∆ q(z|xP). We have three cases to consider.
First, suppose that Pz /∈ T and P /∈ T . Then
xPz /∈ T and xP /∈ T , hence by induction q(z|xP) =
p(z|xP) = p(z|P) for any x, therefore q(z|P) =
p(z|P). We have thus proven the first case.
Second, suppose that Pz /∈ T and P ∈ T . Then, for
any x, we have xPz /∈ T , and:
q(z|P) = max

x∈∆
q(z|xP)

= max( max
x∈∆, xP/∈T

q(z|xP), max
x∈∆, xP∈T

q(z|xP)).

For xP /∈ T , by induction, q(z|xP) = p(z|xP) =
p(z|P), and therefore maxx∈∆, xP/∈T q(z|xP) =
p(z|P). For xP ∈ T , we have q(z|xP) = p(z|xP) ×
xP.m = p(z|P)× xP.b× xP.m. Thus, we have:

max
x∈∆, xP∈T

q(z|xP) = p(z|P)× max
x∈∆, xP∈T

xP.b×xP.m.

But now, because of lines 3 and 4 of Algorithm
1, P.m = maxx∈∆, xP∈T xP.b × xP.m, hence
maxx∈∆, xP∈T q(z|xP) = p(z|P) × P.m. Therefore,
q(z|P) = max(p(z|P), p(z|P)×P.m) = p(z|P)×P.m,
where we have used the fact that P.m ≥ 1 due to line 1
of Algorithm 1. We have thus proven the second case.
Finally, suppose that Pz ∈ T . Then, again,

q(z|P) = max
x∈∆

q(z|xP)
= max(

max
x∈∆, xPz/∈T, xP/∈T

q(z|xP),
max

x∈∆, xPz/∈T, xP∈T
q(z|xP),

max
x∈∆, xPz∈T

q(z|xP) ).

For xPz /∈ T, xP /∈ T , we have q(z|xP) =
p(z|xP) = p(z|P) = Pz.p, where the last equality is
due to the fact that Pz ∈ T . For xPz /∈ T, xP ∈ T , we
have q(z|xP) = p(z|xP)× xP.m = p(z|P)× xP.b×
xP.m = Pz.p× xP.b× xP.m. For xPz ∈ T , we have
q(z|xP) = xPz.q. Overall, we thus have:
q(z|P) = max( Pz.p,

max
x∈∆, xPz/∈T, xP∈T

Pz.p× xP.b× xP.m,
max

x∈∆, xPz∈T
xPz.q ).

Note that xPz ∈ T ⇒ xP ∈ T , and then one can
check that Algorithm 2 exactly computes Pz.q as this

maximum over three maxima, hence Pz.q = q(z|P).

1242



Algorithm 1 Max-ARPA: first pass
1: for Z ∈ T do
2: Z.m← 1
3: for x ∈ ∆ s.t xZ ∈ T do
4: Z.m← max(Z.m,xZ.b× xZ.m)
5: end for
6: end for

Algorithm 2 Max-ARPA: second pass
1: for Z = Pz ∈ T do
2: Pz.q← Pz.p
3: for x ∈ ∆ s.t xP ∈ T do
4: if xPz ∈ T then
5: Pz.q← max(Pz.q,xPz.q)
6: else
7: Pz.q← max(Pz.q,Pz.p× xP.b× xP.m)
8: end if
9: end for

10: end for

4.4 Search

The search for the true optimum derivation is il-
lustrated in Algorithm 3. The algorithm takes as
input the initial proposal distribution g(0)(d) (see
§4.2, Figure 3) and a maximum error � (which we
set to a small constant 0.001 rather than zero, to
avoid problems with floating point precision). In
line 3 we find the optimum derivation d in g(0)

(see §4.5). The variable g∗ stores the maximum
score w.r.t. the current proposal, while the vari-
able f∗ stores the maximum score observed thus
far w.r.t. the true model (note that in line 5 we as-
sess the true score of d). In line 6 we start a loop
that runs until the error falls below �. This error is
the difference (in log-domain) between the proxy
maximum g∗ and the best true score observed thus
far f∗.13 In line 7, we refine the current proposal
using evidence from d (see §4.6). In line 9, we
update the maximum derivation searching through
the refined proposal. In line 11, we keep track of
the best score so far according to the true model,
in order to compute the updated gap in line 6.

4.5 Dynamic Programming

Finding the best derivation in a proposal hyper-
graph is straightforward with standard dynamic
programming. We can compute inside weights
in the max-times semiring in time proportional

13Because g(t) upperbounds f everywhere, in optimisation
we have a guarantee that the maximum of f must lie in the
interval [f∗, g∗) (see Figure 2) and the quantity g∗ − f∗ is
an upperbound on the error that we incur if we early-stop the
search at any given time t. This bound provides a principled
criterion in trading accuracy for performance (a direction that
we leave for future work). Note that most algorithms for ap-
proximate search produce solutions with unbounded error.

Algorithm 3 Exact decoding
1: function OPTIMISE(g(0), �)
2: t← 0 . step
3: d← argmaxd g(t)(d)
4: g∗ ← g(t)(d)
5: f∗ ← f(d)
6: while (q∗ − f∗ ≥ �) do . � is the maximum error
7: g(t+1) ← refine(g(t),d) . update proposal
8: t← t+ 1
9: d← argmaxd g(t)(d) . update argmax

10: g∗ ← g(t)(d)
11: f∗ ← max(f∗, f(d)) . update “best so far”
12: end while
13: return g(t), d
14: end function

to O(|V | + |E|) (Goodman, 1999). Once inside
weights have been computed, finding the Viterbi-
derivation starting from the root is straightforward.
A simple, though important, optimisation con-
cerns the computation of inside weights. The in-
side algorithm (Baker, 1979) requires a bottom-up
traverse of the nodes in V . To do that, we topolog-
ically sort the nodes in V at time t = 0 and main-
tain a sorted list of nodes as we refine g throughout
the search – thus avoiding having to recompute the
partial ordering of the nodes at every iteration.

4.6 Refinement
If a derivation d = argmaxd g(t)(d) is such that
g(t)(d)� f(d), there must be in d at least one n-
gram whose upperbound LM weight is far above
its true LM weight. We then lower g(t) locally by
refining only nonterminal nodes that participate in
d. Nonterminal nodes are refined by having their
LM states extended one word at a time.14

For an illustration, assume we are perform-
ing optimisation with a bigram LM. Suppose
that in the first iteration a derivation d0 =
argmaxd g(0)(d) is obtained. Now consider an
edge in d0

[l, C, r, �] αy1
w−→ [l0, C0, r0, �]

where an empty LM state is made explicit (with an
empty string �) and αy1 represents a target phrase.
We refine the edge’s head [l0, C0, r0, �] by creating
a node based on it, however, with an extended LM
state, i.e., [l0, C0, r0, y1]. This motivates a split
of the set of incoming edges to the original node,
such that, if the target projection of an incoming

14The refinement operation is a special case of a general
finite-state intersection. However, keeping its effect local to
derivations going through a specific node is non-trivial using
the general mechanism and justifies a tailored operation.

1243



edge ends in y1, that edge is reconnected to the
new node as below.

[l, C, r, �] αy1
w−→ [l0, C0, r0, y1]

The outgoing edges from the new node are
reweighted copies of those leaving the original
node. That is, outgoing edges such as

[l0, C0, r0, �] y2β
w−→ [l′, C ′, r′, γ′]

motivate edges such as

[l0, C0, r0, y1] y2β
w⊗w′−−−→ [l′, C ′, r′, γ′]

where w′ = λψqLM (y1y2)/qLM (y2) is a change in LM
probability due to an extended context.

Figure 4 is the logic program that constructs the
refined hypergraph in the general case. In com-
parison to Figure 3, items are now extended to
store an LM state. The input is the original hy-
pergraph G = 〈V,E〉 and a node v0 ∈ V to be
refined by left-extending its LM state γ0 with the
word y. In the program,

〈
uσ

w−→ v
〉

with u,v ∈ V
and σ ∈ ∆∗ represents an edge in E. An item
[l, C, r, γ]v (annotated with a state v ∈ V ) rep-
resents a node (in the refined hypergraph) whose
signature is equivalent to v (in the input hyper-
graph). We start with AXIOMS by copying the
nodes in G. In COPY, edges from G are copied
unless they are headed by v0 and their target pro-
jections end in yγ0 (the extended context). Such
edges are processed by REFINE, which instead of
copying them, creates new ones headed by a re-
fined version of v0. Finally, REWEIGHT contin-
ues from the refined node with reweighted copies
of the edges leaving v0. The weight update repre-
sents a change in LM probability (w.r.t. the upper-
bound distribution) due to an extended context.

5 Experiments

We used the dataset made available by the Work-
shop on Statistical Machine Translation (WMT)
(Bojar et al., 2013) to train a German-English
phrase-based system using the Moses toolkit
(Koehn et al., 2007) in a standard setup. For
phrase extraction, we used both Europarl (Koehn,
2005) and News Commentaries (NC) totalling
about 2.2M sentences.15 For language modelling,
in addition to the monolingual parts of Europarl

15Pre-processing: tokenisation, truecasing and automatic
compound-splitting (German only). Following Durrani et al.
(2013), we set the maximum phrase length to 5.

INPUT
G = 〈V,E〉
v0 = [l0, C0, r0, γ0] ∈ V where γ0 ∈ ∆∗
y ∈ ∆

ITEM [l, C, r, γ ∈ ∆∗]
AXIOMS

[l, C, r, γ]v
v ∈ V

COPY
[l, C, r, α]u

〈
uβ

w−→ v
〉

[l′, C′, r′, α′]v : w
v 6= v0 ∨ αβ 6= σyγ0
α, α′, β, σ ∈ ∆∗

REFINE
[l, C,R, α]u

〈
uβ

w−→ v0
〉

[l0, C0, r0, yγ0] : w

αβ = σyγ0
α, β, σ ∈ ∆∗

REWEIGHT
[l0, C0, r0, yγ0]

〈
v0σ

w−→ v
〉

[l, C, r, γ]v : w ⊗ w′
σ, γ ∈ ∆∗

where w′ = λψ
qLM (yγ0)

qLM (γ0)

Figure 4: Local intersection via LM right state re-
finement. The input is a hypergraph G = 〈V,E〉,
a node v0 ∈ V singly identified by its carry
[l0, C0, r0, γ0] and a left-extension y for its LM
context γ0. The program copies most of the edges〈
uσ

w−→ v
〉
∈ E. If a derivation goes through v0

and the string under v0 ends in yγ0, the program
refines and reweights it.

and NC, we added News-2013 totalling about 25M
sentences. We performed language model interpo-
lation and batch-mira tuning (Cherry and Foster,
2012) using newstest2010 (2,849 sentence pairs).
For tuning we used cube pruning with a large beam
size (k = 5000) and a distortion limit d = 4. Un-
pruned language models were trained using lmplz
(Heafield et al., 2013b) which employs modified
Kneser-Ney smoothing (Kneser and Ney, 1995).
We report results on newstest2012.

Our exact decoder produces optimal translation
derivations for all the 3,003 sentences in the test
set. Table 1 summarises the performance of our
novel decoder for language models of order n = 3
to n = 5. For 3-gram LMs we also varied the dis-
tortion limit d (from 4 to 6). We report the average
time (in seconds) to build the initial proposal, the
total run time of the algorithm, the number of it-
erations N before convergence, and the size of the
hypergraph in the end of the search (in thousands
of nodes and thousands of edges).16

16The size of the initial proposal does not depend on LM
order, but rather on distortion limit (see Figure 3): on aver-
age (in thousands) |V0| = 0.6 and |E0| = 27 with d = 4,
|V0| = 1.3 and |E0| = 70 with d = 5, and |V0| = 2.5 and

1244



n d build (s) total (s) N |V | |E|
3 4 1.5 21 190 2.5 159
3 5 3.5 55 303 4.4 343
3 6 10 162 484 8 725
4 4 1.5 50 350 4 288
5 4 1.5 106 555 6.1 450

Table 1: Performance of the exact decoder in
terms of: time to build g(0), total decoding time in-
cluding build, number of iterations (N), and num-
ber of nodes and edges (in thousands) at the end of
the search.

It is insightful to understand how different as-
pects of the initial proposal impact on perfor-
mance. Increasing the translation option limit (tol)
leads to g(0) having more edges (this dependency
is linear with tol). In this case, the number of
nodes is only minimally affected — due to the pos-
sibility of a few new segmentations. The maxi-
mum phrase length (mpl) introduces in g(0) more
configurations of reordering constraints ([l, C] in
Figure 3). However, not many more, due to C
being limited by the distortion limit d. In prac-
tice, we observe little impact on time performance.
Increasing d introduces many more permutations
of the input leading to exponentially many more
nodes and edges. Increasing the order n of the LM
has no impact on g(0) and its impact on the overall
search is expressed in terms of a higher number of
nodes being locally intersected.

An increased hypergraph, be it due to addi-
tional nodes or additional edges, necessarily leads
to slower iterations because at each iteration we
must compute inside weights in timeO(|V |+|E|).
The number of nodes has the larger impact on the
number of iterations. OS∗ is very efficient in ig-
noring hypotheses (edges) that cannot compete for
an optimum. For instance, we observe that run-
ning time depends linearly on tol only through the
computation of inside weights, while the number
of iterations is only minimally affected.17 An in-

|E0| = 178 with d = 6. Observe the exponential depen-
dency on distortion limit, which also leads to exponentially
longer running times.

17It is possible to reduce the size of the hypergraph
throughout the search using the upperbound on the search
error g∗ − f∗ to prune hypotheses that surely do not stand
a chance of competing for the optimum (Graehl, 2005). An-
other direction is to group edges connecting the same nonter-
minal nodes into one partial edge (Heafield et al., 2013a) —
this is particularly convenient due to our method only visiting
the 1-best derivation from g(d) at each iteration.

n
Nodes at level m LM states at level m

0 1 2 3 4 1 2 3 4
3 0.4 1.2 0.5 - - 113 263 - -
4 0.4 1.6 1.4 0.3 - 132 544 212 -
5 0.4 2.1 2.4 0.7 0.1 142 790 479 103

Table 2: Average number of nodes (in thousands)
whose LM state encode an m-gram, and average
number of unique LM states of order m in the fi-
nal hypergraph for different n-gram LMs (d = 4
everywhere).

creased LM order, for a fixed distortion limit, im-
pacts much more on the number of iterations than
on the average running time of a single iteration.
Fixing d = 4, the average time per iteration is 0.1
(n = 3), 0.13 (n = 4) and 0.18 (n = 5). Fixing a
3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5)
and 0.31 (d = 6). Note the exponential growth
of the latter, due to a proposal encoding exponen-
tially many more permutations.

Table 2 shows the average degree of refine-
ment of the nodes in the final proposal. Nodes
are shown by level of refinement, where m indi-
cates that they store m words in their carry. The
table also shows the number of unique m-grams
ever incorporated to the proposal. This table il-
lustrates well how our decoding algorithm moves
from a coarse upperbound where every node stores
an empty string to a variable-order representation
which is sufficient to prove an optimum derivation.

In our approach a complete derivation is opti-
mised from the proxy model at each iteration. We
observe that over 99% of these derivations project
onto distinct strings. In addition, while the opti-
mum solution may be found early in the search, a
certificate of optimality requires refining the proxy
until convergence (see §4.1). It turns out that most
of the solutions are first encountered as late as in
the last 6-10% of the iterations.

We use the optimum derivations obtained with
our exact decoder to measure the number of search
errors made by beam search and cube pruning with
increasing beam sizes (see Table 3). Beam search
reaches optimum derivations with beam sizes k ≥
500 for all language models tested. Cube prun-
ing, on the other hand, still makes mistakes at
k = 1000. Table 4 shows translation quality
achieved with different beam sizes for cube prun-
ing and compares it to exact decoding. Note that
for k ≥ 104 cube pruning converges to optimum

1245



k
Beam search Cube pruning

3 4 5 3 4 5
10 938 1294 1475 2168 2347 2377
102 19 60 112 613 999 1126
103 0 0 0 29 102 167
104 0 0 0 0 4 7

Table 3: Beam search and cube pruning search er-
rors (out of 3,003 test samples) by beam size using
LMs of order 3 to 5 (d = 4).

order 3 4 5
k d = 4 d = 5 d = 6 d = 4 d = 4
10 20.47 20.13 19.97 20.71 20.69
102 21.14 21.18 21.08 21.73 21.76
103 21.27 21.34 21.32 21.89 21.91
104 21.29 21.37 21.37 21.92 21.93
OS∗ 21.29 21.37 21.37 21.92 21.93

Table 4: Translation quality in terms of BLEU as
a function of beam size in cube pruning with lan-
guage models of order 3 to 5. The bottom row
shows BLEU for our exact decoder.

derivations in the vast majority of the cases (100%
with a 3-gram LM) and translation quality in terms
of BLEU is no different from OS∗. However, with
k < 104 both model scores and translation quality
can be improved. Figure 5 shows a finer view on
search errors as a function of beam size for LMs
of order 3 to 5 (fixed d = 4). In Figure 6, we fix
a 3-gram LM and vary the distortion limit (from 4
to 6). Dotted lines correspond to beam search and
dashed lines correspond to cube pruning.

6 Conclusions and Future Work

We have presented an approach to decoding with
unpruned hypergraphs using upperbounds on the
language model distribution. The algorithm is an
instance of a coarse-to-fine strategy with connec-
tions to A∗ and adaptive rejection sampling known
as OS∗. We have tested our search algorithm us-
ing state-of-the-art phrase-based models employ-
ing robust language models. Our algorithm is able
to decode all sentences of a standard test set in
manageable time consuming very little memory.
We have performed an analysis of search errors
made by beam search and cube pruning and found
that both algorithms perform remarkably well for
phrase-based decoding. In the case of cube prun-
ing, we show that model score and translation

102 103 104

[log] Beam size

100

101

102

103

104

[l
o
g
] 

S
e
a
rc

h
 e

rr
o
rs

Search errors in newstest2012

CP 3-gram
CP 4-gram
CP 5-gram
BS 3-gram
BS 4-gram
BS 5-gram

Figure 5: Search errors made by beam search and
cube pruning as a function of beam-size.

102 103 104

[log] Beam size

100

101

102

103

104

[l
o
g
] 

S
e
a
rc

h
 e

rr
o
rs

Search errors in newstest2012 (3-gram LM)

CP d=4
CP d=5
CP d=6
BS d=4
BS d=5
BS d=6

Figure 6: Search errors made by beam search and
cube pruning as a function of the distortion limit
(decoding with a 3-gram LM).

quality can be improved for beams k < 10, 000.

There are a number of directions that we intend
to investigate to speed up our decoder, such as: (1)
error-safe pruning based on search error bounds;
(2) use of reinforcement learning to guide the de-
coder in choosing which n-gram contexts to ex-
tend; and (3) grouping edges into partial edges,
effectively reducing the size of the hypergraph and
ultimately computing inside weights in less time.

Acknowledgments

The work of Wilker Aziz and Lucia Specia was
supported by EPSRC (grant EP/K024272/1).

1246



References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp

Koehn. 2009. A systematic analysis of transla-
tion model search spaces. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, pages 224–232, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Wilker Aziz, Marc Dymetman, and Sriram Venkatapa-
thy. 2013. Investigations in exact inference for hi-
erarchical translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
472–483, Sofia, Bulgaria, August. Association for
Computational Linguistics.

James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Confer-
ence of the Acoustical Society of America, pages
547–550, Boston, MA, June.

Ondřej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311, June.

Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact sampling and decoding in
high-order hidden Markov models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125–1134, Jeju
Island, Korea, July. Association for Computational
Linguistics.

Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
Lagrangian relaxation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’11, pages 26–37, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 427–436, Stroudsburg, PA,
USA. Association for Computational Linguistics.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 263–
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201–228.

Shay B. Cohen, Robert J. Simmons, and Noah A.
Smith. 2008. Dynamic programming algorithms as
products of weighted logic programs. In Maria Gar-
cia de la Banda and Enrico Pontelli, editors, Logic
Programming, volume 5366 of Lecture Notes in
Computer Science, pages 114–129. Springer Berlin
Heidelberg.

G Dantzig, R Fulkerson, and S Johnson. 1954. So-
lution of a large-scale traveling-salesman problem.
Operations Research, 2:393–410.

Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh’s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114–121, Sofia, Bulgaria,
August. Association for Computational Linguistics.

Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ’10, pages 858–
866, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Marc Dymetman, Guillaume Bouchard, and Simon
Carter. 2012. Optimization and sampling for NLP
from a unified viewpoint. In Proceedings of the
First International Workshop on Optimization Tech-
niques for Human Language Technology, pages 79–
94, Mumbai, India, December. The COLING 2012
Organizing Committee.

Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and
applications. Discrete Applied Mathematics, 42(2-
3):177–201, April.

Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ’01, pages 228–
235, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Joshua Goodman. 1999. Semiring parsing. Comput.
Linguist., 25(4):573–605, December.

Jonathan Graehl. 2005. Relatively useless pruning.
Technical report, USC Information Sciences Insti-
tute.

Peter E. Hart, Nils J. Nilsson, and Bertram Raphael.
1968. A formal basis for the heuristic determina-
tion of minimum cost paths. IEEE Transactions On
Systems Science And Cybernetics, 4(2):100–107.

Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013a. Grouping language model boundary words

1247



to speed k-best extraction from hypergraphs. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
958–968, Atlanta, Georgia, USA, June.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013b. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 690–696, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Kenneth Heafield, Michael Kayser, and Christopher D.
Manning. 2014. Faster Phrase-Based decoding by
refining feature state. In Proceedings of the Associa-
tion for Computational Linguistics, Baltimore, MD,
USA, June.

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144–151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.

Gonzalo Iglesias, Adrià de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 380–388, Athens,
Greece, March. Association for Computational Lin-
guistics.

Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Series in Artificial In-
telligence. Prentice Hall, 1 edition.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Ac-
coustics, Speech, and Signal Processing, 1:181–184.

Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Comput. Linguist.,
25(4):607–615, December.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,

ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit, pages 79–86.

Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine transla-
tion. Natural Language Engineering, 12(1):35–75,
March.

Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the Second Workshop on Syntax
and Structure in Statistical Translation, SSST ’08,
pages 10–18, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):8:1–8:49, August.

Adam Lopez. 2009. Translation as weighted de-
duction. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL ’09, pages 532–540,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation
- Volume 14, DMMT ’01, pages 1–8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1 of ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 108–116, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Sebastian Riedel and James Clarke. 2009. Revisit-
ing optimal decoding for machine translation IBM
model 4. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, NAACL-Short ’09, pages 5–8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

1248



Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.

Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
Lagrangian relaxation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 72–82, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Christoph Tillmann, Stephan Vogel, Hermann Ney,
and A. Zubiaga. 1997. A DP based search using
monotone alignments in statistical translation. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ’97, pages 289–296, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine
translation as a traveling salesman problem. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ’09, pages 333–
341, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

1249


