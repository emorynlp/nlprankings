










































Using Document Summarization Techniques for Speech Data Subset Selection


Proceedings of NAACL-HLT 2013, pages 721–726,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Using Document Summarization Techniques for Speech Data Subset
Selection

Kai Wei∗, Yuzong Liu∗, Katrin Kirchhoff , Jeff Bilmes
Department of Eletrical Engineering

University of Washington
Seattle, WA 98195, USA

{kaiwei,yzliu,katrin,bilmes}@ee.washington.edu

Abstract

In this paper we leverage methods from sub-
modular function optimization developed for
document summarization and apply them to
the problem of subselecting acoustic data. We
evaluate our results on data subset selection
for a phone recognition task. Our framework
shows significant improvements over random
selection and previously proposed methods us-
ing a similar amount of resources.

1 Introduction

Present-day applications in spoken language technol-
ogy (speech recognizers, keyword spotters, etc.) can
draw on an unprecedented amount of training data.
However, larger data sets come with increased de-
mands on computational resources; moreover, they
tend to include redundant information as their size
increases. Therefore, the performance gain curves
of large-scale systems with respect to the amount of
training data often show “diminishing returns”: new
data is often less valuable (in terms of performance
gain) when added to a larger pre-existing data set than
when added to a smaller pre-existing set (e.g.,(Moore,
2003)). Therefore it is of prime importance to de-
velop methods for data subset selection. We distin-
guish two data subselection scenarios: (a) a priori
selection of a data set before (re-)training a system;
in this case the goal is to subselect the existing data
set as well as possible, eliminating redundant infor-
mation; (b) selection for adaptation, where the goal

∗These authors are joint first authors with equal contribu-
tions.

is to tune a system to a known development or test
set. While many studies have addressed the second
scenario, this paper investigates the first: our goal is
to select a smaller subset of the data that fits a given
’budget’ (e.g. maximum number of hours of data) but
provides, to the extent possible, as much information
as the complete data set. Additionally, our selection
method should be a low-resource method that does
not require an already-trained complex system such
as an existing word recognizer.

This problem is akin to unsupervised data ’sum-
marization’. In (Lin and Bilmes, 2009) a novel class
of summarization techniques based on submodular
function optimization were proposed for extractive
document summarization. Interestingly, these meth-
ods can also be applied to speech data ’summariza-
tion’ with only small modifications. In the following
sections we develop a submodular framework for
speech data summarization and evaluate it on a proof-
of-concept phone recognition task.

2 Related Work

Most approaches to data subset selection in speech
have relied on “rank-and-select” approaches that de-
termine the utility of each sample in the data set,
rank all samples according to their utility scores, and
then select the top N samples. In weakly supervised
approaches (e.g.,(Kemp and Waibel, 1998; Lamel
et al., 2002; Hakkani-Tur et al., 2002), utility is re-
lated to the confidence of an existing word recognizer
on new data samples: untranscribed training data is
automatically transcribed using an existing baseline
speech recognizer, and individual utterances are se-
lected as additional training data if they have low

721



confidence. These are active learning approaches
suitable for a scenario where a well-trained speech
recognizer is already available and additional data
for retraining needs to be selected. However, we
would like to reduce available training data ahead of
time with a low-resource approach. In (Chen et al.,
2009) individual samples are selected for the purpose
of discriminative training by considering phone ac-
curacy and the frame-level entropy of the Gaussian
posteriors. (Itoh et al., 2012) use a utility function
consisting of the entropy of word hypothesis N-best
lists and the representativeness of the sample using a
phone-based TF-IDF measure. The latter is compa-
rable to methods used in this paper, though the first
term in their objective function still requires a word
recognizer. In (Wu et al., 2007) acoustic training data
associated with transcriptions is subselected to max-
imize the entropy of the distribution over linguistic
units (phones or words). Most importantly, all these
methods select samples in a greedy fashion without
optimality guarantees. As we will explain in the next
section, greedy selection is near-optimal only when
applied to monotone submodular functions.

3 Submodular Functions

Submodular functions (Edmonds, 1970) have been
widely studied in mathematics, economics, and op-
erations research and have recently attracted interest
in machine learning (Krause and Guestrin, 2011). A
submodular function is defined as follows: Given a fi-
nite ground set of objects (samples) V = {v1, ..., vn}
and a function f : 2V → R+ that returns a real value
for any subset S ⊆ V , f is submodular if ∀A ⊆ B,
and v /∈ B, f(A+ v)− f(A) ≥ f(B + v)− f(B).
That is, the incremental “value” of v decreases when
the set in which v is considered grows from A to B.
Powerful optimization guarantees exist for certain
subtypes of submodular functions. If, for example,
the function is monotone submodular, i.e. ∀A ⊆
B, f(A) ≤ f(B), then it can be maximized, under
a cardinality constraint, by a greedy algorithm that
scales to extremely large data sets, and finds a solu-
tion guaranteed to approximate the optimal solution
to within a constant factor 1− 1/e (Nemhauser et al.,
1978). Submodular functions can be considered the
discrete analog of convexity.

3.1 Submodular Document Summarization

In (Lin and Bilmes, 2011) submodular functions were
recently applied to extractive document summariza-
tion. The problem was formulated as a monotone
submodular function that had to be maximized sub-
ject to cardinality or knapsack constraints:

argmaxS⊆V {f(S) : c(S) ≤ K} (1)

where V is the set of sentences to be summarized, K
is the maximum number of sentences to be selected,
and c(·) ≥ 0 is sentence cost. f(S) was instantiated
by a form of saturated coverage:

fSC(S) =
∑
i∈V

min{Ci(S), αCi(V )} (2)

where Ci(S) =
∑

j∈S wij , and where wij ≥ 0 in-
dicates the similarity between sentences i and j —
Ci : 2

V → R is itself monotone submodular (modu-
lar in fact) and 0 ≤ α ≤ 1 is a saturation coefficient.
fSC(S) is monotone submodular and therefore has
the previously mentioned performance guarantees.
The weighting function w was implemented as the
cosine similarity between TF-IDF weighted n-gram
count vectors for the sentences in the dataset.

3.2 Submodular Speech Summarization

Similar to the procedure described above we can treat
the task of subselecting an acoustic data set as an
extractive summarization problem. For our a priori
data selection scenario we would like to extract those
training samples that jointly are representative of
the total data set. Initial explorations of submodular
functions for speech data can be found in (Lin and
Bilmes, 2009), where submodular functions were
used in combination with a purely acoustic similarity
measure (Fisher kernel). In addition Equation 2 the
facility location function was used:

ffac(S) =
∑
i∈V

max
j∈S

wij (3)

Here our focus is on utilizing methods that move
beyond purely acoustic similarity measures and con-
sider kernels derived from discrete representations
of the acoustic signal. To this end we first run a to-
kenizer over the acoustic signal that converts it into
a sequence of discrete labels. In our case we use a

722



simple bottom-up monophone recognizer (without
higher-level constraints such as a phone language
model) that produces phone labels. We then use the
hypothesized sequence of phonetic labels to compute
two different sentence similarity measures: (a) co-
sine similarity using TF-IDF weighted phone n-gram
counts, and (b) string kernels. We compare their
performance to that of the Fisher kernel as a purely
acoustic similarity measure.
TF-IDF weighted cosine similarity
The cosine similarity between phone sequences si
and sj is computed as

simij =

∑
w∈si tfw,si × tfw,sj × idf

2
w√∑

w∈si tf
2
w,si idf

2
w

√∑
w∈sj tf

2
w,sj idf

2
w

(4)
where tfw,si is the count of n-gram w in si and idfw
is the inverse document count of w (each sentence is
a “document”). We use n = 1, 2, 3.
String kernel
The particular string kernel we use is a gapped,
weighted subsequence kernel of the type described in
(Rousu and Shawe-Taylor, 2005). Formally, we de-
fine a sentence s as a concatenation of symbols from
a finite alphabet Σ (here the inventory of phones) and
an embedding function from strings to feature vec-
tors, φ : Σ∗ → H. The string kernel function K(s, t)
computes the distance between the resulting vectors
for two sentences si and sj . The embedding function
is defined as

φku(s) :=
∑

i:u=s(i)

λ|i| u ∈ Σk (5)

where k is the maximum length of subsequences,
|i| is the length of i, and λ is a penalty parameter
for each gap encountered in the subsequence. K is
defined as

K(si, sj) =
∑
u

〈φu(si), φu(sj)〉wu (6)

where w is a weight dependent on the length of
u, l(u). Finally, the kernel score is normalized by√
K(si, si) · K(sj , sj) to discourage long sentences

from being favored.
Fisher kernel
The Fisher kernel is based on the vector of derivatives
UX of the log-likelihood of the acoustic data (X)

with respect to the parameters in the phone HMMs
θ1, ..., θm for m models, having similarity score:

simij = (max
i′,j′

di′j′)− dij , where dij = ||U ′i − U ′j ||1,

U θX = 5θ logP (X|θ), and U ′X = U
θ1
X ◦ U

θ2
x , ..., ◦U

θm
X .

4 Data and Systems

We evaluate our approach on subselecting training
data from the TIMIT corpus for training a phone rec-
ognizer. Although this not a large-scale data task, it
is an appropriate proof-of-concept task for rapidly
testing different combinations of submodular func-
tions and similarity measures. Our goal is to focus
on acoustic modeling only; we thus look at phone
recognition performance and do not have to take into
account potential interactions with a language model.
We also chose a simple acoustic model, a monophone
HMM recognizer, rather than a more powerful but
computationally complex model in order to ensure
quick experimental turnaround time. Note that the
goal of this study is not to obtain the highest phone
accuracy possible; what is important is the relative
performance of the different subset selection meth-
ods, especially on small data subsets.

The sizes of the training, development and test data
are 4620, 200 and 192 utterances, respectively. Pre-
processing was done by extracting 39-dimensional
MFCC feature vectors every 10 ms, with a window
of 25.6ms. Speaker mean and variance normaliza-
tion was applied. A 16-component Gaussian mixture
monophone HMM system was trained on the full data
set to generate parameters for the Fisher kernel and
phone sequences for the string kernel and TF-IDF
based similarity measures.

Following the selection of subsets (2.5%, 5%, 10%,
20%, 30%, 40%, 50%, 60%, 70% and 80% of the
data, measured as percentage of non-silence speech
frames), we train a 3-state HMM monophone recog-
nizer for all 48 TIMIT phone classes on the result-
ing sets and evaluate the performance on the core
test set of 192 utterances, collapsing the 48 classes
into 39 in line with standard practice (Lee and Hon,
1989). The HMM state output distributions are mod-
eled by diagonal-covariance Gaussian mixtures with
the number of Gaussians ranging between 4 and 64,
depending on the data size.

As a baseline we perform 100 random draws of
the specified subset sizes and average the results.

723



The second baseline consists of the method in (Wu et
al., 2007), where utterances are selected to maximize
the entropy of the distribution over phones in the
selected subset.

5 Experiments

We tested the three different similarity measures de-
scribed above in combination with the submodular
functions in Equations 2 and 3. The parameters of
the gapped string kernel (i.e. the kernel order (k), the
gap penalty (λ), and the contiguous substring length
l) were optimized on the development set. The best
values were λ = 0.1, k = 4, l = 3. We found that
facility location was superior to saturated cover func-
tion across the board.

Comparison of different data subset selection methods 

Phone Accuracy (%)

Pe
rc

en
ta

ge
 o

f S
pe

ec
h 

in
 S

el
ec

te
d 

Su
bs

et

40# 45# 50# 55# 60# 65#

80#

70#

60#

50#

40#

30#

20#

10#

5#

2.5#
string#kernel#
TF7IDF#trigram#
TF7IDF#bigram#
TF7IDF#unigram#
Fisher#kernel#
entropy#
random#

Figure 1: Phone accuracy for different subset sizes; each
block of bars lists, from bottom to top: random baseline,
entropy baseline, Fisher kernel, TF-IDF (unigram), TF-
IDF (bigram), TF-IDF (trigram), string kernel.

Figure 1 shows the performance of the random and
entropy-based baselines as well as the performance
of the facility location function with different sim-
ilarity measures. The entropy-based baseline beats
the random baseline for most percentage cases but
is otherwise the lowest-performing method overall.
Note that this baseline uses the true transcriptions in
line with (Wu et al., 2007) rather than the hypothe-
sized phone labels output by our recognizer. The low
performance and the fact that it is even outperformed
by the random baseline in the 2.5% and 70% cases

Pe
rc

en
ta

ge
 o

f S
pe

ec
h 

in
 S

el
ec

te
d 

Su
bs

et

Phone Accuracy (%)

Comparison of different submodular functions

45

50

55

60

65

2.5 5 10 20 30 40 60 60 70 80

Figure 2: Phone accuracy obtained by random selection,
facility location function, and saturated coverage function
(string kernel similarity measure).

may be because the selection method encourages
highly diverse but not very representative subsets.
Furthermore, the entropy-based baseline utilizes a
non-submodular objective function with a heuristic
greedy search method. No theoretical guarantee of
optimality can be made for the subset found by this
method.

Among the different similarity measures the Fisher
kernel outperforms the baseline methods but has
lower performance than the TF-IDF kernel and the
string kernel. The best performance is obtained with
the string kernel, especially when using small train-
ing data sets (2.5%-10%). The submodular selection
methods yield significant improvements (p < 0.05)
over both the random baseline and over the entropy-
based method.

We also investigated using different submodular
functions, i.e. the facility location function and the
saturated coverage function. Figure 2 shows the per-
formance of the facility location (ffac) and saturated
coverage (fSC) functions in combination with the
string kernel similarity measure. The reason ffac
outperforms fSC is that fSC primarily controls for
over-coverage of any element not in the subset via the
α saturation hyper-parameter. However, it does not
ensure that every non-selected element has good rep-
resentation in the subset. fSC measures the quality of
the subset by how well each individual element out-
side the subset has a surrogate within the subset (via

724



40
45
50
55
60
65

2.5p 5p 10p 20p 30p
40
45
50
55
60
65

2.5p 5p 10p 20p 30p

40
45
50
55
60
65

2.5p 5p 10p 20p 30p
40
45
50
55
60
65

2.5p 5p 10p 20p 30p

TF-IDF bigramTF-IDF unigram

string kernelTF-IDF trigram

Figure 3: Phone accuracy for true vs. hypothesized phone
labels, for string-based similarity measures.

the max function) and hence tends to model complete
coverage better, leading to better results.

Finally we examined whether using hypothesized
phone sequences vs. the true transcriptions has nega-
tive effects. Figure 3 shows that this is not the case:
interestingly, the hypothesized labels even result in
slightly better results. This may be because the rec-
ognized phone sequences are a function of both the
underlying phonetic sequences that were spoken and
the acoustic signal characteristics, such as the speaker
and channel. The true transcriptions, on the other
hand, are able to provide information only about pho-
netic as opposed to acoustic characteristics.

6 Discussion
We have presented a low-resource framework for
acoustic data subset selection based on submodular
function optimization, which was previously devel-
oped for document summarization. Evaluation on a
proof-of-concept task has shown that the method is
successful at selecting data subsets that outperform
subsets selected randomly or by a previously pro-
posed low-resource method. We note that the best
selection strategies for the experimental conditions
tested here involve similarity measures based on a
discrete tokenization of the speech signal rather than
direct acoustic similarity measures.
Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263. The U.S. Government is authorized to re-

produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of Intelli-
gence Advanced Research Projects Activity (IARPA)
or the U.S. Government.

References
B. Chen, S.H Liu, and F.H. Chu. 2009. Training data se-

lection for improving discriminative training of acoustic
models. Pattern Recognition Letters, 30:1228–1235.

J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69–87. Gordon and Breach.

G. Hakkani-Tur, G. Riccardi, and A. Gorin. 2002. Active
learning for automatic speech recognition. In Proc. of
ICASSP, pages 3904–3907.

N. Itoh, T.N. Sainath, D.N. Jiang, J. Zhou, and B. Ramab-
hadran. 2012. N-best entropy based data selection for
acoustic modeling. In Proceedings of ICASSP.

Thomas Kemp and Alex Waibel. 1998. Unsupervised
training of a speech recognizer using TV broadcasts.
In in Proceedings of the International Conference on
Spoken Language Processing (ICSLP-98), pages 2207–
2210.

A. Krause and C. Guestrin. 2011. Submodularity and its
applications in optimized information gathering. ACM
Transactions on Intelligent Systems and Technology,
2(4).

L. Lamel, J.L. Gauvain, and G. Adda. 2002. Lightly
supervised and unsupervised acoustic model training.
Computer, Speech and Language, 16:116 – 125.

K.F. Lee and H.W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models. IEEE
Trans. ASSP, 37:1641–1648.

Hui Lin and Jeff A. Bilmes. 2009. How to select a good
training-data subset for transcription: Submodular ac-
tive selection for sequences. In Proc. Annual Confer-
ence of the International Speech Communication Asso-
ciation (INTERSPEECH), Brighton, UK, September.

H. Lin and J. Bilmes. 2011. A class of submodular
functions for document summarization. In Proceedings
of ACL.

R.K. Moore. 2003. A comparison of the data require-
ments of automatic speech recognition systems and
human listeners. In Proceedings of Eurospeech, pages
2581–2584.

G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978.
An analysis of approximations for maximizing submod-
ular functions-I. Math. Program., 14:265–294.

725



J. Rousu and J. Shawe-Taylor. 2005. Efficien computa-
tion of of gapped substring kernels for large alphabets.
Journal of Machine Leaning Research, 6:13231344.

Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection
for speech recognition. In Proceedings of ASRU.

726


