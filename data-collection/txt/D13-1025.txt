










































Interactive Machine Translation using Hierarchical Translation Models


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 244–254,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Interactive Machine Translation using Hierarchical Translation Models

Jesús González-Rubio, Daniel Ortiz-Martı́nez, José-Miguel Benedı́, Francisco Casacuberta
D. de Sistemas Informáticos y Computación

Universitat Politècnica de València
Camino de Vera s/n, 46021 Valencia (Spain)

{jegonzalez,dortiz,jbenedi,fcn}@dsic.upv.es

Abstract

Current automatic machine translation sys-
tems are not able to generate error-free trans-
lations and human intervention is often re-
quired to correct their output. Alternatively,
an interactive framework that integrates the
human knowledge into the translation pro-
cess has been presented in previous works.
Here, we describe a new interactive ma-
chine translation approach that is able to work
with phrase-based and hierarchical translation
models, and integrates error-correction all in
a unified statistical framework. In our experi-
ments, our approach outperforms previous in-
teractive translation systems, and achieves es-
timated effort reductions of as much as 48%
relative over a traditional post-edition system.

1 Introduction

Research in the field of machine translation (MT)
aims to develop computer systems which are able
to translate between languages automatically, with-
out human intervention. However, the quality of
the translations produced by any automatic MT sys-
tem still remain below than that of human transla-
tion. Typical solutions to reach human-level quality
require a subsequent manual post-editing process.
Such decoupled post-edition solution is rather inef-
ficient and tedious for the human translator. More-
over, it prevents the MT system from taking advan-
tage of the knowledge of the human translator and,
reciprocal, the human translator cannot take advan-
tage of the adapting ability of MT technology.

An alternative way to take advantage of the exist-
ing MT technology is to use them in collaboration
with human translators within a computer-assisted

translation (CAT) or interactive framework (Isabelle
and Church, 1998). The TransType and TransType2
projects (Foster et al., 1998; Langlais and Lapalme,
2002; Barrachina et al., 2009) entailed an interesting
focus shift in CAT technology by aiming interaction
directly at the production of the target text. These
research projects proposed to embed an MT system
within an interactive translation environment. This
way, the human translator can ensure a high-quality
output while the MT system ensures a significant
gain of productivity. Particularly interesting is the
interactive machine translation (IMT) approach pro-
posed in (Barrachina et al., 2009). In this scenario,
a statistical MT (SMT) system uses the source sen-
tence and a previously validated part (prefix1) of its
translation to propose a suitable continuation. Then
the user finds and corrects the next system error,
thereby providing a longer prefix which the system
uses to suggests a new, hopefully better continua-
tion. The reported results showed that IMT can save
a significant amount of human effort.

Barrachina et al,. (2009) provide a thorough de-
scription of the IMT approach and describe algo-
rithms for its practical implementation. Neverthe-
less, we identify two basic problems for which we
think there is room for improvement. The first prob-
lem arises when the system cannot generate the pre-
fix validated by the user. To solve this problem,
the authors simply provide an ad-hoc heuristic error-
correction technique. The second problem is how
the system deals with word reordering. Particularly,
the models used by the system were either mono-

1We use the terms prefix and suffix to denote any sub-string
at the beginning and end respectively of a string of characters
(including spaces and punctuation). These terms do not imply
any morphological significance as they usually do in linguistics.

244



tonic by nature or non-monotonic but heuristically
defined (not estimated from training data).

We work on the foundations of Barrachina et
al., (2009) and provide formal solutions to these
two challenges. On the one hand, we adopt the
statistical formalization of the IMT framework de-
scribed in (Ortiz-Martı́nez, 2011), which includes
a stochastic error-correction model in its formaliza-
tion to address prefix coverage problems. Moreover,
we refine this formalization proposing an alternative
error-correction formalization for the IMT frame-
work (Section 2). Additionally, we also propose a
specific error-correction model based on a statisti-
cal interpretation of the Levenshtein distance (Lev-
enshtein, 1966). These formalizations provide a
unified statistical framework for the IMT model in
comparison to the ad-hoc heuristic error-correction
methods previously used.

In order to address the problem of properly deal
with reordering in IMT, we introduce the use of hi-
erarchical MT models (Chiang, 2005; Zollmann and
Venugopal, 2006). These methods provide a natural
approach to handle long range dependencies and al-
low the incorporation of reordering information into
a consistent statistical framework. Here, we also de-
scribe how state-of-the-art hierarchical MT models
can be extended to handle IMT (Sections 3 and 4).

We evaluate the proposed IMT approach on two
different translation task. The comparative results
against the IMT approach described by Barrachina
et al., (2009) and a conventional post-edition ap-
proach show that our IMT formalization for hier-
archical SMT models indeed outperform other ap-
proaches (Sections 5 and 6). Moreover, it leads to
large reductions in the human effort required to gen-
erate error-free translations.

2 Statistical Framework

2.1 Statistical Machine Translation

Assuming that we are given a sentence s in a source
language, the translation problem can be stated as
finding its translation t in a target language of max-
imum probability (Brown et al., 1993):

t̂ = arg max
t

Pr(t | s) (1)

= arg max
t

Pr(t) · Pr(s | t) (2)

source (s): Para ver la lista de recursos
desired translation (t̂): To view a listing of resources

IT-0 p
ts To view the resources list

IT-1
p To view
k a
ts list of resources

IT-2
p To view a list
k list i
ts list i ng resources

IT-3
p To view a listing
k o
ts o f resources

END p To view a listing of resources

Figure 1: IMT session to translate a Spanish sentence into
English. The desired translation is the translation the hu-
man user wants to obtain. At IT-0, the system suggests
a translation (ts). At IT-1, the user moves the mouse to
accept the first eight characters “To view ” and presses the
a key (k), then the system suggests completing the sen-

tence with “list of resources” (a new ts). Iterations 2 and
3 are similar. In the final iteration, the user accepts the
current translation.

The terms in the latter equation are the lan-
guage model probability Pr(t) that represents the
well-formedness of t (n-gram models are usu-
ally adopted), and the (inverted) translation model
Pr(s | t) that represents the relationship between the
source sentence and its translation.

In practice all of these models (and possibly oth-
ers) are often combined into a log-linear model for
Pr(t | s) (Och and Ney, 2002):

t̂ = arg max
t

{
N∑

n=1

λn · log(fn(t, s))

}
(3)

where fn(t, s) can be any model that represents an
important feature for the translation, N is the num-
ber of models (or features), and λn are the weights
of the log-linear combination.

2.2 Statistical Interactive Machine Translation
Unfortunately, current MT technology is still far
from perfect. This implies that, in order to achieve
good translations, manual post-editing is needed.
An alternative to this decoupled approach (first
MT, then manual correction) is given by the IMT

245



paradigm (Barrachina et al., 2009). Under this
paradigm, translation is considered as an iterative
left-to-right process where the human and the com-
puter collaborate to generate the final translation.

Figure 1 shows an example of the IMT approach.
There, a source Spanish sentence s =”Para ver la
lista de recursos” is to be translated into a target En-
glish sentence t̂. Initially, with no user feedback, the
system suggests a complete translation ts =”To view
the resources list”. From this translation, the user
marks a prefix p =”To view” as correct and begins
to type the rest of the target sentence. Depending on
the system or the user’s preferences, the user might
type the full next word, or only some letters of it (in
our example, the user types the single next charac-
ter “a”). Then, the MT system suggests a new suffix
ts =“list of resources” that completes the validated
prefix and the input the user has just typed (p =”To
view a”). The interaction continues with a new pre-
fix validation followed, if necessary, by new input
from the user, and so on, until the user considers the
translation to be complete and satisfactory.

The crucial step of the process is the production
of the suffix. Again decision theory tells us to max-
imize the probability of the suffix given the avail-
able information. Formally, the best suffix of a given
length will be:

t̂s = arg max
ts

Pr(ts | s,p) (4)

which can be straightforwardly rewritten as:

t̂s = arg max
ts

Pr(p, ts | s) (5)

= arg max
ts

Pr(p, ts) · Pr(s | p, ts) (6)

Note that, since p ts = t, this equation is very
similar to Equation (2). The main difference is that
now the search process is restricted to those target
sentences t that contains p as prefix. This implies
that we can use the same MT models (including
the log-linear approach) if the search procedures are
adequately modified (Och et al., 2003). Finally, it
should be noted that the statistical models are usu-
ally defined at word level, while the IMT process
described in this section works at character level. To
deal with this problem, during the search process it
is necessary to verify the compatibility between t
and p at character level.

2.3 IMT with Stochastic Error-Correction
A common problem in IMT arises when the user sets
a prefix which cannot be explained by the statistical
models. To solve this problem, IMT systems typi-
cally include ad-hoc error-correction techniques to
guarantee that the suffixes can be generated (Bar-
rachina et al., 2009). As an alternative to this heuris-
tic approach, Ortiz-Martı́nez (2011) proposed a for-
malization of the IMT framework that does include
stochastic error-correction models in its statistical
formalization. The starting point of this alternative
IMT formalization accounts for the problem of find-
ing the translation t that, at the same time, better
explains the source sentence s and the prefix given
by the user p:

t̂ = arg max
t

Pr(t | s,p) (7)

= arg max
t

Pr(t) · Pr(s,p | t) (8)

The following naı̈ve Bayes’ assumption is now
made: the source sentence s and the user prefix p are
statistically independent variables given the transla-
tion t, obtaining:

t̂ = arg max
t

Pr(t) · Pr(s | t) · Pr(p | t) (9)

where Pr(t) can be approximated with a language
model, Pr(s | t) can be approximated with a trans-
lation model, and Pr(p | t) can be approximated
by an error correction model that measures the com-
patibility between the user-defined prefix p and the
hypothesized translation t.

Note that the translation result, t̂, given by Equa-
tion (9) may not contain p as prefix because every
translation is compatible with p with a certain prob-
ability. Thus, despite being close, Equation (9) is not
equivalent to the IMT formalization in Equation (6).

To solve this problem, we define an alignment,
a, between the user-defined prefix p and the hy-
pothesized translation t, so that the unaligned words
of t, in an appropriate order, constitute the suffix
searched in IMT. This allows us to rewrite the error
correction probability as follows:

Pr(p | t) =
∑
a

Pr(p,a | t) (10)

To simplify things, we assume that p is mono-
tonically aligned to t, leaving the potential word-
reordering to the language and translation models.

246



Under this assumption, a determines an alignment
for t, such that t = tpts, where tp is fully-aligned to
p and ts remains unaligned. Taking all these things
into consideration, and following a maximum ap-
proximation, we finally arrive to the expression:

(t̂, â) = arg max
t,a

Pr(t)·Pr(s | t)·Pr(p,a | t) (11)

where the suffix required in IMT is obtained as the
portion of t̂ that is not aligned with the user prefix.

In practice, we combine the models in Equa-
tion (11) in a log-linear fashion as it is typically done
in SMT (see Equation (3)).

2.4 Alternative Formalization for IMT with
Stochastic Error-Correction

Alternatively to Equation (11), we can operate from
Equation (9) and reach a different formalization for
IMT with error-correction. We can re-write the first
and last terms of Equation (9) as:

Pr(t) · Pr(p | t) = Pr(p) · Pr(t | p) (12)

As in the previous section, we introduce an align-
ment variable, a, between t and p, giving:

Pr(t | p) =
∑
a

Pr(t,a | p) (13)

=
∑
a

Pr(a | p) · Pr(t | p,a) (14)

If we consider monotonic alignments, a defines
again an alignment between a prefix of the system
translation (tp) and the user prefix, producing the
suffix required in IMT (ts) as the unaligned part.
Thus, we can re-write Pr(t | p,a) as:

Pr(t | p,a) = Pr(tp, ts | p,a) (15)
≈ Pr(tp | p,a) · Pr(ts | p,a) (16)

where Equation (16) has been obtained following a
naı̈ve Bayes’ decomposition.

Combining equations (12), (14), and (16) into
Equation (9), and following a maximum approxima-
tion for the summation of the alignment variable a,
we arrive to the following expression:

(t̂, â) = arg max
t,a

Pr(s |t)·Pr(tp |p,a)·Pr(ts |p,a) (17)

where Pr(p) and Pr(a|p) have been dropped down
because the former does not participate in the maxi-
mization and the latter is assumed uniform.

The terms in this equation can be interpreted sim-
ilarly as those in Equation (9): Pr(s | t) is the trans-
lation model, Pr(tp | p,a) is the error-correction
probability that measures the compatibility between
the prefix tp of the hypothesized translation and the
user-defined prefix p, and Pr(ts | p,a) is the lan-
guage model for the corresponding suffix ts condi-
tioned by the user-defined prefix. Again, in the ex-
periments we combine the different models in a log-
linear fashion.

The main difference between the two alternative
IMT formalization (Equations (11) and (17)) is that
in the latter the suffix to be returned is conditioned
by the user-validated prefix p. Thus, in the fol-
lowing we will refer to Equation (11) as indepen-
dent suffix formalization while we will denote Equa-
tion (17) by conditioned suffix formalization.

3 Statistical Models

We now present the statistical models used to esti-
mate the probability distributions described in the
previous section. Section 3.1 describes the error-
correction model, while Section 3.2 describes the
models for the conditional translation probability.

3.1 Statistical Error-Correction Model

Following the vast majority of IMT systems de-
scribed in the literature, we implement an error-
correction model based on the concept of edit dis-
tance (Levenshtein, 1966). Typically, IMT systems
use non-probabilistic error correction models. The
first stochastic error correction model for IMT was
proposed in (Ortiz-Martı́nez, 2011) and it is based
on probabilistic finite state machines. Here, we pro-
pose a simpler approach which can be seen as a
particular case of the previous one. Specifically,
the proposed approach models the edit distance as a
Bernoulli process where each character of the candi-
date string has a probability pe of being erroneous.
Under this interpretation, the number of characters
that need to be edited E in a sentence of length n
is a random variable that follows a binomial distri-
bution, E ∼ B(n, pe), with parameters n and pe.
The probability of performing exactly k edits in a

247



sentence of n characters is given by the following
probability mass function:

f(k;n, pe) =
n!

k!(n− k)!
pke(1− pe)n−k (18)

Note that this error-correction model penalizes
equally all edit operations. Alternatively, we can
model the distance with a multinomial distribution
and assign different probabilities to different types
of edit operations. Nevertheless, we adhere to the
binomial approximation due to its simplicity.

Finally, we compute the error-correction proba-
bility between two strings from the total number of
edits required to transform the candidate translation
into the reference translation. Specifically, we define
the error-correction distribution in Equation (11) as:

Pr(p,a | t) ≈ |p|!
k!(|p| − k)!

pke(1− pe)|p|−k (19)

where k = Lev(p, ta) is the character-level Lev-
enshtein distance between the user defined prefix p
and the prefix ta of the hypothesized translation t
defined by alignment a. The error-correction prob-
ability Pr(tp | p,a) in Equation (17) is computed
analogously.

The probability of edition pe is the single free pa-
rameter of this formulation. We will use a separate
development corpus to find an adequate value for it.

3.2 Statistical Machine Translation Models

Next sections briefly describe the statistical transla-
tion models used to estimate the conditional proba-
bility distribution Pr(s | t). A detailed description
of each model can be found in the provided citations.

3.2.1 Phrase-Based Translation Models

Phrase-based translation models (Koehn et al.,
2003) are an instance of the noisy-channel approach
in Equation (2). The translation of a source sentence
s is obtained through a generative process composed
of three steps: first, the s is divided into K segments
(phrases), next, each source phrase, s̃, is translated
into a target phrase t̃, and finally the target phrases
are reordered to compose the final translation.

The usual phrase-based implementation of the
translation probability takes a log-linear form:

Pr(s | t) ≈ λ1 · |t|+ λ2 ·K+
K∑

k=1

[
λ3 · log(P (s̃k | t̃k)) + λ4 · d(j)

]
(20)

where P (s̃ | t̃) is the translation probability between
source phrase s̃ and target phrase t̃, and d(j) is a
function (distortion model) that returns the score of
translating the k-th source phrase given that it is sep-
arated j words from the (k−1)-th phrase. Weights λ1
and λ2 play a special role since they are used to con-
trol the number of words and the number of phrases
of the target sentence to be generated, respectively.

3.2.2 Hierarchical Translation Models
Phrase-based models have shown a very strong

performance when translating between languages
that have similar word orders. However, they are not
able to adequately capture the complex relationships
that exist between the word orders of languages of
different families such as English and Chinese. Hi-
erarchical translation models provide a solution to
this challenge by allowing gaps in the phrases (Chi-
ang, 2005):

yu X1 you X2→ have X2 with X1

where subscripts denote placeholders for sub-
phrases. Since these rules generalize over possi-
ble phrases, they act as discontinuous phrase pairs
and may also act as phrase-reordering rules. Hence,
they are not only considerably more powerful than
conventional phrase pairs, but they also integrate re-
ordering information into a consistent framework.

These hierarchical phrase pairs are formalized as
rewrite rules of a synchronous context-free grammar
(CFG) (Aho and Ullman, 1969):

X →< γ,α,∼> (21)

where X is a non-terminal, γ and α are both strings
of terminals (words) and non-terminals , and ∼ is
a one-to-one correspondence between non-terminal
occurrences in γ and α. Given the example above,
γ ≡“yu X1 you X2”, α ≡“have X2 with X1”, and∼
is indicated by the subscript numbers.

Additionally, two glue rules are also defined:

S →<S1X2 , S1X2> S →<X1 , X1>

248



These give the model the option to build only par-
tial translations using hierarchical phrases, and then
combine them serially as in a phrase-based model.

The typical implementation of the hierarchical
translation model also takes the form of a log-linear
model. Let sδ and tδ be the source and target strings
generated by a derivation δ of the grammar. Then,
the conditional translation probability is given by:

Pr(sδ | tδ) ≈ λ1 · |tδ|+ λ2 · |δ|+ λ3 ·#g(δ)+∑
r∈δ

[λ4 · w(r)] (22)

where |δ| denotes the total number of rules used
in δ, #g(δ) returns the number of applications of
the glue rules, r ∈ δ are the rules in δ, and w(r)
is the weight of rule r. Weights λ1 and λ2 have
a similar interpretation as for phrase-based models,
they respectively give some control over the total
number of words and rules that conform the trans-
lation. Additionally, λ3 controls the model’s prefer-
ence for hierarchical phrases over serial combination
of phrases. Note that no distortion model is included
in the previous equation. Here, reordering is defined
at rule level by the one-to-one non-terminal corre-
spondence. In other words, reordering is a property
inherent to each rule and it is the individual score of
each rule what defines, at each step of the derivation,
the importance of reordering.

It should be noted that the IMT formalizations
presented in Section 2 can be applied to other hier-
archical or syntax-based SMT models such as those
described in (Zollmann and Venugopal, 2006; Shen
et al., 2010).

4 Search

In offline MT, the generation of the best trans-
lation for a given source sentence is carried out
by incrementally generating the target sentence2.
This process fits nicely into a dynamic program-
ming (DP) (Bellman, 1957) framework, as hypothe-
ses which are indistinguishable by the models can
be recombined. Since the DP search space grows
exponentially with the size of the input, standard DP
search is prohibitive, and search algorithms usually
resort to a beam-search heuristic (Jelinek, 1997).

2Phrase-based systems follow a left-to-right generation or-
der while hierarchical systems rely on a CYK-like order.

6

1

5

I saw a man with a telescope

2 3

4

I saw a man with a telescope
I saw with a telescope a man

Figure 2: Example of a hypergraph encoding two differ-
ent translations (one solid and one dotted) for the Spanish
sentence “Vi a un hombre con un telescopio”.

Due to the demanding temporal constraints inher-
ent to any interactive environment, performing a full
search each time the user validates a new prefix is
unfeasible. The usual approach is to rely on a certain
representation of the search space that includes the
most probable translations of the source sentence.
The computational cost of this approach is much
lower, as the whole search for the translation must
be carried out only once, and the generated represen-
tation can be reused for further completion requests.

Next, we introduce hypergraphs, the formalism
chosen to represent the search space of both phrase-
based and hierarchical systems (Section 4.1). Then,
we describe the algorithms implemented to search
for suffix completions in them (Section 4.2).

4.1 Hypergraphs

A hypergraph is a generalization of the concept
of graph where the edges (now called hyperedges)
may connect several nodes (hypernodes) at the same
time. Formally, a hypergraph is a weighted acyclic
graph represented by a pair < V, E >, where V is a
set of hypernodes and E is a set of hyperedges. Each
hyperedge e ∈ E connects a head hypernode and a
set of tail hypernodes. The number of tail nodes is
called the arity of the hyperedge and the arity of a
hypergraph is the maximum arity of its hyperedges.

We can use hypergraphs to represent the deriva-
tions for a given CFG. Each hypernode represents
a partial translation generated during the decoding
process. Each ingoing hyperedge represents the rule
with which the corresponding non-terminal was sub-
stituted. Moreover, hypergraphs can represent a
whole set of possible translations. An example is

249



shown in Figure 2. Two alternative translations are
constructed from the leave nodes (1, 2 and 3) up to
the root node (6) of the hypergraph. Additionally,
hypernodes and hyperedges may be shared among
different derivations if they represent the same in-
formation. Thus, we can achieve a compact repre-
sentation of the translation space that allows us to
derive efficient search algorithms.

Note that word-graphs (Ueffing et al., 2002),
which are used to represent the search space for
phrase-based models, are a special case of hyper-
graphs in which the maximum arity is one. Thus,
hypergraphs allow us to represent both phrase-based
and hierarchical systems in a unified framework.

4.2 Suffix Search on Hypergraphs
Now, we describe a unified search process to obtain
the suffix ts that completes a prefix p given by the
user according to the two IMT formulations (Equa-
tion (11) and Equation (17)) described in Section 2.

Given an hypergraph, certain hypernodes define a
possible solution to the maximization defined in the
two IMT formulations. Specifically, only those hy-
pernodes that generate a prefix of a potential trans-
lation are to be taken into account3. The prob-
ability of the solution defined by each hypernode
has two components, namely the probability of the
SMT model (given by the language and translation
models) and the probability of the error-correction
model. On the one hand, the SMT model probabil-
ity is given by the translation of maximum probabil-
ity through the hypernode. On the other hand, the
error-correction probability is computed between p
and the partial translation of maximum probability
actually covered by the hypernode. Among all the
solutions defined by the hypernodes, we finally se-
lect that of maximum probability.

Once the best-scoring hypernode is identified, the
rest of the translation not covered by it is returned as
the suffix completion required in IMT.

5 Experimental Framework

The models and search procedure introduced in the
previous sections were assessed through a series of

3For example, in Figure 2 the hypernodes that generate pre-
fixes are those labeled with numbers 1 (“I saw”), 4 (“I saw with
a telescope) and 6 (“I saw a man with a telescope” and “I saw
with a telescope a man”).

EU (Es/En)
Train Development Test

Sentences 214K 400 800
Token 5.9M / 5.2M 12K / 10K 23K / 20K
Vocabulary 97K / 84K 3K / 3K 5K / 4K

TED (Zh/En)
Train Development Test

Sentences 107K 934 1664
Token 2M / 2M 22K / 20K 33K / 32K
Vocabulary 42K / 52K 4K / 3K 4K / 4K

Table 1: Main figures of the processed EU and TED cor-
pora. K and M stand for thousands and millions of ele-
ments respectively.

IMT experiments with different corpora. These cor-
pora, the experimental methodology, and the evalu-
ation measures are presented in this section.

5.1 EU and TED corpora

We tested the proposed methods in two different
translation tasks each one involving a different lan-
guage pair: Spanish-to-English (Es–En) for the EU
(Bulletin of the European Union) task, and Chinese-
to-English (Zh–En) for the TED (TED4 talks) task.

The EU corpora were extracted from the Bul-
letin of the European Union, which exists in all of-
ficial languages of the European Union and is pub-
licly available on the Internet. Particularly, the cho-
sen Es–En corpus was part of the evaluation of the
TransType2 project (Barrachina et al., 2009). The
TED talks is a collection of recordings of public
speeches covering a variety of topics, and for which
high quality transcriptions and translations into sev-
eral languages are available. The Zh–En corpus
used in the experiments was part of the MT track
in the 2011 evaluation campaign of the workshop on
spoken language translation (Federico et al., 2011).
Specifically, we used the dev2010 partition for de-
velopment and the test2010 partition for test.

We process the Spanish and English parts of the
EU corpus to separate words and punctuation marks
keeping sentences truecase. Regarding the TED cor-
pus, we tokenized and lowercased the English part
(Chinese has no case information), and split Chi-
nese sentences into words with the Stanford word

4www.ted.com

250



segmenter (Tseng et al., 2005). Table 1 shows the
main figures of the processed EU and TED corpora.

5.2 Model Estimation and User Simulation

We used the standard configuration of the Moses
toolkit (Koehn et al., 2007) to estimate one phrase-
based and one hierarchical model for each cor-
pus; log-linear weights were optimized by minimum
error-rate training (Och, 2003) with the development
partitions. Then, the optimized models were used to
generate the word-graphs and hypergraphs with the
translations of the development and test partitions.

A direct evaluation of the proposed IMT proce-
dures involving human users would have been slow
and expensive. Thus, following previous works in
the literature (Barrachina et al., 2009; González-
Rubio et al., 2010), we used the references in the
corpora to simulate the translations that a human
user would want to obtain. Each time the system
suggested a new translation, it was compared to
the reference and the longest common prefix (LCP)
was obtained. Then, the first non-matching charac-
ter was replaced by the corresponding character in
the reference and a new system suggestion was pro-
duced. This process is iterated until a full match with
the reference was obtained.

Finally, we used this user simulation to optimize
the value for the probability of edition pe in the
error-correction model (Section 3.1), and for the log-
linear weights in the proposed IMT formulations. In
this case, these values were chosen so that they min-
imize the estimated user effort required to interac-
tively translate the development partitions.

5.3 Evaluation Measures

Different measures have been adopted to evaluate
the proposed IMT approach. On the one hand, dif-
ferent IMT systems can be compared according to
the effort needed by a human user to generate the de-
sired translations. This effort is usually estimated as
the number of actions performed by the user while
interacting with the system. In the user simulation
described above these actions are: looking for the
next error and moving the mouse pointer to that po-
sition (LCP computation), and correcting errors with
some key strokes. Hence, we implement the follow-
ing IMT effort measure (Barrachina et al., 2009):

Key-stroke and mouse-action ratio (KSMR):
number of key strokes plus mouse movements per-
formed by the user, divided by the total number of
characters in the reference.

On the other hand, we also want to compare the
proposed IMT approach against a conventional CAT
approach without interactivity, such as a decoupled
post-edition system. For such systems, character-
level user effort is usually measured by the Charac-
ter Error Rate (CER). However, it is clear that CER
is at a disadvantage due to the auto-completion ap-
proach of IMT. To perform a fairer comparison be-
tween post-edition and IMT, we implement a post-
editing system with autocompletion. Here, when
the user enters a character to correct some incor-
rect word, the system automatically completes the
word with the most probable word in the task vo-
cabulary. To evaluate the effort of a user using such
a system, we implement the following measure pro-
posed in (Romero et al., 2010):

Post-editing key stroke ratio (PKSR): using
a post-edition system with word-autocompleting,
number of user key strokes divided by the total num-
ber of reference characters.

The counterpart of PKSR in an IMT scenario
is (Barrachina et al., 2009):

Key-stroke ratio (KSR): number of key strokes,
divided by the number of reference characters.

PKSR and KSR are fairly comparable and the rel-
ative difference between them gives us a good es-
timate of the reduction in human effort that can be
achieved by using IMT instead of a conventional
post-edition system.

We also evaluate the quality of the automatic
translations generated by the MT models with the
widespread BLEU score (Papineni et al., 2002).

Finally, we provide both confidence intervals for
the results and statistical significance of the ob-
served differences in performance. Confidence in-
tervals were computed by pair-wise re-sampling as
in (Zhang and Vogel, 2004) while statistical signifi-
cance was computed using the Tukey’s HSD (honest
significance difference) test (Hsu, 1996).

251



EU TED
WG HG WG HG

1-best BLEU [%] 45.0 45.1 11.0 11.2
1000-best Avg. BLEU [%] 43.6 44.2 10.2 11.0

Table 2: BLEU score of the word-graphs (WG) and hy-
pergraphs (HG) used to implement the IMT procedures.

IMT EU TED
Setup PB HT PB HT

ISF 27.4±.5∗ 26.5±.5∗ 53.0±.4∗ 52.3±.4∗
CSF 26.6±.5∗ 25.1±.5F 52.2±.4∗ 50.8±.4F

Table 3: IMT results (KSMR [%]) for the EU and
TED tasks using the independent suffix formalization
(ISF) and the conditioned suffix formalization (CSF). PB
stands for phrase-based model and HT stands for hierar-
chical translation model. For each task, the best result
is displayed boldface, an asterisk ∗ denotes a statistically
significant better result (99% confidence) with respect to
ISF with PB, and a star F denotes a statistically signifi-
cant difference with respect to all the other systems.

6 Results

We start by reporting conventional MT quality re-
sults to test if the generated word-graphs and hyper-
graphs encode translations of similar quality. Ta-
ble 2 displays the quality (BLEU (Papineni et al.,
2002)) of the automatic translations generated for
the test partitions. The lower 1-best BLEU results
obtained for TED show that this is a much more dif-
ficult task than EU. Additionally, the similar aver-
age BLEU results obtained for the 1000-best transla-
tions indicate that word-graphs and hypergraphs en-
code translations of similar quality. Thus, the IMT
systems that use these word-graphs and hypergraphs
can be compared in a fair way.

Then, we evaluated different setups of the pro-
posed IMT approach. Table 3 displays the IMT re-
sults obtained for the EU and TED tasks. We report
KSMR (as a percentage) for the independent suffix
formalization (ISF) and the conditioned suffix for-
malization (CSF) using both phase-based (PB) and
hierarchical (HT) translation models. The KSMR
result of ISF using a phrase-based model can be con-
sidered our baseline since this setup is comparable
to that used in (Barrachina et al., 2009). Results for
HT consistently outperformed the corresponding re-
sults for PB. Similarly, results for CSF were con-

EU TED
PE IMT PE IMT

PKSR [%] KSR [%] PKSR [%] KSR [%]

27.1 14.1 (48%) 40.8 29.7 (27.2%)

Table 4: Estimation of the effort required to translate
the test partition of the EU and TED tasks using post-
editing with word-completion (PE) and IMT under the
independent suffix formalization (IMT). We used hierar-
chical MT in both approaches. In parenthesis we display
the estimated effort reduction of IMT with respect to PE.

sistently better than those for ISF. More specifically,
no statistically significant difference were found be-
tween ISF with HT and CSF with PB but both sta-
tistically outperformed the baseline (ISF with PB).
Finally, CSF with HT statistically outperformed the
other three configurations reducing KSMR by ∼2.2
points with respect to the baseline. We hypothe-
size that the better results of HT can be explained
by its more efficient representation of word reorder-
ing. Regarding the CSF, its better results are due to
the better suffixes that can be obtained by taking into
account the actual prefix validated by the user.

Finally, we compared the estimated human effort
required to translate the test partitions of the EU and
TED corpora with the best IMT configuration (inde-
pendent suffix formalization with hierarchical trans-
lation model) and a conventional post-editing (PE)
CAT system with word-completion. That is, when
the user corrects a character, the PE system auto-
matically proposes a different word that begins with
the given word prefix but, obviously, the rest of the
sentence is not changed. According to the results,
the estimated human effort to generate the error-free
translations was significantly reduced with respect
to using the conventional PE approach. IMT can
save about 48% of the overall eastimated effort for
the EU task and about 27% for the TED task.

7 Summary and Future Work

We have proposed a new IMT approach that uses hi-
erarchical SMT models as its underlying translation
technology. This approach is based on a statistical
formalization previously described in the literature
that includes stochastic error correction. Addition-
ally, we have proposed a refined formalization that
improves the quality of the IMT suffixes by taking

252



into account the prefix validated by the user. More-
over, since word-graphs constitute a particular case
of hypergraphs, we are able to manage both phrase-
based and hierarchical translation models in a uni-
fied IMT framework.

Simulated results on two different translation
tasks showed that hierarchical translation models
outperform phrase-based models in our IMT frame-
work. Additionally, the proposed alternative IMT
formalization also allows to improve the results of
the IMT formalization previously described in the
literature. Finally, the proposed IMT system with
hierarchical SMT models largely reduces the esti-
mated user effort required to generate correct trans-
lations in comparison to that of a conventional post-
edition system. We look forward to corroborating
these result in test with human translators.

There are many ways to build on the work de-
scribed here. In the near future, we plan to explore
the following research directions:

• Alternative IMT scenarios where the user is not
bounded to correct translation errors in a left-
to-right fashion. In such scenarios, the user will
be allowed to correct errors at any position in
the translation while the IMT system will be
required to derive translations compatible with
these isolated corrections.

• Adaptive translation engines that take advan-
tage of the user’s corrections to improve its sta-
tistical models. As the translator works and
corrects the proposed translations, the transla-
tion engine will be able to make better predic-
tions. One of the first works on this topic was
proposed in (Nepveu et al., 2004). More re-
cently, Ortiz-Martı́nez et al. (2010) described a
set of techniques to obtain an incrementally up-
dateable IMT system, solving technical prob-
lems encountered in previous works.

• More sophisticated measures to estimate the
human effort. Specifically, measures that esti-
mate the cognitive load involve in reading, un-
derstanding and detecting an error in a trans-
lation (Foster et al., 2002), in contrast KSMR
simply considers a constant cost. This will lead
to a more accurate estimation of the improve-
ments that may be expected by a human user.

Acknowledgments

Work supported by the European Union 7th Frame-
work Program (FP7/2007-2013) under the Cas-
MaCat project (grans agreement no 287576), by
Spanish MICINN under grant TIN2012-31723, and
by the Generalitat Valenciana under grant ALMPR
(Prometeo/2009/014).

References

Alfred V. Aho and Jeffrey D. Ullman. 1969. Syn-
tax directed translations and the pushdown assembler.
Journal of Computer and Systems Science, 3(1):37–
56, February.

Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi, An-
tonio Lagarda, Hermann Ney, Jesús Tomás, Enrique
Vidal, and Juan-Miguel Vilar. 2009. Statistical ap-
proaches to computer-assisted translation. Computa-
tional Linguistics, 35:3–28, March.

Richard Bellman. 1957. Dynamic Programming.
Princeton University Press, Princeton, NJ, USA, 1 edi-
tion.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263–311.

David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263–270.

M. Federico, L. Bentivogli, M. Paul, and S. Stüker. 2011.
Overview of the iwslt 2011 evaluation campaign. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 11–20.

George Foster, Pierre Isabelle, and Pierre Plamondon.
1998. Target-text mediated interactive machine trans-
lation. Machine Translation, 12(1/2):175–194, Jan-
uary.

George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In Proceedings of the 2002 conference on Empirical
methods in natural language processing - Volume 10,
pages 148–155.

Jesús González-Rubio, Daniel Ortiz-Martı́nez, and Fran-
cisco Casacuberta. 2010. Balancing user effort and
translation error in interactive machine translation via
confidence measures. In Proceedings of the ACL 2010
Conference Short Papers, pages 173–177.

Jason Hsu. 1996. Multiple Comparisons: Theory and
Methods. Chapman and Hall/CRC.

253



Pierre Isabelle and Ken Church. 1998. Special issue on:
New tools for human translators, volume 12. Kluwer
Academic Publishers, January.

Frederick Jelinek. 1997. Statistical methods for speech
recognition. MIT Press.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, demonstration session, June.

Philippe Langlais and Guy Lapalme. 2002. TransType:
development-evaluation cycles to boost translator’s
productivity. Machine Translation, 17(2):77–98,
September.

Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710, February.

Laurent Nepveu, Guy Lapalme, Philippe Langlais, and
George Foster. 2004. Adaptive language and trans-
lation models for interactive machine translation. In
Proceedings of the conference on Empirical Methods
on Natural Language Processing, pages 190–197.

Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302.

Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In Proceedings of the European
chapter of the Association for Computational Linguis-
tics, pages 387–393.

Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 160–167. Association for
Computational Linguistics.

Daniel Ortiz-Martı́nez, Ismael Garcı́a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for inter-
active statistical machine translation. In Proceedings
of the 2010 Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 546–554.

Daniel Ortiz-Martı́nez. 2011. Advances in Fully-
Automatic and Interactive Phrase-Based Statistical

Machine Translation. Ph.D. thesis, Universitat
Politècnica de València. Advisors: Ismael Garcı́a
Varea and Francisco Casacuberta.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318. Associa-
tion for Computational Linguistics.

Veronica Romero, Alejandro H. Toselli, and Enrique Vi-
dal. 2010. Character-level interaction in computer-
assisted transcription of text images. In Proceedings
of the 12th International Conference on Frontiers in
Handwriting Recognition, pages 539–544.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine translation.
Computational Linguistics, 36(4):649–671, Decem-
ber.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing.

Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine trans-
lation. In Proceedings of the conference on Empirical
Methods in Natural Language Processing, pages 156–
163.

Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of The international Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation.

Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 138–141.

254


