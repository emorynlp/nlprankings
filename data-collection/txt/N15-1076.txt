



















































Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 746–755,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Is Your Anchor Going Up or Down?
Fast and Accurate Supervised Topic Models

Thang Nguyen
iSchool and UMIACS

University of Maryland
and National Library of Medicine,

National Institutes of Health
daithang@umiacs.umd.edu

Jordan Boyd-Graber
Computer Science

University of Colorado Boulder
Jordan.Boyd.Graber

@colorado.edu

Jeff Lund,
Kevin Seppi, Eric Ringger

Computer Science
Brigham Young University

{jefflund,kseppi}@byu.edu
ringger@cs.byu.edu

Abstract

Topic models provide insights into document
collections, and their supervised extensions
also capture associated document-level meta-
data such as sentiment. However, inferring
such models from data is often slow and cannot
scale to big data. We build upon the “anchor”
method for learning topic models to capture the
relationship between metadata and latent top-
ics by extending the vector-space representa-
tion of word-cooccurrence to include metadata-
specific dimensions. These additional dimen-
sions reveal new anchor words that reflect spe-
cific combinations of metadata and topic. We
show that these new latent representations pre-
dict sentiment as accurately as supervised topic
models, and we find these representations more
quickly without sacrificing interpretability.

Topic models were introduced in an unsupervised
setting (Blei et al., 2003), aiding in the discovery of
topical structure in text: large corpora can be dis-
tilled into human-interpretable themes that facilitate
quick understanding. In addition to illuminating doc-
ument collections for humans, topic models have
increasingly been used for automatic downstream
applications such as sentiment analysis (Titov and
McDonald, 2008; Paul and Girju, 2010; Nguyen et
al., 2013).

Unfortunately, the structure discovered by unsuper-
vised topic models does not necessarily constitute the
best set of features for tasks such as sentiment analy-
sis. Consider a topic model trained on Amazon prod-
uct reviews. A topic model might discover a topic
about vampire romance. However, we often want to

go deeper, discovering facets of a topic that reflect
topic-specific sentiment, e.g., “buffy” and “spike” for
positive sentiment vs. “twilight” and “cullen” for
negative sentiment. Techniques for discovering such
associations, called supervised topic models (Sec-
tion 2), both produce interpretable topics and predict
metadata values. While unsupervised topic models
now have scalable inference strategies (Hoffman et
al., 2013; Zhai et al., 2012), supervised topic model
inference has not received as much attention and of-
ten scales poorly.

The anchor algorithm is a fast, scalable unsuper-
vised approach for finding “anchor words”—precise
words with unique co-occurrence patterns that can
define the topics of a collection of documents. We
augment the anchor algorithm to find supervised
sentiment-specific anchor words (Section 3). Our
algorithm is faster and just as effective as traditional
schemes for supervised topic modeling (Section 4).

1 Anchors: Speedy Unsupervised Models

The anchor algorithm (Arora et al., 2013) begins with
a V × V matrix Q̄ of word co-occurrences, where V
is the size of the vocabulary. Each word type defines
a vector Q̄i,· of length V so that Q̄i,j encodes the con-
ditional probability of seeing word j given that word
i has already been seen. Spectral methods (Anand-
kumar et al., 2012) and the anchor algorithm are
fast alternatives to traditional topic model inference
schemes because they can discover topics via these
summary statistics (quadratic in the number of types)
rather than examining the whole dataset (proportional
to the much larger number of tokens).

The anchor algorithm takes its name from the idea

746



of anchor words—words which unambiguously iden-
tify a particular topic. For instance, “wicket” might
be an anchor word for the cricket topic. Thus, for any
anchor word a, Q̄a,· will look like a topic distribu-
tion. Q̄wicket,· will have high probability for “bowl”,
“century”, “pitch”, and “bat”; these words are related
to cricket, but they cannot be anchor words because
they are also related to other topics.

Because these other non-anchor words could be
topically ambiguous, their co-occurrence must be ex-
plained through some combination of anchor words;
thus for non-anchor word i,

Q̄i,· =
∑
gk∈G

Ci,kQ̄gk,·, (1)

where G = {g1, g2, . . . , gK} is the set of K anchor
words. The coefficients Ci,k of this linear combina-
tion correspond to the probability of seeing a topic
given a word, from which we can recover the proba-
bility of a word given a topic (represented in a matrix
A) using Bayes’ rule. In our experiments, we follow
Arora et al. (2013) to first estimate Q̄ based on the
training data and then recover the C matrix

C∗i,· = argminCi,·DKL(Q̄i,· ||
∑
gk∈G

Ci,kQ̄gk,·),

where DKL(x, y) denotes the Kullback-Leibler di-
vergence between x and y.

In addition to discovering topics from a given set of
anchor words as described above, Arora et al. (2013)
also provide a geometric interpretation of a process
for finding the needed anchor words. If we view
the rows of Q̄ as points in a high-dimensional space,
the convex hull of those points provides the anchor
words.1

Equation 1 linearly combines anchor words’ co-
occurrence vectors Q̄gk,· to create the representation
of other words. The convex hull corresponds to the
perimeter of the space of all possible co-occurrence
vectors that can be formed from the set of basis an-
chor vectors. However, the convex hull only encodes

1As discussed by Arora et al. (2013), this is a slight simpli-
fication, since the most extreme points will be words that only
appear infrequently. Thus, there is some nuance to choosing
the anchor words. For instance, a key step for effective topic
modeling is choosing a minimum number of documents a word
must appear in before it can be considered an anchor word. (c.f.
Figure 3).

lemon

Toyota

wonderful

iPad

fleece author

heel

author

antilock

cozy

fleece

heel

awful

Toyota

iPad

Figure 1: Graphical intuition behind supervised anchor
words. Anchor words (in gold) form the convex hull of
word co-occurrence probabilities in unsupervised topic
modeling (top). Adding an additional dimension to cap-
ture metadata, such as sentiment, changes the convex hull:
positive words appear above the original 2D plane (under-
lined) and negative words appear below (in outline).

an unsupervised view of the data. To capture topics
informed by metadata such as sentiment, we need
to explicitly represent the combination of words and
metadata.

One problem inherited by the anchor method from
parametric topic models is the determination of the
number of anchor words (and thus topics) to use.
Because word co-occurrence statistics live in an ex-
tremely high-dimensional space, the number of an-
chor words needed to cover all of the data will be
quite high. Thus, Arora et al. (2013) require a user
to specify the number of anchor words a priori (just
as for parametric topic models). They use a form of
the Gram-Schmidt process to find the best words that
enclose the maximum volume of points.

747



Q̄ ⌘

New column(s) encoding
word-sentiment relationship

p(w1|w1) . . .
...
p(wj |wi)

p(w1|w1) . . .
...
p(wj |wi)

S ⌘
p(y(l)|w1)

...

p(y(l)|wi)

Figure 2: We form a new column to capture the relation-
ship between words and each sentiment level: per entry is
the conditional probability of observing a sentiment level
y(l) given an observation of the word wi. Adding all of
these columns to Q̄ to form an augmented matrix S.

2 Supervised Topics: Effective but Slow

Topic models discover a set of topics A. Each topic
is a distribution over the V word types in the cor-
pus. Ai,t is the probability of seeing word i in topic
t. Supervised topic models relate those topics with
predictions of document metadata such as sentiment
by discovering a vector of regression parameters ~µ
that connects topics to per-document observations
yd (Blei and McAuliffe, 2007). Blei and McAuliffe
(2007) treat this as a regression: seeing one word
with topic k in document d means that prediction of
yd should be adjusted by µk. Given a document’s dis-
tribution over topics z̄d, the response yd is normally
distributed with mean ~µ>z̄d.2

Typically, the topics are discovered through a
process of probabilistic inference, either variational
EM (Wang et al., 2009) or Gibbs sampling (Boyd-
Graber and Resnik, 2010). However, these meth-
ods scale poorly to large datasets. Variational infer-
ence requires dozens of expensive passes over the
entire dataset, and Gibbs sampling requires multiple
Markov chains (Nguyen et al., 2014b).

2We are eliding some details in the interest of a more compact
presentation. The topics used by a document, z̄d, are based
on per-token inference of topic assignments; this detail is not
relevant to our contribution, and in Section 4.2 we use existing
techniques to discover documents’ topics.

3 Supervised Anchor Words

Because the anchor algorithm scales so well com-
pared to traditional probabilistic inference, we now
unify the supervised topic models of Section 2 with
the anchor algorithm discussed in Section 1. We do
so by augmenting the matrix Q̄ with an additional
dimension for each metadata attribute, such as senti-
ment. We provide the geometric intuition in Figure 1.

Picture the anchor words projected down to two
dimensions (Lee and Mimno, 2014): each word is a
point, and the anchor words are the vertices of a poly-
gon encompassing every point. Every non-anchor
word can be approximated by a convex combination
of the anchor words (Figure 1, top).

Now add an additional dimension as a column to
Q̄ (Figure 2). This column encodes the metadata
specific to a word. For example, we have encoded
sentiment metadata in a new dimension (Figure 1,
bottom). Neutral sentiment words will stay in the
plane inhabited by the other words, positive senti-
ment words will move up, and negative sentiment
words will move down. For simplicity, we only show
a single additional dimension, but in general we can
add as many dimensions as needed to encode the
metadata.

In this new space some of the original anchor
words may still be anchor words (“author”). Other
words that were near the convex hull boundary in
the unaugmented representation may become an-
chor words in the augmented representation because
they capture both topic and sentiment (“anti-lock” vs.
“lemon”). Finally, extreme sentiment words might
become anchor words in the new higher-dimensional
space because they are so important for explaining
extreme sentiment values (“wonderful” vs. “awful”).

3.1 Words to Sentiment

Having explained how a word is connected to senti-
ment, we now elaborates on how to model that con-
nection using the conditional probability of senti-
ment given a particular word. Assume that sentiment
is discretized into a finite set of L sentiment levels
{y(1), y(2), . . . , y(L)} and that each document is as-
signed to one of these levels. We define a matrix
S of size V × (V + L). The first V columns are
the same as Q̄ and the L additional columns capture
the relationship of a word to each discrete sentiment

748



level.
For each additional column l, Si,(V +l) ≡ p(y =

y(l) |w = i) is the conditional probability of observ-
ing a sentiment level y(l) given an observation of
word i. We compute the conditional probability of a
sentiment level y(l) given word i

Si,(V +l) ≡
∑

d(1 [i ∈ d] · 1
[
yd = y(l)

]
)∑

d 1 [i ∈ d]
, (2)

where the numerator is the number of documents
that contain word type i and have sentiment level
y(l) and the denominator is the number of documents
containing word i.

Given this augmented matrix, we again want to
find the set of anchor words G and coefficients Ci,k
that best capture the relationship between words and
sentiment (c.f. Equation 1)

Si,· =
∑
gk∈G

Ci,kSgk,·. (3)

Because we retain the property that non-anchor
words are explained through a linear combination
of the anchor words, our method retains the same
theoretical guarantees of sampling complexity and
robustness as the original anchor algorithm.

To facilitate direct comparisons, we keep the num-
ber of anchor words fixed in our experiments. Even
so, the introduction of metadata forces the anchor
method to select the words that best capture this
metadata-augmented view of the data. Consequently,
some of the original anchor words will remain, and
some will be replaced by sentiment-specific anchor
words.

4 Quantitative Comparison of Supervised
Topic Models

In this section, we evaluate the effectiveness of our
new method on a binary sentiment classification
problem. Because the supervised anchor algorithm
(SUP ANCHOR) finds anchor words (and thus differ-
ent topics) which capture the sentiment metadata, we
evaluate the degree to which its latent representation
improves upon the original unsupervised anchor algo-
rithm (Arora et al., 2013, ANCHOR) for classification
in terms of both accuracy and speed.

4.1 Sentiment Datasets

We use three common sentiment datasets for eval-
uation: AMAZON product reviews (Jindal and Liu,
2008), YELP restaurant reviews (Jo and Oh, 2011),
and TRIPADVISOR hotel reviews (Wang et al., 2010).
For each dataset, we preprocess by tokenizing and
removing all non-alphanumeric words and stopwords.
As very short reviews are often inscrutable and lack
cues to connect to the sentiment, we only consider
documents with at least thirty words. We also re-
duce the vocabulary size by keeping only words that
appear in a sufficient number of documents: 50 for
AMAZON and YELP datasets, and 150 for TRIPADVI-
SOR (Table 1).

4.2 Documents to Labels

Our goal is to perform binary classification of sen-
timent. Due to a positive skew of the datasets, the
median for all datasets is four out of five. All 5-star
reviews are assigned to y+ and the rest of the reviews
are assigned to y−. Table 1 summarizes the composi-
tion of each dataset and the percentage of documents
with high positive sentiment.3

We compare the effectiveness of different repre-
sentations in predicting high-sentiment documents:
unsupervised topic models (LDA), traditional su-
pervised topic models (SLDA), the unmodified an-
chor algorithm (ANCHOR), our supervised anchor
algorithm (SUP ANCHOR), and a traditional TF-
IDF (Salton, 1968, TF-IDF) representation of the
words.

The anchor algorithm only provides the topic dis-
tribution over words; it does not provide the per-
document assignment of topics needed to represent
the document in a low-dimensional space necessary
for producing a prediction yd. Fortunately, this re-
quires a very quick—because the topics are fixed—
pass over the documents using a traditional topic
model inference algorithm. We use the variational in-
ference implementation for LDA of Blei et al. (2003)4

to obtain z̄d, the topic distribution for document d.5

3Multiclass labeling for each sentiment label also works well,
but binary classification simplifies the analysis and presentation.

4http://www.cs.princeton.edu/˜blei/lda-c/
5For other inference schemes, we use native inference to ap-

ply pre-trained topics to extract DEV and TEST topic proportions.

749



Corpus Train Documents Test Documents Tokens Types Percentage with Positive Sentiment
AMAZON 13,300 3,314 1,031,659 2,662 52.2%

TRIPADVISOR 115,384 28,828 12,752,444 4,867 41.5%
YELP 13,955 3,482 1,142,555 2,585 27.7%

Table 1: Statistics for the datasets employed in the experiments.

Classifiers Given a low-dimensional representa-
tion of a test document, we predict the document’s
sentiment yd. We have already inferred the topic dis-
tribution z̄d for each document, and we use log(z̄d)
as the features for a classifier. Feature vectors from
training data are used to train the classifiers, and fea-
ture vectors from the development or test set are used
to evaluate the classifiers.

We run three standard machine learning classi-
fiers: decision trees (Quinlan, 1986), logistic regres-
sion (Friedman et al., 1998), and a discriminative
classifier. For decision trees (hence TREE) and logis-
tic regression (hence LOGISTIC), we use SKLEARN.6

For the discriminative classifier, we use a linear clas-
sifier with hinge loss (hence HINGE) in Vowpal Wab-
bit.7 Because HINGE outputs a regression value in
[0, 1], we use a threshold 0.5 to make predictions.

Parameter Tuning Parameter tuning is important
in topic models, so we cross-validate: each sentiment
dataset is split randomly into five folds. We used
four folds to form the TRAIN set and reserved the
last fold for the TEST set. All cross-validation results
are averaged over the four held out DEV sets; the
best cross-validation result provides the parameter
settings we use on the TEST set.

For ANCHOR and SUP ANCHOR, the parame-
ter for the document-level Dirichlet prior α is re-
quired for inferring document-topic distributions
given learned topics. Despite selecting this parameter
using grid search, α does not affect our final results.
The same is also true for SLDA: its predictive perfor-
mance does not significantly vary as α varies, given
a fixed number of topics K.8

Anchor algorithms are sensitive to the value of an-
chor thresholdM (the minimum document frequency
for a word to be considered an anchor word). For

6http://scikit-learn.org/stable/
7http://hunch.net/˜vw/
8We use the SLDA implementation by Chong Wang: http:

//www.cs.cmu.edu/˜chongw/slda/ to estimate α.

AMAZON TRIPADVISOR YELP

●

● ●
● ● ●

●
● ● ● ●

●

●
● ● ● ● ●

●
● ● ● ● ●

● ● ●●●●
●● ●● ●

● ● ●
●●●

●● ●● ●

● ● ●
●●●

●● ●● ●

● ● ●●
●●

● ●● ●

●
● ● ● ● ●

● ●
● ●

● ●

● ● ●
● ● ●

●
● ● ● ● ●

0.65
0.70
0.75

0.70
0.72
0.74
0.76
0.78

0.72
0.74
0.76
0.78

0.71
0.73
0.75
0.77

20
40

60
80

100 400 100 400 2400 100 400
M

A
cc

ur
ac

y
Figure 3: Grid search for selecting the word-document
threshold M for SUP ANCHOR based on development set
accuracy.

each number of topics K, we perform a grid search
to find the best value of M . Figure 3 shows the
performance trends.

For LDA, we use the Gibbs sampling implemen-
tation in Mallet.9 For training the model, we run
LDA with 5,000 iterations; and for inference (on DEV
and TEST) of document topic distribution we iterate
100 times, with lag 5 and 50 burn-in iterations. As
Mallet accepts

∑
αi as a parameter, we always ini-

tialize
∑
αi = 1 and only perform a grid search over

different values of β, the hyper-parameter for Dirich-
let prior over the per-topic topic-word distribution,
starting from 0.01 and doubling until reaching 0.5.

4.3 SUP ANCHOR Outperforms ANCHOR

Learning topics that jointly reflect words and meta-
data improves subsequent prediction. The results for
both SUP ANCHOR and ANCHOR on the TEST set
are shown in Figure 4. SUP ANCHOR outperforms
ANCHOR on all datasets. This trend holds consis-
tently for LOGISTIC, TREE, and HINGE methods for
sentiment prediction. For example, with twenty top-
ics on the AMAZON dataset, SUP ANCHOR gives an

9http://mallet.cs.umass.edu/topics.php

750



●

●

●
●

●

●
● ●

●

● ●
●

0.65

0.70

0.74
0.75
0.76
0.77
0.78
0.79

0.74
0.75
0.76
0.77
0.78

A
M

A
Z

O
N

T
R

IPA
D

V
IS

O
R

Y
E

LP

20 40 60 80
Number Of Topics

A
cc

ur
ac

y
method ● ANCHOR LDA SLDA SUP. ANCHOR

Figure 4: Results on TEST fold, SUP ANCHOR outper-
forms ANCHOR, LDA, and SLDA on all three datasets. We
report the results based on LOGISTIC as it produces the
best accuracy consistently for ANCHOR, SUP ANCHOR,
and LDA.

accuracy of 0.71 in comparison to only 0.62 from
ANCHOR. Similarly, with twenty topics on the YELP
dataset, SUP ANCHOR has 0.77 accuracy while AN-
CHOR has 0.74. Our SUP ANCHOR model is able to
incorporate metadata to learn better representations
for predicting sentiment. Moreover, in Section 5 we
show that SUP ANCHOR does not need to sacrifice
topic quality to gain predictive power.

4.4 SUP ANCHOR Outperforms SLDA

More surprising is that SUP ANCHOR also outper-
forms SLDA. Like SUP ANCHOR, SLDA jointly
learns topics and their relation to metadata such as
sentiment. Figure 4 shows that this trend is consistent
on all sentiment datasets. On average, SUP ANCHOR
is 2.2 percent better than SLDA on AMAZON, and 2.0
percent better on both YELP and TRIPADVISOR. Fur-
thermore, SUP ANCHOR is much faster than SLDA.

SLDA performs worse than SUP ANCHOR in part
because SUP ANCHOR is able to jointly find specific
lexical terms that improve prediction. Nguyen et al.
(2013) show that this improves supervised topic mod-
els; forming anchor words around the same strong
lexical cues could discover better topics. In con-
trast, SLDA must discover the relationship through

● ● ● ● ● ● ● ● ● ●

●

0.60

0.65

0.70

0.75 A
M

A
Z

O
N

0 25 50 75 100
Interpolation Percentage

A
cc

ur
ac

y

classifier ● HINGE LOGISTIC TREE

Figure 5: Accuracy on AMAZON with twenty topics.
SUP ANCHOR produces good representations for senti-
ment classification that can be improved by interpolating
with lexical TF-IDF features. The interpolation (x-axis)
ranges from zero (all TF-IDF features) to one hundred (all
SUP ANCHOR topic features).

the proxy of topics.

4.5 Lexical Features

Ramage et al. (2010) show that interpolating topic
and lexical features often provides better classifica-
tion than either alone. Here, we take the same ap-
proach and show how different interpolations of topic
and lexical features create better classifiers. We first
select an interpolation value λ in {0, 0.1, 0.2, . . . , 1},
and we then form a new feature vector by concatenat-
ing λ-weighted topic features with (1− λ)-weighted
lexical features. Figure 5 shows the interplay be-
tween topic features and TF-IDF features10 as the
weight of topic features increases from zero (all TF-
IDF) to one hundred (all SUP ANCHOR topic fea-
tures) percent on the AMAZON dataset (other datasets
are similar). Combining both feature sets is better
than either alone, although the interpolation depends
on the classifier.

4.6 Runtime Analysis

Having shown that SUP ANCHOR outperforms both
ANCHOR and SLDA, in this section we show that
SUP ANCHOR also inherits the runtime efficiency
from ANCHOR. Table 2 summarizes the runtimes
on both AMAZON and TRIPADVISOR; these results
were obtained using a six-core 2.8GHz Intel Xeon
X5660. On the small dataset AMAZON, SUP AN-
CHOR finishes the training within one minute, and
for the larger TRIPADVISOR dataset it completes the

10As before, we do parameter selection on DEV data and report
final TEST results.

751



Dataset Measure SUP ANCHOR LDA SLDA

AMAZON

Preprocessing 32 32 32
Generating Q̄/S 29
Training 33 886 4,762
LDAC inference 38 (train), 13 (dev/test)
Classification <5 <5

TRIPADVISOR

Preprocessing 305 305 305
Generating Q̄/S 262
Training 181 8,158 71,967
LDAC inference 830 (train), 280 (dev/test)
Classification <5 <5

Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets. Blank cells indicate a timing
which does not apply to a particular model. SUP ANCHOR is significantly faster than conventional methods.

● ●
● ●

●
●

●
●

● ● ● ●

0.045

0.050

0.055

0.05

0.06

0.09

0.11

0.13

A
M

A
Z

O
N

T
R

IPA
D

V
IS

O
R

Y
E

LP

20 40 60 80
Number Of Topics

To
pi

c 
In

te
rp

re
ta

bi
lit

y

method ● ANCHOR LDA SLDA SUP. ANCHOR

Figure 6: SUP ANCHOR and ANCHOR produce the same
topic quality. LDA outperforms all other models and pro-
duces the best topics. Performance of SLDA degrades
significantly as the number of topic increases.

learning in around three minutes. The main bottle-
neck for SUP ANCHOR is learning the document dis-
tributions over topics, although even this stage is fast
for known topic distributions. This result is far better
than the twenty hours required by SLDA to train on
TRIPADVISOR.

5 Inspecting Anchors and their Topics

One important evaluation for topic models is how
easy it is for a human reader to understand the top-
ics. In this section, we evaluate topics produced by

each model using topic interpretability (Chang et al.,
2009). Topic interpretability measures how human
users understand topics presented by a topic model-
ing algorithm. We use an automated approximation
of interpretability that uses a reference corpus as a
proxy for which words belong together (Newman
et al., 2010). Using half a million documents from
Wikipedia, we compute the induced normalized pair-
wise mutual information (Lau et al., 2014, NPMI)
on the top ten words in topics as a proxy for inter-
pretability.

Figure 6 shows the NPMI scores for each model.
Unsurprisingly, unsupervised models (LDA) produce
the best topic quality. In contrast, supervised mod-
els must balance metadata (i.e., response variable)
prediction against capturing word meaning. Conse-
quently, SLDA does slightly worse with respect to
topic interpretability.

SUP ANCHOR and ANCHOR produce the same
topic quality consistently on all datasets. Since
SUP ANCHOR and ANCHOR have nearly identical
runtime, SUP ANCHOR is better suited for supervised
tasks because it improves classification without sac-
rificing interpretability. It is possible that regulariza-
tion would improve the interpretability of these top-
ics; Nguyen et al. (2014a) show that adding regular-
ization removes overly frequent words from anchor-
discovered topics.

The topics produced by the ANCHOR and SUP AN-
CHOR algorithms have many similarities. In Table 3,
nearly all of the anchor words discovered by AN-
CHOR are also used by SUP ANCHOR. These anchor
words tend to describe general food types, such as

752



Model Anchor Words and Top Words in Topics
ANCHOR and SUP ANCHOR pizza burger sushi ice garlic hot amp chicken pork french sand-

wich coffee cake steak beer fish
wine wine restaurant dinner menu nice night bar table meal experience

ANCHOR hour wait hour people minutes line long table waiting worth order
late night late ive people pretty love youre friends restaurant open

favorite love favorite ive amazing delicious restaurant eat menu fresh awesome
SUP ANCHOR decent pretty didnt restaurant ordered decent wasnt nice night bad stars

line line wait people long tacos worth order waiting minutes taco

Table 3: Comparing topics generated for the YELP dataset: anchor words shared by both ANCHOR and SUP ANCHOR
are listed. Unique anchor words for each algorithm are listed along with the top ten words for that topic. For clarity,
we pruned words which appear in more than 3000 documents as these words appear in every topic. The distinct
anchor words reflect positive (“favorite”) and negative (“line”) sentiment rather than less sentiment-specific qualities of
restaurants (e.g., restaurants open “late”).

“pizza” or “burger”, and characterize the YELP dataset
well. The similarity of these shared topics explains
why both ANCHOR and SUP ANCHOR achieve simi-
lar topic interpretability scores.

To explain the predictive power of SUP ANCHOR
we must examine the anchor words and topics unique
to both algorithms. The anchor words which are
unique to ANCHOR include a general topic about
wine, and two somewhat coherent topics related to
time. By adding supervision to the model we get
three new anchor words which identify sentiment
ranging from extremely positive reviews mentioning
a favorite restaurant to extremely negative reviews
complaining about long waits.

This general trend is seen across each of the
datasets. For example, ANCHOR and SUP ANCHOR
both discover shared topics describing consumer
goods, but SUP ANCHOR replaces two topics dis-
cussing headphones with topics describing “frustrat-
ing” products and “great” products. Similarly, in
the TRIPADVISOR data, both ANCHOR and SUP AN-
CHOR share topics about specific destinations, but
only SUP ANCHOR discovers a topic describing “dis-
gusting” hotel rooms.

6 Related Work

Improving the scalability of statistical learning has
taken many forms: creating online approximations
of large batch algorithms (Hoffman et al., 2013; Zhai
et al., 2014) or improving the efficiency of sam-
pling (Yao et al., 2009; Hu and Boyd-Graber, 2012;
Li et al., 2014).

These insights have also improved supervised
topic models. For example, Zhu et al. (2013) train the
max-margin supervised topic models MEDLDA (Zhu
et al., 2009) by reformulating the model such that the
hinge loss is included inside a collapsed Gibbs sam-
pler, rather than being applied externally on the sam-
pler using costly SVMs. Using insights from Smola
and Narayanamurthy (2010), the samplers run in par-
allel to train the model. While these advancements
improve the scalability of max-margin supervised
topic models, the improvement is limited by the fact
that the sampling algorithm grows with the number
of tokens.

In contrast, this paper explores a different vein
of research that focuses on using efficient represen-
tations of summary statistics to estimate statistical
models. While this has seen great success in unsu-
pervised models (Cohen and Collins, 2014), it has
increasingly also been applied to supervised mod-
els. Wang and Zhu (2014) show how to use tensor
decomposition to estimate the parameters of SLDA
instead of sampling to find maximum likelihood es-
timates. In contrast, anchor-based methods rely on
non-negative matrix factorization.

We found that a discriminative classifier did not
always perform best on the downstream classification
task. Zhu et al. (2009) make a comprehensive com-
parison between MEDLDA, SLDA, and SVM+LDA,
and they show that SVM+LDA performs worse than
MEDLDA and SLDA on binary classification. It could
be that better feature preprocessing could improve
our performance.

753



Bag-of-words representations are not ideal for sen-
timent tasks. Rubin et al. (2012) introduce Depen-
dency LDA which associates individual word tokens
with different labels; their model also outperforms
linear SVMs on a very large multi-labeled corpus.
Latent variable models that consider grammatical
structure (Sayeed et al., 2012; Socher et al., 2011;
Iyyer et al., 2014) could also be improved through
efficient inference (Cohen and Collins, 2014).

7 Discussion

Supervised anchor word topic modeling provides
a general framework for learning better topic rep-
resentations by taking advantage of both word-
cooccurrence and metadata. Our straightforward ex-
tension (Equation 2) places each word in a vector
space that not only captures co-occurrence with other
terms but also the interaction of the word and its sen-
timent, in contrast to algorithms that only consider
raw words.

While our experiments focus on binary classifica-
tion, the same extension is also applicable to multi-
class classification.

Moreover, supervised anchor word topic model-
ing is fast: it inherits the polynomial-time efficiency
from the original unsupervised anchor word algo-
rithm. It is also effective: it is better at providing
features for classification than unsupervised topic
models and also better than supervised topic models
with conventional inference.

Our supervised anchor word algorithm offers the
ability to quickly analyze datasets without the over-
head of Gibbs sampling or variational inference, al-
lowing users to more quickly understand big data
and to make decisions. Combining bag-of-words
analysis with metadata through efficient, low-latency
topic analysis allows users to have deep insights more
quickly.

Acknowledgments We thank Daniel Petersen, Jim
Martin, and the anonymous reviewers for their in-
sightful comments. This work was supported by the
collaborative NSF Grant IIS-1409287 (UMD) and IIS-
1409739 (BYU). Boyd-Graber is also supported by
NSF grants IIS-1320538 and NCSE-1422492.

References
Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham

Kakade, and Yi-Kai Liu. 2012. A spectral algorithm
for latent Dirichlet allocation. In Proceedings of Ad-
vances in Neural Information Processing Systems.

Sanjeev Arora, Rong Ge, Yoni Halpern, David M. Mimno,
Ankur Moitra, David Sontag, Yichen Wu, and Michael
Zhu. 2013. A practical algorithm for topic model-
ing with provable guarantees. In Proceedings of the
International Conference of Machine Learning.

David M. Blei and Jon D. McAuliffe. 2007. Supervised
topic models. In Proceedings of Advances in Neural
Information Processing Systems.

David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine Learn-
ing Research, 3.

Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
sentiment analysis across languages: Multilingual su-
pervised latent Dirichlet allocation. In Proceedings of
Empirical Methods in Natural Language Processing.

Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean
Gerrish, and David M. Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Proceedings of
Advances in Neural Information Processing Systems.

Shay B. Cohen and Michael Collins. 2014. A provably
correct learning algorithm for latent-variable PCFGs.
In Proceedings of the Association for Computational
Linguistics.

Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
1998. Additive logistic regression: a statistical view of
boosting. Annals of Statistics, 28:2000.

Matthew Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. In
Journal of Machine Learning Research.

Yuening Hu and Jordan Boyd-Graber. 2012. Efficient
tree-based topic modeling. In Proceedings of the Asso-
ciation for Computational Linguistics.

Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip
Resnik. 2014. Political ideology detection using recur-
sive neural networks. In Proceedings of the Association
for Computational Linguistics.

Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of First ACM International
Conference on Web Search and Data Mining.

Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis. In
Proceedings of ACM International Conference on Web
Search and Data Mining.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically eval-
uating topic coherence and topic model quality. In
Proceedings of the European Chapter of the Associa-
tion for Computational Linguistics.

754



Moontae Lee and David Mimno. 2014. Low-dimensional
embeddings for interpretable anchor-based topic infer-
ence. In Proceedings of Empirical Methods in Natural
Language Processing.

Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexander J
Smola. 2014. Reducing the sampling complexity
of topic models. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 891–900. ACM.

David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Conference of the North American Chapter of
the Association for Computational Linguistics.

Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2013. Lexical and hierarchical topic regression. In Pro-
ceedings of Advances in Neural Information Processing
Systems.

Thang Nguyen, Yuening Hu, and Jordan Boyd-Graber.
2014a. Anchors regularized: Adding robustness and
extensibility to scalable topic-modeling algorithms. In
Proceedings of the Association for Computational Lin-
guistics.

Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.
2014b. Sometimes average is best: The importance
of averaging for prediction using MCMC inference in
topic modeling. In Proceedings of Empirical Methods
in Natural Language Processing.

Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In Association for the Advancement of
Artificial Intelligence.

J. R. Quinlan. 1986. Induction of decision trees. 1(1):81–
106, March.

Daniel Ramage, Susan T. Dumais, and Daniel J. Liebling.
2010. Characterizing microblogs with topic models.
In International Conference on Weblogs and Social
Media.

Timothy N. Rubin, America Chambers, Padhraic Smyth,
and Mark Steyvers. 2012. Statistical topic models for
multi-label document classification. Journal of Ma-
chine Learning Research, 88(1-2):157–208, July.

Gerard Salton. 1968. Automatic Information Organiza-
tion and Retrieval. McGraw Hill Text.

Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and
Amy Weinberg. 2012. Grammatical structures for
word-level sentiment detection. In North American
Association of Computational Linguistics.

Alexander Smola and Shravan Narayanamurthy. 2010.
An architecture for parallel topic models. International
Conference on Very Large Databases, 3(1-2):703–710.

Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011. Semi-
supervised recursive autoencoders for predicting senti-

ment distributions. In Proceedings of Empirical Meth-
ods in Natural Language Processing.

Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summarization.
In Proceedings of the Association for Computational
Linguistics.

Yining Wang and Jun Zhu. 2014. Spectral methods for
supervised topic models. In Proceedings of Advances
in Neural Information Processing Systems.

Chong Wang, David Blei, and Li Fei-Fei. 2009. Simulta-
neous image classification and annotation. In Computer
Vision and Pattern Recognition.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In Knowledge Discovery
and Data Mining.

Limin Yao, David Mimno, and Andrew McCallum. 2009.
Efficient methods for topic model inference on stream-
ing document collections. In Knowledge Discovery and
Data Mining.

Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mohamad
Alkhouja. 2012. Mr. LDA: A flexible large scale topic
modeling package using variational inference in mapre-
duce. In Proceedings of World Wide Web Conference.

Ke Zhai, Jordan Boyd-Graber, and Shay B. Cohen. 2014.
Online adaptor grammars with hybrid inference.

Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009. MedLDA:
maximum margin supervised topic models for regres-
sion and classification. In Proceedings of the Interna-
tional Conference of Machine Learning.

Jun Zhu, Xun Zheng, Li Zhou, and Bo Zhang. 2013.
Scalable inference in max-margin topic models. In
Knowledge Discovery and Data Mining.

755


