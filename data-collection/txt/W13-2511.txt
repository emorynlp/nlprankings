










































Building Ontologies from Collaborative Knowledge Bases to Search and Interpret Multilingual Corpora


Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 87–94,
Sofia, Bulgaria, August 8, 2013. c©2013 Association for Computational Linguistics

Building Ontologies from Collaborative Knowledge Bases to Search 
and Interpret Multilingual Corpora 

Yegin Genc Elizabeth A. Lennon Winter Mason Jeffrey V. Nickerson 
Stevens Institute of Technology 

Center for Decision Technologies 
Castle Point on Hudson, Hoboken, NJ USA 

{ygenc, elennon, wmason, jnickerson}@stevens.edu 
 
 

Abstract 

Tools and techniques that automate the inter-
pretation of multilingual corpora are useful 
on many fronts; scholars, as an example, 
could use such tools to more readily pinpoint 
relevant articles from journals in a wide vari-
ety of languages. This work describes tech-
niques to build and characterize ontologies 
using collaborative knowledge bases, e.g., 
Wikipedia. These ontologies can then be used 
to search and classify texts. Originally devel-
oped for monolingual corpora, we extend the 
approach to multilingual texts and test the 
methods with Mandarin scientific abstracts. 
The presented techniques provide a novel and 
efficient mechanism to obtain contextually 
rich ontologies and measure document rele-
vancy within multilingual corpora. 

1 Introduction 
The wealth of data available online in the form 
of unstructured text drives the development of 
tools that automatically extract meaning from 
cross-lingual corpora. Techniques that quantify 
the degree to which texts exhibit similar meaning 
improve a variety of search processes – for ex-
ample, academic research. However, automating 
the interpretation of multilingual corpora re-
quires detecting similarities in meaning, while 
ignoring irrelevant linguistic differences. For 
example, the understanding that emerges from 
the connections and associations among words, 
i.e. context, can manifest very differently in dif-
ferent languages (Goddard, 2011). Furthermore, 
the meanings of words used in natural language 
are often context dependent, and context itself 
both shapes and reveals meaning (Gennaro et al., 
2007).  

For the purposes of this work, an ontology is 
defined as a model that represents word entities 
as concepts and their interrelationships (Lanzen-
berger et al., 2010). In this sense, ontologies rep-

resent the relevant aspects of context. To effec-
tively comprehend cross-lingual corpora, tools 
that can explore the dependencies between lan-
guage and context are needed.  

One way to do this is to make use of well-
understood existing texts that have explicitly 
linked concept graphs. Examples of such texts 
are collaborative knowledge stores, databases 
built up through the contributions of many indi-
viduals.  

The techniques described here use Wikipedia 
to build ontologies from journal article abstracts 
in different languages, which we test on text 
written in Mandarin. In order to compare alterna-
tive ways of deriving ontologies, a set of articles 
that have both Mandarin and English abstracts 
are used as the test corpus.   

The rest of the paper is organized into four 
sections. The background section briefly summa-
rizes prior research relevant to this work. Next, 
the methods section details the processing steps 
used to create and visualize the ontologies for 
three experimental conditions. Sample ontology 
visualizations for each of the experimental condi-
tions are shown. A discussion comparing some 
of the emergent features in each of the three gen-
erated ontologies follows. Finally, we outline 
next steps for the extension of these techniques.  

2 Background 
Translation is used to convey the meaning repre-
sented in one language in another language. Au-
tomated text translation was a goal of early com-
puting (Locke and Booth, 1955), and is still chal-
lenging today. Approaches taken include diction-
ary look-ups, cognate matching, and parallel cor-
pora based methods (Kishida, 2005). Cognate 
matching uses untranslatable terms such as prop-
er nouns or technical terminology as the bases of 
cross-lingual connections. For example, Freitas-
Juniar et al. (2006) leveraged medical terms, 
commonly used across languages, to classify 
medical documents from multiple languages. 

87



Landauer and Littman (1991) used parallel cor-
pora based methods when they created a lan-
guage independent indexing space via Singular 
Value Decomposition to generate a comparable 
corpus. This permitted texts to be represented in 
a language-independent space, solely using the 
terms of the presentation language.  

One early machine translation system, DIO-
NYSUS, used three static knowledge sources: a 
lexicon, an ontological domain model, and a 
text-meaning-representation language in an effort 
to automate translation. The DIONYSUS re-
searchers noted the challenge of developing an 
ontology based on a detailed version of a “con-
structed reality” (Onyshkevych and Nirenburg 
1992). In other words, an ontological model of 
concepts representing a worldview is only as 
good as its ability to capture the breadth and 
depth of the world it attempts to model. Creating 
ontologies for machine translation applications 
arguably require knowledge stores as rich, ex-
pansive, and comprehensive as human language 
itself (Hovy, 2005).       

One challenge related to reliable ontology 
creation is the relevance of the produced ontolo-
gy in the future (Hovy, 2005). That is, word 
meanings morph over time, and so the ontology 
needs to shift also. Moreover, shifts in word 
meanings happen differently in different lan-
guages. Nichols et al. (2006) explored multilin-
gual ontology acquisition using robust minimal 
recursion semantics and machine-readable dic-
tionaries. Though they demonstrated a language-
agnostic tool for automated ontology generation, 
it was still limited to the static database of words 
contained in the dictionaries.  

Attempting to overcome the limitations of 
dictionaries, Gabrilovich and Markovitch (2009) 
turned to Wikipedia to perform what they called 
explicit semantic analysis (ESA). They drew up-
on both the reference and contextual knowledge 
embedded throughout Wikipedia with the goal of 
outperforming statistical methods, like latent se-
mantic analysis (LSA), in computing semantic 
relatedness of texts (Gabrilovich and Mar-
kovitch, 2009). However, in explicit semantic 
analysis, the semantic interpreter, which consists 
of weighted lists of concepts, i.e. Wikipedia arti-
cles, is built directly from Wikipedia’s text, a 
time-consuming process. Sorg and Cimiano (20 -

12) developed an approach leveraging explicit 
semantic analysis for cross-lingual information 
retrieval using Wikipedia.   

Building on the premise that collaborative 
knowledge stores, like Wikipedia, are superior 
for semantic-analysis related tasks, other re-
searchers have mapped extracted word entities 
from Twitter tweets directly to the titles of Wik-
ipedia pages. The reported technique outper-
formed statistically-based, semantic categoriza-
tion methods, specifically LSA and string-edit-
distance (Genc et al. 2011). In addition, the ap-
proach could categorize concepts in short text 
strings, a widely known challenge in semantics 
(Michelson and Macskassy, 2010). In addition, 
using the Wikipedia title pages instead of the 
actual article content enabled a faster semantic 
transform (Genc et al. 2012). Mapping extracted 
entities to online collaborative knowledge bases, 
like Wikipedia, also presents a path to accessing 
an ever-relevant contextual framework based 
upon the most current human knowledge base 
(Michelson and Macskassy 2010).  

3 Methods 
This study compares simplified Chinese Wikipe-
dia and English Wikipedia in their resourceful-
ness to build ontologies. For the comparison, we 
used a sample abstract that is available in both 
Mandarin and English (Figure 1). We construct-
ed ontologies from our sample using both Chi-
nese and English Wikipedia according to the ex-
perimental conditions detailed in section 3.2.  

3.1 Text Segmentation and Entity Extrac-
tion 

To extract entities, atomic, meaningful elements 
of text, we first segmented the texts into phrases 
– single words, bi-grams, and tri-grams – that 
overlap in a sliding window fashion. To give an 
example: the first few words of the English ab-
stract, ‘In recent years, there have’, yielded: {'in', 
'in recent', 'in recent years', 'recent', 'recent years', 
'recent years there', ’years’}. In Mandarin, word 
boundaries are not explicit. Thus, we segmented 
the Chinese version of the abstract into words 
first with the tools from (Youli, 2011), and then 
proceeded to phrase segmentation.  

 

88



 
     Figure 1: Sample journal abstract in Mandarin and English (from Su et al. 2006)

The words and word phrases resulting from 
text segmentation are potential entities. We then 
check which of these phrases match a title in 
Wikipedia. These titles are either a page name in 
Wikipedia domain or a redirection page to an 
entity with an alternate title. Redirections happen 
for alternative names, plurals, closely related 
words, adjectives/adverbs pointing to the corre-
sponding noun, less or more specific forms of 
names, abbreviations, alternative spellings, or 
punctuation and likely misspellings. The poten-
tial entities that have matches to Wikipedia titles 
are then considered existing entities, and are used 
in the ontology generation. 

3.2 Ontology Generation 
Wikipedia offers a network of networks: each 
language domain provides concepts and their 
relationships. These language-specific networks 
connect through the language links given on a 
Wikipedia page for a particular concept, and 
point users to pages with the same conceptual 
meaning in the alternate, target language. It is 
important to note that language links in Wikipe-
dia do not direct the reader to the translation of 
the original content but to another Wikipedia 
page created for the same concept in the desig-
nated language. To give an example, machine 
learning page in English is linked to 机器学习 
(Machine learning) in the Chinese Wikipedia, 

but the contents of these two pages are different; 
the two pages are created and updated by differ-
ent users at different times. 

We build the ontology of a document using 
the entities extracted from the text (see 3.1) and 
the Wikipedia categories of those entities. More 
specifically, we captured the immediate first lev-
el categories of the entities with existing Wik-
ipedia title pages via Wikipedia's API. During 
the process, hidden categories were excluded 
since they are used for administrative purposes. 
Ontologies were constructed according to the 
following experimental conditions. For experi-
ment A, Mandarin entities were extracted from 
the Mandarin version of the abstract, and the 
Chinese Wikipedia (http://zh.wikipedia.org/wiki/ 
Wikipedia:首页) was used to build an ontology. 
For experiment B, Mandarin entities were ex-
tracted from the Mandarin version of the ab-
stract. Next, we identified the corresponding 
English Wikipedia pages for the Mandarin enti-
ties and used the English entities to build the on-
tology from English Wikipedia. Entities without 
a corresponding English page were ignored. For 
experiment C, English entities were extracted 
from the English version of the abstract, and 
English Wikipedia (http://en.wikipedia.org/wiki/ 
Main_Page) was used to build the ontology. The-
se experimental conditions are summarized in 
Table 1. 

 

89



 

Experiment Language of Entities 
Wikipedia 
Language 

A Mandarin Mandarin 
B Mandarin English 
C English English 

Table 1: Summary of Experimental Conditions 

3.3 Ontology Visualization 
For the visualizations, the python library, pypro-
cessing, was used to apply Processing (www.pro-
cessing.org), a platform that allows for the crea-
tion of interactive visualizations. Orange circles 
show extracted entities that landed on Wik-
ipedia titles with existing pages in the re-
spective language. The first-level categories 
associated with those pages were visualized 

as blue circles. A line shows the link back to 
the corresponding entity represented as a 
Wikipedia title. At this time a spring weight-
ing function is used to automate the position-
ing of the items in the bipartite graphs con-
stituting the ontology visualizations. 

4 Results and Discussion 
Figures 2 (below), 3, and 4 (following pages) 
display the ontologies resulting from experi-
mental conditions A, B, and C respectively. Fig-
ure 2 shows several key concepts from the jour-
nal abstract about machine learning in NLP have 
been effectively captured as entities using the 
collective knowledge base of Chinese Wikipedia. 
In Figure 2 the English entity names are given in  

 
Figure 2: Ontology generated using experimental condition A, in which Chinese Wikipedia is used to 
build an ontology from Mandarin entities. The English translation of the entities are given for refer-
ence. Note that all nodes display, but the current algorithm uses the edge of the canvas as x=0, so some 
of the entities may not display as complete circles. 

90



parentheses for reference. A native Mandarin 
speaker translated the Mandarin characters, 
which had been presented as a list of terms. Fig-
ure 3 contains many of the same concepts seen in 
Figure 2. Figure 4, created from the English ab-
stract and English Wikipedia, displays approxi-
mately fifty percent more entities (excluding dis-
ambiguation). The entities associated with the 
disambiguation category currently in figure 4 can 
be filtered out as needed.  

 
 
 
 
 

Table 2 summarizes the number of entities 
and maximum number of categories for each of 
the experiments. A total of eight entities were 
shared among all three experimental conditions.   
 

Experiment # of Entities 
Max # of 1st level 

Categories 
A 20 4 
B 16 10 
C 33 * 11 

Table 2: Summary of Ontology Metrics,             
(* Excludes entities connected to disambiguation 
categories) 
 
 
 

 
Figure 3: Ontology generated using experimental condition B, in which Chinese Wikipedia’s links to 
the English Wikipedia in order to build an ontology from Mandarin entities. 
 

91



 

 
Figure 4: Ontology generated using experimental condition C, in which English Wikipedia is used to 
build an ontology from English entities. 
 

These visualizations yield preliminary in-
sights into the manner in which varying lan-
guages represent concepts in Wikipedia. Com-
paring the ontologies in Figures 2-4 reveals dif-
ferent languages in Wikipedia exhibit different 
breadth. English Wikipedia provided more con-
cepts than the Chinese counterpart for this text 
sample. This is not surprising given the English 
Wikipedia is larger. However, the Mandarin enti-
ties shown in Figure 2 offer a satisfactory repre-
sentation of the text. In addition, the extra con-
cepts from English Wikipedia add little to the 
general understanding of the text, and may even 
distract from the abstract’s key concepts. 

Wikipedia pages from different languages 
generate different ontologies for seemingly simi-
lar concepts. For example, in Experiment A 

(Figure 2), algorithm, information retrieval, and 
complexity (which has the English label ‘compli-
cated’) are connected through the computer sci-
ence category. However, the corresponding Eng-
lish pages of these entities used in experiment B 
(Figure 3) are not connected through any shared 
first-level categories. This suggests English Wik-
ipedia pages are categorized in greater detail, 
making it difficult to capture relationships among 
concepts through the immediate, first-level cate-
gories. In other words, the detailed ontology of 
English Wikipedia may not be as effective a ref-
erence as the simple ontology in Chinese Wik-
ipedia. It could also be that the translation pro-
cess introduces noise.  Identifying and visualiz-
ing the second-level category connections might 

92



provide further insight into the differences be-
tween the two methods.  

5 Summary and Next Steps 
As a context-rich, collaborative knowledge base, 
Wikipedia is ideal for building ontologies. This 
study presented varying approaches to construct-
ing ontologies from simplified Chinese and Eng-
lish Wikipedias, as a first step in evaluating 
cross-lingual corpora. The methods employed in 
this study can be further adopted to extract on-
tologies across multiple languages provided the 
analogous collaborative knowledge stores exist 
in the target languages. The sample ontology 
visualizations generated in this work demonstrat-
ed there are multiple ways to pursue concept rep-
resentation using the Chinese and English ver-
sions of Wikipedia.  

Wikipedia offers networks of concepts in dif-
ferent languages. Networks of different lan-
guages in Wikipedia are mapped through lan-
guage links within pages, but this is rarely a one-
to-one mapping. Thus, we also need ways to 
align ontologies with different levels of explicit-
ness and formalization.  

Future research might build on the visualiza-
tion techniques discussed here in order to explore 
mechanisms for ontology alignment. For exam-
ple, the percentage of entity coexistence within a 
set of ontologies could be used as a metric for the 
alignment of ontologies. In addition, the tech-
niques described here could be used to assess 
semantic similarity using ontologies coming 
from different collaborative data stores in differ-
ent languages. 

Finally, there are two approaches to extracting 
ontologies from cross-lingual corpora: work can 
be translated first and then ontologies extracted, 
or ontologies can be extracted, and then the on-
tologies translated. With more experiments, it 
may be possible to determine which is the best 
order to use, taking into account the corpus, the 
languages involved, and the collaborative data 
stores available.  
 
Acknowledgments 

The authors acknowledge and thank Dorian Zac-
caria for helping with the ontology visualizations 
and Yue Han for help with verification of Chi-
nese character translations. 

References  
Freitas-Junior, H. R., Ribeiro-Neto, B., Vale, R. F., 

Laender, A. H. F., & Lima, L. R. S. (2006). Cate-

gorization-driven cross-language retrieval of med-
ical information. Journal of the American Society 
for Information Science and Technology, 57(4), 
501–510. doi:10.1002/asi.20320 

Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-
based semantic interpretation for natural language 
processing. Journal of Artificial Intelligence Re-
search, 34(2), 443-498. 

Genc, Y., Mason, W., & Nickerson, J. (2012). Seman-
tic transforms using collaborative knowledge ba-
ses. Paper presented at Workshop on Information 
in Networks, New York University, September 28-
29, 2012. Available at SSRN 2154367. 

Genc, Y., Sakamoto, Y., & Nickerson, J. V. (2011). 
Discovering context: Classifying tweets through a 
semantic transform based on Wikipedia, In Pro-
ceedings on Foundations of Augmented Cognition. 
Directing the Future of Adaptive Systems. Orlan-
do, FL. pp. 484-492. 

Gennari, S. P., MacDonald, M. C., Postle, B. R., & 
Seidenberg, M. S. (2007). Context-dependent in-
terpretation of words: Evidence for interactive 
neural processes. Neuroimage, 35(3), 1278-1286. 
doi:10.1016/j.neuroimage.2007.01.015 

Goddard, C. (2011). Semantic Analysis: A Practical 
Introduction (2nd Ed.). Oxford University Press, 
New York, NY.   

Hovy, E. (2005). Methodologies for the reliable con-
struction of ontological knowledge. In Proceed-
ings of the 13th international conference on Con-
ceptual Structures: Common Semantics for Shar-
ing Knowledge (ICCS '05). pp. 91–106. 
doi:10.1007/11524564_6 

Kishida, K. (2005). Technical issues of cross- 
language information retrieval: a review. Infor-
mation Processing & Management, 41(3), 433–
455.  

Landauer, T. K., & Littman, M. L. (1991). A statisti-
cal method for language-independent representa-
tion of the topical content of text segments. In 
Proceedings of the Eleventh International Confer-
ence: Expert Systems and Their Applications, 8, 
pp. 77-85. 

Lanzenberger, M., Sampson, J., & Rester, M. (2010). 
Ontology visualization: Tools and techniques for 
visual representation of semi-structured meta-data. 
Journal of Universal Computer Science, 16(7), 
1036-1054. 

Locke, W. N., & Booth, A. D. (Eds.). (1955). Ma-
chine translation of languages: fourteen essays. 
Published jointly by Technology Press of the Mas-
sachusetts Institute of Technology and Wiley, New 
York, NY.   

Michelson, M. & Macskassy, S. A. (2010). Discover-
ing users' topics of interest on Twitter: A first 

93



look. In Proceedings of the fourth workshop on 
Analytics for noisy unstructured text data (AND 
'10) Toronto, Canada. 
doi:10.1145/1871840.1871852 

Nichols, E., Bond, F., Tanaka, T., & Fujita, S. (2006). 
Multilingual ontology acquisition from multiple 
MRDS. In Proceedings of the 2nd Workshop on 
Ontology Learning and Population, Sydney, Aus-
tralia pp. 10–17. 

Onyshkevych, B. A., & Nirenburg, S. (1992). Lexi-
con, ontology, and text meaning, 289–303. 
doi:10.1007/3-540-55801-2_42 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Sorg, P., & Cimiano, P. (2012). Exploiting Wikipedia 
for cross-lingual and multilingual information re-
trieval. Data & Knowledge Engineering, 74 (2012) 
26–45. doi:10.1016/j.datak.2012.02.003  

Su J.S., Zhang B.F., & Xu X. (2006). Advances in 
machine learning based text categorization. Jour-
nal of Software, 2006,17(9), 1848-1859. doi: 
10.1360/jos171848 

Youli, D. (2011). Chinese Segmentation Analysis 
[Software]. Available from 
http://trac.xapian.org/wiki/GSoC2011/ChineseSeg
mentationAnalysis  

 
 
 
 

94


