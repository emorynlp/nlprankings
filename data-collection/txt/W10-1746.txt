










































JHU System Combination Scheme for WMT 2010


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 311–314,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

JHU System Combination Scheme for WMT 2010

Sushant Narsale
Johns Hopkins University

Baltimore, USA.
sushant@jhu.edu

Abstract

This paper describes the JHU system
combination scheme that was used in
the WMT 2010 submission. The in-
cremental alignment scheme of (Karakos
et.al, 2008) was used for confusion net-
work generation. The system order
in the alignment of each sentence was
learned using SVMs, following the work
of (Karakos et.al, 2010). Additionally,
web-scale n-grams from the Google cor-
pus were used to build language models
that improved the quality of the combi-
nation output. Experiments in Spanish-
English, French-English, German-English
and Czech-English language pairs were
conducted, and the results show approxi-
mately 1 BLEU point and 2 TER points
improvement over the best individual sys-
tem.

1 Introduction

System Combination refers to the method of com-
bining output of multiple MT systems, to pro-
duce a output better than each individual system.
Currently, there are several approaches to ma-
chine translation which can be classified as phrase-
based, hierarchical, syntax-based (Hildebrand and
Vogel, 2008) which are equally good in their trans-
lation quality even though the underlying frame-
works are completely different. The motivation
behind System Combination arises from this di-
versity in the state-of-art MT systems, which sug-
gests that systems with different paradigms make
different errors, and can be made better by com-
bining their strengths.

One approach of combining translations is
based on representing translations by confusion
network and then aligning these confusion net-
works using string alignment algorithms (Rosti

et.al, 2009), (Karakos and Khudanpur, 2008).
Another approach generates features for every
translation to train algorithms for ranking systems
based on their quality and the top ranking output
is considered to be a candidate translation, (Hilde-
brand and Vogel, 2008) is an example of ranking
based combination. We use ideas from ranking
based approaches to learn order in which systems
should be aligned in a confusion network based
approach.

Our approach is based on incremental align-
ment of confusion networks (Karakos et.al, 2008),
wherein each system output is represented by a
confusion network. The confusion networks are
then aligned in a pre-defined order to generate a
combination output. This paper contributes two
enhancements to (Karakos et.al, 2008). First,
use of Support Vector Machines to learn order in
which the system outputs should be aligned. Sec-
ond, we explore use of Google n-grams for build-
ing dynamic language model and interpolate the
resulting language model with a large static lan-
guage model for rescoring of system combination
outputs.

The rest of the paper is organized as follows:
Section 2 illustrates the idea and pipeline of the
baseline combination system; Section 3 gives de-
tails of SVM ranking for learning system order
for combination; Section 4 explains use of Google
n-gram based language models; Results are dis-
cussed in Section 5; Concluding remarks are given
in Section 6;

2 Baseline System Combination

This section summarizes the algorithm for base-
line combination. The baseline combination
pipeline includes three stages:

1. Representing translations by confusion net-
works.

311



2. Generating between system confusion net-
works.

3. Rescoring the final confusion network.

Confusion networks are compressed form of
lattices with a constraint that all paths should pass
through all nodes. Each system output is repre-
sented by an equivalent confusion network. The
per-system confusion networks are aligned one at
a time. The order in which systems are aligned
is usually decided by evaluation of system’s per-
formance. Two alternatives for deciding the sys-
tem order are discussed in Section 3. Inversion-
Transduction Grammar (Wu, 1997) is used for
alignments and the cost function for aligning two
confusion networks is

cost(b1, b2) =
1

|b1||b2|
∑

w∈b1

∑
v∈b2

c(v)c(w)1(w 6= v)

where b1 and b2 are two different bins, |b1| and |b2|
is the number of tokens in b1 and b2 respectively,
c(v) and c(w) are the number of words of token
v and token w. which are in b1 and b2 separately.
The idea of this cost is to compute the probability
that a word from bin b1 is not equal to a word from
bin b2.

cost(b1, b2) = Prob(v 6= w, v ∈ b1, w ∈ b2)

The final confusion network is rescored with a
5-gram language model with Kneser-Ney smooth-
ing. To generate the final output, we need to find
the best (minimum-cost) path through the rescored
confusion network. In the best path every bin in
the network contributes only one word to the out-
put.

Ordering the systems for incremental combina-
tion and use of different language models were the
two components of the pipeline that were experi-
mented with for WMT’2010 shared task. The fol-
lowing sections describe these variations in detail.

3 Learning to Order Systems for
Combination

Determining the order in which systems are
aligned is critical step in our system combination
process. The first few aligned translations/systems
determine the word ordering in the final output and
have a significant influence on the final transla-
tion quality. For the baseline combination the sys-
tems are aligned in the increasing order of (TER-
BLEU) scores. TER and BLEU (Papineni et.al,

2002) scores are calculated over all the sentences
in the training set. This approach to ordering of
systems is static and results in a global order for
all the source segments. An alternative approach
is to learn local order of systems for every source
sentence using a SVM ranker.

3.1 SVM Rank Method
This section describes an approach to order sys-
tems for alignment using SVMs (Karakos et.al,
2010). For each system output a number of fea-
tures are generated, the features fall broadly under
the following three categories:

N-gram Agreements
These features capture the percentage of hypoth-
esis for a source sentence that contain same n-
grams as the candidate translation under consid-
eration. The n-gram matching is position indepen-
dent because phrases often appear in different or-
ders in sentences with same meaning and correct
grammar. The scores for each n-gram are summed
and normalized by sentence length. N-grams of
length 1 · · · 5 are used as five features.

Length Feature
The ratio of length of the translation to the source
sentence is a good indication of quality of the
translation, for a lengthy source sentence a short
translation is most likely to be bad. Here, the ra-
tio of source sentence length to length of the target
sentence is calculated.

Language Model Features
Language models for target language are used to
calculate perplexity of a given translation. The
lower the perplexity the better is the translation
quality. We use two different language models:
(i) a large static 5-gram language model and (ii)a
dynamic language model generated from all the
translations of the same source segment. The
perplexity values are normalized by sentence
length.

Translations in training set are ranked based
on (TER-BLEU) scores. An SVM ranker is then
trained on this set. The SVM ranker (Joachims,
2002) returns a score for each translation, based
on its signed distance from the separating hyper-
plane. This value is used in the combination pro-
cess to weight the contribution of systems to the
final confusion network scores.

312



Table 1: Results for all Language pairs on development set
es-en fr-en cz-en de-en

Combination BLEU TER BLEU TER BLEU TER BLEU TER
BEST SYSTEM 29.27 52.38 26.74 56.88 21.56 58.24 26.53 56.87
BASELINE 28.57 51.61 27.65 55.20 21.01 58.79 26.80 54.54
SVM 28.68 51.99 27.53 55.35 21.56 58.24 26.85 54.9
SVM+NGRAM 29.92 50.92 27.86 55.06 21.80 57.78 27.24 54.86

4 Language Models

In the system combination process, the final con-
fusion networks are rescored with language mod-
els. Language models are widely used to en-
sure a fluent output translation. I explored use of
two language models. The first language model
was trained on the English side of French-English
corpus, UN corpus and English Gigaword cor-
pus made available by WMT. The second lan-
guage model used counts generated from Google
n-grams. It was trained by generating all 1-gram
to 5-grams in the system outputs for a source
segment and then using the N-gram search en-
gine (Lin et.al, 2010) built over Google n-grams
to get the corresponding n-gram counts. The n-
gram counts were used to train a 5-gram language
model with Kneser-Ney smoothing. SRILM
toolkit (Stockle, 2002) was used for training the
language models.

The baseline combinations were rescored only
with the static language model. I always did a
weighted interpolation of the two language mod-
els when using n-gram based language model.

5 Results

Results for four language pairs: Spanish-English,
French-English, Czech-English and German-
English are presented. The training data for
WMT’10 was divided into development and test
set, consisting of 208 and 247 segments respec-
tively. Table 1 shows TER and BLEU scores
on the TEST set for all the four language pairs
in the following settings: (i) Baseline corre-
sponds to procedure described in section 2, (ii)
SVM corresponds to using SVM ranker for learn-
ing order of systems as described in section 3.1
(iii)SVM+N-Grams corresponds to the use of a
SVM ranker along with weighted interpolation of
n-gram language model and the large static lan-
guage model. The ranking SVM was trained us-
ing SVM-light (Joachims, 2002) with a RBF ker-

nel. Two-fold cross-validation was done to pre-
vent over-fitting on development data. All the
scores are with lower-cased outputs, a tri-gram
language model was used to true-case the output
before the final submission. 1-best output from
only the primary systems were used for combina-
tion. The number of systems used for combination
in each language pair are: 6 for Czech-English,
8 in Spanish-English, 14 in French-English and
16 in German-English. The best results for base-
line combination were obtained with 3 systems
for Czech-English, 6 systems for German-English,
3 systems for Spanish-English and 9 systems for
French-English.

From the results, we conclude that for all lan-
guage pairs the combinations with SVM and n-
gram language models show gain over all the other
settings in both TER and BLEU evaluations. How-
ever, use of SVM with only one large language
model shows performance degradation on three
out of four language pairs. Size of training data
(208 segments) could be one reason for the degra-
dation and this issue needs further investigation.
For the final submission, the settings that per-
formed the best on (TER−BLEU)2 scale were cho-
sen.

6 Conclusion

The system combination task gave us an opportu-
nity to evaluate enhancements added to the JHU
system combination pipeline. Experimental re-
sults show that web-scale language models can be
used to improve translation quality, this further un-
derlines the usefulness of web-scale resources like
Google n-grams. Further investigation is needed
to completely understand the reasons for incon-
sistency in the magnitude of gain across different
language pairs. Specifically the impact of training
data on SVMs for ranking in system combination
scenario needs to be analysed.

313



Acknowledgments

This work was partially supported by the DARPA
GALE program Grant No HR0022-06-2-0001. I
would like to thank all the participants of WMT
2010 for their system outputs. I would also like
to thank Prof. Damianos Karakos for his guidance
and support. Many thanks go to the Center for
Language and Speech Processing at Johns Hop-
kins University for availability of their computer
clusters.

References
Almut Silja Hildebrand and Stephan Vogel. 2008.

Combination of Machine Translation Systems via
Hypothesis Selection from Combined N-Best Lists.
In MT at work: Proceedings of the Eight Conference
of Association of Machine Translation in the Amer-
icas, pages 254-261, Waikiki, Hawaii, October. As-
sociation for Machine Translations in the Americas.

Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU System Combination for WMT’09. Proceed-
ings of Fourth Workshop on Statistical Machine
Translation,Athen,Greece, March 2009.

Andreas Stockle. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings International Con-
ference for Spoken Language Processing, Denver,
Colarado, September.

Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing
Xiang and Spyros Matsoukas and Richard Schwartz
and Bonnie J. Dorr 2007. Combining Outputs from
Multiple Machine Translation Systems. In Proceed-
ings of the Third Workshop on Statistical Machine
Transaltion, pages 183-186, Colombus, Ohio, June.
Association for Computational Linguistics.

Damianos Karakos and Sanjeev Khudanpur 2008. Se-
quential System Combination for Machine Transla-
tion of Speech. In Proceedings of IEEE SLT-08, De-
cember 2008.

Damianos Karakos and Jason Smith and Sanjeev Khu-
danpur 2010. Hypothesis Ranking and Two-pass
Approaches for Machine Translation System Com-
bination. In Proceedings of ICASSP-2010, Dallas,
Texas, March 14-19 2010.

Damianos Karakos and Jason Eisner and Sanjeev Khu-
danpur and Markus Dreyer. 2008. Machine Trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 81-84, Colombus, Ohio, June. Associa-
tion for Computational Linguistics.

Dekang Lin and Kenneth Church and Heng Ji and
Satoshi Sekine and David Yarowsky and Shane
Bergsma and Kailash Patil and Emily Pitler Rachel
Lathbury and Vikram Rao and Kapil Dalwani and

Sushant Narsale 2010. New Tools for Web-Scale
N-grams. In the Proceedings of LREC, 2010.

D. Wu 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora.
Computational Linguistics, vol.23,no.3,pp.377-403,
September 1997.

Kishore Papineni and Salim Roukos and Todd Ward
and Wei-Jing Zhu. 2002. BLEU: A method for
automatic evaluation of machine translation. In
Proceedings of 40th Annual Meeting of Associa-
tion for Computational Linguistics, pages 311-318.
Philadelphia, PA, July.

Thorsten Joachims 2002. Optimizing Search Engines
using Clickthrough Data. In Proceedings of ACM
Conference on Knowledge Discovery and Data Min-
ing(KDD), 2002.

314


