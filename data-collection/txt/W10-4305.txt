










































Evaluation Metrics For End-to-End Coreference Resolution Systems


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 28–36,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

Evaluation Metrics For End-to-End Coreference Resolution Systems

Jie Cai and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH

Schloß-Wolfsbrunnenweg 35
69118 Heidelberg, Germany

(jie.cai|michael.strube)@h-its.org

Abstract

Commonly used coreference resolution
evaluation metrics can only be applied to
key mentions, i.e. already annotated men-
tions. We here propose two variants of the
B3 and CEAF coreference resolution eval-
uation algorithms which can be applied
to coreference resolution systems dealing
with system mentions, i.e. automatically
determined mentions. Our experiments
show that our variants lead to intuitive and
reliable results.

1 Introduction

The coreference resolution problem can be di-
vided into two steps: (1) determining mentions,
i.e., whether an expression is referential and
can take part in a coreferential relationship, and
(2) deciding whether mentions are coreferent or
not. Most recent research on coreference res-
olution simplifies the resolution task by provid-
ing the system with key mentions, i.e. already an-
notated mentions (Luo et al. (2004), Denis &
Baldridge (2007), Culotta et al. (2007), Haghighi
& Klein (2007), inter alia; see also the task de-
scription of the recent SemEval task on coref-
erence resolution at http://stel.ub.edu/
semeval2010-coref), or ignores an impor-
tant part of the problem by evaluating on key men-
tions only (Ponzetto & Strube, 2006; Bengtson &
Roth, 2008, inter alia). We follow here Stoyanov
et al. (2009, p.657) in arguing that such evalua-
tions are “an unrealistic surrogate for the original
problem” and ask researchers to evaluate end-to-
end coreference resolution systems.

However, the evaluation of end-to-end coref-
erence resolution systems has been inconsistent
making it impossible to compare the results. Nico-
lae & Nicolae (2006) evaluate using the MUC
score (Vilain et al., 1995) and the CEAF algorithm

(Luo, 2005) without modifications. Yang et al.
(2008) use only the MUC score. Bengtson & Roth
(2008) and Stoyanov et al. (2009) derive variants
from the B3 algorithm (Bagga & Baldwin, 1998).
Rahman & Ng (2009) propose their own variants
of B3 and CEAF. Unfortunately, some of the met-
rics’ descriptions are so concise that they leave too
much room for interpretation. Also, some of the
metrics proposed are too lenient or are more sen-
sitive to mention detection than to coreference res-
olution. Hence, though standard corpora are used,
the results are not comparable.

This paper attempts to fill that desideratum by
analysing several variants of the B3 and CEAF al-
gorithms. We propose two new variants, namely
B3sys and CEAFsys, and provide algorithmic de-
tails in Section 2. We describe two experiments in
Section 3 showing that B3sys and CEAFsys lead to
intuitive and reliable results. Implementations of
B3sys and CEAFsys are available open source along
with extended examples1.

2 Coreference Evaluation Metrics

We discuss the problems which arise when apply-
ing the most prevalent coreference resolution eval-
uation metrics to end-to-end systems and propose
our variants which overcome those problems. We
provide detailed analyses of illustrative examples.

2.1 MUC

The MUC score (Vilain et al., 1995) counts
the minimum number of links between mentions
to be inserted or deleted when mapping a sys-
tem response to a gold standard key set. Al-
though pairwise links capture the information
in a set, they cannot represent singleton en-
tities, i.e. entities, which are mentioned only
once. Therefore, the MUC score is not suitable
for the ACE data (http://www.itl.nist.

1http://www.h-its.org/nlp/download

28



gov/iad/mig/tests/ace/), which includes
singleton entities in the keys. Moreover, the MUC
score does not give credit for separating singleton
entities from other chains. This becomes problem-
atic in a realistic system setup, when mentions are
extracted automatically.

2.2 B3

The B3 algorithm (Bagga & Baldwin, 1998) over-
comes the shortcomings of the MUC score. In-
stead of looking at the links, B3 computes preci-
sion and recall for all mentions in the document,
which are then combined to produce the final pre-
cision and recall numbers for the entire output.

For each mention, the B3 algorithm computes a
precision and recall score using equations 1 and 2:

Precision(mi) =
|Rmi ∩ Kmi |

|Rmi |
(1)

Recall(mi) =
|Rmi ∩ Kmi |

|Kmi |
(2)

where Rmi is the response chain (i.e. the system
output) which includes the mention mi, and Kmi
is the key chain (manually annotated gold stan-
dard) with mi. The overall precision and recall are
computed by averaging them over all mentions.

Since B3’s calculations are based on mentions,
singletons are taken into account. However, a
problematic issue arises when system mentions
have to be dealt with: B3 assumes the mentions in
the key and in the response to be identical. Hence,
B3 has to be extended to deal with system men-
tions which are not in the key and key mentions
not extracted by the system, so called twinless
mentions (Stoyanov et al., 2009).

2.2.1 Existing B3 variants

A few variants of the B3 algorithm for dealing with
system mentions have been introduced recently.
Stoyanov et al. (2009) suggest two variants of the
B3 algorithm to deal with system mentions, B30 and
B3all

2. For example, a key and a response are pro-
vided as below:

Key : {a b c}
Response: {a b d}

B30 discards all twinless system mentions (i.e.
mention d) and penalizes recall by setting
recallmi = 0 for all twinless key mentions (i.e.
mention c). The B30 precision, recall and F-score

2Our discussion of B30 and B
3
all is based on the analysis

of the source code available at http://www.cs.utah.
edu/nlp/reconcile/.

Set 1

System 1
key {a b c}
response {a b d}

P R F
B30 1.0 0.444 0.615
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572

System 2
key {a b c}
response {a b d e}

P R F
B30 1.0 0.444 0.615
B3all 0.375 0.556 0.448
B3r&n 0.375 0.556 0.448
B3sys 0.5 0.556 0.527
CEAFsys 0.4 0.667 0.500

Table 1: Problems of B30

(i.e. F = 2 · Precision·Recall
Precision+Recall

) for the example are
calculated as:

PrB3
0

= 1
2
( 2
2

+ 2
2
) = 1.0

RecB3
0

= 1
3
( 2
3

+ 2
3

+ 0)
.
= 0.444

FB3
0

= 2 × 1.0×0.444
1.0+0.444

.
= 0.615

B3all retains twinless system mentions. It assigns
1/|Rmi | to a twinless system mention as its preci-
sion and similarly 1/|Kmi | to a twinless key men-
tion as its recall. For the same example above, the
B3all precision, recall and F-score are given by:

PrB3
all

= 1
3
( 2
3

+ 2
3

+ 1
3
)

.
= 0.556

RecB3
all

= 1
3
( 2
3

+ 2
3

+ 1
3
)

.
= 0.556

FB3
all

= 2 × 0.556×0.556
0.556+0.444

.
= 0.556

Tables 1, 2 and 3 illustrate the problems with B30
and B3all. The rows labeled System give the origi-
nal keys and system responses while the rows la-
beled B30, B

3
all and B

3
sys show the performance gen-

erated by Stoyanov et al.’s variants and the one
we introduce in this paper, B3sys (the row labeled
CEAFsys is discussed in Subsection 2.3).

In Table 1, there are two system outputs (i.e.
System 1 and System 2). Mentions d and e are
the twinless system mentions erroneously resolved
and c a twinless key mention. System 1 is sup-
posed to be slightly better with respect to preci-
sion, because System 2 produces one more spu-
rious resolution (i.e. for mention e ). However,
B30 computes exactly the same numbers for both
systems. Hence, there is no penalty for erroneous
coreference relations in B30, if the mentions do not
appear in the key, e.g. putting mentions d or e in
Set 1 does not count as precision errors. — B30
is too lenient by only evaluating the correctly ex-
tacted mentions.

29



Set 1 Singletons

System 1
key {a b c}
response {a b d}

P R F
B3all 0.556 0.556 0.556
B3r&n 0.556 0.556 0.556
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572

System 2
key {a b c}
response {a b d} {c}

P R F
B3all 0.667 0.556 0.606
B3r&n 0.667 0.556 0.606
B3sys 0.667 0.556 0.606
CEAFsys 0.5 0.667 0.572

Table 2: Problems of B3all (1)

Set 1 Singletons

System 1
key {a b}
response {a b d}

P R F
B3all 0.556 1.0 0.715
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800

System 2
key {a b}
response {a b d} {i} {j} {k}

P R F
B3all 0.778 1.0 0.875
B3r&n 0.556 1.0 0.715
B3sys 0.556 1.0 0.715
CEAFsys 0.667 1.0 0.800

Table 3: Problems of B3all (2)

B3all deals well with the problem illustrated in
Table 1, the figures reported correspond to in-
tuition. However, B3all can output different re-
sults for identical coreference resolutions when
exposed to different mention taggers as shown in
Tables 2 and 3. B3all manages to penalize erro-
neous resolutions for twinless system mentions,
however, it ignores twinless key mentions when
measuring precision. In Table 2, System 1 and Sys-
tem 2 generate the same outputs, except that the
mention tagger in System 2 also extracts mention
c. Intuitively, the same numbers are expected for
both systems. However, B3all gives a higher preci-
sion to System 2, which results in a higher F-score.

B3all retains all twinless system mentions, as can
be seen in Table 3. System 2’s mention tagger tags
more mentions (i.e. the mentions i, j and k), while
both System 1 and System 2 have identical coref-
erence resolution performance. Still, B3all outputs
quite different results for precision and thus for F-
score. This is due to the credit B3all takes from un-
resolved singleton twinless system mentions (i.e.
mention i, j, k in System 2). Since the metric is ex-
pected to evaluate the end-to-end coreference sys-
tem performance rather than the mention tagging
quality, it is not satisfying to observe that B3all’s
numbers actually fluctuate when the system is ex-
posed to different mention taggers.

Rahman & Ng (2009) apply another variant, de-
noted here as B3r&n. They remove only those twin-
less system mentions that are singletons before ap-
plying the B3 algorithm. So, a system would not
be rewarded by the the spurious mentions which
are correctly identified as singletons during reso-
lution (as has been the case with B3all’s higher pre-
cision for System 2 as can be seen in Table 3).

We assume that Rahman & Ng apply a strategy
similar to B3all after the removing step (this is not
clear in Rahman & Ng (2009)). While it avoids the
problem with singleton twinless system mentions,
B3r&n still suffers from the problem dealing with
twinless key mentions, as illustrated in Table 2.

2.2.2 B3sys
We here propose a coreference resolution evalua-
tion metric, B3sys, which deals with system men-
tions more adequately (see the rows labeled B3sys
in Tables 1, 2, 3, 4 and 5). We put all twinless key
mentions into the response as singletons which en-
ables B3sys to penalize non-resolved coreferent key
mentions without penalizing non-resolved single-
ton key mentions, and also avoids the problem B3all
and B3r&n have as shown in Table 2. All twinless
system mentions which were deemed not coref-
erent (hence being singletons) are discarded. To
calculate B3sys precision, all twinless system men-
tions which were mistakenly resolved are put into
the key since they are spurious resolutions (equiv-
alent to the assignment operations in B3all), which
should be penalized by precision. Unlike B3all,
B3sys does not benefit from unresolved twinless
system mentions (i.e. the twinless singleton sys-
tem mentions). For recall, the algorithm only goes
through the original key sets, similar to B3all and
B3r&n. Details are given in Algorithm 1.

For example, a coreference resolution system
has the following key and response:

Key : {a b c}
Response: {a b d} {i j}

To calculate the precision of B3sys, the key and re-
sponse are altered to:

Keyp : {a b c} {d} {i} {j}
Responsep: {a b d} {i j} {c}

30



Algorithm 1 B3sys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in

response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions

with key to form keyp;
5: Use response to form responsep
6: Through keyp and responsep;
7: Calculate B3 precision P .
8: end if
9: if calculating recall then

10: Discard all the remaining twinless system mentions in
response to from responser;

11: Use key to form keyr
12: Through keyr and responser;
13: Calculate B3 recall R
14: end if
15: Calculate F-score F

So, the precision of B3sys is given by:

PrB3sys =
1

6
( 2
3

+ 2
3

+ 1
3

+ 1
2

+ 1
2

+ 1)
.
= 0.611

The modified key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}

The resulting recall of B3sys is:

RecB3sys =
1

3
( 2
3

+ 2
3

+ 1
3
)

.
= 0.556

Thus the F-score number is calculated as:
FB3sys = 2 ×

0.611×0.556
0.611+0.556

.
= 0.582

B3sys indicates more adequately the performance of
end-to-end coreference resolution systems. It is
not easily tricked by different mention taggers3.

2.3 CEAF

Luo (2005) criticizes the B3 algorithm for using
entities more than one time, because B3 computes
precision and recall of mentions by comparing en-
tities containing that mention. Hence Luo pro-
poses the CEAF algorithm which aligns entities in
key and response. CEAF applies a similarity met-
ric (which could be either mention based or entity
based) for each pair of entities (i.e. a set of men-
tions) to measure the goodness of each possible
alignment. The best mapping is used for calculat-
ing CEAF precision, recall and F-measure.

Luo proposes two entity based similarity met-
rics (Equation 3 and 4) for an entity pair (Ki, Rj)
originating from key, Ki, and response, Rj .

φ3(Ki, Rj) = |Ki ∩ Rj | (3)

φ4(Ki, Rj) =
2|Ki ∩ Rj |

|Ki| + |Rj |
(4)

3Further example analyses can be found in Appendix A.

The CEAF precision and recall are derived from
the alignment which has the best total similarity
(denoted as Φ(g∗)), shown in Equations 5 and 6.

Precision =
Φ(g∗)

∑
i φ(Ri, Ri)

(5)

Recall =
Φ(g∗)

∑
i φ(Ki, Ki)

(6)

If not specified otherwise, we apply Luo’s φ3(⋆, ⋆)
in the example illustrations. We denote the origi-
nal CEAF algorithm as CEAForig.

Detailed calculations are illustrated below:
Key : {a b c}
Response: {a b d}

The CEAForig φ3(⋆, ⋆) are given by:
φ3(K1, R1) = 2 (K1 : {abc}; R1 : {abd})
φ3(K1, K1) = 3
φ3(R1, R1) = 3

So the CEAForig evaluation numbers are:
PrCEAForig =

2

3
= 0.667

RecCEAForig =
2

3
= 0.667

FCEAForig = 2 ×
0.667×0.667
0.667+0.667

= 0.667

2.3.1 Problems of CEAForig
CEAForig was intended to deal with key mentions.
Its adaptation to system mentions has not been ad-
dressed explicitly. Although CEAForig theoreti-
cally does not require to have the same number of
mentions in key and response, it still cannot be di-
rectly applied to end-to-end systems, because the
entity alignments are based on mention mappings.

As can be seen from Table 4, CEAForig fails
to produce intuitive results for system mentions.
System 2 outputs one more spurious entity (con-
taining mention i and j) than System 1 does, how-
ever, achieves a same CEAForig precision. Since
twinless system mentions do not have mappings in
key, they contribute nothing to the mapping simi-
larity. So, resolution mistakes for system mentions
are not calculated, and moreover, the precision is
easily skewed by the number of output entities.
CEAForig reports very low precision for system
mentions (see also Stoyanov et al. (2009)).

2.3.2 Existing CEAF variants

Rahman & Ng (2009) briefly introduce their
CEAF variant, which is denoted as CEAFr&n
here. They use φ3(⋆, ⋆), which results in equal
CEAFr&n precision and recall figures when using
true mentions. Since Rahman & Ng’s experiments
using system mentions produce unequal precision
and recall figures, we assume that, after removing

31



Set 1 Set 2 Singletons

System 1
key {a b c}
response {a b} {c} {i} {j}

P R F
CEAForig 0.4 0.667 0.500
B3sys 1.0 0.556 0.715
CEAFsys 0.667 0.667 0.667

System 2
key {a b c}
response {a b} {i j} {c}

P R F
CEAForig 0.4 0.667 0.500
B3sys 0.8 0.556 0.656
CEAFsys 0.6 0.667 0.632

Table 4: Problems of CEAForig

Set 1 Set 2 Set 3 Singletons

System 1
key {a b c}
response {a b} {i j} {k l} {c}

P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.714 0.556 0.625
CEAFsys 0.571 0.667 0.615

System 2
key {a b c}
response {a b} {i j k l} {c}

P R F
CEAFr&n 0.286 0.667 0.400
B3sys 0.571 0.556 0.563
CEAFsys 0.429 0.667 0.522

Table 5: Problems of CEAFr&n

twinless singleton system mentions, they do not
put any twinless mentions into the other set. In the
example in Table 5, CEAFr&n does not penalize
adequately the incorrectly resolved entities con-
sisting of twinless sytem mentions. So CEAFr&n
does not tell the difference between System 1 and
System 2. It can be concluded from the examples
that the same number of mentions in key and re-
sponse is needed for computing the CEAF score.

2.3.3 CEAFsys

We propose to adjust CEAF in the same way as
we did for B3sys, resulting in CEAFsys. We put
all twinless key mentions into the response as sin-
gletons. All singleton twinless system mentions
are discarded. For calculating CEAFsys precision,
all twinless system mentions which were mistak-
enly resolved are put into the key. For computing
CEAFsys recall, only the original key sets are con-
sidered. That way CEAFsys deals adequately with
system mentions (see Algorithm 2 for details).

Algorithm 2 CEAFsys
Input: key sets key, response sets response
Output: precision P , recall R and F-score F
1: Discard all the singleton twinless system mentions in

response;
2: Put all the twinless annotated mentions into response;
3: if calculating precision then
4: Merge all the remaining twinless system mentions

with key to form keyp;
5: Use response to form responsep
6: Form Map g⋆ between keyp and responsep
7: Calculate CEAF precision P using φ3(⋆, ⋆)
8: end if
9: if calculating recall then

10: Discard all the remaining twinless system mentions in
response to form responser;

11: Use key to form keyr
12: Form Map g⋆ between keyr and responser
13: Calculate CEAF recall R using φ3(⋆, ⋆)
14: end if
15: Calculate F-score F

Taking System 2 in Table 4 as an example, key and
response are altered for precision:

Keyp : {a b c} {i} {j}
Responsep: {a b d} {i j} {c}

So the φ3(⋆, ⋆) are as below, only listing the best
mappings:

φ3(K1, R1) = 2 (K1 : {abc}; R1 : {abd})
φ3(K2, R2) = 1 (K2 : {i}; R2 : {ij})
φ3(∅, R3) = 0 (R3 : {c})
φ3(R1, R1) = 3
φ3(R2, R2) = 2
φ3(R3, R3) = 1

The precision is thus give by:
PrCEAFsys =

2+1+0

3+2+1
= 0.6

The key and response for recall are:
Keyr : {a b c}
Responser: {a b} {c}

The resulting φ3(⋆, ⋆) are:
φ3(K1, R1) = 2(K1 : {abc}; R1 : {ab})
φ3(∅, R2) = 0(R2 : {c})
φ3(K1, K1) = 3
φ3(R1, R1) = 2
φ3(R2, R2) = 1

The recall and F-score are thus calculated as:
RecCEAFsys =

2

3
= 0.667

FCEAFsys = 2 ×
0.6×0.667
0.6+0.667

= 0.632

However, one additional complication arises
with regard to the similarity metrics used by
CEAF. It turns out that only φ3(⋆, ⋆) is suitable
for dealing with system mentions while φ4(⋆, ⋆)
produces uninituitive results (see Table 6).

φ4(⋆, ⋆) computes a normalized similarity for
each entity pair using the summed number of men-
tions in the key and the response. CEAF precision
then distributes that similarity evenly over the re-
sponse set. Spurious system entities, such as the
one with mention i and j in Table 6, are not pe-
nalized. φ3(⋆, ⋆) calculates unnormalized similar-
ities. It compares the two systems in Table 6 ade-
quately. Hence we use only φ3(⋆, ⋆) in CEAFsys.

32



Set 1 Singletons

System 1
key {a b c}
response {a b} {c} {i} {j}

P R F
φ4(⋆, ⋆) 0.4 0.8 0.533
φ3(⋆, ⋆) 0.667 0.667 0.667

System 2
key {a b c}
response {a b} {i j} {c}

P R F
φ4(⋆, ⋆) 0.489 0.8 0.607
φ3(⋆, ⋆) 0.6 0.667 0.632

Table 6: Problems of φ4(⋆, ⋆)

When normalizing the similarities by the num-
ber of entities or mentions in the key (for recall)
and the response (for precision), the CEAF al-
gorithm considers all entities or mentions to be
equally important. Hence CEAF tends to compute
quite low precision for system mentions which
does not represent the system performance ade-
quately. Here, we do not address this issue.

2.4 BLANC

Recently, a new coreference resolution evalua-
tion algorithm, BLANC, has been introduced (Re-
casens & Hovy, 2010). This measure implements
the Rand index (Rand, 1971) which has been orig-
inally developed to evaluate clustering methods.
The BLANC algorithm deals correctly with sin-
gleton entities and rewards correct entities accord-
ing to the number of mentions. However, a ba-
sic assumption behind BLANC is, that the sum of
all coreferential and non-coreferential links is con-
stant for a given set of mentions. This implies that
BLANC assumes identical mentions in key and re-
sponse. It is not clear how to adapt BLANC to sys-
tem mentions. We do not address this issue here.

3 Experiments

While Section 2 used toy examples to motivate our
metrics B3sys and CEAFsys, we here report results
on two larger experiments using ACE2004 data.

3.1 Data and Mention Taggers

We use the ACE2004 (Mitchell et al., 2004) En-
glish training data which we split into three sets
following Bengtson & Roth (2008): Train (268
docs), Dev (76), and Test (107). We use two in-
house mention taggers. The first (SM1) imple-
ments a heuristic aiming at high recall. The second
(SM2) uses the J48 decision tree classifier (Wit-
ten & Frank, 2005). The number of detected men-
tions, head coverage, and accuracy on testing data

SM1 SM2
training mentions 31,370 16,081

twin mentions 13,072 14,179
development mentions 8,045 –

twin mentions 3,371 –
test mentions 8,387 4,956

twin mentions 4,242 4,212
head coverage 79.3% 73.3%
accuracy 57.3% 81.2%

Table 7: Mention Taggers on ACE2004 Data

are shown in Table 7.

3.2 Artificial Setting

For the artificial setting we report results on the
development data using the SM1 tagger. To illus-
trate the stability of the evaluation metrics with
respect to different mention taggers, we reduce
the number of twinless system mentions in inter-
vals of 10%, while correct (non-twinless) ones are
kept untouched. The coreference resolution sys-
tem used is the BART (Versley et al., 2008) reim-
plementation of Soon et al. (2001). The results are
plotted in Figures 1 and 2.

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0.85

 0 0.2 0.4 0.6 0.8 1

F
-s

co
re

 fo
r 

A
C

E
04

 D
ev

el
op

m
en

t D
at

a

Proportion of twinless system mentions used in the experiment

MUC
BCubedsys

BCubed0
BCubedall
BCubedng

Figure 1: Artificial Setting B3 Variants

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0 0.2 0.4 0.6 0.8 1

F
-s

co
re

 fo
r 

A
C

E
04

 D
ev

el
op

m
en

t D
at

a

Proportion of twinless system mentions used in the experiment

MUC
CEAFsys
CEAForig
CEAFng

Figure 2: Artificial Setting CEAF Variants

33



MUC
R Pr F

Soon (SM1) 51.7 53.1 52.4
Soon (SM2) 49.1 69.9 57.7

Table 8: Realistic Setting MUC

Omitting twinless system mentions from the
training data while keeping the number of cor-
rect mentions constant should improve the corefer-
ence resolution performance, because a more pre-
cise coreference resolution model is obtained. As
can be seen from Figures 1 and 2, the MUC-score,
B3sys and CEAFsys follow this intuition.

B30 is almost constant. It does not take twinless
mentions into account. B3all’s curve, also, has a
lower slope in comparison to B3sys and MUC (i.e.
B3all computes similar numbers for worse models).
This shows that the B3all score can be tricked by
using a high recall mention tagger, e.g. in cases
with the worse models (i.e. ones on the left side
of the figures) which have much more twinless
system mentions. The original CEAF algorithm,
CEAForig, is too sensitive to the input system
mentions making it less reliable. CEAFsys is par-
allel to B3sys. Thus both of our metrics exhibit the
same intuition.

3.3 Realistic Setting

3.3.1 Experiment 1

For the realistic setting we compare SM1 and SM2
as preprocessing components for the BART (Ver-
sley et al., 2008) reimplementation of Soon et al.
(2001). The coreference resolution system with
the SM2 tagger performs better, because a better
coreference model is achieved from system men-
tions with higher accuracy.

The MUC, B3sys and CEAFsys metrics have the
same tendency when applied to systems with dif-
ferent mention taggers (Table 8, 9 and 10 and the
bold numbers are higher with a p-value of 0.05,
by a paired-t test). Since the MUC scorer does
not evaluate singleton entities, it produces too low
numbers which are not informative any more.

As shown in Table 9, B3all reports counter-
intuitive results when a system is fed with system
mentions generated by different mention taggers.
B3all cannot be used to evaluate two different end-
to-end coreference resolution systems, because the
mention tagger is likely to have bigger impact than
the coreference resolution system. B30 fails to gen-
erate the right comparison too, because it is too

B3sys B
3
0

R Pr F R Pr F
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4
Bengtson 66.1 81.9 73.1 69.5 74.7 72.0

Table 11: Realistic Setting

lenient by ignoring all twinless mentions.
The CEAForig numbers in Table 10 illustrate the

big influence the system mentions have on preci-
sion (e.g. the very low precision number for Soon
(SM1)). The big improvement for Soon (SM2) is
largely due to the system mentions it uses, rather
than to different coreference models.

Both B3r&n and CEAFr&n show no serious prob-
lems in the experimental results. However, as dis-
cussed before, they fail to penalize the spurious
entities with twinless system mentions adequately.

3.3.2 Experiment 2

We compare results of Bengtson & Roth’s (2008)
system with our Soon (SM2) system. Bengtson &
Roth’s embedded mention tagger aims at high pre-
cision, generating half of the mentions SM1 gen-
erates (explicit statistics are not available to us).

Bengtson & Roth report a B3 F-score for sys-
tem mentions, which is very close to the one for
true mentions. Their B3-variant does not impute
errors of twinless mentions and is assumed to be
quite similar to the B30 strategy.

We integrate both the B30 and B
3
sys variants into

their system and show results in Table 11 (we can-
not report significance, because we do not have ac-
cess to results for single documents in Bengtson &
Roth’s system). It can be seen that, when different
variants of evaluation metrics are applied, the per-
formance of the systems vary wildly.

4 Conclusions

In this paper, we address problems of commonly
used evaluation metrics for coreference resolution
and suggest two variants for B3 and CEAF, called
B3sys and CEAFsys. In contrast to the variants
proposed by Stoyanov et al. (2009), B3sys and
CEAFsys are able to deal with end-to-end systems
which do not use any gold information. The num-
bers produced by B3sys and CEAFsys are able to
indicate the resolution performance of a system
more adequately, without being tricked easily by
twisting preprocessing components. We believe
that the explicit description of evaluation metrics,
as given in this paper, is a precondition for the re-

34



B3sys B
3
0 B

3
all B

3
r&n

R Pr F R Pr F R Pr F R Pr F
Soon (SM1) 65.7 76.8 70.8 57.0 91.1 70.1 65.1 85.8 74.0 65.1 78.7 71.2
Soon (SM2) 64.1 87.3 73.9 54.7 91.3 68.4 64.3 87.1 73.9 64.3 84.9 73.2

Table 9: Realistic Setting B3 Variants

CEAFsys CEAForig CEAFr&n
R Pr F R Pr F R Pr F

Soon (SM1) 66.4 61.2 63.7 62.0 39.9 48.5 62.1 59.8 60.9
Soon (SM2) 67.4 65.2 66.3 60.0 56.6 58.2 60.0 66.2 62.9

Table 10: Realistic Setting CEAF Variants

liabe comparison of end-to-end coreference reso-
lution systems.

Acknowledgements. This work has been
funded by the Klaus Tschira Foundation, Hei-
delberg, Germany. The first author has been
supported by a HITS PhD. scholarship. We
would like to thank Éva Mújdricza-Maydt for
implementing the mention taggers.

References

Bagga, Amit & Breck Baldwin (1998). Algorithms for scor-
ing coreference chains. In Proceedings of the 1st Inter-
national Conference on Language Resources and Evalua-
tion, Granada, Spain, 28–30 May 1998, pp. 563–566.

Bengtson, Eric & Dan Roth (2008). Understanding the value
of features for coreference resolution. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-27
October 2008, pp. 294–303.

Culotta, Aron, Michael Wick & Andrew McCallum (2007).
First-order probabilistic models for coreference resolu-
tion. In Proceedings of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics, Rochester,
N.Y., 22–27 April 2007, pp. 81–88.

Denis, Pascal & Jason Baldridge (2007). Joint determination
of anaphoricity and coreference resolution using integer
programming. In Proceedings of Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics,
Rochester, N.Y., 22–27 April 2007, pp. 236–243.

Haghighi, Aria & Dan Klein (2007). Unsupervised coref-
erence resolution in a nonparametric Bayesian model. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23–30 June 2007, pp. 848–855.

Luo, Xiaoqiang (2005). On coreference resolution perfor-
mance metrics. In Proceedings of the Human Language
Technology Conference and the 2005 Conference on Em-
pirical Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6–8 October 2005, pp. 25–32.

Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla & Salim Roukos (2004). A mention-
synchronous coreference resolution algorithm based on
the Bell Tree. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21–26 July 2004, pp. 136–143.

Mitchell, Alexis, Stephanie Strassel, Shudong Huang &
Ramez Zakhary (2004). ACE 2004 Multilingual Training

Corpus. LDC2005T09, Philadelphia, Penn.: Linguistic
Data Consortium.

Nicolae, Cristina & Gabriel Nicolae (2006). BestCut: A
graph algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods in Nat-
ural Language Processing, Sydney, Australia, 22–23 July
2006, pp. 275–283.

Ponzetto, Simone Paolo & Michael Strube (2006). Exploiting
semantic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proceedings of the Human Language
Technology Conference of the North American Chapter of
the Association for Computational Linguistics, New York,
N.Y., 4–9 June 2006, pp. 192–199.

Rahman, Altaf & Vincent Ng (2009). Supervised models for
coreference resolution. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6-7 August 2009, pp. 968–977.

Rand, William R. (1971). Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846–850.

Recasens, Marta & Eduard Hovy (2010). BLANC: Imple-
menting the Rand index for coreference evaluation. Sub-
mitted.

Soon, Wee Meng, Hwee Tou Ng & Daniel Chung Yong
Lim (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics,
27(4):521–544.

Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & Ellen
Riloff (2009). Conundrums in noun phrase coreference
resolution: Making sense of the state-of-the-art. In Pro-
ceedings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics and
the 4th International Joint Conference on Natural Lan-
guage Processing, Singapore, 2–7 August 2009, pp. 656–
664.

Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang & Alessandro Moschitti (2008). BART: A modular
toolkit for coreference resolution. In Companion Volume
to the Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, Columbus, Ohio,
15–20 June 2008, pp. 9–12.

Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly
& Lynette Hirschman (1995). A model-theoretic corefer-
ence scoring scheme. In Proceedings of the 6th Message
Understanding Conference (MUC-6), pp. 45–52. San Ma-
teo, Cal.: Morgan Kaufmann.

Witten, Ian H. & Eibe Frank (2005). Data Mining: Practical
Machine Learning Tools and Techniques (2nd ed.). San
Francisco, Cal.: Morgan Kaufmann.

Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008). A twin-
candidate model for learning-based anaphora resolution.
Computational Linguistics, 34(3):327–356.

35



A B3sys Example Output

Here, we provide additional examples for analyzing the behavior of B3sys where we systematically vary
system outputs. Since we proposed B3sys for dealing with end-to-end systems, we consider only examples
also containing twinless mentions. The systems in Table 12 and 14 generate different twinless key
mentions while keeping the twinless system mentions untouched. In Table 13 and 15, the number of
twinless system mentions changes through different responses and the number of twinless key mentions
is fixed.

In Table 12, B3sys recall goes up when more key mentions are resolved into the correct set. And the
precision stays the same, because there is no change in the number of the erroneous resolutoins (i.e. the
spurious cluster with mentions i and j). For the examples in Tables 13 and 15, B3sys gives worse precision
to the outputs with more spurious resolutions, and the same recall if the systems resolve key mentions in
the same way. Since the set of key mentions intersects with the set of twinless system mentions in Table
14, we do not have an intuitive explanation for the decrease in precision from response1 to response4.
However, both the F-score and the recall still show the right tendency.

Set 1 Set 2 B3sys
key {a b c d e} P R F
response1 {a b} {i j} 0.857 0.280 0.422
response2 {a b c} {i j} 0.857 0.440 0.581
response3 {a b c d} {i j} 0.857 0.68 0.784
response4 {a b c d e} {i j} 0.857 1.0 0.923

Table 12: Analysis of B3sys 1
Set 1 Set 2 B3sys

key {a b c d e} P R F
response1 {a b c} {i j} 0.857 0.440 0.581
response2 {a b c} {i j k} 0.75 0.440 0.555
response3 {a b c} {i j k l} 0.667 0.440 0.530
response4 {a b c} {i j k l m} 0.6 0.440 0.508

Table 13: Analysis of B3sys 2
Set 1 B3sys

key {a b c d e} P R F
response1 {a b i j} 0.643 0.280 0.390
response2 {a b c i j} 0.6 0.440 0.508
response3 {a b c d i j} 0.571 0.68 0.621
response4 {a b c d e i j} 0.551 1.0 0.711

Table 14: Analysis of B3sys 3
Set 1 B3sys

key {a b c d e} P R F
response1 {a b c i j} 0.6 0.440 0.508
response2 {a b c i j k} 0.5 0.440 0.468
response3 {a b c i j k l} 0.429 0.440 0.434
response4 {a b c i j k l m} 0.375 0.440 0.405

Table 15: Analysis of B3sys 4

36


