Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895–903,

Beijing, August 2010

895

Citation Summarization Through Keyphrase Extraction

Vahed Qazvinian
Department of EECS
University of Michigan
vahed@umich.edu

Dragomir R. Radev

School of Information and

Department of EECS
University of Michigan
radev@umich.edu

Arzucan ¨Ozg¨ur

Department of EECS
University of Michigan
ozgur@umich.edu

Abstract

This paper presents an approach to sum-
marize single scientiﬁc papers, by extract-
ing its contributions from the set of cita-
tion sentences written in other papers. Our
methodology is based on extracting sig-
niﬁcant keyphrases from the set of cita-
tion sentences and using these keyphrases
to build the summary. Comparisons show
how this methodology excels at the task
of single paper summarization, and how it
out-performs other multi-document sum-
marization methods.

1 Introduction

In recent years statistical physicists and computer
scientists have shown great interest in analyzing
complex adaptive systems. The study of such sys-
tems can provide valuable insight on the behav-
ioral aspects of the involved agents with potential
applications in economics and science. One such
aspect is to understand what motivates people to
provide the n + 1st review of an artifact given that
they are unlikely to add something signiﬁcant that
has not already been said or emphasized. Cita-
tions are part of such complex systems where ar-
ticles use citations as a way to mention different
contributions of other papers, resulting in a col-
lective system.

The focus of this work is on the corpora cre-
ated based on citation sentences. A citation sen-
tence is a sentence in an article containing a ci-
tation and can contain zero or more nuggets (i.e.,
non-overlapping contributions) about the cited ar-
ticle. For example the following sentences are a

few citation sentences that appeared in the NLP
literature in past that talk about Resnik’s work.

The STRAND system (Resnik, 1999), for example, uses
structural markup information from the pages, without
looking at their content, to attempt to align them.

Resnik

(1999)

of
language identiﬁcation for ﬁnding Web pages in
the languages of interest.

addressed

issue

the

Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufﬁcient quantities of high quality
data..

The set of citations is important to analyze be-
cause human summarizers have put their effort
collectively but independently to read the target
article and cite its important contributions. This
has been shown in other work too (Elkiss et al.,
2008; Nanba et al., 2004; Qazvinian and Radev,
2008; Mei and Zhai, 2008; Mohammad et al.,
2009).
In this work, we introduce a technique
to summarize the set of citation sentences and
cover the major contributions of the target paper.
Our methodology ﬁrst ﬁnds the set of keyphrases
that represent important information units (i.e.,
nuggets), and then ﬁnds the best set of k sentences
to cover more, and more important nuggets.

Our results conﬁrm the effectiveness of the
method and show that it outperforms other state
of the art summarization techniques. Moreover,
as shown in the paper, this method does not need
to calculate the full cosine similarity matrix for a
document cluster, which is the most time consum-
ing part of the mentioned baseline methods.

1.1 Related Work
Previous work has used citations to produce sum-
maries of scientiﬁc work (Qazvinian and Radev,

896

2008; Mei and Zhai, 2008; Elkiss et al., 2008).
Other work (Bradshaw, 2003; Bradshaw, 2002)
beneﬁts from citations to determine the content of
articles and introduce “Reference Directed Index-
ing” to improve the results of a search engine.

In other work, (Nanba and Okumura, 1999) an-
alyze citation sentences and automatically cate-
gorize citations into three groups using 160 pre-
deﬁned phrase-based rules to support a system for
writing a survey. Previous research has shown
the importance of the citation summaries in un-
derstanding what a paper contributes.
In partic-
ular, (Elkiss et al., 2008) performed a large-scale
study on citation summaries and their importance.
Results from this experiment conﬁrmed that the
“Self Cohesion” (Elkiss et al., 2008) of a citation
summary of an article is consistently higher than
the that of its abstract and that citations contain
additional information that does not appear in ab-
stracts.

Kan et al. (2002) use annotated bibliographies
to cover certain aspects of summarization and sug-
gest using metadata and critical document features
as well as the prominent content-based features to
summarize documents. Kupiec et al. (1995) use
a statistical method and show how extracts can
be used to create summaries but use no annotated
metadata in summarization.

Siddharthan and Teufel describe a new task to
decide the scientiﬁc attribution of an article (Sid-
dharthan and Teufel, 2007) and show high hu-
man agreement as well as an improvement in the
performance of Argumentative Zoning (Teufel,
2005). Argumentative Zoning is a rhetorical clas-
siﬁcation task, in which sentences are labeled as
one of Own, Other, Background, Textual, Aim,
Basis, Contrast according to their role in the au-
thor’s argument. These all show the importance
of citation summaries and the vast area for new
work to analyze them to produce a summary for a
given topic.

The Maximal Marginal Relevance (MMR)
summarization method, which is based on a
greedy algorithm, is described in (Carbonell and
Goldstein, 1998). MMR uses the full similarity
matrix to choose the sentences that are the least
similar to the sentences already selected for the
summary. We selected this method as one of our

Fact
f1: “ Supervised Learning”
f2: “ instance/concept relations”
f3: “Part-of-Speech tagging”
f4: “ﬁltering QA results”
f5: “lexico-semantic information”
f6: “hyponym relations”

Occurrences
5
3
3
2
2
2

Table 2: Nuggets of P03-1001 extracted by anno-
tators.

baseline methods, which we have explained in
more details in Section 4.
2 Data
In order to evaluate our method, we use the ACL
Anthology Network (AAN), which is a collec-
tion of papers from the Computational Linguistics
journal and proceedings from ACL conferences
and workshops and includes more than 13, 000 pa-
pers (Radev et al., 2009). We use 25 manually an-
notated papers from (Qazvinian and Radev, 2008),
which are highly cited articles in AAN. Table 1
shows the ACL ID, title, and the number of cita-
tion sentences for these papers.

The annotation guidelines asked a number of
annotators to read the citation summary of each
paper and extract a list of the main contribu-
tions of that paper. Each item on the list is a
non-overlapping contribution (nugget) perceived
by reading the citation summary. The annota-
tion strictly instructed the annotators to focus on
the citing sentences to do the task and not their
own background on the topic. Then, extracted
nuggets are reviewed and those nuggets that have
only been mentioned by 1 annotator are removed.
Finally, the union of the rest is used as a set of
nuggets representing each paper.

Table 2 lists the nuggets extracted by annotators

for P03-1001.

3 Methodology

Our methodology assumes that each citation sen-
tence covers 0 or more nuggets about the cited
papers, and tries to pick sentences that maximize
nugget coverage with respect to summary length.
These nuggets are essentially represented using
keyphrases. Therefore, we try to extract signiﬁ-
cant keyphrases in order to represent nuggets each
sentence contains. Here, the keyphrases are ex-

897

Title
Statistical Phrase-Based Translation
Learning Surface Text Patterns For A Question Answering System
On-line Large-Margin Training Of Dependency Parsers
Three New Probabilistic Models For Dependency Parsing: An Exploration
A Hierarchical Phrase-Based Model For Statistical Machine Translation
Three Generative, Lexicalized Models For Statistical Parsing
A Statistical Parser For Czech
The Alignment Template Approach To Statistical Machine Translation
Towards Answering Opinion Questions: Separating Facts From Opinions ...
Pseudo-Projective Dependency Parsing
Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ...
Ofﬂine Strategies For Online Question Answering: Answering Questions Before They Are Asked
Improvements In Phrase-Based Statistical Machine Translation
Cut And Paste Based Text Summarization

ACL-ID
N03-1017
P02-1006
P05-1012
C96-1058
P05-1033
P97-1003
P99-1065
J04-4002
D03-1017
P05-1013
W00-0403
P03-1001
N04-1033
A00-2024
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests
A00-1043
C00-1072
W05-1203 Measuring The Semantic Similarity Of Texts
W03-0510
W03-0301 An Evaluation Exercise For Word Alignment
A00-1023
D04-9907
P05-1014
H05-1047
H05-1079

A Question Answering System Supported By Information Extraction
Scaling Web-Based Acquisition Of Entailment Relations
The Distributional Inclusion Hypotheses And Lexical Entailment
A Semantic Approach To Recognizing Textual Entailment
Recognising Textual Entailment With Logical Inference

Sentence Reduction For Automatic Text Summarization
The Automated Acquisition Of Topic Signatures For Text Summarization

The Potential And Limitations Of Automatic Sentence Extraction For Summarization

# citations

180
74
71
66
65
55
54
50
42
40
31
27
24
20
19
19
19
17
15
14
13
12
10
8
9

Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citing
each.

unigrams
bigrams
3-grams
4-grams

unique
229,631
2,256,385
5,125,249
6,713,568

all

7,746,792
7,746,791
7,746,790
7,746,789

max freq
437,308
73,957
3,600
2,408

Table 3: Statistics on the abstract corpus in AAN
used as the background data

pressed using N-grams, and thus these building
units are the key elements to our summarization.
For each citation sentence di, our method ﬁrst ex-
tracts a set of important keyphrases, Di, and then
tries to ﬁnd sentences that have a larger number of
important and non-redundant keyphrases. In order
to take the ﬁrst step, we extract statistically sig-
niﬁcantly frequent N-grams (up to N = 4) from
each citing sentence and use them as the set of
representative keyphrases for that citing sentence.

3.1 Automatic Keyphrase Extraction

A list of keyphrases for each citation sentence can
be generated by extracting N-grams that occur
signiﬁcantly frequently in that sentence compared
to a large corpus of such N-grams. Our method
for such an extraction is inspired by the previ-
ous work by Tomokiyo and Hurst (Tomokiyo and
Hurst, 2003).

A language model, M, is a statistical model
that assigns probabilities to a sequence of N-
grams. Every language model is a probability dis-
tribution over all N-grams and thus the probabili-
ties of all N-grams of the same length sum up to
1. In order to extract keyphrases from a text us-
ing statistical signiﬁcance we need two language
models. The ﬁrst model is referred to as the Back-
ground Model (BM) and is built using a large
text corpus. Here we build the BM using the text
of all the paper abstracts provided in AAN1. The
second language model is called the Foreground
Model (FM) and is the model built on the text
from which keyphrases are being extracted.
In
this work, the set of all citation sentences that cite
a particular target paper are used to build a fore-
ground language model.

Let gi be an N-gram of size i and CM(gi) de-
note the count of gi in the model M. First, we ex-
tract the counts of each N-grams in both the back-
ground (BM) and the foreground corpora (FM).

1http://chernobog.si.umich.edu/clair/anthology/index.cgi

898

MBM =

NBM =

X

1

gi∈{BM∪FM}

X

gi∈{BM∪FM}

CBM(gi)

NFM = X
gi∈FM

CFM(gi)

ˆpFM(gi) = CFM(gi)/NFM
ˆpBM(gi) = (CBM(gi) + 1)/(MBM + NBM)

The last equation is also known as Laplace
smoothing (Manning and Schutze, 2002) and han-
dles the N-grams in the foreground corpus that
have a 0 occurrence frequency in the background
corpus. Next, we extract N-grams from the fore-
ground corpus that have signiﬁcant frequencies
compared to the frequency of the same N-grams
in the background model and its individual terms
in the foreground model.

To measure how randomly a set of consecu-
tive terms are forming an N-gram, Tomokiyo and
Hurst (Tomokiyo and Hurst, 2003) use pointwise
divergence. In particular, for an N-gram of size i,
gi = (w1w2 ··· wi),

δgi (FMikFM1) = ˆpFM(gi) log(

ˆpFM(gi)
j=1 ˆpFM(wj)

Qi

)

This equation shows the extent to which the
terms forming gi have occurred together ran-
domly. In other words, it indicates the extent of in-
formation that we lose by assuming independence
of each word by applying the unigram model, in-
stead of the N-gram model.

In addition, to measure how randomly a se-
quence of words appear in the foreground model
with respect to the background model, we use
pointwise divergence as well. Here, pointwise di-
vergence deﬁnes how much information we lose
by assuming that gi is drawn from the background
model instead of the foreground model:

(Corley and Mihalcea, 2005) applied or uti-
lized lexical based word overlap measures.
{overlap measures, word overlap,
based, utilized lexical}

lexical

Table 4: Example: citation sentence for W05-
1203 written by D06-1621, and its extracted bi-
grams.

We set the criteria of choosing a sequence of
words as signiﬁcant to be whether it has posi-
tive pointwise divergence with respect to both the
background model, and individual terms of the
foreground model. In other words we extract all gi
from FM for which the both properties are posi-
tive:

δgi (FMikBMi) > 0
δgi (FMikFM1) ≥ 0

The equality condition in the second equation
is speciﬁcally set to handle unigrams, in which

ˆpFM(gi) =Qi

j=1 ˆpFM(wj).

In order to handle the text corpora and build-
ing the language models, we have used the CMU-
Cambridge Language Model
toolkit (Clarkson
and Rosenfeld, 1997). We use the set of cita-
tion sentences for each paper to build foreground
language models. Furthermore, we employ this
tool and make the background model using nearly
11,000 abstracts from AAN. Table 3 summarizes
some of the statistics about the background data.
Once keyphrases (signiﬁcant N-grams) of each
sentence are extracted, we remove all N-grams in
which more than half of the terms are stopwords.
For instance, we remove all stopword unigrams,
if any, and all bigrams with at least one stop-
word in them. For 3-grams and 4-grams we use
a threshold of 2 and 3 stopwords respectively. Af-
ter that, the set of remaining N-grams is used to
represent each sentence and to build summaries.
Table 4 shows an example of a citation sentence
from D06-1621 citing W05-1203 (Corley and Mi-
halcea, 2005), and its extracted bigrams.

δgi (FMikBMi) = ˆpFM(gi) log(

ˆpFM(gi)
ˆpBM(gi)

)

3.2 Sentence Selection
After extracting the set of keyphrases for each sen-
tence, di, the sentence is represented using its set

899

of N-grams, denoted by Di. Then, the goal is
to pick sentences (sets) for each paper that cover
more important and non-redundant keyphrases.
Essentially, keyphrases that have been repeated in
more sentences are more important and could rep-
resent more important nuggets. Therefore, sen-
tences that contain more frequent keyphrases are
more important. Based on this intuition we deﬁne
the reward of building a summary comprising a
set of keyphrases S as

f (S) = |S ∩ A|

where A is the set of all keyphrases from sen-
tences not in the summary.

it is non-negative. Second,

The set function f has three main properties.
First,
it is mono-
tone (i.e., For every set v we have f (S + v) ≥
f (S)). Third, f is sub-modular. The submodular-
ity means that for a set v and two sets S ⊆ T we
have

f (S + v) − f (S) ≥ f (T + v) − f (T )

Intuitively, this property implies that adding a set
v to S will increase the reward at least as much
as it would to a larger set T . In the summariza-
tion setting, this means that adding a sentence to
a smaller summary will increase the reward of the
summary at least as much as adding it to a larger
summary that subsumes it. The following theorem
formalizes this and is followed by a proof.

Theorem 1 The reward function f is submodular.

Proof
We start by deﬁning a gain function G of adding
sentence (set) Di to Sk−1 where Sk−1 is the set
of keyphrases in a summary built using k − 1 sen-
tences, and Di is a candidate sentence to be added:

Sk−1 ⊆ Sk
S′k−1 ⊇ S′k
∩i ∩ S′k−1 ⊇ ∩i ∩ S′k
∩i − Sk−1 ⊇ ∩i − Sk
| ∩i −Sk−1| ≥ | ∩i −Sk|
G(Di, Sk−1) ≥ G(Di,Sk)
f (Sk−1 ∪ Di) − f (Sk−1) ≥ f (Sk ∪ Di) − f (Sk)

Here, S′k is the set of all N-grams in the vo-
cabulary that are not present in Sk. The gain of
adding a sentence, Di, to an empty summary is a
non-negative value.

G(Di,S0) = C ≥ 0

By induction, we will get

G(Di,S0) ≥ G(Di,S1) ≥ ··· ≥ G(Di,Sk) ≥ 0
2

Theorem 1 implies the general case of submodu-
larity:

∀m, n, 0 ≤ m ≤ n ≤ |D| ⇒ G(Di,Sm) ≥ G(Di,Sn)

Maximizing this submodular function is an NP-
hard problem (Khuller et al., 1999). A common
way to solve this maximization problem is to start
with an empty set, and in each iteration pick a set
that maximizes the gain. It has been shown be-
fore in (Kulik et al., 2009) that if f is a submod-
ular, nondecreasing set function and f (∅) = 0,
then such a greedy algorithm ﬁnds a set S, whose
gain is at least as high as (1 − 1/e) of the best
possible solution. Therefore, we can optimize the
keyphrase coverage as described in Algorithm 1.

G(Di,Sk−1) = f (Sk−1 ∪ Di) − f (Sk−1)

4 Experimental Setup

Simple investigation through a Venn diagram
proof shows that G can be re-written as

G(Di,Sk−1) = |Di ∩ (∪j6=iDj) − Sk−1|
Let’s denote Di ∩ (∪j6=iDj) by ∩i. The follow-

ing equations prove the theorem.

We use the annotated data described in Section 2.
In summary, the annotation consisted of two parts:
nugget extraction and nugget distribution analy-
sis. Five annotators were employed to annotate
the sentences in each of the 25 citation summaries
and write down the nuggets (non-overlapping con-
tributions) of the target paper. Then using these

900

Summary generated using bigram-based keyphrases

ID
P06-1048:1

Sentence
Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al 2003; Nguyen et al 2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.

J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simpliﬁcation (Chandrasekar, Doran, and Bangalore
1996; Mahesh 1997; Carroll et al 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu
2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.
The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.
A00-2024:9
N03-1026:17 To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000)

P06-2019:5

and Jing (2000).
Jing (2000) was perhaps the ﬁrst to tackle the sentence compression problem.

Table 5: Bigram-based summary generated for A00-1043.

Algorithm 1 The greedy algorithm for summary
generation

k ← the number of sentences in the summary
Di ← keyphrases in di
S ← ∅
for l = 1 to k do

sl ← arg maxDi∈D |Di ∩ (∪j6=iDj)|
S ← S ∪ sl
for j = 1 to |D| do
Dj ← Dj − sl
end for

end for
return S

nugget sets, each sentence was annotated with the
nuggets it contains. This results in a sentence-
fact matrix that helps with the evaluation of the
summary. The summarization goal and the intu-
ition behind the summarizing system is to select a
few (5 in our experiments) sentences and cover as
many nuggets as possible. Each sentence in a cita-
tion summary may contain 0 or more nuggets and
not all nuggets are mentioned an equal number of
times. Covering some nuggets (contributions) is
therefore more important than others and should
be weighted highly.

To capture this property,

the pyramid score
seems the best evaluation metric to use. We use
the pyramid evaluation method (Nenkova and Pas-
sonneau, 2004) at the sentence level to evaluate
the summary created for each set. We beneﬁt
from the list of annotated nuggets provided by the
annotators as the ground truth of the summariza-
tion evaluation. These annotations give the list of
nuggets covered by each sentence in each citation
summary, which are equivalent to the summariza-
tion content unit (SCU) as described in (Nenkova

and Passonneau, 2004).

The pyramid score for a summary is calculated
as follows. Assume a pyramid that has n tiers, Ti,
where tier Ti > Tj if i > j (i.e., Ti is not below
Tj, and that if a nugget appears in more sentences,
it falls in a higher tier.). Tier Ti contains nuggets
that appeared in i sentences, and thus has weight
i. Suppose |Ti| shows the number of nuggets in
tier Ti, and Qi is the size of a subset of Ti whose
members appear in the summary. Further suppose
Q shows the sum of the weights of the facts that
i=1 i × Qi.
In addition, the optimal pyramid score for a sum-
mary with X facts, is

are covered by the summary. Q =Pn

M ax =

n

X

i=j+1

i × |Ti| + j × (X −

n

X

i=j+1

|Ti|)

where j = maxi(Pn

t=i |Tt| ≥ X). The pyra-
mid score for a summary is then calculated as fol-
lows.

P =

Q

M ax

This score ranges from 0 to 1, and a high
score shows the summary contains more heavily
weighted facts.

4.1 Baselines and Gold Standards
To evaluate the quality of the summaries gen-
erated by the greedy algorithm, we compare its
pyramid score in each of the 25 citation sum-
maries with those of a gold standard, a random
summary, and four other methods. The gold stan-
dards are summaries created manually using 5
sentences. The 5 sentences are manually selected
in a way to cover as many nuggets as possible with
higher priority for the nuggets with higher fre-
quencies. We also created random summaries us-
ing Mead (Radev et al., 2004). These summaries

901

are basically a random selection of 5 sentences
from the pool of sentences in the citation sum-
mary. Generally we expect the summaries cre-
ated by the greedy method to be signiﬁcantly bet-
ter than random ones.

In addition to the gold and random summaries,
we also used 4 baseline state of the art sum-
marizers: LexRank,
the clustering C-RR and
C-LexRank, and Maximal Marginal Relevance
(MMR). LexRank (Erkan and Radev, 2004) works
based on a random walk on the cosine similar-
ity of sentences and prints out the most frequently
visited sentences. Said differently, LexRank ﬁrst
builds a network in which nodes are sentences and
edges are cosine similarity values. It then uses the
eigenvalue centralities to ﬁnd the most central sen-
tences. For each set, the top 5 sentences on the list
are chosen for the summary.

The clustering methods, C-RR and C-LexRank,
work by clustering the cosine similarity network
of sentences. In such a network, nodes are sen-
tences and edges are cosine similarity of node
pairs. Clustering would intuitively put nodes with
similar nuggets in the same clusters as they are
more similar to each other. The C-RR method as
described in (Qazvinian and Radev, 2008) uses a
round-robin fashion to pick sentences from each
cluster, assuming that the clustering will put the
sentences with similar facts into the same clus-
ters. Unlike C-RR, C-LexRank uses LexRank to
ﬁnd the most salient sentences in each cluster, and
prints out the most central nodes of each cluster as
summary sentences.

Finally, MMR uses the full cosine similarity
matrix and greedily chooses sentences that are the
least similar to those already selected for the sum-
mary (Carbonell and Goldstein, 1998). In partic-
ular,

M M R = arg min

di∈D−Ahmax

dj∈A

Sim(di, dj)i

where A is the set of sentences in the summary,
initially set to A = ∅. This method is different
from ours in that it chooses the least similar sen-
tence to the summary in each iteration.

4.2 Results and Discussion
As mentioned before, we use the text of the ab-
stracts of all the papers in AAN as the back-

ground, and each citation set as a separate fore-
ground corpus. For each citation set, we use the
method described in Section 3.1 to extract signif-
icant N-grams of each sentence. We then use the
keyphrase set representation of each sentence to
build the summaries using Algorithm 1. For each
of the 25 citation summaries, we build 4 differ-
ent summaries using unigrams, bigrams, 3-grams,
and 4-grams respectively. Table 5 shows a 5-
sentence summary created using algorithm 1 for
the paper A00-1043 (Jing, 2000).

The pyramid scores for different methods are
reported in Figure 1 together with the scores
of gold standards, manually created to cover as
many nuggets as possible in 5 sentences, as
well as summary evaluations of the 4 baseline
methods described above. This Figure shows
how the keyphrase based summarization method
when employing N-grams of size 3 or smaller,
outperforms other baseline systems signiﬁcantly.
More importantly, Figure 1 also indicates that this
method shows more stable results and low varia-
tion in summary quality when keyphrases of size 3
or smaller are employed. In contrast, MMR shows
high variation in summary qualities making sum-
maries that obtain pyramid scores as low as 0.15.
Another important advantage of this method is
that we do not need to calculate the cosine simi-
larity of the pairs of sentences, which would add a
running time of O(|D|2|V |) in the number of doc-
uments, |D|, and the size of the vocabulary |V | to
the algorithm.

5 Conclusion and Future Work

This paper presents a summarization methodol-
ogy that employs keyphrase extraction to ﬁnd im-
portant contributions of scientiﬁc articles. The
summarization is based on citation sentences and
picks sentences to cover nuggets (represented by
keyphrases) or contributions of the target papers.
In this setting the best summary would have as few
sentences and at the same time as many nuggets
as possible. In this work, we use pointwise KL-
divergence to extract statistically signiﬁcant N-
grams and use them to represent nuggets. We
then apply a new set function for the task of sum-
marizing scientiﬁc articles. We have proved that
this function is submodular and concluded that a

902

 

e
r
o
c
S
d
m
a
r
y
P

i

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Gold

Mead

LexRank C−RR C−LexRank MMR 1−gram 2−gram 3−gram 4−gram

Figure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasets
using different methods.

greedy algorithm will result in a near-optimum set
of covered nuggets using only 5 sentences. Our
experiments in this paper conﬁrm that the sum-
maries created based on the presented algorithm
are better than randomly generated summary, and
also outperform other state of the art summariza-
tion methods in most cases. Moreover, we show
how this method generates more stable summaries
with lower variation in summary quality when N-
grams of size 3 or smaller are employed.

A future direction for this work is to perform
post-processing on the summaries and re-generate
sentences that cover the extracted nuggets. How-
ever, the ultimate goal is to eventually develop
systems that can produce summaries of entire
research areas, summaries that will enable re-
searchers to easily and quickly switch between
ﬁelds of research.

One future study that will help us generate
better summaries is to understand how nuggets
are generated by authors.
In fact, modeling the
nugget coverage behavior of paper authors will
help us identify more important nuggets and dis-
cover some aspects of the paper that would oth-

erwise be too difﬁcult by just reading the paper
itself.

6 Acknowledgements

This work is in part supported by the National
Science Foundation grant “iOPENER: A Flexi-
ble Framework to Support Rapid Learning in Un-
familiar Research Domains”, jointly awarded to
University of Michigan and University of Mary-
land as IIS 0705832, and in part by the NIH Grant
U54 DA021519 to the National Center for Inte-
grative Biomedical Informatics.

Any opinions, ﬁndings, and conclusions or rec-
ommendations expressed in this paper are those
of the authors and do not necessarily reﬂect the
views of the supporters.

References
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientiﬁc Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.

Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European

903

Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926–931.

Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004. Classiﬁcation of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation.
In Proceedings
of the 11th SIG Classiﬁcation Research Workshop,
pages 117–134, Chicago, USA.

Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.

Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entiﬁc paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.

Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C¸ elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.

Radev, Dragomir R., Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus.
In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.

Siddharthan, Advaith and Simone Teufel.

2007.
Whose idea was this, and why does it matter? at-
tributing scientiﬁc work to citations. In Proceedings
of NAACL/HLT-07.

Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159–
170.

Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 workshop on Multi-
word expressions, pages 33–40.

Conference on Research and Advanced Technology
for Digital Libraries.

Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries.
In SI-
GIR’98, pages 335–336.

Clarkson, PR and R Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45–148.

Elkiss, Aaron, Siwei Shen, Anthony Fader, G¨unes¸
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51–62.

Erkan, G¨unes¸ and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artiﬁcial Intelligence Research.

Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization.
In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310–315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.

Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.

Khuller, Samir, Anna Moss, and Joseph (Sefﬁ) Naor.
1999. The budgeted maximum coverage problem.
Inf. Process. Lett., 70(1):39–45.

Kulik, Ariel, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints.
In SODA ’09, pages
545–554.

Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
’95, pages 68–73, New York, NY, USA. ACM.

Manning, Christopher D. and Hirich Schutze. 2002.
Foundations of Statistical Natural Language Pro-
cessing.
The MIT Press, Cambridge, Mas-
sachusetts, London, England.

Mei, Qiaozhu and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientiﬁc literature.
In Proceedings of ACL ’08, pages 816–824.

Mohammad, Saif, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using
citations to generate surveys of scientiﬁc paradigms.
In NAACL 2009, pages 584–592, June.

