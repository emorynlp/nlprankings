










































Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250–253,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Extending the METEOR Machine Translation Evaluation Metric to the
Phrase Level

Michael Denkowski and Alon Lavie
Language Technologies Institute

School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA

{mdenkows,alavie}@cs.cmu.edu

Abstract

This paper presents METEOR-NEXT, an ex-
tended version of the METEOR metric de-
signed to have high correlation with post-
editing measures of machine translation qual-
ity. We describe changes made to the met-
ric’s sentence aligner and scoring scheme as
well as a method for tuning the metric’s pa-
rameters to optimize correlation with human-
targeted Translation Edit Rate (HTER). We
then show that METEOR-NEXT improves cor-
relation with HTER over baseline metrics, in-
cluding earlier versions of METEOR, and ap-
proaches the correlation level of a state-of-the-
art metric, TER-plus (TERp).

1 Introduction

Recent focus on the need for accurate automatic
metrics for evaluating the quality of machine trans-
lation output has spurred much development in the
field of MT. Workshops such as WMT09 (Callison-
Burch et al., 2009) and the MetricsMATR08 chal-
lenge (Przybocki et al., 2008) encourage the devel-
opment of new MT metrics and reliable human judg-
ment tasks.

This paper describes our work extending the ME-
TEOR metric to improve correlation with human-
targeted Translation Edit Rate (HTER) (Snover et
al., 2006), a semi-automatic post-editing based met-
ric which measures the distance between MT out-
put and a targeted reference. We identify several
limitations of the original METEOR metric and de-
scribe our modifications to improve performance on
this task. Our extended metric, METEOR-NEXT, is

then tuned to maximize segment-level correlation
with HTER scores and tested against several base-
line metrics. We show that METEOR-NEXT outper-
forms earlier versions of METEOR when tuned to the
same HTER data and approaches the performance of
a state-of-the-art TER-based metric, TER-plus.

2 The METEOR-NEXT Metric

2.1 Traditional METEOR Scoring

Given a machine translation hypothesis and a refer-
ence translation, the traditional METEOR metric cal-
culates a lexical similarity score based on a word-
to-word alignment between the two strings (Baner-
jee and Lavie, 2005). When multiple references are
available, the hypothesis is scored against each and
the reference producing the highest score is used.
Alignments are built incrementally in a series of
stages using the following METEOR matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database. This
matcher is limited to translations into English.

At each stage, one of the above matchers iden-
tifies all possible word matches between the two
translations using words not aligned in previous
stages. An alignment is then identified as the largest
subset of these matches in which every word in each
sentence aligns to zero or one words in the other sen-

250



tence. If multiple such alignments exist, the align-
ment is chosen that best preserves word order by
having the fewest crossing alignment links. At the
end of each stage, matched words are fixed so that
they are not considered in future stages. The final
alignment is defined as the union of all stage align-
ments.

Once an alignment has been constructed, the to-
tal number of unigram matches (m), the number of
words in the hypothesis (t), and the number of words
in the reference (r) are used to calculate precision
(P = m/t) and recall (R = m/r). The parame-
terized harmonic mean of P and R (van Rijsbergen,
1979) is then calculated:

Fmean =
P ·R

α · P + (1 − α) ·R
To account for differences in word order, the min-

imum number of “chunks” (ch) is calculated where a
chunk is defined as a series of matched unigrams that
is contiguous and identically ordered in both sen-
tences. The fragmentation (frag = ch/m) is then
used to calculate a fragmentation penalty:

Pen = γ · fragβ

The final METEOR score is then calculated:

Score = (1 − Pen) · Fmean
The free parameters α, β, and γ can be tuned to

maximize correlation with various types of human
judgments (Lavie and Agarwal, 2007).

2.2 Extending the METEOR Aligner
Traditional METEOR is limited to unigram matches,
making it strictly a word-level metric. By focus-
ing on only one match type per stage, the aligner
misses a significant part of the possible alignment
space. Further, selecting partial alignments based
only on the fewest number of per-stage crossing
alignment links can in practice lead to missing full
alignments with the same number of matches in
fewer chunks. Our extended aligner addresses these
limitations by introducing support for multiple-word
phrase matches and considering all possible matches
in a single alignment stage.

We introduce an additional paraphrase matcher
which matches phrases (one or more successive

words) if one phrase is considered a paraphrase of
the other by a paraphrase database. For English, we
use the paraphrase database developed by Snover et
al. (2009), using techniques presented by Bannard
and Callison-Burch (2005).

The extended aligner first constructs a search
space by applying all matchers in sequence to iden-
tify all possible matches between the hypothesis and
reference. To reduce redundant matches, stem and
synonym matches between pairs of words which
have already been identified as exact matches are not
considered. Matches have start positions and lengths
in both sentences; a word occurring less than length
positions after a match start is said to be covered by
the match. As exact, stem, and synonym matches
will always have length one in both sentences, they
can be considered phrase matches of length one.
Since other matches can cover phrases of different
lengths in the two sentences, matches are now said
to be one-to-one at the phrase level rather than the
word level.

Once all possible matches have been identified,
the aligner identifies the final alignment as the
largest subset of these matches meeting the follow-
ing criteria in order of importance:

1. Each word in each sentence is covered by zero
or one matches

2. Largest number of covered words across both
sentences

3. Smallest number of chunks, where a chunk is
now defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences

4. Smallest sum of absolute distances between
match start positions in the two sentences (pre-
fer to align words and phrases that occur at sim-
ilar positions in both sentences)

The resulting alignment is selected from the full
space of possible alignments and directly optimizes
the statistics on which the the final score will be cal-
culated.

2.3 Extended METEOR Scoring
Once an alignment has been chosen, the METEOR-
NEXT score is calculated using extended versions of

251



the traditional METEOR statistics. We also introduce
a tunable weight vector used to dictate the relative
contribution of each match type. The extended ME-
TEOR score is calculated as follows.

The number of words in the hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply the appropriate module
weight (wi). The weighted Precision and Recall are
then calculated:

P =
∑
iwi ·mi(t)

t
R =

∑
iwi ·mi(r)

r

The minimum number of chunks (ch) is then cal-
culated using the new chunk definition. Once P , R,
and ch are calculated, the remaining statistics and
final score can be calculated as in Section 2.1.

3 Tuning for Post-Editing Measures of
Quality

Human-targeted Translation Edit Rate (HTER)
(Snover et al., 2006), is a semi-automatic assessment
of machine translation quality based on the number
of edits required to correct translation hypotheses. A
human annotator edits each MT hypothesis so that it
is meaning-equivalent with a reference translation,
with an emphasis on making the minimum possible
number of edits. The Translation Edit Rate (TER)
is then calculated using the human-edited transla-
tion as a targeted reference for the MT hypothe-
sis. The resulting scores are shown to correlate well
with other types of human judgments (Snover et al.,
2006).

3.1 Tuning Toward HTER
The GALE (Olive, 2005) Phase 2 unsequestered
data includes HTER scores for multiple Arabic-to-
English and Chinese-to-English MT systems. We
used HTER scores for 10838 segments from 1045
documents from this data set to tune both the orig-
inal METEOR and METEOR-NEXT. Both were ex-
haustively tuned to maximize the length-weighted
segment-level Pearson’s correlation with the HTER
scores. This produced globally optimal α, β, and γ
values for METEOR and optimal α, β, γ values plus
stem, synonym, and paraphrase match weights for

Task α β γ
Adequacy & Fluency 0.81 0.83 0.28
Ranking 0.95 0.50 0.50
HTER 0.70 1.95 0.50
HTER (extended) 0.65 1.95 0.45

Stem Syn Par
0 0.4 0.9

Table 1: Parameter values for various METEOR tasks for
translations into English.

METEOR-NEXT (with the weight of exact matches
fixed at 1). Table 1 compares the new HTER pa-
rameters to those tuned for other tasks including ad-
equacy and fluency (Lavie and Agarwal, 2007) and
ranking (Agarwal and Lavie, 2008).

As observed by Snover et al. (2009), HTER
prefers metrics which are more balanced between
precision and recall: this results in the lowest values
of α for any task. Additionally, non-exact matches
receive lower weights, with stem matches receiving
zero weight. This reflects a weakness in HTER scor-
ing where words with matching stems are treated as
completely dissimilar, requiring full word substitu-
tions (Snover et al., 2006).

4 Experiments

The GALE (Olive, 2005) Phase 3 unsequestered
data includes HTER scores for Arabic-to-English
MT output. We created a test set from HTER scores
of 2245 segments from 195 documents in this data
set. Our evaluation metric (METEOR-NEXT-hter)
was tested against the following established metrics:
BLEU (Papineni et al., 2002) with a maximum N -
gram length of 4, TER (Snover et al., 2006), versions
of METEOR based on release 0.7 tuned for adequacy
and fluency (METEOR-0.7-af) (Lavie and Agarwal,
2007), ranking (METEOR-0.7-rank) (Agarwal and
Lavie, 2008), and HTER (METEOR-0.7-hter). Also
included is the HTER-tuned version of TER-plus
(TERp-hter), a metric with state-of-the-art perfor-
mance in recent evaluations (Snover et al., 2009).
Length-weighted Pearson’s and Spearman’s correla-
tion are shown for all metrics at both the segment
(Table 2) and document level (Table 3). System level
correlations are not shown as the Phase 3 data only
contained the output of 2 systems.

252



Metric Pearson’s r Spearman’s ρ
BLEU-4 -0.496 -0.510
TER 0.539 0.510
METEOR-0.7-af -0.573 -0.561
METEOR-0.7-rank -0.561 -0.556
METEOR-0.7-hter -0.574 -0.562
METEOR-NEXT-hter -0.600 -0.581
TERp-hter 0.627 0.610

Table 2: Segment level correlation with HTER.

Metric Pearson’s r Spearman’s ρ
BLEU-4 -0.689 -0.686
TER 0.675 0.679
METEOR-0.7-af -0.696 -0.699
METEOR-0.7-rank -0.691 -0.693
METEOR-0.7-hter -0.704 -0.705
METEOR-NEXT-hter -0.719 -0.713
TERp-hter 0.738 0.747

Table 3: Document level correlation with HTER.

METEOR-NEXT-hter outperforms all baseline
metrics at both the segment and document level.
Bootstrap sampling indicates that the segment-level
correlation improvements of 0.026 in Pearson’s r
and 0.019 in Spearman’s ρ over METEOR-0.7-hter
are statistically significant at the 95% level. TERp’s
correlation with HTER is still significantly higher
across all categories. Our metric does run signifi-
cantly faster than TERp, scoring approximately 120
segments per second to TERp’s 3.8.

5 Conclusions

We have presented an extended METEOR metric
which shows higher correlation with HTER than
baseline metrics, including traditional METEOR
tuned on the same data. Our extensions are not
specific to HTER tasks; improved alignments and
additional features should improve performance on
any task having sufficient tuning data. Although our
metric does not outperform TERp, it should be noted
that HTER incorporates TER alignments, providing
TER-based metrics a natural advantage. Our metric
also scores segments relatively quickly, making it a
viable choice for tuning MT systems.

Acknowledgements

This work was funded in part by NSF grants IIS-
0534932 and IIS-0915327.

References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu

and m-ter: Evaluation Metrics for High-Correlation
with Human Rankings of Machine Translation Output.
In Proc. of WMT08, pages 115–118.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72.

Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05, pages 597–604.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of WMT09, pages 1–28.

Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. In Proc. of
WMT07, pages 228–231.

George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.

Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL02,
pages 311–318.

Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.

M. Przybocki, K. Peterson, and S Bronsart. 2008.
Official results of the NIST 2008 "Metrics for
MAchine TRanslation" Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA-2006, pages 223–231.

Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with a
Tunable MT Metric. In Proc. of WMT09, pages 259–
268.

C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.

253


