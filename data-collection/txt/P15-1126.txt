



















































Orthogonality of Syntax and Semantics within Distributional Spaces


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1301–1310,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Orthogonality of Syntax and Semantics within Distributional Spaces

Jeff Mitchell
School of Informatics

University of Edinburgh
Edinburgh, EH8 9AB, UK

jeff.mitchell@ed.ac.uk

Mark Steedman
School of Informatics

University of Edinburgh
Edinburgh, EH8 9AB, UK

steedman@inf.ed.ac.uk

Abstract

A recent distributional approach to word-
analogy problems (Mikolov et al., 2013b)
exploits interesting regularities in the
structure of the space of representations.
Investigating further, we find that per-
formance on this task can be related to
orthogonality within the space. Explic-
itly designing such structure into a neu-
ral network model results in represen-
tations that decompose into orthogonal
semantic and syntactic subspaces. We
demonstrate that learning from word-order
and morphological structure within En-
glish Wikipedia text to enable this de-
composition can produce substantial im-
provements on semantic-similarity, pos-
induction and word-analogy tasks.

1 Introduction

Distributional methods have become widely used
across computational linguistics. Recent applica-
tions include predicate clustering for question an-
swering (Lewis and Steedman, 2013), bilingual
embeddings for machine translation (Zou et al.,
2013) and enhancing the coverage of POS tag-
ging (Huang et al., 2013). The popularity of these
methods, stemming from their conceptual simplic-
ity and wide applicability, motivates a deeper anal-
ysis of the structure of the representations they
produce.

Commonly, these representations are made in a
single vector space with similarity being the main
structure of interest. However, recent work by
Mikolov et al. (2013b) on a word-analogy task
suggests that such spaces may have further use-
ful internal regularities. They found that seman-
tic differences, such as between big and small,
and also syntactic differences, as between big and
bigger, were encoded consistently across their

space. In particular, they solved the word-analogy
problems by exploiting the fact that equivalent re-
lations tended to correspond to parallel vector-
differences.

In this paper, we investigate orthogonality be-
tween relations rather than parallelism. While par-
allelism serves to ensure that the same relation
is encoded consistently, our hypothesis is that or-
thogonality serves to ensure that distinct relations
are clearly differentiable. We focus specifically
on semantic and syntactic relations as these are
probably the most distinct classes of properties en-
coded in distributional spaces.

Empirically, we demonstrate that orthogonal-
ity predicts performance on the word-analogy task
for three existing approaches to constructing word
vectors. We also attempt to enhance the weak-
est of these three models by imposing an orthog-
onal structure in its construction. In these exten-
sions, word representations decompose into or-
thogonal semantic and syntactic spaces, and we
use word-order and morphology to drive this sep-
aration. This decomposition also allows us to de-
fine a novel approach to solving the word-analogy
problems and our extended models become com-
petitive with the other two original models. In
addition, we show that the separate semantic and
syntactic sub-spaces gain improved performance
on semantic-similarity and POS-induction tasks
respectively.

Our experiments here are based on models that
construct vector-representations within a model
that predicts the occurence of words in context. In
particular we focus on the CBOW and Skip-gram
models of Mikolov etal. (2013b) and Pennington
et al.’s (2014) GloVe model. These models share
the property of producing a single general repre-
sentation for each word, which can be utilized in
a variety of tasks, from POS tagging to semantic
role labelling. In contrast, here we attempt to de-
compose the representations into separate seman-

1301



●

●

●

●

small

smaller

big

bigger

Figure 1: Geometric relationships between small,
smaller, big and bigger.

tic and syntactic components.

To motivate this decomposition, consider the
analogical reasoning task that Mikolov et al.
(2013b) apply neural embeddings to. In this task,
given vectors for the words big, bigger and small,
we try to predict the vector for smaller. They
find that in practice smaller ≈ small+ bigger−
big produces an estimate that is frequently closer
to the actual representation of smaller than any
other word vector. We can think of the vector
bigger − big as representing the syntactic rela-
tion that holds between an adjective and its com-
parative. Adding this syntactic structure to small
thus ends up at, or near, the relevant comparative,
smaller. Alternatively, we could think of the vec-
tor small−big as representing the semantic differ-
ence between small and big, and adding this rela-
tion to bigger produces a semantic transformation
to smaller.

Mikolov et al. (2013b) represent these sort of
relations in terms of a diagram similar to Figure 1.
The image places the four words in a 2D space and
represents the relations between them in terms of
arrows. The solid black arrows represent the syn-
tactic relations smaller−small and bigger−big,
while the gray dashed arrows represent the seman-
tic differences smaller−bigger and small−big.
Their solution to the analogy problem exploits the
fact that these pairs of relations are approximately
parallel to each other, i.e. that we can approx-
imate smaller − small with bigger − big, or
smaller − bigger with small − big. However,
knowing that opposite sides of the square in Fig-
ure 1 are parallel to each other still leaves open

the question of what happens at the corners. In
other words, what is the relationship between the
semantic differences, e.g. smaller − bigger, and
the syntactic differences, e.g. smaller − small?

In this paper we explore the idea that such se-
mantic and syntactic relations ought to be orthogo-
nal to each other. This hypothesis arises both from
the intuition that such distinct types of informa-
tion ought to be represented distinctly within our
space and also from the observation that solving
the word-analogy task requires that words can be
uniquely identified by combining these vector dif-
ferences and so small − big ought to be easily
differentiable from bigger− big as these relations
point to different end results starting from big. Es-
sentially, orthogonality will make better use of the
volume within the space, spreading words with
different semantic or syntactic characteristics fur-
ther from each other.

In terms of predicting smaller from big, bigger
and small, orthogonality of the relationship be-
tween smaller − bigger and smaller − small
can be expressed in terms of their dot product:

(smaller−bigger)·(smaller−small) = 0 (1)
If all semantic relations were genuinely orthog-

onal to all syntactic relations, then their space
would be decomposable into two orthogonal sub-
spaces: one semantic, the other syntactic. Any
word representation, v, would then be the combi-
nation of a unique semantic vector, b, within the
semantic subspace and a unique syntactic vector,
s, within the syntactic subspace. If b were given a
representation in terms of e components, and s in
terms of f components, then v would have a repre-
sentation in terms of d = e+f components which
would just be the concatenation of the two sets of
components, which we will represent in terms of
the operator ⊕.

v = b⊕ s (2)
Achieving this differentiation within the repre-

sentations requires that the model have a means
of differentiating semantic and syntactic informa-
tion in the raw text. We consider two very simple
approaches for this purpose, based on morpholog-
ical and word order features. Both these types of
features have been previously employed in simple
word co-occurrence models (e.g., McDonald and
Lowe, 1998; Clark, 2003), with bag-of-words and

1302



b−2

wt−2

b−1

wt−1

b1

wt+1

b2

wt+2

�
�
�
��

A
A
A
AK

�
�
�
�
�
�3

Q
Q

Q
Q

Q
Qk

bcontext

6

wt

Figure 2: CBOW model predicting wt from of a
bag-of-words representation, bcontext, of a 4-word
window around it.

lemmatization being good for semantic applica-
tions, while sequential order and suffixes is more
useful for syntax. More recently, Mitchell (2013)
demonstrated that word order could be used to sep-
arate syntactic from semantic structure, but only
within a simple bigram language model, rather
than a neural network model, and without exploit-
ing morphology.

Our enhanced models are based on Mikolov
et al.’s (2013a) CBOW architecture, which is de-
scribed in Section 2. The novel extensions to
it, employing a semantic-syntactic decomposition,
are proposed in Section 3. We then describe our
evaluation tasks and provide their results in Sec-
tions 5 and 6 respectively. These evaluations are
based on the word-analogy dataset of Mikolov et
al. (2013b), a noun-verb similarity task (Mitchell,
2013) and a POS clustering task.

2 Continuous Bag-of-Words Model
(CBOW)

In the original CBOW model, the probability of a
central target word, wt, is predicted from a bag-
of-words representation of the context it occurs in,
as illustrated in Figure 2. This context representa-
tion, bcontext, is a simple sum of the CBOW vec-
tors, bi, that represent each item,wt+i, in a k-word
window either side of the target.

bcontext =
k∑

i=−k,i6=0
bi (3)

For speed, the output layer uses a hierarchi-
cal softmax function (Morin and Bengio, 2005).

Each word is given a Huffman code correspond-
ing to a path through a binary tree, and the output
predicts the binary choices on nodes of the tree
as independent variables. In comparison to the
computational cost of doing the full softmax over
the whole vocabulary, this hierarchical approach is
much more efficient.

Each node is associated with a vector, n, and
the output at that node, given a context vector,
bcontext, is:

p = logistic(n · bcontext) (4)
Here, p is the probability of choosing 1 over 0

at this node of the tree, or equivalently finding a 1
in the Huffman code of wt at the relevant position.

The objective function is the negative log-
likelihood of the data given the model.

O =
∑
− log(p) (5)

Where the sum is over tokens in the training cor-
pus and the relevant nodes in the tree. Training is
then based on stochastic gradient descent, with a
decreasing learning rate.

3 Extensions

3.1 Continuous Sequence of Words (CSOW)

A major feature of the CBOW model is its use of
a bag-of-words representation of the context and
this is achieved by summing over the vectors rep-
resenting words in the input. Although the model
does seem to produce representations that are ef-
fective on both semantic and syntactic tasks, we
want to be able to exploit word order information
to separate these two characteristics. We therefore
need to consider models which do not reduce the
context to a structureless bag-of-words. Modify-
ing the original model to retain the sequential in-
formation in the input is relatively straightforward.
Instead of summing the input representations, we
simply leave them as an ordered sequence of vec-
tors, si.

Then in the output layer, we require a vector for
every input position, i, on every node. In this way,
the output of the network depends on which con-
text word is in which position, rather than just the
set of words, irrespective of position in the input.

The network still learns a single representation
for each word independently of position, but the
output function has more parameters.

1303



p = logistic(
k∑

i=−k,i6=0
ni · si) (6)

Here each node of the tree is associated with one
vector, ni, for each position, i, in the input context,
giving 2k vectors in total at each node.

3.2 Continuous Bag and Sequence of Words
(CBSOW)

Having introduced a sequential version of the
CBOW model, what is really desired is a model
that combines both bag and sequence components.
Each word will have both an e-dimensional bag-
vector b and an f -dimensional sequence-vector s.
The full representation of a word, v, is then the
concatenation of the components of b and s.

Given this structure, the representation of a con-
text of 2k words will be made up of the sum,
bcontext, of their bag vectors, bi, as in the CBOW
model given by Equation 3, along with the ordered
sequence vectors, si, as in the CSOW model. Each
node in the tree then requires both a bag vector, nb,
to handle the bag context, and 2k sequence vec-
tors, nsi , to handle the sequence context vectors,
with probabilities given by:

p = logistic(nb · bcontext +
k∑

i=−k,i6=0
nsi · si) (7)

3.3 Continuous Bag of Morphemes (CBOM)

A second source of information which might be
used to differentiate semantic from syntactic rep-
resentations is morphology. Specifically, English
has the useful characteristic that the written words
themselves can often be broken into a semantic
stem on the left and a syntactic ending on the right.
For example, dancing = dance + ing and swim-
mer = swim + er. In fact, stemming or lemma-
tization is commonly used in constructing distri-
butional vectors precisely because throwing away
the syntactic information helps to enhance their se-
mantic content. Here, we want to use both the left
and right halves separately to enhance both the se-
mantic and syntactic components of the represen-
tations.

Our starting point is to break each word into
a left-hand stem and a right-hand ending using
CELEX (Baayen et al., 1995), as explained in
more detail in Section 4.1.

The simplest model is then to represent each of
these with its own vector, li and ri respectively,
and sum these vectors to form context representa-
tions of words in the input.

lcontext =
k∑

i=−k,i6=0
li (8)

rcontext =
k∑

i=−k,i6=0
ri (9)

The output function takes much the same form
as the original model but now each node needs
both a left and a right vector, corresponding to the
two context representations.

p = logistic(nl · lcontext + nr · rcontext) (10)

3.4 Continuous Bag and Sequence of Words
and Morphemes (CBSOWM)

Finally, we want to incorporate all these elements
in a single model, with the morphological and
word order elements of the model working in har-
mony. In particular, we want the sequential part
of the model to be guided by morphological infor-
mation without being constrained to give all words
with same ending the same representation. Our
solution is to add a constraint term to the objec-
tive function, which penalizes sequence vectors
that stray far from the relevant morphological rep-
resentation. The bag vectors, in contrast, are de-
termined directly by the left hand stems, with all
words having the same stem then sharing the same
bag vector, b = l.

The main structure of the model remains as in
the CBSOW model, with the context being rep-
resented by the sum of bag vectors alongside the
ordered sequence vectors. Output probabilities are
as given by Equation 7, and we add a morpholog-
ical penalty, m, to the objective function.

m =
k∑

i=−k,i6=0

1
2
λ|si − ri|2 (11)

The morphological representations r enter into
the model only through the penalty term, and they
adapt during training solely in terms of this in-
teraction with the sequence vectors. Gradient de-
scent results in the r vectors moving towards the
centre of the corresponding s vectors, and the s
vectors in turn being drawn towards that centre.

1304



The result is to elastically connect all the s vec-
tors corresponding to a single morphological ele-
ment through their r vectors, so that they are drawn
together, but can still develop idiosyncratically if
there is sufficient evidence in the data.

3.5 Application to the Word-Analogy Task

Decomposition of representations into separate se-
mantic and syntactic spaces enables us to utilise a
new approach to solving the word-analogy prob-
lems. Rather than using vector differences to
predict a vector, we can instead construct it by
copying the relevant bag and sequence vectors.
So, since small and smaller share very similar
semantic content, we can use the bag vector of
small as the bag vector of smaller, since that is
where the semantic content is mainly represented:
bsmaller ≈ bsmall. Similarly, we can use the se-
quence vector of bigger as the sequence vector for
smaller, since these words share common syntac-
tic behaviour: ssmaller ≈ sbigger.

The predicted representation of smaller is then
given by the concatenation of the components.

vsmaller ≈ bsmall ⊕ sbigger (12)
We find that this gives the best performance on

the models that use word-order features (CBSOW
and CBSOWM).

4 Training

Our experiments are based on the publicly avail-
able word2vec1 and GloVe2 packages. We mod-
ified the original CBOW code to incorporate the
CBSOW, CBOM and CBSOWM extensions de-
scribed above, and trained models on three En-
glish Wikipedia corpora of varying sizes, includ-
ing the enwik8 and enwik9 files3 suggested in the
word2vec documentation, containing the first 108

and 109 characters of a 2006 download, and also
a full download from 2009. On the smallest 17M
word corpus we explored a range of vector dimen-
sionalities from 10 to 1000. On the larger 120M
and 1.6B word corpus, we trained extended mod-
els with a 200-dimensional semantic component
and a 100-dimensional syntactic component com-
paring to 300-dimensional CBOW, Skip-gram and
GloVe models. The parameter, λ, in Equation 11
was set to 0.1 and the recommended window sizes

1https://code.google.com/p/word2vec/
2http://nlp.stanford.edu/projects/glove/
3http://mattmahoney.net/dc/text.html

of 5, 10 and 15 words either side of the central
word were used as context for the CBOW, Skip-
gram and GloVe models respectively.

4.1 CELEX

We attempted to split all the words in the training
data into a left hand and a right hand using CELEX
(Baayen et al., 1995), an electronic dictionary con-
taining morphological structure. In the cases of
words that were not found in the dictionary and
also those that were found but had no morpholog-
ical substructure, the left hand was just the whole
word and the right hand was a −NULL− token.
For the remaining words, we treated short suf-
fixes as being syntactic inflections and stripped all
these off to leave a left hand ‘semantic’ compo-
nent. The ‘syntactic’ component was then right-
most of these suffixes, with any additional suffixes
being ignored.

5 Evaluation

The hypothesis that orthogonality is useful to word
vector representations is investigated empirically
in two ways. Firstly, we attempt to quantify the
orthogonality that is already implicitly present in
the original CBOW, Skip-gram and GloVe repre-
sentations and relate that to their success in the
word-analogy task. Secondly, the extensions de-
scribed above are evaluated on a number of tasks
in order to evaluate the benefits of their explicit
orthogonality between components.

5.1 Orthogonality within the Original Models

Equation 1 relates orthogonality of vector differ-
ences to their dot product being zero, which cor-
responds to the fact the cosine of 90◦ is zero.
Thus, we can use the cosine as a quantifica-
tion of how close to orthogonal the vector dif-
ferences are and then relate that to performance
on the word-analogy dataset distributed with the
word2vec toolkit.

That task involves predicting a word vector
given vectors for other related words. So, for ex-
ample, given vectors for big, bigger and small,
we would try to predict a vector for smaller. We
then judge the success of this prediction in terms
of whether the predicted vector is in fact closer to
smaller’s actual word vector than any other word
vector. The dataset contains 19,544 items, bro-
ken down into 14 subtasks (e.g. capitals of com-
mon countries or adjective to adverb conversion).

1305



●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

● ●

●

●●

0.0 0.2 0.4 0.6

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

Cosine

P
ro

po
rt

io
n 

C
or

re
ct

● CBOW
Skip−Gram
GLOVE

Figure 3: Proportion Correct against Average Co-
sine.

For each item, we measure the cosine of the an-
gle between the vector differences for the word
we are trying to predict (e.g. smaller − small
and smaller − bigger) and analyze these values
in terms of the success of the model’s prediction,
with smaller cosine values corresponding to angles
that are closer to orthogonal.

5.2 CBOW Extensions
We evaluate the extensions on three tasks. Along-
side the word-analogy problems, we also evalu-
ate the separate semantic and syntactic sub-spaces
on their own individual tasks. The semantic task
correlates predicted semantic similarities with the
noun-verb similarity ratings gathered by Mitchell
(2013), and the remaining task clusters the syntac-
tic representations and evaluates these clusters in
relation to the POS classes found in the Penn Tree-
bank.

On the word-analogy problem we compare to
the original CBOW, Skip-gram and GloVe mod-
els. In the case of these original models and
also the CBOM model, we follow Mikolov et
al.’s (2013b) method for making the word-analogy
predictions in terms of addition and subtraction:
smaller ≈ bigger − big + small. However, in
the case of the CBSOW and CBSOWM models,
we use the novel approach described in Section
3.5: vsmaller ≈ bsmall⊕ sbigger. Similarity is then
based on the cosine measure for all types of repre-
sentation.

The noun-verb similarity task is based on cor-
relating the model’s predicted semantic similarity
for words with human ratings gathered in an on-

● ● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
● ●

0.0 0.2 0.4 0.6

0
50

0
10

00
15

00
20

00

Cosine

F
re

qu
en

cy

● ● ●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
● ●

● CBOW
Skip−Gram
GLOVE

Figure 4: Frequency against Cosine.

line experiment. Such evaluations have been com-
monly used to evaluate distributional representa-
tions, with higher correlations indicating a model
which is more effective at forming vectors whose
relations to each other mirror human notions of se-
mantic similarity. Mitchell (2013) argued that pre-
dicting semantic similarity relations across syntac-
tic categories provided a measure of the extent to
which word representations succeed in separating
semantic from syntactic content, and gathered a
dataset of similarities for noun-verb pairs. Each
rated item consists of a noun paired with a verb,
and the pairs are constucted to range from high se-
mantic similarity, e.g. disappearance - vanish, to
low, e.g. transmitter - grieve. The dataset contains
ratings for 108 different pairs, each of which was
rated by 20 participants. For the CBOW model,
we predict similarities in terms of the cosine mea-
sure for the two word vectors. For the other mod-
els, we predict similarities from cosine applied to
just the bag or left-hand vectors.

The syntactic component of the representations
is evaluated by clustering the vectors and then
comparing the induced classes to the POS classes
found in the Penn Treebank. We use the many-
to-one measure (Christodoulopoulos et al., 2010;
Yatbaz et al., 2012) to determine the extent to
which the clusters agree with the POS classes.
Each cluster is mapped to its most frequent gold
tag and the reported score is the proportion of
word tokens correctly tagged using this mapping.
The clustering itself is a form of k-means cluster-
ing, where similarity is measured in terms of the
cosine measure. Each vector is assigned to a clus-

1306



●

●

● ● ●

● ●

10 20 50 100 200 500 1000

0.
0

0.
1

0.
2

0.
3

0.
4

0.
5

0.
6

Dimensions

A
ve

ra
ge

 C
or

re
la

tio
n

●

●

●

●

●
●

●

●

●

CBOW
CBSOW
CBOM
CBSOWM

Figure 5: Average Correlation on Noun-Verb
Evaluation Task against Size of Representations.

ter based on which cluster centroid it is most sim-
ilar to and then the cluster centroids are updated
given the new cluster assignments and the process
repeats. This clustering was applied to either the
sequence or right-hand vectors in the case of the
CBSOW, CBOM and CBSOWM models, and to
the whole vectors in the case of CBOW. We ran-
domly initialized 45 clusters and then evaluated af-
ter 100 iterations of the k-means algorithm.

6 Results

6.1 Original Models

Figure 3 is a plot of the proportion of correct pre-
dictions made by 100-dimensional CBOW, Skip-
Gram and GloVe models on the word-analogy task
against cosine of the angle between the vector dif-
ferences. The range of the cosine distribution was
broken into twenty intervals and the plotted values
were derived by calculating the proportion correct
and average cosine value within each interval. It
is clear from the resulting curves that cosine is a
fairly strong predictor for all models of whether
the model gets a word-analogy item correct, with
higher rates of success for smaller cosine values
- i.e. angles closer to orthogonality. This is con-
firmed by a significant (p < 0.001) result from
a logistic regression of correctness against cosine
value. Similar results are found for both the se-
mantic subtasks (e.g. capitals of common coun-
tries) and syntactic subtasks (e.g. adjective to ad-
verb conversion) considered separately.

The actual distribution of cosine values for each
type of model is given in Figure 4. This analy-

●
●

● ●
●

●
●

10 20 50 100 200 500 1000

0.
0

0.
1

0.
2

0.
3

0.
4

0.
5

0.
6

0.
7

Dimensions

A
ve

ra
ge

 M
TO

●

●

● ●

● ●

●

●

●

CBOW
CBSOW
CBOM
CBSOWM

Figure 6: Average Many-To-One Evaluation
against Size of Representations.

sis reveals that while the Skip-Gram and GloVe
models have fairly similar cosine distributions, the
CBOW model’s distribution is shifted to the right,
with more angles further from othogonality. This
begs the question of what the effect on perfor-
mance would be if we managed to push more of
the CBOW distribution towards zero, and in the
next section we examine the extensions that im-
plement this idea.

6.2 CBOW Extensions
We first consider the models trained on the smaller
17M word corpus, and the evaluations of these
models on the noun-verb similarity and POS clus-
tering tasks are presented in Figures 5 and 6 re-
spectively. These graphs depict the performance
as the representations grow in size. For the CBOW
model, this is just the dimension of the induced
vectors. For the other models, we consider mod-
els with equal sizes of semantic and syntactic sub-
spaces and report performance against the total di-
mensionality of the combined representation. For
both these tasks, the results were averaged over ten
repetitions of training with random initializations.

On the noun-verb similarity task, morphol-
ogy produces the largest performance gains, with
the CBOM model substantially outperforming the
CBOW model. Word order structure has no clear
impact.

On the syntactic task, in contrast, it is word or-
der that produces reliable gains, with the CBSOW
model clearly improving on the CBOW model.
The simplistic use of morphology in the CBOM
model results in a degradation of performance in

1307



●

●

●

●

● ●
●

10 20 50 100 200 500 1000

0.
0

0.
1

0.
2

0.
3

0.
4

0.
5

0.
6

Dimensions

P
ro

po
rt

io
n 

C
or

re
ct

●

●

●

●

●
●

●

●

●

CBOW
CBSOW
CBOM
CBSOWM

Figure 7: Proportion Correct on the Analogy Task
against Size of Representations.

comparison to the CBOW model, but the CB-
SOWM model’s performance is comparable to
that of the CBSOW for larger representations.

Thus for these two tasks, the CBSOWM re-
sults appear to show a reasonable integration of
morphology and word order information giving
good performance on both semantic and syntac-
tic tasks. This conclusion is borne out the results
of the word-analogy tasks in Figure 7, where the
CBSOWM model outperforms all the other mod-
els. Here, morphology gives the greatest benefit
on its own, as evidenced in the differences be-
tween the CBOW and CBOM models. Nonethe-
less, word order still produces noticeable improve-
ments, with the CBSOW result beating the CBOW
results, and the CBSOWM beating the CBOM at
larger dimensions. There is considerable variation
in the effects on performance among the various
analogy subtasks, but even a task such as capi-
tals of common countries (e.g. predicting Iraq as
having Baghdad as its capital, given that Greece
has Athens) appears to benefit from decomposi-
tion of representations, despite not obviously in-
volving syntactic structure.

Table 1 compares 300-dimensional models
across different sizes of training data. In the
case of the CBSOW, CBOM and CBSOWM mod-
els we use representations with 200 semantic and
100 syntactic dimensions and compare these to
CBOW, Skip-gram and GloVe models of the same
total size. It is clear for all quantities of train-
ing data that all the extensions outperform the ba-
sic CBOW model, with morphology giving greater

Training Words
Model 17M 120M 1.6B
GloVe 29.53% 58.18% 72.54%
Skip-Gram 30.03% 52.67% 62.34%
CBOW 18.47% 38.48% 54.17%
CBSOW 20.83% 42.00% 59.41%
CBOM 44.29% 53.60% 61.87%
CBSOWM 48.92% 63.19% 68.32%

Table 1: Performance of 300-Dimensional Models
on the Word-Analogy Task

gains than word order, and the combined CB-
SOWM model outperforming both. This perfor-
mance advantage of the CBOM over CBSOW ap-
pears to weaken as the training data grows, which
is probably the effect of both the lack of morpho-
logical information for rare words encountered in
the larger datasets and also the diminishing returns
on that information as more data provides better
supervision of the training process. The sequen-
tial information, in contrast, is internal to the train-
ing data and seems to provide the same, or greater,
performance boost as the training set grows.

Comparing the results of our extended models
to the Skip-gram and GloVe models, we can see
that on the two smaller corpora CBSOWM outper-
forms both these models, while on the largest cor-
pus, it only beats the Skip-gram results and GloVe
achieves the best performance. Of course, nei-
ther the Skip-gram nor GloVe models has access to
the morphological information that the CBSOWM
model uses, but the results demonstrate that the
performance of the CBOW model can be sub-
stantially boosted by exploiting a representational
structure that decomposes into semantic and syn-
tactic sub-spaces. Similar methods could in prin-
ciple be applied to most word embedding models,
including Skip-gram and GloVe.

We can also examine the distribution of cosine
values for the new models. Figure 8 compares
the distribution of cosine values for CBOW, CB-
SOW, CBOM and CBSOWM models. Although,
in comparison to the original CBOW model, each
of the extended models shifts the distribution to-
wards zero, i.e. towards orthogonality, this shift
for the CBSOW model is marginal. In contrast,
the CBOM model has a large number of instances
where the cosine is exactly zero, corresponding
to cases where all of the relevant morphological
information is found in CELEX. The remainder

1308



●

●

● ●
●

●

●

●

●

●

●

●

●

●

●
● ● ●

●
●● ● ●

●

●

●

●

●

●

●

●

●

●
● ●● ●

●

●

●

●

●

●

●

●

●

●
● ●● ●

●

●

●

●

●
● ● ●

●

●

●

●

●
●

0.0 0.2 0.4 0.6 0.8

0
10

00
20

00
30

00
40

00

Cosine

F
re

qu
en

cy

● ● ●
●

●

●

●

●

●

●

●

●

●
● ●

● CBOW
CBOM
CBSOW
CBSOWM

Figure 8: Frequency against Cosine.

of the data, however, seems to be less orthogo-
nal than the original CBOW distribution, suggest-
ing that words without a morphological analysis
need a more sophisticated treatment. The shift in
the CBSOWM distribution, in comparison, is less
radically bimodal, with more continuity between
those words with and without morphology. This
reflects the difference in these models handling of
suffixes, with the CBSOWM model’s greater flex-
ibility resulting in gains over the CBOM model on
the POS induction and word analogy tasks.

7 Conclusions

Our experiments demonstrate the utility of orthog-
onality within vector-space representations in a
number of ways. In terms of existing models,
we find that the cosines of vector-differences is
a strong predictor of the performance of CBOW,
Skip-gram and GloVe representations on the word
analogy task, with smaller cosine values - corre-
sponding to angles closer to orthogonality - being
associated with a greater proportion of correct pre-
dictions. With regard to developing new models,
this orthogonality of relationships inspired three
models which used word-order and morphology
to separate semantic and syntactic representations.
These separate sub-spaces were shown to have
enhanced performance in semantic similarity and
POS-induction tasks and the combined representa-
tions showed enhanced performance on the word-
analogy task, using a novel approach to solving
this problem that exploits the decomposable struc-
ture of the representations.

Both Botha and Blunsom (2014) and Luong et

al. (2013) take a more sophisticated approach to
morphology4, constructing a word’s embedding
by recursively combining representations of all
its morphemes, though only within a single non-
decomposed space. Future work ought to pursue
models in which all morphemes contribute both
semantic and syntactic content to the word repre-
sentations.

It would also be desirable to explore more prac-
tical applications of these representations than the
limited evaluations presented here. It seems fea-
sible that our decomposition of representations
could benefit tasks that need to differentiate their
treatment of semantic and syntactic content. In
particular, applications of word embeddings that
mainly involve syntax, such as POS tagging (e.g.,
Tsuboi, 2014) or supertagging for parsing (e.g.,
Lewis and Steedman, 2014), may be a reasonable
starting point.

Acknowledgements

We would like to thank Stella Frank, Sharon Gold-
water and other colleagues along with our review-
ers for criticism, advice and discussion. This
work was supported by ERC Advanced Fellow-
ship 249520 GRAMPLUS and EU Cognitive Sys-
tems project FP7-ICT-270273 Xperience.

References
Harald Baayen, Richard Piepenbrock, and Hedderik

van Rijn. 1995. CELEX2 LDC96L14. Web Down-
load. Philadelphia: Linguistic Data Consortium.

Jan A. Botha and Phil Blunsom. 2014. Compositional
Morphology for Word Representations and Lan-
guage Modelling. In Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML),
Beijing, China.

Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised POS induction: How far have we come? In
Proceedings of EMNLP, pages 575–584.

Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting
of the European Association for Computational Lin-
guistics (EACL), pages 59–66.

4Though not neccessarily better performing. Luong et
al.’s published 50-dimensional embeddings trained on 986M
words scored only 13.57% on the word-analogy task, well
behind 40-dimensional CBOM (34.68%) and CBSOWM
(36.71%) models trained on 17M words.

1309



Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,
Yuhong Guo, and Alexander Yates. 2013. Learn-
ing Representations for Weakly Supervised Natural
Language Processing Tasks. Computational Lin-
guistics, 40:85–120.

Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.

Mike Lewis and Mark Steedman. 2014. A* CCG
parsing with a supertag-factored model. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
990–1000, Doha, Qatar, October. Association for
Computational Linguistics.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 104–113,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Scott McDonald and Will Lowe. 1998. Modelling
functional priming and the associative boost. In Pro-
ceedings of the 20th Annual Meeting of the Cogni-
tive Science Society, pages 675–680. Erlbaum.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Jeff Mitchell. 2013. Learning semantic representa-
tions in a bigram language model. In Proceedings
of the 10th International Conference on Computa-
tional Semantics (IWCS 2013) – Short Papers, pages
362–368, Potsdam, Germany, March. Association
for Computational Linguistics.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS05, pages 246–252.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar, October. Association for Computational Lin-
guistics.

Yuta Tsuboi. 2014. Neural networks leverage corpus-
wide information for part-of-speech tagging. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 938–950, Doha, Qatar, October. Association
for Computational Linguistics.

Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 940–951, Jeju
Island, Korea, July. Association for Computational
Linguistics.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393–1398. ACL.

1310


