










































Manifold Learning for the Semi-Supervised Induction of FrameNet Predicates: An Empirical Investigation


Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 7–16,
Uppsala, Sweden, 16 July 2010. c©2010 Association for Computational Linguistics

Manifold Learning for the Semi-Supervised Induction
of FrameNet Predicates: An Empirical Investigation

Danilo Croce and Daniele Previtali
{croce,previtali}@info.uniroma2.it

Department of Computer Science, Systems and Production
University of Roma, Tor Vergata

Abstract

This work focuses on the empirical inves-
tigation of distributional models for the
automatic acquisition of frame inspired
predicate words. While several seman-
tic spaces, both word-based and syntax-
based, are employed, the impact of ge-
ometric representation based on dimen-
sionality reduction techniques is inves-
tigated. Data statistics are accordingly
studied along two orthogonal perspectives:
Latent Semantic Analysis exploits global
properties while Locality Preserving Pro-
jection emphasizes the role of local reg-
ularities. This latter is employed by em-
bedding prior FrameNet-derived knowl-
edge in the corresponding non-euclidean
transformation. The empirical investiga-
tion here reported sheds some light on the
role played by these spaces as complex
kernels for supervised (i.e. Support Vector
Machine) algorithms: their use configures,
as a novel way to semi-supervised lexical
learning, a highly appealing research di-
rection for knowledge rich scenarios like
FrameNet-based semantic parsing.

1 Introduction

Automatic Semantic Role Labeling (SRL) is a
natural language processing (NLP) technique that
maps sentences to semantic representations and
identifies the semantic roles conveyed by senten-
tial constituents (Gildea and Jurafsky, 2002). Sev-
eral NLP applications have exploited this kind of
semantic representation ranging from Information
Extraction (Surdeanu et al., 2003; Moschitti et al.,
2003)) to Question Answering (Shen and Lapata,
2007), Paraphrase Identification (Pado and Erk,
2005), and the modeling of Textual Entailment re-
lations (Tatu and Moldovan, 2005). Large scale

annotated resources have been used by Seman-
tic Role Labeling methods: they are commonly
developed using a supervised learning paradigm
where a classifier learns to predict role labels
based on features extracted from annotated train-
ing data. One prominent resource has been de-
veloped under the Berkeley FrameNet project as
a semantic lexicon for the core vocabulary of En-
glish, according to the so-called frame seman-
tic model (Fillmore, 1985). Here, a frame is a
conceptual structure modeling a prototypical sit-
uation, evoked in texts through the occurrence of
its lexical units (LU) that linguistically expresses
the situation of the frame. Lexical units of the
same frame share semantic arguments. For ex-
ample, the frame KILLING has lexical units such
as assassin, assassinate, blood-bath, fatal, mur-
derer, kill or suicide that share semantic arguments
such as KILLER, INSTRUMENT, CAUSE, VICTIM.
The current FrameNet release contains about 700
frames and 10,000 LUs. A corpus of 150,000 an-
notated examples sentences, from the British Na-
tional Corpus (BNC), is also part of FrameNet.

Despite the size of this resource, it is un-
der development and hence incomplete: several
frames are not represented by evoking words and
the number of annotated sentences is unbalanced
across frames. It is one of the main reason for the
performance drop of supervised SRL systems in
out-of-domain scenarios (Baker et al., 2007) (Jo-
hansson and Nugues, 2008). The limited cover-
age of FrameNet corpus is even more noticeable
for the LUs dictionary: it only contains 10,000
lexical units, far less than the 210,000 entries in
WordNet 3.0. For example, the lexical unit crown,
according to the annotations, evokes the ACCOU-
TREMENT frame. It refers to a particular sense:
according to WordNet, it is “an ornamental jew-
eled headdress signifying sovereignty”. Accord-
ing to the same lexical resource, this LU has 12
lexical senses and the first one (i.e. “The Crown

7



(or the reigning monarch) as the symbol of the
power and authority of a monarchy”) could evoke
other frames, like LEADERSHIP. In (Pennacchiotti
et al., 2008) and (De Cao et al., 2008), the prob-
lem of LU automatic induction has been treated
in a semi-supervised fashion. First, LUs are mod-
eled by exploiting the distributional analysis of an
unannotated corpus and the lexical information of
WordNet. These representations were used in or-
der to find out frames potentially evoked by novel
words in order to extend the FrameNet dictionary
limiting the effort of manual annotations.

In this work the distributional model of LUs
is further developed. As in (Pennacchiotti et al.,
2008), several word spaces (Pado and Lapata,
2007) are investigated in order to find the most
suitable representation of the properties which
characterize a frame. Two dimensionality reduc-
tion techniques are applied here in this context.
Latent Semantic Analysis (Landauer and Dumais,
1997) uses the Singular Value Decomposition to
find the best subspace approximation of the orig-
inal word space, in the sense of minimizing the
global reconstruction error projecting data along
the directions of maximal variance. Locality Pre-
serving Projection (He and Niyogi, 2003) is a
linear approximation of the nonlinear Laplacian
Eigenmap algorithm: its locality preserving prop-
erties allows to add a set of constraints forcing
LUs that belong to the same frame to be near in
the resulting space after the transformation. LSA
performs a global analysis of a corpus capturing
relations between LUs and removing the noise in-
troduced by spurious directions. However it risks
to ignore lexical senses poorly represented into the
corpus. In (De Cao et al., 2008) external knowl-
edge about LUs is provided by their lexical senses
from a lexical resource (e.g WordNet). In this
work, prior knowledge about the target problem is
directly embedded into the space through the LPP
transformation, by exploiting locality constraints.
Then a Support Vector Machine is employed to
provide a robust acquisition of lexical units com-
bining global information provided by LSA and
the local information provided by LPP into a com-
plex kernel function.

In Section 2 related work is presented. In Sec-
tions 3 the investigated distributional model of
LUs is presented as well as the dimensionality re-
duction techniques. Then, in Section 4 the exper-
imental investigation and comparative evaluations

are reported. Finally, in Section 5 we draw final
conclusions and outline future work.

2 Related Work

As defined in (Pennacchiotti et al., 2008), LU in-
duction is the task of assigning a generic lexical
unit not yet present in the FrameNet database (the
so-called unknown LU) to the correct frame(s).
The number of possible classes (i.e. frames) and
the multiple assignment problem make it a chal-
lenging task. LU induction has been integrated
at SemEval-2007 as part of the Frame Seman-
tic Structure Extraction shared task (Baker et al.,
2007), where systems are requested to assign the
correct frame to a given LU, even when the LU is
not yet present in FrameNet. Several approaches
show low coverage (Johansson and Nugues, 2007)
or low accuracy, like (Burchardt et al., 2005). This
task is presented in (Pennacchiotti et al., 2008) and
(De Cao et al., 2008), where two different mod-
els which combine distributional and paradigmatic
(i.e. lexical) information have been discussed. The
distributional model is used to select a list of frame
suggested by the corpus’ evidences and then the
plausible lexical senses of the unknown LU are
used to re-rank proposed frames.

In order to exploit prior information provided
by the frame theory, the idea underlying is that se-
mantic knowledge can be embedded from exter-
nal sources (i.e the FrameNet database) into the
distributional model of unannotated corpora. In
(Basu et al., 2006) a limited prior knowledge is ex-
ploited in several clustering tasks, in term of pair-
wise constraints (i.e., pairs of instances labeled
as belonging to same or different clusters). Sev-
eral existing algorithms enhance clustering qual-
ity by applying supervision in the form of con-
straints. These algorithms typically utilize the
pairwise constraints to either modify the clustering
objective function or to learn the clustering distor-
tion measure. The approach discussed in (Basu et
al., 2006) employs Hidden Markov Random Fields
(HMRFs) as a probabilistic generative model for
semi-supervised clustering, providing a principled
framework for incorporating constraint-based su-
pervision into prototype-based clustering.

Another possible approach is to directly embed
the prior-knowledge into data representations. The
main idea is to employ effective and efficient algo-
rithms for constructing nonlinear low-dimensional
manifolds from sample data points embedded

8



in high-dimensional spaces. Several algorithms
are defined, including Isometric feature mapping
(ISOMAP) (Tenenbaum et al., 2000), Locally Lin-
ear Embedding (LLE) (Roweis and Saul, 2000),
Local Tangent Space alignment (LTSA) (Zhang
and Zha, 2004) and Locality Preserving Projec-
tion (LPP) (He and Niyogi, 2003) and they have
been successfully applied in several computer vi-
sion and pattern recognition problems. In (Yang
et al., 2006) it is demonstrated that basic nonlinear
dimensionality reduction algorithms, such as LLE,
ISOMAP, and LTSA, can be modified by taking
into account prior information on exact mapping
of certain data points. The sensitivity analysis
of these algorithms shows that prior information
improves stability of the solution. In (Goldberg
and Elhadad, 2009), a strategy to incorporate lexi-
cal features into classification models is proposed.
Another possible approach is the strategy pursued
in recent works on deep learning techniques to
NLP tasks. In (Collobert and Weston, 2008) a
unified architecture for NLP that learns features
relevant to the tasks at hand given very limited
prior knowledge is presented. It embodies the
idea that a multitask learning architecture coupled
with semi-supervised learning can be effectively
applied even to complex linguistic tasks such as
Semantic Role Labeling. In particular, (Collobert
and Weston, 2008) proposes an embedding of lex-
ical information using Wikipedia as source, and
exploits the resulting language model for the mul-
titask learning process. The extensive use of unla-
beled texts allows to achieve a significant level of
lexical generalization in order to better capitalize
on the smaller annotated data sets.

3 Geometrical Embeddings as models of
Frame Semantics

The aim of this distributional approach is to model
frames in semantic spaces where words are repre-
sented from the distributional analysis of their co-
occurrences over a corpus. Semantic spaces are
widely used in NLP for representing the meaning
of words or other lexical entities. They have been
successfully applied in several tasks, such as in-
formation retrieval (Salton et al., 1975) and har-
vesting thesauri (Lin, 1998). The fundamental in-
tuition is that the meaning of a word can be de-
scribed by the set of textual contexts in which it
appears (Distributional Hypothesis as described in
(Harris, 1964)), and that words with similar vec-

tors are semantically related. Contexts are words
appearing together with a LU: such a space mod-
els a generic notion of semantic relatedness, i.e.
two LUs spatially close in the space are likely to
be either in paradigmatic or syntagmatic relation
as in (Sahlgren, 2006). Here, LUs delimit sub-
spaces modeling the prototypical semantic of the
corresponding evoked frames and novel LUs can
be induced by exploiting their projections.

Since a semantic space supports the language
in use from the corpus statistics in an unsuper-
vised fashion, vectors representing LUs can be
characterized by different distributions. For exam-
ple, LUs of the frame KILLING, such as blood-
bath, crucify or fratricide, are statistically infe-
rior in a corpus if compared to a wide-spanning
term as kill. Moreover other ambiguous LUs, as
liquidate or terminate, could appear in sentences
evoking different frames. These problems of data-
sparseness and distribution noise can be over-
come by applying space transformation techniques
augmenting the space expressiveness in model-
ing frame semantics. Semantic space models very
elegantly map words in vector spaces (there are
as many dimensions as words in the dictionary)
and LUs collections into distributions of data-
points. Every distribution implicitly expresses two
orthogonal facets: global properties, as the occur-
rence scores computed for terms across the entire
collection (irrespectively from their word senses
or evoking situation) and local regularities, for ex-
ample the existence of subsets of terms that tend to
be used every time a frame manifests. These also
tend to be closer in the space and should be closer
in the transformed space too. Another important
aspect that a transformation could account is exter-
nal semantic information. In the new space, prior
knowledge can be exploited to gather a more regu-
lar LUs representation and a clearer separation be-
tween subspaces representing different frame se-
mantics.

In the following sections the investigated dis-
tributional model of LUs will be discussed. As
many criteria can be adopted to define a LU con-
text, one of the goals of this investigation is to find
a co-occurrence model that better captures the no-
tion of frames, as described in Section 3.1. Then,
two dimensionality reduction techniques, exploit-
ing semantic space distributions to improve frames
representation, are discussed. In Section 3.2 the
role of global properties of data statistics will be

9



investigated through the Latent Semantic Analy-
sis while in Section 3.3 the Locality Preserving
Projection algorithm will be discussed in order to
combine prior knowledge about frames with local
regularities of LUs obtained from text.

3.1 Choosing the space
Different types of context define spaces with dif-
ferent semantic properties. Such spaces model a
generic notion of semantic relatedness. Two LUs
close in the space are likely to be related by some
type of generic semantic relation, either paradig-
matic (e.g. synonymy, hyperonymy, antonymy)
or syntagmatic (e.g. meronymy, conceptual and
phrasal association), as observed in (Sahlgren,
2006). The target of this work is the construc-
tion of a space able to capture the properties which
characterize a frame, assuming those LUs in the
same frame tend to be either co-occurring or sub-
stitutional words (e.g. murder/kill). Two tradi-
tional word-based co-occurrence models capture
the above property:

Word-based space: Contexts are words, as
lemmas, appearing in a n-window of the LU.
The window width n is a parameter that allows
the space to capture different aspects of a frame:
higher values risk to introduce noise, since a frame
could not cover an entire sentence, while lower
values lead to sparse representations.

Syntax-based space: Contexts words are en-
riched through information about syntactic rela-
tions (e.g. X-VSubj-killer where X is the LU), as
described in (Pado and Lapata, 2007). Two LUs
close in the space are likely to be in a paradig-
matic relation, i.e. to be close in an IS-A hierarchy
(Budanitsky and Hirst, 2006; Lin, 1998). Indeed,
as contexts are syntactic relations, targets with the
same part of speech are much closer than targets
of different types.

3.2 Latent Semantic Analysis
Latent Semantic Analysis (LSA) is an algorithm
presented in (Furnas et al., 1988) afterwards dif-
fused by Landauer (Landauer and Dumais, 1997):
it can be seen as a variant of the Principal Compo-
nent Analysis idea. LSA aims to find the best sub-
space approximation to the original word space,
in the sense of minimizing the global reconstruc-
tion error projecting data along the directions of
maximal variance. It captures term (semantic)
dependencies by applying a matrix decomposi-
tion process called Singular Value Decomposition

(SVD). The original term-by-term matrix M is
transformed into the product of three new matri-
ces: U , S, and V so that M = USV T . Matrix
M is approximated by Ml = UlSlV Tl in which
only the first l columns of U and V are used, and
only the first l greatest singular values are consid-
ered. This approximation supplies a way to project
term vectors into the l-dimensional space using
Yterms = UlS

1/2
l . Notice that the SVD process

accounts for the eigenvectors of the entire original
distribution (matrix M ). LSA is thus an example
of a decomposition process strongly dependent on
a global property. The original statistical informa-
tion aboutM is captured by the new l-dimensional
space which preserves the global structure while
removing low-variant dimensions, i.e. distribu-
tion noise. These newly derived features may be
thought of as artificial concepts, each one repre-
senting an emerging meaning component as a lin-
ear combination of many different words (i.e. con-
texts). Such contextual usages can be used instead
of the words to represent texts. This technique has
two main advantages. First, the overall computa-
tional cost of the model is reduced, as similarities
are computed on a space with much fewer dimen-
sions. Secondly, it allows to capture second-order
relations among LUs, thus improving the quality
of the similarity measure.

3.3 The Locality Preserving Projection
Method

An alternative to LSA, much tighter to local prop-
erties of data, is the Locality Preserving Projection
(LPP ), a linear approximation of the non-linear
Laplacian Eigenmap algorithm introduced in (He
and Niyogi, 2003). LPP is a linear dimensional-
ity reduction method whose goal is, given a set of
LUs x1, x2, .., xm in Rn, to find a transformation
matrix A that maps these m points into a set of
points y1, y2, .., ym in Rk (k � n). LPP achieves
this result through a cascade of processing steps
described hereafter.

Construction of an Adjacency graph. Let G
denote a graph with m nodes. Nodes i and j have
got a weighted connection if vectors xi and xj are
close, according to an arbitrary measure of simi-
larity. There are many ways to build an adjacency
graph. The cosine graph with cosine weighting
scheme is explored: given two vectors xi and xj ,
the weight wij between them is set by

wij = max{0,
cos(xi, xj)− τ
|cos(xi, xj)− τ |

· cos(xi, xj)} (1)

10



where a cosine threshold τ is necessary. The ad-
jacency graph can be represented by using a sym-
metricm×m adjacency matrix, namedW , whose
element Wij contains the weight between nodes i
and j. The method of constructing an adjacency
graph outlined above is correct if the data actually
lie on a low dimensional manifold. Once such an
adjacency graph is obtained, LPP will try to opti-
mally preserve it in choosing projections.

Solve an Eigenmap problem. Compute the
eigenvectors and eigenvalues for the generalized
eigenvector problem:

XLXT a = λXDXT a

where X is a n×m matrix whose columns are the
original m vectors in Rn, D is a diagonal m ×m
matrix whose entries are column (or row) sums of
W , Dii =

∑
j Wij and L = D −W is the Lapla-

cian matrix. The solution of this problem is the
set of eigenvectors a0, a1, .., an−1, ordered accord-
ing to their eigenvalues λ0 < λ1 < .. < λn−1.
LPP projection matrix A is obtained by selecting
the k eigenvectors corresponding to the k smallest
eigenvalues: therefore it is a n × k matrix whose
columns are the selected n-dimensional k eigen-
vectors. Final projection of original vectors into
Rk can be linearly performed by Y = ATX . This
transformation provides a valid kernel that can be
efficiently embedded into a classifier.

Embedding predicate knowledge through
LPPs. While LSA finds a projection, according to
the global properties of the space, LPP tries to pre-
serve the local structures of the data. LPP exploits
the adjacency graph in order to represent neigh-
borhood information. It computes a transforma-
tion matrix which maps data points into a lower di-
mensional subspace. As the construction of an ad-
jacency graph G can be based on any principle, its
definition could account on some external infor-
mation reflecting prior knowledge available about
the task.

In this work, prior knowledge about LUs is em-
bedded by exploiting their membership to frame
dictionaries, thus removing from the graph all con-
nections between LUs xi and xj that do not evoke
the same prototypical situation. More formally
Equation 1 can be rewritten more formally as:

wij = max{0,
cos(xi, xj)− τ
|cos(xi, xj)− τ |

· cos(xi, xj) · δ(i, j)}

where

δ(i, j) =

{
1 iff ∃F s.t. LUi ∈ F ∧ LUj ∈ F
0 otherwise

so the resulting manifold keeps close all LUs
evoking the same frame. Since the number of con-
nections could introduce too many constraints to
the Eigenmap problem, a threshold is introduced
to avoid the space collapse: for each LU, only
the most-similar c connections are selected. The
adoption of the proper a priori knowledge about
the target task can be thus seen as a promising re-
search direction.

4 Empirical Analysis

In this section the empirical evaluation of distribu-
tional models applied to the task of inducing LUs
is presented. Different spaces obtained through
the dimensionality reduction techniques imply dif-
ferent kernel functions used to independently train
different SVMs. Our aim is to investigate the im-
pact of these kernels in capturing both the frames
and LUs’ properties, as well as the effectiveness
of their possible combination.

The problem of LUs’ induction is here treated
as a multi-classification problem, where each LU
is considered as a positive or negative instance of a
frame. We use Support Vector Machines (SVMs),
(Joachims, 1999) a maximum-margin classifier
that realizes a linear discriminative model. In case
of not linearly separable examples, convolution
functions φ(·) can be used in order to transform
the initial feature space into another one, where a
hyperplane that separates the data with the widest
margin can be found. Here new similarity mea-
sures, the kernel functions, can be defined through
the dot-product K(oi, oj) = 〈φ(oi) · φ(oj)〉 over
the new representation. In this way, kernel func-
tions KLSA and KLPP can be induced through
the dimensionality reduction techniques φLSA and
φLPP respectively, as described in sections 3.2
and 3.3. Kernel methods are advantageous be-
cause the combination of of kernel functions can
be integrated into the SVM as they are still kernels.
Consequently, the kernel combination αKLSA +
βKLPP linearly combines the global properties
captured by LSA and the locality constraints im-
posed by the LPP transformation. Here, parame-
ters α and β weight the combination of the two
kernels. The evoking frame for a novel LU is
the one whose corresponding SVM has the high-
est (possibly negative) margin, according to a one-

11



train tune test overall
max 107 35 34 176
avg 28 8 8 44
total 2466 722 723 3911

Table 1: Number of LU examples for each data set
from the 100 frames

vs-all scheme. In order to evaluate the quality of
the presented models, accuracy is measured as the
percentage of LUs that are correctly re-assigned to
their original (gold-standard) frame. As the sys-
tem can suggest more than one frame, different
accuracy levels can be obtained. A LU is cor-
rectly assigned if its correct frame (according to
FrameNet) belongs to the set of the best b pro-
posals by the system (i.e. the first b scores from
the underlying SVMs). Assigning different val-
ues to b, we obtained different levels of accuracy
as the percentage of LUs that is correctly assigned
among the first b proposals, as shown in Table 3.

4.1 Experimental Setup

The adopted gold standard is a subset of the
FrameNet database and it consists of the most 100
represented frames in term of annotated examples
and LUs. As the number of example is extremely
unbalanced across frames1, the LUs dictionary of
each selected frame contains at least 10 LUs. It is
a reasonable amount of information for the SVMs
training and it is still a representative data set, be-
ing composed of 3,911 LUs, i.e. the 55% of the
entire dictionary2 of 7,230 evoking words. All
word spaces are derived from the British National
Corpus (BNC), which is underlying FrameNet and
consisting of about 100 million words for English.
Each selected frame is represented into the BNC
by at least 362 annotated sentences, as the lack
of a reasonable number of examples hardly pro-
duces a good distributional model of LUs. Each
frame’s list of LUs is split into train (60%), tuning
(20%) and test set (20%) and LUs having Part-of-
speech different from verb, noun or adjective are
removed. In Table 1 the number of LUs for each
set, as well as the maximum and the average num-
ber per frame, are summarized.

Four different approaches for the Word Space

1For example the SELF MOTION frame counts 6,248 ex-
amples while 119 frames are represented by less than 10 ex-
amples

2The entire database contains 10,228 LUs and the number
of evoking word is 7,230, without taking in account multiple
frame assignments.

construction are used. The first two correspond to
a Word-Based space, the last to a Syntax-Based,
as described in section 3.1:
Window-n (Wn): contextual features correspond
to the set of the 20,000 most frequent lemmatized
words in the BNC. The association measure be-
tween LUs and contexts is the Point-wise Mu-
tual Information (PMI). Valid contexts for LUs are
fixed to a n-window. Hereafter two window width
values will be investigated: Window5 (W5) and
Window10 (W10).
Sentence (Sent): contextual features are the same
above, but the valid contexts are extended to the
entire sentence length.
SyntaxBased (SyntB): contextual features have
been computed according to the “dependency-
based” vector space discussed3 in (Pado and La-
pata, 2007). Observable contexts here are made of
syntactically-typed co-occurrences within depen-
dency graphs built from the entire set of BNC sen-
tences. The most frequent 20,000 basic features,
i.e. (syntactic relation,lemma) pairs, have been
employed as contextual features corresponding to
PMI scores. Syntactic relations are extracted using
the Minipar parser.

Word space models thus focus on the LUs of the
selected 100 frames and the dimensionality have
been reduced by applying LSA and LPP at a new
size of l = 100. Any prior knowledge informa-
tion is provided to the tuning and test sets during
the LPP transformation: the construction of the
reduced feature space takes in account only LUs
from the train set while remaining predicates are
represented through the LPP linear projection. In
these experiments the cosine threshold τ and the
maximum number of constraints c are estimated
over the tuning set and the best parametrizations
are shown in Table 2. The adopted implementa-
tion of SVM is SVM-Light-TK 4.

4.2 Results

In these experiments the impact of the lexical
knowledge gathered by different word-spaces is
evaluated over the LU induction task. Moreover,
the improvements achieved through LSA and LPP
is measured. SVM classifiers are trained over the
semantic spaces produced through the dimension-

3The Minimal context provided by the De-
pendency Vectors tool is used. It is available at
http://www.nlpado.de/∼sebastian/dv.html

4SVM-Light-TK is available at the url
http://disi.unitn.it/∼moschitt/Tree-Kernel.htm

12



α/β
τ c1.0/0.0 .9/.1 .8/.2 .7/.3 .6/.4 .5/.5 .4/.6 .3/.7 .2/.8 .1/.9 0.0/1.0

W5 0.668 0.669 0.672 0.673 0.669 0.662 0.649 0.632 0.612 0.570 0.033 0.55 5
W10 0.615 0.619 0.618 0.612 0.604 0.597 0.580 0.575 0.565 0.528 0.048 0.65 3
Sent 0.557 0.567 0.580 0.584 0.574 0.564 0.561 0.545 0.523 0.496 0.048 0.80 5
SyntB 0.654 0.664 0.662 0.652 0.651 0.647 0.649 0.634 0.627 0.592 0.056 0.40 3

Table 2: Accuracy at different combination weights of kernel αKLSA + βKLPP (specific baseline is
0.043)

b-1 b-2 b-3 b-4 b-5 b-6 b-7 b-8 b-9 b-10 α/β
W5orig 0,563 0,685 0,733 0,770 0,801 0,835 0,841 0,854 0,868 0,879 -
W10orig 0,510 0,634 0,707 0,776 0,810 0,830 0,841 0,857 0,865 0,875 -
Sentorig 0,479 0,618 0,680 0,734 0,764 0,793 0,813 0,837 0,845 0,852 -
SyntBorig 0,585 0,741 0,803 0,840 0,866 0,874 0,886 0,903 0,907 0,913 -
W5LSA+LPP 0.673 0.781 0.831 0.865 0.881 0.891 0.906 0.912 0.926 0.938 0.7/0.3
W10LSA+LPP 0.619 0.739 0.786 0.818 0.849 0.865 0.878 0.888 0.901 0.909 0.9/0.1
SentLSA+LPP 0.584 0.705 0.766 0.798 0.825 0.835 0.848 0.864 0.876 0.889 0.7/0.3
SyntBLSA+LPP 0.664 0.791 0.840 0.864 0.878 0.893 0.901 0.903 0.907 0.911 0.9/0.1

Table 3: Accuracy of original word-space models (orig) and semantic space models (LSA+LPP) on
best-k proposed frames

ality reduction transformations. Representations
of both semantic spaces are linearly combined as
αKLSA + βKLPP , where kernel weights α and
β are estimated over the tuning set. Both ker-
nels are used even without a combination: a ra-
tio α = 1.0/β = 0.0 denotes the LSA kernel
alone, while α = 0.0/β = 1.0 the LPP kernel. Ta-
ble 2 shows best results, obtained through a RBF
kernel. The Window5 model achieves the high-
est accuracy, i.e. 67% of correct classification,
where a baseline of 4.3% is estimated assigning
LUs to the most likely frame in the training set (i.e.
the one containing the highest number of LUs).
Wider windows achieve lower classification accu-
racy confirming that most of lexical information
tied to a frame is near the LU. The Syntactic-based
word space does not outperform the accuracy of a
word-based space. The combination of both ker-
nels has always provided the best outcome and the
LSA space seems to be more accurate and expres-
sive respect to the LPP one, as shown in Figure
1. In particular LPP alone is extremely unstable,
suggesting that constraints imposed by the prior
knowledge are orthogonal with respect to the cor-
pus statistics.

Further experiments are carried out using the
original co-occurrence space models, to assess im-
provements due to LSA and LPP kernel. In the
latter investigation linear kernel achieved best re-
sults as confirmed in (Bengio et al., 2005), where
the sensitivity to the curse of dimensionality of
a large class of modern learning algorithms (e.g.

0,40 

0,45 

0,50 

0,55 

0,60 

0,65 

0,70 

1.0
/0.

0 

0.9
/0.

1 

0.8
/0.

2 

0.7
/0.

3 

0.6
/0.

4 

0.5
/0.

5 

0.4
/0.

6 

0.3
/0.

7 

0.2
/0.

8 

0.1
/0.

9 

αLSA / βLPP weights 

Window5 

Window10 

Sentence 

SyntaxBased 

Figure 1: Accuracy at different combination
weights of kernel αKLSA + βKLPP

SVM) based on local kernels (e.g. RBF) is ar-
gued. As shown in Table 3, the performance drop
of original (orig) models against the best kernel
combination of LSA and LPP are significant,
i.e. ∼ 10%, showing how the latent semantic
spaces better capture properties of frames, avoid-
ing data-sparseness, dimensionality problem and
low-regularities of data-distribution.

Moreover, Table 3 shows how the accuracy level
largely increases when more than one frame is
considered: at a level b = 3, i.e. the novel
LU is correctly classified if one of the original
frames is comprised in the list (of three frames)
proposed by the system, accuracy is 0.84 (i.e the
SyntaxBased model), while at b = 10 accuracy is

13



LU (# WNsyns) frame 1 frame 2 frame 3 Correct frames
boil.v (5) FOOD FLUIDIC MOTION CONTAINERS CAUSE HARM
clap.v (7) SOUNDS MAKE NOISE COMMUNICATION NOISE BODY MOVEMENT
crown.n (12) LEADERSHIP ACCOUTREMENTS PLACING ACCOUTREMENTS

OBSERVABLE BODYPARTS

school.n (7) EDUCATION TEACHING BUILDINGS LOCALE BY USE
EDUCATION TEACHING
LOCALE BY USE
AGGREGATE

threat.n (4) HOSTILE ENCOUNTER IMPACT COMMITMENT COMMITMENT
tragedy.n (2) TEXT KILLING EMOTION DIRECTED TEXT

Table 4: Proposed 3 frames for each LU (ordered by SVM scores) and correct frames provided by the
FrameNet dictionary. In parenthesis the number of different WordNet lexical senses for each LU.

nearly 0.94 (i.e Window5). It is high enough to
support tasks such as the semi-automatic creation
of new FrameNets. An error analysis indicates that
many misclassifications are induced by a lack in
the frame annotations, especially those concern-
ing polysemic LUs5. Table 4 reports the analysis
of a LU subset where the first 3 frames proposed
for each evoking word are shown, ranked by the
margin of the SMVs. The last column contains the
frames evoked by LUs, according to the FrameNet
dictionary, and the frame names in bold suggest
their correct classification. Some LUs, like threat
(characterized by 4 lexical senses) seem to be mis-
classified: in this case the FrameNet annotation
regards a specific sense that evokes the COMMIT-
MENT frame (e.g. “There was a real threat that
she might have to resign”) without taking in ac-
count other senses like WordNet’s “menace, threat
(something that is a source of danger)” that could
evoke the HOSTILE ENCOUNTER frame. In other
cases proposed frames seem to enrich the LUs dic-
tionary, like BUILDINGS, here evoked by school.

5 Conclusions

The core purpose of this was to present an em-
pirical investigation of the impact of different dis-
tributional models on the lexical unit induction
task. The employed word-spaces, based on dif-
ferent co-occurrence models (either context and
syntax-driven), are used as vector models of the
LU semantics. On these spaces, two dimensional-
ity reduction techniques have been applied. Latent
Semantic Analysis (LSA) exploits global proper-
ties of data distributions and results in a global
model for lexical semantics. On the other hand,
the Locality Preserving Projection (LPP) method,
that exploits regularities in the neighborhood of

5According to WordNet, in our dataset an average of 3.6
lexical senses for each LU is estimated.

each lexical predicate, is also employed in a semi-
supervised manner: local constraints expressing
prior knowledge on frames are defined in the ad-
jacency graph. The resulting embedding is there-
fore expected to determine a new space where re-
gions for LU of a given frame can be more eas-
ily discovered. Experiments have been run using
the resulting spaces for task dependent kernels in
a SVM learning setting. The application of the
FrameNet KB on the 100 best represented frames
showed that a combined use of the global and lo-
cal models made available by LSA and LPP, re-
spectively, achieves the best results, as the 67.3%
of LUs recovers the same frames of the annotated
dictionary. This is a significant improvement with
respect to previous results achieved by the pure
distributional model reported in (Pennacchiotti et
al., 2008).

Future work is required to increase the level
of constraints made available from the semi-
supervised setting of LPP: syntactic informa-
tion, as well as role-related evidence, can be
both accommodated by the adjacency constraints
imposed for LPP. This constitutes a significant
area of research towards a comprehensive semi-
supervised model of frame semantics, entirely
based on manifold learning methods, of which this
study on LSA and LPP is just a starting point.

Acknowledgement We want to acknowledge
Prof. Roberto Basili because this work would not
exist without his ideas, inspiration and invaluable
support.

References

Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
ture extraction. In Proceedings of SemEval-2007,

14



pages 99–104, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.

Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond Mooney. 2006. Probabilistic semi-
supervised clustering with constraints. In Semi-
Supervised Learning, pages 73–102. MIT Press.

Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2005. The curse of dimensionality for lo-
cal kernel machines. Technical report, Departement
d’Informatique et Recherche Operationnelle.

Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic dis-
tance. Computational Linguistics, 32(1):13–47.

Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In
Sprachtechnologie, mobile Kommunikation und lin-
guistische Resourcen, volume 8 of Computer Stud-
ies in Language and Speech. Peter Lang, Frank-
furt/Main.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In In Pro-
ceedings of ICML ’08, pages 160–167, New York,
NY, USA. ACM.

Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining word sense
and usage for modeling frame semantics. In In Pro-
ceedings of STEP 2008, Venice, Italy.

Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222–
254.

G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ’88, New York, USA.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.

Yoav Goldberg and Michael Elhadad. 2009. On the
role of lexical features in sequence labeling. In In
Proceedings of EMNLP ’09, pages 1142–1151, Sin-
gapore.

Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.

Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proceedings of NIPS03, Vancou-
ver, Canada.

T. Joachims. 1999. Making large-Scale SVM Learning
Practical. MIT Press, Cambridge, MA.

Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic
Resources for Scandinavian and Baltic Languages,
at NODALIDA, Tartu, Estonia, May 24.

Richard Johansson and Pierre Nugues. 2008. The
effect of syntactic representation on semantic role
labeling. In Proceedings of COLING, Manchester,
UK, August 18-22.

Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.

Dekang Lin. 1998. Automatic retrieval and clustering
of similar word. In Proceedings of COLING-ACL,
Montreal, Canada.

Alessandro Moschitti, Paul Morarescu, and Sanda M.
Harabagiu. 2003. Open domain information ex-
traction via automatic semantic labeling. In FLAIRS
Conference, pages 397–401.

Sebastian Pado and Katrin Erk. 2005. To cause or
not to cause: Cross-lingual semantic matching for
paraphrase modelling. In Proceedings of the Cross-
Language Knowledge Induction Workshop, Cluj-
Napoca, Romania.

Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.

Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of The Empirical Methods in Natural Language Pro-
cessing (EMNLP 2008) Waikiki, Honolulu, Hawaii.

S.T. Roweis and L.K. Saul. 2000. Nonlinear dimen-
sionality reduction by locally linear embedding. Sci-
ence, 290(5500):2323–2326.

Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.

G. Salton, A. Wong, and C. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18:613Äı̀620.

Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of EMNLP-CoNLL, pages 12–21, Prague.

Mihai Surdeanu, , Mihai Surdeanu, A Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In
In Proceedings of ACL 2003.

Marta Tatu and Dan I. Moldovan. 2005. A seman-
tic approach to recognizing textual entailment. In
HLT/EMNLP.

15



J. B. Tenenbaum, V. Silva, and J. C. Langford. 2000.
A Global Geometric Framework for Nonlinear Di-
mensionality Reduction. Science, 290(5500):2319–
2323.

Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Bar-
low. 2006. Semi-supervised nonlinear dimension-
ality reduction. In 23rd International Conference
on Machine learning, pages 1065–1072, New York,
NY, USA. ACM Press.

Zhenyue Zhang and Hongyuan Zha. 2004. Princi-
pal manifolds and nonlinear dimensionality reduc-
tion via tangent space alignment. SIAM J. Scientific
Computing, 26(1):313–338.

16


