



















































SPred: Large-scale Harvesting of Semantic Predicates


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1222–1232,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

SPred: Large-scale Harvesting of Semantic Predicates

Tiziano Flati and Roberto Navigli
Dipartimento di Informatica
Sapienza Università di Roma

{flati,navigli}@di.uniroma1.it

Abstract

We present SPred, a novel method for the
creation of large repositories of semantic
predicates. We start from existing colloca-
tions to form lexical predicates (e.g., break
∗) and learn the semantic classes that best
fit the ∗ argument. To do this, we extract
all the occurrences in Wikipedia which
match the predicate and abstract its argu-
ments to general semantic classes (e.g.,
break BODY PART, break AGREEMENT,
etc.). Our experiments show that we are
able to create a large collection of seman-
tic predicates from the Oxford Advanced
Learner’s Dictionary with high precision
and recall, and perform well against the
most similar approach.

1 Introduction

Acquiring semantic knowledge from text automat-
ically is a long-standing issue in Computational
Linguistics and Artificial Intelligence. Over the
last decade or so the enormous abundance of in-
formation and data that has become available has
made it possible to extract huge amounts of pat-
terns and named entities (Etzioni et al., 2005), se-
mantic lexicons for categories of interest (Thelen
and Riloff, 2002; Igo and Riloff, 2009), large do-
main glossaries (De Benedictis et al., 2013) and
lists of concepts (Katz et al., 2003). Recently,
the availability of Wikipedia and other collabora-
tive resources has considerably boosted research
on several aspects of knowledge acquisition (Hovy
et al., 2013), leading to the creation of several
large-scale knowledge resources, such as DBPe-
dia (Bizer et al., 2009), BabelNet (Navigli and
Ponzetto, 2012), YAGO (Hoffart et al., 2013),
MENTA (de Melo and Weikum, 2010), to name
but a few. This wealth of acquired knowledge
is known to have a positive impact on important
fields such as Information Retrieval (Chu-Carroll
and Prager, 2007), Information Extraction (Krause

et al., 2012), Question Answering (Ferrucci et al.,
2010) and Textual Entailment (Berant et al., 2012;
Stern and Dagan, 2012).

Not only are these knowledge resources ob-
tained by acquiring concepts and named entities,
but they also provide semantic relations between
them. These relations are extracted from unstruc-
tured or semi-structured text using ontology learn-
ing from scratch (Velardi et al., 2013) and Open
Information Extraction techniques (Etzioni et al.,
2005; Yates et al., 2007; Wu and Weld, 2010;
Fader et al., 2011; Moro and Navigli, 2013) which
mainly stem from seminal work on is-a relation
acquisition (Hearst, 1992) and subsequent devel-
opments (Girju et al., 2003; Pasca, 2004; Snow et
al., 2004, among others).

However, these knowledge resources still lack
semantic information about language units such
as phrases and collocations. For instance, which
semantic classes are expected as a direct object
of the verb break? What kinds of noun does the
adjective amazing collocate with? Recognition of
the need for systems that are aware of the selec-
tional restrictions of verbs and, more in general, of
textual expressions, dates back to several decades
(Wilks, 1975), but today it is more relevant than
ever, as is testified by the current interest in se-
mantic class learning (Kozareva et al., 2008) and
supertype acquisition (Kozareva and Hovy, 2010).
These approaches leverage lexico-syntactic pat-
terns and input seeds to recursively learn the se-
mantic classes of relation arguments. However,
they require the manual selection of one or more
seeds for each pattern of interest, and this selec-
tion influences the amount and kind of semantic
classes to be learned. Furthermore, the learned
classes are not directly linked to existing resources
such as WordNet (Fellbaum, 1998) or Wikipedia.

The goal of our research is to create a large-
scale repository of semantic predicates whose lex-
ical arguments are replaced by their semantic
classes. For example, given the textual expres-
sion break a toe we want to create the correspond-

1222



ing semantic predicate break a BODY PART, where
BODY PART is a class comprising several lexical
realizations, such as leg, arm, foot, etc.

This paper provides three main contributions:

• We propose SPred, a novel approach which
harvests predicates from Wikipedia and gen-
eralizes them by leveraging core concepts
from WordNet.
• We create a large-scale resource made up of

semantic predicates.
• We demonstrate the high quality of our se-

mantic predicates, as well as the generality
of our approach, also in comparison with our
closest competitor.

2 Preliminaries

We introduce two preliminary definitions which
we use in our approach.

Definition 1 (lexical predicate). A lexical pred-
icate w1 w2 . . . wi ∗ wi+1 . . . wn is a regular
expression, where wj are tokens (j = 1, . . . , n), ∗
matches any sequence of one or more tokens, and
i ∈ {0, . . . , n}. We call the token sequence which
matches ∗ the filling argument of the predicate.

For example, a * of milk matches occurrences
such as a full bottle of milk, a glass of milk, a car-
ton of milk, etc. While in principle * could match
any sequence of words, since we aim at general-
izing nouns, in what follows we allow ∗ to match
only noun phrases (e.g., glass, hot cup, very big
bottle, etc.).

Definition 2 (semantic predicate). A semantic
predicate is a sequence w1 w2 . . . wi c wi+1
. . . wn, where wj are tokens (j = 1, . . . , n),
c ∈ C is a semantic class selected from a fixed
set C of classes, and i ∈ {0, . . . , n}.

As an example, consider the semantic predicate
cup of BEVERAGE,1 where BEVERAGE is a se-
mantic class representing beverages. This pred-
icate matches phrases like cup of coffee, cup of
tea, etc., but not cup of sky. Other examples in-
clude: MUSICAL INSTRUMENT is played by, a
CONTAINER of milk, break AGREEMENT, etc.

Semantic predicates mix the lexical information
of a given lexical predicate with the explicit se-
mantic modeling of its argument. Importantly, the
same lexical predicate can have different classes as
its argument, like cup of FOOD vs. cup of BEVER-
AGE. Note, however, that different classes might
convey different semantics for the same lexical

1In what follows we denote the SEMANTIC CLASS in
small capitals and the lexical predicate in italics.

predicate, such as cup of COUNTRY, referring to
cup as a prize instead of cup as a container.

3 Large-Scale Harvesting of Semantic
Predicates

The goal of this paper is to provide a fully auto-
matic approach for the creation of a large repos-
itory of semantic predicates in three phases. For
each lexical predicate of interest (e.g., break ∗):

1. We extract all its possible filling arguments
from Wikipedia, e.g., lease, contract, leg,
arm, etc. (Section 3.1).

2. We disambiguate as many filling arguments
as possible using Wikipedia, obtaining a
set of corresponding Wikipedia pages, e.g.,
Lease, Contract, etc. (Section 3.2).

3. We create the semantic predicates by general-
izing the Wikipedia pages to their most suit-
able semantic classes, e.g., break AGREE-
MENT, break LIMB, etc. (Section 3.3).

We can then exploit the learned semantic predi-
cates to assign the most suitable semantic class to
new filling arguments for the given lexical predi-
cate (Section 3.4).

3.1 Extraction of Filling Arguments
Let π be an input lexical predicate (e.g., break ∗).
We search the English Wikipedia for all the to-
ken sequences which match π, resulting in a list
of noun phrases filling the ∗ argument. We show
an excerpt of the output obtained when searching
Wikipedia for the arguments of the lexical predi-
cate a * of milk in Table 1. As can be seen, a wide
range of noun phrases are extracted, from quanti-
ties such as glass and cup to other aspects, such as
brand and constituent.

The output of this first step is a set Lπ of triples
(a, s, l) of filling arguments a matching the lexi-
cal predicate π in a sentence s of the Wikipedia
corpus, with a potentially linked to a page l (e.g.,
see the top 3 rows in Table 1; l = � if no link is
provided, see bottom rows of the Table).2 Note
that Wikipedia is the only possible corpus that can
be used here for at least two reasons: first, in or-
der to extract relevant arguments, we need a large
corpus of a definitional nature; second, we need
wide-coverage semantic annotations of filling ar-
guments.

3.2 Disambiguation of Filling Arguments
The objective of the second step is to disambiguate
as many arguments in Lπ as possible for the lex-

2We will also refer to l as the sense of a in sentence s.

1223



a full [[bottle]] of milk
a nice hot [[cup]] of milk
a cold [[glass]] of milk
a very big bottle of milk
a brand of milk
a constituent of milk

Table 1: An excerpt of the token sequences
which match the lexical predicate a * of milk in
Wikipedia (filling argument shown in the second
column; following the Wikipedia convention we
provide links in double square brackets).

ical predicate π. We denote Dπ = {(a, s, l) :
l 6= �} ⊆ Lπ as the set of those arguments origi-
nally linked to the corresponding Wikipedia page
(like the top three linked arguments in Table 1).
Therefore, in the rest of this section we will focus
only on the remaining triples (a, s, �) ∈ Uπ, where
Uπ = Lπ \Dπ, i.e., those triples whose arguments
are not semantically annotated. Our goal is to re-
place � with an appropriate sense, i.e., page, for a.
For each such triple (a, s, �) ∈ Uπ, we apply the
following disambiguation heuristics:

• One sense per page: if another occurrence
of a in the same Wikipedia page (indepen-
dent of the lexical predicate) is linked to a
page l, then remove (a, s, �) from Uπ and add
(a, s, l) to Dπ. In other words, we propa-
gate an existing annotation of a in the same
Wikipedia page and apply it to our ambigu-
ous item. For instance, cup of coffee appears
in the Wikipedia page Energy drink in the
sentence “[. . . ] energy drinks contain more
caffeine than a strong cup of coffee”, but this
occurrence of coffee is not linked. How-
ever the second paragraph contains the sen-
tence “[[Coffee]], tea and other naturally caf-
feinated beverages are usually not considered
energy drinks”, where coffee is linked to the
Coffee page. This heuristic naturally reflects
the broadly known assumption about lexi-
cal ambiguity presented in (Yarowsky, 1995),
namely the one-sense-per-discourse heuris-
tic.
• One sense per lexical predicate: if
∃(a, s′, l) ∈ Dπ, then remove (a, s, �) from
Uπ and add (a, s, l) to Dπ. If multiple senses
of a are available, choose the most frequent
one in Dπ. For example, in the page Singa-
porean cuisine the occurrence of coffee in the
sentence “[. . . ] combined with a cup of cof-
fee and a half-boiled egg” is not linked, but
we have collected many other occurrences,
all linked to the Coffee page, so this link

gets propagated to our ambiguous item as
well. This heuristic mimes the one-sense-per-
collocation heuristic presented in (Yarowsky,
1995).
• Trust the inventory: if Wikipedia provides

only one sense for a, i.e., only one page title
whose lemma is a, link a to that page. Con-
sider the instance “At that point, Smith threw
down a cup of Gatorade” in page Jimmy
Clausen; there is only one sense for Gatorade
in Wikipedia, so we link the unannotated oc-
currence to it.

As a result, the initial set of disambiguated ar-
guments in Dπ is augmented with all those triples
for which any of the above three heuristics apply.
Note that Dπ might contain the same argument
several times, occurring in different sentences and
linked many times to the same page or to differ-
ent pages. Notably, the discovery of new links is
made through one scan of Wikipedia per heuristic.
The three disambiguation strategies, applied in the
same order as presented above, contribute to pro-
moting the most relevant sense for a given word.

Finally, let A be the set of arguments in Dπ,
i.e., A := {a : ∃(a, s, l) ∈ Dπ}. For each argu-
ment a ∈ Awe select the majority sense sense(a)
of a and collect the corresponding set of sen-
tences sent(a) marked with that sense. Formally,
sense(a) := argmaxl |{(x, y, z) ∈ Dπ : x =
a∧z = l}| and sent(a) := {s : (a, s, sense(a)) ∈
Dπ}.

3.3 Generalization to Semantic Classes
Our final objective is to generalize the annotated
arguments to semantic classes picked out from a
fixed set C of classes. As explained below, we as-
sume the set C to be made up of representative
synsets from WordNet. We perform this in two
substeps: we first link all our disambiguated argu-
ments to WordNet (Section 3.3.1) and then lever-
age the WordNet taxonomy to populate the seman-
tic classes in C (Section 3.3.2).

3.3.1 Linking to WordNet
So far the arguments in Dπ have been semanti-
cally annotated with the Wikipedia pages they re-
fer to. However, using Wikipedia as our sense in-
ventory is not desirable; in fact, contrarily to other
commonly used lexical-semantic networks such
as WordNet, Wikipedia is not formally organized
in a structured, taxonomic hierarchy. While it is
true that attached to each Wikipedia page there are
one or more categories, these categories just pro-
vide shallow information about the class the page

1224



belongs to. Indeed, categories are not ideal for
representing the semantic classes of a Wikipedia
page for at least three reasons: i) many cate-
gories do not express taxonomic information (e.g.,
the English page Albert Einstein provides cate-
gories such as DEATHS FROM ABDOMINAL AOR-
TIC ANEURYSM and INSTITUTE FOR ADVANCED
STUDY FACULTY); ii) categories are mostly struc-
tured in a directed acyclic graph with multiple par-
ents per category (even worse, cycles are possible
in principle); iii) there is no clear way of iden-
tifying core semantic classes from the large set
of available categories. Although efforts towards
the taxonomization of Wikipedia categories do ex-
ist in the literature (Ponzetto and Strube, 2011;
Nastase and Strube, 2013), the results are of a
lower quality than a hand-built lexical resource.
Therefore, as was done in previous work (Mihal-
cea and Moldovan, 2001; Ciaramita and Altun,
2006; Izquierdo et al., 2009; Erk and McCarthy,
2009; Huang and Riloff, 2010), we pick out our
semantic classes C from WordNet and leverage its
manually-curated taxonomy to associate our argu-
ments with the most suitable class. This way we
avoid building a new taxonomy and shift the prob-
lem to that of projecting the Wikipedia pages –
associated with annotated filling arguments – to
synsets in WordNet. We address this problem in
two steps:

Wikipedia-WordNet mapping. We exploit an
existing mapping implemented in BabelNet (Nav-
igli and Ponzetto, 2012), a wide-coverage
multilingual semantic network that integrates
Wikipedia and WordNet.3 Based on a disam-
biguation algorithm, BabelNet establishes a map-
ping µ : Wikipages → Synsets which links
about 50,000 pages to their most suitable Word-
Net senses.4

Mapping extension. Nevertheless, BabelNet is
able to solve the problem only partially, because it
still leaves the vast majority of the 4 million En-
glish Wikipedia pages unmapped. This is mainly
due to the encyclopedic nature of most pages,
which do not have a counterpart in the WordNet
dictionary. To address this issue, for each un-
mapped Wikipedia page p we obtain its textual
definition as the first sentence of the page.5 Next,

3http://babelnet.org
4We follow (Navigli, 2009) and denote with wip the i-th

sense of w in WordNet with part of speech p.
5According to the Wikipedia guidelines, “The article

should begin with a short declarative sentence, answer-
ing two questions for the nonspecialist reader: What (or
who) is the subject? and Why is this subject notable?”,
extracted from http://en.wikipedia.org/wiki/

we extract the hypernym from the textual defini-
tion of p by applying Word-Class Lattices (Navigli
and Velardi, 2010, WCL6), a domain-independent
hypernym extraction system successfully applied
to taxonomy learning from scratch (Velardi et al.,
2013) and freely available online (Faralli and Nav-
igli, 2013). If a hypernym h is successfully ex-
tracted and h is linked to a Wikipedia page p′

for which µ(p′) is defined, then we extend the
mapping by setting µ(p) := µ(p′). For instance,
the mapping provided by BabelNet does not pro-
vide any link for the page Peter Spence; thanks to
WCL, though, we are able to set the page Jour-
nalist as its hypernym, and link it to the WordNet
synset journalist1n.

This way our mapping extension now covers
539,954 pages, i.e., more than an order of mag-
nitude greater than the number of pages originally
covered by the BabelNet mapping.

3.3.2 Populating the Semantic Classes
We now proceed to populating the semantic
classes in C with the annotated arguments ob-
tained for the lexical predicate π.

Definition 3 (semantic class of a synset). The
semantic class for a WordNet synset S is the class
c among those in C which is the most specific hy-
pernym of S according to the WordNet taxonomy.

For instance, given the synset tap water1n, its se-
mantic class is water1n (while the other more gen-
eral subsumers in C are not considered, e.g., com-
pound2n, chemical

1
n, liquid

3
n, etc).

For each argument a ∈ A for which a
Wikipedia-to-WordNet mapping µ(sense(a))
could be established as a result of the linking
procedure described above, we associate a with
the semantic class of µ(sense(a)). For example,
consider the case in which a is equal to tap water
and sense(a) is equal to the Wikipedia page Tap
water, in turn mapped to tap water1n via µ; we
thus associate tap water with its semantic class
water1n. If more than one class can be found we
add a to each of them.7

Ultimately, for each class c ∈ C, we obtain
a set support(c) made up of all the arguments
a ∈ A associated with c. For instance, sup-
port(beverage1n) = { chinese tea, 3.2% beer, hot
cocoa, cider, . . . , orange juice }. Note that, thanks
to our extended mapping (cf. Section 3.3.1), the
support of a class can also contain arguments not
covered in WordNet (e.g., hot cocoa and tejuino).

Wikipedia:Writing_better_articles.
6http://lcl.uniroma1.it/wcl
7This can rarely happen due to multiple hypernyms avail-

able in WordNet for the same synset.

1225



Pclass(c|π) c support(c)
0.1896 wine1n wine, sack, white wine, red wine, wine in china, madeira wine, claret, kosher wine
0.1805 coffee1n turkish coffee, drip coffee, espresso, coffee, cappucino, caffè latte, decaffeinated coffee, latte
0.1143 herb2n green tea, indian tea, black tea, orange pekoe tea, tea
0.1104 water1n water, seawater
0.0532 beverage1n chinese tea, 3.2% beer, orange soda, boiled water, hot chocolate, hot cocoa, tejuino, cider,

beverage, cocoa, coffee milk, lemonade, orange juice
0.0403 milk1n skim milk, milk, cultured buttermilk, whole milk
0.0351 beer1n 3.2% beer, beer
0.0273 alcohol1n mead, umeshu, kava, rice wine, jägermeister, kvass, sake, gin, rum
0.0182 poison1n poison

Table 2: Highest-probability semantic classes for the lexical predicate π = cup of *, according to our set
C of semantic classes.

Since not all classes are equally relevant to the
lexical predicate π, we estimate the conditional
probability of each class c ∈ C given π on the
basis of the number of sentences which contain an
argument in that class. Formally:

Pclass(c|π) =
∑

a∈support(c) |sent(a)|
Z

, (1)

where Z is a normalization factor calculated as
Z =

∑
c′∈C

∑
a∈support(c′) |sent(a)|. As an ex-

ample, in Table 2 we show the highest-probability
classes for the lexical predicate cup of ∗.

As a result of the probabilistic association of
each semantic class c with a target lexical predi-
cate w1 w2 . . . wi ∗ wi+1 . . . wn, we obtain a
semantic predicate w1 w2 . . . wi c wi+1 . . . wn.

3.4 Classification of new arguments
Once the semantic predicates for the input lexical
predicate π have been learned, we can classify a
new filling argument a of π. However, the class
probabilities calculated with Formula 1 might not
provide reliable scores for several classes, includ-
ing unseen ones whose probability would be 0.

To enable wide coverage we estimate a second
conditional probability based on the distributional
semantic profile of each class. To do this, we per-
form three steps:

1. For each WordNet synset S we create a dis-
tributional vector ~S summing the noun occur-
rences within all the Wikipedia pages p such
that µ(p) = S. Next, we create a distribu-
tional vector for each class c ∈ C as follows:

~c =
∑

S∈desc(c) ~S,

where desc(c) is the set of all synsets
which are descendants of the semantic class
c in WordNet. As a result we obtain a
predicate-independent distributional descrip-
tion for each semantic class in C.

2. Now, given an argument a of a lexical predi-
cate π, we create a distributional vector ~a by
summing the noun occurrences of all the sen-
tences s such that (a, s, l) ∈ Lπ (cf. Section
3.1).

3. Let Ca be the set of candidate semantic
classes for argument a, i.e., Ca contains the
semantic classes for the WordNet synsets of
a as well as the semantic classes associated
with µ(p) for all Wikipedia pages p whose
lemma is a. For each candidate class c ∈ Ca,
we determine the cosine similarity between
the distributional vectors ~c and ~a as follows:

sim(~c,~a) =
~c · ~a
||~c|| ||~a|| .

Then, we determine the most suitable seman-
tic class c ∈ Ca of argument a as the class
with the highest distributional probability, es-
timated as:

Pdistr(c|π, a) =
sim(~c,~a)∑

c′∈Ca sim(~c
′,~a)

. (2)

We can now choose the most suitable class c ∈
Ca for argument a which maximizes the proba-
bility mixture of the distributional probability in
Formula 2 and the class probability in Formula 1:

P (c|π, a) = αPdistr(c|π, a)+(1−α)Pclass(c|π),
(3)

where α ∈ [0, 1] is an interpolation factor.

We now illustrate the entire process of our al-
gorithm on a real example. Given a textual ex-
pression such as virus replicate, we: (i) extract
all the filling arguments of the lexical predicate
* replicate; (ii) link and disambiguate the ex-
tracted filling arguments; (iii) query our system for
the available virus semantic classes (i.e., {virus1n,
virus3n}); (iv) build the distributional vectors for

1226



the candidate semantic classes and the given in-
put argument; (v) calculate the probability mix-
ture. As a result we obtain the following rank-
ing, virus1n:0.250, virus

3
n:0.000894, so that the first

sense of virus in WordNet 3.0 is preferred, being
an “ultramicroscopic infectious agent that repli-
cates itself only within cells of living hosts”.

4 Experiment 1: Oxford Lexical
Predicates

We evaluate on the two forms of output produced
by SPred: (i) the top-ranking semantic classes of a
lexical predicate, as obtained with Formula 1, and
(ii) the classification of a lexical predicate’s argu-
ment with the most suitable semantic class, as pro-
duced using Formula 3. For both evaluations, we
use a lexical predicate dataset built from the Ox-
ford Advanced Learner’s Dictionary (Crowther,
1998).

4.1 Set of Semantic Classes

The selection of which semantic classes to include
in the set C is of great importance. In fact, hav-
ing too many classes will end up in an overly fine-
grained inventory of meanings, whereas an exces-
sively small number of classes will provide lit-
tle discriminatory power. As our set C of seman-
tic classes we selected the standard set of 3,299
core nominal synsets available in WordNet.8 How-
ever, our approach is flexible and can be used with
classes of an arbitrary level of granularity.

4.2 Datasets

The Oxford Advanced Learner’s Dictionary pro-
vides usage notes that contain typical predicates in
various semantic domains in English, e.g., Travel-
ing.9 Each predicate is made up of a fixed part
(e.g., a verb) and a generalizable part which con-
tains one or more nouns.

Examples include fix an election/the vote, bac-
teria/microbes/viruses spread, spend money/sav-
ings/a fortune. In the case that more than one
noun was provided, we split the textual expres-
sion into as many items as the number of nouns.
For instance, from spend money/savings/a fortune
we created three items in our dataset, i.e., spend
money, spend savings, spend a fortune. The split-
ting procedure generated 6,220 instantiated lexical
predicate items overall.

8http://wordnetcode.princeton.edu/
standoff-files/core-wordnet.txt

9http://oald8.oxfordlearnersdictionaries.
com/usage_notes/unbox_colloc/

k Prec@k Correct Total
1 0.94 46 49
2 0.87 85 98
3 0.86 124 145
4 0.83 160 192
5 0.82 194 237
6 0.81 228 282
7 0.80 261 326
8 0.78 288 370
9 0.77 318 414

10 0.76 349 458
11 0.75 379 502
12 0.75 411 546
13 0.75 445 590
14 0.76 479 634
15 0.75 510 678
16 0.75 544 721
17 0.76 577 763
18 0.76 612 806
19 0.76 643 849
20 0.75 671 892

Table 3: Precision@k for ranking the semantic
classes of lexical predicates.

4.3 Evaluating the Semantic Class Ranking

Dataset. Given the above dataset, we general-
ized each item by pairing its fixed verb part with *
(i.e., we keep “verb predicates” only, since they
are more informative). For instance, the three
items bacteria/microbes/viruses spread were gen-
eralized into the lexical predicate * spread. The to-
tal number of different lexical predicates obtained
was 1,446, totaling 1,429 distinct verbs (note that
the dataset might contain the lexical predicate *
spread as well as spread *).10

Methodology. For each lexical predicate we cal-
culated the conditional probability of each seman-
tic class using Formula 1, resulting in a ranking
of semantic classes. To evaluate the top ranking
classes, we calculated precision@k, with k rang-
ing from 1 to 20, by counting all applicable classes
as correct, e.g., location1n is a valid semantic class
for travel to * while emotion1n is not.

Results. We show in Table 3 the precision@k
calculated over a random sample of 50 lexical
predicates.11 As can be seen, while the classes
quality is pretty high with low values of k, per-
formance gradually degrades as we let k increase.
This is mostly due to the highly polysemous nature
of the predicates selected (e.g., occupy *, leave *,
help *, attain *, live *, etc.). We note that high per-
formance, attaining above 80%, can be achieved

10The low number of items per predicate is due to the orig-
inal Oxford resource.

11One lexical predicate did not have any semantic class
ranking.

1227



by focusing up to the first 7 classes output by our
system, with a 94% precision@1.

4.4 Evaluating Classification Performance

Dataset. Starting from the lexical predicate
items obtained as described in Section 4.2, we se-
lected those items belonging to a random sample
of 20 usage notes among those provided by the
Oxford dictionary, totaling 3,245 items. We then
manually tagged each item’s argument (e.g., virus
in viruses spread) with the most suitable seman-
tic class (e.g., virus1n), obtaining a gold standard
dataset for the evaluation of our argument classifi-
cation algorithm (cf. Section 3.4).

Methodology. In this second evaluation we
measure the accuracy of our method at assigning
the most suitable semantic class to the argument
of a lexical predicate item in our gold standard.
We use three customary measures to determine the
quality of the acquired semantic classes, i.e., pre-
cision, recall and F1. Precision is the number of
items which are assigned the correct class (as eval-
uated by a human) over the number of items which
are assigned a class by the system. Recall is the
number of items which are assigned the correct
class over the number of items to be classified. F1
is the harmonic mean of precision and recall.

Tuning. The only parameter to be tuned is the
factor α that we use to mix the two probabilities
in Formula 3 (cf. Section 3.4). For tuning α we
used a held-out set of 8 verbs, randomly sampled
from the lexical predicates not used in the dataset.
We created a tuning set using the annotated argu-
ments in Wikipedia for these verbs: we trained the
model on 80% of the annotated lexical predicate
arguments (i.e., the class probability estimates in
Formula 1) and then applied the probability mix-
ture (i.e., Formula 3) for classifying the remain-
ing 20% of arguments. Finally, we calculated the
performance in terms of precision, recall and F1
with 11 different values of α ∈ {0, 0.1, . . . , 1.0},
achieving optimal performance with α = 0.2.

Results. Table 4 shows the results on the seman-
tic class assignments. Our system shows very high
precision, above 85%, while at the same time at-
taining an adequate 68% recall. We also compared
against a random baseline that randomly selects
one out of all the candidate semantic classes for
each item, achieving only moderate results. A sub-
sequent error analysis revealed the common types
of error produced by our system: terms for which
we could not provide (1) any WordNet concept

Method Precision Recall F1
SPred 85.61 68.01 75.80
Random 40.96 40.96 40.96

Table 4: Performance on semantic class assign-
ment.

(e.g., political corruption) or (2) any candidate se-
mantic class (e.g., immune system).

4.5 Disambiguation heuristics impact
As a follow-up analysis, for each dataset we con-
sidered the impact of each disambiguation heuris-
tic described in Section 3.2 according to how many
times it was triggered. Starting from the entire set
of 1,446 lexical predicates from the Oxford dictio-
nary (see Section 4.3), we counted the number of
argument triples (a, s, l) already disambiguated in
Wikipedia (i.e., l 6= �) and those disambiguated
thanks to our disambiguation strategies. Table
5 shows the statistics. We note that, while the
amount of originally linked arguments is very low
(about 2.5% of total), our strategies are able to
considerably increase the size of the initial set of
linked instances. The most effective strategies ap-
pear to be the One sense per page and the Trust the
inventory, which contribute 26.16% and 31.33%
of the total links, respectively.

Even though most of the triples (i.e., 68 out of
almost 74 million) remain unlinked, the ratio of
distinct arguments which we linked to WordNet
is considerably higher, calculated as 3,723,979
linked arguments over 12,431,564 distinct argu-
ments, i.e., about 30%.

5 Experiment 2: Comparison with
Kozareva & Hovy (2010)

Due to the novelty of the task carried out by SPred,
the resulting output may be compared with only a
limited number of existing approaches. The most
similar approach is that of Kozareva and Hovy
(2010, K&H) who assign supertypes to the argu-
ments of arbitrary relations, a task which resem-
bles our semantic predicate ranking. We therefore
performed a comparison on the quality of the most
highly-ranked supertypes (i.e., semantic classes)
using their dataset of 24 relation patterns (i.e., lex-
ical predicates).

Dataset. The dataset contained 14 lexical pred-
icates (e.g., work for * or * fly to), 10 of which
were expanded in order to semantify their left- and
right-side arguments (e.g., * work for and work
for *); for the remaining 4 predicates just a single

1228



Total Linked in One sense One sense per Trust the Not
triples Wikipedia per page lexical predicate inventory linked

73,843,415 1,795,608 1,433,634 533,946 1,716,813 68,363,414

Table 5: Statistics on argument triple linking for all the lexical predicates in the Oxford dataset.

k Prec@k Correct Total
1 0.88 21 24
2 0.90 43 48
3 0.88 63 72
4 0.89 85 96
5 0.91 109 120
6 0.91 131 144
7 0.92 154 168
8 0.91 175 192
9 0.92 198 216

10 0.92 221 240
11 0.92 242 264
12 0.92 264 288
13 0.91 284 312
14 0.90 304 336
15 0.91 327 360
16 0.91 348 384
17 0.90 367 408
18 0.89 386 432
19 0.89 407 456
20 0.89 429 480

Table 6: Precision@k for the semantic classes of
the relations of Kozareva and Hovy (2010).

side was generalized (e.g., * dress). While most of
the relations apply to persons as a supertype, our
method could find arguments for each of them.

Methodology. We carried out the same evalua-
tion as in Section 4.3. We calculated precision@k
of the semantic classes obtained for each relation
in the dataset of K&H. Because the set of appli-
cable classes was potentially unbounded, we were
not able to report recall directly.

Results. K&H reported an overall accuracy of
the top-20 supertypes of 92%. As can be seen in
Table 6 we exhibit very good performance with in-
creasing values of k. A comparison of Table 3 with
Table 6 shows considerable differences in perfor-
mance between the two datasets. We attribute this
difference to the higher average WordNet poly-
semy of the verbal component of the Oxford pred-
icates (on average 2.64 senses for K&H against
6.52 for the Oxford dataset).

Although we cannot report recall, we list the
number of Wikipedia arguments and associated
classes in Table 7, which provides an estimate of
the extraction capability of SPred. The large num-
ber of classes found for the arguments demon-
strates the ability of our method to generalize to
a variety of semantic classes.

Predicate Number of args Number of classes
cause * 181,401 1,339

live in * 143,628 600
go to * 134,712 867
* cause 92,160 1,244

work in * 79,444 770
* go to 71,794 746

* live in 61,074 541
work on * 58,760 840
work for * 58,332 681
work at * 31,904 511
* work in 24,933 528

* celebrate 23,333 408

Table 7: Number of arguments and associated
classes for the 12 most frequent lexical predicates
of Kozareva and Hovy (2010) extracted by SPred
from Wikipedia.

6 Related work

The availability of Web-scale corpora has led to
the production of large resources of relations (Et-
zioni et al., 2005; Yates et al., 2007; Wu and Weld,
2010; Carlson et al., 2010; Fader et al., 2011).
However, these resources often operate purely at
the lexical level, providing no information on the
semantics of their arguments or relations. Several
studies have examined adding semantics through
grouping relations into sets (Yates and Etzioni,
2009), ontologizing the arguments (Chklovski and
Pantel, 2004), or ontologizing the relations them-
selves (Moro and Navigli, 2013). However, analy-
sis has largely been either limited to ontologizing
a small number of relation types with a fixed in-
ventory, which potentially limits coverage, or has
used implicit definitions of semantic categories
(e.g., clusters of arguments), which limits inter-
pretability. For example, Mohamed et al. (2011)
use the semantic categories of the NELL system
(Carlson et al., 2010) to learn roughly 400 valid
ontologized relations from over 200M web pages,
whereas WiSeNet (Moro and Navigli, 2012) lever-
ages Wikipedia to acquire relation synsets for an
open set of relations. Despite these efforts, no
large-scale resource has existed to date that con-
tains ontologized lexical predicates. In contrast,
the present work provides a high-coverage method
for learning argument supertypes from a broad-
coverage ontology (WordNet), which can poten-
tially be leveraged in relation extraction to ontolo-

1229



gize relation arguments.
Our method for identifying the different seman-

tic classes of predicate arguments is closely related
to the task of identifying selectional preferences.
The most similar approaches to it are taxonomy-
based ones, which leverage the semantic types
of the relations arguments (Resnik, 1996; Li and
Abe, 1998; Clark and Weir, 2002; Pennacchiotti
and Pantel, 2006). Nevertheless, despite their high
quality sense-tagged data, these methods have of-
ten suffered from lack of coverage. As a result,
alternative approaches have been proposed that es-
chew taxonomies in favor of rating the quality of
potential relation arguments (Erk, 2007; Cham-
bers and Jurafsky, 2010) or generating probabil-
ity distributions over the arguments (Rooth et al.,
1999; Pantel et al., 2007; Bergsma et al., 2008;
Ritter et al., 2010; Séaghdha, 2010; Bouma, 2010;
Jang and Mostow, 2012) in order to obtain higher
coverage of preferences.

In contrast, we overcome the data sparsity of
class-based models by leveraging the large quan-
tity of collaboratively-annotated Wikipedia text in
order to connect predicate arguments with their
semantic class in WordNet using BabelNet (Nav-
igli and Ponzetto, 2012); because we map directly
to WordNet synsets, we provide a more readily-
interpretable collocation preference model than
most similarity-based or probabilistic models.

Verb frame extraction (Green et al., 2004) and
predicate-argument structure analysis (Surdeanu
et al., 2003; Yakushiji et al., 2006) are two areas
that are also related to our work. But their gener-
ality goes beyond our intentions, as we focus on
semantic predicates, which is much simpler and
free from syntactic parsing.

Another closely related work is that of Hanks
(2013) concerning the Theory of Norms and Ex-
ploitations, where norms (exploitations) represent
expected (unexpected) classes for a given lexical
predicate. Although our semantified predicates do,
indeed, provide explicit evidence of norms ob-
tained from collective intelligence and would pro-
vide support for this theory, exploitations present
a more difficult task, different from the one ad-
dressed here, due to its focus on identifying prop-
erty transfer between the semantic class and the
exploited instance.

The closest technical approach to ours is that
of Kozareva and Hovy (2010), who use recursive
patterns to induce semantic classes for the argu-
ments of relational patterns. Whereas their ap-
proach requires both a relation pattern and one
or more seeds, which bias the types of semantic
classes that are learned, our proposed method re-

quires only the pattern itself, and as a result is ca-
pable of learning an unbounded number of differ-
ent semantic classes.

7 Conclusions

In this paper we present SPred, a novel approach
to large-scale harvesting of semantic predicates.
In order to semantify lexical predicates we ex-
ploit the wide coverage of Wikipedia to extract
and disambiguate lexical predicate occurrences,
and leverage WordNet to populate the semantic
classes with suitable predicate arguments. As a re-
sult, we are able to ontologize lexical predicate in-
stances like those available in existing dictionaries
(e.g., break a toe) into semantic predicates (such
as break a BODY PART).

For each lexical predicate (such as break ∗),
our method produces a probability distribution
over the set of semantic classes (thus covering the
different expected meanings for the filling argu-
ments) and is able to classify new instances with
the most suitable class. Our experiments show
generally high performance, also in comparison
with previous work on argument supertyping.

We hope that our semantic predicates will en-
able progress in different Natural Language Pro-
cessing tasks such as Word Sense Disambigua-
tion (Navigli, 2009), Semantic Role Labeling
(Fürstenau and Lapata, 2012) or even Textual En-
tailment (Stern and Dagan, 2012) – each of which
is in urgent need of reliable semantics. While we
focused on semantifying lexical predicates, as fu-
ture work we will apply our method to the ontol-
ogization of large amounts of sequences of words,
such as phrases or textual relations (e.g., consid-
ering Google n-grams appearing in Wikipedia).
Notably, our method should, in principle, gener-
alize to any semantically-annotated corpus (e.g.,
Wikipedias in other languages), provided lexical
predicates can be extracted with associated seman-
tic classes.

In order to support future efforts we are releas-
ing our semantic predicates as a freely available
resource.12

Acknowledgments

The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.

Thanks go to David A. Jurgens, Silvia Necşulescu,
Stefano Faralli and Moreno De Vincenzi for their
help.

12http://lcl.uniroma1.it/spred

1230



References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.

2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.

Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional prefer-
ence from unlabeled text. In Proc. of EMNLP, pages
59–68, Stroudsburg, PA, USA.

Christian Bizer, Jens Lehmann, Georgi Kobilarov,
Sören Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia - a crystal-
lization point for the Web of Data. Web Semantics,
7(3):154–165.

Gerlof Bouma. 2010. Collocation Extraction beyond
the Independence Assumption. In Proc. of ACL,
Short Papers, pages 109–114, Uppsala, Sweden.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proc. of AAAI, pages 1306–1313,
Atlanta, Georgia.

Nathanael Chambers and Dan Jurafsky. 2010. Improv-
ing the use of pseudo-words for evaluating selec-
tional preferences. In Proc. of ACL, pages 445–453,
Stroudsburg, PA, USA.

Tim Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb rela-
tions. In Proc. of EMNLP, pages 33–40, Barcelona,
Spain.

Jennifer Chu-Carroll and John Prager. 2007. An exper-
imental study of the impact of information extraction
accuracy on semantic search performance. In Proc.
of CIKM, pages 505–514, Lisbon, Portugal.

Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-Coverage Sense Disambiguation and Infor-
mation Extraction with a Supersense Sequence Tag-
ger. In Proc. of EMNLP, pages 594–602, Sydney,
Australia.

Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187–206.

Jonathan Crowther, editor. 1998. Oxford Advanced
Learner’s Dictionary. Cornelsen & Oxford, 5th edi-
tion.

Flavio De Benedictis, Stefano Faralli, and Roberto
Navigli. 2013. GlossBoot: Bootstrapping multilin-
gual domain glossaries from the Web. In Proc. of
ACL, Sofia, Bulgaria.

Gerard de Melo and Gerhard Weikum. 2010. MENTA:
Inducing Multilingual Taxonomies from Wikipedia.
In Proc. of CIKM, pages 1099–1108, New York, NY,
USA.

Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proc. of EMNLP, pages 440–
449, Stroudsburg, PA, USA.

Katrin Erk. 2007. A Simple, Similarity-based Model
for Selectional Preferences. In Proc. of ACL, pages
216–223, Prague, Czech Republic.

Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,

Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91–134.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proc. of EMNLP, pages 1535–1545,
Edinburgh, UK.

Stefano Faralli and Roberto Navigli. 2013. A Java
framework for multilingual definition and hypernym
extraction. In Proc. of ACL, Comp. Volume, Sofia,
Bulgaria.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.

David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: an overview of the
DeepQA project. AI Magazine, 31(3):59–79.

Hagen Fürstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135–
171.

Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
HLT-NAACL, pages 1–8, Edmonton, Canada.

Rebecca Green, Bonnie J. Dorr, and Philip Resnik.
2004. Inducing Frame Semantic Verb Classes from
WordNet and LDOCE. In Proc. of ACL, pages 375–
382, Barcelona, Spain.

Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. University Press Group Limited.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of COL-
ING, pages 539–545, Nantes, France.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28–61.

Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and artificial intelligence: The
story so far. Artificial Intelligence, 194:2–27.

Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-Specific Semantic Class Taggers from (Al-
most) Nothing. In Proc. of ACL, pages 275–285,
Uppsala, Sweden.

Sean P. Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with Web-based corrobo-
ration. In Proc. of UMSLLS, pages 18–26, Strouds-
burg, PA, USA.

Rubén Izquierdo, Armando Suárez, and German Rigau.
2009. An Empirical Study on Class-Based Word
Sense Disambiguation. In Proc. of EACL, pages
389–397, Athens, Greece.

Hyeju Jang and Jack Mostow. 2012. Inferring se-
lectional preferences from part-of-speech n-grams.
In Proc. of EACL, pages 377–386, Stroudsburg, PA,
USA.

1231



Boris Katz, Jimmy J. Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew W. Bilotti, Sue Felshin, Aaron
Fernandes, Gregory Marton, and Federico Mora.
2003. Integrating Web-based and Corpus-based
Techniques for Question Answering. In Proc. of
TREC, pages 426–435, Gaithersburg, Maryland.

Zornitsa Kozareva and Eduard Hovy. 2010. Learning
Arguments and Supertypes of Semantic Relations
Using Recursive Patterns. In Proc. of ACL, pages
1482–1491, Uppsala, Sweden.

Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Proc.
ACL/HLT, pages 1048–1056, Columbus, Ohio.

Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proc. of ISWC 2012, Part I, pages 263–278,
Boston, MA.

Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217–244.

Rada Mihalcea and Dan Moldovan. 2001. eXtended
WordNet: Progress report. In Proceedings of the
NAACL-01 Workshop on WordNet and Other Lexi-
cal Resources, pages 95–100, Pittsburgh, Penn.

Thahir Mohamed, Estevam Hruschka, and Tom
Mitchell. 2011. Discovering Relations between
Noun Categories. In Proc. of EMNLP, pages 1447–
1455, Edinburgh, Scotland, UK.

Andrea Moro and Roberto Navigli. 2012. WiSeNet:
Building a Wikipedia-based semantic network with
ontologized relations. In Proc. of CIKM, pages
1672–1676, Maui, HI, USA.

Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proc. of IJCAI,
Beijing, China.

Vivi Nastase and Michael Strube. 2013. Transform-
ing wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62–85.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.

Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for Definition and Hypernym
Extraction. In Proc. of ACL, pages 1318–1327, Up-
psala, Sweden.

Roberto Navigli. 2009. Word Sense Disambiguation:
A survey. ACM Computing Surveys, 41(2):1–69.

Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: learning inferential selec-
tional preferences. In Proc. of NAACL, pages 564–
571, Rochester, NY.

Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of CIKM, pages
137–145, New York, NY, USA.

Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proc. of COLING,
pages 793–800, Sydney, Australia.

Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively built
knowledge repository. Artificial Intelligence, 175(9-
10):1737–1756.

Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127–159.

Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proc. of ACL, pages 424–434, Uppsala,
Sweden. ACL.

Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a seman-
tically annotated lexicon via EM-based clustering.
In Proc. of ACL, pages 104–111, Stroudsburg, PA,
USA.

Diarmuid O Séaghdha. 2010. Latent variable models
of selectional preference. In Proc. of ACL, pages
435–444, Uppsala, Sweden.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In NIPS, pages 1297–1304, Cam-
bridge, Mass.

Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proc. of ACL 2012, System Demonstra-
tions, pages 73–78, Jeju Island, Korea.

Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proc. ACL,
pages 8–15, Stroudsburg, PA, USA.

M. Thelen and E. Riloff. 2002. A Bootstrapping
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. In Proc. of EMNLP, pages
214–221, Salt Lake City, UT, USA.

Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3).

Yorick Wilks. 1975. A preferential, pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53–74.

Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. In Proc. of ACL, pages
118–127, Uppsala, Sweden.

Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun’ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proc. of
EMNLP, pages 284–292, Stroudsburg, PA, USA.

David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. of ACL, pages 189–196, Cambridge, MA,
USA.

Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.

Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: open informa-
tion extraction on the web. In Proc. of NAACL-
Demonstrations, pages 25–26, Stroudsburg, PA,
USA.

1232


