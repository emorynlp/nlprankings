










































Instance Selection for Machine Translation using Feature Decay Algorithms


Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272–283,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics

Instance Selection for Machine Translation using Feature Decay
Algorithms

Ergun Biçici
Koç University

34450 Sariyer, Istanbul, Turkey
ebicici@ku.edu.tr

Deniz Yuret
Koç University

34450 Sariyer, Istanbul, Turkey
dyuret@ku.edu.tr

Abstract

We present an empirical study of instance se-
lection techniques for machine translation. In
an active learning setting, instance selection
minimizes the human effort by identifying
the most informative sentences for transla-
tion. In a transductive learning setting, se-
lection of training instances relevant to the
test set improves the final translation qual-
ity. After reviewing the state of the art in
the field, we generalize the main ideas in a
class of instance selection algorithms that use
feature decay. Feature decay algorithms in-
crease diversity of the training set by devalu-
ing features that are already included. We
show that the feature decay rate has a very
strong effect on the final translation quality
whereas the initial feature values, inclusion
of higher order features, or sentence length
normalizations do not. We evaluate the best
instance selection methods using a standard
Moses baseline using the whole 1.6 million
sentence English-German section of the Eu-
roparl corpus. We show that selecting the
best 3000 training sentences for a specific
test sentence is sufficient to obtain a score
within 1 BLEU of the baseline, using 5% of
the training data is sufficient to exceed the
baseline, and a∼ 2 BLEU improvement over
the baseline is possible by optimally selected
subset of the training data. In out-of-domain
translation, we are able to reduce the train-
ing set size to about 7% and achieve a similar
performance with the baseline.

1 Introduction

Statistical machine translation (SMT) makes use
of a large number of parallel sentences, sentences
whose translations are known in the target lan-
guage, to derive translation tables, estimate param-
eters, and generate the actual translation. Not all
of the parallel corpus nor the translation table that
is generated is used during decoding a given set
of test sentences and filtering is usually performed
for computational advantage (Koehn et al., 2007).
Some recent regression-based statistical machine
translation systems rely on a small sized training
data to learn the mappings between source and tar-
get features (Wang and Shawe-Taylor, 2008; Ser-
rano et al., 2009; Bicici and Yuret, 2010). Regres-
sion has some computational disadvantages when
scaling to large number of training instances.

Previous work shows that the more the training
data, the better the translations become (Koehn,
2006). However, with the increased size of the
parallel corpus there is also the added noise, mak-
ing relevant instance selection important. Phrase-
based SMT systems rely heavily on accurately
learning word alignments from the given parallel
corpus. Proper instance selection plays an impor-
tant role in obtaining a small sized training set with
which correct alignments can be learned. Word-
level translation accuracy is also affected by the
number of times a word occurs in the parallel cor-
pus (Koehn and Knight, 2001). Koehn and Knight
find that about 50 examples per word are required
to achieve a performance close to using a bilingual
lexicon in their experiments. Translation perfor-
mance can improve as we include multiple possi-
ble translations for a given word, which increases

272



the diversity of the training set.
Transduction uses test instances, which can

sometimes be accessible at training time, to learn
specific models tailored towards the test set which
also reduces computation by not using the full
training set. Transductive retrieval selects train-
ing data close to the test set given a parallel corpus
and a test set. This work shows that transductive
retrieval of the training set for statistical machine
translation allows us to achieve a performance bet-
ter than using all of the parallel corpus. When se-
lecting training data, we seek to maximize the cov-
erage or the percentage of test source and target
features (i.e. n-grams) found in the training set us-
ing minimal number of target training features and
a fixed number of training instances. Diversifying
the set of training sentences can help us increase
the coverage. We show that target coverage bounds
the achievable BLEU score with a given training
set and small increases can result in large increases
on this BLEU bound.

We develop the feature decay algorithms (FDA)
that aim to maximize the coverage of the target
language features and achieve significant gains in
translation performance. We find that decaying
feature weights has significant effect on the per-
formance. We achieve improvements of ∼2 BLEU
points using about 20% of the available training
data in terms of target words and ∼1 BLEU points
with only about 5%. We show that selecting 3000
instances for a test sentence is sufficient to obtain
a score within 1 BLEU of the baseline. In the out-
of-domain translation task, we are able to reduce
the training set size to its 7% to achieve a similar
performance with the baseline.

The next section reviews related previous work.
We discuss the FDA in section 3. Section 4
presents our coverage and translation results both
in and out-of-domain and includes an instance se-
lection method also designed for improving word
alignment results. We list our contributions in the
last section.

2 Related Work
Transductive learning makes use of test instances,
which can sometimes be accessible at training
time, to learn specific models tailored towards the
test set. Selection of training instances relevant to
the test set improves the final translation quality as

in transductive learning and decreases human ef-
fort by identifying the most informative sentences
for translation as in active learning. Instance se-
lection in a transductive learning framework se-
lects the best instances for a given test set (Lü et
al., 2007). Active learning selects training samples
that will benefit the learning algorithm the most
over the unlabeled dataset U from a labeled train-
ing set L or from U itself after labeling (Banko and
Brill, 2001). Active learning in SMT selects which
instances to add to the training set to improve the
performance of a baseline system (Haffari et al.,
2009; Ananthakrishnan et al., 2010). Recent work
involves selecting sentence or phrase translation
tasks for external human effort (Bloodgood and
Callison-Burch, 2010). Below we present exam-
ples of both with a label indicating whether they
follow an approach close to active learning [AL] or
transductive learning [TL] and in our experiments
we use the transductive framework.

TF-IDF [TL]: Lü et al. (2007) use tf-idf infor-
mation retrieval technique based cosine score to se-
lect a subset of the parallel corpus close to the test
set for SMT training. They outperform the baseline
system when the top 500 training instances per test
sentence are selected. The terms used in their tf-idf
measure correspond to words where this work fo-
cuses on bigram feature coverage. When the com-
bination of the top N selected sentences are used
as the training set, they show increase in the per-
formance at the beginning and decrease when 2000
sentences are selected for each test sentence.

N-gram coverage [AL]: Eck et al. (2005) use
n-gram feature coverage to sort and select training
instances using the following score:

φNGRAM (S) =

∑n
i=1

∑
unseen x ∈ Xi(S) C(x)

|S|
,

(1)
for sentence S with Xi(S) storing the i-grams
found in S and C(x) returning the count of x in
the parallel corpus. φNGRAM score sums over un-
seen n-grams to increase the coverage of the train-
ing set. The denominator involving the length of
the sentence takes the translation cost of the sen-
tence into account. Eck et al. (2005) also note
that longer sentences are more difficult for train-
ing SMT models. In their experiments, they are
not able to reach a performance above the baseline

2

273



system’s BLEU score, which is using all of the par-
allel corpus, but they achieve close performance by
using about 15% of the parallel corpus.

DWDS [AL]: Density weighted diversity sam-
pling (DWDS) (Ambati et al., 2010) score tries to
select sentences containing the n-gram features in
the unlabeled dataset U while increasing the di-
versity among the sentences selected, L (labeled).
DWDS increases the score of a sentence with in-
creasing frequency of its n-grams found in U and
decreases with increasing frequency in the already
selected set of sentences, L, in favor of diversity.
Let PU (x) denote the probability of feature x in U
and CL(x) denote its count in L. Then:

d(S) =

∑
x∈X(S) PU (x)e

−λCL(x)

|X(S)|
(2)

u(S) =

∑
x∈X(S) I(x 6∈ X(L))

|X(S)|
(3)

φDWDS(S) =
2d(S)u(S)
d(S) + u(S)

, (4)

where X(S) stores the features of S and λ is a
decay parameter. d(S) denotes the density of S
proportional to the probability of its features in U
and inversely proportional to their counts in L and
u(S) its uncertainty, measuring the percentage of
new features in S. These two scores are combined
using harmonic mean. DWDS tries to select sen-
tences containing similar features in U with high
diversity. In their active learning experiments, they
selected 1000 training instances in each iteration
and retrained the SMT system.

Log-probability ratios [AL]: Haffari et
al. (2009) develop sentence selection scores using
feature counts in L and U , increasing for frequent
features in U and decreasing for frequent features
in L. They use geometric and arithmetic averages
of log-probability ratios in an active learning
setting where 200 sentences from U are selected
and added to L with their translations for 25
iterations (Haffari et al., 2009). Later, Haffari
et al. (2009) distinguish between features found
in the phrase table, xreg, and features not found,
xoov. OOV features are segmented into subfeatures
(i.e. feature “go to school” is segmented as:
(go to school), (go)(to school), (go to)(school),
(go)(to)(school)). Expected log probability ratio

(ELPR) score is used:

φELPR(S) = 0.4|Xreg(S)|
∑

x∈Xreg(S)

log
PU (x)
PL(x)

+ 0.6|Xoov(S)|
∑

x∈Xoov(S)

∑
h∈H(x)

1
|H(x)|

∑
y∈Yh(x)

log
PU (y)
PL(y)

,

(5)
where H(x) return the segmentations of x and
Yh(x) return the features found in segment h.
φELPR performs better than geometric average in
their experiments (Haffari and Sarkar, 2009).

Perplexity [AL & TL]: Perplexity of the train-
ing instance as well as inter-SMT-system disagree-
ment are also used to select training data for trans-
lation models (Mandal et al., 2008). The increased
difficulty in translating a parallel sentence or its
novelty as found by the perplexity adds to its im-
portance for improving the SMT model’s perfor-
mance. A sentence having high perplexity (a rare
sentence) in L and low perplexity (a common sen-
tence) in U is considered as a candidate for addi-
tion. They are able to improve the performance
of a baseline system trained on some initial corpus
together with additional parallel corpora using the
initial corpus and part of the additional data.

Alignment [TL]: Uszkoreit et al. (2010) mine
parallel text to improve the performance of a base-
line translation model on some initial document
translation tasks. They retrieve similar documents
using inverse document frequency weighted cosine
similarity. Then, they filter nonparallel sentences
using their word alignment performance, which is
estimated using the following score:

score(A) =
∑

(s,t)∈A

ln
p(s, t)
p(s)p(t)

, (6)

where A stands for an alignment between source
and target words and the probabilities are estimated
using a word aligned corpus. The produced paral-
lel data is used to expand a baseline parallel corpus
and shown to improve the translation performance
of machine translation systems.

3 Instance Selection with Feature
Decay

In this section we will describe a class of instance
selection algorithms for machine translation that

3

274



use feature decay, i.e. increase the diversity of the
training set by devaluing features that have already
been included. Our abstraction makes three com-
ponents of such algorithms explicit permitting ex-
perimentation with their alternatives:

• The value of a candidate training sentence as
a function of its features.

• The initial value of a feature.

• The update of the feature value as instances
are added to the training set.

A feature decay algorithm (FDA) aims to max-
imize the coverage of the target language features
(such as words, bigrams, and phrases) for the test
set. A target language feature that does not ap-
pear in the selected training instances will be dif-
ficult to produce regardless of the decoding algo-
rithm (impossible for unigram features). In gen-
eral we do not know the target language features,
only the source language side of the test set is avail-
able. Unfortunately, selecting a training instance
with a particular source language feature does not
guarantee the coverage of the desired target lan-
guage feature. There may be multiple translations
of a feature appropriate for different senses or dif-
ferent contexts. For each source language feature
in the test set, FDA tries to find as many train-
ing instances as possible to increase the chances
of covering the appropriate target language feature.
It does this by reducing the value of the features
that are already included after picking each train-
ing instance. Algorithm 1 gives the pseudo-code
for FDA.

The input to the algorithm is a parallel corpus,
the number of desired training instances, and the
source language features of the test set. We use
unigram and bigram features; adding trigram fea-
tures does not seem to significantly affect the re-
sults. The user has the option of running the algo-
rithm for each test sentence separately, then possi-
bly combining the resulting training sets. We will
present results with these variations in Section 4.

The first foreach loop initializes the value of
each test set feature. We experimented with ini-
tial feature values that are constant, proportional
to the length of the n-gram, or log-inverse of the
corpus frequency. We have observed that the ini-
tial value does not have a significant effect on the

Algorithm 1: The Feature Decay Algorithm
Input: Bilingual corpus U , test set features F ,

and desired number of training
instances N .

Data: A priority queue Q, sentence scores
score, feature values fvalue.

Output: Subset of the corpus to be used as the
training data L ⊆ U .

foreach f ∈ F do1
fvalue(f)← init(f,U)2

foreach S ∈ U do3
score(S)←

∑
f∈features(S) fvalue(f)4

push(Q, S,score(S))5
while |L| < N do6
S ← pop(Q)7
score(S)←

∑
f∈features(S) fvalue(f)8

if score(S) ≥ topval(Q) then9
L ← L ∪ {S}10
foreach f ∈ features(S) do11

fvalue(f)← decay(f,U ,L)12
else13
push(Q, S,score(S))14

quality of training instances selected. The feature
decay rule dominates the behavior of the algorithm
after the first few iterations. However, we prefer
the log-inverse values because they lead to fewer
score ties among candidate instances and result in
faster running times.

The second foreach loop initializes the score for
each candidate training sentence and pushes them
onto a priority queue. The score is calculated as the
sum of the feature values. Note that as we change
the feature values, the sentence scores in the prior-
ity queue will no longer be correct. However they
will still be valid upper bounds because the fea-
ture values only get smaller. Features that do not
appear in the test set are considered to have zero
value. This observation can be used to speed up
the initialization by using a feature index and only
iterating over the sentences that have features in
common with the test set.

Finally the while loop populates the training set
by picking candidate sentences with the highest
scores. This is done by popping the top scoring
candidate S from the priority queue at each itera-
tion. We recalculate its score because the values

4

275



of its features may have changed. We compare the
recalculated score of S with the score of the next
best candidate. If the score of S is equal or better
we are sure that it is the top candidate because the
scores in the priority queue are upper bounds. In
this case we place S in our training set and decay
the values of its features. Otherwise we push S
back on the priority queue with its updated score.

The feature decay function on Line 12 is the
heart of the algorithm. Unlike the choice of fea-
tures (bigram vs trigram) or their initial values
(constant vs log–inverse–frequency) the rate of de-
cay has a significant effect on the performance. We
found it is optimal to reduce feature values at a rate
of 1/n where n is the current training set count
of the feature. The results get significantly worse
with no feature decay. They also get worse with
faster, exponential feature decay, e.g. 1/2n. Ta-
ble 1 presents the experimental results that support
these conclusions. We use the following settings
for the experiments in Section 4:

init(f,U) = 1 or log(|U|/cnt(f,U))

decay(f,U ,L) = init(f,U)
1 + cnt(f,L)

or
init(f,U)
1 + 2cnt(f,L)

init decay en→de de→en
1 none .761 .484 .698 .556
log(1/f) none .855 .516 .801 .604
1 1/n .967 .575 .928 .664
log(1/f) 1/n .967 .570 .928 .656
1 1/2n .967 .553 .928 .653
log(1/f) 1/2n .967 .557 .928 .651

Table 1: FDA experiments. The first two columns
give the initial value and decay formula used for
features. f is the corpus frequency of a feature
and n is its count in selected instances. The next
four columns give the expected coverage of the
source and target language bigrams of a test sen-
tence when 100 training sentences are selected.

4 Experiments
We perform translation experiments on the
English-German language pair using the parallel

corpus provided in WMT’10 (Callison-Burch et
al., 2010). The English-German section of the Eu-
roparl corpus contains about 1.6 million sentences.
We perform in-domain experiments to discriminate
among different instance selection techniques bet-
ter in a setting with low out-of-vocabulary rate. We
randomly select the test set test with 2, 588 tar-
get words and separate development set dev with
26, 178 target words. We use the language model
corpus provided in WMT’10 (Callison-Burch et
al., 2010) to build a 5-gram model.

We use target language bigram coverage, tcov,
as a quality measure for a given training set, which
measures the percentage of the target bigram fea-
tures of the test sentence found in a given training
set. We compare tcov and the translation perfor-
mance of FDA with related work. We also perform
small scale SMT experiments where only a couple
of thousand training instances are used for each test
sentence.

4.1 The Effect of Coverage on Translation

BLEU (Papineni et al., 2001) is a precision based
measure and uses n-gram match counts up to or-
der n to determine the quality of a given transla-
tion. The absence of a given word or translating
it as another word interrupts the continuity of the
translation and decreases the BLEU score even if
the order among the words is determined correctly.
Therefore, the target coverage of an out-of-domain
test set whose translation features are not found in
the training set bounds the translation performance
of an SMT system.

We estimate this translation performance bound
from target coverage by assuming that the miss-
ing tokens can appear randomly at any location of
a given sentence where sentence lengths are nor-
mally distributed with mean 25.6 and standard de-
viation 14.1. This is close to the sentence length
statistics of the German side Europarl corpus used
in WMT’10 (WMT, 2010). We replace all un-
known words found with an UNK token and calcu-
late the BLEU score. We perform this experiment
for 10, 000 instances and repeat for 10 times.

The obtained BLEU scores for target cover-
age values is plotted in Figure 1 with label esti-
mate. We also fit a third order polynomial func-
tion of target coverage 0.025 BLEU scores above
the estimate values to show the similarity with the

5

276



0.0 0.2 0.4 0.6 0.8 1.0
tcov

0.0

0.2

0.4

0.6

0.8

1.0

1.2

B
LE

U

BLEU vs. tcov

estimate
f(x)=ax^3 + bx^2 + cx + d

Figure 1: Effect of coverage on translation perfor-
mance. BLEU bound is a third-order function of
target coverage. High coverage→ High BLEU.

BLEU scores bound estimated, whose parameters
are found to be [0.56, 0.53,−0.09, 0.003] with a
least-squares fit. Figure 1 shows that the BLEU
score bound obtained has a third-order polyno-
mial relationship with target coverage and small
increases in the target coverage can result in large
increases on this BLEU bound.

4.2 Coverage Results

We select N training instances per test sentence
using FDA (Algorithm 1), TF-IDF with bigram
features, NGRAM scoring (Equation 1), DWDS
(Equation 4), and ELPR (Equation 5) techniques
from previous work. For the active learning algo-
rithms, source side test corpus becomes U and the
selected training set L. For all the techniques, we
compute 1-grams and 2-grams as the features used
in calculating the scores and add only one sentence
to the training set at each iteration except for TF-
IDF. We set λ parameter of DWDS to 1 as given
in their paper. We adaptively select the top scor-
ing instance at each step from the set of possible
sentences U with a given scorer φ(.) and add the
instance to the training set, L, until the size of L
reaches N for the related work other than TF-IDF.
We test all algorithms in this transductive setting.

We measure the bigram coverage when all of
the training sentences selected for each test sen-
tence are combined. The results are presented in
Figure 2 where the x-axis is the number of words

of the training set and y-axis is the target cover-
age obtained. FDA has a steep slope in its increase
and it is able to reach target coverage of ∼ 0.84.
DWDS performs worse initially but its target cov-
erage improve after a number of instances are se-
lected due to its exponential feature decay proce-
dure. TF-IDF performs worse than DWDS and it
provides a fast alternative to FDA instance selec-
tion but with some decrease in coverage. ELPR
and NGRAM instance selection techniques per-
form worse. NGRAM achieves better coverage
than ELPR, although it lacks a decay procedure.

When we compare the sentences selected, we
observe that FDA prefers longer sentences due to
summing feature weights and it achieves larger tar-
get coverage value. NGRAM is not able to discrim-
inate between sentences well and a lot of sentences
of the same length get the same score when the un-
seen n-grams belong to the same frequency class.
The statistics of L obtained with the instance se-
lection techniques differ from each other as given
in Table 2, where N = 1000 training instances se-
lected per test sentence. We observe that DWDS
has fewer unique target bigram features than TF-
IDF although it selects longer target sentences.
NGRAM obtains a large number of unique target
bigrams although its selected target sentences have
similar lengths with DWDS and ELPR prefers short
sentences.

Technique Unique bigrams Words per sent tcov
FDA 827,928 35.8 .74

DWDS 412,719 16.7 .67
TF-IDF 475,247 16.2 .65

NGRAM 626,136 16.6 .55
ELPR 172,703 10.9 .35

Table 2: Statistics of the obtained target L forN =
1000.

4.3 Translation Results

We develop separate phrase-based SMT models
using Moses (Koehn et al., 2007) using default set-
tings with maximum sentence length set to 80 and
obtained baseline system score as 0.3577 BLEU.
We use the training instances selected by FDA in

6

277



104 105 106 107

Training Set Size (words)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

tc
o
v

tcov vs. Training Set Size (words)

DWDS
ELPR
FDA
NGRAM
TFIDF

Figure 2: Target coverage curve comparison with previous work. Figure shows the rate of increase in
tcov as the size of L increase.

three learning settings:

L∪ L is the union of the instances selected for
each test sentence.

L∪F L is selected using all of the features found
in the test set.

LI L is the set of instances selected for each test
sentence.

We develop separate Moses systems with each
training set and LI corresponds to developing a
Moses system for each test sentence. L∪ results
are plot in Figure 3 where we increasingly select
N ∈ {100, 200, 500, 1000, 2000, 3000, 5000,
10000} instances for each test sentence for train-
ing. The improvements over the baseline are sta-
tistically significant with paired bootstrap resam-
pling using 1000 samples (Koehn, 2004). As we
select more instances, the performance of the SMT
system increases as expected and we start to see a
decrease in the performance after selecting ∼107
target words. We obtain comparable results for the
de-en direction. The performance increase is likely
to be due to the reduction in the number of noisy or
irrelevant training instances and the increased pre-
cision in the probability estimates in the generated

phrase tables.

105 106 107 108

Training Set Size (words)

0.30

0.31

0.32

0.33

0.34

0.35

0.36

0.37

0.38

B
LE

U

0.3058

0.3318

0.341

0.3645

0.3697

0.3758

0.3622
0.3653

0.3577

BLEU vs. Training Set Size (words)

Figure 3: BLEU vs. the number of target words in
L∪.

L∪F results given in Table 3 show that we can
achieve within 1 BLEU performance using about
3% of the parallel corpus target words (30,000 in-
stances) and better performance using only about
5% (50,000 instances).

The results with LI when building an individ-7

278



# sent # target words BLEU NIST
10,000 449,116 0.3197 5.7788
20,000 869,908 0.3417 6.0053
30,000 1,285,096 0.3492 6.0246
50,000 2,089,403 0.3711 6.1561

100,000 4,016,124 0.3648 6.1331
ALL 41,135,754 0.3577 6.0653

Table 3: Performance for en-de using L∪F . ALL
corresponds to the baseline system using all of the
parallel corpus. bold correspond to statistically
significant improvement over the baseline result.

ual Moses model for each test sentence are given
in Table 4. Individual SMT training and transla-
tion can be preferable due to smaller computational
costs and high parallelizability. As we translate
a single sentence with each SMT system, tuning
weights becomes important. We experiment three
settings: (1) using 100 sentences for tuning, which
are randomly selected from dev.1000, (2) using the
mean of the weights obtained in (1), and (3) us-
ing the weights obtained in the union learning set-
ting (L∪). We observe that we can obtain a perfor-
mance within 2 BLEU difference to the baseline
system by training on 3000 instances per sentence
(underlined) using the mean weights and 1 BLEU
difference using the union weights. We also exper-
imented with increasing the N -best list size used
during MERT optimization (Hasan et al., 2007),
with increased computational cost, and observed
some increase in the performance.

N 100 dev sents Mean Union
1000 0.3149 0.3242 0.3354
2000 0.3258 0.3352 0.3395
3000 0.3270 0.3374 0.3501
5000 0.3217 0.3303 0.3458

Table 4: LI performance for en-de using 100 sen-
tences for tuning or mean of the weights or dev
weights obtained with the union setting.

Comparison with related work: Table 5
presents the translation results compared with pre-
vious work selecting 1000 instances per test sen-
tence. We observe that coverage and translation
performance are correlated. Although the cover-
age increase of DWDS and FDA appear similar,

due to the third-order polynomial growth of BLEU
with respect to coverage, we achieve large BLEU
gains in translation. We observe increased BLEU
gains when compared with the results of TF-IDF,
NGRAM, and ELPR in order.

FDA DWDS TF-IDF NGRAM ELPR
0.3645 0.3547 0.3405 0.2572 0.2268

Table 5: BLEU results using different techniques
with N = 1000. High coverage→ High BLEU.

We note that DWDS originally selects instances
using the whole test corpus to estimate PU (x) and
selects 1000 instances at each iteration. We exper-
imented with both of these settings and obtained
0.3058 and 0.3029 BLEU respectively. Lower
performance suggest the importance of updating
weights after each instance selection step.

4.4 Instance Selection for Alignment

We have shown that high coverage is an integral
part of training sets for achieving high BLEU per-
formance. SMT systems also heavily rely on the
word alignment of the parallel corpus to derive
a phrase table that can be used for translation.
GIZA++ (Och and Ney, 2003) is commonly used
for word alignment and phrase table generation,
which is prone to making more errors as the length
of the training sentence increase (Ravi and Knight,
2010). Therefore, we analyze instance selection
techniques that optimize coverage and word align-
ment performance and at the same time do not
produce very long sentences. Too few words per
sentence may miss the phrasal structure, whereas
too many words per sentence may miss the actual
word alignment for the features we are interested.
We are also trying to retrieve relevant training sen-
tences for a given test sentence to increase the fea-
ture alignment performance.

Shortest: A baseline strategy that can minimize
the training feature set’s size involves selecting the
shortest translations containing each feature.

Co-occurrence: We use co-occurrence of
words in the parallel corpus to retrieve sentences
containing co-occurring items. Dice’s coeffi-
cient (Dice, 1945) is used as a heuristic word align-
ment technique giving an association score for
each pair of word positions (Och and Ney, 2003).

8

279



We define Dice’s coefficient score as:

dice(x, y) =
2C(x, y)
C(x)C(y)

, (7)

where C(x, y) is the number of times x and y co-
occur and C(x) is the count of observing x in the
selected training set. Given a test source sentence,
SU , we can estimate the goodness of a training
sentence pair, (S, T ), by the sum of the alignment
scores:

φdice(SU , S, T ) =

X
x∈X(SU )

|T |X
j=1

X
y∈Y (x)

dice(y, Tj)

|T | log |S| ,

(8)
where X(SU ) stores the features of SU and Y (x)
lists the tokens in feature x. The difficulty of word
aligning a pair of training sentences, (S, T ), can be
approximated by |S||T |. We use a normalization
factor proportional to |T | log |S|.

The average target words per sentence using
φdice drops to 26.2 compared to 36.3 of FDA.
We still obtain a better performance than the base-
line en-de system with the union of 1000 train-
ing instances per sentence with 0.3635 BLEU and
6.1676 NIST scores. Coverage comparison with
FDA shows slight improvement with lower number
of target bigrams and similar trend for others (Fig-
ure 4). We note that shortest strategy achieves bet-
ter performance than both ELPR and NGRAM. We
obtain 0.3144 BLEU and 5.5 NIST scores in the
individual translation task with 1000 training in-
stances per sentence and 0.3171 BLEU and 5.4662
NIST scores when the mean of the weights is used.

4.5 Out-of-domain Translation Results
We have used FDA and dice algorithms to select
training sets for the out-of-domain challenge test
sets used in (Callison-Burch et al., 2011). The
parallel corpus contains about 1.9 million training
sentences and the test set contain 3003 sentences.
We built separate Moses systems using all of the
parallel corpus for the language pairs en-de, de-en,
en-es, and es-en. We created training sets using
all of the features of the test set to select train-
ing instances. The results given in Table 6 show
that we can achieve similar BLEU performance us-
ing about 7% of the parallel corpus target words
(200,000 instances) using dice and about 16% us-
ing FDA. In the out-of-domain translation task, we

are able to reduce the training set size to achieve
a performance close to the baseline. The sample
points presented in the table is chosen proportional
to the relative sizes of the parallel corpus sizes of
WMT’10 and WMT’11 datasets and the training
set size of the peak in Figure 3. We may be able
to achieve better performance in the out-of-domain
task as well. The sample points in Table 6 may be
on either side of the peak.

5 Contributions

We have introduced the feature decay algorithms
(FDA), a class of instance selection algorithms that
use feature decay, which achieves better target cov-
erage than previous work and achieves significant
gains in translation performance. We find that de-
caying feature weights has significant effect on the
performance. We demonstrate that target coverage
and translation performance are correlated, show-
ing that target coverage is also a good indicator of
BLEU performance. We have shown that target
coverage provides an upper bound on the transla-
tion performance with a given training set.

We achieve improvements of ∼2 BLEU points
using about 20% of the available training data in
terms of target words with FDA and ∼ 1 BLEU
points with only about 5%. We have also shown
that by training on only 3000 instances per sen-
tence we can reach within 1 BLEU difference to
the baseline system. In the out-of-domain transla-
tion task, we are able to reduce the training set size
to achieve a similar performance with the baseline.

Our results demonstrate that SMT systems can
improve their performance by transductive train-
ing set selection. We have shown how to select in-
stances and achieved significant performance im-
provements.

References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.

2010. Active learning and crowd-sourcing for ma-
chine translation. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC’10), Valletta, Malta, May. Euro-
pean Language Resources Association (ELRA).

9

280



104 105 106 107

Total Training Set Size (words)

0.3

0.4

0.5

0.6

0.7

0.8

0.9

tc
o
v

tcov vs. Total Training Set Size (words)

FDA
dice
shortest

0 5000 10000 15000 20000 25000 30000 35000 40000
Average Training Set Size (words)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

tc
o
v

tcov vs. Average Training Set Size (words)

FDA
dice
shortest

Figure 4: Target coverage per target words comparison. Figure shows the rate of increase in tcov as
the size of L increase. Target coverage curves for total training set size is given on the left plot and for
average training set size per test sentence on the right plot.

en-de de-en en-es es-en

BLEU
ALL 0.1376 0.2074 0.2829 0.2919
FDA 0.1363 0.2055 0.2824 0.2892
dice 0.1374 0.2061 0.2834 0.2857

# target words ×106
ALL 47.4 49.6 52.8 50.4
FDA 7.9 8.0 8.7 8.2
dice 6.9 7.0 3.9 3.6

% of ALL FDA 17 16 16 16
dice 14 14 7.4 7.1

Table 6: Performance for the out-of-domain task of (Callison-Burch et al., 2011). ALL corresponds to
the baseline system using all of the parallel corpus.

10

281



Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 626–635, Cambridge, MA, October.
Association for Computational Linguistics.

Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages 26–
33, Toulouse, France, July. Association for Computa-
tional Linguistics.

Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the ACL 2010
Joint Fifth Workshop on Statistical Machine Transla-
tion and Metrics MATR, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.

Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 854–
864, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR. Associa-
tion for Computational Linguistics, Uppsala, Sweden,
July.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2011. Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation. Edinburgh, England, July.

Lee R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297–
302.

Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram coverage. In Proceedings of
the 10th Machine Translation Summit, MT Summit X,
pages 227–234, Phuket, Thailand, September.

Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 181–189, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.

Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based ma-

chine translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 415–423, Boulder,
Colorado, June. Association for Computational Lin-
guistics.

Saša Hasan, Richard Zens, and Hermann Ney. 2007.
Are very large N-best lists useful for SMT? In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 57–60, Rochester, New York, April. As-
sociation for Computational Linguistics.

Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Pro-
ceedings of the 2001 Conference on Empirical Meth-
ods in Natural Language Processing.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 177–180, Prague, Czech Republic,
June.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Philipp Koehn. 2006. Statistical machine translation:
the basic, the novel, and the speculative. Tutorial at
EACL 2006.

Yajuan Lü, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343–350, Prague, Czech Republic, June. Association
for Computational Linguistics.

A. Mandal, D. Vergyri, W. Wang, J. Zheng, A. Stol-
cke, G. Tur, D. Hakkani-Tur, and N.F. Ayan. 2008.
Efficient data selection for machine translation. In
Spoken Language Technology Workshop, 2008. SLT
2008. IEEE, pages 261 –264.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In ACL ’02: Proceed-
ings of the 40th Annual Meeting on Association for

11

282



Computational Linguistics, pages 311–318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

Sujith Ravi and Kevin Knight. 2010. Does giza++
make search errors? Computational Linguistics,
36(3):295–302.

Nicolas Serrano, Jesus Andres-Ferrer, and Francisco
Casacuberta. 2009. On a kernel regression approach
to machine translation. In Iberian Conference on Pat-
tern Recognition and Image Analysis, pages 394–401.

Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1101–1109, Beijing, China,
August. Coling 2010 Organizing Committee.

Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155–158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

WMT. 2010. ACL Workshop: Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
July.

12

283


