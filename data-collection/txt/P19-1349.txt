



















































Multi-grained Attention with Object-level Grounding for Visual Question Answering


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3595‚Äì3600
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

3595

Multi-grained Attention with Object-level Grounding
for Visual Question Answering

Pingping Huang, Jianhui Huang, Yuqing Guo, Min Qiao and Yong Zhu
Baidu Inc., Beijing, China

{huangpingping,huangjianhui,guoyuqing,qiaomin,zhuyong}@baidu.com

Abstract

Attention mechanisms are widely used in Vi-
sual Question Answering (VQA) to search for
visual clues related to the question. Most ap-
proaches train attention models from a coarse-
grained association between sentences and im-
ages, which tends to fail on small objects
or uncommon concepts. To address this
problem, this paper proposes a multi-grained
attention method. It learns explicit word-
object correspondence by two types of word-
level attention complementary to the sentence-
image association. Evaluated on the VQA
benchmark, the multi-grained attention model
achieves competitive performance with state-
of-the-art models. And the visualized atten-
tion maps demonstrate that addition of object-
level groundings leads to a better understand-
ing of the images and locates the attended ob-
jects more precisely.

1 Introduction

Visual Question Answering (Antol et al., 2015;
Goyal et al., 2017a) is a multi-modal task requir-
ing to provide an answer to the question with ref-
erence to a given image. Most current VQA sys-
tems resort to deep neural networks and solve the
problem by end-to-end learning. First the question
and the image are encoded into semantic represen-
tations independently. Then the multi-modal fea-
tures are fused into one unified representation for
which the answer is predicted (Malinowski et al.,
2015; Fukui et al., 2016; Anderson et al., 2018).

A key point to a successful VQA system is to
discover the most relevant image regions to the
question. This is commonly resolved by attention
mechanisms, where a spatial attention distribution
highlighting the visual focus is computed accord-
ing to the similarity between the whole question
and image regions (Xu et al., 2015; Yang et al.,
2016; Lu et al., 2016). Although such coarse

Q: What is the man wearing 
around his face?  A: glasses

Up-down Model: nothing Our Model: glasses

Figure 1: An example of VQA and the attention maps
produced by a state-of-the-art model and our model.

sentence-image alignment reports promising re-
sults in general, it sometimes fails to locate small
objects or understand a complicated scenario. For
the example in Figure 1, the question is ‚ÄúWhat is
the man wearing around his face‚Äù. Human has no
difficulty in finding the visual clue on the people‚Äôs
faces, and accordingly provide the correct answer
‚Äúglasses‚Äù. However, by visualizing the attention
map of a state-of-the-art VQA model, we find that
the attention is mistakenly focused on the men‚Äôs
body rather than their faces.

In order to identify related objects more pre-
cisely, this paper proposes a multi-grained atten-
tion mechanism that involves object-level ground-
ing complementary to the sentence-image associ-
ation. Specifically, a matching model is trained on
an object-detection dataset to learn explicit corre-
spondence between the content words in the ques-
tion and their visual counterparts. And the la-
bels of the detected objects are considered and
their similarity with the questions are computed.
Besides, a more sophisticated language model is
adopted for better representation of the question.
Finally the three types of word-object, word-label
and sentence-image attention are accumulated to
enhance the performance.

The contributions of this paper are twofold.
First, this paper proposes a multi-grained attention
mechanism integrating two types of object fea-
tures that were not previously used in VQA atten-



3596

F-RCNN

Image

ELMo

GloVe

Predicted scores of
candidate answers

LabelEmbGObjects labels

Objects features

WordEmbG

WordEmbE SentenceEmb

Fused feature

Question

+ + =

‚àë
Attended object feature 

Multi-grained 
attention

WL WO SO

Figure 2: The architecture of our proposed model. The enhanced modules are illustrated in dot lines.

tion approaches. Second, the deep contextualized
word representation ELMo (Peters et al., 2018) is
firstly adopted in the VQA task to facilitate a bet-
ter question encoding.

2 Proposed Model

The flowchart of the proposed model is illustrated
in Figure 2. We start from the bottom-up top-
down (up-down) model (Teney et al., 2017; An-
derson et al., 2018), which is the winning entry to
the 2017 VQA challenge. Then this model is en-
hanced with two types of object-level groundings
to explore fine-grained information, and a more
sophisticated language model for better question
representation.

2.1 Image Features

We adopt the object-detection-based approach to
represent the input image. Specifically, follow-
ing Anderson et al. (2018), a state-of-the-art object
detection model Faster R-CNN (Ren et al., 2015)
with ResNet-101 (He et al., 2016) as its backbone
is trained on the Visual Genome (VG) (Krishna
et al., 2016) dataset. Then the trained model1

is applied to identify instances of objects with
bounding boxes belonging to certain categories.
The target categories of this detection model con-
tain 1600 objects and 400 attributes.

For each input image, the top-K objects with
the highest confidence scores are selected to rep-
resent the image. For each object, the output of
ResNet‚Äôs pool-flat-5 layer is used as its visual fea-
ture, which is a 2048-dimensional vector vk. Be-
sides, the label of each object‚Äôs category ck is also
kept as a visually grounded evidence. ck is a N -
dimensional one-hot vector, where N is the vo-
cabulary size. Then the input image is represented

1The model is available at
https://github.com/peteanderson80/bottom-up-attention

by both its object features V = [v1,v2, ...,vK ] ‚àà
R2048√óK and object labels C = [c1, c2, ..., cK ] ‚àà
RN√óK .

2.2 Text Features

In our model, text features include token fea-
tures and sentence features for the question, which
are respectively used for fine-grained and coarse-
grained attention computation.

Word Features Let Q = [q1, ..., qT ] ‚àà RN√óT
denote the one-hot representation for the input
question tokens, where T is the question length,
and N is the vocabulary size. Then each token qt
is turned into two word embeddings: GloVe (Pen-
nington et al., 2014) xGt = qtE

G ‚àà RD1 , and
ELMo xEt = ELMo(qt) ‚àà RD2 . D1 and D2 are
the dimensions of GloVe embedding and ELMo
embedding respectively. EG is the GloVe matrix
pre-trained on the Wikipedia & Gigaword2. The
ELMo embedding is dynamically computed by a
L-layer bi-LSTM language model (Hochreiter and
Schmidhuber, 1997). We use the publicly avail-
able pre-trained ELMo model3 to get the contex-
tualized embeddings.

Sentence Features The above two sets of to-
ken embeddings are then concatenated xt =
[xGt ;x

E
t ] ‚àà RD1+D2 , and fed into a GRU (Cho

et al., 2014) to encode the question sentence. The
final hidden state of the GRU i.e., hT ‚àà RD3 is
taken as sentence feature, where D3 is the hidden
state size for GRU.

2.3 Multi-grained Attentions

Word-Label Matching Attention (WL) Object
category labels are high-level semantic representa-
tion compared to visual pixels, and have proven to

2http://nlp.stanford.edu/projects/glove/
3https://github.com/allenai/allennlp



3597

be useful for both visual tasks like scene classifi-
cation (Li et al., 2010) and multi-modal tasks like
image caption and VQA (Wu et al., 2018).

For VQA task, we observed that the seman-
tic similarity between the object category labels
and the words in the question helps to locate the
referred objects. For the input image in Fig-
ure 1, Faster-RCNN detected objects with labels
of ‚Äúman‚Äù, ‚Äúhead‚Äù. Some labels are exactly the
same as or are semantically close to the words in
the question ‚ÄúWhat is the man wearing around his
face?‚Äù. Therefore, we compute the WL attention
vector, that indicates how much weight we should
give to each of theK objects in the image, in terms
of the semantic similarity between the category la-
bels of the objects and the words in the question.
For the k-th object with label ck we encode it into
GloVe embedding4 lGk = ckE

G, and compute its
attention score by measuring its similarity to the
question GloVe embedding as follows:

sWL(XG, lGk ) = argmax
t

cos(xGt , l
G
k )

aWL(XG,LG) = softmax
(
sWL(XG, lGk )

) (1)
where XG =

[
xG1 , ...,x

G
T

]
‚àà RD1√óT is the

GloVe embeddings for the question tokens. LG =[
lG1 , ..., l

G
k

]
‚àà RD1√óK is the GloVe embeddings

for the objects labels. aWL ‚àà RK is the WL atten-
tion vector. In contrast to Anderson et al. (2018)
that only use objects‚Äô visual features without the
labels, and unlike Wu et al. (2018) that discard the
visual features once the labels are generated, we
utilize both category labels and the visual features
to enhance the fine-grained attention with object-
level grounding.

Word-Object Matching Attention (WO) A
word-object matching module is exploited to
directly evaluate how likely a question word
matches a visual object. The pairwise training
structure of the module is shown in Figure 3. The
training set is constructed on the VG object detec-
tion data. Let (c, b) be a positive sample consist-
ing of the annotated object bounding-box b with
category label c, then a negative sample (c, bÃÑ) is
constructed by randomly replacing b with the ob-
ject bÃÑ in the same image, if bÃÑ is not labelled with

4The reason why GloVe embedding alone is used instead
of ELMo for object labels, is that object labels have no con-
text sentence to derive the context-sensitive ELMo embed-
dings.

object: b

category label: c

man
man

ùíô"#ùíó%

object:b
_

couch

loss

ùë†'( ùíô"#, ùíó% ùë†'( ùíô"#, ùíó%
_

ùíó%
_

Figure 3: Label-object matching module trained on VG
object annotation data.

c. Then, each sample (c, b) is represented as fea-
ture vectors (xGc ,vb), where x

G
c is the GloVe em-

bedding of c, and vb is extracted with the same
Faster R-CNN model as described in section 2.1.
At last, a margin-based pairwise ranking loss is
used to train the model:

sWO(xGc ,vb) = œÉ
(
W s

[
f(W cx

G
c ) ‚ó¶ f(W vvb)

])
loss = max

{
0, Œª‚àí sWO(xGc ,vb) + sWO(xGc ,vbÃÑ)

} (2)
where f is ReLU and œÉ is sigmoid activation

function, ‚ó¶ means element wise multiplication.
W c,W v,W s are weight parameters5. And the
margin is set Œª = 0.5.

After sWO is pre-trained, we forwardly select at
most B noun tokens in the question and compute
the WO attention aWO(X,V ) over the K objects
as follows:

aWO(XG,V ) = softmax

(
B‚àë

b=1

sWO(xGb ,vk)

)
(3)

where the parameters of sWO are fine-tuned in
down-streaming VQA task.

Sentence-Object Attention (SO) Following
previous methods of sentence-level question
guided visual attention, we also use the global
semantic of the whole sentence to guide the focus
on relevant objects. Taking sentence feature hT
and objects features V as input, SO attention
vector aSO is computed as follows:

sSO(hT ,vk) = œÉ (W j [f(W vvk) ‚ó¶ f(W thT )])

aSO(hT ,V ) = softmax
(
sSO(hT ,vk)

) (4)
where f is ReLU, œÉ is sigmoid activation func-

tion, and W j ,W v,W t are weight parameters.
5All bias terms are omitted hereafter for simplicity



3598

Method test-devAll Yes/no Numbers Other
Up-down 65.32 81.82 44.21 56.05
Our Model 67.41 83.60 47.02 58.24

Method test-stdAll Yes/no Numbers Other
Prior 25.98 61.20 0.36 1.17
Language-only 44.26 67.01 31.55 27.37
d-LSTM-n-I 54.22 73.46 35.18 41.83
MCB 62.27 78.82 38.28 53.36
Up-down 65.67 82.20 43.90 56.26
Our Model 67.73 83.88 46.60 58.50

Table 1: Result comparison on VQA v2 dataset. Re-
sults of Prior, Language-only, d-SLTM-n-I, MCB are
reported in Goyal et al. (2017a). Result of up-down
model is reported in Teney et al. (2017).

2.4 Multi-modal Fusion and Answer
Prediction

The above three attentions are summed together
for the final attention vector. Then we get the
weighted visual feature vector va ‚àà R2048 for the
image:

a = aWL + aWO + aSO

va =

K‚àë
k=1

akvk
(5)

Then the question feature hT and the attended
visual feature va are transformed into the same
dimension and fused together with element-wise
multiplication, to get the joint representation vec-
tor r ‚àà RD4 .

r = f(W rthT ) ‚ó¶ f(W rvva) (6)

where f is ReLU, W rt,W rv are weight parame-
ters. Following Teney et al. (2017), we treat VQA
task as a classification problem, and use the bi-
nary cross-entropy loss to take multiple marked
answers into consideration:

sÃÇ = œÉ (f(W ar))

loss =

A‚àë
a=1

salog(sÃÇa)‚àí (1‚àí sa)log(sÃÇa)
(7)

where sÃÇ ‚àà RA is the predicted score over allA an-
swer candidates, sa is the target accuracy score6.

3 Experiments and Analysis

3.1 Settings
Experiments are conducted on VQA v2
dataset (Goyal et al., 2017b). Questions are
trimmed to a maximum of T = 14 words. We set

6accuracy = min(#humans that provided that answer
3

, 1), i.e. an an-
swer is accurate if at least 3 markers provided the answer.

Model All Yes/no Numbers Other
Up-down 63.15 80.07 42.87 55.81
+ WL 64.29 81.75 44.34 56.29
+ WO 64.24 82.00 43.69 56.18
+ ELMO 64.15 81.86 44.11 55.98

Table 2: Model analysis results. Models were trained
on train and evaluated on val set.

the number of detected boxes to K = 36, and set
the dimension of GloVe embeddings and ELMo
embeddings toD1 = 300 andD2 = 1024, respec-
tively. The GRU hidden size for question sentence
is D3 = 1024, and the joint representation r is
of dimension D4 = 2048. Noun tokens count
is set as fixed B = 3 with padding7. Candidate
answers are restricted to the correct answers in
the training set that appear more than a threshold,
which results in a number of A = 3129 answer
candidates. Adamax optimizer (Kingma and Ba,
2014) is used with initial learning rate of 0.002,
and we use a learning rate decay schedule that
reduces the learning rate by a factor of 0.1 every 3
epochs after 8 epochs. The batch size is 512.

3.2 Comparisons with the State-of-the-arts
Table 1 shows the result comparison with the base-
line up-down and other methods in single model
setting. Our model outperforms these previous re-
sults, improving the up-down model from 65.32 to
67.41 on test-dev, and from 65.67 to 67.73 on test-
std. This superior performance can be seen in all
the answer types, especially for the most difficult
ones Numbers, where our model gains significant
+2.81/+2.70 improvement on the test-dev/test-std.

3.3 Model Analysis
To understand the effects of different components,
the performance by adding one certain proposed
component to the baseline is reported in Table 2.
Adding our proposed two branches of fine-grained
WL and WO attentions significantly improves the
baseline performance. The result also verifies that
ELMo embeddings combined with GloVe embed-
dings provide more sophisticated text representa-
tions, thus improves the overall performance.

3.4 Study on Attention Maps
To validate the effectiveness of the enhanced atten-
tion mechanism, we visualize the attentions and

7Though B is set as a fixed value during the whole pro-
cess, it can be variable with trivial modifications for the WO
attention computation.



3599

Q: How many dog ears are shown? 
A:  2

Q: Is there a hat on the bench? 
A: yes

Q:Are the stop signs facing normal? 
A: yes

Q: Does his bow tie 
match his pants?  A: yes

Baseline: 
no Baseline: no

Baseline: 4Baseline: no Baseline: noBaseline: no Baseline: no

Our : 2Our: yes Our : yesOur : yes Our : no

Q: Can you see its paws? 
A: yes 

(a) (b) (c) (d) (e) 

Figure 4: Attention map examples (only top-5 salient regions are shown here).

compare them versus those of the up-down model.
As Figure 4 shows, the addition of object-level
groundings leads to a better understanding of the
images and locates the attended objects more pre-
cisely. For example, in Figure 4(a), for question
‚ÄúCan you see its paws?‚Äù, the attention generated
by our method is focused on the ‚Äúpaws‚Äù, while the
baseline does not focus on the key regions as accu-
rate as we do. In Figure 4(b), for the Numbers type
question ‚ÄúHow many dog ears are shown?‚Äù, our
model gives the strongest attention on the ‚Äúear‚Äù
part of the dog, while the baseline model attends
to the whole dog body. For small object clues, our
model shows more advantage. As shown in the
examples in Figure 4(c), Figure 4(d).

We also notice cases where though the final an-
swer is wrong, our model generates appropriate at-
tention maps. As shown in Figure 4(e), for Yes/no
question ‚ÄúDoes his bow tie match his pants?‚Äù, our
model correctly finds ‚Äútie‚Äù and ‚Äúpants‚Äù object re-
gions, but we suspect that the model does not un-
derstand the meaning of ‚Äúmatch‚Äù.

A mean opinion score (MOS) test to quantita-
tively compare our attention mechanism with the
baseline model is also performed. Specifically,
we randomly select 100 cases and generate their
attention maps. Then, we asked subjects to rate
a score from 0 (bad quality), 0.5 (medium qual-
ity) and 1 (excellent quality) to these attention

maps. The distribution of MOS ratings are sum-
marized in Figure 5. The mean scores of our
model 0.8125 wins a large margin over the base-
line model 0.7315, indicating that the attention
maps generated by our attention mechanism are
preferred by human.

0

10

20

30

40

50

60

70

0 0.5 1

N
um

be
r

Mean Opinion Score

Baseline Our

Figure 5: The distribution of Mean Opinion Score.

4 Conclusion

This paper proposes a multi-grained attention
mechanism. It involves both word-object ground-
ing and sentence-image association to capture dif-
ferent degrees of granularity and interpretability
of the images. Visualizations of object-level at-
tention show a clear improvement in the ability of
the model to attend to small details in complicated
scenes.



3600

References
Peter Anderson, Xiaodong He, Chris Buehler, Damien

Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-up and top-down attention for
image captioning and visual question answering. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 6077‚Äì6086.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual Question An-
swering. In International Conference on Computer
Vision (ICCV).

Kyunghyun Cho, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2014. Learn-
ing phrase representations using rnn encoderdecoder
for statistical machine translation. In arXiv preprint
arXiv:1406.1078.

Akira Fukui, Huk Park Dong, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for vi-
sual question answering and visual grounding.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017a. Making the
v in vqa matter: Elevating the role of image under-
standing in visual question answering. In CVPR,
volume 1, page 3.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017b. Making the V
in VQA matter: Elevating the role of image under-
standing in Visual Question Answering. In Confer-
ence on Computer Vision and Pattern Recognition
(CVPR).

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770‚Äì
778.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma,
Michael Bernstein, and Li Fei-Fei. 2016. Visual
genome: Connecting language and vision using
crowdsourced dense image annotations.

Li-Jia Li, Hao Su, Yongwhan Lim, and Li Fei-Fei.
2010. Objects as attributes for scene classification.
In European Conference on Computer Vision, pages
57‚Äì69. Springer.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. CoRR,
abs/1606.00061.

Mateusz Malinowski, Marcus Rohrbach, and Mario
Fritz. 2015. Ask your neurons: A neural-based ap-
proach to answering questions about images. In In
Proceedings of the IEEE International Conference
on Computer Vision, pages 1‚Äì9.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532‚Äì1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015. Faster r-cnn: Towards real-time ob-
ject detection with region proposal networks. In
Advances in neural information processing systems,
pages 91‚Äì99.

Damien Teney, Peter Anderson, Xiaodong He, and An-
ton van den Hengel. 2017. Tips and tricks for visual
question answering: Learnings from the 2017 chal-
lenge. arXiv preprint arXiv:1708.02711.

Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick,
and Anton van den Hengel. 2018. Image captioning
and visual question answering based on attributes
and external knowledge. IEEE transactions on pat-
tern analysis and machine intelligence, 40(6):1367‚Äì
1381.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In International conference on machine learn-
ing, pages 2048‚Äì2057.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
for image question answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 21‚Äì29.

https://arxiv.org/abs/1602.07332
https://arxiv.org/abs/1602.07332
https://arxiv.org/abs/1602.07332
http://arxiv.org/abs/1606.00061
http://arxiv.org/abs/1606.00061

