



















































Cross-document Event Coreference Resolution based on Cross-media Features


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 201–206,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Cross-document Event Coreference Resolution
based on Cross-media Features

Tongtao Zhang1, Hongzhi Li2, Heng Ji1, Shih-Fu Chang2
1Computer Science Department, Rensselaer Polytechnic Institute

{zhangt13, jih}@rpi.edu
2Department of Computer Science, Columbia University
{hongzhi.li, shih.fu.chang}@columbia.edu

Abstract

In this paper we focus on a new problem of
event coreference resolution across televi-
sion news videos. Based on the observa-
tion that the contents from multiple data
modalities are complementary, we develop
a novel approach to jointly encode effec-
tive features from both closed captions
and video key frames. Experiment re-
sults demonstrate that visual features pro-
vided 7.2% absolute F-score gain on state-
of-the-art text based event extraction and
coreference resolution.

1 Introduction

TV news is the medium that broadcasts events,
stories and other information via television. The
broadcast is conducted in programs with the name
of “Newscast”. Typically, newscasts require one
or several anchors who are introducing stories and
coordinating transition among topics, reporters or
journalists who are presenting events in the fields
and scenes that are captured by cameramen. Sim-
ilar to newspapers, the same stories are often re-
ported by multiple newscast agents. Moreover, in
order to increase the impact on audience, the same
stories and events are reported for mutliple times.
TV audience passively receives redundant infor-
mation, and often has difficulty in obtaining clear
and useful digest of ongoing events. These proper-
ties lead to needs for automatic methods to cluster
information and remove redundancy. We propose
a new research problem of event coreference reso-
lution across multiple news videos.

To tackle this problem, a good starting point
is processing the Closed Captions (CC) which
is accompanying videos in newcasts. The CC
is either generated by automatic speech recogni-
tion (ASR) systems or transcribed by a human
stenotype operator who inputs phonetics which are

Figure 1: Similar visual contents improve detec-
tion of a coreferential event pair which has a low
text-based confidence score.
Closed Captions: “It ’s not clear when it was
killed.”; “Jordan just executed two ISIS prisoners,
direct retaliation for the capture of the killing
Jordanian pilot.”

instantly and automatically translated into texts,
where events can be extracted. There exist some
previous event coreference resolution work such
as (Chen and Ji, 2009b; Chen et al., 2009; Lee et
al., 2012; Bejan and Harabagiu, 2010). However,
they only focused on formally written newswire
articles and utilized textual features. Such ap-
proaches do not perform well on CC due to (1).
the propagated errors from upper stream compo-
nents (e.g., automatic speech/stenotype recogni-
tion and event extraction); (2). the incomplete-
ness of information. Different from written news,
newscasts are often limited in time due to fixed
TV program schedules, thus, anchors and journal-
ists are trained and expected to organize reports

201



which are comprehensively informative with com-
plementary visual and CC descriptions within a
short time. These two sides have minimal over-
lapped information while they are inter-dependent.
For example, anchors and reporters introduce the
background story which are not presented in the
videos, and thus the events extracted from CC of-
ten lack information about participants.

For example, as shown in Figure 1, these two
Conflict.Attack event mentions are coreferential.
However, in the first event mention, a mistake
in Closed Caption (“he was killed” → “it was
killed”) makes event extraction and text based
coreference systems unable to detect and link “it”
to the entity of “Jordanian pilot”. Fortunately,
videos often illustrate brief descriptions by vivid
visual contents. Moreover, diverse anchors, re-
porters and TV channels tend to use similar or
identical video contents to describe the same story,
even though they usually use different words and
phrases. Therefore, the challenges in coreference
resolution methods based on text information can
be addressed by incorporating visual similarity. In
this example, the visual similarity between the cor-
responding video frames is high because both of
them show the scene of the Jordanian pilot.

Similar work such as (Kong et al., 2014), (Ra-
manathan et al., 2014), (Motwani and Mooney,
2012) and (Ramanathan et al., 2013) have ex-
plored methods of linking visual materials with
texts. However, these methods mainly focus on
connecting image concepts with entities in text
mentions; and some of them do not clearly distin-
guish entity and event in the documents since the
definition of visual concepts often require both of
them. Moreover, the aforementioned work mainly
focuses on improving visual contents recognition
by introducing text features while our work will
take the opposite route, which takes advantage of
visual information to improve event coreference
resolution.

In this paper, we propose to jointly incorporate
features from both speech (textual) and video (vi-
sual) channels for the first time. We also build a
newscast crawling system that can automatically
accumulate video records and transcribe closed
captions. With the crawler, we created a bench-
mark dataset which is fully annotated with cross-
document coreferential events 1.

1Dataset can be found at
http://www.ee.columbia.edu/dvmm/newDownloads.htm

2 Approach

2.1 Event Extraction
Given unstructured transcribed CC, we extract en-
tities and events and present them in structured
forms. We follow the terminologies used in ACE
(Automatic Content Extraction) (NIST, 2005):
• Entity: an object or set of objects in the world,

such as person, organization and facility.
• Entity mention: words or phrases in the texts

that mention an entity.
• Event: a specific occurrence involving partici-

pants.
• Event trigger: the word that most clearly ex-

presses an event’s occurrence.
• Event argument: an entity, or a temporal expres-

sion or a value that has a certain role (e.g., Time-
Within, Place) in an event.
• Event mention: a sentence (or a text span ex-

tent) that mentions an event, including a distinct
trigger and arguments involved.

2.2 Text based Event Coreference Resolution
Coreferential events are defined as the same spe-
cific occurrence mentioned in different sentences,
documents and transcript texts. Coreferential
events should happen in the same place and within
the same time period, and the entities involved and
their roles should be identical. From the perspec-
tive of extracted events, each specific attribute and
argument from those events should match. How-
ever, mentions for the same event may appear in
forms of diverse words and phrases; and they do
not always cover all arguments or attributes.

To tackle these challenges, we adopt a Maxi-
mum Entropy (MaxEnt) model as in (Chen and
Ji, 2009b). We consider every pair of event men-
tions which share the same event type as a can-
didate and exploit features proposed in (Chen and
Ji, 2009b; Chen et al., 2009). Note that the goal
in (Chen and Ji, 2009b; Chen et al., 2009) was
to resolve event coreference within the same doc-
ument, whereas our scenario yields to a cross-
document/video transcript setting, so we remove
some improper and invalid features. We also in-
vestigated the approaches by (Lee et al., 2012)
and (Bejan and Harabagiu, 2010), but the con-
fidence estimation results from these alternative
methods are not reliable. Moreover, the input
of event coreference are automatic results from
event extraction instead of gold standard, so the
noise and errors significantly impact the corefer-

202



ence performance, especially for unsupervised ap-
proaches (Bejan and Harabagiu, 2010). Neverthe-
less, we still incorporate features from the afore-
mentioned methods. Table 1 shows the features
that constitute the input of the MaxEnt model.

2.3 Visual Similarity

Visual content provides useful cues complemen-
tary with those used in text-based approach in
event coreference resolution. For example, two
coreferential events typically show similar or even
duplicate scenes, objects, and activities in the vi-
sual channel. Coherence of such visual content
has been used in grouping multiple video shots
into the same video story (Hsu et al., 2003), but
it has not been used for event coreference res-
olution. Recent work in computer vision has
demonstrated tremendous progress in large-scale
visual content recognition. In this work, we adopt
the state-of-the-art techniques (Krizhevsky et al.,
2012) and (Simonyan and Zisserman, 2014) that
train robust convolutional neural networks (CNN)
over millions of web images to detect 20,000 se-
mantic categories defined in ImageNet (Deng et
al., 2009) from each image. The 2nd to the
last layer features from such deep network can
be considered as high-level visual representation
that can be used to discriminate various seman-
tic classes (scenes, objects, activity). It has been
found effective in computing visual similarity be-
tween images, by directly computing the L2 dis-
tance of such features or through further met-
ric learning. To compute the similarity between
videos associated with two candidate event men-
tions, we sample multiple frames from each video
and aggregate the similarity scores of the few
most similar image pairs between the videos. Let
{f i1, f i2, ..., , f il } be the key frames sampled from
video Vi and {f j1 , f j2 , ..., , f jl } be key frames sam-
pled from video Vj . All the frames are resized to a
fixed resolution of 256 x 256 and fed into our pre-
trained CNN model. We get the high-level visual
representation Fm = FC7(fm) for each frame fm
from the output of the 2nd to the last fully con-
nected layer (FC7) of CNN model. Fm is a 4096
dimension vector. The visual distance of frames
fm and fn is defined by L2 distance, which is

Dmn = ||FC7(f im)− FC7(f jn)||2. (1)

The distance of video pair (Vi, Vj) is computed as

D̄ij =
1
k
∗

∑
(fm,fn)

Dmn (2)

, where (fm, fn) is the top k of most similar frame
pairs. In our experiment, we use k = 3. Such
aggregation method among the top matches is in-
tended to capture similarity between videos that
share only partially overlapped content.

Each news video story typically starts with an
introduction by an anchor person followed by
news footages showing the visual scenes or activ-
ities of the event. Therefore, when computing vi-
sual similarity, it’s important to exclude the anchor
shot and focus on the story-related clips. Anchor
frame detection (Hsu et al., 2003) is a well studied
problem. In order to detect anchor frames auto-
matically, a face detector is applied to all I-frames
of a video. We can obtain the location and size
of each detected face. After checking the tempo-
ral consistency of the detected faces within each
shot, we get a set of candidate anchor faces. The
detected face regions are further extended to re-
gions of interest that may include hair and upper
body. All the candidate faces detected from the
same video are clustered based on their HSV color
histogram. It is reasonable to assume that the most
frequent face cluster is the one corresponding to
the anchor faces. Once the anchor frames are de-
tected, they are excluded and only the non-anchor
frames are used to compute the visual similarity
between videos associated with event mentions.

2.4 Joint Re-ranking

Using the visual distance calculated from Sec-
tion 2.3, we can rerank the confidence values from
Section 2.2 using the text-based MaxEnt model.
We use the following empirical equation to adjust
the confidence:

W ′ij = Wij ∗ e−
D̄ij
α

+1, (3)

where Wij denotes the original coreference con-
fidence between event mentions i and j, Dij de-
notes the visual distance between the correspond-
ing video frames where the event mentions were
spoken and α is a parameter which is used to ad-
just the impact of visual distance. In the current
implementation, we empirically set it as the aver-
age of pair-wised visual distances between videos
of all event coreference candidates. With this α

203



Category Features Remarks (EMi: the first event mention, EMj : the sec-
ond event mention)

Baseline

type subtype pair of event type and subtype in EMi
trigger pair trigger pair of EMi and EMj
pos pair part-of-speech pair of triggers of EMi and EMj
nominal 1 if the trigger of EMi is nominal
nom number “plural” or “singular” if the trigger of EMi is nominal
pronominal 1 if the trigger of EMi is pronominal
exact match 1 if the trigger spelling in EMi matches that in EMj
stem match 1 if the trigger stem in EMi matches that in EMj
trigger sim the semantic similarity scores between triggers of EMi

and EMj using WordNet(Miller, 1995)
Arguments argument match 1 if arguments holding the same roles in both EMi and

EMj matches

Attributes
mod,pol,gen,ten four event attributes in EMi: modality, polarity, gener-

icity and tense
mod conflict,
pol conflict, gen conflict,
ten conflict

1 if the attributes of EMi and EMj conflict

Table 1: Features for Event Coreference Resolution

we generally enhance the confidence of event pairs
with small visual distances and penalize those with
large ones. An alternative way for setting the alpha
parameter is through cross validation over separate
data partitions.

3 Experiments

3.1 Data and Setting
We establish a system that actively monitors over
100 U.S. major broadcast TV channels such as
ABC, CNN and FOX, and crawls newscasts from
these channels for more than two years (Li et
al., 2013a). With this crawler, we retrieve 100
videos and their correspondent transcribed CC
with the topic of “ISIS”2. This system also tem-
porally aligns the CC text with the transcribed text
from automatic speech recognition following the
methods in (Huang et al., 2003). This provides ac-
curate time alignment between the CC text and the
video frames. As CC consists of capitalized let-
ters, we apply the true-casing tool from Standford
CoreNLP (Manning et al., 2014) on CC. Then we
apply a state-of-the-art event extraction system (Li
et al., 2013b; Li et al., 2014) to extract event men-
tions from CC. We asked two human annotators
to investigate all event pairs and annotate coref-
erential pairs as the ground truth. Kappa coeffi-
cient for measuring inter-annotator agreement is

2abbreviation for Islamic State of Iraq and Syria

74.11%. In order to evaluate our system perfor-
mance, we rank the confidence scores of all event
mention pairs and present the results in Precision
vs. Detection Depth curve. Finally we find the
video frames corresponding to the event mentions,
remove the anchor frames and calculate the visual
similarity between the videos. Our final dataset
consists of 85 videos, 207 events and 848 event
pairs, where 47 pairs are considered coreferential.

We adopt the MaxEnt-based coreference reso-
lution system from (Chen and Ji, 2009b; Chen et
al., 2009) as our baseline, and use ACE 2005 En-
glish Corpus as the training set for the model. A
5-fold cross-validation is conducted on the train-
ing set and the average f-score is 56%. It is lower
than results from (Chen and Ji, 2009a) since we
remove some features which are not available for
the cross-document scenario.

3.2 Results
The peak F-score for the baseline system is
44.23% while our cross-media method boosts it
to 51.43%. Figure 2 shows the improvement af-
ter incorporating the visual information. We adopt
Wilcoxon signed-rank test to determine the signif-
icance between the pairs of precision scores at the
same depth. The z-ratio is 3.22, which shows the
improvement is significant.

For example, the event pair “So why hasn’t
U.S. air strikes targeted Kobani within the city

204



0 50 100 150
0.2

0.4

0.6

0.8

1
Baseline
Our Method

Figure 2: Performance comparison between base-
line and our cross-media method on top 150 pairs.
Circles indicate the peak F-scores.

limits” and “Our strikes continue alongside our
partners.” was mistakenly considered coreferen-
tial by text features. In fact, the former “strikes”
mentions the airstrike and the latter refers to the
war or battle, therefore, they are not coreferential.
The corresponding video shots demonstrate two
different scenes: the former one shows bombing
while the latter shows that the president is giving
a speech about the strike. Thus the visual distance
successfully corrected this error.

3.3 Error Analysis
However, from Figure 2 we can also notice that
there are still some errors caused by the vi-
sual features. One major error type resides in
the negative pairs with both “relatively” high
textual coreference confidence scores and “rela-
tively” high visual similarity. From the text side,
the event pair contains similar events, for exam-
ple: “The Penn(tagon) says coalition air strikes
in and around the Syrian city of Kobani have kill
hundreds of ISIS fighters but more are stream-
ing in even as the air campaign intensifies.” and
“Throughout the day, explosions from coalition
air strikes sent plums of smoke towering into the
sky.”. They talk about two airstrikes during differ-
ent time periods and are not coreferential, but the
baseline system produces a high rank. Our current
approach limits the image frames to those over-
lapped with the speech of an event mention, and in
this error, both videos show “battle” scene, yield-
ing a small visual distance. The aforementioned
assumption that anchors and journalists tend to use
similar videos when describing the same events ,
which may introduce risk of error caused by sim-
ilar text event mentions with similar video shots.
For such errors, one potential solution is to expand
the video frame windows to capture more events
and concepts from videos. Expanding the detec-

tion range to include visual events in the temporal
neighborhood can also differentiate the events.

3.4 Discussion
A systematic way of choosing α in Equation 3 will
be useful. One idea is to adapt the α value for dif-
ferent types of events, e.g., we expect some event
types are more visually oriented than others and
thus use a smaller α value.

We also notice the impact of the errors from the
upstream event extraction system. According to
(Li et al., 2014) the F-score of event trigger label-
ing is 65.3%, and event argument labeling is 45%.
Missing arguments in events is a main problem,
thus the performance on automatically extracted
event mentions is significantly worse. About 20
more coreferential pairs could be detected if events
and arguments are perfectly extracted.

4 Conclusions and Future Work

In this paper, we improved event coreference res-
olution on newscast speech by incorporating vi-
sual similarity. We also build a crawler that pro-
vides a benchmark dataset of videos with aligned
closed captions. This system can also help cre-
ate more datasets to conduct research on video de-
scription generation. In the future, we will focus
on improving event extraction from texts by intro-
ducing more fine-grained cross-media information
such as object, concept and event detection results
from videos. Moreover, joint detection of events
from both sides is our ultimate goal, however, we
need to explore the mapping among events from
both text and visual sides, and automatic detection
of a wide range of objects and events from news
video itself is still challenging.

Acknowledgement

This work was supported by the U.S. DARPA
DEFT Program No. FA8750-13-2-0041, ARL
NS-CTA No. W911NF-09-2-0053, NSF CA-
REER Award IIS-1523198, AFRL DREAM
project, gift awards from IBM, Google, Disney
and Bosch. The views and conclusions contained
in this document are those of the authors and
should not be interpreted as representing the of-
ficial policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.

205



References
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.

Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1412–1422.

Zheng Chen and Heng Ji. 2009a. Event coref-
erence resolution: Algorithm, feature impact and
evaluation. In Proceedings of Events in Emerging
Text Types (eETTs) Workshop, in conjunction with
RANLP, Bulgaria.

Zheng Chen and Heng Ji. 2009b. Graph-based
event coreference resolution. In Proceedings of the
2009 Workshop on Graph-based Methods for Natu-
ral Language Processing, TextGraphs-4, pages 54–
57.

Zheng Chen, Heng Ji, and Robert Haralick. 2009.
A pairwise event coreference model, feature impact
and evaluation for event coreference resolution. In
Proceedings of the Workshop on Events in Emerging
Text Types, eETTs ’09, pages 17–22.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recog-
nition, 2009., pages 248–255.

Winston Hsu, Shih-Fu Chang, Chih-Wei Huang, Lyn-
don Kennedy, Ching-Yung Lin, and Giridharan
Iyengar. 2003. Discovery and fusion of salient mul-
timodal features toward news story segmentation. In
Electronic Imaging 2004, pages 244–258.

Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.
2003. Automatic closed caption alignment based on
speech recognition transcripts. Rapport technique,
Columbia.

Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun,
and Sanja Fidler. 2014. What are you talking about?
text-to-image coreference. In Proceedings of 2014
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3558–3565.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500.

Hongzhi Li, Brendan Jou, Jospeh G Ellis, Daniel Mo-
rozoff, and Shih-Fu Chang. 2013a. News rover:
Exploring topical structures and serendipity in het-
erogeneous multimedia news. In Proceedings of the

21st ACM international conference on Multimedia,
pages 449–450.

Qi Li, Heng Ji, and Liang Huang. 2013b. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 73–82.

Qi Li, Heng Ji, Yu HONG, and Sujian Li. 2014.
Constructing information networks using one single
model. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1846–1851.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39–41, Novem-
ber.

Tanvi S Motwani and Raymond J Mooney. 2012.
Improving video activity recognition using object
recognition and text mining. In Proceedings of the
20th European Conference on Artificial Intelligence,
pages 600–605.

NIST. 2005. The ace 2005 evaluation plan. http:
//www.itl.nist.gov/iad/mig/tests/
ace/ace05/doc/ace05-evaplan.v3.pdf.

Vignesh Ramanathan, Percy Liang, and Li Fei-Fei.
2013. Video event understanding using natural
language descriptions. In Proceedings of 2013
IEEE International Conference on Computer Vision,
pages 905–912.

Vignesh Ramanathan, Armand Joulin, Percy Liang,
and Li Fei-Fei. 2014. Linking people in videos with
their names using coreference resolution. In Com-
puter Vision–ECCV 2014, pages 95–110.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.

206


