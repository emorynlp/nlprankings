



















































Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2857‚Äì2863
Brussels, Belgium, October 31 - November 4, 2018. c¬©2018 Association for Computational Linguistics

2857

Improved Dependency Parsing using Implicit Word Connections Learned
from Unlabeled Data

Wenhui Wang‚Ä† Baobao Chang‚Ä† Mairgup Mansur‚Ä°
‚Ä†Key Laboratory of Computational Linguistics, Peking University, MOE, China

‚Ä°Sogou Technology Inc., Beijing, China
{wenwan,chbb}@pku.edu.cn
{maerhufu}@sogou-inc.com

Abstract

Pre-trained word embeddings and language
model have been shown useful in a lot of tasks.
However, both of them cannot directly cap-
ture word connections in a sentence, which
is important for dependency parsing given its
goal is to establish dependency relations be-
tween words. In this paper, we propose to im-
plicitly capture word connections from unla-
beled data by a word ordering model with self-
attention mechanism. Experiments show that
these implicit word connections do improve
our parsing model. Furthermore, by combin-
ing with a pre-trained language model, our
model gets state-of-the-art performance on the
English PTB dataset, achieving 96.35% UAS
and 95.25% LAS.

1 Introduction

Dependency parsing is a fundamental task for lan-
guage processing which aims to establish syntac-
tic relations between words in a sentence. Graph-
based models (McDonald et al., 2005; McDon-
ald and Pereira, 2006; Carreras, 2007; Koo and
Collins, 2010) and transition-based models (Nivre,
2008; Zhang and Nivre, 2011) are the most suc-
cessful solutions to the challenge.

Recently, neural network methods have been
successfully introduced into dependency parsing.
Deep feed-forward neural network models (Chen
and Manning, 2014; Pei et al., 2015; Weiss et al.,
2015) are proposed firstly. It alleviates the heavy
burden of feature engineering. LSTM networks
(Hochreiter and Schmidhuber, 1997) are then ap-
plied to dependency parsing (Dyer et al., 2015;
Cross and Huang, 2016; Wang and Chang, 2016;
Kiperwasser and Goldberg, 2016; Dozat and Man-
ning, 2016) due to its ability to capture contextual
information. Generative neural network models
(Dyer et al., 2016; Smith et al., 2017; Choe and

Charniak, 2016) also show promising parsing per-
formance.

Different from Machine Translation task where
massive sets of labeled data could be easily ob-
tained, parsing performance is limited by the rel-
atively small size of available treebank. Vinyals
et al. (2015) and Weiss et al. (2015) adopt an ap-
proach of tri-training to augment the labeled data.
They generate large quantities of parse trees by
parsing unlabeled data with two existing parsers
and selecting only the sentences for which the two
parsers produced the same trees. However, the
trees produced this way have noise1 and tend to
be short sentences, since it is easier for different
parsers to get consistent results.

Pre-trained neural networks are another meth-
ods to take advantage of unlabeled data. Pre-
trained word embeddings (Mikolov et al., 2013)
and language model (JoÃÅzefowicz et al., 2016; Pe-
ters et al., 2017, 2018) have been shown useful
in modelling NLP tasks since word embeddings
could capture word semantic information and lan-
guage model could capture contextual information
at the sentence level. However, connections be-
tween words in the sentence cannot be directly
captured by word embeddings or language model,
which are crucial for dependency parsing given
its goal is to establish dependency relations be-
tween words. In this paper, we propose to im-
plicitly model word connections by a word or-
dering model. The purpose of word ordering
model is to generate a well-formed sentence given
a bag of words. We human could make sentences
easily from unordered words since we have syn-
tactic knowledge, thus a model generating well-
formed sentences from the bag of words encodes
syntactic information. In addition, word order-
ing task allows us to use self-attention mechanism

1The tri-training approach accuracy on the tune set is
97.26% UAS (Weiss et al., 2015).



2858

to model connections between words in the sen-
tence. Different from the tri-training approach, our
approach takes advantage of implicit word con-
nections learned by self-attended word ordering
model in an unsupervised way.

Experiments show that pre-trained word or-
dering model significantly improves our depen-
dency parsing model. Ablation tests also show
self-attention mechanism is critical. Moreover,
by combining word ordering model and lan-
guage model, our graph-based dependency pars-
ing model achieves SOTA performance on the En-
glish Penn Treebank (Marcus et al., 1993) with
96.35% UAS and 95.25% LAS.

eating He likesapples

LSTM LSTM LSTM

A bag of words

Self-attention

<BOS> He

Encoder layer

Softmax Softmax Softmax

He likes eating

Decoder layer
Output embeddings
for the bag of words

Average of word 
embeddings

likes

Figure 1: Overview of our word ordering model.

2 Neural Word Ordering Model

The target of word ordering is to generate a well-
formed sentence given a bag of words. To cap-
ture word connections implicated in the sentence,
an LSTM-based word ordering model with self-
attention is proposed. Self-attention mechanism
effectively decides which words in the word bag
are more important in generating the next word. It
improves the ability of our model to capture word
connections. As illustrated in Figure 1, the pro-
posed word ordering model consists of two layers:

Encoder Layer
Given a bag of words w1, w2, ..., wn, we encode
each word by a character-level BiLSTM (cwow1:n),
which could reduce the parameters used in our
model compared with word embeddings. For
the input word of current time-step (wi), a self-
attention layer is utilized to align the word with its

related words, producing its self-attended vector
(sawowi ) as following:

sij = v
TReLU(W sa[cwowi ; c

wo
wj ]) (1)

ait = exp(s
i
t)/Œ£

n
j=1exp(s

i
j) (2)

sawowi = Œ£
n
j=1a

i
jc

wo
wj (3)

The scores si1:n in self-attention explicitly repre-
sent the connections between words.

We then concatenate the character-level word
embedding (cwowi ) and its self-attended vector
(sawowi ):

xwoi = [c
wo
wi ; sa

wo
wi ] (4)

xwoi is fed into the decoder layer to generate the
next word.

Decoder Layer
Given the current input vector (xwoi ), which con-
tains current word information and weighted infor-
mation of related words, a forward LSTM is used
to generate the next word. We initialize the for-
ward LSTM with an average of the input word em-
beddings (cwowi:n). A virtual token „ÄàBOS„Äâ is added
as the input of the first LSTM time-step:

‚àí‚Üí
h woi = LSTM(x

wo
i ,
‚àí‚Üí
h woi‚àí1) (5)

At each time-step, the hidden state
‚àí‚Üí
h woi is utilized

to predict the next word. Due to the output vocab-
ulary is limited in the bag of words, we just com-
pute scores for the given words (w1:n):

soij = v
TReLU(W o[

‚àí‚Üí
h woi ; cd

wo
wj ])+

‚àí‚Üí
h woi

T
Mwdwowj

(6)

To reduce the parameters, each output word is
represented by a character-level BiLSTM embed-
ding (cdwowj ) and a low-dimensional word embed-
ding2 (wdwowj ). M is a matrix projecting a low-
dimensional embedding back up to the dimension-
ality of LSTM hidden states. The scores soi1:n are
then normalized with Softmax, and the word with
max probability is chosen as the next token.

The word ordering model could be trained eas-
ily in an unsupervised manner. Given a large set of
unlabeled sentences, we can just ignore the word

2Character-level representations lack the capacity to dif-
ferentiate between words that have very different meanings
but that are spelled similarly. Low-dimensional word embed-
dings are added to improve the ability.



2859

order of sentence and train the model to gener-
ate the corresponding well-formed sentence in the
training set. To be specific, we minimize the sum
of negative log probabilities of the ground truth
words on the unlabeled data set. Different from
language model, the choice for each decoder step
is limited in the bag of words. Moreover, self-
attention can be introduced into the word order-
ing model since we have known the bag of words,
which could capture the dependency connections
between words. We also pre-train a backward
word ordering model to generate sentences in re-
verse order. The forward and backward models
share character-level BiLSTM embeddings, self-
attention layer, and Softmax layer.

Different from previous word ordering mod-
els (Liu et al., 2015; Schmaltz et al., 2016), self-
attention mechanism is introduced into our model
to capture word connections. Moreover, our more
important goal is to implicitly utilize large-scale
unlabeled data to help dependency parsing.

3 Neural Graph-based Parsing Model

We implement an LSTM-based neural network
model as our graph-based dependency parsing
baseline, which is similar to (Kiperwasser and
Goldberg, 2016; Wang and Chang, 2016). As
shown in the Figure 2, it consists of three layers:

Input vectors

He ‚Ä¶

LSTM LSTM

likes eating

LSTM

H D H D H D

(ùëÜùëêùëúùëüùëíùëéùëüùëê, ùëÜùëêùëúùëüùëíùëôùëéùëèùëíùëô)

Concat

Highway 
networks

for head&dep

Dependent representations

Encoder layer

Output scores

Head representations

Concat Concat

Word Char WOPOS Word Char WOPOS Word Char WOPOS

Figure 2: Overview of neural graph-based dependency
parsing model. WO represents pre-trained vectors from
word ordering model.

Input Layer
Given a n-words input sentence s with words
w1, w2, ..., wn and its POS tags p1, p2, ..., pn. The
input layer creates a sequence of input vectors

x1:n in which each xi is a concatenation of its
word embedding (ewi), POS tag embedding (epi),
character-level BiLSTM embedding (cwi), and
word ordering model pre-trained vector (wowi):

xi = [ewi ; epi ; cwi ;wowi ] (7)

To get the word ordering model pre-trained vec-
tor (wowi), the sentence s is fed into the pre-
trained word ordering model. Following Peters
et al. (2018), we then combine the input vector
(xwoi =[c

wo
wi ; sa

wo
wi ]) and L-layer BiLSTM vectors

(hwoi,j =[
‚àí‚Üí
h woi,j ;

‚Üê‚àí
h woi,j ] | j=1, 2, ..., L) by a Softmax-

normalized weight (Wwoc) and a scalar parameter
(Œ≥):

wowi = Œ≥(W
woc
0 x

wo
i +Œ£

L
j=1W

woc
j h

wo
i,j ) (8)

The parameters of word ordering model are fixed
during the training of parsing model. However,
the weight and scalar parameters are tuned to bet-
ter adapt to it. The combined outputwowi contains
word connections from self-attention, word infor-
mation from character-level embedding and con-
textual information from LSTM.

Encoder & Output Layer
To introduce more contextual information, we en-
code each input element by deep BiLSTMs:

vi = BiLSTM(x1:n, i) (9)

Two different highway networks (Srivastava et al.,
2015) are then used to encode head word repre-
sentations (vhead1:n ) and dependent word represen-
tations (vdep1:n ). For a head-dependent dependency
pair (wh, wd), the dependency arc and label score
are computed by two MLP networks:

ih,d = [v
head
h ; v

dep
d ; v

head
h ÔøΩ v

dep
d ] (10)

sarch,d = W
arc
1 ReLU(W

arc
2 ih,d) (11)

slabelh,d = W
label
1 ReLU(W

label
2 ih,d) (12)

We use the Max-Margin criterion to train our
parsing model, which is the same as (Kiperwasser
and Goldberg, 2016; Wang and Chang, 2016).

4 Experiments

4.1 Datasets

We conduct experiments on the English Penn
Treebank and the CoNLL 09 English dataset. For



2860

Method PTB CONLL 09UAS / LAS UAS / LAS
Bohnet2012 - / - 92.87 / 90.60
Weiss2015 94.26 / 92.41 - / -
Alberti2015 94.23 / 92.36 92.7 / 90.56
Kiperwasser2016 93.9 / 91.9 - / -
Wang2016 94.08 / 91.82 - / -
Andor2016 94.61 / 92.79 93.22 / 91.23
Dozat2016 95.74 / 94.08 95.21 / 93.20
Smith2017 95.8 / 94.6 - / -
Choe2016 95.9 / 94.1 - / -
Baseline 95.12 / 93.98 93.61 / 91.70
Baseline+WO 95.66 / 94.54 94.21 / 92.27
Baseline+LM 96.05 / 94.88 94.50 / 92.70
Baseline+LM&WO 96.35 / 95.25 94.83 / 93.05

Table 1: Results on the English PTB dataset and
CoNLL 09 English dataset. WO represents the pre-
trained word ordering model. LM represents the pre-
trained language model from Peters et al. (2018).

PTB dataset, we follow the standard splits. Us-
ing section 2-21 for training, section 22 as devel-
opment set and 23 as test set. The treebank is
converted to Stanford Basic Dependencies (Marn-
effe et al., 2006) by version 3.3.03 of the Stan-
ford parser. The Stanford POS Tagger (Toutanova
et al., 2003) is used for assigning POS tags. Fol-
lowing previous work, UAS (unlabeled attach-
ment scores) and LAS (labeled attachment scores)
are calculated by excluding punctuation. For the
CoNLL 09 English dataset, we follow the standard
practice and include all punctuation in the evalua-
tion. We pre-train our word ordering model on the
1 billion word benchmark (Chelba et al., 2014).

4.2 Implementation Details
The graph-based dependency parsing model and
word ordering model are optimized with Adam
with an initial learning rate of 2e‚àí3. The Œ≤1 and
Œ≤2 used in Adam are 0.9 and 0.999 respectively.

The following hyper-parameters are used in all
graph-based dependency parsing models: word
embedding size = 300, POS tag embedding size
= 32, character embedding size = 50, word-level
LSTM hidden vector size = 200, word-level BiL-
STM layer number = 3, character-level LSTM
hidden vector size = 50, character-level BiLSTM
layer number = 2, batch size = 32. We also
apply dropout for the input and each layer with
dropout rate of 0.3. We use pre-trained case-
sensitive GloVe embeddings4 to initialize word
embeddings. These word embeddings are fine

3http://nlp.stanford.edu/software/
lex-parser.shtml

4Downloaded from http://nlp.stanford.edu/

tuned with the graph-based dependency parsing
model. The parameters of pre-trained word or-
dering model are fixed during the training of de-
pendency parsing model. For deep BiLSTM, we
concatenate the outputs of each layer as its final
outputs.

For our word ordering model: input character-
level LSTM hidden vector size = 512, input
character-level BiLSTM layer number = 1, word-
level LSTM hidden vector size = 1024, word-level
LSTM layer number = 2, output character-level
LSTM hidden vector size = 512, output character-
level BiLSTM layer number = 1, output low-
dimensional word embedding size = 64, batch size
= 32, dropout for the input and each layer = 0.5.

4.3 Main Results & Ablation Study

Table 1 shows the performance of our model and
previous work on two English benchmarks. Our
model achieves promising results on both datasets.
Two sets of experiments are provided to show the
effectiveness of pre-trained word ordering model.
Although our baseline system is similar to (Kiper-
wasser and Goldberg, 2016; Wang and Chang,
2016) but with subtle differences in architecture,
the baseline could perform much better to our
surprise and thus constitutes a very strong base-
line. Compared with this baseline, introducing
the pre-trained word ordering model achieves a
significant improvement (almost 0.6% UAS gains
for both datasets, p < 0.001). To further show
the effectiveness of word ordering model, we also
implement an even stronger baseline with pre-
trained language model5. Compared with this
much stronger baseline, incorporating pre-trained
word ordering model still achieves a significant
improvement (0.3% UAS gains for both datasets,
p < 0.01). We attribute the improvement to the
ability of word ordering model to capture word
connections, which cannot be directly captured by
language model. Moreover, by combining with
a pre-trained language model, our model outper-
forms current SOTA model from 95.9% UAS to
96.35% UAS on the PTB dataset. The introduction

data/glove.840B.300d.zip.
5We use the pre-trained language model provided by Pe-

ters et al. (2018), which can be downloaded at http://
allennlp.org/elmo. The pre-trained language model
vectors are added in the input layer, which are included in
the same way as word ordering model. Peters et al. (2018)
found the pre-trained language model works extremely well
in six NLP tasks including QA, SRL, and others, we confirm
its effectiveness in parsing task.

http://nlp.stanford.edu/software/lex-parser.shtml
http://nlp.stanford.edu/software/lex-parser.shtml
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://nlp.stanford.edu/data/glove.840B.300d.zip
http://allennlp.org/elmo
http://allennlp.org/elmo


2861

Model UAS / LAS
Baseline+WO 95.66 / 94.54
-Self-attention vectors 95.38 / 94.27
Baseline+LM&WO 96.35 / 95.25
-Self-attention vectors 96.13 / 94.98

Table 2: Ablation tests of self-attention mechanism on
the PTB dataset.

of POS tag features could contribute about 0.2%
improvement in our experiments. The word order-
ing model could be more helpful without POS tag
features and seem to compensate for the lack of
POS tag features.

To show the importance of self-attention mech-
anism, we do ablation tests on the models with
pre-trained word ordering model vectors. We re-
move self-attention vectors by replacing it with
the character-level representations. As shown
in table 2, self-attention further improves depen-
dency parsing. Word connections modeled by self-
attention are important for dependency parsing.

Figure 3 shows an example of word connec-
tions learned by the model, where we use the solid
line to indicate the word connections learned by
the word-ordering model and dashed line to the
expected dependencies. We can see meaningful
overlap could be observed in the example. The
percentage of overlap between connections and
dependency arcs is over 40% for the sentences less
than 10 words. The differences between connec-
tions and dependency arcs are because that our
word ordering model trained without any super-
vised dependency information. The connections
are actually built to increase the likelihood.

This may sound strangely optimistic .

Figure 3: An example of self-attention. The solid line
denotes the mostly attended word when generating the
next word, dashed line denotes the correct dependency.

5 Conclusion

In this paper, we propose to implicitly capture
word connections from large-scale unlabeled data
by a word ordering model with self-attention. Ex-
periments show these features are helpful for de-
pendency parsing. Moreover, with the help of

word ordering model and language model, our
model achieves SOTA results on the PTB dataset.
As for future work, we are testing on languages
other than English.

Acknowledgement

We thank all the anonymous reviewers for their
helpful comments. This work is supported by Na-
tional Natural Science Foundation of China under
Grant No.61876004 and No.61751201. The cor-
responding author of this paper is Baobao Chang.

References
Chris Alberti, David Weiss, Greg Coppola, and Slav

Petrov. 2015. Improved transition-based parsing and
tagging with neural networks. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1354‚Äì1359.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL 2012, July 12-14, 2012, Jeju Island, Korea,
pages 1455‚Äì1465.

Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In EMNLP-
CoNLL, pages 957‚Äì961.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. One billion word benchmark for measur-
ing progress in statistical language modeling. In IN-
TERSPEECH 2014, 15th Annual Conference of the
International Speech Communication Association,
Singapore, September 14-18, 2014, pages 2635‚Äì
2639.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 740‚Äì750.

Do Kook Choe and Eugene Charniak. 2016. Pars-
ing as language modeling. In Proceedings of the
2016 Conference on Empirical Methods in Natural



2862

Language Processing, EMNLP 2016, Austin, Texas,
USA, November 1-4, 2016, pages 2331‚Äì2336.

James Cross and Liang Huang. 2016. Incremental
parsing with minimal features using bi-directional
LSTM. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 2: Short Papers.

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. CoRR, abs/1611.01734.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 334‚Äì343.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In NAACL HLT 2016, The 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, San Diego California, USA,
June 12-17, 2016, pages 199‚Äì209.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997. Long
short-term memory. Neural Computation.

Rafal JoÃÅzefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring the lim-
its of language modeling. CoRR, abs/1602.02410.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. TACL, 4:313‚Äì
327.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1‚Äì11. Association for
Computational Linguistics.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
NAACL HLT 2015, The 2015 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015, pages 113‚Äì122.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313‚Äì330.

Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Lrec, pages 449‚Äì454.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 91‚Äì98. Association for Computa-
tional Linguistics.

Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL. Citeseer.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111‚Äì
3119.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics.

Wenzhe Pei, Tao Ge, and Baobao Chang. 2015. An
effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics, pages 313‚Äì322.

Matthew E. Peters, Waleed Ammar, Chandra Bhaga-
vatula, and Russell Power. 2017. Semi-supervised
sequence tagging with bidirectional language mod-
els. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Vol-
ume 1: Long Papers, pages 1756‚Äì1765.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. CoRR, abs/1802.05365.

Allen Schmaltz, Alexander M. Rush, and Stuart M.
Shieber. 2016. Word ordering without syntax. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016,
pages 2319‚Äì2324.

Noah A. Smith, Chris Dyer, Miguel Ballesteros, Gra-
ham Neubig, Lingpeng Kong, and Adhiguna Kun-
coro. 2017. What do recurrent neural network gram-
mars learn about syntax? In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, EACL 2017, Va-
lencia, Spain, April 3-7, 2017, Volume 1: Long Pa-
pers, pages 1249‚Äì1258.

Rupesh Kumar Srivastava, Klaus Greff, and JuÃàrgen
Schmidhuber. 2015. Highway networks. CoRR,
abs/1505.00387.



2863

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173‚Äì180. Association for Compu-
tational Linguistics.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey E. Hinton. 2015.
Grammar as a foreign language. In Advances in
Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Que-
bec, Canada, pages 2773‚Äì2781.

Wenhui Wang and Baobao Chang. 2016. Graph-based
dependency parsing with bidirectional LSTM. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume 1:
Long Papers.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural net-
work transition-based parsing. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 188‚Äì193.


