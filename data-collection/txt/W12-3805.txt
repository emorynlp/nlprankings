










































Improving Speculative Language Detection using Linguistic Knowledge


Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 37–46, Jeju, Republic of Korea, 13 July 2012. c©2012 Association for Computational Linguistics

Improving Speculative Language Detection using Linguistic Knowledge

Guillermo Moncecchi
Facultad de Ingenierı́a

Universidad de la República
Montevideo, Uruguay

Jean-Luc Minel
Laboratoire MoDyCo
Université Paris Ouest

Nanterre La Défense, France

Dina Wonsever
Facultad de Ingenierı́a

Universidad de la República
Montevideo, Uruguay

Abstract

In this paper we present an iterative method-
ology to improve classifier performance by in-
corporating linguistic knowledge, and propose
a way to incorporate domain rules into the
learning process. We applied the methodol-
ogy to the tasks of hedge cue recognition and
scope detection and obtained competitive re-
sults on a publicly available corpus.

1 Introduction

A common task in Natural Language Processing
(NLP) is to extract or infer factual information from
textual data. In the field of natural sciences this task
turns out to be of particular importance, because
science aims to discover or describe facts from the
world around us. Extracting these facts from the
huge and constantly growing body of research ar-
ticles in areas such as, for example, molecular biol-
ogy, becomes increasingly necessary, and has been
the subject of intense research in the last decade
(Ananiadou et al., 2006). The fields of information
extraction and text mining have paid particular atten-
tion to this issue, seeking to automatically populate
structured databases with data extracted or inferred
from text. In both cases, the problem of speculative
language detection is a challenging one, because it
may correspond to a subjective attitude of the writer
towards the truth value of certain facts, and that in-
formation should not be lost when the fact is ex-
tracted or inferred.

When researchers express facts and relations in
their research articles, they often use speculative lan-
guage to convey their attitude to the truth of what

is said. Hedging, a term first introduced by Lakoff
(1973) to describe ‘words whose job is to make
things fuzzier or less fuzzy’ is ‘the expression of ten-
tativeness and possibility in language use’ (Hyland,
1995), and is extensively used in scientific writing.
Hyland (1996a) reports one hedge in every 50 words
of a corpus of research articles; Light et al. (2004)
mention that 11% of the sentences in MEDLINE
contain speculative language. Vincze et al. (2008)
report that 18% of the sentences in the scientific ab-
stracts section of the Bioscope corpus correspond to
speculations.

Early work on speculative language detection
tried to classify a sentence either as speculative
or non-speculative (see, for example, Medlock and
Briscoe (2007)). This approach does not take into
account the fact that hedging usually affects propo-
sitions or claims (Hyland, 1995) and that sentences
often include more than one of them. When the Bio-
scope corpus (Vincze et al., 2008) was developed
the notions of hedge cue (corresponding to what was
previously called just ‘hedges’ in the literature) and
scope (the propositions affected by the hedge cues)
were introduced. In this context, speculative lan-
guage recognition can be seen as a two-phase pro-
cess: first, the existence of a hedge cue in a sentence
is detected, and second, the scope of the induced
hedge is determined. This approach was first used
by Morante et al. (2008) and subsequently in many
of the studies presented in the CoNLL-2010 Confer-
ence Shared Task (Farkas et al., 2010a), and is the
one used in this paper.

For example, the sentence

(1) This finding {suggests suggests that the BZLF1

37



promoter {may may be regulated by the degree
of squamous differentiation}may}suggests.

contains the word ‘may’ that acts as a hedge cue
(i.e. attenuating the affirmation); this hedge only
affects the propositions included in the subordinate
clause that contains it.

Each of these phases can be modelled (albeit with
some differences, described in the following sec-
tions) as a sequential classification task, using a sim-
ilar approach to that commonly used for named en-
tity recognition or semantic labelling: every word
in the sentence is assigned a class, identifying spans
of text (as, for example, scopes) with, for example, a
special class for the first and last element of the span.
Correctly learning these classes is the computational
task to be solved.

In this paper we present a methodology and ma-
chine learning system implementing it that, based
on previous work on speculation detection, studies
how to improve recognition by analysing learning
errors and incorporating advice from domain experts
in order to solve the errors without hurting overall
performance. The methodology proposes the use of
domain knowledge rules that suggest a class for an
instance, and shows how to incorporate them into
the learning process. In our particular task domain
knowledge is linguistic knowledge, as hedging and
scopes issues are general linguistic devices. In this
paper we are going both terms interchangeably.

The paper is organized as follows. In Section 2
we review previous theoretical work on speculative
language and the main computational approaches to
the task of detecting speculative sentences. Section
3 briefly describes the corpus used for training and
evaluation. In Section 4 we present the specific com-
putational task to which our methodology was ap-
plied. In Section 5 we present the learning method-
ology we propose to use, and describe the system
we implemented, including lexical, syntactic and se-
mantic attributes we experimented with. We present
and discuss the results obtained in Section 6. Finally,
in Section 7 we analyse the approach presented here
and discuss its advantages and problems, suggesting
future lines of research.

2 Related work

The grammatical phenomenon of modality, defined
as ‘a category of linguistic meaning having to do
with the expression of possibility and necessity’
(von Fintel, 2006) has been extensively studied in
the linguistic literature. Modality can be expressed
using different linguistic devices: in English, for ex-
ample, modal auxiliaries (such as ‘could’ or ‘must’),
adverbs (‘perhaps’), adjectives (‘possible’), or other
lexical verbs (‘suggest’,‘indicate’), are used to ex-
press the different ways of modality. Other lan-
guages express modality in different forms, for ex-
ample using the subjunctive mood. Palmer (2001)
considers modality as the grammaticalization of
speakers’ attitudes and opinions, and epistemic
modality, in particular, applies to ‘any modal sys-
tem that indicates the degree of commitment by the
speaker to what he says’.

Although hedging is a concept that is closely
related to epistemic modality, they are different:
modality is a grammatical category, whereas hedg-
ing is a pragmatic position (Morante and Sporleder,
2012). This phenomenon has been theoretically
studied in different domains and particularly in sci-
entific writing (Hyland, 1995; Hyland, 1996b; Hy-
land, 1996a).

From a computational point of view, speculative
language detection is an emerging area of research,
and it is only in the last five years that a relatively
large body of work has been produced. In the re-
mainder of this section, we survey the main ap-
proaches to hedge recognition, particularly in En-
glish and in research discourse.

Medlock and Briscoe (2007) applied a weakly su-
pervised learning algorithm to classify sentences as
speculative or non-speculative, using a corpus they
built and made publicly available. Morante and
Daelemans (2009) not only tried to detect hedge
cues but also to identify their scope, using a met-
alearning approach based on three supervised learn-
ing methods. They achieved an F1 of 84.77 for
hedge identification, and 78.54 for scope detection
(using gold-standard hedge signals) in the Abstracts
sections of the Bioscope corpus.

Task 2 of the CoNLL-2010 Conference Shared
Task (Farkas et al., 2010b) proposed solving the
problem of in-sentence hedge cue phrase identi-

38



fication and scope detection in two different do-
mains (biological publications and Wikipedia arti-
cles), based on manually annotated corpora. The
evaluation criterion was in terms of precision, recall
and F-measure, accepting a scope as correctly clas-
sified if the hedge cue and scope boundaries were
both correctly identified.

The best result on hedge cue identification (Tang
et al., 2010) obtained an F-score of 81.3 using a su-
pervised sequential learning algorithm to learn BIO
classes from lexical and shallow parsing informa-
tion, also including certain linguistic rules. For
scope detection, Morante et al. (2010) obtained an
F-score of 57.3, using also a sequence classification
approach for detecting boundaries (tagged in FOL
format, where the first token of the span is marked
with an F, while the last one is marked with an
L). The attributes used included lexical information,
dependency parsing information, and some features
based on the information in the parse tree.

The approximation of Velldal et al. (2010) for
scope detection was somewhat different: they de-
veloped a set of handcrafted rules, based on depen-
dency parsing and lexical features. With this ap-
proach, they achieved an F-score of 55.3, the third
best for the task. Similarly, Kilicoglu and Bergler
(2010) used a pure rule-based approach based on
constituent parse trees in addition to syntactic de-
pendency relations, and achieved the fourth best F-
score for scope detection, and the highest precision
of the whole task (62.5). In a recent paper, Vell-
dal et al. (2012) reported a better F-score of 59.4 on
the same corpus for scope detection using a hybrid
approach that combined a set of rules on syntactic
features and n-gram features of surface forms and
lexical information and a machine learning system
that selected subtrees in constituent structures.

3 Corpus

The system presented in this paper uses the Bio-
scope corpus (Vincze et al., 2008) as a learning
source and for evaluation purposes. The Bioscope
corpus is a freely available corpus of medical free
texts, biological full papers and biological abstracts,
annotated at a token level with negative and specu-
lative keywords, and at sentence level with their lin-
guistic scope.

Clinical Full Abstract
#Documents 954 9 1273
#Sentences 6383 2670 11871
%Hedge Sentences 13.4 19.4 17.7
#Hedge cues 1189 714 2769

Table 1: Bioscope corpus statistics about hedging

Table 1, extracted from Vincze et al. (2008), gives
some statistics related to hedge cues and sentences
for the three sub corpora included in Bioscope.

For the present study, we usee only the Abstract
sub corpus for training and evaluation. We randomly
separated 20% of the corpus, leaving it for evalu-
ation purposes. We further sub-divided the remain-
ing training corpus, separating another 20% that was
used as a held out corpus. All the models presented
here were trained on the resulting training corpus
and their performance evaluated on the held out cor-
pus. The final results were computed on the previ-
ously unseen evaluation corpus.

4 Task description

From a computational point of view, both hedge
cue identification and scope detection can be seen
as a sequence classification problem: given a sen-
tence, classify each token as part of a hedge cue (or
scope) or not. In almost every classification prob-
lem, two main approaches can be taken (although
many variations and combinations exist in the lit-
erature): build the classifier as a set of handcrafted
rules, which, from certain attributes of the instances,
decide which category it belongs to, or learn the
classifier from previously annotated examples, in a
supervised learning approach.

The rules approach is particularly suitable when
domain experts are available to write the rules, and
when features directly represent linguistic informa-
tion (for example, POS-tags) or other types of do-
main information. It is usually a time-consuming
task, but it probably grasps the subtleties of the lin-
guistic phenomena studied better, making it possible
to take them into account when building the classi-
fier. The supervised learning approach needs tagged
data; in recent years the availability of tagged text

39



has grown, and this type of method has become the
state-of-the-art solution for many NLP problems.

In our particular problem, we have both tagged
data and expert knowledge (represented by the body
of work on modality and hedging), so it seems rea-
sonable to see how we can combine the two methods
to achieve better classification performance.

4.1 Identifying hedge cues
The best results so far for this task used a token
classification approach or sequential labelling tech-
niques, as Farkas et al. (2010b) note. In both cases,
every token in the sentence is assigned a class la-
bel indicating whether or not that word is acting as a
hedge cue. To allow for multiple-token hedge cues,
we identify the first token of the span with the class
B and every other token in the span with I, keeping
the O class for every token not included in the span,
as the following example shows:

(2) The/O findings/O indicate/B that/I MNDA/O
expression/O is/O . . . [ 401.8]

After token labelling, hedge cue identification can
be seen as the problem of assigning the correct class
to each token of an unlabelled sentence. Hedge cue
identification is a sequential classification task: we
want to assign classes to an entire ordered sequence
of tokens and try to maximize the probability of as-
signing the correct classes to every token in the se-
quence, considering the sequence as a whole, not
just as a set of isolated tokens.

4.2 Determining the scope of hedge cues
The second sub-task involves marking the part of the
sentence affected by the previously identified hedge
cue. Scopes are also spans of text (typically longer
than multi-word hedge cues), so we could use the
same reduction to a token classification task. Be-
ing longer, FOL classes are usually used for clas-
sification, identifying the first token of the scope
as F, the last token as L and any other token in
the sentence as O. Scope detection poses an addi-
tional problem: hedge cues cannot be nested, but
scopes (as we have already seen) usually are. In
example 1, the scope of ‘may’ is nested within the
scope of ‘suggests’. To overcome this, Morante
and Daelemans (2009) propose to generate a dif-
ferent learning example for each cue in the sen-

tence. In this setting, each example becomes a pair
〈labelled sentence, hedge cue position〉. So, for ex-
ample 1, the scope learning instances would be:

(3) 〈This/O finding/O suggests/F that/O the/O
BZLF1/O promoter/O may/O be/O
regulated/O by/O the/O degree/O of/O
squamous/O differentiation/L./O, 3〉

(4) 〈This/O finding/O suggests/O that/O the/F
BZLF1/O promoter/O may/O be/O
regulated/O by/O the/O degree/O of/O
squamous/O differentiation/L./O, 8〉

Learning on these instances, and using a similar
approach to the one used in the previous task, we
should be able to identify scopes for previously un-
seen examples. Of course, the two tasks are not in-
dependent: the success of the second one depends
on the success of the first. Accordingly, evaluation
of the second task can be done using gold standard
hedge cues or with the hedge cues learned in the first
task.

5 Methodology and System Description

To approach both sequential learning tasks, we fol-
low a learning methodology (depicted in Figure 1),
that starts with an initial guess of attributes for su-
pervised learning and a learning method, and tries
to improve its performance by incorporating domain
knowledge. We consider that expressing this knowl-
edge through rules (instead of learning features) is a
better way for a domain expert to suggest new use-
ful information or to generalize certain relations be-
tween attributes and classification results when the
learning method cannot achieve this because of in-
sufficient training data. These rules, of course, have
to be converted to attributes to incorporate them into
the learning process. These attributes are what we
call knowledge rules and their generation will be de-
scribed in the Analysis section.

5.1 Preprocessing
Before learning, we propose to add every possible
item of external information to the corpus so as to
integrate different sources of knowledge (either the
result of external analysis or in the form of seman-
tic resources). After this step, all the information
is consolidated into a single structure, facilitating

40



subsequent analysis. In our case, we incorporate
POS-tagging information, resulting from the appli-
cation of the GENIA tagger (Tsuruoka et al., 2005),
and deep syntax information obtained with the ap-
plication of the Stanford Parser (Klein and Manning,
2003), leading to a syntax-oriented representation of
the training data. For a detailed description of the
enriching process, the reader is referred to Moncec-
chi et al. (2010).

5.2 Initial Classifier
The first step for improving performance is, of
course, to select an initial set of learning features,
and learn from training data to obtain the first clas-
sifier, in a traditional supervised learning scenario.
The sequential classification method will depend on
the addressed task. After learning, the classifier is
applied on the held out corpus to evaluate its per-
formance (usually in terms of Precision, Recall and
F1-measure), yielding performance results and a list
of errors for analysis. This information is the source
for subsequent linguistic analysis. As such, it seems
important to provide ways to easily analyse instance
attributes and learning errors. For our tasks, we
have developed visualization tools to inspect the tree
representation of the corpus data, the learning at-
tributes, and the original and predicted classes.

5.3 Analysis
From the classifier results on the held-out corpus,
an analysis phase starts, which tries to incorporate
linguistic knowledge to improve performance.

One typical form of introducing new information
is through learning features: for example, we can
add a new attribute indicating if the current instance
(in our case, a sentence token) belongs to a list of
common hedge cues.

However, linguistic or domain knowledge can
also naturally be stated as rules that suggest the class
or list of classes that should be assigned to instances,
based on certain conditions on features, linguistic
knowledge or data observation. For example, based
on corpus annotation guidelines, a rule could state
that the scope of a verb hedge cue should be the verb
phrase that includes the cue, as in the expression

(5) This finding {suggests suggests that the BZLF1
promoter may be regulated by the degree of

squamous differentiation}suggests.

We assume that these rules take the form ‘if a con-
dition C holds then classify instance X with class Y’.
In the previous example, assuming a FOL format
for scope identification, the token ‘suggest’ should
be assigned class F and the token ‘differentiation’
should be assigned class L, assigning class O to ev-
ery other token in the sentence.

The general problem with these rules is that as
we do not know in fact if they always apply, we do
not want to directly modify the classification results,
but to incorporate them as attributes for the learning
task. To do this, we propose to use a similar ap-
proach to the one used by Rosá (2011), i.e. to incor-
porate these rules as a new attribute, valued with the
class predictions of the rule, trying to ‘help’ the clas-
sifier to detect those cases where the rule should fire,
without ignoring the remaining attributes. In the pre-
vious example, this attribute would be (when the rule
condition holds) valued F or L if the token corre-
sponds to the first or last word of the enclosing verb
phrase, respectively. We have called these attributes
knowledge rules to reflect the fact that they suggest
a classification result based on domain knowledge.

This configuration allows us to incorporate
heuristic rules without caring too much about their
potential precision or recall ability: we expect the
classification method to do this for us, detecting cor-
relations between the rule result (and the rest of the
attributes) and the predicted class.

There are some cases where we do actually want
to overwrite classifier results: this is the case when
we know the classifier has made an error, because
the results are not well-formed. For example, we
have included a rule that modifies the assigned
classes when the classifier has not exactly found one
F token and one L token, as we know for sure that
something has gone wrong. In this case, we decided
to assign the scope based on a series of postprocess-
ing rules: for example, assign the scope of the en-
closing clause in the syntax tree as hedge scope, in
the case of verb hedge cues.

For sequential classification tasks, there is an ad-
ditional issue: sometimes the knowledge rule indi-
cates the beginning of the sequence, and its end can
be determined using the remaining attributes. For
example, suppose the classifier suggests the class

41



Hedge PPOS GPPOS Lemma PScope GPScope Scope
O VP S This O O O
O VP S finding O O O
O VP S suggest O O O
O VP S that O O O
O VP S the O F F
O VP S BZLF1 O O O
O VP S promoter O O O
B VP S may F O O
O VP S be O O O
O VP S regulate O O O
O VP S by O O O
O VP S the O O O
O VP S degree O O O
O VP S of O O O
O VP S squamous O O O
O VP S differentiation L L O
O VP S . O O O

Table 2: Evaluation instance where the scope ending
could not be identified

scope in the learning instance shown in table 2 (us-
ing as attributes the scopes of the parent and grand-
parent constituents for the hedge cue in the syntax
tree). If we could associate the F class suggested
by the classifier with the grand parent scope rule,
we would not be concerned about the prediction for
the last token, because we would knew it would al-
ways correspond to the last token of the grand par-
ent clause. To achieve this, we modified the class we
want to learn, introducing a new class, say X, instead
of F, to indicate that, in those cases, the L token must
not be learned, but calculated in the postprocessing
step, in terms of other attributes’ values (in this ex-
ample, using the hedge cue grandparent constituent
limits). This change also affects the classes of train-
ing data instances (in the example, every training
instance where the scope coincides with the grand
parent scope attribute will have its F-classified to-
ken class changed to X).

In the previous example, if the classifier assigns
class X to the ‘the’ token, the postprocessing step
will change the class assigned to the ‘differentiation’
token to L, no matter which class the classifier had
predicted, changing also the X class to the original
F, yielding a correctly identified scope.

After adding the new attributes and changing the
relevant class values in the training set, the process
starts over again. If performance on the held out cor-
pus improves, these attributes are added to the best
configuration so far, and used as the starting point
for a new analysis. When no further improvement
can be achieved, the process ends, yielding the best

Figure 1: Methodology overview

classifier as a result.
We applied the proposed methodology to the tasks

of hedge cue detection and scope resolution. We
were mainly interested in evaluating whether sys-
tematically applying the methodology would indeed
improve classifier performance. The following sec-
tions show how we tackled each task, and how we
managed to incorporate expert knowledge and im-
prove classification.

5.4 Hedge Cue Identification
To identify hedge cues we started with a sequen-
tial classifier based on Conditional Random Fields
(Lafferty et al., 2001), the state-of-the-art classifi-
cation method used for sequence supervised learn-
ing in many NLP tasks. The baseline configuration
we started with included a size-2 window of surface
forms to the left and right of the current token, pairs
and triples of previous/current surface forms. This
led to a highly precise classifier (an F-measure of
95.5 on the held out corpus). After a grid search
on different configurations of surface forms, lemmas
and POS tags, we found (somewhat surprisingly)
that the best precision/recall tradeoff was obtained
just using a window of size 2 of unigrams of sur-
face forms, lemmas and tokens with a slightly worse
precision than the baseline classifier, but compen-

42



Configuration P R F1
Baseline 95.5 74.0 83.4
Conf1 94.7 80.3 86.9
Conf2 91.3 84.0 87.5

Table 3: Classification performance on the held out cor-
pus for hedge cue detection. Conf1 corresponds to win-
dows of Word, Lemma and POS attributes and Conf2 in-
corporates hedge cue candidates and cooccuring words

sated by an improvement of about six points in re-
call, achieving an F-score of 86.9.

In the analysis step of the methodology we found
that most errors came from False Negatives, i.e.
words incorrectly not marked as hedges. We also
found that those words actually occurred in the train-
ing corpus as hedge cues, so we decided to add new
rule attributes indicating membership to certain se-
mantic classes. After checking the literature, we
added three attributes:

• Hyland words membership: this feature was set
to Y if the word was part of the list of words
identified by Hyland (2005)

• Hedge cue candidates: this feature was set to
Y if the word appeared as a hedge cue in the
training corpus

• Words co-occurring with hedge cue candidates:
this feature was set to Y if the word cooccured
with a hedge cue candidate in the training cor-
pus. This feature is based on the observation
that 43% of the hedges in a corpus of scientific
articles occur in the same sentence as at least
another device (Hyland, 1995).

After adding these attributes and tuning the win-
dow sizes, performance improved to an F-score of
87.5 in the held-out corpus

5.5 Scope identification
To learn scope boundaries, we started with a similar
configuration of a CRF classifier, using a window of
size 2 of surface forms, lemmas and POS-tags, and
the hedge cue identification attribute (either obtained
from the training corpus when using gold standard

hedge cues or learned in the previous step), achiev-
ing a performance of 63.7 in terms of F-measure.
When we incorporated information in the form of a
knowledge rule that suggested the scope of the con-
stituent of the parsing tree headed by the parent node
of the first word of the hedge cue, and an attribute
containing the parent POS-tag, performance rapidly
improved about two points measured in terms of F-
score.

After several iterations, and analyzing classifica-
tion errors, we included several knowledge rules, at-
tributes and postprocessing rules that dramatically
improved performance on the held-out corpus:

• We included attributes for the scope of the next
three ancestors of the first word of the hedge
cue in the parsing tree, and their respective
POS-tags, in a similar way as with the parent.
We also included a trigram with the ancestors
POS from the word upward in the tree.

• For parent and grandparent scopes, we incor-
porated X and Y classes instead of F, and mod-
ified postprocessing to use the last token of the
corresponding scope when one of these classes
was learned.

• We modified the ancestors scopes to reflect
some corpus annotation guidelines or other cri-
teria induced after data examination. For ex-
ample, we decided not to include adverbial
phrases or prepositional phrases at the begin-
ning of scopes, when they corresponded to a
clause, as in

(6) In addition,{unwanted and potentially
hazardous specificities may be
elicited. . .}

• We added postprocessing rules to cope with
cases where (probably due to insufficient train-
ing data), the classifier missclasified certain in-
stances. For example, we forced classification
to use the next enclosing clause (instead of verb
phrase), when the hedge cue was a verb conju-
gated in passive voice, as in

(7) {GATA3 , a member of the GATA family
that is abundantly expressed in the
T-lymphocyte lineage , is thought to
participate in ...}.

43



Configuration Gold-P P R F1
Baseline 66.4 68.6 59.6 63.8
Conf1 68.7 71.3 61.8 66.2
Conf2 73.3 75.6 65.4 70.1
Conf3 80.9 82.1 71.3 76.3
Conf4 88.2 82.0 76.3 79.1

Table 4: Classification performance on the held out cor-
pus. The baseline used a window of Word, Lemma,
POS attributes and hedge cue tag; Conf1 included parent
scopes, Conf2 added grandparents information; Conf3
added postprocessing rules. Finally, Conf4 used adjusted
scopes and incorporated new postprocessing rules

• We excluded references at the end of sentences
from all the calculated scopes.

• We forced classification to the next S,VP or NP
ancestor constituent in the syntax tree (depend-
ing on the hedge cue POS), when full scopes
could not be determined by the statistical clas-
sifier (missing either L or F, or learning more
than one of them in the same sentence).

Table 4 summarizes the results of scope identifi-
cation in the held out corpus. The first results were
obtained using gold-standard hedge cues, while the
second ones used the hedge cues learned in the pre-
vious step (for hedge cue identification, we used the
best configuration we found). In the gold-standard
results, Precision, Recall and the F-measure are
the same because every False Positive (incorrectly
marked scope) implied a False Negative (the missed
right scope).

6 Evaluation

To determine classifier performance, we evaluated
the classifiers found after improvement on the eval-
uation corpus. We also evaluated the less efficient
classifiers to see whether applying the iterative im-
provement had overfitted the classifier to the corpus.
To evaluate scope detection, we used the best con-
figuration found in the evaluation corpus for hedge
cue identification. Tables 5 and 6 show the results
for the hedge cue recognition and scope resolution,
respectively. In both tasks, classifier performance

Configuration P R F1
Baseline 97.9 78.0 86.8
Conf1 95.9 84.9 90.1
Conf2 94.1 88.6 91.3

Table 5: Classification performance on the evaluation
corpus for hedge cue detection

Configuration Gold-P P R F1
Baseline 74.0 71.9 68.1 70.0
Conf1 76.5 74.4 70.2 72.3
Conf2 80.0 77.2 72.9 75.0
Conf3 83.1 80.0 75.2 77.3
Conf4 84.7 80.1 75.8 77.9

Table 6: Classification performance on the evaluation
corpus for scope detection

improved in a similar way to the results obtained on
the held out corpus.

Finally, to compare our results with state-of-the-
art methods (even though that was not the main
objective of the study), we used the corpus of de
CoNLL 2010 Shared Task to train and evaluate our
classifiers, using the best configurations found in the
evaluation corpus, and obtained competitive results
in both subtasks of Task 2. Our classifier for hedge
cue detection achieved an F-measure of 79.9, bet-
ter than the third position in the Shared Task for
hedge identification. Scope detection results (us-
ing learned hedge cues) achieved an F-measure of
54.7, performing better than the fifth result in the
corresponding task, and five points below the best
results obtained so far in the corpus (Velldal et al.,

Hedge cue iden-
tification

Scope detection

Best results 81.7/81.0/81.3 59.6/55.2/57.3
Our results 83.2/76.8/79.9 56.7/52.8/54.7

Table 7: Classification performance compared with
best results in CoNLL Shared Task. Figures represent
Precision/Recall/F1-measure

44



2012). Table 7 summarizes these results in terms of
Precision/Recall/F1-measure.

7 Conclusions and Future Research

In this paper we have presented an iterative method-
ology to improve classifier performance by incor-
porating linguistic knowledge, and proposed a way
to incorporate domain rules to the learning process.
We applied the methodology to the task of hedge
cue recognition and scope finding, improving per-
formance by incorporating information of training
corpus occurrences and co-occurrences for the first
task, and syntax constituents information for the sec-
ond. In both tasks, results were competitive with
the best results obtained so far on a publicly avail-
able corpus. This methodology could be easily used
for other sequential (or even traditional) classifica-
tion tasks.

Two directions are planned for future research:
first, to improve the classifier results by incorporat-
ing more knowledge rules such as those described by
Velldal et al. (2012) or semantic resources, specially
for the scope detection task. Second, to improve the
methodology, for example by adding some way to
select the most common errors in the held out cor-
pus and write rules based on their examination.

References
S. Ananiadou, D. Kell, and J. Tsuj. 2006. Text min-

ing and its potential applications in systems biology.
Trends in Biotechnology, 24(12):571–579, December.

Richárd Farkas, Veronika Vincze, György Móra, János
Csirik, and György Szarvas. 2010a. The CoNLL-
2010 shared task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1–12, Uppsala, Sweden,
July. Association for Computational Linguistics.

Richárd Farkas, Veronika Vincze, György Szarvas,
György Móra, and János Csirik, editors. 2010b. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.

Ken Hyland. 1995. The author in the text: Hedging sci-
entific writing. Hongkong Papers in Linguistics and
Language Teaching, 18:33–42.

Ken Hyland. 1996a. Talking to the academy: Forms of
hedging in science research articles. Written Commu-
nication, 13(2):251–281.

Ken Hyland. 1996b. Writing without conviction? Hedg-
ing in science research articles. Applied Linguistics,
17(4):433–454, December.

Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum Discourse. Continuum.

Halil Kilicoglu and Sabine Bergler. 2010. A high-
precision approach to detecting hedges and their
scopes. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
70–77, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ’03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423–430, Morristown, NJ,
USA. Association for Computational Linguistics.

John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML-01, pages 282–289.

George Lakoff. 1973. Hedges: A study in meaning crite-
ria and the logic of fuzzy concepts. Journal of Philo-
sophical Logic, 2(4):458–508, October.

Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and
statements in between. In Lynette Hirschman and
James Pustejovsky, editors, HLT-NAACL 2004 Work-
shop: BioLINK 2004, Linking Biological Literature,
Ontologies and Databases, pages 17–24, Boston,
Massachusetts, USA, May. Association for Computa-
tional Linguistics.

Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.

Guillermo Moncecchi, Jean-Luc Minel, and Dina Won-
sever. 2010. Enriching the bioscope corpus with lex-
ical and syntactic information. In Workshop in Natu-
ral Language Processing and Web-based Tecnhologies
2010, pages 137–146, November.

Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28–
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.

Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational Linguistics, pages 1–72, February.

Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In EMNLP ’08: Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–724, Morristown, NJ,
USA. Association for Computational Linguistics.

45



Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scopes of hedge cues. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 40–47, Uppsala, Sweden, July.
Association for Computational Linguistics.

R. F. Palmer. 2001. Mood and Modality. Cambridge
Textbooks in Linguistics. Cambridge University Press,
New York.

Aiala Rosá. 2011. Identificación de opiniones de difer-
entes fuentes en textos en español. Ph.D. thesis, Uni-
versidad de la República (Uruguay), Université Paris
Ouest (France), September.

Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 13–17,
Uppsala, Sweden, July. Association for Computational
Linguistics.

Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun’ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382–392.
Springer Berlin Heidelberg, Berlin, Heidelberg.

Erik Velldal, Lilja Øvrelid, and Stephan Oepen. 2010.
Resolving speculation: Maxent cue classification and
dependency-based scope rules. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 48–55, Uppsala, Sweden,
July. Association for Computational Linguistics.

Erik Velldal, Lilja Øvrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational Lin-
guistics, pages 1–64, February.

Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope cor-
pus: biomedical texts annotated for uncertainty, nega-
tion and their scopes. BMC Bioinformatics, 9(Suppl
11):S9+.

Kail von Fintel, 2006. Modality and Language. MacMil-
lan Reference USA.

46


