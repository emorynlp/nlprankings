










































IMASS: An Intelligent Microblog Analysis and Summarization System


Proceedings of the ACL-HLT 2011 System Demonstrations, pages 133–138,
Portland, Oregon, USA, 21 June 2011. c©2011 Association for Computational Linguistics

IMASS: An Intelligent Microblog Analysis and Summarization System 

 

Jui-Yu Weng Cheng-Lun Yang Bo-Nian Chen Yen-Kai Wang Shou-De Lin 
Department of Computer Science and Information Engineering 

National Taiwan University 

{r98922060,r99944042,f92025,b97081,sdlin}@csie.ntu.edu.tw 

 

Abstract 

This paper presents a system to summarize 

a Microblog post and its responses with the 

goal to provide readers a more constructive 

and concise set of information for efficient 

digestion. We introduce a novel two-phase 

summarization scheme. In the first phase, 

the post plus its responses are classified in-

to four categories based on the intention, 

interrogation, sharing, discussion and chat. 

For each type of post, in the second phase, 

we exploit different strategies, including 

opinion analysis, response pair identifica-

tion, and response relevancy detection, to 

summarize and highlight critical informa-

tion to display. This system provides an al-

ternative thinking about machine-

summarization: by utilizing AI approaches, 

computers are capable of constructing dee-

per and more user-friendly abstraction. 

1 Introduction 

As Microblog services such as Twitter have be-

come increasingly popular, it is critical to re-

consider the applicability of the existing NLP 

technologies on this new media sources. Take 

summarization for example, a Microblog user 

usually has to browse through tens or even hun-

dreds of posts together with their responses daily, 

therefore it can be beneficial if there is an intelli-

gent tool assisting summarizing those information.  

Automatic text summarization (ATS) has been 

investigated for over fifty years, but the majority of 

the existing techniques might not be appropriate 

for Microblog write-ups. For instance, a popular 

kind of approaches for summarization tries to iden-

tify a subset of information, usually in sentence 

form, from longer pieces of writings as summary 

(Das and Martins, 2007). Such extraction-based 

methods can hardly be applied to Microblog texts 

because many posts/responses contain only one 

sentence.  
Below we first describe some special characte-

ristics that deviates the Microblog summarization 

task from general text summarization.  

a. The number of sentences is limited, and sen-
tences are usually too short and casual to con-

tain sufficient structural information or cue 

phrases. Unlike normal blogs, there is a strict 

limitation on the number of characters for each 

post (e.g. 140 characters for Twitter and Plurk 

maximum). Microblog messages cannot be 

treated as complete documents so that we can-

not take advantage of the structural information. 

Furthermore, users tend to regard Microblog as 

a chatting board. They write casually with 

slangs, jargons, and incorrect grammar.  

b. Microblog posts can serve several different 
purposes. At least three different types of posts 

are observed in Microblogs, expressing feeling, 

sharing information, and asking questions. 

Structured language is not the only means to 

achieve those goals. For example, people 

sometimes use attachment, as links or files, for 

sharing, and utilize emoticons and pre-defined 

qualifiers to express their feelings.  The diver-

sity of content differ Microblogs from general 

news articles. Consequently, using one mold to 

fit all types of Microblog posts is not sufficient. 

Different summarization schemes for posts 

with different purposes are preferred.  
c. Posts and responses in Microblogs are more 

similar to a multi-persons dialogue corpus. One 

of the main purposes of a Microblog is to serve 

as the fast but not instant communication 

channel among multiple users. Due to the free-

chatting, multi-user characteristics, the topic of 

a post/response thread can drift quickly. Some-

times, the topic of discussion at the end of the 

thread is totally unrelated to that of the post. 

133



This paper introduces a framework that summariz-

es a post with its responses. Motivated by the ab-

ovementioned characteristics of Microblogs, we 

plan to use a two-phase summarization scheme to 

develop different summarization strategies for dif-

ferent type of posts (see Figure 1). In the first 

phase, a post will be automatically classified into 

several categories including interrogation, discus-

sion, sharing and chat based on the intention of the 

users. In the second phase, the system chooses dif-

ferent summarization components for different 

types of posts.  

The novelties of this system are listed below. 

1. Strategically, we propose an underlying 2-phase 
framework for summarizing Microblog posts. 

The system can be accessed online at 

http://mslab.csie.ntu.edu.tw/~fishyz/plurk/. 

2. Tactically, we argue that it is possible to inte-
grate post-intention classification, opinion anal-

ysis, response relevancy and response-pair 

mining to create an intelligent summarization 

framework for Microblog posts and responses. 

We also found that the content features are not 

as useful as the temporal or positional features 

for text mining in Microblog. 

3. Our work provides an alternative thinking about 
ATS. It is possible to go beyond the literal 

meaning of summarization to exploit advanced 

text mining methods to improve the quality and 

usability of a summarization system. 

2 Summarization Framework and Expe-
riments 

Below we discuss our two-phase summarization 

framework and the experiment results on each in-

dividual component. Note that our experiments 

were tested on the Plurk dataset, which is one of 

the most popular micro-blogging platforms in Asia. 

Our observation is that Microblog posts can 

have different purposes. We divide them into four 

categories, Interrogation, Sharing, Discussion, and 

Chat.  

The Interrogation posts are questions asked in 

public with the hope to obtain some useful answers 

from friends or other users. However, it is very 

common that some repliers do not provide mea-

ningful answers. The responses might serve the 

purpose for clarification or, even worse, have noth-

ing to do with the question.  Hence we believe the 

most appropriate summarization process for this 

kind of posts is to find out which replies really re-

spond to the question. We created a response re-

levance detection component to serve as its 

summarization mechanism. 

The Sharing posts are very frequently observed 

in Microblog as Microbloggers like to share inter-

esting websites, pictures, and videos with their 

friends. Other people usually write down their 

comments or feelings on the shared subjects in the 

responses. To summarize such posts, we obtain the 

statistics on how many people have positive, neu-

tral, and negative attitude toward the shared sub-

jects. We introduce the opinion analysis 

component that provides the analysis on whether 

the information shared is recommended by the res-

pondents. 

We also observe that some posts contain charac-

teristics of both Interrogation and Sharing. The 

users may share a hyperlink and ask for others’ 

opinions at the same time. We create a category 

named Discussion for these posts, and apply both 

response ranking and opinion analysis engines on 

this type of posts.  
Finally, there are posts which simply act as the 

solicitation for further chat. For example, one user 
writes “so sad…” and another replies “what hap-
pened”. We name this type of posts/responses as 
Chat. This kind of posts can sometimes involve 
multiple persons and the topic may gradually drift 
to a different one. We believe the plausible sum-
marization strategy is to group different messages 
based on their topics. Therefore for Chat posts, we 
designed a response pair identification system to 
accomplish such goal. We group the related res-
ponses together for display, and the number of 
groups represents the number of different topics in 
this thread.  

Figure 1 shows the flow of our summarization 

 
Figure 1. System architecture  

134



framework. When an input post with responses 

comes in, the system first determines its intention, 

based on which the system adopts proper strategies 

for summarization. Below we discuss the technical 

parts of each sub-system with experiment results. 

2.1 Post Intention Classification 

This stage aims to classify each post into four cat-

egories, Interrogation, Sharing, Discussion, and 

Chat. One tricky issue is that the Discussion label 

is essentially a combination of interrogation and 

sharing labels. Therefore, simply treating it as an 

independent label and use a typical multi-label 

learning method can hurt the performance. We ob-

tain 76.7% (10-fold cross validation) in accuracy 

by training a four-class classifier using the 6-gram 

character language model. To improve the perfor-

mance, we design a decision-tree based framework 

that utilizes both manually-designed rules and dis-

criminant classification engine (see Figure 2). The 

system first checks whether the posts contains 

URLs or pointers to files, then uses a binary clas-

sifier to determine whether the post is interrogative.  

For the experiment, we manually annotate 6000 

posts consisting of 1840 interrogation, 2002 shar-

ing, 1905 chat, and 254 discussion posts. We train 

a 6-gram language model as the binary interroga-

tion classifier. Then we integrate the classifier into 

our system and test on 6000 posts to obtain a test-

ing accuracy of 82.8%, which is significantly bet-

ter than 76.7% with multi-class classification. 

2.2 Opinion Analysis 

Opinion analysis is used to evaluate public prefe-

rence on the shared subject. The system classifies 

responses into 3 categories, positive, negative, and 

neutral. 

Here we design a two-level classification 

framework using Naïve-Bayes classifiers which 

takes advantage of the learned 6-gram language 

model probabilities as features. First of all, we 

train a binary classifier to determine if a post or a 

reply is opinionative. This step is called the subjec-

tivity test. If the answer is yes, we then use another 

binary classifier to decide if the opinion is positive 

or negative. The second step is called the polarity 

test.  

For subjectivity test, we manually annotate 3244 

posts, in which half is subjective and half is objec-

tive. The 10-fold cross validation shows average 

accuracy of 70.5%.  

For polarity test, we exploit the built-in emoti-

cons in Plurk to automatically extract posts with 

positive and negative opinions. We collect 10,000 

positive and 10,000 negative posts as training data 

to train a language model of Naïve Bayes classifier, 

and evaluate on manually annotated data of 3121 

posts, with 1624 positive and 1497 negative to ob-

tain accuracy of 0.722. 

2.3 Response Pair Identification 

Conversation in micro-blogs tends to diverge into 

multiple topics as the number of responses grows. 

Sometimes such divergence may result in res-

ponses that are irrelevant to the original post, thus 

creating problems for summarization. Furthermore, 

because the messages are usually short, it is diffi-

cult to identify the main topics of these dialogue-

like responses using only keywords in the content 

for summarization. Alternatively, we introduce a 

subcomponent to identify Response Pairs in micro-

blogs. A Response Pair is a pair of responses that 

the latter specifically responds to the former. Based 

on those pairs we can then form clusters of mes-

sages to indicate different group of topics and mes-

 
Figure 2. The post classification procedure 

Feature Description Weight 

Backward Refe-

rencing  

Latter response content 

contains former respond-

er’s display name 

0.055 

Forward Refe-

rencing of user 

name 

Former response contains 

latter response’s author’s 

user name 

0.018 

Response position 

difference 

Number of responses in 

between responses 
0.13 

Content similarity Contents’ cosine similari-

ty using n-gram models. 

0.025 

Response time 

difference 

Time difference between 

responses in seconds 

0.012 

Table 1. Feature set with their description and weights 
 

135



sages. 

Looking at the content of micro-blogs, we ob-

serve that related responses are usually adjacent to 

each other as users tend to closely follow whether 

their messages are responded and reply to the res-

ponses from others quickly. Therefore besides con-

tent features, we decide to add the temporal and 

ordering features (See Table 1) to train a classifier 

that takes a pair of messages as inputs and return 

whether they are related. By identifying the re-

sponse pairs, our summarization system is able to 

group the responses into different topic clusters 

and display the clusters separately. We believe 

such functionality can assist users to digest long 

Microblog discussions. 

For experiment, the model is trained using 

LIBSVM (Chang and Lin, 2001) (RBF kernel) 
with 6000 response pairs, half of the training set 

positive and half negative. The positive data can be 

obtained automatically based on Plurk’s built in 

annotation feature. Responses with @user_name 

string in the content are matched with earlier res-

ponses by the author, user_name. Based on the 

learned weights of the features, we observe that 

content feature is not very useful in determining 

the response pairs. In a Microblog dialogue, res-

pondents usually do not repeat the question nor 

duplicate the keywords. We also have noticed that 

there is high correlation between the responses re-

latedness and the number of other responses be-

tween them. For example, users are less likely to 

respond to a response if there have been many rep-

lies about this response already.  Statistical analy-

sis on positive training data shows that the average 

number of responses between related responses is 

2.3.  

We train the classifier using 6000 automatically-

extracted pairs of both positive and negative in-

stances. We manually annotated 1600 pairs of data 

for testing. The experiment result reaches 80.52% 

accuracy in identifying response pairs. The base-

line model which uses only content similarity fea-

ture reaches only 45% in accuracy.   

2.4 Response Relevance Detection 

For interrogative posts, we think the best summary 

is to find out the relevent responses as potential 

answers.  

 We introduce a response relevancy detection 

component for the problem. Similar to previous 

components, we exploit a supervised learning ap-

proach and the features’ weights, learned by 

LIBSVM with RBF kernel, are shown in Table 2. 

Temporal and Positional Features 

A common assertion is that the earlier responses 

have higher probability to be the answers of the 

question. Based on the learned weights, it is not 

surprising that most important feature is the posi-

tion of the response in the response hierarchy. 

Another interesting finding by our system is that 

meaningful replies do not come right away. Res-

ponses posted within ten seconds are usually for 

chatting/clarification or ads from robots. 

Content Features 

We use the length of the message, the cosine simi-

larity of the post and the responses, and the occur-

rence of the interrogative words in response 

sentences as content features.  

Because the interrogation posts in Plurk are rela-

tively few, we manually find a total of 382 positive 

and 403 negative pairs for training and use 10-fold 

cross validation for evaluation. 

We implement the component using LIBSVM 

(RBF Kernel) classifier.  The baseline is to always 

select the first response as the only relevant answer. 

The results show that the accuracy of baseline 

reaches 67.4%, far beyond that of our system 

73.5%.  

3 System Demonstration 

In this section, we show some snapshots of our 

summarization system with real examples using 

Plurk dataset. Our demo system is designed as a 

Feature Weight 

Response position 0.170 

Response time difference 0.008 

Response length 0.003 

Occurrence of interrogative 

words 

0.023 

Content similarity 0.023 

Table 2. Feature set and their weights 

 
Figure 3. The IMASS interface 

136



search engine (see Figure 3). Given a query term, 

our system first returns several posts containing the 

query string under the search bar. When one of the 

posts is selected, it will generate a summary ac-

cording to the detected intention and show it in a 

pop-up frame. We have recorded a video demon-

strating our system. The video can be viewed at   

http://imss-acl11-demo.co.cc/. 

For interrogative posts, we perform the response 

relevancy detection. The summary contains the 

question and relevant answers. Figure 4 is an ex-

ample of summary of an interrogative post. We can 

see that responses other than the first and the last 

responses are filtered because they are less relevant 

to the question. 

 For sharing posts, the summary consists of two 

parts. A pie chart that states the percentage of each 

opinion group is displayed. Then the system picks 

three responses from the majority group or one re-

sponse from each group if there is no significant 

difference. Figure 5 is an example that most 

friends of the user dfrag give positive feedback to 

the shared video link. 

For discussion posts, we combine the response 

relevancy detection subsystem and the opinion 

analysis sub-system for summarization. The former 

first eliminates the responses that are not likely to 

be the answer of the post. The latter then generates 

a summary for the post and relevant responses. The 

result is similar to sharing posts. 

For chat posts, we apply the response pair iden-

tification component to generate the summary. In 

the example, Figure 6, the original Plurk post is 

about one topic while the responses diverge to one 

or more unrelated topics. Our system clearly sepa-

rates the responses into multiple groups. This re-

presentation helps the users to quickly catch up 

with the discussion flow. The users no longer have 

to read interleaving responses from different topics 

and guess which topic group a response is referring 

to.  

 

 
Figure 4. An example of interrogative post. 

 
Figure 6. An Example of chat post 

 

 

 

 

 
Figure 5. An example of sharing post. 

 

137



4 Related Work 

We have not seen many researches focusing on the 

issues of Microblog summarization. We found on-

ly one work that discusses about the issues of 

summarization for Microblogs (Sharifi et al., 2010). 

Their goal, however, is very different from ours as 

they try to summarize multiple posts and do not 

consider the responses. They propose the Phrase 

Reinforcement Algorithm to find the most com-

monly used phrase that encompasses the topic 

phrase, and use these phrases to compose the 

summary. They are essentially trying to solve a 

multi-document summarization problem while our 

problem is more similar to short dialog summariza-

tion because the dialogue nature of Microblogs is 

one of the most challenging part that we tried to 

overcome.  

In dialogue summarization, many researchers 

have pointed out the importance of detecting re-

sponse pairs in a conversation. Zechner (2001) per-

forms an in depth analysis and evaluation in the 

area of open domain spoken dialogue summariza-

tion. He uses decision tree classifier with lexical 

features like POS tags to identify questions and 

applies heuristic rules like maximum distance be-

tween speakers to extract answers. Shrestha and 

McKeown (2004) propose a supervised learning 

method to detect question-answer pairs in Email 

conversations.  Zhou and Hovy (2005) concen-

trates on summarizing dialogue-style technical in-

ternet relay chats using supervised learning 

methods. Zhou further clusters chat logs into sev-

eral topics and then extract some essential response 

pairs to form summaries. Liu et al. (2006) propose 

to identify question paragraph via analyzing each 

participant’s status, and then use cosine measure to 

select answer paragraphs for online news dataset.  

The major differences between our components 

and the systems proposed by others lie in the selec-

tion of features. Due to the intrinsic difference be-

tween the writing styles of Microblog and other 

online sources, our experiments show that the con-

tent feature is not as useful as the position and 

temporal features. 

5 Conclusion 

In terms of length and writing styles, Microblogs 

possess very different characteristics than other 

online information sources such as web blogs and 

news articles. It is therefore not surprising that dif-

ferent strategies are needed to process Microblog 

messages. Our system uses an effective strategy to 

summarize the post/response by first determine the 

intention and then perform different analysis de-

pending on the post types. Conceptually, this work 

intends to convey an alternative thinking about 

machine-summarization. By utilizing text mining 

and analysis techniques, computers are capable of 

providing more intelligent summarization than in-

formation condensation. 

Acknowledgements  

This work was supported by National Science 

Council, National Taiwan University and Intel 

Corporation under Grants NSC99-2911-I-002-001, 

99R70600, and 10R80800. 

References  

Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM : 

a library for support vector machines. Software 

available at http://www.csie.ntu.edu.tw/~cjlin/libsvm 

Dipanjan Das and André F.T. Martins. 2007.  A Survey 

on Automatic Text Summarization. Literature Survey 

for the Language and Statistics II Course. CMU.  

Chuanhan Liu, Yongcheng Wang, and Fei Zheng. 2006. 

Automatic Text Summarization for Dialogue Style. 

In Proceedings of the IEEE International Conference 

on Information Acquisition. 274-278 

Beaux Sharifi, Mark A. Hutton, and Jugal Kalita. 2010. 

Summarizing Microblogs Automatically. In Proceed-

ings of the Human Language Technologies: The 

2010 Annual Conference of the North American 

Chapter of the Association for Computational Lin-

guistics (NAACL-HLT). 685-688 

Lokesh Shrestha and Kathleen McKeown. 2004. Detec-

tion of Question-Answer Pairs in Email Conversa-

tions. In Proceedings of the 23rd International 

Conference on Computational Linguistics (COLING 

2010).  

Klaus Zechner. 2001. Automatic Generation of Concise 

Summaries of Spoken Dialogues in Unrestricted 

Domains. In Proceedings of the 24th ACM-SIGIR 

International Conference on Research and Develop-

ment in Information Retrieval. 199-207. 

Liang Zhou and Eduard Hovy. 2005. Digesting virtual 

geek culture: The summarization of technical internet 

relay chats, in Proceedings of  the 43rd Annual Meet-

ing of the Association for Computational Linguistics 

(ACL 2005). 298-305. 

138


