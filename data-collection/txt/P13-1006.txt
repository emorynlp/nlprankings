



















































Grounded Language Learning from Video Described with Sentences


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–63,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Grounded Language Learning from Video Described with Sentences

Haonan Yu and Jeffrey Mark Siskind
Purdue University

School of Electrical and Computer Engineering
465 Northwestern Ave.

West Lafayette, IN 47907-2035 USA
haonan@haonanyu.com, qobi@purdue.edu

Abstract

We present a method that learns repre-
sentations for word meanings from short
video clips paired with sentences. Un-
like prior work on learning language from
symbolic input, our input consists of video
of people interacting with multiple com-
plex objects in outdoor environments. Un-
like prior computer-vision approaches that
learn from videos with verb labels or im-
ages with noun labels, our labels are sen-
tences containing nouns, verbs, preposi-
tions, adjectives, and adverbs. The cor-
respondence between words and concepts
in the video is learned in an unsupervised
fashion, even when the video depicts si-
multaneous events described by multiple
sentences or when different aspects of a
single event are described with multiple
sentences. The learned word meanings
can be subsequently used to automatically
generate description of new video.

1 Introduction

People learn language through exposure to a rich
perceptual context. Language is grounded by
mapping words, phrases, and sentences to mean-
ing representations referring to the world. Siskind
(1996) has shown that even with referential un-
certainty and noise, a system based on cross-
situational learning can robustly acquire a lexicon,
mapping words to word-level meanings from sen-
tences paired with sentence-level meanings. How-
ever, it did so only for symbolic representations of
word- and sentence-level meanings that were not
perceptually grounded. An ideal system would not
require detailed word-level labelings to acquire
word meanings from video but rather could learn
language in a largely unsupervised fashion, just as
a child does, from video paired with sentences.

There has been recent research on grounded lan-
guage learning. Roy (2002) pairs training sen-
tences with vectors of real-valued features ex-
tracted from synthesized images which depict 2D
blocks-world scenes, to learn a specific set of fea-
tures for adjectives, nouns, and adjuncts. Yu and
Ballard (2004) paired training images containing
multiple objects with spoken name candidates for
the objects to find the correspondence between
lexical items and visual features. Dominey and
Boucher (2005) paired narrated sentences with
symbolic representations of their meanings, au-
tomatically extracted from video, to learn object
names, spatial-relation terms, and event names as
a mapping from the grammatical structure of a
sentence to the semantic structure of the associated
meaning representation. Chen and Mooney (2008)
learned the language of sportscasting by deter-
mining the mapping between game commentaries
and the meaning representations output by a rule-
based simulation of the game. Kwiatkowski et al.
(2012) present an approach that learns Montague-
grammar representations of word meanings to-
gether with a combinatory categorial grammar
(CCG) from child-directed sentences paired with
first-order formulas that represent their meaning.

Although most of these methods succeed in
learning word meanings from sentential descrip-
tions they do so only for symbolic or simple vi-
sual input (often synthesized); they fail to bridge
the gap between language and computer vision,
i.e., they do not attempt to extract meaning rep-
resentations from complex visual scenes. On the
other hand, there has been research on training
object and event models from large corpora of
complex images and video in the computer-vision
community (Kuznetsova et al., 2012; Sadanand
and Corso, 2012; Kulkarni et al., 2011; Ordonez
et al., 2011; Yao et al., 2010). However, most
such work requires training data that labels indi-
vidual concepts with individual words (i.e., ob-

53



jects delineated via bounding boxes in images as
nouns and events that occur in short video clips
as verbs). There is no attempt to model phrasal
or sentential meaning, let alone acquire the ob-
ject or event models from training data labeled
with phrasal or sentential annotations. Moreover,
such work uses distinct representations for differ-
ent parts of speech; i.e., object and event recogniz-
ers use different representations.

In this paper, we present a method that learns
representations for word meanings from short
video clips paired with sentences. Our work dif-
fers from prior work in three ways. First, our input
consists of realistic video filmed in an outdoor en-
vironment. Second, we learn the entire lexicon,
including nouns, verbs, prepositions, adjectives,
and adverbs, simultaneously from video described
with whole sentences. Third we adopt a uniform
representation for the meanings of words in all
parts of speech, namely Hidden Markov Models
(HMMs) whose states and distributions allow for
multiple possible interpretations of a word or a
sentence in an ambiguous perceptual context.

We employ the following representation to
ground the meanings of words, phrases, and sen-
tences in video clips. We first run an object de-
tector on each video frame to yield a set of de-
tections, each a subregion of the frame. In prin-
ciple, the object detector need just detect the ob-
jects rather than classify them. In practice, we
employ a collection of class-, shape-, pose-, and
viewpoint-specific detectors and pool the detec-
tions to account for objects whose shape, pose,
and viewpoint may vary over time. Our methods
can learn to associate a single noun with detections
produced by multiple detectors. We then string to-
gether detections from individual frames to yield
tracks for objects that temporally span the video
clip. We associate a feature vector with each frame
(detection) of each such track. This feature vector
can encode image features (including the identity
of the particular detector that produced that detec-
tion) that correlate with object class; region color,
shape, and size features that correlate with object
properties; and motion features, such as linear and
angular object position, velocity, and acceleration,
that correlate with event properties. We also com-
pute features between pairs of tracks to encode the
relative position and motion of the pairs of objects
that participate in events that involve two partici-
pants. In principle, we can also compute features

between tuples of any number of tracks.

Following Yamoto et al. (1992), Siskind and
Morris (1996), and Starner et al. (1998), we repre-
sent the meaning of an intransitive verb, like jump,
as a two-state HMM over the velocity-direction
feature, modeling the requirement that the par-
ticipant move upward then downward. We rep-
resent the meaning of a transitive verb, like pick
up, as a two-state HMM over both single-object
and object-pair features: the agent moving to-
ward the patient while the patient is as rest, fol-
lowed by the agent moving together with the pa-
tient. We extend this general approach to other
parts of speech. Nouns, like person, can be rep-
resented as one-state HMMs over image features
that correlate with the object classes denoted by
those nouns. Adjectives, like red, round, and big,
can be represented as one-state HMMs over region
color, shape, and size features that correlate with
object properties denoted by such adjectives. Ad-
verbs, like quickly, can be represented as one-state
HMMs over object-velocity features. Intransitive
prepositions, like leftward, can be represented as
one-state HMMs over velocity-direction features.
Static transitive prepositions, like to the left of, can
be represented as one-state HMMs over the rela-
tive position of a pair of objects. Dynamic transi-
tive prepositions, like towards, can be represented
as HMMs over the changing distance between a
pair of objects. Note that with this formulation,
the representation of a verb, like approach, might
be the same as a dynamic transitive preposition,
like towards. While it might seem like overkill
to represent the meanings of words as one-state-
HMMs, in practice, we often instead encode such
concepts with multiple states to allow for temporal
variation in the associated features due to chang-
ing pose and viewpoint as well as deal with noise
and occlusion. Moreover, the general framework
of modeling word meanings as temporally variant
time series via multi-state HMMs allows one to
model denominalized verbs, i.e., nouns that denote
events, as in The jump was fast.

Our HMMs are parameterized with vary-
ing arity. Some, like jump(α), person(α),
red(α), round(α), big(α), quickly(α), and
leftward(α) have one argument, while oth-
ers, like pick-up(α, β), to-the-left-of(α, β), and
towards(α, β), have two arguments (In principle,
any arity can be supported.). HMMs are instanti-
ated by mapping their arguments to tracks. This

54



involves computing the associated feature vector
for that HMM over the detections in the tracks
chosen to fill its arguments. This is done with
a two-step process to support compositional se-
mantics. The meaning of a multi-word phrase
or sentence is represented as a joint likelihood
of the HMMs for the words in that phrase or
sentence. Compositionality is handled by link-
ing or coindexing the arguments of the conjoined
HMMs. Thus a sentence like The person to
the left of the backpack approached the trash-
can would be represented as a conjunction of
person(p0), to-the-left-of(p0, p1), backback(p1),
approached(p0, p2), and trash-can(p2) over the
three participants p0, p1, and p2. This whole
sentence is then grounded in a particular video
by mapping these participants to particular tracks
and instantiating the associated HMMs over those
tracks, by computing the feature vectors for each
HMM from the tracks chosen to fill its arguments.

Our algorithm makes six assumptions. First,
we assume that we know the part of speech Cm
associated with each lexical entry m, along with
the part-of-speech dependent number of states Ic
in the HMMs used to represent word meanings
in that part of speech, the part-of-speech depen-
dent number of features Nc in the feature vec-
tors used by HMMs to represent word meanings in
that part of speech, and the part-of-speech depen-
dent feature-vector computation Φc used to com-
pute the features used by HMMs to represent word
meanings in that part of speech. Second, we pair
individual sentences each with a short video clip
that depicts that sentence. The algorithm is not
able to determine the alignment between multi-
ple sentences and longer video segments. Note
that there is no requirement that the video depict
only that sentence. Other objects may be present
and other events may occur. In fact, nothing pre-
cludes a training corpus with multiple copies of
the same video, each paired with a different sen-
tence describing a different aspect of that video.
Moreover, our algorithm potentially can handle
a small amount of noise, where a video clip is
paired with an incorrect sentence that the video
does not depict. Third, we assume that we already
have (pre-trained) low-level object detectors capa-
ble of detecting instances of our target event par-
ticipants in individual frames of the video. We al-
low such detections to be unreliable; our method
can handle a moderate amount of false positives

and false negatives. We do not need to know
the mapping from these object-detection classes
to words; our algorithm determines that. Fourth,
we assume that we know the arity of each word
in the corpus, i.e., the number of arguments that
that word takes. For example, we assume that
we know that the word person(α) takes one ar-
gument and the word approached(α, β) takes two
arguments. Fifth, we assume that we know the to-
tal number of distinct participants that collectively
fill all of the arguments for all of the words in
each training sentence. For example, for the sen-
tence The person to the left of the backpack ap-
proached the trash-can, we assume that we know
that there are three distinct objects that partic-
ipate in the event denoted. Sixth, we assume
that we know the argument-to-participant map-
ping for each training sentence. Thus, for ex-
ample, for the above sentence we would know
person(p0), to-the-left-of(p0, p1), backback(p1),
approached(p0, p2), and trash-can(p2). The lat-
ter two items can be determined by parsing the
sentence, which is what we do. One can imagine
learning the ability to automatically perform the
latter two items, and even the fourth item above,
by learning the grammar and the part of speech
of each word, such as done by Kwiatkowski et al.
(2012). We leave such for future work.

Figure 1 illustrates a single frame from a po-
tential training sample provided as input to our
learner. It consists of a video clip paired with
a sentence, where the arguments of the words in
the sentence are mapped to participants. From
a sequence of such training samples, our learner
determines the objects tracks and the mapping
from participants to those tracks, together with the
meanings of the words.

The remainder of the paper is organized as fol-
lows. Section 2 generally describes our problem
of lexical acquisition from video. Section 3 intro-
duces our work on the sentence tracker, a method
for jointly tracking the motion of multiple ob-
jects in a video that participate in a sententially-
specified event. Section 4 elaborates on the de-
tails of our problem formulation in the context of
this sentence tracker. Section 5 describes how to
generalize and extend the sentence tracker so that
it can be used to support lexical acquisition. We
demonstrate this lexical acquisition algorithm on a
small example in Section 6. Finally, we conclude
with a discussion in Section 7.

55



1

3

2

0

The person to the left of the backpack carried the trash-can towards the chair.

α
p0

α
p0

β
p1

α
p1

α
p0

β
p2

α
p2

α
p0

β
p3

α
p3

Track 3 Track 0 Track 1 Track 2

Figure 1: An illustration of our problem. Each
word in the sentence has one or more arguments
(α and possibly β), each argument of each word is
assigned to a participant (p0, . . . , p3) in the event
described by the sentence, and each participant
can be assigned to any object track in the video.
This figure shows a possible (but erroneous) in-
terpretation of the sentence where the mapping is:
p0 7→ Track 3, p1 7→ Track 0, p2 7→ Track 1,
and p3 7→ Track 2, which might (incorrectly) lead
the learner to conclude that the word person maps
to the backpack, the word backpack maps to the
chair, the word trash-can maps to the trash-can,
and the word chair maps to the person.

2 General Problem Formulation

Throughout this paper, lowercase letters are used
for variables or hidden quantities while uppercase
ones are used for constants or observed quantities.

We are given a lexicon {1, . . . ,M}, letting m
denote a lexical entry. We are given a sequence
D = (D1, . . . , DR) of video clips Dr, each
paired with a sentence Sr from a sequence S =
(S1, . . . , SR) of sentences. We refer to Dr paired
with Sr as a training sample. Each sentence Sr is
a sequence (Sr,1, . . . , Sr,Lr) of words Sr,l, each an
entry from the lexicon. A given entry may poten-
tially appear in multiple sentences and even mul-
tiple times in a given sentence. For example, the
third word in the first sentence might be the same
entry as the second word in the fourth sentence,
in which case S1,3 = S4,2. This is what allows
cross-situational learning in our algorithm.

Let us assume, for a moment, that we can
process each video clip Dr to yield a sequence
(τr,1, . . . , τr,Ur) of object tracks τr,u. Let us
also assume that Dr is paired with a sen-

tence Sr = The person approached the chair,
specified to have two participants, pr,0 and pr,1,
with the mapping person(pr,0), chair(pr,1), and
approached(pr,0, pr,1). Let us further assume, for
a moment, that we are given a mapping from
participants to object tracks, say pr,0 7→ τr,39
and pr,1 7→ τr,51. This would allow us to
instantiate the HMMs with object tracks for a
given video clip: person(τr,39), chair(τr,51), and
approached(τr,39, τr,51). Let us further assume
that we can score each such instantiated HMM and
aggregate the scores for all of the words in a sen-
tence to yield a sentence score and further aggre-
gate the scores for all of the sentences in the cor-
pus to yield a corpus score. However, we don’t
know the parameters of the HMMs. These con-
stitute the unknown meanings of the words in our
corpus which we wish to learn. The problem is
to simultaneously determine (a) those parameters
along with (b) the object tracks and (c) the map-
ping from participants to object tracks. We do this
by finding (a)–(c) that maximizes the corpus score.

3 The Sentence Tracker

Barbu et al. (2012a) presented a method that first
determines object tracks from a single video clip
and then uses these fixed tracks with HMMs to
recognize actions corresponding to verbs and con-
struct sentential descriptions with templates. Later
Barbu et al. (2012b) addressed the problem of
solving (b) and (c), for a single object track con-
strained by a single intransitive verb, without solv-
ing (a), in the context of a single video clip. Our
group has generalized this work to yield an algo-
rithm called the sentence tracker which operates
by way of a factorial HMM framework. We intro-
duce that here as the foundation of our extension.

Each video clip Dr contains Tr frames. We
run an object detector on each frame to yield a
set Dtr of detections. Since our object detector
is unreliable, we bias it to have high recall but
low precision, yielding multiple detections in each
frame. We form an object track by selecting a sin-
gle detection for that track for each frame. For a
moment, let us consider a single video clip with
length T , with detections Dt in frame t. Further,
let us assume that we seek a single object track
in that video clip. Let jt denote the index of the
detection from Dt in frame t that is selected to
form the track. The object detector scores each
detection. Let F (Dt, jt) denote that score. More-

56



over, we wish the track to be temporally coherent;
we want the objects in a track to move smoothly
over time and not jump around the field of view.
Let G(Dt−1, jt−1, Dt, jt) denote some measure
of coherence between two detections in adjacent
frames. (One possible such measure is consistency
of the displacement ofDt relative toDt−1 with the
velocity of Dt−1 computed from the image by op-
tical flow.) One can select the detections to yield a
track that maximizes both the aggregate detection
score and the aggregate temporal coherence score.

max
j1,...,jT




T∑

t=1

F (Dt, jt)

+

T∑

t=2

G(Dt−1, jt−1, Dt, jt)




(1)

This can be determined with the Viterbi (1967) al-
gorithm and is known as detection-based tracking
(Viterbi, 1971).

Recall that we model the meaning of an in-
transitive verb as an HMM over a time series
of features extracted for its participant in each
frame. Let λ denote the parameters of this HMM,
(q1, . . . , qT ) denote the sequence of states qt that
leads to an observed track, B(Dt, jt, qt, λ) de-
note the conditional log probability of observ-
ing the feature vector associated with the detec-
tion selected by jt among the detections Dt in
frame t, given that the HMM is in state qt, and
A(qt−1, qt, λ) denote the log transition probabil-
ity of the HMM. For a given track (j1, . . . , jT ),
the state sequence that yields the maximal likeli-
hood is given by:

max
q1,...,qT




T∑

t=1

B(Dt, jt, qt, λ)

+
T∑

t=2

A(qt−1, qt, λ)




(2)

which can also be found by the Viterbi algorithm.
A given video clip may depict multiple objects,

each moving along its own trajectory. There may
be both a person jumping and a ball rolling. How
are we to select one track over the other? The key
insight of the sentence tracker is to bias the selec-
tion of a track so that it matches an HMM. This is
done by combining the cost function of Eq. 1 with
the cost function of Eq. 2 to yield Eq. 3, which can
also be determined using the Viterbi algorithm.
This is done by forming the cross product of the

two lattices. This jointly selects the optimal detec-
tions to form the track, together with the optimal
state sequence, and scores that combination.

max
j1,...,jT

q1,...,qT




T∑

t=1

F (Dt, jt)
+B(Dt, jt, qt, λ)

+

T∑

t=2

G(Dt−1, jt−1, Dt, jt)
+A(qt−1, qt, λ)




(3)

While we formulated the above around a sin-
gle track and a word that contains a single partic-
ipant, it is straightforward to extend this so that it
supports multiple tracks and words of higher ar-
ity by forming a larger cross product. When doing
so, we generalize jt to denote a sequence of de-
tections from Dt, one for each of the tracks. We
further need to generalize F so that it computes
the joint score of a sequence of detections, one for
each track, G so that it computes the joint mea-
sure of coherence between a sequence of pairs of
detections in two adjacent frames, and B so that
it computes the joint conditional log probability
of observing the feature vectors associated with
the sequence of detections selected by jt. When
doing this, note that Eqs. 1 and 3 maximize over
j1, . . . , jT which denotes T sequences of detec-
tion indices, rather than T individual indices.

It is further straightforward to extend the above
to support a sequence (S1, . . . , SL) of words Sl
denoting a sentence, each of which applies to dif-
ferent subsets of the multiple tracks, again by
forming a larger cross product. When doing so, we
generalize qt to denote a sequence (qt1, . . . , q

t
L) of

states qtl , one for each word l in the sentence, and
use ql to denote the sequence (q1l , . . . , q

T
l ) and q

to denote the sequence (q1, . . . , qL). We further
need to generalize B so that it computes the joint
conditional log probability of observing the fea-
ture vectors for the detections in the tracks that are
assigned to the arguments of the HMM for each
word in the sentence and A so that it computes the
joint log transition probability for the HMMs for
all words in the sentence. This allows selection
of an optimal sequence of tracks that yields the
highest score for the sentential meaning of a se-
quence of words. Modeling the meaning of a sen-
tence through a sequence of words whose mean-
ings are modeled by HMMs, defines a factorial
HMM for that sentence, since the overall Markov
process for that sentence can be factored into inde-

57



pendent component processes (Brand et al., 1997;
Zhong and Ghosh, 2001) for the individual words.
In this view, q denotes the state sequence for the
combined factorial HMM and ql denotes the factor
of that state sequence for word l. The remainder
of this paper wraps this sentence tracker in Baum
Welch (Baum et al., 1970; Baum, 1972).

4 Detailed Problem Formulation

We adapt the sentence tracker to training a cor-
pus of R video clips, each paired with a sentence.
Thus we augment our notation, generalizing jt

to jtr and q
t
l to q

t
r,l. Below, we use jr to denote

(j1r , . . . , j
Tr
r ), j to denote (j1, . . . , jR), qr,l to de-

note (q1r,l, . . . , q
Tr
r,l ), qr to denote (qr,1, . . . , qr,Lr),

and q to denote (q1, . . . , qR).
We use discrete features, namely natural num-

bers, in our feature vectors, quantized by a binning
process. We assume the part of speech of entry m
is known as Cm. The length of the feature vector
may vary across parts of speech. LetNc denote the
length of the feature vector for part of speech c,
xr,l denote the time-series (x1r,l, . . . , x

Tr
r,l) of fea-

ture vectors xtr,l, associated with Sr,l (which re-
call is some entry m), and xr denote the sequence
(xr,1, . . . , xr,Lr). We assume that we are given
a function Φc(Dtr, j

t
r) that computes the feature

vector xtr,l for the word Sr,l whose part of speech
is CSr,l = c. Note that we allow Φ to be depen-
dent on c allowing different features to be com-
puted for different parts of speech, since we can
determinem and thus Cm from Sr,l. We choose to
haveNc and Φc depend on the part of speech c and
not on the entry m since doing so would be tanta-
mount to encoding the to-be-learned word mean-
ing in the provided feature-vector computation.

The goal of training is to find a sequence λ =
(λ1, . . . , λM ) of parameters λm that best explains
the R training samples. The parameters λm con-
stitute the meaning of the entry m in the lexicon.
Collectively, these are the initial state probabili-
ties am0,k, for 1 ≤ k ≤ ICm , the transition prob-
abilities ami,k, for 1 ≤ i, k ≤ ICm , and the out-
put probabilities bmi,n(x), for 1 ≤ i ≤ ICm and
1 ≤ n ≤ NCm , where ICm denotes the number of
states in the HMM for entry m. Like before, we
could have a distinct Im for each entry m but in-
stead have ICm depend only on the part of speech
of entry m, and assume that we know the fixed I
for each part of speech. In our case, bmi,n is a dis-
crete distribution because the features are binned.

5 The Learning Algorithm

Instantiating the above approach requires a defini-
tion for what it means to best explain the R train-
ing samples. Towards this end, we define the score
of a video clip Dr paired with sentence Sr given
the parameter set λ to characterize how well this
training sample is explained. While the cost func-
tion in Eq. 3 may qualify as a score, it is easier to
fit a likelihood calculation into the Baum-Welch
framework than a MAP estimate. Thus we replace
the max in Eq. 3 with a

∑
and redefine our scor-

ing function as follows:

L(Dr;Sr, λ) =
∑

jr

P (jr|Dr)P (xr|Sr, λ) (4)

The score in Eq. 4 can be interpreted as an ex-
pectation of the HMM likelihood over all possible
mappings from participants to all possible tracks.
By definition, P (jr|Dr) = V (Dr,jr)∑

j′r V (Dr,j
′
r)

, where

the numerator is the score of a particular track se-
quence jr while the denominator sums the scores
over all possible track sequences. The log of the
numerator V (Dr, jr) is simply Eq. 1 without the
max. The log of the denominator can be com-
puted efficiently by the forward algorithm (Baum
and Petrie, 1966). The likelihood for a factorial
HMM can be computed as:

P (xr|Sr, λ) =
∑

qr

∏

l

P (xr,l, qr,l|Sr,l, λ) (5)

i.e., summing the likelihoods for all possible state
sequences. Each summand is simply the joint like-
lihood for all the words in the sentence condi-
tioned on a state sequence qr. For HMMs we have

P (xr,l, qr,l|Sr,l, λ) =
∏

t

a
Sr,l

qt−1r,l ,q
t
r,l∏

n

b
Sr,l
qtr,l,n

(xtr,l,n)

(6)

Finally, for a training corpus of R samples, we
seek to maximize the joint score:

L(D;S, λ) =
∏

r

L(Dr;Sr, λ) (7)

A local maximum can be found by employing
the Baum-Welch algorithm (Baum et al., 1970;
Baum, 1972). By constructing an auxiliary func-
tion (Bilmes, 1997), one can derive the reestima-
tion formulas in Eq. 8, where xtr,l,n = h denotes
the selection of all possible jtr such that the nth

58



ami,k = θ
m
i

R∑

r=1

Lr∑

l=1
s.t.Sr,l=m

Tr∑

t=1

L(qt−1r,l = i, q
t
r,l = k,Dr;Sr, λ

′)

L(Dr;Sr, λ′)︸ ︷︷ ︸
ξ(r,l,i,k,t)

bmi,n(h) = ψ
m
i,n

R∑

r=1

Lr∑

l=1
s.t.Sr,l=m

Tr∑

t=1

L(qtr,l = i, x
t
r,l,n = h,Dr;Sr, λ

′)

L(Dr;Sr, λ′)︸ ︷︷ ︸
γ(r,l,n,i,h,t)

(8)

feature computed by ΦCm(D
t
r, j

t
r) is h. The coef-

ficients θmi and ψ
m
i,n are for normalization.

The reestimation formulas involve occurrence
counting. However, since we use a factorial HMM
that involves a cross-product lattice and use a scor-
ing function derived from Eq. 3 that incorporates
both tracking (Eq. 1) and word models (Eq. 2),
we need to count the frequency of transitions in
the whole cross-product lattice. As an example
of such cross-product occurrence counting, when
counting the transitions from state i to k for the
lth word from frame t − 1 to t, i.e., ξ(r, l, i, k, t),
we need to count all the possible paths through
the adjacent factorial states (jt−1r , q

t−1
r,1 , . . . , q

t−1
r,Lr

)

and (jtr, q
t
r,1, . . . , q

t
r,Lr

) such that qt−1r,l = i and
qtr,l = k. Similarly, when counting the fre-
quency of being at state i while observing h as
the nth feature in frame t for the lth word of
entry m, i.e., γ(r, l, n, i, h, t), we need to count
all the possible paths through the factorial state
(jtr, q

t
r,1, . . . , q

t
r,Lr

) such that qtr,l = i and the nth
feature computed by ΦCm(D

t
r, j

t
r) is h.

The reestimation of a single component HMM
can depend on the previous estimate for other
component HMMs. This dependence happens
because of the argument-to-participant mapping
which coindexes arguments of different compo-
nent HMMs to the same track. It is precisely
this dependence that leads to cross-situational
learning of two kinds: both inter-sentential and
intra-sentential. Acquisition of a word meaning
is driven across sentences by entries that appear
in more than one training sample and within sen-
tences by the requirement that the meanings of all
of the individual words in a sentence be consistent
with the collective sentential meaning.

6 Experiment

We filmed 61 video clips (each 3–5 seconds at
640×480 resolution and 40 fps) that depict a va-
riety of different compound events. Each clip de-
picts multiple simultaneous events between some

S → NP VP
NP → D N [PP]

D → the
N → person | backpack | trash-can | chair

PP → P NP
P → to the left of | to the right of

VP → V NP [ADV] [PPM]
V → picked up | put down | carried | approached

ADV → quickly | slowly
PPM → PM NP

PM → towards | away from
Table 1: The grammar used for our annotation and
generation. Our lexicon contains 1 determiner,
4 nouns, 2 spatial relation prepositions, 4 verbs,
2 adverbs, and 2 motion prepositions for a total of
15 lexical entries over 6 parts of speech.

subset of four objects: a person, a backpack, a
chair, and a trash-can. These clips were filmed
in three different outdoor environments which we
use for cross validation. We manually annotated
each video with several sentences that describe
what occurs in that video. The sentences were
constrained to conform to the grammar in Table 1.
Our corpus of 159 training samples pairs some
videos with more than one sentence and some sen-
tences with more than one video, with an average
of 2.6 sentences per video 1.

We model and learn the semantics of all words
except determiners. Table 2 specifies the arity, the
state number Ic, and the features computed by Φc
for the semantic models for words of each part of
speech c. While we specify a different subset of
features for each part of speech, we presume that,
in principle, with enough training data, we could
include all features in all parts of speech and auto-
matically learn which ones are noninformative and
lead to uniform distributions.

We use an off-the-shelf object detector (Felzen-
szwalb et al., 2010a; Felzenszwalb et al., 2010b)
which outputs detections in the form of scored
axis-aligned rectangles. We trained four object de-
tectors, one for each of the four object classes in

1Our code, videos, and sentential annotations are
available at http://haonanyu.com/research/
acl2013/.

59



c arity Ic Φc
N 1 1 α detector index

V 2 3

α VEL MAG
α VEL ORIENT
β VEL MAG
β VEL ORIENT
α-β DIST
α-β size ratio

P 2 1 α-β x-position
ADV 1 3 α VEL MAG

PM 2 3 α VEL MAG
α-β DIST

Table 2: Arguments and model configurations for
different parts of speech c. VEL stands for veloc-
ity, MAG for magnitude, ORIENT for orientation,
and DIST for distance.

our corpus: person, backpack, chair, and trash-
can. For each frame, we pick the two highest-
scoring detections produced by each object detec-
tor and pool the results yielding eight detections
per frame. Having a larger pool of detections per
frame can better compensate for false negatives in
the object detection and potentially yield smoother
tracks but it increases the size of the lattice and the
concomitant running time and does not lead to ap-
preciably better performance on our corpus.

We compute continuous features, such as veloc-
ity, distance, size ratio, and x-position solely from
the detection rectangles and quantize the features
into bins as follows:
velocity To reduce noise, we compute the veloc-

ity of a participant by averaging the optical flow
in the detection rectangle. The velocity magni-
tude is quantized into 5 levels: absolutely station-
ary, stationary, moving, fast moving, and quickly.
The velocity orientation is quantized into 4 direc-
tions: left, up, right, and down.

distance We compute the Euclidean distance be-
tween the detection centers of two participants,
which is quantized into 3 levels: near, normal,
and far away.

size ratio We compute the ratio of detection area
of the first participant to the detection area of the
second participant, quantized into 2 possibilities:
larger/smaller than.

x-position We compute the difference between
the x-coordinates of the participants, quantized
into 2 possibilities: to the left/right of.

The binning process was determined by a prepro-
cessing step that clustered a subset of the training
data. We also incorporate the index of the detector
that produced the detection as a feature. The par-

ticular features computed for each part of speech
are given in Table 2.

Note that while we use English phrases, like to
the left of, to refer to particular bins of particular
features, and we have object detectors which we
train on samples of a particular object class such
as backpack, such phrases are only mnemonic of
the clustering and object-detector training process.
We do not have a fixed correspondence between
the lexical entries and any particular feature value.
Moreover, that correspondence need not be one-
to-one: a given lexical entry may correspond to a
(time variant) constellation of feature values and
any given feature value may participate in the
meaning of multiple lexical entries.

We perform a three-fold cross validation, taking
the test data for each fold to be the videos filmed in
a given outdoor environment and the training data
for that fold to be all training samples that contain
other videos. For testing, we hand selected 24 sen-
tences generated by the grammar in Table 1, where
each sentence is true for at least one test video.
Half of these sentences (designated NV) contain
only nouns and verbs while the other half (des-
ignated ALL) contain other parts of speech. The
latter are longer and more complicated than the
former. We score each testing video paired with
every sentence in both NV and ALL. To evaluate
our results, we manually annotated the correctness
of each such pair.

Video-sentence pairs could be scored with
Eq. 4. However, the score depends on the sentence
length, the collective numbers of states and fea-
tures in the HMMs for words in that sentence, and
the length of the video clip. To render the scores
comparable across such variation we incorporate a
sentence prior to the per-frame score:

L̂(Dr, Sr;λ) = [L(Dr;Sr, λ)]
1
Tr π(Sr) (9)

where
π(Sr) =

exp

Lr∑

l=1




E(ICSr,l )

+

NCSr,l∑

n=1

E(ZCSr,l ,n)




(10)

In the above, ZCSr,l ,n is the number of bins for
the nth feature of Sr,l of part of speech CSr,l and
E(Y ) = −∑Yy=1 1Y log 1Y = log Y is the entropy
of a uniform distribution over Y bins. This prior
prefers longer sentences which describe more in-
formation in the video.

60



CHANCE BLIND OUR HAND
NV 0.155 0.265 0.545 0.748
ALL 0.099 0.198 0.639 0.786

Table 3: F1 scores of different methods.

Figure 2: ROC curves of trained models and hand-
written models.

The scores are thresholded to decide hits, which
together with the manual annotations, can gener-
ate TP, TN, FP, and FN counts. We select the
threshold that leads to the maximal F1 score on
the training set, use this threshold to compute F1
scores on the test set in each fold, and average F1
scores across the folds.

The F1 scores are listed in the column labeled
Our in Table 3. For comparison, we also report
F1 scores for three baselines: Chance, Blind, and
Hand. The Chance baseline randomly classifies
a video-sentence pair as a hit with probability 0.5.
The Blind baseline determines hits by potentially
looking at the sentence but never looking at the
video. We can find an upper bound on the F1 score
that any blind method could have on each of our
test sets by solving a 0-1 fractional programming
problem (Dinkelbach, 1967) (see Appendix A for
details). The Hand baseline determines hits with
hand-coded HMMs, carefully designed to yield
what we believe is near-optimal performance. As
can be seen from Table 3, our trained models
perform substantially better than the Chance and
Blind baselines and approach the performance of
the ideal Hand baseline. One can further see from
the ROC curves in Figure 2, comparing the trained
and hand-written models on both NV and ALL, that
the trained models are close to optimal. Note that
performance on ALL exceeds that on NV with the
trained models. This is because longer sentences
with varied parts of speech incorporate more in-
formation into the scoring process.

7 Conclusion
We presented a method that learns word mean-
ings from video paired with sentences. Unlike
prior work, our method deals with realistic video
scenes labeled with whole sentences, not indi-
vidual words labeling hand delineated objects or
events. The experiment shows that it can cor-
rectly learn the meaning representations in terms
of HMM parameters for our lexical entries, from
highly ambiguous training data. Our maximum-
likelihood method makes use of only positive sen-
tential labels. As such, it might require more train-
ing data for convergence than a method that also
makes use of negative training sentences that are
not true of a given video. Such can be handled
with discriminative training, a topic we plan to ad-
dress in the future. We believe that this will allow
learning larger lexicons from more complex video
without excessive amounts of training data.

Acknowledgments
This research was sponsored by the Army Re-
search Laboratory and was accomplished under
Cooperative Agreement Number W911NF-10-2-
0060. The views and conclusions contained in this
document are those of the authors and should not
be interpreted as representing the official policies,
either express or implied, of the Army Research
Laboratory or the U.S. Government. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Government purposes, notwith-
standing any copyright notation herein.

A An Upper Bound on the F1 Score of
any Blind Method

A Blind algorithm makes identical decisions on
the same sentence paired with different video
clips. An optimal algorithm will try to find a de-
cision si for each test sentence i that maximizes
the F1 score. Suppose, the ground-truth yields FPi
false positives and TPi true positives on the test
set when si = 1. Also suppose that setting si = 0
yields FNi false negatives. Then the F1 score is

F1 =
1

1 +

∑
i siFPi + (1− si)FNi∑

i 2siTPi︸ ︷︷ ︸
∆

Thus we want to minimize the term ∆. This is an
instance of a 0-1 fractional programming problem
which can be solved by binary search or Dinkel-
bach’s algorithm (Dinkelbach, 1967).

61



References
A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dick-

inson, S. Fidler, A. Michaux, S. Mussman, N. Sid-
dharth, D. Salvi, L. Schmidt, J. Shangguan, J. M.
Siskind, J. Waggoner, S. Wang, J. Wei, Y. Yin, and
Z. Zhang. 2012a. Video in sentences out. In Pro-
ceedings of the Twenty-Eighth Conference on Un-
certainty in Artificial Intelligence, pages 102–112.

A. Barbu, N. Siddharth, A. Michaux, and J. M. Siskind.
2012b. Simultaneous object detection, tracking, and
event recognition. Advances in Cognitive Systems,
2:203–220, December.

L. E. Baum and T. Petrie. 1966. Statistical inference
for probabilistic functions of finite state Markov
chains. The Annals of Mathematical Statistics,
37:1554–1563.

L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occuring in the statistical
analysis of probabilistic functions of Markov chains.
The Annals of Mathematical Statistics, 41(1):164–
171.

L. E. Baum. 1972. An inequality and associated maxi-
mization technique in statistical estimation of proba-
bilistic functions of a Markov process. Inequalities,
3:1–8.

J. Bilmes. 1997. A gentle tutorial of the EM algorithm
and its application to parameter estimation for Gaus-
sian mixture and hidden Markov models. Technical
Report TR-97-021, ICSI.

M. Brand, N. Oliver, and A. Pentland. 1997. Coupled
hidden Markov models for complex action recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
994–999.

D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In Proceedings of the 25th International Conference
on Machine Learning.

W. Dinkelbach. 1967. On nonlinear fractional pro-
gramming. Management Science, 13(7):492–498.

P. F. Dominey and J.-D. Boucher. 2005. Learning to
talk about events from narrated video in a construc-
tion grammar framework. Artificial Intelligence,
167(12):31–61.

P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and
D. Ramanan. 2010a. Object detection with discrim-
inatively trained part-based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(9):1627–1645.

P. F. Felzenszwalb, R. B. Girshick, and D. A.
McAllester. 2010b. Cascade object detection with
deformable part models. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 2241–2248.

G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg. 2011. Baby talk: Understand-
ing and generating simple image descriptions. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1601–1608.

P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. 2012. Collective generation of natural im-
age descriptions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, pages 359–368.

T. Kwiatkowski, S. Goldwater, L. Zettlemoyer, and
M. Steedman. 2012. A probabilistic model of syn-
tactic and semantic acquisition from child-directed
utterances and their meanings. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 234–
244.

V. Ordonez, G. Kulkarni, and T. L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In Proceedings of Neural In-
formation Processing Systems.

D. Roy. 2002. Learning visually-grounded words
and syntax for a scene description task. Computer
Speech and Language, 16:353–385.

S. Sadanand and J. J. Corso. 2012. Action bank: A
high-level representation of activity in video. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1234–1241.

J. M. Siskind and Q. Morris. 1996. A maximum-
likelihood approach to visual event classification. In
Proceedings of the Fourth European Conference on
Computer Vision, pages 347–360.

J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning
mappings. Cognition, 61:39–91.

T. Starner, J. Weaver, and A. Pentland. 1998. Real-
time American Sign Language recognition using
desk and wearable computer based video. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 20(12):1371–1375.

A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymtotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory,
13:260–267.

A. Viterbi. 1971. Convolutional codes and their per-
formance in communication systems. IEEE Trans-
actions on Communication Technology, 19(5):751–
772.

J. Yamoto, J. Ohya, and K. Ishii. 1992. Recogniz-
ing human action in time-sequential images using
hidden Markov model. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 379–385.

62



B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu.
2010. I2T: Image parsing to text description. Pro-
ceedings of the IEEE, 98(8):1485–1508, August.

C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. In Pro-
ceedings of the 19th National Conference on Artifi-
cal intelligence, pages 488–493.

S. Zhong and J. Ghosh. 2001. A new formulation of
coupled hidden Markov models. Technical report,
Department of Electrical and Computer Engineer-
ing, The University of Texas at Austin.

63


