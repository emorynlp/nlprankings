










































Discriminating Non-Native English with 350 Words


Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 101–110,
Atlanta, Georgia, June 13 2013. c©2013 Association for Computational Linguistics

Approved for Public Release; Distribution Unlimited. 13-1876

Discriminating Non-Native English with 350 Words

John Henderson, Guido Zarrella, Craig Pfeifer and John D. Burger
The MITRE Corporation

202 Burlington Road
Bedford, MA 01730-1420, USA

{jhndrsn,jzarrella,cpfeifer,john}@mitre.org

Abstract

This paper describes MITRE’s participation in
the native language identification (NLI) task
at BEA-8. Our best effort performed at an ac-
curacy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the best per-
forming systems. We describe the variety
of machine learning approaches that we ex-
plored, including Winnow, language model-
ing, logistic regression and maximum-entropy
models. Our primary features were word and
character n-grams. We also describe several
ensemble methods that we employed for com-
bining these base systems.

1 Introduction

Investigations into the effect of authors’ latent at-
tributes on language use have a long history in lin-
guistics (Labov, 1972; Biber and Finegan, 1993).
The rapid growth of social media has sparked in-
creased interest in automatically identifying author
attributes such as gender and age (Schler et al., 2006;
Burger and Henderson, 2006; Argamon et al., 2007;
Mukherjee and Liu, 2010; Rao et al., 2010). There
is also a long history of computational aids for lan-
guage pedagogy, both for first- and second-language
acquisition. In particular, automated native language
identification (NLI) is a useful aid to second lan-
guage learning. This is our first foray into NLI,
although we have recently described experiments
aimed at identifying the gender of unknown Twit-
ter authors (Burger et al., 2011). We performed well
using only character and word n-grams as evidence.
In the present work, we apply that same approach

to NLI, and combine it with several other baseline
classifiers.

In the remainder of this paper, we describe our
high-performing system for identifying the native
language of English writers. We explore a varied
set of learning algorithms and present two ensem-
ble methods used to produce a better system than
any of the individuals. In Section 2 we describe the
data and task in detail as well as the evaluation met-
ric. In Section 3 we discuss details of the particular
system configuration that scored best for us. We de-
scribe our experiments in Section 4, including our
exploration of several different classifier types and
parametrizations. In Section 5 we present and an-
alyze performance results, and inspect some of the
features that were useful in discrimination. Finally
in Section 6 we summarize our findings, and de-
scribe possible extensions to the work.

2 Task, data and evaluation

Native Language Identification was a shared task or-
ganized as part of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, 2013. The task was to identify an author’s
native language based on an English essay.

The data provided consisted of a set of 12,100
Test of English as a Foreign Language (TOEFL) ex-
aminations contributed by the Educational Testing
Service (Blanchard et al., to appear). These were
English essays written by native speakers of Arabic,
Chinese, French, German, Hindi, Italian, Japanese,
Korean, Spanish, Telugu, and Turkish. A set of 1000
essays for each language was identified as training
data, along with 100 per language for development,

101



and another 100 per language for a final test set. The
mean length of an essay is 348 words.

The primary evaluation metric for shared task
submissions was simple accuracy: the fraction of the
test essays for which the correct native language was
identified. A baseline accuracy would thus be about
9% (one out of eleven). Results were also reported
in terms of F-measure on a per-language basis. F-
measure is a harmonic mean of precision and recall:
F = 2PRP+R . For the evaluation, the precision de-
nominator was the number of items labeled with a
particular language by the system and the recall de-
nominator was the number of items marked with a
particular language in the reference set.

The training, development, and test sets all had
balanced distributions across the native languages,
so error rates and accuracy did not favor any partic-
ular language in any set.

3 System overview

The systems we used to generate results for the NLI
competition were all machine-learning-based, with
no handwritten rules or features. The final submitted
systems were ensembles built from the outputs and
confidence scores of independent eleven-way multi-
nomial classifiers.

3.1 Features

The features used to build these systems were
language-independent and were generated using the
same infrastructure designed for the experiments de-
scribed in Burger et al. (2011).

We incorporated a variety of binary features into
our systems, each of which was hashed into a 64-bit
numeric representation using MurmurHash3 (Ap-
pleby, 2011). The bulk of our features were case-
sensitive word- and character-based n-grams, in
which a feature was turned “on” if its sequence of
words or characters appeared at least once in the text
of an essay. We also added binary features describ-
ing surface characteristics of the text such as average
word length and word count. Features were sepa-
rated into tracks such that the word unigram “i” and
the character unigram “i” would each generate a dis-
tinct feature.

Part of speech tag n-grams were added to the
feature set after reviewing performance results in

Brooke and Hirst (2012). We used the Stan-
ford log-linear part of speech tagger described in
Toutanova et al. (2003), with the english-left3words-
distsim.tagger pretrained model and the Penn Tree-
bank tagset. The tagger was run on each essay and
outputs were incorporated as sequence features with
n-grams up to length 5.

3.2 Classifiers

Carnie1 is a MITRE-developed linear classifier
that implements the Winnow2 algorithm of Carvalho
and Cohen (2006), generalized for multinomial clas-
sification. Carnie was developed to perform clas-
sification of short, noisy texts with many training
examples. It maintains one weight per feature per
output class, and performs multiplicative updates
that reinforce weights corresponding to the correct
class while penalizing weights associated with the
top-scoring incorrect class. The learner is mistake-
driven and performs an update of size � after an error
or when the ratio of weight masses of the correct and
top incorrect classes is below 1 + δ. It iterates over
the training data, cooling its updates after each itera-
tion. For the purposes of these experiments, an input
to Carnie was the text of a single TOEFL essay, and
the output was the highest scoring class and several
related scores.

SRI’s Language Modeling Toolkit (SRILM) is
a toolkit for sequence modeling that continues to
be relevant after more than a decade of develop-
ment (Stolcke, 2002). It can be used to both build
models of sequence likelihoods and to evaluate like-
lihoods of previously unseen sequences. Building
a multinomial text classifier with a language model
toolkit involves building one model for each target
class and choosing the label whose model gives the
highest probability.

Many smoothing methods are implemented by
SRILM, along with a variety of n-gram filter-
ing techniques. The out-of-the-box default con-
figuration produces trigram models with Good-
Turing smoothing. It worked well for this com-
petition. Using open vocabulary models (-unk),
turning off sentence boundary insertion (-no-sos
-no-eos) and treating each essay as one sentence

1It is named for entertainers who guess personal character-
istics of carnival goers.

102



worked best in our development environment.

LIBLINEAR is a popular open source library for
classification of large, sparse data. We experimented
with several of their standard Support Vector Ma-
chine and logistic regression configurations (Fan et
al., 2008). We selected multiclass `2-regularized
logistic regression with the dual-form solver and
default parameters. Inputs to the model were bi-
nary features generated from a single TOEFL essay.
Features for this model were generated by Carnie.
The model provided probability estimates for each
candidate output class (L1) for each essay, which
were then combined with the outputs of Carnie and
SRILM in an ensemble to produce a single predic-
tion.

3.3 Ensembles
The classifiers described above were selected for in-
clusion as components in a larger ensemble on the
basis of their performance and the observation that
errors committed by these systems were not highly
correlated. We used the entirety of our training data
for construction of each component system, leaving
scant data available for estimating parameters of en-
sembles. This scenario led us to choose naive Bayes
to combine the outputs of the original components.

Given h1, . . . , hk hypothesis labels from k differ-
ent systems, one approximates the conditional like-
lihood of the reference label P (R|H1 . . . Hk) using
the Bayes transform and the development set esti-
mates of P (Hi|R). One investigates all possible la-
bels to decode r∗ = argmaxr P (r)

∏
i P (hi|r). The

class balance in every set we operated on made the
prior P (r) irrelevant for maximization and simpli-
fied many of the denominators along the way. This
is a typical formulation of naive Bayes.

Confidence All of our component systems pro-
duce scores as well as a predicted label. Carnie pro-
duces (non-probability) scores for all of the candi-
date labels, SRILM produces log-probabilities and
perplexities, and LIBLINEAR produces P (h|r), the
likelihood of each of the possible labels. We ex-
perimented with several transformations of those
scores to best use them to predict correctness of
their hypothesis. There were several graphical mod-
els we could use for folding these scores into the
Bayes ensemble, and we chose a simple, discretized

P (H,S|R). We evenly partitioned and relabeled our
system outputs according to their scores (S), and
used those partition labels in the Bayes ensemble.
Thus when a particular reference label was scored
in the ensemble during decoding, both its prediction
and score contributed to the label in the naive Bayes
table lookup.

3.4 Best configuration

We submitted five systems with a variety of con-
figurations. One of our systems was our individual
Carnie system on its own for calibration. The other
four were ensembles.

The best system we submitted was a Bayes en-
semble of the Carnie, SRILM, and LIBLINEAR
components each trained on the train+development
sets. Carnie was trained for twelve iterations with
� = 0.03, δ = 0.05, and a cooling rate of 0.1.
SRILM models were trained for open vocabulary
and the default trigram, Good-Turing setting. Lo-
gistic regression from LIBLINEAR was run with `2
regularization and using the dual form solver.

Parameters for the Bayes model were collected
from the development set when the components
were trained only on the training set. A grid search
was performed over likely candidates for λ, the
Dirichlet parameter, and ρ, the number of score-
based partitions, resulting in λ = 0.03125 and ρ =
2. The grid search was performed with the compo-
nent models trained only on the training set and us-
ing 10-fold cross validation on the development set.

4 Experiments

In all experiments described below, systems were
trained initially on the 9900 training examples alone,
with the 1100 item development set held back to al-
low for hyperparameter estimation. When prepar-
ing our final test set submissions, the development
set was folded into the training data, and all models
were re-trained on this new dataset containing 11000
items.

4.1 Baselines

How hard is the NLI task? Simple baselines of-
ten give us a quick glimpse into what matters in a
NLP task. In Figure 1, we give accuracy results
on ten different baselines we trained on the training

103



Baseline Accuracy(%)
random 9.1
char length 9.6
SRILM(letter unigram) 10.8
word length 12.0
proficiency 14.9
SRILM(letter bigram) 15.1
JS(vowels) 20.6
JS(consonants) 33.8
JS(vowels+consonants) 34.1
JS(bag-of-words) 52.5

Figure 1: Simple baseline development set scores.

set and evaluated on the development set. Predic-
tions based on simple character and word lengths
show only slight gains over random. Using the
high/medium/low proficiency score that accompa-
nied the data similarly gives a tiny amount of infor-
mation over baseline (14.9%). We ignored those rat-
ings elsewhere in our work, to focus on the core task
of prediction based on essay content.

We collected some simple distributions of vowel
and consonant clusters and used them for predic-
tion, scoring with Jensen-Shannon divergence. JS
divergence is a symmetrized form of KL divergence
to alleviate the mathematical problem involved with
missing observations. It has behaved well in the
context of language processing applications (Lee,
1999). The score progression from consonant clus-
ters, to vowel clusters, to words suggests that there
is NLI information scattered at various levels of sur-
face features.

4.2 Varied Carnie configurations
Carnie’s out-of-the-box configuration is one that has
been optimized for application to micro-blogs and
other ungrammatical short texts. While our hypoth-
esis was that this configuration would be well suited
to analysis of English TOEFL essays, we investi-
gated a number of possible techniques to help Carnie
adapt to the new domain.

We began by performing a grid search to select
model hyperparameters that enabled our standard
configuration to generalize well from the training
dataset to the development dataset. These values of
�, δ, and cooling rate were then applied to various
new feature configurations.

The standard configuration included binary fea-
tures for word unigrams and bigrams, character n-
grams of sizes 1 to 5, and surface features. We
experimented here with word trigrams, character 6-
grams, and lowercased character n-grams of sizes 1
to 6. We also added skip bigrams, which were or-
dered word pairs in which 1 to 6 intervening words
were omitted. We incorporated part of speech tags in
a number of ways, including POS n-grams of lengths
1 to 5, POS k-skip bigrams with k ranging from 1 to
6, and POS n-grams in which closed-class POS tags
were replaced with the actual content word used.
We also measured the impact of using frequency-
weighted features.

Our standard approach with Carnie is to perform
multinomial classification using one model trained
on all the data simultaneously. We experimented
with other ways of framing the NLI problem, such
as building eleven binary classifiers, each of which
was trained on all of the data but with the sole task
of accepting or rejecting a single candidate L1. We
also partitioned the training data to build 55 binary
classifiers for all possible pairs of L1s. These bi-
nary classifiers were then combined via a voting
mechanism to select a single winner. This allowed
us to apply focused efforts to improve discrimina-
tion in language pairs which Carnie found challeng-
ing, such as Hindi-Telugu or Japanese-Korean. To
this end, we collected a substantial amount of ad-
ditional out-of-domain training data from the web-
sites lang8.com (70,000 entries) and gohackers.com
(40,000 entries). Although we did not use this
data in our final submission, we performed experi-
ments to measure the value of this new data in the
TOEFL11 domain with no adaptation, with feature
filtering to limit training features to items observed
in the test sets, and with “frustratingly easy” do-
main adaptation, EasyAdapt, described in Daumé
and Marcu (2007).

4.3 Varied SRILM configurations

SRILM offers a number of parameters for ex-
perimentation. We hill-climbed on the train-
ing/development split to select a good configura-
tion. We experimented with n-gram lengths from
1-5 (bag of words through word 5-grams), using the
tokenization given by the NLI organizers. We tried
the lighter weight smoothing techniques offered by

104



System Confidence MRD
Carnie s(h1)/s(h2) 343

s(h1)/
∑

i s(hi) 268
s(h1)− s(h2) 72

SRILM log p(h1)/ log p(h2) 315.7
log p(h1)− log p(h2) 315.3
ppl1(h1)/ppl1(h2) 315.12
ppl1(h1)− ppl1(h2) 260
ppl1 77
log p(h1) 40

MaxEnt
∑

i p(hi) log p(hi) 385.7
(JCarafe) p(h1) 383.15

log p(h1) 383.15
p(h1)/p(h2) 373.75
log p(h1)/ log p(h2) 379.8

LIBLINEAR
∑

i p(hi) log p(hi) 379.8

Figure 2: Confidence candidates measured in Mean Rank
Difference between correct and incorrect labels.

SRILM including Good-Turing, Witten-Bell, Ris-
tad’s natural discounting, both modified and original
Kneser-Ney. We built both closed vocabulary and
open vocabulary language models and with special
symbols added for sentence boundaries.

4.4 Component confidence experiments

Our components generate scores, but those scores
were not always scaled in the same way. Winnow
(in Carnie) is a margin-based, mistake-driven learner
generating scores which are interpretable only as
sums of weights. SRILM produces log p(dj |hi),
but renormalizing those (with priors) into estimates
of p(hi|dj) is unreliable because the different sub-
models are not connected with smoothing. Logistic
regression produces a distribution for p(hi|dj). We
aimed to express these notions of confidence in a
way that was common to all systems. We did this by
relabeling system hypotheses after sorting by confi-
dence, but not all metrics were equally good at this
sorting.

We performed an ad hoc assessment of several
candidate scoring functions. Our goal was to find
functions that best separated correct answers from
incorrect answers in a sorted ranking. We ran several
candidates on our development set and measured the
difference between the mean rank of correct answers
and the mean rank of incorrect answers. Figure 2

displays the results. In each case h1 was the best hy-
pothesis generated by the system and h2 is second
best. p(·) indicates probabilities, s(·) indicates non-
probability scores. We chose those functions with
the highest values.

4.5 Simple models for combination

In this work, we focused our ensembles only on the
output of our individual components, ignoring the
features from the original data that they attempt to
model. The base systems are all trained to minimize
errors, and did not appear to have any particular
preferential capabilities. Thus we rely on them en-
tirely for the primary processing and focus on their
outputs.

In our naive Bayes formulation, the random vari-
ables produced by the component systems (H) need
not take on values directly comparable with the ref-
erence labels to be predicted (R). We experimented
with folding in several one-shot systems that pro-
duced labels in {L, L̄}, for particular native lan-
guage groups, but none of these proved to be good
complements for the components described above.

To cope with decode-time configurations of H
that hadn’t been seen during estimation, we used
a Dirichlet prior on R in this ensemble. A sin-
gle parameter, λ, was introduced. Thus our esti-
mates for P (hi|r) were based on smoothed counts:
c(hi,r)+λ
c(r)+λ|R| . The search for λ was performed using
cross-validation on the development set.

Assignment In many prediction settings, we know
that our evaluation data consists of examples drawn
from a particular allocation of candidate classes.
One can take advantage of this in a probabilistic
setting by doing a global search for the maximum
likelihood assignment of the test documents to the
L1 languages under the constraint that each L1 lan-
guage must have a particular occupancy by the doc-
uments – in this case, an even split. More generally,
once we have p(hi|dj) for each candidate language
hi and document dj , we can find an assignmentA =
{(i, j) : αi,j = 1} that maximizes the likelihood
P (H|D) =

∏
(i,j)∈A p(hi|dj) =

∏
i,j p(hi|dj)αi,j

under the constraints that
∑

i αi,j = |D|/|H| and∑
j αi,j = 1. The first constraint says that each lan-

guage should get an even allocation of documents
assigned to it and the second constraint says that

105



each document should be assigned to only one lan-
guage. This reduces to a maximum weight match-
ing on

∑
i,j αi,j log p(hi|dj). This problem is di-

rectly convertible into a max flow problem or a lin-
ear program. It can be solved with methods such
as the Hungarian algorithm, Ford-Fulkerson, or lin-
ear programming. In our case, we used LPSOLVE2

to find this global maximum. This looks at first
glance like an integer programming problem, but
one can relax the constraints into inequalities and
still be guaranteed that the solution will end up with
all αi,j landing on either zero or one in the right
amounts. We applied this assignment combination
as a post-processing step to the probabilities gener-
ated in the naive Bayes ensemble and also to the raw
LIBLINEAR outputs. The hope in doing this is that
the optimizer will move the less likely assignments
around appropriately while preserving the assign-
ments where it has more confidence. We observed
mixed results on our development set and submitted
two systems using this ensemble technique.

4.6 Other components explored

LIBLINEAR provides an implementation of a linear
SVM as well as a logistic regression package. We
experimented with various combinations of `1- and
`2 -loss SVMs, with both `1 and `2-regularization,
but in the end opted to use the `2-regularized logistic
regression due to slightly superior performance and
the ease with which we could extract eleven values
of P (H) for inclusion in our ensemble.

Another component that was tested in develop-
ment of our ensemble systems was a maximum en-
tropy classifier. This particular effort used the imple-
mentation from JCarafe,3 which uses L-BFGS for
optimization.

We approached the NLI task as document classi-
fication, following a typical JCarafe recipe (Gibson
et al., 2007). The class of the document is the native
language of the author. Each document was treated
as a bag of words, and several classes of features
were extracted: token n-gram frequency, character
n-gram frequency, part of speech n-gram frequency.
The feature mix that produced the best score was
token bigrams and trigrams, character trigrams and

2http://lpsolve.sourceforge.net
3https://github.com/wellner/jcarafe

L1 Mean F Our Best F
GER 1 0.776 1 0.921
ITA 2 0.757 2 0.88
CHI 3 0.723 4 0.85
JPN 4 0.708 5 0.837
FRE 5 0.701 7 0.818
TEL 6 0.667 3 0.802
KOR 7 0.665 6 0.827
TUR 8 0.656 8 0.81
ARA 9 0.65 3 0.872
SPA 10 0.631 10 0.768
HIN 11 0.606 11 0.762

Figure 3: L1s by empirical prediction difficulty. Mean F
incorporates all submissions by all competition teams.

POS trigrams. A feature frequency threshold of 5
was used to curb the number of features.

5 Results

Our best performing ensemble was 82.6% accurate
when scored on the competition test set, and was
composed of Carnie, SRILM, and logistic regres-
sion, using naive Bayes to combine the subsystem
outputs and confidence scores into a single predic-
tion. The best performing subsystem during system
development scored 79.3% on the test set in isola-
tion, demonstrating once again the value of combin-
ing systems that make independent errors.

Certain L1s gave our systems more difficulty than
others. Our best submitted F-measure scores ranged
from 0.921 for German to 0.762 for Hindi. Fig-
ure 3 demonstrates that our systems’ scores were
highly correlated with average scores from all sub-
missions by all teams (R2 = 0.84). From this we
infer that our performance differences between L1s
may be explained by inherent difficulties in certain
languages or by the selection of similar L1s as a part
of the competition task, rather than quirks of our ap-
proach. Our submissions do appear to have a partic-
ular advantage on Arabic and Korean, relative to the
field.

Figure 4 shows the overall performance of our
submissions and subsystems on the development
and test evaluation sets.

Our scores dropped 4 to 5% between development
and test evaluations, representing significant overfit-

106



Configuration dev % test %
Components
base Carnie 82.6
+ trigrams 83.1
+ POS tags 83.6 79.3
1v1 voted Carnie 79.4
SRILM 77.1
MaxEnt 77.7
Linear SVM 81.9
Logistic Regression 83.4
assignment(LR) 82.4
Ensembles
bayes(Carnie,SRILM,LR) 87.3 82.6
assign(Carnie,SRILM,LR) 86.5 82.0
assign(Carnie,SRILM,MaxEnt) 86.4 82.3
bayes(Carnie,SRILM) 86.9 81.7

Figure 4: Results.

ting to the development set. The development set
was used for model selection, ensemble parameteri-
zation, and eventually as additional training data for
final submissions. Later tests showed that this fi-
nal retraining actually reduced the Carnie score by
0.9%.

Figure 4 also shows the effect of various efforts to
improve our baseline Carnie system. Adding part-
of-speech n-grams and word trigrams as features
improved the score on the development set by 1%
in total. Meanwhile many of our experiments with
new types of features yielded no gains. Lowercased
character n-grams, skip bigrams and all non-vanilla
formulations of part-of-speech tags provided no im-
provement and were discarded.

It was observed that all of our systems showed
a strong preference for binary features over
frequency-weighted inputs. In the case of the
JCarafe classifier, switching to binary features
yielded a 10% accuracy gain. Although JCarafe
didn’t provide a gain over the ensemble of Carnie,
SRILM, and LIBLINEAR logistic regression, de-
velopment set results indicated that JCarafe served
capably as a replacement for LIBLINEAR in some
ensembles.

We also measured the impact of using out-of-
domain Japanese and Korean L1 data to train a pair-
wise JPN/KOR system. Only 78.5% of JPN and
KOR texts were correctly identified in our eleven-

Rank L1 Score Feature
14 GER 21.05 (for,example)
40 GER 15.95 (have,to)
55 HIN 14.80 (as,compared,to)
57 ITA 14.60 (I,think,that)
58 TEL 14.18 (and,also)
60 HIN 13.97 (as,compared)
79 TEL 12.82 (the,people)
96 TEL 12.14 (for,a)

101 ITA 11.83 (that,in)
116 ITA 10.94 (think,that)
119 GER 10.93 (has,to)
120 TEL 10.89 (with,the,statement)

Figure 5: Word n-gram features predicting particular L1.

way baseline system. We restricted train and evalu-
ation data to only those two L1s and found our base-
line technique was 86.5% accurate. When we added
our out-of-domain data with no domain adaptation
technique, that score dropped to 82.0%. Removing
features that didn’t appear in our test set only raised
the score to 82.5%. However, the EasyAdapt tech-
nique (Daumé and Marcu, 2007) showed promise.
By making an additional source-specific copy of
each feature, we were able to raise the score to
88.5%. While this result was of limited applicabil-
ity in our final submission, and was therefore not
submitted to the open data competition task, we be-
lieve that this technique may prove useful in en-
abling cross-domain NLI system transfer.

Figure 5 provides a small sample of word-level
features discovered by the Winnow classifier. The
table shows the rank of each n-gram relative to all
features, and the native language that the feature
predicts. The weight assigned by the Winnow2 al-
gorithm is not readily interpretable, although higher
weights indicate a stronger association.

Similarly, the top character n-grams can be seen in
Figure 7, along with manually selected examples of
each. These features can be seen to mainly fall into
several broad categories. There are mentions of the
authors’ home countries as in Korean, Italian and
Turkey. There are also characteristic misspellings
and infelicities such as personnaly, perhaps incor-
rectly modeled from the French personnellement.

It is worth noting that the weights (and thus the
ranks) for the top character n-gram features are

107



System Accuracy (%) Errors
Carnie 80.4 2153
SRILM 74.5 2800
LIBLINEAR 80.8 2116
ensemble-assign 81.9 1990
ensemble-Bayes 82.2 1961

Figure 6: Training set cross-validation results.

higher than for the top word features, indicating that
Winnow found the former to be more informative.

Finally, the top part-of-speech n-gram features are
shown in Figure 8, again with manually selected
examples. These features have similar weights
to the character n-gram features and for the most
part seem to represent ungrammatical constructions
(e.g., the first feature indicates that a personal pro-
noun followed by an uninflected verb predicts Chi-
nese). However, there are some perfectly grammat-
ical items that are indicative of a particular native
language (e.g., as compared to for Hindi). One pos-
sible explanation might be a dominant L2 pedagogy
for that language.

5.1 Cross-validation results
The task organizers requested that the participants
run a ten-fold cross validation on a particular split of
the union of the training and development sets after
the evaluation was over. Results of our leading com-
ponent systems and ensemble systems are presented
in Table 6. These are comparable with the TOEFL-
11 column of Figure 3 in Tetreault et al. (2012).

6 Conclusion

In this paper, we have presented MITRE’s partici-
pation in the native language identification task at
BEA-8. Our best system was a naive Bayes ensem-
ble combining component systems that used Win-
now, language modeling and logistic regression ap-
proaches, all using relatively simple character and
word n-gram features. This ensemble performed at
an accuracy of 82.6% in the eleven-way NLI task,
placing it in a statistical tie with the winning systems
submitted by 29 teams. For individual native lan-
guages, our submission performed best among the
participants on Arabic, as ranked by F-measure.

In addition to the three base systems in our best
ensemble, we experimented with a maximum en-

tropy classifier and an assignment-based ensemble
method. We described a variety of experiments we
performed to determine the best configurations and
settings for the various systems. We also covered
experiments aimed at using out-of-domain data for
several native languages. In future work we will ex-
pand upon these, with the goal of applying domain
adaptation approaches.

One concern with NLI as framed in this evalua-
tion is the interaction between native language and
essay topic. The distribution of topics was very sim-
ilar in the various subcorpora, but in more natural
settings this is unlikely to be the case, and there is
a danger of overtraining on topic, to the detriment
of language identification performance. This is es-
pecially problematic for a highly lexical approach
such as ours. In future work, we intend to explore
the extent of this effect, using topic-based splits of
the corpus. Our initial experiments to remedy this
problem are likely to involve domain adaptation ap-
proaches, such as Daumé and Marcu (2007).

As described above, we have had success using
the Winnow-based system Carnie for other latent au-
thor attributes, such as gender. We would like to ex-
plore ensembles similar to those described here for
these attributes as well.

The techniques described in this paper success-
fully identified an author’s native language 82.6% of
the time using a sample of text averaging less than
350 words in length. Future work could study the
interaction of text length and NLI performance, in-
cluding texts shorter than 140 characters in length.

Acknowledgments

This work was funded under the MITRE Innovation
Program. Approved for Public Release; Distribution
Unlimited: 13-1876.

References

Austin Appleby. 2011. MurmurHash, mur-
mur3. https://sites.google.com/site/
murmurhash/.

Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
Age, gender, and the varieties of self-expression. First
Monday, 12(9), September.

108



Rank L1 Score Feature Snippet
1 KOR 57.34 orea first thing that Korean college students usually buy
2 GER 48.68 ,_tha the fact , that people have less moral values
3 SPA 23.65 omen consequences related with the enviroment and the atmosphere
4 ARA 23.23 _alot becouse you have alot of knowledge
6 TUR 22.84 s_abo their searchings about the products

11 ITA 21.56 Ital the Italian scholastic system
19 TEL 20.19 d_als the whole system and also the concept
20 TUR 19.96 urk in Turkey all young people go to the parties
21 CHI 19.51 Ta Take school teachers for example
23 GER 19.34 _-_ constantly - or as mentioned before even exponentially - breaking
27 JPN 17.62 s_,_I For those reasons , I think
32 FRE 16.90 ndeed Indeed , facts are just applications of ideas
36 JPN 16.57 apan been getting weaker these days in Japan .
37 FRE 16.57 onn I personnaly prefer
38 GER 16.04 ,_bec would be great , because so everyone
41 SPA 15.92 esa its not necesary to ask
47 HIN 15.23 in_i the main idea and concept
53 ITA 14.93 act_ due to the fact that too much
74 ITA 13.00 ,_in academic subjects and , in the mean time
81 TEL 12.74 h_ou cannot do with out a tour guide

Figure 7: Character n-gram features predicting particular L1.

Rank L1 Score Feature Snippet
35 CHI 16.58 (PRP,VB) What if he go and see
43 CHI 15.85 (NNS,POS) products ’s
45 SPA 15.41 (NNS,NNS) companies universities
59 TEL 14.05 (RB,IN,VBG) Usually in schooling
64 TEL 13.95 (DT,NNS,WDT) the topics which
65 TUR 13.71 (IN,DT,IN) after a while
66 TEL 13.69 (IN,VBG) in telling
69 TUR 13.42 (VBG,DT,NNS) learning the ways
70 HIN 13.39 (IN,VBN,TO) as compared to
80 HIN 12.81 (FW) [foreign word]

Figure 8: Part of Speech n-gram features predicting particular L1.

109



Douglas Biber and Edward Finegan, editors. 1993. Soci-
olinguistic Perspectives on Register. Oxford studies in
sociolinguistics. Oxford University Press.

Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. to appear. TOEFL11:
A Corpus of Non-Native English. Technical report,
Educational Testing Service.

Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391–408, Mumbai, India, De-
cember.

John D. Burger and John C. Henderson. 2006. An ex-
ploration of observable features related to blogger age.
In Computational Approaches to Analyzing Weblogs:
Papers from the 2006 AAAI Spring Symposium. AAAI
Press.

John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on twitter.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301–1309, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics.

Vitor R. Carvalho and William W. Cohen. 2006. Single-
pass online learning: performance, voting schemes
and online feature selection. In Proceedings of
the 12th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’06,
pages 548–553, New York, NY, USA. ACM.

Hal Daumé and D Marcu. 2007. Frustratingly easy do-
main adaptation. In Proceedings of the Association for
Computational Linguistics, volume 45, page 256.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

John Gibson, Ben Wellner, and Susan Lubar. 2007.
Adaptive web-page content identification. In Proceed-
ings of the 9th Annual ACM International Workshop
on Web information and Data Management, WIDM
’07, pages 105–112, New York, NY, USA. ACM.

William Labov. 1972. Sociolinguistic Patterns. Conduct
& Communication Series. University of Pennsylvania
Press.

Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, ACL ’99,
pages 25–32, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In 2nd International Workshop on
Search and Mining User-Generated Content. ACM.

Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James Pennebaker. 2006. Effects of age and gender on
blogging. In Computational Approaches to Analyzing
Weblogs: Papers from the 2006 AAAI Spring Sympo-
sium. AAAI Press, March.

Andreas Stolcke. 2002. SRILM—an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 901–904.

Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585–2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.

Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252–259.

110


