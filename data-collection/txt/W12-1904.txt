










































Toward Tree Substitution Grammars with Latent Annotations


NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23–30,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Toward Tree Substitution Grammars with Latent Annotations

Francis Ferraro and Benjamin Van Durme and Matt Post
Center for Language and Speech Processing, and

Human Language Technology Center of Excellence
Johns Hopkins University

Abstract

We provide a model that extends the split-
merge framework of Petrov et al. (2006) to
jointly learn latent annotations and Tree Sub-
stitution Grammars (TSGs). We then conduct
a variety of experiments with this model, first
inducing grammars on a portion of the Penn
Treebank and the Korean Treebank 2.0, and
next experimenting with grammar refinement
from a single nonterminal and from the Uni-
versal Part of Speech tagset. We present quali-
tative analysis showing promising signs across
all experiments that our combined approach
successfully provides for greater flexibility
in grammar induction within the structured
guidance provided by the treebank, leveraging
the complementary natures of these two ap-
proaches.

1 Introduction

Context-free grammars (CFGs) are a useful tool for
describing the structure of language, modeling a va-
riety of linguistic phenomena while still permitting
efficient inference. However, it is widely acknowl-
edged that CFGs employed in practice make unre-
alistic independence and structural assumptions, re-
sulting in grammars that are overly permissive. One
successful approach has been to refine the nonter-
minals of grammars, first manually (Johnson, 1998;
Klein and Manning, 2003) and later automatically
(Matsuzaki et al., 2005; Dreyer and Eisner, 2006;
Petrov et al., 2006). In addition to improving pars-
ing accuracy, the automatically learned latent anno-
tations of these latter approaches yield results that

accord well with human intuitions, especially at the
lexical or preterminal level (for example, separating
demonstrative adjectives from definite articles under
the DT tag). It is more difficult, though, to extend
this analysis to higher-level nonterminals, where the
long-distance interactions among latent annotations
of internal nodes are subtle and difficult to trace.
In another line of work, many researchers have ex-

amined the use of formalisms with an extended do-
main of locality (Joshi and Schabes, 1997), where
the basic grammatical units are arbitrary tree frag-
ments instead of traditional depth-one context-free
grammar productions. In particular, Tree Substitu-
tion Grammars (TSGs) retain the context-free prop-
erties of CFGs (and thus the cubic-time inference)
while at the same time allowing for the modeling of
long distance dependencies. Fragments from such
grammars are intuitive, capturing exactly the sorts of
phrasal-level properties (such as predicate-argument
structure) that are not present in Treebank CFGs and
which are difficult to model with latent annotations.
This paper is motivated by the complementarity

of these approaches. We present our progress in
learning latent-variable TSGs in a joint approach that
extends the split-merge framework of Petrov et al.
(2006). We present our current results on the Penn
and Korean treebanks (Marcus et al., 1993; Han et
al., 2001), demonstrating that we are able to learn
fragments that draw on the strengths of both ap-
proaches. Table 1 situates this work among other
contributions.
In addition to experimenting directly with the

Penn and Korean Treebanks, we also conducted two
experiments in this framework with the Universal

23



CFG TSG
none Charniak ’97 Cohn et al. ’09
manual Klein & Manning ’03 Bansal & Klein ’10
automatic Matsuzaki et al. ’05 This paper

Petrov et al. ’06
Dreyer & Eisner ’06

Table 1: Representative prior work in learning refine-
ments for context-free and tree substitution grammars,
with zero, manual, or automatically induced latent anno-
tations.

POS tagset (Petrov et al., 2011). First, we investigate
whether the tagset can be automatically derived af-
ter mapping all nonterminals to a single, coarse non-
terminal. Second, we begin with the mapping de-
fined by the tagset, and investigate how closely the
learned annotations resemble the original treebank.
Together with our TSG efforts, this work is aimed at
increased flexibility in the grammar induction pro-
cess, while retaining the use of Treebanks for struc-
tural guidance.

2 Background

2.1 Latent variable grammars

Latent annotation learning is motivated by the ob-
served coarseness of the nonterminals in treebank
grammars, which often group together nodes with
different grammatical roles and distributions (such
as the role of NPs in subject and object position).
Johnson (1998) presented a simple parent-annotation
scheme that resulted in significant parsing improve-
ment. Klein and Manning (2003) built on these ob-
servations, introducing a series of manual refine-
ments that captured multiple linguistic phenomena,
leading to accurate and fast unlexicalized parsing.
Later, automated methods for nonterminal refine-
ment were introduced, first splitting all categories
equally (Matsuzaki et al., 2005), and later refin-
ing nonterminals to different degrees (Petrov et al.,
2006) in a split-merge EM framework. This lat-
ter approach was able to recover many of the splits
manually determined by Klein and Manning (2003),
while also discovering interesting, novel clusterings,
especially at the lexical level. However, phrasal-
level analysis of latent-variable grammars is more
difficult. (2006) observed that these grammars could
learn long-distance dependencies through sequences
of substates that place all or most of their weight on

(a) A TSG fragment.

SBAR

IN

for

S

NP VP

TO

to

VP

(b) Equivalent CFG rules.
SBAR→ IN S
IN→ for
S→ NP VP
VP→ TO VP
TO→ to

Figure 1: Simple example of a TSG fragment and an
equivalent representation with a CFG.

particular productions, but such patterns must be dis-
covered manually via extensive analysis.

2.2 Tree substitution grammars
Tree substitution grammars (TSGs) allow for com-
plementary analysis. These grammars employ an ex-
tended domain of locality over traditional context-
free grammars by generalizing the atomic units of the
grammar from depth-one productions to fragments
of arbitrary size. An example TSG fragment along
with equivalent CFG rules are depicted in Figure 1.
The two formalisms areweakly equivalent, and com-
puting the most probable derivation of a sentence
with a TSG can be done in cubic time.
Unfortunately, learning TSGs is not straight-

forward, in large part because TSG-specific re-
sources (e.g., large scale TSG-annotated treebanks)
do not exist. One class of existing approaches,
known as Data-Oriented Parsing, simply uses all the
fragments (Bod, 1993, DOP). This does not scale
well to large treebanks, forcing the use of implicit
representations (Goodman, 1996) or heuristic sub-
sets (Bod, 2001). It has also been generally ob-
served that the use of all fragments results in poor,
overfit grammars, though this can be addressed with
held-out data (Zollmann and Sima’an, 2005) or sta-
tistical estimators to rule out fragments that are un-
likely to generalize (Zuidema, 2007). More recently,
a number of groups have found success employing
Bayesian non-parametric priors (Post and Gildea,
2009; Cohn et al., 2010), which put a downward
pressure on fragment size except where the data
warrant the inclusion of larger fragments. Unfortu-
nately, proper inference under these models is in-
tractable, and though Monte Carlo techniques can

24



provide an approximation, the samplers can be com-
plex, difficult to code, and slow to converge.
This history suggests two approaches to state-split

TSGs: (1) a Bayesian non-parametric sampling ap-
proach (incorporate state-splitting into existing TSG
work), or (2) EM (incorporate TSG induction into
existing state-splitting work). We choose the latter
path, and in the next section will describe our ap-
proach which combines the simplicity of DOP, the
intuitions motivating the Bayesian approach, and the
efficiency of EM-based state-splitting.
In related work, Bansal and Klein (2010) combine

(1996)’s implicit DOP representation with a num-
ber of the manual refinements described in Klein and
Manning (2003). They achieve some of the best re-
ported parsing scores for TSGwork and demonstrate
the complementarity of the tasks, but their approach
is not able to learn arbitrary distributions over frag-
ments, and the state splits are determined in a fixed
pre-processing step. Our approach addresses both of
these limitations.

3 State-Split TSG Induction

In this sectionwe describe howwe combine the ideas
of dop, Bayesian-induced TSGs and Petrov et al.
(2006)’s state-splitting framework.1 We are able to
do so by adding a coupling step to each iteration.
That is, each iteration is of the form:

(1) split all symbols in two,

(2) merge 50% of the splits, and

(3) couple existing fragments.

Because every step results in a new grammar, pro-
duction probabilities are fit to observed data by run-
ning at most 50 rounds of EM after every step listed
above.2 We focus on our contribution — the cou-
pling step— and direct those interested in details re-
garding splitting/merging to (Petrov et al., 2006).
Let T be a treebank and let F be the set of all

possible fragments in T . Define a tree T ∈ T
as a composition of fragments {Fi}ni=1 ⊆ F , with
T = F1 ◦ · · · ◦ Fn. We use X to refer to an arbi-
trary fragment, with rX being the root of X . Two

1Code available at cs.jhu.edu/~ferraro.
2We additionally apply Petrov et al. (2006)’s smoothing step

between split and merge.

fragments X and Y may compose (couple), which
we denote byX ◦Y .3 We assume thatX and Y may
couple only if X ◦ Y is an observed subtree.

3.1 Coupling Procedure
While Petrov et al. (2006) posit all refinements sim-
ulatenously and then retract half, applying this strat-
egy to the coupling step would result in a combina-
torial explosion. We control this combinatorial in-
crease in three ways. First, we assume binary trees.
Second, we introduce a constraint set C ⊆ F that dic-
tates what fragments are permitted to compose into
larger fragments. Third, we adopt the iterative ap-
proach of split-merge and incrementally make our
grammar more complex by forbidding a fragment
from participating in “chained couplings:” X ◦Y ◦Z
is not allowed unless eitherX ◦Y or Y ◦Z is a valid
fragment in the previous grammar (and the chained
coupling is allowed by C). Note that setting C = ∅
results in standard split/merge, while C = F results
in a latently-refined dop-1 model.
We say that ⟨XY⟩ represents a valid coupling ofX

and Y only if X ◦ Y is allowed by C, whereas ⟨XY⟩
represents an invalid coupling ifX◦Y is not allowed
by C. Valid couplings result in new fragments. (We
describe how to obtain C in §3.3.)
Given a constraint set C and a current grammar G,

we construct a new grammar G′. For every fragment
F ∈ G, hypothesize a fragment F ′ = F ◦ C, pro-
vided F ◦ C is allowed byC. In order to add F and
F ′ to G′, we assign an initial probability to both frag-
ments (§3.2), and then use EM to determine appro-
priate weights. We do not explicitly remove smaller
fragments from the grammar, though it is possible
for weights to vanish throughout iterations of EM.
Note that a probabilistic TSG fragment may be

uniquely represented as its constituent CFG rules:
make the root of every internal depth-one subtree
unique (have unit probability) and place the entirety
of the TSG weight on the root depth-one rule. This
representation has multiple benefits: it not only al-
lows TSG induction within the split/merge frame-
work, but it also provides a straight-forward way to
use the inside-outside algorithm.

3Technically, the composition operator (◦) is ambiguous if
there is more than one occurrence of rY in the frontier of X .
Although notation augmentations could resolve this, we rely on
context for disambiguation.

25



3.2 Fragment Probability Estimation
First, we define a count function c over fragments by

c(X) =
∑

T∈P(T )

∑
τ∈T

δX,τ , (1)

where P(T ) is a parsed version of T , τ is a subtree
of T and δX,τ is 1 iff X matches τ .4 We may then
count fragment co-occurrence by∑
Y

c(X ◦ Y ) =
∑

Y :⟨XY⟩

c(X ◦ Y ) +
∑

Y :⟨XY⟩

c(X ◦ Y ).

Prior to running inside-outside, we must re-
allocate the probability mass from the previous frag-
ments to the hypothesized ones. As this is just
a temporary initialization, can we allocate mass
as done when splitting, where each rule’s mass is
uniformly distributed, modulo tie-breaking random-
ness, among its refinement offspring? Split/merge
only hypothesizes that a node should have a particu-
lar refinement, but by learning subtrees our coupling
method hypothesizes that deeper structure may bet-
ter explain data. This leads to the realization that a
symbol may both subsume, and be subsumed by, an-
other symbol in the same coupling step; it is not clear
how to apply the above redistribution technique to
our situation.
However, even if uniform-redistribution could

easily be applied, we would like to be able to indi-
cate how much we “trust” newly hypothesized frag-
ments. We achieve this via a parameter γ ∈ [0, 1]:
as γ → 1, we wish to move more of P [X | rX ]
to P [⟨XY⟩ | rX ]. Note that we need to know which
fragmentsL couple below withX (⟨XL⟩), and which
fragments U couple above (⟨UX⟩).
For reallocation, we remove a fraction of the num-

ber of occurrences of top-couplings of X:

ĉ (X) = 1− γ
∑

Y :⟨XY⟩ c(X ◦ Y )∑
Y c(X ◦ Y )

, (2)

and some proportion of the number of occurrences
of bottom-couplings of X:

sub(X) =
∑

U :⟨UX⟩ c(U ◦X)∑
U,L:⟨UL⟩
rX=rL

c(U ◦ L)
. (3)

4We use a parsed version because there are no labeled inter-
nal nodes in the original treebank.

To prevent division-by-zero (e.g., for pre-terminals),
(2) returns 1 and (3) returns 0 as necessary.
Given any fragmentX in an original grammar, let

ρ be its conditional probability: ρ = P [X | rX ] .
For a new grammar, define the new conditional prob-
ability for X to be

P [X | rX ] ∝ ρ · |ĉ(X)− sub(X)|, (4)

and

P [⟨XY⟩ | rX ] ∝ γρ
c(X ◦ Y )∑
Y c(X ◦ Y )

(5)

for applicable Y .
Taken together, equations (4) and (5) simply say

that X must yield some percentage of its current
mass to its hypothesized relatives ⟨XY⟩, the amount
of which is proportionately determined by ĉ. But we
may also hypothesize ⟨ZX⟩, which has the effect of
removing (partial) occurrences of X .5
Though we would prefer posterior counts of frag-

ments, it is not obvious how to efficiently obtain pos-
terior “bigram” counts of arbitrarily large latent TSG
fragments (i.e., c(X ◦ Y )). We therefore obtain, in
linear time, Viterbi counts using the previous best
grammar. Although this could lead to count sparsity,
in practice our previous grammar provides sufficient
counts across fragments.

3.3 Coupling from Common Subtrees
We now turn to the question of how to acquire the
constraint set C. Drawing on the discussion in §2.2,
the constraint set should, with little effort, enforce
sparsity. Similarly to our experiments in classifi-
cation with TSGs (Ferraro et al., 2012), we extract
a list of the K most common subtrees of size at
most R, which we refer to as F⟨R,K⟩. Note that if
F ∈ F⟨R,K⟩, then all subtreesF ′ ofF must also be in
F⟨R,K⟩.6 Thus, we may incrementally build F⟨R,K⟩
in the following manner: given r, for 1 ≤ r ≤ R,
maintain a ranking S, by frequency, of all fragments
of size r; the key point is that S may be built from
F⟨r−1,K⟩. Once all fragments of size r have been
considered, retain only the top K fragments of the
ranked set F⟨r,K⟩ = F⟨r−1,K⟩ ∪ S.

5If ĉ(X) = sub(X), then define Eqn. (4) to be ρ.
6Analogously, if an n-gram appears K times, then all con-

stituent m-grams, m < n, must also appear at least K times.

26



This incremental approach is appealing for two
reasons: (1) practically, it helps temper the growth
of intermediate rankings F⟨r,K⟩; and (2) it provides
two tunable parametersR andK, which relate to the
base measure and concentration parameter of previ-
ous work (Post and Gildea, 2009; Cohn et al., 2010).
We enforce sparsity by thresholding at every itera-
tion.

4 Datasets

We perform a qualitative analysis of fragments
learned on datasets for two languages: the Ko-
rean Treebank v2.0 (Han and Ryu, 2005) and a
comparably-sized portion of the WSJ portion of the
Penn Treebank (Marcus et al., 1993). The Korean
Treebank (KTB) has predefined splits; to be compa-
rable for our analysis, from the PTB we used §2-3
for training and §22 for validation (we refer to this
as wsj2-3). As described in Chung et al. (2010), al-
though Korean presents its own challenges to gram-
mar induction, the KTB yields additional difficulties
by including a high occurrence of very flat rules (in
5K sentences, there are 13 NP rules with at least four
righthand side NPs) and a coarser nonterminal set
than that of the Penn Treebank. On both sets, we
run for two iterations.
Recall that our algorithm is designed to induce a

state-split TSG on a binarized tree; as neither dataset
is binarized in native form we apply a left-branching
binarization across all trees in both collections as a
preprocessing step. Petrov et al. (2006) found differ-
ent binarization methods to be inconsequential, and
we have yet to observe significant impact of this bi-
narization decision (this will be considered in more
detail in future work).
Recently Petrov et al. (2011) provided a set of

coarse, “universal” (as measured across 22 lan-
guages), part-of-speech tags. We explore here the
interaction of this tagset in our model on wsj2-3: call
thismodified version uwsj2-3, onwhichwe run three
iterations. By further coarsening the PTB tags, we
can ask questions such as: what is the refinement
pattern? Can we identify linguistic phenomena in a
different manner than we might without the univer-
sal tag set? Then, as an extreme, we replace all POS
tags with the same symbol “X,” to investigate what
predicate/argument relationships can be derived: we

(a) Modal construction.

S2

S

NP0 VP0

VP

MD

will

VP0

(b) Modifiable NP.

NP2

NP

NN

president

PP0

(c) Nominal-modification.

NP0

NP

NP

NNP3 NNP1

NNP0

NNP0

(d) PP construction.

PP0

IN

at

NP

NP0 NNP0

(e) Initial Quotation.

SINV1

SINV

SINV

SINV0 ,0

”0

VP

VBZ0

Figure 2: Example fragments learned on wsj2-3.

call this set xwsj2-3 and run four times on it.7

5 Fragment Analysis

In this section we analyze hand-selected preliminary
fragments and lexical clusterings our system learns.

WSJ, §2-3 As Figure 2 illustrates, after two iter-
ations we learn various types of descriptive lexical-
ized and unlexicalized fragments. For example, Fig-
ure 2a concisely creates a four-step modal construc-
tion (will), while 2b demonstrates how a potentially
useful nominal can be formed. Further, learned frag-
ments may generate phrases with multiple nominal
modifiers (2c), and lexicalized PPs (2d).
Note that phrases such as NP0 and VP0 are of-

ten lexicalized themselves (with determiners, com-
mon verbs and other constructions), though omitted
due to space constraints; these lexicalized phrases
could be very useful for 2a (given the incremental

7While the universal tag set has a Korean mapping, the sym-
bols do not coincide with the KTB symbols.

27



(a) Common noun refinements.

NNC

0 경우 이날 현재case this day at the moment

1 국제 경제 세계international economy world

2 관련 발표 보도related announcement report

(b) Verbal inflection.

VV0

NNC2 XSV

하

(c) Adjectival inflection.

VJ0

NNC1 XSJ

하

Figure 3: Clusters and fragments for the KTB.

coupling employed, 2a could not have been further
expanded in two iterations). Figure 2c demonstrates
how TSGs and latent annotations are naturally com-
plementary: the former provides structure while the
latter describes lexical distributions of nominals.
Figure 2e illustrates a final example of syntactic

structure, as we begin to learn how to properly an-
alyze a complex quotation. A full analysis requires
only five TSG rules while an equivalent CFG-only
construction requires eight.

KTB2 To illustrate emergent semantic and syntac-
tic patterns, we focus on common noun (NNC) re-
finements. As seen in Table 3a, top words from
NNC0 represent time expressions and planning-
related. As a comparison, two other refinements,
NNC1 and NNC2, are not temporally representative.
This distinction is important as NNC0 easily yields
adverbial phrases, while the resultant adverbial yield
for either NNC1 or NNC2 is much smaller.
Comparing NNC1 and NNC2, we see that the

highest-ranked members of the latter, which include
report and announcement, can be verbalized by ap-
pending an appropriate suffix. Nouns under NNC1,
such as economy and world, generally are subject
to adjectival, rather than verbal, inflection. Figures
3b and 3c capture these verbal and adjectival inflec-
tions, respectively, as lexicalized TSG fragments.

WSJ, §2-3, Universal Tag Set In the preliminary
work done here, we find that after a small number of
iterations we can identify various cluster classifica-

tions for different POS tags. Figures 4a, 4b and 4c
provide examples for NOUN, VERB and PRON, re-
spectively. For NOUNs we found that refinements
correspond to agentive entities (refinements 0, 1,
e.g., corporations or governments), market or stock
concepts (2), and numerically-modifiable nouns (7).
Some refinements overlapped, or contained common
nouns usable in many different contexts (3).
Similarly for VERBs (4b), we find suggested dis-

tinctions among action (1) and belief/cognition (2)
verbs.8 Further, some verb clusters are formed of
eventive verbs, both general (3) and domain-specific
(0). Another cluster is primarily of copula/auxiliary
verbs (7). The remaining omitted categories appear
to overlap, and only once we examine the contexts
in which they occur do we see they are particularly
useful for parsing FRAGs.
Though NOUN and VERB clusters can be dis-

cerned, there tends to be overlap among refinements
that makes the analysis more difficult. On the other
hand, refinements for PRON (4c) tend to be fairly
clean and it is generally simple to describe each: pos-
sessives (1), personified wh-words (2) and general
wh-words (3). Moreover, both subject (5) and ob-
ject (6) are separately described.
Promisingly, we learn interactions among various

refinements in the form of TSG rules, as illustrated
by Figures 4d-4g. While all four examples involve
VERBs it is enlightening to analyze a VERB’s re-
finement and arguments. For example, the refine-
ments in 4d may lend a simple analysis of financial
actions, while 4e may describe different NP interac-
tions (note the different refinement symbols). Dif-
ferent VERB refinements may also coordinate, as in
4f, where participle or gerund may help modify a
main verb. Finally, note how in 4g, an object pro-
noun correctly occurs in object position. These ex-
amples suggest that even on coarsened POS tags, our
method is able to learn preliminary joint syntactic
and lexical relationships.

WSJ, §2-3, Preterminals as X In this experiment,
we investigate whether the manual annotations of
Petrov et al. (2011) can be re-derived through first
reducing one’s non-terminal tagset to the symbol
X and splitting until finding first the coarse grain

8The next highest-ranked verbs for refinement 1 include re-
ceived, doing and announced.

28



(a) Noun refinements.

NOUN
0 Corp Big Co.
1 Mr. U.S. New
2 Bush prices trading
3 Japan September Nissan
7 year % months

(b) Verb refinements.

VERB
0 says said sell buy rose
1 have had has been made
2 said says say added believe
3 sold based go trading filed
7 is are be was will

(c) Pronoun refinements.

PRON
1 its his your
2 who whom —
3 what whose What
5 it he they
6 it them him

(d) VP structure.

VP0

VERB0 NP

ADJ3 NOUN3

(e) Declarative sentence.

S0

NP4 VP

VERB1 NP1

(f) Multiple VP interactions.

VP0

VP

VERB7 ADVP0

VP

VERB0 NP0

(g) Accusative use.

VP0

VERB0 NP

PRON6

Figure 4: Highest weighted representatives for lexical categories (4a-4c) and learned fragments (4d-4g), for uwsj2-3.

X Universal Tag
0 two market brain NOUN
1 ’s said says VERB
2 % company year NOUN
3 it he they PRON
5 also now even ADV
6 the a The DET
7 10 1 all NUM
9 . – ... .
10 and or but CONJ
12 which that who PRON
13 is was are VERB
14 as of in ADP
15 up But billion ADP

Table 2: Top-three representatives for various refine-
ments of X, with reasonable analogues to Petrov et al.
(2011)’s tags. Universal tag recovery is promising.

tags of the universal set, followed by finer-grain tags
from the original treebank. Due to the loss of lexi-
cal information, we run our system for four iterations
rather than three.
As observed in Table 2, there is strong overlap

observed between the induced refinements and the
original universal tags. Though there are 16 refine-
ments of X , due to lack of cluster coherence not all
are listed. Those tags and unlisted refinements seem
to be interwoven in a non-trivial way. We also see
complex refinements of both open- and closed-class
words occurring: refinements 0 and 2 correspond

with the open-class NOUN, while refinements 3 and
12, and 14 and 15 both correspond with the closed
classes PRON and ADP, respectively. Note that 1
and 13 are beginning to split verbs by auxiliaries.

6 Conclusion

We have shown that TSGs may be encoded and in-
duced within a framework of syntactic latent an-
notations. Results were provided for induction us-
ing the English Penn, and Korean Treebanks, with
further experiments based on the Universal Part of
Speech tagset. Examples shown suggest the promise
of our approach, with future work aimed at exploring
larger datasets using more extensive computational
resources.

Acknowledgements Thank you to the reviewers
for helpful feedback, and to JohnsHopkinsHLTCOE
for providing support. We would also like to thank
Byung Gyu Ahn for graciously helping us analyze
the Korean results. Any opinions expressed in this
work are those of the authors.

References

Mohit Bansal and Dan Klein. 2010. Simple, accurate
parsing with an all-fragments grammar. In Proceed-
ings of ACL, pages 1098–1107. Association for Com-
putational Linguistics.

Rens Bod. 1993. Using an annotated corpus as a stochas-

29



tic grammar. In Proceedings of EACL, pages 37–44.
Association for Computational Linguistics.

Rens Bod. 2001. What is the minimal set of fragments
that achievesmaximal parse accuracy? InProceedings
of ACL, pages 66–73. Association for Computational
Linguistics.

Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI, pages 598–603.

Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages
(SPMRL), pages 49–57, Los Angeles, California,
USA, June.

Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 548–556, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal of
Machine Learning Research, 11:3053–3096, Decem-
ber.

Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In Proceedings of
EMNLP, pages 317–326, Sydney, Australia, July. As-
sociation for Computational Linguistics.

Francis Ferraro, Matt Post, and Benjamin Van Durme.
2012. Judging Grammaticality with Count-Induced
Tree Substitution Grammars. In Proceedings of the
Seventh Workshop in Innovated Use of NLP for Build-
ing Educational Applications.

Joshua Goodman. 1996. Efficient algorithms for pars-
ing the dop model. In Proceedings of EMNLP, pages
143–152.

Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank. Technical report, University
of Pennsylvania.

Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for penn korean treebank. Tech-
nical report, IRCS, University of Pennsylvania.

Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613–632.

Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Be-
yond Words, volume 3, pages 71–122. Springer.

Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-

nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 423–430, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
linguistics, 19(2):313–330.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic cfg with latent annotations. InPro-
ceedings of ACL, pages 75–82, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. InProceedings of ACL-ICCL,
pages 433–440, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.

Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proceedings of ACL-
IJCNLP (short papers), pages 45–48, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Andreas Zollmann and Khalil Sima’an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata Languages and Combinatorics,
10(2/3):367.

Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of EMNLP-CoNLL, pages
551–560.

30


