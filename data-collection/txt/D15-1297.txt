



















































JEAM: A Novel Model for Cross-Domain Sentiment Classification Based on Emotion Analysis


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2503â€“2508,
Lisbon, Portugal, 17-21 September 2015. cÂ©2015 Association for Computational Linguistics.

JEAM: A Novel Model for Cross-Domain Sentiment Classification 

Based on Emotion Analysis 

 

 

Kun-Hu Luo, Zhi-Hong Dengï€ª, Liang-Chen Wei, Hongliang Yu 

School of Electronic Engineering and Computer Science 

Peking University, Beijing, China 

 {dr.tiger126@gmail.com, zhdeng@cis.pku.edu.cn, pkuhaywire@gmail.com, 

yuhongliang324@gmail.com}  
 

  

 

Abstract 

Cross-domain sentiment classification 

(CSC) aims at learning a sentiment 

classifier for unlabeled data in the target 

domain based on the labeled data from a 

different source domain. Due to the 

differences of data distribution of two 

domains in terms of the raw features, the 

CSC problem is difficult and challenging. 

Previous researches mainly focused on 

concepts mining by clustering words 

across data domains, which ignored the 

importance of authorsâ€™ emotion contained 

in data, or the different representations of 

the emotion between domains. In this 

paper, we propose a novel framework to 

solve the CSC problem, by modelling the 

emotion across domains. We first develop 

a probabilistic model named JEAM to 

model authorâ€™s emotion state when 

writing. Then, an EM algorithm is 

introduced to solve the likelihood 

maximum problem and to obtain the latent 

emotion distribution of the author. Finally, 

a supervised learning method is utilized to 

assign the sentiment polarity to a given 

online review. Experiments show that our 

approach is effective and outperforms 

state-of-the-art approaches. 

1 Introduction 

Cross-domain sentiment classification (CSC) is 

the task that learns a sentiment classifier for 

unlabeled data in the target domain based on the 

labeled data from the source domain. With the 

increasing amount of opinion information 

                                                           
ï€ª Corresponding author 

available on the Internet, CSC has become a hot 

spot in recent years. Traditional machine learning 

algorithms often train a classifier utilizing the 

labeled data for CSC. However, in some practical 

cases, we may have many labeled data for some 

domains (source domains) but very few or no 

labeled data for other domains (target domains). 

Due to the differences of the distribution of two 

domains in terms of raw features, e.g. raw term 

frequency, the classifier trained from the source 

domain often performs badly on the target domain. 

To overcome this issue, several feature-based 

studies have been proposed to improve the 

sentiment classification domain adaptation   

[Zhuang et al., 2013; He et al., 2011; Gao and Li, 

2011; Li et al., 2012; Dai et al., 2007; Zhuang et 

al., 2010; Pan et al., 2010; Wang et al., 2011; Long 

et al., 2012; Lin and He, 2009]. 

 Existing studies build various generative 

models to solve the domain adaptation problems 

for CSC. In most cases, the models are trained by 

using the whole corpora without specifying on the 

sentiment of the texts. For example, [Zhuang et al., 

2013] propose a general framework HIDC to mine 

high-level concepts (e.g. word clusters) across 

various domains. However, their learned concepts 

contain many topics not restricted to the sentiment. 

On the other hand, some researchers focus on the 

usage of the sentiment in CSC study [Mitra et al., 

2013a; Mitra et al., 2013b; He et al., 2011]. [He et 

al., 2011] modify JST model [Lin and He, 2009] 

by incorporating word polarity priors through 

adjusting the topic-word Dirichlet priors. 

However, they fail to consider the expression 

differences among various domains.  

To overcome the above issues, we employ 

â€œemotionâ€, for its ubiquity among domains. The 

sentiment words in different domains might vary 

2503



significantly, but the emotion can be effectively 

transferred. For example, when expressing the 

emotion â€œhappinessâ€, one uses â€œbravoâ€ in the 

domain of sport, while â€œyummyâ€ in the domain of 

food. Therefore, we propose an EA framework to 

model the latent emotions which are commonly 

contained in subjective articles and expressed by 

â€œemotional wordsâ€. We infer the sentiment 

polarity of a document based on the emotion state. 

The hierarchy of EA is composed by four layers: 

(1) Sentiment Layer 

Normally, the sentiment of a document is the 

general opinion towards a certain event or object. 

For example, a movie review in IMDB might 

voice the feeling about the movie by a reviewer 

[Yu et al., 2013].  

(2) Emotion Layer 

Based on the emotion classification theories 

in psychology [Plutchik, 2002], the emotion can 

be classified into the basic ones influenced by the 

physiological factors, e.g. happiness, sadness, 

anger, etc., and dozens of complicated ones 

formed under some specific social conditions, e.g. 

shame, guilt, abashment, etc. Additionally, the 

emotion can be classified as positive and negative 

(similar to the sentiment classification) based on 

dimensional models of emotion [Schlosberg, 

1954; Plutchik, 2002; Rubin and Talerico, 2009]. 

Intuitively, we assume that a document tends to 

contain the emotions of similar polarity.  

(3) Lexicon Layer 

To build the connection between words and 

the emotion, we introduce emotional words 

instead of raw word features into our model. By 

utilizing the emotional lexicon MPQA [Wiebe et 

al., 2005], we select groups of strong polar words, 

which get high scores in the emotional lexicon. 

These words are considered highly correlated to 

the certain emotion of the same polarity. And 

these strong polar words have invariant polarity 

across domains. Therefore, the emotion can be 

substantialized by a series of emotional words 

drawn from corresponding probability distribution. 

(4) Expression Layer 

In many practical cases, data come from 

different domains. We suppose that the 

correlation between emotion state and sentiment 

orientation is stable over domains, but one 

emotion may have different expressions when 

domain varies. E.g., â€œsatisfactionâ€ may be 

expressed as â€œinterestingâ€ or â€œattractiveâ€ for a 

book; meanwhile, it may be expressed â€œefficientâ€ 

for an electronics device. Formally, we have 

ğ‘(ğ‘’|ğ‘¦, ğ‘Ÿ1) = ğ‘(ğ‘’|ğ‘¦, ğ‘Ÿ2) = ğ‘(ğ‘’|ğ‘¦)     (1) 
ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ1) â‰  ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ2)         (2) 

where ğ‘’  denotes the emotion, y denotes the 
authorâ€™s sentiment orientation, ğ‘Ÿ1  and ğ‘Ÿ2 
denotes two different domains, and ğ‘¤ğ‘’  denotes 
the emotional words.  

 Along this line, we propose the Joint 

Emotion Analysis Model (named JEAM for 

abbreviation) utilizing the probabilistic methods. 

See details in the next section. 

2 Proposed Model 

2.1 Problem Formulation 

The CSC problem can be formulated as follows: 

Suppose we have two sets of data, denoted as ğ·ğ‘  
and ğ·ğ‘¡, which represent the source domain data 
and the target domain data respectively. In the 

CSC problem, the source domain data consist of 

labeled instances, denoted by  ğ·ğ‘  =

{(ğ‘¥ğ‘–
(ğ‘ )

, ğ‘¦ğ‘–
(ğ‘ )

)}|ğ‘–=1
ğ‘›ğ‘  , where ğ‘¥ğ‘–

(ğ‘ )
âˆˆ â„ğ‘˜  is an input 

vector, ğ‘¦ğ‘–
(ğ‘ )

âˆˆ {0,1} is the output label, and ğ‘›ğ‘  

is the number of documents in ğ·ğ‘ . Unlike that of 
the source domain, the target domain data consists 

of samples without any label information, denoted 

by ğ·ğ‘¡ = {ğ‘¥ğ‘–
(ğ‘¡)

}|ğ‘–=1
ğ‘›ğ‘¡ , where ğ‘¥ğ‘–

(ğ‘¡)
âˆˆ â„ğ‘˜ is an input 

vector, and ğ‘›ğ‘¡ is the number of documents in ğ·ğ‘¡. 
The task of CSC is to leverage the training data of 

source domain ğ·ğ‘   to predict the label ğ‘¦ğ‘–
(ğ‘¡)

 

corresponding to input vector ğ‘¥ğ‘–
(ğ‘¡)

 of target 

domain ğ·ğ‘¡.  

2.2 The JEAM Model 

To model the authorâ€™s emotion state contained in 

the document, we propose the JEAM model based 

on the probabilistic graphical principle. Note that 

all the factors and edges in JEAM are derived 

from the specific concepts and relations in EA, 

e.g., Eq(1) and Eq(2). We draw the graphical 

representation of JEAM in Figure 1, and show the 

notations of this paper in Table 1. 

In Figure 1, y denotes the sentiment 

orientation of the author, which is a latent variable 

in this model. ğ‘’   denotes any emotion (topic) 
generated by y from a conditional 

probability ğ‘(ğ‘’|ğ‘¦). ğ‘’ is also a latent variable in 
this model. ğ‘Ÿ  denotes any data domain, e.g., 
books, dvd, kitchen, and electronics etc. ğ‘‘ 
denotes any document chosen from domain r with 

label y. For documents from the source domain, 

the conditional probability ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦) is known, 
which can be used to supervise the modeling 

process. ğ‘¢ denotes the prior sentiment polarity of 
the corresponding emotional word. In practice, ğ‘¢ 
can be obtained from the emotional lexicon,  

2504



 

 

 

 

 

 

Figure 1. The Graphical representation of JEAM. 

All the latent variables are marked in white, and 

all the observed variables are marked in gray. 

 

Table 1.  Means of Symbols 

 

which classifies a series of words into positive and 

negative categories.  ğ‘¤ğ‘’ denotes any emotional 
word with polarity u, which is chosen over words 

conditioned on emotion e and domain r from 

conditional probability ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢) . In this 
paper, we only select emotional words with strong 

sentiment polarities to represent the vector of the 

document. Therefore, we rebuild the data with the 

help of emotional lexicon cutting out the non-

emotional words. As a result, any word chosen 

from the rebuilt data will be an emotional word, 

which is supposed not to change its polarity in 

different domains. Additionally, the joint 

probability over all the observed variables can be 

defined as follows based on the hidden variables: 

ğ‘(ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢) = âˆ‘ ğ‘(ğ‘’, ğ‘¦, ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢)ğ‘’,ğ‘¦   (3) 

Based on the graphical model, we have: 

ğ‘(ğ‘’, ğ‘¦, ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢) = 

ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢)ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦)ğ‘(ğ‘’|ğ‘¦)ğ‘(ğ‘Ÿ)ğ‘(ğ‘¦)ğ‘(ğ‘¢) (4) 

We need to learn the unobservable 

probabilities (e.g.,  ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢), ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦),  
ğ‘(ğ‘’|ğ‘¦), ğ‘(ğ‘¦) ) to infer the hidden emotion 
distribution. Therefore, we develop an 

Expectation-Maximization (EM) algorithm to 

maximize the log likelihood of generating the 

whole dataset and obtain the iterative formula in 

E-step as follows: 

ğ‘(ğ‘’, ğ‘¦|ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢) =  

ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢)ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦)ğ‘(ğ‘’|ğ‘¦)ğ‘(ğ‘Ÿ)ğ‘(ğ‘¦)ğ‘(ğ‘¢)
âˆ‘ ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢)ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦)ğ‘(ğ‘’|ğ‘¦)ğ‘(ğ‘Ÿ)ğ‘(ğ‘¦)ğ‘(ğ‘¢)ğ‘’,ğ‘¦

 (5) 

where all the factors are calculated in M-step 

similar to PLSA and HIDC (Hoffman, 1999; 

Zhuang et al., 2013). 

2.3 CSC via JEAM 

To use JEAM to solve CSC problems, we adopt 

two optimizations: 

First, we supervise the EM optimization with 

the polarity information of emotional words and 

instances respectively in the source domain. On 

the one hand, we estimate ğ‘(ğ‘’, ğ‘¦|ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢) 
utilizing the polarity label of the emotional words.  

Let the emotion set ğ¸ be divided into positive set 
ğ¸ğ‘  and negative set  ğ¸ğ‘› . We set 

ğ‘(ğ‘’ğ‘– , ğ‘¦|ğ‘¤ğ‘’ , ğ‘‘, ğ‘Ÿ, ğ‘¢ğ‘¤ğ‘’) = 0 during the whole EM 

process when the polarities of the emotion and 

current emotional word are different. On the other 

hand, we estimate the probability ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦) with 
the label information of instances in the source 

domain. When the document is from the source 

domain, we set ğ‘(ğ‘‘|ğ‘Ÿ, ğ‘¦) = 0  if  ğ‘¦   is 
different with the ground truth. 

Second, we reconstruct the document as 

follows, 

ğ‘‘âˆ— = [ğ‘’1, ğ‘’2, â€¦ ğ‘’ğ‘š], ğ‘’ğ‘– = {
    1 +

|ğ‘Šğ‘’ğ‘–
ğ‘Ÿ|

âˆ‘ |ğ‘Šğ‘’ğ‘—
ğ‘Ÿ|ğ‘šğ‘—=1

, ğ‘–ğ‘“ ğ‘Šğ‘’ğ‘–
ğ‘Ÿ â‰  âˆ…

 
0, ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘ 

     (6) 

where [ğ‘’1, ğ‘’2, â€¦ ğ‘’ğ‘š]  is the distribution over 
emotions, ğ‘Šğ‘’ğ‘–

ğ‘Ÿ = {ğ‘¤ğ‘’|ğ‘’ğ‘¤ğ‘’
ğ‘Ÿ = ğ‘–} , ğ‘’ğ‘¤ğ‘’

ğ‘Ÿ =

ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘’  ğ‘(ğ‘’|ğ‘¤ğ‘’ , ğ‘Ÿ) , and ğ‘(ğ‘’|ğ‘¤ğ‘’ , ğ‘Ÿ)  can be 
computed based on  ğ‘(ğ‘¤ğ‘’|ğ‘’, ğ‘Ÿ, ğ‘¢) , ğ‘(ğ‘’|ğ‘¦), ğ‘(ğ‘¢) 
and ğ‘(ğ‘Ÿ) obtained after EM algorithm. The main 
function of this step is to process a new given 

document faster, avoiding training JEAM again 

with the new input. Finally, a machine learning 

method Support Vector Machine (SVM) is 

introduced to train a classifier with the labeled 

data from the source domain and assign polarities 

to documents from the target domain based on our 

reconstructed data.  

3 Experiments 

3.1 Experimental Setup 

We demonstrate the effectiveness of JEAM on the 

Multi-Domain sentiment data set [Blitzer et al., 

2007] which contains four types (domains) of 

real-world product documents taken from 

Amazon.com, which are books, dvd, electronics 

and kitchen. We randomly select 1800 documents 

from the one domain (source domain) and 200 

documents from another domain (target domain). 

Then, we train a sentiment classifier using 

documents selected from the source domain and 

ğ‘’ Emotion 
ğ‘¤ğ‘’ Emotional word 
ğ‘Ÿ Domain 
ğ‘‘ Document  
ğ‘¢ Prior sentiment polarity of the emotional word 
ğ‘¦ Sentiment polarity of the document 
ğ‘‹ All the observed variables 
ğœƒ All the model parameters 

2505



assign labels to documents selected from the 

target domain, which generates 12 classification 

tasks. We preform 10 random selections and 

report the average results over 10 different runs. 

We use MPQA subjective lexicon 1  as the 

emotional lexicon. In our experiments, only 

strongly subjective clues are considered as 

emotional words, consisting of 1717 positive and 

3621 negative words. We rebuild the dataset by 

cutting out the non-emotional words. For 

experiment parameters, we set ğ‘ = 25, ğ‘› = 25, 
and ğ‘‡ = 100  after plenty of experiments. 
Considering the data in practice, the sentiment 

orientation y has only two forms, positive or 

negative. Note that we do neither instance 

selection nor complicated feature selection (only 

filter the low-frequency words) to our proposed 

method and other methods in comparison. 

3.2 Experimental Result 

Performance of Emotional Words 
We show the effectiveness of introducing 

emotional words to solve the CSC problem. In 

JEAM, we reconstruct the documents by cutting 

out the non-emotional words. To compare the 

classification accuracy on the original documents 

and the reconstructed (emotional) documents, we 

choose two common classification algorithms, 

linear SVM and PLSA (topic size=10) for 

experiment respectively. The experiment results 

shows that both SVM and PLSA perform better 

on the emotional documents (60.43% and 60.48%) 

than on the original documents (57.73% and 

56.69%) for the average accuracy over 12 

classification tasks.  

Effectiveness of using domain information 
and word polarity 
We show the effectiveness of using domain 

information and word polarity, which are 

employed in our approach. For this purpose, we 

repeat the experiment without introducing domain 

and word polarity (node u and node r) into the 

model. Figure 2 shows the results. As it is clear, 

the highest performance can be achieved when 

domain information and word polarity are both 

used, while the lowest performance is obtained 

when neither of them is used.  

Comparison with the Baselines 

We compare our proposed approach with PLSA, 

SVM, SFA [Pan et al., 2010], JST [He et al., 2011] 

and HIDC [Zhuang et al., 2013]. The 

experimental results of the 12 classification tasks 

are shown in Figure 3. It can be observed that our  

                                                           
1 http://www.cs.pitt.edu/mpqa 

 

 

 

 

 

 

 

 

 

 

Figure 2: Effectiveness of using domain 

information and word polarity 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 3: Comparison with the Baselines 

 

 

proposed approach outperforms all the other 

approaches in general. Note that in order to obtain 

a more precise comparison of the algorithms, we 

do neither the instance selection nor the 

complicated feature selection. The result of our 

proposed approach can possibly be improved with 

the help of these selection strategies. 

4 Conclusion 

In this paper, we propose a novel framework to 

solve the CSC problem, by modelling emotions 

across domains. We deeply analyze the relation 

between the author and the document based on the 

emotion theories in the field of psychology. Along 

this line, we propose a framework named EA, 

which takes the emotions and domains into 

account. Based on EA, we propose a novel model 

named JEAM to model the authorâ€™s emotion state 

for Cross-domain sentiment classification. We 

conduct extensive experiments on real datasets to 

evaluate JEAM. The experiment results show that 

emotion plays an important role in CSC and 

JEAM outperforms existing state-of-the-art 

methods on the task of CSC. 

2506



5 Acknowledgement 

This work is partially supported by Project 

61170091 supported by National Natural Science 

Foundation of China and Project 2015AA015403 

supported by the National High Technology 

Research and Development Program of China 

(863 Program). We would also like to thank the 

anonymous reviewers for their helpful comments. 

 

References 

John Blitzer, Mark Dredze, Fernando Pereira, et 

al.2007. Biographies, bollywood, boom-boxes and 

blenders: Domain adaptation for sentiment 

classification. In ACL, volume 7, pages 440â€“447. 

Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong 

Yu. 2007. Co-clustering based classification for out-

of-domain documents. In Proceedings of the 13th 

ACM SIGKDD international conference on 

Knowledge discovery and data mining, pages 210â€“ 

219. ACM. 

Hal Daumâ€²e III. 2009. Frustratingly easy domain 

adaptation. arXiv preprint arXiv:0907.1815.  

Sheng Gao and Haizhou Li. 2011. A cross-domain 

adaptation method for sentiment classification using 

probabilistic latent analysis. In Proceedings of the 

20th ACM international conference on Information 

and knowledge management, pages 1047â€“1052. 

ACM. 

Boqing Gong, Kristen Grauman, and Fei Sha. 2013. 

Connecting the dots with landmarks: 

Discriminatively learning domain-invariant features 

for unsupervised domain adaptation. In Proceedings 

of The 30th International Conference on Machine 

Learning, pages 222â€“230. 

Yulan He, Chenghua Lin, and Harith Alani. 2011. 

Automatically extracting polarity-bearing topics for 

cross-domain sentiment classification. In 

Proceedings of the 49th Annual Meeting of the 

Association for Computational Linguistics: Human 

Language Technologies-Volume 1, pages 123â€“131. 

Association for Computational Linguistics.  

Thomas Hofmann. 1999. Probabilistic latent semantic 

indexing. In Proceedings of the 22nd annual 

international ACM SIGIR conference on Research 

and development in information retrieval, pages 50â€“

57. ACM. 

Shoushan Li, Chu-Ren Huang, Guodong Zhou, and 

Sophia Yat Mei Lee. 2010. Employing personal/ 

impersonal views in supervised and semisupervised 

sentiment classification. In Proceedings of the 48th 

annual meeting of the association for computational 

linguistics, pages 414â€“423. Association for 

Computational Linguistics. 

Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and 

Xiaoyan Zhu. 2012. Cross-domain co-extraction of 

sentiment and topic lexicons. In Proceedings of the 

50th Annual Meeting of the Association for 

Computational Linguistics: Long Papers-Volume 1, 

pages 410â€“419. Association for Computational 

Linguistics. 

Chenghua Lin and Yulan He. 2009. Joint sentiment/ 

topic model for sentiment analysis. In Proceedings 

of the 18th ACMconference on Information and 

knowledge management, pages 375â€“384. ACM. 

Mingsheng Long, Jianmin Wang, Guiguang Ding, Wei 

Cheng, Xiang Zhang, and Wei Wang. 2012. Dual 

transfer learning. In SDM, pages 540â€“551. SIAM. 

Mitra Mohtarami. Man lan, and chew lim tan. 2013a. 

from semantic to emotional space in probabilistic 

sense sentiment analysis. In the 27th AAAI 

Conference on Artificial Intelligence. 

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang 

Yang, and Zheng Chen. 2010. Cross-domain 

sentiment classification via spectral feature 

alignment. In Proceedings of the 19th international 

conference on World wide web, pages 751â€“760. 

ACM. 

Bo Pang and Lillian Lee. 2008. Opinion mining and 

sentiment analysis. Foundations and trends in 

information retrieval, 2(1-2):1â€“135. 

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 

2002. Thumbs up?: sentiment classification using 

machine learning techniques. In Proceedings of the 

ACL-02 conference on Empirical methods in 

natural language processing-Volume 10, pages 79â€“

86. Association for Computational Linguistics. 

Robert Plutchik. 1980. Emotion: A psychoevolutionary 

synthesis. Harpercollins College Division.  

Robert Plutchik. 2002. Emotion and life. 

David C Rubin and Jennifer M Talarico. 2009. A 

comparison of dimensional models of emotion: 

Evidence from emotions, prototypical events, 

autobiographical memories, and words. Memory, 

17(8):802â€“808. 

Harold Schlosberg. 1954. Three dimensions of 

emotion. Psychological review, 61(2):81. 

Hua Wang, Heng Huang, Feiping Nie, and Chris Ding. 

2011. Cross-language web page classification via 

dual knowledge transfer using nonnegative matrix 

trifactorization. In Proceedings of the 34th 

international ACM SIGIR conference on Research 

and development in Information Retrieval, pages 

933â€“942. ACM. 

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 

2005. Annotating expressions of opinions and 

emotions in language. Language resources and 

evaluation, 39(2-3):165â€“210. 

2507



Hongliang Yu, Zhi-Hong Deng, and Shiyingxue Li. 

2013. Identifying sentiment words using an 

optimization-based model without seed words. In 

ACL (2), pages 855â€“859. 

Fuzhen Zhuang, Ping Luo, Peifeng Yin, Qing He, and 

Zhongzhi Shi. 2013. Concept learning for 

crossdomain text classification: A general 

probabilistic framework. In Proceedings of the 

Twenty-Third international joint conference on 

Artificial Intelligence, pages 1960â€“1966. AAAI 

Press. 

Fuzhen Zhuang, Ping Luo, Changying Du, Qing He, 

Zhongzhi Shi, and Hui Xiong. 2014. Triplex transfer 

learning: exploiting both shared and distinct 

concepts for text classification. Cybernetics, IEEE 

Transactions on, 44(7):1191â€“1203. 

2508


