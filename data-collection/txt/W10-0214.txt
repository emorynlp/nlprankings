










































Recognizing Stances in Ideological On-Line Debates


Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Recognizing Stances in Ideological On-Line Debates

Swapna Somasundaran

Dept. of Computer Science

University of Pittsburgh

Pittsburgh, PA 15260

swapna@cs.pitt.edu

Janyce Wiebe

Dept. of Computer Science and

The Intelligent Systems Program

University of Pittsburgh

Pittsburgh, PA 15260

wiebe@cs.pitt.edu

Abstract

This work explores the utility of sentiment and

arguing opinions for classifying stances in ide-

ological debates. In order to capture arguing

opinions in ideological stance taking, we con-

struct an arguing lexicon automatically from

a manually annotated corpus. We build su-

pervised systems employing sentiment and ar-

guing opinions and their targets as features.

Our systems perform substantially better than

a distribution-based baseline. Additionally,

by employing both types of opinion features,

we are able to perform better than a unigram-

based system.

1 Introduction

In this work, we explore if and how ideologi-

cal stances can be recognized using opinion analy-

sis. Following (Somasundaran and Wiebe, 2009),

stance, as used in this work, refers to an overall po-

sition held by a person toward an object, idea or

proposition. For example, in a debate “Do you be-

lieve in the existence of God?,” a person may take a

for-existence of God stance or an against existence

of God stance. Similarly, being pro-choice, believ-

ing in creationism, and supporting universal health-

care are all examples of ideological stances.

Online web forums discussing ideological and po-

litical hot-topics are popular.1 In this work, we are

1http://www.opposingviews.com,

http://wiki.idebate.org, http://www.createdebate.com and

http://www.forandagainst.com are examples of such debating

websites.

interested in dual-sided debates (there are two pos-

sible polarizing sides that the participants can take).

For example, in a healthcare debate, participants can

take a for-healthcare stance or an against-healthcare

stance. Participants generally pick a side (the web-

sites provide a way for users to tag their stance)

and post an argument/justification supporting their

stance.

Personal opinions are clearly important in ideo-

logical stance taking, and debate posts provide out-

lets for expressing them. For instance, let us con-

sider the following snippet from a universal health-

care debate. Here the writer is expressing a nega-

tive sentiment2 regarding the government (the opin-

ion spans are highlighted in bold and their targets,

what the opinions are about, are highlighted in ital-

ics).

(1) Government is a disease pretending to be its

own cure. [side: against healthcare]

The writer’s negative sentiment is directed toward

the government, the initiator of universal healthcare.

This negative opinion reveals his against-healthcare

stance.

We observed that arguing, a less well explored

type of subjectivity, is prominently manifested in

ideological debates. As used in this work, arguing is

a type of linguistic subjectivity, where a person is ar-

guing for or against something or expressing a belief

about what is true, should be true or should be done

2As used in this work, sentiment is a type of linguistic sub-

jectivity, specifically positive and negative expressions of emo-

tions, judgments, and evaluations (Wilson and Wiebe, 2005;

Wilson, 2007; Somasundaran et al., 2008).

116



in his or her view of the world (Wilson and Wiebe,

2005; Wilson, 2007; Somasundaran et al., 2008).

For instance, let us consider the following snippet

from a post supporting an against-existence of God

stance.

(2) Obviously that hasn’t happened, and to be

completely objective (as all scientists should

be) we must lean on the side of greatest evi-

dence which at the present time is for evolu-

tion. [side: against the existence of God]

In supporting their side, people not only express

their sentiments, but they also argue about what is

true (e.g., this is prominent in the existence of God

debate) and about what should or should not be done

(e.g., this is prominent in the healthcare debate).

In this work, we investigate whether sentiment

and arguing expressions of opinion are useful for

ideological stance classification. For this, we ex-

plore ways to capture relevant opinion information

as machine learning features into a supervised stance

classifier. While there is a large body of resources

for sentiment analysis (e.g., the sentiment lexicon

from (Wilson et al., 2005)), arguing analysis does

not seem to have a well established lexical resource.

In order to remedy this, using a simple automatic ap-

proach and a manually annotated corpus,3 we con-

struct an arguing lexicon. We create features called

opinion-target pairs, which encode not just the opin-

ion information, but also what the opinion is about,

its target. Systems employing sentiment-based and

arguing-based features alone, or both in combina-

tion, are analyzed. We also take a qualitative look

at features used by the learners to get insights about

the information captured by them.

We perform experiments on four different ideo-

logical domains. Our results show that systems us-

ing both sentiment and arguing features can perform

substantially better than a distribution-based base-

line and marginally better than a unigram-based sys-

tem. Our qualitative analysis suggests that opinion

features capture more insightful information than

using words alone.

The rest of this paper is organized as follows: We

first describe our ideological debate data in Section

2. We explain the construction of our arguing lexi-

con in Section 3 and our different systems in Section

3MPQA corpus available at http://www.cs.pitt.edu/mpqa.

4. Experiments, results and analyses are presented in

Section 5. Related work is in Section 6 and conclu-

sions are in Section 7.

2 Ideological Debates

Political and ideological debates on hot issues are

popular on the web. In this work, we analyze the fol-

lowing domains: Existence of God, Healthcare, Gun

Rights, Gay Rights, Abortion and Creationism. Of

these, we use the first two for development and the

remaining four for experiments and analyses. Each

domain is a political/ideological issue and has two

polarizing stances: for and against.

Table 2 lists the domains, examples of debate top-

ics within each domain, the specific sides for each

debate topic, and the domain-level stances that cor-

respond to these sides. For example, consider the

Existence of God domain in Table 2. The two

stances in this domain are for-existence of God and

against-existence of God. “Do you believe in God”,

a specific debate topic within this domain, has two

sides: “Yes!!” and “No!!”. The former corresponds

to the for-existence of God stance and the latter maps

to the against-existence of God stance. The situa-

tion is different for the debate “God Does Not Ex-

ist”. Here, side “against” corresponds to the for-

existence of God stance, and side “for” corresponds

to the against-existence of God stance.

In general, we see in Table 2 that, while specific

debate topics may vary, in each case the two sides

for the topic correspond to the domain-level stances.

We download several debates for each domain and

manually map debate-level stances to the stances

for the domain. Table 2 also reports the number

of debates, and the total number of posts for each

domain. For instance, we collect 16 different de-

bates in the healthcare domain which gives us a total

of 336 posts. All debate posts have user-reported

debate-level stance tags.

2.1 Observations

Preliminary inspection of development data gave us

insights which shaped our approach. We discuss

some of our observations in this section.

Arguing Opinion

We found that arguing opinions are prominent

when people defend their ideological stances. We

117



Domain/Topics stance1 stance2

Healthcare (16 debates, 336 posts) for against

Should the US have universal health-

care

Yes No

Debate: Public insurance option in

US health care

Pro Con

Existence of God (7 debates, 486

posts)

for against

Do you believe in God Yes!! No!!

God Does Not Exist against for

Gun Rights (18 debates, 566 posts) for against

Should Guns Be Illegal against for

Debate: Right to bear arms in the US Yes No

Gay Rights (15 debates, 1186 posts) for against

Are people born gay Yes No

Is homosexuality a sin No Yes

Abortion (13 debates, 618 posts) for against

Should abortion be legal Yes No

Should south Dakota pass the abor-

tion ban

No Yes

Creationism (15 debates, 729 posts) for against

Evolution Is A False Idea for against

Has evolution been scientifically

proved

It has

not

It has

Table 1: Examples of debate topics and their stances

saw an instance of this in Example 2, where the par-

ticipant argues against the existence of God. He ar-

gues for what (he believes) is right (should be), and

is imperative (we must). He employs “Obviously”

to draw emphasis and then uses a superlative con-

struct (greatest) to argue for evolution.

Example 3 below illustrates arguing in a health-

care debate. The spans most certainly believe and

has or must do reveal arguing (ESSENTIAL, IM-

PORTANT are sentiments).

(3) ... I most certainly believe that there are

some ESSENTIAL, IMPORTANT things

that the government has or must do [side: for

healthcare]

Observe that the text spans revealing arguing can

be a single word or multiple words. This is differ-

ent from sentiment expressions that are more often

single words.

Opinion Targets

As mentioned previously, a target is what an

opinion is about. Targets are vital for determining

stances. Opinions by themselves may not be as in-

formative as the combination of opinions and tar-

gets. For instance, in Example 1 the writer supports

an against-healthcare stance using a negative senti-

ment. There is a negative sentiment in the example

below (Example 4) too. However, in this case the

writer supports a for-healthcare stance. It is by un-

derstanding what the opinion is about, that we can

recognize the stance.

(4) Oh, the answer is GREEDY insurance com-

panies that buy your Rep & Senator. [side: for

healthcare]

We also observed that targets, or in general items

that participants from either side choose to speak

about, by themselves may not be as informative as

opinions in conjunction with the targets. For in-

stance, Examples 1 and 3 both speak about the gov-

ernment but belong to opposing sides. Understand-

ing that the former example is negative toward the

government and the latter has a positive arguing

about the government helps us to understand the cor-

responding stances.

Examples 1, 3 and 4 also illustrate that there

are a variety of ways in which people support

their stances. The writers express opinions about

government, the initiator of healthcare and insur-

ance companies, and the parties hurt by government

run healthcare. Participants group government and

healthcare as essentially the same concept, while

they consider healthcare and insurance companies

as alternative concepts. By expressing opinions re-

garding a variety of items that are same or alternative

to main topic (healthcare, in these examples), they

are, in effect, revealing their stance (Somasundaran

et al., 2008).

3 Constructing an Arguing Lexicon

Arguing is a relatively less explored category in sub-

jectivity. Due to this, there are no available lexicons

with arguing terms (clues). However, the MPQA

corpus (Version 2) is annotated with arguing sub-

jectivity (Wilson and Wiebe, 2005; Wilson, 2007).

There are two arguing categories: positive arguing

and negative arguing. We use this corpus to gener-

ate a ngram (up to trigram) arguing lexicon.

The examples below illustrate MPQA arguing an-

notations. Examples 5 and 7 illustrate positive argu-

118



ing annotations and Example 6 illustrates negative

arguing.

(5) Iran insists its nuclear program is purely

for peaceful purposes.

(6) Officials in Panama denied that Mr. Chavez

or any of his family members had asked for

asylum.

(7) Putin remarked that the events in Chechnia

“could be interpreted only in the context

of the struggle against international terror-

ism.”

Inspection of these text spans reveal that arguing an-

notations can be considered to be comprised of two

pieces of information. The first piece of information

is what we call the arguing trigger expression. The

trigger is an indicator that an arguing is taking place,

and is the primary component that anchors the argu-

ing annotation. The second component is the ex-

pression that reveals more about the argument, and

can be considered to be secondary for the purposes

of detecting arguing. In Example 5, “insists”, by it-

self, conveys enough information to indicate that the

speaker is arguing. It is quite likely that a sentence

of the form “X insists Y” is going to be an arguing

sentence. Thus, “insists” is an arguing trigger.

Similarly, in Example 6, we see two arguing trig-

gers: “denied” and “denied that”. Each of these can

independently act as arguing triggers (For example,

in the constructs “X denied that Y” and “X denied

Y”). Finally, in Example 7, the arguing annotation

has the following independent trigger expressions

“could be * only”, “could be” and “could”. The wild

card in the first trigger expression indicates that there

could be zero or more words in its place.

Note that MPQA annotations do not provide this

primary/secondary distinction. We make this dis-

tinction to create general arguing clues such as “in-

sist”. Table 3 lists examples of arguing annotations

from the MPQA corpus and what we consider as

their arguing trigger expressions.

Notice that trigger words are generally at the be-

ginning of the annotations. Most of these are uni-

grams, bigrams or trigrams (though it is possible for

these to be longer, as seen in Example 7). Thus, we

can create a lexicon of arguing trigger expressions

Positive arguing annotations Trigger Expr.

actually reflects Israel’s determination ... actually

am convinced that improving ... am convinced

bear witness that Mohamed is his ... bear witness

can only rise to meet it by making ... can only

has always seen usama bin ladin’s ... has always

Negative Arguing Annotations Trigger Expr.

certainly not a foregone conclusion certainly not

has never been any clearer has never

not too cool for kids not too

rather than issuing a letter of ... rather than

there is no explanation for there is no

Table 2: Arguing annotations from the MPQA corpus and

their corresponding trigger expressions

by extracting the starting n-grams from the MPQA

annotations. The process of creating the lexicon is

as follows:

1. Generate a candidate Set from the annotations

in the corpus. Three candidates are extracted

from the stemmed version of each annotation:

the first word, the bigram starting at the first

word, and the trigram starting at the first word.

For example, if the annotation is “can only rise

to meet it by making some radical changes”,

the following candidates are extracted from it:

“can”, “can only” and “can only rise”.

2. Remove the candidates that are present in the

sentiment lexicon from (Wilson et al., 2005) (as

these are already accounted for in previous re-

search). For example, “actually”, which is a

trigger word in Table 3, is a neutral subjectivity

clue in the lexicon.

3. For each candidate in the candidate Set,

find the likelihood that it is a reliable indi-

cator of positive or negative arguing in the

MPQA corpus. These are likelihoods of the

form: P (positive arguing|candidate) =
#candidate is in a positive arguing span

#candidate is in the corpus

and P (negative arguing|candidate) =
#candidate is in a negative arguing span

#candidate is in the corpus

4. Make a lexicon entry for each candidate con-

sisting of the stemmed text and the two proba-

bilities described above.

This process results in an arguing lexicon

with 3762 entries, where 3094 entries have

119



P (positive arguing|candidate) > 0; and 668
entries have P (negative arguing|candidate) > 0.
Table 3 lists select interesting expressions from the

arguing lexicon.

Entries indicative of Positive Arguing

be important to, would be better, would need to, be just the, be

the true, my opinion, the contrast, show the, prove to be, only

if, on the verge, ought to, be most, youve get to, render, man-

ifestation, ironically, once and for, no surprise, overwhelming

evidence, its clear, its clear that, it be evident, it be extremely,

it be quite, it would therefore

Entries indicative of Negative Arguing

be not simply, simply a, but have not, can not imagine, we dont

need, we can not do, threat against, ought not, nor will, never

again, far from be, would never, not completely, nothing will,

inaccurate and, inaccurate and, find no, no time, deny that

Table 3: Examples of positive argu-

ing (P (positive arguing|candidate) >
P (negative arguing|candidate)) and negative
arguing (P (negative arguing|candidate) >
P (positive arguing|candidate))from the arguing
lexicon

4 Features for Stance Classification

We construct opinion target pair features, which are

units that capture the combined information about

opinions and targets. These are encoded as binary

features into a standard machine learning algorithm.

4.1 Arguing-based Features

We create arguing features primarily from our ar-

guing lexicon. We construct additional arguing fea-

tures using modal verbs and syntactic rules. The lat-

ter are motivated by the fact that modal verbs such

as “must”, “should” and “ought” are clear cases of

arguing, and are often involved in simple syntactic

patterns with clear targets.

4.1.1 Arguing-lexicon Features

The process for creating features for a post using

the arguing lexicon is simple. For each sentence in

the post, we first determine if it contains a positive or

negative arguing expression by looking for trigram,

bigram and unigram matches (in that order) with the

arguing lexicon. We prevent the same text span from

matching twice – once a trigram match is found, a

substring bigram (or unigram) match with the same

text span is avoided. If there are multiple arguing ex-

pression matches found within a sentence, we deter-

mine the most prominent arguing polarity by adding

up the positive arguing probabilities and negative ar-

guing probabilities (provided in the lexicon) of all

the individual expressions.

Once the prominent arguing polarity is deter-

mined for a sentence, the prefix ap (arguing positive)

or an (arguing negative) is attached to all the content

words in that sentence to construct opinion-target

features. In essence, all content words (nouns, verbs,

adjectives and adverbs) in the sentence are assumed

to be the target. Arguing features are denoted as ap-

target (positive arguing toward target) and an-target

(negative arguing toward target).

4.1.2 Modal Verb Features for Arguing

Modals words such as “must” and “should” are

usually good indicators of arguing. This is a small

closed set. Also, the target (what the arguing is

about) is syntactically associated with the modal

word, which means it can be relatively accurately

extracted by using a small set of syntactic rules.

For every modal detected, three features are cre-

ated by combining the modal word with its subject

and object. Note that all the different modals are

replaced by “should” while creating features. This

helps to create more general features. For exam-

ple, given a sentence “They must be available to

all people”, the method creates three features “they

should”, “should available” and “they should avail-

able”. These patterns are created independently of

the arguing lexicon matches, and added to the fea-

ture set for the post.

4.2 Sentiment-based Features

Sentiment-based features are created independent of

arguing features. In order to detect sentiment opin-

ions, we use a sentiment lexicon (Wilson et al.,

2005). In addition to positive (+) and negative (−)

words, this lexicon also contains subjective words

that are themselves neutral (=) with respect to po-

larity. Examples of neutral entries are “absolutely”,

“amplify”, “believe”, and “think”.

We find the sentiment polarity of the entire sen-

tence and assign this polarity to each content word in

the sentence (denoted, for example, as target+). In

order to detect the sentence polarity, we use the Vote

120



and Flip algorithm from Choi and Cardie (2009).

This algorithm essentially counts the number of pos-

itive, negative and neutral lexicon hits in a given ex-

pression and accounts for negator words. The algo-

rithm is used as is, except for the default polarity

assignment (as we do not know the most prominent

polarity in the corpus). Note that the Vote and Flip

algorithm has been developed for expressions but we

employ it on sentences. Once the polarity of a sen-

tence is determined, we create sentiment features for

the sentence. This is done for all sentences in the

post.

5 Experiments

Experiments are carried out on debate posts from the

following four domains: Gun Rights, Gay Rights,

Abortion, and Creationism. For each domain, a cor-

pus with equal class distribution is created as fol-

lows: we merge all debates and sample instances

(posts) from the majority class to obtain equal num-

bers of instances for each stance. This gives us a

total of 2232 posts in the corpus: 306 posts for the

Gun Rights domain, 846 posts for the Gay Rights

domain, 550 posts for the Abortion domain and 530

posts for the Creationism domain.

Our first baseline is a distribution-based baseline,

which has an accuracy of 50%. We also construct

Unigram, a system based on unigram content infor-

mation, but no explicit opinion information. Un-

igrams are reliable for stance classification in po-

litical domains (as seen in (Lin et al., 2006; Kim

and Hovy, 2007)). Intuitively, evoking a particular

topic can be indicative of a stance. For example,

a participant who chooses to speak about “child”

and “life” in an abortion debate is more likely from

an against-abortion side, while someone speaking

about “woman”, “rape” and “choice” is more likely

from a for-abortion stance.

We construct three systems that use opinion in-

formation: The Sentiment system that uses only the

sentiment features described in Section 4.2, the Ar-

guing system that uses only arguing features con-

structed in Section 4.1, and the Arg+Sent system

that uses both sentiment and arguing features.

All systems are implemented using a standard im-

plementation of SVM in the Weka toolkit (Hall et

al., 2009). We measure performance using the accu-

racy metric.

5.1 Results

Table 4 shows the accuracy averaged over 10 fold

cross-validation experiments for each domain. The

first row (Overall) reports the accuracy calculated

over all 2232 posts in the data.

Overall, we notice that all the supervised systems

perform better than the distribution-based baseline.

Observe that Unigram has a better performance than

Sentiment. The good performance of Unigram indi-

cates that what participants choose to speak about is

a good indicator of ideological stance taking. This

result confirms previous researchers’ intuition that,

in general, political orientation is a function of “au-

thors’ attitudes over multiple issues rather than pos-

itive or negative sentiment with respect to a sin-

gle issue” (Pang and Lee, 2008). Nevertheless, the

Arg+Sent system that uses both arguing and senti-

ment features outperforms Unigram.

We performed McNemar’s test to measure the dif-

ference in system behaviors. The test was performed

on all pairs of supervised systems using all 2232

posts. The results show that there is a significant dif-

ference between the classification behavior of Uni-

gram and Arg+Sent systems (p < 0.05). The dif-
ference between classifications of Unigram and Ar-

guing approaches significance (p < 0.1). There is
no significant difference in the behaviors of all other

system pairs.

Moving on to detailed performance in each do-

main, we see that Unigram outperforms Sentiment

for all domains. Arguing and Arg+Sent outperform

Unigram for three domains (Guns, Gay Rights and

Abortion), while the situation is reversed for one do-

main (Creationism). We carried out separate t-tests

for each domain, using the results from each test fold

as a data point. Our results indicate that the perfor-

mance of Sentiment is significantly different from

all other systems for all domains. However there is

no significant difference between the performance of

the remaining systems.

5.2 Analysis

On manual inspection of the top features used by

the classifiers for discriminating the stances, we

found that there is an overlap between the content

words used by Unigram, Arg+Sent and Arguing. For

121



Domain (#posts) Distribution Unigram Sentiment Arguing Arg+Sent

Overall (2232) 50 62.50 55.02 62.59 63.93

Guns Rights (306) 50 66.67 58.82 69.28 70.59

Gay Rights (846) 50 61.70 52.84 62.05 63.71

Abortion (550) 50 59.1 54.73 59.46 60.55

Creationism (530) 50 64.91 56.60 62.83 63.96

Table 4: Accuracy of the different systems

example, in the Gay Rights domain, “understand”

and “equal” are amongst the top features in Uni-

gram, while “ap-understand” (positive arguing for

“understand”) and “ap-equal” are top features for

Arg+Sent.

However, we believe that Arg+Sent makes finer

and more insightful distinctions based on polarity of

opinions toward the same set of words. Table 5 lists

some interesting features in the Gay Rights domain

for Unigram and Arg+Sent. Depending on whether

positive or negative attribute weights were assigned

by the SVM learner, the features are either indicative

of for-gay rights or against-gay rights. Even though

the features for Unigram are intuitive, it is not ev-

ident if a word is evoked as, for example, a pitch,

concern, or denial. Also, we do not see a clear sep-

aration of the terms (for e.g., “bible” is an indicator

for against-gay rights while “christianity” is an indi-

cator for for-gay rights)

The arguing features from Arg+Sent seem to

be relatively more informative – positive arguing

about “christianity”, “corinthians”, “mormonism”

and “bible” are all indicative of against-gay rights

stance. These are indeed beliefs and concerns that

shape an against-gay rights stance. On the other

hand, negative arguings with these same words de-

note a for-gay rights stance. Presumably, these oc-

cur in refutations of the concerns influencing the op-

posite side. Likewise, the appeal for equal rights

for gays is captured positive arguing about “liberty”,

“independence”, “pursuit” and “suffrage”.

Interestingly, we found that our features also cap-

ture the ideas of opinion variety and same and alter-

native targets as defined in previous research (So-

masundaran et al., 2008) – in Table 5, items that

are similar (e.g., “christianity” and “corinthians”)

have similar opinions toward them for a given stance

(for e.g., ap-christianity and ap-corinthians belong

to against-gay rights stance while an-christianity and

an-corinthians belong to for-gay rights stance). Ad-

ditionally, items that are alternatives (e.g. “gay” and

“heterosexuality”) have opposite polarities associ-

ated with them for a given stance, that is, positive

arguing for “heterosexuality” and negative arguing

for “gay” reveal the the same stance.

In general, unigram features associate the choice

of topics with the stances, while the arguing features

can capture the concerns, defenses, appeals or de-

nials that signify each side (though we do not ex-

plicitly encode these fine-grained distinctions in this

work). Interestingly, we found that sentiment fea-

tures in Arg+Sent are not as informative as the argu-

ing features discussed above.

6 Related Work

Generally, research in identifying political view-

points has employed information from words in the

document (Malouf and Mullen, 2008; Mullen and

Malouf, 2006; Grefenstette et al., 2004; Laver et al.,

2003; Martin and Vanberg, 2008; Lin et al., 2006;

Lin, 2006). Specifically, Lin et al. observe that peo-

ple from opposing perspectives seem to use words

in differing frequencies. On similar lines, Kim and

Hovy (2007) use unigrams, bigrams and trigrams for

election prediction from forum posts. In contrast,

our work specifically employs sentiment-based and

arguing-based features to perform stance classifica-

tion in political debates. Our experiments are fo-

cused on determining how different opinion expres-

sions reinforce an overall political stance. Our re-

sults indicate that while unigram information is re-

liable, further improvements can be achieved in cer-

tain domains using our opinion-based approach. Our

work is also complementary to that by Greene and

Resnik (2009), which focuses on syntactic packag-

ing for recognizing perspectives.

122



For Gay Rights Against Gay Rights

Unigram Features

constitution, fundamental, rights, suffrage, pursuit, discrimina-

tion, government, happiness, shame, wed, gay, heterosexual-

ity, chromosome, evolution, genetic, christianity, mormonism,

corinthians, procreate, adopt

pervert, hormone, liberty, fidelity, naval, retarded, orientation, pri-

vate, partner, kingdom, bible, sin, bigot

Arguing Features from Arg+Sent

ap-constitution, ap-fundamental, ap-rights, ap-hormone,

ap-liberty, ap-independence, ap-suffrage, ap-pursuit, ap-

discrimination, an-government, ap-fidelity, ap-happiness,

an-pervert, an-naval, an-retarded, an-orientation, an-shame,

ap-private, ap-wed, ap-gay, an-heterosexuality, ap-partner,

ap-chromosome, ap-evolution, ap-genetic, an-kingdom, an-

christianity, an-mormonism, an-corinthians, an-bible, an-sin,

an-bigot, an-procreate, ap-adopt,

an-constitution, an-fundamental, an-rights, an-hormone,

an-liberty, an-independence, an-suffrage, an-pursuit, an-

discrimination, ap-government, an-fidelity, an-happiness,

ap-pervert, ap-naval, ap-retarded, ap-orientation, ap-shame,

an-private, an-wed, an-gay, ap-heterosexuality, an-partner,

an-chromosome, an-evolution, an-genetic, ap-kingdom, ap-

christianity, ap-mormonism, ap-corinthians, ap-bible, ap-sin,

ap-bigot, ap-procreate, an-adopt

Table 5: Examples of features associated with the stances in Gay Rights domain

Discourse-level participant relation, that is,

whether participants agree/disagree has been found

useful for determining political side-taking (Thomas

et al., 2006; Bansal et al., 2008; Agrawal et

al., 2003; Malouf and Mullen, 2008). Agree-

ment/disagreement relations are not the main focus

of our work. Other work in the area of polarizing po-

litical discourse analyze co-citations (Efron, 2004)

and linking patterns (Adamic and Glance, 2005). In

contrast, our focus is on document content and opin-

ion expressions.

Somasundaran et al. (2007b) have noted the use-

fulness of the arguing category for opinion QA. Our

tasks are different; they use arguing to retrieve rele-

vant answers, but not distinguish stances. Our work

is also different from related work in the domain of

product debates (Somasundaran and Wiebe, 2009)

in terms of the methodology.

Wilson (2007) manually adds positive/negative

arguing information to entries in a sentiment lexi-

con from (Wilson et al., 2005) and uses these as ar-

guing features. Our arguing trigger expressions are

separate from the sentiment lexicon entries and are

derived from a corpus. Our n-gram trigger expres-

sions are also different from manually created regu-

lar expression-based arguing lexicon for speech data

(Somasundaran et al., 2007a).

7 Conclusions

In this paper, we explore recognizing stances in ide-

ological on-line debates. We created an arguing lex-

icon from the MPQA annotations in order to recog-

nize arguing, a prominent type of linguistic subjec-

tivity in ideological stance taking. We observed that

opinions or targets in isolation are not as informative

as their combination. Thus, we constructed opinion

target pair features to capture this information.

We performed supervised learning experiments

on four different domains. Our results show that

both unigram-based and opinion-based systems per-

form better than baseline methods. We found that,

even though our sentiment-based system is able to

perform better than the distribution-based baseline,

it does not perform at par with the unigram system.

However, overall, our arguing-based system does as

well as the unigram-based system, and our system

that uses both arguing and sentiment features obtains

further improvement. Our feature analysis suggests

that arguing features are more insightful than uni-

gram features, as they make finer distinctions that

reveal the underlying ideologies.

References

Lada A. Adamic and Natalie Glance. 2005. The political

blogosphere and the 2004 u.s. election: Divided they

blog. In LinkKDD.

Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan

Srikant, and Yirong Xu. 2003. Mining newsgroups

using networks arising from social behavior. In WWW.

Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.

The power of negative thinking: Exploiting label dis-

agreement in the min-cut classification framework. In

123



Proceedings of the 22nd International Conference on

Computational Linguistics (COLING-2008).

Yejin Choi and Claire Cardie. 2009. Adapting a polarity

lexicon using integer linear programming for domain-

specific sentiment classification. In Proceedings of

the 2009 Conference on Empirical Methods in Natu-

ral Language Processing, pages 590–598, Singapore,

August. Association for Computational Linguistics.

Miles Efron. 2004. Cultural orientation: Classifying

subjective documents by cocitation analysis. In AAAI

Fall Symposium on Style and Meaning in Language,

Art, and Music.

Stephan Greene and Philip Resnik. 2009. More than

words: Syntactic packaging and implicit sentiment. In

Proceedings of Human Language Technologies: The

2009 Annual Conference of the North American Chap-

ter of the Association for Computational Linguistics,

pages 503–511, Boulder, Colorado, June. Association

for Computational Linguistics.

Gregory Grefenstette, Yan Qu, James G. Shanahan, and

David A. Evans. 2004. Coupling niche browsers and

affect analysis for an opinion mining application. In

Proceeding of RIAO-04, Avignon, FR.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard

Pfahringer, Peter Reutemann, and Ian H. Witten.

2009. The weka data mining software: An update. In

SIGKDD Explorations, Volume 11, Issue 1.

Soo-Min Kim and Eduard Hovy. 2007. Crystal: Ana-

lyzing predictive opinions on the web. In Proceedings

of the 2007 Joint Conference on Empirical Methods

in Natural Language Processing and Computational

Natural Language Learning (EMNLP-CoNLL), pages

1056–1064.

Michael Laver, Kenneth Benoit, and John Garry. 2003.

Extracting policy positions from political texts using

words as data. American Political Science Review,

97(2):311–331.

Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and

Alexander Hauptmann. 2006. Which side are you

on? Identifying perspectives at the document and sen-

tence levels. In Proceedings of the 10th Conference on

Computational Natural Language Learning (CoNLL-

2006), pages 109–116, New York, New York.

Wei-Hao Lin. 2006. Identifying perspectives at the doc-

ument and sentence levels using statistical models. In

Proceedings of the Human Language Technology Con-

ference of the NAACL, Companion Volume: Doctoral

Consortium, pages 227–230, New York City, USA,

June. Association for Computational Linguistics.

Robert Malouf and Tony Mullen. 2008. Taking sides:

Graph-based user classification for informal online po-

litical discourse. Internet Research, 18(2).

Lanny W. Martin and Georg Vanberg. 2008. A ro-

bust transformation procedure for interpreting political

text. Political Analysis, 16(1):93–100.

Tony Mullen and Robert Malouf. 2006. A preliminary

investigation into sentiment analysis of informal po-

litical discourse. In AAAI 2006 Spring Symposium

on Computational Approaches to Analysing Weblogs

(AAAI-CAAW 2006).

Bo Pang and Lillian Lee. 2008. Opinion mining and

sentiment analysis. Foundations and Trends in Infor-

mation Retrieval, Vol. 2(1-2):pp. 1–135.

Swapna Somasundaran and Janyce Wiebe. 2009. Rec-

ognizing stances in online debates. In Proceedings

of the Joint Conference of the 47th Annual Meeting

of the ACL and the 4th International Joint Conference

on Natural Language Processing of the AFNLP, pages

226–234, Suntec, Singapore, August. Association for

Computational Linguistics.

Swapna Somasundaran, Josef Ruppenhofer, and Janyce

Wiebe. 2007a. Detecting arguing and sentiment in

meetings. In SIGdial Workshop on Discourse and Di-

alogue, Antwerp, Belgium, September.

Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,

and Veselin Stoyanov. 2007b. Qa with attitude: Ex-

ploiting opinion type analysis for improving question

answering in on-line discussions and the news. In In-

ternational Conference on Weblogs and Social Media,

Boulder, CO.

Swapna Somasundaran, Janyce Wiebe, and Josef Rup-

penhofer. 2008. Discourse level opinion interpreta-

tion. In Proceedings of the 22nd International Con-

ference on Computational Linguistics (Coling 2008),

pages 801–808, Manchester, UK, August.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out

the vote: Determining support or opposition from con-

gressional floor-debate transcripts. In Proceedings of

the 2006 Conference on Empirical Methods in Natural

Language Processing, pages 327–335, Sydney, Aus-

tralia, July. Association for Computational Linguistics.

Theresa Wilson and Janyce Wiebe. 2005. Annotating

attributions and private states. In Proceedings of ACL

Workshop on Frontiers in Corpus Annotation II: Pie in

the Sky.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.

2005. Recognizing contextual polarity in phrase-level

sentiment analysis. In hltemnlp2005, pages 347–354,

Vancouver, Canada.

Theresa Wilson. 2007. Fine-grained Subjectivity and

Sentiment Analysis: Recognizing the Intensity, Polar-

ity, and Attitudes of private states. Ph.D. thesis, Intel-

ligent Systems Program, University of Pittsburgh.

124


