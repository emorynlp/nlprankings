Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788–796,

Beijing, August 2010

788

Entity-Focused Sentence Simpliﬁcation for Relation Extraction

Makoto Miwa1 Rune Sætre1 Yusuke Miyao2

Jun’ichi Tsujii1,3,4

1Department of Computer Science, The University of Tokyo

2National Institute of Informatics

3School of Computer Science, University of Manchester

4National Center for Text Mining

mmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,

yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jp

Abstract

Relations between entities in text have
been widely researched in the natu-
ral language processing and information-
extraction communities. The region con-
necting a pair of entities (in a parsed
sentence) is often used to construct ker-
nels or feature vectors that can recognize
and extract interesting relations. Such re-
gions are useful, but they can also incor-
porate unnecessary distracting informa-
tion.
In this paper, we propose a rule-
based method to remove the information
that is unnecessary for relation extraction.
Protein–protein interaction (PPI) is used
as an example relation extraction problem.
A dozen simple rules are deﬁned on out-
put from a deep parser. Each rule specif-
ically examines the entities in one target
interaction pair. These simple rules were
tested using several PPI corpora. The PPI
extraction performance was improved on
all the PPI corpora.

1 Introduction

Relation extraction (RE) is the task of ﬁnding a
relevant semantic relation between two given tar-
get entities in a sentence (Sarawagi, 2008). Some
example relation types are person–organization
relations (Doddington et al., 2004), protein–
protein interactions (PPI), and disease–gene as-
sociations (DGA) (Chun et al., 2006). Among
the possible RE tasks, we chose the PPI extrac-
tion problem. PPI extraction is a major RE task;

around 10 corpora have been published for train-
ing and evaluation of PPI extraction systems.

Recently, machine-learning methods, boosted
by NLP techniques, have proved to be effec-
tive for RE. These methods are usually intended
to highlight or select the relation-related regions
in parsed sentences using feature vectors or ker-
nels. The shortest paths between a pair of enti-
ties (Bunescu and Mooney, 2005) or pair-enclosed
trees (Zhang et al., 2006) are widely used as focus
regions. These regions are useful, but they can in-
clude unnecessary sub-paths such as appositions,
which cause noisy features.

In this paper, we propose a method to remove
information that is deemed unnecessary for RE.
Instead of selecting the whole region between
a target pair,
the target sentence is simpliﬁed
into simpler, pair-related, sentences using general,
task-independent, rules. By addressing particu-
larly the target entities, the rules do not affect im-
portant relation-related expressions between the
target entities. We show how rules of two groups
can be easily deﬁned using the analytical capabil-
ity of a deep parser with speciﬁc examination of
the target entities. Rules of the ﬁrst group can re-
place a sentence with a simpler sentence, still in-
cluding the two target entities. The other group of
rules can replace a large region (phrase) represent-
ing one target entity, with just a simple mention of
that target entity. With only a dozen simple rules,
we show that we can solve several simple well-
known problems in RE, and that we can improve
the performance of RE on all corpora in our PPI
test-set.

789

2 Related Works

The general paths, such as the shortest path or
pair-enclosed trees (Section 1), can only cover
a part of the necessary information for relation
extraction. Recent machine-learning methods
speciﬁcally examine how to extract the missing
information without adding too much noise. To
ﬁnd more representative regions, some informa-
tion from outside the original regions must be
included. Several tree kernels have been pro-
posed to extract such regions from the parse
structure (Zhang et al., 2006). Also the graph
kernel method emphasizes internal paths with-
out ignoring outside information (Airola et al.,
2008). Composite kernels have been used to com-
bine original information with outside informa-
tion (Zhang et al., 2006; Miwa et al., 2009).

The approaches described above are useful,
but they can include unnecessary information that
distracts learning.
Jonnalagadda and Gonzalez
(2009) applied bioSimplify to relation extraction.
BioSimplify is developed to improve their link
grammar parser by simplifying the target sentence
in a general manner, so their method might re-
move important information for a given target re-
lation. For example, they might accidentally sim-
plify a noun phrase that is needed to extract the
relation. Still, they improved overall PPI extrac-
tion recall using such simpliﬁcations.

To remove unnecessary information from a sen-
tence, some works have addressed sentence sim-
pliﬁcation by iteratively removing unnecessary
phrases. Most of this work is not task-speciﬁc;
it is intended to compress all information in a tar-
get sentence into a few words (Dorr et al., 2003;
Vanderwende et al., 2007). Among them, Vickrey
and Koller (2008) applied sentence simpliﬁcation
to semantic role labeling. With retaining all argu-
ments of a verb, Vickrey simpliﬁed the sentence
by removing some information outside of the verb
and arguments.

3 Entity-Focused Sentence

Simpliﬁcation

We simplify a target sentence using simple rules
applicable to the output of a deep parser called
Mogura (Matsuzaki et al., 2007), to remove noisy

information for relation extraction. Our method
relies on the deep parser; the rules depend on the
Head-driven Phrase Structure Grammar (HPSG)
used by Mogura, and all the rules are written for
the parser Enju XML output format. The deep
parser can produce deep syntactic and semantic
information, so we can deﬁne generally applica-
ble comprehensive rules on HPSG with speciﬁc
examination of the entities.

For sentence simpliﬁcation in relation extrac-
tion, the meaning of the target sentence itself is
less important than maintaining the truth-value of
the relation (interact or not). For that purpose,
we deﬁne rules of two groups: clause-selection
rules and entity-phrase rules. A clause-selection
rule constructs a simpler sentence (still includ-
ing both target entities) by removing noisy infor-
mation before and after the relevant clause. An
entity-phrase rule simpliﬁes an entity-containing
region without changing the truth-value of the re-
lation. By addressing the target entities particu-
larly, we can deﬁne rules for many applications,
and we can simplify target sentences with less
danger of losing relation-related mentions. The
rules are summarized in Table 1.

Our method is different from the sentence sim-
pliﬁcation in other systems (ref. Section 2). First,
our method relies on the parser, while bioSimplify
by Jonnalagadda and Gonzalez (2009) is devel-
oped for the improvement of their parser. Second,
our method tries to keep only the relation-related
regions, unlike other general systems including
bioSimplify which tried to keep all information in
a sentence. Third, our entity-phrase rules modify
only the entity-containing phrases, while Vickrey
and Koller (2008) tries to remove all information
outside of the target verb and arguments.

3.1 Clause-selection Rules

In compound or complex sentences, it is natural
to assume that one clause includes both the target
entities and the relation-related information. It can
also be assumed that the remaining sentence parts,
outside the clause, contain less related (or noisy)
information. The clause-selection rules simplify a
sentence by retaining only the clause that includes
the target entities (and by discarding the remain-
der of the sentence). We deﬁne three types of

790

Rule Group

Rule Type

Clause Selection

Sentence Clause
Relative Clause

Copula

Entity Phrase

Apposition

Exempliﬁcation

Parentheses
Coordination

Example (original → simpliﬁed )

... A that interacts with B. → A interacts with B.

#
1 We show that A interacts with B. → A interacts with B.
2
1 A is a protein that interacts with B. → A interacts with B.
2
4
2
3

a protein, A → A
a protein (A) → A
protein and A → A

proteins, such as A → A

Table 1: Rules for Sentence Simpliﬁcation. (# is the rule count. A and B are the target entities.)

(a)

S
\\\\\\\
bbbbbbb
VP
...
\\\\\\\
bbbbbbb
V
...
N*
\\\\\\\
[[[[[
bbbbbbb
ccccc
N*
... ENTITY ...
bbbbbbb

(copular)

NP-REL

S-REL

\\\\\\\
...
[[[[[
ccccc
... ENTITY ...

A

is a

protein

that

interacts with B .

(b)

S
\\\\\\\
bbbbbbb
...
N*
[[[[[
[[[[[
ccccc
ccccc
... ENTITY ...
... ENTITY ...

A

interacts with B .

Figure 1: Copula Rule.
(a) is simpliﬁed to (b).
The arrows represent predicate–argument rela-
tions.

(a)

N*
\\\\\\\
bbbbbbb
N*
...
]]]]]]]]]]]]]
bbbbbbb
PN
N*
(apposition)
[[[[[
ccccc
... ENTITY ...

(b)

N*
[[[[[
ccccc
... ENTITY ...

A

protein

,

A

Figure 2: Apposition Rule.

clause-selection rules for sentence clauses, rela-
tive clauses, and copula. The sentence clause rule
ﬁnds the (smallest) clause that includes both tar-
get entities. It then replaces the original sentence
with the clause. The relative clause rules con-
struct a simple sentence from a relative clause and
the antecedent.
If this simple sentence includes
the target entities, it is used instead of the orig-
inal sentence. We deﬁne two rules for the case
where the antecedent is the subject of the relative
clause. One rule is used when the relative clause
includes both the target entities. The other rule is
used when the antecedent includes one target en-
tity and the relative clause includes the other tar-
get entity. The copula rule is for sentences that

include copular verbs (e.g. be, is, become, etc).
The rule constructs a simple sentence from a rel-
ative clause with the subject of the copular verb
as the antecedent subject of the clause. The rule
replaces the target sentence with the constructed
sentence, if the relative clause includes one target
entity and the subject of a copular verb includes
the other target entity, as shown in Figure 1.

3.2 Entity-phrase Rules

Even the simple clauses (or paths between two
target entities) include redundant or noisy expres-
sions that can distract relation extraction. Some
of these expressions are related to the target enti-
ties, but because they do not affect the truth-value
of the relation, they can be deleted to make the
path simple and clear. The target problem affects
which expressions can be removed. We deﬁne
four types of rules for appositions, exempliﬁca-
tions, parentheses, and coordinations. Two appo-
sition rules are deﬁned to select the correct ele-
ment from an appositional expression. One ele-
ment modiﬁes or deﬁnes the other element in ap-
position, but the two elements represent the same
information from the viewpoint of PPI. If the tar-
get entity is in one of these elements, removing the
other element does not affect the truth-value of the
interaction. Many of these apposition expressions
are identiﬁed by the deep parser. The rule to se-
lect the last element is presented in Figure 2. Four
exempliﬁcation rules are deﬁned for the two ma-
jor types of expressions using the phrases “includ-
ing” or “such as”. Exempliﬁcation is represented
by hyponymy or hypernymy. As for appositions,
the truth-value of the interaction does not change
whether we use the speciﬁc mention or the hyper-
class that the mention represents. Two parenthe-
ses rules are deﬁned. Parentheses are useful for
synonyms, hyponyms, or hypernyms (ref. the two

c
c
7
7
N
N
R
R
5
5
791

reset rules {apply all the rules again}
P ← parse S
repeat

r ← next rule {null if no more rules}
if r is applicable to P then

P ← apply r to P
S ← sentence extracted from P
break (Goto 3)

1: S ← input sentence
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: until r is null
14: return S

until r is null

end if

Figure 3: Pseudo-code for sentence simpliﬁca-
tion.

former rules). Three coordination rules are de-
ﬁned. Removing other phrases from coordinated
expressions that include a target entity does not
affect the truth-value of the target relation. Two
rules are deﬁned for simple coordination between
two phrases (e.g. select left or right phrase), and
one rule is deﬁned to (recursively) remove one
element from lists of more than two coordinated
phrases (while maintaining the coordinating con-
junction, e.g. “and”).

3.3 Sentence Simpliﬁcation
To simplify a sentence, we apply rules repeatedly
until no more applications are possible as pre-
sented in Figure 3. After one application of one
rule, the simpliﬁed sentence is re-parsed before
attempting to apply all the rules again. This is be-
cause we require a consistent parse tree as a start-
ing point for additional applications of the rules,
and because a parser can produce more reliable
output for a partly simpliﬁed sentence than for the
original sentence. Using this method, we can also
backtrack and seek out conversion errors by exam-
ining the cascade of partly simpliﬁed sentences.

4 Evaluation

To elucidate the effect of the sentence simpliﬁ-
cation, we applied the rules to ﬁve PPI corpora
and evaluated the PPI extraction performance. We
then analyzed the errors. The evaluation settings
will be explained in Section 4.1. The results of the
PPI extraction will be explained in Section 4.2. Fi-
nally, the deeper analysis results will be presented

in Section 4.3.

extraction

state-of-the-art PPI

4.1 Experimental Settings
The
system
AkaneRE by Miwa et al. (2009) was used to
evaluate our approach. The system uses a com-
bination of three feature vectors: bag-of-words
(BOW), shortest path (SP), and graph features.
Classiﬁcation models are trained with a support
vector machine (SVM), and AkaneRE (with
Mogura) is used with default parameter settings.
The following two systems are used for a state-
of-the-art comparison: AkaneRE with multiple
parsers and corpora (Miwa et al., 2009), and
Airola et al. (2008) single-parser, single-corpus
system.

The rules were evaluated on the BioIn-
fer (Pyysalo et al., 2007), AIMed (Bunescu et al.,
2005), IEPA (Ding et al., 2002), HPRD50 (Fun-
del et al., 2006), and LLL (N´edellec, 2005) cor-
pora1. Table 2 shows the number of positive (in-
teracting) vs. all pairs. One duplicated abstract in
the AIMed corpus was removed.

These corpora have several differences in their
deﬁnition of entities and relations (Pyysalo et al.,
2008). In fact, BioInfer and AIMed target all oc-
curring entities related to the corpora (proteins,
genes, etc). On the other hand, IEPA, HPRD50,
and LLL only use limited named entities, based
either on a list of entity names or on a named en-
tity recognizer. Only BioInfer is annotated for
other event types in addition to PPI, including
static relations such as protein family member-
ship. The sentence lengths are also different. The
duplicated pair-containing sentences contain the
following numbers of words on average: 35.8 in
BioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 in
HPRD50, and 33.4 in LLL.

For BioInfer, AIMed, and IEPA, each corpus is
split into training, development, and test datasets2.
The training dataset from AIMed was the only
training dataset used for validating the rules. The
development datasets are used for error analysis.
The evaluation was done on the test dataset, with
models trained using training and development

1http://mars.cs.utu.fi/PPICorpora/

GraphKernel.html

2This split method will be made public later.

792

BioInfer
pos
1,848
256
425
2,534

all
7,108
928
1,618
9,653

training

development

test
all

AIMed

IEPA

pos
684
102
194
980

all
4,072
608
1,095
5,775

pos
256
23
56
335

all
630
51
136
817

HPRD50
pos
all
-
-
-
-
-
-
433
163

LLL

pos
-
-
-
164

all
-
-
-
330

Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.

Rule

No Application
Clause Selection

Entity Phrase

ALL

BioInfer

Applied
0
4,313
22,066
26,281

F

62.5
63.5
60.5
62.9

AUC
83.0
83.9
80.9
82.1

Applied
0
2,569
7,784
10,783

AIMed

F

61.2
62.5
61.2
60.2

AUC
87.9
88.2
86.1
85.7

Applied
0
307
1,031
1,343

IEPA
F

73.4
75.0
72.7
75.4

AUC
82.5
83.7
83.3
85.7

Table 3: Performance of PPI Extraction on test datasets. “Applied” represents the number of times the
rules are applied on the corpus. “No Application” means PPI extraction without sentence simpliﬁcation.
ALL is the case all rules are used. The top scores for each corpus are shown in bold.

datasets). Ten-fold cross-validation (CV) was
done to facilitate comparison with other existing
systems. For HPRD50 and LLL, there are insuf-
ﬁcient examples to split the data, so we use these
corpora only for comparing the scores and statis-
tics. We split the corpora for the CV, and mea-
sured the F -score (%) and area under the receiver
operating characteristic (ROC) curve (AUC) as
recommended in (Airola et al., 2008). We count
each occurrence as one example because the cor-
rect interactions must be extracted for each occur-
rence if the same protein name occurs multiple
times in a sentence.

In the experiments, the rules are applied in the
following order:
sentence–clause, exempliﬁca-
tion, apposition, parentheses, coordination, cop-
ula, and relative-clause rules. Furthermore, if the
same rule is applicable in different parts of the
parse tree, then the rule is ﬁrst applied closest to
the leaf-nodes (deepest ﬁrst). The order of the
rules is arbitrary; changing it does not affect the
results much. We conducted ﬁve experiments us-
ing the training and development dataset in IEPA,
each time with a random shufﬂing of the order of
the rules; the results were 77.8±0.26 in F -score
and 85.9±0.55 in AUC.
4.2 Performance of PPI Extraction

The performance after rule application was bet-
ter than the baseline (no application) on all the
corpora, and most rules could be frequently ap-
plied. We show the PPI extraction performance on

Rule

No Application
Sentence Clause
Relative Clause

Copula

Clause Selection

Apposition

Exempliﬁcation

Parentheses
Coordination
Entity Phrase

ALL

Applied
0
145
7
0
152
64
33
90
417
605
763

F

72.9
71.6
73.3
72.9
71.4
73.2
72.9
72.9
73.6
74.1
75.0

AUC
84.5
83.8
84.1
84.5
83.4
84.6
84.7
85.1
85.4
86.6
86.6

Table 4: Performance of PPI Extraction on
HPRD50.

Rule

No Application
Sentence Clause
Relative Clause

Copula

Clause Selection

Apposition

Exempliﬁcation

Parentheses
Coordination
Entity Phrase

ALL

Applied
0
135
42
0
178
197
0
56
322
602
761

F

79.0
81.3
78.8
79.0
81.0
79.6
79.0
79.5
84.2
83.8
82.9

AUC
84.6
85.2
84.6
84.6
85.6
83.9
84.6
85.8
89.4
90.1
90.5

Table 5: Performance of PPI Extraction on LLL.

BioInfer, AIMed, and IEPA with rules of different
groups in Table 3. The effect of using rules of
different types for PPI extraction from HPRD50
and LLL is reported in Table 4 and Table 5. Ta-
ble 6 shows the number of times each rule was
applied in an “apply all-rules” experiment. The
usability of the rules depends on the corpus, and
different combinations of rules produce different

793

Rule
S. Cl.
R. Cl.
Copula
Cl. Sel.
Appos.
Exempl.
Paren.
Coord.
E. Foc.
Sum

B AIMed
2,346
212
57
2,615
1,100
127
2,158
4,783
8,168
10,783

3,960
287
60
4,307
3,845
383
2,721
15,025
21,974
26,281

IEPA
300
17
1
318
99
11
235
680
1,025
1,343

H LLL
135
24
0
159
198
0
88
316
602
761

150
5
0
155
69
33
91
415
608
763

Table 6: Distribution of the number of rules ap-
plied when all rules are applied. B:BioInfer, and
H:HPRD50 corpora.

Rules

F

AUC
79.8
83.7
88.7
86.6
90.5

Miwa et al.
AUC
F
86.4
89.3
87.8
87.9
90.8

68.3
65.2
76.6
74.9
86.7

Airola et al.
AUC
F
81.9
84.8
85.1
79.7
83.4

61.3
56.4
75.1
63.4
76.8

B
60.0
A 54.9
I
77.8
H 75.0
L
82.9

Table 7: Comparison with the results by Miwa et
al. (2009) and Airola et al. (2008). The results
with all rules are reported.

results. For the clause-selection rules, the per-
formance was as good as or better than the base-
line for all corpora, except for HPRD50, which
indicates that the pair-containing clauses also in-
clude most of the important information for PPI
extraction. Clause selection rules alone could im-
prove the overall performance for the BioInfer and
AIMed corpora. Entity-phrase rules greatly im-
proved the performance on the IEPA, HPRD50,
and LLL corpora, although these rules degraded
the performance on the BioInfer and AIMed cor-
pora. These phenomena hold even if we use small
parts of the two corpora, so this is not because of
the size of the corpora.

We compare our results with the results by
Miwa et al. (2009) and Airola et al. (2008) in Ta-
ble 7. On three of ﬁve corpora, our method pro-
vides better results than the state-of-the-art system
by Airola et al. (2008), and also provides com-
parable results to those obtained using multiple
parsers and corpora (Miwa et al., 2009) despite
the fact that our method uses one parser and one
corpus at a time. We cannot directly compare our
result with Jonnalagadda and Gonzalez (2009) be-
cause the evaluation scheme, the baseline system,

Parenthesis, Coordination] To
[FP→TN][Sentence,
characterize the AAV functions mediating this effect,
cloned AAV type 2 wild-type or mutant genomes were
transfected into simian virus 40 (SV40)-transformed
hamster cells together with the six HSV replication genes
(encoding UL5, UL8, major DNA-binding protein, DNA
polymerase, UL42 , and UL52) which together are
necessary and sufﬁcient for the induction of SV40 DNA
ampliﬁcation (R. Heilbronn and H. zur Hausen, J. Virol.
63:3683-3692, 1989). (BioInfer.d760.s0)
[TP→FN][Coordination] Both the GT155-calnexin and
the GT155-CAP-60 interactions were dependent on the
presence of a correctly modiﬁed oligosaccharide group
on GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)
[TN→TN][Coordination, Parenthesis] Leptin may act as
a negative feedback signal to the hypothalamic control of
appetite through suppression of neuropeptide Y (NPY)
secretion and stimulation of cocaine and amphetamine
regulated transcript (CART) . (IEPA.d190.s454)

Figure 4: A rule-related error, a critical error, and
a parser-related error. Regions removed by the
rules are underlined, and target proteins are shown
in bold. Predictions, applied rules, and sentence
IDs are shown.
[FN→TP][Sentence, Coordination] WASp contains a
binding motif for the Rho GTPase CDC42Hs as well as
verprolin / coﬁlin-like actin-regulatory domains , but no
speciﬁc actin structure regulated by CDC42Hs-WASp has
been identiﬁed. (BioInfer.d795.s0)
[FN→TP][Parenthesis, Apposition] The protein Raf-1 , a
key mediator of mitogenesis and differentiation, associates
with p21ras (refs 1-3) . (AIMed.d124.s1055)
[FN→TP][Sentence,
Parenthesis] On the basis of
far-Western blot and plasmon resonance (BIAcore)
experiments, we show here that recombinant bovine
prion protein (bPrP) (25-242) strongly interacts with the
catalytic alpha/alpha’ subunits of protein kinase CK2
(also termed ’casein kinase 2’) (IEPA.d197.s479)

Figure 5: Correctly simpliﬁed cases. The ﬁrst
sentence is a difﬁcult (not PPI) relation, which is
typed as “Similar” in the BioInfer corpus.

and test parts differ.

4.3 Analysis
We trained models using the training datasets
and classiﬁed the examples in the development
datasets. Two types of analysis were performed
based on these results: simpliﬁcation-based and
classiﬁcation-based analysis.

For the simpliﬁcation-based analysis, we com-
pared positive (interacting) and negative pair sen-
tences that produce the exact same (inconsistent)
sentence after protein names normalization and

794

Before simpliﬁcation
After simpliﬁcation

No Error

No Application
Number of Errors
Number of Pairs

Coordination

Sentence
Parenthesis

Exempliﬁcation

Apposition

BioInfer
FP
TP
TN FN
3
0
0
3
0
0
0
0
0

2
2
2
6
0
2
0
0
0

FN
TP
18
3
0
21
0
0
0
0
0

TN
FP
35
3
32
70
20
4
5
2
1

FN
TP
14
0
4
18
4
0
0
0
0

AIMed
FP
TP
TN FN
21
21
8
7
1
2
30
30
1
2
0
0
0
0
0
0
0
0

TN
FP
8
0
4
12
0
4
0
0
0

IEPA

FN
TP
3
0
0
3
0
0
0
0
0

FP
TP
TN FN
0
0
0
0
0
0
0
0
0

2
1
0
3
0
0
0
0
0

TN Not Affected
FP
32
4
7
1
0
1
40
5
1
0
0
0
0
0
0
0
0
0

Table 8: Distribution of sentence simpliﬁcation errors compared to unsimpliﬁed predictions with their
types (on the three development datasets). TP, True Positive; TN, True Negative; FN, False Negative;
FP, False Positive. “No Error” means that simpliﬁcation was correct; “No Application” means that no
rule could be applied; Other rule names mean that an error resulted from that rule application. “Not
Affected” means that the prediction outcome did not change.

simpliﬁcation in the training dataset. The numbers
of such inconsistent sentences are 7 for BioIn-
fer, 78 for AIMed, and 1 for IEPA. The few in-
consistencies in BioInfer and IEPA are from er-
rors by the rules, mainly triggered by parse errors.
The frequent inconsistencies in AIMed are mostly
from inconsistent annotations. For example, even
if all coordinated proteins are either interacting or
not, only the ﬁrst protein mention is annotated as
interacting.

For

the classiﬁcation-based analysis, we
speciﬁcally examine simpliﬁed pairs that were
predicted differently before and after the simpliﬁ-
cation. Pairs predicted differently before and after
rule application were selected: 100 random pairs
from BioInfer and all 90 pairs from AIMed. For
IEPA, all 51 pairs are reported. Simpliﬁed results
are classiﬁed as errors when the rules affect a re-
gion unrelated to the entities in the smallest sen-
tence clause. The results of analysis are shown in
Table 8. There were 34 errors in BioInfer, and 11
errors in AIMed. Among the errors, there were
ﬁve critical errors (in two sentences, in AIMed).
Critical errors mean that the pairs lost relation-
related mentions, and the errors are the only er-
rors which caused the changes in the truth-value
of the relation. There was also a rule-related er-
ror (in BioInfer), which means that rules with cor-
rect parse results affect a region unrelated to the
entities, and parse errors (parser-related errors).
Figure 4 shows the rule-related error in BioInfer,
one critical error in AIMed, and one parser-related

error in IEPA.

5 Discussion

Our end goal is to provide consistent relation
extraction for real tasks. Here we discuss the
“safety” of applying our simpliﬁcation rules, the
difﬁculties in the BioInfer and AIMed corpora, the
reduction of errors, and the requirements for such
a general (PPI) extraction system.

Our rules are applicable to sentences, with little
danger of changing the relation-related mentions.
Figure 5 shows three successfully simpliﬁed cases
(“No Error” cases from Table 8). The sentence
simpliﬁcation leaves sufﬁcient information to de-
termine the value of the relation in these exam-
ples. Relation-related mentions remained for most
of the simpliﬁcation error cases. There were only
ﬁve critical errors, which changed the truth-value
of the relation, out of 46 errors in 241 pairs shown
in Table 8. Please note that some rules can be
dangerous for other relation extraction tasks. For
example, the sentence clause rule could remove
modality information (negation, speculation, etc.)
modifying the clause, but there are few such cases
in the PPI corpora (see Table 8). Also, the task of
hedge detection (Morante and Daelemans, 2009)
can be solved separately, in the original sentences,
after the interacting pairs have been found. For
example, in the BioNLP shared task challenge
and the BioInfer corpus, interaction detection and
modality are treated as two different tasks. Once
other NLP tasks, like static relation (Pyysalo et

795

6 Conclusion

We proposed a method to simplify sentences, par-
ticularly addressing the target entities for relation
extraction. Using a few simple rules applicable
to the output of a deep parser called Mogura,
we showed that sentence simpliﬁcation is effec-
tive for relation extraction. Applying all the rules
improved the performance on three of the ﬁve
corpora, while applying only the clause-selection
rules raised the performance for the remaining two
corpora as well. We analyzed the simpliﬁcation
results, and showed that the simple rules are ap-
plicable with little danger of changing the truth-
values of the interactions.

The main contributions of this paper are: 1) ex-
planation of general sentence simpliﬁcation rules
using HPSG for relation extraction, 2) presenting
evidence that application of the rules improve re-
lation extraction performance, and 3) presentation
of an error analysis from two viewpoints: simpli-
ﬁcation and classiﬁcation results.

As future work, we are planning to reﬁne and
complete the current set of rules, and to cover
the shortcomings of the deep parser. Using these
rules, we can then make better use of the parser’s
capabilities. We will also attempt to apply our
simpliﬁcation rules to other relation extraction
problems than those of PPI.

Acknowledgments

This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientiﬁc Research (C) (General) (MEXT,
Japan).

al., 2009) or coreference resolution, become good
enough, they can supplement or even substitute
some of the proposed rules.

There are different difﬁculties in the BioInfer
and AIMed corpora. BioInfer includes more com-
plicated sentences and problems than the other
corpora do, because 1) the apposition, coordi-
nation, and exempliﬁcation rules are more fre-
quently used in the BioInfer corpus than in the
other corpora (shown in Table 6), 2) there were
more errors in the BioInfer corpus than in other
corpora among the simpliﬁed sentences (shown
in Table 8), and 3) BioInfer has more words per
sentence and more relation types than the other
corpora. AIMed contains several annotation in-
consistencies as explained in Section 4.3. These
inconsistencies must be removed to properly eval-
uate the effect of our method.

Simpliﬁcation errors are mostly caused by
parse errors. Our rule speciﬁcally examines a part
of parser output; a probability is attached to the
part. The probability is useful for deﬁning the or-
der of rule applications, and the n-best results by
the parser are useful to ﬁx major errors such as co-
ordination errors. By using these modiﬁcations of
rule applications and by continuous improvement
in parsing technology for the biomedical domain,
the performance on the BioInfer and AIMed cor-
pora will be improved also for the all rules case.

The PPI extraction system lost the ability to
capture some of the relation-related expressions
left by the simpliﬁcation rules. This indicates
that the system used to extract some relations (be-
fore simpliﬁcation) by using back-off features like
bag-of-words. The system can reduce bad effects
caused by parse errors, but it also captures the an-
notation inconsistencies in AIMed. Our simpli-
ﬁcation (without errors) can capture more general
expressions needed for relation extraction. To pro-
vide consistent PPI relation extraction in a general
setting (e.g. for multiple corpora or for other pub-
lic text collections), the parse errors must be dealt
with, and a relation extraction system that can cap-
ture (only) general relation-related expressions is
needed.

796

References
Airola, Antti, Sampo Pyysalo, Jari Bj¨orne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of the BioNLP 2008 work-
shop.

Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction.
In HLT ’05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
724–731.

Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artiﬁcial Intelli-
gence in Medicine, 33(2):139–155.

Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun’ichi Tsujii.
2006. Extraction of gene-
disease relations from medline using domain dictio-
naries and machine learning. In The Paciﬁc Sympo-
sium on Biocomputing (PSB), pages 4–15.

Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases?
Paciﬁc Symposium on Biocomputing,
pages 326–337.

Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) program: Tasks, data, and evalua-
tion. In Proceedings of LREC’04, pages 837–840.

Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In in Proceedings of Work-
shop on Automatic Summarization, pages 1–8.

Fundel, Katrin, Robert K¨uffner, and Ralf Zimmer.
2006. Relex—relation extraction using dependency
parse trees. Bioinformatics, 23(3):365–371.

Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simpliﬁcation aids protein-protein
interaction extraction.
In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine, pages 109–114, November.

Matsuzaki, Takuya, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2007. Efﬁcient HPSG parsing with supertag-
ging and cfg-ﬁltering. In IJCAI’07: Proceedings of
the 20th international joint conference on Artiﬁcal
intelligence, pages 1671–1676, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Miwa, Makoto, Rune Sætre, Yusuke Miyao, and
Jun’ichi Tsujii.
Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, June.

2009.

Morante, Roser and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28–36, Boulder, Colorado, June. Association for
Computational Linguistics.

N´edellec, Claire. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL’05 Workshop.

Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.

Pyysalo, Sampo, Antti Airola, Juho Heimonen, Jari
Bj¨orne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of ﬁve protein-protein inter-
action corpora.
In BMC Bioinformatics, volume
9(Suppl 3), page S6.

Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, and
Jun’ichi Tsujii.
2009. Static relations: a piece
in the biomedical information extraction puzzle.
In BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 1–9, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.

Sarawagi, Sunita.

Information extraction.
Foundations and Trends in Databases, 1(3):261–
377.

2008.

Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simpliﬁca-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606–1618.

Vickrey, David and Daphne Koller. 2008. Sentence
simpliﬁcation for semantic role labeling.
In Pro-
ceedings of ACL-08: HLT, pages 344–352, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both ﬂat and structured features.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 825–832. Association for
Computational Linguistics.

