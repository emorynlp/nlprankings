



















































Learning Spatial Knowledge for Text to 3D Scene Generation


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Learning Spatial Knowledge for Text to 3D Scene Generation

Angel X. Chang, Manolis Savva and Christopher D. Manning
Stanford University

{angelx,msavva,manning}@cs.stanford.edu

Abstract

We address the grounding of natural lan-
guage to concrete spatial constraints, and
inference of implicit pragmatics in 3D en-
vironments. We apply our approach to the
task of text-to-3D scene generation. We
present a representation for common sense
spatial knowledge and an approach to ex-
tract it from 3D scene data. In text-to-
3D scene generation, a user provides as in-
put natural language text from which we
extract explicit constraints on the objects
that should appear in the scene. The main
innovation of this work is to show how
to augment these explicit constraints with
learned spatial knowledge to infer missing
objects and likely layouts for the objects
in the scene. We demonstrate that spatial
knowledge is useful for interpreting natu-
ral language and show examples of learned
knowledge and generated 3D scenes.

1 Introduction

To understand language, we need an understanding
of the world around us. Language describes the
world and provides symbols with which we rep-
resent meaning. Still, much knowledge about the
world is so obvious that it is rarely explicitly stated.
It is uncommon for people to state that chairs are
usually on the floor and upright, and that you usu-
ally eat a cake from a plate on a table. Knowledge
of such common facts provides the context within
which people communicate with language. There-
fore, to create practical systems that can interact
with the world and communicate with people, we
need to leverage such knowledge to interpret lan-
guage in context.
Spatial knowledge is an important aspect of the

world and is often not expressed explicitly in nat-
ural language. This is one of the biggest chal-

Figure 1: Generated scene for “There is a room
with a chair and a computer.” Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.

lenges in grounding language and enabling natu-
ral communication between people and intelligent
systems. For instance, if we want a robot that can
follow commands such as “bring me a piece of
cake”, it needs to be imparted with an understand-
ing of likely locations for the cake in the kitchen
and that the cake should be placed on a plate.
The pioneering WordsEye system (Coyne and

Sproat, 2001) addressed the text-to-3D task and is
an inspiration for our work. However, there are
many remaining gaps in this broad area. Among
them, there is a need for research into learning spa-
tial knowledge representations from data, and for
connecting them to language. Representing un-
stated facts is a challenging problem unaddressed
by prior work and the focus of our contribution.
This problem is a counterpart to the image descrip-
tion problem (Kulkarni et al., 2011; Mitchell et al.,
2012; Elliott and Keller, 2013), which has so far
remained largely unexplored by the community.
We present a representation for this form of spa-

tial knowledge that we learn from 3D scene data
and connect to natural language. We will show
how this representation is useful for grounding
language and for inferring unstated facts, i.e., the
pragmatics of language describing physical envi-
ronments. We demonstrate the use of this repre-
sentation in the task of text-to-3D scene genera-

2028



Room

Table

Plate

Cake

color(red)“There is a room with 
a table and a cake. 

There is a red chair to 
the right of the table.”

a) Scene TemplateInput Text

supports(o0,o1) supports(o0,o2)

right(o2,o1)

o3

cake

c) 3D Scene

o0

room

o1

table

o2

chair

supports(o1,o4)

supports(o4,o3)o4

plate

Parse

Infer

Ground

Layout

b) Geometric Scene

Render

View

Chair

Figure 2: Overview of our spatial knowledge representation for text-to-3D scene generation. We parse
input text into a scene template and infer implicit spatial constraints from learned priors. We then ground
the template to a geometric scene, choose 3Dmodels to instantiate and arrange them into a final 3D scene.

tion, where the input is natural language and the
desired output is a 3D scene.
We focus on the text-to-3D task to demonstrate

that extracting spatial knowledge is possible and
beneficial in a challenging scenario: one requiring
the grounding of natural language and inference of
rarelymentioned implicit pragmatics based on spa-
tial facts. Figure 1 illustrates some of the inference
challenges in generating 3D scenes from natural
language: the desk was not explicitly mentioned
in the input, but we need to infer that the computer
is likely to be supported by a desk rather than di-
rectly placed on the floor. Without this inference,
the user would need to be much more verbose with
text such as “There is a room with a chair, a com-
puter, and a desk. The computer is on the desk, and
the desk is on the floor. The chair is on the floor.”

Contributions We present a spatial knowledge
representation that can be learned from 3D scenes
and captures the statistics of what objects occur
in different scene types, and their spatial posi-
tions relative to each other. In addition, we model
spatial relations (left, on top of, etc.) and learn a
mapping between language and the geometric con-
straints that spatial terms imply. We show that
using our learned spatial knowledge representa-
tion, we can infer implicit constraints, and generate
plausible scenes from concise natural text input.

2 Task Definition and Overview

We define text-to-scene generation as the task of
taking text that describes a scene as input, and gen-
erating a plausible 3D scene described by that text
as output. More concretely, based on the input
text, we select objects from a dataset of 3D models
and arrange them to generate output scenes.
The main challenge we address is in transform-

ing a scene template into a physically realizable 3D
scene. For this to be possible, the system must be

able to automatically specify the objects present
and their position and orientation with respect to
each other as constraints in 3D space. To do so, we
need to have a representation of scenes (§3). We
need good priors over the arrangements of objects
in scenes (§4) and we need to be able to ground
textual relations into spatial constraints (§5). We
break down our task as follows (see Figure 2):
Template Parsing (§6.1): Parse the textual de-
scription of a scene into a set of constraints on the
objects present and spatial relations between them.
Inference (§6.2): Expand this set of constraints by
accounting for implicit constraints not specified in
the text using learned spatial priors.
Grounding (§6.3): Given the constraints and pri-
ors on the spatial relations of objects, transform the
scene template into a geometric 3D scenewith a set
of objects to be instantiated.
Scene Layout (§6.4): Arrange the objects and op-
timize their placement based on priors on the rel-
ative positions of objects and explicitly provided
spatial constraints.

3 Scene Representation

To capture the objects present and their arrange-
ment, we represent scenes as graphs where nodes
are objects in the scene, and edges are semantic re-
lationships between the objects.
We represent the semantics of a scene using a

scene template and the geometric properties using
a geometric scene. One critical property which is
captured by our scene graph representation is that
of a static support hierarchy, i.e., the order in which
bigger objects physically support smaller ones: the
floor supports tables, which support plates, which
can support cakes. Static support and other con-
straints on relationships between objects are rep-
resented as edges in the scene graph.

2029



0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
p(Scene|Knife+Table)

Kitchen Counter Dining TableLiving Room Kitchen

Figure 3: Probabilities of different scene types
given the presence of “knife” and “table”.

Figure 4: Probabilities of support for some most
likely child object categories given four different
parent object categories, from top left clockwise:
dining table, bookcase, room, desk.

3.1 Scene Template
A scene template T = (O, C, Cs) consists of a
set of object descriptions O = {o1, . . . , on} and
constraints C = {c1, . . . , ck} on the relationships
between the objects. A scene template also has a
scene type Cs.
Each object oi, has properties associated with

it such as category label, basic attributes such as
color and material, and number of occurrences in
the scene. For constraints, we focus on spatial re-
lations between objects, expressed as predicates of
the form supported_by(oi, oj) or left(oi, oj)where
oi and oj are recognized objects.1 Figure 2a shows
an example scene template. From the scene tem-
plate we instantiate concrete geometric 3D scenes.
To infer implicit constraints on objects and spa-
tial support we learn priors on object occurrences
in 3D scenes (§4.1) and their support hierarchies
(§4.2).

3.2 Geometric Scene
We refer to the concrete geometric representation
of a scene as a “geometric scene”. It consists of
a set of 3D model instances – one for each ob-
ject – that capture the appearance of the object. A
transformation matrix that represents the position,
orientation, and scaling of the object in a scene is
also necessary to exactly position the object. We
generate a geometric scene from a scene template
by selecting appropriate models from a 3D model
database and determining transformations that op-

1Our representation can also support other relationships
such as larger(oi, oj).

timize their layout to satisfy spatial constraints. To
inform geometric arrangement we learn priors on
the types of support surfaces (§4.2) and the relative
positions of objects (§4.4).

4 Spatial Knowledge

Our model of spatial knowledge relies on the idea
of abstract scene types describing the occurrence
and arrangement of different categories of objects
within scenes of that type. For example, kitchens
typically contain kitchen counters on which plates
and cups are likely to be found. The type of scene
and category of objects condition the spatial rela-
tionships that can exist in a scene.
We learn spatial knowledge from 3D scene data,

basing our approach on that of Fisher et al. (2012)
and using their dataset of 133 small indoor scenes
created with 1723 Trimble 3D Warehouse mod-
els (Fisher et al., 2012).

4.1 Object Occurrence Priors
We learn priors for object occurrence in different
scene types (such as kitchens, offices, bedrooms).

Pocc(Co|Cs) = count(Co in Cs)
count(Cs)

This allows us to evaluate the probability of dif-
ferent scene types given lists of object occurring
in them (see Figure 3). For example given input of
the form “there is a knife on the table” then we are
likely to generate a scene with a dining table and
other related objects.

4.2 Support Hierarchy Priors
We observe the static support relations of objects
in existing scenes to establish a prior over what ob-
jects go on top of what other objects. As an exam-
ple, by observing plates and forks on tables most
of the time, we establish that tables are more likely
to support plates and forks than chairs. We esti-
mate the probability of a parent category Cp sup-
porting a given child category Cc as a simple con-
ditional probability based on normalized observa-
tion counts.2

Psupport(Cp|Cc) = count(Cc on Cp)
count(Cc)

We show a few of the priors we learn in Figure 4
as likelihoods of categories of child objects being
statically supported by a parent category object.

2The support hierarchy is explicitly modeled in the scene
dataset we use.

2030



Figure 5: Predicted positions using learned rela-
tive position priors for chair given desk (top left),
poster-room (top right), mouse-desk (bottom left),
keyboard-desk (bottom right).

4.3 Support Surface Priors
To identify which surfaces on parent objects sup-
port child objects, we first segment parent models
into planar surfaces using a simple region-growing
algorithm based on (Kalvin and Taylor, 1996). We
characterize support surfaces by the direction of
their normal vector, limited to the six canonical
directions: up, down, left, right, front, back. We
learn a probability of supporting surface normal di-
rection Sn given child object category Cc. For ex-
ample, posters are typically found on walls so their
support normal vectors are in the horizontal di-
rections. Any unobserved child categories are as-
sumed to havePsurf (Sn = up|Cc) = 1 sincemost
things rest on a horizontal surface (e.g., floor).

Psurf (Sn|Cc) = count(Cc on surface with Sn)
count(Cc)

4.4 Relative Position Priors
We model the relative positions of objects based
on their object categories and current scene type:
i.e., the relative position of an object of category
Cobj is with respect to another object of category
Cref and for a scene type Cs. We condition on the
relationship R between the two objects, whether
they are siblings (R = Sibling) or child-parent
(R = ChildParent).

Prelpos(x, y, θ|Cobj , Cref , Cs, R)
When positioning objects, we restrict the search
space to points on the selected support surface.
The position x, y is the centroid of the target ob-
ject projected onto the support surface in the se-
mantic frame of the reference object. The θ is the
angle between the front of the two objects. We rep-
resent these relative position and orientation pri-
ors by performing kernel density estimation on the

Relation P (relation)

inside(A,B) V ol(A∩B)
V ol(A)

outside(A,B) 1 - V ol(A∩B)
V ol(A)

left_of(A,B) V ol(A∩ left_of (B))
V ol(A)

right_of(A,B) V ol(A∩ right_of (B))
V ol(A)

near(A,B) 1(dist(A, B) < tnear)
faces(A,B) cos(front(A), c(B)− c(A))

Table 1: Definitions of spatial relation using
bounding boxes. Note: dist(A, B) is normalized
against the maximum extent of the bounding box
of B. front(A) is the direction of the front vector
of A and c(A) is the centroid of A.

Keyword Top Relations and Scores
behind (back_of, 0.46), (back_side, 0.33)
adjacent (front_side, 0.27), (outside, 0.26)
below (below, 0.59), (lower_side, 0.38)
front (front_of, 0.41), (front_side, 0.40)
left (left_side, 0.44), (left_of, 0.43)
above (above, 0.37), (near, 0.30)
opposite (outside, 0.31), (next_to, 0.30)
on (supported_by, 0.86), (on_top_of, 0.76)
near (outside, 0.66), (near, 0.66)
next (outside, 0.49), (near, 0.48)
under (supports, 0.62), (below, 0.53)
top (supported_by, 0.65), (above, 0.61)
inside (inside, 0.48), (supported_by, 0.35)
right (right_of, 0.50), (lower_side, 0.38)
beside (outside, 0.45), (right_of, 0.45)

Table 2: Map of top keywords to spatial relations
(appropriate mappings in bold).

observed samples. Figure 5 shows predicted posi-
tions of objects using the learned priors.

5 Spatial Relations

We define a set of formal spatial relations that we
map to natural language terms (§5.1). In addi-
tion, we collect annotations of spatial relation de-
scriptions from people, learn a mapping of spatial
keywords to our formal spatial relations, and train
a classifier that given two objects can predict the
likelihood of a spatial relation holding (§5.2).

5.1 Predefined spatial relations
For spatial relations we use a set of predefined rela-
tions: left_of, right_of, above, below, front, back,
supported_by, supports, next_to, near, inside, out-
side, faces, left_side, right_side.3 These are mea-
sured using axis-aligned bounding boxes from the
viewer’s perspective; the involved bounding boxes
are compared to determine volume overlap or clos-
est distance (for proximity relations; see Table 1).

3Wedistinguish left_of(A,B) asA being left of the left edge
of the bounding box of B vs left_side(A,B) as A being left of
the centroid of B.

2031



Feature # Description
delta(A, B) 3 Delta position (x, y, z) between the centroids of A and B
dist(A, B) 1 Normalized distance (wrt B) between the centroids of A and B
overlap(A, f(B)) 6 Fraction of A inside left/right/front/back/top/bottom regions wrt B: V ol(A∩f(B))

V ol(A)

overlap(A, B) 2 V ol(A∩B)
V ol(A)

and V ol(A∩B)
V ol(B)

support(A, B) 2 supported_by(A, B) and supports(A, B)

Table 3: Features for trained spatial relations predictor.

Figure 6: Our data collection task.

Since these spatial relations are resolvedwith re-
spect to the view of the scene, they correspond to
view-centric definitions of spatial concepts.

5.2 Learning Spatial Relations

We collect a set of text descriptions of spatial rela-
tionships between two objects in 3D scenes by run-
ning an experiment on Amazon Mechanical Turk.
We present a set of screenshots of scenes in our
dataset that highlight particular pairs of objects and
we ask people to fill in a spatial relationship of the
form “The __ is __ the __” (see Fig 6). We col-
lected a total of 609 annotations over 131 object
pairs in 17 scenes. We use this data to learn pri-
ors on view-centric spatial relation terms and their
concrete geometric interpretation.
For each response, we select one keyword from

the text based on length. We learn a mapping of
the top 15 keywords to our predefined set of spa-
tial relations. We use our predefined relations on
annotated spatial pairs of objects to create a binary
indicator vector that is set to 1 if the spatial relation
holds, or zero otherwise. We then create a simi-
lar vector for whether the keyword appeared in the
annotation for that spatial pair, and then compute
the cosine similarity of the two vectors to obtain
a score for mapping keywords to spatial relations.
Table 2 shows the obtained mapping. Using just
the top mapping, we are able to map 10 of the 15

Above

Above On

Left Right

Front Behind

Figure 7: High probability regions where the cen-
ter of another object would occur for some spatial
relations with respect to a table: above (top left),
on (top right), left (mid left), right (mid right), in
front (bottom left), behind (bottom right).

keywords to an appropriate spatial relation. The 5
keywords that are not well mapped are proximity
relations that are not well captured by our prede-
fined spatial relations.
Using the 15 keywords as our spatial relations,

we train a log linear binary classifier for each key-
word over features of the objects involved in that
spatial relation (see Table 3). We then use this
model to predict the likelihood of that spatial re-
lation in new scenes.
Figure 7 shows examples of predicted likeli-

hoods for different spatial relations with respect to
an anchor object in a scene. Note that the learned
spatial relations are much stricter than our prede-
fined relations. For instance, “above” is only used
to referred to the area directly above the table, not
to the region above and to the left or above and in
front (which our predefined classifier will all con-
sider to be above). In our results, we showwe have
more accurate scenes using the trained spatial re-
lations than the predefined ones.

2032



Dependency Pattern Example Text

{tag:VBN}=verb >nsubjpass {}=nsubj >prep ({}=prep >pobj {}=pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj].
attribute(verb,pobj)(nsubj,pobj) material(chair,wood)
{}=dobj >cop {} >nsubj {}=nsubj The chair[nsubj] is red[dobj].

attribute(dobj)(nsubj,dobj) color(chair,red)
{}=dobj >cop {} >nsubj {}=nsubj >prep ({}=prep >pobj {}=pobj) The table[nsubj] is next[dobj] to[prep] the chair[pobj].

spatial(dobj)(nsubj, pobj) next_to(table,chair)
{}=nsubj >advmod ({}=advmod >prep ({}=prep >pobj {}=pobj)) There is a table[nsubj] next[advmod] to[prep] a chair[pobj].

spatial(advmod)(nsubj, pobj) next_to(table,chair)

Table 4: Example dependency patterns for extracting attributes and spatial relations.

6 Text to Scene generation

We generate 3D scenes from brief scene descrip-
tions using our learned priors.

6.1 Scene Template Parsing
During scene template parsing we identify the
scene type, the objects present in the scene, their
attributes, and the relations between them. The
input text is first processed using the Stanford
CoreNLP pipeline (Manning et al., 2014). The
scene type is determined by matching the words
in the utterance against a list of known scene types
from the scene dataset.
To identify objects, we look for noun phrases

and use the head word as the category, filtering
with WordNet (Miller, 1995) to determine which
objects are visualizable (under the physical object
synset, excluding locations). We use the Stanford
coreference system to determine when the same
object is being referred to.
To identify properties of the objects, we extract

other adjectives and nouns in the noun phrase. We
alsomatch dependency patterns such as “X ismade
of Y” to extract additional attributes. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we identify
a set of associated keywords to be used later for
querying the 3D model database.
Dependency patterns are also used to extract

spatial relations between objects (see Table 4 for
some example patterns). We use Semgrex patterns
to match the input text to dependencies (Cham-
bers et al., 2007). The attribute types are deter-
mined from a dictionary using the text express-
ing the attribute (e.g., attribute(red)=color, at-
tribute(round)=shape). Likewise, spatial relations
are looked up using the learned map of keywords
to spatial relations.
As an example, given the input “There is a room

with a desk and a red chair. The chair is to the left

of the desk.” we extract the following objects and
spatial relations:
Objects category attributes keywords

o0 room room
o1 desk desk
o2 chair color:red chair, red

Relations: left(o2, o1)

6.2 Inferring Implicits
From the parsed scene template, we infer the pres-
ence of additional objects and support constraints.
We can optionally infer the presence of addi-

tional objects from object occurrences based on the
scene type. If the scene type is unknown, we use
the presence of known object categories to pre-
dict the most likely scene type by using Bayes’
rule on our object occurrence priors Pocc to get
P (Cs|{Co}) ∝ Pocc({Co}|Cs)P (Cs). Once we
have a scene type Cs, we sample Pocc to find ob-
jects that are likely to occur in the scene. We re-
strict sampling to the top n = 4 object categories.
We can also use the support hierarchy priors

Psupport to infer implicit objects. For instance, for
each object oi we find the most likely supporting
object category and add it to our scene if not al-
ready present.
After inferring implicit objects, we infer the sup-

port constraints. Using the learned text to prede-
fined relation mapping from §5.2, we can map the
keywords “on” and “top” to the supported_by re-
lation. We infer the rest of the support hierarchy
by selecting for each object oi the parent object oj
that maximizes Psupport(Coj |Coi).
6.3 Grounding Objects
Once we determine from the input text what ob-
jects exist and their spatial relations, we select 3D
models matching the objects and their associated
properties. Each object in the scene template is
grounded by querying a 3D models database with

2033



 There is a desk and 
a keyboard and a 

monitor. 

Input Text Basic +Support Hierarchy +Relative Positions

 There is a coffee table 
and there is a lamp 

behind the coffee table. 
There is a chair in front of 

the coffee table. 

UPDATE UPDATE

No Relations Predefined Relations Learned Relations

Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support
Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of
spatial relations (No Relations), scoring using Predefined Relations and Learned Relations.

the appropriate category and keywords.
We use a 3D model dataset collected from

Google 3DWarehouse by prior work in scene syn-
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags. In addition, we semi-automatically annotated
models with object category labels (roughly 270
classes). We used model tags to set these labels,
and verified and augmented them manually.
In addition, we automatically rescale models so

that they have physically plausible sizes and orient
them so that they have a consistent up and front
direction (Savva et al., 2014). We then indexed all
models in a database that we query at run-time for
retrieval based on category and tag labels.

6.4 Scene Layout
Once we have instantiated the objects in the scene
by selecting models, we aim to optimize an over-
all layout score L = λobjLobj + λrelLrel that is
a weighted sum of object arrangement Lobj score
and constraint satisfaction Lrel score:
Lobj =

∑
oi

Psurf (Sn|Coi)
∑

oj∈F (oi)
Prelpos(·)

Lrel =
∑
ci

Prel(ci)

where F (oi) are the sibling objects and parent ob-
ject of oi. We use λobj = 0.25 and λrel = 0.75 for
the results we present.
We use a simple hill climbing strategy to find a

reasonable layout. We first initialize the positions

Figure 9: Generated scene for “There is a room
with a desk and a lamp. There is a chair to the
right of the desk.” The inferred scene hierarchy is
overlayed in the center.

of objects within the scene by traversing the sup-
port hierarchy in depth-first order, positioning the
children from largest to first and recursing. Child
nodes are positioned by first selecting a supporting
surface on a candidate parent object through sam-
pling of Psurf . After selecting a surface, we sam-
ple a position on the surface based on Prelpos. Fi-
nally, we check whether collisions exist with other
objects, rejecting layouts where collisions occur.
We iterate by randomly jittering and repositioning
objects. If there are any spatial constraints that are
not satisfied, we also remove and randomly repo-
sition the objects violating the constraints, and it-
erate to improve the layout. The resulting scene is
rendered and presented to the user.

7 Results and Discussion

We show examples of generated scenes, and com-
pare against naive baselines to demonstrate learned
priors are essential for scene generation. We

2034



Figure 10: Generated scene for “There is a room
with a poster bed and a poster.”

Figure 11: Generated scene for “living room”.

also discuss interesting aspects of using spatial
knowledge in view-based object referent resolu-
tion (§7.2) and in disambiguating geometric inter-
pretations of “on” (§7.3).

Model Comparison Figure 8 shows a compari-
son of scenes generated by our model versus sev-
eral simpler baselines. The top row shows the im-
pact of modeling the support hierarchy and the rel-
ative positions in the layout of the scene. The bot-
tom row shows that the learned spatial relations
can give a more accurate layout than the naive
predefined spatial relations, since it captures prag-
matic implicatures of language, e.g., left is only
used for directly left and not top left or bottom
left (Vogel et al., 2013).

Figure 12: Left: chair is selected using “the chair
to the right of the table” or “the object to the right of
the table”. Chair is not selected for “the cup to the
right of the table”. Right: Different view results
in different chair being selected for the input “the
chair to the right of the table”.

7.1 Generated Scenes
Support Hierarchy Figure 9 shows a generated
scene along with the input text and support hier-
archy. Even though the spatial relation between
lamp and desk was not mentioned, we infer that the
lamp is supported by the top surface of the desk.

Disambiguation Figure 10 shows a generated
scene for the input “There is a room with a poster
bed and a poster”. Note that the system differen-
tiates between a “poster” and a “poster bed” – it
correctly selects and places the bed on the floor,
while the poster is placed on the wall.

Inferring objects for a scene type Figure 11
shows an example of inferring all the objects
present in a scene from the input “living room”.
Some of the placements are good, while others can
clearly be improved.

7.2 View-centric object referent resolution
After a scene is generated, the user can refer to ob-
jects with their categories andwith spatial relations
between them. Objects are disambiguated by both
category and view-centric spatial relations. We use
the WordNet hierarchy to resolve hyponym or hy-
pernym referents to objects in the scene. In Fig-
ure 12 (left), the user can select a chair to the right
of the table using the phrase “chair to the right of
the table” or “object to the right of the table”. The
user can then change their viewpoint by rotating
and moving around. Since spatial relations are re-
solved with respect to the current viewpoint, a dif-
ferent chair is selected for the same phrase from a
different viewpoint in the right screenshot.

7.3 Disambiguating “on”
As shown in §5.2, the English preposition “on”,
when used as a spatial relation, corresponds
strongly to the supported_by relation. In our
trained model, the supported_by feature also has
a high positive weight for “on”.
Our model for supporting surfaces and hierar-

chy allows interpreting the placement of “A on
B” based on the categories of A and B. Fig-
ure 13 demonstrates four different interpretations
for “on”. Given the input “There is a cup on the
table” the system correctly places the cup on the
top surface of the table. In contrast, given “There
is a cup on the bookshelf”, the cup is placed on a
supporting surface of the bookshelf, but not nec-
essarily the top one which would be fairly high.

2035



Figure 13: From top left clockwise: “There is a
cup on the table”, “There is a cup on the book-
shelf”, “There is a poster on the wall”, “There is
a hat on the chair”. Note the different geometric
interpretations of “on”.

Given the input “There is a poster on the wall”, a
poster is pasted on the wall, while with the input
“There is a hat on the chair” the hat is placed on
the seat of the chair.

7.4 Limitations

While the system shows promise, there are still
many challenges in text-to-scene generation. For
one, we did not address the difficulties of resolving
objects. A failure case of our system stems from
using a fixed set of categories to identify visualiz-
able objects. For example, the sense of “top” refer-
ring to a spinning top, and other uncommon object
types, are not handled by our system as concrete
objects. Furthermore, complex phrases including
object parts such as “there’s a coat on the seat of
the chair” are not handled. Figure 14 shows some

Figure 14: Left: A water bottle instead of wine
bottle is selected for “There is a bottle of wine on
the table in the kitchen”. In addition, the selected
table is inappropriate for a kitchen. Right: A floor
lamp is incorrectly selected for the input “There is
a lamp on the table”.

example cases where the context is important in
selecting an appropriate object and the difficulties
of interpreting noun phrases.
In addition, we rely on a few dependency pat-

terns for extracting spatial relations so robustness
to variations in spatial language is lacking. We
only handle binary spatial relations (e.g., “left”,
“behind”) ignoringmore complex relations such as
“around the table” or “in the middle of the room”.
Though simple binary relations are some of the
most fundamental spatial expressions and a good
first step, handling more complex expressions will
do much to improve the system.
Another issue is that the interpretation of sen-

tences such as “the desk is covered with paper”,
which entails many pieces of paper placed on the
desk, is hard to resolve. With a more data-driven
approach we can hope to link such expressions to
concrete facts.
Finally, we use a traditional pipeline approach

for text processing, so errors in initial stages
can propagate downstream. Failures in depen-
dency parsing, part of speech tagging, or coref-
erence resolution can result in incorrect interpre-
tations of the input language. For example, in
the sentence “there is a desk with a chair in front
of it”, “it” is not identified as coreferent with
“desk” so we fail to extract the spatial relation
front_of(chair, desk).

8 Related Work

There is related prior work in the topics of mod-
eling spatial relations, generating 3D scenes from
text, and automatically laying out 3D scenes.

8.1 Spatial knowledge and relations

Prior work that required modeling spatial knowl-
edge has defined representations specific to the
task addressed. Typically, such knowledge is man-
ually provided or crowdsourced – not learned from
data. For instance, WordsEye (Coyne et al., 2010)
uses a set of manually specified relations. The
NLP community has explored grounding text to
physical attributes and relations (Matuszek et al.,
2012; Krishnamurthy and Kollar, 2013), gener-
ating text for referring to objects (FitzGerald et
al., 2013) and connecting language to spatial re-
lationships (Vogel and Jurafsky, 2010; Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Most
of this work focuses on learning a mapping from
text to formal representations, and does not model

2036



implicit spatial knowledge. Many priors on real
world spatial facts are typically unstated in text and
remain largely unaddressed.

8.2 Text to Scene Systems
Early work on the SHRDLU system (Winograd,
1972) gives a good formalization of the linguis-
tic manipulation of objects in 3D scenes. By re-
stricting the discourse domain to a micro-world
with simple geometric shapes, the SHRDLU sys-
tem demonstrated parsing of natural language in-
put for manipulating scenes. However, generaliza-
tion to more complex objects and spatial relations
is still very hard to attain.
More recently, a pioneering text-to-3D scene

generation prototype system has been presented by
WordsEye (Coyne and Sproat, 2001). The authors
demonstrated the promise of text to scene genera-
tion systems but also pointed out some fundamen-
tal issues which restrict the success of their system:
much spatial knowledge is required which is hard
to obtain. As a result, users have to use unnatural
language (e.g., “the stool is 1 feet to the south of
the table”) to express their intent. Follow up work
has attempted to collect spatial knowledge through
crowd-sourcing (Coyne et al., 2012), but does not
address the learning of spatial priors.
We address the challenge of handling natural

language for scene generation, by learning spatial
knowledge from 3D scene data, and using it to in-
fer unstated implicit constraints. Our work is simi-
lar in spirit to recent work on generating 2D clipart
for sentences using probabilistic models learned
from data (Zitnick et al., 2013).

8.3 Automatic Scene Layout
Work on scene layout has focused on determining
good furniture layouts by optimizing energy func-
tions that capture the quality of a proposed layout.
These energy functions are encoded from design
guidelines (Merrell et al., 2011) or learned from
scene data (Fisher et al., 2012). Knowledge of ob-
ject co-occurrences and spatial relations is repre-
sented by simple models such as mixtures of Gaus-
sians on pairwise object positions and orientations.
We leverage ideas from this work, but they do not
focus on linking spatial knowledge to language.

9 Conclusion and Future Work

We have demonstrated a representation of spatial
knowledge that can be learned from 3D scene data

and how it corresponds to natural language. We
also showed that spatial inference and grounding is
critical for achieving plausible results in the text-
to-3D scene generation task. Spatial knowledge is
critically useful not only in this task, but also in
other domains which require an understanding of
the pragmatics of physical environments.
We only presented a deterministic approach for

mapping input text to the parsed scene template.
An interesting avenue for future research is to
automatically learn how to parse text describing
scenes into formal representations by using more
advanced semantic parsing methods.
We can also improve the representation used for

spatial priors of objects in scenes. For instance, in
this paper we represented support surfaces by their
orientation. We can improve the representation by
modeling whether a surface is an interior or exte-
rior surface.
Another interesting line of future work would

be to explore the influence of object identity in de-
termining when people use ego-centric or object-
centric spatial reference models, and to improve
resolution of spatial terms that have different in-
terpretations (e.g., “the chair to the left of John” vs
“the chair to the left of the table”).
Finally, a promising line of research is to explore

using spatial priors for resolving ambiguities dur-
ing parsing. For example, the attachment of “next
to” in “Put a lamp on the table next to the book” can
be readily disambiguated with spatial priors such
as the ones we presented.

Acknowledgments

We thank the anonymous reviewers for their
thoughtful comments. We gratefully acknowl-
edge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program under
Air Force Research Laboratory (AFRL) contract
no. FA8750-13-2-0040. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.

References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.

2037



Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.

Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.

Bob Coyne, Richard Sproat, and Julia Hirschberg.
2010. Spatial relations in text-to-scene conversion.
InComputational Models of Spatial Language Inter-
pretation, Workshop at Spatial Cognition.

BobCoyne, Alexander Klapheke, MasoudRouhizadeh,
Richard Sproat, and Daniel Bauer. 2012. Annota-
tion tools and knowledge representation for a text-to-
scene system. Proceedings of COLING 2012: Tech-
nical Papers.

Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP).

Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics (TOG).

Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).

Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP).

Alan D. Kalvin and Russell H. Taylor. 1996. Super-
faces: Polygonal mesh simplification with bounded
error. Computer Graphics and Applications, IEEE.

Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.

Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Computer Vision
and Pattern Recognition (CVPR), 2011 IEEE Con-
ference on.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics: System Demonstrations.

Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference onMa-
chine Learning (ICML).

Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh
Agrawala, and Vladlen Koltun. 2011. Interactive
furniture layout using interior design guidelines. In
ACM Transactions on Graphics (TOG).

George A. Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM.

Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, andHal Daumé III. 2012.
Midge: Generating image descriptions from com-
puter vision detections. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.

Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.

Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.

Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013. Implicatures and nested beliefs in approxi-
mate Decentralized-POMDPs. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.

Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.

C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).

2038


