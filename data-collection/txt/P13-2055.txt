



















































Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 306–311,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Enhanced and Portable Dependency Projection Algorithms
Using Interlinear Glossed Text

Ryan Georgi
University of Washington
Seattle, WA 98195, USA

rgeorgi@uw.edu

Fei Xia
University of Washington
Seattle, WA 98195, USA

fxia@uw.edu

William D. Lewis
Microsoft Research

Redmond, WA 98052, USA
wilewis@microsoft.com

Abstract

As most of the world’s languages are
under-resourced, projection algorithms
offer an enticing way to bootstrap the
resources available for one resource-
poor language from a resource-rich lan-
guage by means of parallel text and
word alignment. These algorithms,
however, make the strong assumption
that the language pairs share common
structures and that the parse trees will
resemble one another. This assump-
tion is useful but often leads to errors
in projection. In this paper, we will
address this weakness by using trees
created from instances of Interlinear
Glossed Text (IGT) to discover pat-
terns of divergence between the lan-
guages. We will show that this method
improves the performance of projection
algorithms significantly in some lan-
guages by accounting for divergence be-
tween languages using only the partial
supervision of a few corrected trees.

1 Introduction
While thousands of languages are spoken
in the world, most of them are considered
resource-poor in the sense that they do not
have a large number of electronic resources
that can be used to build NLP systems. For
instance, some languages may lack treebanks,
thus making it difficult to build a high-quality
statistical parser.

One common approach to address this prob-
lem is to take advantage of bitext between a
resource-rich language (e.g., English) and a
resource-poor language by projecting informa-
tion from the former to the latter (Yarowsky
and Ngai, 2001; Hwa et al., 2004). While pro-

jection methods can provide a great deal of in-
formation at minimal cost to the researchers,
they do suffer from structural divergence be-
tween the language-poor language (aka target
language) and the resource-rich language (aka
source language).
In this paper, we propose a middle ground

between manually creating a large-scale tree-
bank (which is expensive and time-consuming)
and relying on the syntactic structures pro-
duced by a projection algorithm alone (which
are error-prone).
Our approach has several steps. First, we

utilize instances of Interlinear Glossed Text
(IGT) following Xia and Lewis (2007) as seen
in Figure 1(a) to create a small set of parallel
dependency trees through projection and then
manually correct the dependency trees. Sec-
ond, we automatically analyze this small set
of parallel trees to find patterns where the cor-
rected data differs from the projection. Third,
those patterns are incorporated to the projec-
tion algorithm to improve the quality of pro-
jection. Finally, the features extracted from
the projected trees are added to a statisti-
cal parser to improve parsing quality. The
outcome of this work are both an enhanced
projection algorithm and a better parser for
resource-poor languages that require a mini-
mal amount of manual effort.

2 Previous Work
For this paper, we will be building upon
the standard projection algorithm for depen-
dency structures as outlined in Quirk et al.
(2005) and illustrated in Figure 1. First, a
sentence pair between resource-rich (source)
and resource-poor (target) languages is word
aligned [Fig 1(a)]. Second, the source sen-
tence is parsed by a dependency parser for
the source language [Fig 1(b)]. Third, sponta-

306



siwA ne pAnI se GadZe ko BarA

Sita filled the clay-pot with water

Sita erg water with clay-pot acc filled

(a) An Interlinear Glossed Text (IGT) instance in Hindi
and word alignment between the gloss line and the
English translation.

Sita

filled

the

clay-pot with

water
(b) Dependency parse of English translation.

siwA

BarA

the

GadZe se

pAnI
(c) English words are replaced with Hindi words and

spontaneous word “the” are removed from the tree.

siwA

BarA

GadZese

pAnIne ko

(d) Siblings in the tree are reordered based on the word
order of the Hindi sentence and spontaneous Hindi
words are attached as indicated by dotted lines. The
words pAnI and se are incorrectly inverted, as indi-
cated by the curved arrow.

Figure 1: An example of projecting a depen-
dency tree from English to Hindi.

neous (unaligned) source words are removed,
and the remaining words are replaced with
corresponding words in the target side [Fig
1(c)]. Finally, spontaneous target words are
re-attached heuristically and the children of a
head are ordered based on the word order in
the target sentence [Fig 1(d)]. The resulting
tree may have errors (e.g., pAni should depend
on se in Figure 1(d)), and the goal of this study
is to reduce common types of projection errors.
In Georgi et al. (2012a), we proposed a

method for analyzing parallel dependency cor-
pora in which word alignment between trees
was used to determine three types of edge con-
figurations: merged, swapped, and spon-
taneous. Merged alignments were those in
which multiple words in the target tree aligned
to a single word in the source tree, as in Figure
2. Swapped alignments were those in which
a parent node in the source tree aligned to a

child in the target tree and vice-versa. Finally,
spontaneous alignments were those for which
a word did not align to any word on the other
side. These edge configurations could be de-
tected from simple parent–child edges and the
alignment (or lack of) between words in the
language pairs. Using these simple, language-
agnostic measures allows one to look for diver-
gence types such as those described by Dorr
(1994).
Georgi et al. (2012b) described a method

in which new features were extracted from
the projected trees and added to the feature
vectors for a statistical dependency parser.
The rationale was that, although the projected
trees were error-prone, the parsing model
should be able to set appropriate weights of
these features based on how reliable these fea-
tures were in indicating the dependency struc-
ture. We started with the MSTParser (Mc-
Donald et al., 2005) and modified it so that the
edges from the projected trees could be used
as features at parse time. Experiments showed
that adding new features improved parsing
performance.
In this paper, we use the small training cor-

pus built in Georgi et al. (2012b) to improve
the projection algorithm itself. The improved
projected trees are in turn fed to the statistical
parser to further improve parsing results.

3 Enhancements to the projection
algorithm

We propose to enhance the projection algo-
rithm by addressing the three alignment types
discussed earlier:
1. Merge: better informed choice for head

for multiply-aligned words.
2. Swap: post-projection correction of fre-

quently swapped word pairs.
3. Spontaneous: better informed attach-

ment of target spontaneous words.

The detail of the enhancements are ex-
plained below.

3.1 Merge Correction
“Merged” words, or multiple words on the tar-
get side that align to a single source word, are
problematic for the projection algorithm be-
cause it is not clear which target word should
be the head and which word should be the

307



rAma buxXimAna lagawA hE
Ram intelligent seem be-Pres
“Ram seems intelligent”

seems
VBZ

Ram
NNP

intelligent
JJ

lagawA
seems

ram
Ram

buxXimAna
intelligent

hE
be-Pres

Figure 2: An example of merged alignment,
where the English word seems align to two
Hindi words hE and lagawA. Below the IGT
are the dependency trees for English and
Hindi. Dotted arrows indicate word align-
ment, and the solid arrow indicates that hE
should depend on lagawA.

dependent. An example is given in Figure 2,
where the English word seems align to two
Hindi words hE and lagawA.

On the other hand, from the small amount
of labeled training data (i.e., a set of hand-
corrected tree pairs), we can learn what kind
of source words are likely to align to multiple
target words, and which target word is likely to
the head. The process is illustrated in Figure
3. In this example, the target words tm and
tn are both aligned with the source word si
whose POS tag is POSi, and tm appears before
tn in the target sentence. Going through the
examples of merged alignments in the training
data, we keep a count for the POS tag of the
source word and the position of the head on
the target side.1 Based on these counts, our
system will generate rules such as the ones in
Figure 3(c) which says if a source word whose
POS is POSi aligns to two target words, the
probability of the right target word depending
on the left one is 75%, and the probability of
the left target word depending on the right one
is 25%. We use maximum likelihood estimate
(MLE) to calculate the probability.

The projection algorithm will use those rules
to handle merged alignment; that is, when a
source word aligns to multiple target words,
the algorithm determines the direction of de-
pendency edge based on the direction prefer-
ence stored in the rules. In addition to rules for

1We use the position of the head, not the POS tag of
the head, because the POS tags of the target words are
not available when running the projection algorithm on
the test data.

si
POSi

tm

tn

(a) Alignment between a source word and two target
words, and one target word tm is the parent of the
other word tn.

tm tn to ... tp
(b) Target sentence showing the “left” dependency be-

tween tm and tn.

POSi → left 0.75
POSi → right 0.25

(c) Rules for handling merged alignment

Figure 3: Example of merged alignment and
rules derived from such an example

an individual source POS tag, our method also
keeps track of the overall direction preference
for all the merged examples in that language.
For merges in which the source POS tag is un-
seen or there are no rules for that tag, this
language-wide preference is used as a backoff.

3.2 Swap Correction
An example of swapped alignment is in Figure
4(a), where (sj , si) is an edge in the source
tree, (tm, tn) is an edge in the target tree, and
sj aligns to tn and si aligns to tm. Figure
1(d) shows an error made by the projection
algorithm due to swapped alignment. In order
to correct such errors, we count the number
of (POSchild, POSparent) dependency edges in
the source trees, and the number of times that
the directions of the edges are reversed on the
target side. Figure 4(b) shows a possible set of
counts resulting from this approach. Based on
the counts, we keep only the POS pairs that
appear in at least 10% of training sentences
and the percentage of swap for the pairs are
no less than 70%.2 We say that those pairs
trigger a swap operation.
At the test time, swap rules are applied as a

post-processing step to the projected tree. Af-
ter the projected tree is completed, our swap
handling step checks each edge in the source
tree. If the POS tag pair for the edge triggers

2These thresholds are set empirically.

308



si
POSi

tm

tn

sj
POSj

(a) A swapped alignment between source words sj and
si and target words tm and tn.

POS Pair Swaps Total %
(POSi, POSj) → 16 21 76
(POSk, POSl) → 1 1 100
(POSn, POSo) → 1 10 10

(b) Example set of learned swap rules. Swaps counts the
number of times the given (child, parent) pair is seen
in a swap configuration in the source side, and total
is the number of times said pair occurs overall.

Figure 4: Example swap configuration and col-
lected statistics.

j

l m n

o p

h

i k l

m n

o p

h

i k

j

Figure 5: Swap operation: on the left is the
original tree; on the right is the tree after
swapping node l with its parent j.

a swap operation, the corresponding nodes in
the projected tree will be swapped, as illus-
trated in Figure 5.

3.3 Spontaneous Reattachment
Target spontaneous words are difficult to han-
dle because they do not align to any source
word and thus there is nothing to project to
them. To address this problem, we collect two
types of information from the training data.
First, we keep track of all the lexical items
that appear in the training trees, and the rel-
ative position of their head. This lexical ap-
proach may be useful in handling closed-class
words which account for a large percentage of
spontaneous words. Second, we use the train-
ing trees to determine the favored attachment
direction for the language as a whole.
At the test time, for each spontaneous word

in the target sentence, if it is one of the words
for which we have gathered statistics from the
training data, we attach it to the next word
in the preferred direction for that word. If the

word is unseen, we attach it using the overall
language preference as a backoff.

3.4 Parser Enhancements
In addition to above enhancements to the pro-
jection algorithm itself, we train a dependency
parser on the training data, with new features
from the projected trees following Georgi et al.
(2012b). Furthermore, we add features that
indicate whether the current word appears in
a merge or swap configuration. The results
of this combination of additional features and
improved projection is shown in Table 1(b).

4 Results

For evaluation, we use the same data sets as
in Georgi et al. (2012b), where there is a small
number (ranging from 46 to 147) of tree pairs
for each of the eight languages. The IGT
instances for those tree pairs come from the
Hindi Treebank (Bhatt et al., 2009) and the
Online Database of Interlinear Text (ODIN)
(Lewis and Xia, 2010).
We ran 10-fold cross validation and reported

the average of 10 runs in Table 1. The top ta-
ble shows the accuracy of the projection algo-
rithm, and the bottom table shows parsing ac-
curacy of MSTParser with or without adding
features from the projected trees. In both ta-
bles, the Best row uses the enhanced projec-
tion algorithm. The Baseline rows use the
original projection algorithm in Quirk et al.
(2005) where the word in the parentheses in-
dicates the direction of merge. The Error Re-
duction row shows the error reduction of the
Best system over the best performing baseline
for each language. The No Projection row in
the second table shows parsing results when
no features from the projected trees are added
to the parser, and the last row in that table
shows the error reduction of the Best row over
the No Projection row.
Table 1 shows that using features from the

projected trees provides a big boost to the
quality of the statistical parser. Furthermore,
the enhancements laid out in Section 3 im-
prove the performance of both the projection
algorithm and the parser that uses features
from projected trees. The degree of improve-
ment may depend on the properties of a par-
ticular language pair and the labeled data we

309



(a) The accuracies of the original projection algorithm (the Baselin rows) and the enhanced algorithm (the Best
row) on eight language pairs. For each language, the best performing baseline is in italic. The last row shows
the error reduction of the Best row over the best performing baseline, which is calculated by the formula
ErrorRate = Best−BestBaseline100−BestBaseline × 100

YAQ WLS HIN KKN GLI HUA GER MEX
Best 88.03 94.90 77.44 91.75 87.70 90.11 88.71 93.05
Baseline (Right) 87.28 89.80 57.48 90.34 86.90 79.31 88.03 89.57
Baseline (Left) 84.29 89.80 68.11 88.93 76.98 79.54 88.03 89.57
Error Reduction 5.90 50.00 29.26 14.60 6.11 51.66 5.68 33.37

(b) The parsing accuracies of the MSTParser with or without new features extracted from projected trees. There
are two error reduction rows: one is with respect to the best performing baseline for each language, the other
is with respect to No Projection where the parser does not use features from projected trees.

YAQ WLS HIN KKN GLI HUA GER MEX
Best 89.28 94.90 81.35 92.96 81.35 88.74 92.93 93.05
Baseline (Right) 88.28 94.22 78.03 92.35 80.95 87.59 90.48 92.43
Baseline (Left) 87.88 94.22 79.64 90.95 80.95 89.20 90.48 92.43
No Projection 66.08 91.32 65.16 80.75 55.16 72.22 62.72 73.03
Error Reduction (BestBaseline) 8.53 11.76 8.40 7.97 2.10 -4.26 25.74 8.19
Error Reduction (No Projection) 68.39 41.24 46.47 63.43 58.41 59.47 81.04 74.23

Table 1: System performance on eight languages: Yaqui (YAQ), Welsh (WLS), Hindi (HIN),
Korean (KKN), Gaelic (GLI), Hausa (HUA), German (GER), and Malagasy (MEX).

have for that language pair. For instance,
swap is quite common for the Hindi-English
pair because postpositions depend on nouns
in Hindi whereas nouns depend on preposi-
tions in English. As a result, the enhancement
for the swapped alignment alone results in a
large error reduction, as in Table 2. This ta-
ble shows the projection accuracy on the Hindi
data when each of the three enhancements is
turned on or off. The rows are sorted by de-
scending overall accuracy, and the row that
corresponds to the system labeled “Best” in
Table 1 is in bold.

5 Conclusion

Existing projection algorithms suffer from the
effects of structural divergence between lan-
guage pairs. We propose to learn common di-
vergence types from a small number of tree
pairs and use the learned rules to improve pro-
jection accuracy. Our experiments show no-
table gains for both projection and parsing
when tested on eight language pairs. As IGT
data is available for hundreds of languages
through the ODIN database and other sources,
one could produce a small parallel treebank
for a language pair after spending a few hours
manually correcting the output of a projec-
tion algorithm. From the treebank, a bet-
ter projection algorithm and a better parser
can be built automatically using our approach.

Spont Swap Merge Direction Accuracy
X X Left 78.07
X X Informed 77.44

X Left 76.69
X Informed 76.06

X Left 69.49
X Informed 68.96

Left 68.11
Informed 67.58

X X Right 66.32
X Right 64.97

X Right 58.84
Right 57.48

Table 2: Projection accuracy on the Hindi
data, with the three enhancements turning
on or off. The “spont” and “swap” columns
show a checkmark when the enhancements
are turned on. The merge direction indicates
whether a left or right choice was made as a
baseline, or whether the choice was informed
by the rules learned from the training data.

While the improvements for some languages
are incremental, the scope of coverage for this
method is potentially enormous, enabling the
rapid creation of tools for under-resourced lan-
guages of all kinds at a minimal cost.

Acknowledgment

This work is supported by the National Sci-
ence Foundation Grant BCS-0748919. We
would also like to thank the reviewers for help-
ful comments.

310



References

Rajesh Bhatt, Bhuvana Narasimhan, Martha
Palmer, Owen Rambow, Dipti Misra
Sharma, and Fei Xia. A multi-
representational and multi-layered treebank
for Hindi/Urdu. In ACL-IJCNLP ’09: Pro-
ceedings of the Third Linguistic Annotation
Workshop. Association for Computational
Linguistics, August 2009.

Bonnie Jean Dorr. Machine translation di-
vergences: a formal description and pro-
posed solution. Computational Linguistics,
20:597–633, December 1994.

R Georgi, F Xia, and W D Lewis. Measur-
ing the Divergence of Dependency Struc-
tures Cross-Linguistically to Improve Syn-
tactic Projection Algorithms. In Proceedings
of the Sixth International Conference on
Language Resources and Evaluation (LREC
2012), Istanbul, Turkey, May 2012a.

Ryan Georgi, Fei Xia, and William D Lewis.
Improving Dependency Parsing with Inter-
linear Glossed Text and Syntactic Projec-
tion. In Proceedings of the 24th Interna-
tional Conference on Computational Lin-
guistics (COLING 2012), Mumbai, India,
December 2012b.

Rebecca Hwa, Philip Resnik, Amy Weinberg,
Clara Cabezas, and Okan Kolak. Bootstrap-
ping parsers via syntactic projection across
parallel texts. Natural Language Engineer-
ing, 1(1):1–15, 2004.

William D Lewis and Fei Xia. Developing
ODIN: A Multilingual Repository of Anno-
tated Language Data for Hundreds of the
World’s Languages. 2010.

R. McDonald, F. Pereira, K. Ribarov, and
J. Hajič. Non-projective dependency parsing
using spanning tree algorithms. Proceedings
of the conference on Human Language Tech-
nology and Empirical Methods in Natural
Language Processing, pages 523–530, 2005.

Chris Quirk, Arul Menezes, and Colin Cherry.
Dependency treelet translation: Syntacti-
cally informed phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics. Mi-
crosoft Research, 2005.

Fei Xia and William D Lewis. Multilin-
gual Structural Projection across Interlin-
ear Text. In Human Language Technologies:
The Annual Conference of the North Amer-
ican Chapter of the Association for Compu-
tational Linguistics (NAACL), 2007.

David Yarowsky and Grace Ngai. Inducing
multilingual POS taggers and NP bracketers
via robust projection across aligned corpora.
In Second meeting of the North American
Association for Computational Linguistics
(NAACL), Stroudsburg, PA, 2001. Johns
Hopkins University.

311


