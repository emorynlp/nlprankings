



















































RelAgent: Entity Detection and Normalization for Diseases in Clinical Records: a Linguistically Driven Approach


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 477–481,
Dublin, Ireland, August 23-24, 2014.

RelAgent: Entity Detection and Normalization for Diseases in
Clinical Records: a Linguistically Driven Approach

Sv Ramanan
RelAgent Tech Pvt Ltd

Adyar, Chennai
India

ramanan@relagent.com

Senthil Nathan
RelAgent Tech Pvt Ltd

Adyar, Chennai
India

senthil@relagent.com

Abstract

We refined the performance of Co-
coa/Peaberry, a linguistically moti-
vated system, on extracting disease en-
tities from clinical notes in the train-
ing and development sets for Task 7.
Entities were identified in noun chunks
by use of dictionaries, and events (‘The
left atrium is dilated’) through our own
parser and predicate-argument struc-
tures. We also developed a mod-
ule to map the extracted entities to
the SNOMED subset of UMLS. The
module is based on direct matching
against UMLS entries through regu-
lar expressions derived from a small
set of morphological transformations,
along with priority rules when multi-
ple UMLS entries were matched. The
performance on training and develop-
ment sets was 81.0% and 83.3% respec-
tively (Task A), and the UMLS match-
ing scores were respectively 75.3% and
78.2% (Task B). However, the perfor-
mance against the test set was low
by comparison, 72.0% for Task A and
63.9% for Task B, even while the pure
UMLS mapping score was reasonably
high (relaxed score in Task B = 91.2%).
We speculate that our moderate perfor-
mance on the test set derives primarily
from chunking/parsing errors.

1 Introduction

The increasing use of electronic health records,
both for satisfying mandatory requirements as

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence
details: http://creativecommons.org/licenses/by/
4.0/

well as for administrative reasons, has cre-
ated a need for systems to automatically tag
and normalize disease/sign/symptom men-
tions. Statistically significant correlations
extracted from automated analysis of large
databases of clinical records are felt to be use-
ful in detecting phenotype-genotype correla-
tions (reviewed in Kohane (2011)), phenotype-
phenotype correlations (Roque et al., 2011) as
well as in continuous monitoring of events such
as adverse reactions and even early detection
of outbreaks of epidemics/infectious diseases
(Botsis et al., 2013; Collier, 2012). In this
context, Task 7 of SemEval 2014, which is
a continuation of the ShARe/CLEF eHealth
2013 task (Pradhan et al., 2013), provides a
testbed to evaluate systems that automatically
tag and normalize mentions of diseases, signs
and symptoms in clinical records, which in-
clude discharge summaries and echo, radiology
and ECG reports.

Our system consists of (i) Cocoa, a chunk-
based entity tagger and (ii) Peaberry, a parser,
followed by a module for predicate-argument
structure. We have tested the system in a va-
riety of tasks, such as detecting and normal-
izing mentions of chemicals, proteins/genes,
diseases and action terms in the BioCreative
13 Chemdner and CTD tasks (Ramanan and
Senthil Nathan, 2013a; Ramanan and Senthil
Nathan, 2013b), as well as in detecting cel-
lular and pathological events in the BioNLP
cancer genetics task (Ramanan and Senthil
Nathan, 2013c); we also participated in the
eHealth 2013 task (Ramanan et al., 2013d).
Throughout, we have retained a common core
platform for simultaneous detection of a mul-
tiplicity of entity types as well as for chunk-
ing and parsing; we restrict task-specific op-
timization primarily to post-processing mod-
ules. While this strategy may not be optimal

477



for any individual task, we feel that it is neces-
sary for multi-document spanning tasks such
as literature-based discovery (Swanson, 1988),
where connections are established across a va-
riety of scales, e.g. from molecular events
to patho-physiological phenotypes. Moreover,
these linkages need to be made across a mul-
tiplicity of documents from various sources,
which encompass a linguistic range from com-
plex syntactical utterances in biomedical pub-
lications to free-form phrase-centered clinical
notes.

We refined performance against the pro-
vided training and development sets, with
reasonable performance in Task A (relaxed
f = 0.94, strict f = 0.81 − 0.83, strict recall
0.80 − 0.82). A module to match text from
gold-annotated exact spans to UMLS codes
also achieved reasonable performance for Task
B (relaxed accuracy = 0.94−96). However, the
results against from the test set were quite low
for Task A, (relaxed f = 0.87, strict f = 0.72,
strict recall = 0.70) as well as for Task B
(strict f = 0.64). Comparatively, the module
for UMLS normalization fared better (relaxed
f = 0.91 in Task B). We speculate that the
test set contains entities that are rare in the
training/development sets which were chun-
ked incorrectly, and also that the parse errors
in the test set arose from syntactic structures
missing in the training sets. It is possible that
a post-processing statistical module trained on
a combination of gold annotations as well as
linguistic output may be needed for improv-
ing the performance of our system on clinical
notes.

2 System description

The basic structure of the entity-tagging
system is unchanged from that used in
Share/CLEF eHealth 13 (Pradhan et al.,
2013) and BioNLP-ST 13. In summary, the
system comprises of a sentence splitter, fol-
lowed by a TBL-based POS tagger and chun-
ker, entity tagging at the single-token level,
a module to handle multi-word entities, a
noun phrase coordination module, a depen-
dency parser (Ramanan and Senthil Nathan,
2013c), and finally a semantic module to tag
disease-related events.

The generic system has dictionaries and

morphological rules for detecting diseases and
body parts. However, there are many exten-
sions needed for clinical notes, which (i) make
extensive use of common words and phrases
for describing symptoms, which requires word
sense disambiguation, (ii) use unusual phrases
for signs and symptoms and (iii) are full of
undefined acronyms. We isolated such special-
ization to disease-related entities within noun
phrases in clinical documents inside a subrou-
tine in the multi-word tagger module. These
were identified by a frequency-based analysis
of words and phrases in the training and de-
velopment corpora. Thus, a few ambiguous
words and phrases such as ’crackles’, ’com-
plaints’, ’mass effect’ and ’focal consolidation’
were tagged as disease markers regardless of
context. Generally, however, even common
clinical words such as ‘redness’ and ‘swelling’
were tagged only in the presence of neighbor-
ing context words. The appearance of major
body parts such as ‘Abdomen’, ‘Neck, ‘Ex-
tremities’ at the beginning of a line followed
by a colon or a hyphen was taken as a dis-
course reference marker for the rest of the line
to tag acronyms such as ‘NT/ND’ and dan-
gling adjectives such as ‘soft’ and ‘warm’. Very
common acronyms (≈ 100) both for anatomi-
cal parts (‘LUQ’) and diseases (‘DMII’) were
also tagged inside the specialized subroutine,
as were common abbreviations (‘regurg’ for re-
gurgitation) and words with common spelling
errors. Finally, some event/process words
which we found to almost always represent
clinical conditions in the training text were
tagged as disease markers. Examples are ‘as-
piration’, agitation’ and ‘confusion’.

We also extended our generic event pro-
cessing module with a task-specific routine
to take into account descriptions of (mostly)
signs/symptoms specific to clinical documents.
These fall into several categories: (i) abnor-
mal changes in body parts or organ systems,
such as ‘The left atrium was moderately en-
larged’, ‘Nose is bloody’ and ’redistribution of
pulmonary blood flow’ (ii) symptoms such as
’The patient was unable to walk’, ’His speech
was slurred’, ’He had difficulty breathing’ and
’alteration of consciousness’ (iii) changes in pa-
rameters marked by phrases/clauses such as
’elevation of troponin’, ‘QR interval was pro-

478



longed’ and ’decreased blood sugar’. Certain
environmental conditions such as ‘exposure to
asbestos’ were also handled. Finally, events
with a default animate theme were tagged
regardless of their actual arguments to han-
dle sentences/phrases where our syntax mod-
ule failed to extract the correct theme or the
theme is to be inferred from the discourse; the
≈ 40 words in this set included verbs such
as ‘vomit’, ‘shivering’, ‘lethargic’, ‘violent’ and
‘somnolent’.

The above treatment served to demar-
cate spans for diseases that overlap with
the gold annotations. The system merges
words/phrases denoting a body part with ad-
joining words that denote diseases, and also
merges words denoting severity into the dis-
ease span, since our system design strategy
was to generate the longest contiguous span
that can refer to a disease. However, the pri-
mary score in the shared task are with re-
spect to exact matches with the gold anno-
tations. We therefore wrote a small post-
processing module to omit words in an approx-
imate match that refer to severity (‘acute’) as
well as to excise phrases dealing with intra-
organ parts or their location (such as ‘lobes’
or ‘left/right’) - such words/phrases are usu-
ally omitted from the UMLS descriptions of
diseases to which the gold annotations hew
closely. Also, we noticed that certain words
such as ‘wounds’ and ’lesions’ do not embed
an anatomical entity within their description
in the gold annotations. Yet another point is
that, while parameters are marked up as in-
dicative of a symptom only when they take on
abnormal values (‘elevated LDL’), the direc-
tion of change is almost always omitted from
the gold annotations. Descriptors of the pa-
tient (‘He’) are also excised. Altogether, we
constructed about 40 rules to trim the approx-
imate span into one more conformant to the
exact form in the gold annotations.

Task B requires mapping diseases phrases
into the SNOMED subset of UMLS as spec-
ified in the task description. We proceeded
on the assumption that the exact (gold)
entity spans were constructed by annota-
tors to closely map into the UMLS descrip-
tions. Accordingly, we used the text as
defined by the gold spans and attempted

to map them directly into the UMLS def-
initions after some preprocessing steps that
constructed a regular expression: (a) com-
mon spelling errors were corrected (b) body
part and disease acronyms were expanded
(c) common variants were added as alter-
nates i.e. ‘tumou?rs?’ were expanded into
‘(tumou?r|neoplasm|carcinoma)s?’ (d) adjec-
tival and nominal variants were added e.g.
both ‘atrium’ and ‘atrial’ were converted into
‘(atri)(al?|um)’, and more generally, adjec-
tival endings were generalized, for example,
the ending ‘ic’ was converted into ‘(i[ac]|ism)’.
(e) singular and plural forms were converted
into choices e.g. ‘artery’ was rendered as
‘arter(y|ies)’.

Altogether, we have ≈ 120 rules for vari-
ant morphological forms, covering adjectives,
nouns and number. The resulting regular ex-
pression was directly matched (using ‘grep’)
against UMLS text entries. Generally, sev-
eral matches were found. Matches against the
defining entry (the first one) were prioritized,
otherwise the entry with the largest CUID was
taken. Finally, we noted that some UMLS
CUID’s were preferred to others; for exam-
ple, ‘C0007115 - Malignant neoplasm of thy-
roid’ is preferred to ‘C0549473 - Thyroid car-
cinoma’. The preferred choices were inferred
from gold annotation frequencies, and corre-
spond to ≈ 100 remapping rules.

3 Results and Discussion

With a few minor changes to the system used
in the Share/CLEF 2013, we obtained a re-
laxed f-measure in Task A of 0.88 in the
training and development sets. Thereafter
we alternately refined performance in Task A
against the provided training set using the
development set as a testbed, or vice versa.
As described in the last section, these re-
finements took the form of adding context-
sensitive rules for disease-related words and
phrases in order of their frequencies in the
training/development sets. While we could
thereby improve performance against both
training and development sets (relaxed f =
0.94), we noticed that improvements in the
performance against the training set did not
correlate with better performance against the
development set and vice versa, probably im-

479



plying that 6% or more of the entities are
unique to each set, or that we were unable to
catch similarities. A similar orthogonal situa-
tion resulted in our attempt to improve perfor-
mance against exact matches on the training
and development sets, strict f = 0.81 − 0.83,
strict recall 0.80 − 0.82. The observation of
orthogonal entity sets in different datasets for
about 6% of entities is seemingly validated in
the test set, where the results showed a re-
laxed f = 0.87, which is quite close to the
baseline performance (0.88 in the Share/CLEF
2013 task); the highest scoring system had re-
laxed f = 0.91 by comparison. We speculate
that our insistence on contextual clues for en-
tity tagging is another cause for low relaxed
performance on the test set.

Performance of the system for exact
matches on the test set (strict f = 0.72)
suffered greatly in comparison to the train-
ing/development sets. This could be partly
ascribed to the 7% lower performance on the
relaxed f-score (i.e. we missed many entities
altogether) from 0.94 in training/development
sets to 0.87 in the test set. Even account-
ing for this, there is an additional perfor-
mance drop of about 3−4% in exact match on
the test set compared to training/development
sets. One implication is that that our rule-
base method for pruning approximate matches
to exact spans is probably sub-optimal, and
should be supplemented or replaced by a sta-
tistical algorithm. As noted earlier, gold anno-
tations are probably made by annotators with
respect to UMLS definitions, and have some
degree of arbitrariness associated with them
depending on the granularity of the UMLS def-
inition e.g. in the choice of whether to remove
or retain a body location in the gold span.
Given the size of the UMLS definition set, a
statistical approach is probably likely to do
better than a rule-based system in the task of
reducing approximate matches to exact spans.

The poor performance in Task A (strict
recall = 0.70) directly impinges on our low
‘strict’ score in Task B (= 0.64); this score is
simply a product of the strict recall in Task A
and the accuracy of mapping to UMLS, where
the latter score is given by the Task B ‘relaxed’
score (= 0.91). An interesting feature is the
mapping accuracy for our system on the test

set suffered a relatively small drop when com-
pared to the mapping accuracies on the train-
ing and development sets, which were 0.94 and
0.96 respectively. We interpret this reasonably
high figure for the mapping score (the best
among the top 10+ teams in Task B) as vali-
dation of our hypothesis that gold annotations
are made with respect to UMLS definitions,
which also strengthens the case (made above)
for the need to incorporate a (semi-)statistical
approach for pruning overlap matches to exact
matches in our system.

Clinical documents are terse and full of
phrasal observations and incomplete sen-
tences, often with missing punctuation. We
have adapted a linguistically based system
to detect disease-related entities and events
with moderate performance; our observation
on the training/development sets is that most
errors arise from parsing/ chunking errors
on grammatically incomplete phrases. The
second task, namely mapping disease-related
entities/events to SNOMED/UMLS, requires
tagged entity spans to correspond closely to
UMLS definitions; system performance in this
regard can probably be usefully supplemented
by statistical approaches. Given proper entity
spans, a small set of morphological transfor-
mations gives high performance in mapping
to UMLS ID’s. We speculate that a chunk-
annotated corpus of clinical records may help
in improving performance for linguistically de-
rived systems.

References

Isaac S. Kohane. 2011. Using electronic health
records to drive discovery in disease genomics.
Nat Rev Genet. 2011 Jun;12(6):417-28.

Francisco S. Roque, Peter B. Jensen, Henri-
ette Schmock, Marlene Dalgaard, Massimo An-
dreatta, Thomas Hansen, Karen Soeby, Soren
Bredkjor, Anders Juul, Thomas Werge, Lars J.
Jensen and Soren Brunak. 2011. Using elec-
tronic patient records to discover disease corre-
lations and stratify patient cohorts. PLoS Comp.
Bio. 7(8):e1002141.

Sv Ramanan and Senthil Nathan. 2013. Perfor-
mance of a multi-class biomedical tagger on the
BioCreative IV CTD task. Proceedings of the
Fourth BioCreative Challenge Evaluation Work-
shop vol. 1. Bethesda, MD.

480



Sv Ramanan and Senthil Nathan. 2013. Adapt-
ing Cocoa a multi-class entity detector for the
CHEMDNER task of BioCreative IV. Proceed-
ings of the Fourth BioCreative Challenge Eval-
uation Workshop vol. 2. Bethesda, MD.

Sv Ramanan and Senthil Nathan. 2013. Perfor-
mance and limitations of the linguistically moti-
vated Cocoa/Peaberry system in a broad biomed-
ical domain. Proceedings of Workshop. BioNLP
Shared Task 2013. ACL. Sofia.

Sv Ramanan, Shereen Broido and Senthil Nathan.
2013. Performance of a multi-class biomedi-
cal tagger on clinical records. Proceedings of
ShARe/CLEF eHealth Evaluation Labs.

Don R. Swanson. 1988. Migraine and Magnesium:
Eleven Neglected Connections. Persp. Bio. Med.
31(4), 526-557.

Sameer Pradhan, Noemie Elhadad, Brett R.
South, David Martinez, Lee Christensen, Amy
Vogel, Hanna Suominen, Wendy W. Chapman
and Guergana Savova. 2013. Online Work-
ing Notes of the CLEF 2013 Evaluation Labs
and Workshop. Proceedings of ShARe/CLEF
eHealth Evaluation Labs, 23-26 September, Va-
lencia, Spain

Taxiarchis Botsis , Michael D. Nguyen , Emily
J. Woo, Marianthi Markatou and Robert Ball.
2011. Text mining for the Vaccine Adverse
Event Reporting System: medical text classifica-
tion using informative feature selection. J Am
Med Inform Assoc. 2011 Sep-Oct;18(5):631-8

Nigel Collier. 2012. Uncovering text mining: A
survey of current work on web-based epidemic
intelligence. Glob Public Health. Aug 2012;
7(7): 731-749.

481


