Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 689–697,

Beijing, August 2010

689

Exploring variation across biomedical subdomains

Tom Lippincott and Diarmuid ´O S´eaghdha and Lin Sun and Anna Korhonen

Computer Laboratory

University of Cambridge

{tl318,do242,ls418,alk23}@cam.ac.uk

Abstract

Previous research has demonstrated the
importance of handling differences be-
tween domains such as “newswire” and
“biomedicine” when porting NLP systems
from one domain to another. In this paper
we identify the related issue of subdomain
variation, i.e., differences between subsets
of a domain that might be expected to be-
have homogeneously. Using a large corpus
of research articles, we explore how subdo-
mains of biomedicine vary across a variety
of linguistic dimensions and discover that
there is rich variation. We conclude that
an awareness of such variation is necessary
when deploying NLP systems for use in
single or multiple subdomains.

Introduction

1
One of the most noticeable trends in the past
decade of Natural Language Processing (NLP) re-
search has been the deployment of language pro-
cessing technology to meet the information re-
trieval and extraction needs of scientists in other
disciplines. This meeting of ﬁelds has proven mu-
tually beneﬁcial: scientists increasingly rely on
automated tools to help them cope with the expo-
nentially expanding body of publications in their
ﬁeld, while NLP researchers have been spurred to
address new conceptual problems in theirs. Among
the fundamental advances from the NLP perspec-
tive has been the realisation that tools which per-
form well on textual data from one source may fail
to do so on another unless they are tailored to the
new source in some way. This has led to signiﬁ-
cant interest in the idea of contrasting domains and
the concomitant problem of domain adaptation,

as well as the production of manually annotated
domain-speciﬁc corpora.1

One deﬁnition of domain variation associates
it with differences in the underlying probability
distributions from which different sets of data are
drawn (Daum´e III and Marcu, 2006). The concept
also mirrors the notion of variation across thematic
subjects and the corpus-linguistic notions of reg-
ister and genre (Biber, 1988). In addition to the
differences in vocabulary that one would expect
to observe, domains can vary in many linguistic
variables that affect NLP systems. The scientiﬁc
domain which has received the most attention (and
is the focus of this paper) is the biomedical domain.
Notable examples of corpus construction projects
for the biomedical domain are PennBioIE (Kulick
et al., 2004) and GENIA (Kim et al., 2003). These
corpora have been used to develop systems for a
range of processing tasks, from entity recognition
(Jin et al., 2006) to parsing (Hara et al., 2005) to
coreference resolution (Nguyen and Kim, 2008).
An implicit assumption in much previous work
on biomedical NLP has been that particular subdo-
mains of biomedical literature – typically molec-
ular biology – can be used as a model of biomed-
ical language in general. For example, GENIA
consists of abstracts dealing with a speciﬁc set
of subjects in molecular biology, while PennBioIE
covers abstracts in two specialised domains, cancer
genomics and the behaviour of a particular class
of enzymes. This assumption of representative-
ness is understandable because linguistic annota-
tion is labour-intensive and it may not be worth-
while to produce annotated corpora for multiple
subdomains within a single discipline if there is lit-

1A workshop dedicated to domain adaptation is collocated

with ACL 2010.

690

tle task-relevant variation across those subdomains.
However, such conclusions should not be made
before studying the actual degree of difference be-
tween the subdomains of interest.

One of the principal goals of this paper is to map
how the concept of “biomedical language”, often
construed as a monolithic entity, is composed of
diverse patterns of behaviour at more ﬁne-grained
topical levels. Hence we study linguistic variation
in a broad biomedical corpus of abstracts and full
papers, the PMC Open Access Subset.2 We select
a range of lexical and structural phenomena for
quantitative investigation. The results indicate that
common subdomains for resource development are
not representative of biomedical text in general and
furthermore that different linguistic features often
partition the subdomains in quite different ways.

2 Related Work
A number of researchers have explored the dif-
ferences between non-technical and scientiﬁc lan-
guage. Biber and Gray (2010) describe two
distinctive syntactic characteristics of academic
writing which set it apart from general English.
Firstly, in academic writing additional information
is most commonly integrated by pre- and post-
modiﬁcation of phrases rather than by the addi-
tion of extra clauses. Secondly, academic writing
places greater demands on the reader by omitting
non-essential information, through the frequent
use of passivisation, nominalisation and noun com-
pounding. Biber and Gray also show that these ten-
dencies towards “less elaborate and less explicit”
language have become more pronounced in recent
history.

We now turn to corpus studies that focus on
biomedical writing. Verspoor et al. (2009) use
measurements of lexical and structural variation
to demonstrate that Open Access and subscription-
based journal articles in a speciﬁc domain (mouse
genomics) are sufﬁciently similar that research on
the former can be taken as representative of the lat-
ter. While their primary goal is different from ours
and they do not consider variation across multiple
domains, they do compare their mouse genomics
corpus with small reference corpora drawn from

2http://www.ncbi.nlm.nih.gov/pmc/

about/openftlist.html

newswire and general biomedical sources. This
analysis unsurprisingly ﬁnds differences between
the domain and newswire corpora across many
linguistic dimensions; more interestingly for our
purposes, the comparison of domain text to the
broader biomedical superdomain shows a more
complex picture with similarities in some aspects
(e.g., passivisation and negation) and dissimilari-
ties in others (e.g., sentence length, semantic fea-
tures).

Friedman et al. (2002) document the “sublan-
guages” associated with two biomedical domains:
clinical reports and molecular biology articles.
They set out restricted ontologies and frequent co-
occurrence templates for the two domains and dis-
cuss the similarities and differences between them,
but they do not perform any quantitative analysis.
Other researchers have focused on speciﬁc phe-
nomena, rather than cataloguing a broad scope
of variation. Cohen et al. (2008) carry out a de-
tailed analysis of argument realisation with respect
to verbs and nominalisations, using the GENIA
and PennBioIE corpora. Nguyen and Kim (2008)
compare the behaviour of anaphoric pronouns in
newswire and biomedical corpora; they improve
the performance of a pronoun resolver by incorpo-
rating their observations, thus demonstrating the
importance of capturing domain-speciﬁc phenom-
ena. Nguyen and Kim’s ﬁndings are discussed in
more detail in Section 5.4 below.

3 Subdomains in the OpenPMC Corpus

The Open Access Subset of PubMed (OpenPMC)
is the largest publicly available corpus of full-text
articles in the biomedical domain. OpenPMC is
comprised of 169,338 articles drawn from 1233
medical journals, totalling approximately 400 mil-
lion words. The NIH maintains a one-to-many
mapping from journals to 122 subject areas (NIH,
2009b). This covers about 400 of the OpenPMC
journals, but these account for over 70% of the
database by byte size and word count. Journals are
assigned up to ﬁve subject areas with the majority
assigned one (69%) or two (26%) subjects. In this
paper we adopt the OpenPMC subject areas (e.g.
“Pulmonary Medicine”, “Genetics”, “Psychiatry”)
as the basis for subdomain comparison.

691

distribution of data across the subjects we use, by
word-count, with lighter-coloured areas represent-
ing data that is assigned multiple subjects. These
subjects provide a convenient starting point for di-
viding the corpus into subdomains (hereafter, “sub-
domain” will be used rather than “subject”). We
also add a reference subdomain, “Newswire”, com-
posed of a 6 million word random sample from the
English Gigaword corpus (Graff et al., 2005). The
ﬁnal data set has a total of 39 subdomains.

Articles in the OpenPMC corpus are formatted
according to a standard XML tag set (NIH, 2009a).
We ﬁrst convert each article to plain text, ignoring
“non-content” elements such as tables and formulas,
and split the result into sentences, aggregating the
results by subdomain.

4.2 Feature extraction
We investigate subdomain variation in our cor-
pus across a range of lexical, syntactic, sentential
and discourse features. The corpus is lemmatised,
tagged and parsed using the C&C pipeline (Cur-
ran et al., 2007) with the adapted part-of-speech
and lexical category tagging models produced by
Rimell and Clark (2009) for biomedical parsing.

From this output we count occurrences of noun,
verb, adjective and adverb lemmas, part-of-speech
(POS) tags, grammatical relations (GRs), chunks,
and lexical categories. The lemma features are
Zipﬁan-distributed items from an open class, so
we have experimented with ﬁltering low-frequency
items at various thresholds to reduce noise and
improve processing speed. The other feature sets
can be viewed as closed classes, where ﬁltering is
unnecessary.

Since verbs are central to the meaning and struc-
ture of sentences, we consider their special behav-
ior by constructing features for each verb’s dis-
tribution over other grammatical properties. Sev-
eral grammatical properties are captured by pairing
each verb with its POS (indicating e.g. tense, such
as present, past, and present participle). Voice is de-
termined from additional annotation output by the
C&C parser. Table 1 shows the POS-distribution
for the verb “restrict”, in two subdomains from
the corpus. Finally, we record distributions over
verb subcategorization frames (SCFs) taken by
each verb, and over the GRs it participates in.

Figure 1: OpenPMC word count by subdomain,
dark colouring indicates data assigned single sub-
domain, each lighter shade indicates an additional
overlapping subdomain

4 Methodology
4.1 Data selection and preprocessing
An important initial question was how to treat data
with multiple classiﬁcations: we only consider
journals assigned a single subdomain, to avoid
the added complexity of interactions in data from
overlapping subdomains. To ensure sufﬁcient data
for comparing a variety of linguistic features, we
discard the subdomains with less than one mil-
lion words meeting the single-subdomain criterion.
After review, we also drop the “Biology” subdo-
main, which appears to function as a catch-all for
many loosely related areas. Figure 1 shows the

Genetics
Medical Informatics
Neoplasms
Molecular Biology
Biochemistry
Public Health
Environmental Health
Microbiology
Medicine
Biotechnology
Neurology
Science
Communicable Diseases
Cell Biology
Rheumatology
Critical Care
Tropical Medicine
Physiology
Pulmonary Medicine
Endocrinology
Virology
Botany
Vascular Diseases
Ophthalmology
Genetics, Medical
Embryology
Psychiatry
Biomedical Engineering
Veterinary Medicine
Pediatrics
Gastroenterology
Geriatrics
Pharmacology
Obstetrics
Education
Complementary Therapies
Ethics
0

10

20

30

40

Word count (millions)

692

Subdomain
Medical Informatics
Cell Biology

VB VBG VBN VBP VBZ
.35
.14

.29
.43

.06
.05

.09
.10

.21
.29

Table 1: Distribution over POS tags for verb “re-
strict”, in two subdomains

SCFs were extracted using a system of Preiss et al.
(2007).

To facilitate a more robust and interpretable anal-
ysis of vocabulary differences, we estimate a “topic
model” of the corpus with Latent Dirichlet Analy-
sis (Blei et al., 2003) using the MALLET toolkit.3
As preprocessing we divide the corpus into arti-
cles, removing stopwords and words shorter than
3 characters. The Gibbs sampling procedure is
parameterised to induce 100 topics, each giving a
coherent cluster of related words learned from the
data, and to run for 1000 iterations. We collate the
predicted distribution over topics for each article
in a subdomain, weighted by article wordcount, to
produce a topic distribution for the subdomain.

4.3 Measurements of divergence
Our goal is to illustrate the presence or absence
of differences between the feature sets, and to do
so we calculated the Jensen-Shannon divergence
and the Pearson correlation. Jensen-Shannon diver-
gence is a ﬁnite symmetric measurement of the di-
vergence between probability distributions, while
Pearson correlation quantiﬁes the linear relation-
ship between two real-valued samples.

The count-features are weighted, for a given
subdomain, by the feature’s log-likelihood be-
tween the subdomain’s data and the rest of the
corpus. Log-likelihood has been shown to perform
well when comparing counts of potentially low-
frequency features (Rayson and Garside, 2000)
such as found in Zipﬁan-distributed data. This
serves to place more weight in the comparison on
items that are distinctive of the subdomain with
respect to the entire corpus.

While the count-features are treated as a single
distribution for the purposes of JSD, the verbwise-
features are composed of many distributions, one
for each verb lemma. Our approach is to com-
bine the JSD of the verbs, weighted by the log-

3http://mallet.cs.umass.edu

likelihood of the verb lemma between the two
subdomains in question, and normalize the dis-
tances to the interval [0, 1]. Using the lemma’s log-
likelihood assumes that, when a verb’s distribution
behaves differently in a subdomain, its frequency
changes as well.

We present the results as dendrograms and
heat maps. Dendrograms are tree structures that
illustrate the results of hierarchical clustering.
We perform hierarchical clustering on the inter-
subdomain divergences for each set of features.
The algorithm begins with each instance (in our
case, subdomains) as a singleton cluster, and re-
peatedly joins the two most similar clusters until
all the data is clustered together. The order of these
merges is recorded as a tree structure that can be
visualized as a dendrogram in which the length of
a branch represents the distance between its child
nodes. Similarity between clusters is calculated us-
ing average distance between all members, known
as “average linking”.

Heat maps show the pairwise calculation of
a metric in a grid of squares, where square
(x, y) is shaded according to the value of
metric(subx, suby). For our measurements of
JSD, black represents 0 (i.e. identical distributions)
and white represents the metric’s theoretical maxi-
mum of 1. We also inscribe the actual value inside
each square. Dendrograms are tree structures that
illustrate the hierarchical clustering procedure de-
scribed above. The dendrograms present all 39
subdomains, while for readability the heatmaps
present 12 subdomains selected for representative-
ness.

5 Results

Different thresholds for ﬁltering low-frequency
terms had little effect on the divergence measures,
and served mainly to improve processing time. We
therefore report results using a cutoff of 150 occur-
rences (over the entire 234 million word data set)
and log-likelihood weights. The results of Pearson
correlation and JSD show similar trends, and due
to its speciﬁc design for comparing distributions
we only report the latter.

693

5.1 Vocabulary and lexical features
Differences in vocabulary are what ﬁrst comes to
mind when describing subdomains. Word features
are fundamental components for systems such as
POS taggers and lexicalised parsers; one therefore
expects that these systems will be affected by vari-
ation in lexical distributions. Figure 2a uses JSD
calculated on each subdomain’s distribution over
100 LDA-induced topics to compare vocabulary
distributions. Subdomains related to molecular
biology (Genetics, Molecular Biology) show the
smallest divergences, an interesting fact since these
are heavily used in building resources for BioNLP.
The dendrogram shows a rough division into “pub-
lic policy”, “patient-centric”, “applied” and “mi-
croscopic” subdomains, with the distance between
unrelated subdomains such as Biochemistry and
Pediatrics almost as large as their respective differ-
ences from Newswire.

We omit ﬁgures for variation over noun, verb
and adjective lemmas due to space restrictions; in
general, these correlate with the variation in LDA
topics though there are some differences. Figure 2b
shows JSD calculated on distributions over adverb
lemmas. Part of the variation is due to character-
istic markers of scientiﬁc argument (“therefore”,
“signiﬁcantly”, “statistically”). A more interesting
factor is the coining of domain-speciﬁc adverbs,
an example of the tendency in scientiﬁc text to use
complex lexical items and premodiﬁers rather than
additional clauses. This also has the effect of mov-
ing subdomain-speciﬁc objects and processes from
verbs and nouns to adverbs. This behavior seems
non-continuous, in that subdomains either make
heavy, or almost no, use of it: for example, Pedi-
atrics has no subdomain-speciﬁc items among the
its ten top adverbs by log-likelihood, while Neo-
plasms has “histologically”, “immunohistochemi-
cally” and “subcutaneously”. These information-
dense terms could prove useful for tasks like auto-
matic curation of subdomain vocabularies, where
they imply relationships between their components,
the items they modify, etc.

5.2 Verb distributional behavior
Modelling verb behavior is important for both syn-
tactic (Collins, 2003) and semantic (Korhonen et
al., 2008) processing, and subdomains are known

to conscript verbs into speciﬁc roles that change the
distributions of their syntactic properties (Roland
and Jurafsky, 1998). The four properties we con-
sidered verbs’ distributions over (SCF, POS, GR
and voice) produced similar inter-subdomain JSD
values. Figure 2c demonstrates how verbs differ
between subdomains with respect to SCFs. For
example, while the Pediatrics subdomain uses the
verb “govern” in a single SCF among its 12 pos-
sibilities, the Genetics subdomain distributes its
usage over 7 of them. Two subdomains may both
use “restrict” with high frequency (e.g. Molecular
Biology and Ethics), but with different frequency
distributions over SCFs.

5.3 Syntax
It is difﬁcult to measure syntactic complexity accu-
rately without access to a hand-annotated treebank,
but it is well-known that sentence length corre-
lates strongly with processing difﬁculty (Collins,
1996). The ﬁrst column of Table 2 gives average
sentence lengths (excluding punctuation and “sen-
tences” of fewer than three words) for selected
domains. All standard errors are < 0.1. It is clear
that all biomedical subdomains typically use longer
sentences than newswire, though there is also vari-
ation within biomedicine, from an average length
of 27 words in Molecular Biology to 24.5 words
in Pediatrics.

“Packaging” information in complex pre- and/or
post-modiﬁed noun phrases is a characteristic fea-
ture of academic writing (Biber and Gray, 2010).
This increases the information density of a sen-
tence but brings with it syntactic and semantic
ambiguities. For example, the difﬁculty of resolv-
ing the internal structure of noun-noun compounds
and strings of prepositional phrases has been the fo-
cus of ongoing research in NLP; these phenomena
have also been identiﬁed as signiﬁcant challenges
in biomedical language processing (Rosario and
Hearst, 2001; Schuman and Bergler, 2006). The
second and third columns of Table 2 present aver-
age lengths for full noun phrases, deﬁned as every
word dominated by a head noun in the grammat-
ical relation graph for a sentence, and for base
nominals, deﬁned as nouns plus premodifying ad-
jectives and nouns only. All standard errors are
≤ 0.01. Newswire text uses the simplest noun

694

(a) LDA-induced distribution over topics

(b) Adverb lemma frequencies

(c) Verb distributions over subcategorization frames

Figure 2: Subdomain variation plotted as heat maps and dendrograms

695

Sentence length

Full NP length

Base nominal length

Mol. Biology
Genetics
Cell Biology
Ethics
PMC Average
Biochemistry
Neoplasms
Psychiatry
Critical Care
Therapeutics
Comm. Diseases
Med. Informatics
Pediatrics
Newswire

27.0 Biochemistry
26.6 Genetics
26.3 Critical Care
26.2 Neoplasms
25.9 PMC Average
25.8 Pediatrics
25.5 Med. Informatics
25.3 Comm. Diseases
25.0 Therapeutics
24.9 Mol. Biology
24.9 Psychiatry
24.6 Ethics
24.6 Cell Biology
19.1 Newswire

4.03 Biochemistry
3.90 Neoplasms
3.86 Mol. Biology
3.85 Genetics
3.85 PMC Average
3.84 Cell Biology
3.84 Critical Care
3.81 Med. Informatics
3.80 Comm. Diseases
3.79 Therapeutics
3.77 Psychiatry
3.69 Pediatrics
3.55 Ethics
3.18 Newswire

1.85
1.85
1.84
1.83
1.80
1.80
1.80
1.78
1.78
1.75
1.75
1.73
1.65
1.60

Table 2: Average sentence, NP and base nominal lengths across domains

phrase structures; there is notable variation across
PMC domains. Full NP and base nominal lengths
do not always correlate; for example, Cell Biol-
ogy uses relatively long base NPs (nominalisations
and multitoken names in particular) but relatively
simple full NP structures.

5.4 Coreference
Resolving coreferential terms is a crucial and chal-
lenging task when extracting information from
texts in any domain. Nguyen and Kim (2008)
compare the use of pronouns in the newswire
and biomedical domains, using the GENIA cor-
pus as representative of the latter. Among the dif-
ferences observed between the domains were the
absence of any personal pronouns other than third-
person neuter pronouns in the GENIA corpus, and
a greater proportion of demonstrative pronouns in
GENIA than in the ACE or MUC newswire cor-
pora. Corroborating the importance of domain
modelling, Nguyen and Kim demonstrate that tai-
loring a pronoun resolution system to speciﬁc prop-
erties of the biomedical domain improves perfor-
mance.

As our corpus is not annotated for coreference
we restrict our attention to types that are reliably
coreferential: masculine/feminine personal pro-
nouns (he, she and case variations), neuter personal
pronouns (they, it and variations) and deﬁnite NPs
with demonstrative determiners such as this and

that. To ﬁlter out pleonastic pronouns we used a
combination of the C+C parser’s pleonasm tag and
heuristics based on Lappin and Leass (1994). To
ﬁlter out the most common class of non-anaphoric
demonstrative NPs we simply discarded any match-
ing the pattern this. . . paper|study|article.
Table 3 presents statistics for selected types of
coreferential noun phrases in a number of domains.
The results generally agree with the ﬁndings of
Nguyen and Kim (2008): biomedical text is on
average 200 times less likely than news text to
use gendered pronouns and twice as likely to use
anaphoric deﬁnite noun phrases. At the domain
level, however, there is clear variation within the
biomedical corpus.
In contrast to Nguyen and
Kim’s observations about GENIA some domains
do make non-negligible use of gendered pronouns,
most notably Ethics (usually to refer to other schol-
ars) and domains such as Psychiatry and Pediatrics
where studies of actual patients are common. All
biomedical domains use demonstrative NPs more
frequently than newswire and only one (Ethics)
matches newswire for frequent use of neuter 3rd-
person pronouns.

6 Conclusion

In this paper we have explored the phenomenon
of linguistic variation at a ﬁner-grained level than
previous NLP research, focusing on subdomains

696

Pronouns (neuter, 3rd)

Pronouns (non-neuter, 3rd)

Demonstrative NPs

Ethics
Newswire
Therapeutics
Med. Informatics
Psychiatry
Pediatrics
PMC Average
Genetics
Critical Care
Mol. Biology
Biochemistry
Neoplasms
Cell Biology
Comm. Diseases

0.0658 Newswire
0.0607 Ethics
0.0354 Pediatrics
0.0346 Psychiatry
0.0342 Comm. Diseases
0.0308 Therapeutics
0.0284 PMC Average
0.0275 Critical Care
0.0272 Neoplasms
0.0258 Med. Informatics
0.0251 Genetics
0.0227 Mol. Biology
0.0217 Biochemistry
0.0213 Cell Biology

0.0591
Genetics
0.0037
Med. Informatics
0.0015
Biochemistry
0.0009
Ethics
0.0009
Mol. Biology
0.0005
PMC Average
0.0005
Cell Biology
0.0004
Comm. Diseases
0.0002
Neoplasms
0.0002
Psychiatry
0.0001
Critical Care
2.5 × 10−5 Therapeutics
2.0 × 10−5
Pediatrics
1.5 × 10−5 Newswire

0.0275
0.0263
0.0263
0.0260
0.0251
0.0226
0.0210
0.0207
0.0205
0.0201
0.0201
0.0192
0.0191
0.0118

Table 3: Frequency of coreferential types (proportion of all NPs) across domains

rather than traditional domains such as “newswire”
and “biomedicine”. We have identiﬁed patterns of
variation across dimensions of vocabulary, syntax
and discourse that are known to be of importance
for NLP applications. While the magnitude of vari-
ation between subdomains is unsurprisingly less
pronounced than between coarser domains, sub-
domain variation clearly does exist and should be
taken into account when considering the generalis-
ability of systems trained and evaluated on speciﬁc
subdomains, for example molecular biology.

Future work includes directly evaluating the ef-
fect of subdomain variation on practical tasks, in-
vestigating further dimensions of variation such
as nominalisation usage and learning alternative
subdomain taxonomies directly from the corpus
text. Ultimately, we expect that a more nuanced
understanding of subdomain effects will have tan-
gible beneﬁts for many applications of scientiﬁc
language processing.

Acknowledgements
This work was supported by EPSRC grant
EP/G051070/1, the Royal Society (AK) and a
Dorothy Hodgkin Postgraduate Award (LS).

References
Biber, Douglas and Bethany Gray. 2010. Challeng-
ing stereotypes about academic writing: Complex-

ity, elaboration, explicitness. Journal of English for
Academic Purposes, 9(1):2–20.

Biber, Douglas. 1988. Variation Across Speech and

Writing. Cambridge University Press, Cambridge.

Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Cohen, K. Bretonnel, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9):e3158.

Collins, Michael John. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of ACL-96, Santa Cruz, CA.

Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589–637.

Curran, James, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the ACL-07 Demo and
Poster Sessions, Prague, Czech Republic.

Daum´e III, Hal and Daniel Marcu. 2006. Domain
adaptation for statistical classiﬁers. Journal of Ar-
tiﬁcial Intelligence Research, 26:101–126.

Friedman, Carol, Pauline Kraa, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222–235.

Graff, David, Junbo Kong, Ke Chen, and Kazuaki
Maeda, 2005. English Gigaword Corpus, 2nd Edi-
tion. Linguistic Data Consortium.

697

Hara, Tadayoshi, Yusuke Miyao, and Jun’ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Pro-
ceedings of IJCNLP-05, Jeju Island, South Korea.

Roland, Douglas and Daniel Jurafsky. 1998. How
verb subcategorization frequencies are affected by
corpus choice. In Proceedings of COLING-ACL-98,
Montreal, Canada.

Rosario, Barbara and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via
a domain-speciﬁc lexical hierarchy. In Proceedings
of EMNLP-01, Pittsburgh, PA.

Schuman, Jonathan and Sabine Bergler. 2006. Post-
in pro-
nominal prepositional phrase attachment
teomics.
In Proceedings of the HLT-NAACL-06
BioNLP Workshop on Linking Natural Language
and Biology, New York, NY.

Verspoor, Karin, K Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of tradi-
tional and Open Access scientiﬁc journals are simi-
lar. BMC Bioinformatics, 10:183.

Jin, Yang, Ryan T. McDonald, Kevin Lerman, Mark A.
Mandel, Steven Carroll, Mark Y. Liberman, Fer-
nando C. Pereira, Raymond S. Winters, and Peter S.
White. 2006. Automated recognition of malignancy
mentions in biomedical literature. BMC Bioinfor-
matics, 7:492.

Kim, J.-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180–
i182.

Korhonen, Anna, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classiﬁca-
tion of verbs in biomedical texts.
In Proceedings
of COLING-08, Manchester, UK.

Kulick, Seth, Ann Bies, Mark Liberman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein,
Lyle Ungar, Scott Winters, and Pete White. 2004.
Integrated annotation for biomedical information ex-
traction.
In Proceedings of the HLT-NAACL-04
Workshop on Linking Biological Literature, Ontolo-
gies and Databases, Boston, MA.

Lappin, Shalom and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535–561.

Nguyen, Ngan L.T. and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical text. In Proceed-
ings of COLING-08, Manchester, UK.

NIH.

2009a.

Journal publishing tag set.

http://dtd.nlm.nih.gov/publishing/.

NIH.

2009b.

of
medicine:
terms.
http://wwwcf.nlm.nih.gov/serials/journals/index.cfm.

National

Journal

subject

library

Preiss, Judita, E.J. Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of ACL-07, Prague, Czech
Republic.

Rayson, Paul and Roger Garside. 2000. Comparing
corpora using frequency proﬁling. In Proceedings
of the ACL-00 Workshop on Comparing Corpora,
Hong Kong.

Rimell, Laura and Stephen Clark.

Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain.
Journal of Biomedical Informatics,
42(5):852–865.

2009.

