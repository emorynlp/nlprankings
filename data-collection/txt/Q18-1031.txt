








































Generating Sentences by Editing Prototypes

Kelvin Guu*2 Tatsunori B. Hashimoto*1,2 Yonatan Oren1 Percy Liang1,2
(* equal contribution)

1Department of Computer Science 2Department of Statistics
Stanford University

{kguu,thashim,yonatano}@stanford.edu pliang@cs.stanford.edu

Abstract

We propose a new generative language model
for sentences that first samples a prototype
sentence from the training corpus and then ed-
its it into a new sentence. Compared to tra-
ditional language models that generate from
scratch either left-to-right or by first sampling
a latent sentence vector, our prototype-then-
edit model improves perplexity on language
modeling and generates higher quality outputs
according to human evaluation. Furthermore,
the model gives rise to a latent edit vector that
captures interpretable semantics such as sen-
tence similarity and sentence-level analogies.

1 Introduction

The ability to generate sentences is core to many
NLP tasks, including machine translation, summa-
rization, speech recognition, and dialogue. Most
neural models for these tasks are based on recur-
rent neural language models (NLMs), which gener-
ate sentences from scratch, often in a left-to-right
manner (Bengio et al., 2003). It is often observed
that such NLMs suffer from the problem of favoring
generic utterances such as “I don’t know” (Li et al.,
2016). At the same time, naive strategies to increase
diversity have been shown to compromise grammat-
icality (Shao et al., 2017), suggesting that current
NLMs may lack the inductive bias to faithfully rep-
resent the full diversity of complex utterances.

Indeed, it is difficult even for humans to write
complex text from scratch in a single pass; we of-
ten create an initial draft and incrementally revise it
(Hayes and Flower, 1986). Inspired by this process,

Overpriced , overrated , and tasteless food .
The food here is ok but not worth the price .

I definitely recommend this restaurante .

Sample from 
the training set

Edit using
a�en�on

Edit Vector
The food here is ok but not worth the price .

Prototype

The food is mediocre and not worth the ridiculous price .

GenerationGeneration

The food is good but not worth the horrible customer service .
The food here is not worth the drama .

The food is not worth the price .

Figure 1: The prototype-then-edit model generates a sen-
tence by sampling a random example from the training set
and then editing it using a randomly sampled edit vector.

we propose a new unconditional generative model
of text which we call the prototype-then-edit model,
illustrated in Figure 1. It first samples a random pro-
totype sentence from the training corpus, and then
invokes a neural editor, which draws a random “edit
vector” and generates a new sentence by attending
to the prototype while conditioning on the edit vec-
tor. The motivation is that sentences from the corpus
provide a high quality starting point: they are gram-
matical, naturally diverse, and exhibit no bias to-
wards shortness or vagueness. The attention mech-
anism (Bahdanau et al., 2015) of the neural editor
strongly biases the generation towards the prototype,
and therefore it needs to solve a much easier prob-
lem than generating from scratch.

We train the neural editor by maximizing
an approximation to the generative model’s log-
likelihood. This objective is a sum over lexically-

437

Transactions of the Association for Computational Linguistics, vol. 6, pp. 437–450, 2018. Action Editor: Trevor Cohn.
Submission batch: 9/2017; Revision batch: 12/2017; Published 7/2018.

c©2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.



similar sentence pairs in the training set, which we
can scalably approximate using locality sensitive
hashing. We also show empirically that most lexi-
cally similar sentences are also semantically similar,
thereby endowing the neural editor with additional
semantic structure. For example, we can use the
neural editor to perform a random walk from a seed
sentence to traverse semantic space.

We compare our prototype-then-edit model to ap-
proaches that generate from scratch on both lan-
guage generation quality and semantic properties.
For the former, our model generates higher qual-
ity generations according to human evaluations, and
improves perplexity by 13 points on the Yelp cor-
pus and 7 points on the One Billion Word Bench-
mark. For the latter, we show that latent edit vec-
tors outperform standard sentence variational au-
toencoders (Bowman et al., 2016) on semantic sim-
ilarity, locally-controlled text generation, and a sen-
tence analogy task.

2 Problem statement

Our primary goal is to learn a generative model of
sentences for use as a language model.1 In partic-
ular, we model sentence generation as a prototype-
then-edit process:

1. Select prototype: Given a training corpus of
sentencesX , randomly sample a prototype sen-
tence x′ from a prototype distribution p(x′) (in
our case, uniform over X ).

2. Edit: Sample an edit vector z (encoding the
type of edit to be performed) from an edit prior
p(z). Then, feed the edit vector z and the previ-
ously selected prototype x′ into a neural editor
pedit(x | x′, z), which generates a new sentence
x.

Under this model, the likelihood of a sentence is:

p(x) =
∑

x′∈X
p(x | x′)p(x′) (1)

p(x | x′) = Ez∼p(z)
[
pedit(x | x′, z)

]
, (2)

1 For many applications such as machine translation or dia-
logue generation, there is a context (e.g. foreign sentence, di-
alogue history), which can be supplied to both the prototype
selector and the neural editor. This paper focuses on the uncon-
ditional case, proposing an alternative to LSTM based language
models.

where both prototype x′ and edit vector z are latent
variables.

Our formulation stems from the observation that
many sentences in a large corpus can be represented
as minor transformations of other sentences. For ex-
ample, in the Yelp restaurant review corpus (Yelp,
2017) we find that 70% of the test set is within word-
token Jaccard distance 0.5 of a training set sentence,
even though almost no sentences are repeated ver-
batim. This implies that a neural editor which mod-
els lexically similar sentences should be an effective
generative model for large parts of the test set.

A secondary goal for the neural editor is to cap-
ture certain semantic properties; we focus on the fol-
lowing two in particular:

1. Semantic smoothness: an edit should be able to
alter the semantics of a sentence by a small and
well-controlled amount, while multiple edits
should make it possible to accumulate a larger
change.

2. Consistent edit behavior: the edit vector z
should model/control the variation in the type
of edit that is performed. When we apply
the same edit vector on different sentences,
the neural editor should perform semantically
analogous edits across the sentences.

In Section 4, we show that the neural editor can
successfully capture both properties, as reported by
human evaluations.

3 Approach

We would like to train our neural editor pedit(x |
x′, z) by maximizing the marginal likelihood (Equa-
tion 1) via gradient ascent, but the objective cannot
be computed exactly because it involves a sum over
all prototypes x′ (expensive) and an expectation over
the edit prior p(z) (no closed form).

We therefore propose two approximations to over-
come these challenges:

1. We lower bound the sum over latent prototypes
x′ (in Equation 1) by only summing over x′ that
are lexically similar to x.

2. We lower bound the expectation over the edit
prior (in Equation 2) using the evidence lower
bound (ELBO) (Jordan et al., 1999; Doersch,
2016) which can be effectively approximated.

438



We describe and motivate these approximations in
Sections 3.1 and 3.2, respectively. In Section 3.3,
we combine the two approximations to give the final
objective. Sections 3.4 and 3.5 drill down further
into our specific model architecture.

3.1 Approximate sum on prototypes, x′

Equation 1 defines the probability of generating a
sentence x as the total probability of reaching x
via edits from every prototype x′ ∈ X . However,
most prototypes are unrelated and should have very
small probability of transforming into x. Therefore,
we approximate the summation over prototypes by
only considering prototypes x′ that have high lex-
ical overlap with x. To that end, define a lexical
similarity neighborhood as:

N (x) def= {x′ ∈ X : dJ(x, x′) < 0.5},

where dJ(x, x′) is the Jaccard distance between x
and x′ (treating each as a set of word tokens).

We will now lower bound log p(x) in two
ways: (i) we will sum over only prototypes in the
neighborhood N (x) rather than over the entire
training set X as discussed above; (ii) we will
push the log inside the summation using Jensen’s
inequality, as is standard with variational lower
bounds. Recall that the distribution over prototypes
is uniform (p(x′) = 1/|X |), and define R(x) =
log(|N (x)|/|X |). The derivation is as follows:

log p(x) = log

[∑

x′∈X
p(x | x′)p(x′)

]

(i)

≥ log


 ∑

x′∈N (x)
p(x | x′)p(x′)


 (3)

= log


|N (x)|−1

∑

x′∈N (x)
p(x | x′)


+R(x)

(ii)

≥ |N (x)|−1
∑

x′∈N (x)
log p(x | x′)

︸ ︷︷ ︸
def
= LEX(x)

+R(x).

Assuming the neighborhood size |N (x)| is constant
across x, then LEX(x) is a lower bound of log p(x)
up to constants. For each x, the neighborhood
N (x) can be efficiently precomputed with locality
sensitive hashing (LSH) and minhashing. The full
procedure is described in Appendix 6.

Note that LEX(x) is still intractable to compute
because each log p(x|x′) term involves an expecta-
tion over the edit prior p(z) (Equation 2). We ad-
dress this in Section 3.2, but first, an interlude.

Interlude: lexical similarity semantics. So far,
we have motivated lexical similarity neighborhoods
via computational considerations, but we found that
lexical similarity training also captures semantic
similarity. One can certainly construct sentences
with small lexical distance that differ semantically
(e.g., insertion of the word “not”). However, since
we mine sentences from a corpus grounded in real
world events, most lexically similar sentences are
also semantically similar. For example, given “my
son enjoyed the delicious pizza”, we are far more
likely to see “my son enjoyed the delicious maca-
roni”, versus “my son hated the delicious pizza”.

Human evaluations of 250 edit pairs sampled
from lexical similarity neighborhoods on the Yelp
corpus support this conclusion. 35.2% of the sen-
tence pairs were judged to be exact paraphrases,
while 84% of the pairs were judged to be at least
roughly equivalent. Sentence pairs were negated or
change in topic only 7.2% of the time. Thus, a neu-
ral editor trained on this distribution should prefer-
entially generate semantically similar edits.

Note that semantic similarity is not needed if we
are only interested in modeling the distribution p(x).
But it does enable us to learn an edit model p(x|x′)
that prefers semantically meaningful edits, which we
explore in Section 4.3.

3.2 Approximate expectation on edit vectors, z
In Section 3.1, we approximated the marginal like-
lihood log p(x) by LEX(x), which is a summation
over terms of the form:

log p(x | x′) = logEz∼p(z)
[
pedit(x | x′, z)

]
. (4)

Unfortunately the expectation over p(z) has no
closed form, and naively approximating it by Monte
Carlo sampling z ∼ p(z) will have unacceptably
high variance, because pedit(x | x′, z) will be almost
zero for nearly all z sampled from p(z), while being
large for a few important but rare values.

To address this, we introduce an inverse neural
editor q(z | x′, x): given a prototype x′ and a revised
sentence x, it generates edit vectors that are likely to

439



map x′ to x, concentrating probability on the few
rare but important values of z.

We can then use the evidence lower bound
(ELBO) to lower bound Equation 4:

log p(x|x′) ≥ Ez∼q(z|x′,x)
[
log pedit(x | x′, z)

]
︸ ︷︷ ︸

Lgen

− KL(q(z | x′, x) ‖ p(z))︸ ︷︷ ︸
LKL

def
= ELBO(x, x′).

Since Lgen is an expectation over q(z | x′, x) instead
of p(x), it can be effectively Monte Carlo estimated
by sampling z ∼ q(z | x′, x). The second term,
LKL, penalizes the difference between q(z | x′, x)
and p(x), which is necessary for the lower bound
to hold. A thorough introduction to the ELBO is
provided in Doersch (2016).

Note that q(z | x′, x) and pedit(x | x′, z) combine
to form a variational autoencoder (VAE) (Kingma
and Welling, 2014), where q(z | x′, x) is the varia-
tional encoder and pedit(x | x′, z) is the variational
decoder.

3.3 Final objective

Combining the lower bounds LEX(x) and
ELBO(x, x′), our final approximation of the
log-likelihood is

∑

x′∈N (x)
ELBO(x, x′).

We optimize this objective using stochastic gradi-
ent ascent with respect to Θ = (Θp,Θq), where Θp
are the parameters for the neural editor and Θq are
the parameters for the inverse neural editor.

3.4 Model architecture

To recap, our model features three components: the
neural editor pedit(x | x′, z), the edit prior p(z),
and the inverse neural editor q(z | x′, x). We detail
each of these components below.

Neural editor pedit(x | x′, z). We implement our
neural editor as a left-to-right sequence-to-sequence
model with attention, where the prototype x′ is the

input sequence and the revised sentence x is the out-
put sequence. We employ an encoder-decoder archi-
tecture similar to Wu (2016), extending it to condi-
tion on an edit vector z by concatenating z to the
input of the decoder at each time step.

The prototype encoder is a 3-layer bidirectional
LSTM. The inputs to each layer are the concatena-
tion of the forward and backward hidden states of
the previous layer, with the exception of the first
layer, which takes word vectors initialized using
GloVe (Pennington et al., 2014).

The decoder is a 3-layer LSTM with attention. At
each time step, the hidden state of the top layer is
used to compute attention over the top-layer hidden
states of the prototype encoder. The resulting atten-
tion context vector is then concatenated with the de-
coder’s top-layer hidden state and used to compute a
softmax distribution over output tokens.

Edit prior p(z). We sample the edit vector z from
the prior by first sampling its scalar length znorm ∼
Unif(0, 10) and then sampling its direction zdir (a
unit vector) from the uniform distribution on the unit
sphere. The resulting z = znorm ·zdir. As we will see
later, this particular choice of the prior enables us to
easily compute LKL.

Inverse neural editor q(z | x′, x). Given an edit
pair (x′, x), the inverse neural editor must infer what
vectors z are likely to map x′ to x.

Suppose that x′ and x only differed by a single
word w. Then one might propose that the edit vector
z should be equal to the word vector for w. Gener-
alizing this intuition to multi-word edits, we would
like multi-word insertions to be represented as the
sum of the inserted word vectors, and similarly for
deletions.

Formally, define I = x\x′ to be the set of words
added to x′, and D = x′\x to be the words deleted.
We represent the difference between x′ and x using
the following vector:

f
(
x, x′

)
=
∑

w∈I
Φ (w)⊕

∑

w∈D
Φ (w)

where Φ(w) is the word vector for word w and ⊕
denotes concatenation. The word embeddings Φ are
parameters of q. In our work, we initialize Φ(w) to
be 300-dimensional GloVe vectors.

440



Since we construct our edit vectors as the sum of
word vectors, and similarities between word vectors
have traditionally been measured with cosine simi-
larity, we design q to add noise to perturb the direc-
tion of the vector f . In particular, a sample from q is
simply a perturbed version of f : obtained by adding
von-Mises Fisher (vMF) noise, and we perturb the
magnitude of f by adding uniform noise. We visu-
alize this perturbation process in Figure 2.

f(x, x') random rotation random rescaling
vMF {

z

Uniform

Figure 2: The inverse neural editor q outputs a perturbed
version of f(x, x′). The perturbation process is a random
rotation (according to the von-Mises Fisher distribution)
followed by a random rescaling (according to the uniform
distribution).

Formally, let fnorm = ‖f‖ and fdir = f/fnorm. Let
vMF (v;µ, κ) denote a vMF distribution over points
v on the unit sphere (i.e., directions) with mean vec-
tor µ and concentration parameter κ (in such a distri-
bution, the log-likelihood of a point decays linearly
with its cosine similarity to µ, and the rate of decay
is controlled by κ). Finally, define:

q(zdir | x′, x) = vMF (zdir; fdir, κ)
q(znorm | x′, x) = Unif(znorm; [f̃norm, f̃norm + �])

where f̃norm = min(fnorm, 10 − �) is the truncated
norm. The resulting edit vector is z = zdir · znorm.

The inverse neural editor q is parameterized by
the word vectors Φ and has hyperparameters κ and
�. Further details are provided in Section 3.5.

3.5 Details of the inverse neural editor

Differentiating w.r.t. Θq. To maximize our
training objective, we must be able to compute
∇ΘqELBO(x, x′) = ∇ΘqLgen −∇ΘqLKL.

To compute ∇ΘqLgen, we use a reparameteriza-
tion trick. Specifically, we can rewrite z ∼ q(z |
x′, x) as z = h(α) where h is a deterministic func-
tion differentiable with respect to Θq and α ∼ p(α)
is an auxiliary random variable not depending on Θq
(the details of h and α are given in Appendix 6). We

can then write:

∇ΘqLgen = ∇ΘqEz∼q(z|x′,x)
[
log pedit(x | x′, z)

]

= Eα∼p(α)
[
∇Θq log pedit(x | x′, h(α))

]
.

This moves the derivative inside the expectation.
The inner derivative can now be computed via stan-
dard backpropagation.

Next, we turn to∇ΘqLKL. First, note that:

LKL = KL(q(znorm|x′, x)‖p(znorm))
+ KL(q(zdir|x′, x)‖p(zdir)). (5)

It is easy to verify that the first KL term does not
depend on Θq. The second term has the closed form

KL(vMF(µ, κ)‖vMF(µ, 0)) = κ
Id/2(κ) + Id/2−1(κ)

d−2
2κ

Id/2−1(κ)− d−22κ
− log(Id/2−1(κ))− log(Γ(d/2))
+ log(κ)(d/2− 1)− (d− 2) log(2)/2, (6)

where In(κ) is the modified Bessel function of
the first kind, Γ is the gamma function, and d
is the dimensionality of f . We can see that
this too is constant with respect to Θq via the
following intuition: both the KL divergence and
the prior do not change under rotations, and
thus we can see KL(vMF(µ, κ)‖vMF(µ, 0))) =
KL(vMF(e1, κ)‖vMF(e1, 0))) by rotating µ to the
first canonical basis vector. Hence∇ΘqLKL = 0.

Comparison with existing VAE encoders. Our
design of q differs from the typical choice of a
standard normal distribution (Bowman et al., 2016;
Kingma and Welling, 2014) for two reasons:

First, by construction, edit vectors are sums of
word vectors and since cosine distances are tradi-
tionally used to measure distances between word
vectors, it would be natural to encode distances be-
tween edit vectors by the cosine distance. The von-
Mises Fisher distribution captures this idea, as the
log likelihood decays with cosine similarity.

Second, our design of q allows us to explicitly
control the tradeoff between the two terms in our ob-
jective, Lgen and LKL. Note from equations 5 and 6
that LKL is purely a function of the hyperparameters
� and κ, and can thus be controlled exactly. By tak-
ing κ→ 0 and � to the maximum norm, we can drive

441



LKL arbitrarily close to 0. As a tradeoff, smaller val-
ues of κ produce a noisier edit vector, leading to a
smaller Lgen. We find a good balance by tuning κ.

In contrast, when using a Gaussian variational en-
coder, the KL term takes a different value per ex-
ample and cannot be explicitly controlled. Conse-
quently, Bowman et al. (2016) and others have ob-
served that training tends to aggressively drive these
KL terms to zero, leading to uninformative values
of z — even when multiplying LKL by a carefully
tuned and annealed importance weight.

4 Experiments

We divide our experimental results into two parts. In
Section 4.2, we evaluate the merits of the prototype-
then-edit model as a generative modeling strategy,
measuring its improvements on language modeling
(perplexity) and generation quality (human evalua-
tions of diversity and plausibility). In Section 4.3,
we focus on the semantics learned by the model and
its latent edit vector space. We demonstrate that
it possesses interpretable semantics, enabling us to
smoothly control the magnitude of edits, incremen-
tally optimize sentences for target properties, and
perform analogy-style sentence transformations.

4.1 Datasets
We evaluate perplexity on the Yelp review corpus
(YELP, Yelp (2017)) and the One Billion Word Lan-
guage Model Benchmark (BILLIONWORD, Chelba
(2013)). For qualitative evaluations of generation
quality and semantics, we focus on YELP as our pri-
mary test case, as we found that human judgments
of semantic similarity were much better calibrated
in this focused setting.

For both corpora, we used the named-entity rec-
ognizer (NER) in spaCy2 to replace named entities
with their NER categories. We replaced tokens out-
side the top 10,000 most frequent tokens with an
“out-of-vocabulary” token.

4.2 Generative modeling
We compare NEURALEDITOR as a language model
against the following baseline language models:

1. NLM: a standard left-to-right neural language
model generating from scratch. For fair com-

2honnibal.github.io/spaCy

parison, we use the exact same architecture as
the decoder of NEURALEDITOR.

2. KN5: a standard 5-gram Kneser-Ney language
model in KenLM (Heafield et al., 2013).

3. MEMORIZATION: generates by sampling a
sentence from the training set.

Perplexity. We start by evaluating NEURALEDI-
TOR’s value as a language model, measured in terms
of perplexity. We use the likelihood lower bound
in Equation 3, where we sum over training set in-
stances within Jaccard distance < 0.5, and for the
VAE term in NEURALEDITOR, we use the one-
sample approximation to the lower bound used in
Kingma (2014) and Bowman (2016).

To evaluate NEURALEDITOR’s perplexity, we use
linear smoothing with NLM to account for rare
sentences not within our Jaccard distance thresh-
old. This smoothing corresponds to occasionally
sampling a special prototype sentence that can be
edited into any other sentence and we use a smooth-
ing weight of 0.1 (for full details, see Appendix
6). We find NEURALEDITOR improves perplexity
over NLM and KN5. Table 1 shows that this is the
case for both YELP and the more general BILLION-
WORD, which contains substantially fewer test-set
sentences close to the training set. On YELP, we
surpass even the best ensemble of NLM and KN5,
while on BILLIONWORD we nearly match their per-
formance.

Comparing each model at a per-sentence level, we
see that NEURALEDITOR drastically improves log-
likelihood for a significant number of sentences in
the test set (Figure 3). Proximity to a prototype
seems to be the chief determiner of NEURALEDI-
TOR’s performance.

Model
Perplexity

(Yelp)
Perplexity
(BILLIONWORD)

KN5 56.546 78.361
KN5+MEMORIZATION 55.184 73.468
NLM 39.026 55.146
NLM+MEMORIZATION 38.086 50.969
NLM+KN5 37.312 47.472
NEURALEDITOR(κ = 0) 26.87 48.755
NEURALEDITOR(κ = 25) 27.41 48.921

Table 1: Perplexity of NEURALEDITOR with the two
VAE parameters κ outperform all methods on YELP and
all non-ensemble methods on BILLIONWORD.

442



Figure 3: NEURALEDITOR outperforms NLM on examples similar to those in the training set (left panel, point size
indicates number of training set examples with Jaccard distance < 0.5). The N-gram baseline (right) shows no such
behavior, with NLM outperforming KN5 on most examples.

Prototype x′ Revision x
this place gets <cardinal> stars
for its diversity in its menu .

this place gets <cardinal> stars
although not for the prices .

great food and the happy hour
deals were out of this world .

the deals are great and the food
is out of this world .

i’ve been going to <person> for
<date> and i used to really like
this place .

i’ve been going to this place for
<date> now and love it .

their food is great , and you can’t
beat the price .

you can’t beat the service and
food here .

Table 2: Edited generations are substantially different
from the sampled prototypes.

Since NEURALEDITOR draws its strength from
sentences in the training set, we also compared
against a simpler alternative, in which we ensem-
ble NLM and MEMORIZATION (retrieval without
edits). NEURALEDITOR performs dramatically bet-
ter than this alternative. Table 2 also qualitatively
demonstrates that sentences generated by NEU-
RALEDITOR are substantially different from the
original prototypes.

Human evaluation. We now turn to human eval-
uation of generation quality, focusing on grammati-
cality and plausibility. We evaluated plausibility by
asking human raters, “How plausible is it for this
sentence to appear in the corpus?” on a scale of
1– 3. We evaluate generations from NEURALED-
ITOR against an NLM with a temperature parame-
ter on the per-token softmax3 as well as a baseline
which generates sentences by randomly sampling
from the training set and replacing synonyms, where
the probability of substitution follows exp(sij/τ),
where sij is the cosine similarity between the origi-
nal word and its synonym according to GloVe word
vectors.

Decreasing the temperature parameter below 1 is
3 If si is the softmax logit for tokenwi and τ is a temperature

parameter, the temperature-adjusted distribution is p(wi) ∝
exp(si/τ).

a popular technique for suppressing incoherent and
ungrammatical sentences. Many NLM systems have
noted an undesirable tradeoff between grammatical-
ity and diversity, where a temperature low enough to
enforce grammaticality results in short and generic
utterances (Li et al., 2016).

Figure 4 illustrates that both the grammatical-
ity and plausibility of NEURALEDITOR without any
temperature annealing is on par with the best tuned
temperature for NLM, with a far higher diversity, as
measured by the discrete entropy over unigram fre-
quencies. We also find that decreasing the tempera-
ture of NEURALEDITOR can be used to slightly im-
prove the grammaticality, without substantially re-
ducing the diversity of the generations.

Comparing with the synonym substitution model,
we find both models have high plausibility, since
synonym substitution maintains most of the words,
but low grammaticality compared to both NEU-
RALEDITOR and the NLM. Additionally, applying
synonym substitutions to training examples has ex-
tremely low coverage – none of the sentences in the
test set can be generated via synonym substitution,
and thus this baseline has higher perplexity than all
other baselines in Table 1.

A key advantage of edit-based models thus
emerges: Prototypes sampled from the training set
organically inject diversity into the generation pro-
cess, even if the temperature of the decoder in NEU-
RALEDITOR is zero. Hence, we can keep the de-
coder at a very low temperature to maximize gram-
maticality and plausibility, without sacrificing diver-
sity. In contrast, a zero temperature NLM would
collapse to outputting one generic sentence.

This also suggests that the temperature parame-
ter for NEURALEDITOR captures a more natural no-
tion of diversity — a temperature of 1.0 encourages
more aggressive extrapolation from the training set

443



Figure 4: NEURALEDITOR provides plausibility and grammaticality on par with the best, temperature-tuned language
model without any loss of diversity as a function of temperature. Results are based on 400 human evaluations.

while lower temperatures favor more conservative
mimicking. This is likely to be more useful than
the tradeoff for generation-from-scratch, where low
temperature also affects the diversity of generations.

Categorizing edits. To better understand the be-
havior of NEURALEDITOR, we measured the fre-
quency with which random edits from NEURALED-
ITOR matched known syntactic transformations.

We use the rule-based transformations defined in
He (2015) as our set of transformations to test, and
search the corpus for sentences where these rules can
be applied. We then apply the rule-based transfor-
mation, and measure the log-likelihood that NEU-
RALEDITOR generates the transformed outputs. We
find that the edit model assigns relatively high prob-
ability to the identity map (no edits), followed by
simple reordering transformations such as reorder-
ing to/that Clauses (It is ADJP to/that SBAR/S →
To S/BARS is ADJP). Of the rules, active / passive
receives the lowest probability, partially due to the
rarity of passive voice sentences in the Yelp corpus
(Table 3).

In all cases, the model assigns substantially higher
probability to these rule-based transformations over
editing to random sentences or shuffling the tokens
randomly to match the Levenstein distance of each
rule-based transform.

4.3 Semantics of NEURALEDITOR

In this section, we investigate the learned semantics
of NEURALEDITOR, focusing on the two desiderata
discussed in Section 2: semantic smoothness, and
consistent edit behavior.

In order to establish a baseline for these proper-
ties, we consider existing sentence generation tech-
niques which can sample semantically similar sen-
tences. The most similar language modeling ap-
proach which can capture semantics is the sentence

variational autoencoder (SVAE) which imposes se-
mantic structure onto a latent vector space, but uses
the latent vector to represent the entire sentence,
rather than just an edit.

To use the SVAE to “edit” a target sentence into
a semantically similar sentence, we perturb its un-
derlying latent sentence vector and then decode the
result back into a sentence — the same method used
in Bowman et al. (2016).

Semantic smoothness. A good editing system
should have fine-grained control over the semantics
of a sentence: i.e., each edit should only alter the se-
mantics of a sentence by a small and well-controlled
amount. We call this property semantic smoothness.

To study smoothness, we first generate an “edit
sequence” by randomly selecting a prototype sen-
tence, and then repeatedly editing via NEURALED-
ITOR (with edits drawn from the edit prior p(z)) to
produce a sequence of revisions. We then ask human
annotators to rate the size of the semantic changes
between revisions. An example is given in Table 4.

We compare to two baselines, one based upon the
sentence variational autoencoder (SVAE) and an-
other baseline which simply samples similar sen-
tences from the training set according to average
word vector similarity (COSINE).

For SVAE, we generate a similar sequence of sen-
tences by first encoding the prototype sentence, and
then decoding after the addition of a random Gaus-
sian with variance 0.4.4 This process is repeated to
produce a sequence of sentences which we can view
as the SVAE equivalent of the edit sequence.

For COSINE, we generate sentences from the
training set using exponentiated cosine similarity be-

4The variance was selected so that SVAE and NEURALED-
ITOR have the same average human similarity judgement be-
tween two successive sentences. This avoids situations where
SVAE produces completely unrelated sentence due to the per-
turbation size.

444



Type of edit − log(p) per token Example Transformed example
Identity 0.33± 0.006 It is important to remain watchful. It is important to remain watchful.
to Clause reordering 1.62± 0.156 It is important to remain watchful. To remain watchful is important.
Quotative verb reordering 2.02± 0.0359 They announced that the president

will restructure the division.
The president will restructure the
division, they announced.

Conjunction reversal 2.52± 0.0520 We should march because winter is
coming.

Winter is coming, because of this,
we should march.

Genitive reordering 2.678± 0.0477 The best restaurant of New York. New York’s best restaurant.
Active / passive 3.271± 0.0298 The talk was denied by the boycott

group spokesman.
The boycott group spokesman de-
nied the talk.

Random sentence reordering 4.42± 0.026 It is important to remain watchful. It remain is to important watchful.
Editing to random sentences 6.068± 0.084

Table 3: NEURALEDITOR assigns high probabilities to the syntactic transformations defined in He (2015) compared
baselines of editing to random sentences or randomly reordering tokens to match the Levenstein distance of a syntactic
edit. Small transformations, such as clause reordering, receive higher probability than more structural edits such as
changing from active to passive voice.

NEURALEDITOR SVAE
this food was amazing one of the best i’ve tried, service was fast and great. this food was amazing one of the best i’ve tried, service was fast and great.
this is the best food and the best service i’ve tried in <gpe>. this place is a great place to go if you want a quick bite.
some of the best <norp> food i’ve had in <date> i’ve lived in <gpe>. the food was good, but the service was terrible.
i have to say this is the best <norp> food i’ve had in <gpe>. this is the best <norp> food in <gpe>.
best <norp> food i’ve had since moving to <gpe> <date>. this place is a great place to go if you want to eat.
this was some of the best <norp> food i’ve had in the <gpe>. this is the best <norp> food in <gpe>.

Table 4: Example random walks from NEURALEDITOR, where the top sentence is the prototype.

tween averaged word vectors. The temperature pa-
rameter for the exponential was selected as before to
match the average human similarity judgement.

Figure 5 shows that NEURALEDITOR frequently
generates paraphrases despite being trained on lex-
ical similarity, and only 1% of edits are unrelated
from the prototype. In contrast, SVAE often repeats
sentences exactly, and when it makes an edit it is
equally likely to generate unrelated sentences. CO-
SINE performs even worse likely due to the difficulty
of retrieving similar sentences for rare and long sen-
tences.

Less Similar By Turk EvaluationDegenerate

Figure 5: Compared with baselines, NEURALEDITOR
frequently generates paraphrases and similar sentences
while avoiding unrelated and degenerate ones.6

Qualitatively (Table 4), NEURALEDITOR seems

to generate long, diverse sentences which smoothly
change over time, while the SVAE biases towards
short sentences with several semantic jumps, pre-
sumably due to the difficulty of training a suffi-
ciently informative SVAE encoder.

Smoothly controlling sentences. We now show
that we can selectively choose edits sampled from
NEURALEDITOR to incrementally optimize a sen-
tence towards desired attributes. This task serves as
a useful measure of semantic coverage: if an edit
model has high coverage over sentences that are se-
mantically similar to a prototype, it should be able to
satisfy the target attribute while deviating minimally
from the prototype’s original meaning.

We focus on controlling two simple attributes:
compressing a sentence to below a desired length
(e.g., 7 words), and inserting a target keyword into
the sentence (e.g., “service” or “pizza”).

Given a prototype sentence, we try to discover a
semantically similar sentence satisfying the target

6 545 similarity assessments pairs were collected through
Amazon Mechanical Turk following Agirre (2014), with the
same scale and prompt. Similarity judgements were converted
to descriptions by defining Paraphrase (5), Roughly Equivalent
(4-3), Same Topic (2-1), Unrelated (0).

445



Figure 6: NEURALEDITOR can shorten sentences (left), include common words (center, the word ‘service’) and rarer
words (right ‘pizza’) while maintaining similarity.

NEURALEDITOR SVAE

the coffee ice cream was one of the best i’ve ever tried. the coffee ice cream was one of the best i’ve ever tried.
some of the best ice cream we’ve ever had! the <unk> was very good and the food was good.
just had the best ice - cream i’ve ever had! the food was good, but not great.
some of the best pizza i’ve ever tasted! the food was good, but not great.
that was some of the best pizza i’ve had in the area. the food was good, but the service was n’t bad.

Table 5: Examples of word inclusion trajectories for ‘pizza’. NEURALEDITOR produces smooth chains that lead to
word inclusion, but the SVAE gets stuck on generic sentences.

attribute using the following procedure: First, we
generate 1,000 edit sequences using the procedure
described earlier. Then, we select the sequence with
highest likelihood whose endpoint possesses the tar-
get attribute. We repeat this process for a large num-
ber of prototypes.

We use almost the same procedure for the SVAE,
but instead of selecting by highest likelihood, we
select the sequence whose endpoint has shortest la-
tent vector distance from the prototype (as this is the
SVAE’s metric of semantic similarity).

In Figure 6, we then aggregate the sentences from
the collected edit sequences, and plot their seman-
tic similarity to the prototype against their success
in satisfying the target attribute. Not surprisingly, as
target attribute satisfaction rises, semantic similar-
ity drops. However, we also see that NEURALED-
ITOR sacrifices less semantic similarity to achieve
the same level of attribute satisfaction as SVAE.
SVAE is reasonable on tasks involving common
words (such as the word service), but fails when the
model is asked to generate rarer words such as pizza.
Examples from these word inclusion problems show
that SVAE often becomes stuck generating short,
generic sentences (Table 5).

Consistent edit behavior: sentence analogies. In
the previous results, we showed that edit models
learn to generate semantically similar sentences. We
now assess whether the edit vector possesses glob-

ally consistent semantics. Specifically, applying the
same edit vector to different sentences should result
in semantically analogous edits.

For example, if we have an edit vector which ed-
its the sentence x1 = “this was a good restaurant”
into x2 = “this was the best restaurant”. Given a
new sentence y1 = “The cake was great”, we expect
applying the same edit vector to result in y2 = “The
cake was the greatest”.

Formally, suppose we have two sentences, x1 and
x2, which are related by some underlying semantic
relation r. Given a new sentence y1, we would like
to find a y2 such that the same relation r holds be-
tween y1 and y2.

Our approach is to estimate the edit vector be-
tween x1 and x2 as ẑ = f(x1, x2) — the mode
of the inverse neural editor q. We then apply this
edit vector to y1 using the neural editor to yield
ŷ2 = argmaxxpedit(x | y1, ẑ).

Since it is difficult to output ŷ2 exactly matching
y2, we take the top k candidate outputs of pedit (us-
ing beam search) and evaluate whether the gold y2
appears among the top k elements.

We generate the semantic relations r using prior
evaluations for word analogies (Mikolov et al.,
2013a; Mikolov et al., 2013b). We leverage these
to generate a new dataset of sentence analogies, us-
ing a simple strategy: given an analogous word pair
(w1, w2), we mine the Yelp corpus for sentence pairs

446



Google Microsoft Method

gram4-superlative gram3-comparative family JJR_JJS VB_VBD VBD_VBZ NN_NNS VB_VBZ JJ_JJR

0.45 0.85 1.0 0.75 0.63 0.82 0.82 0.61 0.77 GloVE

0.75 0.75 0.29 0.79 0.57 0.60 0.58 0.41 0.24 Edit vector (top 10)
0.60 0.32 0.01 0.45 0.16 0.23 0.17 0.01 0.06 Edit vector (top 1)
0.10 0.10 0.09 0.10 0.08 0.14 0.15 0.05 0.03 Sampling (top 10)

Table 6: Edit vectors capture one-word sentence analogies with performance close to lexical analogies.

Example 1 Example 2
Context he comes home tired and happy . i went with a larger group to <person> ’s .

Edit + was - is + good - better
Result = he came home happy and tired . = i went to <person> ’s with a large group .

Table 7: Examples of lexical analogies correctly answered by NEURALEDITOR. Sentence pairs generating the analogy
relationship are shortened to only their lexical differences.

(x1, x2) such that x1 is transformed into x2 by in-
sertingw1 and removingw2 (allowing for reordering
and inclusion/exclusion of stop words).

For this task, we initially compared against the
SVAE, but it had a top-k accuracy close to zero.
Hence, we instead compare to SAMPLING which is
a baseline which randomly samples an edit vector
ẑ ∼ p(z), instead using ẑ derived from f(x1, x2).

We also compare our accuracies to the simpler
task of solving the word, rather than sentence-level
analogies in (Mikolov et al., 2013a) using GloVe.
This task is substantially simpler, since the goal is to
identify a single word (such as “good:better::bad:?”)
instead of an entire sentence. Despite this, the top-
10 performance of our model in Table 6 is nearly as
good as the performance of GloVe vectors on the
simpler lexical analogy task. In some categories,
NEURALEDITOR at top-10 actually performs better
than word vectors, since NEURALEDITOR has an
understanding of which words are likely to appear
in the context of a Yelp review. Examples in Table
7 show the model is accurate and captures lexical
analogies requiring word reorderings.

5 Related work and discussion

Our work connects with a broad literature on
attention-based neural models, retrieval-augmented
text generation, semantically meaningful represen-
tations, and nonparametric statistics.

Based upon recurrent neural networks and
sequence-to-sequence architectures (Sutskever et
al., 2014), neural language models (Bengio et al.,
2003) have been widely used due to their flexibility
and performance across a wide range of NLP tasks

(Kalchbrenner and Blunsom, 2013; Hahn and Mani,
2000; Ritter et al., 2011). Our work is motivated by
an emerging consensus that attention-based mecha-
nisms (Bahdanau et al., 2015) can substantially im-
prove performance on various sequence to sequence
tasks by capturing more information from the input
sequence (Vaswani et al., 2017). Our work extends
the applicability of attention mechanisms beyond
sequence-to-sequence models by allowing models to
attend to randomly sampled sentences.

There is a growing literature on applying retrieval
mechanisms to augment text generation models. For
example, in the image captioning literature, Hodosh
(2013), Kuznetsova (2013) and Mason (2014) pro-
posed to generate image captions by first retriev-
ing a prototype caption based on an image con-
text, and then applying sentence compression to tai-
lor the prototype to a particular image. More re-
cently, Song (2016) ensembled a retrieval system
and an NLM for dialogue, using the NLM to trans-
form the retrieved utterance, and Gu (2017) used
an off-the-shelf search engine system to retrieve
and condition on training set examples. Although
these approaches also edit text from the training set,
these papers solve a fundamentally different prob-
lem since they solve conditional generation prob-
lems, and retrieve prototypes based on a context,
where as our task is unconditional and thus there is
no context which we can use to retrieve.

Our work treats the prototype x′ as a latent vari-
able rather than being given by a retrieval mecha-
nism, and marginalizes over all possible prototypes
— a challenge which motivates our new lexical sim-
ilarity training method in Section 3.1. Practically,

447



marginalization over x′ makes our model attend to
training examples based on similarity of output se-
quences, while prior retrieval models attend to ex-
amples based on similarity of the input sequences.

In terms of generation techniques that capture
semantics, the sentence variational autoencoder
(SVAE) (Bowman et al., 2016) is closest to our work
in that it attempts to impose semantic structure on a
latent vector space. However, the SVAE’s latent vec-
tor is meant to represent the entire sentence, whereas
the neural editor’s latent vector represents an edit.
Our results from Section 4.3 suggest that local vari-
ation over edits is easier to model than global varia-
tion over sentences.

Our use of lexical similarity neighborhoods is
comparable to context windows in word vector train-
ing (Mikolov et al., 2013a). More generally, results
in manifold learning demonstrate that a weak met-
ric such as lexical similarity can be used to extract
semantic similarity through distributional statistics
(Tenenbaum et al., 2000; Hashimoto et al., 2016).

From a generative modeling perspective, edit-
ing randomly sampled training sentences closely
resembles nonparametric kernel density estimation
(Parzen, 1962) where one samples points from a
training set, and adds noise to smooth the den-
sity. Our edit model is the text equivalent of Gaus-
sian noise, and our training mechanism is a type of
learned smoothing kernel.

Prototype-then-edit is a semi-parametric ap-
proach that remembers the entire training set and
uses a neural editor to generalize meaningfully be-
yond the training set. The training set provides a
strong inductive bias — that the corpus can be char-
acterized by prototypes surrounded by semantically
similar sentences reachable by edits. Beyond im-
provements on generation quality as measured by
perplexity, the approach also reveals new semantic
structures via the edit vector.

Reproducibility. All code, data and experiments
are available on the CodaLab platform at https:
//bit.ly/2rHsWAX.

Acknowledgements. We thank the reviewers and
editor for their insightful comments. This work was
funded by DARPA CwC program under ARO prime
contract no. W911NF-15-1-0462.

6 Appendix

Construction of the LSH. The LSH maps a sen-
tence to lexically similar sentences in the corpus,
representing a graph over sentences. We apply
breadth-first search (BFS) over the LSH sentence
graph started at randomly selected seed sentences
and uniformly sample this set to form the training
set.

Reparameterization trick for q. First, note that
we can write znorm ∼ q(znorm|x′, x) as znorm =
hnorm(αnorm)

def
= f̃norm + αnorm where αnorm ∼

Unif(0, �). Furthermore, Wood (1994) present a
function hdir and auxiliary random variable αdir,
such that zdir = hdir(αdir) is distributed according
to a vMF with mean f and concentration κ. We can
then define z = h(α) def= hdir(αdir) · hnorm(αnorm).

Figure 7: Small amounts of smoothing are sufficient to
make NEURALEDITOR outperform the baseline NLM.

Smoothing for language models. As a language
model, NEURALEDITOR does not place probability
on any test sentence which is sufficiently dissimi-
lar from all training set sentences. In order to avoid
this problem, we can consider a special prototype
sentence ‘∅’ which can be edited into any sentence,
and draw this special prototype with probability p∅.
Concretely, we write:

p(x) =
∑

x′∈X∪{∅}
pedit(x|x′)pprior(x′)

= (1− p∅)
∑

x′∈X

1

|X |pedit(x|x
′) + p∅ pNLM(x).

This linearly smoothes between our edit model
(pedit) and the NLM (pNLM) since our decoder is
identical to the NLM, and thus conditioning on the
special ∅ token reduces to using a NLM.

Empirically, we observe that even small values of
p∅ produces low perplexity (Figure 7) corresponding
to the observation that smoothing of NEURALED-
ITOR is only necessary to avoid degenerate log-
likelihoods on a very small subset of the test set.

448



References

E. Agirre, C. Banea, C. Cardie, D. M. Cer, M. T. Diab,
A. Gonzalez-Agirre, W. Guo, R. Mihalcea, G. Rigau,
and J. Wiebe. 2014. SemEval-2014 Task 10: Mul-
tilingual semantic textual similarity. In International
Conference on Computational Linguistics (COLING),
pages 81–91.

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural ma-
chine translation by jointly learning to align and trans-
late. In International Conference on Learning Repre-
sentations (ICLR).

Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of ma-
chine learning research, 3(0):1137–1155.

S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Joze-
fowicz, and S. Bengio. 2016. Generating sentences
from a continuous space. In Computational Natural
Language Learning (CoNLL), pages 10–21.

C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants,
P. Koehn, and T. Robinson. 2013. One billion word
benchmark for measuring progress in statistical lan-
guage modeling. arXiv preprint arXiv:1312.3005.

C. Doersch. 2016. Tutorial on variational autoencoders.
arXiv preprint arXiv:1606.05908.

J. Gu, Y. Wang, K. Cho, and V. O. Li. 2017. Search
engine guided non-parametric neural machine transla-
tion. arXiv preprint arXiv:1705.07267.

U. Hahn and I. Mani. 2000. The challenges of automatic
summarization. Computer, 33.

T. B. Hashimoto, D. Alvarez-Melis, and T. S. Jaakkola.
2016. Word embeddings as metric recovery in seman-
tic spaces. Transactions of the Association for Com-
putational Linguistics (TACL), 4:273–286.

J. R. Hayes and L. S. Flower. 1986. Writing research and
the writer. American psychologist, 41(10):1106–1113.

H. He, A. G. II, J. Boyd-Graber, and H. D. III.
2015. Syntax-based rewriting for simultaneous ma-
chine translation. In Empirical Methods in Natural
Language Processing (EMNLP), pages 55–64.

K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language model
estimation. In Association for Computational Linguis-
tics (ACL), pages 690–696.

M. Hodosh, P. Young, and J. Hockenmaier. 2013. Fram-
ing image description as a ranking task: Data, models
and evaluation metrics. Journal of Artificial Intelli-
gence Research (JAIR), 47:853–899.

M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine Learning, 37:183–233.

N. Kalchbrenner and P. Blunsom. 2013. Recurrent con-
tinuous translation models. In Empirical Methods in

Natural Language Processing (EMNLP), pages 1700–
1709.

D. P. Kingma and M. Welling. 2014. Auto-encoding
variational Bayes. arXiv preprint arXiv:1312.6114.

P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and
Y. Choi. 2013. Generalizing image captions for
image-text parallel corpus. In Association for Com-
putational Linguistics (ACL), pages 790–796.

J. Li, M. Galley, C. Brockett, J. Gao, and W. B. Dolan.
2016. A diversity-promoting objective function for
neural conversation models. In Human Language
Technology and North American Association for Com-
putational Linguistics (HLT/NAACL), pages 110–119.

R. Mason and E. Charniak. 2014. Domain-specific im-
age captioning. In Computational Natural Language
Learning (CoNLL), pages 2–10.

T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013a.
Efficient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781.

T. Mikolov, W. Yih, and G. Zweig. 2013b. Linguis-
tic regularities in continuous space word representa-
tions. In Human Language Technology and North
American Association for Computational Linguistics
(HLT/NAACL), volume 13, pages 746–751.

E. Parzen. 1962. On estimation of a probability density
function and mode. Annals of Mathematical Statistics,
33:1065–1076.

J. Pennington, R. Socher, and C. D. Manning. 2014.
GloVe: Global vectors for word representation. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543.

A. Ritter, C. Cherry, and W. B. Dolan. 2011. Data-driven
response generation in social media. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 583–593.

L. Shao, S. Gouws, D. Britz, A. Goldie, B. Strope, and
R. Kurzweil. 2017. Generating high-quality and
informative conversation responses with sequence-to-
sequence models. In Empirical Methods in Natural
Language Processing (EMNLP), pages 2210–2219.

Y. Song, R. Yan, X. Li, D. Zhao, and M. Zhang. 2016.
Two are better than one: An ensemble of retrieval-
and generation-based dialog systems. arXiv preprint
arXiv:1610.07149.

I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Se-
quence to sequence learning with neural networks. In
Advances in Neural Information Processing Systems
(NIPS), pages 3104–3112.

J. B. Tenenbaum, V. D. Silva, and J. C. Langford. 2000.
A global geometric framework for nonlinear dimen-
sionality reduction. Science, pages 2319–2323.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.

449



2017. Attention is all you need. arXiv preprint
arXiv:1706.03762.

A. T. Wood. 1994. Simulation of the von mises fisher
distribution. Communications in statistics-simulation
and computation, pages 157–164.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi,
W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Yelp. 2017. Yelp Dataset Challenge, Round 8. https:
//www.yelp.com/dataset_challenge.

450


