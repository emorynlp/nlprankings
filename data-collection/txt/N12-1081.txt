










































Comparing HMMs and Bayesian Networks for Surface Realisation


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 636–640,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Comparing HMMs and Bayesian Networks for Surface Realisation

Nina Dethlefs
Heriot-Watt University
Edinburgh, Scotland

n.s.dethlefs@hw.ac.uk

Heriberto Cuayáhuitl
German Research Centre for Artificial Intelligence

Saarbrücken, Germany
heriberto.cuayahuitl@dfki.de

Abstract

Natural Language Generation (NLG) systems
often use a pipeline architecture for sequen-
tial decision making. Recent studies how-
ever have shown that treating NLG decisions
jointly rather than in isolation can improve the
overall performance of systems. We present
a joint learning framework based on Hierar-
chical Reinforcement Learning (HRL) which
uses graphical models for surface realisation.
Our focus will be on a comparison of Bayesian
Networks and HMMs in terms of user satis-
faction and naturalness. While the former per-
form best in isolation, the latter present a scal-
able alternative within joint systems.

1 Introduction

NLG systems have traditionally used a pipeline ar-
chitecture which divides the generation process into
three distinct stages. Content selection chooses
‘what to say’ and constructs a semantic form. Ut-
terance planning organises the message into sub-
messages and surface realisation maps the seman-
tics onto words. Recently, a number of studies
have pointed out that many decisions made at these
distinct stages require interrelated, rather than iso-
lated, optimisations (Angeli et al., 2010; Lemon,
2011; Cuayáhuitl and Dethlefs, 2011a; Dethlefs and
Cuayáhuitl, 2011a). The key feature of a joint archi-
tecture is that decisions of all three NLG stages share
information and can be made in an interrelated fash-
ion. We present a joint NLG framework based on
Hierarchical RL and focus, in particular, on the sur-
face realisation component of joint NLG systems.

We compare the user satisfaction and naturalness
of surface realisation using Hidden Markov Models
(HMMs) and Bayesian Networks (BNs) which both
have been suggested as generation spaces—spaces
of surface form variants for a semantic concept—
within joint NLG systems (Dethlefs and Cuayáhuitl,
2011a; Dethlefs and Cuayáhuitl, 2011b) and in iso-
lation (Georgila et al., 2002; Mairesse et al., 2010).

2 Surface Realisation for Situated NLG

We address the generation of navigation instruc-
tions, where e.g. the semantic form (path(target =
end of corridor) ∧ (landmark = lif t ∧ dir =
left)) can be expressed as ‘Go to the end of the
corridor’, ‘Head to the end of the corridor past the
lift on your left’ and many more. The best realisa-
tion depends on the space (types and properties of
spatial objects), the user (position, orientation, prior
knowledge) and decisions of content selection and
utterance planning. These can be interrelated with
surface realisation, for example:

(1) ‘Follow this corridor and go past the lift on your
left. Then turn right at the junction.’

(2) ’Pass the lift and turn right at the junction.’

Here, (1) is appropriate for a user unfamiliar with the
space and a high information need, so that more in-
formation should be given. For a familiar user, how-
ever, who may know where the lift is, it is redundant
and (2) is preferable, because it is more efficient. An
unfamiliar user may get confused with just (2).

In this paper, we distinguish navigation of des-
tination (‘go back to the office’), direction (‘turn
left’), orientation (‘turn around’), path (‘follow the

636



corridor’) and straight’ (‘go forward’) in the GIVE
corpus (Gargett et al., 2010). Users can react to an
instruction by performing the action, performing an
undesired action, hesitating or requesting help.

3 Jointly Learnt NLG: Hierarchical RL
with Graphical Models

In a joint framework, each subtask of content selec-
tion, utterance planning and surface realisation has
knowledge of the decisions made in the other two
subtasks. In an isolated framework, this knowledge
is absent. In the joint case, the relationship between
hierarchical RL and graphical models is that the lat-
ter provide feedback to the former’s surface realisa-
tion decisions according to a human corpus.

Hierarchical RL Our HRL agent consists of a
hierarchy of discrete-time Semi-Markov Decision
Processes, or SMDPs, M ij defined as 4-tuples <
Sij, A

i
j , T

i
j , R

i
j >, where i and j uniquely identify

a model in the hierarchy. These SMDPs represent
generation subtasks, e.g. generating destination in-
structions. Sij is a set of states, A

i
j is a set of ac-

tions, and T ij is a probabilistic state transition func-
tion that determines the next state s′ from the current
state s and the performed action a. Rij(s

′, τ |s, a) is
a reward function that specifies the reward that an
agent receives for taking an action a in state s last-
ing τ time steps. Since actions in SMDPs may take
a variable number of time steps to complete, the ran-
dom variable τ represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter correspond to
SMDPs and yield cumulative rewards. The goal of
each SMDP is to find an optimal policy π∗ that max-
imises the reward for each visited state, according
to π∗ij(s) = arg maxa∈A Q

∗i
j(s, a), where Q

i
j(s, a)

specifies the expected cumulative reward for execut-
ing action a in state s and then following π∗. Please
see (Dethlefs and Cuayáhuitl, 2011b) for details on
the design of the hierarchical RL agent and the inte-
gration of graphical models for surface realisation.

Hidden Markov Models Representing surface re-
alisation as an HMM can be roughly defined as the
converse of POS tagging. While in POS tagging we
map an observation string of words onto a hidden
sequence of POS tags, in NLG we face the oppo-

.

.

.

go

walk

into

to

point

room room room room

point point point

to to to

into into into

walk walk walk

go go go

process
spatial 

relation relatum detail

. . .

direc. direc. direc. direc.

Figure 1: Example trellis for an HMM for destination
instructions (not all states and transitions are shown).
Dashed arrows show paths that occur in the corpus.

site scenario. Given an observation sequence of se-
mantic symbols, we want to map it onto a hidden
most likely sequence of words. We treat states as
representing surface realisations for (observed) se-
mantic classes, so that a sequence of states s0...sn
represents phrases or sentences. An observation se-
quence o0...on consists of a finite set of semantic
symbols specific to an instruction type. Each symbol
has an observation likelihood bs(o)t giving the prob-
ability of observing o in state s at time t. We created
the HMMs and trained the transition and emission
probabilities from the GIVE corpus using the Baum-
Welch algorithm. Please see Fig. 1 for an example
HMM and (Dethlefs and Cuayáhuitl, 2011a) for de-
tails on using HMMs for surface realisation.

Bayesian Networks Representing a surface re-
aliser as a BN, we can model the dynamics between
semantic concepts and their realisations. A BN mod-
els a joint probability distribution over a set of ran-
dom variables and their dependencies based on a di-
rected acyclic graph, where each node represents a
variable Yj with parents pa(Yj). Due to the Markov
condition, each variable depends only on its parents,
resulting in a unique joint probability distribution
p(Y ) = Πp(Yj|pa(Yj)), where every variable is as-
sociated with a conditional probability distribution

637



Destination

Verb

Destination

Direction

Values: {left/right,

straight, empty}
Values: {go, keep going,

walk, continue, return,

get, you need, you want,

empty, ... }

Information
Values: {high, low}

Destination

Preposition

Values:{into, in,

to, towards, until,

empty, ...}

Destination

Relatum
Values:{landmark,

room}

Need

Figure 2: BN for generating destination instructions.

p(Yj|pa(Yj)). The meaning of random variables
corresponds to semantic symbols. The values of ran-
dom variables correspond to surface variants of a se-
mantic symbol. Figure 2 shows an example BN with
two main dependencies. First, the random variable
‘information need’ influences the inclusion of op-
tional semantic constituents and the process of the
utterance (‘destination verb’). Second, a sequence
of dependencies spans from the verb to the end of
the utterance (‘destination relatum’). The first de-
pendency is based on the intuition that more detail
is needed in an instruction for users with high infor-
mation need (e.g. with little prior knowledge).1 The
second dependency is based on the hypothesis that
the value of one constituent can be estimated based
on the previous constituent. In the future, we may
compare different configurations and effects of word
order. Given the word sequence represented by lex-
ical and syntactic variables Y0...Yn, and situation-
based variables Yn+1...Ym, we can compute the pos-
terior probability of a random variable Yj . The pa-
rameters of the BNs were estimated using MLE.
Please see (Dethlefs and Cuayáhuitl, 2011b) for de-
tails on using BNs for surface realisation within a
joint learning framework.

4 Experimental Setting

We compare instructions generated with the
HMMs and BNs according to their user sat-
isfaction and their naturalness. The learn-

1This is key to the joint treatment of content selection and
surface realisation: if an utterance is not informative in terms
of content, it will receive bad rewards, even with good surface
realisation choices (and vice versa).

ing agent is trained using the reward function
Reward = User satisfaction × P (w0 . . . wn) ×
CAS.2 User satisfaction is a function of task
success and the number of user turns based on
the PARADISE framework3 (Walker et al., 1997)
and CAS refers to the proportion of repetition
and variation in surface forms. Our focus in
this short paper is on P (w0 . . . wn) which rewards
the agent for having generated a surface form se-
quence w0 . . . wn. In HMMs, this corresponds to
the forward probability—obtained from the For-
ward algorithm—of observing the sequence in the
data. In BNs, P (w0 . . . wn) corresponds to P (Yj =
vx|pa(Yj) = vy), the posterior probability given the
chosen values vx and vy of random variables and
their dependencies. We assign a reward of −1 for
each action to prevent loops.

5 Experimental Results

User satisfaction Our trained policies learn the
same content selection and utterance planning be-
haviour reported by (Dethlefs and Cuayáhuitl,
2011b). These policies contribute to the user sat-
isfaction of instructions. BNs and HMMs however
differ in their surface realisation choices. Figure
3 shows the performance in terms of average re-
wards over time for both models within the joint
learning framework and in isolation.4 For ease of
comparison, a learning curve using a greedy policy
is also shown. It always chooses the most likely
surface form according to the human corpus with-
out taking other tradeoffs into account. Within the
joint framework, both BNs and HMMs learn to gen-
erate context-sensitive surface forms that balance
the tradeoffs of the most likely sequence (accord-
ing to the human corpus) and the one that best cor-
responds to the user’s information need (e.g., using
nick names of rooms for familiar users). The BNs

2This reward function, the simulated environment and train-
ing parameters were adapted from (Dethlefs and Cuayáhuitl,
2011b) to allow a comparison with related work in using graph-
ical models for surface realisation. Simulation is based on uni-
and bigrams for the spatial setting and Naive Bayes Classifica-
tion for user reactions to system instructions.

3See (Dethlefs et al., 2010) for evidence of the correlation
between user satisfaction, task success and dialogue length.

4In the isolated case, subtasks of content selection, utterance
planning and surface realisation are blind regarding the deci-
sions made by other subtasks, but in the joint case they are not.

638



10
3

10
4

10
5

−20

−19

−18

−17

−16

−15

−14

−13

−12

−11

−10

A
ve

ra
ge

 R
ew

ar
d

Episodes

 

 

BNs Joint
BNs Isolated
HMMs Joint
HMMs Isolated
Greedy

Figure 3: Performance of HMMs, BNs and a greedy base-
line in conjunction and isolation of the joint framework.

reach an average reward5 of −11.53 and outper-
form the HMMs (average −11.64) only marginally
by less than one percent. BNs and HMMs improve
the greedy baseline by 6% (p < 0.0001, r = 0.90).
While BNs reach the same performance in isola-
tion of the joint framework, the performance of
HMMs deteriorates significantly to an average re-
ward of −12.12. This corresponds to a drop of 5%
(p < 0.0001, r = 0.79) and is nearly as low as the
greedy baseline. HMMs thus reach a comparable
performance to BNs as a result of the joint learning
architecture: the HRL agent will discover the non-
optimal behaviour that is caused by the HMM’s lack
of context-awareness (due to their independence as-
sumptions) and learn to balance this drawback by
learning a more comprehensive policy itself. For the
more context-aware BNs this is not necessary.

Naturalness We compare the instructions gener-
ated with HMMs and BNs regarding their human-
likeness based on the Kullback-Leibler (KL) diver-
gence. It computes the difference between two prob-
ability distributions. For evidence of its usefulness
for measuring naturalness, cf. (Cuayáhuitl, 2010).
We compare human instructions (based on strings)
drawn from the corpus against strings generated by
the HMMs and BNs to see how similar both are to
human authors. Splitting the human instructions in
half and comparing them to each other indicates how
similar human authors are to each other. It yields a
KL score of 1.77 as a gold standard (the lower the
better). BNs compared with human data obtain a
score of 2.83 and HMMs of 2.80. The difference in

5The average rewards of agents have negative values due to
the negative reward of −1 the agent receives for each action.

terms of similarity with humans for HMMs and BNs
in a joint NLG model is not significant.

Discussion While HMMs reach comparable user
satisfaction and naturalness to BNs in a joint system,
they show a 5% lower performance in isolation. This
is likely caused by their conditional independence
assumptions: (a) the Markov assumption, (b) the
stationary assumption, and (c) the observation inde-
pendence assumption. Even though these can make
HMMs easier to train and scale than more structured
models such as BNs, it also puts them in a disadvan-
tage concerning context-awareness and accuracy as
shown by our results. In contrast, the random vari-
ables of BNs allow them to keep a structured model
of the space, user, and relevant content selection and
utterance planning choices. BNs are thus able to
compute the posterior probability of a surface form
based on all relevant properties of the current situa-
tion (not just the occurrence in a corpus). While BNs
also place independence assumptions on their vari-
ables, they usually overcome the problem of lacking
context-awareness by their dependencies across ran-
dom variables. However, BNs also face limitations.
Given the dependencies they postulate, they are typ-
ically more data intensive and less scalable than less
structured models such as HMMs. This can be prob-
lematic for large domains such as many real world
applications. Regarding their application to surface
realisation, we can argue that while BNs are the best
performing model in isolation, HMMs represent a
cheap and scalable alternative especially for large-
scale problems in a joint NLG system.

6 Conclusion and Future Work

We have compared the user satisfaction and natural-
ness of instructions generated with HMMs and BNs
in a joint HRL model for NLG. Results showed that
while BNs perform best in isolation, HMMs repre-
sent a cheap and scalable alternative within the joint
framework. This is particularly attractive for large-
scale, data-intensive systems. While this paper has
focused on instruction generation, the hierarchical
approach in our learning framework helps to scale
up to larger NLG tasks, such as text or paragraph
generation. Future work could test this claim, com-
pare other graphical models, such as dynamic BNs,
and aim for a comprehensive human evaluation.

639



Acknowledgements This research was funded by
the European Commission’s FP7 programmes under
grant agreement no. 287615 (PARLANCE) and no.
ICT-248116 (ALIZ-E).

References

Angeli, G., Liang, P. and D. Klein (2010). A simple
domain-independent probabilistic approach to gener-
ation , Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP) .

Cuayáhuitl, H., Renals, S., Lemon, O. and H. Shimodaira
(2010). Evaluation of a Hierarchical Reinforcement
Learning Spoken Dialogue System, Computer Speech
and Language 24.

Cuayáhuitl, H., and N. Dethlefs (2011a). Spatially-
Aware Dialogue Control Using Hierarchical Rein-
forcement Learning, ACM Transactions on Speech
and Language Processing (Special Issue on Machine
Learning for Robust and Adaptive Spoken Dialogue
Systems 7(3).

Dethlefs, N. and H. Cuayáhuitl, 2011. Hierarchical Re-
inforcement Learning and Hidden Markov Models for
Task-Oriented Natural Language Generation, In Proc.
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL-HLT).

Dethlefs, N. and H. Cuayáhuitl, 2011. Combining Hi-
erarchical Reinforcement Learning and Bayesian Net-
works for Natural Language Generation in Situated
Dialogue, In Proceedings of the 13th European Work-
shop on Natural Language Generation (ENLG).

Dethlefs, N., Cuayáhuitl, H., Richter, K.-F., Andonova,
E. and J. Bateman, 2010. Evaluating Task Success in
a Dialogue System for Indoor Navigation, In Proceed-
ings of the Workshop on the Semantics and Pragmatics
of Dialogue (SemDial).

Gargett, A., Garoufi, K., Koller, A. and K. Striegnitz
(2010). The GIVE-2 Corpus of Giving Instructions
in Virtual Environments, Proc. of the 7th International
Conference on Language Resources and Evaluation.

Georgila, K., Fakotakis, N. and Kokkinakis, G. (2002).
Stochastic Language Modelling for Recognition and
Generation in Dialogue Systems. TAL (Traitement au-
tomatique des langues) Journal, Vol. 43(3).

Lemon, O. (2011). Learning what to say and how to say
it: joint optimization of spoken dialogue management
and Natural Language Generation, Computer Speech
and Language 25(2).

Mairesse, F., Gašić, M., Jurčı́ček, F., Keizer, S., Thom-
son, B., Yu, K. and S. Young (2010). Phrase-based
statistical language generation using graphical models
and active learning, Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics.

Walker, M., Litman, D., Kamm, C. and A. Abella (1997).
PARADISE: A Framework for Evaluating Spoken Di-
alogue Agents, Proceedings of the Annual Meeting of
the Association for Computational Linguistic (ACL).

640


