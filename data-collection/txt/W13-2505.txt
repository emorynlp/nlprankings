










































Chinese-Japanese Parallel Sentence Extraction from Quasi-Comparable Corpora


Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34–42,
Sofia, Bulgaria, August 8, 2013. c©2013 Association for Computational Linguistics

Chinese–Japanese Parallel Sentence Extraction
from Quasi–Comparable Corpora

Chenhui Chu, Toshiaki Nakazawa, Sadao Kurohashi
Graduate School of Informatics, Kyoto University

Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan

{chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp

Abstract
Parallel sentences are crucial for statistical
machine translation (SMT). However, they
are quite scarce for most language pairs,
such as Chinese–Japanese. Many studies
have been conducted on extracting parallel
sentences from noisy parallel or compara-
ble corpora. We extract Chinese–Japanese
parallel sentences from quasi–comparable
corpora, which are available in far larger
quantities. The task is significantly more
difficult than the extraction from noisy
parallel or comparable corpora. We ex-
tend a previous study that treats parallel
sentence identification as a binary classifi-
cation problem. Previous method of clas-
sifier training by the Cartesian product is
not practical, because it differs from the
real process of parallel sentence extrac-
tion. We propose a novel classifier train-
ing method that simulates the real sentence
extraction process. Furthermore, we use
linguistic knowledge of Chinese character
features. Experimental results on quasi–
comparable corpora indicate that our pro-
posed approach performs significantly bet-
ter than the previous study.

1 Introduction

In statistical machine translation (SMT) (Brown
et al., 1993; Koehn et al., 2007), the quality
and quantity of the parallel sentences are cru-
cial, because translation knowledge is acquired
from a sentence–level aligned parallel corpus.
However, except for a few language pairs, such
as English–French, English–Arabic and English–
Chinese, parallel corpora remain a scarce re-
source. The cost of manual construction for paral-
lel corpora is high. As non–parallel corpora are far
more available, constructing parallel corpora from
non–parallel corpora is an attractive research field.

Non–parallel corpora include various levels of
comparability: noisy parallel, comparable and
quasi–comparable. Noisy parallel corpora con-
tain non–aligned sentences that are nevertheless
mostly bilingual translations of the same docu-
ment, comparable corpora contain non–sentence–
aligned, non–translated bilingual documents that
are topic–aligned, while quasi–comparable cor-
pora contain far more disparate very–non–parallel
bilingual documents that could either be on the
same topic (in–topic) or not (out–topic) (Fung and
Cheung, 2004). Most studies focus on extracting
parallel sentences from noisy parallel corpora or
comparable corpora, such as bilingual news ar-
ticles (Zhao and Vogel, 2002; Utiyama and Isa-
hara, 2003; Munteanu and Marcu, 2005; Tillmann,
2009; Abdul-Rauf and Schwenk, 2011), patent
data (Utiyama and Isahara, 2007; Lu et al., 2010)
and Wikipedia (Adafre and de Rijke, 2006; Smith
et al., 2010). Few studies have been conducted
on quasi–comparable corpora. Quasi–comparable
corpora are available in far larger quantities than
noisy parallel or comparable corpora, while the
parallel sentence extraction task is significantly
more difficult.

While most studies are interested in language
pairs between English and other languages, we
focus on Chinese–Japanese, where parallel cor-
pora are very scarce. This study extracts
Chinese–Japanese parallel sentences from quasi–
comparable corpora. We adopt a system pro-
posed by Munteanu and Marcu (2005), which is
for parallel sentence extraction from comparable
corpora. We extend the system in several aspects
to make it even suitable for quasi–comparable cor-
pora. The core component of the system is a clas-
sifier which can identify parallel sentences from
non–parallel sentences. Previous method of clas-
sifier training by the Cartesian product is not prac-
tical, because it differs from the real process of
parallel sentence extraction. We propose a novel

34



Translated Ja

sentences as 

queriesSMT

IR: top N results

Common

Chinese

characters

Candidate 

sentence 

pairs

Parallel

sentencesFiltering 

Chinese 

corpora

Japanese 

corpora

Classifier
Probabilistic

dictionary

(2)

(1)

(3)

(4)

Zh-Ja parallel 

corpus

5k sentences

Figure 1: Parallel sentence extraction system.

method of classifier training and testing that sim-
ulates the real sentence extraction process, which
guarantees the quality of the extracted sentences.
Since Chinese characters are used both in Chi-
nese and Japanese, they can be powerful linguistic
clues to identify parallel sentences. Therefore, we
use Chinese character features, which significantly
improve the accuracy of the classifier. We con-
duct parallel sentence extraction experiments on
quasi–comparable corpora, and evaluate the qual-
ity of the extracted sentences from the perspective
of MT performance. Experimental results show
that our proposed system performs significantly
better than the previous study.

2 Parallel Sentence Extraction System

The overview of our parallel sentence extraction
system is presented in Figure 1. Source sentences
are translated to target language using a SMT sys-
tem (1). We retrieve the top N documents from tar-
get language corpora with a information retrieval
(IR) framework, using the translated sentences as
queries (2). For each source sentence, we treat
all target sentences in the retrieved documents as
candidates. Then, we pass the candidate sentence
pairs through a sentence ratio filter and a word–
overlap–based filter based on a probabilistic dic-
tionary, to reduce the candidates keeping more re-
liable sentences (3). Finally, a classifier trained on
a small number of parallel sentences, is used to
identify the parallel sentences from the candidates
(4). A parallel corpus is needed to train the SMT
system, generate the probabilistic dictionary and
train the classifier.

Our system is inspired by Munteanu and Marcu

(2005), however, there are several differences. The
first difference is query generation. Munteanu and
Marcu (2005) generate queries by taking the top
N translations of each source word according to
the probabilistic dictionary. This method is im-
precise due to the noise in the dictionary. In-
stead, we adopt a method proposed by Abdul–
Rauf and Schwenk (2011). We translate the source
sentences to target language with a SMT system
trained on the parallel corpus. Then use the trans-
lated sentences as queries. This method can gen-
erate more precise queries, because phrase–based
MT is better than word–based translation.

Another difference is that we do not conduct
document matching. The reason is that docu-
ments on the same topic may not exist in quasi–
comparable corpora. Instead, we retrieve the top
N documents for each source sentence. In com-
parable corpora, it is reasonable to only use the
best target sentence in the retrieved documents as
candidates (Abdul-Rauf and Schwenk, 2011). In
quasi–comparable corpora, it is important to fur-
ther guarantee the recall. Therefore, we keep all
target sentences in the retrieved documents as can-
didates.

Our system also differs by the way of classi-
fier training and testing, which is described in Sec-
tion 3 in detail.

3 Binary Classification of Parallel
Sentence Identification

Parallel sentence identification from non–parallel
sentences can be seen as a binary classification
problem (Munteanu and Marcu, 2005; Tillmann,
2009; Smith et al., 2010; Ştefǎnescu et al., 2012).

35



Since the quality of the extracted sentences is de-
termined by the accuracy of the classifier, the clas-
sifier becomes the core component of the extrac-
tion system. In this section, we first describe the
training and testing process, then introduce the
features we use for the classifier.

3.1 Training and Testing

Munteanu and Marcu (2005) propose a method of
creating training and test instances for the classi-
fier. They use a small number of parallel sentences
as positive instances, and generate non–parallel
sentences from the parallel sentences as negative
instances. They generate all the sentence pairs
except the original parallel sentence pairs in the
Cartesian product, and discard the pairs that do not
fulfill the condition of a sentence ratio filter and a
word–overlap–based filter. Furthermore, they ran-
domly discard some of the non–parallel sentences
when necessary, to guarantee the ratio of negative
to positive instances smaller than five for the per-
formance of the classifier.

Creating instances by using the Cartesian prod-
uct is not practical, because it differs from the real
process of parallel sentence extraction. Here, we
propose a novel method of classifier training and
testing that simulates the real parallel sentence ex-
traction process. For training, we first select 5k
parallel sentences from a parallel corpus. Then
translate the source side of the selected sentences
to target language with a SMT system trained on
the parallel corpus excluding the selected parallel
sentences. We retrieve the top N documents from
the target language side of the parallel corpus, us-
ing the translated sentences as queries. For each
source sentence, we consider all target sentences
in the retrieved documents as candidates. Finally,
we pass the candidate sentence pairs through a
sentence ratio filter and a word–overlap–based fil-
ter, and get the training instances. We treat the
sentence pairs that exist in the original 5k parallel
sentences as positive instances, while the remain-
der as negative instances. Note that positive in-
stances may be less than 5k, because some of the
parallel sentences do not pass the IR framework
and the filters. For the negative instances, we also
randomly discard some of them when necessary,
to guarantee the ratio of negative to positive in-
stances smaller than five. Test instances are gen-
erated by another 5k parallel sentences from the
parallel corpus using the same method.

There are several merits of the proposed
method. It can guarantee the quality of the ex-
tracted sentences, because of the similarity be-
tween the real sentence extraction process. Also,
features from the IR results can be used to further
improve the accuracy of the classifier. The pro-
posed method can be evaluated not only on the
test sentences that passed the IR framework and
the filters, but also on all the test sentences, which
is similar to the evaluation for the real extraction
process. However, there is a limitation of our
method that a both sentence–level and document–
level aligned parallel corpus is needed.

3.2 Features
3.2.1 Basic Features
The following features are the basic features we
use for the classifier, which are proposed by
Munteanu and Marcu (2005):

• Sentence length, length difference and length
ratio.

• Percentage of words on each side that have a
translation on the other side (according to the
probabilistic dictionary).

• Alignment features:

– Percentage and number of words that
have no connection.

– The top three largest fertilities.
– Length of the longest contiguous con-

nected span.
– Length of the longest unconnected sub-

string.

Alignment features are extracted from the align-
ment results of the parallel and non–parallel sen-
tences used as instances for the classifier. Note
that alignment features may be unreliable when
the quantity of non–parallel sentences is signifi-
cantly larger than parallel sentences.

3.2.2 Chinese Character Features
Different from other language pairs, Chinese and
Japanese share Chinese characters. In Chinese
the Chinese characters are called Hanzi, while in
Japanese they are called Kanji. Hanzi can be di-
vided into two groups, Simplified Chinese (used
in mainland China and Singapore) and Traditional
Chinese (used in Taiwan, Hong Kong and Macau).
The number of strokes needed to write characters

36



用饱和盐水洗饱和盐水洗饱和盐水洗饱和盐水洗涤乙醚相相相相，用无水硫酸无水硫酸无水硫酸无水硫酸镁干燥干燥干燥干燥。

エーテル相相相相を飽和飽和飽和飽和食塩水塩水塩水塩水で洗洗洗洗浄し，無水硫酸無水硫酸無水硫酸無水硫酸マグネシウムで乾燥乾燥乾燥乾燥した。

Wash ether phase with saturated saline,  and dry it with anhydrous magnesium.

Zh:

Ja:

Ref:

Figure 2: Example of common Chinese characters in a Chinese–Japanese parallel sentence pair.

Meaning snow love begin

TC 雪 (U+96EA) 愛 (U+611B) 發 (U+767C)
SC 雪 (U+96EA) 爱(U+7231) 发(U+53D1)
Kanji 雪 (U+96EA) 愛 (U+611B) 発 (U+767A)

Table 1: Examples of common Chinese characters
(TC denotes Traditional Chinese and SC denotes
Simplified Chinese).

has been largely reduced in Simplified Chinese,
and the shapes may be different from those in Tra-
ditional Chinese. Because Kanji characters origi-
nated from ancient China, many common Chinese
characters exist between Hanzi and Kanji. Table 1
gives some examples of common Chinese char-
acters in Traditional Chinese, Simplified Chinese
and Japanese with their Unicode.

Since Chinese characters contain significant se-
mantic information, and common Chinese charac-
ters share the same meaning, they can be valuable
linguistic clues for many Chinese–Japanese NLP
tasks. Many studies have exploited common Chi-
nese characters. Tan et al. (1995) used the occur-
rence of identical common Chinese characters in
Chinese and Japanese (e.g. “snow” in Table 1) in
automatic sentence alignment task for document–
level aligned text. Goh et al. (2005) detected com-
mon Chinese characters where Kanji are identical
to Traditional Chinese, but different from Simpli-
fied Chinese (e.g. “love” in Table 1). Using a Chi-
nese encoding converter1 that can convert Tradi-
tional Chinese into Simplified Chinese, they built
a Japanese–Simplified Chinese dictionary partly
using direct conversion of Japanese into Chinese
for Japanese Kanji words. Chu et al. (2011) made
use of the Unihan database2 to detect common
Chinese characters which are visual variants of
each other (e.g. “begin” in Table 1), and proved
the effectiveness of common Chinese characters
in Chinese–Japanese phrase alignment. Chu et
al. (2012a) exploited common Chinese charac-
ters in Chinese word segmentation optimization,
which improved the translation performance.

In this study, we exploit common Chinese char-
1http://www.mandarintools.com/zhcode.html
2http://unicode.org/charts/unihan.html

acters in parallel sentence extraction. Chu et
al. (2011) investigated the coverage of common
Chinese characters on a scientific paper abstract
parallel corpus, and showed that over 45% Chi-
nese Hanzi and 75% Japanese Kanji are common
Chinese characters. Therefore, common Chinese
characters can be powerful linguistic clues to iden-
tify parallel sentences.

We make use of the Chinese character map-
ping table created by Chu et al. (2012b) to de-
tect common Chinese characters. Following fea-
tures are used. We use an example of Chinese–
Japanese parallel sentence presented in Figure 2 to
explain the features in detail, where common Chi-
nese characters are in bold and linked with dotted
lines.

• Number of Chinese characters on each side
(Zh: 18, Ja: 14).

• Percentage of Chinese characters out of all
characters on each side (Zh: 18/20=90%, Ja:
14/32=43%).

• Ratio of Chinese character numbers on both
sides (18/14=128%).

• Number of n–gram common Chinese charac-
ters (1–gram: 12, 2–gram: 6, 3–gram: 2, 4–
gram: 1).

• Percentage of n–gram common Chinese char-
acters out of all n–gram Chinese characters
on each side (Zh: 1–gram: 12/18=66%, 2–
gram: 6/16=37%, 3–gram: 2/14=14%, 4–
gram: 1/12=8%; Ja: 1–gram: 12/14=85%,
2–gram: 6/9=66%, 3–gram=: 2/5=40%, 4–
gram: 1/3=33%).

Note that Chinese character features are only
applicable to Chinese–Japanese. However, since
Chinese and Japanese character information is a
kind of cognates (words or languages which have
the same origin), the similar idea can be applied to
other language pairs by using cognates. Cognates
among European languages have been shown ef-
fective in word alignments (Kondrak et al., 2003).
We also can use cognates for parallel sentence ex-
traction.

37



3.3 Rank Feature

One merit of our classifier training and testing
method is that features from the IR results can be
used. Here, we use the ranks of the retrieved doc-
uments returned by the IR framework as feature.

4 Experiments

We conducted classification and translation exper-
iments to evaluate the effectiveness of our pro-
posed parallel sentence extraction system.

4.1 Data

4.1.1 Parallel Corpus
The parallel corpus we used is a scientific
paper abstract corpus provided by JST3 and
NICT4. This corpus was created by the Japanese
project “Development and Research of Chinese–
Japanese Natural Language Processing Technol-
ogy”, containing various domains such as chem-
istry, physics, biology and agriculture etc. This
corpus is aligned in both sentence–level and
document–level, containing 680k sentences and
100k articles.

4.1.2 Quasi–Comparable Corpora
The quasi–comparable corpora we used are scien-
tific paper abstracts collected from academic web-
sites. The Chinese corpora were collected from
CNKI5, containing 420k sentences and 90k arti-
cles. The Japanese corpora were collected from
CiNii6 web portal, containing 5M sentences and
880k articles. Note that since the paper abstracts
in these two websites were written by Chinese and
Japanese researchers respectively through differ-
ent periods, documents on the same topic may not
exist in the collected corpora. We investigated
the domains of the Chinese and Japanese corpora
in detail. We found that most documents in the
Chinese corpora belong to the domain of chem-
istry. While the Japanese corpora contain various
domains such as chemistry, physics, biology and
computer science etc. However, the domain infor-
mation is unannotated in both corpora.

4.2 Classification Experiments

We conducted experiments to evaluate the accu-
racy of the proposed method of classification, us-

3http://www.jst.go.jp
4http://www.nict.go.jp
5http://www.cnki.net
6http://ci.nii.ac.jp

ing different 5k parallel sentences from the paral-
lel corpus as training and test data.

4.2.1 Settings
• Probabilistic dictionary: We took the top

5 translations with translation probability
larger than 0.1 created from the parallel cor-
pus.

• IR tool: Indri7 with the top 10 results.

• Segmenter: For Chinese, we used a
segmenter optimized for Chinese–Japanese
SMT (Chu et al., 2012a). For Japanese, we
used JUMAN (Kurohashi et al., 1994).

• Alignment: GIZA++8.

• SMT: We used the state–of–the–art phrase–
based SMT toolkit Moses (Koehn et al.,
2007) with default options, except for the dis-
tortion limit (6→20).

• Classifier: LIBSVM9 with 5–fold cross–
validation and radial basis function (RBF)
kernel.

• Sentence ratio filter threshold: 2.

• Word–overlap–based filter threshold: 0.25.

• Classifier probability threshold: 0.5.

4.2.2 Evaluation
We evaluate the performance of classification by
computing precision, recall and F–value, defined
as:

precision = 100× classified well
classified parallel

, (1)

recall = 100× classified well
true parallel

, (2)

F − value = 2× precision× recall
precision + recall

. (3)

Where classified well is the number of pairs
that the classifier correctly identified as parallel,
classified parallel is the number of pairs that
the classifier identified as parallel, true parallel
is the number of real parallel pairs in the test set.
Note that we only use the top 1 result identified as
parallel by the classifier for evaluation.

7http://www.lemurproject.org/indri
8http://code.google.com/p/giza-pp
9http://www.csie.ntu.edu.tw/∼cjlin/libsvm

38



Features Precision Recall F–value
Munteanu+ 2005 88.43 85.20/79.76 86.78/83.87
+Chinese character 91.62 93.63/87.66 92.61/89.60
+Rank 92.15 94.53/88.50 93.32/90.29

Table 2: Classification results for the filtered test
sentences (before “/”) and all the test sentences
(after “/”).

4.2.3 Results
We conducted classification experiments, compar-
ing the following three experimental settings:

• Munteanu+ 2005: Only using the features
proposed by Munteanu and Marcu (2005).

• +Chinese character: Add the Chinese charac-
ter features.

• +Rank: Further add the rank feature.

Results evaluated for the test sentences that
passed the IR framework and the filters, and all
the test sentences are shown in Table 2. We can
see that the Chinese character features can signifi-
cantly improve the accuracy. The accuracy can be
further improved by the rank feature.

4.3 Translation Experiments

We extracted parallel sentences from the quasi–
comparable corpora, and evaluated Chinese–to–
Japanese MT performance by appending the ex-
tracted sentences to two baseline settings.

4.3.1 Settings
• Baseline: Using all the 680k parallel sen-

tences in the parallel corpus as training data
(containing 11k sentences of chemistry do-
main).

• Tuning: Using another 368 sentences of
chemistry domain.

• Test: Using another 367 sentences of chem-
istry domain.

• Language model: 5–gram LM trained on the
Japanese side of the parallel corpus (680k
sentences) using SRILM toolkit10.

• Classifier probability threshold: 0.6.
10http://www.speech.sri.com/projects/srilm

Classifier # sentences
Munteanu+ 2005 (Cartesian) 27,077
Munteanu+ 2005 (Proposed) 5,994
+Chinese character (Proposed) 3,936
+Rank (Proposed) 3,516

Table 3: Number of extracted sentences.

The reason we evaluate on chemistry domain is
the one we described in Section 4.1.2 that most
documents in the Chinese corpora belong to the
domain of chemistry. We keep all the sentence
pairs rather than the top 1 result (used in the clas-
sification evaluation) identified as parallel by the
classifier. The other settings are the same as the
ones used in the classification experiments.

4.3.2 Results

Numbers of extracted sentences using different
classifiers are shown in Table 3, where

• Munteanu+ 2005 (Cartesian): Classifier
trained using the Cartesian product, and only
using the features proposed by Munteanu and
Marcu (2005).

• Munteanu+ 2005 (Proposed): Classifier
trained using the proposed method, and only
using the features proposed by Munteanu and
Marcu (2005).

• +Chinese character (Proposed): Add the Chi-
nese character features.

• +Rank (Proposed): Further add the rank fea-
ture.

We can see that the extracted number is signif-
icantly decreased by the proposed method com-
pared to the Cartesian product, which may indi-
cate the quality improvement of the extracted sen-
tences. Adding more features further decreases the
number.

We conducted Chinese–to–Japanese translation
experiments by appending the extracted sentences
to the baseline. BLEU–4 scores for experiments
are shown in Table 4. We can see that our proposed
method of classifier training performs better than
the Cartesian product. Adding the Chinese charac-
ter features and rank feature further improves the
translation performance significantly.

39



Example 1

Zh: 最后最后最后最后，本文说明了说明了说明了说明了光学算符的物理意义的物理意义的物理意义的物理意义。。。。
(Finally, this article explains the physical meaning of the optical operator.)

Ja: 最後に最後に最後に最後に化学ポテンシャルの物理的意味についての物理的意味についての物理的意味についての物理的意味について簡単に説明説明説明説明した。した。した。した。
(Finally, briefly explain the physical meaning of the chemical potential.)

Example 2

Zh: 发射光谱分析法的检出限发射光谱分析法的检出限发射光谱分析法的检出限发射光谱分析法的检出限及其测量方法的探讨。
(Discussion of detection limit and measurement methods of emission spectral  analysis method.)

Ja: 光電測光法による発光分光分析方法の検出限界。発光分光分析方法の検出限界。発光分光分析方法の検出限界。発光分光分析方法の検出限界。
(Detection limit of emission spectral analysis method by photoelectric photometry.)

Figure 3: Examples of extracted sentences (parallel subsentential fragments are in bold).

System BLEU
Baseline 38.64
Munteanu+ 2005 (Cartesian) 38.10
Munteanu+ 2005 (Proposed) 38.54
+Chinese character (Proposed) 38.87†

+Rank (Proposed) 39.47‡∗

Table 4: BLEU scores for Chinese–to–Japanese
translation experiments (“†” and “‡” denotes the
result is better than “Munteanu+ 2005 (Cartesian)”
significantly at p < 0.05 and p < 0.01 respec-
tively, “*” denotes the result is better than “Base-
line” significantly at p < 0.01).

4.3.3 Discussion
The translation results indicate that compared to
the previous study, our proposed method can ex-
tract sentences with better qualities. However,
when we investigated the extracted sentences, we
found that most of the extracted sentences are
not sentence–level parallel. Instead, they contain
many parallel subsentential fragments. Figure 3
presents two examples of sentence pairs extracted
by “+Rank (Proposed)”, where parallel subsenten-
tial fragments are in bold. We investigated the
alignment results of the extracted sentences. We
found that most of the parallel subsentential frag-
ments were correctly aligned with the help of the
parallel sentences in the baseline system. There-
fore, translation performance was improved by ap-
pending the extracted sentences. However, it also
led to many wrong alignments among the non–
parallel fragments which are harmful to transla-
tion. In the future, we plan to further extract
these parallel subsentential fragments, which can
be more effective for SMT (Munteanu and Marcu,
2006).

5 Related Work

As parallel sentences trend to appear in similar
document pairs, many studies first conduct doc-
ument matching, then identify the parallel sen-

tences from the matched document pairs (Utiyama
and Isahara, 2003; Fung and Cheung, 2004;
Munteanu and Marcu, 2005). Approaches with-
out document matching also have been proposed
(Tillmann, 2009; Abdul-Rauf and Schwenk, 2011;
Ştefǎnescu et al., 2012). These studies directly re-
trieve candidate sentence pairs, and select the par-
allel sentences using some filtering methods. We
adopt a moderate strategy, which retrieves candi-
date documents for sentences.

The way of parallel sentence identification can
be specified with two different approaches: bi-
nary classification (Munteanu and Marcu, 2005;
Tillmann, 2009; Smith et al., 2010; Ştefǎnescu
et al., 2012) and translation similarity measures
(Utiyama and Isahara, 2003; Fung and Cheung,
2004; Abdul-Rauf and Schwenk, 2011). We adopt
the binary classification approach with a novel
classifier training and testing method and Chinese
character features.

Few studies have been conducted for extract-
ing parallel sentences from quasi–comparable cor-
pora. We are aware of only two previous efforts.
Fung and Cheung (2004) proposed a multi-level
bootstrapping approach. Wu and Fung (2005) ex-
ploited generic bracketing Inversion Transduction
Grammars (ITG) for this task. Our approach dif-
fers from the previous studies that we extend the
approach for comparable corpora in several as-
pects to make it work well for quasi–comparable
corpora.

6 Conclusion and Future Work

In this paper, we proposed a novel method of clas-
sifier training and testing that simulates the real
parallel sentence extraction process. Furthermore,
we used linguistic knowledge of Chinese charac-
ter features. Experimental results of parallel sen-
tence extraction from quasi–comparable corpora
indicated that our proposed system performs sig-
nificantly better than the previous study.

40



Our approach can be improved in several as-
pects. One is bootstrapping, which has been
proven effective in some related works (Fung and
Cheung, 2004; Munteanu and Marcu, 2005). In
our system, bootstrapping can be done not only
for extension of the probabilistic dictionary, but
also for improvement of the SMT system used to
translate the source language to target language for
query generation. Moreover, as parallel sentences
rarely exist in quasi–comparable corpora, we plan
to extend our system to parallel subsentential frag-
ment extraction. Our study showed that Chi-
nese character features are helpful for Chinese–
Japanese parallel sentence extraction. We plan to
apply the similar idea to other language pairs by
using cognates.

References
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Par-

allel sentence generation from comparable corpora
for improved smt. Machine Translation, 25(4):341–
375.

Sisay Fissaha Adafre and Maarten de Rijke. 2006.
Finding similar sentences across multiple languages
in wikipedia. In Proceedings of EACL, pages 62–69.

Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Association for Computational
Linguistics, 19(2):263–312.

Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2011. Japanese-chinese phrase alignment
using common chinese characters information. In
Proceedings of MT Summit XIII, pages 475–482, Xi-
amen, China, September.

Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawahara,
and Sadao Kurohashi. 2012a. Exploiting shared
Chinese characters in Chinese word segmentation
optimization for Chinese-Japanese machine transla-
tion. In Proceedings of the 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT’12), Trento, Italy, May.

Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-
hashi. 2012b. Chinese characters mapping table of
Japanese, Traditional Chinese and Simplified Chi-
nese. In Proceedings of the Eighth Conference on
International Language Resources and Evaluation
(LREC’12), Istanbul, Turkey, May.

Dan Ştefǎnescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In Proceedings of the 16th Annual
Conference of the European Association for Ma-
chine Translation (EAMT’12), Trento, Italy, May.

Pascale Fung and Percy Cheung. 2004. Multi-level
bootstrapping for extracting parallel sentences from
a quasi-comparable corpus. In Proceedings of Col-
ing 2004, pages 1051–1057, Geneva, Switzerland,
Aug 23–Aug 27. COLING.

Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Building a Japanese-Chinese dic-
tionary using kanji/hanzi conversion. In Proceed-
ings of the International Joint Conference on Natu-
ral Language Processing, pages 670–681.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.

Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical transla-
tion models. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 46–48.

Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22–28.

Bin Lu, Tao Jiang, Kapo Chow, and Benjamin K. Tsou.
2010. Building a large english-chinese parallel cor-
pus from comparable patents and its experimental
application to smt. In Proceedings of the 3rd Work-
shop on Building and Using Comparable Corpora,
LREC 2010, pages 42–49, Valletta, Malta, May.

Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504, December.

Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 81–88, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.

Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
403–411, Los Angeles, California, June. Associa-
tion for Computational Linguistics.

41



Chew Lim Tan and Makoto Nagao. 1995. Automatic
alignment of Japanese-Chinese bilingual texts. IE-
ICE Transactions on Information and Systems, E78-
D(1):68–76.

Christoph Tillmann. 2009. A beam-search extrac-
tion algorithm for comparable data. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 225–228, Suntec, Singapore, August. Associ-
ation for Computational Linguistics.

Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning japanese-english news articles
and sentences. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 72–79, Sapporo, Japan, July. Associ-
ation for Computational Linguistics.

Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT summit XI, pages 475–482.

Dekai Wu and Pascale Fung. 2005. Inversion trans-
duction grammar constraints for mining parallel sen-
tences from quasi-comparable corpora. In IJCNLP,
pages 257–268.

Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web abilingual news col-
lections. In Proceedings of the 2002 IEEE Interna-
tional Conference on Data Mining, pages 745–748,
Maebashi City, Japan. IEEE Computer Society.

42


