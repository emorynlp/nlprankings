










































Recurrent Convolutional Neural Networks for Discourse Compositionality


Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119–126,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

Recurrent Convolutional Neural Networks for Discourse Compositionality

Nal Kalchbrenner
Department of Computer Science

Oxford University
nkalch@cs.ox.ac.uk

Phil Blunsom
Department of Computer Science

Oxford University
pblunsom@cs.ox.ac.uk

Abstract

The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of sen-
tences, so do sentences combine to form
the meaning of paragraphs, dialogues and
general discourse. We introduce both a
sentence model and a discourse model cor-
responding to the two levels of composi-
tionality. The sentence model adopts con-
volution as the central operation for com-
posing semantic vectors and is based on
a novel hierarchical convolutional neural
network. The discourse model extends the
sentence model and is based on a recur-
rent neural network that is conditioned in
a novel way both on the current sentence
and on the current speaker. The discourse
model is able to capture both the sequen-
tiality of sentences and the interaction be-
tween different speakers. Without feature
engineering or pretraining and with simple
greedy decoding, the discourse model cou-
pled to the sentence model obtains state of
the art performance on a dialogue act clas-
sification experiment.

1 Introduction

There are at least two levels at which the mean-
ing of smaller linguistic units is composed to form
the meaning of larger linguistic units. The first
level is that of sentential compositionality, where
the meaning of words composes to form the mean-
ing of the sentence or utterance that contains them
(Frege, 1892). The second level extends beyond
the first and involves general discourse composi-
tionality, where the meaning of multiple sentences
or utterances composes to form the meaning of
the paragraph, document or dialogue that com-
prises them (Korta and Perry, 2012; Potts, 2011).

The problem of discourse compositionality is the
problem of modelling how the meaning of general
discourse composes from the meaning of the sen-
tences involved and, since the latter in turn stems
from the meaning of the words, how the meaning
of discourse composes from the words themselves.

Tackling the problem of discourse composition-
ality promises to be central to a number of differ-
ent applications. These include sentiment or topic
classification of single sentences within the con-
text of a longer discourse, the recognition of di-
alogue acts within a conversation, the classifica-
tion of a discourse as a whole and the attainment
of general unsupervised or semi-supervised repre-
sentations of a discourse for potential use in di-
alogue tracking and question answering systems
and machine translation, among others.

To this end much work has been done on mod-
elling the meaning of single words by way of se-
mantic vectors (Turney and Pantel, 2010; Col-
lobert and Weston, 2008) and the latter have found
applicability in areas such as information retrieval
(Jones et al., 2006). With regard to modelling
the meaning of sentences and sentential compo-
sitionality, recent proposals have included sim-
ple additive and multiplicative models that do
not take into account sentential features such as
word order or syntactic structure (Mitchell and
Lapata, 2010), matrix-vector based models that
do take into account such features but are lim-
ited to phrases of a specific syntactic type (Ba-
roni and Zamparelli, 2010) and structured mod-
els that fully capture such features (Grefenstette et
al., 2011) and are embedded within a deep neu-
ral architecture (Socher et al., 2012; Hermann and
Blunsom, 2013). It is notable that the additive
and multiplicative models as well as simple, non-
compositional bag of n-grams and word vector av-
eraging models have equalled or outperformed the
structured models at certain phrase similarity (Bla-
coe and Lapata, 2012) and sentiment classifica-

119



tion tasks (Scheible and Schütze, 2013; Wang and
Manning, 2012).

With regard to discourse compositionality, most
of the proposals aimed at capturing semantic as-
pects of paragraphs or longer texts have focused
on bag of n-grams or sentence vector averaging
approaches (Wang and Manning, 2012; Socher et
al., 2012). In addition, the recognition of dialogue
acts within dialogues has largely been treated in
non-compositional ways by way of language mod-
els coupled to hidden Markov sequence models
(Stolcke et al., 2000). Principled approaches to
discourse compositionality have largely been un-
explored.

We introduce a novel model for sentential com-
positionality. The composition operation is based
on a hierarchy of one dimensional convolutions.
The convolutions are applied feature-wise, that is
they are applied across each feature of the word
vectors in the sentence. The weights adopted in
each convolution are different for each feature, but
do not depend on the different words being com-
posed. The hierarchy of convolution operations
involves a sequence of convolution kernels of in-
creasing sizes (Fig. 1). This allows for the com-
position operation to be applied to sentences of
any length, while keeping the model at a depth
of roughly

√
2l where l is the length of the sen-

tence. The hierarchy of feature-wise convolution
operations followed by sigmoid non-linear acti-
vation functions results in a hierarchical convo-
lutional neural network (HCNN) based on a con-
volutional architecture (LeCun et al., 2001). The
HCNN shares with the structured models the as-
pect that it is sensitive to word order and adopts a
hierarchical architecture, although it is not based
on explicit syntactic structure.

We also introduce a novel model for discourse
compositionality. The discourse model is based
on a recurrent neural network (RNN) architecture
that is a powerful model for sequences (Sutskever
et al., 2011; Mikolov et al., 2010). The model
aims at capturing two central aspects of discourse
and its meaning: the sequentiality of the sentences
or utterances in the discourse and, where applica-
ble, the interactions between the different speak-
ers. The underlying RNN has its recurrent and out-
put weights conditioned on the respective speaker,
while simultaneously taking as input at every turn
the sentence vector for the current sentence gener-
ated through the sentence model (Fig. 2).

Recurrent Convolutional Neural Networks for Discourse Compositionality

Nal Kalchbrenner
Department of Computer Science

Oxford University
nkalch@cs.ox.ac.uk

Phil Blunsom
Department of Computer Science

Oxford University
pblunsom@cs.ox.ac.uk

Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.

We here introduce

1 Introduction

2 Compositionality Models

2.1 Sentence Model
2.2 Discourse Model

3

4 Credits

This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.

5 Introduction

Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse

Sentence (n)

Agent (n)

Agent (n� 1)

Words (n)

Class (n)

Class (n� 1)

k = 2 k = 3 k = 4

6 General Instructions

Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors’ names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.

6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e
(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format

Recurrent Convolutional Neural Networks for Discourse Compositionality

Nal Kalchbrenner
Department of Computer Science

Oxford University
nkalch@cs.ox.ac.uk

Phil Blunsom
Department of Computer Science

Oxford University
pblunsom@cs.ox.ac.uk

Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.

We here introduce

1 Introduction

2 Compositionality Models

2.1 Sentence Model
2.2 Discourse Model

3

4 Credits

This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.

5 Introduction

Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse

Sentence (n)

Agent (n)

Agent (n� 1)

Words (n)

Class (n)

Class (n� 1)

k = 2 k = 3 k = 4

6 General Instructions

Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors’ names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.

6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e
(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format

Recurrent Convolutional Neural Networks for Discourse Compositionality

Nal Kalchbrenner
Department of Computer Science

Oxford University
nkalch@cs.ox.ac.uk

Phil Blunsom
Department of Computer Science

Oxford University
pblunsom@cs.ox.ac.uk

Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.

We here introduce

1 Introduction

2 Compositionality Models

2.1 Sentence Model
2.2 Discourse Model

3

4 Credits

This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.

5 Introduction

Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse

Sentence (n)

Agent (n)

Agent (n� 1)

Words (n)

Class (n)

Class (n� 1)

k = 2 k = 3 k = 4

6 General Instructions

Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors’ names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.

6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e
(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format

Recurrent Convolutional Neural Networks for Discourse Compositionality

Nal Kalchbrenner
Department of Computer Science

Oxford University
nkalch@cs.ox.ac.uk

Phil Blunsom
Department of Computer Science

Oxford University
pblunsom@cs.ox.ac.uk

Abstract
The compositionality of meaning extends
beyond the single sentence. Just as words
combine to form the meaning of an sen-
tence, so do sentences in turn combine se-
quentially to form the meaning of general
discourse. Discourse may take the form
of paragraphs, soliloqui or conversations
between multiple speakers. The problem
of cross-sentential compositionality is the
problem of modelling how the meaning
of the various forms of discourse arises
from the meaning of the utterances and the
words involved.

We here introduce

1 Introduction

2 Compositionality Models

2.1 Sentence Model
2.2 Discourse Model

3

4 Credits

This document has been adapted from the instruc-
tions for earlier ACL proceedings, including those
for ACL-2012 by Maggie Li and Michael White,
those from ACL-2010 by Jing-Shing Chang and
Philipp Koehn, those for ACL-2008 by Johanna D.
Moore, Simone Teufel, James Allan, and Sadaoki
Furui, those for ACL-2005 by Hwee Tou Ng and
Kemal Oflazer, those for ACL-2002 by Eugene
Charniak and Dekang Lin, and earlier ACL and
EACL formats. Those versions were written by
several people, including John Chen, Henry S.
Thompson and Donald Walker. Additional ele-
ments were taken from the formatting instructions
of the International Joint Conference on Artificial
Intelligence.

5 Introduction

Word vectors
Compositionality
Discourse compositionality
*Applications in context of a discourse

Sentence (n)

Agent (n)

Agent (n� 1)

Words (n)

Class (n)

Class (n� 1)

k = 2 k = 3 k = 4

6 General Instructions

Manuscripts must be in two-column format. Ex-
ceptions to the two-column format include the ti-
tle, authors’ names and complete addresses, which
must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines
in Subsection 6.5). Type single-spaced. Start
all pages directly under the top margin. See the
guidelines later regarding formatting the first page.
The manuscript should be printed single-sided and
its length should not exceed the maximum page
limit described in Section 8. Do not number the
pages.

6.1 Electronically-available resources
ACL 2013 provides this description in LATEX2e
(acl2013.tex) and PDF format (acl2013.pdf),
along with the LATEX2e style file used to format

Figure 1: A hierarchical convolutional neural net-
work for sentential compositionality. The bottom
layer represents a single feature across all the word
vectors in the sentence. The top layer is the value
for that feature in the resulting sentence vector.
Lines represent single weights and color coded
lines indicate sharing of weights. The parameter
k indicates the size of the convolution kernel at
the corresponding layer.

We experiment with the discourse model cou-
pled to the sentence model on the task of recog-
nizing dialogue acts of utterances within a conver-
sation. The dataset is given by 1134 transcribed
and annotated telephone conversations amounting
to about 200K utterances from the Switchboard
Dialogue Act Corpus (Calhoun et al., 2010).1 The
model is trained in a supervised setting without
previous pretraining; word vectors are also ran-
domly initialised. The model learns a probability
distribution over the dialogue acts at step i given
the sequence of utterances up to step i, the se-
quence of acts up to the previous step i−1 and the
binary sequence of agents up to the current step
i. Predicting the sequence of dialogue acts is per-
formed in a greedy fashion.2

We proceed as follows. In Sect. 2 we give the
motivation and the definition for the HCNN sen-
tence model. In Sect. 3 we do the same for the
RCNN discourse model. In Sect. 4 we describe
the dialogue act classification experiment and the
training procedure. We also inspect the discourse
vector representations produced by the model. We
conclude in Sect. 5.

1The dataset is available at compprag.
christopherpotts.net/swda.html

2Code and trained model available at nal.co

120



S

I O ix i-1 P(x )i

i-1H

Hello HAL do you read me s i

Figure 2: Recurrent convolutional neural network
(RCNN) discourse model based on a RNN archi-
tecture. At each step the RCNN takes as input
the current sentence vector si generated through
the HCNN sentence model and the previous label
xi−1 to predict a probability distribution over the
current label P (xi). The recurrent weights Hi−1

are conditioned on the previous agent ai−1 and
the output weights are conditioned on the current
agent ai. Note also the sentence matrix Ms of the
sentence model and the hierarchy of convolutions
applied to each feature that is a row in Ms to pro-
duce the corresponding feature in si.

2 Sentence Model

The general aim of the sentence model is to com-
pute a vector for a sentence s given the sequence
of words in s and a vector for each of the words.
The computation captures certain general consid-
erations regarding sentential compositionality. We
first relate such considerations and we then pro-
ceed to give a definition of the model.

2.1 Sentential compositionality

There are three main aspects of sentential compo-
sitionality that the model aims at capturing. To
relate these, it is useful to note the following basic
property of the model: a sentence s is paired to the
matrix Ms whose columns are given sequentially
by the vectors of the words in s. A row in Ms cor-
responds to the values of the corresponding feature
across all the word vectors. The first layer of the
network in Fig. 1 represents one such row of Ms,
whereas the whole matrix Ms is depicted in Fig.
2. The three considerations are as follows.

First, at the initial stage of the composition,
the value of a feature in the sentence vector is
a function of the values of the same feature in
the word vectors. That is, the m-th value in the
sentence vector of s is a function of the m-th
row of Ms. This aspect is preserved in the ad-
ditive and multiplicative models where the com-
position operations are, respectively, addition +
and component-wise multiplication �. The cur-
rent model preserves the aspect up to the compu-
tation of the sentence vector s by adopting one-
dimensional, feature-wise convolution operations.
Subsequently, the discourse model that uses the
sentence vector s includes transformations across
the features of s (the transformation S in Fig. 2).

The second consideration concerns the hierar-
chical aspect of the composition operation. We
take the compositionality of meaning to initially
yield local effects across neighbouring words and
then yield increasingly more global effects across
all the words in the sentence. Composition oper-
ations like those in the structured models that are
guided by the syntactic parse tree of the sentence
capture this trait. The sentence model preserves
this aspect not by way of syntactic structure, but
by adopting convolution kernels of gradually in-
creasing sizes that span an increasing number of
words and ultimately the entire sentence.

The third aspect concerns the dependence of the
composition operation. The operation is taken to
depend on the different features, but not on the dif-
ferent words. Word specific parameters are intro-
duced only by way of the learnt word vectors, but
no word specific operations are learnt. We achieve
this by using a single convolution kernel across a
feature, and by utilizing different convolution ker-
nels for different features. Given these three as-
pects of sentential compositionality, we now pro-
ceed to describe the sentence model in detail.

2.2 Hierarchical Convolutional Neural
Network

The sentence model is taken to be a CNN where
the convolution operation is applied one dimen-
sionally across a single feature and in a hierarchi-
cal manner. To describe it in more detail, we first
recall the convolution operation that is central to
the model. Then we describe how we compute the
sequence of kernel sizes and how we determine the
hierarchy of layers in the network.

121



k

k

k

k

m

m

m

m

1

1

2

23

3

4

4

(k   m)1*

Figure 3: Convolution of a vector m with a kernel
k of size 4.

2.2.1 Kernel and One-dimensional
Convolution

Given a sentence s and its paired matrix Ms, let
m be a feature that is a row in Ms. Before
defining kernels and the convolution operation,
let us consider the underlying operation of local
weighted addition. Let w1, ..., wk be a sequence
of k weights; given the feature m, local weighted
addition over the first k values of m gives:

y = w1m1 + ...+ wkmk (1)

Then, a kernel simply defines the value of k
by specifying the sequence of weights w1, ..., wk
and the one-dimensional convolution applies local
weighted addition with the k weights to each sub-
sequence of values of m.

More precisely, let a one-dimensional kernel k
be a vector of weights and assume |k| ≤ |m|,
where | · | is the number of elements in a vec-
tor. Then we define the discrete, valid, one-
dimensional convolution (k ∗m) of kernel k and
feature m by:

(k ∗m)i :=
k∑

j=1

kj ·mk+i−j (2)

where k = |k| and |k ∗m| = |m| − k + 1. Each
value in k ∗m is a sum of k values of m weighted
by values in k (Fig. 3). To define the hierarchical
architecture of the model, we need to define a se-
quence of kernel sizes and associated weights. To
this we turn next.

2.2.2 Sequence of Kernel Sizes
Let l be the number of words in the sentence
s. The sequence of kernel sizes 〈kli〉i≤t depends

only on the length of s and itself has length t =
d
√

2le − 1. It is given recursively by:

kl1 = 2, k
l
i+1 = k

l
i + 1, k

l
t = l −

t−1∑
j=1

(klj − 1)

(3)
That is, kernel sizes increase by one until the re-
sulting convolved vector is smaller or equal to the
last kernel size; see for example the kernel sizes in
Fig. 1. Note that, for a sentence of length l, the
number of layers in the HCNN including the input
layer will be t + 1 as convolution with the cor-
responding kernel is applied at every layer of the
model. Let us now proceed to define the hierarchy
of layers in the HCNN.

2.2.3 Composition Operation in a HCNN
Given a sentence s, its length l and a sequence
of kernel sizes 〈kli〉i≤t, we may now give the
recursive definition that yields the hierarchy of
one-dimensional convolution operations applied
to each feature f that is a row in Ms. Specifi-
cally, for each feature f , let Kfi be a sequence of
t kernels, where the size of the kernel |Kfi | = kli.
Then we have the hierarchy of matrices and corre-
sponding features as follows:

M1f,: = M
s
f,: (4)

Mi+1f,: = σ( K
f
i ∗Mif,: + bif ) (5)

for some non-linear sigmoid function σ and bias
bif , where i ranges over 1, ..., t. In sum, one-
dimensional convolution is applied feature-wise to
each feature of a matrix at a certain layer, where
the kernel weights depend both on the layer and
the feature at hand (Fig. 1). A hierarchy of matri-
ces is thus generated with the top matrix being a
single vector for the sentence.

2.2.4 Multiple merged HCNNs
Optionally one may consider multiple parallel
HCNNs that are merged according to different
strategies either at the top sentence vector layer or
at intermediate layers. The weights in the word
vectors may be tied across different HCNNs. Al-
though potentially useful, multiple merged HC-
NNs are not used in the experiment below.

This concludes the description of the sentence
model. Let us now proceed to the discourse model.

122



Open the pod bay doors HAL DaveI'm afraid I can't do thats

S S

i s i+1

I

H

O O

i

i i+1

P(x ) P(x     )  i+1ix i-1

I

xi

Figure 4: Unravelling of a RCNN discourse model to depth d = 2. The recurrent Hi and output Oi

weights are conditioned on the respective agents ai.

3 Discourse Model

The discourse model adapts a RNN architecture
in order to capture central properties of discourse.
We here first describe such properties and then de-
fine the model itself.

3.1 Discourse Compositionality
The meaning of discourse - and of words and utter-
ances within it - is often a result of a rich ensemble
of context, of speakers’ intentions and actions and
of other relevant surrounding circumstances (Ko-
rta and Perry, 2012; Potts, 2011). Far from cap-
turing all aspects of discourse meaning, we aim
at capturing in the model at least two of the most
prominent ones: the sequentiality of the utterances
and the interactions between the speakers.

Concerning sequentiality, just the way the
meaning of a sentence generally changes if words
in it are permuted, so does the meaning of a para-
graph or dialogue change if one permutes the sen-
tences or utterances within. The change of mean-
ing is more marked the larger the shift in the order
of the sentences. Especially in tasks where one is
concerned with a specific sentence within the con-
text of the previous discourse, capturing the order
of the sentences preceding the one at hand may be
particularly crucial.

Concerning the speakers’ interactions, the
meaning of a speaker’s utterance within a dis-
course is differentially affected by the speaker’s
previous utterances as opposed to other speakers’

previous utterances. Where applicable we aim at
making the computed meaning vectors reflect the
current speaker and the sequence of interactions
with the previous speakers. With these two aims
in mind, let us now proceed to define the model.

3.2 Recurrent Convolutional Neural Network
The discourse model coupled to the sentence
model is based on a RNN architecture with inputs
from a HCNN and with the recurrent and output
weights conditioned on the respective speakers.

We take as given a sequence of sentences or ut-
terances s1, ..., sT , each in turn being a sequence
of words si = yi1...y

i
l , a sequence of labels

x1, ..., xT and a sequence of speakers or agents
a1, ..., aT , in such way that the i-th utterance is
performed by the i-th agent and has label xi. We
denote by si the sentence vector computed by way
of the sentence model for the sentence si. The
RCNN computes probability distributions pi for
the label at step i by iterating the following equa-
tions:

hi = σ( Ixi−1 + H
i−1hi−1 + Ssi + bh) (6)

pi = softmax(O
ihi + bo) (7)

where I,Hi,Oi are corresponding weight matri-

ces for each agent ai and softmax(y)k =
eyk∑
j e

yj

returns a probability distribution. Thus pi is taken
to model the following predictive distribution:

pi = P (xi|x<i, s≤i, a≤i) (8)

123



Dialogue Act Label Example Train (%) Test (%)
Statement And, uh, it’s a legal firm office. 36.9 31.5
Backchannel/Acknowledge Yeah, anything could happen. 18.8 18.2
Opinion I think that would be great. 12.7 17.1
Abandoned/Uninterpretable So, - 7.6 8.6
Agreement/Accept Yes, exactly. 5.5 5.0
Appreciation Wow. 2.3 2.2
Yes−No−Question Is that what you do? 2.3 2.0
Non−Verbal [Laughter], [Throat-clearing] 1.7 1.9
Other labels (34) 12.2 13.5
Total number of utterances 196258 4186
Total number of dialogues 1115 19

Table 1: Most frequent dialogue act labels with examples and frequencies in train and test data.

An RCNN and the unravelling to depth d = 2 are
depicted respectively in Fig. 2 and Fig. 4. With
regards to vector representations of discourse, we
take the hidden layer hi as the vector represent-
ing the discourse up to step i. This concludes the
description of the discourse model. Let us now
consider the experiment.

4 Predicting Dialogue Acts

We experiment with the prediction of dialogue
acts within a conversation. A dialogue act spec-
ifies the pragmatic role of an utterance and helps
identifying the speaker’s intentions (Austin, 1962;
Korta and Perry, 2012). The automated recog-
nition of dialogue acts is crucial for dialogue
state tracking within spoken dialogue systems
(Williams, 2012). We first describe the Switch-
board Dialogue Act (SwDA) corpus (Calhoun et
al., 2010) that serves as the dataset in the experi-
ment. We report on the training procedure and the
results and we make some qualitative observations
regarding the discourse representations produced
by the model.

4.1 SwDA Corpus

The SwDA corpus contains audio recordings and
transcripts of telephone conversations between
multiple speakers that do not know each other and
are given a topic for discussion. For a given utter-
ance we use the transcript of the utterance, the dia-
logue act label and the speaker’s label; no other an-
notations are used in the model. Overall there are
42 distinct dialogue act labels such as Statement
and Opinion (Tab.1). We adopt the same data split
of 1115 train dialogues and 19 test dialogues as
used in (Stolcke et al., 2000).

4.2 Objective Function and Training

We minimise the cross-entropy error of the pre-
dicted and the true distributions and include an
l2 regularisation parameter. The RCNN is trun-
cated to a depth d = 2 so that the prediction of
a dialogue act depends on the previous two utter-
ances, speakers and dialogue acts; adopting depths
> 2 has not yielded improvements in the experi-
ment. The derivatives are efficiently computed by
back-propagation (Rumelhart et al., 1986). The
word vectors are initialised to random vectors of
length 25 and no pretraining procedure is per-
formed. We minimise the objective using L-BFGS
in mini-batch mode; the minimisation converges
smoothly.

4.3 Prediction Method and Results

The prediction of a dialogue act is performed in
a greedy fashion. Given the two previously pre-
dicted acts x̂i−1, x̂i−2, one chooses the act x̂i that
has the maximal probability in the predicted dis-
tribution P (xi). The LM-HMM model of (Stol-
cke et al., 2000) learns a language model for each
dialogue act and a Hidden Markov Model for the
sequence of dialogue acts and it requires all the
utterances in a dialogue in order to predict the dia-
logue act of any one of the utterances. The RCNN
makes the weaker assumption that only the utter-
ances up to utterance i are available to predict the
dialogue act x̂i. The accuracy results of the mod-
els are compared in Tab. 3.

4.4 Discourse Vector Representations

We inspect the discourse vector representations
that the model generates. After a dialogue is pro-
cessed, the hidden layer h of the RCNN is taken

124



Center A: Do you repair your own car? A: – I guess we can start. A: Did you use to live around here?
Dialogue B: I try to, whenever I can. B: Okay. B: Uh, Redwood City.
First NN A: Do you do it every day? A: I think for serial murder – A: Can you stand up in it?

B: I try to every day. B: Uh-huh. B: Uh, in parts.
Second NN A: Well, do you have any children? A: The USSR – wouldn’t do it A: [Laughter] Do you have any kids

that you take fishing?
B: I’ve got one. B: Uh-huh. B: Uh, got a stepdaughter.

Third NN A: Do you manage the money? A: It seems to me there needs A: Is our five minutes up?
to be some ground, you know,
some rules –

B: Well, I, we talk about it. B: Uh-huh. B: Uh, pretty close to it.
Fourth NN A: Um, do you watch it every A: It sounds to me like, uh, A: Do you usually go out, uh,

Sunday? you are doing well. with the children or without them?
B: [Breathing] Uh, when I can. B: My husband’s retired. B: Well, a variety.

Table 2: Short dialogues and nearest neighbours (NN).

Accuracy (%)
RCNN 73.9

LM-HMM trigram 71.0
LM-HMM bigram 70.6

LM-HMM unigram 68.2
Majority baseline 31.5
Random baseline 2.4

Table 3: SwDA dialogue act tagging accuracies.
The LM-HMM results are from (Stolcke et al.,
2000). Inter-annotator agreement and theoretical
maximum is 84%.

to be the vector representation for the dialogue
(Sect. 3.2). Table 2 includes three randomly cho-
sen dialogues composed of two utterances each;
for each dialogue the table reports the four near-
est neighbours. As the word vectors and weights
are initialised randomly without pretraining, the
word vectors and the weights are induced during
training only through the dialogue act labels at-
tached to the utterances. The distance between
two word, sentence or discourse vectors reflects
a notion of pragmatic similarity: two words, sen-
tences or discourses are similar if they contribute
in a similar way to the pragmatic role of the utter-
ance signalled by the associated dialogue act. This
is suggested by the examples in Tab. 2, where a
centre dialogue and a nearest neighbour may have
some semantically different components (e.g. “re-
pair your own car” and “manage the money”), but
be pragmatically similar and the latter similarity is
captured by the representations. In the examples,
the meaning of the relevant words in the utter-
ances, the speakers’ interactions and the sequence
of pragmatic roles are well preserved across the
nearest neighbours.

5 Conclusion

Motivated by the compositionality of meaning
both in sentences and in general discourse, we
have introduced a sentence model based on a novel
convolutional architecture and a discourse model
based on a novel use of recurrent networks. We
have shown that the discourse model together with
the sentence model achieves state of the art results
in a dialogue act classification experiment with-
out feature engineering or pretraining and with
simple greedy decoding of the output sequence.
We have also seen that the discourse model pro-
duces compelling discourse vector representations
that are sensitive to the structure of the discourse
and promise to capture subtle aspects of discourse
comprehension, especially when coupled to fur-
ther semantic data and unsupervised pretraining.

Acknowledgments

We thank Ed Grefenstette and Karl Moritz Her-
mann for great conversations on the matter. The
authors gratefully acknowledge the support of the
Clarendon Fund and the EPSRC.

References
[Austin1962] John L. Austin. 1962. How to do things

with words. Oxford: Clarendon.

[Baroni and Zamparelli2010] Marco Baroni and
Roberto Zamparelli. 2010. Nouns are vectors, ad-
jectives are matrices: Representing adjective-noun
constructions in semantic space. In EMNLP, pages
1183–1193.

[Blacoe and Lapata2012] William Blacoe and Mirella
Lapata. 2012. A comparison of vector-based rep-
resentations for semantic composition. In EMNLP-
CoNLL, pages 546–556.

125



[Calhoun et al.2010] Sasha Calhoun, Jean Carletta, Ja-
son M. Brenier, Neil Mayo, Dan Jurafsky, Mark
Steedman, and David Beaver. 2010. The nxt-format
switchboard corpus: a rich resource for investigat-
ing the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evaluation,
44(4):387–419.

[Collobert and Weston2008] R. Collobert and J. We-
ston. 2008. A unified architecture for natural lan-
guage processing: Deep neural networks with mul-
titask learning. In International Conference on Ma-
chine Learning, ICML.

[Frege1892] Gottlob Frege. 1892. Über Sinn
und Bedeutung. Zeitschrift für Philosophie und
philosophische Kritik, 100.

[Grefenstette et al.2011] Edward Grefenstette,
Mehrnoosh Sadrzadeh, Stephen Clark, Bob
Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional
models of meaning. CoRR, abs/1101.0309.

[Hermann and Blunsom2013] Karl Moritz Hermann
and Phil Blunsom. 2013. The Role of Syntax in
Vector Space Models of Compositional Semantics.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Sofia, Bulgaria, August. Association
for Computational Linguistics. Forthcoming.

[Jones et al.2006] Rosie Jones, Benjamin Rey, Omid
Madani, and Wiley Greiner. 2006. Generating
query substitutions. In WWW, pages 387–396.

[Korta and Perry2012] Kepa Korta and John Perry.
2012. Pragmatics. In Edward N. Zalta, editor, The
Stanford Encyclopedia of Philosophy. Winter 2012
edition.

[LeCun et al.2001] Y. LeCun, L. Bottou, Y. Bengio, and
P. Haffner. 2001. Gradient-based learning applied
to document recognition. In Intelligent Signal Pro-
cessing, pages 306–351. IEEE Press.

[Mikolov et al.2010] Tomas Mikolov, Martin Karafiát,
Lukas Burget, Jan Cernocký, and Sanjeev Khudan-
pur. 2010. Recurrent neural network based lan-
guage model. In INTERSPEECH, pages 1045–
1048.

[Mitchell and Lapata2010] Jeff Mitchell and Mirella
Lapata. 2010. Composition in distributional models
of semantics. Cognitive Science, 34(8):1388–1429.

[Potts2011] Christopher Potts. 2011. Pragmatics.
In Ruslan Mitkov, editor, The Oxford Handbook
of Computational Linguistics. Oxford University
Press, 2 edition.

[Rumelhart et al.1986] D. E. Rumelhart, G. E. Hinton,
and R. J. Williams. 1986. Learning internal repre-
sentations by error propagation. MIT Press Compu-
tational Models Of Cognition And Perception Series,
page 318362.

[Scheible and Schütze2013] Christian Scheible and
Hinrich Schütze. 2013. Cutting recursive autoen-
coder trees. CoRR, abs/1301.2811.

[Socher et al.2012] Richard Socher, Brody Huval,
Christopher D. Manning, and Andrew Y. Ng. 2012.
Semantic Compositionality Through Recursive
Matrix-Vector Spaces. In Proceedings of the 2012
Conference on Empirical Methods in Natural
Language Processing (EMNLP).

[Stolcke et al.2000] Andreas Stolcke, Klaus Ries, Noah
Coccaro, Elizabeth Shriberg, Rebecca A. Bates,
Daniel Jurafsky, Paul Taylor, Rachel Martin,
Carol Van Ess-Dykema, and Marie Meteer. 2000.
Dialog act modeling for automatic tagging and
recognition of conversational speech. Computa-
tional Linguistics, 26(3):339–373.

[Sutskever et al.2011] Ilya Sutskever, James Martens,
and Geoffrey E. Hinton. 2011. Generating text with
recurrent neural networks. In ICML, pages 1017–
1024.

[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: Vec-
tor space models of semantics. J. Artif. Intell. Res.
(JAIR), 37:141–188.

[Wang and Manning2012] Sida Wang and Christo-
pher D. Manning. 2012. Baselines and bigrams:
Simple, good sentiment and topic classification. In
ACL (2), pages 90–94.

[Williams2012] Jason D. Williams. 2012. A belief
tracking challenge task for spoken dialog systems.
In NAACL-HLT Workshop on Future Directions and
Needs in the Spoken Dialog Community: Tools and
Data, SDCTD ’12, pages 23–24, Stroudsburg, PA,
USA. Association for Computational Linguistics.

126


