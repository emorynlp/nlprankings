



















































Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 440–450,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Show Me Your Evidence – an Automatic Method for Context Dependent
Evidence Detection

Ruty Rinott1, Lena Dankin1, Carlos Alzate2, Mitesh M. Khapra3,

Ehud Aharoni1, Noam Slonim1

1IBM Research - Haifa, Mount Carmel, Haifa, 31905, Israel,
2IBM Research - Ireland, Damastown Industrial Estate, Dublin 15, Ireland,

3IBM Research - Bangalore, India,
{rutyr,lenad,aehud,noams}@il.ibm.com

carlos.alzate@ie.ibm.com mikhapra@in.ibm.com

Abstract
Engaging in a debate with oneself or others to
take decisions is an integral part of our day-to-
day life. A debate on a topic (say, use of per-
formance enhancing drugs) typically proceeds
by one party making an assertion/claim (say,
PEDs are bad for health) and then providing
an evidence to support the claim (say, a 2006
study shows that PEDs have psychiatric side
effects). In this work, we propose the task of
automatically detecting such evidences from
unstructured text that support a given claim.
This task has many practical applications in
decision support and persuasion enhancement
in a wide range of domains. We first introduce
an extensive benchmark data set tailored for
this task, which allows training statistical mod-
els and assessing their performance. Then, we
suggest a system architecture based on super-
vised learning to address the evidence detec-
tion task. Finally, promising experimental re-
sults are reported.

1 Introduction
In recent years there has been a growing interest in
the area of argumentation mining (Green et al., 2014;
Cardie et al., 2015; Wells, 2014). Part of this awak-
ening is the The DebaterTM project1 whose goal is to
develop technologies that will assist humans to de-
bate and reason, e.g., by automatically suggesting argu-
ments relevant to an examined topic. The minimal def-
inition of such an argument (Walton, 2009) is a set of
statements, made up of three parts – a claim (aka con-
clusion, proposition), a set of evidence (aka premises),
and an inference from the evidence to the claim. Need-
less to say, evidence plays a critical role in a persuasive
argument.

In most debate related skills, such as natural lan-
guage understanding and generation, humans currently
have an inherent advantage over a machine. However,
in the ability to provide high quality and diverse evi-
dence, machines have a very promising potential, being

1http://researcher.ibm.com/researcher/
view_group.php?id=5443

able to swiftly process large quantities of information.
Nonetheless, since most of the relevant information is
represented by unstructured text, successfully exploit-
ing these resources requires the ability to identify evi-
dence in free text. This is exactly the focus of our work.
Specifically, we formally define the task of evidence
detection, introduce an architecture for attacking this
problem, and demonstrate its performance over dedi-
cated manually labeled data.

Before defining the task formally, we introduce three
concepts which will be used throughout this paper.
These concepts were earlier defined in (Aharoni et al.,
2014) and we use the same definitions here. Topic: a
short phrase that frames the discussion. Claim: a gen-
eral, concise statement that directly supports or con-
tests the topic. Context Dependent Evidence (CDE):
a text segment that directly supports a claim in the con-
text of the topic. The first three rows of Table 1 show
examples of a topic, a claim and CDE.

For the purpose of this work, we assume that we
are given a concrete topic, a relevant claim, and po-
tentially relevant documents, provided either manually
or by automatic methods (Cartright et al., 2011; Levy
et al., 2014). Our task, which we term Context Depen-
dent Evidence Detection (CDED), is to automatically
pinpoint CDE within these documents. We further re-
quire that a detected CDE is reasonably well phrased,
and easily understandable in the given context, so that it
can be instantly and naturally used to support the claim
in a discussion. Table 1 gives examples of valid CDE
(V) and non-valid CDE (X) according to the definition
mentioned above.

It is well recognized that one can support a claim us-
ing different types of evidence (Rieke and Sillars, 2001;
Seech, 2008). Furthermore, for different use cases, dif-
ferent evidence types could be more suitable. Corre-
spondingly, we develop a classification approach that
is able to identify and distinguish between three com-
mon evidence types (Rieke and Sillars, 1984; Seech,
2008):

• Study Results of a quantitative analysis of data,
given as numbers, or as conclusions. (Table 1 S1);

2Note ibuprofen is considered a PED

440



Topic: Use of performance enhancing drugs
(PEDs) in professional sports
Claim A: PEDs can be harmful to athletes
health
S1: A 2006 study examined 320 athletes for
psychiatric side effects induced by anabolic
steroid use. The study found a higher incidence
of mood disorders in these athletes compared to
a control group.

V

S2: The International Agency for Research on
Cancer classifies androgenic steroids as “Prob-
ably carcinogenic to humans.”

V

S3: Rica Reinisch, a triple Olympic cham-
pion and world record-setter at the Moscow
Games in 1980, has suffered numerous mis-
carriages and recurring ovarian cysts following
drug abuse.

V

S4: The UN estimates that there are more than
50 million regular users of heroin, cocaine and
synthetic drugs.

X

S5: FDA does not approve ibuprofen2 for ba-
bies younger than six months due to risk of
liver damage.

X

S6: Doping can ultimately damage your health. X
Claim B: Use of PED is inline with the spirit
of sport
S7: Professor Savulescu, a philosopher and
bioethicist, believes that biological manipula-
tion embodies the sports spirit: the capacity to
improve ourselves on the basis of reason and
judgment.

V

Table 1: Examples for defined concepts. The V/X in-
dicates if the candidate is a CDE to the claim above it,
according to our definition.

• Expert Testimony by a person / group / commit-
tee / organization with some known expertise / au-
thority on the topic. (Table 1 S2, S7);

• Anecdotal A description of an episode(s), cen-
tered on individual(s) or clearly located in place
and/or in time. (Table 1 S3);

Examining the valid and non-valid CDEs in Table
1 it should be clear that the distinction between them
is often quite subtle. For example, it is possible that
a piece of text has the characteristics of a certain evi-
dence type, but does not support the claim (see S4 in
Table 1). It is also possible that a piece of text supports
the claim, but is irrelevant in the context of the topic
(see S5 in Table 1). It could also be the case that a piece
of text entails the claim, but adds no new information
to support it (see S6 in Table 1).

We present here a pipeline architecture, relying on
supervised learning, to handle the different aspects of
CDED which shows promising results over a variety
of topics. We demonstrate that the proposed solution

and features can generalize well, namely that models
learned over different topics can perform reasonably
well on an entirely new topic. On average, for a signif-
icant fraction of claims the proposed system succeeds
to propose relevant CDE amongst its top 4 predictions,
and properly determines the evidence type. Further-
more we show that we are able to automatically pin-
point claims for which the performance of the system
are of even greater quality, enabling the user to obtain
higher precision for these claims.

We believe that the ability to automatically provide
evidence for given claims will have many practical
uses, helping layman and professionals in different do-
mains, to reach decisions and prepare for discussions,
from a lawyer presenting a case in court, to a politician
considering a new policy.

2 Related work
CDED is related to several other information retrieval
and NLP tasks. Probably the closest of which is the
relatively unexplored task of Evidence Retrieval (ER)
(Cartright et al., 2011; Bellot et al., 2013). However,
while ER focus is on identifying whole documents, in
CDED the goal is to pinpoint a typically much shorter
text segment which can be used directly to support a
claim. Furthermore, ER is typically performed for fac-
tual assertions, while in CDED one may want to con-
sider a wider range of claim types (Rieke and Sillars,
2001), cf. claim B in Table 1.

Another important line of related work is the Textual
Entailment (TE) framework (Dagan et al., 2009; Glick-
man et al., 2005). A text fragment, T, is said to entail a
textual hypothesis H if the truth of H can be most likely
inferred from T. While TE can be an important com-
ponent in a CDED approach, and perhaps vice versa,
the tasks are quite different. Namely, the goal of TE is
detecting semantic inference while the goal of CDED
is to provide evidence which can enhance the persua-
sion of a claim. For example, common instances of TE
are rephrases or summarizations of a sentence, how-
ever they cannot serve to support a claim within a dis-
cussion, as they merely repeat it (Table 1, S6). On the
other hand, an anecdotal story may have strong emo-
tional impact that will effectively support a claim dur-
ing a discussion, although the truth of the claim cannot
be inferred from such evidence. Furthermore, similar
to ER, TE focuses only on factual assertions, while we
focus on a wider range of claims (Rieke and Sillars,
2001), cf. claim B in Table 1.

Question answering (QA) (Dang et al., 2007) also
has some similar aspects to the proposed task, although
aiming at a very different goal, which is to provide an
explicit – typically unique and concise – answer, to a
question.

The proposed CDED task should be seen as an-
other contribution in the emerging field of argumen-
tation mining, with several important distinct charac-
teristics. Previous works suggested extracting full ar-

441



Topics Claims Articles withCDE CDE
avg. % of

claims with
CDE

avg. # CDE per
claim

Study 30 1587 136 1018 31 (22) 2.2 (0.9)
Expert 37 1702 214 1896 46 (22) 1.9 (0.8)
Anecdotal 22 1137 70 382 17 (11) 2.0 (1.6)
Total 39 1734 274 3057 60 (17) 2.9 (3.7)

Table 2: ’Topics’ indicate the number of topics included for each CDE type. This determines the number of claims
considered for each type. The next columns indicate the number of articles in which at least one CDE was found;
the total number of CDE detected for each type; the average percent of claims for which at least one CDE was
found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple
sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution
across topics are given in parenthesis where relevant.

guments (Mochales Palau and Moens, 2009), analyz-
ing argument structure (Peldszus, 2014), and identi-
fying relations between arguments (Cabrio and Vil-
lata, 2012; Ghosh et al., 2014). Other works focused
on specific domains such as evidence-based legal doc-
uments (Mochales Palau and Moens, 2011; Ashley
and Walker, 2013), online debates (Cabrio and Villata,
2012; Boltužić and Šnajder, 2014), and product re-
views (Villalba and Saint-Dizier, 2012; Yessenalina et
al., 2010). In addition, some works based on machine-
learning techniques, used the same topic in training
and testing (Rosenfeld and Kraus, 2015; Boltužić and
Šnajder, 2014), relying on features from the topic itself
in identifying arguments. In contrast, here, we focus
on detecting an essential constituent of an argument –
the evidence – rather then detecting whole arguments,
or detecting other argument parts like claims (Levy et
al., 2014; Lippi and Torroni, 2015). In addition, we do
not limit ourselves to a particular domain, nor assume
that the topic of the discussion is known in advance. Fi-
nally, we aim to pinpoint evidence in a clearly defined
context, given by the pre–specified claim. Thus, the de-
veloped system should not only find pieces of text that
have general evidence characteristics but further iden-
tify which of these candidates can be used to support
a specific claim. Hence, as we demonstrate in our re-
sults, an essential part of a CDED system should be
dedicated to model and assess the semantic relation of
a candidate evidence to the given claim and topic.

3 Data

Since CDED is a new and rather complicated task, it is
beneficial to examine and understand the nature of the
data before moving on to developing a working solu-
tion. We therefore start by explaining the manual data
annotation process, and several important observations
over the resulting data.

To train and assess the classifiers in our system we
rely on data collected by the procedure described in
(Aharoni et al., 2014). Briefly, given a topic and a cor-
responding relevant claim, extracted from a Wikipedia
article by human annotators, the annotators were asked
to mark corresponding evidence – text segments sup-

porting the claim. To limit the amount of time anno-
tators spend on these tasks, labeling was restricted to
the article in which the claim was found. The task was
split into two stages. First, in the detection stage, five
annotators read the article, and mark all CDE candi-
dates they locate. Next, in the confirmation stage all the
candidates suggested by the annotators are presented to
another set of five annotators, which confirm or reject
each candidate, and determine the type(s) of accepted
candidates. Candidates which were confirmed by the
majority of the annotators are considered CDE, and are
assigned the type(s) suggested by at least three annota-
tors.

A total of 547 Wikipedia articles associated with 58
different topics were annotated through this procedure.
The topics were selected at random from Debatabase3

covering a wide variety of domains, from atheism to
the role of wind power in future energy supply. Out
of these topics, 39 were selected at random for train-
ing and testing the classifiers included in the system.
We refer to these data as the train and test data. The
remaining 19 topics were used for tuning various fea-
ture parameters, and developing auxiliary classifiers, as
described in Section 5. We refer to these data as the
held-out data.

In the 39 topics comprising the train and test data, a
total of 3, 057 distinct CDE were found in 274 articles
(See Table 2). The data is highly unbalanced towards
non CDE sentences. For example, for type Study, only
31% of the claims had at least one CDE. Of these 31%
claims, on average, a claim was associated with 2.17
CDEs. Further, on average these 2.17 CDEs together
span 1.5 sentences, whereas an average article in our
data consists of 150 sentences. In other words, even for
claims with at least one CDE of type Study, on average
only 2% of the sentences in the claim’s article are part
of such Study CDE.

In general, CDE in the examined data varied in
length from less than a sentence to more than a para-
graph. However, 90% of these CDE were composed of
segments of up to three sentences within the same para-
graph. Furthermore, in 95% of the cases, CDEs were

3http://idebate.org/debatabase

442



Figure 1: Schematic description of the CDED system proposed in this work.

comprised of full sentences. Examining CDEs that
start or end mid-sentence, reveals that in most cases the
CDE is more concise in these boundaries, but is still a
valid CDE when extending the boundaries to include
the full sentence. We therefore decided not to address
this issue here, and we extend all CDE boundaries to
full sentences.

Apparently CDE of type Study and type Expert are
far more common in Wikipedia compared to Anecdotal
CDE. We expect this distribution to change in other less
scientifically inclined corpora.

Finally, the variance between different topics was
substantial, as depicted in Table 2 (refer to the stan-
dard deviations mentioned in parenthesis). For exam-
ple, the percentage of claims with Expert CDE varies
from 10% in the topic banning gambling to 95% in
the topic US responsibility for the Mexican drug wars.
This observed variability obviously adds to the diffi-
culty and complexity of the task.

In the experiments reported in this paper, out of the
39 topics in the train and test data, we exclude from
the evaluation of each type, topics that had less than
three CDE of that type. This leaves a total of 30, 37,
and 22 topics for types Expert, Study, and Anecdotal,
respectively.

The current work is the first to report results over
these CDE data, which are more than 4 times larger
compared to the data released in (Aharoni et al., 2014).
These data are now freely available for research pur-
poses 4.

4 System Architecture

The input to our system is a topic, a set of related arti-
cles and a set of relevant claims detected within these
articles. Given this input, our system provides the user
with a ranked list of candidate CDEs, originating from
the text in the claim’s article, for an automatically se-
lected subset of the input claims.

In general, we observe that a text segment should
satisfy three criteria to be considered CDE of a specific
type. It must be coherent; it must have characteristics

4https://www.research.ibm.com/haifa/
dept/vst/mlta_data.shtml

of the relevant Evidence type; and finally, of course, it
should support the claim.

In addition to these observations, we note that a pri-
ori, we do not expect all claims to be supported by
all CDE types (Park and Cardie, 2014). For example,
opinion claims like claim B in Table 1 are expected to
be less supported by Study evidence compared to fac-
tual claims, like claim A in Table 1. Moreover, as ev-
ident from Table 2, many claims do not have any as-
sociated CDE in the same article. Thus, the system
performance may naturally improve if it will propose
candidate CDE of a particular type, only to an auto-
matically identified subset of the input claims.

Based on these observations, we are led to suggest an
architecture which approaches CDED via a pipeline of
modular components. Each of these components relies
upon the results of its precedents, and is specifically
designed to address a single aspect of those mentioned
above. The resulting architecture is depicted in Figure
1. Briefly, in the proposed architecture, the first two
components are context-free, i.e., focused on the gen-
eral characteristics of a candidate, still not taking into
account the context of the claim, nor the topic. The
third component is context-dependent, considering the
relation of the candidate to the claim and topic. Finally,
the fourth component aims to identify a subset of the
claims for which CDE will be proposed.

We consider all text segments composed of one,
two or three consecutive sentences, included within the
same paragraph as candidates (see Section 5 for more
details). Given a set of such candidate CDEs – or sim-
ply, candidates – the first component, termed the co-
herence component, estimates the coherence of each
candidate. For example, consider CDE S1 in Table
1. A candidate which includes only the second sen-
tence is incoherent, as it includes critical unresolved
anaphora, that cannot be understood without the pre-
vious sentence. In parallel, the second component,
termed the evidence characteristics component, esti-
mates the extent to which the candidate’s statistical sig-
nature matches that of the examined evidence type. For
example, if no quantitative analysis of data is reported,
the candidate typically cannot be considered Study evi-
dence, regardless of the claim and topic. Next, we only

443



retain candidates for which the average score of the first
two components was relatively high, aiming to further
focus our attention on the most promising candidates.

The retained candidates are then considered by the
context-dependent component which aims to determine
if the examined candidate indeed supports the provided
claim in the context of the topic. Thus, this component
ranks all retained candidates with respect to each claim.
Finally, the claim selection component aims to rank all
input claims, according to the probability that CDEs
are indeed found amongst top-ranking candidate for the
claim.

Dividing the overall task into sub-tasks has several
benefits. First, it allows training each component over
its most suitable data, in which the signal of the relevant
features is easier to capture. For example, many of the
features for the context-dependent component aim to
determine the semantic relatedness between the claim
and a candidate. If one would have tried to tackle the
entire CDED task simultaneously, the training data for
this component would have been masked by many can-
didates that are highly related to the claim, although
are not CDE – e.g., definitions of some aspects of the
claim. These candidates would have blurred the sig-
nal that should be captured by the semantic relatedness
features, as they represent candidates with negative la-
bels that are nonetheless semantically related to the
claim. By separating the tasks, we allow the context-
dependent component to avoid this inherent difficulty,
and train over much cleaner data.

Second, our pipeline allows efficient handling of the
CDED task in terms of run time. Semantic relatedness
features are often relatively complex and demanding in
terms of run time. The significant filtering done af-
ter the context-free stage, reduces the number of candi-
dates for which we have to calculate these features.

Finally, we note that some of the modular compo-
nents we develop as part of the pipeline might be of
interest by themselves. For example, context-free evi-
dence detection might be useful in cases in which the
claim and topic are not defined (Lippi and Torroni,
2015).

Naturally, we expect that different evidence types
will have different characteristics. For example, num-
bers are expected to be more common in CDE of type
Study compared to CDE of type Expert. Anecdotal
CDE is perhaps expected to be less semantically related
to the corresponding claim, as it may have a more asso-
ciative relation to the claim, compared to CDE of types
Study or type Expert. Correspondingly, all components
are developed, trained, and assessed, independently for
each CDE type.

In summary, the full flow of our system upon receiv-
ing a new topic with associated articles and claims, is
as follows:

1. All articles are split into sentences, and all con-
secutive segments up to three sentences within a
paragraph are generated as candidates.

2. Each candidate is assigned a score by the two
components in the context-free stage and their
scores are averaged. 5

3. A dynamic programming algorithm selects a com-
plete coverage of the article by non-overlapping
candidates with the maximal average context-free
score. The rest of the candidates are discarded.

4. The remaining candidates across all articles are
sorted and only the top 15% of these candidates
are retained. 6

5. For each claim, the context-dependent component
ranks all retained candidates within the claim’s ar-
ticle with respect to the claim.

6. The claim selection component considers all
claims and the candidates ranked with respect to
each claim and assigns a score per claim. If the
claim–score is below a pre–computed threshold,
no candidate CDE will be presented for that claim.

All components are based on a Logistic Regression
(LR) classifier, and the class probability is used as the
candidate score.

5 Technical approach
In this section, we provide more technical details for
each of the components in our architecture.

5.1 Coherence component
This component aims to score a candidate according
to its coherence. For example, a candidate with an
unresolved anaphora, or one that breaks a quotation
in the middle, is expected to receive a relatively low
score. As mentioned, this component considers all text
segments composed of 1–3 consecutive sentences in-
cluded within the same paragraph. This decision is
based on the observation that such segments cover 90%
of CDE in the labeled data. Reaching a full cover-
age requires examining segments up to 25 sentences,
which would vastly increase run time, for a relatively
small gain. Thus, for example, for a single paragraph
with five sentences, our system will examine a total of
5 + 4 + 3 = 12 candidates. For an article including 30
such paragraphs, a total of 360 candidates will be con-
sidered. During training, segments that conform to a
labeled CDE were considered positive examples, while
segments that overlap a labeled CDE, but either include
additional sentence(s), or exclude part of the CDE sen-
tences were considered negative examples.

Dominant features for this classifier included:
presence of incomplete quotes; presence of con-
trast related conjunctive adverbs – e.g., however,

5With additional training data, we might be able to learn
a more sophisticated function to combine both scores.

6This percentage was determined according to perfor-
mance on the held-out data set. We have also experimented
with methods where the threshold is score–based rather than
percentage–based, which gave similar results.

444



nevertheless; segment length; and presence of un-
resolved co-references.

5.2 Evidence characteristics component
This component aims to estimate to what extent a can-
didate represents evidence of a certain type. The train
and test data for this component consisted of all text
segments composed of 1-3 consecutive sentences, in-
cluded within the same paragraph. Positive examples
are all labeled CDE of the corresponding evidence type.
Negative examples are all candidates that do not over-
lap labeled CDE of the relevant type, including CDE of
other types.

The dominant features for the classifier used in this
component relied on the following mechanisms:

• Lexicons – including external lexicons (the Har-
vard IV-4 dictionary) and manually and automat-
ically compiled in-house lexicons. Specifically,
for each evidence type, we manually compiled a
lexicon of words characterizing this type by look-
ing at examples from the held-out data. This re-
sulted with high–precision / low–recall lexicons.
For example, for type Expert we used a lexi-
con of words describing persons and organizations
that may have some relevant expertise, such as:
economist, philosopher, court. In addi-
tion, we used the held-out data to automatically
learn wider lexicons of words that are significantly
associated with each type. All the in-house lexi-
cons are described in detail in the supplementary
material.

• Named Entity Recognition (NER). We used the
Stanford NER (Finkel et al., 2005) to extract
named entities such as person and organization,
and an in-house NER (Lally et al., 2012) to extract
more fine grained categories such as ”educational
organization” and ”leader”.

• Patterns. We used regular expressions to repre-
sent features like: does that candidate contain a
quote; does it contain a citation; does it contain
numeric quantitative results. In addition, we gen-
erated complex regular expressions which com-
bine the above lexicons with NER results to cap-
ture patterns indicative of different types. For ex-
ample, the pattern [Person/organization, 0 to 10
wildcard words, an opinion verb - such as believe,
conclude, etc.] was highly indicative of Expert ev-
idence (cf. Table 1 S7).

• Subjectivity classifier. We manually labeled
1, 750 sentences, selected at random from articles
in the held-out data, as either subjective or objec-
tive. Next, each sentence was represented by a
concatenation of two feature vectors – (i) a bag-
of-words representation, limited to a handcrafted
subjectivity lexicon containing 100 words; (ii) a
bag-of-patterns representation based on patterns

observed as frequent in the subjective sentences,
detected by a modification of the SPM algorithm
(Srikant and Agrawal, 1996). An LR classifier
was then trained over the labeled sentences.

5.3 Context-dependent component
The goal of this component is to estimate whether a
candidate can be used to support a claim while dis-
cussing the given topic. The training data for this com-
ponent are [topic / claim / CDE] triplets. Triplets in
which the CDE and claim were linked in the labeled
data – namely, the CDE was identified as evidence for
the claim – were considered as positive examples. Neg-
ative examples were generated by combining claims
and CDEs detected in the same topic and article, but
that were not linked in our labeled data.

The features for the classifier used in this component
can be conceptually divided into four types: (i) Seman-
tic relatedness between the candidate and the claim (ii)
Semantic relatedness between text related to the candi-
date and the claim (iii) Relative location of the can-
didate with respect to the claim and (iv) sentiment–
agreement between the candidate and the claim.

In general, we rely on two methods to assess the se-
mantic relatedness between two texts. The first is based
on the cosine similarity between TF-IDF vectors repre-
senting each text. Before constructing the TF-IDF vec-
tors each text is augmented with acronym expansions,
and lexical relations (including antonym, derivationally
related and pertainym) from WordNet (Miller, 1995).
The second, relies on the average cosine similarity be-
tween the Word2Vec (Mikolov et al., 2013) representa-
tion of all pairs of words in the two texts, where in each
pair one word is taken from the first text and the other
word from the second.

For each of these two methods, we consider the se-
mantic relatedness between the claim and: Specified
slots in the candidate as detected by an in-house slot
grammar parser (McCord, 1990; McCord et al., 2012);
The entire candidate text; The header/sub-header of the
section/subsection containing the candidate; Titles of
citations referred to from the candidate.

5.4 Claim selection component
The goal of this component is to rank all claims accord-
ing to the probability that the claim’s article includes
CDE of the relevant type, associated with the claim.
The training data consisted of all claims, where pos-
itive examples included claims for which at least one
CDE of the relevant type existed in the labeled data and
negative examples included all remaining claims.

A thresholding mechanism on the component score
is used to determine the claims for which candidates
will be presented. This threshold was selected by opti-
mizing the F1 score over the set of held-out topics.

The features used by this component exploited three
types of information:

• Claim properties: We used the held-out data to

445



generate two types of lexicons. The first lexicon
is generated separately per evidence type. It in-
cludes claim words that were found to be signifi-
cantly associated with positive examples, namely
with claims for which CDE were found. For
example, for type Study, this lexicon included
words such as lead, result, development
and significant. The second lexicon aimed
to characterize words that are significantly associ-
ated with factual claims vs. non–factual claims,
with the expectation that certain evidence types
might be more/less common for each of these
two claim categories. For this, 550 randomly
selected claims were annotated as factual/non–
factual. Words identified as characterizing factual
claims included increase, important, and
relate, while words like natural, freedom,
and right were found dominant for non-factual
claims.

• Claim’s relevance to topic and article: We ex-
pect that when an article’s main topic is highly re-
lated to the claim, it will more likely include CDE
for that claim. Similarly we expect that for claims
at the heart of the topic, CDE is more likely to be
provided. These properties are assessed by mea-
suring the semantic relatedness between (i) the
claim and the content of the claim’s article and (ii)
the claim and topic.

• Properties of claim’s article : Specifically,
we mainly consider the scores provided by the
context-dependent component to all candidates
examined in the claim’s article. If the observed
scores are relatively high/low, we expect the arti-
cle to be more/less likely to include evidence of
the considered type. Various statistics of these
scores, such as the maximum score and the stan-
dard deviation are used as features aiming to cap-
ture this intuition.

6 Experimental Results

6.1 Evaluation

We evaluated our approach using the Leave-One-Out
schema: for every topic, we trained the classifiers using
the claims and associated CDE in all other topics and
then applied the resulting models to the left out topic.

In general, we consider a candidate as true-positive if
it includes all sentences included in the CDE and no ad-
ditional sentences. However, for our analysis it is also
interesting to separate between (i) errors in selecting
the segment boundaries and (ii) errors of down the line
components that are affected by these errors. Thus, we
also include the overlap measure where we consider a
candidate as true-positive if at least one sentence within
it overlaps a sentence in a labeled CDE.

Our final assessment measure is the mean recipro-
cal rank (MRR), that is the inverse of the rank of the

first CDE detected for a particular claim, averaged over
all claims selected by the claim selection component.
This is motivated by the observation that in most prac-
tical use cases, it is usually more important to be able to
support many claims, than to provide all the CDE avail-
able for a single claim. We define the MRR of a claim
with no CDE (errors of the claim selection component)
to be 0.

Finally, we report the macro-averaged results over
the different topics, that is all topics have the same
weight regardless the amount of labeled claims and la-
beled CDE detected for them. The rational behind this
is that we wish to ensure that our system does rea-
sonably well across all topics examined. We note that
micro-averaging gave overall similar results.

6.2 Comparison to Baselines

To assess the necessity and contribution of the differ-
ent components we compare our full pipeline to partial
pipelines, where some of the component are disabled
or replaced by simple baselines. These baselines are
described below.

First, we consider the No Context-Free Stage
(NCFS) baseline which aims to assess the contribution
of the context-free stage by skipping this stage, and
passing all candidates directly to the context-dependent
component.

Next, we consider the Basic Claim Selection (BCS)
baseline which replaces the claim selection component.
It ranks claims according to the top score of the candi-
date CDE for the claim. A threshold was selected on
top of the training data, such that the average percent-
age of claims passing the threshold is equal to the av-
erage percentage of claims with CDEs in the labeled
data.

Since, to the best of our knowledge, this is the first
work to address CDED, there is no prior-art to com-
pare our results to. However, to ensure that this task
is indeed empirically different from related tasks, and
demands a specialized pipeline to handle, we compare
with two baselines that are often used in related tasks.

The BM25 basline handles CDED as an IR task,
where the claim represents the query, and all CDE can-
didates represent the documents in a standard IR set-
ting. After pre-processing, which includes tokeniza-
tion, stop word removal, and stemming (Porter, 1997)
we use BM25 (Robertson et al., 1996) to rank all rel-
evant candidates according to their similarity to the
query, namely to the input claim.

The W2V baseline handles CDED as a purely se-
mantic relatedness task using state of the art seman-
tic relatedness measure of Word2Vec (Mikolov et al.,
2013). Thus, we use the average cosine similarity be-
tween the Word2Vec representations of all words in a
given candidate to all words in the claim, to rank all
relevant candidates with respect to each claim.

446



Type MRR MRR overlap
Pipeline NCFS W2V BM25 Pipeline NCFS W2V BM25

Study 0.37 0.19 0.09 0.14 0.51 0.39 0.24 0.23
Expert 0.41 0.29 0.28 0.15 0.58 0.52 0.50 0.24
Anecdotal 0.18 0.04 0.04 0.04 0.31 0.11 0.11 0.11

Table 3: Macro-averaged MRR for each CDE type. Only claims with CDE in the labeled data were considered in
these results.

6.3 Results

We start by assessing the proposed pipeline prior to the
claim selection component. Table 3 reports the MRR
following the context-dependent component when fil-
tering out claims for which no CDE were found in the
labeled data.
Impact of context free stage: Comparing the pipeline
performance to the baseline using only the context de-
pendent component (NCFS baseline), the results in-
dicate the necessity of the context-free stage in our
pipeline. That is, assessing the coherence of candi-
dates, as well as their evidence characteristics, seems
to be essential to properly address CDED. In particu-
lar, the fact that the gain is observed both in the MRR
measure and in the MRR-overlap measure suggests that
both the context-free components are valuable.
Impact of context dependent stage: Comparing the
NCFS basline to W2V and BM25 baselines shows that
for type Study, the context-dependent component alone
still has an advantage over a single semantic related-
ness feature. Observing feature weights learned by the
LR classifier, we estimate that much of this advantage
is due to also taking into consideration semantic relat-
edness of the claim to texts related to the candidate,
namely the header of the section containing the candi-
date and titles of citations referred to from the candi-
date.

For types Expert and Anecdotal the performance of
the context-dependent component are similar to those
of the W2V baseline. For type Expert, this suggests
that most of the signal in the context-dependent com-
ponent comes from semantic relatedness between the
claim and candidate CDE. Results for type Anecdotal
are significantly lower. This was somewhat expected,
given the smaller size of Anecdotal data available to
train our classifiers (Table 2). The declined perfor-
mance of the W2V and BM25 baselines for this type,
further suggests that the semantic relatedness of CDE
and claims for this type are less direct.
Impact of detecting segment boundaries: Compar-
ing the overlap MRR measure to the exact MRR high-
lights that identifying the correct segment boundaries is
still a challenge, and once we improve this aspect, we
can expect a significant improvement in the results.
Impact of claim selection component: We next turn
to assess the contribution of the claim selection compo-
nent. Table 4 compares the final MRR results – at the
end of the pipeline – for claims selected by the claim
selection component, vs. claims selected by the BCS

Type Pipeline BCS All claims
Study 0.25 0.16 0.12
Expert 0.34 0.23 0.20
Anecdotal 0.04 0.05 0.03

Table 4: Macro-averaged MRR over: 1) claims se-
lected by the claim selection component, 2) claims se-
lected by basic claim selection, and 3) all claims.

baseline. Additionally, to demonstrate the value of
claim selection in general, we add results when consid-
ering all claims. For types Expert and Study the claim
selection component shows a clear advantage over the
baselines. Furthermore, the improved performance is
achieved when passing a higher percentage of claims
than the BCS baseline (34% vs 31% for Study and 52%
vs 46% for Expert, Figure 2). Admittedly, for Anecdo-
tal CDE the performance of claim selection are poor.
For this component the small sample size for Anecdo-
tal CDE was even more acute – there were only 151
claims with CDE of type Anecdotal – thus few positive
examples to train this component.

Recall that the claim selection component’s thresh-
old was tuned over the held-out data to optimize the
F1 measure with respect to claims with/without CDE.
However, for some applications one may favor higher
precision at the expense of providing candidate CDEs
for less claims. Figure 2 shows that indeed, for type
Study, considering more strict thresholds of the claim
selection component monotonically improves the sys-
tem’s overall precision, as reflected by the improved
MRR. Similar results were obtained for type Expert.

Figure 2: MRR and average fraction of passed claims
as function of the claim selection threshold for type
Study. Arrow indicates threshold used to obtain the re-
sults in Table 4.

6.4 Examples of System Performance
To provide some intuition for the results of our sys-
tem, Table 5 shows the 4 top ranking candidate CDE

447



According to econometric studies, negative side ef-
fects of aid can include an unbalanced appreciation
of the recipient’s currency, increasing corruption,
and adverse political effects such as postponements
of necessary economic and democratic reforms.

X

Many econometric studies in recent years have sup-
ported the view that development aid has no effect
on the speed with which countries develop.

V

An inquiry into aid effectiveness by the UK All
Party Parliamentary Group (APPG) for Debt, Aid
and Trade featured evidence from Rosalind Eyben,
a Fellow at the Institute of Development Studies.

X

A very large part of the spend money on develop-
ment aid is simply wasted uselessly. According to
Gerbert van der Aa, for the Netherlands, only 33% of
the development aid is successful, another 33% fails
and of the remaining 33% the effect is unclear. This
means that for example for the Netherlands, 1.33 to
2.66 billion is lost as it spends 4 billion in total of
development aid.

V

Table 5: Top ranking candidates for the claim aid is
ineffective in the context of the topic trade vs. aid

of type Study for the claim aid is ineffective in the con-
text of the topic trade vs. aid. Among these, 2 were
indeed labeled as CDE. The other two exemplify com-
mon errors of our system. Candidate 1 can be used to
support a highly related claim such as aid has nega-
tive side effects, but does not directly support the claim
under consideration. Candidate 3 mentions a relevant
study, but does not present its results, hence cannot be
used to support the claim.

7 Conclusions and Future Work

We have provided the definitions for the CDED task,
and described a system architecture that addresses the
issues at the heart of the task. We assessed the perfor-
mance of the proposed approach over a novel bench-
mark dataset, demonstrating the validity of our archi-
tecture, and the necessity of all its components.

There are still many open issues to address and di-
rections in which to expand the task and labeled data
which we hope to address in future work.

In this paper we define CDE only in the context of
supporting a claim. However, in many scenarios pro-
viding counter evidence can also be very useful. As
evidence supporting and contesting a claim share many
semantic and syntactic features, we believe that detect-
ing both cases simultaneously might be easier to ac-
complish, although to enhance the practical use of such
a solution, one may need to develop an additional com-
ponent, determining the polarity of the detected CDE.

Another natural direction to pursue is expanding the
documents which are considered for CDED beyond the
article containing the claim. These can include ad-
ditional Wikipedia articles and other resources such
as newspaper archives, scientific literature, blogs, etc.
This poses additional challenges in gathering labeled
data, as it will require a mechanism to decide which

documents to label per claim and will probably increase
the number of documents to be labeled. Expanding to
additional corpora will probably require development
of additional features, to capture signals unique to each
corpus. For example, in newspaper archives, the iden-
tity of the author might prove an important feature.

Finally, in this work we used manually identified
claims and articles. Combining a CDED solution with
recent works in the field of argumentation mining (Car-
tright et al., 2011; Levy et al., 2014; Lippi and Torroni,
2015), may give rise to a new generation of methods,
that will be able to automatically construct relevant ar-
guments on demand, for a variety of topics.

Acknowledgments
The authors would like to thank Oren Tsur, Vikas C.
Raykar, Matan Orbach and Ido Dagan for many helpful
discussions.

References
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel

Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfre-
und, and Noam Slonim. 2014. A benchmark dataset
for automatic detection of claims and evidence in the
context of controversial topics. In Proceedings of
the First Workshop on Argumentation Mining, pages
64–68, Baltimore, Maryland, June. Association for
Computational Linguistics.

Kevin D. Ashley and Vern R. Walker. 2013. Toward
constructing evidence-based legal arguments using
legal decision documents and machine learning. In
Proceedings of the Fourteenth International Confer-
ence on Artificial Intelligence and Law, ICAIL ’13,
pages 176–180, New York, NY, USA. ACM.

Patrice Bellot, Antoine Doucet, Shlomo Geva, Sairam
Gurajada, Jaap Kamps, Gabriella Kazai, Mar-
ijn Koolen, Arunav Mishra, Veronique Moriceau,
Josiane Mothe, Michael Preminger, Eric SanJuan,
Ralf Schenkel, Xavier Tannier, Martin Theobald,
Matthew Trappett, and Qiuyue Wang. 2013.
Overview of inex 2013. In CLEF Lab Reports, Va-
lencia, Spain, September.

Filip Boltužić and Jan Šnajder. 2014. Back up your
stance: Recognizing arguments in online discus-
sions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 49–58, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.

Elena Cabrio and Serena Villata. 2012. Combin-
ing textual entailment and argumentation theory for
supporting online debates interactions. In ACL (2),
pages 208–212.

Claire Cardie, Nancy Green, Iryna Gurevych, Graeme
Hirst, Diane Litman, Smaranda Muresan, Georgios
Petasis, Manfred Stede, Marilyn Walker, and Janyce
Wiebe, editors. 2015. Proceedings of the Second

448



Workshop on Argumentation Mining. Association
for Computational Linguistics, Denver, Colorado,
June.

Marc-Allen Cartright, Henry A. Feild, and James Al-
lan. 2011. Evidence finding using a collection
of books. In Proceedings of the 4th ACM Work-
shop on Online Books, Complementary Social Media
and Crowdsourcing, BooksOnline ’11, pages 11–18,
New York, NY, USA. ACM.

I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(04).

Hoa Trang Dang, Diane Kelly, and Jimmy J. Lin. 2007.
Overview of the trec 2007 question answering track.
In TREC.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 363–370, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,
Mark Aakhus, and Matthew Mitsui. 2014. Analyz-
ing argumentative discourse units in online interac-
tions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 39–48, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.

Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
A probabilistic classification approach for lexical
textual entailment. In Manuela M. Veloso and Sub-
barao Kambhampati, editors, AAAI, pages 1050–
1055. AAAI Press / The MIT Press.

Nancy Green, Kevin Ashley, Diane Litman, Chris
Reed, and Vern Walker, editors. 2014. Proceed-
ings of the First Workshop on Argumentation Min-
ing. Association for Computational Linguistics,
Baltimore, Maryland, June.

Adam Lally, John M. Prager, Michael C. McCord, Bra-
nimir Boguraev, Siddharth Patwardhan, James Fan,
Paul Fodor, and Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How watson reads a clue. IBM Jour-
nal of Research and Development, 56(3):2.

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context depen-
dent claim detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1489–
1500, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.

Marco Lippi and Paolo Torroni. 2015. Context-
independent claim detection for argumentation min-
ing. In Proceedings of the Twenty Fourth Inter-
national Joint Conference on Artificial Intelligence.
AAAI Press.

Michael C. McCord, William J. Murdock, and Bill K.
Boguraev. 2012. Deep parsing in watson. IBM J.
Res. Dev., 56(3):264–278, May.

Michael C. McCord. 1990. Slot grammar: A sys-
tem for simpler construction of practical natural lan-
guage grammars. In R. Studer, editor, Natural Lan-
guage and Logic: Proc. of the International Scien-
tific Symposium, Hamburg, FRG, pages 118–145.
Springer, Berlin, Heidelberg.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

George A. Miller. 1995. Wordnet: A lexical database
for english. COMMUNICATIONS OF THE ACM,
38:39–41.

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: the detection, clas-
sification and structure of arguments in text. In Pro-
ceedings of the Twelfth International Conference on
Artificial Intelligence and Law (ICAIL 2009),, pages
98–109. ACM.

Raquel Mochales Palau and Marie-Francine Moens.
2011. Argumentation mining. Artificial Intelligence
and Law, 19(1):1–22.

Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the First Workshop
on Argumentation Mining, pages 29–38, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.

Andreas Peldszus. 2014. Towards segment-based
recognition of argumentation structure in short texts.
In Proceedings of the First Workshop on Argumen-
tation Mining, pages 88–97, Baltimore, Maryland,
June. Association for Computational Linguistics.

Martin F. Porter. 1997. Readings in information re-
trieval. chapter An Algorithm for Suffix Stripping,
pages 313–316. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.

Richard D. Rieke and Malcolm O. Sillars. 1984. Argu-
mentation and the decision making process. Harper
Collins, New York, NY, USA.

Richard D. Rieke and Malcolm O. Sillars. 2001. Argu-
mentation and Critical Decision Making. Longman.

Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1996. Okapi at trec-3. pages 109–126.

Ariel Rosenfeld and Sarit Kraus. 2015. Providing ar-
guments in discussions based on the prediction of
human argumentative behavior. In Proceedings of
the Twenty-Ninth AAAI Conference on Artificial In-
telligence (AAAI-15).

Zachary Seech. 2008. Writing Philosophy Papers.
Cengage Learning.

449



Ramakrishnan Srikant and Rakesh Agrawal. 1996.
Mining sequential patterns: Generalizations and per-
formance improvements. In Proceedings of the 5th
International Conference on Extending Database
Technology: Advances in Database Technology,
EDBT ’96, pages 3–17, London, UK, UK. Springer-
Verlag.

Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opin-
ion analysis. In Computational Models of Argument
- Proceedings of COMMA 2012, Vienna, Austria,
September 10-12, 2012, pages 23–34.

Douglas Walton. 2009. Argumentation theory: A very
short introduction. In Guillermo Simari and Iyad
Rahwan, editors, Argumentation in Artificial Intel-
ligence, pages 1–22. Springer US.

Simon Wells. 2014. Argument mining: Was ist
das? In Proceedings of the 14th International Work-
shop on Computational Models of Natural Argu-
ment, CMNA14.

Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010. Automatically generating annotator ratio-
nales to improve sentiment classification. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
ACLShort ’10, pages 336–341, Stroudsburg, PA,
USA. Association for Computational Linguistics.

450


