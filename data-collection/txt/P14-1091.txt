



















































Knowledge-Based Question Answering as Machine Translation


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967â€“976,
Baltimore, Maryland, USA, June 23-25 2014. cÂ©2014 Association for Computational Linguistics

Knowledge-Based Question Answering as Machine Translation

Junwei Baoâ€  âˆ—, Nan Duanâ€¡ , Ming Zhouâ€¡ , Tiejun Zhaoâ€ 
â€ Harbin Institute of Technology

â€¡Microsoft Research
baojunwei001@gmail.com

{nanduan, mingzhou}@microsoft.com
tjzhao@hit.edu.cn

Abstract

A typical knowledge-based question an-
swering (KB-QA) system faces two chal-
lenges: one is to transform natural lan-
guage questions into their meaning repre-
sentations (MRs); the other is to retrieve
answers from knowledge bases (KBs) us-
ing generated MRs. Unlike previous meth-
ods which treat them in a cascaded man-
ner, we present a translation-based ap-
proach to solve these two tasks in one u-
nified framework. We translate questions
to answers based on CYK parsing. An-
swers as translations of the span covered
by each CYK cell are obtained by a ques-
tion translation method, which first gener-
ates formal triple queries as MRs for the
span based on question patterns and re-
lation expressions, and then retrieves an-
swers from a given KB based on triple
queries generated. A linear model is de-
fined over derivations, and minimum er-
ror rate training is used to tune feature
weights based on a set of question-answer
pairs. Compared to a KB-QA system us-
ing a state-of-the-art semantic parser, our
method achieves better results.

1 Introduction

Knowledge-based question answering (KB-QA)
computes answers to natural language (NL) ques-
tions based on existing knowledge bases (KBs).
Most previous systems tackle this task in a cas-
caded manner: First, the input question is trans-
formed into its meaning representation (MR) by
an independent semantic parser (Zettlemoyer and
Collins, 2005; Mooney, 2007; Artzi and Zettle-
moyer, 2011; Liang et al., 2011; Cai and Yates,

âˆ—This work was finished while the author was visiting Mi-
crosoft Research Asia.

2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski
et al., 2013; Berant et al., 2013); Then, the answer-
s are retrieved from existing KBs using generated
MRs as queries.

Unlike existing KB-QA systems which treat se-
mantic parsing and answer retrieval as two cas-
caded tasks, this paper presents a unified frame-
work that can integrate semantic parsing into the
question answering procedure directly. Borrow-
ing ideas from machine translation (MT), we treat
the QA task as a translation procedure. Like MT,
CYK parsing is used to parse each input question,
and answers of the span covered by each CYK cel-
l are considered the translations of that cell; un-
like MT, which uses offline-generated translation
tables to translate source phrases into target trans-
lations, a semantic parsing-based question trans-
lation method is used to translate each span into
its answers on-the-fly, based on question patterns
and relation expressions. The final answers can be
obtained from the root cell. Derivations generated
during such a translation procedure are modeled
by a linear model, and minimum error rate train-
ing (MERT) (Och, 2003) is used to tune feature
weights based on a set of question-answer pairs.

Figure 1 shows an example: the question direc-
tor of movie starred by Tom Hanks is translated to
one of its answers Robert Zemeckis by three main
steps: (i) translate director of to director of ; (ii)
translate movie starred by Tom Hanks to one of it-
s answers Forrest Gump; (iii) translate director of
Forrest Gump to a final answer Robert Zemeckis.
Note that the updated question covered by Cell[0,
6] is obtained by combining the answers to ques-
tion spans covered by Cell[0, 1] and Cell[2, 6].

The contributions of this work are two-fold: (1)
We propose a translation-based KB-QA method
that integrates semantic parsing and QA in one
unified framework. The benefit of our method
is that we donâ€™t need to explicitly generate com-
plete semantic structures for input questions. Be-

967



Cell[0, 6] 

Cell[2, 6] 

Cell[0, 1] 

director of movie starred by Tom Hanks 

(ii) movie starred by Tom Hanks âŸ¹ Forrest Gump 

(iii) director of Forrest Gump âŸ¹ Robert Zemeckis 

(i) director of âŸ¹ director of 

Figure 1: Translation-based KB-QA example

sides which, answers generated during the transla-
tion procedure help significantly with search space
pruning. (2) We propose a robust method to trans-
form single-relation questions into formal triple
queries as their MRs, which trades off between
transformation accuracy and recall using question
patterns and relation expressions respectively.

2 Translation-Based KB-QA

2.1 Overview
Formally, given a knowledge base KB and an N-
L question Q, our KB-QA method generates a set
of formal triples-answer pairs {ã€ˆD,Aã€‰} as deriva-
tions, which are scored and ranked by the distribu-
tion P (ã€ˆD,Aã€‰|KB,Q) defined as follows:

exp{âˆ‘Mi=1 Î»i Â· hi(ã€ˆD,Aã€‰,KB,Q)}âˆ‘
ã€ˆDâ€² ,Aâ€² ã€‰âˆˆH(Q) exp{

âˆ‘M
i=1 Î»i Â· hi(ã€ˆDâ€² ,Aâ€²ã€‰,KB,Q)}

â€¢ KB denotes a knowledge base1 that stores a
set of assertions. Each assertion t âˆˆ KB is in
the form of {eIDsbj , p, eIDobj}, where p denotes
a predicate, eIDsbj and e

ID
obj denote the subject

and object entities of t, with unique IDs2.

â€¢ H(Q) denotes the search space {ã€ˆD,Aã€‰}. D
is composed of a set of ordered formal triples
{t1, ..., tn}. Each triple t = {esbj , p, eobj}ji âˆˆ
D denotes an assertion in KB, where i and
j denotes the beginning and end indexes of
the question span from which t is trans-
formed. The order of triples in D denotes
the order of translation steps from Q to A.
E.g., ã€ˆdirector of, Null, director of ã€‰10, ã€ˆTom

1We use a large scale knowledge base in this paper, which
contains 2.3B entities, 5.5K predicates, and 18B assertions. A
16-machine cluster is used to host and serve the whole data.

2Each KB entity has a unique ID. For the sake of conve-
nience, we omit the ID information in the rest of the paper.

Hanks, Film.Actor.Film, Forrest Gumpã€‰62 and
ã€ˆForrest Gump, Film.Film.Director, Robert
Zemeckisã€‰60 are three ordered formal triples
corresponding to the three translation steps in
Figure 1. We define the task of transforming
question spans into formal triples as question
translation. A denotes one final answer ofQ.

â€¢ hi(Â·) denotes the ith feature function.

â€¢ Î»i denotes the feature weight of hi(Â·).

According to the above description, our KB-
QA method can be decomposed into four tasks as:
(1) search space generation for H(Q); (2) ques-
tion translation for transforming question spans in-
to their corresponding formal triples; (3) feature
design for hi(Â·); and (4) feature weight tuning for
{Î»i}. We present details of these four tasks in the
following subsections one-by-one.

2.2 Search Space Generation

We first present our translation-based KB-QA
method in Algorithm 1, which is used to generate
H(Q) for each input NL question Q.

Algorithm 1: Translation-based KB-QA
1 for l = 1 to |Q| do
2 for all i, j s.t. j âˆ’ i = l do
3 H(Qji ) = âˆ…;
4 T = QTrans(Qji ,KB);
5 foreach formal triple t âˆˆ T do
6 create a new derivation d;
7 d.A = t.eobj ;
8 d.D = {t};
9 update the model score of d;

10 insert d toH(Qji );
11 end
12 end
13 end
14 for l = 1 to |Q| do
15 for all i, j s.t. j âˆ’ i = l do
16 for all m s.t. i â‰¤ m < j do
17 for dl âˆˆ H(Qmi ) and dr âˆˆ H(Qjm+1) do
18 Qupdate = dl.A+ dr.A;
19 T = QTrans(Qupdate,KB);
20 foreach formal triple t âˆˆ T do
21 create a new derivation d;
22 d.A = t.eobj ;
23 d.D = dl.D

â‹ƒ
dr.Dâ‹ƒ{t};

24 update the model score of d;
25 insert d toH(Qji );
26 end
27 end
28 end
29 end
30 end
31 returnH(Q).

968



The first half (from Line 1 to Line 13) gen-
erates a formal triple set T for each unary span
Qji âˆˆ Q, using the question translation method
QTrans(Qji ,KB) (Line 4), which takesQji as the
input. Each triple t âˆˆ T returned is in the form of
{esbj , p, eobj}, where esbjâ€™s mention occurs inQji ,
p is a predicate that denotes the meaning expressed
by the context of esbj in Qji , eobj is an answer of
Qji based on esbj , p and KB. We describe the im-
plementation detail of QTrans(Â·) in Section 2.3.

The second half (from Line 14 to Line 31) first
updates the content of each bigger spanQji by con-
catenating the answers to its any two consecutive
smaller spans covered by Qji (Line 18). Then,
QTrans(Qji ,KB) is called to generate triples for
the updated span (Line 19). The above operations
are equivalent to answering a simplified question,
which is obtained by replacing the answerable
spans in the original question with their corre-
sponding answers. The search spaceH(Q) for the
entire question Q is returned at last (Line 31).

2.3 Question Translation

The purpose of question translation is to translate
a span Q to a set of formal triples T . Each triple
t âˆˆ T is in the form of {esbj , p, eobj}, where esbjâ€™s
mention3 occurs inQ, p is a predicate that denotes
the meaning expressed by the context of esbj in
Q, eobj is an answer to Q retrieved from KB us-
ing a triple query q = {esbj , p, ?}. Note that if
no predicate p or answer eobj can be generated,
{Q, Null,Q} will be returned as a special triple,
which sets eobj to be Q itself, and p to be Null.
This makes sure the un-answerable spans can be
passed on to the higher-level operations.

Question translation assumes each span Q is a
single-relation question (Fader et al., 2013). Such
assumption simplifies the efforts of semantic pars-
ing to the minimum question units, while leaving
the capability of handling multiple-relation ques-
tions (Figure 1 gives one such example) to the out-
er CYK-parsing based translation procedure. Two
question translation methods are presented in the
rest of this subsection, which are based on ques-
tion patterns and relation expressions respectively.

2.3.1 Question Pattern-based Translation
A question pattern QP includes a pattern string
QPpattern, which is composed of words and a slot

3For simplicity, a cleaned entity dictionary dumped from
the entire KB is used to detect entity mentions inQ.

Algorithm 2:QP-based Question Translation
1 T = âˆ…;
2 foreach entity mention eQ âˆˆ Q do
3 Qpattern = replace eQ inQ with [Slot];
4 foreach question patternQP do
5 ifQpattern ==QPpattern then
6 E = Disambiguate(eQ,QPpredicate);
7 foreach e âˆˆ E do
8 create a new triple query q;
9 q = {e,QPpredicate, ?};

10 {Ai} = AnswerRetrieve(q,KB);
11 foreach A âˆˆ {Ai} do
12 create a new formal triple t;
13 t = {q.esbj , q.p,A};
14 t.score = 1.0;
15 insert t to T ;
16 end
17 end
18 end
19 end
20 end
21 return T .

symbol [Slot], and a KB predicate QPpredicate,
which denotes the meaning expressed by the con-
text words in QPpattern.

Algorithm 2 shows how to generate formal
triples for a span Q based on question pattern-
s (QP-based question translation). For each en-
tity mention eQ âˆˆ Q, we replace it with [Slot]
and obtain a pattern string Qpattern (Line 3). If
Qpattern can match one QPpattern, then we con-
struct a triple query q (Line 9) using QPpredicate
as its predicate and one of the KB entities re-
turned by Disambiguate(eQ,QPpredicate) as it-
s subject entity (Line 6). Here, the objective of
Disambiguate(eQ,QPpredicate) is to output a set
of disambiguated KB entities E in KB. The name
of each entity returned equals the input entity
mention eQ and occurs in some assertions where
QPpredicate are the predicates. The underlying
idea is to use the context (predicate) information to
help entity disambiguation. The answers of q are
returned by AnswerRetrieve(q,KB) based on q
and KB (Line 10), each of which is used to con-
struct a formal triple and added to T for Q (from
Line 11 to Line 16). Figure 2 gives an example.

Question patterns are collected as follows: First,
5W queries, which begin with What, Where, Who,
When, or Which, are selected from a large scale
query log of a commercial search engine; Then, a
cleaned entity dictionary is used to annotate each
query by replacing all entity mentions it contains
with the symbol [Slot]. Only high-frequent query
patterns which contain one [Slot] are maintained;

969



ğ“                      : who is the director of Forrest Gump 

ğ“ ğ“Ÿğ’‘ğ’‚ğ’•ğ’•ğ’†ğ’“ğ’     : who is the director of [Slot] 

ğ“ ğ“Ÿğ’‘ğ’“ğ’†ğ’…ğ’Šğ’„ğ’‚ğ’•ğ’† : Film.Film.Director 

ğ’’                     : <Forrest Gump, Film.Film.Director, ?> 

ğ’•                      : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 

KB 

Figure 2: QP-based question translation example

Lastly, annotators try to manually label the most-
frequent 50,000 query patterns with their corre-
sponding predicates, and 4,764 question patterns
with single labeled predicates are obtained.

From experiments (Table 3 in Section 4.3) we
can see that, question pattern based question trans-
lation can achieve high end-to-end accuracy. But
as human efforts are needed in the mining proce-
dure, this method cannot be extended to large scale
very easily. Besides, different users often type the
questions with the same meaning in different NL
expressions. For example, although the question
Forrest Gump was directed by which moviemaker
means the same as the question Q in Figure 2, no
question pattern can cover it. We need to find an
alternative way to alleviate such coverage issue.

2.3.2 Relation Expression-based Translation
Aiming to alleviate the coverage issue occurring in
QP-based method, an alternative relation expres-
sion (RE) -based method is proposed, and will be
used when the QP-based method fails.

We define REp as a relation expression set for
a given KB predicate p âˆˆ KB. Each relation ex-
pressionRE âˆˆ REp includes an expression string
REexpression, which must contain at least one con-
tent word, and a weight REweight, which denotes
the confidence thatREexpression can represent pâ€™s
meaning in NL. For example, is the director of
is one relation expression string for the predicate
Film.Film.Director, which means it is usually used
to express this relation (predicate) in NL.

Algorithm 3 shows how to generate triples for
a question Q based on relation expressions. For
each possible entity mention eQ âˆˆ Q and a K-
B predicate p âˆˆ KB that is related to a KB enti-
ty e whose name equals eQ, Sim(eQ,Q,REp) is
computed (Line 5) based on the similarity between
question context and REp, which measures how
likely Q can be transformed into a triple query

Algorithm 3:RE-based Question Translation
1 T = âˆ…;
2 foreach entity mention eQ âˆˆ Q do
3 foreach e âˆˆ KB s.t. e.name==eQ do
4 foreach predicate p âˆˆ KB related to e do
5 score = Sim(eQ,Q,REp);
6 if score > 0 then
7 create a new triple query q;
8 q = {e, p, ?};
9 {Ai} = AnswerRetrieve(q,KB);

10 foreach A âˆˆ {Ai} do
11 create a new formal triple t;
12 t = {q.esbj , q.p,A};
13 t.score = score;
14 insert t to T ;
15 end
16 end
17 end
18 end
19 end
20 sort T based on the score of each t âˆˆ T ;
21 return T .

q = {e, p, ?}. If this score is larger than 0, which
means there are overlaps betweenQâ€™s context and
REp, then q will be used as the triple query of Q,
and a set of formal triples will be generated based
on q andKB (from Line 7 to Line 15). The compu-
tation of Sim(eQ,Q,REp) is defined as follows:âˆ‘
n

1
|Q| âˆ’ n+ 1 Â· {

âˆ‘
Ï‰nâˆˆQ,Ï‰n

â‹‚
eQ=Ï†

P (Ï‰n|REp)}

where n is the n-gram order which ranges from 1
to 5, Ï‰n is an n-gram occurring inQ without over-
lapping with eQ and containing at least one con-
tent word, P (Ï‰n|REp) is the posterior probability
which is computed by:

P (Ï‰n|REp) = Count(Ï‰n,REp)âˆ‘
Ï‰â€²nâˆˆREp Count(Ï‰

â€²
n,REp)

Count(Ï‰,REp) denotes the weighted sum of
times that Ï‰ occurs inREp:

Count(Ï‰,REp) =
âˆ‘

REâˆˆREp
{#Ï‰(RE) Â· REweight}

where #Ï‰(RE) denotes the number of times that
Ï‰ occurs inREexpression, andREweight is decided
by the relation expression extraction component.

Figure 3 gives an example, where n-grams with
rectangles are the ones that occur in bothQâ€™s con-
text and the relation expression set of a given pred-
icate p = Film.F ilm.Director. Unlike the QP-
based method which needs a perfect match, the

970



ğ“                                  : Forrest Gump was directed by which moviemaker 

ğ“¡ğ“”ğ‘­ğ’Šğ’ğ’.ğ‘­ğ’Šğ’ğ’.ğ‘«ğ’Šğ’“ğ’†ğ’„ğ’•ğ’ğ’“ : is directed by 

was directed and written by 

is the moviemaker of 

was famous as the director of 

â€¦ 

ğ’’                                   : <Forrest Gump, Film.Film.Director, ?> 

ğ’•                                    : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 

KB 

Figure 3: RE-based question translation example

RE-based method allows fuzzy matching between
Q andREp, and records this (Line 13) in generat-
ed triples, which is used as features later.

Relation expressions are mined as follows: Giv-
en a set of KB assertions with an identical predi-
cate p, we first extract all sentences from English
Wiki pages4, each of which contains at least one
pair of entities occurring in one assertion. Then,
we extract the shortest path between paired entities
in the dependency tree of each sentence as an RE
candidate for the given predicate. The intuition is
that any sentence containing such entity pairs oc-
cur in an assertion is likely to express the predi-
cate of that assertion in some way. Last, all rela-
tion expressions extracted are filtered by heuristic
rules, i.e., the frequency must be larger than 4, the
length must be shorter than 10, and then weighted
by the pattern scoring methods proposed in (Ger-
ber and Ngomo, 2011; Gerber and Ngomo, 2012).
For each predicate, we only keep the relation ex-
pressions whose pattern scores are larger than a
pre-defined threshold. Figure 4 gives one relation
expression extraction example. The statistics and
overall quality of the relation expressions are list-
ed in Section 4.1.

{Forrest Gump, Robert Zemeckis} 
{Titanic, James Cameron} 
{The Dark Knight Rises, Christopher Nolan} 

Paired entity of a 
KB predicate  

ğ‘=Film.Film.Director 

Passage retrieval  
from Wiki pages 

Relation expression 
weighting 

Robert Zemeckis is the director of Forrest Gump 
James Cameron is the moviemaker of Titanic 
The Dark Knight Rises is directed by Christopher Nolan 

is the director of           ||| 0.25 
is the moviemaker of   ||| 0.23 
is directed by                 ||| 0.20 

Figure 4: RE extraction example

4http://en.wikipedia.org/wiki/Wikipedia:Database download

2.3.3 Question Decomposition
Sometimes, a question may provide multiple con-
straints to its answers. movie starred by Tom Han-
ks in 1994 is one such question. All the films as
the answers of this question should satisfy the fol-
lowing two constraints: (1) starred by Tom Hanks;
and (2) released in 1994. It is easy to see that such
questions cannot be translated to single triples.

We propose a dependency tree-based method to
handle such multiple-constraint questions by (i)
decomposing the original question into a set of
sub-questions using syntax-based patterns; and (ii)
intersecting the answers of all sub-questions as the
final answers of the original question. Note, ques-
tion decomposition only operates on the original
question and question spans covered by complete
dependency subtrees. Four syntax-based patterns
(Figure 5) are used for question decomposition. If
a question matches any one of these patterns, then
sub-questions are generated by collecting the path-
s between n0 and each ni(i > 0) in the pattern,
where each n denotes a complete subtree with a
noun, number, or question word as its root node,
the symbol âˆ— above prepâˆ— denotes this preposition
can be skipped in matching. For the question men-
tioned at the beginning, its two sub-questions gen-
erated are movie starred by Tom Hanks and movie
starred in 1994, as its dependency form matches
pattern (a). Similar ideas are used in IBM Wat-
son (Kalyanpur et al., 2012) as well.

ğ‘£ğ‘’ğ‘Ÿğ‘ 

ğ‘›0 

ğ‘ğ‘Ÿğ‘’ğ‘âˆ— 

ğ‘›1 

ğ‘ğ‘Ÿğ‘’ğ‘âˆ— 

ğ‘›ğ‘˜ 

â€¦ 

â€¦ 

ğ‘£ğ‘’ğ‘Ÿğ‘ 

ğ‘›0 ğ‘›1 ğ‘ğ‘Ÿğ‘’ğ‘ 

(a) 

ğ‘›2 

ğ‘£ğ‘’ğ‘Ÿğ‘ 

ğ‘›0 

ğ‘›1 

(c) 

and ğ‘›2 

ğ‘ğ‘Ÿğ‘’ğ‘âˆ— 

ğ‘£ğ‘’ğ‘Ÿğ‘ 

ğ‘›0 

ğ‘›1 

(d) 

ğ‘ğ‘Ÿğ‘’ğ‘âˆ— and ğ‘£ğ‘’ğ‘Ÿğ‘ 

ğ‘›2 

ğ‘ğ‘Ÿğ‘’ğ‘âˆ— 

(b) 

Figure 5: Four syntax-based patterns for question
decomposition

As dependency parsing is not perfect, we gen-
erate single triples for such questions without con-
sidering constraints as well, and add them to the
search space for competition. hsyntax constraint(Â·)

971



is used to boost triples that are converted from sub-
questions generated by question decomposition.
The more constraints an answer satisfies, the bet-
ter. Obviously, current patterns used canâ€™t cover
all cases but most-common ones. We leave a more
general pattern mining method for future work.

2.4 Feature Design
The objective of our KB-QA system is to seek the
derivation ã€ˆDÌ‚, AÌ‚ã€‰ that maximizes the probability
P (ã€ˆD,Aã€‰|KB,Q) described in Section 2.1 as:
ã€ˆDÌ‚, AÌ‚ã€‰ = argmax

ã€ˆD,Aã€‰âˆˆH(Q)
P (ã€ˆD,Aã€‰|KB,Q)

= argmax
ã€ˆD,Aã€‰âˆˆH(Q)

Mâˆ‘
i=1

Î»i Â· hi(ã€ˆD,Aã€‰,KB,Q)

We now introduce the feature sets {hi(Â·)} that are
used in the above linear model:

â€¢ hquestion word(Â·), which counts the number of
original question words occurring inA. It pe-
nalizes those partially answered questions.

â€¢ hspan(Â·), which counts the number of spans
in Q that are converted to formal triples. It
controls the granularity of the spans used in
question translation.

â€¢ hsyntax subtree(Â·), which counts the number
of spans inQ that are (1) converted to formal
triples, whose predicates are not Null, and
(2) covered by complete dependency subtrees
at the same time. The underlying intuition
is that, dependency subtrees of Q should be
treated as units for question translation.

â€¢ hsyntax constraint(Â·), which counts the num-
ber of triples in D that are converted from
sub-questions generated by the question de-
composition component.

â€¢ htriple(Â·), which counts the number of triples
in D, whose predicates are not Null.
â€¢ htripleweight(Â·), which sums the scores of all

triples {ti} in D as
âˆ‘

tiâˆˆD ti.score.

â€¢ hQPcount(Â·), which counts the number of
triples in D that are generated by QP-based
question translation method.

â€¢ hREcount(Â·), which counts the number of
triples in D that are generated by RE-based
question translation method.

â€¢ hstaticranksbj (Â·), which sums the static rank
scores of all subject entities in Dâ€™s triple set
as

âˆ‘
tiâˆˆD ti.esbj .static rank.

â€¢ hstaticrankobj (Â·), which sums the static rank
scores of all object entities inDâ€™s triple set asâˆ‘

tiâˆˆD ti.eobj .static rank.

â€¢ hconfidenceobj (Â·), which sums the confidence
scores of all object entities inDâ€™s triple set asâˆ‘

tâˆˆD t.eobj .confidence.

For each assertion {esbj , p, eobj} stored in KB,
esbj .static rank and eobj .static rank denote the
static rank scores5 for esbj and eobj respectively;
eobj .confidence rank represents the probability
p(eobj |esbj , p). These three scores are used as fea-
tures to rank answers generated in QA procedure.

2.5 Feature Weight Tuning
Given a set of question-answer pairs {Qi,Arefi }
as the development (dev) set, we use the minimum
error rate training (MERT) (Och, 2003) algorithm
to tune the feature weights Î»Mi in our proposed
model. The training criterion is to seek the feature
weights that can minimize the accumulated errors
of the top-1 answer of questions in the dev set:

Î»Ì‚M1 = argmin
Î»M1

Nâˆ‘
i=1

Err(Arefi , AÌ‚i;Î»M1 )

N is the number of questions in the dev set, Arefi
is the correct answers as references of the ith ques-
tion in the dev set, AÌ‚i is the top-1 answer candi-
date of the ith question in the dev set based on
feature weights Î»M1 , Err(Â·) is the error function
which is defined as:

Err(Arefi , AÌ‚i;Î»M1 ) = 1âˆ’ Î´(Arefi , AÌ‚i)

where Î´(Arefi , AÌ‚i) is an indicator function which
equals 1 when AÌ‚i is included in the reference set
Arefi , and 0 otherwise.
3 Comparison with Previous Work

Our work intersects with two research directions:
semantic parsing and question answering.

Some previous works on semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Wong and Mooney,

5The static rank score of an entity represents a general
indicator of the overall quality of that entity.

972



2007; Kwiatkowski et al., 2010; Kwiatkowski
et al., 2011) require manually annotated logical
forms as supervision, and are hard to extend result-
ing parsers from limited domains, such as GEO,
JOBS and ATIS, to open domains. Recent work-
s (Clarke and Lapata, 2010; Liang et al., 2013)
have alleviated such issues using question-answer
pairs as weak supervision, but still with the short-
coming of using limited lexical triggers to link NL
phrases to predicates. Poon (2013) has proposed
an unsupervised method by adopting grounded-
learning to leverage the database for indirect su-
pervision. But transformation from NL questions
to MRs heavily depends on dependency parsing
results. Besides, the KB used (ATIS) is limited as
well. Kwiatkowski et al. (2013) use Wiktionary
and a limited manual lexicon to map POS tags to
a set of predefined CCG lexical categories, which
aims to reduce the need for learning lexicon from
training data. But it still needs human efforts to de-
fine lexical categories, which usually can not cover
all the semantic phenomena.

Berant et al. (2013) have not only enlarged the
KB used for Freebase (Google, 2013), but also
used a bigger lexicon trigger set extracted by the
open IE method (Lin et al., 2012) for NL phrases
to predicates linking. In comparison, our method
has further advantages: (1) Question answering
and semantic parsing are performed in an join-
t way under a unified framework; (2) A robust
method is proposed to map NL questions to their
formal triple queries, which trades off the mapping
quality by using question patterns and relation ex-
pressions in a cascaded way; and (3) We use do-
main independent feature set which allowing us to
use a relatively small number of question-answer
pairs to tune model parameters.

Fader et al. (2013) map questions to formal
(triple) queries over a large scale, open-domain
database of facts extracted from a raw corpus by
ReVerb (Fader et al., 2011). Compared to their
work, our method gains an improvement in two
aspects: (1) Instead of using facts extracted us-
ing the open IE method, we leverage a large scale,
high-quality knowledge base; (2) We can han-
dle multiple-relation questions, instead of single-
relation queries only, based on our translation
based KB-QA framework.

Espana-Bonet and Comas (2012) have proposed
an MT-based method for factoid QA. But MT in
there work means to translate questions into n-

best translations, which are used for finding simi-
lar sentences in the document collection that prob-
ably contain answers. Echihabi and Marcu (2003)
have developed a noisy-channel model for QA,
which explains how a sentence containing an an-
swer to a given question can be rewritten into that
question through a sequence of stochastic opera-
tions. Compared to the above two MT-motivated
QA work, our method uses MT methodology to
translate questions to answers directly.

4 Experiment

4.1 Data Sets
Following Berant et al. (2013), we use the same
subset of WEBQUESTIONS (3,778 questions) as
the development set (Dev) for weight tuning in
MERT, and use the other part of WEBQUES-
TIONS (2,032 questions) as the test set (Test). Ta-
ble 1 shows the statistics of this data set.

Data Set # Questions # Words
WEBQUESTIONS 5,810 6.7

Table 1: Statistics of evaluation set. # Questions is
the number of questions in a data set, # Words is
the averaged word count of a question.

Table 2 shows the statistics of question patterns
and relation expressions used in our KB-QA sys-
tem. As all question patterns are collected with hu-
man involvement as we discussed in Section 2.3.1,
the quality is very high (98%). We also sample
1,000 instances from the whole relation expression
set and manually label their quality. The accuracy
is around 89%. These two resources can cover 566
head predicates in our KB.

# Entries Accuracy
Question Patterns 4,764 98%
Relation Expressions 133,445 89%

Table 2: Statistics of question patterns and relation
expressions.

4.2 KB-QA Systems
Since Berant et al. (2013) is one of the latest
work which has reported QA results based on a
large scale, general domain knowledge base (Free-
base), we consider their evaluation result on WE-
BQUESTIONS as our baseline.

Our KB-QA system generates the k-best deriva-
tions for each question span, where k is set to 20.

973



The answers with the highest model scores are
considered the best answers for evaluation. For
evaluation, we follow Berant et al. (2013) to al-
low partial credit and score an answer using the F1
measure, comparing the predicted set of entities to
the annotated set of entities.

One difference between these two systems is the
KB used. Since Freebase is completely contained
by our KB, we disallow all entities which are not
included by Freebase. By doing so, our KB pro-
vides the same knowledge as Freebase does, which
means we do not gain any extra advantage by us-
ing a larger KB. But we still allow ourselves to
use the static rank scores and confidence scores of
entities as features, as we described in Section 2.4.

4.3 Evaluation Results

We first show the overall evaluation results of our
KB-QA system and compare them with baselineâ€™s
results on Dev and Test. Note that we do not re-
implement the baseline system, but just list their
evaluation numbers reported in the paper. Com-
parison results are listed in Table 3.

Dev (Accuracy) Test (Accuracy)
Baseline 32.9% 31.4%
Our Method 42.5% (+9.6%) 37.5% (+6.1%)

Table 3: Accuracy on evaluation sets. Accuracy is
defined as the number of correctly answered ques-
tions divided by the total number of questions.

Table 3 shows our KB-QA method outperforms
baseline on both Dev and Test. We think the po-
tential reasons of this improvement include:

â€¢ Different methods are used to map NL phras-
es to KB predicates. Berant et al. (2013)
have used a lexicon extracted from a subset
of ReVerb triples (Lin et al., 2012), which
is similar to the relation expression set used
in question translation. But as our relation
expressions are extracted by an in-house ex-
tractor, we can record their extraction-related
statistics as extra information, and use them
as features to measure the mapping quality.
Besides, as a portion of entities in our KB
are extracted from Wiki, we know the one-
to-one correspondence between such entities
and Wiki pages, and use this information in
relation expression extraction for entity dis-
ambiguation. A lower disambiguation error
rate results in better relation expressions.

â€¢ Question patterns are used to map NL context
to KB predicates. Context can be either con-
tinuous or discontinues phrases. Although
the size of this set is limited, they can actually
cover head questions/queries6 very well. The
underlying intuition of using patterns is that
those high-frequent questions/queries should
and can be treated and solved in the QA task,
by involving human effort at a relative small
price but with very impressive accuracy.

In order to figure out the impacts of question
patterns and relation expressions, another exper-
iment (Table 4) is designed to evaluate their in-
dependent influences, where QPonly and REonly
denote the results of KB-QA systems which only
allow question patterns and relation expressions in
question translation respectively.

Settings Test (Accuracy) Test (Precision)
QPonly 11.8% 97.5%
REonly 32.5% 73.2%

Table 4: Impacts of question patterns and relation
expressions. Precision is defined as the num-
ber of correctly answered questions divided by the
number of questions with non-empty answers gen-
erated by our KB-QA system.

From Table 4 we can see that the accuracy of
REonly on Test (32.5%) is slightly better than
baselineâ€™s result (31.4%). We think this improve-
ment comes from two aspects: (1) The quality of
the relation expressions is better than the quality
of the lexicon entries used in the baseline; and
(2) We use the extraction-related statistics of re-
lation expressions as features, which brings more
information to measure the confidence of map-
ping between NL phrases and KB predicates, and
makes the model to be more flexible. Meanwhile,
QPonly perform worse (11.8%) than REonly, due
to coverage issue. But by comparing the precision-
s of these two settings, we find QPonly (97.5%)
outperforms REonly (73.2%) significantly, due to
its high quality. This means how to extract high-
quality question patterns is worth to be studied for
the question answering task.

As the performance of our KB-QA system re-
lies heavily on the k-best beam approximation, we
evaluate the impact of the beam size and list the
comparison results in Figure 6. We can see that as

6Head questions/queries mean the questions/queries with
high frequency and clear patterns.

974



we increase k incrementally, the accuracy increase
at the same time. However, a larger k (e.g. 200)
cannot bring significant improvements comparing
to a smaller one (e.g., 20), but using a large k has
a tremendous impact on system efficiency. So we
choose k = 20 as the optimal value in above ex-
periments, which trades off between accuracy and
efficiency.

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

5 20 50 100 200

Accuracy on Test 

Accuracy

Figure 6: Impacts of beam size on accuracy.

Actually, the size of our systemâ€™s search space
is much smaller than the one of the semantic parser
used in the baseline.This is due to the fact that, if
triple queries generated by the question translation
component cannot derive any answer from KB, we
will discard such triple queries directly during the
QA procedure. We can see that using a small k
can achieve better results than baseline, where the
beam size is set to be 200.

4.4 Error Analysis

4.4.1 Entity Detection
Since named entity recognizers trained on Penn
TreeBank usually perform poorly on web queries,
We instead use a simple string-match method to
detect entity mentions in the question using a
cleaned entity dictionary dumped from our KB.
One problem of doing so is the entity detection
issue. For example, in the question who was Es-
therâ€™s husband ?, we cannot detect Esther as an
entity, as it is just part of an entity name. We need
an ad-hoc entity detection component to handle
such issues, especially for a web scenario, where
users often type entity names in their partial or ab-
breviation forms.

4.4.2 Predicate Mapping
Some questions lack sufficient evidences to detec-
t predicates. where is Byron Nelson 2012 ? is an
example. Since each relation expression must con-
tain at least one content word, this question cannot
match any relation expression. Except for Byron

Nelson and 2012, all the others are non-content
words.

Besides, ambiguous entries contained in rela-
tion expression sets of different predicates can
bring mapping errors as well. For the follow-
ing question who did Steve Spurrier play pro
football for? as an example, since the unigram
play exists in both Film.Film.Actor and Ameri-
can Football.Player.Current Team â€™s relation ex-
pression sets, we made a wrong prediction, which
led to wrong answers.

4.4.3 Specific Questions
Sometimes, we cannot give exact answers to
superlative questions like what is the first book
Sherlock Holmes appeared in?. For this example,
we can give all book names where Sherlock
Holmes appeared in, but we cannot rank them
based on their publication date , as we cannot
learn the alignment between the constraint word
first occurred in the question and the predicate
Book.Written Work.Date Of First Publication
from training data automatically. Although we
have followed some work (Poon, 2013; Liang
et al., 2013) to handle such special linguistic
phenomena by defining some specific operators,
it is still hard to cover all unseen cases. We leave
this to future work as an independent topic.

5 Conclusion and Future Work

This paper presents a translation-based KB-QA
method that integrates semantic parsing and QA
in one unified framework. Comparing to the base-
line system using an independent semantic parser
with state-of-the-art performance, we achieve bet-
ter results on a general domain evaluation set.

Several directions can be further explored in the
future: (i) We plan to design a method that can
extract question patterns automatically, using ex-
isting labeled question patterns and KB as weak
supervision. As we discussed in the experiment
part, how to mine high-quality question patterns is
worth further study for the QA task; (ii) We plan
to integrate an ad-hoc NER into our KB-QA sys-
tem to alleviate the entity detection issue; (iii) In
fact, our proposed QA framework can be general-
ized to other intelligence besides knowledge bases
as well. Any method that can generate answers to
questions, such as the Web-based QA approach,
can be integrated into this framework, by using
them in the question translation component.

975



References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-

strapping semantic parsers from conversations. In
EMNLP, pages 421â€“432.

Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-
moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL (Tutorial Abstracts),
page 2.

Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533â€“
1544.

Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL, pages 423â€“433.

James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411â€“441.

Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
ACL.

Cristina Espana-Bonet and Pere R. Comas. 2012. Full
machine translation for factoid question answering.
In EACL, pages 20â€“29.

Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open information
extraction. In EMNLP, pages 1535â€“1545.

Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL, pages 1608â€“1618.

Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In ISWC.

Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012.
Extracting multilingual natural-language patterns
for rdf predicates. In ESWC.

Google. 2013. Freebase. In http://www.freebase.com.

Aditya Kalyanpur, Siddharth Patwardhan, Branimir
Boguraev, Adam Lally, and Jennifer Chu-Carroll.
2012. Fact-based question decomposition in deep-
qa. IBM Journal of Research and Development,
56(3):13.

Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP, pages 1223â€“1233.

Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In EMNLP, pages 1512â€“1523.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP, pages 1545â€“1556.

Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL, pages 590â€“599.

Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389â€“446.

Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In AKBC-WEKEX, pages 84â€“
88.

Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing, pages 311â€“324.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160â€“
167.

Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL, pages 933â€“943.

Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In HLT-NAACL.

Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Vol. 2, pages 1050â€“
1055.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658â€“666.

Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678â€“687.

976


