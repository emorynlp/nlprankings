










































LELIE: A Tool Dedicated to Procedure and Requirement Authoring


Proceedings of the EACL 2012 Workshop on Computational Linguistics and Writing, pages 35–38,
Avignon, France, April 23, 2012. c©2012 Association for Computational Linguistics

LELIE: A Tool Dedicated to Procedure and Requirement Authoring

Flore Barcellini, Corinne Grosse
CNAM, 41 Rue Gay Lussac,

Paris, France,
Flore.Barcellini@cnam.fr

Camille Albert, Patrick Saint-Dizier
IRIT-CNRS, 118 route de Narbonne,

31062 Toulouse cedex France
stdizier@irit.fr

Abstract

This short paper relates the main features of
LELIE, phase 1, which detects errors made
by technical writers when producing pro-
cedures or requirements. This results from
ergonomic observations of technical writers
in various companies.

1 Objectives

The main goal of the LELIE project is to produce
an analysis and a piece of software based on lan-
guage processing and artificial intelligence that
detects and analyses potential risks of different
kinds (first health and ecological, but also social
and economical) in technical documents. We con-
centrate on procedural documents and on require-
ments (Hull et al. 2011) which are, by large, the
main types of technical documents used in compa-
nies.

Given a set of procedures (e.g., production
launch, maintenance) over a certain domain pro-
duced by a company, and possibly given some
domain knowledge (ontology, terminology, lexi-
cal), the goal is to process these procedures and to
annotate them wherever potential risks are identi-
fied. Procedure authors are then invited to revise
these documents. Similarly, requirements, in par-
ticular those related to safety, often exhibit com-
plex structures (e.g., public regulations, to cite the
worse case): several embedded conditions, nega-
tion, pronouns, etc., which make their use difficult,
especially in emergency situations. Indeed, proce-
dures as well as safety requirements are dedicated
to action: little space should be left to personal
interpretations.

Risk analysis and prevention in LELIE is based
on three levels of analysis, each of them potentially
leading to errors made by operators in action:

1. Detection of inappropriate ways of writing:
complex expressions, implicit elements, com-
plex references, scoping difficulties (connec-
tors, conditionals), inappropriate granularity
level, involving lexical, semantic and prag-
matic levels, inappropriate domain style,

2. Detection of domain incoherencies in proce-
dures: detection of unusual ways of realizing
an action (e.g., unusual instrument, equip-
ment, product, unusual value such as temper-
ature, length of treatment, etc.) with respect
to similar actions in other procedures or to
data extracted from technical documents,

3. Confrontation of domain safety requirements
with procedures to check if the required safety
constraints are met.

Most industrial areas have now defined author-
ing recommendations on the way to elaborate,
structure and write procedures of various kinds.
However, our experience with technical writers
shows that those recommendations are not very
strictly followed in most situations. Our objective
is to develop a tool that checks ill-formed struc-
tures with respect to these recommendations and
general style considerations in procedures and re-
quirements when they are written.

In addition, authoring guidelines do not specify
all the aspects of document authoring: our investi-
gations on author practices have indeed identified
a number of recurrent errors which are linguistic
or conceptual which are usually not specified in
authoring guidelines. These errors are basically
identified from the comprehension difficulties en-
countered by technicians in operation using these
documents to realize a task or from technical writ-
ers themselves which are aware of the errors they
should avoid.

35



2 The Situation and our contribution

Risk management and prevention is now a major
issue. It is developed at several levels, in particu-
lar via probabilistic analysis of risks in complex
situations (e.g., oil storage in natural caves). De-
tecting potential risks by analyzing business errors
on written documents is a relatively new approach.
It requires the taking into account of most of the
levels of language: lexical, grammatical and style
and discourse.

Authoring tools for simplified language are not
a new concept; one of the first checkers was de-
veloped at Boeing1, initially for their own simpli-
fyed English and later adapted for the ASD Sim-
plified Technical English Specification2. A more
recent language checking system is Acrolinx IQ by
Acrolinx3. Some technical writing environments
also include language checking functionality, e.g.,
MadPak4. Ament (2002) and Weiss (2000) devel-
oped a number of useful methodological elements
for authoring technical documents and error iden-
tification and correction.

The originality of our approach is as follows.
Authoring recommendations are made flexible and
context-dependent, for example if negation is not
allowed in instructions in general, there are, how-
ever, cases where it cannot be avoided because
the positive counterpart cannot so easily be formu-
lated, e.g., do not dispose of the acid in the sewer.
Similarly, references may be allowed if the refer-
ent is close and non-ambiguous. However, this
requires some knowledge.

Following observations in cognitive ergonomics
in the project, a specific effort is realized concern-
ing the well-formedness (following grammatical
and cognitive standards) of discourse structures
and their regularity over entire documents (e.g.,
instruction or enumerations all written in the same
way).

The production of procedures includes some
controls on contents, in particular action verb argu-
ments, as indicated in the second objective above,
via the Arias domain knowledge base, e.g., avoid-
ing typos or confusions among syntactically and
semantically well-identified entities such as instru-
ments, products, equipments, values, etc.

1http://www.boeing.com/phantom/sechecker/
2ASD-STE100, http://www.asd-ste100.org/
3http://www.acrolinx.com/
4http://www.madcapsoftware.com/products/

madpak/

There exists no real requirement analysis sys-
tem based on language that can check the qual-
ity and the consistency of large sets of authoring
recommendations. The main products are IBM
Doors and Doors Trek5, Objecteering6, and Re-
qtify7, which are essentially textual databases with
advanced visual and design interfaces, query facil-
ities for retrieving specific requirements, and some
traceability functions carried out via predefined
attributes. These three products also include a for-
mal language (essentially based on attribute-value
pairs) that is used to check some simple forms of
coherence among large sets of requirements.

The authoring tool includes facilities for French-
speaking authors who need to write in English,
supporting typical errors they make via ‘language
transfer’ (Garnier, 2011). We will not address this
point here.

This project, LELIE, is based on the TextCoop
system (Saint-Dizier, 2012), a system dedicated
to language analysis, in particular discourse (in-
cluding the taking into account of long-distance
dependencies). This project also includes the Arias
action knowledge base that stores prototypical ac-
tions in context, and can update them. It also in-
cludes an ASP (Answer Set Programming) solver
8 to check for various forms of incoherence and in-
completeness. The kernel of the system is written
in SWI Prolog, with interfaces in Java. The project
is currently realized for French, an English version
is under development.

The system is based on the following principles.
First, the system is parameterized: the technical
writer may choose the error types he wants to be
checked, and the severity level for each error type
when there are several such levels (e.g., there are
several levels of severity associated with fuzzy
terms which indeed show several levels of fuzzi-
ness). Second, the system simply tags elements
identified as errors, the correction is left to the
author. However, some help or guidelines are of-
fered. For example, guidelines for reformulating
a negative sentence into a positive one are pro-
posed. Third, the way errors are displayed can be
customized to the writer’s habits.

We present below a kernel system that deals

5http://www.ibm.com/software/awdtools/
doors/

6http://www.objecteering.com/
7http://www.geensoft.com/
8For an overview of ASP see Brewka et al. (2011).

36



with the most frequent and common errors made
by technical writers independently of the technical
domain. This kernel needs an in-depth customiza-
tion to the domain at stake. For example, the verbs
used or the terminological preferences must be im-
plemented for each industrial context. Our system
offers the control operations, but these need to be
associated with domain data.

Finally, to avoid the variability of document for-
mats, the system input is an abstract document
with a minimal number of XML tags as required
by the error detection rules. Managing and trans-
forming the original text formats into this abstract
format is not dealt with here.

3 Categorizing language and conceptual
errors found in technical documents

In spite of several levels of human proofreading
and validation, it turns out that texts still contain
a large number of situations where recommenda-
tions are not followed. Reasons are analyzed in e.g.
e.g., (Béguin, 2003), (Mollo et al., 2004, 2008).

Via ergonomics analysis of the activity of techni-
cal writers, we have identified several layers of re-
current error types, which are not in general treated
by standard text editors such as Word or Visio, the
favorite editors for procedures.

Here is a list of categories of errors we have
identified. Some errors are relevant for a whole
document, whereas others must only be detected in
precise constructions (e.g., in instructions, which
are the most constrained constructions):

• General layout of the document: size of sen-
tences, paragraphs, and of the various forms
of enumerations, homogeneity of typography,
structure of titles, presence of expected struc-
tures such as summary, but also text global or-
ganization following style recommendations
(expressed in TextCoop via a grammar), etc.

• Morphology: in general passive constructions
and future tenses must be avoided in instruc-
tions.

• Lexical aspects: fuzzy terms, inappropriate
terms such as deverbals, light verb construc-
tions or modals in instructions, detection of
terms which cannot be associated, in partic-
ular via conjunctions. This requires typing
lexical data.

• Grammatical complexity: the system checks
for various forms of negation, referential
forms, sequences of conditional expressions,
long sequences of coordination, complex
noun complements, and relative clause em-
beddings. All these constructions often make
documents difficult to understand.

• Uniformity of style over a set of instructions,
over titles and various lists of equipments,
uniformity of expression of safety warnings
and advice.

• Correct position in the document of specific
fields: safety precautions, prerequisites, etc.

• Structure completeness, in particular com-
pleteness of case enumerations with respect
to to known data, completeness of equipment
enumerations, via the Arias action base.

• Regular form of requirements: context of
application properly written (e.g., via con-
ditions) followed by a set of instructions.

• Incorrect domain value, as detected by Arias.

When a text is analyzed, the system annotates
the original document (which is in our current
implementation a plain text, a Word or an XML
document): revisions are only made by technical
writers.

Besides tags which must be as explicit as possi-
ble, colors indicate the severity level for the error
considered (the same error, e.g., use of fuzzy term,
can have several severity levels). The most severe
errors must be corrected first. At the moment, we
propose four levels of severity:

ERROR Must be corrected.

AVOID Preferably avoid this usage, think about
an alternative,

CHECK this is not really bad, but it is recom-
mended to make sure this is clear; this is also
used to make sure that argument values are
correct, when a non-standard one is found.

ADVICE Possibly not the best language realiza-
tion, but this is probably a minor problem. It
is not clear whether there are alternatives.

The model, the implementation and the results
are presented in detail in (Barcellini et al., 2012).

37



4 Perspectives

We have developed the first phase of the LELIE
project: detecting authoring errors in technical
documents that may lead to risks. We identified a
number of errors: lexical, business, grammatical,
and stylistic. Errors have been identified from er-
gonomics investigations. The system is now fully
implemented on the TextCoop platform and has
been evaluated on a number of documents. It is
now of much interest to evaluate user’s reactions.

We have implemented the system kernel. The
main challenge ahead of us is the customization to
a given industrial context. This includes:

• Accurately testing the system on the com-
pany’s documents so as to filter out a few
remaining odd error detections,

• Introducing the domain knowledge via the
domain ontology and terminology, and en-
hancing the rules we have developed to take
every aspect into account,

• Analyzing and incorporating into the system
the authoring guidelines proper to the com-
pany that may have an impact on understand-
ing and therefore on the emergence of risks,

• Implementing the interfaces between the orig-
inal user documents and our system, with the
abstract intermediate representation we have
defined,

• Customizing the tags expressing errors to the
users profiles and expectations, and enhanc-
ing correction schemas.

When sufficiently operational, the kernel of the
system will be made available on line, and proba-
bly the code will be available in open-source mode
or via a free or low cost license.

Acknowledgements

This project is funded by the French National Re-
search Agency ANR. We also thanks reviewers
and the companies that showed a strong interest in
our project, let us access to their technical docu-
ments and allowed us to observed their technical
writers.

References
Kurt Ament. 2002. Single Sourcing. Building modular

documentation, W. Andrew Pub.

Flore Barcellini, Camille Albert, Corinne Grosse,
Patrick Saint-Dizier. 2012. Risk Analysis and Pre-
vention: LELIE, a Tool dedicated to Procedure and
Requirement Authoring, LREC 2012, Istanbul.

Patrice Béguin. 2003. Design as a mutual learning pro-
cess between users and designers, Interacting with
computers, 15 (6).

Sarah Bourse, Patrick Saint-Dizier. 2012. A Repository
of Rules and Lexical Resources for Discourse Struc-
ture Analysis: the Case of Explanation Structures,
LREC 2012, Istanbul.

Gerhard Brewka, Thomas Eiter, Mirosław
Truszczyński. 2011. Answer set programming at
a glance. Communications of the ACM 54 (12),
92–103.

Marie Garnier. 2012. Automatic correction of adverb
placement errors: an innovative grammar checker
system for French users of English, Eurocall’10 pro-
ceedings, Elsevier.

Walther Kintsch. 1988. The Role of Knowledge in Dis-
course Comprehension: A Construction-Integration
Model, Psychological Review, vol 95-2.

Elizabeth C. Hull, Kenneth Jackson, Jeremy Dick. 2011.
Requirements Engineering, Springer.

William C. Mann, Sandra A. Thompson. 1988. Rhetor-
ical Structure Theory: Towards a Functional Theory
of Text Organisation, TEXT 8 (3), 243–281. Sandra
A. Thompson. (ed.), 1992. Discourse Description:
diverse linguistic analyses of a fund raising text,
John Benjamins.

Dan Marcu. 1997. The Rhetorical Parsing of Natural
Language Texts, ACL’97.

Dan Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization, MIT Press.

Vanina Mollo, Pierre Falzon. 2004. Auto and allo-
confrontation as tools for reflective activities. Ap-
plied Ergonomics, 35 (6), 531–540.

Vanina Mollo, Pierre Falzon. 2008. The development of
collective reliability: a study of therapeutic decision-
making, Theoretical Issues in Ergonomics Science,
9(3), 223–254.

Dietmar Rösner, Manfred Stede. 1992. Customizing
RST for the Automatic Production of Technical
Manuals, In Robert Dale et al. (eds.) Aspects of
Automated Natural Language Generation. Berlin:
Springer, 199–214.

Dietmar Rösner, Manfred Stede. 1994. Generating
multilingual technical documents from a knowledge
base: The TECHDOC project, In: Proc. of the Inter-
national Conference on Computational Linguistics,
COLING-94, Kyoto.

Patrick Saint-Dizier. 2012. Processing Natural Lan-
guage Arguments with the TextCoop Platform, Jour-
nal of Argumentation and Computation.

Edmond H. Weiss. 2000. Writing remedies. Practical
exercises for technical writing, Oryx Press.

38


