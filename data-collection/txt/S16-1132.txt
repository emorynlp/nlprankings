



















































ICL00 at SemEval-2016 Task 3: Translation-Based Method for CQA System


Proceedings of SemEval-2016, pages 857â€“860,
San Diego, California, June 16-17, 2016. cÂ©2016 Association for Computational Linguistics

 

 

ICL00 at SemEval-2016 Task 3: Translation-Based Method for CQA 

System 

 

 
Minghua Zhang   Yunfang Wu 

Institute of Computational Linguistics, Peking University 

 

 

 

Abstract 

We participate in the English subtask B and C 

at SemEval-2016 Task 3 â€œCommunity Ques-

tion Answeringâ€. This paper is concerned with 

the description of our participating system. We 

propose a ranking model that combines a trans-

lation model with the cosine similarity-based 

method. Compared to the traditional bag of 

words method, the proposed model is more ef-

fective because the relationships between 

words can be explicitly modeled through word-

to-word translation probabilities. Experiments 

conducted on the official test data demonstrate 

that our proposed ranking method obtains 

promising results. 

1 Introduction 

The SemEval-2016 Task 3 (Nakov et al., 2016) 

Community Question Answering (CQA) covers a 

full task on CQA and which is, therefore, closer to 

a real application. To facilitate the participation of 

non IR/QA scholar to the task, the search engine 

step is already carried out which means that the task 

organizers explicitly provides the set of potential 

answers to be reranked. That is to say, given a new 

question (aka original question) and the set of the 

first 10 related questions (retrieved by a search en-

gine) in subtask B, our system will focus on rerank-

ing the related questions according to their similar-

ity with the original question. Similar to subtask B, 

in subtask C that is the main English subtask, Given 

a new question and the set of the first 10 related 

questions, each associated with its first 10 com-

ments appearing in its thread, we will rerank the 100 

comments (10 questions * 10 comments) according 

to their relevance with respect to the original ques-

tion. 

Note that the subtask B will give us enough tools 

to solve the main subtask. And therefore, our paper 

will give an overall description of the system based 

on subtask B. In section 2, we will briefly discuss 

an important difference between the subtask C and 

subtask B in the course of reranking. 

However, the major challenge for subtask B, as 

for most of CQA systems, is the word mismatch be-

tween the original question and the related question. 

For example, â€œWhere I can buy good oil for mas-

sage?â€ and â€œIs there any place I can find scented 

massage oils in Qatar?â€ are two very similar ques-

tions, but they only have a few words in common. 

To solve the word mismatch problem, we focus on 

translation-based approaches in this paper.  

The remainder of this paper is organized as fol-

lows. Section 2 introduces the methods used in the 

ranking model clearly. Section 3 presents the trans-

lation probabilities Estimation. We will talk about 

an overview of the system in section 4. Section 5 

presents the experimental results. In Section 6, we 

conclude with ideas for future research. 

2 Ranking Approach 

In SemEval-2016 Task 3, the dataset file is a se-

quence of question pair instances. Each instance 

contain an original question and a thread which con-

sists of a potentially related question, together with 

10 comments for it. Next, let Q-Q denotes the set of 

all original-related question pairs in the archive, Q-

Q = {â€¦, [(orgi, rel1), (orgi, rel2), ..., (orgi, relj), â€¦, 

(orgi, rel10)], â€¦}. So, the goal of subtask B is to re-

rank the related questions according to the 

Score(orgi, relj). Typically, this score can be mod-

eled by the probability that orgi is generated by relj. 

Thus, the following part of this section focus on how 

to calculate P(orgi|relj). 

857



 

 

However, the job of subtask C is to rerank the 

100 comments according to their relevance with re-

spect to each original question. For clarity, let Q-C 

denotes the set of all question-comment pairs, Q-C 

= {â€¦, [(orgi, relj, Cij1), (orgi, relj, Cij2), ..., (orgi, relj, 

Cijk), â€¦, (orgi, relj, Cij100)], â€¦}. To begin the process, 

we apply the tool which is designed for subtask B to 

calculate the relevance between original question 

and related question, then regard it as a weight. In 

the next step, we make use of translation model to 

obtain the relevance between original question and 

comment. Finally, we will rerank the comments ac-

cording to the Score(orgi, Cijk) which can be written 

as: 

ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘— =
ğ‘ƒ(ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—)

âˆ‘ ğ‘ƒ(ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—â€²)
ğ‘›
ğ‘—â€²=1

                   (1) 

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘œğ‘Ÿğ‘”ğ‘– , ğ¶ğ‘–ğ‘—ğ‘˜) = ğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘— âˆ— ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ (ğ‘œğ‘Ÿğ‘”ğ‘–|ğ¶ğ‘–ğ‘—ğ‘˜)   (2) 

where orgi is the original question, relj is the related 

question, and Cijk is the comment for relj. 

2.1 Word-Based Translation Model 

Previous work (Jeon et al., 2005) were the first to 

apply the translation based method to CQA, subse-

quent work (Xue et al., 2008) proposed to linearly 

combine language model and word-based transla-

tion model into a unified framework. The experi-

ments show that this model gains better perfor-

mance than both the language model and the word-

based translation model. Following Xue et al. 

(2008), this model can be written as:  

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘œğ‘Ÿğ‘”ğ‘– , ğ‘Ÿğ‘’ğ‘™ğ‘—) = âˆ ğ‘ƒ(ğ‘¤|ğ‘Ÿğ‘’ğ‘™ğ‘—)ğ‘¤âˆˆğ‘œğ‘Ÿğ‘”ğ‘–      (3) 

ğ‘ƒ(ğ‘¤|ğ‘Ÿğ‘’ğ‘™ğ‘—) = ğ›¼
#(ğ‘¤,ğ‘Ÿğ‘’ğ‘™ğ‘—)

|ğ‘Ÿğ‘’ğ‘™ğ‘—|
+ ğ›½âˆ‘ ğ‘ƒ(ğ‘¤|ğ‘¡)

#(ğ‘¡,ğ‘Ÿğ‘’ğ‘™ğ‘—)

|ğ‘Ÿğ‘’ğ‘™ğ‘—|
ğ‘¡âˆˆğ‘Ÿğ‘’ğ‘™ğ‘—

+

ğ›¾
#(ğ‘¤,ğµ)

|ğµ|
                     (4) 

where #(w, relj) and #(t, relj) is the frequency of term 

w and t in relj respectively, B denotes the whole ar-

chive, |relj| and |B| denote the length of relj and B 

respectively, P(w|t) denotes the translation probabil-

ity from word t to word w. 

2.2 Combination of Cosine similarity and 
Translation Method 

We compared the performances of a unigram lan-

guage model and a cosine similarity-based method 

on the development dataset, which demonstrated 

that the cosine similarity method outperforms the 

language model. Therefore, we propose to linearly 

combine the cosine similarity and translation model 

into a ranking model, which can be written as: 

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘œğ‘Ÿğ‘”ğ‘– , ğ‘Ÿğ‘’ğ‘™ğ‘—) = ğ›¼ğ‘ƒğ‘ğ‘œğ‘ (ğ‘œğ‘Ÿğ‘”ğ‘– , ğ‘Ÿğ‘’ğ‘™ğ‘—) +

ğ›½ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ 
â€² (ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—)                                                          (5) 

ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ (ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—) = âˆ (âˆ‘ ğ‘ƒ(ğ‘¤|ğ‘¡)
#(ğ‘¡,ğ‘Ÿğ‘’ğ‘™ğ‘—)

|ğ‘Ÿğ‘’ğ‘™ğ‘—|
ğ‘¡âˆˆğ‘Ÿğ‘’ğ‘™ğ‘—

)ğ‘¤âˆˆğ‘œğ‘Ÿğ‘”ğ‘–  (6) 

ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ 
â€² (ğ‘œğ‘Ÿğ‘”

ğ‘–
|ğ‘Ÿğ‘’ğ‘™ğ‘—) =

10
âˆ’ log2 ğ‘ƒğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ (ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—)
â„ (7) 

where Pcos(ğ‘œğ‘Ÿğ‘”ğ‘–, ğ‘Ÿğ‘’ğ‘™ğ‘—) denotes the cosine similarity. 

Ptrans(ğ‘œğ‘Ÿğ‘”ğ‘–|ğ‘Ÿğ‘’ğ‘™ğ‘—) denotes the translation probabili-

ties. The two parts are not in an order of magnitude. 

So when we obtained the two similarity scores, the 

translation probabilities have to be transformed ac-

cording to the formula (7). 

3 Translation Probabilities Estimation 

3.1 Parallel Corpus Collection 

The performance of the proposed ranking model 

heavily depends on the quality of the translation 

probabilities. Therefore, besides designing transla-

tion based ranking method, another important prob-

lem is how to learn good word-to-word translation 

probabilities. 

In the given training dataset, the similarity rela-

tionship between the related question and original 

question can be accessed from the attribute 

â€œRELQ_RELEVANCE2ORGQâ€ belonging to the 

tag â€œRelQuestionâ€, which can be PerfectMatch, Rel-

evant and Irrelevant. When the attribute value takes 

PerfectMatch or Relevant, thereâ€™s a strong possibil-

ity that the original question and relevant question 

express similar meanings with different words. So, 

it is natural to use the matching original-relevant 

question pairs as the â€œparallel corpusâ€ to estimate 

word-to-word translation probabilities. Furthermore, 

it is easy to realize that if one original question is 

similar with two different relevant questions simul-

taneously, then the two relevant questions would 

also express similar meanings. As an example, from 

the initial parallel corpus {[org1, rel1], [org1, rel2], 

[org1, rel3], [org2, rel4], [org2, rel5]}, we can obtain 

the new big parallel corpus {[org1, rel1], [org1, rel2], 

[org1, rel3], [rel1, rel2], [rel1, rel3], [rel2, rel3], [org2, 

858



 

 

rel4], [org2, rel5], [rel4, rel5]} through the simple ex-

tension method. 

In the IBM translation model 1, sentences are 

normally translated from one language into another 

language. But in our task, the similar sentence pairs 

are written in the same language, the correspond-

ence of words is not as strong as in the bilingual sen-

tence pair. The word-to-word translation probabili-

ties can be learned with either part as the source lan-

guage and the other part as the target. Accordingly, 

the training data is doubled. When there is a parallel 

corpus consisting of the similar sentence pairs, the 

training module will utilize IBM translation model 

1 incorporating an EM-based algorithm to learn the 

word-to-word translation probabilities. 

3.2 Consolidation Method 

The parallel sentence pairs are written in the same 

language. If source sentences contain word â€œwiâ€ and 

target sentences contain word â€œwjâ€, we can obtain 

the word-to-word translation probabilities ğ‘ƒ(ğ‘¤ğ‘—|ğ‘¤ğ‘–). 

Conversely, the word "wi" can also appear in target 

sentences and the word "wj" appear in source sen-

tences. So, we can obtain the reverse translation 

probabilities ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘—)  through the same training 

process. Then we assume that the reverse translation 

probabilities are additional information to improve 

the word-to-word translation probabilities, and the 

combination of both will be consolidation beneficial. 

So we linearly combine the trained word-to-word 

translation probabilities: 

ğ‘ƒğ‘™ğ‘–ğ‘›(ğ‘¤ğ‘–|ğ‘¤ğ‘—) = ğ›¾ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘—) + (1 âˆ’ ğ›¾)ğ‘ƒ(ğ‘¤ğ‘—|ğ‘¤ğ‘–)   (8) 

To see how much the consolidation strategy ben-

efits the rerank task, we introduce two baseline 

methods for comparison. The first method denotes 

the initial word-to-word translation probabilities, 

and the second denote the reverse translation prob-

abilities. Table 1 reports the experimental results of 

Subtask B on the development dataset. We can see 

that the consolidation strategy outperforms the two 

baseline methods in our task. From the experimental 

result, we can see that the reverse translation proba-

bilities do have some positive effects. 

                                                                                                            
1 Defaultdict is a subclass of the dict that calls a factory func-

tion to supply missing values in Python. 

4 System overview  

In the following part, we will introduce the details 

of the implementation. The whole calculation pro-

cess can be divided into two main modules: Pre-pro-

cessing and Estimate. 

 

Pre-processing. This module tries to extract the 

subject text and the main body of questions from the 

XML-formatted input file first of all, then combine 

the two parts together to form the original question 

and related question. Next, the system makes word 

segmentation for each pairs of original question and 

related question, and removes stop words at the 

same time. Furthermore, Porter stemmer is em-

ployed to extract stem, which will be beneficial to 

learn good word-to-word translation probabilities. 

For the sake of saving the evaluation times, we ex-

ecute statistical calculations on the word list which 

represents the original question and related question 

after segmentation. For example, there is a words 

list [massage, oil, where, buy, good, oil, massage], 

weâ€™ll obtain a dictionary {massage:2, oil:2, where:1, 

buy:1, good:1} after statistics, and we also know 

that the length of list is seven. 

Estimate: In this stage, the system loads the word-

to-word translation probability table, which we se-

lect defaultdict1 as its storage structure. Finally, we 

compute the similarity score of original question 

and related question according to our ranking model 

which linearly combines the cosine similarity and 

translation model. In subtask B, the labels Perfect-

Match and Relevant should be regarded as equal, so 

our goal is to rank the PerfectMatch and Relevant 

candidates at the top, in any order, and the Irrelevant 

candidates at the bottom, also in any order. 

5 Experiments 

In this section, experiments are conducted on DEV 

Model Trans Prob MAP 

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘œğ‘Ÿğ‘”ğ‘– , ğ‘Ÿğ‘’ğ‘™ğ‘—) 

ğ‘ƒ(ğ‘¤ğ‘—|ğ‘¤ğ‘–) 0.7312 

ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘—) 0.7376 

ğ‘ƒğ‘™ğ‘–ğ‘›(ğ‘¤ğ‘–|ğ‘¤ğ‘—) 0.7415 
 

Table 1:  The impact of consolidation strategy. 

859



 

 

dataset to demonstrate the performance of our pro-

posed ranking model. 

5.1 Data Set and Evaluation Metrics 

The official dataset contains TRAIN, DEV and 

TEST. The development dataset is intended to be 

used as a development-time evaluation dataset as 

we develop our systems. However, when submitting 

prediction results, we will add the development da-

taset to training data as well. The total available data 

of the TRAIN part is made up of 267 original ques-

tions and 2669 related questions for Subtask B, plus 

26690 related comments for Subtask C. DEV da-

taset which were manually double-checked and are 

very reliable consist of 50 original questions, 500 

related questions, and 5,000 comments. As far as the 

TEST is concerned, the task organizers provide par-

ticipants with 70 original questions, so, there would 

be 700 predictions for subtask B and 7,000 predic-

tions for subtask C. The official scorer will provide 

a number of evaluation measures to assess the qual-

ity of the output of a system, but the official evalu-

ation measure towards which all systems will be 

evaluated and ranked is Mean Average Precision 

(MAP). 

5.2 Results 

Five types of baselines are used to compare with our 

proposed ranking model. Table 2 presents the MAP 

performance comparison of different methods on 

DEV dataset in subtask B. Row 1 to row 5 are the 

IR engine default ordering, Language Model, Co-

sine Similarity-based method, Translation Model 

and Translation-based Language model. Row 5 is 

our proposed translation-based reranking method. 

Compared with other baselines approaches, our pro-

posed model received good results. Finally, the 

competition result for our primary submissions are 

0.7511 against 0.7475 baseline in Subtask B, and 

0.4919 against 0.4036 baseline in Subtask C. 

6 Conclusions and Future Work 

In this paper, we propose a ranking model that com-

bines a translation model with the cosine-based sim-

ilarity method to solve the rerank task in CQA. Ex-

periments on test data demonstrate the effectiveness 

of our method. 

There are some ways in which this research could 

be continued. First, we plan to apply neural machine 

translation (Bahdanau et al., 2014) to learn good 

translation probabilities. In addition, phrase-based 

translation model for question retrieval (Zhou et al., 

2011) have shown superior performance compared 

to word-based translation models. So it is necessary 

to try this method to further improve the perfor-

mance. 

Acknowledgments 

This work is supported by National Natural Science 

Foundation of China (61371129), National High 

Technology Research and Development Program of 

China (2015AA015403), Key Program of Social 

Science foundation of China (12&ZD227). 

References  

Bahdanau D, Cho K, Bengio Y. Neural Machine Trans-

lation by Jointly Learning to Align and Translate[J]. 

Eprint Arxiv, 2014. 

J. Jeon, W. B. Croft, and J. H. Lee. Finding similar 

questions in large question and answer archives. In 

Proceedings of the 14th ACM Conference on 

Information and Knowledge Management, pages 

84â€“90, 2005. 

Preslav Nakov, Llu\'{i}s M\`{a}rquez, Alessandro Mos-

chitti, Walid Magdy, Hamdy Mubarak, Abed Alhakim 

Freihat, Jim Glass, and Bilal Randeree. 2016. 

SemEval-2016 task 3: Community Question Answer-

ing. In Proceedings of the 10th International Workshop 

on Semantic Evaluation, SemEval â€™16, San Diego, CA. 

X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models 

for question and answer archives. In Proceedings of 

SIGIR, pages 475-482. 

Zhou G, Cai L, Zhao J, et al. Phrase-Based Translation 

Model for Question Retrieval in Community Question 

Answer Archives[C]// Proceedings of the 49th Annual 

Meeting of the Association for Computational Lin-

guistics: Human Language Technologies - Volume 1. 

Association for Computational Linguistics, 2011:653-

662. 

# Methods MAP 

1 IR-engine 0.7135 

2 LM 0.7248 

3 Cosine 0.7287 

4 Trans 0.7342 

5 Trans-LM 0.7360 

6 Trans-Cosine 0.7415 
 

Table 2: Comparison of different methods in subtask B. 

860


