










































Using Out-of-Domain Data for Lexical Addressee Detection in Human-Human-Computer Dialog


Proceedings of NAACL-HLT 2013, pages 221‚Äì229,
Atlanta, Georgia, 9‚Äì14 June 2013. c¬©2013 Association for Computational Linguistics

Using Out-of-Domain Data for Lexical Addressee Detection in
Human-Human-Computer Dialog

Heeyoung Lee1‚àó Andreas Stolcke2 Elizabeth Shriberg2
1Dept. of Electrical Engineering, Stanford University, Stanford, California, USA

2Microsoft Research, Mountain View, California, USA
heeyoung@stanford.edu, {anstolck,elshribe}@microsoft.com

Abstract

Addressee detection (AD) is an important
problem for dialog systems in human-human-
computer scenarios (contexts involving mul-
tiple people and a system) because system-
directed speech must be distinguished from
human-directed speech. Recent work on AD
(Shriberg et al., 2012) showed good results
using prosodic and lexical features trained on
in-domain data. In-domain data, however, is
expensive to collect for each new domain. In
this study we focus on lexical models and in-
vestigate how well out-of-domain data (either
outside the domain, or from single-user sce-
narios) can fill in for matched in-domain data.
We find that human-addressed speech can be
modeled using out-of-domain conversational
speech transcripts, and that human-computer
utterances can be modeled using single-user
data: the resulting AD system outperforms
a system trained only on matched in-domain
data. Further gains (up to a 4% reduction in
equal error rate) are obtained when in-domain
and out-of-domain models are interpolated.
Finally, we examine which parts of an utter-
ance are most useful. We find that the first
1.5 seconds of an utterance contain most of
the lexical information for AD, and analyze
which lexical items convey this. Overall, we
conclude that the H-H-C scenario can be ap-
proximated by combining data from H-C and
H-H scenarios only.

‚àóWork done while first author was an intern with Microsoft.

1 Introduction

Before a spoken dialog system can recognize and in-
terpret a user‚Äôs speech, it should ideally determine
if speech was even meant to be interpreted by the
system. We refer to this task as addressee detec-
tion (AD). AD is often overlooked, especially in tra-
ditional single-user scenarios, because with the ex-
ception of self-talk, side-talk or background speech,
the majority of speech is usually system-directed.
As dialog systems expand to more natural contexts
and multiperson environments, however, AD can be-
come a crucial part of the system‚Äôs operational re-
quirements. This is particularly true for systems in
which explicit system addressing (e.g., push-to-talk
or required keyword addressing) is undesirable.

Past research on addressee detection has focused
on human-human (H-H) settings, such as meetings,
sometimes with multimodal cues (op den Akker and
Traum, 2009). Early systems relied primarily on re-
jection of H-H utterances either because they could
not be interpreted (Paek et al., 2000), or because they
yielded low speech recognition confidence (Dowd-
ing et al., 2006). Some systems combine gaze
with lexical and syntactic cues to detect H-H speech
(Katzenmaier et al., 2004). Others use relatively
simple prosodic features based on pitch and energy
in addition to those derived from automatic speech
recognition (ASR) (Reich et al., 2011).

With some exceptions (Bohus and Horvitz, 2011;
Shriberg et al., 2012), relatively little work has
looked at the human-human-computer (H-H-C) sce-
nario, i.e. at contexts involving two or more people
who interact both with a system and with each other.

221



Shriberg et al. (2012) found that novel prosodic
features were more accurate than lexical or seman-
tic features based on speech recognition for the ad-
dressee task. The corpus, also used herein, is com-
prised of H-H-C dialog in which roughly half of the
computer-addressed speech consisted of a small set
of fixed commands. While the word-based features
map directly to the commands, they had trouble
distinguishing all other (noncommand) computer-
directed speech from human-directed speech. This
is because addressee detection in the H-H-C sce-
nario becomes even more challenging when the sys-
tem is designed for natural speech, i.e., utterances
that are conversational in form and not limited to
command phrases with restricted syntax. Further-
more, H-H utterances can be about the domain of
the system (e.g., discussing the dialog task), mak-
ing AD based on language content more difficult.
The prosodic features were good at both types of
distinctions‚Äîeven improving performance signifi-
cantly when combined with true-word (cheating)
lexical features that have 100% accuracy on the
commands. Nevertheless, the prior work showed
that lexical n-grams are useful for addressee detec-
tion in the H-H-C scenario.

A problem with lexical features is that they are
highly task- and domain-dependent. As with other
language modeling tasks, one usually has to collect
matched training data in significant quantities. Data
collection is made more cumbersome and expensive
by the multi-user aspect of the scenario. Thus, for
practical reasons alone, it would be much better if
the language models for AD could be trained on
out-of-domain data, and if whatever in-domain data
is needed could be limited to single-user interac-
tion. We show in this paper that precisely this train-
ing scenario is feasible and achieves results that are
comparable or better than using completely matched
H-H-C training data.

In addition to studying the role of out-of-domain
data for lexical AD models, we also examine which
words are useful, and how soon in elapsed time they
are available. Whereas most prior work in AD has
looked at processing of entire utterances, we con-
sider an online processing version where AD deci-
sions are to be made as soon as possible after an
utterance was initiated. We find that most of the
addressee-relevant lexical information can be found

Figure 1: Conversational Browser dialog system en-
vironment with multi-human scenario

in the first 1.5 seconds, and analyze which words
convey this information.

2 Data

We use in-domain and out-of-domain data from var-
ious sources. The corpora used in this work differ in
size, domain, and scenario.

2.1 In-domain data
In-domain data is collected from interactions be-
tween two users and a ‚ÄúConversational Browser‚Äù
(CB) spoken dialog system. We used the same
methodology as Shriberg et al. (2012), but using ad-
ditional data. As depicted in Figure 1, the system
shows a browser on a large TV screen and users
are asked to use natural language for a variety of
information-seeking tasks. For more details about
the dialog system and language understanding ap-
proach, see Hakkani-TuÃàr et al. (2011a; 2011b).

We split the in-domain data into training, devel-
opment, and test sets, preserving sessions. Each ses-
sion is about 5 to 40 minutes long. Even though
the whole conversation is recorded, only the seg-
ments captured by the speech recognition system
are used in our experiments. Each utterance seg-
ment belongs to one of four types: computer-
command (C-command), comprising navigational
commands to the system, computer-noncommand
(C-noncommand), which are computer-directed ut-
terances other than commands, human-directed (H),
and mixed (M) utterances, which contain a combina-

222



Table 1: In-domain corpus

(a) Sizes, distribution, and ASR word error rates of in-
domain utterance types

Data set Train Dev Test WER
Transcribed words 6,490 11,298 9,486
ASR words 4,649 6,360 5,514 59.3%
H (%) 19.1 48.6 37.0 87.6%
C-noncomm. (%) 38.3 27.8 32.2 32.6%
C-command (%) 39.9 18.7 27.2 19.7%
M (%) 2.7 4.9 3.6 69.6%

(b) Example utterances by type

Type Example
H Do you want to watch a

movie?
C-noncommand How is the weather today?
C-command Scroll down, Go back.
M Show me sandwich shops.

Oh, are you vegetarian?

tion of human- and computer-directed speech. The
sizes and distribution of all utterance types, as well
as sample utterances are shown in Table 1.

The ASR system used in the system was based on
off-the-shelf acoustic models and had only the lan-
guage model adapted to the domain, using very lim-
ited data. Consequently, as shown in the right-most
column of Table 1(a), the word error rates (WERs)
are quite high, especially for human-directed utter-
ances. While these could be improved with tar-
geted effort, we consider this a realistic application
scenario, where in-domain training data is typically
scarce, at least early in the development process.
Therefore, any lexically based AD methods need to
be robust to poor ASR accuracy.

2.2 Out-of-domain data
To replace the hard-to-obtain in-domain H-H-C data
for training, we use the four out-of-domain corpora
(two H-C and two H-H) shown in Table 2.

Single-user CB data comes from the same Con-
versational Browser system as the in-domain data,
but with only one user present. This data can there-
fore be used for modeling H-C speech. Bing anchor
text (Huang et al., 2010) is a large n-gram corpus of
anchor text associated with links on web pages en-

Table 2: Out-of-domain corpora. ‚ÄúSingle-user CB‚Äù
is a corpus collected in same environment as the H-
H-C in-domain data, except that only a single user
was present.

Corpus Addressee Size
Single-user CB H-C 21.9k words
Bing anchor text H-C 1.3B bigrams
Fisher H-H 21M words
ICSI meetings H-H 0.7M words

Single user CB, 
Bing ‚Äì  

out-of-domain 

in-domain  
(HC) 

in-domain 
(HH) 

Fisher, ICSI 
meeting ‚Äì  

out-of-domain 

Language model for 
human directed 
utterances (H)  

Language model for 
computer directed 

utterances (C)  

ùëÉ(ùë§|ùêª) 

1

ùë§
ùëôùëúùëî

ùëÉ(ùë§|ùê∂)

ùëÉ(ùë§|ùêª)
 

ùëÉ(ùë§|ùê∂) 

Figure 2: Language model-based score computation
for addressee detection

countered by the Bing search engine. When users
want to follow a link displayed on screen, they usu-
ally speak a variant of the anchor text for the link.
We hypothesized that this corpus might aid the mod-
eling of computer-noncommand type utterances in
which such ‚Äúverbal clicks‚Äù are frequent. Fisher tele-
phone conversations and ICSI meetings are both cor-
pora of human-directed speech. The Fisher corpus
(Cieri et al., 2004) comprises two-person telephone
conversations between strangers on prescribed top-
ics. The ICSI meeting corpus (Janin et al., 2003)
contains multiparty face-to-face technical discus-
sions among colleagues.

3 Method

3.1 Language modeling for addressee detection
We use a lexical AD system that is based on mod-
eling word n-grams in the two addressee-based ut-
terance classes, H (for H-H) and C (for H-C utter-
ances). This approach is similar to language model-
based approaches to speaker and language recogni-
tion, and was shown to be quite effective for this
task (Shriberg et al., 2012). Instead of making
hard decisions, the system outputs a score that is

223



the length-normalized likelihood ratio of the two
classes:

1

|w|
log

P (w|C)
P (w|H)

, (1)

where |w| is the number of words in the recognition
output w for an utterance. P (w|C) and P (w|H) are
obtained from class-specific language models. Fig-
ure 2 gives a flow-chart of the score computation.

Class likelihoods are obtained from standard tri-
gram backoff language models, using Witten-Bell
discounting for smoothing (Witten and Bell, 1991).
For combining various training data sources, we use
language model adaptation by interpolation (Bel-
legarda, 2004). First, a separate model is trained
from each source. The probability estimates from
in-domain and out-of-domain models are then aver-
aged in a weighted fashion:

P (wk|hk) = ŒªPin(wk|hk) + (1‚àí Œª)Pout(wk|hk)
(2)

where wk is the k-th word, hk is the (n ‚àí 1)-gram
history for the wordwk. Œª is the interpolation weight
and is obtained by tuning a task-related metric on the
development set. We investigated optimizing Œª for
either model perplexity or classification accuracy, as
discussed below.

3.2 Part-of-speech-based modeling
So far we have only been modeling the lexical forms
of words in utterances. If we encounter a word never
before seen, it would appear as an out-of-vocabulary
item in all class-specific language models, and not
contribute much to the decision. More generally, if
a word is rare, its n-gram statistics will be unreliable
and poorly modeled by the system. (The sparseness
issue is exacerbated by small amounts of training
data as in our scenario.)

One common approach to deal with data sparse-
ness in language modeling is to model n-grams over
word classes rather than raw words (Brown et al.,
1992). For example, if we have an utterance How
is the weather in Paris?, the addressee probabilities
are likely to be similar had we seen London instead
of Paris. Therefore, replacing words with properly
chosen word class labels can give better generaliza-
tion from the observed training data. Among the
many methods proposed to class words for language
modeling purposes we chose part-of-speech (POS)

tagging over other, purely data-derived classing al-
gorithms (Brown et al., 1992), for two reasons. First,
our goal here is not to minimize the perplexity of the
data, but to enhance discrimination among utterance
classes. Second, a data-driven class inference algo-
rithm would suffer from the same sparseness issues
when it comes to unseen and rare words (as no ro-
bust statistics are available to infer an unseen word‚Äôs
best class in the class induction step). A POS tag-
ger, on the other hand, can do quite well on unseen
words, using context and morphological cues.

A hidden Markov model tagger using POS-
trigram statistics and context-independent class
membership probabilities was used for tagging all
LM training data. The tagger itself had been
trained on the Switchboard (conversational tele-
phone speech) transcripts of the Penn Treebank-
3 corpus (Marcus et al., 1999), and used the 39
Treebank POS labels. To strike a compromise be-
tween generalization and discriminative power in
the language model, we retained the topN most fre-
quent word types from the in-domain training data
as distinct tokens, and varied N as a metaparam-
eter. Barzilay and Lee (2003) used a similar idea
to generalize patterns by substituting words with
slots. This strategy will tend to preserve words that
are either generally frequent function and domain-
independent words, capturing stylistic and syntac-
tic patterns, or which are frequent domain-specific
words, and can thus help characterize computer-
directed utterances.

Here is a sample sentence and its transformed ver-
sion:

Original: Let‚Äôs find an Italian restaurant
around this area.
POS-tagged: Let‚Äôs find an JJ NN around this
area.

The words except Italian and restaurant are un-
changed because they are in the list of N most fre-
quent words. We transformed all training and test
data in this fashion and then modeled n-gram statis-
tics as before. The one exception was the Bing
anchor-text data, which was only available in the
form of word n-grams (the sentence context required
for accurate POS tagging was missing).

224



Table 3: Addressee detection performance (EER) with different training sets

ASR Transcript
Baseline (in-domain only) 31.1 17.3
Fisher+ICSI, Single-user CB+Bing (out-of-domain only) 27.8 14.2
Baseline + Fisher+ICSI, Single CB + Bing (both-all) 26.9 14.0
Baseline + ICSI, Single-user CB (both-small) 26.6 13.0

3.3 Evaluation metrics

Typically, an application-dependent threshold would
be applied to the decision score to convert it into a
binary decision. The optimal threshold is a func-
tion of prior class probabilities and error costs. As
in Shriberg et al. (2012), we used equal error rate
(EER) to compare systems, since we are interested
in the discriminative power of the decision score in-
dependent of priors and costs. EER is the probability
of false detections and misses at the operating point
at which the two types of errors are equally proba-
ble. A prior-free metric such as EER is more mean-
ingful than classification accuracy because the utter-
ance type distribution is heavily skewed (Table 1),
and because the rate of human- versus computer-
directed speech can vary widely depending on the
particular people, domain, and context. We also use
classification accuracy (based on data priors) in one
analysis below, because EERs are not comparable
for different test data subdivisions.

3.4 Online model

The actual dialog system used in this work pro-
cesses utterances after receiving an entire segment
of speech from the recognition subsystem. How-
ever, we envision that a future version of the sys-
tem would perform addressee detection in an online
manner, making a decision as soon as enough evi-
dence is gathered. This raises the question how soon
the addressee can be detected once the user starts
speaking. We simulate this processing mode using a
windowed AD model.

As shown in Figure 3, we define windows start-
ing at the beginning of the utterance and investigate
how AD performance changes as a function of win-
dow size. We use only the words and n-grams falling
completely within a given window. For example, the
word find would be excluded from Window 1 in Fig-

  Let‚Äôs          find       an     Italian    restaurant   around  this         area 

Window 1 

Window 2 ‚Ä¶ 

Figure 3: The window model

ure 3.
The benefit of early detection in this case is that

once speech is classified as human-directed, it does
not need to be sent to the speech recognizer and sub-
sequent semantic processing. This saves processing
time, especially if processing happens on a server.
Based on the window model performance, we can
assess the feasibility of an online AD model, which
can be approached by shifting the detection window
through time and finding addressee changes.

4 Results and Discussion

Table 3 compares the performance of our system us-
ing various training data sources. For diagnostic pur-
poses we also compare performance based on recog-
nized words (the realistic scenario) to that based on
human transcripts (idealized, best-case word recog-
nition).

Somewhat surprisingly, the system trained on out-
of-domain data alone performs better by 3.3 EER
points on ASR output and 3.1 points on transcripts
compared to the in-domain baseline. Combining
in-domain and out-of-domain data (both-all, both-
small) gives about 1 point additional EER gain. Note
that training on in-domain data plus the smaller-size
out-of-domain corpora (both-small) is better than
using all available data (both-all).

Figure 4 shows the detection error trade-off
(DET) between false alarm and miss errors for the

225



8 7 6 

5 

4 

3 
2 1 

Figure 4: Detection error trade-off (DET) curves for
the systems in Table 3. Thin lines at the top right
corner use ASR output (1-4); thick lines at the bot-
tom left corner use reference transcripts (5-8). Each
line number represents one of the systems in Table 3:
1,5 = in-domain only, 2,6 = out-of-domain only, 4,7
= both-all, 3,8 = both-small.

systems in Table 3. The DET plot depicts perfor-
mance not only at the EER operating point (which
lies on the diagonal), but over the range of possible
trade-offs between false alarm and miss error rates.
As can be seen, replacing or combining in-domain
data with out-of-domain data gives clear perfor-
mance gains, regardless of operating point (score
threshold), and for both reference and recognized
words.

Figure 5 shows H-H vs. H-C classification accu-
racies on each of the four utterance subtypes listed
in Table 1. It is clear that computer-command ut-
terances are the easiest to classify; the accuracy is
more than 90% using transcripts, and more than 85%
using ASR output. This is not surprising, since
commands are from a fixed small set of phrases.
The biggest gain from use of out-of-domain data
is found for computer-directed noncommand utter-
ances. This is helpful, since in general it is the
noncommand computer-directed utterances (rather
than the commands) that are highly confusable with
human-directed utterances: both use unconstrained
natural language. We note that H-H utterance are
very poorly recognized in the ASR condition when
only out-of-domain data is used. This may be be-

20

30

40

50

60

70

80

90

100

baseline(in-
domain only)

out-of-domain
only

both-all

both-small

ASR REF 

Figure 5: AD accuracies by utterance type

Table 4: Perplexities (computed on dev set ASR
words) by utterance type, for different training cor-
pora. Interpolation refers to the combination of the
three models listed in each case.

Test class
Training set H-C H-H
In-domain H-C (ASR) 257 1856
Single-user CB 104 1237
Bing anchor text 356 789
Interpolation 58 370
In-domain H-H (ASR) 887 1483
Fisher 995 795
ICSI meeting 2007 1583
Interpolation 355 442

cause the human-human corpora used in training
consist of transcripts, whereas the ASR output for
human-directed utterances is very errorful, creating
a severe train-test mismatch.

As for the optimization of the mixing weight Œª,
we found that minimizing perplexity on the devel-
opment set of each class is effective. This is a
standard optimization approach for interpolated lan-
guage models, and can be carried out efficiently us-
ing an expectation maximization algorithm. We also
tried search-based optimization using the classifica-
tion metric (EER) as the criterion. While this ap-
proach could theoretically give better results (since
perplexity is not a discriminative criterion) we found
no significant improvement in our experiments.

226



Table 4 shows the perplexities by class of lan-
guage models trained on different corpora. We can
take these as an indication of training/test mismatch
(lower perplexity indicating better match). We also
find substantial perplexity reductions from interpo-
lating models. In order to make perplexities compa-
rable, we trained all models using the union of the
vocabularies from the different sources.

In spite of perplexity being a good way to opti-
mize the weighting of sources, it is not clear that it
is a good criterion for selecting data sources. For
example, we see that the Fisher model has a much
lower perplexity on H-H utterances than the ICSI
meeting model. However, as reflected in Table 3,
the H language model that leaves out the Fisher data
actually performed better. The most likely expla-
nation is that the Fisher corpus is an order of mag-
nitude larger than the ICSI corpus, and that sheer
data size, not stylistic similarity, may account for the
lower perplexity of the Fisher model. Further inves-
tigation is needed regarding good criteria for corpus
selection for classification tasks such as AD.

Table 5 shows the EER performance of the POS-
based model, for various sizes N of the most-
frequent word list. We observe that the partial re-
placement of words with POS tags indeed improves
over the baseline model performance, by 1.5 points
on ASR output and by 1.1 points on transcripts.
We also see that the gain over the corresponding
word-only model is largest for the in-domain base-
line model, and less or non-existent for the out-of-
domain model. This is consistent with the notion
that the in-domain model suffers the most from data
sparseness, and therefore has the most to gain from
better generalization.

Interpolating with out-of-domain data still helps
here. The optimal N differs for ASR output versus
transcripts. The POS-based model with N = 300
improves the EER by 0.5 points on ASR output,
and N = 1000 improves the EER by 0.8 points on
transcripts. Here we use relatively large amounts of
training data, thus the performance gain is smaller,
though still meaningful.

Figure 6 shows the performance of the system
using time windows anchored at the beginnings of
utterances. We incrementally increase the window
width from 0.5 seconds to 3 seconds and compare
results to using full utterances. The leveling off of

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.5 1 1.5 2 3 full

Eq
u

al
 e

rr
o

r 
ra

te
 

Window width (seconds) 

ASR baseline

ASR outside

ASR both-all

ASR both-small

REF baseline

REF outside

REF both-all

REF both-small

Figure 6: Simulated online performance on incre-
mental windows

Table 6: The top 15 first words in utterances

ASR H-C Transcript H-C ASR H-H Transcript H-H
go go play I

scroll scroll go ohh
start start is so
show stop it yeah
stop show what it‚Äôs
bing find this you

search Bing show uh
find search how okay
play pause bing what

pause play select it
look look okay and
what uh does that‚Äôs
select what start is
how how so no
the ohh I we

the error plots indicates that most addressee infor-
mation is contained in the first 1 to 1.5 seconds,
although some additional information is found in
the later part of utterances (the plots never level off
completely). This pattern holds for both in-domain
and out-of-domain training, as well as for combined
models.

To give an intuitive understanding of where this
early addressee-relevant information comes from,
we tabulated the top 15 word unigrams in each ut-
terance class, are shown in Table 6. Note that
the substantial differences between the third and
fourth columns in the table reflect the high ASR
error rate for human-directed utterances, whereas

227



Table 5: Performance of POS-based model with various top-N word lists (EER)

Training data top100 top200 top300 top400 top500 top1000 top2000 Original
ASR baseline 31.6 31.0 29.6 30.1 30.2 31.4 31.5 31.1

out-of-domain only 36.5 37.0 37.2 36.9 36.8 36.6 37.3 27.8
both-all 28.2 26.6 26.1 26.7 27.4 26.9 27.6 26.9
both-small 28.0 26.5 26.2 26.6 26.4 26.3 26.5 26.6

REF baseline 17.1 16.2 16.6 17.1 16.7 17.0 17.2 17.3
out-of-domain only 17.6 17.6 17.5 17.2 17.1 17.2 18.1 14.2
both-all 12.5 12.5 12.5 12.7 12.8 13.2 13.5 14.0
both-small 13.0 13.2 12.8 13.2 12.8 12.2 12.7 13.0

for computer-directed utterances, the frequent first
words are mostly recognized correctly.

In computer-directed utterances we see mostly
command verbs, which, due to the imperative syn-
tax of these commands occur in utterance-initial po-
sition. Human-directed utterances are characterized
by subject pronouns such as I and it, or answer parti-
cles such as yeah and okay, which likewise occur in
initial position. Based on word frequency and syn-
tax alone it is thus clear why the beginnings of utter-
ances contain strong lexical cues.

5 Conclusion

We explored the use of outside data for training
lexical addressee detection systems for the human-
human-computer scenario. Advantages include sav-
ing the time and expense of an in-domain data col-
lection, as well as performance gains even when
some in-domain data is available. We show that H-
C training data can be obtained from a single-user
H-C collection, and that H-H speech can be mod-
eled using general conversational speech. Using the
outside training data, we obtain results that are even
better than results using matched (but smaller) H-
H-C training data. Results can be improved consid-
erably by adapting H-C and H-H language models
with small amounts of matched H-H-C data, via in-
terpolation. The main reason for the improvement is
better detection of computer-directed noncommand
utterances, which tend to be confusable with human-
directed utterances. Another effective way to over-
come scarce training data is to replace the less fre-
quent words with part-of-speech labels. In both
baseline and interpolated model, we found that POS-

based models that keep an appropriate number of the
topN most frequent word types can further improve
the system‚Äôs performance.

In a second study we found that the most salient
phrases for lexical addressee detection occur within
the first 1 to 1.5 seconds of speech in each utter-
ance. It reflects a syntactic tendency of class-specific
words to occur utterance-initially, which shows the
feasibility of the online AD system.

Acknowledgments

We thank our Microsoft colleagues Madhu
Chinthakunta, Dilek Hakkani-TuÃàr, Larry Heck,
Lisa Stiefelman, and Gokhan TuÃàr for developing
the dialog system used in this work, as well as for
many valuable discussions. Ashley Fidler was in
charge of much of the data collection and annotation
required for this study. We also thank Dan Jurafsky
for useful feedback.

228



References
Regina Barzilay and Lillian Lee. 2003. Learning to

paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings HLT-NAACL
2003, pages 16‚Äì23, Edmonton, Canada.

Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93‚Äì108.

Dan Bohus and Eric Horvitz. 2011. Multiparty turn tak-
ing in situated dialog: Study, lessons, and directions.
In Proceedings ACL SIGDIAL, pages 98‚Äì109, Port-
land, OR.

Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467‚Äì479.

Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings 4th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 69‚Äì71, Lisbon.

John Dowding, Richard Alena, William J. Clancey,
Maarten Sierhuis, and Jeffrey Graham. 2006. Are
you talking to me? dialogue systems supporting mixed
teams of humans and robots. In Proccedings AAAI
Fall Symposium: Aurally Informed Performance: Inte-
grating Machine Listening and Auditory Presentation
in Robotic Systems, Washington, DC.

Dilek Hakkani-TuÃàr, Gokhan Tur, and Larry Heck. 2011a.
Research challenges and opportunities in mobile appli-
cations [dsp education]. IEEE Signal Processing Mag-
azine, 28(4):108 ‚Äì110.

Dilek Z. Hakkani-TuÃàr, GoÃàkhan TuÃàr, Larry P. Heck, and
Elizabeth Shriberg. 2011b. Bootstrapping domain de-
tection using query click logs for new domains. In
Proceedings Interspeech, pages 709‚Äì712.

Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansang Wang, and Fritz Behr. 2010. Exploring web
scale language models for search query processing. In
Proceedings 19th International Conference on World
Wide Web, pages 451‚Äì460, Raleigh, NC.

Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The ICSI meeting corpus. In Pro-
ceedings IEEE ICASSP, volume 1, pages 364‚Äì367,
Hong Kong.

Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings 6th International Conference
on Multimodal Interfaces, ICMI, pages 144‚Äì151, New
York, NY, USA. ACM.

Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, catalog item LDC99T42.

Rieks op den Akker and David Traum. 2009. A com-
parison of addressee detection methods for multiparty
conversations. In Proceedings of Diaholmia, pages
99‚Äì106.

Tim Paek, Eric Horvitz, and Eric Ringger. 2000. Con-
tinuous listening for unconstrained spoken dialog. In
Proceedings ICSLP, volume 1, pages 138‚Äì141, Bei-
jing.

Daniel Reich, Felix Putze, Dominic Heger, Joris Ijssel-
muiden, Rainer Stiefelhagen, and Tanja Schultz. 2011.
A real-time speech command detector for a smart con-
trol room. In Proceedings Interspeech, pages 2641‚Äì
2644, Florence.

Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-TuÃàr,
and Larry Heck. 2012. Learning when to listen:
Detecting system-addressed speech in human-human-
computer dialog. In Proceedings Interspeech, Port-
land, OR.

Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085‚Äì
1094.

229


