



















































Grounded Semantic Parsing for Complex Knowledge Extraction


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756–766,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Grounded Semantic Parsing for Complex Knowledge Extraction

Ankur P. Parikh∗
School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu

Hoifung Poon Kristina Toutanova
Microsoft Research

Redmond, WA, USA
hoifung,kristout@microsoft.com

Abstract

Recently, there has been increasing interest in
learning semantic parsers with indirect super-
vision, but existing work focuses almost ex-
clusively on question answering. Separately,
there have been active pursuits in leveraging
databases for distant supervision in informa-
tion extraction, yet such methods are often
limited to binary relations and none can han-
dle nested events. In this paper, we gener-
alize distant supervision to complex knowl-
edge extraction, by proposing the first ap-
proach to learn a semantic parser for extract-
ing nested event structures without annotated
examples, using only a database of such com-
plex events and unannotated text. The key idea
is to model the annotations as latent variables,
and incorporate a prior that favors semantic
parses containing known events. Experiments
on the GENIA event extraction dataset show
that our approach can learn from and extract
complex biological pathway events. More-
over, when supplied with just five example
words per event type, it becomes competitive
even among supervised systems, outperform-
ing 19 out of 24 teams that participated in the
original shared task.

1 Introduction

The goal of semantic parsing is to map text into
a complete and detailed meaning representation
(Mooney, 2007). Supervised approaches for learn-
ing a semantic parser require annotated examples,

∗ This research was conducted during the author’s intern-
ship at Microsoft Research.

Information
Extraction

ID TYPE CAUSE THEME TRIGGER

T1 PROTEIN - - BCL

T2 PROTEIN - - RFLAT

T3 PROTEIN - - IL-10

E1 POS-REG T1 E2 stimulates

E2 NEG-REG T3 T2 inhibition

(NEG-REG,BCL,RFLAT)

(NEG-REG,IL-10,RFLAT)

Complex Event (POS-REG,BCL,(NEG-REG,IL-10,RFLAT))

GENIA Event 
Annotation

Figure 1: Given sentence “BCL stimulates inhibition
of RFLAT by IL-10”, information extraction focuses on
classifying simple relations among entities (top), whereas
ideally we want to extract the complex event that captures
important contextual information (middle), as exempli-
fied by the GENIA event annotation (bottom).

which are expensive and time-consuming to acquire
(Zelle and Mooney, 1993; Zettlemoyer and Collins,
2005; Zettlemoyer and Collins, 2007). As a re-
sult, there has been rising interest in learning se-
mantic parsers from indirect supervision. Examples
include unsupervised approaches that leverage dis-
tributional similarity by recursive clustering (Poon
and Domingos, 2009; Poon and Domingos, 2010;
Titov and Klementiev, 2011), semi-supervised ap-
proaches that learn from dialog context (Artzi and
Zettlemoyer, 2011), grounded approaches that learn
from annotated question-answer pairs (Clarke et al.,
2010; Liang et al., 2011) or virtual worlds (Chen and
Mooney, 2011; Artzi and Zettlemoyer, 2013).

Such progress is exciting, but most applications
focus on question answering, where the semantic
parser is used to convert natural-language questions
into formal queries. In contrast, complex knowl-
edge extraction represents a relatively untapped ap-

756



(a) (b) (c)

(POS-REG,BCL,(NEG-REG,IL-10,RFLAT))

(NEG-REG,TP53,(POS-REG,BCL,IL-2))

(POS-REG,AKT2,(POS-REG,IL-4,ERBB2))

(NEG-REG,(POS-REG,BCL,IL-2),BRAF)

…
…

BCL stimulates inhibition of RFLAT by IL-10. The ability of IL-10 to block RFLAT requires BCL.

stimulates

inhibition

IL-10

BCL

RFLAT

THEMECAUSE

POS-REG

CAUSETHEME

NEG-REG

requires

BCLability

RFLAT

THEME CAUSE

POS-REG

CAUSE

THEME

NEG-REG

RAISE

IL-10

blockThe NEG-REG

RAISING

NULL

Figure 2: Grounded semantic parsing for complex knowledge extraction: (a) input database of complex events, without
textual annotation; (b) event extraction as semantic parsing; (c) a complex sentence that requires RAISING.

plication area for semantic parsing, with great po-
tential. Text with valuable information has been
undergoing rapid growth across scientific and busi-
ness disciplines alike. A prominent example is
PubMed (www.ncbi.nlm.nih.gov/pubmed), which
contains over 24 million biomedical research arti-
cles and grows by over one million each year. Re-
search on information extraction abounds, but it
tends to focus on classifying simple relations among
entities, so is incapable of extracting the preva-
lent complex knowledge with nested event struc-
tures. Figure 1 illustrates this problem with an
example sentence “BCL stimulates inhibition of
RFLAT by IL-10”. Traditional information extrac-
tion would be content with extracting two binary
relation instances (NEG-REG,BCL,RFLAT) and
(NEG-REG,IL-10,RFLAT), where NEG-REG
represents a negative regulation (i.e., inhibition).
However, the sentence also discloses important con-
textual information, i.e., BCL regulates RFLAT by
stimulating the inhibitive effect of IL-10, and like-
wise the inhibition of RFLAT by IL-10 is con-
trolled by BCL. Such context-specific knowledge
is crucial in translational medicine: imagine a tar-
geted therapy that tries to suppress RFLAT by in-
ducing either BCL or IL-10, without taking into
account their interdependency. As Figure 1 shows,
this knowledge can be represented by events with
nested structures (e.g., the THEME argument of E1
is an event E2), as exemplified by the GENIA event
extraction dataset (Kim et al., 2009).

Complex knowledge extraction can be naturally
framed as a semantic parsing problem, with the
event structure represented by a semantic parse; see

Figure 2. However, annotating example sentences
is expensive and time-consuming. GENIA is the
only corpus of its kind by far; its annotation took
years and its scope is limited to the narrow domain
of transcription in human blood cells. In contrast,
databases are usually available. For example, due
to the central importance of biological pathways in
understanding diseases and developing drug targets,
there exist many pathway databases (Schaefer et al.,
2009; Kanehisa, 2002; Cerami et al., 2011). Lim-
ited by manual curation, they are incomplete and not
up-to-date, thereby the need for automated extrac-
tion. But compared to question answering, knowl-
edge extraction can derive more leverage from such
databases via distant supervision (Craven and Kum-
lien, 1999; Mintz et al., 2009). The key insight is
that databases can be used to automatically anno-
tate sentences with a relation if the arguments of a
known instance co-occur in the sentence. This learn-
ing paradigm, however, has never been applied to
extracting nested events.

In this paper, we propose the first approach to
learn a semantic parser from a database of complex
events and unannotated text, by generalizing dis-
tant supervision to complex knowledge extraction.
The key idea is to recover the latent annotations via
EM, guided by a structured prior that favors seman-
tic parses containing known events in the database,
in the form of virtual evidence (Pearl, 1988; Subra-
manya and Bilmes, 2007). Experiments on the GE-
NIA dataset demonstrate the promise of this direc-
tion. Our GUSPEE (GroUnded Semantic Parsing
for Event Extraction) system can successfully learn
from and extract complex events, without requiring

757



textual annotations (Figure 2). Moreover, after in-
corporating prototype-driven learning using just five
example words for each event type, GUSPEE be-
comes competitive even among supervised systems,
outperforming 19 out of 24 teams that participated
in the GENIA event extraction shared task. With
significant information loss (skipping event triggers
and, most importantly, the nested event structures),
it is possible to reduce GENIA events to binary re-
lations so that existing distant-supervision methods
are applicable. Yet even in such an evaluation tai-
lored for existing methods, our system still outper-
formed them by a wide margin.

2 Related Work

Existing approaches for GENIA event extraction are
supervised methods that either used a carefully en-
gineered classification pipeline (Bjorne et al., 2009;
Quirk et al., 2011) or applied joint inference (Riedel
et al., 2009; Poon and Vanderwende, 2010; Riedel
and McCallum, 2011). Poon and Vanderwende
(2010) used a dependency-based formulation that re-
sembled our semantic parsing one, but learned from
supervised data. Classification approaches first need
to classify words into event triggers, where distant
supervision is not directly applicable.

In distant supervision (Craven and Kumlien,
1999; Mintz et al., 2009), if two entities are known
to have a binary relation in the database, their co-
occurrence in a sentence justifies labeling the in-
stance with the relation. This assumption is often
incorrect, and Riedel et al. (2010) introduced latent
variables to model the uncertainty; the model was
later improved by Hoffmann et al. (2011). GUS-
PEE generalizes this idea to structured prediction
where the latent annotations are not simple classifi-
cation decisions, but nested events. Krishnamurthy
and Mitchell (2012) and Reddy et al. (2014) took
an important step toward this direction, by learning
a semantic parser based on combinatorial categorial
grammar (CCG) from Freebase and web sentences.
However, Krishnamurthy and Mitchell (2012) still
learned from binary relations, using only simple sen-
tences (of length ten or less). Reddy et al. (2014)
learned from n-ary relations as well, yet their for-
mulation only allows relations between entities, not
relations between relations. Thus their approach

cannot represent nested events, let alone extract-
ing them. And like Krishnamurthy and Mitchell
(2012), Reddy et al. (2014) focused on simple text
and excluded sentences where entities were not de-
pendency neighbors (i.e., not directly connected in
the ungrounded graph), as well as sentences with un-
known entities. While such restrictions do not im-
pede parsing simple questions in their evaluation,
their approach is not directly applicable to com-
plex knowledge extraction. Reschke et al. (2014)
also generalized distant supervision to n-ary rela-
tions for extracting template-based events, but sim-
ilar to Reddy et al. (2014), they did not consider
nested events.

Distant supervision can be viewed as a special
case of the more general paradigm of grounded
learning from a database. Clarke et al. (2010) and
Liang et al. (2011) used the database to determine
if a candidate semantic parse would yield the anno-
tated answer, whereas distant supervision uses the
database to determine if a relation instance is con-
tained therein. Our GUSPEE system is inspired
by grounded unsupervised semantic parsing (GUSP)
(Poon, 2013) and shares a similar semantic repre-
sentation. GUSP, like most grounded learning ap-
proaches, applied to question answering and did not
leverage distant supervision. GUSPEE can thus be
viewed as an extension of GUSP to leverage distant
supervision for complex knowledge extraction.

Grounding in GUSPEE is materialized by virtual
evidence favoring semantic structures that conform
with the database. The idea of virtual evidence was
first introduced by Pearl (1988) and later applied in
several applications such as Subramanya and Bilmes
(2007). Unlike in prior work, the virtual evidence
in GUSPEE involves non-local factors (comparing a
semantic parse with complex events in the database)
and presents a major challenge to efficient learning.

Existing semantic parsers often adopt highly ex-
pressive formalisms such as CCG (Steedman, 2000).
Such formalisms are extremely powerful, but also
difficult to learn. We instead adopted a dependency-
based formalism (Poon and Domingos, 2009; Liang
et al., 2011; Poon, 2013). Moreover, following Poon
and Domingos (2009), Krishnamurthy and Mitchell
(2012), Poon (2013), we started with syntactic de-
pendency parses, and focused on annotating nodes
and edges with semantic states.

758



3 Grounded Semantic Parsing for Event
Extraction

We use the GENIA event extraction task (Kim et
al., 2009) as a representative example of complex
knowledge extraction. The goal is to identify biolog-
ical events from text, including the trigger words and
arguments (Figure 1, bottom). There are nine event
types, including simple ones such as Expression
and Transcription that can only have one THEME
argument, Binding that can have more than one
THEME argument, and regulations that can have both
THEME and CAUSE arguments. Protein annotations
are given as input.

We formulate this task as semantic parsing and
present our GUSPEE system (Figure 2). The core
of GUSPEE is a tree HMM (Section 3.1), which
extracts events from a sentence by annotating its
syntactic dependency tree with event and argument
states. In training, GUSPEE takes as input unan-
notated text and a database of complex events, and
learns the tree HMM using EM, guided by grounded
learning from the database via virtual evidence.

3.1 Problem Formulation
Let t be a syntactic dependency tree for a sentence,
with nodes ni and dependency edges di,j (nj is a
child of ni). A semantic parse of t is an assign-
ment z that maps each node to an event state and
each dependency to an argument state. The semantic
state of a protein word is fixed to that protein annota-
tion. Basic event states are the nine event types and
NULL (signifying a non-event, e.g., “The” in Figure
2 (c)). Basic argument states are THEME, CAUSE,
and NULL. Additional states will be introduced later
in Section 3.2 and 3.4.

GUSPEE models z, t by a tree HMM:

Pθ(z, t) =
∏
m

PEMIT(tm|zm, θ)·PTRANS(zm|zπ(m), θ)

where θ are the emission and transition parameters,
m ranges over the nodes and dependency edges,
π(nj) = di,j and π(di,j) = ni. Note that this formu-
lation implicitly assumes a fixed underlying directed
tree, while the words and dependencies may vary.

Semantic parsing finds the most probable seman-
tic assignment given the dependency tree:

z∗ = arg max
z

logPθ(z|t) = arg max
z

logPθ(z, t)

In training, GUSPEE takes as input a set of complex
events (database K) and syntactic dependency trees
(unannotated text T ), and maximizes the likelihood
of T augmented by virtual evidence φK(z).

θ∗ = arg max
θ

logPθ(T |K)

= arg max
θ

∑
t∈T

log
∑
z

Pθ(z, t) · φK(z)

Virtual evidence is analogous to a Bayesian prior,
but applies to variable states rather than model pa-
rameters (Subramanya and Bilmes, 2007).

3.2 Handling Syntax-Semantics Mismatch

For simple sentences such as the one in Figure 2(b),
the complex event can be represented by a seman-
tic parse using only basic states. In general, how-
ever, syntax and semantics often diverge. For ex-
ample, in Figure 2(c), “requires” triggers the top
POS-NEG event that has a THEME argument trig-
gered by “block”, but “ability” stands in between the
two; likewise for “block” and “IL-10”. Addition-
ally, mismatch could stem from errors in the syntac-
tic parse. In such cases, the correct semantic parse
can no longer be represented by basic states alone.
Following GUSP (Poon, 2013), we introduced a new
argument state RAISING which, if assigned to a de-
pendency, would require that the parent and child be
assigned the same basic event state. We also intro-
duce a corresponding RAISE version for each non-
null event state, to signify that the word derives its
basic state from RAISING of a child. RAISING is
related to but not identical with type raising in CCG
and other grammars. For simplicity, we did not use
other complex states explored in Poon (2013).

3.3 Virtual Evidence for Grounded Learning

Grounded learning in GUSPEE is attained by incor-
porating the virtual evidence φK(z), which favors
the z’s containing known events in K and penal-
izes those containing unknown events. Intuitively,
this can be accomplished by identifying events in z
and comparing them with events in K. But this is
not robust as individual events and mentions may be
fragmental and incomplete. Insisting on matching
an event in full would miss partial matches that still
convey valuable supervision. Proteins are given as

759



input and can be mapped to event arguments a pri-
ori. Matching sub-events with only one protein argu-
ment would be too noisy without direct supervision
on triggers. We thus consider matching minimum
sub-events with two protein arguments.

Specifically, we preprocessed complex events
in K to identify minimum logical forms contain-
ing two protein arguments from each complex
event, where arguments not directly leading to
either protein are skipped. For example, the
complex event in Figure 1 would generate three
sub-events: (NEG-REG,IL-10,RFLAT),
(POS-REG,BCL,(NEG-REG,-,RFLAT)),
(POS-REG,BCL,(NEG-REG,IL-10,-)),
where - signifies underspecification. We denote the
set of such sub-events as S(K).

Likewise, given a semantic parse z, for every pro-
tein pair in z, we would convert the minimum se-
mantic parse subtree spanning the two proteins into
the canonical logical form and compare it with el-
ements in S(K). If the minimum subtree contains
NULL, either in an event or argument state, it sig-
nifies a non-event and would be ignored. Other-
wise, the canonical form is derived by collapsing
RAISING states. For example, in both Figure 2 (b)
and (c), the minimum subtree spanning the proteins
IL-10 and RFLAT is converted into the same logi-
cal form of (NEG-REG,IL-10,RFLAT). We de-
note the set of such logical forms as E(z).

Formally, the virtual evidence in GUSPEE are:

φK(z) = exp
∑

e∈E(z)
σ(e,K)

where

σ(e,K) =
{
κ : e ∈ S(K)
−κ : e /∈ S(K)

In distant supervision, where z is simply a binary
relation, it is trivial to evaluate φK(z). (In fact,
the original distant supervision algorithm is exactly
equivalent to this form, with κ = ∞.) In GUSPEE,
however, z is a semantic parse and evaluating E(z)
and σ(e,K) involves a global factor that does not
decompose into local dependencies as the tree HMM
Pθ(z, t). The naive way to compute the augmented
likelihood (Section 3.1) is thus intractable.

3.4 Efficient Learning with Virtual Evidence

To render learning tractable, the key idea is to aug-
ment the local event and argument states so that they
contain sufficient information for evaluating φK(z).
Specifically, the semantic state z(ni) needs to repre-
sent not only the semantic assignment to ni (e.g., a
NEG-REG event trigger), but also the set of (possi-
bly incomplete) sub-events in the subtree under ni.
We accomplished this by representing the semantic
paths from ni to proteins in the subtree. For
example, in Figure 2 (b), the augmented state of “in-
hibition” would be (NEG-REG→THEME→RFLAT,
NEG-REG→CAUSE→IL-10). To facilitate
canonicalization and sub-event comparison, a
path containing NULL will be skipped, and
RAISING will be collapsed. E.g., in Fig-
ure 2(c), the augmented state of “ability”
would become (NEG-REG→THEME→RFLAT,
NEG-REG→CAUSE→IL-10).

With these augmented states, φK(z) decomposes
into local factors. The proteins under ni are known
a priori, as well as the children containing them. Se-
mantic paths from ni to proteins can thus be com-
puted by imposing consistency constraints for each
child. Namely, for child nj that contains protein p,
the semantic path from ni to p should result from
combining z(ni), z(di,j), and the semantic path
from nj to p. The minimum sub-events spanning
two proteins under ni, if any, can be derived from
the semantic paths in the augmented state. Note that
if both proteins come from the same child nj , the
pair needs not be considered at ni, as their minimum
spanning sub-event, if any, would be under nj and
already be factored in there.

The number of augmented states isO(sp), and the
number of sub-event evaluations isO(s·p2), where s
is the number of distinct semantic paths, and p is the
number of proteins in the subtree. Below, we show
how s, p can be constrained to reasonable ranges to
make computation efficient.

First, consider s. The number of semantic paths
is theoretically unbounded since a path can be ar-
bitrarily long. However, semantic paths contained
in a database event are bounded in length and can
be precomputed from the database (the maximum
in GENIA is four). Longer paths can be repre-
sented by a special dummy path signifying that they

760



would not match any database events. Likewise, cer-
tain sub-paths would not occur in database events.
E.g., in GENIA, simple events cannot take events
as arguments, so paths containing sub-paths such
as Expression → Transcription are also ille-
gitimate and can be represented same as the above.
We also notice that for regulation events with other
regulation events as arguments, the semantics can
be compressed into a single regulation event, e.g.,
POS-REG→NEG-REG is semantically equivalent
with NEG-REG, as the collective effect of a posi-
tive regulation on top of a negative one is negative.
Therefore, when evaulating the semantic path from
ni to a protein during dynamic programming, we
would collapse consecutive regulation events in the
child path, if any. This further reduces the length of
semantic paths to at most three (regulation - regula-
tion - simple event - protein).

Next, we notice that p is bounded to begin with,
but it could be quite large. When a sentence con-
tains many proteins (i.e., large p), it often stems
from conjunction of proteins, as in “TP53 regulates
many downstream targets such as ABCB1, AFP,
APC, ATF3, BAX”. All proteins in the conjunct
play a similar role in their respective events, such as
THEME in the above example among “ABCB1, AFP,
APC, ATF3, BAX”, and so share the same semantic
paths. Therefore, prior to learning, we preprocessed
the sentences to condense each conjunct into a sin-
gle effective protein node. We identified conjunction
by Stanford dependencies (conj ∗). In GENIA, this
reduces the maximum number of effective protein
nodes to two for the vast majority of sentences (over
90%). Both representation and evaluation are now
reasonably efficient. To further speed up learning, in
our experiments we only trained on sentences with
at most two effective protein nodes, as this already
performed quite well. Training on GENIA took 1.5
hours and semantic parsing of a sentence took less
than a second (with one i7 core at 2.4 GHz).

Unlike RAISING, the augmented states intro-
duced in this section are specific to GENIA events.
However, the rules to canonicalize states are general
and can potentially be adapted to other domains. An
alternative strategy to combat state explosion is by
embedding the discrete states in a low-dimensional
vector space (Socher et al., 2013), which is a direc-
tion for future research.

3.5 Features
The GUSPEE model uses log-linear models for the
emission and transition probabilities and trains using
feature-rich EM (Berg-Kirkpatrick et al., 2010). The
features are:
Word emission I[lemma = l, zm = n];
Dependency emission I[dependency = d, zm =
e] where e /∈ {NULL, RAISE};
Transition I[zm = a, zπ(m) = b] where a, b /∈
{NULL, RAISE}.

To modulate the model complexity, GUSPEE
imposes a standard L2 prior on the weights, and
includes the following features with fixed weights:

• WNULL: apply to NULL states;
• WRAISE−P : apply to protein RAISING;
• WRAISE−E : apply to event RAISING.
The advantage of a feature-rich representation is

flexibility in feature engineering. Here, we ex-
cluded NULL and RAISE in dependency emission
and transition features, and regulated them sepa-
rately to enable parameter tying for better general-
ization.

4 Experiments

4.1 Evaluation on GENIA Event Extraction
In principle, we can learn GUSPEE from any path-
way database. However, evaluation is challenging as
these databases do not contain textual annotations.
Prior work on distant supervision resorted to sam-
pling and annotating new extractions. This is effec-
tive for comparing among distant-supervision sys-
tems, but it cannot be used to compare them with
supervised learning. Moreover, as annotation is con-
ducted by the authors or crowdsourcing, consistency
and quality are hard to control.

We thus adopted a novel approach to evaluation
by simulating a grounded learning scenario using
the GENIA event extraction dataset (Kim et al.,
2009). Specifically, we generated a set of complex
events from the annotations of training sentences as
the database. The annotations were discarded after-
wards and GUSPEE learned from the database and
unannotated text alone. The learned model was then
applied to semantic parsing of test sentences and
evaluated on event precision, recall, and F1. This

761



Event Type Rec. Prec. F1
Expression 50.8 41.9 45.9

Transcription 18.3 14.0 15.9
Catabolism 0 0 0

Phosphorylation 36.2 43.6 39.5
Localization 0 0 0

Binding 24.0 42.6 30.7
Regulation 2.5 5.0 3.3

Positive regulation 11.4 21.4 14.9
Negative regulation 4.4 16.4 6.9

Total Event F1 19.1 29.4 23.2

Table 1: GENIA event extraction results of GUSPEE

evaluation methodology enables us to assess the true
accuracy and compare head-to-head with supervised
methods.

GENIA contains 800 abstracts for training and
150 for development. It also has a test set, but its
annotation is not made public. Therefore, we used
the training set for grounded learning and develop-
ment, and reserved the development set for testing.
The majority events are Regulation (including
Positive regulation, Negative regulation).
See Kim et al. (2009) for details. We processed
all sentences using SPLAT (Quirk et al., 2012),
to conduct tokenization, part-of-speech tagging,
and constituency parsing. We then postprocessed
the parses to obtain Stanford dependencies (de
Marneffe et al., 2006). During development on the
training data, we found the following parameters
(Section 3) to perform quite well and used them in
all subsequent experiments: κ = 20, WNULL = 4,
WRAISE−P = 2, WRAISE−E = −6, L2 prior = 0.1.
Interestingly, we found that encouraging protein
RAISING is beneficial, which probably stems from
the fact that proteins are often separated from event
triggers by noun modifiers, such as “the BCL gene”,
“IL-10 protein”.

Table 1 shows GUSPEE’s results on GENIA
event extraction. Note that this event-based eval-
uation is rather stringent, as it considers an event
incorrect if one of its argument events is not com-
pletely correct, thus an incorrect event will render
all its upstream events incorrect. See Kim et al.
(2009) for details. For comparison, Table 2 shows
the results of MSR11, a state-of-the-art supervised
system. MSR11 also provides a upper bound for the
supervised version of GUSPEE, as the latter is much
less engineered.

Event Type Rec. Prec. F1
Expression 76.4 81.5 78.8

Transcription 49.4 73.6 59.1
Catabolism 65.6 80.0 74.4

Phosphorylation 73.9 84.5 78.9
Localization 74.6 75.8 75.2

Binding 48.0 50.9 49.4
Regulation 32.5 47.1 38.6

Positive regulation 38.7 51.7 44.3
Negative regulation 35.9 54.9 43.9

Total Event F1 50.2 62.6 55.7

Table 2: GENIA event extraction results of state-of-the-
art supervised system MSR11 (Quirk et al., 2011).

Not surprisingly, grounded learning with GUS-
PEE still lags behind supervised learning. MSR11
used a rich set of features, including POS tags, linear
and dependency n-grams, etc. Also, it is expected
that indirect supervision do not provide as effective
signals as direct supervision. However, the com-
parison reveals a particularly interesting contrast.
Event types such as Expression, Catabolism,
Phosphorylation, and Localization are rela-
tively easy, yet GUSPEE performed rather poorly
on them. Simple events do not admit multiple ar-
guments, so they appear less often in the virtual evi-
dence, and grounded learning has difficulty learning
these event types, especially their triggers. In light
of this, it’s actually remarkable that GUSPEE still
learned a substantial portion of them.

4.2 Prototype-Driven Learning

While full-blown annotations are undoubtedly ex-
pensive and time-consuming to generate, it is rather
easy for a domain expert to provide a few trig-
ger words per event type, such as “expression”,
“expressed” for Expression. This motivates us
to explore prototype-driven learning (Haghighi and
Klein, 2006) in combination with grounded learn-
ing. Specifically, we simulated expert selection by
picking the top five most frequent trigger words
for each event type from training data. We then
augmented grounded learning in GUSPEE by in-
corporating word emission features for each proto-
type word and the corresponding event state, e.g.,
I[lemma = express, zm = Expression]. The
weights are fixed to a large number (five in our case).
Table 3 shows the results with prototypes, which
improved substantially. Not surprisingly, simple

762



Event Type Rec. Prec. F1
Expression 55.3 88.3 68.0

Transcription 50.0 39.1 43.9
Catabolism 52.4 100.0 68.9

Phosphorylation 61.7 82.9 70.7
Localization 52.8 100.0 69.1

Binding 20.2 92.7 33.2
Regulation 24.1 64.0 35.0

Positive regulation 17.4 63.8 27.4
Negative regulation 8.4 52.8 14.5

Total Event F1 27.9 72.2 40.2

Table 3: GENIA event extraction results of GUSPEE
with five prototype words per event type

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

30

35

40

GENIA Event Extraction F1

Incomplete Database

Incomplete Text

Figure 3: GENIA Event F1 of GUSPEE with prototypes,
using incomplete database or text.

events such as Catabolism benefited the most from
prototypes, as they have fewer variations in trig-
gers. While the F1 score 40.2 still lags behind
the supervised state of the art, it would have been
competitive compared to the 24 teams participat-
ing in the original shared task, outperforming 19 of
them (the top 5th system scored an F1 of 40.5, see
www.nactem.ac.uk/tsujii/GENIA/SharedTask
/results/results-master.html, Task 1).

4.3 Database-Text Mismatch

In our simulation of grounded learning, every event
in the database is mentioned in some text and vice
versa. In practice, however, there is usually a mis-
match between database and text: the unannotated
text generally contains more facts than are already
populated in the database; conversely, a database
fact may not be explicitly mentioned in the text.

The GENIA dataset offers an excellent opportu-
nity to study the robustness of grounded learning
in light of such mismatch. Specifically, we simu-

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
15

20

25

30

35

40
GENIA Event Extraction F1

10% Database

50% Database

Figure 4: GENIA Event F1 of GUSPEE with prototypes,
using a fraction of database and increasing amount of
unannotated text.

lated a grounded learning scenario with an incom-
plete database by populating events from the anno-
tations of a random fraction of training text, and then
learning GUSPEE with this database and all training
text. Likewise, we simulated a scenario with incom-
plete text using the training event database in full,
but only a fraction of unannotated text.

Figure 3 shows the results of GUSPEE with proto-
types as the fraction varies between 0.1 and 1, by av-
eraging five random runs. In both scenarios, the F1
score degrades smoothly as the fraction gets smaller.
Precision stays roughly the same while recall grad-
ually degrades (curves not shown). This shows that
GUSPEE is reasonably robust. Not surprisingly, the
degradation is steeper with incomplete database than
with incomplete text.

To further investigate the effect of unannotated
text, we also randomly sampled a fraction of
database events for grounded supervision, and eval-
uated GUSPEE with increasing amounts of unanno-
tated text. Figure 4 shows the results by averaging
nine random runs. The F1 increases steadily with
additional unannotated text, mainly due to rising re-
call (curves not shown). This suggests that GUSPEE
could potentially benefit from more unannotated text
and is reasonably robust even when some text is not
relevant to the available events. As expected, more
grounded supervision (50% vs. 10% database) led
to substantially better F1 and lower variation.

4.4 Error Analysis

Upon manual inspection, we found that syntactic er-
rors considerably affect performance. Poon (2013)

763



introduced complex states such as Sinking and
Implicit to combat syntax-semantics mismatch,
which could also be incorporated into GUSPEE.
Improving syntactic parsing, either separately by
adapting to the biomedical domain, or jointly along
with semantic parsing, is another important future
direction. GUSPEE achieved better precision than
recall, especially when learning with prototypes, and
might benefit from augmenting prototypes by distri-
butional similarity (Haghighi and Klein, 2006).

4.5 Comparison with Existing Distant
Supervision Approaches

Existing distant supervision approaches are not di-
rectly applicable to extracting nested events. How-
ever, we can convert the extraction task into clas-
sifying minimum sub-events between proteins, for
which existing methods can be applied. Specifically,
we used binary sub-events in S(K) (Section 3.3) for
distant supervision, and evaluated on classifying test
sentences. This would enable an interesting compar-
ison with GUSPEE, as the latter also derived indirect
supervision from S(K) alone. Textual annotations
of triggers and nested event structures in GUSPEE
output were ignored, and prototypes were not used
to enable a fair comparison. For distant supervision,
we used the state-of-the-art MultiR system (Hoff-
mann et al., 2011) with standard lexical and syntac-
tic features (Mintz et al., 2009). MultiR can be used
for supervised learning by fixing relations according
to the sentence-level annotations, which provides a
supervised upper bound.

Table 4 shows the results. GUSPEE outperformed
MultiR by a wide margin, improving F1 by 24%.
Surprisingly, GUSPEE even surpassed the super-
vised upper bound of MultiR. This suggests that
our semantic parsing formulation not only is supe-
rior in representation power, but also facilitates bet-
ter learning. We also experimented with sharing
parameters among related sub-events in a MultiR-
like model, but it did not improve the performance.
Upon close inspection, we found that MultiR mainly
scored on Binding events and failed almostly en-
tirely on the more difficult Regulation events.
GUSPEE was able to extract Regulation events,
but incurred some precision errors.

Method Rec. Prec. F1 (Class.)
MultiR 11.2 21.7 14.8

MultiR (Super.) 12.1 24.4 16.2
GUSPEE 22.9 15.3 18.4

Table 4: Classification results on GENIA when events are
simplified to binary relations for distant supervision.

5 Summary

We generalize distant supervision to complex
knowledge extraction and propose the first approach
to learn a semantic parser from a database of nested
events and unannotated text. Experiments on GE-
NIA event extraction showed that our GUSPEE sys-
tem could learn from and extract such complex
events, and was competitive even among supervised
systems after incorporating a few easily-obtainable
prototype event trigger words.

Future directions include: PubMed-scale path-
way extraction; application to other domains; in-
corporating additional complex states to address
syntax-semantics mismatch; learning vector-space
representations for complex states; joint syntactic-
semantic parsing; incorporating reasoning and other
sources of indirect supervision.

References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping

semantic parsers from conversations. In Proceedings
of the 2011 Conference on Empirical Methods in Nat-
ural Language Processing.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1:49–62.

Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein.
2010. Painless unsupervised learning with features. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.

Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP Workshop.

Ethan G Cerami, Benjamin E Gross, Emek Demir, Igor
Rodchenkov, Özgün Babur, Nadia Anwar, Nikolaus
Schultz, Gary D Bader, and Chris Sander. 2011. Path-
way commons, a web resource for biological path-
way data. Nucleic acids research, 39(suppl 1):D685–
D690.

764



David Chen and Ray Mooney. 2011. Learning to in-
terpret natural language navigation instructions from
observations. In Proceedings of the Twenty Sixth Na-
tional Conference on Artificial Intelligence.

James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from
world’s response. In Proceedings of the 2010 Con-
ference on Natural Language Learning.

M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge bases by extracting information from
text sources. In Proceedings of the 7th International
Conference on Intelligent Systems for Molecular Biol-
ogy, pages 77–86. AAAI Press.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation, pages 449–454,
Genoa, Italy. ELRA.

Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of the
Forty Fourth Annual Meeting of the Association for
Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Forty
Ninth Annual Meeting of the Association for Compu-
tational Linguistics.

Minoru Kanehisa. 2002. The kegg database. Silico Sim-
ulation of Biological Processes, 247:91–103.

Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Junichi Tsujii. 2009. Overview of
BioNLP-09 Shared Task on event extraction. In Pro-
ceedings of the BioNLP Workshop.

Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
EMNLP-12.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the Forty Ninth Annual Meeting of
the Association for Computational Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Forty Seventh
Annual Meeting of the Association for Computational
Linguistics.

Raymond J. Mooney. 2007. Learning for semantic pars-
ing. In Proceedings of the Eighth International Con-
ference on Computational Linguistics and Intelligent
Text Processing, pages 311–324, Mexico City, Mex-
ico. Springer.

J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann, San Francisco, CA.

Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10, Singapore. ACL.

Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontological induction from text. In Proceedings
of the Forty Eighth Annual Meeting of the Association
for Computational Linguistics, pages 296–305, Upp-
sala, Sweden. ACL.

Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical liter-
ature. In Proceedings of Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.

Hoifung Poon. 2013. Grounded unsupervised semantic
parsing. In Proceedings of the Fifty First Annual Meet-
ing of the Association for Computational Linguistics.

Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. Msr-nlp entry in bionlp
shared task 2011. In Proc. BioNLP.

Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wentau
Yih, and Lucy Vanderwende. 2012. MSR SPLAT, a
language analysis toolkit. In Proceedings of NAACL
HLT Demonstration Session.

Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-answer
pairs. Transactions of the Association for Computa-
tional Linguistics.

Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,
Christopher D Manning, and Daniel Jurafsky. 2014.
Event extraction using distant supervision. In Lan-
guage Resources and Evaluation Conference (LREC).

Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.

Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proc. BioNLP.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteen European
Conference on Machine Learning.

Carl F. Schaefer, Kira Anthony, Shiva Krupa, Jeffrey
Buchoff, Matthew Day, Timo Hannay, and Ken-
neth H. Buetow. 2009. PID: The pathway interaction
database. Nucleic Acids Research, 37:674–679.

765



Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the Fifty
First Annual Meeting of the Association for Computa-
tional Linguistics.

Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.

Amarnag Subramanya and Jeff Bilmes. 2007. Virtual
evidence for training speech recognizers using par-
tially labeled data. In Proceedings of Human Lan-
guage Technologies: The 2007 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics.

Ivan Titov and Alexandre Klementiev. 2011. A bayesian
model for unsupervised semantic parsing. In Proceed-
ings of the Forty Ninth Annual Meeting of the Associ-
ation for Computational Linguistics.

John M. Zelle and Ray Mooney. 1993. Learning seman-
tic grammars with constructive inductive logic pro-
gramming. In Proceedings of the Eleventh National
Conference on Artificial Intelligence.

Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammers. In
Proceedings of the Twenty First Conference on Un-
certainty in Artificial Intelligence, pages 658–666, Ed-
inburgh, Scotland. AUAI Press.

Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed ccg grammars for parsing to logical
form. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.

766


