










































An Analysis of Memory-based Processing Costs using Incremental Deep Syntactic Dependency Parsing


Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37–46,
Sofia, Bulgaria, August 8, 2013. c©2013 Association for Computational Linguistics

An Analysis of Memory-based Processing Costs using Incremental
Deep Syntactic Dependency Parsing∗

Marten van Schijndel

The Ohio State University

vanschm@ling.osu.edu

Luan Nguyen

University of Minnesota

lnguyen@cs.umn.edu

William Schuler

The Ohio State University

schuler@ling.osu.edu

Abstract

Reading experiments using naturalistic

stimuli have shown unanticipated facili-

tations for completing center embeddings

when frequency effects are factored out.

To eliminate possible confounds due to

surface structure, this paper introduces a

processing model based on deep syntac-

tic dependencies. Results on eye-tracking

data indicate that completing deep syntac-

tic embeddings yields significantly more

facilitation than completing surface em-

beddings.

1 Introduction

Self-paced reading and eye-tracking experiments

have often been used to support theories about

inhibitory effects of working memory operations

in sentence processing (Just and Carpenter, 1992;

Gibson, 2000; Lewis and Vasishth, 2005), but it

is possible that many of these effects can be ex-

plained by frequency (Jurafsky, 1996; Hale, 2001;

Karlsson, 2007). Experiments on large naturalis-

tic text corpora (Demberg and Keller, 2008; Wu et

al., 2010; van Schijndel and Schuler, 2013) have

shown significant memory effects at the ends of

center embeddings when frequency measures have

been included as separate factors, but these mem-

ory effects have been facilitatory rather than in-

hibitory.

Some of the memory-based measures that pro-

duce these facilitatory effects (Wu et al., 2010; van

Schijndel and Schuler, 2013) are defined in terms

of initiation and integration of connected compo-

nents of syntactic structure,1 with the presumption

∗*Thanks to Micha Elsner and three anonymous review-
ers for their feedback. This work was funded by an Ohio State
University Department of Linguistics Targeted Investment
for Excellence (TIE) grant for collaborative interdisciplinary
projects conducted during the academic year 2012–13.

1Graph theoretically, the set of connected components

that referents that belong to the same connected

component may cue one another using content-

based features, while those that do not must rely

on noisier temporal features that just encode how

recently a referent was accessed. These measures,

based on left-corner parsing processes (Johnson-

Laird, 1983; Abney and Johnson, 1991), abstract

counts of unsatisfied dependencies from noun or

verb referents (Gibson, 2000) to cover all syntactic

dependencies, motivated by observations of Dem-

berg and Keller (2008) and Kwon et al. (2010) of

the inadequacies of Gibson’s narrower measure.

But these experiments use naturalistic stimuli

without constrained manipulations and therefore

might be susceptible to confounds. It is possible

that the purely phrase-structure-based connected

components used previously may ignore some in-

tegration costs associated with filler-gap construc-

tions, making them an unsuitable generalization of

Gibson-style dependencies. It is also possible that

the facilitatory effect for integration operations in

naturally-occurring stimuli may be driven by syn-

tactic center embeddings that arise from modifiers

(e.g. The CEO sold [[the shares] of the com-

pany]), which do not require any dependencies

to be deferred, but which might be systematically

under-predicted by frequency measures, produc-

ing a confound with memory measures when fre-

quency measures are residualized out.

In order to eliminate possible confounds due to

exclusion of unbounded dependencies in filler-gap

constructions, this paper evaluates a processing

model that calculates connected components on

deep syntactic dependency structures rather than

surface phrase structure trees. This model ac-

counts unattached fillers and gaps as belonging

to separate connected components, and therefore

performs additional initiation and integration op-

of a graph 〈V, E〉 is the set of maximal subsets of
it {〈V1, E1〉, 〈V2, E2〉, ...} such that any pair of vertices in
each Vi can be connected by edges in the corresponding Ei.

37



a) Noun Phrase

Relative Clause

Sentence w. Gap

Verb Phrase w. Gap

Sentence w. Gap

millionsstole

say

officials

who

Noun Phrase

personthe

b)

i1

i2 i3

i4

i5

i6

i7
1

2

1

2

1

1
say

officials

stole

who

person

millionsthe
0

0

0

0

0

00

Figure 1: Graphical representation of (a) a single

connected component of surface syntactic phrase

structure corresponding to (b) two connected com-

ponents of deep syntactic dependency structure for

the noun phrase the person who officials say stole

millions, prior to the word say. Connections es-

tablished prior to the word say are shown in black;

subsequent connections are shown in gray.

erations in filler-gap constructions as hypothesized

by Gibson (2000) and others. Then, in order to

control for possible confounds due to modifier-

induced center embedding, this refined model is

applied to two partitions of an eye-tracking cor-

pus (Kennedy et al., 2003): one consisting of sen-

tences containing only non-modifier center em-

beddings, in which dependencies are deferred, and

the other consisting of sentences containing no

center embeddings or containing center embed-

dings arising from attachment of final modifiers,

in which no dependencies are deferred. Processing

this partitioned corpus with deep syntactic con-

nected components reveals a significant increase

in facilitation in the non-modifier partition, which

lends credibility to the observation of negative

integration cost in processing naturally-occurring

sentences.

2 Connected Components

The experiments described in this paper evalu-

ate whether inhibition and facilitation in reading

correlate with operations in a hierarchic sequen-

tial prediction model that initiate and integrate

connected components of hypothesized syntactic

structure during incremental parsing. The model

used in these experiments refines previous con-

nected component models by allowing fillers and

gaps to occur in separate connected components

of a deep syntactic dependency graph (Mel’čuk,

1988; Kintsch, 1988), even when they belong to

the same connected component when defined on

surface structure.

For example, the surface syntactic phrase struc-

ture and deep syntactic dependency structure for

the noun phrase the person who officials say stole

millions are shown in Figure 1.2 Notice that af-

ter the word officials, there is only one connected

component of surface syntactic phrase structure

(from the root noun phrase to the verb phrase with

gap), but two disjoint connected components of

deep syntactic dependency structure (one ending

at i3, and another at i5). Only the deep syntactic

dependency structure corresponds to familiar (Just

and Carpenter, 1992; Gibson, 1998) notions of

how memory is used to store deferred dependen-

cies in filler-gap constructions. The next section

will describe a generalized categorial grammar,

which (i) can be viewed as context-free, to seed a

latent-variable probabilistic context-free grammar

to accurately derive parses of filler-gap construc-

tions, and (ii) can be viewed as a deep syntactic

dependency grammar, defining dependencies for

connected components in terms of function appli-

cations.

3 Generalized Categorial Grammar

In order to evaluate memory effects for hypothe-

sizing unbounded dependencies between referents

of fillers and referents of clauses containing gaps,

a memory-based processor must define connected

components in terms of deep syntactic dependen-

cies (including unbounded dependencies) rather

than in terms of surface syntactic phrase structure

trees. To do this, at least some phrase structure

edges must be removed from the set of connec-

tions that define a connected component.

Because these unbounded dependencies are not

represented locally in the original Treebank for-

mat, probabilities for operations on these modified

2Following Mel’čuk (1988) and Kintsch (1988),
the graphical dependency structure adopted here uses
positionally-defined labels (‘0’ for the predicate label, ‘1’
for the first argument ahead of a predicate, ‘2’ for the last
argument behind, etc.) but includes unbounded dependen-
cies between referents of fillers and referents of clauses
containing gaps. It is assumed that semantically-labeled
structures would be isomorphic to the structures defined
here, but would generalize across alternations such as active
and passive constructions, for example.

38



connected components are trained on a corpus an-

notated with generalized categorial grammar de-

pendencies for ‘gap’ arguments at all categories

that subsume a gap (Nguyen et al., 2012). This

representation is similar to the HPSG-like repre-

sentation used by Hale (2001) and Lewis and Va-

sishth (2005), but has a naturally-defined depen-

dency structure on which to calculate connected

components. This generalized categorial grammar

is then used to identify the first sign that introduces

a gap, at which point a deep syntactic connected

component containing the filler can be encoded

(stored), and a separate deep syntactic connected

component for a clause containing a gap can be

initiated.

A generalized categorial grammar (Bach, 1981)

consists of a set U of primitive category types;

a set O of type-constructing operators allowing a

recursive definition of a set of categories C =def
U ∪ (C × O × C); a set X of vocabulary items;

a mapping M from vocabulary items in X to se-

mantic functions with category types in C; and

a set R of inference rules for deriving functions

with category types in C from other functions with

category types in C. Nguyen et al. (2012) use

primitive category types for clause types (e.g. V

for finite verb-headed clause, N for noun phrase

or nominal clause, D for determiners and pos-

sessive clauses, etc.), and use the generalized set

of type-constructing operators to characterize not

only function application dependencies between

arguments immediately ahead of and behind a

functor (-a and -b, corresponding to ‘\’ and ‘/’ in

Ajdukiewicz-Bar-Hillel categorial grammars), but

also long-distance dependencies between fillers

and categories subsuming gaps (-g), dependencies

between relative pronouns and antecedent modif-

icands of relative clauses (-r), and dependencies

between interrogative pronouns and their argu-

ments (-i), which remain unsatisfied in derivations

but function to distinguish categories for content

and polar questions. A lexicon can then be de-

fined in M to introduce lexical dependencies and

obligatory pronominal dependencies using num-

bered functions for predicates and deep syntactic

arguments, for example:

the⇒ (λi (0 i)=the) : D

person⇒ (λi (0 i)=person) : N-aD

who⇒ (λk i (0 i)=who ∧ (1 i)=k) : N-rN

officials⇒ (λi (0 i)=officials) : N

the

D

person

N-aD

N
Aa

who

N-rN

officials

N

say

V-aN-bV

stole

V-aN-bN

millions

N

V-aN
Ae

V-gN
Ga

V-aN-gN
Ag

V-gN
Ac

V-rN
Fc

N
R

Figure 2: Example categorization of the noun

phrase the person who officials say stole millions.

say⇒ (λi (0 i)=say) : V-aN-bV

stole⇒ (λi (0 i)=stole) : V-aN-bN

millions⇒ (λi (0 i)=millions) : N

Inference rules in R are then defined to com-

pose arguments and modifiers and propagate gaps.

Arguments g of type d ahead of functors h of

type c-ad are composed by passing non-local de-

pendencies ψ ∈ {-g, -i, -r} × C from premises to

conclusion in all combinations:

g:d h: c-ad ⇒ ( fc-ad g h): c (Aa)

g:dψ h: c-ad ⇒ λk ( fc-ad (g k) h): cψ (Ab)

g:d h: c-adψ⇒ λk ( fc-ad g (h k)): cψ (Ac)

g:dψ h: c-adψ⇒ λk ( fc-ad (g k) (h k)): cψ (Ad)

Similar rules compose arguments behind functors:

g: c-bd h:d ⇒ ( fc-bd g h): c (Ae)

g: c-bdψ h:d ⇒ λk ( fc-bd (g k) h): cψ (Af)

g: c-bd h:dψ⇒ λk ( fc-bd g (h k)): cψ (Ag)

g: c-bdψ h:dψ⇒ λk ( fc-bd (g k) (h k)): cψ (Ah)

These rules use composition functions fc-ad
and fc-bd for initial and final arguments, which de-

fine dependency edges numbered v from referents

of predicate functors i to referents of arguments j,

where v is the number of unsatisfied arguments

ϕ1...ϕv ∈ {-a, -b} ×C in a category label:

fuϕ1..v−1-ac
def
= λg h i ∃ j (v i)= j ∧ (g j) ∧ (h i) (1a)

fuϕ1..v−1-bc
def
= λg h i ∃ j (v i)= j ∧ (g i) ∧ (h j) (1b)

R also contains inference rules to compose mod-

ifier functors g of type u-ad ahead of modifi-

cands h of type d:

g: u-ad h:c⇒ ( fIM g h):c (Ma)

g: u-adψ h:c⇒ λk ( fIM (g k) h):cψ (Mb)

g: u-ad h:cψ⇒ λk ( fIM g (h k)):cψ (Mc)

39



∃i1 j1.. iℓ jℓ ... ∧ (g
ℓ:c/d { jℓ} iℓ) xt

∃i1 j1.. iℓ ... ∧ ((g
ℓ f ):c iℓ)

xt ⇒ f :d (–Fa)

∃i1 j1.. iℓ jℓ ... ∧ (g
ℓ:c/d { jℓ} iℓ) xt

∃i1 j1.. iℓ jℓiℓ+1 ... ∧ (g
ℓ:c/d { jℓ} iℓ) ∧ ( f :e iℓ+1)

xt ⇒ f :e (+Fa)

∃i1 j1.. iℓ−1 jℓ−1iℓ ... ∧ (g
ℓ:d iℓ)

∃i1 j1.. iℓ jℓ ... ∧ (( f g
ℓ):c/e { jℓ} iℓ)







































g:d h:e⇒ ( f g h):c or

g:d h:e⇒ λk( f (g k) h):c or

g:d h:e⇒ λk( f g (h k)):c or

g:d h:e⇒ λk( f (g k) (h k)):c

(–La)

∃i1 j1.. iℓ−1 jℓ−1iℓ ... ∧ (g
ℓ−1:a/c { jℓ−1} iℓ−1) ∧ (gℓ:d iℓ)

∃i1 j1.. iℓ−1 jℓ−1 ... ∧ (g
ℓ−1 ◦ ( f gℓ):a/e { jℓ−1} iℓ−1)







































g:d h:e⇒ ( f g h):c or

g:d h:e⇒ λk( f (g k) h):c or

g:d h:e⇒ λk( f g (h k)):c or

g:d h:e⇒ λk( f (g k) (h k)):c

(+La)

Figure 3: Basic processing productions of a right-corner parser.

g: u-adψ h:cψ⇒ λk ( fIM (g k) (h k)):cψ (Md)

or for modifier functors behind a modificand:

g:c h: u-ad ⇒ ( fFM g h):c (Me)

g:cψ h: u-ad ⇒ λk ( fFM (g k) h):cψ (Mf)

g:c h: u-adψ⇒ λk ( fFM g (h k)):cψ (Mg)

g:cψ h: u-adψ⇒ λk ( fFM (g k) (h k)):cψ (Mh)

These rules use composition functions fIM and fFM
for initial and final modifiers, which define depen-

dency edges numbered ‘1’ from referents of mod-

ifier functors i to referents of modificands j:

fIM
def
= λg h j ∃i (1 i)= j ∧ (g i) ∧ (h j) (2a)

fFM
def
= λg h j ∃i (1 i)= j ∧ (g j) ∧ (h i) (2b)

R also contains inference rules for hypothesiz-

ing gaps -gd for arguments and modifiers:3

g: c-ad ⇒ λk ( fc-ad {k} g): c-gd (Ga)

g: c-bd ⇒ λk ( fc-ad {k} g): c-gd (Gb)

g:c⇒ λk ( fIM {k} g):c-gd (Gc)

and for attaching fillers e, d-re, d-ie as gaps -gd:

g:e h: c-gd ⇒ λi ∃ j (g i) ∧ (h i j):e (Fa)

g:d-re h: c-gd ⇒ λk j ∃i (g k i) ∧ (h i j): c-re (Fb)

g:d-ie h: c-gd ⇒ λk j ∃i (g k i) ∧ (h i j): c-ie (Fc)

3Since these unary inferences perform no explicit compo-
sition, they are defined to use only initial versions composi-
tion functions fc-ad and fIM.

and for attaching modificands as antecedents of

relative pronouns:

g:e h:c-rd ⇒ λi ∃ j (g i) ∧ (h i j):e (R)

An example derivation of the noun phrase the per-

son who officials say stole millions using these

rules is shown in Figure 2. The semantic expres-

sion produced by this derivation consists of a con-

junction of terms defining the edges in the graph

shown in Figure 1b.

This GCG formulation captures many of the in-

sights of the HPSG-like context-free filler-gap no-

tation used by Hale (2001) or Lewis and Vasishth

(2005): inference rules with adjacent premises can

be cast as context-free grammars and weighted us-

ing probabilities, which allow experiments to cal-

culate frequency measures for syntactic construc-

tions. Applying a latent variable PCFG trainer

(Petrov et al., 2006) to this formulation was shown

to yield state-of-the-art accuracy for recovery of

unbounded dependencies (Nguyen et al., 2012).

Moreover, the functor-argument dependencies in

a GCG define deep syntactic dependency graphs

for all derivations, which can be used in incremen-

tal parsing to calculate connected components for

memory-based measures.

4 Incremental Processing

In order to obtain measures of memory opera-

tions used in incremental processing, these GCG

inference rules are combined into a set of parser

40



∃i1 j1.. in jn.. iℓ jℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ:c/d { jℓ} iℓ) xt

∃i1 j1.. in jn.. iℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ ((gℓ( f ′{ jn} f )):c iℓ)

xt ⇒ λk( f
′{k} f ):d

(–Fb)

∃i1 j1.. in jn.. iℓ jℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ:c/d { jℓ} iℓ) xt

∃i1 j1.. in jn.. iℓ jℓiℓ+1 ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ:c/d { jℓ} iℓ) ∧ (( f ′{ jn} f ):e iℓ+1)

xt ⇒ λk( f
′{k} f ):e

(+Fb)

∃i1 j1.. in jn.. iℓ−1 jℓ−1iℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ:d iℓ)

∃i1 j1.. in jn.. iℓ jℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (( f gℓ) ◦ ( f ′{ jn}):cψ/e { jℓ} iℓ)

g:d h:e⇒ λk( f g ( f
′{k} h)):cψ (–Lb)

∃i1 j1.. in jn.. iℓ−1 jℓ−1iℓ ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ−1:a/cψ { jℓ−1} iℓ−1) ∧ (gℓ:d iℓ)

∃i1 j1.. in jn.. iℓ−1 jℓ−1 ... ∧ (g
n:y/zψ { jn} in) ∧ ... ∧ (gℓ−1 ◦ ( f gℓ) ◦ ( f ′{ jn}):a/e { jℓ−1} iℓ−1)

g:d h:e⇒ λk( f g ( f
′{k} h)):cψ (+Lb)

Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.

productions, similar to those of the ‘right corner’

parser of van Schijndel and Schuler (2013), ex-

cept that instead of recognizing shallow hierarchi-

cal sequences of connected components of surface

structure, the parser recognizes shallow hierarchi-

cal sequences of connected components of deep

syntactic dependencies. This parser exploits the

observation (van Schijndel et al., in press) that left-

corner parsers and their variants do not need to ini-

tiate or integrate more than one connected compo-

nent at each word. These two operations are then

augmented with rules to introduce fillers and at-

tach fillers as gaps.

This parser is defined on incomplete connected

component states which consist of an active sign

(with a semantic referent and syntactic form or

category) lacking an awaited sign (also with a ref-

erent and category) yet to come. Semantic func-

tions of active and awaited signs are simplified to

denote only sets of referents, with gap arguments

(λk) stripped off and handled by separate con-

nected components. Incomplete connected com-

ponents, therefore, always denote semantic func-

tions from sets of referents to sets of referents.

This paper will notate semantic functions of

connected components using variables g and h, in-

complete connected component categories as c/d

(consisting of an active sign of category c and an

awaited sign of category d), and associations be-

tween them as g:c/d. The semantic representa-

tion used here is simply a deep syntactic depen-

dency structure, so a connected component func-

tion is satisfied if it holds for some output ref-

erent i given input referent j. This can be no-

tated ∃i j (g:c/d { j} i), where the set { j} is equiva-

lent to (λ j′ j
′= j). Connected component functions

that have a common referent j can then be com-

posed into larger connected components:4

∃i jk (g { j} i) ∧ (h {k} j) ⇔ ∃i j (g◦h {k} i) (3)

Hierarchies of ℓ connected compo-

nents can be represented as conjunctions:

∃i1 j1... iℓ jℓ (g
1:c1/d1 { j1} i1) ∧ ... ∧ (gℓ:cℓ/dℓ { jℓ} iℓ).

This allows constraints such as unbounded depen-

dencies between referents of fillers and referents

of clauses containing gaps to be specified across

connected components by simply plugging vari-

ables for filler referents into argument positions

for gaps.

A nondeterministic incremental parser can now

be defined as a deductive system, given an input

sequence consisting of an initial connected com-

ponent state of category T/T, corresponding to an

existing discourse context, followed by a sequence

of observations x1, x2, . . . , processed in time order.

As each xt is encountered, it is connected to an ex-

isting connected component or it introduces a new

disjoint component using the productions shown

in Figures 3, 4, and 5.

4These are connected components of dependency struc-
ture resulting from one or more composition functions being
composed, with each function’s output as the previous func-
tion’s second argument. This uses a standard definition of
function composition: (( f ◦ g) x) = ( f (g x)).

41



∃i1 j1.. iℓ−1 jℓ−1iℓ ... ∧ (g
ℓ:d iℓ)

∃i1 j1.. iℓ jℓ ... ∧ (( f g
ℓ) ◦ (λh k i (h k)):a/eψ { jℓ} iℓ)

g:d h:eψ⇒ ( f g h):c (–Lc)

∃i1 j1.. iℓ−1 jℓ−1iℓ ... ∧ (g
ℓ−1:a/c { jℓ−1} iℓ−1) ∧ (gℓ:d iℓ)

∃i1 j1.. iℓ−1 jℓ−1 ... ∧ (g
ℓ−1 ◦ ( f gℓ) ◦ (λh k i (h k)):a/eψ { jℓ−1} iℓ−1)

g:d h:eψ⇒ ( f g h):c (+Lc)

∃i1 j1.. iℓ jℓ ... ∧ (g
ℓ−1:c/dψ { jℓ−1} iℓ−1) ∧ (gℓ:dψ/e { jℓ} iℓ)

∃i1 j1.. iℓ−1 jℓ−1 ... ∧ (g
ℓ−1 ◦ (λh i∃ j(h j)) ◦ gℓ:c/e { jℓ−1} iℓ−1)

(+N)

Figure 5: Additional processing productions for hypothesizing filler-gap attachment.

Operations on dependencies that can be derived

from surface structure (see Figure 3) are taken

directly from van Schijndel and Schuler (2013).

First, if an observation xt can immediately fill

the awaited sign of the last connected component

gℓ:c/d, it is hypothesized to do so, turning this

incomplete connected component into a complete

connected component (gℓ f ):c (Production –Fa); or

if the observation can serve as an initial sub-sign

of this awaited sign, it is hypothesized to form a

new complete sign f :e in a new component with xt
as its first observation (Production +Fa). Then,

if either of these resulting complete signs gℓ:d

can immediately attach as an initial child of the

awaited sign of the most recent connected com-

ponent gℓ−1:a/c, it is hypothesized to merge and

extend this connected component, with xt as the

last observation of the completed connected com-

ponent (Production +La); or if it can serve as an

initial sub-sign of this awaited sign, it is hypoth-

esized to remain disjoint and form its own con-

nected component (Production –La). The side

conditions of La productions are defined to unpack

gap propagation (instances of λk that distinguish

rules Aa–h and Ma–h) from the inference rules

in Section 3, because this functionality will be re-

placed with direct substitution of referent variables

into subordinate semantic functions, below.

The Nguyen et al. (2012) GCG was defined

to pass up unbounded dependencies, but in in-

cremental deep syntactic dependency processing,

unbounded dependencies are accounted as sepa-

rate connected components. When hypothesizing

an unbounded dependency, the processing model

simply cues the active sign of a previous connected

component containing a filler without completing

the current connected component. The four +F,

–F, +L, and –L operations are therefore combined

with applications of unary rules Ga–c for hypoth-

esizing referents as fillers for gaps (providing f ′

in the equations in Figure 4). Productions –Fb

and +Fb fill gaps in initial children, and Produc-

tions –Lb and +Lb fill gaps in final children. Note

that the Fb and Lb productions apply to the same

types of antecedents as Fa and La productions re-

spectively, so members of these two sets of pro-

ductions cannot be applied together.

Applications of rules Fa–c and R for introduc-

ing fillers are applied to store fillers as existentially

quantified variable values in Lc productions (see

Figure 5). These Lc productions apply to the same

type of antecedent as La and Lb productions, so

these also cannot be applied together.

Finally, connected components separated by

gaps which are no longer hypothesized (ψ) are

reattached by a +N production. This +N pro-

duction may then be paired with a –N production

which yields its antecedent unchanged as a conse-

quent. These N productions apply to antecedents

and consequents of the same type, so they may be

applied together with one F and one L production,

but since the +N production removes in its conse-

quent a ψ argument required in its antecedent, it

may not apply more than once in succession (and

applying the –N production more than once in suc-

cession has no effect).

An incremental derivation of the noun phrase

the person who officials say stole millions, using

these productions, is shown in Figure 6.

5 Evaluation

The F, L, and N productions defined in the pre-

vious section can be made probabilistic by first

computing a probabilistic context-free grammar

(PCFG) from a tree-annotated corpus, then trans-

forming that PCFG model into a model of prob-

abilities over incremental parsing operations us-

ing a grammar transform (Schuler, 2009). This

allows the intermediate PCFG to be optimized us-

ing an existing PCFG-based latent variable trainer

42



∃i0 (.. :T/T {i0} i0) the

∃i0 i2 (.. :T/T {i0} i0) ∧ (.. :N/N-aD {i2} i2)
+Fa,–La,–N

person

∃i0 i2 (.. :T/T {i0} i0) ∧ (.. :N/V-rN {i2} i2)
–Fa,–La,–N

who

∃i0 i2 i3 (.. :T/T {i0} i0) ∧ (.. :N/V-gN {i3} i2)
+Fa,+Lc,–N

officials

∃i0 i2 i3 i5 (.. :T/T {i0} i0) ∧ (.. :N/V-gN {i3} i2) ∧ (.. :V-gN/V-aN-gN {i5} i5)
+Fa,–La,–N

say

∃i0 i2 i6 (.. :T/T {i0} i0) ∧ (.. :N/V-aN {i6} i2)
+Fb,+La,+N

stole

∃i0 i2 i7 (.. :T/T {i0} i0) ∧ (.. :N/N {i7} i2)
+Fa,+La,–N

millions

∃i0 (.. :T/T {i0} i0)
–Fa,+La,–N

Figure 6: Derivation of the person who officials say stole millions, showing connected components with

unique referent variables (calculated according to the equations in Section 4). Semantic functions are

abbreviated to ‘..’ for readability. This derivation yields the following lexical relations: (0 i1)=the,

(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the following

argument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.

(Petrov et al., 2006). When applied to the output

of this trainer, this transform has been shown to

produce comparable accuracy to that of the origi-

nal Petrov et al. (2006) CKY parser (van Schijn-

del et al., 2012). The transform used in these ex-

periments diverges from that of Schuler (2009), in

that the probability associated with introducing a

gap in a filler-gap construction is reallocated from

a –F–L operation to a +F–L operation (to encode

the previously most subordinate connected com-

ponent with the filler as its awaited sign and be-

gin a new disjoint connected component), and the

probability associated with resolving such a gap is

reallocated from an implicit –N operation to a +N

operation (to integrate the connected component

containing the gap with that containing the filler).

In order to verify that the modifications to the

transform correctly reallocate probability mass for

gap operations, the goodness of fit to reading

times of a model using this modified transform

is compared against the publicly-available base-

line model from van Schijndel and Schuler (2013),

which uses the original Schuler (2009) transform.5

To ensure a valid comparison, both parsers are

trained on a GCG-reannotated version of the Wall

Street Journal portion of the Penn Treebank (Mar-

cus et al., 1993) before being fit to reading times

using linear mixed-effects models (Baayen et al.,

2008).6 This evaluation focuses on the process-

ing that can be done up to a given point in a sen-

tence. In human subjects, this processing includes

both immediate lexical access and regressions that

5The models used here also use random slopes to reduce
their variance, which makes them less anticonservative.

6The models are built using lmer from the lme4 R package
(Bates et al., 2011; R Development Core Team, 2010).

aid in the integration of new information, so the

reading times of interest in this evaluation are log-

transformed go-past durations.7

The first and last word of each line in the

Dundee corpus, words not observed at least 5

times in the WSJ training corpus, and fixations af-

ter long saccades (>4 words) are omitted from the

evaluation to filter out wrap-up effects, parser in-

accuracies, and inattention and track loss of the

eyetracker. The following predictors are centered

and used in each baseline model: sentence posi-

tion, word length, whether or not the previous or

next word were fixated upon, and unigram and bi-

gram probabilities.8 Then each of the following

predictors is residualized off each baseline before

being centered and added to it to help residualize

the next factor: length of the go-past region, cumu-

lative total surprisal, total surprisal (Hale, 2001),

and cumulative entropy reduction (Hale, 2003).9

All 2-way interactions between these effects are

7Go-past durations are calculated by summing all fixa-
tions in a region of text, including regressions, until a new
region is fixated, which accounts for additional processing
that may take place after initial lexical access, but before the
next region is processed. For example, if one region ends at
word 5 in a sentence, and the next fixation lands on word 8,
then the go-past region consists of words 6-8 while go-past
duration sums all fixations until a fixation occurs after word
8. Log-transforming eye movements and fixations may make
their distributions more normal (Stephen and Mirman, 2010)
and does not substantially affect the results of this paper.

8For the n-gram model, this study uses the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-
cus et al., 1993), the written portion of the British National
Corpus (BNC Consortium, 2007), and the Dundee corpus
(Kennedy et al., 2003) smoothed with modified Kneser-Ney
(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).

9Non-cumulative metrics are calculated from the final
word of the go-past region; cumulative metrics are summed
over the go-past region.

43



included as predictors along with the predictors

from the previous go-past region (to account for

spillover effects). Finally, each model has sub-

ject and item random intercepts added in addition

to by-subject random slopes (cumulative total sur-

prisal, whether the previous word was fixated, and

length of the go-past region) and is fit to centered

log-transformed go-past durations.10

The Akaike Information Criterion (AIC)

indicates that the gap-reallocating model

(AIC = 128,605) provides a better fit to reading

times than the original model (AIC = 128,619).11

As described in Section 1, previous findings of

negative integration cost may be due to a confound

whereby center-embedded constructions caused

by modifiers, which do not require deep syntac-

tic dependencies to be deferred, may be driving

the effect. Under this hypothesis, embeddings

that do not arise from final adjunction of mod-

ifiers (henceforth canonical embeddings) should

yield a positive integration cost as found by Gib-

son (2000).

To investigate this potential confound, the

Dundee corpus is partitioned into two parts. First,

the model described in this paper is used to anno-

tate the Dundee corpus. From this annotated cor-

pus, all sentences are collected that contain canon-

ical embeddings and lack modifier-induced em-

beddings.12 This produces two corpora: one con-

sisting entirely of canonical center-embeddings

such as those used in self-paced reading exper-

iments with findings of positive integration cost

(e.g. Gibson 2000), the other consisting of the

remainder of the Dundee corpus, which contains

sentences with canonical embeddings but also in-

cludes modifier-caused embeddings.

The coefficient estimates for integration oper-

ations (–F+L and +N) on each of these corpora

are then calculated using the baseline described

above. To ensure embeddings are driving any ob-

served effect rather than sentence wrap-up effects,

the first and last words of each sentence are ex-

cluded from both data sets. Integration cost is

measured by the amount of probability mass the

parser allocates to –F+L and +N operations, accu-

10Each fixed effect that has an absolute t-value greater than
10 when included in a random-intercepts only model is added
as a random slope by-subject.

11The relative likelihood of the original model to the gap-
sensitive model is 0.0009 (n = 151,331), which suggests the
improvement is significant.

12Modifier-induced embeddings are found by looking for
embeddings that arise from inference rules Ma-h in Section 3.

Model coeff std err t-score

Canonical -0.040 0.010 -4.05

Other -0.017 0.004 -4.20

Table 1: Fixed effect estimates for integration cost

when used to fit reading times over two partitions

of the Dundee corpus: one containing only canon-

ical center embeddings and the other composed of

the rest of the sentences in the corpus.

mulated over each go-past region, and this cost is

added as a fixed effect and as a random slope by

subject to the mixed model described earlier.13

The fixed effect estimate for cumulative inte-

gration cost from fitting each corpus is shown

in Table 1. Application of Welch’s t-test shows

that the difference between the estimated distri-

butions of these two parameters is highly signif-

icant (p < 0.0001).14 The strong negative corre-

lation of integration cost to reading times in the

purely canonical corpus suggests canonical (non-

modifier) integrations contribute to the finding of

negative integration cost.

6 Conclusion

This paper has introduced an incremental parser

capable of using GCG dependencies to distinguish

between surface syntactic embeddings and deep

syntactic embeddings. This parser was shown to

obtain a better fit to reading times than a surface-

syntactic parser and was used to parse the Dundee

eye-tracking corpus in two partitions: one consist-

ing of canonical embeddings that require deferred

dependencies and the other consisting of sentences

containing no center embeddings or center em-

beddings arising from the attachment of clause-

final modifiers, in which no dependencies are de-

ferred. Using linear mixed effects models, com-

pletion (integration) of canonical center embed-

dings was found to be significantly more nega-

tively correlated with reading times than comple-

tion of non-canonical embeddings. These results

suggest that the negative integration cost observed

in eye-tracking studies is at least partially due to

deep syntactic dependencies and not due to con-

founds related to surface forms.

13Integration cost is residualized off the baseline before be-
ing centered and added as a fixed effect.

14Integration cost is significant as a fixed effect (p = 0.001)
in both partitions: canonical (n = 16,174 durations) and
non-canonical (n = 131,297 durations).

44



References

Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233–250.

R. Harald Baayen, D. J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390–412.

Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1–12.

Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using S4
classes.

BNC Consortium. 2007. The british national corpus.

Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.

W. Nelson Francis and Henry Kucera. 1979. The
brown corpus: A standard corpus of present-day
edited american english.

Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1–
76.

Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95–126,
Cambridge, MA. MIT Press.

John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159–166, Pittsburgh, PA.

John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.

Philip N. Johnson-Laird. 1983. Mental models: to-
wards a cognitive science of language, inference,
and consciousness. Harvard University Press, Cam-
bridge, MA, USA.

Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137–
194.

Marcel Adam Just and Patricia A. Carpenter. 1992. A
capacity theory of comprehension: Individual differ-
ences in working memory. Psychological Review,
99:122–149.

Fred Karlsson. 2007. Constraints on multiple center-
embedding of clauses. Journal of Linguistics,
43:365–392.

Alan Kennedy, James Pynte, and Robin Hill. 2003.
The Dundee corpus. In Proceedings of the 12th Eu-
ropean conference on eye movement.

Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163–182.

Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal
relative clauses in korean. Language, 86(3):561.

Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375–419.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Igor Mel’čuk. 1988. Dependency syntax: theory and
practice. State University of NY Press, Albany.

Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ’12), Mumbai,
India.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING/ACL’06).

R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.

William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ’09, pages
344–352, Boulder, Colorado. Association for Com-
putational Linguistics.

Damian G. Stephen and Daniel Mirman. 2010. Inter-
actions dominate the dynamics of visual cognition.
Cognition, 115(1):154–165.

Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing.

Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.

45



Marten van Schijndel, Andy Exley, and William
Schuler. 2012. Connectionist-inspired incremental
PCFG parsing. In Proceedings of CMCL 2012. As-
sociation for Computational Linguistics.

Marten van Schijndel, Andy Exley, and William
Schuler. in press. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science.

Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL’10), pages 1189–1198.

46


