










































Putting Human Assessments of Machine Translation Systems in Order


Proceedings of the 7th Workshop on Statistical Machine Translation, pages 1–9,
Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

Putting Human Assessments of Machine Translation Systems in Order

Adam Lopez

Human Language Technology Center of Excellence

Johns Hopkins University

Abstract

Human assessment is often considered the

gold standard in evaluation of translation sys-

tems. But in order for the evaluation to

be meaningful, the rankings obtained from

human assessment must be consistent and

repeatable. Recent analysis by Bojar et

al. (2011) raised several concerns about the

rankings derived from human assessments of

English-Czech translation systems in the 2010

Workshop on Machine Translation. We extend

their analysis to all of the ranking tasks from

2010 and 2011, and show through an exten-

sion of their reasoning that the ranking is nat-

urally cast as an instance of finding the mini-

mum feedback arc set in a tournament, a well-

known NP-complete problem. All instances

of this problem in the workshop data are ef-

ficiently solvable, but in some cases the rank-

ings it produces are surprisingly different from

the ones previously published. This leads to

strong caveats and recommendations for both

producers and consumers of these rankings.

1 Introduction

The value of machine translation depends on its util-

ity to human users, either directly through their use

of it, or indirectly through downstream tasks such

as cross-lingual information extraction or retrieval.

It is therefore essential to assess machine transla-

tion systems according to this utility, but there is a

widespread perception that direct human assessment

is costly, unreproducible, and difficult to interpret.

Automatic metrics that predict human utility have

therefore attracted substantial attention since they

are at least cheap and reproducible given identical

data conditions, though they are frequently and cor-

rectly criticized for low interpretability and correla-

tion with true utility. Their use (and abuse) remains

contentious.

The organizers of the annual Workshop on Ma-

chine Translation (WMT) have taken a strong stance

in this debate, asserting the primacy of human eval-

uation. Every annual report of their findings since

2007 has included a variant of the following state-

ment:

It is our contention that automatic mea-

sures are an imperfect substitute for hu-

man assessment of translation quality.

Therefore, we define the manual evalua-

tion to be primary, and use the human

judgments to validate automatic metrics.

(Callison-Burch et al., 2011)

The workshop’s human evaluation component has

been gradually refined over several years, and as a

consequence it has produced a fantastic collection of

publicly available data consisting primarily of pair-

wise judgements of translation systems made by hu-

man assessors across a wide variety of languages

and tasks. Despite superb effort in the collection of

these assessments, less attention has been focused

on the final product derived from them: a totally-

ordered ranking of translation systems participating

in each task. Many of the official workshop results

depend crucially on this ranking, including the eval-

uation of both machine translation systems and auto-

matic metrics. Considering the enormous costs and

consequences of the ranking, it is important to ask:

is the method of constructing it accurate? The num-

ber of possible rankings is combinatorially large—

with at least ten systems (accounting for more than

1



half the cases we analyzed) there are over three mil-

lion possible rankings, and with at least twenty (oc-

curring a few times), there are over 1018 possible
rankings. Exceptional care is therefore required in

producing the rankings.

Bojar et al. (2011) observed a number of discrep-

ancies in the ranking of English-Czech systems from

the 2010 workshop, making these questions ever

more pressing. We extend their analysis in several

ways.

1. We show, through a logical extension of their

reasoning about flaws in the evaluation, that

the final ranking can be naturally cast as an in-

stance of the minimal feedback arc set problem,

a well-known NP-Hard problem.

2. We analyze 25 tasks that were evaluated using

pairwise assessments from human annotators in

2010 and 2011.

3. We produce new rankings for each of the tasks,

which are in some cases surprisingly different

from the published rankings.

4. We identify a new set of concerns about sources

of error and uncertainty in the data.

2 Human Assessment as Pairwise Ranking

The workshop has conducted a variety of different

manual evaluation tasks over the last several years,

but its mainstay has been the relative ranking task.

Assessors are presented with a source sentence fol-

lowed by up to five translations, and are asked to

rank the translations from best to worst, with ties

allowed. Since it is usually infeasible to collect in-

dividual judgements for all sentences for all pairs of

systems on each task, consecutive sequences of three

sentences were randomly sampled from the test data,

with each sentence in each sequence presented to the

same annotator. Some samples were presented mul-

tiple times to the same assessor or to multiple asses-

sors in order to measure intra- and inter-annotator

agreement rates. Since there are often more than

five systems participating in the campaign, the can-

didate translations are likewise sampled from a pool

consisting of the machine translations and a human

reference translation, which is included for quality

JHU 1 JHU≺BBN-COMBO
BBN-COMBO 2 JHU≺RWTH

RWTH 3 JHU≺RWTH-COMBO
RWTH-COMBO 3 JHU≺CMU

CMU 4 BBN-COMBO≺RWTH
BBN-COMBO≺RWTH-COMBO

BBN-COMBO≺CMU
RWTH≡RWTH-COMBO

RWTH≺CMU
RWTH-COMBO≺CMU

Figure 1: Example human relative ranking of five sys-

tems (left) and the inferred pairwise rankings (right) on

a single sentence from the WMT 2010 German-English

campaign.

control purposes. It is important to note that the al-

gorithm used to compute the published final rank-

ings included all of this data, including comparisons

against the reference and the redundant assessments

used to compute inter-annotator agreement.

The raw data obtained from this process is a large

set of assessments. Each assessment consists of a

list of up to five systems (including the reference),

and a partial or total ordering of the list. The relative

ranking of each pair of systems contained in the list

is then taken to be their pairwise ranking. Hence a

single assessment of five systems yields ten implicit

pairwise rankings, as illustrated in Figure 1.

3 From Pairwise to Total Ranking

Given these pairwise rankings, the question now be-

comes: how do we decide on a total ordering of

the systems? In the WMT evaluation, this total or-

dering has two critical functions: it is published as

the official ranking of the participating systems; and

it is used as the ground truth against which auto-

matic evaluation metrics are graded, using Spear-

man’s rank correlation coefficient (without ties) as

the measure of accuracy. Choosing a total order is

non-trivial: there are N ! possible orderings of N
systems. Even with relatively small N of the work-
shop, this number can grow extremely large (over

1025 in the worst case of 25 systems).
The method used to generate the published rank-

ings is simple. For each system A among the set
S of ranked systems (which includes the reference),

2



compute the number of times that A is ranked better
than or equivalent to any system B ∈ S, and then
divide by the total number of comparisons involv-

ing A, yielding the following statistic for system A,
which we call WMT-OFFICAL.

score(A) =
∑

B∈S count(A � B)∑
B∈S,3∈{≺,≡,�}, count(A3B)

(1)

The systems are ranked according to this statistic,

with higher scores resulting in a better rank.

Bojar et al. (2011) raise many concerns about this

method for ranking the systems. While we refer the

reader to their paper for a detailed analysis, we focus

on two issues here:

• Since ties are rewarded, systems may be un-
duly rewarded for merely being similar to oth-

ers, rather than clearly better. This is of particu-

lar concern since there is often a cohort of very

similar systems in the pool, such as those based

on very similar techniques.

• Since the reference is overwhelmingly favored
by the assessors, those systems that are more

frequently compared against the reference in

the random sample will be unfairly penalized.

These observations suggest that the statistic

should be changed to reward only outright wins in

pairwise comparisons, and to lessen the number of

comparisons to the reference. While they do not

recommend a specific sampling rate for comparisons

against the reference, the logical conclusion of their

reasoning is that it should not be sampled at all. This

yields the following statistic similar to one reported

in the appendices of the WMT proceedings, which

we call HEURISTIC 2.

score(A) =

∑
B∈S−ref count(A ≺ B)∑

B∈S−ref,3∈{≺,≡,�}, count(A3B)
(2)

However, the analysis by Bojar et al. (2011) goes

further and suggests disregarding the effect of ties

altogether by removing them from the denominator.

This yields their final recommended statistic, which

we call BOJAR.

score(A) =

∑
B∈S−ref count(A ≺ B)∑

B∈S−ref,3∈{≺,�}, count(A3B)
(3)

Superficially, this appears to be an improve-

ment. However, we observe in the rankings that

two anonymized commercial systems, denoted ON-

LINEA and ONLINEB, consistently appear at or near

the top of the rankings in all tasks. It is natural to

wonder: even if we leave out the reference from

comparisons, couldn’t a system still be penalized

simply by being compared against ONLINEA and

ONLINEB more frequently than its competitors? On

the other hand, couldn’t a system be rewarded sim-

ply by being compared against a bad system more

frequently than its competitors?

There are many possible decisions that we could

make, each leading to a different ranking. However,

there is a more fundamental problem: each of these

heuristic scores is based on statistics aggregated over

completely incomparable sets of data. Any total

ordering of the systems must make a decision be-

tween every pair of systems. When that ranking is

computed using scores computed with any of Equa-

tions 1 through 3, we aggregate over completely dif-

ferent sets of sentences, rates of comparison with

other systems, and even annotators! Deriving sta-

tistical conclusions from such comparisons is at best

suspect. If we want to rank A and B relative to each
other, it would be more reliable to aggregate over

the same set of sentences, same rates of comparison,

and the same annotators. Fortunately, we have this

data in abundance: it is the collection of pairwise

judgements that we started with.

4 Pairwise Ranking as a Tournament

The human assessments are a classic example of a

tournament. A tournament is a graph of N vertices
with exactly

(
N
2

)
directed edges—one between each

pair of vertices. The edge connecting each pair of

vertices A and B points to whichever vertex which
is worse in an observed pairwise comparison be-

tween them. Tournaments are a natural represen-

tation of many ranking problems, including search

results, transferable voting systems, and ranking of

sports teams.1

Consider the simple weighted tournament de-

picted in Figure 2. This tournament is acyclic, which

means that we can obtain a total ordering of the ver-

1The original motivating application was modeling the peck-

ing order of chickens (Landau, 1951).

3



A

B

C

D

3

2

1

1

1

2

Consistent ranking: A ≺ B ≺ C ≺ D

Ranking according to Eq. 1: A ≺ C ≺ B ≺ D

Figure 2: A weighted tournament and two different rank-

ings of its vertices.

tices that is consistent with all of the pairwise rank-

ings simply by sorting the vertices topologically. We

start by choosing the vertex with no incoming edges

(i.e. the one that wins in all pairwise comparisons),

place it at the top of the ranking, and remove it along

with all of its outgoing edges from the graph. We

then repeat the procedure with the remaining ver-

tices in the graph, placing the next vertex behind

the first one, and so on. The result is a ranking that

preserves all of the pairwise rankings in the original

graph.

This example also highlights a problem in Equa-

tion 1. Imagine an idealized case in which the con-

sistent ranking of the vertices in Figure 2 is their true

ranking, and furthermore that this ranking is unam-

biguous: that is, no matter how many times we sam-

ple the comparison A with B, the result is always
that A ≺ B, and likewise for all vertices. If the
weights in this example represented the number of

random samples for each system, then Equation 1

will give the inaccurate ranking shown, since it pro-

duces a score of 25 for B and
2
4 for C.

Tournaments can contain cycles, and as we will

show this is often the case in the WMT data. When

this happens, a reasonable solution is to minimize

the discrepancy between the ranking and the ob-

served data. We can do this by reversing a set of

edges in the graph such that (1) the resulting graph

is acyclic, and (2) the summed weights of the re-

versed edges is minimized. A set of edges satisfying

these constraints is called the minimum feedback arc

set (Figure 3).

The feedback arc set problem on general graphs

E

F

G

H

3

2

1

2

1

2

Figure 3: A tournament with a cycle on vertices E, F ,
and G. The dotted edge is the only element of a minimum
feedback arc set: reversing it produces an acyclic graph.

Algorithm 1 Minimum feedback arc set solver

Input: Graph G = (V,E), weights w : E → R+
Initialize all costs to∞
Let cost(∅)← 0
Add ∅ to agenda A
repeat

Let R̂← argminR∈A cost(R)
Remove R̂ from A . R̂ is a partial ranking
Let U ← V \R̂ . set of unranked vertices
for each vertex v ∈ U do

Add R̂ ∪ v to agenda
Let c←

∑
v′∈U :〈v′,v〉∈E w(〈v′, v〉)

Let d← cost(R̂) + c
Let cost(R̂∪{v})← min(cost(R̂∪{v}), d)

until argminR∈A cost(h) = V

is one of the 21 classic problems shown to be

NP-complete by Karp (1972).2 Finding the mini-

mum feedback arc set in a tournament was shown

to be NP-hard by Alon (2006) and Charbit et al.

(2007). However, the specific instances exhibited

in the workshop data tend to have only a few cy-

cles, so a relatively straightforward algorithm (for-

malized above for completeness) solves them ex-

actly without much difficulty. The basic idea is to

construct a dynamic program over the possible rank-

ings. Each item in the dynamic program represents

a ranking of some subset of the vertices. An item

is extended by choosing one of the unranked ver-

tices and appending it to the hypothesis, adding to

its cost the weights of all edges from the other un-

ranked vertices to the newly appended vertex (the

2Karp proved NP-completeness of the decision problem that

asks whether there is a feedback arc set of size k; NP-hardness
of the minimization problem follows.

4



Task name #sys #pairs Task name #sys #pairs

2010 Czech-English 12 5375 2011 English-French individual 17 9086

2010 English-Czech 17 13538 2011 English-German syscomb 4 4374

2010 English-French 19 7962 2011 English-German individual 22 12996

2010 English-German 18 13694 2011 English-Spanish syscomb 4 5930

2010 English-Spanish 16 5174 2011 English-Spanish individual 15 11130

2010 French-English 24 8294 2011 French-English syscomb 6 3000

2010 German-English 25 10424 2011 French-English individual 18 6986

2010 Spanish-English 14 11307 2011 German-English syscomb 8 3844

2011 Czech-English syscomb 4 2602 2011 German-English individual 20 9079

2011 Czech-English individual 8 4922 2011 Spanish-English syscomb 6 4156

2011 English-Czech syscomb 2 2686 2011 Spanish-English individual 15 5652

2011 English-Czech individual 10 17875 2011 Urdu-English tunable metrics 8 6257

2011 English-French syscomb 2 880

Table 1: The set of tasks we analyzed, including the number of participating systems (excluding the reference, #sys),

and the number of implicit pairwise judgements collected (including the reference, #pairs).

edges to be reversed). This hypothesis space should

be familiar to most machine translation researchers

since it closely resembles the search space defined

by a phrase-based translation model (Koehn, 2004).

We use Dijkstra’s algorithm (1959) to explore it ef-

ficiently; the complete algorithm is simply a gener-

alization of the simple algorithm for acyclic tourna-

ments described above.

5 Experiments and Analysis

We experimented with 25 relative ranking tasks pro-

duced by WMT 2010 (Callison-Burch et al., 2010)

and WMT 2011 (Callison-Burch et al., 2011); the

full set is shown in Table 1. For each task we con-

sidered four possible methods of ranking the data:

sorting by any of Equation 1 through 3, and sort-

ing consistent with reversal of a minimum feedback

arc set (MFAS). To weight the edges for the latter

approach, we simply used the difference in num-

ber of assessments preferring one system over the

other; that is, an edge from A to B is weighted
count(A ≺ B)− count(A � B). If this quantity is
negative, there is instead an edge from B to A. The
purpose of this simple weighting is to ensure a so-

lution that minimizes the number of disagreements

with all available evidence, counting each pairwise

comparison as equal.3

3This is not necessarily the best choice of weighting. For

instance, (Bojar et al., 2011) observe that human assessments of

WMT-OFFICIAL MFAS BOJAR

(Eq 1) (Eq 3)

ONLINE-B CU-MARECEK ONLINE-B

CU-BOJAR ONLINE-B CU-BOJAR

CU-MARECEK CU-BOJAR CU-MARECEK

CU-TAMCHYNA CU-TAMCHYNA CU-TAMCHYNA

UEDIN CU-POPEL CU-POPEL

CU-POPEL UEDIN UEDIN

COMMERCIAL2 COMMERCIAL1 COMMERCIAL2

COMMERCIAL1 COMMERCIAL2 COMMERCIAL1

JHU JHU JHU

CU-ZEMAN CU-ZEMAN CU-ZEMAN

38 0 69

Table 2: Different rankings of the 2011 Czech-English

task. Only the MFAS ranking is acyclic with respect to

pairwise judgements. The final row indicates the weight

of the voilated edges.

An MFAS solution written in Python took only a

few minutes to produce rankings for all 25 tasks on a

2.13 GHz Intel Core 2 Duo processor, demonstrating

that it is completely feasible despite being theoreti-

cally intractible. One value of computing this solu-

tion is that it enables us to answer several questions,

shorter sentences tend to be more consistent with each other, so

perhaps they should be weighted more highly. Unfortunately,

it is not clear how to evaluate alternative weighting schemes,

since there is no ground truth for such meta-evaluations.

5



ONLINEB LIUM ≺ ONLINEB 1 RWTH-COMBO
RWTH-COMBO UPV-COMBO ≺ CAMBRIDGE 6 CMU-HYPOSEL-COMBO

CMU-HYPOSEL-COMBO JHU ≺ CAMBRIDGE 1 DCU-COMBO
CAMBRIDGE LIMSI ≺ UEDIN 1 ONLINEB

LIUM LIMSI ≺ CMU-HYPOSEL-COMBO 1 LIUM
DCU-COMBO LIUM-COMBO ≺ CAMBRIDGE 1 CMU-HEAFIELD-COMBO

CMU-HEAFIELD-COMBO LIUM-COMBO ≺ NRC 3 UPV-COMBO
UPV-COMBO RALI ≺ UEDIN 1 NRC

NRC RALI ≺ UPV-COMBO 4 CAMBRIDGE
UEDIN RALI ≺ JHU 1 UEDIN

JHU RALI ≺ LIUM 3 JHU-COMBO
LIMSI LIG ≺ UEDIN 6 LIMSI

JHU-COMBO BBN-COMBO ≺ NRC 3 RALI
LIUM-COMBO BBN-COMBO ≺ UEDIN 5 LIUM-COMBO

RALI BBN-COMBO ≺ UPV-COMBO 5 BBN-COMBO
LIG BBN-COMBO ≺ JHU 4 JHU

BBN-COMBO RWTH ≺ UPV-COMBO 3 RWTH
RWTH CMU-STATXFER ≺ JHU 1 LIG

CMU-STATXFER CMU-STATXFER ≺ LIG 1 ONLINEA
ONLINEA ONLINEA ≺ RWTH 1 CMU-STATXFER
HUICONG ONLINEA ≺ JHU 2 HUICONG

DFKI HUICONG ≺ LIG 3 DFKI
CU-ZEMAN DFKI ≺ RWTH 3 GENEVA

GENEVA DFKI ≺ CMU-STATXFER 1 CU-ZEMAN

Table 3: 2010 French-English reranking with MFAS solver. The left column shows the optimal ranking, while the

center shows the pairwise rankings that are violated by this ranking, along with their edge weights. The right column

shows the ranking under WMT-OFFICIAL (Eq. 1), originally published as two separate tables.

both about the pairwise data itself, and the proposed

heuristic ranking of Bojar et al. (2011).

5.1 Cycles in the Pairwise Rankings

Our first experiment checks for cycles in the tourna-

ments. Only nine were acyclic, including all eight

of the system combination tasks, each of which con-

tained only a handful of systems. The most inter-

esting, however, is the 2011 English-Czech individ-

ual task. This task is notable because the heuristic

rankings do not produce a ranking that is consistent

with all of the pairwise judgements, even though one

exists. The three rankings are illustrated side-by-

side in Table 2. One obvious problem is that neither

heuristic score correctly identifies CU-MARECEK as

the best system, even though it wins pairwise com-

parisons against all other systems (the WMT 2011

proceedings do identify it as a winner, despite not

placing it in the highest rank).

On the other hand, the most difficult task to dis-

entangle is the 2010 French-English task (Table 3),

which included 25 systems (individual and system

combinations were evaluated as a group for this task,

despite being reported in separate tables in official

results). Its optimal ranking with MFAS still vio-

lates 61 pairwise ranking samples — there is sim-

ply no sensible way to put these systems into a to-

tal order. On the other hand, the heuristic rankings

based on Equations 1 through 3 violate even more

comparisons: 107, 108, and 118, respectively. Once

again we see a curious result in the top of the heuris-

tic rankings, with system ONLINEB falling several

spots below the top position in the heurstic ranking,

despite losing out only to LIUM by one vote.

Our major concern, however, is that over half of

the tasks included cycles of one form or another in

the tournaments. This represents a strong inconsis-

6



tency in the data.

5.2 Evaluation of Heuristic Scores

Taking the analysis above further, we find that the

total number of violations of pairwise preferences

across all tasks stands at 396 for the MFAS solution,
and at 1140, 1215, 979 for Equations 1 through 3.
This empirically validates the suggestion by Bojar

et al. (2011) to remove ties from both the numera-

tor and denominator of the heuristic measure. On

the other hand, despite the intuitive arguments in its

favor, the empirical evidence does not strongly fa-

vor any of the heuristic measures, all of which are

substantially worse than the MFAS solution.

In fact, HEURISTIC 2 (Eq. 2) fails quite spec-

tacularly in one case: on the ranking of the sys-

tems produced by the tunable metrics task of WMT

2011 (Figure 4). Apart from producing a ranking

very inconsistent with the pairwise judgements, it

achieves a Spearman’s rank correlation coefficent

of 0.43 with the MFAS solution. By comparison,

WMT-OFFICIAL (Eq. 1) produces the best ranking,

with a correlation of 0.93 with the MFAS solution.

The two heuristic measures obtain an even lower

correlation of 0.19 with each other. This difference

in the two rankings was noted in the WMT 2011

report; however comparison with the MFAS ranker

suggests that the published rankings according to the

official metric are about as accurate as those based

on other heuristic metrics.

6 Discussion

Unfortunately, reliably ranking translation systems

based on human assessments appears to be a difficult

task, and it is unclear that WMT has succeeded yet.

Some results presented here, such as the complete

inability to obtain a sensible ordering on the 2010

French-English task—or to produce an acyclic tour-

nament on more than half the tasks—indicate that

further work is needed, and we feel that the pub-

lished results of the human assessment should be re-

garded with a healthy skepticism. There are many

potential sources of uncertainty in the data:

• It is quite rare that one system is uniformly bet-
ter than another. Rather, one system will tend

to perform better in aggregate across many sen-

tences. The number of sentences on which this

MFAS Ranking HEURISTIC 2 Ranking

CMU-BLEU CU-SEMPOS-BLEU

CMU-BLEU-SINGLE NUS-TESLA-F

CU-SEMPOS-BLEU CMU-BLEU

RWTH-CDER CMU-BLEU-SINGLE

CMU-METEOR STANFORD-DCP

STANFORD-DCP CMU-METEOR

NUS-TESLA-F RWTH-CDER

SHEFFIELD-ROSE SHEFFIELD-ROSE

Table 4: Rankings of the WMT 2011 tunable metrics

task. MFAS finds a near-optimal solution, violating only

six judgements with reversals of CMU-METEOR ≺ CMU-
BLEU and STANFORD-DCP ≺ CMU-BLEU-SINGLE. In
contrast, the HEURISTIC2 (Eq. 2) solution violates 103

pairwise judgements.

improvement can be reliably observed will vary

greatly. In many cases, it may be less than the

number of samples.

• Individual assessors may be biased or mali-
cious.

• The reliability of pairwise judgements varies
with sentence length, as noted by Bojar et al.

(2011).

• The pairwise judgements are not made directly,
but inferred from a larger relative ranking.

• The pairwise judgements are not independent,
since each sample consists of consecutive sen-

tences from the same document. It is likely

that some systems are systematically better or

worse on particular documents.

• The pairwise judgements are not independent,
since many of the assessments are intention-

ally repeated to assess intra- and inter-annotator

agreement.

• Many of the systems will covary, since they are
often based on the same underlying techniques

and software.

How much does any one or all of these factors

affect the final ranking? The technique described

above does not even attempt to address this ques-

tion. Indeed, modeling this kind of data still ap-

pears to be unsolved: a recent paper by Wauthier

7



and Jordan (2011) on modeling latent annotator bias

presents one of the first attempts at solving just one

of the above problems, let alone all of them.

Simple hypothesis testing of the type reported in

the workshop results is simply inadequate to tease

apart the many interacting effects in this type of

data and may lead to many unjustified conclusions.

The tables in the Appendix of Callison-Burch et al.

(2011) report p-values of up to 1%, computed for
every pairwise comparison in the dataset. However,

there are over two thousand comparisons in this ap-

pendix, so even at an error rate of 1% we would ex-

pect more than twenty to be wrong. Making matters

worse, many of the p-values are in fact much than
higher than 1%. It is quite reasonable to assume

that hundreds of the pairwise rankings inferred from

these tables are incorrect, or at least meaningless.

Methods for multiple hypothesis testing (Benjamini

and Hochberg, 1995) should be explored.

In short, there is much work to be done. This pa-

per has raised more questions than it answered, but

we offer several recommendations.

• We recommend against using the metric pro-
posed by Bojar et al. (2011). While their anal-

ysis is very insightful, their proposed heuristic

metric is not substantially better than the met-

ric used in the official rankings. If anything, an

MFAS-based ranking should be preferred since

it can minimize discrepancies with the pairwise

rankings, but as we have discussed, we believe

this is far from a complete solution.

• Reconsider the use of total ordering, especially
for the evaluation of automatic metrics. As

demonstrated in this paper, there are many pos-

sible ways to generate a total ordering, and the

choice of one may be arbitrary. In some cases

there may not be enough evidence to support a

total ordering, or the evidence is contradictory,

and committing to one may be a source of sub-

stantial noise in the gold standard for evaluating

automatic metrics.

• Consider a pilot study to clearly identify which
sources of uncertainty in the data affect the

rankings and devise methods to account for it,

which may involve redesigning the data collec-

tion protocol. The current approach is designed

to collect data for a variety of different goals,

including intra- and inter-annotator agreement,

pairwise coverage, and maximum throughput.

However, some of goals are at cross-purposes

in that they make it more difficult to make reli-

able statistical inferences about any one aspect

of the data. Additional care should be taken

to minimize dependencies between the samples

used to produce the final ranking.

• Encourage further detailed analysis of the ex-
isting datasets, perhaps through a shared task.

The data that has been amassed so far through

WMT is the best available resource for mak-

ing progress on solving the difficult problem of

producing reliable and repeatable human rank-

ings of machine translation systems. However,

this problem is not solved yet, and it will re-

quire sustained effort to make that progress.

Acknowledgements

Thanks to Ondrěj Bojar, Philipp Koehn, and Mar-

tin Popel for very helpful discussion related to this

work, the anonymous reviewers for detailed and

helpful comments, and Chris Callison-Burch for en-

couraging this investigation and for many explana-

tions and additional data from the workshop.

References

N. Alon. 2006. Ranking tournaments. SIAM Journal on

Discrete Mathematics, 20(1):137–142.

Y. Benjamini and Y. Hochberg. 1995. Controlling the

false discovery rate: a practical and powerful approach

to multiple testing. Journal of the Royal Statistical

Society, 57:289–300.

O. Bojar, M. Ercegovčević, M. Popel, and O. F. Zaidan.

2011. A grain of salt for the WMT manual evaluation.

In Proc. of WMT.

C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,

M. Przybocki, and O. Zaidan. 2010. Findings of the

2010 joint workshop on statistical machine translation

and metrics for machine translation. In Proc. of WMT.

C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.

2011. Findings of the 2011 workshop on statistical

machine translation. In Proc. of WMT.

P. Charbit, S. Thomass, and A. Yeo. 2007. The minimum

feedback arc set problem is NP-hard for tournaments.

Combinatorics, Probability and Computing, 16.

8



E. W. Dijkstra. 1959. A note on two problems in connex-

ion with graphs. Numerische Mathematik, 1:269–271.

R. M. Karp. 1972. Reducibility among combinatorial

problems. In Symposium on the Complexity of Com-

puter Computations.

P. Koehn. 2004. Pharaoh: a beam search decoder for

phrase-based statistical machine translation models.

In Proc. of AMTA.

H. G. Landau. 1951. On dominance relations and

the structure of animal societies: I effect of inher-

ent characteristics. Bulletin of Mathematical Biology,

13(1):1–19.

F. L. Wauthier and M. I. Jordan. 2011. Bayesian bias

mitigation for crowdsourcing. In Proc. of NIPS.

9


