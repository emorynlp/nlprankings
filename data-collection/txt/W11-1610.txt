










































Paraphrastic Sentence Compression with a Character-based Metric: Tightening without Deletion


Workshop on Monolingual Text-To-Text Generation, pages 84–90,

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84–90,
Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics

Paraphrastic Sentence Compression with a Character-based Metric:
Tightening without Deletion

Courtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 and
Benjamin Van Durme1,2

1Department of Computer Science
2Human Language Technology Center of Excellence

Johns Hopkins University

Abstract

We present a substitution-only approach to
sentence compression which “tightens” a sen-
tence by reducing its character length. Replac-
ing phrases with shorter paraphrases yields
paraphrastic compressions as short as 60% of
the original length. In support of this task,
we introduce a novel technique for re-ranking
paraphrases extracted from bilingual corpora.
At high compression rates1 paraphrastic com-
pressions outperform a state-of-the-art dele-
tion model in an oracle experiment. For fur-
ther compression, deleting from oracle para-
phrastic compressions preserves more mean-
ing than deletion alone. In either setting, para-
phrastic compression shows promise for sur-
passing deletion-only methods.

1 Introduction

Sentence compression is the process of shortening a
sentence while preserving the most important infor-
mation. Because it was developed in support of ex-
tractive summarization (Knight and Marcu, 2000),
much of the previous work considers deletion-based
models, which extract a subset of words from a long
sentence to create a shorter sentence such that mean-
ing and grammar are maximally preserved. This
framework imposes strict constraints on the task and
does not accurately model human-written compres-
sions, which tend to be abstractive rather than ex-
tractive (Marsi et al., 2010). This is one sense in
which paraphrastic compression can improve exist-
ing compression methodologies.

1Compression rate is defined as the compression length over
original length, so lower values indicate shorter sentences.

We distinguish two non-identical notions of sen-
tence compression: making a sentence substantially
shorter versus “tightening” a sentence by remov-
ing unnecessary verbiage. We propose a method to
tighten sentences with just substitution and no dele-
tion operations. Using paraphrases extracted from
bilingual text and re-ranked on monolingual data,
our system selects the set of paraphrases that min-
imizes the character length of a sentence.

While not currently the standard, character-based
lengths have been considered before in compres-
sion, and we believe that it is relevant for current
and future applications. Character lengths have been
used for document summarization (DUC 2004, Over
and Yen (2004)), summarizing for mobile devices
(Corston-Oliver, 2001), and subtitling (Glickman et
al., 2006). Although in the past strict word limits
have been imposed for various documents, informa-
tion transmitted electronically is often limited by the
number of bytes, which directly relates to the num-
ber of characters. Mobile devices, SMS messages,
and microblogging sites such as Twitter are increas-
ingly important for quickly spreading information.
In this context, it is important to consider character-
based constraints.

We examine whether paraphrastic compression
allows more information to be conveyed in the same
number of characters as deletion-only compressions.
For example, the length constraint of Twitter posts or
tweets is 140 characters, and many article lead sen-
tences exceed this limit. A paraphrase substitution
oracle compresses the sentence in the table below to
76% of its original length (162 to 123 characters; the
first is the original). The compressed tweet is 140

84



characters, including spaces 17-character shortened
link to the original article.2

Congressional leaders reached a last-gasp agreement
Friday to avert a shutdown of the federal government,
after days of haggling and tense hours of brinksman-
ship.
Congress made a final agreement Fri. to avoid govern-
ment shutdown, after days of haggling and tense hours
of brinkmanship. on.wsj.com/h8N7n1

In contrast, using deletion to compress to the same
length may not be as expressive:

Congressional leaders reached agreement Friday to
avert a shutdown of federal government, after haggling
and tense hours. on.wsj.com/h8N7n1

This work presents a model that makes paraphrase
choices to minimize the character length of a sen-
tence. An oracle paraphrase-substitution experiment
shows that human judges rate paraphrastic compres-
sions higher than deletion-based compressions. To
achieve further compression, we shortened the or-
acle compressions using a deletion model to yield
compressions 80% of the original sentence length
and compared these to compressions generated us-
ing just deletions. Manual evaluation found that
the oracle-then-deletion compressions to preserve
more meaning than deletion-only compressions at
uniform compression rates.

2 Related work

Most of the previous research on sentence compres-
sion focuses on deletion using syntactic informa-
tion, (e.g., Galley and McKeown (2007), Knight
and Marcu (2002), Nomoto (2009), Galanis and An-
droutsopoulos (2010), Filippova and Strube (2008),
McDonald (2006), Yamangil and Shieber (2010),
Cohn and Lapata (2008), Cohn and Lapata (2009),
Turner and Charniak (2005)). Woodsend et al.
(2010) incorporate paraphrase rules into a deletion
model. Previous work in subtitling has made one-
word substitutions to decrease character length at
high compression rates (Glickman et al., 2006).
More recent approaches in steganography have used
paraphrase substitution to encode information in text
but focus on grammaticality, not meaning preserva-
tion (Chang and Clark, 2010). Zhao et al. (2009) ap-
plied an adaptable paraphrasing pipeline to sentence

2Taken from the main page of http://wsj.com, April 9, 2011.

compression, optimizing for F-measure over a man-
ually annotated set of gold standard paraphrases.

Sentence compression has been considered be-
fore in contexts outside of summarization, such as
headline, title, and subtitle generation (Dorr et al.,
2003; Vandeghinste and Pan, 2004; Marsi et al.,
2009). Corston-Oliver (2001) deleted characters
from words to shorten the character length of sen-
tences. To our knowledge character-based compres-
sion has not been examined before with the surging
popularity and utility of Twitter.

3 Sentence Tightening

The distinction between tightening and compression
can be illustrated by considering how much space
needs to be preserved. In the case of microblogging,
often a sentence has just a few too many characters
and needs to be “tightened”. On the other hand, if a
sentence is much longer than a desired length, more
drastic compression is necessary. The first subtask
is relevant in any context with strict word or charac-
ter limits. Some sentences may not be compressible
beyond a certain limit. For example, we found that
near 10% of the compressions generated by Clarke
and Lapata (2008) were identical to the original sen-
tence. In situations where the sentence must meet
a minimum length, tightening can be used to meet
these requirements.

Multi-reference translations provide an instance
of the natural length variation of human-generated
sentences. These translations represent different
ways to express the foreign same sentence, so there
should be no meaning lost between the different ref-
erence translations. The character-based length of
different translations of a given sentence varies on
average by 80% when compared to the shortest sen-
tence in a set.3 This provides evidence that sen-
tences can be tightened to some extent without los-
ing any meaning.

Through the lens of sentence tightening, we con-
sider whether paraphrase substitutions alone can
yield compressions competitive with a deletion at
the same length. A character-based compression
rate is crucial in this framework, as two compres-

3This value will vary by collection and with the number of
references: for example, the NIST05 Arabic reference set has a
mean compression rate of 0.92 with 4 references per set.

85



sions having the same character-based compres-
sion rate may have different word-based compres-
sion rates. The advantage of a character-based sub-
stitution model is in choosing shorter words when
possible, freeing space for more content words. Go-
ing by word length alone would exclude the many
paraphrases with fewer characters than the original
phrase and the same number of words (or more).

3.1 Paraphrase Acquisition
To generate paraphrases for use in our experiments,
we took the approach described by Bannard and
Callison-Burch (2005), which extracts paraphrases
from bilingual parallel corpora. Figure 1 illustrates
the process. A phrase to be paraphrased, like thrown
into jail, is found in a German-English parallel cor-
pus. The corresponding foreign phrase (festgenom-
men) is identified using word alignment and phrase
extraction techniques from phrase-based statistical
machine translation (Koehn et al., 2003). Other oc-
currences of the foreign phrase in the parallel corpus
may align to another English phrase like jailed. Fol-
lowing Bannard and Callison-Burch, we treated any
English phrases that share a common foreign phrase
as potential paraphrases of each other.

As the original phrase occurs several times and
aligns with many different foreign phrases, each of
these may align to a variety of other English para-
phrases. Thus, thrown into jail not only paraphrases
as jailed, but also as arrested, detained, impris-
oned, incarcerated, locked up, taken into custody,
and thrown into prison . Moreover, because the
method relies on noisy and potentially inaccurate
word alignments, it is prone to generate many bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, owners, and protection.

To rank candidates, Bannard and Callison-Burch
defined the paraphrase probability p(e2|e1) based
on the translation model probabilities p(e|f) and
p(f |e) from statistical machine translation. Follow-
ing Callison-Burch (2008), we refine selection by re-
quiring both the original phrase and paraphrase to
be of the same syntactic type, which leads to more
grammatical paraphrases.

Although many excellent paraphrases are ex-
tracted from parallel corpora, many others are un-
suitable and the translation score does not always
accurately distinguish the two. Therefore, we re-

Paraphrase Monlingual Bilingual
study in detail 1.00 0.70

scrutinise 0.94 0.08
consider 0.90 0.20

keep 0.83 0.03
learn 0.57 0.10
study 0.42 0.07

studied 0.28 0.01
studying it in detail 0.16 0.05

undertook 0.06 0.06

Table 1: Candidate paraphrases for study in detail with
corresponding approximate cosine similarity (Monolin-
gual) and translation model (Bilingual) scores.

ranked our candidates based on monolingual distri-
butional similarity, employing the method described
by Van Durme and Lall (2010) to derive approxi-
mate cosine similarity scores over feature counts us-
ing single token, independent left and right contexts.
Features were computed from the web-scale n-gram
collection of Lin et al. (2010). As 5-grams are the
highest order of n-gram in this collection, the al-
lowable set of paraphrases have at most four words
(which allows at least one word of context).

To our knowledge this is the first time such tech-
niques have been used in combination in order to
derive higher quality paraphrase candidates. See Ta-
ble 1 for an example.

The monolingual-filtering technique we describe
is by no means limited to paraphrases extracted from
bilingual corpora. It could be applied to other data-
driven paraphrasing techniques (see Madnani and
Dorr (2010) for a survey). Although it is particularly
well suited to the bilingual extracted corpora, since
the information that it adds is orthogonal to that
model, it would presumably add less to paraphras-
ing techniques that already take advantage of mono-
lingual distributional similarity (Pereira et al., 1993;
Lin and Pantel, 2001; Barzilay and Lee, 2003).

In order to evaluate the paraphrase candidates
and scoring techniques, we randomly selected 1,000
paraphrase sets where the source phrase was present
in the corpus described in Clarke and Lapata (2008).
For each phrase and set of candidate paraphrases, we
extracted all of the contexts from the corpus in which
the source phrase appeared. Human judges were
presented each sentence with the original phrase and
the same sentences with each paraphrase candidate

86



... letzteWoche wurden in Irland fünf Landwirte festgenommen , weil sie verhindern wollten

... last week five farmers were thrown into jail in Ireland because they resisted ...

...

Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und getötet .

Quite a few journalists have disappeared or have been imprisoned , tortured and killed .

Figure 1: Using a bilingual parallel corpus to extract paraphrases.

substituted in. Each paraphrase substitution was
graded based on the extent to which it preserved
the meaning and affected the grammaticality of the
sentence. While both the bilingual translation score
and monolingual cosine similarity positively corre-
lated with human judgments, the monolingual score
proved a stronger predictor of quality in both dimen-
sions. Using Kendall’s tau correlation coefficient,
the agreement between the ranking imposed by the
monolingual score and human ratings surpassed that
of the original ranking as derived during the bilin-
gual extraction, for both meaning and grammar.4 In
our substitution framework, we ignore the transla-
tion probabilities and use only the approximate co-
sine similarity in the paraphrase decision task.

4 Framework for Sentence Tightening

Our sentence tightening approach uses a dynamic
programming strategy to find the combination of
non-overlapping paraphrases that minimizes a sen-
tence’s character length. The threshold of the mono-
lingual score for paraphrases can be varied to widen
or narrow the search space, which may be further in-
creased by considering any lexical paraphrases not
subject to syntactic constraints. Sentences with a
compression rate as low as 0.6 can be generated
without thresholding the paraphrase scores. Because
the system can generate multiple paraphrased sen-
tences of equal length, we apply two layers of filter-
ing to generate a single output. First we calculate a
word-overlap score between the original and candi-
date sentences to favor compressions similar to the
original sentence; then, from among the sentences

4For meaning and grammar respectively, τ = 0.28 and 0.31
for monolingual scores and 0.19 and 0.15 for bilingual scores.

with the highest word overlap, we select the com-
pression with the best language model score.

Higher paraphrase thresholds guarantee more ap-
propriate paraphrases but yield longer compressions.
Using a cosine-similarity threshold of 0.95, the av-
erage compression rate is 0.968, which is consider-
ably longer than the compressions using no thresh-
old (0.60). In these experiments we did not syntac-
tically constrain paraphrases. However, we believe
that our monolingual refining of paraphrase sets im-
proves paraphrase selection and is a reasonable al-
ternative to using syntactic constraints.

In case judges favor compressions that have high
word overlap with the original sentence, we com-
pressed the longest sentence from each set of ref-
erence translations (Huang et al., 2002) and ran-
domly chose a sentence from the set of reference
translations to use as the standard for comparison.
Paraphrastic compressions were generated at cosine-
similarity thresholds ranging from 0.60 to 0.95.
We implemented a state-of-the-art deletion model
(Clarke and Lapata, 2008) to generate deletion-only
compressions. We fixed the compression length
to ±5 characters of the length of each paraphras-
tic compression, in order to isolate the compression
quality from the effect of compression rate (Napoles
et al., 2011). Manual evaluation used Amazon’s
Mechanical Turk with three-way redundancy and
positive and negative controls to filter bad workers.
Meaning and grammar judgments were collected us-
ing two 5-point scales (5 being the highest score).

5 Evaluation

The initial results of our substitution system show
room for improvement in future work (Table 2). We
believe this is due to erroneous paraphrase substi-

87



System Grammar Meaning CompR Cos.
Substitution 3.8 3.7 0.97 0.95

Deletion 4.1 4.0 0.97 -
Substitution 3.4 3.2 0.89 0.85

Deletion 4.0 3.8 0.89 -
Substitution 3.1 3.0 0.85 0.75

Deletion 3.9 3.7 0.85 -
Substitution 2.9 2.9 0.82 0.65

Deletion 3.8 3.5 0.82 -

Table 2: Mean ratings of compressions using just deletion
or substitution at different paraphrase thresholds (Cos.).
Deletion performed better in all settings.

tutions, since phrases with the same syntactic cate-
gory and distributional similarity are not necessarily
semantically identical. Illustrative examples include
WTO for United Nations and east or west for south.
Because the quality of the multi-reference transla-
tions is not uniformly high, for the following exper-
iment we used a dataset of English newspaper arti-
cles.

To control against these errors and test the viabil-
ity of a substitution-only approach, we generated all
possible paraphrase substitutions above a threshold
of 0.80 within a set of 20 randomly chosen sentences
from the written corpus of Clarke and Lapata (2008).
We solicited humans to make a ternary decision of
whether a paraphrase was acceptable in the context
(good, bad, or not sure). We applied our model to
generate compressions using only paraphrase substi-
tutions on which all three annotators agreed that the
paraphrase was good. The oracle generated com-
pressions with an average compression rate of 0.90.

On the same set of original sentences, we used
the deletion model to generate compressions con-
strained to ±5 characters of the length of the ora-
cle compression. Next, we examined whether apply-
ing the deletion model to paraphrastic compressions
would improve compression quality. In manual eval-
uation along the dimensions of grammar and mean-
ing, both the oracle compressions and oracle-plus-
deletion compressions outperformed the deletion-
only compressions at uniform lengths (Table 3)5.
These results suggest that improvements in para-
phrase acquisition will make our system competitive
with deletion-only models.

5Paraphrastic compressions were rated significantly higher
for meaning, p < 0.05

Model Grammar Meaning CompR
Oracle 4.1 4.3 0.90

Deletion 4.0 4.1 0.90
Gold 4.3 3.8 0.75

Oracle+deletion 3.4 3.7 0.80
Deletion 3.2 3.4 0.80

Table 3: Mean ratings of compressions generated by a
substitution oracle, deletion only, deletion on the oracle
compression, and the gold standard. Being able to choose
the best paraphrases would enable our substitution model
to outperform the deletion model.

6 Conclusion

This work shows promise for the use of only sub-
stitution in the task of sentence tightening. There
are myriad possible extensions and improvements
to this method, most notably richer features be-
yond paraphrase length. We do not currently use
syntactic information in our paraphrastic compres-
sion model because it places limits on the number
of paraphrases available for a sentence and thereby
limits the possible compression rate. The current
method for paraphrase extraction does not include
certain types of rewriting, such as passivization, and
should be extended to incorporate even more short-
ening paraphrases. Future work can directly apply
these methods to Twitter and extract additional para-
phrases and abbreviations from Twitter and/or SMS
data. Our substitution approach can be improved by
applying more sophisticated techniques to choosing
the best candidate compression, or by framing it as
an optimization problem over more than just mini-
mal length. Overall, we find these results to be en-
couraging for the possibility of sentence compres-
sion without deletion.

Acknowledgments

We are grateful to John Carroll for helping us obtain
the RASP parser. This research was partially funded
by the JHU Human Language Technology Center of
Excellence. This research was funded in part by the
NSF under grant IIS-0713448. The views and find-
ings are the authors’ alone.

References
Colin Bannard and Chris Callison-Burch. 2005. Para-

phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.

88



Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.

Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.

Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591–599. Association for Computational Linguistics.

James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399–429.

Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.

Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637–674.

Simon Corston-Oliver. 2001. Text compaction for dis-
play on very small screens. In Proceedings of the
NAACL Workshop on Automatic Summarization.

Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.

Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference. Association for Computational Linguis-
tics.

Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.

Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.

Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning, pages 45–52. Associa-
tion for Computational Linguistics.

Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.

Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization – Step one: Sentence compression. In
Proceedings of AAAI.

Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91–107.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.

Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343–360.

Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.

Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–388.

Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2009. Is sentence compression an NLG
task? In Proceedings of the 12th European Workshop
on Natural Language Generation.

Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2010. On the limits of sentence com-
pression by deletion. Empirical Methods in Natural
Language Generation, pages 45–66.

Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.

Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of ACL, Workshop on Monolingual Text-To-Text Gen-
eration.

Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.

Paul Over and James Yen. 2004. An introduction to
DUC 2004: Intrinsic evaluation of generic news text
summarization systems. In Proceedings of DUC 2004
Document Understanding Workshop, Boston.

Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In ACL-93.

Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.

Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.

Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization.

89



Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.

Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
synchronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.

Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.

90


