



















































Aspect Level Sentiment Classification with Deep Memory Network


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214‚Äì224,
Austin, Texas, November 1-5, 2016. c¬©2016 Association for Computational Linguistics

Aspect Level Sentiment Classification with Deep Memory Network

Duyu Tang, Bing Qin‚àó, Ting Liu
Harbin Institute of Technology, Harbin, China

{dytang, qinb, tliu}@ir.hit.edu.cn

Abstract

We introduce a deep memory network for
aspect level sentiment classification. Un-
like feature-based SVM and sequential neural
models such as LSTM, this approach explic-
itly captures the importance of each context
word when inferring the sentiment polarity of
an aspect. Such importance degree and tex-
t representation are calculated with multiple
computational layers, each of which is a neu-
ral attention model over an external memory.
Experiments on laptop and restaurant datasets
demonstrate that our approach performs com-
parable to state-of-art feature based SVM sys-
tem, and substantially better than LSTM and
attention-based LSTM architectures. On both
datasets we show that multiple computational
layers could improve the performance. More-
over, our approach is also fast. The deep mem-
ory network with 9 layers is 15 times faster
than LSTM with a CPU implementation.

1 Introduction

Aspect level sentiment classification is a fundamen-
tal task in the field of sentiment analysis (Pang and
Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given
a sentence and an aspect occurring in the sentence,
this task aims at inferring the sentiment polarity (e.g.
positive, negative, neutral) of the aspect. For ex-
ample, in sentence ‚Äúgreat food but the service was
dreadful!‚Äù, the sentiment polarity of aspect ‚Äúfood‚Äù
is positive while the polarity of aspect ‚Äúservice‚Äù is

‚àó Corresponding author.

negative. Researchers typically use machine learn-
ing algorithms and build sentiment classifier in a su-
pervised manner. Representative approaches in liter-
ature include feature based Support Vector Machine
(Kiritchenko et al., 2014; Wagner et al., 2014) and
neural network models (Dong et al., 2014; Lakkara-
ju et al., 2014; Vo and Zhang, 2015; Nguyen and
Shirai, 2015; Tang et al., 2015a). Neural models are
of growing interest for their capacity to learn text
representation from data without careful engineer-
ing of features, and to capture semantic relations be-
tween aspect and context words in a more scalable
way than feature based SVM.

Despite these advantages, conventional neural
models like long short-term memory (LSTM) (Tang
et al., 2015a) capture context information in an im-
plicit way, and are incapable of explicitly exhibiting
important context clues of an aspect. We believe that
only some subset of context words are needed to in-
fer the sentiment towards an aspect. For example,
in sentence ‚Äúgreat food but the service was dread-
ful!‚Äù, ‚Äúdreadful‚Äù is an important clue for the aspect
‚Äúservice‚Äù but ‚Äúgreat‚Äù is not needed. Standard LST-
M works in a sequential way and manipulates each
context word with the same operation, so that it can-
not explicitly reveal the importance of each context
word. A desirable solution should be capable of ex-
plicitly capturing the importance of context words
and using that information to build up features for
the sentence after given an aspect word. Further-
more, a human asked to do this task will selectively
focus on parts of the contexts, and acquire informa-
tion where it is needed to build up an internal repre-
sentation towards an aspect in his/her mind.

214



In pursuit of this goal, we develop deep memo-
ry network for aspect level sentiment classification,
which is inspired by the recent success of compu-
tational models with attention mechanism and ex-
plicit memory (Graves et al., 2014; Bahdanau et al.,
2015; Sukhbaatar et al., 2015). Our approach is
data-driven, computationally efficient and does not
rely on syntactic parser or sentiment lexicon. The
approach consists of multiple computational layers
with shared parameters. Each layer is a content- and
location- based attention model, which first learn-
s the importance/weight of each context word and
then utilizes this information to calculate continu-
ous text representation. The text representation in
the last layer is regarded as the feature for sentiment
classification. As every component is differentiable,
the entire model could be efficiently trained end-to-
end with gradient descent, where the loss function is
the cross-entropy error of sentiment classification.

We apply the proposed approach to laptop and
restaurant datasets from SemEval 2014 (Pontiki et
al., 2014). Experimental results show that our ap-
proach performs comparable to a top system using
feature-based SVM (Kiritchenko et al., 2014). On
both datasets, our approach outperforms both LST-
M and attention-based LSTM models (Tang et al.,
2015a) in terms of classification accuracy and run-
ning speed. Lastly, we show that using multiple
computational layers over external memory could
achieve improved performance.

2 Background: Memory Network

Our approach is inspired by the recent success of
memory network in question answering (Weston et
al., 2014; Sukhbaatar et al., 2015). We describe the
background on memory network in this part.

Memory network is a general machine learning
framework introduced by Weston et al. (2014). It-
s central idea is inference with a long-term memo-
ry component, which could be read, written to, and
jointly learned with the goal of using it for predic-
tion. Formally, a memory network consists of a
memory m and four components I , G, O and R,
where m is an array of objects such as an array of
vectors. Among these four components, I convert-
s input to internal feature representation, G updates
old memories with new input, O generates an out-

put representation given a new input and the current
memory state, R outputs a response based on the
output representation.

Let us take question answering as an example to
explain the work flow of memory network. Given
a list of sentences and a question, the task aims to
find evidences from these sentences and generate an
answer, e.g. a word. During inference, I component
reads one sentence si at a time and encodes it into a
vector representation. Then G component updates a
piece of memory mi based on current sentence rep-
resentation. After all sentences are processed, we
get a memory matrix m which stores the semantics
of these sentences, each row representing a sentence.
Given a question q, memory network encodes it into
vector representation eq, and thenO component uses
eq to select question related evidences from memo-
ry m and generates an output vector o. Finally, R
component takes o as the input and outputs the final
response. It is worth noting that O component could
consist of one or more computational layers (hop-
s). The intuition of utilizing multiple hops is that
more abstractive evidences could be found based on
previously extracted evidences. Sukhbaatar et al.
(2015) demonstrate that multiple hops could uncov-
er more abstractive evidences than single hop, and
could yield improved results on question answering
and language modeling.

3 Deep Memory Network for Aspect Level
Sentiment Classification

In this section, we describe the deep memory net-
work approach for aspect level sentiment classifica-
tion. We first give the task definition. Afterwards,
we describe an overview of the approach before p-
resenting the content- and location- based attention
models in each computational layer. Lastly, we de-
scribe the use of this approach for aspect level senti-
ment classification.

3.1 Task Definition and Notation
Given a sentence s = {w1, w2, ..., wi, ...wn} con-
sisting of n words and an aspect word wi 1 occur-
ring in sentence s, aspect level sentiment classifica-
tion aims at determining the sentiment polarity of

1In practice, an aspect might be a multi word expression
such as ‚Äúbattery life‚Äù. For simplicity we still consider aspect
as a single word in this definition.

215



sentence s towards the aspect wi. For example, the
sentiment polarity of sentence ‚Äúgreat food but the
service was dreadful!‚Äù towards aspect ‚Äúfood‚Äù is pos-
itive, while the polarity towards aspect ‚Äúservice‚Äù is
negative. When dealing with a text corpus, we map
each word into a low dimensional, continuous and
real-valued vector, also known as word embedding
(Mikolov et al., 2013; Pennington et al., 2014). All
the word vectors are stacked in a word embedding
matrix L ‚àà Rd√ó|V |, where d is the dimension of
word vector and |V | is vocabulary size. The word
embedding of wi is notated as ei ‚àà Rd√ó1, which is a
column in the embedding matrix L.

3.2 An Overview of the Approach

We present an overview of the deep memory net-
work for aspect level sentiment classification.

Given a sentence s = {w1, w2, ..., wi, ...wn} and
the aspect word wi, we map each word into its em-
bedding vector. These word vectors are separated
into two parts, aspect representation and context rep-
resentation. If aspect is a single word like ‚Äúfood‚Äù or
‚Äúservice‚Äù, aspect representation is the embedding of
aspect word. For the case where aspect is multi word
expression like ‚Äúbattery life‚Äù, aspect representation
is an average of its constituting word vectors (Sun et
al., 2015). To simplify the interpretation, we consid-
er aspect as a single word wi. Context word vectors
{e1, e2 ... ei‚àí1, ei+1 ... en} are stacked and regarded
as the external memory m ‚àà Rd√ó(n‚àí1), where n is
the sentence length.

An illustration of our approach is given in Figure
1, which is inspired by the use of memory network
in question answering (Sukhbaatar et al., 2015). Our
approach consists of multiple computational layers
(hops), each of which contains an attention layer and
a linear layer. In the first computational layer (hop
1), we regard aspect vector as the input to adaptively
select important evidences from memory m through
attention layer. The output of attention layer and the
linear transformation of aspect vector2 are summed
and the result is considered as the input of next layer
(hop 2). In a similar way, we stack multiple hop-
s and run these steps multiple times, so that more
abstractive evidences could be selected from the ex-

2In preliminary experiments, we tried directly using aspect
vector without a linear transformation, and found that adding a
linear layer works slightly better.

LinearAttention

‚àë

wi

LinearAttention

‚àë

LinearAttention

‚àë

aspect word

hop 1

hop 2

hop 3

ùë§1 , ùë§2 ‚Ä¶ ùë§ùëñ‚àí1 , ùë§ùëñ , ùë§ùëñ+1 ‚Ä¶ ùë§ùëõ‚àí1 , ùë§ùëõ

context words context words

sentence:

word embedding

softmax

ùõº1ùõº2 ùõºùëòùõº3

ùë£ùëê1 ùë£ùëéùë†ùëùùëíùëêùë°

ùëî1

Linear

Tanh

Linear

ùë£ùëê2 ùë£ùëéùë†ùëùùëíùëêùë°

ùëî2

Linear

Tanh

Linear

ùë£ùëêùëò ùë£ùëéùë†ùëùùëíùëêùë°

ùëîùëò

Linear

Tanh

Linear

‚Ä¶‚Ä¶

softmax

‚Ä¶‚Ä¶

Figure 1: An illustration of our deep memory network with
three computational layers (hops) for aspect level sentimen-

t classification.

ternal memory m. The output vector in last hop is
considered as the representation of sentence with re-
gard to the aspect, and is further used as the feature
for aspect level sentiment classification.

It is helpful to note that the parameters of attention
and linear layers are shared in different hops. There-
fore, the model with one layer and the model with
nine layers have the same number of parameters.

3.3 Content Attention

We describe our attention model in this part. The
basic idea of attention mechanism is that it assign-
s a weight/importance to each lower position when
computing an upper level representation (Bahdanau
et al., 2015). In this work, we use attention model
to compute the representation of a sentence with re-
gard to an aspect. The intuition is that context words
do not contribute equally to the semantic meaning of
a sentence. Furthermore, the importance of a word
should be different if we focus on different aspect.
Let us again take the example of ‚Äúgreat food but the
service was dreadful!‚Äù. The context word ‚Äúgreat‚Äù
is more important than ‚Äúdreadful‚Äù for aspect ‚Äúfood‚Äù.
On the contrary, ‚Äúdreadful‚Äù is more important than
‚Äúgreat‚Äù for aspect ‚Äúservice‚Äù.

Taking an external memory m ‚àà Rd√ók and an
aspect vector vaspect ‚àà Rd√ó1 as input, the attention
model outputs a continuous vector vec ‚àà Rd√ó1. The
output vector is computed as a weighted sum of each

216



piece of memory in m, namely

vec =

k‚àë

i=1

Œ±imi (1)

where k is the memory size, Œ±i ‚àà [0, 1] is the weight
of mi and

‚àë
i Œ±i = 1. We implement a neural

network based attention model. For each piece of
memory mi, we use a feed forward neural network
to compute its semantic relatedness with the aspect.
The scoring function is calculated as follows, where
Watt ‚àà R1√ó2d and batt ‚àà R1√ó1.

gi = tanh(Watt[mi; vaspect] + batt) (2)

After obtaining {g1, g2, ... gk}, we feed them to a
softmax function to calculate the final importance
scores {Œ±1, Œ±2, ... Œ±k}.

Œ±i =
exp(gi)‚àëk
j=1 exp(gj)

(3)

We believe that such an attention model has two
advantages. One advantage is that this model could
adaptively assign an importance score to each piece
of memory mi according to its semantic relatedness
with the aspect. Another advantage is that this at-
tention model is differentiable, so that it could be
easily trained together with other components in an
end-to-end fashion.

3.4 Location Attention

We have described our neural attention framework
and a content-based model in previous subsection.
However, the model mentioned above ignores the lo-
cation information between context word and aspec-
t. Such location information is helpful for an atten-
tion model because intuitively a context word closer
to the aspect should be more important than a farther
one. In this work, we define the location of a context
word as its absolute distance with the aspect in the
original sentence sequence3. On this basis, we study
four strategies to encode the location information in
the attention model. The details are described below.

3The location of a context word could also be measured by
its distance to the aspect along a syntactic path. We leave this
as a future work as we prefer to developing a purely data-driven
approach without using external parsing results.

‚Ä¢ Model 1. Following Sukhbaatar et al. (2015),
we calculate the memory vector mi with

mi = ei ÔøΩ vi (4)

where ÔøΩ means element-wise multiplication and
vi ‚àà Rd√ó1 is a location vector for word wi. Every
element in vi is calculated as follows,

vki = (1‚àí li/n)‚àí (k/d)(1‚àí 2√ó li/n) (5)

where n is sentence length, k is the hop number and
li is the location of wi.
‚Ä¢ Model 2. This is a simplified version of Model

1, using the same location vector vi for wi in differ-
ent hops. Location vector vi is calculated as follows.

vi = 1‚àí li/n (6)

‚Ä¢ Model 3. We regard location vector vi as a pa-
rameter and compute a piece of memory with vector
addition, namely

mi = ei + vi (7)

All the position vectors are stacked in a position
embedding matrix, which is jointly learned with gra-
dient descent.
‚Ä¢ Model 4. Location vectors are also regarded as

parameters. Different from Model 3, location rep-
resentations are regarded as neural gates to control
how many percent of word semantics is written into
the memory. We feed location vector vi to a sigmoid
function œÉ, and calculatemi with element-wise mul-
tiplication:

mi = ei ÔøΩ œÉ(vi) (8)

3.5 The Need for Multiple Hops
It is widely accepted that computational models that
are composed of multiple processing layers have the
ability to learn representations of data with multiple
levels of abstraction (LeCun et al., 2015). In this
work, the attention layer in one layer is essentially
a weighted average compositional function, which
is not powerful enough to handle the sophisticated
computationality like negation, intensification and
contrary in language. Multiple computational lay-
ers allow the deep memory network to learn repre-
sentations of text with multiple levels of abstraction.
Each layer/hop retrieves important context words,

217



and transforms the representation at previous level
into a representation at a higher, slightly more ab-
stract level. With the composition of enough such
transformations, very complex functions of sentence
representation towards an aspect can be learned.

3.6 Aspect Level Sentiment Classification

We regard the output vector in last hop as the fea-
ture, and feed it to a softmax layer for aspect level
sentiment classification. The model is trained in a
supervised manner by minimizing the cross entropy
error of sentiment classification, whose loss func-
tion is given below, where T means all training in-
stances, C is the collection of sentiment categories,
(s, a) means a sentence-aspect pair.

loss = ‚àí
‚àë

(s,a)‚ààT

‚àë

c‚ààC
P gc (s, a) ¬∑ log(Pc(s, a)) (9)

Pc(s, a) is the probability of predicting (s, a) as cat-
egory c produced by our system. P gc (s, a) is 1 or
0, indicating whether the correct answer is c. We
use back propagation to calculate the gradients of
all the parameters, and update them with stochastic
gradient descent. We clamp the word embeddings
with 300-dimensional Glove vectors (Pennington et
al., 2014), which is trained from web data and the
vocabulary size is 1.9M4. We randomize other pa-
rameters with uniform distribution U(‚àí0.01, 0.01),
and set the learning rate as 0.01.

4 Experiment

We describe experimental settings and report empir-
ical results in this section.

4.1 Experimental Setting

We conduct experiments on two datasets from Se-
mEval 2014 (Pontiki et al., 2014), one from laptop
domain and another from restaurant domain. Statis-
tics of the datasets are given in Table 1. It is worth
noting that the original dataset contains the fourth
category - conflict, which means that a sentence ex-
presses both positive and negative opinion towards
an aspect. We remove conflict category as the num-
ber of instances is very tiny, incorporating which

4Available at: http://nlp.stanford.edu/projects/glove/.

will make the dataset extremely unbalanced. Evalu-
ation metric is classification accuracy.

Dataset Pos. Neg. Neu.
Laptop-Train 994 870 464
Laptop-Test 341 128 169
Restaurant-Train 2164 807 637
Restaurant-Test 728 196 196

Table 1: Statistics of the datasets.

4.2 Comparison to Other Methods

We compare with the following baseline methods on
both datasets.

(1) Majority is a basic baseline method, which
assigns the majority sentiment label in training set
to each instance in the test set.

(2) Feature-based SVM performs state-of-the-art
on aspect level sentiment classification. We compare
with a top system using ngram features, parse fea-
tures and lexicon features (Kiritchenko et al., 2014).

(3) We compare with three LSTM models (Tang
et al., 2015a)). In LSTM, a LSTM based recurrent
model is applied from the start to the end of a sen-
tence, and the last hidden vector is used as the sen-
tence representation. TDLSTM extends LSTM by
taking into account of the aspect, and uses two LST-
M networks, a forward one and a backward one, to-
wards the aspect. TDLSTM+ATT extends TDLST-
M by incorporating an attention mechanism (Bah-
danau et al., 2015) over the hidden vectors. We use
the same Glove word vectors for fair comparison.

(4) We also implement ContextAVG, a simplistic
version of our approach. Context word vectors are
averaged and the result is added to the aspect vector.
The output is fed to a softmax function.

Experimental results are given in Table 2. Our
approach using only content attention is abbreviat-
ed to MemNet (k), where k is the number of hops.
We can find that feature-based SVM is an extremely
strong performer and substantially outperforms oth-
er baseline methods, which demonstrates the impor-
tance of a powerful feature representation for aspect
level sentiment classification. Among three recur-
rent models, TDLSTM performs better than LSTM,
which indicates that taking into account of the as-
pect information is helpful. This is reasonable as the
sentiment polarity of a sentence towards different as-

218



Laptop Restaurant
Majority 53.45 65.00
Feature+SVM 72.10 80.89
LSTM 66.45 74.28
TDLSTM 68.13 75.63
TDLSTM+ATT 66.24 74.31
ContextAVG 61.22 71.33
MemNet (1) 67.66 76.10
MemNet (2) 71.14 78.61
MemNet (3) 71.74 79.06
MemNet (4) 72.21 79.87
MemNet (5) 71.89 80.14
MemNet (6) 72.21 80.05
MemNet (7) 72.37 80.32
MemNet (8) 72.05 80.14
MemNet (9) 72.21 80.95

Table 2: Classification accuracy of different methods on laptop
and restaurant datasets. Best scores in each group are in bold.

pects (e.g. ‚Äúfood‚Äù and ‚Äúservice‚Äù) might be different.
It is somewhat disappointing that incorporating at-
tention model over TDLSTM does not bring any im-
provement. We consider that each hidden vector of
TDLSTM encodes the semantics of word sequence
until the current position. Therefore, the model of
TDLSTM+ATT actually selects such mixed seman-
tics of word sequence, which is weird and not an in-
tuitive way to selectively focus on parts of contexts.
Different from TDLSTM+ATT, the proposed mem-
ory network approach removes the recurrent calcula-
tor over word sequence and directly apply attention
mechanism on context word representations.

We can also find that the performance of Contex-
tAVG is very poor, which means that assigning the
same weight/importance to all the context words is
not an effective way. Among all our models from
single hop to nine hops, we can observe that using
more computational layers could generally lead to
better performance, especially when the number of
hops is less than six. The best performances are
achieved when the model contains seven and nine
hops, respectively. On both datasets, the proposed
approach could obtain comparable accuracy com-
pared to the state-of-art feature-based SVM system.

4.3 Runtime Analysis

We study the runtime of recurrent neural models and
the proposed deep memory network approach with
different hops. We implement all these approaches
based on the same neural network infrastructure, use
the same 300-dimensional Glove word vectors, and
run them on the same CPU server.

Method Time cost
LSTM 417
TDLSTM 490
TDLSTM + ATT 520
MemNet (1) 3
MemNet (2) 7
MemNet (3) 9
MemNet (4) 15
MemNet (5) 20
MemNet (6) 24
MemNet (7) 26
MemNet (8) 27
MemNet (9) 29

Table 3: Runtime (seconds) of each training epoch on the
restaurant dataset.

The training time of each iteration on the restau-
rant dataset is given in Table 3. We can find that
LSTM based recurrent models are indeed compu-
tationally expensive, which is caused by the com-
plex operations in each LSTM unit along the word
sequence. Instead, the memory network approach
is simpler and evidently faster because it does not
need recurrent calculators of sequence length. Our
approach with nine hops is almost 15 times faster
than the basic LSTM model.

4.4 Effects of Location Attention

As described in Section 3.4, we explore four strate-
gies to integrate location information into the atten-
tion model. We incorporate each of them separate-
ly into the basic content-based attention model. It
is helpful to restate that the difference between four
location-based attention models lies in the usage of
location vectors for context words. In Model 1 and
Model 2, the values of location vectors are fixed and
calculated in a heuristic way. In Model 3 and Model
4, location vectors are also regarded as the parame-
ters and jointly learned along with other parameters
in the deep memory network.

219



(a) Aspect: service, Answer: -1, Prediction: -1

hop 1 hop 2 hop 3 hop 4 hop 5
great 0.20 0.15 0.14 0.13 0.23
food 0.11 0.07 0.08 0.12 0.06
but 0.20 0.10 0.10 0.12 0.13
the 0.03 0.07 0.08 0.12 0.06
was 0.08 0.07 0.08 0.12 0.06

dreadful 0.20 0.45 0.45 0.28 0.40
! 0.19 0.08 0.08 0.12 0.07

(b) Aspect: food, Answer: +1, Prediction: -1

hop 1 hop 2 hop 3 hop 4 hop 5
great 0.22 0.12 0.14 0.12 0.20
but 0.21 0.11 0.10 0.11 0.12
the 0.03 0.11 0.08 0.11 0.06

service 0.11 0.11 0.08 0.11 0.06
was 0.04 0.11 0.08 0.11 0.06

dreadful 0.22 0.32 0.45 0.32 0.43
! 0.16 0.11 0.08 0.11 0.07

Table 4: Examples of attention weights in different hops for aspect level sentiment classification. The model only uses content
attention. The hop columns show the weights of context words in each hop, indicated by values and gray color. This example shows

the results of sentence ‚Äúgreat food but the service was dreadful!‚Äù with ‚Äúfood‚Äù and ‚Äúservice‚Äù as the aspects.

(a) Aspect: service, Answer: -1, Prediction: -1

hop 1 hop 2 hop 3 hop 4 hop 5
great 0.08 0.10 0.10 0.09 0.09
food 0.08 0.07 0.07 0.07 0.07
but 0.10 0.15 0.16 0.13 0.11
the 0.07 0.07 0.07 0.07 0.07
was 0.07 0.07 0.07 0.07 0.07

dreadful 0.52 0.48 0.48 0.50 0.52
! 0.07 0.07 0.07 0.07 0.07

(b) Aspect: food, Answer: +1, Prediction: +1

hop 1 hop 2 hop 3 hop 4 hop 5
great 0.31 0.26 0.32 0.28 0.32
but 0.14 0.18 0.15 0.18 0.15
the 0.08 0.05 0.08 0.05 0.07

service 0.09 0.09 0.09 0.08 0.09
was 0.09 0.08 0.09 0.08 0.08

dreadful 0.18 0.21 0.18 0.22 0.19
! 0.11 0.12 0.10 0.11 0.10

Table 5: Examples of attention weights in different hops for aspect level sentiment classification. The model also takes into account
of the location information (Model 2). This example is as same as the one we use in Table 4.

1 2 3 4 5 6 7 8 9
0.72

0.73

0.74

0.75

0.76

0.77

0.78

0.79

0.8

0.81

Number of hops

A
cc

ur
ac

y

 

 

Content
+ Location 1
+ Location 2
+ Location 3
+ Location 4

Figure 2: Classification accuracy of different attention models
on the restaurant dataset.

Figure 2 shows the classification accuracy of each
attention model on the restaurant dataset. We can
find that using multiple computational layers could
consistently improve the classification accuracy in
all these models. All these models perform compa-
rably when the number of hops is larger than five.
Among these four location-based models, we pre-
fer Model 2 as it is intuitive and has less compu-
tation cost without loss of accuracy. We also find
that Model 4 is very sensitive to the choice of neural

gate. Its classification accuracy decreases by almost
5 percentage when the sigmoid operation over loca-
tion vector is removed.

4.5 Visualize Attention Models

We visualize the attention weight of each context
word to get a better understanding of the deep mem-
ory network approach. The results of context-based
model and location-based model (Model 2) are giv-
en in Table 4 and Table 5, respectively.

From Table 4(a), we can find that in the first hop
the context words ‚Äúgreat‚Äù, ‚Äúbut‚Äù and ‚Äúdreadful‚Äù con-
tribute equally to the aspect ‚Äúservice‚Äù. While after
the second hop, the weight of ‚Äúdreadful‚Äù increases
and finally the model correctly predict the polarity
towards ‚Äúservice‚Äù as negative. This case shows the
effects of multiple hops. However, in Table 4(b),
the content-based model also gives a larger weight
to ‚Äúdreadful‚Äù when the target we focus on is ‚Äúfood‚Äù.
As a result, the model incorrectly predicts the po-
larity towards ‚Äúfood‚Äù as negative. This phenomenon
might be caused by the neglect of location informa-
tion. From Table 5(b), we can find that the weight
of ‚Äúgreat‚Äù is increased when the location of context
word is considered. Accordingly, Model 2 predict-

220



s the correct sentiment label towards ‚Äúfood‚Äù. We
believe that location-enhanced model captures both
content and location information. For instance, in
Table 5(a) the closest context words of the aspect
‚Äúservice‚Äù are ‚Äúthe‚Äù and ‚Äúwas‚Äù, while ‚Äúdreadful‚Äù has
the largest weight.

4.6 Error Analysis
We carry out an error analysis of our location en-
hanced model (Model 2) on the restaurant dataset,
and find that most of the errors could be sum-
marized as follows. The first factor is non-
compositional sentiment expression. This model
regards single context word as the basic computa-
tional unit and cannot handle this situation. An
example is ‚Äúdessert was also to die for!‚Äù, where
the aspect is underlined. The sentiment expres-
sion is ‚Äúdie for‚Äù, whose meaning could not be
composed from its constituents ‚Äúdie‚Äù and ‚Äúfor‚Äù.
The second factor is complex aspect expression
consisting of many words, such as ‚Äúask for the
round corner table next to the large window.‚Äù This
model represents an aspect expression by averag-
ing its constituting word vectors, which could not
well handle this situation. The third factor is senti-
mental relation between context words such as nega-
tion, comparison and condition. An example is ‚Äúbut
dinner here is never disappointing, even if the prices
are a bit over the top‚Äù. We believe that this is caused
by the weakness of weighted average composition-
al function in each hop. There are also cases when
comparative opinions are expressed such as ‚Äúi ‚Äôve
had better japanese food at a mall food court‚Äù.

5 Related Work

This work is connected to three research areas in nat-
ural language processing. We briefly describe relat-
ed studies in each area.

5.1 Aspect Level Sentiment Classification
Aspect level sentiment classification is a fine-
grained classification task in sentiment analysis,
which aims at identifying the sentiment polarity of
a sentence expressed towards an aspect (Pontiki et
al., 2014). Most existing works use machine learn-
ing algorithms, and build sentiment classifier from
sentences with manually annotated polarity label-
s. One of the most successful approaches in liter-

ature is feature based SVM. Experts could design
effective feature templates and make use of external
resources like parser and sentiment lexicons (Kir-
itchenko et al., 2014; Wagner et al., 2014). In re-
cent years, neural network approaches (Dong et al.,
2014; Lakkaraju et al., 2014; Nguyen and Shirai,
2015; Tang et al., 2015a) are of growing attention for
their capacity to learn powerful text representation
from data. However, these neural models (e.g. L-
STM) are computationally expensive, and could not
explicitly reveal the importance of context evidences
with regard to an aspect. Instead, we develop simple
and fast approach that explicitly encodes the con-
text importance towards a given aspect. It is worth
noting that the task we focus on differs from fine-
grained opinion extraction, which assigns each word
a tag (e.g. B,I,O) to indicate whether it is an aspec-
t/sentiment word (Choi and Cardie, 2010; Irsoy and
Cardie, 2014; Liu et al., 2015). The aspect word in
this work is given as a part of the input.

5.2 Compositionality in Vector Space

In NLP community, compositionality means that
the meaning of a composed expression (e.g. a
phrase/sentence/document) comes from the mean-
ings of its constituents (Frege, 1892). Mitchell and
Lapata (2010) exploits a variety of addition and
multiplication functions to calculate phrase vector.
Yessenalina and Cardie (2011) use matrix multipli-
cation as compositional function to compute vec-
tors for longer phrases. To compute sentence rep-
resentation, researchers develop denoising autoen-
coder (Glorot et al., 2011), convolutional neural net-
work (Kalchbrenner et al., 2014; Kim, 2014; Yin
and SchuÃàtze, 2015), sequence based recurrent neu-
ral models (Sutskever et al., 2014; Kiros et al., 2015;
Li et al., 2015b) and tree-structured neural network-
s (Socher et al., 2013; Tai et al., 2015; Zhu et al.,
2015). Several recent studies calculate continuous
representation for documents with neural networks
(Le and Mikolov, 2014; Bhatia et al., 2015; Li et al.,
2015a; Tang et al., 2015b; Yang et al., 2016).

5.3 Attention and Memory Networks

Recently, there is a resurgence in computational
models with attention mechanism and explicit mem-
ory to learn representations of texts (Graves et al.,
2014; Weston et al., 2014; Sukhbaatar et al., 2015;

221



Bahdanau et al., 2015). In this line of research,
memory is encoded as a continuous representation
and operations on memory (e.g. reading and writ-
ing) are typically implemented with neural network-
s. Attention mechanism could be viewed as a com-
positional function, where lower level representa-
tions are regarded as the memory, and the func-
tion is to choose ‚Äúwhere to look‚Äù by assigning
a weight/importance to each lower position when
computing an upper level representation. Such at-
tention based approaches have achieved promising
performances on a variety of NLP tasks (Luong et
al., 2015; Kumar et al., 2015; Rush et al., 2015).

6 Conclusion

We develop deep memory networks that capture im-
portances of context words for aspect level senti-
ment classification. Compared with recurrent neu-
ral models like LSTM, this approach is simpler
and faster. Empirical results on two datasets veri-
fy that the proposed approach performs comparable
to state-of-the-art feature based SVM system, and
substantively better than LSTM architectures. We
implement different attention strategies and show
that leveraging both content and location informa-
tion could learn better context weight and text rep-
resentation. We also demonstrate that using multi-
ple computational layers in memory network could
obtain improved performance. Our potential future
plans are incorporating sentence structure like pars-
ing results into the deep memory network.

Acknowledgments

We would especially want to thank Xiaodan Zhu for
running their system on our setup. We greatly thank
Yaming Sun for tremendously helpful discussions.
We also thank the anonymous reviewers for their
valuable comments. This work was supported by the
National High Technology Development 863 Pro-
gram of China (No. 2015AA015407), National Nat-
ural Science Foundation of China (No. 61632011
and No.61273321).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly

learning to align and translate. International Confer-
ence on Learning Representations (ICLR).

Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.
2015. Better document-level sentiment analysis from
rst discourse parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2212‚Äì2218.

Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the ACL 2010 Conference
Short Papers, pages 269‚Äì274. Association for Compu-
tational Linguistics.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 49‚Äì54.

Gottlob Frege. 1892. On sense and reference. Ludlow
(1997), pages 563‚Äì584.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513‚Äì520.

Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. arXiv preprint arX-
iv:1410.5401.

Ozan Irsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In Proceedings
of the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 720‚Äì728.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 655‚Äì665.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1746‚Äì1751.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detecting
aspects and sentiment in customer reviews. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), pages 437‚Äì442.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in Neural Information Processing Systems,
pages 3276‚Äì3284.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,
Robert English, Brian Pierce, Peter Ondruska, Ishaan

222



Gulrajani, and Richard Socher. 2015. Ask me any-
thing: Dynamic memory networks for natural lan-
guage processing. arXiv preprint arXiv:1506.07285.

Himabindu Lakkaraju, Richard Socher, and Chris Man-
ning. 2014. Aspect specific sentiment analysis using
hierarchical deep learning. In NIPS Workshop on Deep
Learning and Representation Learning.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Proceed-
ings of The 31nd International Conference on Machine
Learning, pages 1188‚Äì1196.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. Nature, 521(7553):436‚Äì444.

Jiwei Li, Thang Luong, and Dan Jurafsky. 2015a. A hi-
erarchical neural autoencoder for paragraphs and doc-
uments. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics, pages
1106‚Äì1115.

Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard Hov-
y. 2015b. When are tree structures necessary for
deep learning of representations? In Proceedings of
the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 2304‚Äì2314.

Pengfei Liu, Shafiq Joty, and Helen Meng. 2015. Fine-
grained opinion mining with recurrent neural network-
s and word embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1433‚Äì1443.

Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1‚Äì167.

Thang Luong, Hieu Pham, and Christopher D. Manning.
2015. Effective approaches to attention-based neural
machine translation. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1412‚Äì1421.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-
do, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111‚Äì3119.

Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388‚Äì1429.

Thien Hai Nguyen and Kiyoaki Shirai. 2015. Phrasernn:
Phrase recursive neural network for aspect-based sen-
timent analysis. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 2509‚Äì2514.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1‚Äì135.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing,
pages 1532‚Äì1543.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspec-
t based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27‚Äì35.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 379‚Äì389.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1631‚Äì
1642.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and
Rob Fergus. 2015. End-to-end memory networks. In
Advances in Neural Information Processing Systems,
pages 2431‚Äì2439.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2015. Modeling mention,
context and entity with neural networks for entity dis-
ambiguation. Proceedings of the Twenty-Fourth Inter-
national Joint Conference on Artificial Intelligence (I-
JCAI 2015), pages 1333‚Äì1339.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing systems,
pages 3104‚Äì3112.

Kai Sheng Tai, Richard Socher, and Christopher D. Man-
ning. 2015. Improved semantic representations from
tree-structured long short-term memory networks. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 1556‚Äì
1566.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Li-
u. 2015a. Target-Dependent Sentiment Classification
with Long Short Term Memory. ArXiv preprint arX-
iv:1512.01100.

Duyu Tang, Bing Qin, and Ting Liu. 2015b. Document
modeling with gated recurrent neural network for sen-
timent classification. Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1422‚Äì1432.

Duy-Tin Vo and Yue Zhang. 2015. Target-dependent
twitter sentiment classification with rich automatic

223



features. In Proceedings of the Twenty-Fourth Inter-
national Joint Conference on Artificial Intelligence (I-
JCAI 2015), pages 1347‚Äì1353.

Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and Lami-
a Tounsi. 2014. Dcu: Aspect-based polarity classifi-
cation for semeval task 4. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 223‚Äì229.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2014.
Memory networks. arXiv preprint arXiv:1410.3916.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex
Smola, and Eduard Hovy. 2016. Hierarchical atten-
tion networks for document classification. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguis-
tics.

Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 172‚Äì
182.

Wenpeng Yin and Hinrich SchuÃàtze. 2015. Multichan-
nel variable-size convolution for sentence classifica-
tion. In Proceedings of the Nineteenth Conference
on Computational Natural Language Learning, pages
204‚Äì214.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015.
Long short-term memory over tree structures. In Pro-
ceedings of The 32nd International Conference on Ma-
chine Learning, pages 1604‚Äì1612.

224


