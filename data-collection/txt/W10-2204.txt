



















































Maximum Likelihood Estimation of Feature-Based Distributions


Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 28–37,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

Maximum Likelihood Estimation of Feature-based Distributions

Jeffrey Heinz and Cesar Koirala

University of Delaware

Newark, Delaware, USA

{heinz,koirala}@udel.edu

Abstract

Motivated by recent work in phonotac-

tic learning (Hayes and Wilson 2008, Al-

bright 2009), this paper shows how to de-

fine feature-based probability distributions

whose parameters can be provably effi-

ciently estimated. The main idea is that

these distributions are defined as a prod-

uct of simpler distributions (cf. Ghahra-

mani and Jordan 1997). One advantage

of this framework is it draws attention to

what is minimally necessary to describe

and learn phonological feature interactions

in phonotactic patterns. The “bottom-up”

approach adopted here is contrasted with

the “top-down” approach in Hayes and

Wilson (2008), and it is argued that the

bottom-up approach is more analytically

transparent.

1 Introduction

The hypothesis that the atomic units of phonology

are phonological features, and not segments, is one

of the tenets of modern phonology (Jakobson et

al., 1952; Chomsky and Halle, 1968). Accord-

ing to this hypothesis, segments are essentially

epiphenomenal and exist only by virtue of being

a shorthand description of a collection of more

primitive units—the features. Incorporating this

hypothesis into phonological learning models has

been the focus of much influential work (Gildea

and Jurafsky, 1996; Wilson, 2006; Hayes and Wil-

son, 2008; Moreton, 2008; Albright, 2009).

This paper makes three contributions. The first

contribution is a framework within which:

1. researchers can choose which statistical in-

dependence assumptions to make regarding

phonological features;

2. feature systems can be fully integrated into

strictly local (McNaughton and Papert, 1971)

(i.e. n-gram models (Jurafsky and Martin,

2008)) and strictly piecewise models (Rogers

et al., 2009; Heinz and Rogers, 2010) in

order to define families of provably well-

formed, feature-based probability distribu-

tions that are provably efficiently estimable.

The main idea is to define a family of distribu-

tions as the normalized product of simpler distri-

butions. Each simpler distribution can be repre-

sented by a Probabilistic Deterministic Finite Ac-

ceptor (PDFA), and the product of these PDFAs

defines the actual distribution. When a family of

distributions F is defined in this way, F may have
many fewer parameters than if F is defined over
the product PDFA directly. This is because the pa-

rameters of the distributions are defined in terms

of the factors which combine in predictable ways

via the product. Fewer parameters means accurate

estimation occurs with less data and, relatedly, the

family contains fewer distributions.

This idea is not new. It is explicit in Facto-

rial Hidden Markov Models (FHMMs) (Ghahra-

mani and Jordan, 1997; Saul and Jordan, 1999),

and more recently underlies approaches to de-

scribing and inferring regular string transductions

(Dreyer et al., 2008; Dreyer and Eisner, 2009).

Although HMMs and probabilistic finite-state au-

tomata describe the same class of distributions

(Vidal et al., 2005a; Vidal et al., 2005b), this paper

presents these ideas in formal language-theoretic

and automata-theoretic terms because (1) there are

no hidden states and is thus simpler than FHMMs,

(2) determinstic automata have several desirable

properties crucially used here, and (3) PDFAs

add probabilities to structure whereas HMMs add

structure to probabilities and the authors are more

comfortable with the former perspective (for fur-

ther discussion, see Vidal et al. (2005a,b)).

The second contribution illustrates the main

idea with a feature-based bigram model with a

28



strong statistical independence assumption: no

two features interact. This is shown to capture ex-

actly the intuition that sounds with like features

have like distributions. Also, the assumption of

non-interacting features is shown to be too strong

because like sounds do not have like distributions

in actual phonotactic patterns. Four kinds of fea-

tural interactions are identified and possible solu-

tions are discussed.

Finally, we compare this proposal with Hayes

and Wilson (2008). Essentially, the model here

represents a “bottom-up” approach whereas theirs

is “top-down.” “Top-down” models, which con-

sider every set of features as potentially interact-

ing in every allowable context, face the difficult

problem of searching a vast space and often re-

sort to heuristic-based methods, which are diffi-

cult to analyze. To illustrate, we suggest that the

role played by phonological features in the phono-

tactic learner in Hayes and Wilson (2008) is not

well-understood. We demonstrate that classes of

all segments but one (i.e. the complement classes

of single segments) play a significant role, which

diminishes the contribution provided by natural

classes themselves (i.e. ones made by phonologi-

cal features). In contrast, the proposed model here

is analytically transparent.

This paper is organized as follows. §2 reviews
some background. §3 discusses bigram models
and §4 defines feature systems and feature-based
distributions. §5 develops a model with a strong
independence assumption and §6 discusses feat-
ural interaction. §7 dicusses Hayes and Wilson
(2008) and §8 concludes.

2 Preliminaries

We start with mostly standard notation. P(A) is
the powerset of A. Σ denotes a finite set of sym-
bols and a string over Σ is a finite sequence of
these symbols. Σ+ and Σ∗ denote all strings over
this alphabet of nonzero but finite length, and of

any finite length, respectively. A function f with
domain A and codomain B is written f : A → B.
When discussing partial functions, the notation ↑
and ↓ indicate for particular arguments whether
the function is undefined and defined, respectively.

A language L is a subset of Σ∗. A stochastic
language D is a probability distribution over Σ∗.
The probability p of word w with respect to D is
written PrD(w) = p. Recall that all distributions
D must satisfy∑w∈Σ∗ PrD(w) = 1. If L is lan-

guage then PrD(L) =
∑

w∈L PrD(w). Since all
distributions in this paper are stochastic languages,

we use the two terms interchangeably.

A Probabilistic Deterministic Finite-

state Automaton (PDFA) is a tuple

M = 〈Q,Σ, q0, δ, F, T 〉 where Q is the state
set, Σ is the alphabet, q0 is the start state, δ is
a deterministic transition function, F and T are
the final-state and transition probabilities. In

particular, T : Q × Σ → R+ and F : Q → R+
such that

for all q ∈ Q, F (q) +
∑
σ∈Σ

T (q, σ) = 1. (1)

PDFAs are typically represented as labeled di-

rected graphs (e.g. M′ in Figure 1).
A PDFA M generates a stochastic language

DM. If it exists, the (unique) path for a word w =
a0 . . . ak belonging to Σ∗ through a PDFA is a
sequence 〈(q0, a0), (q1, a1), . . . , (qk, ak)〉, where
qi+1 = δ(qi, ai). The probability a PDFA assigns
tow is obtained by multiplying the transition prob-
abilities with the final probability along w’s path if
it exists, and zero otherwise.

PrDM(w) =

(
k∏

i=0

T (qi, ai)

)
·F (qk+1) (2)

if d̂(q0, w)↓ and 0 otherwise

A stochastic language is regular deterministic iff

there is a PDFA which generates it.

The structural components of a PDFAM is the
deterministic finite-state automata (DFA) given by

the states Q, alphabet Σ, transitions δ, and initial
state q0 of M. By the structure of a PDFA, we
mean its structural components.1 Each PDFAM
defines a family of distributions given by the pos-

sible instantiations of T and F satisfying Equa-
tion 1. These distributions have at most |Q|· (|Σ|+
1) parameters (since for each state there are |Σ|
possible transitions plus the possibility of finality.)

These are, for all q ∈ Q and σ ∈ Σ, the proba-
bilities T (q, σ) and F (q). To make the connection
to probability theory, we sometimes write these as

Pr(σ | q) and Pr(# | q), respectively.
We define the product of PDFAs in terms of

co-emission probabilities (Vidal et al., 2005a).

Let M1 = 〈Q1,Σ1, q01, δ1, F1, T1〉 and M2 =
1This is up to the renaming of states so PDFA with iso-

morphic structural components are said to have the same
structure.

29



〈Q2,Σ2, q02, δ2, F2, T2〉 be PDFAs. The proba-
bility that σ1 is emitted from q1 ∈ Q1 at the
same moment σ2 is emitted from q2 ∈ Q2 is
CT (σ1, σ2, q1, q2) = T1(q1, σ1)·T2(q2, σ2). Sim-
ilarly, the probability that a word simultaneously

ends at q1 ∈ Q1 and at q2 ∈ Q2 is CF (q1, q2) =
F1(q1)·F2(q2).
Definition 1 The normalized co-emission product

of PDFAs M1 and M2 is M = M1 × M2 =
〈Q,Σ, q0, δ, F, T 〉 where

1. Q, q0, and F are defined in terms of the
standard DFA product over the state space

Q1 ×Q2 (Hopcroft et al., 2001).

2. Σ = Σ1 × Σ2
3. For all 〈q1, q2〉 ∈ Q and 〈σ1, σ2〉 ∈

Σ, δ(〈q1, q2〉, 〈σ1, σ2〉) = 〈q′1, q′2〉 iff
δ1(q1, σ1) = q′1 and δ2(q2, σ2) = q′2.2

4. For all 〈q1, q2〉 ∈ Q,
(a) let Z(〈q1, q2〉) = CF (〈q1, q2〉) +∑

〈σ1,σ2〉∈Σ CT (σ1, σ2, q1, q2) be the
normalization term; and

(b) F (〈q1, q2〉) = CF (q1,q2)Z ; and
(c) for all 〈σ1, σ2〉 ∈ Σ,

T (〈q1, q2〉, 〈σ1, σ2〉) =
CT (〈σ1,σ2,q1,q2〉)

Z

In other words, the numerators of T and F are
defined to be the co-emission probabilities, and

division by Z ensures that M defines a well-
formed probability distribution.3 The normalized

co-emission product effectively adopts a statisti-

cal independence assumption between the states

ofM1 andM2. If S is a list of PDFAs, we write⊗
S for their product (note order of product is ir-

relevant up to renaming of the states).

The maximum likelihood (ML) estimation of

regular deterministic distributions is a solved

problem when the structure of the PDFA is known

(Vidal et al., 2005a; Vidal et al., 2005b; de la

Higuera, 2010). Let S be a finite sample of words
drawn from a regular deterministic distribution D.
The problem is to estimate parameters T and F of

2Note that restricting δ to cases when σ1 = σ2 obtains
the standard definition of δ = δ1× δ2 (Hopcroft et al., 2001).
The reason we maintain two alphabets becomes clear in §4.

3Z(〈q1, q2〉) is less than one whenever either F1(q1) or
F2(q2) are neither zero nor one.

M so that DM approaches D using the widely-
adopted ML criterion (Equation 3).

(T̂ , F̂ ) = argmax
T,F

(∏
w∈S

PrM(w)

)
(3)

It is well-known that if D is generated by some
PDFAM′ with the same structural components as
M, then the ML estimate of S with respect to M
guarantees that DM approaches D as the size of
S goes to infinity (Vidal et al., 2005a; Vidal et al.,
2005b; de la Higuera, 2010).

Finding the ML estimate of a finite sample S
with respect to M is simple provided M is de-
terministic with known structural components. In-

formally, the corpus is passed through the PDFA,

and the paths of each word through the corpus are

tracked to obtain counts, which are then normal-

ized by state. LetM = 〈Q,Σ, δ, q0, F, T 〉 be the
PDFA whose parameters F and T are to be esti-

mated. For all states q ∈ Q and symbols σ ∈ Σ,
The ML estimation of the probability of T (q, σ)
is obtained by dividing the number of times this

transition is used in parsing the sample S by the
number of times state q is encountered in the pars-
ing of S. Similarly, the ML estimation of F (q) is
obtained by calculating the relative frequency of

state q being final with state q being encountered
in the parsing of S. For both cases, the division is
normalizing; i.e. it guarantees that there is a well-

formed probability distribution at each state. Fig-

ure 1 illustrates the counts obtained for a machine

M with sample S = {abca}.4 Figure 1 shows
a DFA with counts and the PDFA obtained after

normalizing these counts.

3 Strictly local distributions

In formal language theory, strictly k-local lan-
guages occupy the bottom rung of a subregular

hierarchy which makes distinctions on the basis

of contiguous subsequences (McNaughton and Pa-

pert, 1971; Rogers and Pullum, to appear; Rogers

et al., 2009). They are also the categorical coun-

terpart to stochastic languages describable with n-
gram models (where n = k) (Garcia et al., 1990;
Jurafsky and Martin, 2008). Since stochastic lan-

guages are distributions, we refer to strictly k-
local stochastic languages as strictly k-local distri-

4Technically,M is neither a simple DFA or PDFA; rather,
it has been called a Frequency DFA. We do not formally de-
fine them here, see de la Higuera (2010).

30



A:1

a :2

b:1

c:1

A:1/5

a:2/5

b:1/5

c:1/5

M M′

Figure 1:M shows the counts obtained by parsing
it with sample S = {abca}. M′ shows the proba-
bilities obtained after normalizing those counts.

butions (SLDk). We illustrate with SLD2 (bigram

models) for ease of exposition.

For an alphabet Σ, SL2 distributions have
(|Σ| + 1)2 parameters. These are, for all σ, τ ∈
Σ ∪ {#}, the probabilities Pr(σ | τ). The proba-
bility of w = σ1 . . . σn is given in Equation 4.

Pr(w) def= Pr(σ1 | #)× Pr(σ2 | σ1)
× . . .× Pr(# | σn)

(4)

PDFA representations of SL2 distributions have

the following structure: Q = Σ ∪ {#}, q0 = #,
and for all q ∈ Q and σ ∈ Σ, it is the case that
δ(q, σ) = σ.
As an example, the DFA in Figure 2 provides

the structure of PDFAs which recognize SL2 dis-

tributions with Σ = {a, b, c}. Plainly, the param-
eters of the model are given by assigning proba-

bilities to each transition and to the ending at each

state. In fact, for all σ ∈ Σ and τ ∈ Σ ∪ {#},
Pr(σ | τ) is T (τ, σ) and Pr(# | τ) is F (τ).
It follows that the probability of a particular path

through the model corresponds to Equation 4. The

structure of a SL2 distribution for alphabet Σ is
given byMSL2(Σ).
Additionally, given a finite sample S ⊂ Σ∗, the

ML estimate of S with respect to the family of
distributions describable with MSL2(Σ) is given
by counting the parse of S through MSL2(Σ) and
then normalizing as described in §2. This is equiv-
alent to the procedure described in Jurafsky and

Martin (2008, chap. 4).

4 Feature-based distributions

This section first introduces feature systems. Then

it defines feature-based SL2 distributions which

make the strong independence assumption that no

two features interact. It explains how to find

b

a

c

 b

a

 c

 b

a

c

 b

a
 c

#

a

b

c

Figure 2: MSL2({a, b, c}) represents the structure
of SL2 distributions when Σ = {a, b, c}.

F G

a + -

b + +

c - +

Table 1: An example of a feature system with Σ =
{a, b, c} and two features F and G.

the ML estimate of samples with respect to such

distributions. This section closes by identifying

kinds of featural interactions in phonotactic pat-

terns, and discusses how such interactions can be

addressed within this framework.

4.1 Feature systems

Assume the elements of the alphabet share prop-

erties, called features. For concreteness, let each

feature be a total function F : Σ → VF , where
the codomain VF is a finite set of values. A fi-
nite vector of features F = 〈F1, . . . , Fn〉 is called
a feature system. Table 1 provides an example

of a feature system with F = 〈F,G〉 and values
VF = VG = {+,−}.
We extend the domain of all features F ∈ F

to Σ+, so that F (σ1 . . . σn) = F (σ1) . . . F (σn).
For example, using the feature system in Table 1,

F (abc) = + + − and G(abc) = − + +. We
also extend the domain of F to all languages:
F (L) = ∪w∈Lf(w). We also extend the notation
so that F(σ) = 〈F1(σ), . . . , Fn(σ)〉. For example,
F(c) = 〈−F,+G〉 (feature indices are included
for readability).

For feature F : Σ → VF , let F−1 be the inverse
function with domain VF and codomain P(Σ).
For example in Table 1, G−1(+) = {b, c}. F−1
is similarly defined, i.e. F−1(〈−F,+G〉) = {c}.

31



If, for all arguments ~v, F−1(~v) is nonempty then
the feature system is exhaustive. If, for all argu-

ments ~v such that F−1(~v) is nonempty, it is the
case that |F−1(~v)| = 1 then the feature system is
distinctive. E.g. the feature system in Table 1 in

not exhaustive since F−1(〈−F,−G〉) = ∅, but it is
distinctive since where F−1 is nonempty, it picks
out exactly one element of the alphabet.

Generally, phonological feature systems for a

particular language are distinctive but not exhaus-

tive. Any feature system F can be made exhaustive
by adding finitely many symbols to the alphabet

(since F is finite). Let Σ′ denote an alphabet ob-
tained by adding to Σ the fewest symbols which
make F exhaustive.
Each feature system also defines a set of indi-

cator functions VF =
⋃

f∈F(Vf × {f}) with do-
main Σ such that 〈v, f〉(σ) = 1 iff f(σ) = v and
0 otherwise. In the example in Table 1, VF =
{+F,−F,+G,−G} (omitting angle braces for
readability). For all f ∈ F, the set VFf is the
VF restricted to f . So continuing our example,
VFF = {+F,−F}.
4.2 Feature-based distributions

We now define feature-based SL2 distributions un-

der the strong independence assumption that no

two features interact. For feature system F =
〈F1 . . . Fn〉, there are n PDFAs, one for each fea-
ture. The normalized co-emission product of these

PDFAs essentially defines the distribution. For

each Fi, the structure of its PDFA is given by
MSL2(VFi). For example, MF = MSL2(VF )
andMG = MSL2(VG) in figures 3 and 4 illustrate
the finite-state representation of feature-based SL2
distributions given the feature system in Table 1.5

The states of each machine make distinctions ac-

cording to features F and G, respectively. The pa-

rameters of these distributions are given by assign-

ing probabilities to each transition and to the end-

ing at each state (except for Pr(# | #)).6
Thus there are 2|VF| +∑F∈F |VFF |2 + 1 pa-

rameters for feature-based SL2 distributions. For

example, the feature system in Table 1 defines a

distribution with 2· 4 + 22 + 22 + 1 = 17 param-
5For readability, featural information in the states and

transitions is included in these figures. By definition, the
states and transitions are only labeled with elements of VF
and VG, respectively. In this case, that makes the structures
of the two machines identical.

6It is possible to replace Pr(# | #) with two parameters,
Pr(# | #F ) Pr(# | #G), but for ease of exposition we do
not pursue this further.

-F
-F

+F
+F

-F

+F

-F

+F

#

Figure 3: MF represents a SL2 distribution with
respect to feature F.

-G
-G

+G
+G

-G

+G

-G

+G

#

Figure 4: MG represents a SL2 distribution with
respect to feature G.

eters, which include Pr(# | +F ), Pr(+F | #),
Pr(+F | +F ), Pr(+F | −F ), . . . , the G equiva-
lents, and Pr(# | #). Let SLD2F be the family of
distributions given by all possible parameter set-

tings (i.e. all possible probability assignments for

eachMSL2(VFi) in accordance with Equation 1.)
The normalized co-emission product defines the

feature-based distribution. For example, the struc-

ture of the product of MF and MG is shown in
Figure 5.

As defined, the normalized co-emission product

can result in states and transitions that cannot be

interpreted by non-exhaustive feature systems. An

example of this is in Figure 5 since 〈−F,−G〉 is
not interpretable by the feature system in Table 1.

We make the system exhaustive by letting Σ′ =
Σ ∪ {d} and setting F(d) = 〈−F,−G〉.
What is the probability of a given b in the

feature-based model? According to the normal-

ized co-emission product (Defintion 1), it is

Pr(a | b) = Pr(〈+F,−G〉 | 〈+F,+G〉) =
Pr(+F | +F )·Pr(−G | +G)

Z

where Z = Z(〈+F,+G〉) equals∑
σ∈Σ′

Pr(F (σ) | +F )·Pr(G(σ) | +G)

+ (Pr(# | +F )·Pr(# | +G)
Generally, for an exhuastive distinctive feature

system F = 〈F1, . . . , Fn〉, and for all σ, τ ∈ Σ,

32



#

+F,-G

+F,-G

+F ,+G

+F ,+G

-F,+G

-F,+G

-F,-G

-F,-G

+F,-G

+F ,+G

-F,+G

-F,-G

+F,-G

+F ,+G

-F,+G

-F,-G

+F,-G

+F ,+G
-F,+G

-F,-G

+F,-G

+F ,+G

-F,+G-F,-G

Figure 5: The structure of the product ofMF andMG.

the Pr(σ | τ) is given by Equation 5. First, the
normalization term is provided. Let

Z(τ) =
∑
σ∈Σ

 ∏
1≤i≤n

Pr(Fi(σ) | Fi(τ))


+
∏

1≤i≤n
Pr(# | Fi(τ))

Then

Pr(σ | τ) =
∏

1≤i≤n Pr(Fi(σ) | Fi(τ))
Z(τ)

(5)

The probabilities Pr(σ | #) and Pr(# | τ)
are similarly decomposed into featural parameters.

Finally, like SL2 distributions, the probability of a

word w ∈ Σ∗ is given by Equation 4. We have
thus proved the following.

Theorem 1 The parameters of a feature-based

SL2 distribution define a well-formed probability

distribution over Σ∗.

Proof It is sufficient to show for all τ ∈ Σ ∪ {#}
that

∑
σ∈Σ∪{#} Pr(σ | τ) = 1 since in this

case, Equation 4 yields a well-formed probability

distribution over Σ∗. This follows directly from
the definition of the normalized co-emission

product (Definition 1). ���

The normalized co-emission product adopts a

statistical independence assumption, which here is

between features since each machine represents a

single feature. For example, consider Pr(a | b) =
Pr(〈−F,+G〉 | 〈+F,+G〉). The probability
Pr(〈−F,+G〉 | 〈+F,+G〉) cannot be arbitrar-
ily different from the probabilities Pr(−F | +F )

and Pr(+G | +G); it is not an independent pa-
rameter. In fact, because Pr(a | b) is computed
directly as the normalized product of parameters

Pr(−F | +F ) and Pr(+G | +G), the assump-
tion is that the features F and G do not interact. In
other words, this model describes exactly the state

of affairs one expects if there is no statistical in-

teraction between phonological features. In terms

of inference, this means if one sound is observed

to occur in some context (at least contexts dis-

tinguishable by SL2 models), then similar sounds

(i.e. those that share many of its featural values)

are expected to occur in this context as well.

4.3 ML estimation

The ML estimate of feature-based SL2 distribu-

tions is obtained by counting the parse of a sample

through each feature machine, and normalizing the

results. This is because the parameters of the dis-

tribution are the probabilities on the feature ma-

chines, whose product determines the actual dis-

tribution. The following theorem follows imme-

diately from the PDFA representation of feature-

based SL2 distributions.

Theorem 2 Let F = 〈F1, . . . Fn〉 and let D be de-
scribed by M = ⊗1≤i≤nMSL2(VF i). Consider
a finite sample S drawn from D. Then the ML es-
timate of S with respect to SLD2F is obtained by
finding, for each Fi ∈ F, the ML estimate of Fi(S)
with respect toMSL2(VF i).
Proof The ML estimate of S with respect to
SLD2F returns the parameter values that maxi-

mize the likelihood of S within the family SLD2F.
The parameters of D ∈SLD2F are found on the

33



states of each MSL2(VFi). By definition, each
MSL2(VFi) describes a probability distribution
over Fi(Σ∗), as well as a family of distributions.
Therefore finding the MLE of S with respect to
SLD2F means finding the MLE estimate of Fi(S)
with respect to eachMSL2(VFi).
Optimizing the ML estimate of Fi(S) for

each Mi = MSL2(VFi) means that as |Fi(S)|
increases, the estimates T̂Mi and F̂Mi approach
the true values TMi and FMi . It follows that
as |S| increases, T̂M and F̂M approach the true
values of TM and FM and consequently DM
approaches D. ���

4.4 Discussion

Feature-based models can have significantly fewer

parameters than segment-based models. Con-

sider binary feature systems, where |VF| = 2|F|.
An exhaustive feature system with 10 binary fea-

tures describes an alphabet with 1024 symbols.

Segment-based bigram models have (1024+1)2 =
1, 050, 625 parameters, but the feature-based one
only has 40 + 40 + 1 = 81 parameters! Con-
sequently, much less training data is required to

accurately estimate the parameters of the model.

Another way of describing this is in terms of ex-

pressivity. For given feature system, feature-based

SL2 distributions are a proper subset of SL2 dis-

tributions since, as the the PDFA representations

make clear, every feature-based distribution can be

described by a segmental bigram model, but not

vice versa. The fact that feature-based distribu-

tions have potentially far fewer parameters is a re-

flection of the restrictive nature of the model. The

statistical independence assumption constrains the

system in predictable ways. The next section

shows exactly what feature-based generalization

looks like under these assumptions.

5 Examples

This section demonstrates feature-based gener-

alization by comparing it with segment-based

generalization, using a small corpus S =
{aaab, caca, acab, cbb} and the feature system
in Table 1. Tables 2 and 3 show the results of

ML estimation of S with respect to segment-based
SL2 distributions (unsmoothed bigram model)

and feature-based SL2 distributions, respectively.

Each table shows the Pr(σ | τ) for all σ, τ ∈
{a, b, c, d,#} (where F(d) = 〈−F,−G〉), for

σ
P(σ | τ )

a b c d #

a 0.29 0.29 0.29 0. 0.14

b 0. 0.25 0. 0. 0.75

τ c 0.75 0.25 0. 0. 0.
d 0. 0. 0. 0. 0.

# 0.5 0. 0.5 0. 0.

Table 2: ML estimates of parameters of segment-

based SL2 distributions.

σ
P(σ | τ )

a b c d #

a 0.22 0.43 0.17 0.09 0.09

b 0.32 0.21 0.09 0.13 0.26

τ c 0.60 0.40 0. 0 0.
d 0.33 0.67 0 0 0

# 0.25 0.25 0.25 0.25 0.

Table 3: ML estimates of parameters of feature-

based SL2 distributions.

ease of comparison.

Observe the sharp divergence between the two

models in certain cells. For example, no words be-

gin with b in the sample. Hence the segment-based
ML estimates of Pr(b | #) is zero. Conversely,
the feature-based ML estimate is nonzero because

b, like a, is +F, and b, like c, is +G, and both a
and c begin words. Also, notice nonzero probabil-
ities are assigned to d occuring after a and b. This
is because F(d) = 〈−F,−G〉 and the following
sequences all occur in the corpus: [+F][-F] (ac),
[+G][-G] (ca), and [-G][-G] (aa). On the other
hand, zero probabilities are assigned to d ocurring
after c and d because there are no cc sequences in
the corpus and hence the probability of [-F] occur-

ing after [-F] is zero.

This simple example demonstrates exactly how

the model works. Generalizations are made on the

basis of individual features, not individual sym-

bols. In fact, segments are truly epiphenomenal in

this model, as demonstrated by the nonzero prob-

abilties assigned to segments outside the original

feature system (here, this is d). To sum up, this
model captures exactly the idea that the distribu-

tion of segments is conditioned on the distribu-

tions of its features.

34



6 Featural interaction

In many empirical cases of interest, features do

interact, which suggests the strong independence

assumption is incorrect for modeling phonotactic

learning.

There are at least four kinds of featural inter-

action. First, different features may be prohib-

ited from occuring simultaneously in certain con-

texts. As an example of the first type consider

the fact that both velars and nasal sounds occur

word-initially in English, but the velar nasal may

not. Second, specific languages may prohibit dif-

ferent features from simultaneously occuring in all

contexts. In English, for example, there are syl-

labic sounds and obstruents but no syllabic obstru-

ents. Third, different features may be universally

incompatible: e.g. no vowels are both [+high] and

[+low]. The last type of interaction is that different

features may be prohibited from occuring syntag-

matically. For example, some languages prohibit

voiceless sounds from occuring after nasals.

Although the independence assumption is too

strong, it is still useful. First, it allows researchers

to quantify the extent to which data can be ex-

plained without invoking featural interaction. For

example, following Hayes and Wilson (2008), we

may be interested in how well human acceptabil-

ity judgements collected by Scholes (1966) can be

explained if different features do not interact. Af-

ter training the feature-based SL2 model on a cor-

pus of word initial onsets adapted from the CMU

pronouncing dictionary (Hayes and Wilson, 2008,

395-396) and using a standard phonological fea-

ture system (Hayes, 2009, chap. 4), it achieves

a correlation (Spearman’s r) of 0.751.7 In other
words, roughly three quarters of the acceptability

judgements are explained without relying on feat-

ural interaction (or segments).

Secondly, the incorrect predictions of the model

are in principle detectable. For example, recall

that English has word-inital velars and nasals, but

no word-inital velar nasals. A one-cell chi-squared

test can determine whether the observed number

of [#N] is significantly below the expected number
according to the feature-based distribution, which

could lead to a new parameter being adopted to

describe the interaction of the [dorsal] and [nasal]

7We use the feature chart in Hayes (2009) because it con-
tains over 150 IPA symbols (and not just English phonemes).
Featural combinations not in the chart were assumed to be
impossible (e.g. [+high,+low]) and were zeroed out.

features word-initially. The details of these proce-

dures are left for future research and are likely to

draw from the rich literature on Bayesian networks

(Pearl, 1989; Ghahramani, 1998).

More important, however, is this framework al-

lows researchers to construct the independence as-

sumptions they want into the model in at least two

ways. First, universally incompatible features can

be excluded. For example, suppose [-F] and [-G]

in the feature system in Table 1 are anatomically

incompatible like [+low] and [+high]. If desired,

they can be excluded from the model essentially

by zeroing out any probability mass assigned to

such combinations and re-normalizing.

Second, models can be defined where multiple

features are permitted to interact. For example,

suppose features F and G from Table 1 are em-

bedded in a larger feature system. The machine

in Figure 5 can be defined to be a factor of the

model, and now interactions between F and G will

be learned, including syntagmatic ones. The flex-

ibility of the framework and the generality of the

normalized co-emission product allow researchers

to consider feature-based distributions which al-

low any two features to interact but which pro-

hibit three-feature interactions, or which allow any

three features to interact but which prohibit four-

feature interactions, or models where only certain

features are permitted to interact but not others

(perhaps because they belong to the same node in a

feature geometry (Clements, 1985; Clements and

Hume, 1995).8

7 Hayes and Wilson (2008)

This section introduces the Hayes and Wilson

(2008) (henceforth HW) phonotactic learner and

shows that the contribution features play in gener-

alization is not as clear as previously thought.

HW propose an inductive model which ac-

quires a maxent grammar defined by weighted

constraints. Each constraint is described as a se-

quence of natural classes using phonological fea-

tures. The constraint format also allows reference

to word boundaries and at most one complement

class. (The complement class of S ⊆ Σ is Σ/S.)
For example, the constraint

*#[ˆ -voice,+anterior,+strident][-approximant]

means that in word-initial C1C2 clusters, if C2 is a

nasal or obstruent, then C1 must be [s].

8Note if all features are permitted to interact, this yields
the segmental bigram model.

35



Hayes and Wilson maxent models r

features & complement classes 0.946

no features & complement classes 0.937

features & no complement classes 0.914

no features & no complement classes 0.885

Table 4: Correlations of different settings versions

of HW maxent model with Scholes data.

HW report that the model obtains a correlation

(Spearman’s r) of 0.946 with blick test data from
Scholes (1966). HW and Albright (2009) attribute

this high correlation to the model’s use of natural

classes and phonological features. HW also report

that when the model is run without features, the

grammar obtained scores an r value of only 0.885,
implying that the gain in correlation is due specif-

ically to the use of phonological features.

However, there are two relevant issues. The first

is the use of complement classes. If features are

not used but complement classes are (in effect only

allowing the model to refer to single segments and

the complements of single segments, e.g. [t] and

[ˆt]) then in fact the grammar obtained scores an

r value of 0.936, a result comparable to the one
reported.9 Table 4 shows the r values obtained by
the HW learner under different conditions. Note

we replicate the main result of r = 0.946 when
using both features and complement classes.10

This exercise reveals that phonological features

play a smaller role in the HW phonotactic learner

than previously thought. Features are helpful, but

not as much as complement classes of single seg-

ments (though features with complement classes

yields the best result by this measure).

The second issue relates to the first: the question

of whether additional parameters are worth the

gain in empirical coverage. Wilson and Obdeyn

(2009) provide an excellent discussion of the

model comparison literature and provide a rigor-

ous comparative analysis of computational mod-

eleling of OCP restrictions. Here we only raise the

questions and leave the answers to future research.

Compare the HW learners in the first two rows

in Table 4. Is the ∼ 0.01 gain in r score worth
the additional parameters which refer to phono-

9Examination of the output grammar reveals heavy re-
liance on the complement class [ˆs], which is not surprising
given the discussion of [sC] clusters in HW.

10This software is available on Bruce Hayes’ webpage:
http://www.linguistics.ucla.edu/

people/hayes/Phonotactics/index.htm.

logically natural classes? Also, the feature-based

SL2 model in §4 only receives an r score of 0.751,
much lower than the results in Table 4. Yet this

model has far fewer parameters not only because

the maxent models in Table 4 keep track of tri-

grams, but also because of its strong independence

assumption. As mentioned, this result is infor-

mative because it reveals how much can be ex-

plained without featural interaction. In the con-

text of model comparison, this particular model

provides an inductive baseline against which the

utility of additional parameters invoking featural

interaction ought to be measured.

8 Conclusion

The current proposal explicitly embeds the Jakob-

sonian hypothesis that the primitive unit of

phonology is the phonological feature into a

phonotactic learning model. While this paper

specifically shows how to integrate features into

n-gram models to describe feature-based strictly

n-local distributions, these techniques can be ap-

plied to other regular deterministic distributions,

such as strictly k-piecewise models, which de-
scribe long-distance dependencies, like the ones

found in consonant and vowel harmony (Heinz, to

appear; Heinz and Rogers, 2010).

In contrast to models which assume that all

features potentially interact, a baseline model

was specifically introduced under the assumption

that no two features interact. In this way, the

“bottom-up” approach to feature-based general-

ization shifts the focus of inquiry to the featural

interactions necessary (and ultimately sufficient)

to describe and learn phonotactic patterns. The

framework introduced here shows how researchers

can study feature interaction in phonotactic mod-

els in a systematic, transparent way.

Acknowledgments

We thank Bill Idsardi, Tim O’Neill, Jim Rogers,

Robert Wilder, Colin Wilson and the U. of

Delaware’s phonology/phonetics group for valu-

able discussion. Special thanks to Mark Ellison

for helpful comments, to Adam Albright for illu-

minating remarks on the types of featural interac-

tion in phonotactic patterns, and to Jason Eisner

for bringing to our attention FHMMs and other re-

lated work.

36



References

Adam Albright. 2009. Feature-based generalisation
as a source of gradient acceptability. Phonology,
26(1):9–41.

Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper & Row, New York.

G.N. Clements and Elizabeth V. Hume. 1995. The
internal organization of speech sounds. In John A.
Goldsmith, editor, The handbook of phonological
theory, chapter 7. Blackwell, Cambridge, MA.

George N. Clements. 1985. The geometry of phono-
logical features. Phonology Yearbook, 2:225–252.

Colin de la Higuera. 2010. Grammatical Inference:
Learning Automata and Grammars. Cambridge
University Press.

Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 101–110, Singa-
pore, August.

Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with finite-state methods. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1080–1089,
Honolulu, October.

Pedro Garcia, Enrique Vidal, and José Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325–338.

Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2):245–273.

Zoubin Ghahramani. 1998. Learning dynamic
bayesian networks. In Adaptive Processing of
Sequences and Data Structures, pages 168–197.
Springer-Verlag.

Daniel Gildea and Daniel Jurafsky. 1996. Learn-
ing bias and phonological-rule induction. Compu-
tational Linguistics, 24(4).

Bruce Hayes and ColinWilson. 2008. Amaximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379–440.

Bruce Hayes. 2009. Introductory Phonology. Wiley-
Blackwell.

Jeffrey Heinz and James Rogers. 2010. Estimating
strictly piecewise distributions. In Proceedings of
the 48th AnnualMeeting of the Association for Com-
putational Linguistics, Uppsala, Sweden.

Jeffrey Heinz. to appear. Learning long-distance
phonotactics. Linguistic Inquiry, 41(4).

John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
2001. Introduction to Automata Theory, Languages,
and Computation. Boston, MA: Addison-Wesley.

Roman Jakobson, C. Gunnar, M. Fant, and Morris
Halle. 1952. Preliminaries to Speech Analysis.
MIT Press.

Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, Upper
Saddle River, NJ, 2nd edition.

Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.

Elliot Moreton. 2008. Analytic bias and phonological
typology. Phonology, 25(1):83–127.

Judea Pearl. 1989. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kauffman.

James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.

James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2009. On languages piecewise testable in
the strict sense. In Proceedings of the 11th Meeting
of the Assocation for Mathematics of Language.

Lawrence K. Saul and Michael I. Jordan. 1999. Mixed
memory markov models: Decomposing complex
stochastic processes as mixtures of simpler ones.
Machine Learning, 37(1):75–87.

Robert J. Scholes. 1966. Phonotactic grammaticality.
Mouton, The Hague.

Enrique Vidal, Franck Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005a. Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis andMachine
Intelligence, 27(7):1013–1025.

Enrique Vidal, Frank Thollard, Colin de la Higuera,
Francisco Casacuberta, and Rafael C. Carrasco.
2005b. Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis andMachine
Intelligence, 27(7):1026–1039.

Colin Wilson and Marieke Obdeyn. 2009. Simplifying
subsidiary theory: statistical evidence from arabic,
muna, shona, and wargamay. Johns Hopkins Uni-
versity.

Colin Wilson. 2006. Learning phonology with sub-
stantive bias: An experimental and computational
study of velar palatalization. Cognitive Science,
30(5):945–982.

37


