267

Coling 2010: Poster Volume, pages 267–275,

Beijing, August 2010

Exploring the Data-Driven Prediction of Prepositions in English

Anas Elghafari

Detmar Meurers

Holger Wunsch

Seminar f¨ur Sprachwissenschaft

Universit¨at T¨ubingen

{aelgafar,dm,wunsch}@sfs.uni-tuebingen.de

Abstract

1

Introduction

Prepositions in English are a well-known
challenge for language learners, and the
computational analysis of preposition us-
age has attracted signiﬁcant attention.
Such research generally starts out by de-
veloping models of preposition usage for
native English based on a range of fea-
tures, from shallow surface evidence to
deep linguistically-informed properties.

While we agree that ultimately a com-
bination of shallow and deep features is
needed to balance the preciseness of ex-
emplars with the usefulness of generaliza-
tions to avoid data sparsity, in this paper
we explore the limits of a purely surface-
based prediction of prepositions.

Using a web-as-corpus approach, we in-
vestigate the classiﬁcation based solely on
the relative number of occurrences for tar-
get n-grams varying in preposition usage.
We show that such a surface-based ap-
proach is competitive with the published
state-of-the-art results relying on complex
feature sets.

Where enough data is available, in a sur-
prising number of cases it thus is possible
to obtain sufﬁcient information from the
relatively narrow window of context pro-
vided by n-grams which are small enough
to frequently occur but
large enough
to contain enough predictive information
about preposition usage.

The correct use of prepositions is a well-known
difﬁculty for learners of English, and correspond-
ingly the computational analysis of preposition
usage has attracted signiﬁcant attention in re-
cent years (De Felice and Pulman, 2007; De Fe-
lice, 2008; Lee and Knutsson, 2008; Gamon et
al., 2008; Chodorow et al., 2007; Tetreault and
Chodorow, 2008a, 2008b).

As a point of reference for the detection of
preposition errors in learner language, most of
the research starts out by developing a model of
preposition usage for native English. For this
purpose, virtually all previous approaches em-
ploy a machine learning setup combining a range
of features, from surface-based evidence to deep
linguistically-informed properties. The overall
task is approached as a classiﬁcation problem
where the classes are the prepositions and the in-
stances to be classiﬁed are the contexts, i.e., the
sentences with the prepositions omitted.

A focus of the previous literature is on the ques-
tion which linguistic and lexical features are the
best predictors for preposition usage. Linguistic
features used include the POS tags of the sur-
rounding words, PP attachment sites, WordNet
classes of PP object and modiﬁed item. Lexical
features used include the object of the PP and the
lexical item modiﬁed by the PP. Those syntactic,
semantic and lexical features are then extracted
from the training instances and used by the ma-
chine learning tool to predict the missing preposi-
tion in a test instance.

While we agree that ultimately a combination
of shallow and linguistically informed features is
needed to balance the preciseness of exemplars

268

with the usefulness of generalizations to avoid
data sparsity problems, in this paper we want to
explore the limits of a purely surface-based pre-
diction of prepositions. Essentially, our ques-
tion is how much predictive information can be
found in the immediate distributional context of
the preposition.
Is it possible to obtain n-gram
contexts for prepositions which are small enough
to occur frequently enough in the available train-
ing data but large enough to contain enough pre-
dictive information about preposition usage?

This perspective is related to that underlying
the variation-n-gram approach for detecting errors
in the linguistic annotation of corpora (Dickin-
son and Meurers, 2003; Dickinson and Meurers,
2005; Boyd et al., 2008). Under that approach, er-
rors in the annotation of linguistic properties (lexi-
cal, constituency, or dependency information) are
detected by identifying units which recur in the
corpus with sufﬁcient identical context so as to
make variation in their annotation unlikely to be
correct. In a sense, the recurring n-gram contexts
are used as exemplar references for the local do-
mains in which the complex linguistic properties
are established. The question now is to what ex-
tent basic1 n-gram contexts can also be success-
fully used to capture the linguistic properties and
relations determining preposition usage, explor-
ing the trade-off expressed in the question ending
the previous paragraph.

To address this question, in this paper we make
use of a web-as-corpus approach in the spirit of
Lapata and Keller (2005). We employ the Yahoo
search engine to investigate a preposition classiﬁ-
cation setup based on the relative number of web
counts obtained for target n-grams varying in the
preposition used. We start the discussion with a
brief review of key previous approaches and the
results they obtain for the preposition classiﬁca-
tion task in native English text.
In section 2,
we then describe the experimental setup we used

1While Dickinson and Meurers (2005) also employ dis-
continuous n-grams, we here focus only on contiguous n-
gram contexts. Using discontinuous n-gram contexts for
preposition prediction could be interesting to explore in the
future, once, as a prerequisite for the effective generation
of discontinuous n-grams, heuristics have been identiﬁed for
when which kind of discontinuities should be allowed to arise
for preposition classiﬁcation contexts.

for our exploration and discuss our results in sec-
tion 3.

1.1 Previous work and results
The previous work on the preposition prediction
task varied in i) the features selected, ii) the num-
ber of prepositions tackled, and iii) the training
and testing corpora used.

De Felice (2008) presents a system that (among
other things) is used to predict the correct prepo-
sition for a given context. The system tackles the
nine most frequent prepositions in English: of, to,
in, for, on, with, at, by, from. The approach uses a
wide variety of syntactic and semantic features:
the lexical item modiﬁed by the PP, the lexical
item that occurs as the object of the preposition,
the POS tags of three words to the left and three
words to the right of the preposition, the grammat-
ical relation that the preposition is in with its ob-
ject, the grammatical relation the preposition is in
with the word modiﬁed by the PP, and the Word-
Net classes of the preposition’s object and the lex-
ical item modiﬁed by the PP. De Felice (2008) also
used a named entity recognizer to extract general-
izations about which classes of named entities can
occur with which prepositions. Further, the verbs’
subcategorization frames were taken as features.
For features that used lexical sources (WordNet
classes, verbs subcategorization frames), only par-
tial coverage of the training and testing instances
is available.

The overall accuracy reported by De Felice
(2008) for this approach is 70.06%, testing on sec-
tion J of the British National Corpus (BNC) after
training on the other sections. As the most exten-
sive discussion of the issue, using an explicit set
of prepositions and a precisely speciﬁed and pub-
licly accessible test corpus, De Felice (2008) is
well-suited as a reference approach. Correspond-
ingly, our study in this paper is based on the same
set of prepositions and the same test corpus.

Gamon et al. (2008) introduce a system for the
detection of a variety of learner errors in non-
native English text, including preposition errors.
For the preposition task, the authors combine the
outputs of a classiﬁer and a language model. The
language model is a 5-gram model trained on the
English Gigaword corpus. The classiﬁer is trained

269

on Encarta encyclopedia and Reuters news text.
It operates in two stages: The presence/absence
classiﬁer predicts ﬁrst whether a preposition needs
to be inserted at a given location. Then, the choice
classiﬁer determines which preposition is to be in-
serted. The features that are extracted for each
possible insertion site come from a six-token win-
dow around the possible insertion site. Those fea-
tures are the relative positions, POS tags, and sur-
face forms of the tokens in that window. The
choice classiﬁer predicts one of 13 prepositions:
in, for, of, on, to, with, at, by, as, from, since,
about, than, and other. The accuracy of the choice
classiﬁer, the part of the system to which the work
at hand is most similar, is 62.32% when tested on
text from Encarta and Reuters news.

Tetreault and Chodorow (2008a) present a sys-
tem for detecting preposition errors in learner text.
Their approach extracts a total of 25 features from
the local contexts: the adjacent words, the heads
of the nearby phrases, and the POS tags of all
those. They combine word-based features with
POS tag features to better handle cases where a
word from the test instance has not been seen
in training. For each test instance, the system
predicts one of 34 prepositions.
In training and
testing performed on the Encarta encyclopedia,
Reuters news text and additional training material
an accuracy ﬁgure of 79% is achieved.

Bergsma et al. (2009) extract contextual fea-
tures from the Google 5-gram corpus to train an
SVM-based classiﬁer for predicting prepositions.
They evaluate on 10 000 sentences taken from the
New York Times section of the Gigaword corpus,
and achieve an accuracy of 75.4%.

Following De Felice (2008, p. 66), we summa-
rize the main results of the mentioned approaches
to preposition prediction for native text in Fig-
ure 1.2 Since the test sets and the prepositions tar-
geted differ between the approaches, such a com-
parison must be interpreted with caution. In terms
of the big picture, it is useful to situate the results
with respect to the majority baseline reported by
De Felice (2008). It is obtained by always choos-
ing of as the most common preposition in section
J of the BNC. De Felice also reports another inter-

esting ﬁgure included in Figure 1, namely the ac-
curacy of the human agreement with the original
text, averaged over two English native-speakers.

Approach
Gamon et al. (2008)
Tetreault and Chodorow (2008a)
Bergsma et al. (2009)
De Felice (2008) system
Majority baseline (of)
Human agreement

Accuracy
62.32%
79.00%
75.50%
70.06%
26.94%
88.60%

Figure 1: Preposition prediction results

2 Experiments

2.1 Data
As our test corpus, we use section J of the BNC,
the same corpus used by De Felice (2008). Based
on the tokenization as given in the corpus, we
join the tokens with a single space, which also
means that punctuation characters end up as sep-
arate, white-space separated tokens. We select all
sentences that contain one or more prepositions,
using the POS annotation in the corpus to iden-
tify the prepositions. The BNC is POS-annotated
with the CLAWS-5 tagset, which distinguishes the
two tags PRF for of and PRP for all other preposi-
tions.3 We mark every occurrence of these prepo-
sition tags in the corpus, yielding one prediction
task for each marked preposition. For example,
the sentence (1) yields four prediction tasks, one
for each of the prepositions for, of, from, and in in
the sentence.

(1) But for the young, it is rather a question
of the scales falling from their eyes, and
having nothing to believe in any more.

In each task, one preposition is masked using
the special marker -*-MASKED-*-. Figure 2
shows the four marked-up prediction tasks result-
ing for example (1).

Following De Felice (2008), we focus our ex-
periments on the top nine prepositions in the
BNC: of, to, in, for, on, with, at, by, from. For

2The Gamon et al. (2008) result differs from the one re-

ported in De Felice (2008); we rely on the original paper.

3http://www.natcorp.ox.ac.uk/docs/URG/

posguide.html#guidelines

270

But -*-MASKED-*-for the young , it is
rather a question of the scales falling
from their eyes , and having nothing to
believe in any more .

But for the young , it is rather a
question -*-MASKED-*-of the scales
falling from their eyes , and having
nothing to believe in any more .

But for the young , it is rather
a question of the scales falling
-*-MASKED-*-from their eyes , and having
nothing to believe in any more .

But for the young , it is rather a
question of the scales falling from
their eyes , and having nothing to
believe -*-MASKED-*-in any more .

Figure 2: Four prediction tasks for example (1)

each occurrence of these nine prepositions in sec-
tion J of the BNC, we extract one prediction task,
yielding a test set of 522 313 instances.

Evaluating on this full test set would involve a
prohibitively large number of queries to the Ya-
hoo search engine. We therefore extract a ran-
domly drawn subset of 10 000 prediction tasks.
From this subset, we remove all prediction tasks
which are longer than 4000 characters in length,
as Yahoo only supports queries up to that length.
Finally, in a web-as-corpus setup, the indexing of
the web pages performed by the search engine es-
sentially corresponds to the training step in a typi-
cal machine learning setup. In order to avoid test-
ing on the training data, we thus need to ensure
that the test cases are based on text not indexed by
the search engine. To exclude any such cases, we
query the search engine with each complete sen-
tence that a prediction task is based on and remove
any prediction task for which the search engine re-
turns hits for the complete sentence. The ﬁnal test
set consists of 8060 prediction tasks.4

2.2 Experimental Setup
Recall that the general issue we are interested in
is whether one can obtain sufﬁcient information
from the relatively narrow distributional window
of context provided by n-grams which are small
enough to occur frequently enough in the training
data but large enough to contain enough predic-

4For a copy of the test set, just send us an email.

tive information about preposition usage for the
instances to be classiﬁed. By using a web-as-
corpus approach we essentially try to maximize
the training data size. For the n-gram size, we ex-
plore the use of a maximum order of 7, containing
the preposition in the middle and three words of
context on either side.

For each prediction task, we successively insert
one of the nine most frequent prepositions into
the marked preposition slot of the 8060 n-grams
obtained from the test set. Thus, for each pre-
diction task, we get a cohort consisting of nine
different individual queries, one query for each
potential preposition. For example, the second
prediction task of Figure 2 yields the cohort of
nine queries in Figure 3 below, where the candi-
date prepositions replace the location marked by
-*-MASKED-*-of. The correct preposition of
is stripped off and kept for later use in the evalua-
tion step.

1. rather a question of the scales

falling

2. rather a question to the scales

falling

3. rather a question in the scales

falling

...

9. rather a question from the scales

falling

Figure 3: Cohort of nine queries resulting for the
second prediction task of Figure 2

In cases where a preposition is closer than four
words to the beginning or the end of the corre-
sponding sentence, a lower-order n-gram results.
For example, in the ﬁrst prediction task in Fig-
ure 2, the preposition occurs already as the sec-
ond word in the sentence, thus not leaving enough
context to the left of the preposition for a sym-
metric 7-gram. Here, the truncated asymmetric 5-
gram “But <prep> the young ,” includ-
ing only one word of context on the left would
get used.

We issue each query in a cohort to the Ya-
hoo search engine, and determine the number
of hits returned for that query. To that end,
we use Yahoo’s BOSS service, which offers a

271

JSON interface supporting straightforward auto-
mated queries. As part of its response to a query,
the BOSS service includes the deephits ﬁeld,
which gives an “approximate count that reﬂects
duplicate documents and all documents from a
host”.5 In other words, this number is an approx-
imate measure of how many web pages there are
that contain the search pattern.

With the counts for all nine queries in a cohort
retrieved from Yahoo, we select the preposition of
the query with the highest count. For the cases
in which none of the counts in a 7-gram cohort is
greater than zero, we use one of two strategies:

In the baseline condition, for all n-gram cohorts
with zero counts (5160 out of the 8060 cases) we
predict the most frequent preposition of, i.e., the
majority baseline. This results in an overall accu-
racy of 50%.

In the full back-off condition, we explore the
trade-off between the predictive power of the n-
gram as context and the likelihood of having seen
this n-gram in the training material, i.e., ﬁnding
it on the web. In this paper we never abstract or
generalize away from the surface string (e.g., by
mapping all proper names to an abstract name tag;
but see the outlook discussion at the end of the pa-
per), so the only option for increasing the number
of occurrences of an n-gram is to approximate it
with multiple shorter n-grams.

Concretely, if no hits could be found for any of
the queries in a cohort, we back off to the sum
of the hits for the two overlapping 6-grams con-
structed in the way illustrated in Figure 4.

[rather a question of the scales falling]

[rather a question of the scales]

[a question of the scales falling]

⇓

Figure 4: Two overlapping 6-grams approximate
a 7-gram for back-off.

If still no hits can be obtained after backing off
to 6-grams for any of the queries in a cohort, the
system backs off further to overlapping 5-grams,
and so on, down to trigrams.6

5Cited

from http://developer.yahoo.com/

search/boss/boss_guide/ch02s02.html

6When backing off, the left-most and the right-most tri-

3 Results
Figure 5 shows the results of the full back-off
approach. Compared to the baseline condition,
accuracy goes up signiﬁcantly to 76.5%. Thus,
the back-off strategy is effective in increasing the
amount of available data using lower-order n-
grams. This increase of data is also reﬂected in
the number of cases with zero counts for a cohort,
which goes down to none.

Correct
Incorrect
Total
Accuracy

Full back-off
6166
1894
8060
76.5%

Figure 5: Overall results of our experiments.

Figure 6 provides a detailed analysis of the
back-off experiment.
It lists back-off sequences
separately for each maximum n-gram order. The
prediction tasks for which a full 7-gram can be
extracted are displayed in the third column, with
back-off orders of 6 down to 3. Prediction tasks
for which only asymmetric 6-grams can be ex-
tracted follow in column 4, and so on until 4-
grams. There are no predictions tasks that are
shorter than four words. Therefore, n-grams with
a length of less than 4 do not occur.

The “sum” column shows the combined results
of the full 7-gram prediction tasks and the pre-
diction tasks involving truncated, asymmetric n-
grams of lower orders.

There are 6999 prediction tasks for which full
7-grams can be extracted. The remaining 1061
of the 8060 prediction tasks are the cases where
the system extracts only asymmetric lower-order
n-grams, for the reasons explained in section 2.2.
For 2195 of the 6999 7-gram prediction tasks,
we ﬁnd full 7-gram contexts on the web, of which
1931 lead to a correct prediction, and 264 to an
incorrect one, leaving 4804 prediction tasks still
to be solved through the back-off approach. Thus,
full 7-gram contexts lead to high-quality predic-
tions at 88% precision, but they are rare and with
a recall of 28,7% cover only a fraction of all cases.
gram do not include the target preposition of the original 7-
gram. However, this only affects 13 cases, cf. Figure 6.

272

Total
Predictions

correct
incorrect

Requiring back-off
Precision
Recall

Predictions

correct
incorrect

Still requiring back-off
Predict. orders 7+6

correct
incorrect

Precision
Recall

Predictions

correct
incorrect

Still requiring back-off
Predict. orders 7 – 5

correct
incorrect

Precision
Recall

Predictions

correct
incorrect

Still requiring back-off
Predict. orders 7 – 4

correct
incorrect

Precision
Recall

Predictions

correct
incorrect

Still requiring back-off
Predict. orders 7 – 3

correct
incorrect

Precision
Recall

sum

8060
2900
2495
405
5160
86%
32.6%

2028
1620
408
2776
4223
3551
672
84.1%
56.1%

2180
1542
638
873
6782
5419
1363
79.9%
86.1%

905
488
417
31
7806
5998
1808
76.8%
99.5%

47
21
26
0

8060
6166
1894
76.5%
100%

7-grams

(3 + prep + 3)

6-grams

(truncated 7-gram)

5-grams

(truncated 7-gram)

4-grams

(truncated 7-gram)

656
379
326
53
277
86%
79.6%

182
119
91
28
63

76.5%
59.1%

223
207
147
60
16
71%
90.2%

6999
2195
1931
264
4804
88%
28.7%

Back-off order 6

2028
1620
408
2776
4223
3551
672
84.1%
56.1%

Back-off order 5

2020
1411
609
756
6243
4962
1281
79.5%
86.8%

160
131
29
117
539
457
82
84.8%
79.6%

743
382
361
13
6986
5344
1642
76.5%
99.8%

13
5
8

0

6999
5349
1650
76.4%
100%

Back-off order 4

106
68
38
11
645
525
120
81.4%
97.9%

56
38
18
7
175
129
46
73.7%
94.9%

Back-off order 3

11
7
4

0
656
532
124
81.1%
100%

7

3
4

0
182
132
50
72.5%
100%

16
6
10
0
223
153
70
68.6%
100%

Figure 6: The results of our experiments

273

As discussed above, we use the same set of
prepositions and test corpus as De Felice (2008),
but only make use of 8060 test cases. Figure 8
shows that the accuracy stabilizes quickly after
about 1000 predictions, so that the difference in
the size of the test set should have no impact on
the reported results.

Figure 7: Development of precision and recall in
relation to back-off order

Approximating 7-grams with two overlapping
6-grams as the ﬁrst back-off step provides the
evidence needed to correctly predict 1620 addi-
tional prepositions, with 408 additional false pre-
dictions. The number of correctly solved predic-
tion tasks thus rises to 3551, and the number of
incorrect predictions rises to 672. This back-off
step almost doubles recall (56.1%). At the same
time, precision drops to 84.1%. For 2776 pre-
diction tasks, a further back-off step is necessary
since still no evidence can be found for them. This
pattern repeats with the back-off steps that fol-
low. To summarize, by adding more data using
less restricted contexts, more prediction tasks can
be solved. The better coverage however comes at
the price of reduced precision: Less speciﬁc con-
texts are worse predictors of the correct preposi-
tion than more speciﬁc contexts.

Figure 7 visualizes the development of preci-
sion and recall with full and truncated 7-grams
counted together as in the “sum” column in Fig-
ure 6. With each back-off step, more prediction
tasks can be solved (as shown by the rising recall
curve). At the same time, the overall quality of
the predictions drops due to the less speciﬁc con-
texts (as shown by the slightly dropping precision
curve). While the curve for recall rises steeply,
the curve for precision remains relatively ﬂat. The
back-off approach thus succeeds in adding data
while preserving prediction quality.

Figure 8: The accuracy of the n-gram prediction
stabilizes quickly.

4 Conclusions and Outlook
In this paper, we explored the potential and the
limits of a purely surface-based strategy of pre-
dicting prepositions in English.
The use of
surface-based n-grams ensures that fully speciﬁc
exemplars of a particular size are stored in train-
ing, but avoiding abstractions in this way leads to
the well-known data sparsity issues. We showed
that using a web-as-corpus approach maximizing
the size of the “training data”, one can work with
n-grams which are large enough to predict the oc-
currence of prepositions with signiﬁcant precision
while at the same time ensuring that these speciﬁc
n-grams have actually been encountered during
“training”, i.e., evidence for them can be found
on the web.

For the random sample of the BNC section J
we tested on, the surface-based approach results
in an accuracy of 77% for the 7-gram model with
back-off to overlapping shorter n-grams. It thus
outperforms De Felice’s (2008) machine learning

274

approach which uses the same set of prepositions
and the full BNC section J as test set. In broader
terms, the result of our surface-based approach
is competitive with the state-of-the art results for
preposition prediction in English using machine
learning to combine sophisticated sets of lexical
and linguistically motivated features.

In this paper, we focused exclusively on the
impact of n-gram size on preposition prediction.
Limiting ourselves to pure surface-based informa-
tion made it possible to maximize the “training
data” by using a web-as-corpus approach. Return-
ing from this very speciﬁc experiment to the gen-
eral issue, there are two well-known approaches
to remedy the data sparseness problem arising
from storing large, speciﬁc surface forms in train-
ing. On the one hand, one can use smaller ex-
emplars, which is the method we used as back-
off in our experiments in this paper. This only
works if the exemplars contain enough context for
the linguistic property or relation that we need to
capture the predictive power. On the other hand,
one can abstract parts of the surface-based train-
ing instances to more general classes. The cru-
cial question this raises is which generalizations
preserve the predictive power of the exemplars
and can reliably be identiﬁed. The linguistically-
informed features used in the previous approaches
in the literature naturally provide interesting in-
stances of answers to this question.
In the fu-
ture, we intend to compare the results we ob-
tained using the web-as-corpus approach with one
based on the Google-5-gram corpus to study us-
ing controlled, incremental shallow-to-deep fea-
ture development which abstractions or linguistic
generalizations best preserve the predictive con-
text while lowering the demands on the size of the
training data.

Turning to a linguistic issue, it could be use-
ful to distinguish between lexical and functional
prepositions when reporting test results. This is
an important distinction because the information
needed to predict functional prepositions typically
is in the local context, whereas the information
needed to predict lexical prepositions is not nec-
essarily present locally. To illustrate, a competent
human speaker presented with the sentence John
is dependent
his brother and asked to ﬁll in

the missing preposition, would correctly pick on.
This is a case of a functional preposition where
the relevant information is locally present: the ad-
jective dependent selects on. On the other hand,
the sentence John put his bag
the table is
more problematic, even for a human, since both
on and under are reasonable choices; the infor-
mation needed to predict the omitted preposition
in this case is not locally present.
In line with
the previous research, in the work in this paper
we made predictions for all prepositions alike. In
the future, it could be useful to annotate the test
set so that one can distinguish functional and lex-
ical uses and report separate ﬁgures for these two
classes in order to empirically conﬁrm their dif-
ferences with respect to locality.

References
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale n-gram models for lexical disam-
biguation. In IJCAI’09: Proceedings of the 21st in-
ternational jont conference on Artiﬁcal intelligence,
pages 1507–1512, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Boyd, Adriane, Markus Dickinson, and Detmar Meur-
ers. 2008. On detecting errors in dependency tree-
banks. Research on Language and Computation,
6(2):113–137.

Chodorow, Martin, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions.
In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions, pages 25–30,
Prague, Czech Republic, June.

De Felice, Rachele and Stephen Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions, pages 45–50, Prague, Czech Republic,
June. Association for Computational Linguistics.

De Felice, Rachele. 2008. Automatic Error Detection
in Non-native English. Ph.D. thesis, St Catherine’s
College, University of Oxford.

Dickinson, Markus and W. Detmar Meurers. 2003.
Detecting errors in part-of-speech annotation.
In
Proceedings of the 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-03), pages 107–114, Budapest,
Hungary.

Dickinson, Markus and W. Detmar Meurers. 2005.
Detecting errors in discontinuous structural anno-

275

In Proceedings of the 43rd Annual Meet-
tation.
ing of the Association for Computational Linguistics
(ACL’05), pages 322–329.

Gamon, Michael,

Jianfeng Gao, Chris Brockett,
Alexander Klementiev, William Dolan, Dmitriy Be-
lenko, and Lucy Vanderwende. 2008. Using con-
textual speller techniques and language modeling
for esl error correction. In Proceedings of IJCNLP,
Hyderabad, India.

Lapata, Mirella and Frank Keller.

2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1):1–30, February.

Lee, John and Ola Knutsson. 2008. The role of pp
attachment in preposition generation. In Gelbukh,
A., editor, Proceedings of CICLing 2008.

Tetreault, Joel and Martin Chodorow. 2008a. Na-
tive judgments of non-native usage: Experiments
in preposition error detection.
In Proceedings of
COLING-08, Manchester.

Tetreault, Joel and Martin Chodorow. 2008b. The ups
and downs of preposition error detection in esl writ-
ing. In Proceedings of COLING-08, Manchester.

