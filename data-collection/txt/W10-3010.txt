



















































A High-Precision Approach to Detecting Hedges and their Scopes


Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 70–77,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

A High-Precision Approach to Detecting Hedges and Their Scopes

Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering

Concordia University
1455 de Maisonneuve Blvd. West

Montréal, Canada
{h kilico,bergler}@cse.concordia.ca

Abstract

We extend our prior work on specula-
tive sentence recognition and speculation
scope detection in biomedical text to the
CoNLL-2010 Shared Task on Hedge De-
tection. In our participation, we sought
to assess the extensibility and portability
of our prior work, which relies on linguis-
tic categorization and weighting of hedg-
ing cues and on syntactic patterns in which
these cues play a role. For Task 1B,
we tuned our categorization and weight-
ing scheme to recognize hedging in bio-
logical text. By accommodating a small
number of vagueness quantifiers, we were
able to extend our methodology to de-
tecting vague sentences in Wikipedia arti-
cles. We exploited constituent parse trees
in addition to syntactic dependency rela-
tions in resolving hedging scope. Our re-
sults are competitive with those of closed-
domain trained systems and demonstrate
that our high-precision oriented methodol-
ogy is extensible and portable.

1 Introduction

Natural language is imbued with uncertainty,
vagueness, and subjectivity. However, informa-
tion extraction systems generally focus on ex-
tracting factual information, ignoring the wealth
of information expressed through such phenom-
ena. In recent years, the need for information ex-
traction and text mining systems to identify and
model such extra-factual information has increas-
ingly become clear. For example, online product
and movie reviews have provided a rich context
for analyzing sentiments and opinions in text (see
Pang and Lee (2008) for a recent survey), while
tentative, speculative nature of scientific writing,
particularly in biomedical literature, has provided

impetus for recent research in speculation detec-
tion (Light et al., 2004). The term hedging is often
used as an umbrella term to refer to an array of
extra-factual phenomena in natural language and
is the focus of the CoNLL-2010 Shared Task on
Hedge Detection.

The CoNLL-2010 Shared Task on Hedge De-
tection (Farkas et al., 2010) follows in the steps
of the recent BioNLP’09 Shared Task on Event
Extraction (Kim et al., 2009), in which one task
(speculation and negation detection) was con-
cerned with notions related to hedging in biomed-
ical abstracts. However, the CoNLL-2010 Shared
Task differs in several aspects. It sheds light on
the pervasiveness of hedging across genres and do-
mains: in addition to biomedical abstracts, it is
concerned with biomedical full text articles as well
as with Wikipedia articles. Both shared tasks have
been concerned with scope resolution; however,
their definitions of scope are fundamentally differ-
ent: the BioNLP’09 Shared Task takes the scope
of a speculation instance to be an abstract seman-
tic object (an event), thus a normalized logical
form. The CoNLL-2010 Shared Task, on the other
hand, defines it as a textual unit based on syntac-
tic considerations. It is also important to note that
hedging in scientific writing is a core aspect of the
genre (Hyland, 1998), while it is judged to be a
flaw which has to be eradicated in Wikipedia ar-
ticles. Therefore, hedge detection in these genres
serves different purposes: explicitly encoding the
factuality of a scientific claim (doubtful, probable,
etc.) versus flagging unreliable text.

We participated in both tasks of the CoNLL-
2010 Shared Task: namely, detection of sentences
with uncertainty (Task 1) and resolution of uncer-
tainty scope (Task 2). Since we pursued both of
these directions in prior work, one of our goals in
participating in the shared task was to assess how
our approach generalized to previously unseen
texts, even genres. Towards this goal, we adopted

70



an open-domain approach, where we aimed to use
previously developed techniques to the extent pos-
sible. Among all participating groups, we distin-
guished ourselves as the one that fully worked in
an open-domain setting. This approach worked
reasonably well for uncertainty detection (Task 1);
however, for the scope resolution task, we needed
to extend our work more substantially, since the
notion of scope was fundamentally different than
what we adopted previously. The performance
of our system was competitive; in terms of F-
measure, we were ranked near the middle in Task
1, while a more significant focus on scope reso-
lution resulted in fourth place ranking among fif-
teen systems. We obtained the highest precision
in tasks focusing on biological text. Considering
that we chose not to exploit the training data pro-
vided to the full extent, we believe that our system
is viable in terms of extensibility and portability.

2 Related Work

Several notions related to hedging have been pre-
viously explored in natural language processing.
In the news article genre, these have included
certainty, modality, and subjectivity. For ex-
ample, Rubin et al. (2005) proposed a four di-
mensional model to categorize certainty in news
text: certainty level, focus, perspective and time.
In the context of TimeML (Pustejovsky et al.,
2005), which focuses on temporal expressions
in news articles, event modality is encoded us-
ing subordination links (SLINKs), some of which
(MODAL,EVIDENTIAL) indicate hedging (Saurı́ et
al., 2006). Saurı́ (2008) exploits modality and
polarity to assess the factuality degree of events
(whether they correspond to facts, counter-facts or
possibilities), and reports on FactBank, a corpus
annotated for event factuality (Saurı́ and Puste-
jovsky, 2009). Wiebe et al. (2005) consider
subjectivity in news articles, and focus on the
notion of private states, encompassing specula-
tions, opinions, and evaluations in their subjectiv-
ity frames.

The importance of speculative language in
biomedical articles was first acknowledged by
Light et al. (2004). Following work in this area
focused on detecting speculative sentences (Med-
lock and Briscoe, 2007; Szarvas, 2008; Kilicoglu
and Bergler, 2008). Similar to Rubin et al.’s
(2005) work, Thompson et al. (2008) proposed
a categorization scheme for epistemic modality in

biomedical text according to the type of infor-
mation expressed (e.g., certainty level, point of
view, knowledge type). With the availability of the
BioScope corpus (Vincze et al., 2008), in which
negation, hedging and their scopes are annotated,
studies in detecting speculation scope have also
been reported (Morante and Daelemans, 2009;
Özgür and Radev, 2009). Negation and uncer-
tainty of bio-events are also annotated to some ex-
tent in the GENIA event corpus (Kim et al., 2008).
The BioNLP’09 Shared Task on Event Extraction
(Kim et al., 2009) dedicated a task to detecting
negation and speculation in biomedical abstracts,
based on the GENIA event corpus annotations.

Ganter and Strube (2009) elaborated on the link
between vagueness in Wikipedia articles indicated
by weasel words and hedging. They exploited
word frequency measures and shallow syntactic
patterns to detect weasel words in Wikipedia ar-
ticles.

3 Methods

Our methodology for hedge detection is essen-
tially rule-based and relies on a combination of
lexical and syntactic information. Lexical infor-
mation is encoded in a simple dictionary, and rel-
evant syntactic information is identified using the
Stanford Lexicalized Parser (Klein and Manning,
2003). We exploit constituent parse trees as well
as corresponding collapsed dependency represen-
tations (deMarneffe et al., 2006), provided by the
parser.

3.1 Detecting Uncertainty in Biological Text

For detecting uncertain sentences in biological text
(Task 1B), we built on the linguistically-inspired
system previously described in detail in Kilicoglu
and Bergler (2008). In summary, this system relies
on a dictionary of lexical speculation cues, derived
from a set of core surface realizations of hedging
identified by Hyland (1998) and expanded through
WordNet (Fellbaum, 1998) synsets and UMLS
SPECIALIST Lexicon (McCray et al., 1994) nom-
inalizations. A set of lexical certainty markers (un-
hedgers) are also included, as they indicate hedg-
ing when they are negated (e.g., know). These
hedging cues are categorized by their type (modal
auxiliaries, epistemic verbs, approximative adjec-
tives, etc.) and are weighted to reflect their cen-
tral/peripheral contribution to hedging, inspired by
the fuzzy model of Hyland (1998). We use a scale

71



of 1-5, where 5 is assigned to cues most central
to hedging and 1 to those that are most periph-
eral. For example, the modal auxiliary may has
a weight of 5, while a relatively weak hedging
cue, the epistemic adverb apparently, has a weight
of 2. The weight sum of cues in a sentence in
combination with a predetermined threshold de-
termines whether the sentence in question is un-
certain. Syntax, generally ignored in other stud-
ies on hedging, plays a prominent role in our ap-
proach. Certain syntactic constructions act as cues
(e.g., whether- and if -complements), while others
strengthen or weaken the effect of the cue associ-
ated with them. For example, a that-complement
taken by an epistemic verb increases the hedging
score contributed by the verb by 2, while lack of
any complement decreases the score by 1.

For the shared task, we tuned this categoriza-
tion and weighting scheme, based on an analy-
sis of the biomedical full text articles in training
data. We also adjusted the threshold. We elim-
inated some hedging cue categories completely
and adjusted the weights of a small number of
the remaining cues. The eliminated cue categories
included approximative adverbs (e.g., generally,
largely, partially) and approximative adjectives
(e.g., partial), often used to “manipulate preci-
sion in quantification” (Hyland, 1998). The other
eliminated category included verbs of effort (e.g.,
try, attempt, seek), also referred to as rationalising
narrators (Hyland, 1998). The motivation behind
eliminating these categories was that cues belong-
ing to these categories were never annotated as
hedging cues in the training data. The elimination
process resulted in a total of 147 remaining hedg-
ing cues. Additionally, we adjusted the weights of
several other cues that were not consistently anno-
tated as cues in the training data, despite our view
that they were strong hedging cues. One example
is the epistemic verb predict, previously assigned a
weight of 4 based on Hyland’s analysis. We found
its annotation in the training data somewhat incon-
sistent, and lowered its weight to 3, thus requiring
a syntactic strengthening effect (an infinitival com-
plement, for example) for it to qualify as a hedging
cue in the current setting (threshold of 4).

3.2 Detecting Uncertainty in Wikipedia
Articles

Task 1W was concerned with detecting uncer-
tainty in Wikipedia articles. Uncertainty in this

context refers more or less to vagueness indicated
by weasel words, an undesirable feature accord-
ing to Wikipedia policy. Analysis of Wikipedia
training data provided by the organizers revealed
that there is overlap between weasel words and
hedging cues described in previous section. We,
therefore, sought to adapt our dictionary of hedg-
ing cues to the task of detecting vagueness in
Wikipedia articles. Similar to Task 1B, changes
involved eliminating cue categories and adjusting
cue weights. In addition, however, we also added
a previously unconsidered category of cues, due
to their prominence in Wikipedia data as weasel
words. This category (vagueness quantifiers (Lap-
pin, 2000)) includes words, such as some, several,
many and various, which introduce imprecision
when in modifier position. For instance, in the ex-
ample below, both some and certain contribute to
vagueness of the sentence.

(1) Even today, some cultures have certain in-
stances of their music intending to imitate
natural sounds.

For Wikipedia uncertainty detection, eliminated
categories included verbs and nouns concerning
tendencies (e.g., tend, inclination) in addition to
verbs of effort. The only modal auxiliary consis-
tently considered a weasel word was might; there-
fore, we only kept might in this category and elim-
inated the rest (e.g., may, would). Approxima-
tive adverbs, eliminated in detecting uncertainty
in biological text, not only were revived for this
task, but also their weights were increased as they
were more central to vagueness expressions. Be-
sides these changes in weighting and categoriza-
tion, the methodology for uncertainty detection in
Wikipedia articles was essentially the same as that
for biological text. The threshold we used in our
submission was, similarly, 4.

3.3 Scope Resolution for Uncertainty in
Biological Text

Task 2 of the shared task involved hedging scope
resolution in biological text. We previously tack-
led this problem within the context of biological
text in the BioNLP’09 Shared Task (Kilicoglu and
Bergler, 2009). That task defined the scope of
speculation instances as abstract, previously ex-
tracted bio-events. Our approach relied on find-
ing an appropriate syntactic dependency relation
between the bio-event trigger word identified in

72



earlier steps and the speculation cue. The cate-
gory of the hedging cue constrained the depen-
dency relations that are deemed appropriate. For
example, consider the sentence in (2a), where in-
volves is a bio-event trigger for a Regulation
event and suggest is a speculation cue of epis-
temic verb type. The first dependency relation
in (2b) indicates that the epistemic verb takes a
clausal complement headed by the bio-event trig-
ger. The second indicates that that is the comple-
mentizer. This cue category/dependency combi-
nation licenses the generation of a speculation in-
stance where the event indicated by the event trig-
ger represents the scope.

(2) (a) The results suggest that M-CSF induc-
tion of M-CSF involves G proteins, PKC
and NF kappa B.

(b) ccomp(suggest,involves)
complm(involves,that)

Several other cue category/dependency combi-
nations sought for speculation scope resolution are
given in Table 1. X represents a token that is nei-
ther a cue nor a trigger (aux: auxiliary, dobj: direct
object, neg: negation modifier).

Cue Category Dependency
Modal auxiliary (may) aux(Trigger,Cue)
Conditional (if ) complm(Trigger,Cue)
Unhedging noun dobj(X,Cue)
(evidence) ccomp(X,Trigger)

neg(Cue,no)

Table 1: Cue categories with examples and the de-
pendency relations to search

In contrast to this notion of scope being an ab-
stract semantic object, Task 2 (BioScope corpus,
in general) conceptualizes hedge scope as a con-
tinuous textual unit, including the hedging cue it-
self and the biggest syntactic unit the cue is in-
volved in (Vincze et al., 2008). This fundamen-
tal difference in conceptualization limits the di-
rect applicability of our prior approach to this
task. Nevertheless, we were able to use our work
as a building block in extending scope resolution
heuristics. We further augmented it by exploiting
constituent parse trees provided by Stanford Lex-
icalized Parser. These extensions are summarized
below.

3.3.1 Exploiting parse trees
The constituent parse trees contribute to scope
resolution uniformly across all hedging cue cate-
gories. We simply determine the phrasal node that
dominates the hedging cue and consider the tokens
within that phrase as being in the scope of the cue,
unless they meet one of the following exclusion
criteria:

1. Exclude tokens within post-cue sentential
complements (indicated by S and SBAR
nodes) introduced by a small number of
discourse markers (thus, whereas, because,
since, if, and despite).

2. Exclude punctuation marks at the right
boundary of the phrase

3. Exclude pre-cue determiners and adverbs at
the left boundary of the phrase

For example, in the sentence below, the verb
phrase that included the modal auxiliary may also
included the complement introduced by thereby.
Using the exclusion criteria 1 and 2, we excluded
the tokens following SPACER from the scope:

(3) (a) . . . motifs may be easily compared with
the results from BEAM, PRISM and
SPACER, thereby extending the SCOPE
ensemble to include a fourth class of
motifs.

(b) CUE: may
SCOPE: motifs may be easily compared
with the results from BEAM, PRISM
and SPACER

3.3.2 Extending dependency-based heuristics
The new scope definition was also accommodated
by extending the basic dependency-based heuris-
tics summarized earlier in this section. In addition
to finding the trigger word that satisfies the ap-
propriate dependency constraint with the hedging
cue (we refer to this trigger word as scope head,
henceforth), we also considered the other depen-
dency relations that the scope head was involved
in. These relations, then, were used in right ex-
pansion and left expansion of the scope. Right ex-
pansion involves finding the rightmost token that
is in a dependency relation with the scope head.
Consider the sentence below:

(4) The surprisingly low correlations between
Sig and accuracy may indicate that the ob-
jective functions employed by motif finding

73



programs are only a first approximation to bi-
ological significance.

The epistemic verb indicate has as its scope
head the token approximation, due to the existence
of a clausal complement dependency (ccomp) be-
tween them. On the other hand, the rightmost to-
ken of the sentence, significance, has a preposi-
tional modifier dependency (prep to) with approx-
imation. It is, therefore, included in the scope of
indicate. Two dependency types, adverbial clause
modifier (advcl) and conjunct (conj), were ex-
cluded from consideration when the rightmost to-
ken is sought, since they are likely to signal new
discourse units outside the scope.

In contrast to right expansion, which applies
to all hedging cue categories, left expansion ap-
plies only to a subset. Left expansion involves
searching for a subject dependency governed by
the scope head. The dependency types descend-
ing from the subject (subj) type in the Stanford de-
pendency hierarchy are considered: nsubj (nom-
inal subject), nsubjpass (passive nominal sub-
ject), csubj (clausal subject) and csubjpass (pas-
sive clausal subject). In the following example,
the first token, This, is added to the scope of likely
through left expansion (cop: copula).

(5) (a) This is most likely a conservative esti-
mate since a certain proportion of inter-
actions remain unknown . . .

(b) nsubj(likely,This)
cop(likely,is)

Left expansion was limited to the following cue
categories, with the additional constraints given:

1. Modal auxiliaries, only when their scope
head takes a passive subject (e.g., they is
added to the scope of may in they may be an-
notated as pseudogenes).

2. Cues in adjectival categories, when they are
in copular constructions (e.g., Example (5)).

3. Cues in several adjectival ad verbal cate-
gories, when they take infinitival comple-
ments (e.g., this is added to the scope of ap-
pears in However, this appears to add more
noise to the prediction without increasing the
accuracy).

After scope tokens are identified using the parse
tree as well as via left and right expansion, the al-
gorithm simply sets as scope the continuous tex-
tual unit that includes all the scope tokens and the

hedging cue. Since, likely is the hedging cue and
This and estimate are identified as scope tokens in
Example (5), the scope associated with likely be-
comes This is most likely a conservative estimate.

We found that citations, numbers and punc-
tuation marks occurring at the end of sentences
caused problems in scope resolution, specifically
in biomedical full text articles. Since they are
rarely within any scope, we implemented a simple
stripping algorithm to eliminate them from scopes
in such documents.

4 Results and Discussion

The official evaluation results regarding our sub-
mission are given in Table 2. These results were
achieved with the threshold 4, which was the opti-
mal threshold on the training data.

Prec. Recall F-score Rank
Task 1B 92.07 74.94 82.62 12/24
Task 1W 67.90 46.02 54.86 10/17
Task 2 62.47 49.47 55.21 4/15

Table 2: Evaluation results

In Task 1B, we achieved the highest precision.
However, our relatively low recall led to the place-
ment of our system in the middle. Our system al-
lows adjusting precision versus recall by setting
the threshold. In fact, setting the threshold to 3 af-
ter the shared task, we were able to obtain overall
better results (Precision=83.43, Recall=84.81, F-
score=84.12, Rank=8/24). However, we explicitly
targeted precision, and in that respect, our submis-
sion results were not surprising. In fact, we iden-
tified a new type of hedging signalled by coordi-
nation (either . . . or . . . as well as just or) in the
training data. An example is given below:

(6) (a) It will be either a sequencing error or a
pseudogene.

(b) CUE: either-or
SCOPE: either a seqeuncing error or a
pseudogene

By handling this class to some extent, we could
have increased our recall, and therefore, F-score
(65 out of 1,044 cues in the evaluation data for
biological text involved this class). However, we
decided against treating this class, as we believe
it requires a slightly different treatment due to its
special semantics.

74



In participating in Task 1W, our goal was to
test the ease of extensibility of our system. In
that regard, our results show that we were able
to exploit the overlap between our hedging cues
and the weasel words. The major difference we
noted between hedging in two genres was the
class of vagueness quantifiers, and, with little ef-
fort, we extended our system to consider them.
We also note that setting the threshold to 3 after
the shared task, our recall and F-score improved
significantly (Precision=63.21, Recall=53.67, F-
score=58.05, Rank=3/17).

Our more substantial effort for Task 2 resulted
in a better overall ranking, as well as the highest
precision in this task. In contrast to Task 1, chang-
ing the threshold in this task did not have a pos-
itive effect on the outcome. We also measured
the relative contribution of the enhancements to
scope resolution. The results are presented in Ta-
ble 3. Baseline is taken as the scope resolution al-
gorithm we developed in prior work. These results
show that: a) scope definition we adopted earlier
is essentially incompatible with the BioScope def-
inition b) simply taking the phrase that the hedg-
ing cue belongs to as the scope provides relatively
good results c) left and right expansion heuristics
are needed for increased precision and recall.

Prec. Recall F-score
Baseline 3.29 2.61 2.91
Baseline+ Left/
right expansion

25.18 20.03 22.31

Parse tree 49.20 39.10 43.58
Baseline+
Parse tree

50.66 40.27 44.87

All 62.47 49.47 55.21

Table 3: Effect of scope resolution enhancements

4.1 Error Analysis

In this section, we provide a short analysis of the
errors our system generated, focusing on biologi-
cal text.

Since our dictionary of hedging cues is incom-
plete and we did not attempt to expand it for Task
1B, we had a fair number of recall errors. As
we mentioned above, either-or constructions oc-
cur frequently in the training and evaluation data,
and we did not attempt to handle them. Addition-
ally, some lexical cues, such as feasible and im-
plicate, do not appear in our dictionary, causing

further recall errors. The weighting scheme also
affects recall. For example, the adjective appar-
ent has a weight of 2, which is not itself sufficient
to qualify a sentence as uncertain (with a thresh-
old of 4) (7a). On the other hand, when it takes
a clausal complement, the sentence is considered
uncertain (7b). The first sentence (7a) causes a re-
call error.

(7) (a) An apparent contradiction between the
previously reported number of cycling
genes . . .

(b) . . . it is apparent that the axonal termini
contain a significantly reduced number
of varicosities . . .

In some cases, syntactic constructions that play
a role in determining the certainty status of a sen-
tence cannot be correctly identified by the parser,
often leading to recall errors. For example, in the
sentence below, the clausal complement construc-
tion is missed by the parser. Since the verb indi-
cate has weight 3, this leads to a recall error in the
current setting.

(8) . . . indicating that dMyc overexpression can
substitute for PI3K activation . . .

Adjusting the weights of cues worked well gen-
erally, but also caused unexpected problems, due
to what seem like inconsistencies in annotation.
The examples below highlight the effect of low-
ering the weight of predict from 4 to 3. Exam-
ples (9a) and (9b) are almost identical on surface
and our system predicted both to be uncertain, due
to the fact that predicted took infinitival comple-
ments in both cases. However, only (9a) was an-
notated as uncertain, leading to a precision error in
(9b).

(9) (a) . . . include all protein pairs predicted to
have posterior odds ratio . . .

(b) Protein pairs predicted to have a poste-
rior odds ratio . . .

The error cases in scope resolution are more
varied. Syntax has a larger role in this task, and
therefore, parsing errors tend to affect the results
more directly. In the following example, dur-
ing left-expanding the scope of the modal auxil-
iary could, RNAi screens, rather than the full noun
phrase fruit fly RNAi screens, is identified as the
passive subject of the scope head (associated), be-
cause an appropriate modifier dependency cannot

75



be found between the noun phrase head screens
and either of the modifiers, fruit and fly.

(10) . . . was to investigate whether fruit fly RNAi
screens of conserved genes could be asso-
ciated with similar tick phenotypes and tick
gene function.

In general, the simple mechanism to exploit
constituent parse trees was useful in resolving
scope. However, it appears that a nuanced ap-
proach based on cue categories could enhance the
results further. In particular, the current mecha-
nism does not contribute much to resolving scopes
of adverbial cues. In the following example, parse
tree mechanism does not have any effect, leading
to both a precision and a recall error in scope res-
olution.

(11) (a) . . . we will consider tightening the defi-
nitions and possibly splitting them into
different roles.

(b) FP: possibly
FN: possibly splitting them into differ-
ent roles

Left/right expansion strategies were based on
the analysis of training data. However, we en-
countered errors caused by these strategies where
we found the annotations contradictory. In Exam-
ple (12a), the entire fragment is in the scope of
thought, while in (12b), the scope of suggested
does not include it was, even though on surface
both fragments are very similar.

(12) (a) . . . the kinesin-5 motor is thought to play
a key role.

(b) . . . it was suggested to enhance the nu-
clear translocation of NF-κB.

Post-processing in the form of citation stripping
was simplistic, and, therefore, was unable to han-
dle complex cases, as the one shown in the exam-
ple below. The algorithm is only able to remove
one reference at the end.

(13) (a) . . . it is possible that some other sig-
nalling system may operate with Semas
to confine dorsally projecting neurons to
dorsal neuropile [3],[40],[41].

(b) FP: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile [3],[40],
FN: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile

5 Conclusions

Rather than developing a dedicated methodology
that exclusively relies on the data provided by or-
ganizers, we chose to extend and refine our prior
work in hedge detection and used the training
data only in a limited manner: to tune our sys-
tem in a principled way. With little tuning, we
achieved the highest precision in Task 1B. We
were able to capitalize on the overlap between
hedging cues and weasel words for Task 1W and
achieved competitive results. Adapting our pre-
vious work in scope resolution to Task 2, how-
ever, was less straightforward, due to the incom-
patible definitions of scope. Nevertheless, by re-
fining the prior dependency-based heuristics with
left and right expansion strategies and utilizing a
simple mechanism for parse tree information, we
were able to accommodate the new definition of
scope to a large extent. With these results, we con-
clude that our methodology is portable and easily
extensible.

While the results show that using the parse tree
information for scope resolution benefited our per-
formance greatly, error analysis presented in the
previous sections also suggests that a finer-grained
approach based on cue categories could further
improve results, and we aim to explore this exten-
sion further.

References

Marie-Catherine deMarneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449–
454.

Richárd Farkas, Veronika Vincze, György Móra, János
Csirik, and György Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1–12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.

Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.

Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ing Wikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173–176.

76



Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V., Amsterdam, Nether-
lands.

Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11:s10.

Halil Kilicoglu and Sabine Bergler. 2009. Syntac-
tic dependency based heuristics for biological event
extraction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 119–127.

Jin-Dong Kim, Tomoko Ohta, and Jun’ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.

Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1–9.

Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Meeting of the Association for Computational
Linguistics, pages 423–430.

Shalom Lappin. 2000. An intensional parametric se-
mantics for vague quantifiers. Linguistics and Phi-
losophy, 23(6):599–620.

Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In BioLINK 2004: Linking
Biological Literature, Ontologies and Databases,
pages 17–24.

Alexa T. McCray, Suresh Srinivasan, and Allen C.
Browne. 1994. Lexical methods for managing vari-
ation in biomedical terminologies. In Proceedings
of the 18th Annual Symposium on Computer Appli-
cations in Medical Care, pages 235–239.

Ben Medlock and Ted Briscoe. 2007. Weakly su-
pervised learning for hedge classification in scien-
tific literature. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics, pages 992–999.

Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28–36.

Arzucan Özgür and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398–1407.

Bo Pang and Lillian Lee. 2008. Sentiment Analysis
and Opinion Mining. Now Publishers Inc, Boston,
MA.

James Pustejovsky, Robert Knippen, Jessica Littman,
and Roser Saurı́. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39(2):123–164.

Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2005. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce Wiebe, ed-
itors, Computing Attitude and Affect in Text: The-
ories and Applications, volume 20, pages 61–76.
Springer Netherlands, Dordrecht.

Roser Saurı́ and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.

Roser Saurı́, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In Proceedings of 19th International FLAIRS
Conference.

Roser Saurı́. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.

György Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of the 46th Meeting
of the Association for Computational Linguistics,
pages 281–289.

Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of LREC 2008 Workshop on Building
and Evaluating Resources for Biomedical Text Min-
ing.

Veronika Vincze, György Szarvas, Richárd Farkas,
György Móra, and János Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11:S9.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165–210.

77


