



















































A Sentiment-aligned Topic Model for Product Aspect Rating Prediction


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192–1202,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

A Sentiment-aligned Topic Model for Product Aspect Rating Prediction

Hao Wang
School of Computing Science

Simon Fraser University
Burnaby, BC, Canada
hwa63@sfu.ca

Martin Ester
School of Computing Science

Simon Fraser University
Burnaby, BC, Canada
ester@sfu.ca

Abstract

Aspect-based opinion mining has attracted
lots of attention today. In this paper, we
address the problem of product aspect rat-
ing prediction, where we would like to ex-
tract the product aspects, and predict as-
pect ratings simultaneously. Topic mod-
els have been widely adapted to jointly
model aspects and sentiments, but exist-
ing models may not do the prediction task
well due to their weakness in sentiment
extraction. The sentiment topics usually
do not have clear correspondence to com-
monly used ratings, and the model may
fail to extract certain kinds of sentiments
due to skewed data. To tackle this prob-
lem, we propose a sentiment-aligned topic
model(SATM), where we incorporate two
types of external knowledge: product-
level overall rating distribution and word-
level sentiment lexicon. Experiments on
real dataset demonstrate that SATM is ef-
fective on product aspect rating prediction,
and it achieves better performance com-
pared to the existing approaches.

1 Introduction

Online reviews have become an important source
of information for consumers. People tend to read
reviews to help them compare products, and make
informed decisions. As the volume of product re-
views continues to grow, it is often impossible to
read all of them, which calls for efficient methods
for opinion mining. Nowadays, for each product,
many websites aggregate the overall rating of re-
views, and display its distribution. However, this
cannot provide detailed information. For exam-
ple, two products may have similar overall rating
distributions, while people talk about different un-
satisfactory aspects. This problem has inspired a

new line of research on aspect-level opinion min-
ing(Hu and Liu, 2004).

An aspect refers to a rateable feature, such as
staff and location in hotel reviews, or size and
battery for digital camera reviews. In this paper,
we deal with the problem of product aspect rat-
ing prediction. The input is a collection of prod-
ucts, and each product is associated with a set of
reviews. The goal is to extract the corpus-level as-
pects, and predict the aspect ratings for each prod-
uct. This kind of fine-grained sentiment analysis
will help users efficiently digest the reviews, and
gain more insight into the product quality.

The product aspect rating prediction problem
usually involves two subtasks: aspect extraction
and sentiment identification(Titov and McDonald,
2008b). Given some text, we would like to know
what aspects it talks about, and what kind of sen-
timents are expressed. For example, given a sen-
tence “the room is filthy”, we would like to know
that it talks about the aspect “room”. Also, “filthy”
is a sentiment word, and it expresses strongly neg-
ative sentiment towards the aspect “room”.

Topic models(Blei et al., 2003; Hofmann, 1999)
have been popular in aspect-based opinion min-
ing(Liu, 2012). Existing works have used topic
models to extract only aspects(Titov and McDon-
ald, 2008a; Brody and Elhadad, 2010; Chen et
al., 2013), or jointly model aspects and sentiments
(Mei et al., 2007; Lin and He, 2009; Li et al.,
2010; Jo and Oh, 2011; Moghaddam and Ester,
2011; Lakkaraju et al., 2011; Sauper et al., 2011;
Mukherjee and Liu, 2012; Lazaridou et al., 2013;
Moghaddam and Ester, 2013; Kim et al., 2013). In
the joint modelling approaches, a sentiment topic
is usually modelled as a sentiment label-word dis-
tribution, analogous to the topic-word distribution
in standard topic models. However, the difference
is that the sentiment topics need to be ordered. If
the model is to be applied for aspect rating predic-
tion, the sentiment topics should have clear cor-

1192



respondence to the ratings. Suppose there are 5
sentiment topics with sentiment labels from 1 to
5. The sentiment topic with label i is expected
to correspond to the rating i on the 1-5 rating
scale. For example, the sentiment topic with label
5 should have high probability over positive sen-
timent words, so it expresses highly positive sen-
timent, which matches our natural interpretation
for the rating 5. In this case, sentiment labels and
ratings are aligned. However, in a standard topic
model, the learned sentiment topics may not have
clear correspondence with different ratings. Also,
if the positive reviews are dominant in the data, the
topic model may fail to capture the negative sen-
timents with any sentiment topic, so no sentiment
labels are matched with low ratings. If the senti-
ment labels are not correctly aligned to the ratings,
we cannot use these sentiment labels to predict as-
pect ratings. Consequently, the aspect rating pre-
diction accuracy is compromised, and the method
is less practical. We call this the sentiment label
alignment problem. To tackle this problem, mod-
els in the literature usually use some seed words
for each sentiment topic to define Dirichlet priors
with asymmetric concentration parameter vectors
(Sauper et al., 2011; Kim et al., 2013), or use seed
words to initialize word assignment to sentiment
topic(Lin and He, 2009), or both(Li et al., 2010;
Jo and Oh, 2011). However, these seed words
are usually arbitrarily selected, and how to define
asymmetric priors is not clear, especially when we
would like to capture more than two (positive and
negative) kinds of sentiments.

In this paper, we propose a sentiment-aligned
topic model(SATM) for product aspect rating pre-
diction, which focuses the sentiment label align-
ment problem. We use two kinds of external
knowledge: the product overall rating distribution,
and a sentiment lexicon. For each product, the
overall rating distribution is available on most on-
line review websites. It provides the big picture of
the product-level sentiments. In SATM, for each
product and each aspect, we define a multinomial
distribution over sentiment labels, with prior pa-
rameterized by the overall rating distribution. Sen-
timent lexicon is constructed by linguistic experts,
and every word in the lexicon is associated with a
sentiment polarity score(Taboada et al., 2011). We
treat the polarity score as an extra word feature
in a semi-supervised framework. By incorporat-
ing both product-level and word-level knowledge

into the model, the sentiment labels can be aligned
with ratings, and the extracted sentiment topics
can capture different kinds of sentiments, ranging
from highly positive to highly negative. Experi-
ments on a TripAdvisor dataset demonstrate that
our method can effectively deal with the sentiment
label alignment problem, and outperforms state-
of-the-art methods in terms of product aspect rat-
ing prediction accuracy.

2 Related work

Several methods have been proposed for product
aspect rating prediction, and many of them are
based on topic models.

In (Lu et al., 2009), the authors studied the prob-
lem of generating an aspect rating summary for
short comments. The text was first preprocessed
into phrases of the format <headterm, sentiment
word>, and the headterms are clustered by Struc-
tured PLSA to find K major aspects. Then, phrase
ratings are predicted by either Local Prediction
or Global Prediction, and they are aggregated to
get aspect ratings. The method in (Brody and El-
hadad, 2010) also first uses topic models to find as-
pects. Then, for each aspect, it extracts all the rel-
evant adjectives, and builds a conjunction graph.
A label propagation algorithm(Zhu and Ghahra-
mani, 2002) is used on the graph to learn the senti-
ment polarity score of adjective words. Although
this approach is not proposed for aspect rating pre-
diction, it can be used for this task if the polar-
ity scores of adjective words are aggregated for
each aspect. All the methods above perform as-
pect extraction and sentiment identification sepa-
rately, while our approach takes a joint modelling
approach so that different subtasks can potentially
reinforce with each other. To demonstrate this, we
use these methods as baselines in our experiments.

Wang et al. worked on the Latent Aspect Rat-
ing Analysis problem(Wang et al., 2010; Wang
et al., 2011), the task of inferring aspect ratings
for each review and the relative weights review-
ers have placed on each aspect. In (Wang et al.,
2010), aspect keywords are provided as user input,
and a two-stage method, called Latent Rating Re-
gression(LRR), is proposed. The first stage uses
a bootstrapping algorithm to obtain more related
words for each aspect, and segments the document
content. In the second stage, the overall rating is
“generated” as weighted combination of the latent
aspect ratings, and LRR is used to infer both the

1193



weights and aspect ratings. Their follow-up work
(Wang et al., 2011) does not need keyword speci-
fication from users, and replaces the bootstrapping
method with a topic model. However, both meth-
ods implicitly require that each review talks about
all aspects, which is not always true due to the data
sparsity in online reviews.

In (Moghaddam and Ester, 2011), ILDA was
proposed for product aspect rating prediction.
Later, it was extended to FLDA (Moghaddam
and Ester, 2013) to address the cold start prob-
lem, when there are few reviews associated with
a product. Similar to (Lu et al., 2009), in ILDA
and FLDA, a preprocessing step parses the text
into phrases of the format <headterm, sentiment
word>, and a review is modelled as a bag of
phrases. We also adopt this assumption in our
model. The method in (Sauper et al., 2011; Sauper
and Barzilay, 2013) does not use phrases, but in-
stead uses “snippets”, and an snippet is a short
sentence or phrase. However, the sentiment label
alignment problem is not well addressed in these
models, which limits their practicality. ILDA
and FLDA did not deal with this problem. The
model in (Sauper et al., 2011; Sauper and Barzi-
lay, 2013) follows the most common approach of
using seed words to define asymmetric priors. It
supports only two kinds of sentiment topics: pos-
itive and negative, while how to define asymmet-
ric priors for more sentiment topics becomes un-
clear. More importantly, the prior approach may
not work well in practice(see Experiment Section).
Lakkaraju et al. try to tackle the sentiment label
alignment problem by assuming that the overall
rating is generated as response variable(Lakkaraju
et al., 2011), with the sentiment topic propor-
tions as features. However, how the sentiment la-
bels are related to ratings is still unknown until
learned, and we may not get the desired alignment.
Lazaridou et al. attempt to connect sentiment la-
bels with ratings by Kronecker symbol, but this
method only applies to three sentiment polarities:
−1(negative), 0(neutral), +1(positive), and it does
not explore the word-level lexicon, which is also
an important source of knowledge.

Another line of research on product aspect rat-
ing prediction or summarization does not use topic
models, but relies mainly on word frequency and
grammatical relations(Hu and Liu, 2004; Popescu
and Etzioni, 2005; Blair-goldensohn et al., 2008),
or specialized review selection(Long et al., 2014).

In this case, the extracted aspect words need to
be clustered manually. For example, picture and
photo may refer to the same aspect in digital cam-
era reviews. By comparison, topic modelling ap-
proaches extract aspect words and cluster them si-
multaneously.

Our method incorporates the product overall
rating distributions and sentiment lexicons into
the model, so it is also related to topic models
which use observed features or domain knowl-
edge(Mimno and McCallum, 2008; Andrzejewski
et al., 2009; Andrzejewski et al., 2011). Mimno et
al. introduces two general frameworks to integrate
observed features into the generative process:
downstream and upstream topic models(Mimno
and McCallum, 2008). In the context of aspect-
based opinion mining, MaxEnt-LDA(Zhao et al.,
2010) integrates a discriminative maximum en-
tropy component to help separate aspect words
and sentiment words. The SAS model (Mukherjee
and Liu, 2012) uses seed words to provide guid-
ance for aspect discovery, and MC-LDA (Chen et
al., 2013) uses must-links and cannot-links to ex-
tract coherent aspects. However, MaxEnt-LDA,
SAS and MC-LDA cannot be used for aspect rat-
ing prediction, since they fail to identify the senti-
ment polarity of sentiment words.

3 Method

3.1 Preliminaries

We first introduce several key concepts used in our
model.

Products: Let P = {P1, P2, . . . } be a set of
products. Each product Pi is associated with a
set of reviews Di = {d1, d2, . . . dNi}, and also an
overall rating distribution Yi. Yi is a multinomial
distribution on R ratings. It is available on most
online review websites, and usually R = 5.

Aspects: An aspect is a rateable feature of a
product, and each aspect is modelled as a distribu-
tion over aspect words. The number of aspects is
predefined as K.

Sentiment topics: A sentiment topic is mod-
elled as a distribution over sentiment words, and
each sentiment topic is associated with a sentiment
label. To make it consistent with commonly used
rating scale, we assume there are R sentiment la-
bels, corresponding to the R ratings. The chal-
lenge is that sentiment labels with higher values
are expected to be associated with sentiment top-
ics which express more positive sentiments, so that

1194



we can match sentiment labels with ratings.
Phrases: An opinion phrase f =< h, m > is a

pair of aspect word h and sentiment word m, such
as < room, filthy >(Lu et al., 2009; Moghad-
dam and Ester, 2011). For each product Pi, we
first parse the related reviews Di into phrases Fi,
and each product can be modelled as a bag of
phrases.

Sentiment lexicons : A sentiment lexicon L is
a list of sentiment words, and each word m ∈ L is
associated with a sentiment polarity score sm. sm
can take T values. Note that the lexicon L usually
only covers a small subset of sentiment words in
the whole vocabulary.

Sentiment association: The sentiment label
takes R values, and there are T different values for
the polarity score in the sentiment lexicon. How-
ever, the relation between sentiment labels and po-
larity scores are unknown. If we have training in-
stances where a sentiment word m is associated
with both a sentiment label rm and polarity score
sm, we can build a classifier, where the explana-
tory variable for the classifier is a sentiment label,
and outcome is the polarity score. In this case,
H(sm|rm) can be interpreted as the probability of
observing a polarity score sm, given its sentiment
label rm. We refer to this probability H as senti-
ment association. This is a key component in our
model. It naturally bridges the gap between sen-
timent labels and polarity scores, and captures the
uncertainty in their relations. Note that H can be
trained independent of the topic model part. For
each training instance, suppose the sentiment word
is m ∈ L, we need to know its sentiment label
rm and polarity score sm. sm can be retrieved di-
rectly from the sentiment lexicon, and rm can be
either manually or automatically annotated. For
example, suppose the word m appears in review
d, we can assign the overall rating of d as its sen-
timent label. In this case, each word m ∈ L can
be associated with multiple training instances that
have the same value for sm but different sentiment
labels rm. We adopt this approach to automati-
cally annotate sentiment labels, and details are de-
scribed in the Experiments section.

3.2 Problem definition

The product aspect rating prediction problem can
be defined as follows. The input is a set of prod-
ucts P . Each product Pi has a bag of phrases Fi,
and an overall rating distribution Yi over R rat-

Figure 1: Graphical model of SATM

ings. The output is the K corpus-level aspects,
and for each product, we predict its ratings on the
K aspects, also in the [1, R] rating scale. We as-
sume products in P are in the same category so
they share the same aspects.

3.3 The SATM model

We introduce the Sentiment-aligned Topic
Model(SATM) in this section, and its graphical
representation is shown in Figure 1. Note that the
sentiment association H is observed, because it is
trained independently of the topic model part.

At the word level, each observed phrase <
h,m > is associated with two latent variables:
aspect z and sentiment label r. Aspect z models
what aspect this phrase talks about, and r deter-
mines the sentiment of m. If m is in the senti-
ment lexicon, we assume r is also responsible for
generating a word feature vm, based on the senti-
ment association H , which is equal to its polarity
score sm in the lexicon. In this case, the observed
data becomes (< h, m >, vm), and the latent sen-
timent label r is responsible for generating both
word m, and word feature vm. For example, for
the phrase <room, filthy>, we observe a word fea-
ture v = −5, since the sentiment polarity score for
the word “filthy” is−5. Given H , sentiment labels
1 or 2 are more likely to generate a word feature
−5. Also, people tend to use “filthy” to express
low ratings, like 1 or 2, so the sentiment labels and
ratings can be aligned.

At the product level, for each product p and
each aspect k, we define a multinomial distribu-
tion λp,k over R sentiment labels. Since Yp already
gives us the big picture about the overall senti-

1195



ment expressed on this product, we assume λp,k
is drawn from a dirichlet distribution Dir(πp,k)
with asymmetric concentration parameters, where
πp,k = f(Yp, ωk, ωb). We can use a linear
parametrization, and set

f(Yp, ωk, ωb) = ω1kYp + ω
0
k + ω

b (1)

ω1k captures the influence of the product overall
rating distribution, and can favour certain sen-
timent labels in the prior. ω0k and ω

b are the
aspect-specific and corpus-level bias, respectively.
Through this linear parametrization, we build a di-
rect matching between sentiment label i and rating
i. For example, for a product p, if its overall rat-
ing distribution Yp has high probability over rat-
ing 4, for aspect k, we assume its product-aspect-
sentiment label distribution also has high probabil-
ity on sentiment label 4 in the prior. The actual as-
pect rating is affected by both the text which talks
about aspect k, and also the prior.

To sum up, we assume the generative process as
follows:

• For each aspect k = 1, 2, . . .K,
– draw an aspect-word distribution ϕak ∼

Dir(βa)
– For each sentiment label r = 1, 2, . . . R,

draw an aspect-sentiment label-word
distribution ϕsk,r ∼ Dir(βs)

• For each product p ∈ P ,
– draw a product-aspect distribution θp ∼

Dir(α)
– for each aspect k, draw a product-

aspect-sentiment label distribu-
tion λp,k ∼ Dir(πp,k) where
πp,k = f(Yp, ωk, ωb)

• For each phrase f =< h, m > of product p,
1. Draw an aspect z from θp
2. Draw a sentiment label r from λp,z
3. Draw an aspect word h from ϕaz
4. Draw a sentiment word m from ϕsz,r.

If m ∈ L, generate a word feature vm
based on H .

By integrating out θ, ϕ and λ, the joint proba-
bility can be defined as:

P (z, r, h, m,v|α, βa, βs, π,H) =
P (z|α)P (r|z, π)P (h|z, βa)

P (m|z, r, βs)P (v|r,H) (2)

3.4 Inference

We use Gibbs Sampling(Griffiths and Steyvers,
2004) to estimate the posterior distribution given
the observed data.

We jointly sample the aspect z and sentiment
label r for the ith phrase < h,m > of product p,
given the assignments of other phrases:

P (zi = k, ri = l|z−i, r−i, h,m, v) ∝

(np,k + α)
nak,h + β

a∑
h′(n

a
k,h′ + β

a)
np,k,l + πp,k,l∑

l′(np,k,l′ + πp,k,l′)
nsk,l,m + β

s∑
m′(n

s
k,l,m′ + β

s)
g(m, l)

(3)

where g(m, l) = H(vm|l) if m ∈ L. In this
case, when we sample the sentiment label r for this
phrase, the probability of generating word feature
vm from r is also considered. For example, the
word “excellent” has a word feature value vm = 5.
Based on H , the probability of generating a word
feature 5 is higher for sentiment labels with larger
values. If m /∈ L, there is no g(m, l) term, since
no word feature is associated with this phrase. In
Equation 3, np,k is the number of times aspect k
is assigned to phrases of product p, and nak,h is
the number of times aspect word h is assigned to
aspect k. np,k,l is the number of times sentiment
label l is assigned to aspect k for product p, and
nsk,l,m is the number of times sentiment word m
is assigned to aspect k and sentiment label l. All
these counts exclude assignments for the current
phrase < h, m >.

Based on the samples, we can estimate λp,k,r as:

λp,k,r =
np,k,r + πp,k,r∑

r′(np,k,r′ + πp,k,r′)
(4)

Since sentiment labels and atings are aligned, the
aspect rating tpk of product p on aspect k can be
simply calculated as the expectation of λp,k:

tpk =
∑

r

λp,k,r · r (5)

4 Experiments

In this section, we describe the experiments and
analyze the results.

1196



4.1 Dataset
We use the TripAdvisor dataset1(Wang et al.,
2010) for evaluation, since in this dataset, reviews
are not only associated with overall ratings, but
also with ground truth aspect ratings on 7 aspects:
value, room, location, cleanliness, check in/front
desk, service, business service. All the ratings in
the dataset are in the range from 1 star to 5 stars.
We first remove reviews with any missing aspect
ratings or very short reviews(less than three sen-
tences). Then we adopt the dependency parser
technique to identify opinion phrases, and collect
phrases with adjective sentiment words. The de-
pendency parser can deal with conjunctions, nega-
tions and bigram aspect words, and it results in the
best performance according to (Moghaddam and
Ester, 2012). Some sample phrases are shown in
Table 1. All words are converted into lower case,
and we remove phrases containing words that ap-
pear no more than 10 times or stop words. Since
we are only interested in product-level aspect rat-
ing prediction, for each product, we aggregate all
the review overall ratings to get the overall rating
distribution. The statistics of the dataset is shown
in Table 2. The average rating is the rating av-
eraged over all reviews and all products. As we
can see, positive reviews are dominant in the data,
which raises the challenge of discovering negative
sentiment topics.

Sentences Phrases
The room, facing the
courtyard, was large and
comfortable.

<room, large>,
<room, comfortable>

The room was not really
clean.

<room, no clean>

Internet access was
available.

<Internet access,
available>

Table 1: Sample extracted phrases

#Products #Reviews Avg rating #Phrases
1850 61306 4.03 740982

Table 2: Statistics of the dataset

4.2 Metrics
We use three evaluation metrics for comparison.

RMSE: Root-mean-square error is used to mea-
sure the difference between the predicted aspect

1http://sifaka.cs.uiuc.edu/˜wang296/
Data/index.html

ratings and ground truth aspect ratings. It is de-
fined as:

RMSE =

√∑
p

∑
k(tpk − t̂pk)2
|P | ×K (6)

where tpk is the predicted aspect rating for product
p on aspect k, and t̂pk is the ground truth.

Precision@N: For each aspect k, we rank the
hotels based on their predicted aspect ratings, and
get the top N results. A hotel is considered rele-
vant if its ground truth aspect rating is in the top
10% of the ground truth aspect ratings of all ho-
tels. Precision@N is defined as the percentage of
the top N results that are relevant:

Precision@N =
|{relevant hotels} ∩ {top N ranked hotels}|

N
(7)

We use N = 10, and the result is averaged over K
aspects.

ρhotel: Pearson correlation across hotels(Wang
et al., 2010) is defined as:

ρhotel =
∑

k ρ(tk, t̂k)
K

(8)

where tk is the predicted aspect rating vector for
all hotels on aspect k, and t̂k is the corresponding
ground truth vector. ρ(tk, t̂k) is the Pearson cor-
relation between these two vectors. It measures
how the predicted ratings of aspect k can preserve
the order in the ground truth(Wang et al., 2010).
If we can predict an aspect-specific ranking sim-
ilar to the ground truth, we can use the predicted
aspect ratings to answer questions like “Is hotel a
better than hotel b on aspect k?”

4.3 Baselines

The first three baselines are Local Prediction,
Global Prediction and Graph Propagation.
They all separate aspect extraction and sentiment
identification. For each phrase f =< h,m > from
review d of product p, we first find the aspect as-
signment of this phrase. Then, we use three meth-
ods to get the phrase rating. Local Prediction(Lu et
al., 2009) simply uses the overall rating of d as its
phrase rating. Global Prediction(Lu et al., 2009)
trains a multi-class classifier to classify the senti-
ment word m into a rating category r ∈ 1, 2 . . . R,

1197



Figure 2: Method for aspect extraction in Local
Prediction, Global Prediction and Graph Propaga-
tion

then assigns r as the phrase rating. Graph Prop-
agation(Brody and Elhadad, 2010) builds a con-
junction graph for sentiment words, and uses a La-
bel Propagation algorithm on the graph to learn the
sentiment polarity score for each sentiment word.
The score of m is set as phrase rating. Finally,
we aggregate all the phrases of each aspect to pre-
dict the aspect ratings. To apply these methods in
our experiments, in the aspect extraction step, we
adapt our model to extract only aspects, as shown
in Figure 2. In this simplified model, no sentiment
labels is involved, and the latent aspect explains
both the aspect word and sentiment word.

ILDA(Moghaddam and Ester, 2011) was pro-
posed for aspect rating prediction, but it fails to
deal with the sentiment label alignment problem,
so it cannot be directly used for this task. We adopt
the common approach of providing seed words to
set priors for each sentiment topic.

LRR(Wang et al., 2010) was proposed to pre-
dict aspect ratings for each review, but it can also
be used to predict product aspect ratings by ag-
gregating all the reviews of a product into a single
“h-review”(Wang et al., 2011). First, we can run
a topic model to learn aspects, and annotate each
sentence with an aspect. Then LRR is applied on
the annotated sentences to predict aspect ratings.
This approach provided the best result, according
to (Wang et al., 2011). In the first step we use the
sentence-LDA(Jo and Oh, 2011) to annotate sen-
tences, which is slightly different from the original
method, but still provides a good analogy.

We also test two simplified version of the SATM
model. First, we remove the part which involves
sentiment lexicons, so we only use the product
overall rating distribution. We call this method
SATM-O. Second, we use only sentiment lexi-
cons, ignoring the influence of overall rating dis-

tribution. We call it SATM-L. These two baselines
can help us identify how the sentiment lexicon and
overall rating distribution can improve the results,
if used separately.

Our last baseline simply uses the overall rating
of a hotel as its aspect ratings. For each hotel, its
overall rating is defined as the average overall rat-
ing of its reviews. This method is referred to as
Overall.

4.4 Experimental Setup

For all topic modelling based approaches, the
number of aspects is set to 7. Since we can evalu-
ate aspect rating prediction only on the predefined
aspects, we need to ensure the discovered aspects
match the predefined aspects. To do this, we adopt
the common approach of providing a few seed
words for each aspect as priors, as in (Wang et al.,
2010). The seed words are listed in Table 3. There
may be better methods to use seed words for as-
pect discovery (Jagarlamudi et al., 2012; Mukher-
jee and Liu, 2012), and it would be interesting to
combine their methods with ours. However, this
is beyond the scope of this paper, and we list it as
future work.

Aspects Seed words
Value value, price, worth
Room room, rooms

Location location
Cleanliness room, dirty, smelled, clean

Check in/front desk staff
Service service, breakfast, food

Business service internet, wifi

Table 3: Seed words for aspect discovery

We use 5 sentiment labels in SATM, SATM-
L and SATM-O, as this is the number of dis-
tinct ratings. The lexicon L used in our experi-
ment is part of (Taboada et al., 2011) where words
are associated with polarity scores in the range
[−5,−1] ∪ [1, 5]. We observe that words with po-
larity score 1 and−1 express too weak sentiments,
so we discard them in our experiment. To get
training instances for sentiment association H , we
treat each appearance of word m ∈ L in the data as
one training instance. The polarity score sm is di-
rectly retrieved from L, and the sentiment label rm
is the overall rating of review d where m appears.
This approach avoids the need for manual annota-
tion of sentiment labels, and the annotation result
captures the characteristics of the dataset. How-

1198



ever, all training instances in a review will have
the same sentiment label, which means that we as-
sume all sentiment words in a review express the
same sentiment, no matter what aspects they talk
about. This is not true, thus will introduce noise to
the training. To reduce noise, for words with pos-
itive polarity score, we ignore their appearance in
reviews with rating 1 and 2, since we assume pos-
itive sentiment words rarely express negative sen-
timents, even if they appear in negative reviews.
Therefore, H(sm|rm) = 0 for rm = 1, 2 and sm
in the range [2, 5]. A similar method is used to deal
with words with negative polarity score.

For Global Prediction, in (Lu et al., 2009), the
prior for the multi-class classifier is uniform, while
in our experiment, for product p, we used product
overall rating distribution on r as the prior for rat-
ing category r, which achieves better results than
uniform prior.

The Graph Propagation method requires a small
set of sentiment words as seeds, from which the al-
gorithm can learn sentiment score for other words.
The method in (Brody and Elhadad, 2010) con-
structs these seed words based on morphology in
an unsupervised way, and can only support two
kinds of sentiment: positive and negative. In our
experiment, since the sentiment lexicon is avail-
able, the sentiment seed words are from the lexi-
con, and we update the polarity score for those not
in the lexicon.

For ILDA, since we need to provide seed words
as priors for sentiment topics, we have two op-
tions, and we use both for experiment. First,
we can employ the common approach of using
two sentiment labels(R=2, positive and negative).
Then, words with positive polarity scores in lexi-
con L are used as priors for the positive sentiment
topic, and similarly words with negative polarity
scores for negative sentiment topic. An alternative
approach is to use 5 sentiment labels(R=5). It pro-
vides finer grained sentiment extraction, but raises
the question of how to choose seed words for each
sentiment topic. To do this, we use the full senti-
ment lexicon in (Taboada et al., 2011), where sen-
timent words have polarity score in the range of
[−5,−1] ∪ [1, 5]. We divide the lexicon, and use
words with polarity score 4 and 5 as prior for the
sentiment topic with label 5. Then, words with po-
larity score 2 and 3 are used for the sentiment topic
with label 4, and so on.

For all topic modelling based approaches, we

set the number of iterations for Gibbs Sampling
to 3000, and take samples from the markov chain
every 50 iterations after a burn-in period of 1000
iterations. In SATM and SATM-O, for all aspects
k, we need to choose the parameters ωk and also
wb. We use a small portion of dataset with ground
truth to choose the best value, and we set ω1k = 20,
w0k = 0.01, w

b = 0. Automatically learning these
parameters are feasible. One possible option is to
use stochastic EM sampling scheme, as in (Mimno
and McCallum, 2008). For the LRR implementa-
tion2, we use the default parameters included in
the package, and train the model with seed words
provided by the author(Wang et al., 2010).

4.5 Results

The experimental results are listed in Table 4. For
RMSE, the smaller the better, while for the other
two measures, the larger the better. Graph Prop-
agation, ILDA and SATM-L do not use the over-
all ratings(except for training sentiment associa-
tion H), so we group them together. Similarly
we group Local Prediction, Global Prediction,
SATM-O and SATM. The Overall method is a spe-
cial baseline that does not do any aspect based pre-
diction. For the LRR method, after the first step of
sentence annotation, we notice that sentence-LDA
fails to annotate the “h-review” of some hotel with
all 7 aspects, mainly because these hotels are as-
sociated with less reviews. In this case, the LRR
model will fail in the second step, so we do not
include LRR in Table 4. Instead, we compared
our method with LRR on a subset of products that
comment on all aspects based on the sentence an-
notation. There are 1533 hotels in this subset, and
the result is shown in Table 5. Note that our exper-
imental results for LRR are far worse than those
reported in the original paper(Wang et al., 2011).
We believe this maybe due to different parameter
settings, or due to the choice of different reviews.

We observe that SATM achieves the best RMSE
value, i.e., it produces the most accurate aspect rat-
ing prediction. The Overall method does better in
ranking all the hotels(ρhotel), but SATM is better
at ranking top hotels(P@10). When we compare
the results of SATM with SATM-L and SATM-
O, we find that the good performance of SATM
is mainly due to the use of the overall rating distri-
bution. On one hand, this is reasonable, since in-

2http://sifaka.cs.uiuc.edu/˜wang296/
Codes/LARA.zip

1199



Sentiment label Top sentiment words
1 old, dirty, worn, older, dark, stained, broken, dated, outdated, bad
2 small, tiny, little, noisy, single, double, uncomfortable, smaller, larger, narrow
3 large, double, big, mini, hard, main, huge, twin, single, jacuzzi
4 nice, comfortable, modern, clean, new, good, great, flat, big, comfy
5 large, huge, great, beautiful, big, lovely, separate, spacious, wonderful, excellent

Table 6: Top sentiment words for aspect “room” with different sentiment labels

Methods RMSE P@10 ρhotel
ILDA,R=2 1.202 0.30 0.193
ILDA,R=5 1.096 0.257 0.222

Graph Propagation 0.718 0.271 0.442
SATM-L 0.774 0.443 0.483

Local Prediction 0.572 0.486 0.761
Global Prediction 0.625 0.30 0.778

SATM-O 0.429 0.80 0.841
SATM 0.384 0.814 0.854
Overall 0.415 0.80 0.863

Table 4: Experimental results except LRR

Methods RMSE P@10 ρhotel
LRR 1.018 0.3 0.404

SATM 0.373 0.829 0.849

Table 5: Experimental comparison with LRR

tuitively aspect ratings usually do not diverge too
far from the overall rating, especially for hotels
with higher overall ratings. As we can see from
the result of Overall, the overall rating has good
correlation with aspect ratings, and using overall
rating only is already a strong predictor for as-
pect ratings. Also, in most cases, methods using
overall ratings(Overall and the four methods in the
middle of Table 4) are better than others(first four
methods). On the other hand, we should not rely
only on the overall rating distribution. By incor-
porating the sentiment lexicon, for RMSE, SATM
achieves 10% improvement over SATM-O and 7%
improvement than Overall. Also, the overall rating
may not always be a good aspect rating predictor,
depending on the dataset.

To take a closer look at cases where the over-
all rating is not a good aspect rating predictor,
we evaluate the RMSE on different subsets of ho-
tels. We divide the hotels into different overall rat-
ing ranges: [1.2), [2,3), [3,4) and [4,5]. The re-
sults are shown in Table 7. Going from the [4,5]
group to [1,2) group, the overall rating becomes
less and less reliable to predict aspect ratings, and
the gain of SATM increases compared to SATM-

Methods [1,2) [2,3) [3,4) [4-5]
Local Prediction 0.789 0.772 0.621 0.456
Global Prediction 1.013 0.884 0.584 0.567

SATM-O 0.703 0.564 0.446 0.359
SATM 0.606 0.494 0.394 0.332
Overall 0.735 0.612 0.431 0.320

Table 7: RMSE on hotels with different overall
rating ranges

O and Overall. For a hotel with higher overall
rating(good hotel), its aspect ratings are closer to
the overall rating. This matches our intuition that
good hotels are expected to be good on most as-
pects, if not on all aspects. For a hotel with av-
erage and lower overall rating, the average differ-
ence between aspect ratings and overall rating is
larger. In this case, the overall rating can not tell
us the whole story, which calls for aspect based
prediction. Our method achieves the best RMSE
gain on this group of hotels.

4.6 Qualitative analysis
To provide a qualitative analysis, we can list the
top words for the aspect-sentiment label-word dis-
tributions. In Table 6, we list them for the aspect
“room”, with 5 different sentiment labels. We ob-
serve that, as the sentiment label value increases,
the sentiment topics express more and more pos-
itive sentiments. This means the sentiment labels
and ratings are indeed aligned, so that we can use
these sentiment labels to predict ratings.

5 Conclusion and future work

In this paper, we proposed a sentiment aligned
topic model(SATM) for product aspect rating pre-
diction. By incorporating the overall rating distri-
bution and a sentiment lexicon, our SATM model
can align sentiment labels with ratings. Experi-
ments on a TripAdvisor dataset demonstrate the
effectiveness of SATM on aspect rating prediction.

In SATM, for each product and each aspect, the
multinomial distribution over sentiment labels has

1200



prior parameterized by product overall rating dis-
tribution. We assume linear dependency, but it will
be interesting to explore other dependencies. An-
other direction is to learn the parameters ωk auto-
matically, so that ωk can be different for different
k, capturing the influence of the overall rating on
different aspects.

Finally, we assume each phrase is associated
with one latent aspect. However, aspects may
be correlated. For example, the phrase <room,
filthy> gives us information about the aspect room
and also the aspect cleanliness. To deal with this
problem, we can relax the assumption that one
phrase talks about one aspect, or we can model
correlation among aspects.

Acknowledgments

This research is supported by NSERC Discovery
Grant. The authors thank Dr. Maite Taboada for
providing the sentiment lexicon.

References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.

2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ’09, pages 25–32, New York,
NY, USA. ACM.

David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirich-
let allocation using first-order logic. In Proceedings
of the Twenty-Second International Joint Conference
on Artificial Intelligence - Volume Volume Two, IJ-
CAI’11, pages 1171–1177. AAAI Press.

Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan,
George A. Reis, Ryan Mcdonald, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.

Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
’10, pages 804–812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malú Castellanos, and Riddhiman Ghosh.

2013. Exploiting domain knowledge in aspect ex-
traction. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2013, 18-21 October 2013, Grand Hy-
att Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL, pages
1655–1667.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228–5235,
April.

Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22Nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’99,
pages 50–57, New York, NY, USA. ACM.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.

Jagadeesh Jagarlamudi, Hal Daumé, III, and
Raghavendra Udupa. 2012. Incorporating lex-
ical priors into topic models. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ’12, pages 204–213, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Yohan Jo and Alice H. Oh. 2011. Aspect and sen-
timent unification model for online review analy-
sis. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ’11, pages 815–824, New York, NY, USA.
ACM.

Suin Kim, Jianwen Zhang, Zheng Chen, Alice H.
Oh, and Shixia Liu. 2013. A hierarchical aspect-
sentiment model for online reviews. In Proceedings
of the Twenty-Seventh AAAI Conference on Artificial
Intelligence, July 14-18, 2013, Bellevue, Washing-
ton, USA.

Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indra-
jit Bhattacharya, and Srujana Merugu. 2011. Ex-
ploiting coherence for the simultaneous discovery
of latent facets and associated sentiments. In Pro-
ceedings of the Eleventh SIAM International Con-
ference on Data Mining, SDM 2011, April 28-30,
2011, Mesa, Arizona, USA, pages 498–509.

Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, ACL 2013, 4-9 August 2013,
Sofia, Bulgaria, Volume 1: Long Papers, pages
1630–1639.

1201



Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence, AAAI
2010, Atlanta, Georgia, USA, July 11-15, 2010.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ’09, pages
375–384, New York, NY, USA. ACM.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.

Chong Long, Jie Zhang, Minlie Huang, Xiaoyan Zhu,
Ming Li, and Bin Ma. 2014. Estimating feature rat-
ings through an effective review selection approach.
Knowl. Inf. Syst., 38(2):419–446.

Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of the 18th International
Conference on World Wide Web, WWW ’09, pages
131–140, New York, NY, USA. ACM.

Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of the 16th International Conference on
World Wide Web, WWW ’07, pages 171–180, New
York, NY, USA. ACM.

David M. Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In the Conference
on Uncertainty in Artificial Intelligence, pages 411–
418.

Samaneh Moghaddam and Martin Ester. 2011. Ilda:
Interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ’11, pages 665–674, New
York, NY, USA. ACM.

Samaneh Moghaddam and Martin Ester. 2012. On the
design of lda models for aspect-based opinion min-
ing. In Proceedings of the 21st ACM International
Conference on Information and Knowledge Man-
agement, CIKM ’12, pages 803–812, New York,
NY, USA. ACM.

Samaneh Moghaddam and Martin Ester. 2013. The
flda model for aspect-based opinion mining: Ad-
dressing the cold start problem. In Proceedings of
the 22Nd International Conference on World Wide
Web, WWW ’13, pages 909–918.

Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ’12, pages 339–348, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ’05, pages 339–346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Christina Sauper and Regina Barzilay. 2013. Auto-
matic aggregation by joint modeling of aspects and
values. J. Artif. Int. Res., 46(1):89–127, January.

Christina Sauper, Aria Haghighi, and Regina Barzilay.
2011. Content models with attitude. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 350–358,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267–307, June.

Ivan Titov and Ryan McDonald. 2008a. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th International Conference on
World Wide Web, WWW ’08, pages 111–120, New
York, NY, USA. ACM.

Ivan Titov and Ryan T. McDonald. 2008b. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL 2008, Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, pages 308–316.

Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data:
A rating regression approach. In Proceedings of
the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’10,
pages 783–792, New York, NY, USA. ACM.

Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’11, pages 618–
626, New York, NY, USA. ACM.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 56–
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.

1202


