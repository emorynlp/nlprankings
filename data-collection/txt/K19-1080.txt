



















































Self-Adaptive Scaling for Learnable Residual Structure


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 862–870
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

862

Self-Adaptive Scaling Approach for Learnable Residual Structure

Fenglin Liu1, Meng Gao3, Yuanxin Liu4,5 and Kai Lei2∗
1ADSPLAB, School of ECE, Peking University, Shenzhen, China

2ICNLAB, School of ECE, Peking University, Shenzhen, China
3School of ICE, Beijing University of Posts and Telecommunications, Beijing, China
4Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China

5School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
fenglinliu98@pku.edu.cn, gaomeng@bupt.edu.cn

liuyuanxin@iie.ac.cn, leik@pkusz.edu.cn

Abstract

Residual has been widely applied to build deep
neural networks with enhanced feature propa-
gation and improved accuracy. In the litera-
ture, multiple variants of residual structure are
proposed. However, most of them are manu-
ally designed for particular tasks and datasets
and the combination of existing residual struc-
tures has not been well studied. In this work,
we propose the Self-Adaptive Scaling (SAS)
approach that automatically learns the design
of residual structure from data. The proposed
approach makes the best of various residual
structures, resulting in a general architecture
covering several existing ones. In this man-
ner, we construct a learnable residual struc-
ture which can be easily integrated into a wide
range of residual-based models. We evaluate
our approach on various tasks concerning dif-
ferent modalities, including machine transla-
tion (IWSLT-2015 EN-VI and WMT-2014 EN-
DE, EN-FR), image classification (CIFAR-
10 and CIFAR-100), and image captioning
(MSCOCO). Empirical results show that the
proposed approach consistently improves the
residual-based models and exhibits desirable
generalization ability. In particular, by incor-
porating the proposed approach to the Trans-
former model, we establish new state-of-the-
arts on the IWSLT-2015 EN-VI low-resource
machine translation dataset.

1 Introduction

Recently, residual learning attracts considerable at-
tention in training deep neural networks, and many
efforts have been devoted to study the utilization
of residual structure in tasks across a broad span
of fields, including but not limited to computer
vision (He et al., 2016a; Huang et al., 2017; He
et al., 2016b; Szegedy et al., 2017) and natural
language processing (Vaswani et al., 2017; Devlin

∗Corresponding Author

·

F(x,W)

addition

x

(a) ResNet

·

F(x,W)

addition

y

LN

x

(d) Transformer

y

T(x)

·

F(x,W)

addition

x

(c) HighWay Net

��

y

·

F(x,W)

addition

y

(b) Inception-v4 

�

x

0.1~0.3

Figure 1: Various types of residual structures: (a)
ResNet (He et al., 2016a); (b) Inception Net (Szegedy
et al., 2017); (c) Highway Net (Srivastava et al., 2015);
(d) Transformer (Vaswani et al., 2017), where LN rep-
resents layer normalization (Ba et al., 2016).

et al., 2019). Residual structure, which alleviates
the so-called gradient exploding or vanishing prob-
lem in optimization (He et al., 2016a), enables the
training of neural networks with great depth by
building skip connections between layers.

Generally, the residual structures (as illustrated
in Figure 1) can be formulated as:

y = G(α · x+ β · F(x,W)) (1)

where x denotes the input (i.e., the skip connec-
tion), F denotes the residual function (i.e., residual
branch) parameterized by W , and y is the output
of the residual block. The balance between x and
F is governed by the weights α and β, followed
by G, which could be either identity mapping or



863

normalization.

Previous works on residual structure design-
ing, which differ in the way that the information
flows are regulated, mainly concern two elements,
namely the mapping formulation (weight assign-
ment) and the normalization mechanism. As shown
in Figure 1, ResNet (He et al., 2016a), Inception-v4
(Szegedy et al., 2017) and Highway Net (Srivastava
et al., 2015) explored the question on how should
the residual connection be incorporated into the
existing neural network structures so that the best
improvements can be achieved. Recently, Trans-
former (Vaswani et al., 2017) applied the layer nor-
malization (Ba et al., 2016) to help the optimiza-
tion of the non-linear transformation (i.e., the F)
to some extent. At the same time, He et al. (2016b)
observed considerably worse results when they uti-
lized batch normalization (Ioffe and Szegedy, 2015)
after the residual connection, the reason for which
batch normalization is less employed in the residual
structure.

Despite their respective advantages and success
in certain fields, we argue that the structures are
only particular cases of a more general one, which
necessitates further insights into possible combina-
tions. However, the determination of an effective
combination may require prior knowledge of the
data distribution, which is not always available, or
extensive hyper-parameter exploration, which is
inefficient.

In this paper, we aim at constructing a compre-
hensive and flexible residual structure. To this end,
we propose the Self-Adaptive Scaling approach. In
the residual structure, the proposed approach au-
tomatically computes scaling factors to adjust the
mapping formulation and the normalization mech-
anism, respectively. By assigning different impor-
tance to the skip connection, the residual branch
and a normalized result, the scaling factors adap-
tively controls the topology of the residual building
blocks.

As a result, the structure learned by our pro-
posed approach can be easily generalized to various
kinds of tasks and data, dispensing with the time-
consuming architecture search, to some extent. The
proposed learnable residual structure can be easily
integrated into existing residual-based models. We
evaluate the proposed approach on representative
residual models for various tasks. The experiment
results and analyses attest to our argument and the
effectiveness of the proposal.

Overall, the contributions are summarized as
followed:

• We proposed a novel self-adaptive scaling
(SAS) approach to acquire a learnable residual
structure, which allows deep neural models to
automatically learn the residual structure and
can cover different types of existing ones.

• The proposed approach is simple and can
be easily applied to a wide range of exist-
ing residual-based models. According to our
empirical studies, the SAS can enable exist-
ing models to achieve consistent performance
gains, demonstrating its generalization ability
to a wide range of existing systems.

• The experimental results on the IWSLT-2015
EN-VI show that SAS helps the Transformer-
Base model to perform even better than the
Transformer-Big model and, encouragingly,
we establish a new state-of-the-art on this low-
resource machine translation dataset.

2 Related Work

In recent years, the application of residual struc-
ture to deep neural networks has become an ac-
tive research topic (He et al., 2016a; Srivastava
et al., 2015; He et al., 2016b; Vaswani et al., 2017;
Szegedy et al., 2017). In the studies on residual
architectures, there are two problems of interest.
The first is how should the information from the
skip connection and the residual branch be well
balanced so that the best improvements can be
achieved. The second is how should the neural
network with residual connections be optimized
so that its representation capability could be fully
mined. These two types of problems are mainly ad-
dressed by designing appropriate mapping formu-
lation and normalization mechanism, respectively,
and we refer to them as On the Connection Problem
and On the Optimization Problem.

On the connection problem. There are roughly
three lines of methods to control the balance in
residual connections: identity mapping, constant
scaling ratio and adjusted scaling ratio. He et al.
(2016b) designed five types of shortcut connections
and discussed the possible residual connections in
detail. Based on their theory and experiments, they
argued that “keeping a ‘clean’ information path is
helpful for easing optimization”. The reason is that
with scaling, the gradient of the residual suffers



864

from the gradient exploding or vanishing problem,
which hinders the deep neural network from effi-
cient optimization. Szegedy et al. (2017) adopted
constant scaling to govern the residual balance in
deep inception networks, which, despite its decent
performance, is relatively inflexible. Highway net-
work (Srivastava et al., 2015) is among the very
first endeavors to implant residual structures into
deep neural networks. It built a highway connec-
tion from the input to the output, where a transform
gate was proposed to control the balance of the skip
connection x and the residual branch F(x,W), as
opposed to the identity mapping.

On the optimization problem. In the realm of
computer vision, PreAct-ResNet (He et al., 2016b)
demonstrated that it is helpful to apply batch nor-
malization to x, instead of F(x,W). In other
words, the batch normalization acts on the output
of the previous block. For natural language process-
ing, the popular Transformer (Vaswani et al., 2017)
makes use of residual connection in conjunction
with layer normalization to build the model archi-
tecture and achieves record-setting performance.
Layer normalization is widely believed to be help-
ful for stabilizing training and facilitating conver-
gence. According to our experiments and analyses,
the layer normalization can indeed facilitate opti-
mization and therefore improve the overall perfor-
mance of the model.

Different from existing work, we summarize the
combination of normalization and residual con-
nection in existing works with a general form
y = α ∗ x+ β ∗ F + γ ∗ LN(x+ F), where the
mapping formulation and the normalization mech-
anism are both taken into account. By changing
the scaling factors α, β and γ, the topology of the
residual block can be adaptively adjusted, resulting
in a learnable residual structure. The learned archi-
tecture distinguishes itself from the previous ones
with generality and flexibility.

Our work is also related to the line of research
on neural architecture search (Zoph and Le, 2017),
where the network structure is also automatically
learned by algorithm. However, neural architecture
search requires sampling architecture descriptions
based on predicted probability from a controller
network that is optimized via reinforcement learn-
ing, which is time-consuming. But our proposed
approach can be trained directly with the loss func-
tions of different tasks.

No. α β γ Architecture
1 0 0 1 LN(x+ F) (Vaswani et al., 2017)
2 1 1 0 x+ F (He et al., 2016a)
3 1 β 0 x+ β ∗ F (Szegedy et al., 2017)
4 α 1 0 α ∗ x+ F (He et al., 2016b)

Table 1: Four particular cases in formula (3), which
cover four representative residual structures, i.e., Trans-
former (Vaswani et al., 2017), ResNet (He et al.,
2016a), Inception-v4 (Szegedy et al., 2017) and
shortcut-only gating proposed in He et al. (2016b).

3 Architecture

In this section, we first briefly introduce the Scal-
ing Gate in Section 3.1, which is used to predict
the scaling factors for the mapping formulation
and the normalization mechanism. Then, based
on the scaling factors, in Section 3.2, we describe
how to adaptively make the best of different types
of residual structure to build a learnable residual
structure.

3.1 Scaling Gate

The Scaling Gate should be able to predict reason-
able scaling factors. Our motivation stems from the
superior performance of Feed-Forward Network
used in Vaswani et al. (2017). When selecting
the activation function, since we expect the final
predicted value of the scaling factors to cover the
range of 0∼1, we applied the Sigmoid activation
function to the original outputs of the scaling gate.
As a result, the scaling gate takes the x ∈ Rh and
the F(x,W) ∈ Rh as input and computes the out-
put through two linear transformations with a Tanh
activation in between:

S(x,F) = Tanh([x;F ]Wf + bf )Wff + bff (2)

where [;] denotes concatenation operation, Wf ∈
R2h×h and Wff ∈ Rh×1 are the parameters to be
learned, and S(x,F) is followed by a Sigmoid
activation function.

In HighWay Net (Srivastava et al., 2015), the
inputs of the Transform Gate only involve the in-
formation of x and the structure only contains one
layer of linear transformation, i.e., xW + b. In
contrary, we integrate the information from x and
F and build the structure with two layers of lin-
ear transformation, which strengthens the scaling
gate’s expressive power. The difference is illus-
trated in Figure 2, and as illustrated in Table 6, our
scaling gate is experimentally found to perform
better.



865

No. α β Architecture Remarks
1 0 0 LN(x+ F) The residual structure of Transformer (Vaswani et al., 2017).
2 0 1 F The well-trained F is sufficient in representation ability.
3 1 0 x F is poor-trained or x is sufficient in representation ability.
4 1 1 x+ F The residual structure of ResNet (He et al., 2016a).
5 1 β x+ β ∗ F The residual structure of Inception-v4 (Szegedy et al., 2017).
6 α 1 α ∗ x+ F The residual structure of shortcut-only gating (He et al., 2016b).

7 1-β β (1− β) ∗ x+ β ∗ F+
β(1− β) ∗ LN(x+ F) The combination of Highway Net (He et al., 2016b) and Transformer.

Table 2: Seven special cases in formula (4), which include four representative structures, i.e., Transformer (Vaswani
et al., 2017), ResNet (He et al., 2016a), Inception-v4 (Szegedy et al., 2017) and Highway Net (Srivastava et al.,
2015).

S (x,F)

·

F(x,W)

addition

x

(b) Scaling Gate

��

y

T(x)

·

F(x,W)

addition

x

(a) Transform Gate 

��

y

Figure 2: The difference between the Transform Gate
in Highway Net (Srivastava et al., 2015) and the pro-
posed Scaling Gate.

3.2 Self-Adaptive Scaling Approach

It is intuitive to combine different types of residual
structure through adjustable scaling factors. There-
fore, we reformulate the residual block as follows:

y = α ∗ x+ β ∗ F + γ ∗ LN(x+ F) (3)

where α, β and γ can be predicted by scaling gates
with different parameters, and LN stands for layer
normalization (Ba et al., 2016).

By choosing certain values for α, β and γ, we
can get several special cases, as shown in Table
1. However, from the summarized cases, we can
easily find that if we set γ = (1 − α)(1 − β),
the approach can also cover these four baselines.
Especially, it is essential to decrease the parameters
to achieve the same purpose1. Thus, the final self-
adaptive scaling approach can be defined by the
following formula:

y = α ∗ x+ β ∗ F + (1− α)(1− β) ∗ LN(x+ F) (4)

where α and β act as the scaling factors predicted
as aforementioned. From the above formula, we

1The empirical results also show that (1 − α)(1 − β)
performs better than γ (94.05 accuracy vs. 93.95 accuracy in
CIFAR-10).

can obtain seven special cases in Table 2. In all,
our proposed self-adaptive scaling approach is not
only the general form of several existing structures,
which is able to encourage the model to take full
advantage of different types of residual structures,
but also gives rise to a learnable residual structure,
which can be automatically learned by deep neural
models from the data.

4 Experiment

In this section, we evaluate the proposed approach
on three representative tasks in the natural language
processing field, computer vision field, and cross-
modal scenario, that is, image classification, ma-
chine translation and image captioning. We first
briefly introduce the baseline models for compari-
son, the datasets, the metrics and implementation
details, followed by the discussions about the ex-
perimental results. Since our major concern is the
combination of different components in residual
units, we keep the internal structure (i.e., the resid-
ual function F(x,W)) of each component unaf-
fected. The training and inference strategies also
remain the same as the original models. For more
details, please refer to the cited publications.

4.1 Machine Translation

Baselines, Datasets, Metrics and Settings. For
the task of machine translation, we adopt the pop-
ular Transformer (Vaswani et al., 2017), which
is a strong baseline. The model is implemented
with the code from tensor2tensor (Vaswani et al.,
2018). Transformer follows the encoder-decoder
paradigm, but it replaces the self-recursive opera-
tion in RNNs with the self-attention that summa-
rizes all context. In each Transformer block, the
self-attended results are post-processed by residual
connection and layer normalization.

There are 133K, 4.5M, and 36M training



866

Method EN-VI EN-DE EN-FR

Transformer-Base (Vaswani et al., 2017)

Baseline 30.9 27.5 38.2
+ Proposal 32.0 27.6 38.4

Transformer-Big (Vaswani et al., 2017)

Baseline 31.6 28.5 41.0
+ Proposal 32.2 28.7 41.3

Table 3: Results (BLEU) on the machine translation task. Higher means better. The proposal brings consistent and
substantial improvements.

Dataset Baseline + Proposal Improvements

PreAct-ResNet (Vaswani et al., 2017)

CIFAR-10 93.66 94.05 +0.39
CIFAR-100 71.29 72.91 +1.62

Table 4: Results (Accuracy (%)) on the image classification task, averaging over 5 runs. Higher is better. The
proposal consistently outperforms the baselines as in machine translation. Especially, better improvements are
achieved for models on CIFAR-100.

pairs in the IWSLT-2015 English-Vietnamese
(EN-VI) (Cettolo et al., 2015), WMT-2014
English-German (EN-DE) and English-French
(EN-FR), respectively. tst2012 and tst2013
are selected as the development and test
sets, respectively, for EN-VI. For EN-DE,
we use newstest2013 and newstest2014;
and for EN-FR, newstest2012+2013 and
newstest2014 are selected. For experiments
on the two WMT datasets, we follow the imple-
mentation settings in Vaswani et al. (2017). For
experiments on the IWSLT EN-VI dataset, we set
the batch size equal to 4096 and train on single
GPU, as it is relatively small. For all datasets, we
use a single model by averaging the last 10 check-
points to produce the results with beam search of 4
and length penalty of 0.6.

Results. The results of machine translation are
presented in Table 3. Under both the Base and
the Big configuration, our proposal consistently
boosts the performance of the Transformer base-
line. When equipped with our proposed approach,
the Transformer-Base model even transcends the
big version, which is three times as large in size, on
the EN-VI translation task. It shows that the scal-
ing factors indeed help adjust the residual structure
to the data distribution, which is very efficient in
exploiting the expressive power of deep residual
networks. Encouragingly, an union of the proposal
and the Transformer-Big model achieves substan-
tial improvement, and it outperforms the state-of-

the-art method (Huang et al., 2018) in the EN-VI
low resource dataset.

4.2 Image Classification

Baselines, Datasets, Metrics and Settings. In
the computer vision field, we benchmark our pro-
posed learnable residual structure with residual-
based image classification systems, i.e., Pre-
Activated ResNet (PreAct-ResNet) (He et al.,
2016b). ResNet-110 consists of 54 double-layer
residual blocks, which makes it non-trivial to op-
timize. To demonstrate that our SAS is applicable
to such deep framework, we select ResNet-110
in the experiments. We retain most of the hyper-
parameters in He et al. (2016b), with the exception
of the weight decay rate, which is set to 0.0002, so
as to guarantee more stable training. Both CIFAR-
10 and CIFAR-100 (Krizhevsky, 2009) are com-
prised of colored images for classification. CIFAR-
100, which contains 100 classes, appears to be more
difficult as compared to CIFAR-10, where there
are only 10 classes. Following common practice
(He et al., 2016b; Srivastava et al., 2015), accuracy
rate of classification over 5 runs are reported as
the evaluation results. In Srivastava et al. (2015)
and He et al. (2016b), they found that it may be
beneficial to attach more importance to the skip
connection x for initialization, i.e., the bias term
in the transform gate should be initialized with a
negative value. Following this practice, in image
classification task, we set the bias bff for α and
β to 3 and -3, respectively, at the start of training.



867

Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr SPICE

GLIED (Liu et al., 2019c) 80.4 - - 39.6 28.9 58.8 129.3 22.6

Transformer (Vaswani et al., 2017)

Baseline 80.2 64.9 50.7 39.0 28.4 58.6 126.3 21.7
+ Proposal 81.2 65.4 51.2 39.3 28.7 59.0 129.8 22.6

Improvements +1.0 +0.5 +0.5 +0.3 +0.3 +0.4 +3.5 +0.9

Table 5: Performance on the MSCOCO Karpathy test split. Higher is better in all columns. The baseline enjoys
a comfortable improvement with the proposed approach. Additionally, we report the performance of the recently
published state-of-the-art GLIED, as we can see, our approach helps the Transformer captioning model outper-
forms GLIED substantially in terms of CIDEr, which further demonstrates the effectiveness of our approach.

The remaining weights are initialized in the same
way as in (He et al., 2016b).

Results. As can be seen in Table 4, consistent
improvements are also obtained over the baseline
model, which is much deeper than the six-layer
Transformer model. The increase in accuracy is
0.39 and 1.62 on the CIFAR-10 and CIFAR-100
dataset, respectively. This demonstrates that our
proposal also works in deeper cases. It comes to
our notice that the proposal induces better improve-
ments on the more challenging CIFAR-100 dataset.
This is presumably that the learnable structure can
make the best of each component in the residual
building block, which allows more flexible fitting
into the multi-class image distribution, resulting in
a larger space for improvement in the 100 classes
scenario.

4.3 Image Captioning
Baselines, Datasets, Metrics and Settings. To
further demonstrate the generalization ability of
our proposed approach, we conduct experiments
on the task of image captioning. The experiments
are based on the multi-head attention mechanism
(Vaswani et al., 2017), which has recently shown
great potential and is competitive with the most ad-
vanced models (Liu et al., 2019c,b), for the reason
of which we choose it as our baseline to examine
the performance of our approach on the multidisci-
plinary task.

There are several datasets that consist of image-
sentence pairs. Our reported results are evalu-
ated on the popular Microsoft COCO (MSCOCO)
(Chen et al., 2015) dataset, which contains 123,287
images. Each image in the dataset is paired with
5 sentences. The results are reported using the
widely-used publicly-available splits in the work of
Karpathy and Li (2015). The MSCOCO validation
and test set contain 5,000 images each. Following

common practice (Liu et al., 2018, 2019a), we re-
place caption words that occur less than 5 times
in the training set with the generic unknown word
token UNK, resulting in 9,567 words.

We adopt SPICE, CIDEr, BLEU, METEOR and
ROUGE for testing. They are previously used as
evaluation methods for image captioning, we re-
port the results using the MSCOCO captioning
evaluation toolkit (Chen et al., 2015). Among the
metrics, BLEU (Papineni et al., 2002) and ME-
TEOR (Banerjee and Lavie, 2005) are originally
designed to evaluate the performance of machine
translation systems. ROUGE is widely used to ex-
amine the quality of machine-produced summaries.
SPICE and CIDEr are bespoke metrics for image
captioning, which measures scene graph and n-
gram matching, respectively, and we refer to them
as primary indicators of model performance.

Results. The results on Karpathy test split
(Karpathy and Li, 2015) are reported in Table 5.
By using our proposed learnable residual structure,
improvements of 3.5 points and 0.9 points in terms
of CIDEr and SPICE respectively can be achieved,
further demonstrating the effectiveness and gen-
eralization capabilities of our approach to a wide
range of tasks. More encouragingly, the proposed
approach helps the baseline model achieves 129.8
CIDEr score, an improvement over GLIED (Liu
et al., 2019c) by 0.5.

5 Analysis

In this section, we conduct several analyses to give
further insights into our proposed approach, which
are based on the image classification task and we
adopt PreAct-ResNet-110 (He et al., 2016b) as the
baseline model.

Analysis on Scaling Gate. Table 6 summarizes
the obtained results when applying the Transform



868

19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Residual Block

0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

1.00

V
a

lu
e

beta

19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Residual Block

0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

1.00

V
a
lu

e

alpha

19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Residual Block

0.00

0.10

0.20

0.30

0.40

0.50

0.60

0.70

0.80

0.90

1.00

V
a

lu
e

(1-alpha)(1-beta)

Figure 3: Illustrations of the averaged value of α (left) and β (middle), together with the corresponding (1−α)(1−
β) (right), which are predicted by the proposed approach for each residual block in pretrained PreAct-ResNet-110
on CIFAR-10.

Methods CIFAR-10

Baseline (PreAct-ResNet-110) 93.66
+ Transform Gate 92.15
+ Scaling Gate (Single Layer) 92.73

+ Scaling Gate (Full Model) 93.82

Table 6: The effects of applying the Transform Gate
from Highway Net (Srivastava et al., 2015) and the
proposed Scaling Gate to the PreAct-ResNet-110 (He
et al., 2016b), where the performance is evaluated by
Accuracy(%). The single layer Scaling Gate takes the
form S(x,F) = [x;F ]Wf + bf .

Gate in Highway Net and our Scaling Gate to
PreAct-ResNet-110, as well as the results of the
vanilla model. As we can see, when equipped with
Transform Gate, the effect is counter-productive on
CIFAR-10 dataset. This indicates that the informa-
tion from x along is not robust and effective enough
to predict the scaling factors in the residual struc-
ture. The single layer version of our Scaling Gate
takes into account the residual branch F , thereby
improving over the Transform Gate. It is worth
mentioning that compared with the Transform Gate
(T (x) = xW Tf +bTf ), which has (h×h)+h learn-
able parameters, Scaling Gate (Single Layer) only
introduces (2h×1)+1 learnable parameters, which
is much more efficient. By modeling the scaling
factor with Scaling Gate (Full Model), a 0.16 points
promotion is achieved over the baseline on CIFAR-
10, which further demonstrates the advantages and
effectiveness of the proposed Scaling Gate.

Analysis on Self-Adaptive Scaling. Averaging
over 10,000 experimental examples, we display in
Figure 3 the value of α, β and (1−α)(1−β) in each
residual block of the pretrained PreAct-ResNet-110
on CIFAR-10. The filter sizes for the blocks at the
bottom, middle and top of the model are different.
We only show the representative blocks at the mid-
dle of the model due to space limitation. The first
column shows that in almost all cases, α is greater
than 0.9, which indicates that identity mapping is

Architecture Acc.(%)

x+ F (Baseline) (He et al., 2016b) 93.66
α ∗ x+ β ∗ F + (1− α)(1− β) ∗ BN(x+ F) 93.23
α ∗ x+ β ∗ F + (1− α)(1− β) ∗ LN(x+ F) 94.05

Table 7: Results on CIFAR-10 using the PreAct-
ResNet-110 with the batch/layer normalization. The
batch normalization is less effective than the layer nor-
malization in residual structure.

very helpful to information transfer and eases the
optimization of deep neural networks. This can
be attributed to the facilitated backward propaga-
tion of error signals by identity mappings. It is
shown in the second column that as the number
of network layers increases, the value of β grows
simultaneously, which indicates that the represen-
tation ability of the residual branch F is stronger
when it comes closer to the output of the model.
The main reason is that the error signal passed to
F is more adequate in the upper blocks, which is
beneficial to optimization. As can be seen from
the third column, more importance is assigned to
the normalized result when it comes to the lower
parts of the entire architecture, which means that
the value of (1 − α)(1 − β) is larger. This is be-
cause that in the underlying static blocks of deep
neural networks, the guidance from error signal is
weak and the optimization is unstable, thus making
the introduction of layer normalization necessary.

In all, the proposed approach regulates the in-
formation from individual components with scal-
ing factors to build the learnable residual structure,
which helps make the best of different type of resid-
ual structure, resulting in an effective combination
for better performance.

Analysis on Using Batch Normalization. The
batch normalization is used commonly in the field
of computer vision. Therefore, we replace the layer
normalization with the batch normalization in the
proposed approach to see the difference. As shown



869

19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Residual Block

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.45

0.50

G
r
a

d
ie

n
t 

N
o

r
m

x+F

19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36

Residual Block

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

0.40

0.45

0.50

G
r
a
d

ie
n

t 
N

o
r
m

"+ SAS"

Figure 4: Gradient norm of the output of residual blocks of two different structures that are based on PreAct-
ResNet-110. The values are calculated as an average of over 10,000 random examples in the training set of
CIFAR-10. We conduct analyses on the blocks from the model’s middle part. The SAS denotes the self-adaptive
scaling approach.

in Table 7, applying batch normalization has a neg-
ative effect on the performance. Most importantly,
it lags behind the baseline by a noticeable mar-
gin, which shows that batch normalization is less
effective than layer normalization in the residual
structure. We speculate that layer normalization is
able to mitigate the training issue in the form of
exploding gradient induced by the adjusted ratio
provided by the layer normalization’s own param-
eters, while batch normalization could not, which
could be intuitively derived by the framework in
Hanin and Rolnick (2018). It is probably the reason
why He et al. (2016b) observed considerably worse
results when they applied batch normalization on
the residual structure.

Analysis on Better Optimization Capability.
To understand how the proposed approach helps
the optimization of deep neural models, we inspect
into the gradient norm of the output of each resid-
ual block in the pre-trained PreAct-ResNet-110 on
CIFAR-10. The gradients are averaged over 10,000
randomly selected training examples. As shown
in the left plot of Figure 4, the gradients of the
“x + F” structure are basically the same, indicat-
ing that all the residual blocks have similar speed
for gradient descent and optimization. In contrast,
the right plot of Figure 4 reflects that more gra-
dients are allocated to the lower blocks with the
help of SAS, and the overall gradient values are
greater. This phenomenon is interesting and finally
gives rise to better results, as shown in Table 4. We
conjecture that the layers distant from the model
output cannot receive adequate guidance from the
error signal, thus requiring more gradient for op-
timization. Moreover, since the layer normaliza-
tion is able to stabilize the information flow and
accelerate convergence, adaptively incorporating
the residual structure with layer normalization can
also facilitate optimization. By allocating more

importance to layer normalization in the residual
blocks via scaling factors, the layers at the bottom
of the network can be better optimized, which is in
line with the foregoing analysis on Self-Adaptive
Scaling approach.

6 Conclusion

In this work, we focus on building a learnable resid-
ual structure, which automatically learns the de-
sign of residual structure from data, instead of the
handy-crafted designs in previous work. We pro-
pose the Self-Adaptive Scaling approach to achieve
this goal, which combines various residual struc-
tures via the predicted scaling factors, resulting in
a general residual structure covering several exist-
ing models. The proposed approach is simple and
can be easily integrated into existing residual-based
models. Experiments on the machine translation,
image classification and image captioning tasks
validate the effectiveness of the proposed method,
which successfully promotes the performance of
all the strong baselines. This also demonstrates the
generalization ability of our method. In particu-
lar, when being applied to the recently proposed
Transformer model, our approach establishes new
state-of-the-arts on the IWSLT EN-VI low resource
machine translation task, which further substanti-
ates its efficiency. Detailed analyses prove that
the proposed approach can also promote the opti-
mization ability of deep neural networks, and is
conducive to exerting the expressive power of ex-
isting models.

Acknowledgments

This work was partially supported by the
Shenzhen Fundamental Research Project (No.
ZDSYS201802051831427). We thank all the
anonymous reviewers for their constructive com-
ments and suggestions.



870

References
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.

2016. Layer normalization. CoRR, abs/1607.06450.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In ACL
Workshop.

Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, Roldano Cattoni, and Marcello Federico.
2015. The iwslt 2015 evaluation campaign. In
IWSLT 2015, International Workshop on Spoken
Language Translation.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C. Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. CoRR,
abs/1504.00325.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT.

Boris Hanin and David Rolnick. 2018. How to start
training: The effect of initialization and architecture.
In NeurIPS.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016a. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2016, Las Ve-
gas, NV, USA, June 27-30, 2016, pages 770–778.
IEEE Computer Society.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016b. Identity mappings in deep residual net-
works. In ECCV.

Gao Huang, Zhuang Liu, Laurens van der Maaten, and
Kilian Q. Weinberger. 2017. Densely connected con-
volutional networks. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017, pages
2261–2269. IEEE Computer Society.

Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong
Zhou, and Li Deng. 2018. Towards neural phrase-
based machine translation. In ICLR.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings
of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015,
volume 37 of JMLR Workshop and Conference Pro-
ceedings, pages 448–456. JMLR.org.

Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2015, Boston, MA, USA,
June 7-12, 2015, pages 3128–3137. IEEE Computer
Society.

Alex Krizhevsky. 2009. Learning multiple layers of
features from tiny images. Technical report, Com-
puter Science Department, University of Toronto.

Fenglin Liu, Meng Gao, Tianhao Zhang, and Yuexian
Zou. 2019a. Exploring semantic relationships for
image captioning without parallel data. In ICDM.

Fenglin Liu, Yuanxin Liu, Xuancheng Ren, Xiaodong
He, Kai Lei, and Xu Sun. 2019b. Aligning visual
regions and textual concepts for semantic-grounded
image representations. In NeurIPS.

Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Kai Lei,
and Xu Sun. 2019c. Exploring and distilling cross-
modal information for image captioning. In Pro-
ceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence, IJCAI 2019,
Macao, China, August 10-16, 2019, pages 5095–
5101. ijcai.org.

Fenglin Liu, Xuancheng Ren, Yuanxin Liu, Houfeng
Wang, and Xu Sun. 2018. simnet: Stepwise image-
topic merging network for generating detailed and
comprehensive image captions. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, Brussels, Belgium, Octo-
ber 31 - November 4, 2018, pages 137–149. Associ-
ation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 311–318. ACL.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. CoRR,
abs/1505.00387.

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
and Alexander A. Alemi. 2017. Inception-v4,
inception-resnet and the impact of residual connec-
tions on learning. In AAAI.

Ashish Vaswani, Samy Bengio, Eugene Brevdo,
François Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Lukasz Kaiser, Nal Kalchbrenner, Niki
Parmar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2tensor for neural machine
translation. In AMTA.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Barret Zoph and Quoc V. Le. 2017. Neural architec-
ture search with reinforcement learning. In 5th Inter-
national Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Con-
ference Track Proceedings. OpenReview.net.

http://arxiv.org/abs/1607.06450
https://aclanthology.info/papers/W05-0909/w05-0909
https://aclanthology.info/papers/W05-0909/w05-0909
https://aclanthology.info/papers/W05-0909/w05-0909
http://arxiv.org/abs/1504.00325
http://arxiv.org/abs/1504.00325
https://www.aclweb.org/anthology/N19-1423/
https://www.aclweb.org/anthology/N19-1423/
https://www.aclweb.org/anthology/N19-1423/
http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture
http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture
https://doi.org/10.1109/CVPR.2016.90
https://doi.org/10.1109/CVPR.2016.90
https://doi.org/10.1007/978-3-319-46493-0_38
https://doi.org/10.1007/978-3-319-46493-0_38
https://doi.org/10.1109/CVPR.2017.243
https://doi.org/10.1109/CVPR.2017.243
https://openreview.net/forum?id=HktJec1RZ
https://openreview.net/forum?id=HktJec1RZ
http://jmlr.org/proceedings/papers/v37/ioffe15.html
http://jmlr.org/proceedings/papers/v37/ioffe15.html
http://jmlr.org/proceedings/papers/v37/ioffe15.html
https://doi.org/10.1109/CVPR.2015.7298932
https://doi.org/10.1109/CVPR.2015.7298932
https://doi.org/10.1109/CVPR.2015.7298932
https://www.cs.toronto.edu/~kriz/cifar.html
https://www.cs.toronto.edu/~kriz/cifar.html
https://doi.org/10.24963/ijcai.2019/708
https://doi.org/10.24963/ijcai.2019/708
https://www.aclweb.org/anthology/D18-1013/
https://www.aclweb.org/anthology/D18-1013/
https://www.aclweb.org/anthology/D18-1013/
http://www.aclweb.org/anthology/P02-1040.pdf
http://www.aclweb.org/anthology/P02-1040.pdf
http://arxiv.org/abs/1505.00387
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806
https://aclanthology.info/papers/W18-1819/w18-1819
https://aclanthology.info/papers/W18-1819/w18-1819
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/7181-attention-is-all-you-need
https://openreview.net/forum?id=r1Ue8Hcxg
https://openreview.net/forum?id=r1Ue8Hcxg

