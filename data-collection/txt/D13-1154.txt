










































Using Crowdsourcing to get Representations based on Regular Expressions


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476–1480,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Using crowdsourcing to get representations based on regular expressions

Anders Søgaard and Hector Martinez and Jakob Elming and Anders Johannsen
Center for Language Technology

University of Copenhagen
DK-2300 Copenhagen S

{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dk

Abstract

Often the bottleneck in document classifica-
tion is finding good representations that zoom
in on the most important aspects of the doc-
uments. Most research uses n-gram repre-
sentations, but relevant features often occur
discontinuously, e.g., not. . . good in sentiment
analysis. In this paper we present experi-
ments getting experts to provide regular ex-
pressions, as well as crowdsourced annota-
tion tasks from which regular expressions can
be derived. Somewhat surprisingly, it turns
out that these crowdsourced feature combina-
tions outperform automatic feature combina-
tion methods, as well as expert features, by a
very large margin and reduce error by 24-41%
over n-gram representations.

1 Introduction

Finding good representations of classification prob-
lems is often glossed over in the literature. Sev-
eral authors have emphasized the need to pay more
attention to finding such representations (Wagstaff,
2012; Domingos, 2012), but in document classifica-
tion most research still uses n-gram representations.

This paper considers two document classification
problems where such representations seem inade-
quate. The problems are answer scoring (Burstein
et al., 1998), on data from stackoverflow.com, and
multi-attribute sentiment analysis (McAuley et al.,
2012). We argue that in order to adequately repre-
sent such problems we need discontinuous features,
i.e., regular expressions.

The problem with using regular expressions as
features is of course that even with a finite vocab-

ulary we can generate infinitely many regular ex-
pressions that match our documents. We suggest to
use expert knowledge or crowdsourcing in the loop.
In particular we present experiments where standard
representations are augmented with features from a
few hours of manual work, by machine learning ex-
perts or by turkers.

Somewhat surprisingly, we find that features de-
rived from crowdsourced annotation tasks lead to the
best results across the three datasets. While crowd-
sourcing of annotation tasks has become increasing
popular in NLP, this is, to the best of our knowledge,
the first attempt to crowdsource the problem of find-
ing good representations.

1.1 Related work

Musat et al. (2012) design a collaborative two-player
game for sentiment annotation and collecting a sen-
timent lexicon. One player guesses the sentiment of
a text and picks a word from it that is representative
of its sentiment. The other player also provides a
guess observing only this word. If the two guesses
agree, both players get a point. The idea of gam-
ifying the problem of finding good representations
goes beyond crowdsourcing, but is not considered
here. Boyd-Graber et al. (2012) crowdsource the
feature weighting problem, but using standard rep-
resentations. The work most similar to ours is prob-
ably Tamuz et al. (2011), who learn a ’crowd kernel’
by asking annotators to rate examples by similarity,
providing an embedding that promotes feature com-
binations deemed relative when measuring similar-
ity.

1476



BoW Exp AMT
n P (1) m µx m µx m µx

STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331
TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285
APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289

Table 1: Characteristics of the n×m data sets

2 Experiments

Data The three datasets used in our experi-
ments come from two sources, namely stackover-
flow.com and ratebeer.com. The two beer review
datasets (TASTE and APPEARANCE) are described
in McAuley et al. (2012) and available for down-
load.1 Each input example is an unstructured review
text, and the associated label is the score assigned to
taste or appearance by the reviewer. We randomly
sample about 152k data points, as well as 500 exam-
ples for experiments with experts and turks.

We extracted the STACKOVERFLOW dataset from
a publicly available data dump,2, and we briefly de-
scribe our sampling process here. We select pairs of
answers, where one is ranked higher than the other
by stackoverflow.com users. Obviously the answers
submitted first have a better chance of being ranked
highly, so we also require that the highest ranked
answer was submitted last. From this set of answer
pairs, we randomly sample 97,519 pairs, as well as
500 examples for our experiments with experts and
turks.

Our experiments are classification experiments
using the same learning algorithm in all experi-
ments, namely L1-regularized logistic regression.
We don’t set any parameters The only differences
between our systems are in the feature sets. Results
are from 5-fold cross-validation. The four feature
sets are described below: BoW, HI, Exp and AMT.

For motivating using regular expressions, con-
sider the following sentence from a review of John
Harvard’s Grand Cru:

(1) Could have been more flavorful.

The only word carrying direct sentiment in this
sentence is flavorful, which is positive, but the sen-
tence is a negative evaluation of the Grand Cru’s

1http://snap.stanford.edu/data/web-RateBeer.html
2http://www.clearbits.net/torrents/2076-aug-2012

taste. The trigram been more flavorful seems neg-
ative at first, but in the context of negation or in a
comparative, it can become positive again. How-
ever, note that this trigram may occur discontinu-
ously, e.g., in been less watery and more flavorful.
In order to match such occurrences, we need simple
regular expressions, e.g.,:

been.*more.*flavorful

This is exactly the kind of regular expressions we
asked experts to submit, and that we derived from
the crowdsourced annotation tasks. Note that the
sentence says nothing about the beer’s appearance,
so this feature is only relevant in TASTE, not in
APPEARANCE.

BoW and BoW+HI Our most simple baseline ap-
proach is a bag-of-words model of unigram features
(BoW). We lower-case our data, but leave in stop
words. We also introduce a semantically enriched
unigram model (BoW)+HI, where in addition to
representing what words occur in a text, we also
represent what Harvard Inquirer (HI)3 word classes
occur in it. The HI classes are used to generate
features from the crowdsourced annotation tasks,
so the semantically enriched unigram model is an
important baseline in our experiments below.

BoW+Exp In order to collect regular expressions
from experts, we set up a web interface for query-
ing held-out portions of the datasets with regular ex-
pressions that reports how occurrences of the sub-
mitted regular expressions correlate with class. We
used the Python re syntax for regular expressions
after augmenting word forms with POS and seman-
tic classes from the HI. Few of the experts made use
of the POS tags, but many regular expressions in-
cluded references to HI classes.

3http://www.wjh.harvard.edu/ inquirer/homecat.htm

1477



Regular expressions submitted by participants
were visible to other participants during the exper-
iment, and participants were allowed to work to-
gether. Participants had 15 minutes to familiarize
themselves with the syntax used in the experiments.
Each query was executed in 2-30 seconds.

Seven researchers and graduate students spent
five effective hours querying the datasets with
regular expressions. In particular, they spent three
hours on the Stack Exchange dataset, and one hour
on each of the two RateBeer datasets. One had to
leave an hour early. So, in total, we spent 20 person
hours on Stack Exchange, and seven person hours
on each of the RateBeer datasets. In the five hours,
we collected 1,156 regular expressions for the
STACKOVERFLOW dataset, and about 650 regular
expressions for each of the two RateBeer datasets.
Exp refers to these sets of regular expressions. In
our experiments below we concatenate these with
the BoW features to form BoW+Exp.

BoW+AMT For each dataset, we also had 500 held-
out examples annotated by three turkers each, using
Amazon Mechanical Turk,4 obtaining 1,500 HITs
for each dataset. The annotators were presented with
each text, a review or an answer, twice: once as run-
ning text, once word-by-word with bullets to tick off
words. The annotators were instructed to tick off
words or phrases that they found predictive of the
text’s sentiment or answer quality. They were not in-
formed about the class of the text. We chose this an-
notation task, because it is relatively easy for annota-
tors to mark spans of text with a particular attribute.
This set-up has been used in other applications, in-
cluding NER (Finin et al., 2010) and error detection
(Dahlmeier et al., 2013). The annotators were con-
strained to tick off at least three words, including
one closed class item (closed class items were col-
ored differently). Finally, we only used annotators
with a track record of providing high-quality anno-
tations in previous tasks. It was clear from the aver-
age time spent by annotators that annotating STACK-
OVERFLOW was harder than annotating the Rate-
beer datasets. The average time spent on a Rate-
beer HIT was 44s, while for STACKOVERFLOW it
was 3m:8s. The mean number of words ticked off

4www.mturk.com

BoW HI Exp AMT
STACKOVERF 0.655 0.654 0.683 0.739
TASTE 0.798 0.797 0.798 0.867
APPEARANCE 0.758 0.760 0.761 0.859

Table 2: Results using all features

was between 5.6 and 7, with more words ticked off
in STACKOVERFLOW. The maximum number of
words ticked off by an annotator was 41. We spent
$292.5 on the annotations, including a trial round.
This was supposed to match, roughly, the cost of the
experts consulted for BoW+Exp.

The features generated from the annotations were
constructed as follows: We use a sliding window of
size 3 to extract trigrams over the possibly discon-
tinuous words ticked off by the annotators. These
trigrams were converted into regular expressions by
placing Kleene stars between the words. This gives
us a manually selected subset of skip trigrams. For
each skip trigram, we add copies with one or more
words replaced by one of their HI classes.

Feature combinations This subsection introduces
some harder baselines for our experiments, consid-
ered in Experiment #2. The simplest possible way
of combining unigram features is by considering n-
gram models. An n-gram extracts features from a
sliding window (of size n) over the text. We call this
model BoW(N = n). Our BoW(N = 1) model
takes word forms as features, and there are obvi-
ously more advanced ways of automatically combin-
ing such features.

Kernel representations We experimented with ap-
plying an approximate feature map for the addi-
tive χ2-kernel. We used two sample steps, result-
ing in 4N + 1 features. See Vedaldi and Zimmer-
man (2011) for details.

Deep features We also ran denoising autoen-
coders (Pascal et al., 2008), previously applied
to a wide range of NLP tasks (Ranganath et al.,
2009; Socher et al., 2011; Chen et al., 2012), with
2N nodes in the middle layer to obtain a deep
representation of our datasets from χ2-BoW input.
The network was trained for 15 epochs. We set the
drop-out rate to 0.0 and 0.3.

Summary of feature sets The feature sets – BoW,

1478



102 103

N

60

62

64

66

68

70

72

74

A
cc
u
ra
cy

StackOverflow

χ2 −BoW
χ2 −BoW+HI
χ2 −BoW+Exp
χ2 −BoW+AMT

102 103

N

70

75

80

85

90

A
cc
u
ra
cy

Taste

χ2 −BoW
χ2 −BoW+HI
χ2 −BoW+Exp
χ2 −BoW+AMT

102 103

N

70

75

80

85

90

A
cc
u
ra
cy

Appearance

χ2 −BoW
χ2 −BoW+HI
χ2 −BoW+Exp
χ2 −BoW+AMT

Figure 1: Results selecting N features using χ2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE. The
x-axis is logarithmic scale.

102 103

N

60

62

64

66

68

70

72

74

A
cc
u
ra
cy

χ2 −BoW(N=1)
χ2 −BoW(N=2)
χ2 −kernel
χ2 −dauto
χ2 −BoW+AMT

102 103

N

70

75

80

85

90

A
cc
u
ra
cy

χ2 −BoW(N=1)
χ2 −BoW(N=2)
χ2 −kernel
χ2 −dauto
χ2 −BoW+AMT

102 103

N

70

72

74

76

78

80

82

84

A
cc
u
ra
cy

χ2 −BoW(N=1)
χ2 −BoW(N=2)
χ2 −kernel
χ2 −dauto
χ2 −BoW+AMT

Figure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, and
APPEARANCE. The x-axis is logarithmic scale.

Exp and AMT – are very different. Their character-
istics are presented in Table 1. P (1) is the class dis-
tribution, e.g., the prior probability of positive class.
n is the number of data points, m the number of
features. Finally, µx is the average density of data
points. One observation is of course that the expert
feature set Exp is much smaller than BoW and AMT,
but note also that the expert features fire about 150
times more often on average than the BoW features.
HI is only a small set of additional features.

3 Results

Experiment #1: BoW vs. Exp and AMT We present
results using all features, as well as results obtained
after selecting k features as ranked by a simple χ2

test. The results using all collected features are pre-
sented in Table 2. The error reduction on STACK-
OVERFLOW when adding crowdsourced features to
our baseline model (BoW+AMT), is 24.3%. On
TASTE, it is 34.2%. On APPEARANCE, it is 41.0%.

The BoW+AMT feature set is bigger than those of
the other models. We therefore report results using
the top-k features as ranked by a simple χ2 test.
The result curves are presented in the three plots in

Fig. 1. With +500 features, BoW+AMT outperforms
the other models by a large margin.

Experiment #2: AMT vs. more baselines The
BoW baseline uses a standard representation that,
while widely used, is usually thought of as a weak
baseline. BoW+HIT did not provide a stronger base-
line. We also show that bigram features, kernel-
based decomposition and deep features do not pro-
vide much stronger baselines either. The result
curves are presented in the three plots in Fig. 2.
BoW+AMT is still significantly better than all other
models with +500 features. Since autoencoders
are consistently worse than denoising autoencoders
(drop-out 0.3), we only plot denoising autoencoders.

4 Conclusion

We presented a new method for deriving feature
representations from crowdsourced annotation tasks
and showed how it leads to 24%-41% error reduc-
tions on answer scoring and multi-aspect sentiment
analysis problems. We saw no significant improve-
ments using features contributed by experts, kernel
representations or learned deep representations.

1479



References
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal

Daume. 2012. Besting the quiz master: Crowdsourc-
ing incremental classification games. In NAACL.

Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In ACL.

Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei
Sha. 2012. Marginalized denoising autoencoders for
domain adaptation. In ICML.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English. In Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL.

Pedro Domingos. 2012. A few useful things to know
about machine learning. In CACM.

Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In NAACL Workshop on Creating Speech
and Language Data with Amazon’s Mechanical Turk.

Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect re-
views. In ICDM.

Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-
ings. 2012. Sentiment analysis using a novel human
computation game. In Workshop on the People’s Web
Meets NLP, ACL.

Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders. In
ICML.

Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It’s not you, it’s me: detecting flirting and its
misperception in speed-dates. In NAACL.

Richard Socher, Eric Huan, Jeffrey Pennington, Andrew
Ng, and Christopher Manning. 2011. Dynamic pool-
ing and unfolding recursive autoencoders for para-
phrase detection. In NIPS.

Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and
Adam Tauman Kalai. 2011. Adaptively learning the
crowd kernel. In ICML.

Andrea Vedaldi and Andrew Zisserman. 2011. Efficient
additive kernels via explicit feature maps. In CVPR.

Kiri Wagstaff. 2012. Machine learning that matters. In
ICML.

1480


