



















































Minimum Bayes Risk based Answer Re-ranking for Question Answering


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 424–428,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Minimum Bayes Risk based Answer Re-ranking for Question Answering

Nan Duan
Natural Language Computing

Microsoft Research Asia
nanduan@microsoft.com

Abstract

This paper presents two minimum Bayes
risk (MBR) based Answer Re-ranking
(MBRAR) approaches for the question
answering (QA) task. The first approach
re-ranks single QA system’s outputs by
using a traditional MBR model, by mea-
suring correlations between answer can-
didates; while the second approach re-
ranks the combined outputs of multiple
QA systems with heterogenous answer ex-
traction components by using a mixture
model-based MBR model. Evaluation-
s are performed on factoid questions se-
lected from two different domains: Jeop-
ardy! and Web, and significant improve-
ments are achieved on all data sets.

1 Introduction

Minimum Bayes Risk (MBR) techniques have
been successfully applied to a wide range of nat-
ural language processing tasks, such as statisti-
cal machine translation (Kumar and Byrne, 2004),
automatic speech recognition (Goel and Byrne,
2000), parsing (Titov and Henderson, 2006), etc.
This work makes further exploration along this
line of research, by applying MBR technique to
question answering (QA).

The function of a typical factoid question an-
swering system is to automatically give answers to
questions in most case asking about entities, which
usually consists of three key components: ques-
tion understanding, passage retrieval, and answer
extraction. In this paper, we propose two MBR-
based Answer Re-ranking (MBRAR) approaches,
aiming to re-rank answer candidates from either
single and multiple QA systems. The first one
re-ranks answer outputs from single QA system
based on a traditional MBR model by measuring
the correlations between each answer candidates

and all the other candidates; while the second one
re-ranks the combined answer outputs from multi-
ple QA systems based on a mixture model-based
MBR model. The key contribution of this work is
that, our MBRAR approaches assume little about
QA systems and can be easily applied to QA sys-
tems with arbitrary sub-components.

The remainder of this paper is organized as fol-
lows: Section 2 gives a brief review of the QA task
and describe two types of QA systems with differ-
ent pros and cons. Section 3 presents two MBRAR
approaches that can re-rank the answer candidates
from single and multiple QA systems respectively.
The relationship between our approach and pre-
vious work is discussed in Section 4. Section 5
evaluates our methods on large scale questions s-
elected from two domains (Jeopardy! and Web)
and shows promising results. Section 6 concludes
this paper.

2 Question Answering

2.1 Overview

Formally, given an input question Q, a typical fac-
toid QA system generates answers on the basis of
the following three procedures:

(1) Question Understanding, which determines
the answer type and identifies necessory informa-
tion contained in Q, such as question focus and
lexical answer type (LAT). Such information will
be encoded and used by the following procedures.

(2) Passage Retrieval, which formulates queries
based on Q, and retrieves passages from offline
corpus or online search engines (e.g. Google and
Bing).

(3) Answer Extraction, which first extracts an-
swer candidates from retrieved passages, and then
ranks them based on specific ranking models.

424



2.2 Two Types of QA Systems
We present two different QA sysytems, which are
distinguished from three aspects: answer typing,
answer generation, and answer ranking.

The 1st QA system is denoted as Type-
Dependent QA engine (TD-QA). In answer typing
phase, TD-QA assigns the most possible answer
type T̂ to a given question Q based on:

T̂ = argmax
T

P (T |Q)

P (T |Q) is a probabilistic answer-typing mod-
el that is similar to Pinchak and Lin (2006)’s
work. In answer generation phase, TD-QA uses
a CRF-based Named Entity Recognizer to detect
all named entities contained in retrieved passages
with the type T̂ , and treat them as the answer can-
didate space H(Q):

H(Q) =
∪

k

Ak

In answer ranking phase, the decision rule de-
scribed below is used to rank answer candidate s-
pace H(Q):

Â = argmax
A∈H(Q)

P (A|T̂ , Q)

= argmax
A∈H(Q)

∑

i

λi · hi(A, T̂ , Q)

where {hi(·)} is a set of ranking features that
measure the correctness of answer candidates, and
{λi} are their corresponding feature weights.

The 2ed QA system is denoted as Type-
Independent QA engine (TI-QA). In answer typ-
ing phase, TI-QA assigns top N , instead of the
best, answer types TN (Q) for each question Q.
The probability of each type candidate is main-
tained as well. In answer generation phase, TI-
QA extracts all answer candidates from retrieved
passages based on answer types in TN (Q), by the
same NER used in TD-QA. In answer ranking
phase, TI-QA considers the probabilities of differ-
ent answer types as well:

Â = argmax
A∈H(Q)

P (A|Q)

= argmax
A∈H(Q)

∑

T∈TN (Q)
P (A|T, Q) · P (T |Q)

On one hand, TD-QA can achieve relative high
ranking precision, as using a unique answer type
greatly reduces the size of the candidate list for

ranking. However, as the answer-typing model is
far from perfect, if prediction errors happen, TD-
QA can no longer give correct answers at all.

On the other hand, TI-QA can provide higher
answer coverage, as it can extract answer candi-
dates with multiple answer types. However, more
answer candidates with different types bring more
difficulties to the answer ranking model to rank the
correct answer to the top 1 position. So the rank-
ing precision of TI-QA is not as good as TD-QA.

3 MBR-based Answering Re-ranking

3.1 MBRAR for Single QA System
MBR decoding (Bickel and Doksum, 1977) aims
to select the hypothesis that minimizes the expect-
ed loss in classification. In MBRAR, we replace
the loss function with the gain function that mea-
sure the correlation between answer candidates.
Thus, the objective of the MBRAR approach for
single QA system is to find the answer candidate
that is most supported by other candidates under
QA system’s distribution, which can be formally
written as:

Â = argmax
A∈H(Q)

∑

Ak∈H(Q)
G(A,Ak) · P (Ak|H(Q))

P (Ak|H(Q)) denotes the hypothesis distribu-
tion estimated on the search space H(Q) based on
the following log-linear formulation:

P (Ak|H(Q)) =
exp(β · P (Ak|Q))∑

A′∈H exp(β · P (A
′ |Q))

P (Ak|Q) is the posterior probability of the answer
candidate Ak based on QA system’s ranking mod-
el, β is a scaling factor which controls the distri-
bution P (·) sharp (when β > 1) or smooth (when
β < 1).

G(A, Ak) is the gain function that denotes the
degree of how Ak supports A. This function can
be further expanded as a weighted combination of
a set of correlation features as:

∑
j λj ·hj(A,Ak).

The following correlation features are used in
G(·):

• answer-level n-gram correlation feature:

hanswer(A, Ak) =
∑

ω∈A
#ω(Ak)

where ω denotes an n-gram in A, #ω(Ak)
denotes the number of times that ω occurs in
Ak.

425



• passage-level n-gram correlation feature:

hpassage(A, Ak) =
∑

ω∈PA
#ω(PAk)

where PA denotes passages from which A
are extracted. This feature measures the de-
gree of Ak supports A from the context per-
spective.

• answer-type agreement feature:

htype(A, Ak) = δ(TA, TAi)

δ(TA, TAk) denotes an indicator function that
equals to 1 when the answer types of A and
Ak are the same, and 0 otherwise.

• answer-length feature that is used to penalize
long answer candidates.

• averaged passage-length feature that is used
to penalize passages with a long averaged
length.

3.2 MBRAR for Multiple QA Systems
Aiming to apply MBRAR to the outputs from N
QA systems, we modify MBR components as fol-
lows.

First, the hypothesis space HC(Q) is built by
merging answer candidates of multiple QA sys-
tems:

HC(Q) =
∪

i

Hi(Q)

Second, the hypothesis distribution is defined
as a probability distribution over the combined
search space of N component QA systems and
computed as a weighted sum of component model
distributions:

P (A|HC(Q)) =
N∑

i=1

αi · P (A|Hi(Q))

where α1, ..., αN are coefficients with following
constraints holds1: 0 ≤ αi ≤ 1 and

∑N
i=1 αi = 1,

P (A|Hi(Q)) is the posterior probability of A esti-
mated on the ith QA system’s search space Hi(Q).

Third, the features used in the gain function G(·)
can be grouped into two categories, including:

• system-independent features, which includes
all features described in Section 3.1 for single
system based MBRAR method;

1For simplicity, the coefficients are equally set: αi =
1/N .

• system-dependent features, which measure
the correctness of answer candidates based
on information provided by multiple QA sys-
tems:

– system indicator feature hsys(A, QAi),
which equals to 1 when A is generated
by the ith system QAi, and 0 otherwise;

– system ranking feature hrank(A, QAi),
which equals to the reciprocal of the
rank position of A predicted by QAi. If
QAi fails to generate A, then it equals
to 0;

– ensemble feature hcons(A), which e-
quals to 1 when A can be generated by
all individual QA system, and 0 other-
wise.

Thus, the MBRAR for multiple QA systems can
be finally formulated as follows:

Â = argmax
A∈HC(Q)

∑

Ai∈HC(Q)
G(A, Ai) · P (Ai|HC(Q))

where the training process of the weights in the
gain function is carried out with Ranking SVM2

based on the method described in Verberne et al.
(2009).

4 Related Work

MBR decoding have been successfully applied to
many NLP tasks, e.g. machine translation, pars-
ing, speech recognition and etc. As far as we
know, this is the first work that applies MBR prin-
ciple to QA.

Yaman et al. (2009) proposed a classifica-
tion based method for QA task that jointly uses
multiple 5-W QA systems by selecting one opti-
mal QA system for each question. Comparing to
their work, our MBRAR approaches assume few
about the question types, and all QA systems con-
tribute in the re-ranking model. Tellez-Valero et
al. (2008) presented an answer validation method
that helps individual QA systems to automatical-
ly detect its own errors based on information from
multiple QA systems. Chu-Carroll et al. (2003) p-
resented a multi-level answer resolution algorithm
to merge results from the answering agents at the
question, passage, and answer levels. Grappy et al.

2We use SV MRank (Joachims, 2006) that can be found-
ed at www.cs.cornell.edu/people/tj/svm light/svm rank.html/

426



(2012) proposed to use different score combina-
tions to merge answers from different QA system-
s. Although all methods mentioned above leverage
information provided by multiple QA systems, our
work is the first time to explore the usage of MBR
principle for the QA task.

5 Experiments

5.1 Data and Metric

Questions from two different domains are used
as our evaluation data sets: the first data set in-
cludes 10,051 factoid question-answer pairs se-
lected from the Jeopardy! quiz show3; while the
second data set includes 360 celebrity-asking web
questions4 selected from a commercial search en-
gine, the answers for each question is labeled by
human annotators.

The evaluation metric Succeed@n is defined as
the number of questions whose correct answers
are successfully ranked to the top n answer can-
didates.

5.2 MBRAR for Single QA System

We first evaluate the effectiveness of our MBRAR
for single QA system. Given the N-best answer
outputs from each single QA system, together with
their ranking scores assigned by the corresponding
ranking components, we further perform MBRAR
to re-rank them and show resulting numbers on t-
wo evaluation data sets in Table 1 and 2 respec-
tively.

Both Table 1 and Table 2 show that, by lever-
aging our MBRAR method on individual QA sys-
tems, the rankings of correct answers are consis-
tently improved on both Jeopardy! and web ques-
tions.

Joepardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885

MBRAR 2,372 2,784 2,982
TI-QA 2,527 3,397 3,821

MBRAR 2,628 3,500 3,931

Table 1: Impacts of MBRAR for single QA system
on Jeopardy! questions.

We also notice TI-QA performs significantly
better than TD-QA on Jeopardy! questions, but
worse on web questions. This is due to fac-
t that when the answer type is fixed (PERSON for

3http://www.jeopardy.com/
4The answers of such questions are person names.

Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146

MBRAR 99 130 148
TI-QA 95 122 136

MBRAR 97 126 143

Table 2: Impacts of MBRAR for single QA system
on web questions.

celebrity-asking questions), TI-QA will generate
candidates with wrong answer types, which will
definitely deteriorate the ranking accuracy.

5.3 MBRAR for Multiple QA Systems

We then evaluate the effectiveness of our MBRAR
for multiple QA systems. The mixture model-
based MBRAR method described in Section 3.2
is used to rank the combined answer outputs from
TD-QA and TI-QA, with ranking results shown in
Table 3 and 4.

From Table 3 and Table 4 we can see that, com-
paring to the ranking performances of single QA
systems TD-QA and TI-QA, MBRAR using two
QA systems’ outputs shows significant improve-
ments on both Jeopardy! and web questions. Fur-
thermore, comparing to MBRAR on single QA
system, MBRAR on multiple QA systems can pro-
vide extra gains on both questions sets as well.

Jeopardy! Succeed@1 Succeed@2 Succeed@3
TD-QA 2,289 2,693 2,885
TI-QA 2,527 3,397 3,821

MBRAR 2,891 3,668 4,033

Table 3: Impacts of MBRAR for multiple QA sys-
tems on Jeopardy! questions.

Web Succeed@1 Succeed@2 Succeed@3
TD-QA 97 128 146
TI-QA 95 122 136

MBRAR 108 137 152

Table 4: Impacts of MBRAR for multiple QA sys-
tems on web questions.

6 Conclusions and Future Work

In this paper, we present two MBR-based answer
re-ranking approaches for QA. Comparing to pre-
vious methods, MBRAR provides a systematic
way to re-rank answers from either single or multi-
ple QA systems, without considering their hetero-
geneous implementations of internal components.

427



Experiments on questions from two different do-
mains show that, our proposed method can sig-
nificantly improve the ranking performances. In
future, we will add more QA systems into our M-
BRAR framework, and design more features for
the MBR gain function.

References
P. J. Bickel and K. A. Doksum. 1977. Mathematical

Statistics: Basic Ideas and Selected Topics. Holden-
Day Inc.

Jennifer Chu-Carroll, Krzysztof Czuba, John Prager,
and Abraham Ittycheriah. 2003. In Question An-
swering, Two Heads Are Better Than One. In pro-
ceeding of HLT-NAACL.

Vaibhava Goel and William Byrne. 2000. Minimum
bayes-risk automatic speech recognition, Computer
Speech and Language.

Arnaud Grappy, Brigitte Grau, and Sophie Ros-
set. 2012. Methods Combination and ML-based
Re-ranking of Multiple Hypothesis for Question-
Answering Systems, In proceeding of EACL.

Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time, In proceeding of KDD.

Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statisti-cal Machine
Translation. In proceeding of HLT-NAACL.

Christopher Pinchak and Dekang Lin. 2006. A Prob-
abilistic Answer Type Model. In proceeding of EA-
CL.

Ivan Titov and James Henderson. 2006. Bayes Risk
Minimization in Natural Language Parsing. Techni-
cal report.

Alberto Tellez-Valero, Manuel Montes-y-Gomez, Luis
Villasenor-Pineda, and Anselmo Penas. 2008. Im-
proving Question Answering by Combining Multiple
Systems via Answer Validation. In proceeding of CI-
CLing.

Suzan Verberne, Clst Ru Nijmegen, Hans Van Hal-
teren, Clst Ru Nijmegen, Daphne Theijssen, Ru Ni-
jmegen, Stephan Raaijmakers, Lou Boves, and Clst
Ru Nijmegen. 2009. Learning to rank qa data. e-
valuating machine learning techniques for ranking
answers to why-questions. In proceeding of SIGIR
workshop.

Sibel Yaman, Dilek Hakkani-Tur, Gokhan Tur, Ralph
Grishman, Mary Harper, Kathleen R. McKe-
own, Adam Meyers, Kartavya Sharma. 2009.
Classification-Based Strategies for Combining Mul-
tiple 5-W Question Answering Systems. In proceed-
ing of INTERSPEECH.

428


