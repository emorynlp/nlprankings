










































How to Drink from a Fire Hose: One Person Can Annoscribe One Million Utterances in One Month


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257–260,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

How to Drink from a Fire Hose:
One Person Can Annoscribe 693 Thousand Utterances in One Month

David Suendermann, Jackson Liscombe, Roberto Pieraccini
SpeechCycle Labs
New York, USA

{david, jackson, roberto}@speechcycle.com

Abstract

.
Transcription and semantic annotation
(annoscription) of utterances is crucial
part of speech performance analysis and
tuning of spoken dialog systems and other
natural language processing disciplines.
However, the fact that these are manual
tasks makes them expensive and slow. In
this paper, we will discuss how anno-
scription can be partially automated. We
will show that annoscription can reach a
throughput of 693 thousand utterances per
person month under certain assumptions.

1 Introduction
Ever since spoken dialog systems entered the com-
mercial market in the mid 1990s, the caller’s
speech input is subject to collection, transcription,
and often also semantic annotation. Utterance
transcriptions and annotations (annoscriptions) are
used to measure speech recognition and spoken
language understanding performance of the appli-
cation. Furthermore, they are used to improve
speech recognition and application functionality
by tuning grammars, introducing new transitions
in the call flow to cover more of the callers’ de-
mands, or changing prompt wording or applica-
tion logic to influence the speech input. Anno-
scriptions are also crucial for training statistical
language models and utterance classifiers for call
routing or other unconstrained speech input con-
texts (Gorin et al., 1997). Since very recently, sta-
tistical methods are used to replace conventional
rule-based grammars in every recognition context
of commercial spoken dialog systems (Suender-
mann et al., 2009b). This replacement is only
possible by collecting massive amounts of anno-
scribed data from all contexts of an application.
To give the reader an idea of what massive means
in this case, in (Suendermann et al., 2009b), we
used 2,184,203 utterances to build a complex call
routing system. In (Suendermann et al., 2009a),
4,293,898 utterances were used to localize an En-
glish Internet troubleshooting application to Span-
ish.

Considering that professional service providers
may charge as much as 50 US cents for annoscrib-
ing a single utterance, the usage of these amounts

of data seems prohibitive since costs for such a
project could potentially add up to several million
US dollars. Furthermore, one has to consider the
average speed of annoscription which rarely ex-
ceeds 1000 utterances per hour and person. This
means that the turn-around of a project as men-
tioned above would be several years unless teams
of many people work simultaneously. However,
the integration of the work of a large team be-
comes the more tricky the more people are in-
volved. This is especially true for the annotation
portion since it requires a thorough understand-
ing of the spoken dialog system’s domain and de-
sign and very often can only be conducted under
close supervision by the interaction designer in
charge of the project. Furthermore, there are cru-
cial issues related to intra- and inter-labeler incon-
sistency becoming more critical the more people
work on the same or similar recognition contexts
of a given project.

This paper is to show how it is possible to au-
tomate large portions of both transcription and an-

notation while meeting human performance1 stan-
dards. As an example case, we show how the pro-
posed automation techniques can increase anno-
scription speed to nearly 693 thousand utterances
per person and month.

2 Automatic Transcription

2.1 Two Fundamentals
Automatic transcription of spoken utterances may
not sound as something new to the reader. In
fact, the entire field of automatic speech recogni-
tion is about machine transcription. So, why is it
worth dedicating a full section to something well-
covered in research and industry for half a cen-
tury? The reason is the demand for achieving hu-
man performance as formulated in the introduc-
tion which, as is also well-known, cannot be satis-
fied by any of the large-vocabulary speech recog-
nizers ever developed. In order to demonstrate that
there is indeed a way to achieve human transcrip-
tion performance using automatic speech recogni-
tion, we would like to refer to two fundamental
observations on the performance of speech recog-

1In this paper, performance stands for quality or accuracy
of transcription or annotation. It does not refer to speed or
throughput.

257



nition:

(1) Speech recognition performance can be very
high for contexts of constrained vocabulary. An
example is the recognition of isolated letters in
the scope of a name spelling task as discussed
in (Waibel and Lee, 1990) that achieved a word
error rate of only 1.1%. In contrast, the word error
rate of large-vocabulary continuous speech recog-
nition can be as high as 40 to 65% on telephone
speech (Yuk and Flanagan, 1999).

(2) The positive dependence between speech
recognition performance and amount of data used
to train acoustic and language models, so far, did
not reach a saturation point even considering bil-
lions of training tokens (Och, 2006).

Both of these fundamentals can be applied to the
transcription task for utterances collected on spo-
ken dialog production systems as follows:

(1) The vocabulary of spoken dialog systems can
be rather complex. E.g., the caller utterances used
for the localization project mentioned in Section 1
distinguish more than 13,000 types. However,
the nature of commercial spoken dialog applica-
tions being mostly system-driven strongly con-
strains the vocabulary in many recognition con-
texts. E.g., when the prompt reads

You can say: recording problems, new
installation, frozen screen, or won’t turn
on

callers mostly respond things matching the pro-
posed phrases, occasionally altering the wording,
and only seldomly using completely unexpected
utterances.
(2) The continuous data feed available on high-
traffic spoken dialog systems in production pro-
cessing millions of calls per month can provide
large numbers of utterances for every possible
recognition context. Even if the context appears to
be of a simple nature, as for a yes/no question, the
continuous collection of more data will still have
an impact on the performance of a language model
built using this data.

2.2 How to Achieve Human Performance

Even though we have suggested that the recog-
nition performance in many contexts of spoken
dialog systems may be very high, we have still
not shown how our observations can be utilized to
achieve human performance as demanded in Sec-
tion 1. How would a context-dependent speech
recognizer respond when the caller says some-
thing completely unexpected such as let’s wreck a
nice beach when asked for the cell phone number?
While a human transcriber may still be able to cor-
rectly transcribe this sentence, automatic speech
recognition will certainly fail even with the largest
possible training set. The answer to this question
is that the speech recognizer should not respond at
all in this case but admit that it had trouble rec-
ognizing this utterance. Rejection of hypotheses

based on confidence scores is common practice in
many speech and language processing tasks and
is heavily used in spoken dialog systems to avoid
mis-interpretation of user inputs.

So, we now know that we can limit automatic
transcriptions to hypotheses of a minimum relia-
bility. However, how do we prove that this limited
set resembles human performance? What is actu-
ally human performance? Does the human make
errors transcribing? And, if so, how do we mea-
sure human error? What do we compare it against?

To err is human. Accordingly, there is an error
associated with manual transcription which can
only be estimated by comparing somebody’s tran-
scription with somebody else’s due to a lack of
ground truth. Preferably, one should have a good
number of people transcribe the same speech ut-
terances and than compute the average word error
rate comparing every transcription batch with ev-
ery other producing a reliable estimate of the man-
ual error inherent to the transcription task of spo-
ken dialog system utterances. In order to do so,
we compared transcriptions of 258,843 utterances
collected from a variety of applications and recog-
nition contexts partially shared by up to six tran-
scribers and found that they averaged at an inter-
transcriber word error rate of WER0 = 1.3%.

Now, for every recognition context a language
model had been trained, we performed automatic
speech recognition on held-out test sets of N =
1000 utterances producing N hypotheses and their
associated confidence scores P = {p1, . . . pN}.
Now, we determined that minimum confidence
threshold p0 for which the word error rate between
the set of hypotheses and manual reference tran-
scriptions was not statistically significantly greater
than WER0:

p0 = arg min
p∈P

WER(V (p)) ˜6> WER0; (1)

V (p) = {ν1, . . . , νK} : νk ∈ {1, . . . , N}, pνk ≥ p.

Statistical significance was achieved when the
delta resulted in a p value greater than 0.05 using
the χ2 calculus. For the number of test utterances,
1000, this point is reached when the word error
on the test set falls below WER1 = 2.2%. This
means that Equation 2.2’s ‘not statistically signifi-
cantly greater than’ sign can be replaced by a reg-
ular smaller-than sign as

WER ˜6> WER0 ⇔ WER < WER1. (2)

This essentially means that there is a chance that
the error produced by automatic transcription is
greater than that of manual transcription, however,
on the test set it could not be found to be of signifi-
cance. Requesting to lower the p value or even de-
manding that the test set performance falls below
the reported manual error can drastically lower the
automation rate and, in the latter case, is not even
reasonable—how can a machine possibly commit

258



tr
ai

n
in

g
u
tt

er
an

ce
s

. tran
scrip

tio
n

au
to

m
atio

n
rate

. training date

Figure 1: Dependency between amount of training
data and transcription automation rate

less errors than a human being as it is trained on
human transcriptions?

As a proof of concept, we ran automatic tran-
scription against the same set of utterances used
to determine the manual transcription error, and
we found that the average word error rate between
manual and automatic annotation was as low as
1.1% for all utterances whose confidence score ex-
ceeded the context-dependent threshold trained as
described above. In this initial experiment, a total
of 60,608 utterances, i.e., 23.4%, had been auto-
mated.

2.3 On Automation Rate

Formally, transcription automation rate is the ra-
tio of utterances whose confidence exeeded p0 in
Equation 2.2:

transcription automation rate =
|V (p0)|

N
(3)

where |V | refers to the cardinality of the set V ,
i.e., the number of V ’s members.

The above example’s transcription automation
rate of 23.4% does not yet sound tremendously
high, so we should look at what can be done to
increase the automation rate as much as possible.
It is predictable that the two fundamentals formu-
lated in Section 2.1 have a large impact on recog-
nition performance and, hence, the transcription
automation rate:

(1) In large-scale experiments, we were able to
show a significant (negative) correlation between
the annotation automation rate and task complex-
ity. Since this study does not fit the present paper’s
scope, we will refrain from reporting on details at
this point.
(2) As an example which influence the amount of
training data can have on the transcription automa-
tion rate, Figure 1 shows statistics drawn from
twenty runs of language model training carried out
over the course of seven months while collecting
more and more data.

3 Automatic Annotation
Semantic annotation of utterances into one of a fi-
nal set of classes is a task which may require pro-

found understanding of the application and recog-
nition context the specific utterances were col-
lected in. Examples include simple contexts such
as yes/no questions which may be easily manage-
able also by annotators unfamiliar with the ap-
plication, high-resolution open prompt contexts
with hundreds of technical and highly application-
specific classes, or number collection contexts al-
lowing for billions of classes. All these contexts
can benefit from two rules which help to signifi-
cantly reduce an annotator’s workload:

(A) Never do anything twice. This simple state-
ment means that there should be functionality built
into the annotation software or the underlying
database that

• lets the annotator process multiple utterances
with identical transcription in a single step and

• makes sure that whenever a new utterance shows
up with a transcription identical to a formerly an-
notated one, the new utterance gets assigned the
same class automatically.

Figure 2 demonstrates the impact of Rule (A) with
two typical examples. The first is a yes/no context
allowing for the additional global commands help,
hold, agent, repeat, and i don’t know. The other is
an open prompt context distinguishing 79 classes.

When using the token/type distinction, the im-
pact of Rule (A) is that annotation effort becomes
linear with the number of types to work on. While
the ratio between types and tokens in a given cor-
pus can be very small (i.e., the automation rate is
very high, e.g., 95% in the above yes/no example),
this ratio reaches saturation at some point. In the
yes/no example, there is only a gradual difference
between the automation rates for 10 thousand and
1 million utterances. Hence, at a certain point, the
effort becomes virtually linear with the number of
tokens to be processed.

(B) Predict as much as possible. Most of the
recognition contexts for which utterances are tran-
scribed and annotated use grammars to implement
speech recognition functionality. Many of these

an
n
o
ta

ti
o
n

au
to

m
at

io
n

ra
te

. training utterances

Figure 2: Dependency between number of col-
lected utterances and annotation automation rate
based on Rule (A) for two different contexts

259



Table 1: Annotation automation rates for three dif-
ferent recognition contexts based on Rule (B)

.
grammar #symptoms ann. auto. rate
modem type 43 70.3%
blue/black/snow 10 77.0%
yes/no 10 88.6%

grammars will be rule-based grammars. Even if
the grammars are statistical, most often, earlier
in time, rule-based grammars had been used in
the same recognition context. Hence, we can as-
sume that we are given rule-based grammars for
many recognition contexts of the dialog system
in question. Per definition, rule-based grammars
shall contain canonical rules expressing the rela-
tionship between expected utterances in a given
context and the semantic classes these utterances
are to be associated with. Consequently, when-
ever for an utterance recorded in the context un-
der consideration there is a rule in the grammar,
it provides the correct class for this utterance, and
it can be excluded from annotation. These rules
can be strongly extended to allow for complex pre-
fix and suffix rules, repetitions, sub-grammars &c.
making sure that the majority of utterances will
be covered by the rule-based grammars thereby
minimizing the annotation effort. Table 1 shows
three example grammars of different complex-
ity: One that collects the type of the caller’s mo-
dem, one for the identification of a TV set’s pic-
ture color (blue/black/snow), and a yes/no con-
text with global commands. Annotation automa-
tion rates for these grammars that were not specif-
ically tuned for maximizing automation but di-
rectly taken from the production dialog systems
varied between 70.3% and 88.6%.

To never ever touch a formerly annotated utter-
ance type again and to blindly rely on (mayby out-
dated or erroneous) rule-based grammars to pro-
vide baseline annotations may result in annota-
tion mistakes, possibly major ones when frequent
utterances are concerned. So, how do we make
sure that high annotation performance standards
are met?

To answer this question, the authors have de-
veloped a set of techniques called C7 taking care
of completeness, consistency, congruence, corre-
lation, confusion, coverage, and corpus size of an
annotation set (Suendermann et al., 2008). The
mentioned techniques are also useful in the fre-
quent event of changes to the number or scope of
annotation classes. This can happen e.g. due to
functional changes to the application, changes to
prompts, user behavior, or to contexts preceeding
the current annotation context. Another frequent
reason is the introduction of additional classes to
enlarge the scope of the current context2.

2In a specific context, callers may be asked whether they
want A, B, or C, but they may respond D. The introduc-
tion of a new class D which the application is able to handle

4 693 Thousand Utterances
Finally, we want to return to the initial statement
of this paper claiming that one person is able to
annoscribe 693 thousand utterances within one
month. An approximated automation rate of 80%
for transcription and 90% for annotation is possi-
ble when there is already a massive database of
annoscriptions available to be exploited for au-
tomation. These rates result in about 139 thou-
sand transcriptions and 69 thousand annotations
outstanding. At a pace of 1000 transcribed or 2000
annotated utterances per hour, the required time
would be 139 hours transcription and 35 hours an-

notation which averages at 40 hours per week3.

5 Conclusion
This paper has demonstrated how automated
annoscription of utterances collected in the
production scope of spoken dialog systems can
effectively accelerate this conventionally entirely
manual effort. When allowing for some overtime,
we have shown that a single person is able to
produce 693 thousand annoscriptions within one
month.

References

A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).

F. Och. 2006. Challenges in Machine Translation. In
Proc. of the TC-Star Workshop, Barcelona, Spain.

D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2008. C5.
In Proc. of the SLT, Goa, India.

D. Suendermann, J. Liscombe, K. Dayanidhi, and
R. Pieraccini. 2009a. Localization of Speech
Recognition in Spoken Dialog Systems: How Ma-
chine Translation Can Make Our Lives Easier. In
Proc. of the Interspeech, Brighton, UK.

D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009b. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.

A. Waibel and K.-F. Lee. 1990. Readings in Speech
Recognition. Morgan Kaufmann, San Francisco,
USA.

D. Yuk and J. Flanagan. 1999. Telephone Speech
Recognition Using Neural Networks and Hidden
Markov Models. In Proc. of the ICASSP, Phoenix,
USA.

requires the re-annotation of all utterances falling into D’s
scope.

3The original title of this paper claimed that one person
could annoscribe even one million utterances in a month.
However, after receiving multiple complaints about the un-
lawfulness of a 58-hour workweek, we had to change the title
accordingly to avoid disputes with the Department of Labor.
Furthermore, as discussed earlier, at the starting point of an
annoscription project, automation rates are much lower than
later.

260


