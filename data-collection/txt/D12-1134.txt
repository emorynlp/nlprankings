










































Identifying Event-related Bursts via Social Media Activities


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1466–1477, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Identifying Event-related Bursts via Social Media Activities

Wayne Xin Zhao†, Baihan Shu†, Jing Jiang‡, Yang Song†, Hongfei Yan†∗ and Xiaoming Li†
†School of Electronics Engineering and Computer Science, Peking University

‡School of Information Systems, Singapore Management University
{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cn

jingjiang@smu.edu.sg, lxm@pku.edu.cn

Abstract

Activities on social media increase at a dra-
matic rate. When an external event happens,
there is a surge in the degree of activities re-
lated to the event. These activities may be
temporally correlated with one another, but
they may also capture different aspects of an
event and therefore exhibit different bursty
patterns. In this paper, we propose to iden-
tify event-related bursts via social media activ-
ities. We study how to correlate multiple types
of activities to derive a global bursty pattern.
To model smoothness of one state sequence,
we propose a novel function which can cap-
ture the state context. The experiments on a
large Twitter dataset shows our methods are
very effective.

1 Introduction

Online social networks (e.g., Twitter, Facebook,
Myspace) significantly influence the way we live.
Activities on social media increase at a dramatic
rate. Millions of users engage in a diverse range
of routine activities on social media such as posting
blog messages, images, videos or status messages,
as well as interacting with items generated by oth-
ers such as forwarding messages. When an event
interesting to a certain group of individuals takes
place, there is usually a surge in the degree of ac-
tivities related to the event (e.g., a sudden explosion
of tweets). Since social media activities may indi-
cate the happenings of external events, can we lever-
age on the rich social media activities to help iden-
tify meaningful external events? This is the research
problem we study in this paper. By external events,
we refer to real-world events that happen external to
the online space.

∗Corresponding author.

2 4 6 8 100

20

40

60

80

100

120

140

Time index

 

 

all tweets
retweets
url embedded
tweetsNoise

(a) Query=“Amazon.”

2 4 6 8 100

50

100

150

Time index

 

 

all tweets
retweets
url embedded
tweetsNoise

(b) Query=“Eclipse”.

Figure 1: The amount of activities within a 10-hour
window for two queries. Three types of activities
are considered: (1) posting a tweet (upward triangle),
(2) retweet (downward triangle), (3) posting a URL-
embedded tweet (excluding retweet) (filled circle). As
explained in Table 1, both bursts above are noisy.

Mining events from text streams is usually
achieved by detecting bursty patterns (Swan and Al-
lan, 2000; Kleinberg, 2003; Fung et al., 2005). How-
ever, previous work has mostly focused on tradi-
tional text streams such as scientific publications and
news articles. There is still a lack of systematic in-
vestigations into the problem of identifying event-
related bursty patterns via social media activities.
There are at least two basic characteristics of social
media that make the problem more interesting and
challenging.

First, social media involve various types of activ-
ities taking place in real time. These activities may
be temporally correlated with one another, but they
may also capture different aspects of an event and
therefore exhibit different bursty patterns. Most of
previous methods (Swan and Allan, 2000; Klein-
berg, 2003; Fung et al., 2005) deal with a single type
of textual activities. When applied to social media,
they oversimplify the complex nature of online so-

1466



Bursty Activity Time # in Sr # in Su # in St Noisy?
Sr,St 23:00∼23:59, Nov. 23, 2009 108 5 147 Y

See Fig. 1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson “@twilight: from rob cont .
- i hope you are looking forward to eclipse as much as i am .” has been retweeted many times.

Su,St 07:00∼07:59, Jul. 25, 2009 6 122 133 Y
See Fig. 1(a) [Query=Amazon] major bursty reason: Advertisement tweets like “@fitnessjunkies amazon.com deals

: http://tinyurl.com/lakz3h.” have been posted many times.
St,Su,Sr 09:00∼9:59, Oct. 9, 2009 1562 423 2848 N

[Query=Nobel] major bursty reason: The news “Obama won Nobel Peace Prize” flood Twitter.

Table 1: Examples of bursts. The first two bursts are judged as noise since they do not correspond to any meaningful
external events. In fact, the reasons why a burst appears in social media can be quite diverse. In this paper, we only
focus on event-related bursts. St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotes
retweet.

cial activities, and therefore they may not be well
suitable to social media. Let us consider a moti-
vating example. Figure 1 shows the change of the
amount of activities of three different types over a
10-hour time window for two queries. If we consider
only the total number of tweets, we can see that for
both queries there is a burst. However, neither of the
two bursts corresponds to a real-world event. The
first burst was caused by the broadcast of an adver-
tisement from several Twitter bots, and the second
burst was caused by numerous retweets of a status
update of a movie star1. The detailed explanations
of why the two bursts are noisy are also shown in
Table 1. On the other hand, interestingly, we can see
that not all the activity streams display noisy bursty
patterns at the same time. It indicates that we may
make use of multiple views of different activities
to detect event-related bursts. The intuition is that
using multiple types of activities may help learn a
better global picture of event-related bursty patterns.
Learning may also be more resistant to noisy bursts.

Second, in social media, burst detection is chal-
lenged by irregular, unpredictable and spurious
noisy bursts. To overcome this challenge, a reason-
able assumption is that a burst corresponding to a
real event should not fluctuate too much within a
relatively short time window. To illustrate it, we
present an example in Figure 2, in which we first
use a simple threshold method to detect bursts and
then analyze the effect of local smoothness. In par-
ticular, if the amount of activities at a certain time
is above a pre-defined threshold, we set its state to
1, which indicates a bursty state. Otherwise, we set
the state to 0. Figure 2(a) shows that for the query
“Eclipse,” with a threshold of 50, the state sequence
for the time window we consider is “0000100000.”

1The reasons for these bursts were revealed by manually
checking the tweets during the corresponding periods.

1 2 3 4 5 6 7 8 9 10
0

50

100

150
Correct
0000000000

Threshold
0000100000

(a) Query=“Eclipse”.

1 2 3 4 5 6 7 8 9 10
0

500

1000

1500

2000

2500

3000
Correct
0000011111

Threshold
0000011111

(b) Query=“Nobel”.

Figure 2: Analysis of the effect of local smoothness on
threshold method. It shows two examples of threshold
methods for burst detection in a 10-hour window. The
red line denotes the bursty threshold. If the number of
activities is above the threshold in one time interval, the
state of this time interval is judge as bursty. Detailed de-
scriptions of these cases are shown in Table 1.

Although there is a burst in this sequence, its dura-
tion is very short. In fact, this is the first example
shown in Table 1, which is a noisy burst. In con-
trast, in Figure 2(b), the state sequence for the query
“Nobel” is “0000011111,” in which the longer and
smoother burst corresponds to a true event. A good
function for evaluating the smoothness of a state se-
quence should be able to discriminate these cases
and model the context of state sequences effectively.

With its unique characteristics and challenges,
there is an emergent need to deeply study the prob-
lem of event-related burst detection via social me-
dia activities. In this paper, we conduct a system-
atic investigation on this problem. We formulate
this problem as burst detection from time series of
social media activities. We develop an optimiza-
tion model to learn bursty patterns based on multiple
types of activities. We propose to detect bursts by
considering both local state smoothness and correla-
tion across multiple streams. We define a function to

1467



quantitatively measure local smoothness of one sin-
gle state sequence. We systematically evaluate three
types of activities for burst detection on a large Twit-
ter dataset and analyze different properties of these
three streams for burst detection.

2 Problem Definition

Before formally introducing our problems, we first
define some basic concepts.
Activity: An activity refers to some type of action
that users perform when they are interested in some
topic or event.
Activity Stream: An activity stream of length
N and type m is a sequence of numbers
(nm1 , n

m
2 , ..., n

m
N ), where each n

m
i denotes the

amount of activities of type m that occur during the
ith time interval.
Query: A query Q is a sequence of terms q1, ..., q|Q|
which can represent the information needs of users.
For example, an example query related to President
Obama is “barack obama.”
Event-related Burst: Given a query Q, an event-
related burst is defined as a period [ts, te] in which
some event related with Q takes place, where ts and
te are the start timestamp and end timestamp of the
event period respectively. During the event period
the amount of activities is significantly higher than
average.

Based on these definitions, our task is to try to
identify event-related bursts via multiple social me-
dia activity streams.

3 Identifying Event-related Bursts from
Social Media

In this section, we discuss how to identify event-
related bursts via social media activities. For-
mally, given a query Q, we first build M ac-
tivity streams related with Q on T timestamps:
{(nm1 , ..., nmT )}Mm=1. The definition of activity in our
methods is very general; it includes various types of
social media activities, including textual and non-
textual activities, e.g., a click on a shared photo and
a link formation between two users.

Given the input, we try to infer a state sequence
over these T timestamps: z = (z1, ..., zT ), where
zi is 1 or 0. 1 indicates a time point within a burst
while 0 indicates a non-bursty time point.

3.1 Modeling a Single Activity Stream

3.1.1 Generation function
In probability theory and statistics, the Poisson

distribution2 is a discrete probability distribution
that can measure the probability of a given number
of “activities” occurring in a fixed time interval. We
use the Poisson distribution to study the probability
of observing the number of social media activities,
and we treat one hour as one time interval in this
paper.
Homogeneous Poisson Distribution The genera-
tive probability of the ith number in one activity
stream of type m is defined as f(nmi , i, z

m
i ) =

(λzm
i

)n
m
i exp(−λzm

i
)

nmi !
, where λ0 is the (normal) expec-

tation of the number of activities in one time inter-
val. If one state is bursty, it would emit activities
with a faster rate and result in a larger expectation
λ1. We can set λ1 = λ0 × ρ, where ρ > 1.
Heterogeneous Poisson Distribution The two-state
machine in (Kleinberg, 2003) used two global refer-
ences for all the time intervals: one for bursty and
the other for non-bursty. In our experiments, we ob-
serve temporal patterns of user behaviors, i.e., activ-
ities in some hours are significantly more than those
in the others. Instead of using fixed global rates λ0
and λ1, we try to model temporal patterns of user
behaviors by parameterizing λ(·) with the time in-
dex. By following (Ihler et al., 2006), we use a
set of hour-specific rates {λ1,h}24h=1 and {λ0,h}24h=1.3
Given a time index h, we set λ0,h to be the expecta-
tion of the number of activities in hth time interval
every day, then we have λ1,h = λ0,h × ρ. In this
paper, ρ is empirally set as 1.5.

3.1.2 Smoothness of a State Sequence
For burst detection, the major aim is to identify

steady and meaningful bursts and to discard tran-
sient and spurious bursts. Given a state sequence
z1z2...zT , to quantitatively measure the smoothness
and compactness of it, we introduce some measures.

One simple method is to count the number of
change in the state sequence. Formally, we use the
following formula:

g1(z) = T −
T−1∑
i=1

I(zi 6= zi+1), (1)

2http://en.wikipedia.org/wiki/Poisson distribution
3We can also make the rates both day-specific and hour-

specific, i.e., {λ(·),d,h}h∈{1,...,24},d∈{1,...,7}.

1468



where T is length of the state sequence and I(·)
is an indicator function which returns 1 only if the
statement is true. Let us take the state sequence
“0000100000” (shown in Figure 2(a)) as an example
to see how g1 works. State changes 0pos=4 → 1pos=5
and 1pos=5 → 0pos=6 each incur a cost of 1, there-
fore g1(0000100000) = 10 − 2 = 8. Similarly, we
can get g1(0000000000) = 10. There is a cost dif-
ference between these two sequences, i.e., ∆g1 = 2.
Kleinberg (2003) uses state transition probabilities
to model the smoothness of state sequences. With
simple derivations, we can show that Kleinberg’s
model essentially also uses a cost function that is
linear in terms of the number of state changes in a
sequence, and therefore similar to g1.

In social media, very short noisy bursts like
“0000100000” are very frequent. To discard such
noises, we may multiply g1 by a big cost factor to
punish short-term fluctuations. However, it is not
sensitive to the state context4 and may affect the
detection of meaningful bursts. For example, state
change 0pos=4 → 1pos=5 in “0000111100” would
receive the same cost as that of 0pos=4 → 1pos=5
in “0000100000” although the later is more like a
noise.

To better measure the smoothness of a state se-
quence , we propose a novel context-sensitive func-
tion, which sums the square of the length of the max-
imum subsequences in which all states are the same.
Formally, we have

g2(z1, z2, ..., zT ) =
∑

si<ei

(ei − si + 1)2, (2)

where si and ei are the start index and end in-
dex of the ith subsequence respectively. To define
“maximum”, we have the constraints zsi = zsi+1 =
... = zei , zsi−1 6= zsi , zei 6= zei+1. For example,
g2(0000000000)= 102 = 100, g2(0000100000)=
42 + 12 + 52 = 42, we can see that ∆g2 =
100 − 42 = 58, which is significantly larger than
∆g1(= 2). g2 rewards the continunity of state se-
quences while punish the fluctuating changes, and it
is context-sensitive. State change 0pos=4 → 1pos=5
in “0000111100” receives a cost of 4,5 which is

4Here context refers to the window of hidden state se-
quences.

5Indeed, g2 is not designed for a single state change but for
the overall smoothness patterns, so we choose a referring se-
quence generated by making the corresponding state negative to
compute the cost, i.e., |g2(0000011110)−g2(0000001110)| =
4.

much smaller than that of 0pos=4 → 1pos=5 in
“0000100000”. g2 is also sensitive to the po-
sition of state changes, e.g., g2(0000100000) 6=
g2(0100000000).

3.2 Burst Detection from a Single Activity
Stream

Given an activity stream (nm1 , ..., n
m
T ), we would

like to infer a state sequence over these T times-
tamps, i.e., to find out the most possible state se-
quence z = (zm1 , ..., z

m
T ) based on the data, where

zmi = 1 or 0. We formulate this problem as
an optimization problem. The cost of a state se-
quence includes two parts: generation of activities
and smoothness of the state sequence. The objective
function is to find a state sequence which incurs the
minimum cost. Formally, we define the total cost
function as

Cost(z) = −
T∑

i=1

log f(nmi , i, z
m
i )︸ ︷︷ ︸

generating cost

+
(
− Φ(zm1 , ..., zmT )× γ1

)
︸ ︷︷ ︸

smoothness cost

,

(3)
where γ1 > 0 is a scaling factor which balance these
two parts. Φ(·) function is the smoothness function,
and we can set it as either g1(·) or g2(·).

To seek the optimal state sequence, we can min-
imize Equation 3. However, exact inference is hard
due to the exponential search space. Instead of ex-
amining the smoothness of the whole state sequence,
we propose to measure the smoothness of all the L-
length subsequences, so called “local smoothness”.
The assumption is that the states in a relatively short
time window should not change too much. The new
objective function is defined as

Cost(z) = −
T∑

i=1

log f(nmi , i, z
m
i ) (4)

−
(∑

i≥L
Φ(zmi , ..., z

m
i+L−1)

)
× γ1.

The objective function in Equation 4 can be
solved efficiently by a dynamic programming algo-
rithm shown in Algorithm 1. The time complexity
of this algorithm is O(T · 2L). Note that the meth-
ods we present in Equation 4 and Algorithm 1 are
quite general. They are independent of the concrete
forms of f(·) and Φ(·), which leaves room for flexi-
ble adaptation or extension in specific tasks. In pre-
vious methods (Kleinberg, 2003), L is often fixed as

1469



2. Indeed, as shown in Figure 2, in some cases, we
may need a longer window to infer the global pat-
terns. In our model, L can be tuned based on real
datasets. We can seek a trade-off between efficiency
and length of context windows.

Algorithm 1: Dynamic Programming for Equation 4.
d[i][s][zi...zi−L+1] denotes the minimum cost of the first1
i timestamps with the state subsequence: zi...zi−L+1 and
zi = s;
set d[0][·][·] = 0;2
set c1[i] = log f(nmi , i, z

m
i );3

set c2[i] = Φ(zi, ..., zi−L+1);4
b, b′: previous and current state window are represented as5
L-bit binary numbers;

for i = 1 to T do6
for s = 0 to 1 do7

for b = 0 to 2L − 1 do8
b′ = (b << 1|s)&(1 << L− 1);9
d[i][s][b′]←−10
min(d[i][s][b′], d[i−1][s][b]+c1[i]+c2[i]);

end11
end12

end13

3.3 Correlating Multiple Activity Streams

In this section, we discuss how to correlate multi-
ple activity streams to learn a global bursty patterns.
The hidden state sequences corresponding to these
activity streams are not fully independent. An ex-
ternal event may intricate surges in multiple activity
streams simultaneously.

We propose to correlate multiple activity streams
in an optimization model. The idea is that activ-
ity streams related with one query might be depen-
dent, i.e., the states of multiple activity streams on
the same timestamp tend to be the same6; if not,
it would incur a cost. To implement this idea, we
develop an optimization model. For convenience,
we call the states of each activity stream as “local
states” while the overall states learnt from multiple
activity streams as “global states”.

The idea is that although various activity streams
are different in the scale of frequencies, they tend to
share similar trend patterns. We incorporate the cor-
relation between local states on the same timestamp.

6In our experiments, we compute the cross correlation be-
tween different streams with a lag factor δ, we find the cross
correlation achieves maximum consistantly when δ = 0.

Formally, we have

Cost(Z) =

M∑
m=1

{
−

T∑
i=1

log f(nmi , i, z
m
i )

−
∑
i≥L

Φ(zmi , ..., z
m
i+L−1)× γ1

}

+

T∑
i=1

∑
m1,m2

I(zm1i 6= z
m2
i )× γ2, (5)

where I(·) is indicator function, and γ2 is the cost
when a pair of states are different across multiple
streams on the same timestamp.

The objective function in Equation 5 can be
solved by a dynamic programming algorithm pre-
sented in Algorithm 2. The time complexity of this
algorithm is O(T · 2M ·L+M ). Generally, L can be
set as one small value, e.g., L =2 to 6, and we can
select just a few representative activity streams, i.e.,
M =2 to 6. In this case, the algorithm can be effi-
cient.

Algorithm 2: Dynamic Programming for Equation 5.
d[i][z1i ...z

1
i−L+1; ...; z

M
i ...z

M
i−L+1] denotes the minimum1

cost of the first i timestamps with the local state
subsequence zmi ...z

m
i−L+1 in the mth stream;

set d[0][...] = 0;2
bl, bl

′
: previous and current state windows represented as3

M · L-bit binary numbers;
c[i, bl, bl

′
] denotes all the cost in the tth timestamp;4

for i = 1 to T do5
for bl = 0 to 2M·L − 1 do6

deriving current local state sequences bl
′

from bl;7
d[i][b′l]←−8
min(d[i][bl

′
], d[i− 1][bl] + c[i, bl, bl

′
]);

end9
end10

Given M types of activity streams, we can get
M (local) state sequences {(zm1 , ..., zmT )}Mm=1. The
next question is how to learn a global state sequence
(zG1 , ..., z

G
T ) based on local state sequences. Here we

give a few options:
CONJUNCT: we set a global state zi as bursty if

all local states are bursty, i.e., zGi = ∩Mm=1zmi .
DISJUNCT: we set a global state zi as bursty if

one of the local states is bursty, i.e., zGi = ∪Mm=1zmi .
BELIEF: we set a global state zi as the most con-

fident local state, i.e., zGi = argmaxmbelief(z
m
i ).

The belief(·) function can be defined as the ratio be-
tween generating costs from states zmi and 1 − zmi :
belief(zmi ) =

f(nmi ,i,z
m
i )

f(nmi ,i,1−zmi )
.

1470



Table 2: Basic statistics of our golden test collection.
# of queries 17

Aver. # of event-related bursts per query 19
Min. bursty interval 3 hours
Max. bursty interval 163 hours
Aver. bursty interval 17.8 hours

L2G: we treat the states of one local stream as the
global states.

4 Experiments

4.1 Construction of Test Collection
We test our algorithms on a large Twitter dataset,
which contains about 200 million tweets and ranges
from July, 2009 to December 2009. We manually
constructed a list of 17 queries that have high vol-
umes of relevant tweets during this period. These
queries have a very broad coverage of topics. Exam-
ple queries are “Barack Obama”, “Apple”, “Earth-
quake”, “F1” and “Nobel Prize”. For each query, we
invite two senior graduate students to manually iden-
tify their golden bursty intervals, and each bursty in-
terval is represented as a pair of timestamps in terms
of hours. Specifically, to generate the golden stan-
dard, given a query, the judges first manually gen-
erate a candidate list of external events7; then for
each event, they look into the tweets within the cor-
responding period and check whether there is a surge
on the frequency of tweets. If so, the judges fur-
ther determine the start timepoint and end timepoint
of it. If there is a conflict, a third judge will make
the final decision. We used Cohen’s kappa coeffi-
cient to measure the agreement of between the first
two judges, which turned out to be 0.67, indicating a
good level of agreement8. We present basic statistics
of the test collection in Table 2.

4.2 Evaluation Metrics
Before introducing our evaluation metrics, we first
define the Bursty Interval Overlap Ratio (BIOR)

BIOR(f,X ) =
∑

f ′∈X ∆l(f, f
′)

L(f)
,

f is a bursty interval, ∆l(f, f ′) is the length of
overlap between f ′ and f , L(f) is the length of

7We refer to some gold news resources, e.g., Google News
and Yahoo! News.

8http://en.wikipedia.org/wiki/Cohen’s kappa

Figure 3: Examples to illustrate BIOR. X0, X1
and X2 are three sets of bursty intervals. X0
and X2 consist of one interval, and X1 consists of
two intervals. BIOR(f,X0)=1, BIOR(f,X1)=0.5 and
BIOR(f,X2)=0.5.

bursty period of f . X is a set of bursty intervals,
BIOR measures the proportion of the timestamps in
f which are covered by one of bursty intervals in
X . We use BIOR to measure partial match of inter-
vals, because a system may not return all the exact
bursty intervals9. We show some examples of BIOR
in Figure 3.

We use modified Precision, Recall and F as ba-
sic measures. Given one query, P, R and F can be
defined as follows

R =

∑
f∈B I

(
1

|Mf |BIOR(f,M) > 0.5

)
|B|

,

P =
1
|M|

∑
f ′∈M

(BIOR(f ′,B)),

F =
2× P ×R

P + R
,

where M is the set of bursty intervals identified
by one candidate method, B is the set of bursty in-
tervals in golden standards, and Mf is the set of in-
tervals which overlap with f in M. We incorporate
the factor 1|Mf | in Recall to penalize the incontin-
uous coverage of the golden interval, and we also
require that the overlap ratio with penalized factor
is higher than a threshold of 0.5. Given two sets of
bursty intervals which have the same value of BIOR,
we prefer the one with fewer intervals. In Figure 3,
we can easily derive X1 and X2 have the same value

9A simple evaluation method is that we label each one hour
time slot as being part of a burst or not and compare with the
gold standard. However, in our experiments, we find that some
methods tend to break one meaningful burst into small parts and
easier to be affected by small fluctuations although they may
have a good coverage of bursty points. This is why we adopt a
different evaluation approach.

1471



Table 3: Average cross-correlation between different
streams.

St Sr Su
St 1 0.830235 0.851514
Sr 0.830235 1 0.59905
Su 0.851514 0.59905 1

of BIOR, when computing Recall, we prefer X2 to
X1 since X2 consists of only one complete inter-
val while X1 consists of two inconsecutive intervals.
I(·) is an indicator function which returns 1 only if
the statement if true. In our experiments, we use the
average of R, P and F over all test queries.

4.3 Experiment Setup
Selecting activity streams

We consider three types of activity streams in
Twitter: 1) posting a tweet, denoted as St; 2) for-
warding a tweet (retweet), denoted as Sr; 3) post-
ing a URL-embedded tweet, denoted as Su. It is
natural to test the performance of St in discover-
ing bursty patterns, while Su and Sr measure the
influence of external events on users in Twitter in
two different aspects. Sr: An important convention
in Twitter is the “retweeting” mechanism, through
which users can actively spread the news or related
information; Su: Another characteristic of Twitter is
that the length of tweets is limited to 140 characters,
which constrains the capacity of information. Users
often embed a URL link in the tweets to help others
know more about the corresponding information.

We compute the average cross correlation be-
tween different activity streams for these 17 queries
in our test collection, and we summarize the results
in Table 3. We can see that both Sr and Su have a
high correlation with St, and Sr has a relatively low
correlation with Su. 10

Methods for comparisons
S(·): using Equation 4 and considers a single ac-

tivity stream, namely St, Su and Sr.
MBurst(·): using Equation 5 and considers mul-

tiple activity streams.
To compare our methods with previous methods,

we adopt the following baselines:
StateMachine: This is the method proposed

in (Kleinberg, 2003). We use heterogeneous Poisson
10We also consider the frequencies of unique users by hours,

however, we find it has a extremely high correlation coefficient
with St, about 0.99, so we do not incorporate it.

function as generating functions instead of binomial
function Cnk because sometimes it is difficult to get
the exact total number n in social media.

Threshold: If we find that the count in one time
interval is higher than a predefined threshold, it is
treated as a burst. The threshold is set as 1.5 times
of the average number.

PeakFinding: This is the method proposed
in (Marcus et al., 2011), which aims to automatically
discover peaks from tweets.

Binomial: This is the method proposed in (Fung et
al., 2007a), which uses a cumulative binomial distri-
bution with a base probability estimated by remov-
ing abnormal frequencies.

As for multiple-stream burst detection, to the best
of our knowledge, the only existing work is pro-
posed by (Yao et al., 2010), which is supervised and
requires a considerable amount of training time, so
we do not compare our work with it. We compare
our method with the following heuristic baselines:

SimpleConjunct: we first find the optimal state se-
quences for each single activity stream. We then de-
rive a global state sequence by taking the conjunc-
tion of all local states.

SimpleDisjunct: we first find the optimal state se-
quences for each single activity stream, and then we
derive a global state sequence by take the disjunction
of all local states.

Another possible baseline is that we first merge
all the activities, then apply the single-stream algo-
rithm. However, in our data set, we find that the
number of activities in St is significantly larger than
that of the two types. St dominantly determines the
final performance, so we do not incorporate it here
as a comparison.

4.4 Experimental Results

Preliminary results on a single stream
We first examine the performance of our proposed

method on a single stream. Note that, our method
in Equation 4 has two merits: 1) the length of lo-
cal window can be tuned on different datasets; 2) a
novel state smoothness function is adopted.

We set the Φ function in Equation 4 respectively
as g1 and g2, and apply our proposed methods to
three streams (St,Sr,Su) mentioned above. Note
that, when L = 2 and Φ = g1, our method becomes
the algorithm in (Kleinberg, 2003). We tune the pa-
rameter γ1 in Equation 4 from 2 to 20 with a step of
2. We record the best F performance and compute

1472



the corresponding standard deviation. In Table 5, we
can observe that 1) streams St and Sr perform better
than Su; 2) the length of local window significantly
affects the performance; 3) g2 is much better than g1
in our proposed burst detection algorithm; 4) gen-
erally speaking, a longer window size (L = 3, 4)
performs better than the most common used size 2
in (Kleinberg, 2003).

We can see that our proposed method is more ef-
fective than the other baselines. The major reason is
that none of these methods consider state smooth-
ness in a systematic way. In our preliminary ex-
periments, we find that these baselines usually out-
put a lot of bursts, most of which are broken mean-
ingful bursts. To overcome this, baseline method
StateMachine (g1 + L = 2) requires larger ρ and
γ1, which may discard relatively small meaningful
bursts; while our proposed single stream method
(g2 + L = 3, 4) tends to identify steady and con-
secutive bursts through the help of longer context
window and context sensitive smoothness function
g2, it is more suitable to be applied to social media
for burst detection.

Compared with the other baselines, (Kleinberg,
2003) is still one good and robust baseline since it
models the state smoothness partially. These prelim-
inary findings indicate that state smoothness is very
important for burst detection, and the length of state
context window will affect the performance signifi-
cantly.

To get a deep analysis of the performance of dif-
ferent streams, we set up three classes, and each
class corresponds to a single stream. Since for each
query, we can obtain multiple results in different ac-
tivity streams, we further categorize the 17 anno-
tated queries to the stream which leads to the opti-
mal performance on that query. Interestingly, we can
see: 1) the url stream gives better performance on
queries about big companies because users in Twit-
ter usually talk about the release of new products
or important evolutionary news via url-embedded
tweets; 2) the retweet stream gives better perfor-
mance on queries which correspond to unexpected
or significant events, e.g., diasters. It is consistent
with our intuitions that users in Twitter do actively
spread such information. Combining previous anal-
ysis of Table 5, overall we find the retweet stream is
more capable to identify bursts which correspond to
significant events.

Table 4: Categorization of 17 queries according to the
optimal performance.

Streams Queries
url Apple,Microsoft,Nokia, climate

retweet bomb,crash,earthquake,typhoon,
F1,Google,Olympics

all tweet Amazon, eclipse, Lakers,
NASA, Nobel Prize, Barack Obama

Table 5: Performance (average F) on a single stream.
“??” indicates that the improvement our proposed single-
stream methodg2,L=4 over all the other baselines is ac-
cepted at the confidence level of 0.95, i.e., StateMachine,
PeakingFinding, Binomial and Threshold.

Φ L St Sr Su
4 0.545/0.015 0.543/0.037 0.451/0.036

g2 3 0.536/0.013 0.549??/0.019 0.464/0.025
2 0.468/0.055 0.542/0.071 0.455/0.045
4 0.513/0.059 0.546/0.058 0.465/0.047

g1 3 0.469/0.055 0.542/0.071 0.455/0.045
2 0.396/0.043 0.489/0.074 0.374/0.035

StateMachine 0.396 0.489 0.374
PeakFinding 0.410 0.356 0.302

Binomial 0.315 0.420 0.341
Threshold 0.195 0.181 0.175

Preliminary results on multiple streams
After examining the basic results on a single

stream, we continue to evaluate the performance of
our proposed models on multiple activity streams.
For MBurst in Equation 5, we have three parame-
ters to set, namely L, γ1 and γ2. We do a grid search
for both γ1 and γ2 from 1 to 12 with a step of 1, and
we also examine the performance when L = 2, 3, 4.
We can see that MBurst has four candidate meth-
ods to derive global states from local states; for L2G,
we use the states of St as the final states, and we em-
pirically find that it performs best compared with the
other two streams in L2G.

Recall that our proposed single-stream method
is better than all the other single-stream baselines,
so here single-best denotes our method in Equa-
tion 4 (Φ = g2, L = 4) on Sr. For SimpleConjunct
and SimpleDisjunct, we first find the optimal state
sequences for each single activity stream using our
proposed method in Equation 4 (Φ = g2, L = 4),
and then we derive a global state sequence by take
the conjunction or disjunction of all local states re-
spectively.

Besides the best performance, we further compute
the average of the top 10 results of each method
by tuning parameters to check the average perfor-

1473



Table 6: Performance (average F) on multiple streams.
“?” indicates that the improvement our proposed
multiple-stream method over our proposed single-stream
method at the confidence level of 0.9 in terms of average
performance.

Methods best average
single-best (g2 + Sr) 0.549 0.526

SimpleConjunct 0.548 -
SimpleDisjunct 0.465 -

MBurst+CONJUNCTr,t,u 0.555 0.548
MBurst+DISJUNCTr,t,u 0.576 0.570?

MBurst+BELIEFr,t,u 0.568 0.561
MBurst+L2Gr,t,u(t) 0.574 0.567
MBurst+L2Gr,t,u(r) 0.560 0.558

mance. The average performance can show the sta-
bility of models in some degree. If one model out-
puts the maximum in a very limited set of parame-
ters, it may not work well in real data, especially in
social media.

In Table 6, we can see MBurst+DISJUNCTr,t,u
gives the best performance. MBurst performs
consistently better than single-best which is a very
strong single-stream method, especially for average
performance. MBurst+DISJUNCTr,t,u has an im-
provement of average performance over single-best
by 8.4%. And simply combining three different
streams may hit results (SimpleConjunct and Sim-
pleDisjunct). It indicates that MBurst is more sta-
ble and shows a higher performance.

For different methods to derive global bursty pat-
terns, we can see that MBurst+DISJUNCT per-
forms best while MBurst+CONJUNCT performs
worst. Interestingly, however, SimpleConjunct is
better than SimpleDisjunct, the major reason is that
MBurst performs a local-state correlation of mul-
tiple activity streams to correct possible noisy fluc-
tuations from single streams before the conjunction
or disjunction of local states. After such correlation,
the performance of each activity stream should im-
prove. To see this, we present the optimal results of a
single stream without/with local-state correlation in
Table 7. Local-state correlation significantly boosts
the performance of a single stream. Indeed, we find
that the step of local-state correlation is more impor-
tant for our multiple stream algorithm than the step
of how to derive global states based on local states.

We test our MBurst algorithm with the setting:
T = 4416, L = 4 and M = 3, and for all the test

Table 7: Comparison between the optimal results of a
single stream with/without local-state correlation.

all retweet retweet url
without 0.536 0.549 0.464

with 0.574 0.560 0.547

2 4 6 8 10 12
0.54

0.55

0.56

0.57

0.58

0.59

0.6

γ
1

A
ve

ra
ge

 F

 

 

MBurst+or
single−best

(a) γ2 = 4, varying γ1.

2 4 6 8 10 12
0.54

0.55

0.56

0.57

0.58

0.59

0.6

γ
2

A
ve

ra
ge

 F

 

 

MBurst+or
single−best

(b) γ1 = 11, varying γ2.

Figure 4: Parameter sensitivity of MBurst + DIS-
JUNCT.

queries, our algorithm can respond in 2 seconds 11,
which is efficient to be deployed in social media.

Parameter sensitivity
We have shown the performance of different pa-

rameter settings for single stream algorithm in Ta-
ble 5. Next, we check parameter sensitivity in
MBurst. In our experiments, we find a longer lo-
cal window (L = 3, 4) is better than L = 2, so
we first set L = 4, then we select parameter set-
tings of γ2 = 4 and γ1 = 11, which give best per-
formance for MBurst+DISJUNCT. We vary one
with the other fixed to see how one single parame-
ter affects the performance. The results are shown in
Figure 4, and we can see MBurst+DISJUNCT is
consistently better than single-best.

5 Related Work

Our work is related to burst detection from text
streams. Pioneered by the automaton model pro-
posed in (Kleinberg, 2003), many techniques have
been proposed for burst detection such as the χ2-
test based method (Swan and Allan, 2000), the
parameter-free method (Fung et al., 2005) and mov-
ing average method (Vlachos et al., 2004). Our work
is related to the applications of these burst detection
algorithms for event detection (He et al., 2007; Fung
et al., 2007b; Shan et al., 2012; Zhao et al., 2012).

11All experiments are tested in a Mac PC, 2.4GHz Intel Core
2 Duo.

1474



Some recent work try to identify hot trends (Math-
ioudakis and Koudas, 2010; Zubiaga et al., 2011;
Budak et al., 2011; Naaman et al., 2011) or make
use of the burstiness (Sakaki et al., 2010; Aramki
et al., 2011; Marcus et al., 2011) in social media.
However, few of these methods consider modeling
the local smoothness of one state sequence in a sys-
tematic way and often use a fixed window length of
2.

Little work considers making use of different
types of social media activities for burst detection.
(Yao et al., 2010; Kotov et al., 2011; Wang et al.,
2007; Wang et al., 2009) conducted some prelim-
inary studies of mining correlated bursty patterns
from multiple sources. However, they either highly
relies on high-quality training datasets or require a
considerable amount of training time. Online social
activities are dynamic, with a large number of new
items generated continuously. In such a dynamic
setting, burst detection algorithms should effectively
collect evidence, efficiently adjust prediction models
and respond to the users as social media activities
evolve. Therefore it is not suitable to deploy such
algorithms in social media.

Our work is also similar to studies which aim
to mine and leverage knowledge from social me-
dia (Mathioudakis et al., 2010; Ruiz et al., 2012;
Morales et al., 2012). We share the common point
with these studies that we try to utilize the under-
lying rich knowledge in social media, while our fo-
cus of this work is quite different from theirs, i.e., to
identify event-related bursts.

Another line of related research is Twitter related
studies (Kwak et al., 2010; Sakaki et al., 2010). Our
proposed methods can provide event-related bursts
for downstream applications.

6 Conclusion

In this paper, we propose to identify event-related
bursts via social media activities. We propose one
optimization model to correlate multiple activity
streams to learn the bursty patterns. To better mea-
sure local smoothness of the state sequence, we pro-
pose a novel state cost function. We test our meth-
ods in a large Twitter dataset. The experiment re-
sults show that our methods are both effective and
efficient. Our work can provide a preliminary un-
derstanding of the correlation between the happen-
ings of events and the degree of online social media
activities.

Finally, we present a few promising directions
which may potentially improve or enrich current
work.

1) Variable-length context. In this paper, L is a
pre-determined parameter which controls the size of
context window. It cannot be modified when the al-
gorithm runs. A large L will significantly increases
the algorithm complexity, and we may not need a
large L for all the states in a Markov chain. This
problem can be addressed by using the variable-
length hidden Markov model (Wang et al., 2006),
which is able to learn the “minimum” context length
for accurately determining each state.

2) Incorporation of more useful features. Our
current model mainly considers temporal variations
of streaming data and searches the surge patterns ex-
isting in it. In some cases, simple frequency infor-
mation may not be capable to identify all the mean-
ingful bursts. It can be potentially useful to leverage
up more features to help filter out noisy bursts, e.g.,
semantic information (Zhao et al., 2010).

3) Modeling multi-modality data. We have ex-
amined our multi-stream algorithm by using three
different activity streams. These streams are textual-
based. It will be interesting to check our algorithm in
multi-modality data streams. E.g., in Facebook, we
may collect a stream consisting of the daily frequen-
cies of photo sharing and another stream consisting
of the daily frequencies of text status updates.

4) Evaluation of the identified bursts. In most
of previous work, they seldom construct a gold stan-
dard for quantitative test, instead they qualitatively
evaluate their methods. In our work, we invite hu-
man judges to generate the gold standard. It is time-
consuming, and the bias from human judges cannot
be completely eliminated although more judges can
be invited. A possible evaluation method is to exam-
ine the identified bursts in downstream applications,
e.g., event detection.

Acknowledgement

This work is partially supported by NSFC Grant
61073082, 60933004 and 70903008. Xin Zhao is
supported by Google PhD Fellowship (China). We
thank the insightful comments from Junjie Yao and
the anonymous reviewers.

1475



References
Eiji Aramki, Sachiko Maskawa, and Mizuki Morita.

2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568–1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.

Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.
2011. Structural trend analysis for online social net-
works. Proc. VLDB Endow., 4, July.

Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB.

Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007a. Time-dependent event hierar-
chy construction. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ’07.

Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, and
Philip S. Yu. 2007b. Time-dependent event hierarchy
construction. In SIGKDD.

Qi He, Kuiyu Chang, and Ee-Peng Lim. 2007. Analyz-
ing feature trajectories for event detection. In SIGIR.

Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD, pages 207–216, New
York, NY, USA. ACM.

J. Kleinberg. 2003. Bursty and hierarchical structure in
streams. Data Mining and Knowledge Discovery.

Alexander Kotov, ChengXiang Zhai, and Richard Sproat.
2011. Mining named entities with temporally corre-
lated bursts from multilingual web news streams. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM, pages
237–246.

Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a
news media? In WWW ’10: Proceedings of the 19th
international conference on World wide web, pages
591–600.

Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: aggregating and visualizing
microblogs for event exploration. In Proceedings of
the 2011 annual conference on Human factors in com-
puting systems, CHI ’11.

Michael Mathioudakis and Nick Koudas. 2010. Twit-
termonitor: trend detection over the twitter stream.
In Proceedings of the 2010 international conference
on Management of data, SIGMOD ’10, pages 1155–
1158.

Michael Mathioudakis, Nick Koudas, and Peter Marbach.
2010. Early online identification of attention gather-
ing items in social media. In Proceedings of the third
ACM international conference on Web search and data
mining, WSDM ’10, pages 301–310, New York, NY,
USA. ACM.

Gianmarco De Francisci Morales, Aristides Gionis, and
Claudio Lucchese. 2012. From chatter to headlines:
harnessing the real-time web for personalized news
recommendation. In WSDM, pages 153–162.

Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twitter.
JASIST, 62(5):902–918.

Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-
tides Gionis, and Alejandro Jaimes. 2012. Correlat-
ing financial time series with micro-blogging activity.
pages 513–522.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. WWW, pages 851–860,
New York, NY, USA. ACM.

Dongdong Shan, Wayne Xin Zhao, Rishan Chen, Shu
Baihan, Hongfei Yan, and Xiaoming Li. 2012.
Eventsearch: A system for event discovery and re-
trieval on multi-type historical data. In KDD’12, De-
mostration.

Russell Swan and James Allan. 2000. Automatic gener-
ation of overview timelines. In SIGIR.

Michail Vlachos, Christopher Meek, Zografoula Vagena,
and Dimitrios Gunopulos. 2004. Identifying similari-
ties, periodicities and bursts for online search queries.
In SIGMOD.

Yi Wang, Lizhu Zhou, Jianhua Feng, Jianyong Wang, and
Zhi-Qiang Liu. 2006. Mining complex time-series
data by learning markovian models. In Proceedings
of the Sixth International Conference on Data Min-
ing, ICDM, pages 1136–1140, Washington, DC, USA.
IEEE Computer Society.

Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pat-
terns from coordinated text streams. In Proceedings
of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining.

Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, WSDM, pages 192–201.

Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin. 2010.
Temporal and social context based burst detection
from folksonomies. In AAAI.

Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,
Hongfei Yan, and Xiaoming Li. 2010. Context mod-
eling for ranking and tagging bursty features in text

1476



streams. In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ’10.

Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,
and Xiaoming Li. 2012. A novel burst-based text
representation model for scalable event detection. In
ACL’12.

Arkaitz Zubiaga, Damiano Spina, Vı́ctor Fresno, and
Raquel Martı́nez. 2011. Classifying trending topics:
a typology of conversation triggers on twitter. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM.

1477


