



















































KCDC: Word Sense Induction by Using Grammatical Dependencies and Sentence Phrase Structure


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 351–354,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

KCDC: Word Sense Induction by Using Grammatical Dependencies and
Sentence Phrase Structure

Roman Kern
Know-Center
Graz, Austria

rkern@know-center.at

Markus Muhr
Know-Center
Graz, Austria

mmuhr@know-center.at

Michael Granitzer
Graz University of Technology,

Know-Center
Graz, Austria

mgrani@know-center.at

Abstract

Word sense induction and discrimination
(WSID) identifies the senses of an am-
biguous word and assigns instances of this
word to one of these senses. We have build
a WSID system that exploits syntactic and
semantic features based on the results of
a natural language parser component. To
achieve high robustness and good general-
ization capabilities, we designed our sys-
tem to work on a restricted, but grammat-
ically rich set of features. Based on the
results of the evaluations our system pro-
vides a promising performance and robust-
ness.

1 Introduction

The goal of the SemEval-2 word sense induc-
tion and discrimination task, see Manandhar et
al. (2010), is to identify the senses of ambiguous
nouns and verbs in an unsupervised manner and to
label unseen instances of these words with one of
the induced senses. The most common approach
towards this task is to apply clustering or graph
partitioning algorithms on a representation of the
words that surround an ambiguous target word, see
for example Niu et al. (2007) and Pedersen (2007).
We followed this approach by employing a cluster-
ing algorithm to detect the individual senses, but
focused on generating feature sets different to the
mainstream approach. Our feature sets utilize the
output of a linguistic processing pipeline that cap-
tures the syntax and semantics of sentence parts
closely related with the target word.

2 System Overview

The base of our system is to apply a parser on the
sentence in which the target word occurs. Contex-
tual information, for example the sentences sur-
rounding the target sentence, are currently not

exploited by our system. To analyze the sen-
tences we applied the Stanford Parser (Version
1.6.2), which is based on lexicalized probabilis-
tic context free grammars, see Klein and Man-
ning (2003). This open-source parser not only ex-
tracts the phrase structure of a given sentence, but
also provides a list of so called grammatical rela-
tions (typed dependencies), see de Marneffe et al.
(2006). These relations reflect the dependencies
between the words within the sentence, for exam-
ple the relationship between the verb and the sub-
ject. See Chen et al. (2009) for an application of
grammatical dependencies for word sense disam-
biguation.

2.1 Feature Extraction
The phrase structure and the grammatical depen-
dencies are sources for the feature extraction stage.
To illustrate the result of the parser and feature ex-
traction stages we use an example sentence, where
the target word is the verb “file”:

Afterward , I watched as a butt-ton of good , but
misguided people filed out of the theater , and
immediately lit up a smoke .

2.1.1 Grammatical Dependency Features
The Stanford Parser provides 55 different gram-
matical dependency types. Figure 2 depicts the list
of the grammatical dependencies identified by the
Stanford Parser for the example sentence. Only a
limited subset of these dependencies are selected
to build the grammatical feature set. This subset
has been defined based on preliminary tests on the
trial dataset. For verbs only dependencies that rep-
resent the association of a verb with prepositional
modifiers and phrasal verb particles are selected
(prep, prepc, prt). If the verb is not associated
with a preposition or particle, a synthetic “miss-
ing” feature is added instead (!prep, !prt). For
nouns the selected dependencies are the preposi-
tions (for head nouns that are the object of a prepo-
sition) and noun compound modifiers (pobj, nn).

351



Figure 1: Phrase tree of the example sentence. The noun phrase “misguided people” is connected to the
target word via the nsubj dependency and the phrase “the theater” is associated with the target verb via
the prep and pobj dependencies.

Figure 2: List of grammatical dependencies as de-
tected by the Stanford Parser.

If the noun is associated with a verb the grammati-
cal dependencies of this verb are also added to the
feature set.

The name of the dependency and the word (i.e.
preposition or particle) are used to construct the
grammatical feature. The different features are
weighted. The weights have been derived from
their frequencies within the trial dataset and listed
in table 1. For the example sentence the extracted
grammatical features are:

’out’, ’of’, prep, prt

2.1.2 Phrase Term Features
The second set of features are generated from the
sentence phrase structure. In figure 1 the parse tree
for the example sentence is depicted.

Again we tried to keep the feature set as small
as possible. Starting with the target word only
phrases that are directly associated with the am-
biguous word are selected. To identify these
phrases the grammatical dependencies are ex-
ploited. For nouns as target words the associated
verb is searched at first. Given a verb the phrases
containing the head noun of a subject or object re-
lationship are identified. If the verb is accompa-

Feature Weight
prepc, prt, nn, pobj 0.9
prep 0.45
!prep, !prt 0.5
’prepositions’, ’particles’ 0.97

Table 1: Weights of the grammatical features,
which were derived from their distribution within
the trial dataset.

nied by a preposition, the phrase carrying the ob-
ject of the preposition is also added. All nouns and
adjectives from these these phrases are then col-
lected. The phrase words together with the verb,
prepositions and particles are lemmatized using
tools also provided by the Stanford Parser project.

The weights of the phrase term features are
based on the frequency of the words within the
training dataset, where N is the total number of
sentences and Nf is the number of sentences in
which the lemmatized phrase term occurs in:

weightf = log(
N

Nf + 1
) + 1 (1)

In our example sentence the extracted phrase
term features are:

of, misguided, file, theater, people, out

2.2 Phrase Term Expansion
The feature space of the phrase terms is expected
to be very sparse. Additionally different phrase
terms may have similar semantics. Therefore the
phrase terms are optionally expanded with asso-
ciated terms, where semantically similar terms
should be associated with the same terms.

To calculate the statistics for term expansion we
used the training dataset (although other datasets

352



would be more suitable for this purpose). The
dataset is split into sentences. Stopwords and
rarely used words, which occur in less than 3 sen-
tences, were removed. The remaining words were
finally lemmatized. For a given phrase term the
top 100 associated terms are used to build the
feature set. The association weight between two
terms is based on the Pointwise Mutual Informa-
tion:

weightpmi(ti, tj) =
log2(

P (ti|tj)
P (tj)

)

log2(
1

P (tj)
)

(2)

For example the top 10 associated terms for
theater are:

theater.n, movie.n, opera.n,
vaudeville.n, wnxt-abc.n, imax.n,
orpheum.n, pullulate.v, projector.n,
psychomania.n

2.3 Sense Induction
To detect the individual senses within the training
dataset we applied unsupervised machine learning
techniques. For each ambiguous word a matrix
- M|Instances|×|Features| - is created and a clus-
tering algorithm is applied, namely the Growing
k-Means, see Daszykowski et al. (2002). This
algorithm needs the number of clusters and cen-
troids as initialization parameters, where the initial
centroids are calculated using a directed random
seed finder as described in Arthur and Vassilvitskii
(2007). We used the Jensen-Shannon Divergence
function for the grammatical dependency features
and the Cosine Similarity for the phrase term fea-
ture sets as relatedness function.

For each cluster number we re-run the clus-
tering with different random initial centroids (30
times) and for each run we calculate a cluster qual-
ity criterion. The overall cluster quality criterion is
the mean of all feature quality criteria, which are
calculated based on the set of clusters the feature
occurs in - Cf - the number of instances of each
cluster - Nc - and the number of instances within
a cluster where the feature occurs in - Nc,f :

FQCf =
weightf
|Cf | ∗

∑
c∈Cf

Nc,f
Nc

(3)

QCrun = FQCf (4)

The cluster quality criterion is calculated for
each run and the combination of the mean and
standard deviations are then used to calculate a
stability criterion to detect the number of clusters,
which is based on the intuition that the correct

cluster count yields the lowest variation of QC
values:

SCk =
mean(QC)
stdev(QC) (5)

Starting with two clusters the number of clusters
is incremented until the stability criterion starts to
decline. For the cluster number with the highest
stability criterion the run with the highest qual-
ity criterion is selected as final clustering solution.
The result of the sense induction processing is a
list of centroids for the identified clusters.

2.4 Sense Assignment
The final processing step is to assign an instance
of an ambiguous word to one of the pre-calculated
senses. The sentence with the target word is pro-
cessed exactly like the training sentences to gener-
ate a set of features. Finally the word is assigned
to the sense cluster with the maximum relatedness.

3 System Configurations & Results

Our system can be configured to use a combina-
tion of feature sets for the word sense induction
and discrimination calculations: a) KCDC-GD:
Grammatical dependency features, b) KCDC-PT:
Phrase terms features, c) KCDC-PC: Expanded
phrase term features, d) KCDC-PCGD: All train-
ing sentences are first processed by using the ex-
panded phrase term features and then by using
the grammatical dependency features with an ad-
ditional feature that encodes the cluster id found
by the phrase features.

In the evaluation we also submitted multiple
runs of the same configuration1 to assess the in-
fluence of the random initialization of the cluster-
ing algorithm. Judging from the results the ran-
dom seeding has no pronounced impact and it in-
fluence should decrease when the number of clus-
tering runs for each cluster number is increased.

All configurations found on average about 3
senses for target words in the test set (2.8 for verbs,
3.3 for nouns), with exception of the KCDC-PT
configuration which identified only 1.5 senses on
average. In the gold standard the number of senses
for verbs is 3.12 and for nouns 4.46, which shows
that the stability criterion tends to underestimate
the number of senses slightly.

To compare the performance of the differ-
ent configurations, one can use the average rank
within the evaluation result lists. Judging from the

1labeled KCDC-GD-2, KCDC-GDC for configuration ’a’
and KCDC-PC-2 for the configuration ’c’

353



rankings, the configurations that utilize the gram-
matical dependencies and the expanded phrase
terms provide similar performance. The config-
uration that takes the phrase terms directly as fea-
tures comes in last, which is expected due to the
sparse nature of the feature representation and the
low number of detected senses.

Comparing the performance of our system with
the two baselines shows that our system did out-
perform the random baseline in all evaluation runs
and the most frequent baseline (MFS) in all runs
with the exception of the F-Score based unsuper-
vised evaluation, where the MFS baseline has not
been beaten by any system. Although none of our
submitted configurations was ranked first in any of
the evaluations, their ranking was still better than
average, with the exception of the KCDC-PT con-
figuration.

Another observation that can be made is the dif-
ference in performance between nouns and verbs.
Our system, especially the grammatical depen-
dency based configurations, is tailored towards
verbs. Therefore the better performance of verbs
in the evaluation is in line with the expectations.

When looking at the results of the individual tar-
get words one can notice that for a set of words
the quality of the sense detection is above average.
For 16 of the 100 words a V-Measure of more than
30% in at least one configuration was achieved
(average: 7.8%)2. This can be seen as indicator
that our selection of features is effective for a spe-
cific group of words. For the remaining words an
according feature set has to be developed in future
work.

4 Conclusion

For the SemEval 2010 word sense induction and
discrimination task we have tried to build a system
that uses a minimal amount of information while
still providing a competitive performance. This
system contains a parser component to analyze the
phrase structure of a sentence and the grammat-
ical dependencies between words. The extracted
features are then clustered to detect the senses of
ambiguous words. In the evaluation runs our sys-
tem did demonstrate a satisfying performance for
a number of words.

The design of our system offers a wide range
of possible enhancements. For example the inte-

2The best performing target words are: root.v,
presume.v, figure.v, weigh.v, cheat.v

gration of preposition disambiguation and noun-
phrase co-reference resolution could help to fur-
ther improve the word sense discrimination effec-
tiveness.

Acknowledgments

The Know-Center is funded within the Austrian COMET

Program - Competence Centers for Excellent Technologies -

under the auspices of the Austrian Federal Ministry of Trans-

port, Innovation and Technology, the Austrian Federal Min-

istry of Economy, Family and Youth and by the State of

Styria. COMET is managed by the Austrian Research Pro-

motion Agency FFG. Results are partially funded by the EU-

ROSTARS project 4811 MAKIN’IT.

References
D. Arthur and S. Vassilvitskii. 2007. k-means++:

The advantages of careful seeding. In Proceedings
of the eighteenth annual ACM-SIAM symposium on
Discrete algorithms, page 10271035. Society for In-
dustrial and Applied Mathematics Philadelphia, PA,
USA.

Ping Chen, Wei Ding, Chris Bowes, and David Brown.
2009. A fully unsupervised word sense disambigua-
tion method using dependency knowledge. Human
Language Technology Conference.

M Daszykowski, B Walczak, and D L Massart. 2002.
On the optimal partitioning of data with K-means,
growing K-means, neural gas, and growing neural
gas. Journal of chemical information and computer
sciences, 42(6):1378–89.

M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - ACL ’03, pages 423–430.

Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 Task 14: Word Sense Induction & Disam-
biguation. In Proceedings of SemEval-2, Uppsala,
Sweden, ACL.

Zheng-yu Niu, Dong-hong Ji, and Chew-lim Tan.
2007. I2R: Three Systems for Word Sense Discrim-
ination, Chinese Word Sense Disambiguation, and
English Word Sense Disambiguation. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations. ACL.

T. Pedersen. 2007. Umnd2: Senseclusters applied to
the sense induction task of senseval-4. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations. ACL.

354


