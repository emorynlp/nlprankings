409

Coling 2010: Poster Volume, pages 409–417,

Beijing, August 2010

Recognizing Relation Expression between Named Entities based on

Inherent and Context-dependent Features of Relational words

Toru Hirano†, Hisako Asano‡, Yoshihiro Matsuo†, Genichiro Kikui†

†NTT Cyber Space Laboratories, NTT Corporation

‡Innovative IP Architecture Center, NTT Communications Corporation

hirano.tohru@lab.ntt.co.jp

hisako.asano@ntt.com

{matsuo.yoshihiro,kikui.genichiro}@lab.ntt.co.jp

Abstract

This paper proposes a supervised learn-
ing method to recognize expressions that
show a relation between two named en-
tities, e.g., person, location, or organiza-
tion. The method uses two novel fea-
tures, 1) whether the candidate words in-
herently express relations and 2) how the
candidate words are inﬂuenced by the past
relations of two entities. These features
together with conventional syntactic and
contextual features are organized as a tree
structure and are fed into a boosting-based
classiﬁcation algorithm. Experimental re-
sults show that the proposed method out-
performs conventional methods.

1 Introduction

Much attention has recently been devoted to us-
ing enormous amount of web text covering an ex-
ceedingly wide range of domains as a huge knowl-
edge resource with computers. To use web texts as
knowledge resources, we need to extract informa-
tion from texts that are merely sequences of words
and convert them into a structured form. Although
extracting information from texts as a structured
form is difﬁcult, relation extraction is a way that
makes it possible to use web texts as knowledge
resources.

The aim of relation extraction is to extract se-
mantically related named entity pairs, X and Y ,
and their relation, R, from a text as a struc-
tured form [X, Y , R]. For example, the triple
[Yukio Hatoyama, Japan, prime minister] would
be extracted from the text “Yukio Hatoyama is the
prime minister of Japan”. This extracted triple

provides important information used in informa-
tion retrieval (Zhu et al., 2009) and building an
ontology (Wong et al., 2010).

It is possible to say that all named entity pairs
that co-occur within a text are semantically related
in some way. However, we deﬁne that named en-
tity pairs are semantically related if they satisfy
either of the following rules:

• One entity is an attribute value of the other.
• Both entities are arguments of a predicate.
Following the above deﬁnition, explicit and im-
plicit relations should be extracted. An explicit re-
lation means that there is an expression that shows
the relation between a named entity pair in a given
text, while an implicit relation means that there is
no such expression. For example, the triple [Yukio
Hatoyama, Kunio Hatoyama, brother] extracted
from the text “Yukio Hatoyama, the Democratic
Party, is Kunio Hatoyama’s brother” is an explicit
relation. In contrast, the triple [Yukio Hatoyama,
the Democratic Party, member] extracted from the
same text is an implicit relation because there is
no expression showing the relation (e.g. member)
between “Yukio Hatoyama” and “the Democratic
Party” in the text.

Extracting triples [X, Y , R] from a text in-
volves two tasks. One is detecting semantically
related pairs from named entity pairs that co-occur
in a text and the other is determining the rela-
tion between a detected pair. For the former task,
various supervised learning methods (Culotta and
Sorensen, 2004; Zelenko et al., 2003; Hirano et
al., 2007) and bootstrapping methods (Brin, 1998;
Pantel and Pennacchiotti, 2006) have been ex-
plored to date.
In contrast, for the latter task,

410

only a few methods have been proposed so far
(Hasegawa et al., 2004; Banko and Etzioni, 2008;
Zhu et al., 2009). We therefore addressed the
problem of how to determine relations between a
given pair.

We used a three-step approach to address this
problem. The ﬁrst step is to recognize an expres-
sion that shows explicit relations between a given
named entity pair in a text. If no such expression
is recognized, the second step is to estimate the
relationship that exists between a given named en-
tity pair that has an implicit relation. The last step
is to identify synonyms of the relations that are
recognized or estimated in the above steps. In this
paper, we focus on the ﬁrst step. The task is se-
lecting a phrase from the text that contains a re-
lation expression linking a given entity pair and
outputting the expression as one showing the rela-
tionship between the pair.

In our preliminary experiment, it was found
that using only structural features of a text, such
as syntactic or contextual features, is not good
enough for a number of examples. For instance,
the two Japanese sentences shown in Figure 1
have the same syntactic structure but (a) contains a
relation expression and (b) does not. We therefore
assume there are clues for recognizing relation
expressions other than conventional syntactic and
contextual information. In this paper, we propose
a supervised learning method that includes two
novel features of relational words as well as con-
ventional syntactic and contextual features. The
novel features of our method are:

Inherent Feature: Some words are able to ex-
press the relations between named entities
and some are not. Thus, it would be useful to
know the words that inherently express these
relations.

Context-dependent Feature: There are a num-
ber of typical relationships that change as
time passes, such as “dating” ⇒ “engage-
ment” ⇒ “marriage” between persons. Fur-
thermore, present relations are inﬂuenced by
the past relations of a given named entity
pair. Thus, it would be useful to know the
past relations between a given pair and how
the relations change as time passes.

In the rest of this paper, Section 2 references re-
lated work, Section 3 outlines our method’s main
features and related topics, Section 4 describes our
experiments and experimental results, and Section
5 brieﬂy summarizes key points and future work
to be done.

2 Related Work

The “Message Understanding Conference” and
“Automatic Content Extraction” programs have
promoted relational extraction. The task was stud-
ied so as to extract predeﬁned semantic relations
of entity pairs in a text. Examples include the
supervised learning method cited in (Kambhatla,
2004; Culotta and Sorensen, 2004; Zelenko et al.,
2003) and the bootstrapping method cited in (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000). Recently, open information extrac-
tion (Open IE), a novel domain-independent ex-
traction paradigm, has been suggested (Banko and
Etzioni, 2008; Hasegawa et al., 2004). The task is
to detect semantically related named entity pairs
and to recognize the relation between them with-
out using predeﬁned relations.

Our work is a kind of open IE, but our approach
differs from that of previous studies. Banko
(2008) proposed a supervised learning method us-
ing conditional random ﬁelds to recognize the re-
lation expressions from words located between a
given pair. Hasegawa (2004) also proposed a rule-
based method that selects all words located be-
tween a given pair as a relation expression if a
given named entities appear within ten words. The
point of these work is that they selected relation
expressions only from the words located between

Figure 1: Same syntactic examples

Yumei
-desu.
Yumei
-desu.
D
04
04
-san-wa
Suzuki
-san-wa
Suzuki
D
03
03
-no
Kacho
Kacho
-no
D
02
02
Osaka Fucho
-no
-no
Osaka Fucho
(a)
01
01
, a manager
(a)Mr.Suzuki
is famous
.
03
02
, administration office
(b)Mr.Suzuki
04
Government
, is famous
06
07
05

.
08

Yumei
-desu.
Yumei
-desu.
D
08
08
Suzuki
-san-wa
-san-wa
Suzuki
07
07
Soumukyoku
-no
Soumukyoku
-no
06
06
Osaka Fucho
-no
-no
Osaka Fucho
(b)
05
05

D

D

of Osaka Prefectural Government
, 
01

in Osaka Prefectural

411

given entities in the text, because as far as English
texts are concerned, 86% of the relation expres-
sions of named entity pairs appear between the
pair (Banko and Etzioni, 2008). However, our tar-
get is Japanese texts, in which only 26% of entity
pair relation expressions appear between the pair.
Thus, it is hard to incorporate previous approaches
into a Japanese text.

To solve the problem, our task was to select a
phrase from the entire text that would include a
relation expression for connecting a given pair.

3 Recognizing Relation Expressions

between Named Entities

To recognize the relation expression for a given
pair, we need to select a phrase that includes an
expression that shows the relation between a given
entity pair from among all noun and verb phrases
in a text. Actually, there are two types of candi-
date phrases in this case. One is from a sentence
that contains a given pair (intra-sentential), and
the other is from a sentence that does not (inter-
sentential). For example, the triple [Miyaji21,
Ishii22, taiketsu12] extracted from the following
text is inter-sentential.

(S-1) Chumokoku11-no taiketsu12-ga

mamonaku13 hajimaru14.
(The showcase11 match12 will start14 soon13.)

(S-2) Ano Miyaji21-to Ishii22-toiu

kanemochi23-niyoru yume24-no

kikaku25.

(The dream24 event25 between the rich mens23,
Miyaji21 and Ishii22.)

According to our annotated data shown in Ta-
ble 2, 53% of the semantically-related named en-
tity pairs are intra-sentential and 12% are inter-
sentential. Thus, we ﬁrst select a phrase from
those in a sentence that contains a given pair, and
if no phrase is selected, select one from the rest of
the sentences in a text.

We propose a supervised learning method that
uses two novel features of relational words as
well as conventional syntactic and contextual fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classiﬁca-
tion algorithm (Kudo and Matsumoto, 2004). The
highest-scoring phrase is then selected if the score

exceeds a given threshold. Finally, the head of the
selected phrase is output as the relation expression
of a given entity pair.

The method consists of four parts: preprocess-
ing (POS tagging and parsing), feature extraction,
classiﬁcation, and selection.
In this section, we
describe the idea behind using our two novel fea-
tures and how they are implemented to recognize
the relation expressions of given pairs. Before
that, we will describe our proposed method’s con-
ventional features.

3.1 Conventional Features

Syntactic feature

To recognize the intra-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative syntactic structure that consists
of given entities and their relation expression. For
example, there is a structure for which the com-
mon parent phrase of the given pair, X = “Ha-
toyama Yukio32” and Y = “Hatoyama Kunio33”,
has the relation expression, R = “ani34” in the
Japanese sentence S-3. Figure 2 shows the depen-
dency tree of sentence S-3.

(S-3) Minshuto31-no Hatoyama Yukio32-wa

Hatoyama Kunio33-no ani34-desu.
(Yukio Hatoyama32, the Democratic Party31,
is Kunio Hatoyama33’s brother34.)

To use a discriminative structure for each can-
didate, we make a minimum tree that consists of
given entities and the candidate where each phrase
is represented by a case marker “CM”, a depen-
dency type “DT”, an entity class, and the string
and POS of the candidate (See Figure 3).

Figure 2: Dependency tree of sentence S-3

Ani
-desu.
Ani
-desu.
34
34

D

Hatoyama Kunio
-no
Hatoyama Kunio
-no
33
33

D
Hatoyama Yukio
-wa
Hatoyama Yukio
-wa
32
32

D
-no
Minshuto
Minshuto
-no
31
31

412

Figure 3: Intra-sentential feature tree

Contextual Feature

To recognize the inter-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative contextual structure that con-
sists of given entities and their relation expression.
Here, we use a Salient Referent List (SRL) to ob-
tain contextual structure. The SRL is an empirical
sorting rule proposed to identify the antecedent
of (zero) pronouns (Nariyama, 2002), and Hirano
(2007) proposed a way of applying SRL to rela-
tion detection. In this work, we use this way to
apply SRL to recognize inter-sentential relation
expressions.

We applied SRL to each candidate as follows.
First, from among given entities and the candi-
date, we choose the one appearing last in the text
as the root of the tree. We then append noun
phrases, from the chosen one to the beginning of
the text, to the tree depending on case markers,
“wa” (topicalised subject), “ga” (subject), “ni”
(indirect object),“wo” (object), and “others”, with
the following rules. If there are nodes of the same
case marker already in the tree, the noun phrase
is appended as a child of the leaf node of them.
In other cases, the noun phrase is appended as a
child of the root node. For example, we get the
SRL tree shown in Figure 4 with the given entity
pair, X = “Miyaji21” and Y = “Ishii22”, and the
candidate, “taiketsu12”, with the text (S-1, S-2).

To use a discriminative SRL structure, we make
a minimum tree that consists of given entities and
the candidate where each phrase is represented by
an entity class, and the string and POS of the can-
didate (See Figure 5). In this way, there is a prob-
lem when the candidate is a verb phrase, because

Figure 4: Salient referent list tree

only noun phrases are appended to the SRL tree.
If the candidate is a verb phrase, we cannot make
a minimum tree that consists of given entities and
the candidate.

To solve this problem, a candidate verb phrase
is appended to the feature tree using a syntactic
structure.
In a dependency tree, almost all verb
phrases have some parent or child noun phrases
that are in the SRL tree. Thus, candidate verb
phrases are appended as offspring of these noun
phrases represented syntactically as “parent” or
“child”. For example, when given the entity pair,
X = “Miyaji21” and Y = “Ishii22”, and the can-
didate, “hajimaru14” from the text (S-1, S-2), a
feature tree cannot be made because the candi-
date is not in an SRL tree. By extending the way
the syntactic structure is used, “hajimaru14” has a
child node “taiketsu12”, which is in an SRL tree,
and this makes it possible to make the feature tree
shown in Figure 6.

3.2 Proposed Features

To recognize intra-sentential or inter-sentential re-
lation expressions for given pairs, we assume
there are clues other than syntactic and contex-
tual information. Thus, we propose inherent and

Figure 5: Inter-sentential feature tree

Phrase
Phrase

Candidate
Candidate

CM:φCM:φ DT:ODT:O

Phrase
Phrase

Phrase
Phrase

Y:person
Y:person

CM:no DT:DDT:D
CM:no

X:person
X:person

CM:wa DT:DDT:D
CM:wa

STR:Ani
STR:Ani
34
34

POS:Noun
POS:Noun
Inh:1
Inh:1
C
:1
C
:1
rank
rank
:0.23
:0.23

C
C
prob
prob

Ishii
Ishii
22
22

ga: Taiketsu
ga: Taiketsu
12
12

others: Miyaji
others: Miyaji
21
21
others: Chumoku
others: Chumoku
11
11

Y:person
Y:person

Candidate
Candidate

SRL:ga
SRL:ga

Inh:1
Inh:1

POS:Noun
POS:Noun

STR:Taiketsu
STR:Taiketsu
12
12
:0.23
:0.23

C
:1
C
:1
rank
rank

C
C
prob
prob

X:person
X:person

SRL:others
SRL:others

413

Figure 6: Extended inter-sentential feature tree

context-dependent features of relational words.

Inherent Feature of Relational words

Some words are able to express the relations be-
tween named entities and some are not. For exam-
ple, the word “mother” can express a relation, but
the word “car” cannot. If there were a list of words
that could express relations between named enti-
ties, it would be useful to recognize the relation
expression of a given pair. As far as we know,
however, no such list exists in Japanese. Thus,
we estimate which words are able to express rela-
tions between entities. Here, we assume that al-
most all verbs are able to express relations, and
accordingly we focus on nouns.

When the relation expression, R, of an entity
pair, X and Y , is a noun, it is possible to say “Y is
R of X” or “Y is X’s R”. Here, we can say noun
R takes an argument X. In linguistics, this kind
of noun is called a relational noun. Grammatically
speaking, a relational noun is a simple noun, but
because its meaning describes a “relation” rather
than a “thing”, it is used to describe relations just
as prepositions do. To estimate which nouns are
able to express the relations between named enti-
ties, we use the characteristics of relational nouns.
In linguistics, many researchers describe the rela-
tionship between possessives and relational nouns
(Chris, 2008). Thus, we use the knowledge that
in the patterns “B of A” or “A’s B”, if word B is
a relational noun, the corresponding word A be-
longs to a certain semantic category. In contrast,
if word B is not a relational noun, the correspond-
ing word A belongs to many semantic categories
(Tanaka et al., 1999). Figure 7 shows scattering
of the semantic categories of “mother” and “car”

Figure 7: Scattering of semantic category of
“mother” (left) and “car” (right).

acquired by the following way.

First, we acquired A and B using the patterns
“A no B”1 from a large Japanese corpus, then
mapped words A into semantic categories C= {
c1, c2,··· , cm } using a Japanese lexicon (Ikehara
et al., 1999). Next, for each word B, we calcu-
lated a scattering score Hc(B) using the semantic
category of corresponding words A. Finally, we
estimated whether a word is a relational noun by
using k-NN estimation with positive and negative
examples. As estimated results, “Inh:1” shows
that it is a relational noun and “Inh:0” shows that
it is not. In both cases, the result is appended to
the feature tree as a child of the candidate node
(See Figure 3, 5, or 6).

Hc(B) = −∑c∈C
P (c|B) =

P (c|B)logmP (c|B)
f req(c, B)
f req(B)

In our experiments, we acquired 55,412,811
pairs of A and B from 1,698,798 newspaper ar-
ticles and 10,499,468 weblog texts. As training
data, we used the words of relation expressions as
positive examples and other words as negative ex-
amples.

Context-dependent Feature of Relational
words

There are a number of typical relationships that
change as time passes, such as “dating” ⇒ “en-
gagement” ⇒ ‘marriage” between persons. Fur-
thermore, present relations are affected by the past
relations of a given named entity pair. For in-
stance, if the past relations of a given pair are “dat-
ing” and “engagement” and one of the candidates
is “marriage”, “marriage” would be selected as the
relation expression of the given pair. Therefore, if

1“B of A” or “A’s B” in English.

Y:person
Y:person

SRL:ga
SRL:ga
Candidate
Candidate

X:person
X:person
SRL:others
SRL:others

POS:Verb
POS:Verb

Dep:Child
Dep:Child

Inh:0
Inh:0

STR:Hajimaru
STR:Hajimaru
14
14
:0.00
:0.00

C
C
prob
prob

C
:2
C
:2
rank
rank

y
c
n
e
u
q
e
r
F
e
v
i
t

 

l

a
e
R

Semantic categories

y
c
n
e
u
q
e
r
F
 
e
v
i
t
a
e
R

l

Semantic categories

414

Pair of entity class

hperson,personi

rm

dating

hperson,personi

engagement

rn

dating
marriage

engagement

marriage

engagement

wedding
president

hperson,organizationi

vice president

vice president

hperson,organizationi

researcher

horganization,organizationi

alliance

hlocation,locationi

neighbour

CEO
fellow
manager
member
alliance

accommodated

acquisition

mutual consultation

support

visit
war

PT (rn|rm) Count(rm, rn)

0.050
0.050
0.040
0.157
0.065
0.055
0.337
0.316
0.095
0.526
0.103
0.078
0.058
0.027
0.027
0.022
0.015
0.012
0.077
0.015
0.010

102
101
82
786
325
276

17,081
16,056
4,798

61
12
9

8,358
3,958
3,863
2,670
1,792
1,492
78,170
15,337
10,226

hlocation,locationi

war

mutual consultation

support

Table 1: Examples of calculated relation trigger model between entity classes deﬁned by IREX

we know the past relations of the given pair and
the typical relational change that occurs as time
passes, it would be useful to recognize the rela-
tion expression of a given pair.

In this paper, we represent typical relational
changes that occur as time passes by a simple re-
lation trigger model PT (rn|rm). Note that rm
is a past relation and rn is a relation affected by
rm. This model disregards the span between rn
and rm. To make the trigger model, we automat-
ically extract triples [X, Y , R] from newspaper
articles and weblog texts, which have time stamps
of the document creation. Using these triples with
time stamps for each entity pair, we sort rela-
tions in order of time and count pairs of present
and previous relations. For example, if we ex-
tract “dating” occurring for an entity pair on Jan-
uary 10, 1998, “engagement” occurring on Febru-
ary 15, 2001, and “marriage” occurring on De-
cember 24, 2001, the pairs hdating, engagementi,
hdating, marriagei, and hengagement, marriagei
are counted. The counted score is then summed

up by the pair of entity class and the trigger model
is calculated by the following formula.

PT (rn|rm) =

Count(rm, rn)

∑rn Count(rm, rn)

For the evaluation, we extracted triples by
named entity recognition (Suzuki et al., 2006), re-
lation detection (Hirano et al., 2007), and the pro-
posed method using the inherent features of rela-
tional words described in Section 3.2. A total of
10,463,232 triples were extracted from 8,320,042
newspaper articles and weblog texts with time
stamps made between January 1, 1991 and June
30, 2006. As examples of the calculated relation
trigger model, Table 1 shows the top three proba-
bility relations rn of several relations rm between
Japanese standard named entity classes deﬁned
in the IREX workshop2. For instance, the rela-
tion “fellow” has the highest probability of being
changed from the relation “researcher” between
person and organization as time passes.

2http://nlp.cs.nyu.edu/irex/

415

To obtain the past relations of a given pair in
the input text, we again used the triples with time
stamps extracted as above. The only relations we
use as past relations, Rm = {rm1, rm2,··· , rmk},
are those of a given pair whose time stamps are
older than the input text.
Finally, we calcu-
lated probabilities with the following formula us-
ing the past relations Rm and the trigger model
PT (rn|rm).

PT (rn|Rm) = max{PT (rn|rm1),

PT (rn|rm2),··· , PT (rn|rmk )}

Using this calculated probability, we ranked
candidates and appended the rank “Crank” and
the probability score “Cprob” to the feature tree
as a child of the candidate node (See Figure 3,
5, or 6). For example, if the past relations Rm
were “dating” and “engagement” and candidates
were “marriage”, “meeting’, “eating”, or “drink-
ing”, the candidates probabilities were calculated
and ranked as “marriage” (Cprob:0.15, Crank:1),
“meeting” (Cprob:0.08, Crank:2), etc.

3.3 Classiﬁcation Algorithms
Several structure-based learning algorithms have
been proposed so far (Collins and Duffy, 2002;
Suzuki et al., 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto’s
boosting-based algorithm using sub-trees as fea-
tures, which is implemented as a BACT system.

Given a set of training examples each of which
is represented as a tree labeling whether the can-
didate is the relation expression of a given pair or
not, the BACT system learns that a set of rules
is effective in classifying. Then, given a test in-
stance, the BACT system classiﬁes using a set of
learned rules.

4 Experiments

We conducted experiments using texts from
Japanese newspaper articles and weblog texts to
test the proposed method for both intra- and inter-
sentential tasks. In the experiments, we compared
the following methods:

Conventional Features: trained by conventional
syntactic features for intra-sentential tasks as

Relation Types

Intra-sentential
Inter-sentential

Explicit

Implicit

Total

#

9,178
2,058
5,992
17,228

Table 2: Details of the annotated data

described in Section 3.1, and contextual fea-
tures for inter-sentential tasks as described in
Section 3.1.

+Inherent Features: trained by conventional
features plus inherent features of relational
words described in Section 3.2.

++Context-dependent FeaturesTM: trained

by conventional and inherent features plus
context-dependent
relational
words with the trigger model described in
Section 3.2.

features of

++Context-dependent FeaturesCM: trained

features of

and inherent

features
by conventional
plus context-dependent
rela-
tional words with a cache model. We
evaluated this method to compare it with
Context-dependent FeaturesTM to show the
effectiveness of the proposed trigger model.
The cache model is a simple way to use past
relations in which the probability PC(rcand)
calculated by the following formula and the
rank based on the probability is appended to
every candidate feature tree.

PC(rcand) = |rcand in past relations|

|past relations|

4.1 Settings
We used 6,200 texts from Japanese newspapers
and weblogs dated from January 1, 2004 to June
30, 2006, manually annotating the semantic rela-
tions between named entities for experiment pur-
poses. There were 17,228 semantically-related
entity pairs as shown in Table 2.
In an intra-
sentential experiment, 17,228 entity pairs were
given, but only 9,178 of them had relation expres-
sions.
In contrast, in an inter-sentential experi-
ment, 8,050 entity pairs excepted intra-sentential

416

P recision

Recall

+Inherent Features

Conventional Features

63.5 ％ (3,436/5,411)
67.2 ％ (4,036/6,001)
++Context-dependent FeaturesTM 70.7 ％ (4,460/6,312)
++Context-dependent FeaturesCM 67.5 ％ (4,042/5,987)

37.4 ％ (3,436/9,178)
43.9 ％ (4,036/9,178)
48.6 ％ (4,460/9,178)
44.0 ％ (4,042/9,178)

Table 3: Experimental results of intra-sentential

P recision

Recall

+Inherent Features

Conventional Features

70.1 ％ (579/825)
77.1 ％ (719/932)
++Context-dependent FeaturesTM 75.2 ％ (794/1,055)
++Context-dependent FeaturesCM 74.3 ％ (732/985)

28.1 ％ (579/2,058)
34.9 ％ (719/2,058)
38.5 ％ (794/2,058)
35.5 ％ (732/2,058)

F

0.471
0.531
0.576
0.533

F

0.401
0.480
0.510
0.481

Table 4: Experimental result of inter-sentential

were given, but only 2,058 of them had relation
expressions.

points, respectively. McNemar test results also
showed the method’s effectiveness.

We conducted ﬁve-fold cross-validation over
17,228 entity pairs so that sets of pairs from a sin-
gle text were not divided into the training and test
sets. In the experiments, all features were auto-
matically acquired using a Japanese POS tagger
(Fuchi and Takagi, 1998) and dependency parser
(Imamura et al., 2007).

4.2 Results

Tables 3 and 4 show the performance of several
methods for intra-sentential and inter-sentential.
P recision is deﬁned as the percentage of cor-
rect relation expressions out of recognized ones.
Recall is the percentage of correct relation ex-
pressions from among the manually annotated
ones. The F measure is the harmonic mean of
precision and recall.

A comparison with the Conventional Fea-
tures and Inherent Features method for intra-
/inter-sentential tasks indicates that the proposed
method using inherent features of relational words
improved intra-sentential tasks F by 0.06 points
and inter-sentential tasks F by 0.08 points. Us-
ing a statistical test (McNemar Test) demonstrably
showed the proposed method’s effectiveness.

A comparison with the Inherent Features and
Context-dependent FeaturesTM method showed
that the proposed method using context-dependent
features of relational words improved intra-/inter-
sentential task performance by 0.045 and 0.03

To further compare the usage of context-
trigger models, and cache
dependent features,
used Context-dependent
models, we
also
FeaturesCM method for comparison.
Tables
3 and 4 show that our proposed trigger model
performed better than the cache model, and
McNemar test results showed that there was a
signiﬁcant difference between the models. The
reason the trigger model performed better than
the cache model is that the trigger model correctly
recognized the relation expressions that did not
appear in the past relations of a given pair. Thus,
we can conclude that using typical relationships
that change as time passes helps to recognize
relation expressions between named entities.

5 Conclusion

We proposed a supervised learning method that
employs inherent and context-dependent features
of relational words and uses conventional syntac-
tic or contextual features to improve both intra-
and inter-sentential relation expression recogni-
tion. Our experiments demonstrated that
the
method improves the F measure and thus helps
to recognize relation expressions between named
entities.

In future work, we plan to estimate implicit re-
lations between named entities and to identify re-
lational synonyms.

417

References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM conference
on Digital libraries, pages 85–94.

Banko, Michele and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting on Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 28–36.

Brin, Sergey.

1998. Extracting patterns and rela-
In WebDB Work-
tions from the world wide web.
shop at 6th International Conference on Extending
Database Technology, pages 172–183.

Chris, Barker, 2008.

Semantics: An international
language meaning, chap-
handbook of natural
ter Possessives and relational nouns. Walter De
Gruyter Inc.

Collins, Michael and Nigel Duffy. 2002. Convolution
kernels for natural language. Advances in Neural
Information Processing Systems, 14:625–632.

Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
In Pro-
dency tree kernels for relation extraction.
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 423–429.

Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese
morphological analyzer using word co-occurrence
- jtag.
In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, volume 1, pages 409–413.

Hasegawa, Takaaki, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora.
In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 415–422.

Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting semantic relations between named
entities in text using contextual features.
In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 157–160.

Ikehara, Satoru, Masahiro Miyazaki, Satoru Shirai,
Akio Yoko, Hiromi Nakaiwa, Kentaro Ogura, Masa-
fumi Oyama, and Yoshihiko Hayashi. 1999. Ni-
hongo Goi Taikei (in Japanese). Iwanami Shoten.

Imamura, Kenji, Genichiro Kikui, and Norihito Ya-
suda. 2007. Japanese dependency parsing using se-
quential labeling for semi-spoken language. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 225–228.

Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 178–181.

Kudo, Taku and Yuji Matsumoto. 2004. A boosting
algorithm for classiﬁcation of semi-structured text.
In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing, pages
301–308.

Nariyama, Shigeko. 2002. Grammar for ellipsis res-
olution in japanese.
In Proceedings of the 9th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 135–
145.

Pantel, Patrick and Marco Pennacchiotti.

2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations.
In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 113–120.

Suzuki, Jun, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical directed acyclic
graph kernel: Methods for structured natural lan-
guage data.
In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 32–39.

Suzuki, Jun, Erik McDermott, and HIdeki Isozaki.
2006. Training conditional random ﬁelds with mul-
tivariate evaluation measures. In Proceedings of the
43th Annual Meeting on Association for Computa-
tional Linguistics.

Tanaka, Shosaku, Yoichi Tomiura, and Toru Hitaka.
1999.
Classiﬁcation of syntactic categories of
nouns by the scattering of semantic categories (in
japanese). Transactions of Information Processing
Society of Japan, 40(9):3387–3396.

Wong, Wilson, Wei Liu, and Mohammed Bennamoun.
2010. Acquiring semantic relations using the web
for constructing lightweight ontologies. In Proceed-
ings of the 13th Paciﬁc-Asia Conference on Knowl-
edge Discovery and Data Mining.

Zelenko, Dmitry, Chinatsu Aone,

and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083–1106.

Zhu, Jun, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships.
In Pro-
ceedings of the 18th international conference on
World Wide Web, pages 101–110.

