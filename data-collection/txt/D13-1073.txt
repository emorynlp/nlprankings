










































Mining Scientific Terms and their Definitions: A Study of the ACL Anthology


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 780–790,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Mining Scientific Terms and their Definitions:
A Study of the ACL Anthology

Yiping Jin1 Min-Yen Kan1,2 Jun-Ping Ng1 Xiangnan He1 ∗
1Department of Computer Science

2Interactive and Digital Media Institute
National University of Singapore

13 Computing Drive, Singapore 117417
{yiping, kanmy, junping, xiangnan}@comp.nus.edu.sg

Abstract

This paper presents DefMiner, a supervised
sequence labeling system that identifies scien-
tific terms and their accompanying definitions.
DefMiner achieves 85% F1 on a Wikipedia
benchmark corpus, significantly improving
the previous state-of-the-art by 8%.

We exploit DefMiner to process the ACL An-
thology Reference Corpus (ARC) – a large,
real-world digital library of scientific arti-
cles in computational linguistics. The re-
sulting automatically-acquired glossary rep-
resents the terminology defined over several
thousand individual research articles.

We highlight several interesting observations:
more definitions are introduced for conference
and workshop papers over the years and that
multiword terms account for slightly less than
half of all terms. Obtaining a list of popular
defined terms in a corpus of computational lin-
guistics papers, we find that concepts can of-
ten be categorized into one of three categories:
resources, methodologies and evaluation met-
rics.

1 Introduction

Technical terminology forms a key backbone in
scientific communication. By coining formalized
terminology, scholars convey technical information
precisely and compactly, augmenting the dissemi-
nation of scientific material. Collectively, scholarly

∗This research is supported by the Singapore National Re-
search Foundation under its International Research Centre @
Singapore Funding Initiative and administered by the IDM Pro-
gramme Office.

compilation efforts result in reference sources such
as printed dictionaries, ontologies and thesauri.

While online versions are now common in many
fields, these are still largely compiled manually, re-
lying on costly human editorial effort. This leads
to resources that are often outdated or stale with re-
spect to the current state-of-the-art. Another indirect
result of this leads to a second problem: lexical re-
sources tend to be general, and may contain multi-
ple definitions for a single term. For example, the
term “CRF” connotes “Conditional Random Fields”
in most modern computational linguistics literature;
however, there are many definitions for this acronym
in Wikipedia. Because only one correct sense ap-
plies, readers may need to expend effort to identify
the appropriate meaning of a term in context.

We address both issues in this work by automat-
ically extracting terms and definitions directly from
primary sources: scientific publications. Since most
new technical terms are introduced in scientific pub-
lications, our extraction process addresses the bottle-
neck of staleness. Second, since science is organized
into disciplines and sub-disciplines, we can exploit
this inherent structure to gather contextual informa-
tion about a term and its definition.

Aside from performance improvements, the key
contributions of our work are in 1) recasting the
problem as a sequence labeling task and exploring
suitable learning architectures, 2) our proposal and
validation of the the use of shallow parsing and de-
pendency features to target definition extraction, and
3) analyzing the ACL Anthology Reference Cor-
pus from statistical, chronological and lexical view-
points.

780



2 Related Work

The task of definition mining has attracted a fair
amount of research interest. The output of such sys-
tems can be used to produce glossaries or answer
definition questions. The primary model for this
task in past work has been one of binary classifi-
cation: does a sentence contain a definition or not?
Existing methods can be cast into three main cat-
egories, namely rule-based (Muresan and Klavans,
2002; Westerhout and Monachesi, 2007), supervised
machine learning (Fahmi and Bouma, 2006; Wester-
hout, 2009), and semi-supervised approaches (Nav-
igli and Velardi, 2010; Reiplinger et al., 2012).

Rule-based approaches are intuitive and efficient,
and were adopted in early research. Here, system
performance is largely governed by the quality of
the rules. Muresan and Klavans (2002) developed a
rule-based system to extract definitions from online
medical articles. The system first selects candidates
using hand-crafted cue-phrases (e.g. is defined as,
is called; analogous to “IS-A” phrases), further fil-
tering the candidates with grammar analysis. West-
erhout and Monachesi (2007) augmented the set of
rules with part-of-speech (POS) tag patterns, achiev-
ing an F2 of 0.43.

While such manually-crafted expert rules have
high precision, they typically suffer from low re-
call. Definitions can be expressed in a variety of
ways, making it difficult to develop an exhaustive
set of rules to locate all definition sentences. To ad-
dress low recall, later research adopted data-driven
machine learning approaches. Fahmi and Bouma
(2006) made used of supervised machine learning to
extract definitions from a corpus of Dutch Wikipedia
pages in the medical domain. They showed that
a baseline approach which simply classifies every
first sentence as a definition works surprisingly well,
achieving an accuracy of 75.9% (undoubtedly due to
the regular structure and style for Wikipedia). Their
final system, based on the important feature of sen-
tence position, was augmented with surface-level
features (bag-of-words, bigrams, etc.) and syntac-
tic features (type of determiner, position of the sub-
ject in the sentence). Their study with three different
learners – naı̈ve Bayes, maximum entropy (MaxEnt)
and the support vector machine (SVM) – showed
that MaxEnt gave the best results (92.2% accurate).

Westerhout (2009) worked on a hybrid approach,
augmenting a machine learner to a set of hand-
written rules. A random forest classifier is used to
exploit linguistic and structural features. Informed
by Fahmi and Bouma (2006)’s study, she included
article and noun types in her feature set. Lexico-
structural cues like the layout of the text are also
exploited. She evaluated the performance of dif-
ferent cue phrases including the presence of “IS-
A” phrases, other verbs, punctuations and pronouns.
The highest F2 score of 0.63 is reported for the “IS-
A” pattern.

Since 2009 the focus of the research shifted to
methods not limited to feature engineering. Borg
et al. (2009) implemented a fully automated system
to extract and rank definitions based on genetic al-
gorithms and genetic programming. They defined
two sub-problems including 1) acquiring the rela-
tive importance of linguistic forms, and 2) learn-
ing of new linguistic forms. Starting with a small
set of ten simple hand-coded features, such as hav-
ing sequence “FW IS” (FW is a tag for foreign
word) or containing keyword identified by the sys-
tem, the system is able to learn simple rules such as
“NN is a NN”. However, their system is optimized
for similar “IS-A” patterns, as was used in (West-
erhout, 2009). Their system, achieving an average
f-measure of 0.25, also performs poorer than ma-
chine learning systems which exploit more specific
features.

To cope with the generality of patterns, Navigli
and Velardi (2010) proposed the three-step use of
directed acyclic graphs, called Word-Class Lattices
(WCLs), to classify a Wikipedia dataset of defini-
tions. They first replace the uncommon words in the
sentences with a wildcard (*), generating a set of
“star patterns”. Star patterns are then clustered ac-
cording to their similarity. For each cluster, a WCL
is generated by aligning the sentences in the cluster
to form a generalized sentence. Although they re-
ported a higher precision and recall compared with
previous work, the result for WCL (F1 of 75.23%)
is not significantly better than the baseline system
which exploits only star patterns (F1 of 75.05%)
without generating the directed graphs.

Reiplinger et al. (2012) took a semi-supervised
approach. They employed bootstrapping to ex-
tract glossary sentences from scientific articles in

781



the ACL Anthology Reference Corpus (ACL ARC)
(Bird et al., 2008). Their results show that bootstrap-
ping is useful for definition extraction. Starting from
a few initial term-definition pairs and hand-written
patterns as seeds, their system iteratively acquires
new term-definition pairs and new patterns.

We note that these previous systems rely heav-
ily on lexico-syntactic patterns. They neither suf-
ficiently exploit the intrinsic characteristics of the
term and definition, nor invest effort to localize them
within the sentence1. Given the significant structure
in definitions, we take a more fine-grained approach,
isolating the term and its definition from sentences.
According to Pearson (1996), a definition can be for-
mally expressed as:

X = Y + distinguishing characteristics,

where “X” is the definiendum (defined term; here-
after, term), and “Y + distinguishing characteristics”
can be understood as the definiens (the term’s defini-
tion; hereafter, definition). The connector “=” can be
replaced by a verb such as “define”, “call”, a punc-
tuation mark or other phrase. Our task is thus to find
pairs of terms and their associated definitions in in-
put scholarly articles. The sentence-level task of de-
ciding whether a sentence s is a definition sentence
or not, is thus a simplification of our task.

3 Methodology

Our DefMiner system is based on the sequence la-
beling paradigm – it directly assigns an annotation
ai from A ∈ {(T)erm,(D)efinition,(O)ther} for each
input word wi. We post-process our labeler’s results
to achieve parity with the simplified definition sen-
tence task: When we detect both a term’s and defini-
tion’s presence in a sentence, we deem the sentence
a definition sentence. To be clear, this is a require-
ment; when we detect only either a term or a def-
inition, we filter these out as false positives and do
not include them as system output – by definition in
DefMiner, terms must appear within the same sen-
tence as their definitions.

To train our classifier, we need a corpus of defi-
nition sentences where all terms and definitions are

1While Navigli and Velardi (2010) tagged terms and defini-
tions explicitly in their corpus, their evaluation restricts itself to
the task of definition sentence identification.

annotated. While Navigli and Velardi (2010) com-
piled the WCL definition corpus from the English
Wikipedia pages, we note that Wikipedia has stylis-
tic conventions that make detection of definitions
much easier than in the general case (i.e., “The first
paragraph defines the topic with a neutral point of
view”2). This makes it unsuitable for training a gen-
eral extraction system from scholarly text.

As such, we choose to construct our own dataset
from articles collected from the ACL ARC, follow-
ing (Reiplinger et al., 2012). We compiled a corpus
– the W00 corpus, named for its prefix in the ACL
Anthology – derived from a total of 234 workshop
papers published in 2000. Due to limited resources
and time, only one individual (the first author) per-
formed the corpus annotation. We built three dis-
joint prototype classifiers to further filter the sen-
tences. The prototype classifiers are based on sur-
face patterns, keyphrases and linguistic phenomena
(# of NP, # of VP, etc.). We took all the 2,512 sen-
tences marked as definition sentences by at least one
of our individual prototypes and proceeded to an-
notate all of them. In total, 865 of the total 2,512
sentences were real definition sentences.

The annotation instance is a single token (includ-
ing single word or punctuation mark). Each token
wi was marked with ai from A ∈ {T,D,O} depend-
ing on whether it is part of a term, a definition or
neither (other). Therefore, a sentence that is not a
definition sentence would have all its tokens marked
as O. The corpus and its annotations are available
for comparative study3.

We use Conditional Random Fields (CRFs) (Laf-
ferty et al., 2001) to extract the term and defini-
tion from input. We incorporate evidence (features)
drawn from several levels of granularity: from the
word-, sentence- and document-levels, not limiting
ourselves to the window of previous n words. CRFs
allow us to encode such features which may not be
conditionally independent. We use the open source
implementation, CRF++ (Kudo, 2005) in our work4.

One straightforward approach is to train indepen-
dent classifiers for terms and definitions, which we

2http://en.wikipedia.org/wiki/Wikipedia:
Manual_of_Style/Lead_section

3http://wing.comp.nus.edu.sg/downloads/
term_definition_mining/

4http://code.google.com/p/crfpp/

782



test in Section 4.1. While simple, this is suboptimal
as it ignores the correlation between the presence
of the two components. Term identification (in the
guise of key-word/-phrase extraction) is well studied
and common features such as tf.idf and English or-
thography achieve satisfactory results (Nguyen and
Kan, 2007). In contrast, definitions exhibit more
flexible structure and hence are more difficult to dis-
tinguish from normal English text.

As such, we further investigate a serial archi-
tecture where we perform the classifications in se-
quence. I.e., first utilizing the results from term clas-
sification, and then incorporating them into defini-
tion classification. This two-stage architecture is ex-
plored later in Section 4.2

Our expanded feature set is an amalgamation of
related works and utilizes a mix of simple lexical,
orthography, dictionary lookup and corpus features
(here, idf ). Note that each feature class may derive
more than one feature (e.g., for the POS tag feature,
we extract features from not only the current word
but the surrounding contextual words as well). We
now enumerate the feature classes (FCs) that we
exploit, marking whether they apply to the (W)ord,
(S)entence or (D)ocument levels:

FC1) Lexical (W): The word, POS tag, stemmed word,
and if the word contains a signal suffix5.
FC2) Orthography (W): Whether the word is 1) capital-
ized or 2) mixed case; whether the word contains 3) hy-
phens or 4) digits.
FC3) Keyphrase List (W): Whether the word is in the
keyphrase list of the origin document. We use the open
source KEA keyphrase extraction system (Witten et al.,
1999) to extract 20 keyphrases for each document.
FC4) Corpus (W): Discretized Inverse Document Fre-
quency (idf ), calculated as log(Nc ), where N is the total
number of documents and c is the number of occurrences
of the word in all the documents. IDF values are dis-
cretize into eight uniform partitions.
FC5) Position (D,S): The 1) Section ID, 2) name and 3)
the sentence’s relative position in the document.
FC6) Has acronym (S): Whether the sentence contains an
acronym. We use Stanford dependency parser (Cer et al.,
2010) to parse the sentences. We deem the sentence to
contain an acronym if the dependency type “abbrev” is
present in the output of the parser.
FC7) Surface pattern (S): Whether the sentence contain

5The suffixes we extract are “-ion”,“-ity”,“-tor”,“-ics”,“-
ment”,“-ive” and “-ic”.

<term > defined (as|by) <definition>
define(s)? <term> as <definition>
definition of <term> <definition>
<term> a measure of <definition>
<term> is DT <definition> (that|which|where)
<term> comprise(s)? <definition>
<term> consist(s)? of <definition>
<term> denote(s)? <definition>
<term> designate(s)? <definition>
<definition> (is|are|also) called <term>
<definition> (is|are|also) known as <term>

Table 1: Hand-crafted surface patterns used in DefMiner.

one of the hand-crafted pattern, as listed in Table 1. The
list is compiled from previous works and augmented
based on our observations on the corpus.

Long Distance Features. During develop-
ment, we noticed that the syntactic variation of
the definition might benefit from features that
identify long-distance dependencies. As such, we
further studied the impact of including additional
features developed from the shallow (chunk) and
dependency parses of the input. Compared to
the above features, these features are much more
computationally-intensive.

FC8) Shallow tag (W): Shallow parsing tag for each word
(e.g., np, vp). We used OpenNLP toolkit to shallow parse
the sentences. (Baldridge, 2005)
FC9) Shallow pattern (S): If the shallow parsing sequence
contains one of the seven parse patterns listed in Table 2.
We also give some example sentences which can be de-
tected by the patterns.
FC10) Governor (W): The word that the current word de-
pends on in a binary dependency relation. (e.g., for the
phrase computational linguistics, the governor of the
word computational is linguistics).
FC11) Dependency path distance (W): Distance from the
current word to the root of the sentence in the dependency
tree.
FC12) Typed dependency path (W): The dependency
path from the current word to the root of the sentence
(recording the dependency types instead of the words in
the path).

783



Pattern Example
NP : NP JavaRAP : An open-source implementa-

tion of the RAP
NP is * NP IR is the activity of obtaining informa-

tion resources
NP is * NP
that/of/which

NLP is a subject which is well studied

NP or NP Conditional Random field or CRF tack-
les ...

known as NP The corpus of English Wikipedia pages,
known as EnWiki

NP ( * NP) Hidden Markov Model (HMM) is used
to solve ...

NP defined by/as
* NP

The accuracy is defined by the produc-
tion of ...

Table 2: Hand-crafted shallow parsing patterns used in
DefMiner.

4 Evaluation

We now assess the overall effectiveness of
DefMiner, at both the word and sentence level. Ad-
ditionally, we want to ascertain the performance
changes as we add features to an informed lexical
baseline. We not only benchmark DefMiner’s per-
formance over our own W00 collection, but also
compare DefMiner against previous published work
on the definition sentence identification task on the
WCL (English Wikipedia) corpus.

4.1 Single-Pass Extraction from W00
We run ten-fold cross validation on our annotated
W00 corpus. We first evaluate our results at word
level, calculating the precision, recall, and F1 scores
for each incrementally enhanced feature set. We
present results on the corpus in the top portion of
Table 3 (Rows 1–9).

We calculate both micro and macro- (category)
averaged F1 scores for term and definition extrac-
tion. Fmicro assigns equal weight to every token,
while Fmacro gives equal weight to every category.
As definition tokens greatly outnumber term tokens
in our corpus (roughly 6:1), we feel that the macro-
average is a better indicator of the balance between
term and definition identification.

Our baseline system makes use of basic word
and POS tag sequences as features (FC1), which
are common to baselines in other sequence labeling
works. We can see that most features result in per-
formance improvements to the baseline, especially
for recall. Interestingly, although the shallow pars-
ing and dependency features we use are rather sim-

ple, they effectively improve the performance of the
system. In System 7, we only use the seven shallow
parsing patterns shown in Table 2, but the Fmacro
measure improves 3%. Our best single-stage sys-
tem (System 9 in Table 3) boosts recall for term and
definition classification by 7% and 5%, respectively,
without sacrificing precision. The Fmacro measure
is improved from 0.44 to 0.48.

Unexpectedly, the inclusion of the position fea-
tures cause performance to drop. One possible rea-
son is that the authors of scientific papers have
more flexibility to choose the positions to present
definitions. This makes the position feature much
less indicative (compared to running on a corpus of
Wikipedia articles). Due to this observation, we ex-
clude the position features when carrying out fol-
lowing experiments.

4.2 Serial Term and Definition Classification
We now investigate the two-stage, serial archi-
tecture where the system first performs term
classification before definition classification (i.e.,
term�definition). We provision the second-stage
definition classifier with three additional features
from the first-stage term classification output:
whether the current word (1) is a term, and (2) ap-
pears before or (3) after a term.

Row 10 shows this resulting system, which we
coin as DefMiner. Interestingly, there is a 10% in-
crease in the precision of definition classification.
With the two-stage classifier, Fmacro score further
increases from 0.48 to 0.51. The results verify our
intuition that term classification does help in defini-
tion classification. Pipelining in the opposite direc-
tion (definition�term; Row 11) does not show any
improvement. We posit that since the advantage is
only in a single direction, joint inference may be less
likely to yield benefits.

To determine the upper bound performance that
could result from proper term identification, we pro-
vided correct, oracular term labels from our ground
truth annotations in our corpus to the second-stage
definition classifier. This scenario effectively upper-
bounds the performance that perfect term knowledge
has on definition classification. The results of this
system in Row 12 indicates a strong positive influ-
ence on definition extraction, improving definition
extraction from 49% to 80%, a leap of 31%. This

784



System / Feature Class (cf Section 3) Term Definition Overall
P R F1 P R F1 Fmicro Fmacro

1: Baseline (FC1) 0.49 0.34 0.40 0.41 0.49 0.45 0.45 0.44
2: (1) + Orthography (FC2) 0.46 0.35 0.40 0.42 0.51 0.46 0.46 0.44
3: (2) + Dictionary (FC3) 0.48 0.36 0.41 0.41 0.49 0.44 0.44 0.43
4: (3) + Corpus (FC4) 0.50 0.35 0.41 0.40 0.52 0.45 0.45 0.44
5: (4) + Position (FC5) 0.47 0.37 0.42 0.36 0.48 0.41 0.41 0.41
6: (4) + Shallow parsing tag (FC8) 0.51 0.38 0.43 0.41 0.50 0.45 0.45 0.44
7: (6) + Shallow parse pattern (FC9) 0.50 0.40 0.45 0.42 0.52 0.47 0.47 0.47
8: (7) + Surface pattern (FC7) 0.49 0.39 0.44 0.43 0.53 0.48 0.48 0.47
9: (8) + Dependency + acronym
(FC6,10,11,12)

0.50 0.41 0.45 0.45 0.54 0.49 0.49 0.48

10 [DefMiner]: (9) + 2-stage 0.50 0.41 0.45 0.55 0.58 0.56 0.55 0.51
11: (9) + Reverse 2-stage 0.50 0.40 0.44 0.45 0.54 0.49 0.49 0.48
12: (9) + Term Oracle N/A N/A N/A 0.79 0.82 0.80 N/A N/A

Table 3: 10-fold cross validation word-level performance over different system configurations on our W00 corpus.

motivates future work as how to improve the perfor-
mance of the term classifier so as to reap the benefits
possible with our two-stage classifier.

4.3 Comparative results over the WCL Dataset

For most of the related research reviewed, we could
neither obtain their source code nor the corpora used
in their work, making comparative evaluation diffi-
cult. To the best of our knowledge, Reiplinger et
al. (2012) is the only attempt to extract definitions
from the ACL ARC corpus, which is a superset of
our W00 corpus. It would be desirable to have a di-
rect comparison with their work, but their evaluation
method is mainly based on human judges and their
reported coverage of 90% is only for a sample, short
list of domain terms they defined in advance.

To directly compare with the previous, more com-
plex state-of-the-art system from (Navigli and Ve-
lardi, 2010), we evaluate DefMiner on the defini-
tion sentence detection task. For the sentence-level
evaluation, we calculate the P/R/F1 score based on
whether the sentence is a definition sentence. We
applied DefMiner on their whole WCL annotated
corpus, reporting results in Table 4. We random-
ized the definition and none-definition sentences in
their corpus and applied 10-folds cross validation.
In each each iteration we used 90% of the sentences
for training and 10% for testing.

Compared to their results reported in (Navigli and

System Token Level Sentence
Term Definition Level

P / R / F1 P / R / F1 P / R / F1
DefMiner .82/.78/.80 .82/.79/.81 .92/.79/.85
N&V ’10 – / – / – – / – / – .99/.61/.77

Table 4: Comparative performance over the WCL.

Velardi, 2010), DefMiner improves overall F1 by
8%. While certainly less precise (precision of 92%
versus 99%), recall is improved over their consid-
erably more complex WCL-3 algorithm by almost
20%. Even using just the simple heuristic of only
classifying sentences that have identified terms as
well as definitions as definition sentences, DefMiner
serves to competitively identify definition sentences.

4.4 Manual Inspection of DefMiner Output

To gauge the noise from our system outside of our
cross-validation experiments, we conducted a man-
ual inspection of results over other workshop papers
from other years (2001 and 2002), as a sanity check.
DefMiner identifies 703 and 1,217 sentences in W01
and W02 as definition sentences separately.

Overall, 77.8% of the extracted sentences are real
definition sentences, while the remaining are false
positives.

785



4.4.1 Analysis of Common Errors

The P/R/F1 score by itself only gives a hint
of the system’s overall performance. We are also
interested to study the common errors made by our
system, which could help us engineer better features
for improving DefMiner. As our two-stage classifier
still lags behind the system with oracular term
labels by 24% in F1 for definition detection (Sec-
tion 4.2), we believe there is still much room for
improvement. We show three example misclassified
sentences that represent the major types of errors
we observed, where DefMiner’s output annotations
follow tokens marked as part of terms or definitions.

1) A PSS/TERM thus contains abstract linguistic values
for closed features ( tense/DEF ,/DEF mood/DEF ,/DEF
voice/DEF ,/DEF number/DEF ,/DEF gender/DEF
,/DEF etc/DEF ./DEF ) .

This first instance shows that DefMiner tends to
mark the first several tokens as “TERM” while the
real term appears somewhere else in the sentence.
The actual term being defined is “closed features”
instead of “PSS”. Many terms in the training set
appear at the beginning of the sentence and are
preceded by a determinant. “PSS” is also likely
to receive a high IDF and orthographic shape
(capitalized) score and therefore are misclassified
as terms. It may be useful to thus model the (usual)
distance between the term and its definition in a
feature in future work.

2) Similarly , ‘I’/TERM refers to an/DEF interior/DEF
character/DEF and/DEF ‘L’/DEF indicates/DEF
the/DEF last/DEF character/DEF of/DEF a/DEF
word/DEF .

DefMiner is occasionally confused when encoun-
tering recursive definition or multiple definitions in
a single sentence. Sentence 2 contains two parallel
definitions. The classifier fails to classify “L”
as a separate term, incorporating it as part of the
definition. One possible improvement is to break the
original sentence into clauses that are independent
from each other, perhaps by using even simple
surface cues such as coordinating conjunctions
marked by commas or “and”.

3) Again one could argue that the ability to convey
such uncertainty and reliability information to a non-
specialist/TERM is a/DEF key/DEF advantage/DEF
of/DEF textual/DEF summaries/DEF over/DEF
graphs/DEF .

Another difficult problem faced by the classifier
is the lack of contextual information. In sentence
3), if we just look at part of the sentence “a non-
specialist is a key advantage of textual summaries
over graphs”, without trying to understand the mean-
ing of the sentence, we may well conclude that it is
a definition sentence because of the cue phrase “is
a”. But clearly, the whole sentence is not a defini-
tion sentence. More sophisticated features based on
the sentence parse tree have to be exploited to detect
such false positive examples.

5 Insights from the Definitions Extracted
from the ACL ARC

In this second half of the paper, we apply DefMiner
to gain insights on the distributional and lexical
properties of terms and definitions that appear in
the large corpus of computational linguistics publi-
cations represented by the ACL ARC (Bird et al.,
2008). The ARC consists of 10,921 scholarly publi-
cations from ACL venues, of which our earlier W00
corpus is a subset (n.b., as such, there is a small
amount of overlap). We trained a model using the
whole of the W00 corpus and used the obtained clas-
sifier to identify a list of terms and definitions for
each publication in the ACL ARC.

Inspecting such output gives us an understanding
of the properties of definition components, eventu-
ally helping the community to define better features
to capture them, as well as intrinsically deepening
our knowledge of the natural language of definitions
and the structure of scientific discourse.

5.1 Demographics

From a term’s perspective we can introspect prop-
erties of the enclosing paper, the host sentence, the
term itself and its definition.

At the document level, we can analyze document
metadata: its venue (journal, conference or work-
shop published) and year of publication.

786



At the sentence level, we analyze the position of
the sentences that are definition sentences.

Focusing on terms, we want to find out in more
detail the technical terminology that is defined. Are
they different from general keyphrases? What type
of entities are defined? What words do these terms
consist of? What structures are common?

We are interested in analogous questions when
focusing on the accompanying definitions. How
many words or clauses do definition sentences
consist of? Do we lose a lot of recall by restricting
definitions to a single sentence? Are embedded
definitions (definitions embedded with other defini-
tions) common?

We highlight some specific findings from our ba-
sic analyses here:

Where do definitions occur? As terms are usu-
ally defined on first use, we expect the distribution
of definition sentences to skew towards the begin-
ning portions of a scientific document as input. We
count the occurrences of definition sentences in each
of ten equally-sized (by number of sentences) non-
overlapping partitions. The results are shown in Fig-
ure 1, aligning with our intuition: The first three
quantiles contribute almost 40% of all detected def-
inition sentences, while the last three quantiles con-
tain only 17.8%.

5898	   6258	   6574	   6202	   5512	   5020	   4341	   3593	   2931	   2233	  

0	  
2000	  
4000	  
6000	  
8000	  

1	   2	   3	   4	   5	   6	   7	   8	   9	   10	  

Fr
eq

ue
nc
y	  

Quan,le	  (1	  =	  first	  10%;	  10	  =	  last	  10%)	  

Figure 1: Occurrences of definitions within different seg-
ments of an article.

How long are the detected terms and defini-
tions? Figure 2 shows the detected aggregate dis-
tributions. Over 54% of the detected terms are sin-
gle tokens, where majority of the remaining 45% of
terms being multi-word terms of six words or less.
Among the single token terms, a further analysis
reveals that 17.4% are detected as single-character
variables, 34.9% are acronyms (consisting of more
than one capitalized letters), while the remaining
47.7% are normal words. Definitions, in contrast,

are longer and more varied, with a peak length of
nine words. Slightly over half of the definitions have
a length of 5–16 words; 75% have lengths between
3 and 23 words.

54.15	  

27.84	  

8.9	   4.29	   2.4	   1.32	   0.65	   0.33	   0.23	   0.15	  
0	  
10	  
20	  
30	  
40	  
50	  
60	  

1	   2	   3	   4	   5	   6	   7	   8	   9	   10	  

Pe
rc
en

ta
ge
	  

Number	  of	  Words	  

(t) Term length distribution.

0	  

1	  

2	  

3	  

4	  

5	  

6	  

1	   6	   11	   16	   21	   26	   31	   36	   41	   46	   51	   56	  

Pe
rc
en

ta
ge
	  

Number	  of	  Words	  

(b) Definition length distribution.

Figure 2: Length distributions of (top) terms, (bottom)
definitions.

In Table 5, we present the 10 most frequent POS
tag bigrams for terms and definitions. We can see
that among terms, a sequence of consecutive two
nouns is most common, making up four out of the
top five bigrams. We notice that determiners and
prepositions are absent from the term list but are
common in definitions.

5.2 Inspection of Definition over Time

The ACL ARC covers journal articles, conference
and workshop proceedings over a few decades. As
with other fields in recent years, the amount of
computational linguistics literature has steadily in-
creased over the number of years.

We study if definitions appear as frequently in dif-
ferent types of scientific articles (e.g. journals, con-
ference papers or workshop papers). We also want
to investigate if there is a significant shift in the dis-
tribution of definitions across years. In Figure 3,
we present the density of definitions (defined as the

787



Term Definition
NNP NNP DT NN
NN NN NNP NNP
NNP NN NN IN
NN NNP IN DT
JJ NN NN NN
NN JJ JJ NN
NNP : NN :
: NNP DT JJ
NN NNS NNS IN
NN ” NN NNS

Table 5: Most frequent POS bigrams for terms and defi-
nitions.

percentage of sentences that are identified as defini-
tion sentences), for these three different categories
of publications6.

In Figure 3, the three data series overlap each
other, so we cannot conclude definitions appear
more often in one type of papers than another. How-
ever, as a side effect, we see that while the definition
density for journal papers remain relatively constant,
for conference and workshop papers the number of
definitions extracted per sentence has increased no-
ticeably over time. The average number of defini-
tions presented in conference papers, for instance,
increased more than 100% in the 40 years repre-
sented in the ACL ARC.

The increasing number of definitions alone does
not show that new knowledge is introduced at a
faster rate, as definitions may be repeated. To con-
trol for this effect, we also need to know which def-
initions are new or defined in previous year(s). We
studied this effect in more detail for the relatively
smaller set of journal papers (Figure 4). For jour-
nal papers, the number of definitions of previously
introduced terms in each year against the number of
new definitions. We say a definition is new when the
detected term was not identified in any article (not
limited to journals) in previous years.

We see that the number of new terms being de-
fined also increases with the years. But the increase
is much slower than that for the total definitions. The
area between the two lines denotes the definitions

6The ACL ARC is organized by the venue of the publication,
which is associated to a category. For the assigned category for
each venue please refer to Appendix A.

where the same term has been multiply defined from
the same or previous years as the current year under
investigation.

5.3 Trends
We can use terms and definitions to also intro-
spect how the computational linguistics literature
has changed over time. Table 6 shows a subset of
the most frequently defined terms in the ACL ARC,
where we exclude single-character terms (“vari-
ables”).

WordNet (292) Part Of Speech (45)
Precision (172) Probablistic CFG (43)
Recall (167) FrameNet (38)
Noun phrase (97) Conditional Random Field (29)
Word sense disambiguation (60) Inverse document frequency (28)
Support Vector Machine (60) PropBank (27)
Hidden Markov Model (54) Context Free Grammar (25)
Latent Semantic Analysis (57) Accuracy (20)

Table 6: Subset of most frequently defined terms. Raw
counts in parentheses. Variations of the same term (e.g.
plurals, acronyms) are collapsed into one instance.

To be expected, these popular terms are mostly
specific to computational linguistics. From our
observation, we can fit these terms into one of
three categories, including 1) resources (WordNet,
FrameNet, PropBank), 2) methodologies (SVM,
HMM, LSA), and 3) evaluation metrics (Precision,
Recall, Accuracy). We feel that the final category
of evaluation metrics is more general and would be
shared among other scientific disciplines.

An interesting analysis that follows from this cat-
egorization is that we can study major trends and
changes in the research directions of the commu-
nity. This can help to draw the attention of re-
searchers to emerging trends. We illustrate this ap-
proach in Figure 5 that focuses on three sequence
labeling methodologies that have been used to ad-
dress similar problems – namely, hidden Markov
model (HMM), maximum entropy Markov model
(MEMM), and conditional random fields (CRF) –
during the period from 1989 to 2006 (where we have
sufficient data points). From the early 90s, we see
that HMM was a clear favorite. However since 2000,
MEMM gained in popularity and use. Lafferty et
al. (2001) introduced CRFs in 2001 and the new
methodology was widely adopted soon after that.

788



0.000	  

0.005	  

0.010	  

0.015	  

0.020	  

0.025	  

0.030	  

0.035	  

1967	   1973	   1979	   1982	   1987	   1993	   1999	   2005	  

De
fin

i&
on

	  D
en

si
ty
	  

Year	  

Conference	  Papers	   Journals	   Workshops	  

Figure 3: Occurrences of definitions across publication cat-
egories.

0	  

50	  

100	  

150	  

200	  

250	  

300	  

350	  

1980	   1982	   1985	   1988	   1992	   1996	   2001	   2004	  

#	  
Co

lle
ct
ed

	  D
efi

ni
.o

ns
	  

Year	  

New	  Terms	   Previously	  Defined	  Terms	  

Figure 4: Relative proportions of new and recurring defini-
tions in journal papers.

Figure 5: The occurrence of definitions for various se-
quence labeling methodologies over the years.

6 Conclusions and Future Work

We study the task of identifying definition sentences,
as a two-part entity containing a term and its ac-
companying definition. Unlike previous work, we
propose the harder task of delimiting the component
term and definitions, which admits sequence label-
ing methodologies as a compatible solution.

Leveraging the current best practice of using con-
ditional random fields, we contribute two additional
ideas that lead to DefMiner, a state-of-the-art schol-
arly definition mining system. First, we show that
shallow parsing and dependency parse features that
may provide additional non-local information, are
useful in improving task performance. Second,
viewing the problem as two correlated subproblems
of term and definition extraction, we measure the
tightness and dependency of the correlation. We

find that a two-stage sequential learning architecture
(term first, definition second) leads to best perfor-
mance. DefMiner outperforms the state of the art,
and we feel is fit for macroscopic analysis of scien-
tific corpora, despite significant noise.

We thus deployed DefMiner on the ACL An-
thology Reference Corpus. We demonstrate how
DefMiner can yield insights into both the struc-
ture and semantics of terms and definitions, in both
static and diachronic modes. We think future work
could pursue more in-depth analysis of the distribu-
tional and demographic properties of automatically
extracted lexica. We can use the lexicon obtained
from different years to carry out trend prediction,
which we have illustrated here. Downstream sys-
tems may predict which term will become popular,
or could alert an author if their definition of a term
significantly differs from the original source.

We hope to tackle the annotation bottleneck in fu-
ture work on definition extraction, common in many
data-driven learning fields. We plan to explore iter-
ative, semi-supervised methods to best manage hu-
man effort to maximize the effectiveness of future
annotation.

In addition, with respect to modeling, although
we showed that doing definition classification before
term classification does not improve over our single-
stage classifier, we hope to study whether suitable
joint inference models can benefit from the interac-
tion between the two classification processes.

789



References

Jason Baldridge. 2005. The opennlp project.
Steven Bird, Robert Dale, Bonnie J. Dorr, Bryan Gib-

son, Mark T. Joseph, Min-Yen Kan, Dongwon Lee,
Brett Powley, Dragomir R. Radev, and Yee Fan Tan.
2008. The ACL anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation Conference (LREC08), pages 1755–1759, Mar-
rakech, Morocco.

Claudia Borg, Mike Rosner, and Gordon Pace. 2009.
Evolutionary algorithms for definition extraction. In
Proceedings of the 1st Workshop on Definition Extrac-
tion, WDE ’09, pages 26–32, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Val-
letta, Malta.

Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL workshop on Learning Structured In-
formation in Natural Language Applications, Trento,
Italy.

Taku Kudo. 2005. Crf++: Yet another crf toolkit. Soft-
ware available at http://crfpp. sourceforge. net.

John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, ICML ’01, pages 282–
289, San Francisco, CA, USA.

Smaranda Muresan and Judith Klavans. 2002. A method
for automatically building and evaluating dictionary
resources. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Las Palmas, Ca-
nary Islands, Spain.

Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318–1327, Uppsala, Sweden.

Thuy Dung Nguyen and Min-Yen Kan. 2007. Keyphrase
extraction in scientific publications. In Proceedings of
International Conference on Asian Digital Libraries,
pages 317–326, Hanoi, Vietnam.

Jennifer Pearson. 1996. The expression of definitions in
specialised texts: a corpus-based analysis. In Proceed-
ings of Euralex 96.

Melanie Reiplinger, Ulrich Schafer, and Magdalena Wol-
ska. 2012. Extracting glossary sentences from
scholarly articles: a comparative evaluation of pattern
bootstrapping and deep analysis. In Proceedings of
the ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries, ACL ’12, pages 55–65, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Eline Westerhout and Paola Monachesi. 2007. Extrac-
tion of dutch definitory contexts for elearning pur-
poses. In Proceedings of Computational Linguistics
(CLIN 2007).

Eline Westerhout. 2009. Definition extraction using
linguistic and structural features. In Proceedings of
the 1st Workshop on Definition Extraction, WDE ’09,
pages 61–67, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Ian H Witten, Gordon W Paynter, Eibe Frank, Carl
Gutwin, and Craig G Nevill-Manning. 1999. Kea :
Practical automatic keyphrase extraction. Computer,
pp:254–255.

790


