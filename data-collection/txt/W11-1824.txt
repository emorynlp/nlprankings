










































Double Layered Learning for Biological Event Extraction from Text


Proceedings of BioNLP Shared Task 2011 Workshop, pages 153–154,
Portland, Oregon, USA, 24 June, 2011. c©2011 Association for Computational Linguistics

Double Layered Learning for Biological Event Extraction from Text

Ehsan Emadzadeh, Azadeh Nikfarjam, Graciela Gonzalez
Arizona State University / Tempe, AZ 85283, USA

ehsan.emadzadeh@asu.edu, azadeh.nikfarjam@asu.edu
graciela.gonzalez@asu.edu

Abstract

This paper presents our approach (referred to
as BioEvent) for protein-level complex event
extraction, developed for the GENIA task
(Kim et al., 2011b) of the BioNLP Shared
Task 2011 (Kim et al., 2011a). We devel-
oped a double layered machine learning ap-
proach which utilizes a state-of-the-art mini-
mized feature set for each of the event types.
We improved the best performing system
of BioNLP 2009 overall, and ranked first
amongst 15 teams in finding “Localization”
events in 201112. BioEvent is available at
http://bioevent.sourceforge.net/

1 Introduction

A biological event refers to a specific kind of inter-
action between biological entities. Events consist
of two parts: event triggers and event arguments.
Event extraction can be very challenging when deal-
ing with complex events with multiple or nested ar-
guments; for example, events themselves can be an
argument for other events.

2 Methods

In general, to detect an event mentioned in text, the
event trigger should be identified first, then comple-
mented with event arguments. We divided the train-
ing and testing tasks into two phases: trigger detec-
tion and argument detection.

1Using the “Approximate Span without Event Trigger
Matching/Approximate Recursive” metric

2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/-
SharedTask/evaluation.shtml

2.1 Event Trigger Detection

The trigger detection problem can be modeled as
a multi-class classification of a word or combina-
tion of words (phrase). Instead of using all possible
phrases in the training text as examples for the clas-
sifier, we only included those that were known trig-
gers in the training set. For the official shared task
submission we used SV M light (Joachims, 1999).
Detailed explanation of the trigger detection process
includes three main steps: pre-processing, training
of the SVM models, and combining SVM results.

Pre-processing. All tokenized documents pro-
vided by the shared task organizers (Stenetorp et al.,
2011) were converted to database records. Then dif-
ferent sets of attributes were defined and calculated
for words, sentences and documents.

Training SVM models and Combining Results.
We trained 9 different binary SVM models using
one-vs-many approach. One of the challenging tasks
was to compare the results of different SVM models,
given that each had different feature sets and their
confidence values were not directly comparable and
needed to be calibrated properly before comparing.
We tried three approaches: 1) selecting the SVM re-
sult with highest positive distance to hyperplane, 2)
using a trained decision tree and 3) using another
SVM trained for voting. Model J48 from the WEKA
library (Hall et al., 2009) was trained based on SVM
distances for the training set examples and expected
outputs. In the third approach, we tried SVM for
voting, which generated better results than the deci-
sion tree. Last two approaches consist of two layers
of classifiers which first layer includes event types
classifiers and second layer generates final decision

153



Event type Bioevent Turku09
Gene expression 71.88 70.84
Transcription 47.62 47.14
Protein catabolism 60.87 60.87
Phosphorylation 75.14 73.39
Localization 61.49 59.68
Binding 34.42 35.97
Regulation 24.03 22.26
Positive regulation 33.41 31.84
Negative regulation 18.89 18.58
ALL-TOTAL 44.69 43.54

Table 1: F-Value from our BioEvent system compared to
Turku09 (Bjorne et al., 2009) results, using Approximate
Span/Approximate Recursive matching

based on first layer outputs.

2.2 Arguments detection and Post-processing

Similar to trigger detection, argument detection can
be modeled for a classification task by assigning an
argument type label to each possible combination
of an event trigger and a biological entity in a sen-
tence. We obtained entities from a1 files, as well as
the supportive analysis data provided by the shared
task organizers (Bjorne et al., 2009). After gener-
ating events using SVM classification, we merged
them with the output from the Turku system to gen-
erate the final result. For common events (detected
by both systems) we used the arguments detected by
the Turku system.

3 Results

Since we tried to improve upon the best performing
system in the 2009 competition (Turku09), we com-
pare the results of our system and Turku09’s on the
2011 test set. Table 1 shows the performance of our
proposed system and that of Turku09. We see that
Binding was our worst event (negative change), Lo-
calization the most improved, no change for Protein
Catabolism, and only a slight improvement in Neg-
ative Regulation.

4 Conclusion and future work

In this research we focused on event trigger detec-
tion by applying a SVM-based model. SVM is very
sensitive to parameters and further tuning of param-

eters can improve the overall result. Furthermore,
we want to evaluate our method independently and
find the contribution of each modification to the fi-
nal result. Our method is generalizable to other do-
mains by using proper train-set and finding useful
attributes for new event types.

Acknowledgments
The authors would like to thank Ryan Sullivan
for his helps during this research. EE and GG
acknowledge partial funding from NLM Contract
HHSN276201000031C.

References
Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,

Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Computational Linguistics,
(June):10–18.

M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10–18.

T. Joachims. 1999. Making large scale SVM learn-
ing practical. Advances in Kernel Methods - Support
Vector Learnin, (B. Schölkopf and C. Burges and A.
Smola (ed.)).

Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun’ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.

Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.

Pontus Stenetorp, Goran Topić, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun’ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.

154


