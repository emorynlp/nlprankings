



















































A Discrete CVAE for Response Generation on Short-Text Conversation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1898â€“1908,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

1898

A Discrete CVAE for Response Generation on Short-Text Conversation

Jun Gao1âˆ—, Wei Bi2â€ , Xiaojiang Liu2, Junhui Li1, Guodong Zhou1, Shuming Shi2
1School of Computer Science and Technology, Soochow University, Suzhou, China

imgaojun@gmail.com, {lijunhui,gdzhou}@suda.edu.cn
2Tencent AI Lab, Shenzhen, China

{victoriabi, kieranliu, shumingshi}@tencent.com

Abstract

Neural conversation models such as encoder-
decoder models are easy to generate bland and
generic responses. Some researchers propose
to use the conditional variational autoencoder
(CVAE) which maximizes the lower bound on
the conditional log-likelihood on a continu-
ous latent variable. With different sampled la-
tent variables, the model is expected to gen-
erate diverse responses. Although the CVAE-
based models have shown tremendous poten-
tial, their improvement of generating high-
quality responses is still unsatisfactory. In this
paper, we introduce a discrete latent variable
with an explicit semantic meaning to improve
the CVAE on short-text conversation. A major
advantage of our model is that we can exploit
the semantic distance between the latent vari-
ables to maintain good diversity between the
sampled latent variables. Accordingly, we pro-
pose a two-stage sampling approach to enable
efficient diverse variable selection from a large
latent space assumed in the short-text conver-
sation task. Experimental results indicate that
our model outperforms various kinds of gener-
ation models under both automatic and human
evaluations and generates more diverse and in-
formative responses.

1 Introduction

Open-domain response generation (Perez-Marin,
2011; Sordoni et al., 2015) for single-round short
text conversation (Shang et al., 2015), aims at
generating a meaningful and interesting response
given a query from human users. Neural gener-
ation models are of growing interest in this topic
due to their potential to leverage massive conver-
sational datasets on the web. These generation
models such as encoder-decoder models (Vinyals

âˆ—Work done when Jun Gao was interning at Tencent AI
Lab.

â€ Corresponding author

and Le, 2015; Shang et al., 2015; Wen et al.,
2015), directly build a mapping from the in-
put query to its output response, which treats all
query-response pairs uniformly and optimizes the
maximum likelihood estimation (MLE). However,
when the models converge, they tend to output
bland and generic responses (Li et al., 2016a,c;
Serban et al., 2016).

Many enhanced encoder-decoder approaches
have been proposed to improve the quality of gen-
erated responses. They can be broadly classified
into two categories (see Section 2 for details):
(1) One that does not change the encoder-decoder
framework itself. These approaches only change
the decoding strategy, such as encouraging diverse
tokens to be selected in beam search (Li et al.,
2016a,b); or adding more components based on
the encoder-decoder framework, such as the Gen-
erative Adversarial Network (GAN)-based meth-
ods (Xu et al., 2017; Zhang et al., 2018; Li et al.,
2017) which add discriminators to perform adver-
sarial training; (2) The second category modifies
the encoder-decoder framework directly by incor-
porating useful information as latent variables in
order to generate more specific responses (Yao
et al., 2017; Zhou et al., 2017). However, all these
enhanced methods still optimize the MLE of the
log-likelihood or the complete log-likelihood con-
ditioned on their assumed latent information, and
models estimated by the MLE naturally favor to
output frequent patterns in training data.

Instead of optimizing the MLE, some re-
searchers propose to use the conditional varia-
tional autoencoder (CVAE), which maximizes the
lower bound on the conditional data log-likelihood
on a continuous latent variable (Zhao et al., 2017;
Shen et al., 2017). Open-domain response gener-
ation is a one-to-many problem, in which a query
can be associated with many valid responses. The
CVAE-based models generally assume the latent



1899

variable follows a multivariate Gaussian distribu-
tion with a diagonal covariance matrix, which can
capture the latent distribution over all valid re-
sponses. With different sampled latent variables,
the model is expected to decode diverse responses.
Due to the advantage of the CVAE in modeling the
response generation process, we focus on improv-
ing the performance of the CVAE-based response
generation models.

Although the CVAE has achieved impressive
results on many generation problems (Yan et al.,
2016; Sohn et al., 2015), recent results on response
generation show that the CVAE-based generation
models still suffer from the low output diversity
problem. That is multiple sampled latent variables
result in responses with similar semantic mean-
ings. To address this problem, extra guided sig-
nals are often used to improve the basic CVAE.
Zhao et al. (2017) use dialogue acts to capture
the discourse variations in multi-round dialogues
as guided knowledge. However, such discourse
information can hardly be extracted for short-text
conversation.

In our work, we propose a discrete CVAE (DC-
VAE), which utilizes a discrete latent variable with
an explicit semantic meaning in the CVAE for
short-text conversation. Our model mitigates the
low output diversity problem in the CVAE by ex-
ploiting the semantic distance between the latent
variables to maintain good diversity between the
sampled latent variables. Accordingly, we propose
a two-stage sampling approach to enable efficient
selection of diverse variables from a large latent
space assumed in the short-text conversation task.

To summarize, this work makes three contri-
butions: (1) We propose a response generation
model for short-text conversation based on a DC-
VAE, which utilizes a discrete latent variable with
an explicit semantic meaning and could generate
high-quality responses. (2) A two-stage sampling
approach is devised to enable efficient selection
of diverse variables from a large latent space as-
sumed in the short-text conversation task. (3) Ex-
perimental results show that the proposed DCVAE
with the two-stage sampling approach outper-
forms various kinds of generation models under
both automatic and human evaluations, and gen-
erates more high-quality responses. All our code
and datasets are available at https://ai.tencent.com/
ailab/nlp/dialogue.

2 Related Work

In this section, we briefly review recent advance-
ment in encoder-decoder models and CVAE-based
models for response generation.

2.1 Encoder-decoder models

Encoder-decoder models for short-text conver-
sation (Vinyals and Le, 2015; Shang et al.,
2015) maximize the likelihood of responses given
queries. During testing, a decoder sequentially
generates a response using search strategies such
as beam search. However, these models frequently
generate bland and generic responses.

Some early work improves the quality of gener-
ated responses by modifying the decoding strat-
egy. For example, Li et al. (2016a) propose to
use the maximum mutual information (MMI) to
penalize general responses in beam search during
testing. Some later studies alter the data distri-
butions according to different sample weighting
schemes, encouraging the model to put more em-
phasis on learning samples with rare words (Naka-
mura et al., 2018; Liu et al., 2018). As can be seen,
these methods focus on either pre-processing the
dataset before training or post-processing the re-
sults in testing, with no change to encoder-decoder
models themselves.

Some other work use encoder-decoder models
as the basis and add more components to refine
the response generation process. Xu et al. (2017)
present a GAN-based model with an approximate
embedding layer. Zhang et al. (2018) employ
an adversarial learning method to directly opti-
mize the lower bounder of the MMI objective (Li
et al., 2016a) in model training. These models
employ the encoder-decoder models as the gen-
erator and focus on how to design the discrimi-
nator and optimize the generator and discrimina-
tor jointly. Deep reinforcement learning is also
applied to model future reward in chatbot after
an encoder-decoder model converges (Li et al.,
2016c, 2017). The above methods directly inte-
grate the encoder-decoder models as one of their
model modules and still do not actually modify the
encoder-decoder models.

Many attentions have turned to incorporate use-
ful information as latent variables in the encoder-
decoder framework to improve the quality of gen-
erated responses. Yao et al. (2017) consider that
a response is generated by a query and a pre-
computed cue word jointly. Zhou et al. (2017) uti-

https://ai.tencent.com/ailab/nlp/dialogue
https://ai.tencent.com/ailab/nlp/dialogue


1900

lize a set of latent embeddings to model diverse
responding mechanisms. Xing et al. (2017) intro-
duce pre-defined topics from an external corpus to
augment the information used in response gener-
ation. Gao et al. (2019) propose a model that in-
fers latent words to generate multiple responses.
These studies indicate that many factors in con-
versation are useful to model the variation of a
generated response, but it is nontrivial to extract
all of them. Also, these methods still optimize the
MLE of the complete log-likelihood conditioned
on their assumed latent information, and the model
optimized with the MLE naturally favors to out-
put frequent patterns in the training data. Note
that we apply a similar latent space assumption as
used in (Yao et al., 2017; Gao et al., 2019), i.e.
the latent variables are words from the vocabulary.
However, they use a latent word in a factorized
encoder-decoder model, but our model uses it to
construct a discrete CVAE and our optimization
algorithm is entirely different from theirs.

2.2 The CVAE-based models

A few works indicate that it is worth trying to
apply the CVAE to dialogue generation which is
originally used in image generation (Yan et al.,
2016; Sohn et al., 2015) and optimized with the
variational lower bound of the conditional log-
likelihood. For task-oriented dialogues, Wen
et al. (2017) use the latent variable to model in-
tentions in the framework of neural variational in-
ference. For chit-chat multi-round conversations,
Serban et al. (2017) model the generative process
with multiple levels of variability based on a hier-
archical sequence-to-sequence model with a con-
tinuous high-dimensional latent variable. Zhao
et al. (2017) make use of the CVAE and the latent
variable is used to capture discourse-level varia-
tions. Gu et al. (2019) propose to induce the la-
tent variables by transforming context-dependent
Gaussian noise. Shen et al. (2017) present a con-
ditional variational framework for generating spe-
cific responses based on specific attributes. Yet,
it is observed in other tasks such as image cap-
tioning (Wang et al., 2017) and question genera-
tion (Fan et al., 2018) that the CVAE-based gen-
eration models suffer from the low output diver-
sity problem, i.e. multiple sampled variables point
to the same generated sequences. In this work,
we utilize a discrete latent variable with an inter-
pretable meaning to alleviate this low output di-

versity problem on short-text conversation.
We find that Zhao et al. (2018) make use of a

set of discrete variables that define high-level at-
tributes of a response. Although they interpret
meanings of the learned discrete latent variables
by clustering data according to certain classes (e.g.
dialog acts), such latent variables still have no ex-
act meanings. In our model, we connect each la-
tent variable with a word in the vocabulary, thus
each latent variable has an exact semantic mean-
ing. Besides, they focus on multi-turn dialogue
generation and presented an unsupervised discrete
sentence representation learning method learned
from the context while our concentration is pri-
marily on single-turn dialogue generation with no
context information.

3 Proposed Models

3.1 DCVAE and Basic Network Modules

Following previous CVAE-based generation mod-
els (Zhao et al., 2017), we introduce a latent vari-
able z for each input sequence and our goal is to
maximize the lower bound on the conditional data
log-likelihood p(y|x), where x is the input query
sequence and y is the target response sequence:

log p(y|x) â‰¥ Ezâˆ¼q(z|y,x)[log p(y|x, z)]
âˆ’DKL(q(z|y,x)||p(z|x)). (1)

Here, p(z|x)/q(z|y,x)/p(y|x, z) is parameterized
by the prior/posterior/generation network respec-
tively. DKL(q(z|y,x)||p(z|x)) is the Kullback-
Leibler (KL) divergence between the posterior and
prior distribution. Generally, z is set to follow a
Gaussian distribution in both the prior and poste-
rior networks. As mentioned in the related work,
directly using the above CVAE formulation causes
the low output diversity problem. This observation
is also validated in the short-text conversation task
in our experiments.

Now, we introduce our basic discrete CVAE for-
mulation to alleviate the low output diversity prob-
lem. We change the continuous latent variable z to
a discrete latent one with an explicit interpretable
meaning, which could actively control the gener-
ation of the response. An intuitive way is to con-
nect each latent variable with a word in the vocab-
ulary. With a sampled latent z from the prior (in
testing)/posterior network (in training), the gener-
ation network will take the query representation
together with the word embedding of this latent



1901

ğ‘(ğ‘§|ğ’™) ğ‘(ğ‘§|ğ’š, ğ’™)

ğ·ğ¾ğ¿(ğ‘||ğ‘)

ğ¬0 ğ¬1 ğ¬2

ğğ‘ğ‘§ ğğ‘§

ğ¡ğ‘§

ğ‘¦0 ğ‘¦1 ğ‘¦2

ğ¡1 ğ¡2 ğ¡3

ğ¡1 ğ¡2 ğ¡3
ğ‘¥1 ğ‘¥2 ğ‘¥3

ğ‘¦1 ğ‘¦2 ğ‘¦3

ğ¡y
q

ğ¡x
q

ğ¡1 ğ¡2 ğ¡3

ğ›¼1,1 ğ›¼1,2 ğ›¼1,3
ğ¡ğ‘¥
ğ‘”

ğ‘¥1 ğ‘¥2 ğ‘¥3

ğ¡1 ğ¡2 ğ¡3
ğ‘¥1 ğ‘¥2 ğ‘¥3

ğğ‘§ ğğ‘ğ‘§

ğ¡ğ‘§

(c) Generation Network

(b) Posterior Network(a) Prior Network

TrainingTesting

ğ¡ğ‘¥
ğ‘

ğ¬3

ğ‘¦3

BOW Loss

ğ¡1
ğ‘ ğ¡2

ğ‘ ğ¡3
ğ‘

ğ‘¦1 ğ‘¦2 ğ‘¦3

Sequential Loss

ğ’‰ğ‘¥
ğ‘”

MLP

Figure 1: The architecture of the proposed discrete CVAE. ecz and ez are embeddings of a cluster and a word
sampled from the estimated discrete distributions. ecz is only applied when the two-stage sampling approach in
Section 3.2 is used. If ecz is applied, the latent representation hz is the sum of ecz and ez; otherwise, hz is ez . Î±
denotes the attention weight. âŠ• denotes the sum of input vectors.

variable as the input to decode the response. Here,
we assume that a single word is enough to drive
the generation network to output diverse responses
for short text conversation, in which the response
is generally short and compact.

A major advantage of our DCVAE is that for
words with far different meanings, their word
embeddings (especially that we use a good pre-
trained word embedding corpus) generally have a
large distance and drive the generation network to
decode scattered responses, thus improve the out-
put diversity. In the standard CVAE, zâ€™s assumed
in a continuous space may not maintain the se-
mantic distance as in the embedding space and di-
verse zâ€™s may point to the same semantic meaning,
in which case the generation network is hard to
train well with such confusing information. More-
over, we can make use of the semantic distance be-
tween latent variables to perform better sampling
to approximate the objective during optimization,
which will be introduced in Section 3.2.

The latent variable z is thus set to follow a cat-
egorical distribution with each dimension corre-
sponding to a word in the vocabulary. Therefore
the prior and posterior networks should output cat-
egorical probability distributions:

pÎ¸(z|x) = softmax(gÎ¸(x)), (2)
qÏ†(z|y,x) = softmax(fÏ†(y,x)), (3)

where Î¸ and Ï† are parameters of the two networks

respectively. The KL distance of these two distri-
butions can be calculated in a closed form solu-
tion:

DKL(q(z|y,x)||p(z|x)) =âˆ‘
zâˆˆZ q(z|y,x) log

q(z|y,x)
p(z|x) , (4)

where Z contains all words in the vocabulary. In
the following, we present the details of the prior,
posterior and generation network.
Prior network p(z|x): It aims at inferring the la-
tent variable z given the input sequence x. We first
obtain an input representation hpx by encoding the
input query x with a bi-directional GRU and then
compute gÎ¸(x) in Eq. 2 as follows:

gÎ¸(x) = W2 Â· tanh(W1hpx + b1) + b2, (5)

where Î¸ contains parameters in both the bidirec-
tional GRU and Eq. 5.
Posterior network q(z|y,x): It infers a latent
variable z given a input query x and its target re-
sponse y. We construct both representations for
the input and the target sequence by separated bi-
directional GRUâ€™s, then add them up to compute
fÏ†(y,x) in Eq. 3 to predict the probability of z:

fÏ†(y,x)=W4 Â·tanh(W3(hqx+hqy)+b3)+b4, (6)

where Ï† contains parameters in the two encoding
functions and Eq. 6. Note that the parameters of



1902

the encoding functions are not shared in the prior
and posterior network.
Generation network p(y|x, z): We adopt an
encoder-decoder model with attention (Luong
et al., 2015) used in the decoder. With a sam-
pled latent variable z, a typical strategy is to com-
bine its representation, which in this case is the
word embedding ez of z, only in the beginning
of decoding. However, many previous works ob-
serve that the influence of the added information
will vanish over time (Yao et al., 2017; Gao et al.,
2019). Thus, after obtaining an attentional hid-
den state at each decoding step, we concatenate
the representation hz of the latent variable and the
current hidden state to produce a final output in
our generation network.

3.2 A Two-Stage Sampling Approach
When the CVAE models are optimized, they tend
to converge to a solution with a vanishingly small
KL term, thus failing to encode meaningful infor-
mation in z. To address this problem, we follow
the idea in (Zhao et al., 2017), which introduces
an auxiliary loss that requires the decoder in the
generation network to predict the bag-of-words in
the response y. Specifically, the response y is now
represented by two sequences simultaneously: yo
with word order and ybow without order. These
two sequences are assumed to be conditionally in-
dependent given z and x. Then our training objec-
tive can be rewritten as:

J(Î˜) =Ezâˆ¼q(z|y,x)[log p(y|x, z)]
âˆ’DKL(q(z|y,x)||p(z|x))
+ Ezâˆ¼q(z|y,x)[log p(ybow|x, z)],

(7)

where p(ybow|x, z) is obtained by a multilayer
perceptron hb = MLP(x, z):

p(ybow|x, z) =
|y|âˆ
t=1

exp(hbyt)âˆ‘
jâˆˆV exp(h

b
j)
, (8)

where |y| is the length of y, yt is the word index
of t-th word in y, and V is the vocabulary size.

During training, we generally approximate
Ezâˆ¼q(z|y,x)[log p(y|x, z)] by sampling N times of
z from the distribution q(z|y,x). In our model,
the latent space is discrete but generally large since
we set it as the vocabulary in the dataset 1. The vo-

1Note that we remove special tokens including UNK (un-
known token), BOS (start of sentence) and EOS (end of sen-
tence) in the latent space such that our model will only select
meaningful words as the latent variables.

cabulary consists of words that are similar in syn-
tactic or semantic. Directly sampling z from the
categorical distribution in Eq. 3 cannot make use
of such word similarity information.

Hence, we propose to modify our model in Sec-
tion 3.1 to consider the word similarity for sam-
pling multiple accurate and diverse latent zâ€™s. We
first cluster z âˆˆ Z into K clusters c1, . . . , cK .
Each z belongs to only one of the K clusters and
dissimilar words lie in distinctive groups. We use
the K-means clustering algorithm to group zâ€™s us-
ing a pre-trained embedding corpus (Song et al.,
2018). Then we revise the posterior network to
perform a two-stage cluster sampling by decom-
posing q(z|y,x) as :

q(z|y,x) =
âˆ‘
k

q(z|x,y, ck)q(ck|x,y)

= q(z|x,y, ckz)q(ckz |x,y). (9)

That is, we first compute q(ckz |y,x), which is
the probability of the cluster that z belongs to
conditioned on both x and y. Next, we com-
pute q(z|x,y, ckz), which is the probability dis-
tribution of z conditioned on the x, y and the
cluster ckz . When we perform sampling from
q(z|x,y), we can exploit the following two-stage
sampling approach: first sample the cluster based
on q(ck|x,y); next sample a specific z from zâ€™s
within the sampled cluster based on q(z|x,y, ckz).

Similarly, we can decompose the prior distribu-
tion p(z|x) accordingly for consistency:

p(z|x) = p(z|x, ckz)p(ckz |x). (10)

In testing, we can perform the two-stage sampling
according to p(ck|x) and p(z|x, ckz). Our full
model is illustrated in Figure 1.
Network structure modification: To modify
the network structure for the two-stage sampling
method, we first compute the probability of each
cluster given x in the prior network (or x and y in
the posterior network) with a softmax layer (Eq. 5
or Eq. 6 followed by a softmax function). We then
add the input representation and the cluster em-
bedding ecz of a sampled cluster cz , and use an-
other softmax layer to compute the probability of
each z within the sampled cluster. In the genera-
tion network, the representation of z is the sum of
the cluster embedding ecz and its word embedding
ez .
Network pre-training: To speed up the conver-
gence of our model, we pre-extract keywords from



1903

each query using the TF-IDF method. Then we
use these keywords to pre-train the prior and pos-
terior networks. The generation network is not
pre-trained because in practice it converges fast in
only a few epochs.

4 Experimental Settings

Next, we describe our experimental settings in-
cluding the dataset, implementation details, all
compared methods, and the evaluation metrics.

4.1 Dataset

We conduct our experiments on a short-text con-
versation benchmark dataset (Shang et al., 2015)
which contains about 4 million post-response pairs
from the Sina Weibo 2, a Chinese social platforms.
We employ the Jieba Chinese word segmenter 3 to
tokenize the queries and responses into sequences
of Chinese words. We use a vocabulary of 50,000
words (a mixture of Chinese words and charac-
ters), which covers 99.98% of words in the dataset.
All other words are replaced with <UNK>. We
randomly hold out two subsets as the development
and test dataset, each containing 900 pairs.

4.2 Implementation Details

We use single-layer bi-directional GRU for the
encoder in the prior/posterior/generation network,
and one-layer GRU for the decoder in the gener-
ation network. The dimension of all hidden vec-
tors is 1024. The cluster embedding dimension is
620. Except that the word embeddings are initial-
ized by the word embedding corpus (Song et al.,
2018), all other parameters are initialized by sam-
pling from a uniform distribution [âˆ’0.1, 0.1]. The
batch size is 128. We use Adam optimizer with a
learning rate of 0.0001. For the number of clus-
ters K in our method, we evaluate four different
values (5, 10, 100, 1000) using automatic metrics
and setK to 10 which tops the four options empir-
ically. It takes about one day for every two epochs
of our model on a Tesla P40 GPU, and we train
ten epochs in total. During testing, we use beam
search with a beam size of 10.

4.3 Compared Methods

In our work, we focus on comparing various meth-
ods that model p(y|x) differently. We compare
our proposed discrete CVAE (DCVAE) with the

2http://weibo.com
3https://github.com/fxsjy/jieba

two-stage sampling approach to three categories
of response generation models:

1. Baselines: Seq2seq, the basic encoder-
decoder model with soft attention mechanism
(Bahdanau et al., 2015) used in decoding and
beam search used in testing; MMI-bidi (Li et al.,
2016a), which uses the MMI to re-rank results
from beam search.

2. CVAE (Zhao et al., 2017): We adjust the orig-
inal work which is for multi-round conversation
for our single-round setting. For a fair compar-
ison, we utilize the same keywords used in our
network pre-training as the knowledge-guided fea-
tures in this model.

3. Other enhanced encoder-decoder models: Hi-
erarchical Gated Fusion Unit (HGFU) (Yao
et al., 2017), which incorporates a cue word
extracted using pointwise mutual information
(PMI) into the decoder to generate meaning-
ful responses; Mechanism-Aware Neural Machine
(MANM) (Zhou et al., 2017), which introduces la-
tent embeddings to allow for multiple diverse re-
sponse generation.

Here, we do not compare RL/GAN-based meth-
ods because all our compared methods can replace
their objectives with the use of reward functions
in the RL-based methods or add a discriminator
in the GAN-based methods to further improve the
overall performance. However, these are not the
contribution of our work, which we leave to fu-
ture work to discuss the usefulness of our model
as well as other enhanced generation models com-
bined with the RL/GAN-based methods.

4.4 Evaluation
To evaluate the responses generated by all com-
pared methods, we compute the following auto-
matic metrics on our test set:

1. BLEU: BLEU-n measures the average n-gram
precision on a set of reference responses. We re-
port BLEU-n with n=1,2,3,4.

2. Distinct-1 & distinct-2 (Li et al., 2016a): We
count the numbers of distinct uni-grams and bi-
grams in the generated responses and divide the
numbers by the total number of generated uni-
grams and bi-grams in the test set. These metrics
can be regarded as an automatic metric to evaluate
the diversity of the responses.

http://weibo.com
https://github.com/fxsjy/jieba


1904

Method Automatic Metrics Human Evaluation
BLEU-1 BLEU-2 BLEU-3 BLEU-4 dist-1 dist-2 quality accept good

Seq2seq 0.63Â± 0.14 0.49Â± 0.16 0.35Â± 0.14 0.21Â± 0.12 0.03 0.08 1.64Â± 0.30 49% 15%
MMI-bidi 0.54Â± 0.17 0.39Â± 0.18 0.28Â± 0.15 0.17Â± 0.13 0.04 0.11 1.71Â± 0.34 52% 19%
CVAE 0.60Â± 0.13 0.43Â± 0.16 0.30Â± 0.14 0.18Â± 0.11 0.03 0.06 1.60Â± 0.33 47% 13%
MANM 0.62Â± 0.14 0.48Â± 0.15 0.34Â± 0.14 0.22Â± 0.12 0.05 0.14 1.73Â± 0.35 53% 21%
HGFU 0.52Â± 0.11 0.38Â± 0.14 0.27Â± 0.12 0.16Â± 0.11 0.08 0.27 1.63Â± 0.39 51% 12%
DCVAE 0.64Â± 0.14 0.49Â± 0.16 0.35Â± 0.15 0.22Â± 0.13 0.08 0.24 2.03Â± 0.34 73% 30%

Table 1: The automatic and human evaluation results of all compared methods. Note that the acceptable ratio is
the percentage of responses with 2 or 3 points.

Three annotators from a commercial annotation
company are recruited to conduct our human eval-
uation. Responses from different models are shuf-
fled for labeling. 300 test queries are randomly
selected out, and annotators are asked to indepen-
dently score the results of these queries with dif-
ferent points in terms of their quality: (1) Good
(3 points): The response is grammatical, seman-
tically relevant to the query, and more impor-
tantly informative and interesting; (2) Acceptable
(2 points): The response is grammatical, semanti-
cally relevant to the query, but too trivial or generic
(e.g.,â€œæˆ‘ä¸çŸ¥é“(I donâ€™t know)â€, â€œæˆ‘ä¹Ÿæ˜¯(Me
too)â€, â€œæˆ‘å–œæ¬¢(I like it)â€ etc.); (3) Failed (1 point):
The response has grammar mistakes or irrelevant
to the query.

5 Experimental Results and Analysis

In the following, we will present results of all com-
pared methods and conduct a case study on such
results. Then, we will perform further analysis of
our proposed method by varying different settings
of the components designed in our model.

5.1 Results on All Compared Methods

Results on automatic metrics are shown on the
left-hand side of Table 1. From the results we
can see that our proposed DCVAE achieves the
best BLEU scores and the second best distinct ra-
tios. The HGFU has the best dist-2 ratio, but its
BLEU scores are the worst. These results indicate
that the responses generated by the HGFU are less
close to the ground true references. Although the
automatic evaluation generally indicates the qual-
ity of generated responses, it can not accurately
evaluate the generated response and the automatic
metrics may not be consistent with human percep-
tions (Liu et al., 2016). Thus, we consider human
evaluation results more reliable.

For the human evaluation results on the right-

hand side of Table 1, we show the mean and stan-
dard deviation of all test results as well as the per-
centage of acceptable responses (2 or 3 points) and
good responses (3 points only). Our proposed DC-
VAE has the best quality score among all com-
pared methods. Moreover, DCVAE achieves a
much higher good ratio, which means it generates
more informative and interesting responses. Be-
sides, the HGFUâ€™s acceptable and good ratios are
much lower than our model indicating that it may
not maintain enough response relevance when en-
couraging diversity. This is consistent with the re-
sults of the automatic evaluation in Table 1. We
also notice that the CVAE achieves the worst hu-
man annotation score. This validates that the orig-
inal CVAE for open-domain response generation
does not work well and our proposed DCVAE is
an effective way to improve the CVAE for better
output diversity.

5.2 Case Study
Figure 2 shows four example queries with their re-
sponses generated by all compared methods. The
Seq2seq baseline tends to generate less informa-
tive responses. Though MMI-bidi can select dif-
ferent words to be used, its generated responses
are still far from informative. MANM can avoid
generating generic responses in most cases, but
sometimes its generated response is irrelevant to
the query, as shown in the left bottom case. More-
over, the latent responding mechanisms in MANM
have no explicit or interpretable meaning. Similar
results can be observed from HGFU. If the PMI
selects irrelevant cue words, the resulting response
may not be relevant. Meanwhile, responses gener-
ated by our DCVAE are more informative as well
as relevant to input queries.

5.3 Different Sizes of the Latent Space
We vary the size of the latent space (i.e., sampled
word space Z) used in our proposed DCVAE. Fig-



1905

Input åˆšå’Œå«å“²é€šäº†ç”µè¯ï¼Œä»–å¿ƒæ€å¾ˆå¹³å’Œï¼Œ æˆ¿å­ï¼Œå°é™¢ï¼Œè½¦å­ï¼Œè½è‹±ç¼¤çº·çš„é—¨å‰ï¼Œ
è¯´ç°åœ¨æœ‰æ—¶é—´æ‰¾ä»–æ‰“çƒäº†ã€‚ é™è°§çš„ç”Ÿæ´»ã€‚
I just talked to Wei Zhe on the phone and he was calm. The house, the courtyard, the car,
I said now I had time to play ball with him. the door of fallen petals, quiet life.

Seq2seq æ˜¯çš„ï¼Œæˆ‘ä¹Ÿæ˜¯è¿™æ ·ã€‚ è¿™æ˜¯åœ¨å“ªå•Šï¼Ÿåœ¨å“ªé‡Œï¼Ÿ
Yes, me too. Where is this? Where?

MMI- æ­å–œæ­å–œï¼æ­å–œæ­å–œï¼ å“‡ï¼Œå¥½æ¼‚äº®ï¼Œå¥½æƒ³å»
bidi Congratulations! Congratulations! Wow, itâ€™s beautiful, I want to go there

MANM æˆ‘ä¹Ÿæƒ³çŸ¥é“ä»–æ˜¯è° å¥½å–œæ¬¢ï¼Œå¥½å–œæ¬¢ã€‚
I wonder who he is, too I like it, I like it.

HGFU æˆ‘ä¹Ÿæƒ³çŸ¥é“ä»–æ˜¯è°[æ®è¯´] å¥½ç¾çš„åœ°æ–¹ï¼Œå¥½ç¾çš„åœ°æ–¹[å­¦ä¹ ]
I wonder who he is, too [reputedly] What a beautiful place, what a beautiful place [study]

DCVAE å“ˆå“ˆï¼Œæˆ‘ä¹Ÿå–œæ¬¢çœ‹ä½ æ‰“çƒ[æ‰“çƒ] å¥½æ¼‚äº®çš„æˆ¿å­ï¼Œå¥½å–œæ¬¢[ç”Ÿæ´»]
Haha, I like watching you play ball, too [play ball] What a beautiful house, I love it [life]

Input è¿™ä½é›·ç¥é•¿å¾—æœ‰ç‚¹çœ¼ç†Ÿ ä¸‹åˆåª’ä½“å…¬å¼€è¯¾ï¼ç°åœ¨ç¡äº†â€¦æ™šä¸Šè§
This god of thunder looks a little familiar I have an open media class this afternoon!

Now Iâ€™m going to bed...See you tonight

Seq2seq å“ˆå“ˆå“ˆå“ˆå“ˆï¼Œç¬‘æ­»æˆ‘äº†ï¼ è¾›è‹¦äº†ï¼Œæ³¨æ„èº«ä½“ï¼
Hahahahaha, you are killing me! Youâ€™ve been working hard, take care of your health!

MMI ç¬‘æ­»æˆ‘äº† è¾›è‹¦äº†ï¼Œå¥½å¥½ä¼‘æ¯ï¼
-bidi You are killing me Youâ€™ve been working hard, have a good rest!

MANM è¿™æ˜¯è¦é—¹å“ªæ ·å•Šã€‚ è¾›è‹¦äº†ï¼Œæ³¨æ„èº«ä½“ï¼
What is this for. Youâ€™ve been working hard, take care of your health!

HGFU å°¼ç›ï¼Œè¿™æ˜¯è¦é€†å¤©å•Šï¼[å“‡å¡] åŠ æ²¹åŠ æ²¹åŠ æ²¹åŠ æ²¹åŠ æ²¹[å°¼ç›]
Holy crap, you are going against the worldï¼[wow] Come on, come on, come on [holy crap]

DCVAE é›·ç¥é•¿çš„å¥½æ¼‚äº®å•Šã€‚[è¿™ä½] æˆ‘ä¹Ÿæƒ³ç¡äº†ï¼Œæ˜å¤©è¿˜è¦ä¸Šç­[ä¸‹åˆ]
The god of thunder is so beautiful. [this] I want to sleep too, I need to work tomorrow [afternoon]

Figure 2: Examples of the generated responses. The sampled latent words (z) are showed in the brackets.

ure 3 shows the automatic and human evaluation
results on the latent space setting to the top 10k,
20k, all words in the vocabulary. On the automatic
evaluation results, if the sampled latent space is
getting larger, the BLEU-4 score increases but the
distinct ratios drop. We find out that though the
DCVAE with a small latent space has a higher
distinct-1/2 ratio, many generated sentences are
grammatically incorrect. This is also why the
BLEU-4 score decreases. On the human evalua-
tion results, all metrics improve with the use of
a larger latent space. This is consistent with our
motivation that open-domain short-text conversa-
tion covers a wide range of topics and areas, and
the top frequent words are not enough to capture
the content of most training pairs. Thus a small la-
tent space, i.e. the top frequent words only, is not
feasible to model enough latent information and a
large latent space is generally favored in our pro-
posed model.

5.4 Analysis on the Two-Stage Sampling

We further look into whether the two-stage sam-
pling method is effective in the proposed DCVAE.
Here, the One-Stage method corresponds to the
basic formulation in Section 3.1 with no use of
the clustering information in the prior or posterior
network. Results on both automatic and human
evaluation metrics are shown in Figure. 4(a) and

BLEU4 distinct-1 distinct-2
0.0

0.1

0.2

0.3

0.4

0.5

0.
18

0.
16

0.
32

0.
18

0.
15

0.
31

0.
22

0.
08

0.
24

DCVAE(10K)
DCVAE(20K)
DCVAE

Quality Acceptable Good
0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

1.
54

0.
54

0.
16

1.
57

0.
57

0.
16

2.
03

0.
73

0.
30

DCVAE(10K)
DCVAE(20K)
DCVAE

Figure 3: Different sizes of the latent space used in the
DCVAE: automatic evaluation (left) and human evalu-
ation (right).

4(b). We can observe that the performance of the
DCVAE without the two-stage sampling method
drops drastically. This means that the proposed
two-stage sampling method is important for the
DCVAE to work well.

Besides, to validate the effectiveness of clus-
tering, we implemented a modified DCVAE
(DCVAE-CD) that uses a pure categorical distri-
bution in which each variable has no exact mean-
ing. That is, the embedding of each latent vari-
able does not correspond to any word embed-
ding. Automatic evaluation results of this modi-
fied model are shown in Figure. 4(c). We can see
that DCVAE-CD performs worse, which means
the distribution on word vocabulary is important
in our model.



1906

BLEU4 distinct-1 distinct-2
0.0

0.1

0.2

0.3

0.4

0.5

0.
21

0.
05

0.
14

0.
22

0.
08

0.
24

DCVAE(One-Stage)
DCVAE(Two-Stage)

(a)

Quality Acceptable Good
0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

1.
59

0.
59

0.
17

2.
03

0.
73

0.
30

DCVAE(One-Stage)
DCVAE(Two-Stage)

(b)

BLEU4 distinct-1 distinct-2
0.0

0.1

0.2

0.3

0.4

0.5

0.
22

0.
07

0.
200.

22

0.
08

0.
24

DCVAE-CD
DCVAE

(c)

Figure 4: (a)/(b): Automatic/human evaluation on the DCVAE with/without the two-stage sampling approach. (c):
Automatic evaluation on our proposed DCVAE and the modified DCVAE that uses a pure categorical distribution
(DCVAE-CD) in which each variable has no exact meaning.

6 Conclusion

In this paper, we have presented a novel response
generation model for short-text conversation via a
discrete CVAE. We replace the continuous latent
variable in the standard CVAE by an interpretable
discrete variable, which is set to a word in the vo-
cabulary. The sampled latent word has an explicit
semantic meaning, acting as a guide to the genera-
tion of informative and diverse responses. We also
propose to use a two-stage sampling approach to
enable efficient selection of diverse variables from
a large latent space, which is very essential for our
model. Experimental results show that our model
outperforms various kinds of generation models
under both automatic and human evaluations.

Acknowledgements

This work was supported by National Natural Sci-
ence Foundation of China (Grant No. 61751206,
61876120).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of the
2015 International Conference on Learning Repre-
sentations.

Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu,
and Xuanjing Huang. 2018. A reinforcement learn-
ing framework for natural question generation using
bi-discriminators. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 1763â€“1774.

Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, and Shum-
ing Shi. 2019. Generating multiple diverse re-
sponses for short-text conversation. In Proceedings
of the AAAI Conference on Artificial Intelligence,
pages 6383â€“6390.

Xiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, and
Sunghun Kim. 2019. DialogWAE: Multimodal
response generation with conditional wasserstein
auto-encoder. In Proceedings of the 2019 Interna-
tional Conference on Learning Representations.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110â€“119.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. A
simple, fast diverse decoding algorithm for neural
generation. arXiv preprint arXiv:1611.08562.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016c. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192â€“
1202.

Jiwei Li, Will Monroe, Tianlin Shi, SeÌbastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2157â€“2169.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 2122â€“2132.



1907

Yahui Liu, Wei Bi, Jun Gao, Xiaojiang Liu, Jian Yao,
and Shuming Shi. 2018. Towards less generic re-
sponses in neural conversation models: A statistical
re-weighting method. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2769â€“2774.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412â€“1421.

Ryo Nakamura, Katsuhito Sudoh, Koichiro Yoshino,
and Satoshi Nakamura. 2018. Another diversity-
promoting objective function for neural dialogue
generation. arXiv preprint arXiv:1811.08100.

Diana Perez-Marin. 2011. Conversational Agents and
Natural Language Interaction: Techniques and Ef-
fective Practices: Techniques and Effective Prac-
tices. IGI Global.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In Pro-
ceedings of AAAI Conference on Artificial Intelli-
gence, pages 3776â€“3784.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2017. A hierarchical latent variable
encoder-decoder model for generating dialogues. In
Proceedings of AAAI Conference on Artificial Intel-
ligence, pages 3295â€“3301.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics,
pages 1577â€“1586.

Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi
Niu, Yang Zhao, Akiko Aizawa, and Guoping Long.
2017. A conditional variational framework for dia-
log generation. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, pages 504â€“509.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances
in Neural Information Processing Systems, pages
3483â€“3491.

Yan Song, Shuming Shi, Jing Li, and Haisong Zhang.
2018. Directional skip-gram: Explicitly distinguish-
ing left and right context for word embeddings. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 175â€“180.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
196â€“205.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Liwei Wang, Alexander Schwing, and Svetlana Lazeb-
nik. 2017. Diverse and accurate image description
using a variational auto-encoder with an additive
gaussian encoding space. In Advances in Neural In-
formation Processing Systems, pages 5756â€“5766.

Tsung-Hsien Wen, Milica Gasic, Nikola MrksÌŒicÌ, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1711â€“1721.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017. Latent intention dialogue mod-
els. In International Conference on Machine Learn-
ing, pages 3732â€“3741.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In Proceedings of AAAI
Conference on Artificial Intelligence, pages 3351â€“
3357.

Zhen Xu, Bingquan Liu, Baoxun Wang, SUN
Chengjie, Xiaolong Wang, Zhuoran Wang, and
Chao Qi. 2017. Neural response generation via gan
with an approximate embedding layer. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 617â€“626.

Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak
Lee. 2016. Attribute2image: Conditional image
generation from visual attributes. In European
Conference on Computer Vision, pages 776â€“791.
Springer.

Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan
Zhao, and Rui Yan. 2017. Towards implicit content-
introducing for generative short-text conversation
systems. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2190â€“2199.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
Xiujun Li, Chris Brockett, and Bill Dolan. 2018.
Generating informative and diverse conversational
responses via adversarial information maximization.
In Advances in Neural Information Processing Sys-
tems, pages 1810â€“1820.



1908

Tiancheng Zhao, Kyusong Lee, and Maxine Eskenazi.
2018. Unsupervised discrete sentence representa-
tion learning for interpretable neural dialog gener-
ation. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 654â€“664.

Ganbin Zhou, Ping Luo, Rongyu Cao, Fen Lin,
Bo Chen, and Qing He. 2017. Mechanism-aware
neural machine for dialogue response generation. In
Proceedings of AAAI Conference on Artificial Intel-
ligence, pages 3400â€“3407.


