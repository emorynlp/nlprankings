



















































AmritaCEN at SemEval-2016 Task 11: Complex Word Identification using Word Embedding


Proceedings of SemEval-2016, pages 1022–1027,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

AmritaCEN at SemEval-2016 Task 11: Complex Word Identification using
Word Embedding

Sanjay S.P and Anand Kumar M and Soman K P
Centre for Computational Engineering & Networking (CEN)

Amrita School of Engineering, Coimbatore
Amrita Vishwa Vidyapeetham

Amrita University
India

sanjay.poongs@gmail.com, m anandkumar@cb.amrita.edu, kp soman@amrita.edu

Abstract

Complex word identification task focuses on
identifying the difficult word from English
sentence for a Non-Native speakers. Non-
Native speakers are those who don’t have En-
glish as their native language. It is a sub-
task for lexical simplification. We have ex-
perimented with word embedding features, or-
thographic word features, similarity features
and POS tag features which improves the per-
formance of the classification. In addition to
the SemEval 2016 results we have evaluated
the training data by varying the vector dimen-
sion size and obtained the best possible size
for producing better performance. The SVM
learning algorithm will attains constant and
maximum accuracy through linear classifier.
We achieve a G-score of 0.43 and 0.54 on
computing complex words for two systems.

1 Introduction

Complex Word Identification (CWI) is the task of
identifying difficult words for a Non-Native speaker.
The main objective of Complex Word Identification
is to simplify and enhance the perplexing words in
the sentences for Non-Native speakers. Complex
Word Identification is used in Lexical Simplifica-
tion (LS). The Lexical Simplification is important
for all Text Simplification (Gustavo Henrique Paet-
zold, 2006). The perspective of each user is dif-
ferent therefore the task Complex Word Identifica-
tion have been demanding one. For distinguish com-
plex words for individual user will be a tedious task,
therefore the CWI task is a difficult one, so we are
identifying the complex words for a group of users.

CWI could be used in summarization of sto-
ries or generate abstract of a book. Shortening
a paragraph would be difficult without removing
the complex words, since the shortened text with
complex words would be very difficult or confus-
ing for non-native users to read and understand.
We can make a story more simple for kids to read
by replacing complex words with most common
words (Tomoyuki Kajiwara et al., 2013). The main
ways to identify the complex words are Threshold-
based and Classification-based approches (Gustavo
Henrique Paetzold, 2006). In Threshold-based ap-
proach, length, number of syllables, count of ambi-
guity and word frequency are considered as features.
If the values are above the threshold, then the word
is said to be a complex word. In the Classifier-based
approach, the word features are trained and the word
features which are similar to the trained word fea-
tures are considered to be a complex word.

2 Complex Word Identification (CWI) -
Task Description

The SemEval-2016 organizers have released three
files cwi training, cwi training allannotations, and
cwi testing. The first two files are training files and
the third file is a testing file. Our task is to find the
words in the testing file and label each one as 1 if
the given word is a complex word, or 0 if the word
is not a complex word.

The training allannotations file contains tags of
20 authors for given sentences. The training file con-
tains a single tag for the sentences representing the
group of 20 authors in allannotations file. Each word
is tagged complex if any of the author tagged it as 1.

1022



If the all authors tagged it as non-complex then it
is tagged as 0. We trained a binary classifier on the
cwi training data to identify the complex words in
the testing data.

3 Neural word embedding

From the given sentences, unique words are con-
verted into a more meaningful mathematical vec-
tor representation called a Neural word embedding
(Tomas Mikolov et al., 2013). For a given word, the
vector formed by the word is related to the semantic
and syntactic contexts of the word, and also related
to the the neighboring occurrence of the word. To
produce the maximum vector of given size. The w0
is the given word where the conditional probability
is given in the Figure 1, where j is the index of actual
output in the output layer.

3.1 Gensim - Implementation
The Gensim word2vec uses skip-gram and CBOW
models. The word2vec run two passes to the training
data. The first pass will collect the required unique
words and build the tree structure. The second pass
will train the neural model (Tomas Mikolov et al.,
2013). We can vary the dimensions of the vector
space model.

4 Methodology

The input data to our system is in the form of sen-
tences but the output requires only word to tagged,
so we cannot train the classifier with sentence fea-
tures. The sentence with the word to be identified
are tokenized. We have given the tokenized data
into a word embedding models, it produced vectors
for each words. we have acquired similarity features
with nearby words. The other features like POS tag-
ging and orthographic word features are obtained
from the word and these features does not depend
on the sentence. All the features are incorporated
and saved in a text file. The text file is given to
a SVM classifier which compares with the trained
model and generates the result. The working pro-
cess is given in Figure 1.

5 Feature Extraction

The features, are extracted from the given word in
the testing data. We have extracted four types of fea-

tures, namely orthographic features, similarity fea-
tures, word embedding features and POS tag fea-
tures. In these four types of features the ortho-
graphic feature based on the word itself, it pro-
vides the shape features of the word. The other two
features namely similarity features and word em-
bedding features are extracted from the word2vec
model which is mentioned above. The Part Of
Speech (POS) tag features is obtained with the help
of NLTK tool kit and all the features are incorpo-
rated for training.

5.1 Orthographic features

Orthographic word features do not depend on the
sentence. They are generic features of a given word
like length of the word, number of syllables, ambi-
guity count and frequency. The count of the char-
acters in the corresponding target word which pro-
vides the length of the word. The syllable feature
gives the number of syllables present in the word,
using the pyhyphen package we have retrieved the
features. We used NLTK WordNet synsets (Steven
Bird, 2006) for obtaining the ambiguity of the word.
WordNet has an built-in dictionary from which num-
ber of ambiguity present for the word is obtained
(George A. Miller, 1995; ?). The ambiguity is de-
fined as the number of words that have similar mean-
ing. A word is said to be more familiar if the word
has more meaning. In the frequency feature, we
count occurrences of the word in both training and
testing data. These word features improve the accu-
racy.

5.2 Similarity features

These features represent how similar the word oc-
curs to the neighboring words. We have derived four
similarity values. Let us assume that the given word
is represented as w0 then the similarity values are
attained as [w0&w−2][w0&w−1][w0&w1][w0&w2].
The similarity values are acquired from the
word2vec model using Gensim.

5.3 Word embedding features

All the training and testing data were combined
and passed into word2vec model. In word2vec, the
unique words are obtained and its corresponding
vector representations are created for all the words.
We can vary the size of the vector while creating the

1023



Figure 1: Working model

Figure 2: Training accuracy

1024



vector representation model. We have obtained vari-
ous results by varying the vector size. We varied the
vector size, getting good accuracy after a vector size
of 150, as shown in Figure 3. Finally concluded the
best vector size as 500 at which we have obtained
reasonable results for all other features. The curve
tends to decrease after vector size of 500.

5.4 POS tag features
The Part Of Speech tag is taken for five words, by
assuming the current word as w0, then the POS tag-
ging is taken for w−2, w−1, w0, w1, w2. We are us-
ing NLTK (Natural Language Tool Kit) for POS
tagging. The NLTK is an open source tool kit for
python[3]. We implemented 2 systems. The first
system trained an SVM without including the POS
tagging. In the second system, POS tagging with
other features has been included. The POS tag in-
creases the accuracy of the data but it might reduce
the recall.

6 Classifier

The SVM classifier is used for classification. The
data is classified based on the features of the test
data. For a given training point (xi, yi) it finds
out the hyper plane wt(x) + b = 0. Here we
have used LIBSVM (Chang and Lin, 2011), which
is integrated tool for Support Vector Classification
(SVC) with regression and distributed estimation
[1]. Python LIBSVM supports cross-validation. It
is built on the C/C++ core SVM performance.

7 Data set:

We were provided with 2,237 training words and
88,221 testing words. In the training data there are
706 (31.56%) complex words and 1531 (68.4%) non
complex words. The training file is in the format of
<sentences> <word> <position> <1 or 0>. 1 in-
dicates if the word is complex and 0 indicates if the
word is not complex. The test data is in the format
of <sentences> <word> <position>.

8 Experimental results

The Figure 3 gives our system’s precision, recall,
and F-score on the CWI training data. We tried vary-
ing the vector size in the word2vec model. Word fea-
tures include length, number of syllables, ambiguity

Figure 3: F-score accuracy

count, and frequency. We have four possible com-
binations and the test results are been tabled in Ta-
ble 1. The first combination includes word2vec fea-
tures. The second combination includes word2vec
and word features. The third combination includes
word2vec and similarity features. The fourth combi-
nation includes word2vec, similarity and word fea-
tures. By varying the size and features the precision
and recall tend to combine at a point called break
though point. After the break though point the F-
score becomes steady. The break through point for
data is 150. Above which the accuracy tends to be
linear, After the break through point the curve tends
to decrease after 500. Therefore, we have concluded
at a vector size of 500 and including all the features
we have achieved the maximum accuracy. From the
Figure 3 we can clearly see the F-score gradually
decreasing at 550.

The accuracy graph steadily increases and reaches
a maximum at 500 then accuracy tends to steadily
decreases. Table 1, provides the additional results
experimented by us.

8.1 SemEval Results:

We have submitted 2 files w2vecSimPos and
w2vecSim. The first file w2vecSimPos is extracted
by including all the four features mentioned namely
word embedding, similarity, special word features,
and POS tag features. The second file consist of
word embedding, similarity and special word fea-
tures. In the second file we have not included POS
tag features. From the table, we can clearly see by
including POS tag features there is in an improve-
ment in the accuracy of the system. F-score denotes
harmonic mean between precision and recall. G-
score denotes harmonic mean between accuracy and
recall. By including the POS tag as a feature the ac-

1025



Table 1: Additional Results
Embedding Size Word2vec Word2vec Word2vec Word2vec

+ Orthographic + Similarity + Orthographic+ Similarity
20 0.27721 0.35542 0.30112 0.38263
50 0.511586453 0.513227513 0.495495495 0.505829596
100 0.657097289 0.657643312 0.669316375 0.679904686
150 0.941852118 0.951833214 0.949466192 0.954903364
200 0.943287868 0.94751977 0.950537634 0.955587393
250 0.944723618 0.954154728 0.946543122 0.953471725
300 0.943042538 0.951149425 0.95149786 0.955523673
350 0.948497854 0.949567723 0.955777461 0.955587393
400 0.948424069 0.950395399 0.955714286 0.95764537
450 0.946236559 0.951079137 0.951428571 0.95764537
500 0.946599424 0.951516245 0.954960686 0.952101501
550 0.944723618 0.949640288 0.953604568 0.957706093
600 0.93410572 0.93487699 0.935158501 0.940836941
650 0.932016353 0.933913358 0.930656314 0.934688976
700 0.927825678 0.928478303 0.927098887 0.928424859

curacy is improved but the recall is decreased there-
fore G-score is also decreased.

Table 2: SemEval Results.
System W2VecSimPos W2VecSim

Accuracy 0.743 0.627
Precision 0.060 0.061
Recall 0.306 0.486
F-Score 0.100 0.109
G-Score 0.434 0.547

9 Conclusion

The proposed model with features specially derived
for identifying complex words produced good re-
sults. The word embedding features played a ma-
jor role compared with other features. By increas-
ing the size of vectors our accuracy increased con-
stantly. We tried all possible combinations with the
four features mentioned in the Table 1 and conclude
that the word embedding features alone also have a
good accuracy. Our systems have generated a good
precision compared with the baseline. Among 45
systems, we ranked 27th position for G-score evalu-
ation. Although, the frequency counts and embed-
ding features are reasonable for a given resource-
constraint task, if we have provided for English lan-
guage in general it might have provide with better
accuracy.

References
Chih-Chung Chang and Chih-Jen Lin. (2011). LIB-

SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST).2(3), p.27.

Christiane Fellbaum. (1998). WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.

Daniel Ferres, Montserrat Marimon, Horacio Saggion.
(2015). A Web-based Text Simplification System for
English. Procesamiento del Lenguaje Natural, 55,
191-194.

Dany Amiot. (2004). Between compounding and
derivation Elements of word-formation corresponding.
Morphology and its demarcations: Selected papers
from the 11th Morphology meeting, Vienna, Vol. 264.

George A. Miller. (1995). WordNet: A Lexical Database
for English. Communications of the ACM Vol. 38, No.
11: 39-41.

Gustavo Henrique Paetzold. (2015). Reliable Lexical
Simplification for Non-Native Speakers. NAACL-HLT
2015 Student Research Workshop (SRW).

La Pointe, Linda B and Randall W. Engle. (1990). Sim-
ple and complex word spans as measures of working
memory capacity. Journal of Experimental Psychol-
ogy: Learning, Memory, and Cognition 16.6 (1990):
1118.

Lucia Specia, Dhwaj Raj, Marco Turchi. (2010). Ma-
chine translation evaluation versus quality estimation.
Machine translation, 24(1), pp.39-50.

Or Biran, Samuel Brody, Noemie Elhadad. (2011).
Putting it simply: a context-aware approach to lexical

1026



simplification. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies: short papers-Volume
2. Association for Computational Linguistics.

Radim Rehurek and Petr Sojra. (2010). Software frame-
work for topic modelling with large corpora. Proceed-
ings of the LREC 2010 Workshop on New Challenges
for NLP Frameworks, ELRA, p.45-50

Edward Loper and Steven Bird. (2006). NLTK: the
natural language toolkit. Proceedings of the ACL-
02 Workshop on Effective tools and methodologies
for teaching natural language processing and compu-
tational linguistics-Volume 1. Association for Compu-
tational Linguistics, 2002.

Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean.
(2010). Efficient estimation of word representations in
vector space. rXiv preprint arXiv:1301.3781.

Tomoyuki Kajiwara, Hiroshi Matsumoto, Kazuhide Ya-
mamoto. (2013). Selecting Proper Lexical Para-
phrase for Children. Proceedings of the Twenty-Fifth
Conference on Computational Linguistics and Speech
Processing (ROCLING 2013).

William Snyder. (2001). On the nature of syntactic vari-
ation: Evidence from complex predicates and complex
word-formation. Language Vol. 77, No. 2, pp. 324-
342.

1027


