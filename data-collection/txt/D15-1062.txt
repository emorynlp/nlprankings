



















































Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 536–540,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Semantic Relation Classification via Convolutional Neural Networks with
Simple Negative Sampling

Kun Xu1, Yansong Feng∗1, Songfang Huang2 and Dongyan Zhao1
1ICST, Peking University, Beijing, China

2IBM China Research Lab, Beijing, China
{xukun,fengyansong,zhaodongyan}@pku.edu.cn

huangsf@cn.ibm.com

Abstract

Syntactic features play an essential role in
identifying relationship in a sentence. Pre-
vious neural network models directly work
on raw word sequences or constituent
parse trees, thus often suffer from irrele-
vant information introduced when subjects
and objects are in a long distance. In this
paper, we propose to learn more robust re-
lation representations from shortest depen-
dency paths through a convolution neu-
ral network. We further take the relation
directionality into account and propose a
straightforward negative sampling strategy
to improve the assignment of subjects and
objects. Experimental results show that
our method outperforms the state-of-the-
art approaches on the SemEval-2010 Task
8 dataset.

1 Introduction

The relation extraction (RE) task can be defined as
follows: given a sentence S with a pair of nomi-
nals e1 and e2, we aim to identify the relationship
between e1 and e2. RE is typically investigated
in a classification style, where many features have
been proposed, e.g., Hendrickx et al. (2010) de-
signed 16 types of features including POS, Word-
Net, FrameNet, dependency parse features, etc.
Among them, syntactic features are considered to
bring significant improvements in extraction accu-
racy (Bunescu and Mooney, 2005a). Earlier at-
tempts to encode syntactic information are mainly
kernel-based methods, such as the convolution tree
kernel (Qian et al., 2008), subsequence kernel
(Bunescu and Mooney, 2005b), and dependency
tree kernel (Bunescu and Mooney, 2005a).

With the recent success of neural networks in
natural language processing, different neural net-
work models are proposed to learn syntactic fea-
tures from raw sequences of words or constituent

parse trees (Zeng et al., 2014; Socher et al., 2012),
which have been proved effective, but, often suf-
fer from irrelevant subsequences or clauses, espe-
cially when subjects and objects are in a longer
distance. For example, in the sentence, “The
[singer]e1 , who performed three of the nominated
songs, also caused a [commotion]e2 on the red
carpet”, the who clause is used to modify subject
e1, but is unrelated to the Cause-Effect relation-
ship between singer and commotion. Incorporat-
ing such information into the model will hurt the
extraction performance. We therefore propose to
learn a more robust relation representation from
a convolution neural network (CNN) model that
works on the simple dependency path between
subjects and objects, which naturally characterizes
the relationship between two nominals and avoids
negative effects from other irrelevant chunks or
clauses.

Our second contribution is the introduction of
a negative sampling strategy into the CNN mod-
els to address the relation directionality, i.e., prop-
erly assigning the subject and object within a re-
lationship. In the above singer example, (singer,
commotion) hold the Cause-Effect relation, while
(commotion, singer) not. Previous works (Liu
et al., 2015) do not fully investigate the differ-
ences between subjects and objects in the utter-
ance, and simply transform a (K+1)-relation task
into a (2×K+1) classification task, where 1 is the
other relation. Interestingly, we find that depen-
dency paths naturally offer the relative positions
of subjects and objects through the path directions.
In this paper, we propose to model the relation di-
rectionality by exploiting the dependency path to
learn the assignments of subjects and objects us-
ing a straightforward negative sampling method,
which adopts the shortest dependency path from
the object to the subject as a negative sample. Ex-
perimental results show that the negative sampling
method significantly improves the performance,

536



and our model outperforms the-state-of-the-art
methods on the SemEval-2010 Task 8 dataset.

2 The Shortest Path Hypothesis

If e1 and e2 are two nominals mentioned in the
same sentence, we assume that the shortest path
between e1 and e2 describes their relationship.
This is because (1) if e1 and e2 are arguments of
the same predicate, then their shortest path should
pass through that predicate; (2) if e1 and e2 belong
to different predicate-argument structures, their
shortest path will pass through a sequence of pred-
icates, and any consecutive predicates will share
a common argument. Note that, the order of the
predicates on the path indicates the proper assign-
ments of subjects and objects for that relation. For
example, in Figure 1, the dependency path consec-
utively passes through carried and receives, which
together implies that in the Instrument-Agency re-
lation, the subject and object play a sender and re-
ceiver role, respectively.

burst by pressure 

max pooling

tanh(W2 x     )

W3 x       

W1 x       

Input 

Lookup Table 

Convolution

Dependency Feature

Output

Window Processing 

nsubjpass prep dobj 

Hidden layer 1

Hidden layer 2

caused 

Figure 2: Architecture of the convolution neural
network.

3 A Convolutional Neural Network
Model

Our model successively takes the shortest depen-
dency path (i.e, the words, dependency edge direc-
tions, and dependency labels) from the subject to
the object as input, passes it through the lookup

table layer, produces local features around each
node on the dependency path, and combines these
features into a global feature vector that are then
fed to a softmax classifier. Each dimension of the
output vector indicates the confidence score of the
corresponding relation.

In the lookup table step, each node (i.e. word,
label or arrow) in the dependency path is trans-
formed into a vector by looking up the embedding
matrix We ∈ Rd×|V|, where d is the dimension of
a vector and V is a set of all nodes we consider.

Convolution To capture the local features
around each node of the dependency path, we con-
sider a fixed size window of nodes around each
node in the window processing component, pro-
ducing a matrix of node features of fixed size
dw × 1, where dw = d × w and w is the window
size. This matrix can be built by concatenating
the vectors of nodes within the window, which are
then fed to the convolutional layer.

In the convolutional layer, we use a linear trans-
formation W1 ∈ Rn1×dw to extract local features
around each window of the given sequence, where
n1 is the size of hidden layer 1. The resulting ma-
trix Z has size of n1 × t, where t is the number of
nodes in the input dependency path.

We can see that Z captures local contextual in-
formation in the dependency path. However, iden-
tifying a relationship requires considerations for
the whole dependency path. We therefore perform
a max pooling over Z to produce a global feature
vector in order to capture the most useful local
features produced by the convolutional layer (Col-
lobert et al., 2011), which has a fixed size of n1,
independent of the dependency path length.

Dependency based Relation Representation
To extract more meaningful features, we choose
hyperbolic tanh as the non-linearity function in the
second hidden layer, which has the advantage of
being slightly cheaper to compute, while leaving
the generalization performance unchanged. W2 ∈
Rn2×n1 is the linear transformation matrix, where
n2 is the size of hidden layer 2. The output vec-
tor can be considered as higher level syntactic fea-
tures, which is then fed to a softmax classifier.

Objective Function and Learning The softmax
classifier is used to predict a K-class distribution
d(x), where K is the size of all possible rela-
tion types, and the transformation matrix is W3 ∈
RK×n2 . We denote t(x) ∈ RK×1 as the target

537



receiver recipientInstrument-Agency receiver carried radio through 

The recipient receives the call through a miniature radio receiver carried on his person

receives recipient 

Figure 1: The shortest dependency path representation for an example sentence from SemEval-08.

Train Strategy Test Strategy F1(%)
Blind Blind 79.3

Sighted Blind 81.3
Sighted Sighted 89.2

Table 1: Performances on the development set
with different train and testing strategies.

distribution vector1: the entry tk(x) is the proba-
bility that the dependency path describes the k-th
relation. We compute the cross entropy error be-
tween t(x) and d(x), and further define the objec-
tive function over all training data:

J(θ) = −
∑

x

K∑
k=1

tk(x) log dk(x) + λ||θ||2

where θ = (We,W1,W2,W3) is the set of model
parameters to be learned, and λ is a vector of reg-
ularization parameters. The model parameters θ
can be efficiently computed via backpropagation
through network structures. To minimize J(θ),
we apply stochastic gradient descent (SGD) with
AdaGrad (Duchi et al., 2011) in our experiments.

4 Negative Sampling

We start by presenting three pilot experiments on
the development set. In the first one, we assume
that the assignment of the subject and object for
a relation is not given (blind), we simply extract
features from e1 to e2, and test it in a blind set-
ting as well. In the second one, we assume that
the assignment is given (sighted) during training,
but still blind in the test phase. The last one is as-
sumed to give the assignment during both training
and test steps. The results are listed in Table 1.

The third experiment can be seen as an upper
bound, where we do not need to worry about the
assignments of subjects and objects. By com-
paring the first and the second one, we can see
that when adding assignment information during
training, our model can be significantly improved,

1Note that, there may be more than one relation existing
between two nominals. A dependency path thus may corre-
spond to multiple relations.

indicating that our dependency based representa-
tion can be used to learn the assignments of sub-
jects/objects, and injecting better understandings
of such assignments during training is crucial to
the performance. We admit that models with more
complex structures can better handle these con-
siderations. However, we find that this can be
achieved by simply feeding typical negative sam-
ples to the model and let the model learn from such
negative examples to correctly choose the right as-
signments of subjects and objects. In practice, we
can treat the opposite assignments of subjects and
the objects as negative examples. Note that, the
dependency path of the wrong assignment is dif-
ferent from that of the correct assignment, which
essentially offers the information for the model to
learn to distinguish the subject and the object.

5 Experimental Evaluation

We evaluate our model on the SemEval-2010 Task
8 (Hendrickx et al., 2010), which contains 10,717
annotated examples, including 8,000 instances for
training and 2,717 for test. We randomly sampled
2,182 samples from the training data for valida-
tion.

Given a sentence, we first find the shortest de-
pendency path connecting two marked nominals,
resulting in two dependency paths corresponding
to two opposite subject/object directions, and then
make predictions for the two paths, respectively.
We choose the relation other if and only if both
predictions are other. And for the rest cases, we
choose the non-other relation with highest confi-
dence as the output, since ideally, for a non-other
instance, our model will output the correct label
for the right subject/object direction and an other
label for the wrong direction. We evaluate our
models by macro-averaged F1 using the official
evaluation script.

We initialized We with 50-dimensional word
vectors trained by Turian et al. (2010). We tuned
the hyper parameters using the development set for
each experimental setting. The hyper parameters
include w, n1, n2, and regularization parameters

538



Method Feature Sets F1
SVM 16 types of features 82.2
RNN - 74.8

+POS, NER, WordNet 77.6
MVRNN - 79.1

+POS, NER, WordNet 82.4
CNN - 78.9
(Zeng et al., 2014) +WordNet,words around nominals 82.7
DepNN +NER 83.6
depCNN - 81.3
depLCNN - 81.9
depLCNN +WordNet,words around nominals 83.7
depLCNN+NS - 84.0

+WordNet,words around nominals 85.6

Table 2: Comparisons of our models with other
methods on the SemEval 2010 task 8.

Negative sampling schemes F1
No negative examples 81.3
Randomly sampled negative examples from NYT 83.5
Dependency paths from the object to subject 85.4

Table 3: Comparisons of different negtive sam-
pling methods on the development set.

for We, W1, W2 and W3. The best setting was ob-
tained with the values: 3, 200, 100, 10−4, 10−3,
10−4 and 2× 10−3, respectively.
Results and Discussion Table 2 summarizes the
performances of our model, depLCNN+NS(+),
and state-of-the-art models, SVM (Hendrickx et
al., 2010), RNN, MV-RNN (Socher et al., 2012),
CNN (Zeng et al., 2014) and DepNN (Liu et al.,
2015). For fair comparisons, we also add two
types of lexical features, WordNet hypernyms and
words around nominals, as part of input vector to
the final softmax layer.

We can see that our vanilla depLCNN+NS,
without extra lexical features, still outperforms, by
a large margin, previously reported best systems,
MVRNN+ and CNN+, both of which have taken
extra lexical features into account, showing that
our treatment to dependency path can learn a ro-
bust and effective relation representation. When
augmented with similar lexical features, our de-
pLCNN+NS further improves by 1.6%, signifi-
cantly better than any other systems.

Let us first see the comparisons among plain
versions of depLCNN (taking both dependency di-
rections and labels into account), depCNN (con-
sidering the directions of dependency edges only),
MVRNN and CNN, which all work in a 2×K+1
fashion. We can see that the both of our depCNN
and depLCNN outperforms MVRNN and CNN by
at least 2.2%, indicating that our treatment is better

than previous conventions in capturing syntactic
structures for relation extraction. And note that de-
pLCNN, with extra considerations for dependency
labels, performs even better than depCNN, show-
ing that dependency labels offer more discrimina-
tive information that benefits the relation extrac-
tion task.

And when we compare plain depLCNN and
depLCNN+NS (without lexical features), we can
see that our Negative Sampling strategy brings an
improvement of 2.1% in F1. When both of the
two models are augmented with extra lexical fea-
tures, our NS strategy still gives an improvement
of 1.9%. These comparisons further show that our
NS strategy can drive our model to learn proper
assignments of subjects/objects for a relation.

Next, we will have a close look at the effect
of our Negative Sampling method. We conduct
additional experiments on the development set to
compare two different negative sampling methods.
As a baseline, we randomly sampled 8,000 nega-
tive examples from the NYT dataset (Chen et al.,
2014). For our proposed NS, we create a nega-
tive example from each non-other instance in the
training set, 6,586 in total. As shown in Table 2,
it is no doubt that introducing more negative ex-
amples improves the performances. We can see
that our model still benefits from the randomly
sampled negative examples, which may help our
model learn to refine the margin between the pos-
itive and negative examples. However, with sim-
ilar amount of negative examples, treating the re-
versed dependency paths from objects to subjects
as negative examples can achieve a better perfor-
mance (85.4% F1), improving random samples by
1.9%. This again proves that dependency paths
provide useful clues to reveal the assignments of
subjects and objects, and a model can learn from
such reversed paths as negative examples to make
correct assignments. Beyond the relation extrac-
tion task, we believed the proposed Negative Sam-
pling method has the potential to benefit other
NLP tasks, which we leave for future work.

6 Conclusion

In this paper, we exploit a convolution neural net-
work model to learn more robust and effective re-
lation representations from shortest dependency
paths for relation extraction. We further pro-
pose a simple negative sampling method to help
make correct assignments for subjects and objects

539



within a relationship. Experimental results show
that our model significantly outperforms state-of-
the-art systems and our treatment to dependency
paths can well capture the syntactic features for
relation extraction.

Acknowledgments

We would like to Sheng Zhang, Weiwei Sun
and Liwei Chen for their helpful discussions.
This work was supported by National High
Technology R&D Program of China (Grant No.
2015AA015403, 2014AA015102), Natural Sci-
ence Foundation of China (Grant No. 61202233,
61272344, 61370055) and the joint project with
IBM Research. Any correspondence please refer
to Yansong Feng.

References
Razvan C. Bunescu and Raymond J. Mooney. 2005a.

A shortest path dependency kernel for relation ex-
traction. In HLT/EMNLP 2005, Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, Pro-
ceedings of the Conference, 6-8 October 2005, Van-
couver, British Columbia, Canada.

Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Advances in Neural Information Processing Sys-
tems 18 [Neural Information Processing Systems,
NIPS 2005, December 5-8, 2005, Vancouver, British
Columbia, Canada], pages 171–178.

Liwei Chen, Yansong Feng, Songfang Huang, Yong
Qin, and Dongyan Zhao. 2014. Encoding relation
requirements for relation extraction via joint infer-
ence. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, ACL
2014, June 22-27, 2014, Baltimore, MD, USA, Vol-
ume 1: Long Papers, pages 818–827.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
SemEval-2, Uppsala, Sweden.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng WANG. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 285–
290, Beijing, China, July. Association for Computa-
tional Linguistics.

Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-
ing Zhu, and Peide Qian. 2008. Exploiting con-
stituent dependencies for tree kernel-based semantic
relation extraction. In COLING 2008, 22nd Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference, 18-22 August 2008,
Manchester, UK, pages 697–704.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju
Island, Korea, pages 1201–1211.

Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In ACL
2010, Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, July 11-
16, 2010, Uppsala, Sweden, pages 384–394.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344, Dublin, Ireland, August. Dublin
City University and Association for Computational
Linguistics.

540


