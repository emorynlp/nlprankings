



















































Semantic Grounding in Dialogue for Complex Problem Solving


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 841–850,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Semantic Grounding in Dialogue for Complex Problem Solving 
 
 

Xiaolong Li Kristy Elizabeth Boyer  
 

Department of Computer Science 
North Carolina State University 

Department of Computer Science 
North Carolina State University 

Raleigh, NC, 27695 Raleigh, NC, 27695 
xli30@ncsu.edu keboyer@ncsu.edu 

 
  

 
Abstract 

Dialogue systems that support users in 
complex problem solving must interpret 
user utterances within the context of a dy-
namically changing, user-created problem 
solving artifact. This paper presents a novel 
approach to semantic grounding of noun 
phrases within tutorial dialogue for com-
puter programming. Our approach per-
forms joint segmentation and labeling of 
the noun phrases to link them to attributes 
of entities within the problem-solving envi-
ronment. Evaluation results on a corpus of 
tutorial dialogue for Java programming 
demonstrate that a Conditional Random 
Field model performs well, achieving an 
accuracy of 89.3% for linking semantic 
segments to the correct entity attributes. 
This work is a step toward enabling dia-
logue systems to support users in increas-
ingly complex problem-solving tasks.  

1 Introduction 

In the dialogue systems research community, there 
is growing recognition that dialogue systems need 
to support users in increasingly complex tasks. To 
move in this direction, dialogue systems must per-
form natural language understanding within richer 
and richer contexts, and this understanding in-
cludes semantic interpretation of user utterances 
(Traum, et al., 2012, Rudnicky, et al., 1999). Pre-
vious approaches for semantic interpretation in-
clude domain-specific grammars (Lemon et al., 
2001) and open-domain parsers together with a 
domain-specific lexicon (Rosé, 2000). However, 

existing techniques are not sufficient to support 
increasingly complex problem-solving dialogues 
due to several challenges. For example, domain-
specific grammars become intractable when ap-
plied to more ill-formed domains,	   and open-
domain parsers may not perform well across do-
mains (McClosky et al., 2010).  

The call for addressing these limitations is par-
ticularly strong for dialogue systems that help peo-
ple learn, such as tutorial dialogue systems. 
Today’s tutorial dialogue systems engage in natu-
ral language dialogue in support of tasks such as 
solving qualitative physics problems (VanLehn et 
al., 2002), understanding computer architecture 
and physics (Graesser et al., 2004), and predicting 
behavior of electrical circuits (Dzikovska et al., 
2011). Although these systems differ in many 
ways, they have an important commonality: in or-
der to semantically interpret user dialogue utter-
ances, these systems ground the utterances in a 
fixed domain description that is an integral part of 
the engineered system. This characteristic is shared 
by most dialogue systems, which ground their dia-
logue in manually defined domain-specific ontolo-
gies, such as for the task of booking flights (Allen, 
et al., 2001), checking bus schedules (Raux, 2004), 
and finding restaurants (Young et al., 2007).  

These task-oriented domains, though they pre-
sent a rich set of research challenges, stand in stark 
contrast to a complex problem-solving domain in 
which the user is creating an artifact to solve a 
problem. Yet the psychology literature tells us that 
complex problem solving is an essential activity in 
human learning (Greiff et al., 2013; Mayer et al., 
2006; Funke, 2010). In such a domain, understand-
ing user dialogue utterances involves grounding 
them within an infinite set of possible user-created 

841



artifacts, not within a system ontology. This paper 
focuses on the complex problem-solving domain of 
introductory computer programming.  In this do-
main the user might say, for example, “Is myVar-
iable supposed to be an int?” where 
myVariable refers to the name of a variable 
within the computer program that the user has cre-
ated. The semantic interpretation task in this case 
is akin to situated dialogue where user utterances 
must be grounded within a physical environment 
(Liu et al., 2014, Gorniak et al., 2007). However, 
even these situated dialogue models typically rely 
on a world defined by a limited number of entities 
(e.g., a chair or a cup).  

To address these challenges, this paper presents 
a step toward semantic grounding for complex 
problem-solving dialogues, in which the number of 
potential entities (e.g., a Java variable or a piece of 
code) is infinite. The present work focuses on the 
semantic understanding of noun phrases, which 
tend to bear significant semantic information for 
each utterance. Although noun phrases are typical-
ly small in their number of tokens, their complexi-
ty and semantics vary in important ways. For 
example, in the domain of computer programming, 
two similar noun phrases such as “the 2 dimen-
sional array” and “the 3 dimensional array” refer to 
two different entities within the problem-solving 
artifact. Inferring the semantic structure of the 
noun phrases is necessary to differentiate these two 
references within a dialogue, to ground them in the 
task, and to respond to them appropriately.  

This noun phrase grounding task is similar to 
coreference resolution, which discovers the rela-
tionship between pairs of noun phrases in a piece 
of natural language text (Culotta, Wick, & 
Mccallum, 2007; Lappin & Leass, 1994). Howev-
er, different from coreference resolution, noun 
phrase grounding links natural language expres-
sions to entities in a real world environment. The 
current approach leverages the structure of noun 
phrases, mapping their segments to attributes of 
entities to which they should be semantically 
linked. In order to overcome the limitation of need-
ing to fully enumerate the entities in the environ-
ment, we represent the entities as automatically 
extracted vectors of attributes. We then perform 
joint segmentation and labeling of the noun 
phrases in user utterances to map them to the entity 
vectors (used to describe entities within the envi-
ronment). This mapping of noun phrases to real-

world attributes is the grounding task focused on in 
this work. The results show that a Conditional 
Random Field performs well for this task, achiev-
ing 89.3% accuracy. Moreover, even in the ab-
sence of lexical features (using only dependency 
parse features and parts of speech), the model 
achieves 71.3% accuracy, indicating that it may be 
tolerant to unseen words. The flexibility of this 
approach is due in part to the fact that it does not 
rely on a syntactic parser’s ability to accurately 
segment within noun phrases, but rather includes 
parse features as just one type of feature among 
several made available to the model. Finally, in 
contrast to methods based on bag-of-words such as 
latent semantic analysis, the proposed approach 
models the structure of noun phrases to facilitate 
specific grounding within an artifact. 

 The remainder of this paper is structured as fol-
lows. Section 2 presents related work on semantic 
interpretation and on natural language interpreta-
tion for tutorial dialogue. Section 3 describes the 
corpus and highlights some of the characteristics of 
dialogue for complex problem solving. The seman-
tic interpretation approach is introduced in Section 
4, with the experiments and results presented in 
Section 5. Section 6 concludes with important di-
rections for future work.  

2 Related Work 

The approach presented in this paper draws upon a 
rich foundation of research in semantic interpreta-
tion and specifically upon dialogue interpretation 
for tutorial dialogues. Each of these areas of relat-
ed work is discussed in turn.  

2.1 Semantic Interpretation 

The current work is closely related to several well-
established research directions within the computa-
tional linguistics literature: semantic role labeling, 
semantic parsing, and language grounding. Seman-
tic tagging assigns a semantic role label to text 
segments in a sentence (Pradhan, et. al, 2004). The 
set of semantic roles are relatively coarse-grained, 
not mapping to specific entities within the world. 
In contrast, the approach used in this paper does 
perform semantic role labeling, but the semantic 
grounding of these text segments are extracted at 
the same time. Semantic parsing addresses a more 
complex problem than semantic role labeling: in-

842



terpreting the semantic structure of a sentence. Su-
pervised semantic parsing requires a target logical 
form for each sentence, which is costly (Zettle-
moyer et. al, 2012). Unsupervised methods rely on 
accurate dependency parsing, and the semantics 
learned with unsupervised methods are not directly 
grounded in a domain (Poon et al., 2009). Our ap-
proach does not require a logical form or accurate 
parse in order to train the model. 

Another aspect of semantic interpretation in-
volves language grounding, which links natural 
language to representations of entities in the (often 
physical) world directly. Matuszek et al. (2012) 
propose a joint language/perception model to learn 
attribute names in a physical environment. Barnard 
et al. (2003) learn interpretation of segments of 
images in words with a number of models. Liu et 
al. (2014) label the referential entities in a collabo-
rative discourse with graph mapping. All of these 
approaches work in scenarios in which the number 
of entities is limited. This is different from the case 
of a complex problem-solving domain in which 
there could be infinitely many combinations of 
entities and surface forms of the problem-solving 
artifact. Thus, building an entity graph to model 
the relationships between entities would be intrac-
table. Grounding based on semantic interpretation 
using our approach will address this problem since 
it first narrows down the category of entity for a 
noun phrase and then grounds within a family of 
factorized vectors.  

2.2 Language Understanding in Tutorial Dia-
logue Systems  

All dialogue systems employ some form of seman-
tic interpretation. Within tutorial dialogue, some 
dialogue interpretation relies on a manually de-
fined domain-specific grammar and lexicon (Lem-
on et al., 2001, Evens et al., 2005). CIRCSIM-
Tutor (Evens et al., 2005), a tutorial dialogue sys-
tem in the domain of cardiovascular physiology, 
uses a set of finite state transducers and a domain-
specific lexicon. Such domain-specific grammars 
are successful within well-formed domains but 
become unwieldy in larger or ill-defined domains.  

Another approach is to employ an open-domain 
parser in combination with domain-specific 
knowledge. CARMEL (Rosé, 2000) is a natural 

language understanding component that has been 
used in multiple dialogue systems (Zinn et al., 
2000; Litman, 2004; VanLehn et al., 2002). 
CARMEL uses a semantic interpretation frame-
work that performs semantic interpretation with 
semantic constructor functions during syntactic 
parsing. The semantic interpretation employs en-
coded domain-specific semantic knowledge and a 
frame-based representation.  

 Dzikovska et al. (2007) proposed an approach 
that divides the logical form representation of ut-
terances and the knowledge representation ontolo-
gy in order to make a NLU component adaptable 
for multiple domains. The logical form representa-
tion contains high-level word sense and semantic 
role labels. Then, a contextual interpreter is em-
ployed for mapping between the logical form and 
the domain ontology. This work still relies on an 
open domain parser to generate the logical forms. 

Different from all of the approaches mentioned 
above, AutoTutor (Graesser et al., 2004) uses la-
tent semantic analysis (LSA) to evaluate students’ 
utterances by comparing them to a handcrafted 
expected answer. LSA represents semantics as a 
high-dimensional vector and computes similarity 
between pieces of text. As a bag-of-words ap-
proach, LSA does not capture the kind of semantic 
structure that facilitates specific language ground-
ing in an environment.  

3 Corpus of Complex Problem Solving Di-
alogue 

Complex problem solving is defined within the 
psychology literature as the process of reaching a 
goal state by applying multiple problem solving 
skills, when the desired goal state cannot simply be 
reached by applying one from a set of existing so-
lution patterns (Greiff et al., 2013; Mayer et al., 
2006; Funke, 2010). Dialogue surrounding com-
plex problem solving is therefore grounded within 
a problem-solving artifact that could have infinite-
ly many surface forms. The complex problem-
solving domain that is the focus of this paper is 
computer programming, specifically Java pro-
gramming, and the corpus under consideration re-
flects textual tutorial dialogue exchanged between 
two humans in support of that problem solving.  

843



The corpus was collected within a tutorial dia-
logue study in which human tutors and students 
interacted through a tutorial dialogue interface that 
supported remote textual communication (Boyer et 
al., 2011) The tutorial dialogue interface (Figure 1) 
consists of two windows that display interactive 
components: the student’s Java code, the compila-
tion or execution output associated with the code, 
and the textual dialogue messages between the stu-
dent and tutor. All of the information in these two 
windows was synchronized between the student’s 
screen and tutor’s screen in real time.  

The corpus contains 45 Java programming tuto-
rial sessions from student-tutor pairs, with a total 
of 4857 utterances, an average of 108 utterances 
per session. For the current work, six of these tuto-
rial sessions were manually annotated for their se-
mantic grounding (as described in Section 5), a 
total of 758 utterances. The problem students 
solved during this tutorial dialogue involved creat-
ing, traversing, and modifying parallel arrays. This 
task was challenging for students and represented a 
complex problem-solving effort since the students 
were novices who were enrolled in an introductory 
computer programming class.  

The dialogues within this domain are character-
ized by situated features that pertain to the pro-
gramming task. A portion of user utterances refer 

to general Java knowledge, and in these cases se-
mantic interpretation can be accomplished by 
mapping to a domain-specific ontology (e.g., 
Dzikovska et al., 2007). In contrast, many utter-
ances refer to concrete entities within the dynami-
cally changing, user-created programing artifact. 
Identifying these entities correctly is crucial for 
generating specific tutorial dialogue moves. A dia-
logue excerpt is shown in Figure 2.  

4 Methodology 

To ground the dialogue utterances as described in 
the previous section, our approach focuses first 
upon noun phrases, which contain rich semantic 
information. This section introduces the approach, 
based on Conditional Random Fields, to jointly 
segment the noun phrases and link those segments 
to entities within the domain. 

4.1 Noun Phrases in Domain Language 

A noun phrase is defined as “a phrase which has a 
noun (or indefinite pronoun) as its head word, or 
which performs the same grammatical function as 
such a phrase” (Crystal, 1997). The syntactic struc-
ture of a noun phrase consists of dependents which 
could include determiners, adjectives, prepositional 
phrases, or even a clause. For example, let us con-

Figure 1. Tutorial dialogue interface. 

844



sider the noun phrase “a 2 dimensional array”. Its 
head is “array” and its dependents are “a” as the 
determiner and “2 dimensional” as an adjective 
phrase. In this simple case the syntactic boundaries 
also indicate semantic segments, as these depend-
ents indicate one or more attributes of the head. If 
this relationship were always true, the semantic 
structure understanding task would be a labeling 
task that only requires assigning a semantic tag to 
each syntactic segment of the noun phrase. But this 
is not always true, in part because a syntactic par-
ser trained on an open-domain corpus will not nec-
essarily perform well on domain language 
(McClosky et al., 2010). For example, in the noun 
phrase “the outer for loop,” which also occurs in 
the Java programming corpus, the head of the noun 
phrase is “for loop,” but the syntactic parse (gener-
ated by the Stanford parser) of this noun phrase 
understandably (but incorrectly) identifies this 
head as part of a prepositional phrase (Figure 3). 

To address this challenge, this paper utilizes a 
joint segmentation and semantic labeling approach 
that does not rely on accurate syntactic parsing 
within noun phrases. In this approach the head and 
dependents of each noun phrase are each referred 
to as a segment, with exactly one segment per de-
pendent, and one or more words per segment. Iden-

tifying these segments correctly is essential to cor-
rect assignment of semantic tags. Pipeline methods 
for semantic segmentation rely on stable perfor-
mance of an open domain parser, but as described 
above, this assumption is not desirable for ground-
ing some domain language. We therefore utilize 
joint segmentation and labeling and apply a Condi-
tional Random Field approach (Lafferty, 2001), a 
natural choice for the sequential data segmentation 
and labeling problem. 

 
Figure 3. A parse of “the outer for loop” from Stanford 

Parser. 
 

NP 

NP PP 

DT JJ IN NP 

NN the outer for 

loop 

Dialogue Excerpt Corresponding Problem-Solving Artifact 

Figure 2. Dialogue excerpt from the corpus. 

public class PostalFrame extends JFrame implements ActionListener { 
/** the label for the zip code */ 
    private JLabel lblZip; 
     
     /** the text field for the zip code */ 
    private JTextField txtZip; 

       …… 
 /** the translation table */ 
    private int table[][]; 
     
     /** the numerical representation of the zip code */ 
    private int zipCode; 
 
    /** 
     * Answers a PostalFrame object to create a simple GUI 
     * to produce a postal bar code for the user's zip code 
     */ 

   …… 
public void actionPerformed(ActionEvent e) { 
 
        String zip = new String(); 
 
        if (e.getSource() == btnZip) { 
            // take text from textfield and trim off any 
            // whitespace at either end 
            zip = txtZip.getText().trim(); 

…… 

Tutor we also have the zipCode int, which  
can be turned into a string 

Student isn't that also declared in the same 
place? 

Tutor yes, but txtZip isn't a String, it's a text 
field for the gui 

 so look in actionPerformed 
Tutor yeah, see how they get the text from 

it? 
Tutor you could copy and use that code if 

you wanted 
Student okay, i'll try that. 
Tutor you’ll want to use the .trim() part too 
Student yeah, you had it 
 

 

 

 

 

 

 

845



4.2 Description Vector 

The goal is to ground each noun phrase to attrib-
utes of entities within the problem-solving artifact, 
which constitutes the “world” in this domain. To 
do this, we will link each semantic segment in a 
noun phrase to an attribute of an entity in the 
world. Because the world can contain any of an 
infinite set of user-created entities, representation 
cannot rely upon exhaustively enumerating the 
entities. To represent an entity in the domain, we 
define a description vector V which defines the 
attribute types for entities in the domain. Then, an 
entity O in the domain is represented uniquely by 
an instance of V. The values of each Vi indicate the 
value of the attribute i of O, as illustrated in Table 
1. This definition of the description vector relies 
upon the structure of the domain by factorizing the 
attributes of entities. 

With this representation, grounding a noun 
phrase involves linking each segment of the noun 
phrase to an attribute in the description vector. 
Formally, we represent a noun phrase as a series of 
segments: 

NP =< s1, s2,..., sk >  
where si is the ith segment in this noun phrase. A 

noun phrase is also a sequence of words: 
NP =< w1,w2,...,wn >  

where each wj is the jth word in the noun phrase. 
Therefore each segment is a series of words:    

si =< wj,wj+1,...,wj+l−1 >  
where l is the length of semantic segment i. Given a noun phrase, the segmentation problem 
is thus choosing a segmentation that maximizes the 
following conditional probability: 

p(< s1, s2,..., sk > | < w1,w2,...,wn >)  
Complementary to the segmentation problem is the 
semantic linking problem, which is to link si to an 
attribute ai, which is the label of the ith attribute in 
the entity description vector. That is, we wish to 
maximize the probability of the attribute label se-
quence a given the segments of the noun phrase: 

p(< a1,a2,...,ak > | < s1, s2,..., sk >)  
Taking consecutive words with the same attrib-

ute label as the same semantic segment, the noun 
phrase segmentation and semantic linking problem 
is then:  

argmax
a

p(< a1,a2,...,an > | < w1,w2,...,wn >)  

In the tag sequence <a1, a2, …, an>, if ai and ai+1 
are the same, then wi and wi+1 are assigned to the 
same semantic segment with tag ai. The process of 
segmentation and semantic linking is illustrated in 
Figure 4.  

 
Figure 4. Segmentation and semantic linking of NP 

“a 2 dimensional array.” 

4.3 Joint Segmentation and Labeling 

In order to perform this joint segmentation and 
labeling, we utilize a Conditional Random Field 
(CRF), which is a classic approach for sequence 
segmentation and labeling (Lafferty et al., 2001). 
Given the linear nature of our data, we employ a 
linear chain CRF. Specifically, given a sequence of 
words w, the probability of a label sequence a is 
defined as  

p(a |w) = 1
Z(w)

exp( λ j f j (i,w,ai,ai−1)j=1
m

∑i=1
n

∑ )  
where fj(i,w,ai,ai-1) is a feature function. The 
weights λ j of this feature function are learned 
within the training process. The normalization 
function Z(w) is the sum over the weighted feature 
function for all possible label sequences:  

Z(w) = exp( λ j f j (i,w,ai,ai−1)j=1
m

∑i=1
n

∑ )a∑  
The optimal labeling â  is the one that maximizes 
the likelihood of the training set, where K is the 
number of noun phrases in the corpus. 

â = argmax( logP(a(i) |w(i) )
i=1

K
∑ )

 
 
 
 
 

“a 2 dimensional array" 

w1 w2 w3 w4  

NUM ARR_DIM ARR_DIM CATEG. 

a1 a2 a3 a4  

s1 s2  s3 

NUM ARR_DIM CATEG. 

846



4.4 Features 

Next, we introduce the features used to train the 
CRF. The feature function fj(i,w,ai,ai-1) was defined 
as a binary function, in which w is a feature value. 
We use both lexical and syntactic features. In a 
trained CRF model, the value of fi(i,w,ai,ai-1) is 
known given a combination of parameters 
(i,w,ai,ai-1). The features used in the CRF model 
include words themselves, word lemmas, parts of 
speech, and dependency relationships from the 
syntactic parse. The word itself, lemmatized words 
and parts-of-speech have all been shown useful 
within segmentation and labeling tasks, so they are 
made available here (Xue et al., 2004). Each of 
these features is represented as categorical data. 
For example, a word is represented as its index in a 
list of all of the words that appeared in the corpus.  

The dependency structure of natural language 
has also been shown to be important in semantic 
interpretation (Poon et al., 2009).  This paper em-
ploys a dependency feature vector extracted from 
dependency parses. The head word of each noun 
phrase is the root of the dependency tree. Each de-
pendent is a sub-tree directly under the head. We 
design the dependency feature as a sequence of 
dependency labels as follows. 

Given a dependency tree, words in each seman-
tic segment of the noun phrase are assigned a tag 
according to the relationship between them and the 
head. The relationship between each segment and 

head is defined by the dependency type in the de-
pendency tree. For example, the dependency tree 
of “a 2 dimensional array” is shown in Figure 5. 
The dependency features are 
<det,amod,amod,root>. In this way, the dependen-
cy information from an open-domain parser is en-
coded as a feature to the semantic grounding 
model.  

Figure 5. Dependency structure of  
“a 2 dimensional array.” 

5 Experiments & Results 

The goal of the experiments is to determine how 
well the trained CRF can segment noun phrases 
and link these segments to the correct attribute of 
entities in the world. This section presents the ex-
periments using CRFs trained and tested on the 
Java programming tutorial dialogue corpus. As 
described below, the results were evaluated by 
comparing with manually labeled data.   

Noun phrases from the tutorial dialogues were 
first manually extracted and annotated as to their 
slots in the description vector described in Section 

Attributes Meaning (in Java programming) Example 
CATEGORY Category of an entity Method, Variable, etc.  
NAME Variable name; often user-created extractDigit 
VAR_TYPE Type of variable int, String, etc. 
NUMBER Number of entities 2 
IN_CLASS The class that contains this entity postalFrame 
IN_METHOD The method that contains this entity actionPerformed 
DIR_PARENT Direct parent entity For_Statement, Method 
LINE_NUMBER Line number 67 
SUPER_CLASS Superclass of this entity JFrame 
MODIFIER Access modifier public, private, etc. 
ARRAY_TYPE Type of Array int, char, etc. 
ARRAY_DIMENSION Dimension of array 2, 1 

OBJ_CLASS The class an object instantiates PostalBarCode 
RETURN_TYPE Return type String, int, etc. 
OTHER Other attributes the, extra, etc. 

Table 1. Elements of entity description vector to which noun phrases are mapped. 
 

head 

det 

array 

a 2 dimensional 

amod 

dependent 2 dependent 1 

847



4.2. There were 364 grounded noun phrases ex-
tracted manually from the six tutorial dialogue ses-
sions used in the current work. Each of these noun 
phrases extracted has one or multiple correspond-
ing entities in the programming artifact. Since each 
word in a noun phrase is linked to an element in 
the description vector, the indices in this vector 
were used as the label for each word. Annotation 
of all 346 noun phrases was performed by one an-
notator, and 20% of the noun phrases (70 noun 
phrases) were doubly annotated by an independent 
second annotator. The percent agreement was 
85.3% and the Kappa was 0.765.  

To extract features, the lemmatization and syn-
tactic parsing were performed with the Stanford 
CoreNLP toolkit (Manning et al., 2014). Then, a 
CRF was trained to predict the label for each word 
in a new noun phrase.  The training was performed 
with the crfChain toolbox (Schmidt, 2008). 

We use ten-fold cross-validation to evaluate the 
performance of the CRF in this problem. Results 
with different feature combinations are shown in 
Table 2. Manually labeled data were taken as 
ground truth for computing accuracy, which is de-
fined as the percentage of segments correctly la-
beled. 

Recall that consecutive words with the same la-
bel in a noun phrase are treated as a segment. 
Therefore, if a segment sCRF identified by the CRF 
has the same boundary and the same label as a 
segment sHuman in the noun phrase containing sCRF, 
this segment sCRF will be counted as a correct seg-
ment. Otherwise, sCRF will be counted as incorrect. 
The accuracy is then calculated as the number of 
correct segments identified by the CRF divided by 
the number of segments annotated manually. As 
can be seen in Table 2, all of the models perform 
substantially better than a minimal majority class 
baseline of 43%, which would result from taking 
each word as a segment and assigning it with the 
most frequent attribute label.  

The results demonstrate important characteris-
tics of the segmentation and labeling model. First, 
unlike most previous semantic interpretation work, 
our semantic interpretation of noun phrases does 
not rely on accurate syntactic parse within noun 
phrases. Rather, we use a dependency parse from 
an open-domain parser as only one of several types 
of features provided to the model. These depend-
ency features improved the model in most feature 
combinations (Table 2). The feature combination 

of words, lemmas, and dependency parses 
achieved the best accuracy, which is 4.8% higher 
than the model that only used word features. This 
difference is statistically significant (Wilcoxon 
rank-sum test; n=10; p=0.02).  
 

features accuracy 
word 84.5% 
word + lemma 85.5% 
Word + Dep 87.22% 
lemma + Dep 89.1% 
word + lemma + Dep 89.3% 
word + lemma + POS 86.9% 
word + lemma + POS + Dep 88.7% 
POS + Dep 71.3% 

Table 2. Labeling accuracy. 

Notably, the combination of part-of-speech fea-
tures and dependency parse features still performed 
at 71.3% accuracy, indicating that to some extent, 
the method may be tolerant to unseen words. 

6 Conclusion and Future Work 

This paper has presented a technique for semantic 
grounding of noun phrases in a complex problem-
solving domain, tutorial dialogue for computer 
programming. By performing joint segmentation 
and labeling of noun phrases from user utterances, 
and mapping those segments to attributes of enti-
ties within the problem solving artifact, we have 
made a first step toward grounding complex prob-
lem-solving dialogue within a dynamically chang-
ing artifact from a potentially infinite set of surface 
forms. While trained on a small subset of the cor-
pus, the high accuracy of this model indicates that 
it may be successfully applied to the larger corpus 
without extensive additional manual annotations.  

Several directions of future work are very prom-
ising. In order to fully support users in complex 
problem-solving dialogues, the field must move 
toward richer grounding of natural language utter-
ances within complex artifacts across many do-
mains. Additionally, generating specific and 
tailored dialogue feedback grounded in the artifact 
is a complementary area of research that holds the 
potential to increase the effectiveness of dialogue 
systems for supporting problem solving. It is hoped 
that this line of investigation will lead to dialogue 
systems that smoothly support a much broader 
range of human endeavors.  

848



Acknowledgments 
The authors wish to thank the members of the 
LearnDialogue group, especially Joseph Wiggins, 
at North Carolina State University for their helpful 
input. This work is supported in part by the Na-
tional Science Foundation through grants IIS-
1409639 and the STARS Alliance, CNS-1042468. 
Any opinions, findings, conclusions, or recom-
mendations expressed in this report are those of the 
authors, and do not necessarily represent the offi-
cial views, opinions, or policy of the National Sci-
ence Foundation. 

References 
Allen, J. F., Byron, D. K., Dzikovska, M., Ferguson, G., 

Galescu, L., & Stent, A. (2001). Toward Conversa-
tional Human-Computer Interaction. AI Magazine, 
22(4), 27. 

Barnard, K., Forsyth, D., & Jordan, M. I. (2003). 
Matching Words and Pictures. Journal of Machine 
Learning Research, 3, 1107–1135. 

Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, 
M. D., Vouk, M. A., & Lester, J. C. (2011). Investi-
gating the Relationship Between Dialogue Structure 
and Tutoring Effectiveness: A Hidden Markov Mod-
eling Approach. International Journal of Artificial 
Intelligence in Education (IJAIED), 21(1), 65–81. 

Crystal, D. (1997). A Dictionary of Linguistics and 
Phonetics (4th ed.). Oxford University Press. 

Culotta, A., Wick, M., & Mccallum, A. (2007). First-
Order Probabilistic Models for Coreference Resolu-
tion. In Proceedings of the 2007 Annual Conference 
of the North American Chapter of the Association for 
Computational Linguistics (NAACL) (pp. 81–88). 

Dzikovska, M. O., Allen, J. F., & Swift, M. D. (2007). 
Linking Semantic and Knowledge Representations in 
a Multi-Domain Dialogue System. Journal of Logic 
and Computation, 18(3), 405–430. Retrieved from 
http://logcom.oxfordjournals.org/cgi/doi/10.1093/log
com/exm067 

Dzikovska, M. O., Isard, A., Bell, P., Moore, J. D., 
Steinhauser, N., & Campbell, G. (2011). BEETLE II: 
An Adaptable Tutorial Dialogue System. Proceed-
ings of the 12th Annual SIGdial Meeting on Dis-
course and Dialogue, 338–340. 

Evens, M., & Michael, J. (2005). One-on-One Tutoring 
by Humans and Computers. Psychology Press. 

Funke, J. (2010). Complex problem solving: A case for 
complex cognition? Cognitive Processing, 11(2), 
133–142. 

Gorniak, P., & Roy, D. (2007). Situated Language Un-
derstanding as Filtering Perceived Affordances. Cog-
nitive Science, 31(2), 197–231. 

Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H., 
Ventura, M., Olney, A., & Louwerse, M. M. (2004). 
AutoTutor: A Tutor with Dialogue in Natural Lan-
guage. Behavior Research Methods, Instruments, & 
Computers, 36(2), 180–192. 

Greiff, S., Wüstenberg, S., Holt, D. V., Goldhammer, 
F., & Funke, J. (2013). Computer-based Assessment 
of Complex Problem Solving: Concept, Implementa-
tion, and Application. Educational Technology Re-
search and Development, 61(3), 407–421. 

Lafferty, J., McCallum, A., & Pereira, F. C. (2001). 
Conditional Random Fields: Probabilistic Models for 
Segmenting and Labeling Sequence Data. In Pro-
ceedings of the International Conference on Machine 
Learning (pp. 282–289). 

Lappin, S., & Leass, H. J. (1994). An Algorithm for 
Pronominal Anaphora Resolution. Computational 
Linguistics, 20(4), 535–561. 

Lemon, O., Bracy, A., Gruenstein, A., & Peters, S. 
(2001). The WITAS Multi-Modal Dialogue System I. 
In Proceedings of INTERSPEECH (pp. 1559–1562). 

Litman, D. J. (2004). ITSPOKE  : An Intelligent Tutor-
ing Spoken Dialogue System. In Demonstration Pa-
pers at the 2004 Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) (pp. 5–8). 

Liu, C., She, L., Fang, R., & Chai, J. Y. (2014). Proba-
bilistic Labeling for Efficient Referential Grounding 
Based On Collaborative Discourse. In Proceedings of 
the 52nd Annual Meeting of the Association for 
Computational Linguistics (ACL) (pp. 13–18). 

Manning, C. D., Bauer, J., Finkel, J., & Bethard, S. J. 
(2014). The Stanford CoreNLP Natural Language 
Processing Toolkit. In the 52nd Annual Meeting of 
the Association for Computational Linguistics: Sys-
tem Demonstrations (pp. 55–60). 

Matuszek, C., Fitzgerald, N., Zettlemoyer, L., Bo, L., & 
Fox, D. (2012). A Joint Model of Language and Per-
ception for Grounded Attribute Learning. In Pro-
ceedings of the 29th International Conference on 
Machine Learning. 

Mayer, R. E., & Wittrock, M. C. (2006). Problem Solv-
ing. Handbook of Educational Psychology, 2, 287–
303. 

849



McClosky, D., Charniak, E., & Johnson, M. (2010). 
Automatic Domain Adaptation for Parsing. In Pro-
ceedings of the 2010 Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) (pp. 28–36). 

Poon, H., & Domingos, P. (2009). Unsupervised Se-
mantic Parsing. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP) (pp. 1–10). 

Pradhan, S. S., Ward, W., Hacioglu, K., Martin, J. H., & 
Jurafsky, D. (2004). Shallow Semantic Parsing using 
Support Vector Machines. In Proceedings of the 
2004 Annual Conference of the North American 
Chapter of the Association for Computational Lin-
guistics (NAACL) (pp. 233–240). 

Raux, A., & Eskenazi, M. (2004). Non-Native Users in 
the Let’s Go!! Spoken Dialogue System: Dealing 
with Linguistic Mismatch. In Proceedings of the 
2004 North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL) (pp. 217–
224). 

Rosé, C. P. (2000). A Framework for Robust Semantic 
Interpretation. In Proceedings of the 1st North Amer-
ican Chapter of the Association for Computational 
Linguistics Conference (NAACL) (pp. 311–318). 

Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., 
Shern, R., Lenzo, K., … Oh, A. (1999). Creating 
Natural Dialogs in the Carnegie Mellon Communica-
tor System. In Proceedings of the Sixth European 
Conference on Speech Communication and Technol-
ogy, (EUROSPEECH) (Vol. 4, pp. 1531–1534). 

Schmidt, M., & Swersky, K. (2008). 
http://www.cs.ubc.ca/~schmidtm/Software/crfChain.
html. 

Traum, D., Devault, D., Lee, J., Wang, Z., & Marsella, 
S. (2012). Incremental Dialogue Understanding and 
Feedback for Multiparty, Multimodal Conversation. 
Intelligent Virtual Agents, 7502, 275–288. 

VanLehn, K., Jordan, P. W., Rosé, C. P., Bhembe, D., 
Bottner, M., Gaydos, A., … Roque, A. (2002). The 
Architecture of Why2-Atlas: A Coach for Qualitative 
Physics Essay Writing. In Proceedings of the Sixth 
International Conference on Intelligent Tutoring Sys-
tems (Vol. 2363, pp. 158–167). Springer. 

Xue, N., & Palmer, M. (2004). Calibrating Features for 
Semantic Role Labeling. In Proceedings of the 2004 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) (pp. 88–94). 

Young, S., Schatzmann, J., Weilhammer, K., & Ye, H. 
(2007). The Hidden Information State Approach to 

Dialog Management. Acoustics, Speech and Signal 
Processing (Vol. 4, pp. 149-152). 

Zettlemoyer, L. S., & Collins, M. (2012). Learning to 
Map Sentences to Logical Form: Structured Classifi-
cation with Probabilistic Categorial Grammars. In 
Proceedings of the Twenty First Conference on Un-
certainty in Artificial Intelligence (pp. 658–666). 

Zinn, C., Moore, J. D., Core, M. G., & Varges, S. 
(2000). The BE&E Tutorial Learning Environment ( 
BEETLE ). In Proceedings of the Seventh Workshop 
on the Semantics and Pragmatics of Dialogue. 

 

850


