










































HyTER: Meaning-Equivalent Semantics for Translation Evaluation


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162–171,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

HyTER: Meaning-Equivalent Semantics for Translation Evaluation

Markus Dreyer
SDL Language Weaver

6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA

mdreyer@sdl.com

Daniel Marcu
SDL Language Weaver

6060 Center Drive, Suite 150
Los Angeles, CA 90045, USA

dmarcu@sdl.com

Abstract

It is common knowledge that translation is
an ambiguous, 1-to-n mapping process, but
to date, our community has produced no em-
pirical estimates of this ambiguity. We have
developed an annotation tool that enables us
to create representations that compactly en-
code an exponential number of correct trans-
lations for a sentence. Our findings show that
naturally occurring sentences have billions of
translations. Having access to such large sets
of meaning-equivalent translations enables us
to develop a new metric, HyTER, for transla-
tion accuracy. We show that our metric pro-
vides better estimates of machine and human
translation accuracy than alternative evalua-
tion metrics.

1 Motivation

During the last decade, automatic evaluation met-
rics (Papineni et al., 2002; Snover et al., 2006; Lavie
and Denkowski, 2009) have helped researchers ac-
celerate the pace at which they improve machine
translation (MT) systems. And human-assisted met-
rics (Snover et al., 2006) have enabled and sup-
ported large-scale U.S. government sponsored pro-
grams, such as DARPA GALE (Olive et al., 2011).
However, these metrics have started to show signs of
wear and tear.

Automatic metrics are often criticized for provid-
ing non-intuitive scores – few researchers can ex-
plain to casual users what a BLEU score of 27.9
means. And researchers have grown increasingly
concerned that automatic metrics have a strong bias

towards preferring statistical translation outputs; the
NIST (2008, 2010), MATR (Gao et al., 2010) and
WMT (Callison-Burch et al., 2011) evaluations held
during the last five years have provided ample ev-
idence that automatic metrics yield results that are
inconsistent with human evaluations when compar-
ing statistical, rule-based, and human outputs.

In contrast, human-informed metrics have other
deficiencies: they have large variance across human
judges (Bojar et al., 2011) and produce unstable re-
sults from one evaluation to another (Przybocki et
al., 2011). Because evaluation scores are not com-
puted automatically, systems developers cannot au-
tomatically tune to human-based metrics.

Table 1 summarizes the dimensions along which
evaluation metrics should do well and the strengths
and weaknesses of the automatic and human-
informed metrics proposed to date. Our goal is
to develop metrics that do well along all these di-
mensions. The fundamental insight on which our
research relies is that the failures of current auto-
matic metrics are not algorithmic: BLEU, Meteor,
TER (Translation Edit Rate), and other metrics ef-
ficiently and correctly compute informative distance
functions between a translation and one or more hu-
man references. We believe that these metrics fail
simply because they have access to sets of human
references that are too small. If we had access to
the set of all correct translations of a given sentence,
we could measure the minimum distance between a
translation and the set. When a translation is perfect,
it can be found in the set, so it requires no editing to
produce a perfect translation. Therefore, its score
should be zero. If the translation has errors, we can

162



Desiderata Auto. Manu. HyTER
Metric is intuitive N Y Y
Metric is computed automatically Y N Y
Metric is stable and reproducible from one evaluation to another Y N Y
Metric works equally well when comparing human and automatic outputs
and when comparing rule-based, statistical-based, and hybrid engines

N Y Y

System developers can tune to the metric Y N Y
Metric helps developers identify deficiencies of MT engines N N Y

Table 1: Desiderata of evaluation metrics: Current automatic and human metrics, proposed metric.

efficiently compute the minimum number of edits
(substitutions, deletions, insertions, moves) needed
to rewrite the translation into the “closest” reference
in the set. Current automatic evaluation metrics do
not assign their best scores to most perfect transla-
tions because the set of references they use is too
small; their scores can therefore be perceived as less
intuitive.

Following these considerations, we developed an
annotation tool that enables one to efficiently create
an exponential number of correct translations for a
given sentence, and present a new evaluation met-
ric, HyTER, which efficiently exploits these mas-
sive reference networks. In the rest of the paper, we
first describe our annotation environment, process,
and meaning-equivalent representations that we cre-
ate (Section 2). We then present the HyTER met-
ric (Section 3). We show that this new metric pro-
vides better support than current metrics for machine
translation evaluation (Section 4) and human trans-
lation proficiency assessment (Section 5).

2 Annotating sentences with exponential
numbers of meaning equivalents

2.1 Annotation tool

We have developed a web-based annotation tool
that can be used to create a representation encoding
an exponential number of meaning equivalents
for a given sentence. The meaning equivalents
are constructed in a bottom-up fashion by typing
translation equivalents for larger and larger phrases.
For example, when building the meaning equiv-
alents for the Spanish phrase “el primer ministro
italiano Silvio Berlusconi”, the annotator first types
in the meaning equivalents for “primer ministro”
– 〈prime-minister; PM; prime minister; head of
government; premier; etc.〉; “italiano” – 〈Italian〉;

and “Silvio Berlusconi” – 〈Silvio Berlusconi;
Berlusconi〉. The tool creates a card that stores
all the alternative meanings for a phrase as a
determinized FSA and gives it a name in the target
language that is representative of the underly-
ing meaning-equivalent set: [PRIME-MINISTER],
[ITALIAN], and [SILVIO-BERLUSCONI]. Each base
card can be thought of expressing a semantic con-
cept. A combination of existing cards and additional
words can be subsequently used to create larger
meaning equivalents that cover increasingly larger
source sentence segments. For example, to create
the meaning equivalents for “el primer ministro ital-
iano” one can drag-and-drop existing cards or type
in new words: 〈the [ITALIAN] [PRIME-MINISTER];
the [PRIME-MINISTER] of Italy〉; to create the
meaning equivalents for “el primer ministro italiano
Silvio Berlusconi”, one can drag-and-drop and type:
〈[SILVIO-BERLUSCONI] , [THE-ITALIAN-PRIME-
MINISTER]; [THE-ITALIAN-PRIME-MINISTER] ,
[SILVIO-BERLUSCONI]; [THE-ITALIAN-PRIME-
MINISTER] [SILVIO-BERLUSCONI] 〉. All meaning
equivalents associated with a given card are ex-
panded and used when that card is re-used to create
larger meaning-equivalent sets.

The annotation tool supports, but does not en-
force, re-use of annotations created by other anno-
tators. The resulting meaning equivalents are stored
as recursive transition networks (RTNs), where each
card is a subnetwork; if needed, these non-cyclic
RTNs can be automatically expanded into finite-
state acceptors (FSAs, see Section 3).

2.2 Data and Annotation Protocols

Using the annotation tool, we have created meaning-
equivalent annotations for 102 Arabic and 102 Chi-
nese sentences – a subset of the “progress set” used
in the 2010 Open MT NIST evaluation (the average

163



sentence length was 24 words). For each sentence,
we had access to four human reference translations
produced by LDC and five MT system outputs,
which were selected by NIST to cover a variety of
system architectures (statistical, rule-based, hybrid)
and performances. For each MT output, we also had
access to sentence-level HTER scores (Snover et al.,
2006), which were produced by experienced LDC
annotators.

We have experimented with three annotation pro-
tocols:

• Ara-A2E and Chi-C2E: Foreign language natives
built English networks starting from foreign lan-
guage sentences.
• Eng-A2E and Eng-C2E: English natives built En-

glish networks starting from “the best translation”
of a foreign language sentence, as identified by
NIST.
• Eng*-A2E and Eng*-C2E: English natives built

English networks starting from “the best transla-
tion”, but had access to three additional, indepen-
dently produced human translations to boost their
creativity.

Each protocol was implemented independently by
at least three annotators. In general, annotators need
to be fluent in the target language, familiar with the
annotation tool we provide and careful not to gen-
erate incorrect paths, but they do not need to be lin-
guists.

2.3 Exploiting multiple annotations

For each sentence, we combine all networks that
were created by the different annotators. We eval-
uate two different combination methods, each of
which combines networks N1 and N2 of two anno-
tators (see an example in Figure 1):

(a) Standard union U(N1, N2): The standard
finite-state union operation combines N1 and N2
on the whole-network level. When traversing
U(N1, N2), one can follow a path that comes from
either N1 or N2.

(b) Source-phrase-level union SPU(N1, N2): As
an alternative, we introduce SPU, a more fine-
grained union which operates on sub-sentence seg-
ments. Here we exploit the fact that each annotator
explicitly aligned each of her various subnetworks

N1
the level of approval was close to zero

the approval rate practically

the approval level was close to zero

the approval rate about equal to

(a)

was zero
the approval rate

the level of approval

the approval level
close to

practically

about equal to

(b)

N2

Figure 1: (a) Finite-state union versus (b) source-phrase-
level union (SPU). The former does not contain the path
“the approval level was practically zero”.

for a given sentence to a source span of that sen-
tence. Now for each pair of subnetworks (S1, S2)
from N1 and N2, we build their union if they are
compatible; two subnetworks S1, S2 are defined to
be compatible if they are aligned to the same source
span and have at least one path in common.

The purpose of SPU is to create new paths by mix-
ing paths from N1 and N2. In Figure 1, for example,
the path “the approval level was practically zero” is
contained in the SPU, but not in the standard union.
We build SPUs using a dynamic programming al-
gorithm that builds subnetworks bottom-up, build-
ing unions of intermediate results. Two larger sub-
networks can be compatible only if their recursive
smaller subnetworks are compatible. Each SPU con-
tains at least all paths from the standard union.

2.4 Empirical findings

Now that we have described how we created partic-
ular networks for a given dataset, we describe some
empirical findings that characterize our annotation
process and the created networks.

Meaning-equivalent productivity. When we
compare the productivity of the three annotation
protocols in terms of the number of reference trans-
lations that they enable, we observe that target lan-
guage natives that have access to multiple human
references produce the largest networks. The me-
dian number of paths produced by one annotator un-
der the three protocols varies from 7.7 × 105 paths
for Ara-A2E, to 1.4 × 108 paths for Eng-A2E, to
5.9 × 108 paths for Eng*-A2E; in Chinese, the me-

164



dians vary from 1.0× 105 for Chi-C2E, to 1.7× 108
for Eng-C2E, to 7.8× 109 for Eng*-C2E.

Protocol productivity. When we compare the
annotator time required by the three protocols, we
find that foreign language natives work faster – they
need about 2 hours per sentence – while target lan-
guage natives need 2.5 hours per sentence. Given
that target language natives build significantly larger
networks and that bilingual speakers are in shorter
supply than monolingual ones, we conclude that us-
ing target language annotators is more cost-effective
overall.

Exploiting multiple annotations. Overall, the me-
dian number of paths produced by a single annota-
tor for A2E is 1.5 × 106, two annotators (randomly
picked per sentence) produce a median number of
4.7 × 107 (Union), for all annotators together it is
2.1× 1010 (Union) and 2.1× 1011 (SPU). For C2E,
these numbers are 5.2× 106 (one), 1.1× 108 (two),
and 2.6×1010 (all, Union) and 8.5×1011 (all, SPU).

Number of annotators and annotation time. We
compute the minimum number of edits and length-
normalized distance scores required to rewrite ma-
chine and human translations into translations found
in the networks produced by one, two, and three
annotators. We find that the length-normalized dis-
tances do not vary by more than 1% when adding the
meaning equivalents produced by a third annotator.
We conclude that 2-3 annotators per sentence pro-
duce a sufficiently large set of alternative meaning
equivalents, which takes 4-7.5 hours. We are cur-
rently investigating alternative ways to create net-
works more efficiently.

Grammaticality. For each of the four human trans-
lation references and each of the five machine trans-
lation outputs (see Section 2.2), we algorithmically
find the closest path in the annotated networks of
meaning equivalents (see Section 3). We presented
the resulting 1836 closest paths extracted from the
networks (2 language pairs ×102 sentences ×9 hu-
man/machine translations) to three independent En-
glish speakers. We asked each English path to be
labeled as grammatical, grammatical-but-slightly-
odd, or non-grammatical. The metric is harsh: paths
such as “he said that withdrawing US force with-

out promoting security would be cataclysmic” are
judged as non-grammatical by all three judges al-
though a simple rewrite of “force” into “forces”
would make this path grammatical. We found that
90% of the paths are judged as grammatical and
96% as grammatical or grammatical-but-slightly-
odd, by at least one annotator. We interpret these
results as positive: the annotation process leads to
some ungrammatical paths being created, but most
of the closest paths to human and machine outputs,
those that matter from an evaluation perspective, are
judged as correct by at least one judge.

Coverage. We found it somewhat disappoint-
ing that networks that encode billions of meaning-
equivalent translations for a given sentence do not
contain every independently produced human ref-
erence translation. The average length-normalized
edit distance (computed as described in Section 3)
between an independently produced human refer-
ence and the corresponding network is 19% for
Arabic-English and 34% for Chinese-English across
the entire corpus. Our analysis shows that about
half of the edits are explained by several non-
content words (“the”, “of”, “for”, “their”, “,”) be-
ing optional in certain contexts; several “obvious”
equivalents not being part of the networks (“that”–
“this”; “so”–“accordingly”); and spelling alterna-
tives/errors (“rockstrom”–“rockstroem”). We hy-
pothesize that most of these ommissions/edits can be
detected automatically and dealt with in an appropri-
ate fashion. The rest of the edits would require more
sophisticated machinery, to figure out, for example,
that in a particular context pairs like “with”–”and”
or “that”–”therefore” are interchangeable.

Given that Chinese is significantly more under-
specified compared to Arabic and English, it is con-
sistent with our intuition to see that the average mini-
mal distance is higher between Chinese-English ref-
erences and their respective networks (34%) than
between Arabic-English references and their respec-
tive networks (19%).

3 Measuring translation quality with large
networks of meaning equivalents

In this section, we present HyTER (Hybrid Trans-
lation Edit Rate), a novel metric that makes use of
large reference networks.

165



<ts>

<root>

Reference RTNReordered
hypothesis

� σ

¬

¬

¬

¬

�

�

�

�

Levenshtein

lazy composition

Πx

LS

H(x,Y)

Y

Hypothesis
x

convert

Figure 2: Defining the search space H(x,Y) through (lazy) composition. x is a translation hypothesis “where train
station is”, Y contains all correct translations. Πx may be defined in various ways, here local reordering (k = 3) is
used.

HyTER is an automatically computed version of
HTER (Snover et al., 2006); HyTER computes the
minimum number of edits between a translation x
and an exponentially sized reference set Y , which is
encoded as a Recursive Transition Network (RTN).
Perfect translations have a HyTER score of 0.

General Setup. The unnormalized HyTER score
is defined as in equation (1) where Πx is a set of
permutations of the hypothesis x, d(x, x′) is the dis-
tance between x and a permutation x′—typically
measured as the number of reordering moves be-
tween the two—and LS(x′, y) is the standard Lev-
enshtein distance (Levenshtein, 1966) between x′

and y, defined as the minimum number of insertions,
deletion, and substitutions. We normalize uhyter by
the number of words in the found closest path.

uhyter(x,Y) def= min
x′∈Πx,
y∈Y

d(x, x′) + LS(x′, y) (1)

We treat this minimization problem as graph-
based search. The search space over which we mini-
mize is implicitly represented as the Recursive Tran-
sition Network H (see equation (2)), where Πx is
encoded as a weighted FSA that represents the set
of permutations of x with their associated distance
costs, and LS is the one-state Levenshtein trans-
ducer whose output weight for a string pair (x,y) is
the Levenshtein distance between x, and y, and the
symbol ◦ denotes composition. The model is de-
picted in Figure 2.

H(x,Y) def= Πx ◦ LS ◦ Y (2)

Permutations. We define an FSA Πx that allows
permutations according to certain constraints. Al-
lowing all permutations of the hypothesis x would
increase the search space to factorial size and make
inference NP-complete (Cormode and Muthukrish-
nan, 2007). We use local-window constraints (see,
e.g., Kanthak et al. (2005)), where words may move
within a fixed window of size k; these constraints
are of size O(n) with a constant factor k, where n is
the length of the translation hypothesis x.

Lazy Evaluation. For efficiency, we use lazy evalu-
ation when defining the search space H(x,Y). This
means we never explicitly compose Πx, LS, and Y .
Parts of the composition that our inference algorithm
does not explore are not constructed, saving compu-
tation time and memory. Permutation paths in Πx
are constructed on demand. Similarly, the reference
set Y is expanded on demand, and large parts may
remain unexpanded.1

Exact Inference. To compute uhyter(x,Y), we de-
fine the composition H(x,Y) and can apply any
shortest-path search algorithm (Mohri, 2002). We
found that using the A* algorithm (Hart et al., 1972)
was the most efficient; we devised an A* heuristic
similar to Karakos et al. (2008).

Runtime. Computing the HyTER score takes 30
ms per sentence on networks by single annotators
(combined all-annotator networks: 285 ms) if no

1These on-demand operations are supported by the OpenFst
library (Allauzen et al., 2007); specifically, to expand the RTNs
into FSAs we use the Replace operation.

166



Arabic-English Chinese-English
Metric Human mean Machine mean m/h Human mean Machine mean m/h
[100-0]-BLEU, 1 ref 59.90 69.14 1.15 71.86 84.34 1.17
[100-0]-BLEU, 3 refs 41.49 57.44 1.38 54.25 75.22 1.39
[100-0]-Meteor, 1 ref 60.13 65.70 1.09 66.81 73.66 1.10
[100-0]-Meteor, 3 refs 55.98 62.91 1.12 62.95 70.68 1.12
[100-0]-TERp, 1 ref 35.87 46.48 1.30 53.58 71.70 1.34
[100-0]-TERp, 3 refs 27.08 39.52 1.46 41.79 60.61 1.45
HyTER U 18.42 34.94 1.90 27.98 52.08 1.86
HyTER SPU 17.85 34.39 1.93 27.57 51.73 1.88
[100-0]-Likert 5.26 50.37 9.57 4.35 48.37 11.12

Table 2: Scores assigned to human versus machine translations, under various metrics. Each score is normalized to
range from 100 (worst) to 0 (perfect translation).

reordering is used. These numbers increase to 143
ms (1.5 secs) for local reordering with window size
3, and 533 ms (8 secs) for window size 5. Many
speedups for computing the score with reorderings
are possible, but we will see below that using re-
ordering does not give consistent improvements (Ta-
ble 3).

Output. As a by-product of computing the HyTER
score, one can obtain the closest path itself, for er-
ror analysis. It can be useful to separately count the
numbers of insertions, deletions, etc., and inspect
the types of error. For example, one may find that
a particular system output tends to be missing the fi-
nite verb of the sentence or that certain word choices
were incorrect.

4 Using meaning-equivalent networks for
machine translation evaluation

We now present experiments designed to measure
how well HyTER performs, compared to other eval-
uation metrics. For these experiments, we sample 82
of the 102 available sentences; 20 sentences are held
out for future use in optimizing our metric.

4.1 Differentiating human from machine
translation outputs

We score the set of human translations and machine
translations separately, using several popular met-
rics, with the goal of determining which metric per-
forms better at separating machine translations from
human translations. To ease comparisons across dif-
ferent metrics, we normalize all scores to a number
between 0 (best) and 100 (worst). Table 2 shows
the normalized mean scores for the machine trans-

lations and human translations under multiple au-
tomatic and one human evaluation metric (Likert).
The quotient of interest, m/h, is the mean score for
machine translations divided by the mean score for
the human translations: the higher this number, the
better a metric separates machine from human pro-
duced outputs.

Under HyTER, m/h is about 1.9, which shows
that the HyTER scores for machine translations are,
on average, almost twice as high as for human trans-
lations. Under Likert – a score assigned by hu-
man annotators who compare pairs of sentences at
a time–, the quotient is higher, suggesting that hu-
man raters make stronger distinctions between hu-
man and machine translations. The quotient is lower
under the automatic metrics Meteor (Version 1.3,
(Denkowski and Lavie, 2011)), BLEU and TERp
(Snover et al., 2009). These results show that
HyTER separates machine from human translations
better than alternative metrics.

4.2 Ranking MT systems by quality
We rank the five machine translation systems ac-
cording to several widely used metrics (see Fig-
ure 3). Our results show that BLEU, Meteor and
TERp do not rank the systems in the same way
as HTER and humans do, while the HyTER met-
ric yields the correct ranking. Also, separation be-
tween the quality of the five systems is higher un-
der HyTER, HTER, and Likert than under alterna-
tive metrics.

4.3 Correlations with HTER
We know that current metrics (e.g., BLEU, Meteor,
TER) correlate well with HTER and human judg-

167



Arabic-English
Size Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)
1 .653 .529 .541 .512 .675 .452 .547 .643 (.661) .647 (.655)
2 .645 .614 .636 .544 .706 .599 .649 .733 (.741) .735 (.732)
4 .739 .782 .804 .710 .803 .782 .803 .827 (.840) .831 (.838)
8 .741 .809 .822 .757 .818 .796 .833 .827 (.828) .830 (.825)
16 .868 .840 .885 .815 .887 .824 .862 .888 (.890) .893 (.894)
32 .938 .945 .957 .920 .948 .930 .947 .938 (.935) .940 (.936)
64 .970 .973 .979 .964 .973 .966 .968 .964 (.960) .966 (.961)

Chinese-English
Size Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)
1 .713 .495 .557 .464 .608 .569 .594 .708 (.721) .668 (.681)
2 .706 .623 .673 .569 .655 .639 .651 .713 (.716) .702 (.701)
4 .800 .628 .750 .593 .734 .651 .726 .822 (.825) .820 (.814)
8 .810 .745 .778 .783 .808 .754 .754 .852 (.856) .854 (.845)
16 .881 .821 .887 .811 .884 .826 .844 .912 (.914) .914 (.908)
32 .915 .873 .918 .911 .930 .851 .911 .943 (.942) .941 (.937)
64 .950 .971 .976 .979 .973 .952 .970 .962 (.958) .958 (.957)

Table 3: Document-level correlations of various scores to HTER. Meteor, BLEU and TERp are shown with 1 and 4
references each, HyTER is shown with the two combination methods (U and SPU), and with reordering (r5).

!"#$%

!!#!% !&#'%

&"#'%

()#*%

'&%

!"%

!&%

&"%

&&%

("%

(&%

)% *% '% !% &%

!"#$%&'

!"#$%

!&#'%

(!#)%((#*%

"'#&%

!+%

!'%

(+%

('%

"+%

"'%

'+%

,% !% (% "% '%

!"#$%

!"#$%
!&#'%

&"#$%&"#(%

$"#(%

!)%

!&%

&)%

&&%

$)%

$&%

')%

*% +% "% !% &%

!"#$%&'&

!"#$%
!!#&%

'$#(%'"#)%

)!#*%

!+%

!'%

'+%

''%

)+%

)'%

,+%

*% $% "% !% '%

!"#$%&'()&

!"#$%

"&#'%
"$#(%"$#)%

*&#(%

!'%

!+%

"'%

"+%

*'%

*+%

)''%

)% &% ,% $% +%

!"#$%&'&(#)&

!"#$%

!$#&%

'&#(% ')#)%

$&#)%

!(%

!*%

'(%

'*%

$(%

$*%

+(%

,% "% )% &% *%

!"#$%&'&(#)*&

!!"!#
!$"%#

%&"'#%&"'#

%("'#

!&#

!)#

%&#

%)#

$&#

$)#

*&#

+# '# ,# (# )#

!"#"$%&'('%")*'

!"#$%

!!#!%

!$#&% !$#'%

&(#!%

)"%

)!%

!"%

!!%

&"%

&!%

("%

*% +% '% )% !%

!"#$%&'&()*+&

Figure 3: Five MT systems (Chinese-English), scored by
8 different metrics. The x-axis shows the five systems, the
y-axis shows the [100-0] normalized scores, with 0 cor-
responding to a perfect translation. (Note that the scale
is similar in all eight graphs.) HTER and HyTER show a
similar pattern and similar ranking of the systems.

ments on large test corpora (Papineni et al., 2002;
Snover et al., 2006; Lavie and Denkowski, 2009).
We believe, however, that the field of MT will be
better served if researchers have access to metrics
that provide high correlation at the sentence level as
well. To this end, we estimate how well various met-
rics correlate with the Human TER (HTER) metric
for corpora of increasingly larger sizes.

Table 3 shows Pearson correlations between
HTER and various metrics for scoring documents

of size s =1, 2, 4, . . . , and 64 sentences. To get
more reliable results, we create 50 documents per
size s, where each document is created by select-
ing s sentences at random from the available 82 sen-
tences. For each document, there are 5 translations
from different systems, so we have 250 translated
documents per size. For each language and size, we
score the 250 documents under HTER and the other
metrics and report the Pearson correlation. Our re-
sults show that for large documents, all metrics cor-
relate well with HTER. However, as the sizes of the
documents decrease, and especially at the sentence
level, HyTER provides especially high correlation
with HTER as compared to the other metrics. As
a side note, we can see that using reordering when
computing the HyTER score does not give consis-
tently better results – see the (r5) numbers, which
searched over hypothesis permutations within a lo-
cal window of size 5; this shows that most reorder-
ings are already captured in the networks. In all ex-
periments, we use BLEU with plus-one smoothing
(Lin and Och, 2004).

5 Using meaning-equivalent networks for
human translation evaluation

In this section, we present a use case for the HyTER
metric outside of machine translation.

168



5.1 Setup and problem

Language Testing units assess the translation profi-
ciency of thousands of applicants interested in per-
forming language translation work for the US Gov-
ernment. Job candidates typically take a written test
in which they are asked to translate four passages
(i.e., paragraphs) of increasing difficulty into En-
glish. The passages are at difficulty levels 2, 2+, 3,
and 4 on the Interagency Language Roundable (ILR)
scale.2 The translations produced by each candidate
are manually reviewed to identify mistranslation,
word choice, omission, addition, spelling, grammar,
register/tone, and meaning distortion errors. Each
passage is then assigned one of five labels: Success-
fully Matches the definition of a successful transla-
tion (SM); Mostly Matches the definition (MM); In-
termittently Matches (IM); Hardly Matches (HM);
Not Translated (NT) for anything where less than
50% of a passage is translated.

We have access to a set of more than 100 rules that
agencies practically use to assign each candidate an
ILR translation proficiency level: 0, 0+, 1, 1+, 2, 2+,
3, and 3+. For example, a candidate who produces
passages labeled as SM, SM, MM, IM for difficulty
levels 2, 2+, 3, and 4, respectively, is assigned an
ILR level of 2+.

We investigate whether the assessment process
described above can be automated. To this end, we
obtained the exam results of 195 candidates, where
each exam result consists of three passages trans-
lated into English by a candidate, as well as the
manual rating for each passage translation (i.e., the
gold labels SM, MM, IM, HM, or NT). 49 exam re-
sults are from a Chinese exam, 71 from a Russian
exam and 75 from a Spanish exam. The three pas-
sages in each exam are of difficulty levels 2, 2+, and
3; level 4 is not available in our data set. In each
exam result, the translations produced by each can-
didate are sentence-aligned to their respective for-
eign sentences. We applied the passage-to-ILR map-
ping rules described above to automatically create a
gold overall ILR assessment for each exam submis-
sion. Since the languages used here have only 3 pas-
sages each, some rules map to several different ILR
ratings. Table 4 shows the label distribution at the

2See http://www.govtilr.org/skills/
AdoptedILRTranslationGuidelines.htm.

Lang. 0 0+ 1 1+ 2 2+ 3 3+
Chi. 0.0 8.2 40.8 65.3 59.2 10.2 4.1 0.0
Rus. 0.0 2.8 12.7 42.3 60.6 46.5 25.4 5.6
Spa. 0.0 1.3 33.3 66.7 88.0 24.0 4.0 0.0
All 0.0 3.6 27.7 57.4 70.8 28.7 11.8 2.1

Table 4: Percentage of exams with ILR levels 0, 0+, . . . ,
3+ as gold labels. Multiple levels per exam possible.

ILR assessment level across all languages.

5.2 Experiments

We automatically assess the proficiency of candi-
dates who take a translation exam. We treat this as a
classification task where, for each translation of the
three passages, we predict the three passage assess-
ment labels as well as one overall ILR rating.

In support of our goal, we asked annotators to cre-
ate an English HyTER network for each foreign sen-
tence in the exams. These HyTER networks then
serve as English references for the candidate transla-
tions. The median number of paths in these HyTER
networks is 1.6× 106 paths/network.

In training, we observe a set of submitted exam
translations, each of which is annotated with three
passage-level ratings and one overall ILR rating.
We develop features (Section 5.3) that describe each
passage translation in its relation to the HyTER net-
works for the passage. We then train a classifier to
predict passage-level ratings given the passage-level
features that describe the candidate translation. As
classifier, we use a multi-class support-vector ma-
chine (SVM, Krammer and Singer (2001)). In de-
coding, we observe a set of exams without their rat-
ings, derive the features and use the trained SVM to
predict ratings of the passage translations. We then
derive an overall ILR rating based on the predicted
passage-level ratings. Since our dataset is small we
run 10-fold cross-validation.

5.3 Features

We define features describing a candidate’s transla-
tion with respect to the corresponding HyTER refer-
ence networks. Each of the feature values is com-
puted based on a passage translation as a whole,
rather than sentence-by-sentence. As features, we
use the HyTER score, as well as the number of in-
sertions, deletions, substitutions, and insertions-or-
deletions. We use these numbers normalized by the

169



Level Measure Baseline HyTER-enabled
All Accuracy 72.31 90.77

2 or better
Precision 85.62 82.11
Recall 84.93 98.63
F1 85.27 89.62

Table 5: Predicting final ILR ratings for candidate exams.

length of the passage, as well as unnormalized. We
also use n-gram precisions (for n=1,. . . ,20) as fea-
tures.

5.4 Results

We report the accuracy on predicting the overall ILR
rating of the 195 exams (Table 5). The results in 2
or better show how well we predict a performance
level of 2, 2+, 3 or 3+. It is important to retrieve
such relatively good exams with high recall, so that
a manual review QA process can confirm the choices
while avoid discarding qualified candidates. The re-
sults show that high recall is reached while preserv-
ing good precision. Since we have several possible
gold labels per exam, precision and recall are com-
puted similar to precision and recall in the NLP task
of word alignment. F1(P,R) = 2PRP+R is the har-
monic mean of precision and recall. The row All
shows the accuracy in predicting ILR performance
labels overall. As a baseline method we assign the
most frequent label per language; these are 1+ for
Chinese, and 2 for Russian and Spanish. The results
in Table 5 strongly suggest that the process of as-
signing a proficiency level to human translators can
be automated.

6 Discussion

We have introduced an annotation tool and process
that can be used to create meaning-equivalent net-
works that encode an exponential number of trans-
lations for a given sentence. We have shown that
these networks can be used as foundation for devel-
oping improved machine translation evaluation met-
rics and automating the evaluation of human trans-
lation proficiency. We plan to release the OpenMT
HyTER networks to the community after the 2012
NIST Open MT evaluation. We believe that our
meaning-equivalent networks can be used to support
interesting research programs in semantics, para-
phrase generation, natural language understanding,

generation, and machine translation.

Acknowledgments

We thank our colleagues and the anonymous review-
ers for their valuable feedback. This work was par-
tially sponsored by DARPA contract HR0011-11-
C-0150 and TSWG contract N41756-08-C-3020 to
Language Weaver Inc.

References

C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: a general and efficient
weighted finite-state transducer library. In Proceed-
ings of the 12th international conference on Implemen-
tation and application of automata, page 1123.

O. Bojar, M. Ercegovčević, M. Popel, and O. Zaidan.
2011. A grain of salt for the wmt manual evaluation.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 1–11, Edinburgh, Scot-
land, July. Association for Computational Linguistics.

C. Callison-Burch, Ph. Koehn, Ch. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 22–64,
Edinburgh, Scotland, July. Association for Computa-
tional Linguistics.

G. Cormode and S. Muthukrishnan. 2007. The string
edit distance matching problem with moves. ACM
Transactions on Algorithms (TALG), 3(1):1–19.

M. Denkowski and A. Lavie. 2011. Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalua-
tion of Machine Translation Systems. In Proceedings
of the EMNLP 2011 Workshop on Statistical Machine
Translation.

Q. Gao, N. Bach, S. Vogel, B. Chen, G. Foster, R. Kuhn,
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
et al., 2010. Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics
(MATR), pages 121–126.

P.E. Hart, N.J. Nilsson, and B. Raphael. 1972. Cor-
rection to ”A formal basis for the heuristic determina-
tion of minimum cost paths”. ACM SIGART Bulletin,
pages 28–29, December. ACM ID: 1056779.

S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts,
pages 167–174.

D. Karakos, J. Eisner, S. Khudanpur, and M. Dreyer.
2008. Machine translation system combination using

170



ITG-based alignments. Proc. ACL-08: HLT, Short Pa-
pers (Companion Volume), page 8184.

K. Krammer and Y. Singer. 2001. On the algorithmic im-
plementation of multi-class SVMs. In Proc. of JMLR.

A. Lavie and M. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3):105–115.

L. I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions and reversals. Soviet Physics
Doklady, 10:707–710.

C.Y. Lin and F.J. Och. 2004. Orange: a method for eval-
uating automatic evaluation metrics for machine trans-
lation. In Proceedings of the 20th international con-
ference on Computational Linguistics, page 501. As-
sociation for Computational Linguistics.

M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321–350.

J. Olive, C. Christianson, and J. McCary, editors. 2011.
Handbook of Natural Language Processing and Ma-
chine Translation. DARPA Global Autonomous Lan-
guage Exploitation. Springer.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
311–318, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.

M. Przybocki, A. Le, G. Sanders, S. Bronsart, S. Strassel,
and M. Glenn, 2011. GALE Machine Translation
Metrology: Definition, Implementation, and Calcula-
tion, chapter 5.4, pages 583–811. Springer.

M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of the
Association for Machine Translation in the Americas,
pages 223–231.

M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the EACL.

171


