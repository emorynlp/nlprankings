623

Coling 2010: Poster Volume, pages 623–629,

Beijing, August 2010

A Post-processing Approach to Statistical Word Alignment  
Reflecting Alignment Tendency between Part-of-speeches 

Jae-Hee Lee1, Seung-Wook Lee1, Gumwon Hong1, 

Young-Sook Hwang2, Sang-Bum Kim2,  Hae-Chang Rim1 

1Dept. of Computer and Radio Communications Engineering, Korea University 

2Institute of Future Technology, SK Telecom 

1{jlee,swlee,gwhong,rim}@nlp.korea.ac.kr, 

2{yshwang,sangbum.kim}@sktelecom.com 

 

Abstract 

Statistical word alignment often suffers 
from  data  sparseness.  Part-of-speeches 
are  often incorporated  in NLP  tasks  to 
reduce  data  sparseness.  In  this  paper, 
we attempt to mitigate such problem by 
reflecting  alignment  tendency  between 
part-of-speeches 
to  statistical  word 
alignment.  Because  our  approach  does 
not  rely  on  any  language-dependent 
knowledge, it is very simple and purely 
statistic  to  be  applied  to  any  language 
pairs. End-to-end evaluation shows that 
the  proposed  method  can  improve  not 
only  the  quality  of  statistical  word 
alignment  but  the  performance  of  sta-
tistical machine translation. 

1 

Introduction 

Word  alignment  is  defined  as  mapping  corre-
sponding  words  in  parallel  text.  A  word 
aligned  parallel  corpora  are  very  valuable  re-
sources  in  NLP.  They  can  be  used  in  various 
applications  such  as  word  sense  disambigua-
tion,  automatic  construction  of  bilingual  lexi-
con, and statistical machine translation (SMT). 
In  particular,  the  initial  quality  of  statistical 
word alignment dominates the quality of SMT 
(Och  and  Ney  2000;  Ganchev  et  al.,  2008); 
almost all current SMT systems basically refer 
to  the  information  inferred  from  word  align-
ment result. 

One of the widely used approaches to statis-
tical  word  alignment  is  based  on  the  IBM 
models  (Brown  et  al.,  1993).  IBM  models  are 
constructed  based  on  words’  co-occurrence 
and  positional  information.  If  sufficient  train-

ing  data  are  given,  IBM  models  can  be  suc-
cessfully  applied  to  any  language  pairs.  How-
ever, for minority language pairs such as Eng-
lish-Korean  and  Swedish-Japanese,  it  is  very 
difficult  to  obtain  large  amounts  of  parallel 
corpora.  Without  sufficient  amount  of  parallel 
corpus,  it  is  very  difficult  to  learn  the  correct 
correspondences  between  words  that  infre-
quently occur in the training data. 

Part-of-speeches  (POS),  which  represent 
morphological classes of words, can give valu-
able  information  about  individual  words  and 
their  neighbors.  Identifying  whether  a  word  is 
a noun or a verb can let us predict which words 
are likely to be mapped in word alignment and 
which  words  are likely  to  occur in  its  vicinity 
in target sentence generation. 

Many  studies  incorporate  POS  information 
in  SMT.  Some  researchers  perform  POS  tag-
ging on their bilingual training data (Lee et al., 
2006;  Sanchis  and  Sánchez,  2008).  Some  of 
them  replace  individual  words  as  new  words, 
such  as  in  “word/POS”  form,  producing  new, 
extended  vocabulary.  The  advantage  of  this 
approach  is  that  POS  information  can  help  to 
resolve  lexical  ambiguity  and  thus  improve 
translation quality. 

On the other hand, Koehn et al. (2007) pro-
pose  a  factored  translation  model  that  can  in-
corporate any linguistic factors  including POS 
information  in  phrase-based  SMT.  The  model 
provides  a  generalized  representation  of  a 
translation model, because it can map multiple 
source and target factors. 

Although all of these approaches are shown 
to improve SMT performance by utilizing POS 
information,  we  observe  that  the  influence  is 
virtually marginal in two ways: 

624

 
 

Figure 1. An example of inaccurate word alignment 

 

1)  The POS information tagged to each word 
may  help  to  disambiguate  in  selecting 
word  correspondences,  but  the  increased 
vocabulary can also make the training data 
more sparse. 

handle 

2)  The factored translation model may help to 
out-of-vocabulary 
effectively 
(OOV)  by  incorporating  many  linguistic 
factors, but it still crucially relies on the in-
itial  quality  of  word  alignment  that  will 
dominate the translation probabilities. 

This  paper  focuses  on  devising  a  better 
method  for  incorporating  POS  information  in 
word  alignment.  It  attempts  to  answer  the  fol-
lowing questions: 
1)  Can  the  information  regarding  POS  align-
ment  tendency  affect  the  post-processing 
of word alignment? 

2)  Can  the  result  of  word  alignment  affected 
by  such  information  help  improving  the 
quality of SMT? 

2  POS Alignment Tendency 

Despite the language pairs, words with similar 
POSs often correspond to each other in statisti-
cal word alignment. Similarly, words with dif-
ferent  POSs are seldom  aligned.  For  example, 
Korean  proper  nouns  very  often  align  with 
English  proper  nouns  very  often  but  seldom 
align  with  English  adverbs.  We  believe  that 
this  phenomenon  occurs  not  only  on  English-
Korean  pairs  but  also  on  most  of  other  lan-
guage pairs.  

Thus,  in  this  study  we  hypothesize  that  all 
source  language  (SL)  POSs  have  some  rela-
tionship  with target language  (TL)  POSs.  Fig-
ure  1  exemplifies  some  results  of  using  the 
IBM  Models  in  English-Korean  word  align-
ment. As can be seen in the figure, the English 
word  “visiting”  is  incorrectly  and  excessively 
aligned to four Korean morphemes “maejang”, 

“chat”, “yeoseong”, and “gogaek”. One reason 
for  this  is  the  sparseness  of  the  training  data; 
the only correct Korean morpheme “chat” does 
not sufficiently co-occur with “visiting” in the 
training data. However, it is generally believed 
that an English verb is more likely aligned to a 
Korean  verb rather than  a Korean  noun.  Like-
wise,  we  suppose  that  among  many  POSs, 
there  are  strong  relationships  between  similar 
POSs  and  relatively  weak  relationships  be-
tween different POSs. We hypothesize that the 
discovery of such relationships in advance can 
lead to better word alignment results. 

 In this paper, we propose a  new method to 
obtain  the  relationship  from  word  alignment 
results. The relationships among POSs, hence-
forth  the  POS  alignment  tendency,  can  be 
identified  by  the  probability  of  the  given  POS 
pairs’  alignment  result  where  the  source  lan-
guage  POS  and  the  target  language  POS  co-
occur in bilingual sentences. We formulate this 
idea using the maximum likelihood estimation 
as follows: 

     (            |   ( )     ( ))   

 

    

∑

     (                ( )     ( ))

  *          +

     (              ( )     ( ))

 

 

where  f  and  e  denote  source  word  and  target 
word  respectively.  count()  is  a  function  that 
returns the number of co-occurrence of f and e 
when  they  are  aligned  (or  not  aligned).  Then, 
we  adjust  the  formula  with  the  existing  align-
ment score between f and e. 

     (    )         (   )   
             (     ) (            |   ( )     ( )) 

 

 indicates  the  alignment  prob-
where 
ability  estimated  by  the  IBM  models.     is  a 
weighting parameter to interpolate the reliabili-
ties  of  both  alignment  factors.  In  the  expe-

 
 

PIBM

(

ef
)|

625

 
 

Figure 2. An example of word alignment modification 

 

riment,     is  empirically  set  to  improve  the 
word alignment performance ( =0.5). 
 

3  Modifying Alignment 

Based  on  the  new  scoring  scheme  as  intro-
duced  in  the  previous  section,  we  modify  the 
result of the initial word alignment. The modi-
fication  is  performed  in  the  following  proce-
dure: 

1.  For each source word f that has out-bound 

alignment link other than null, 

2.  Find the target word e that has the maxi-
mum  alignment  score  according  to  the 
proposed  alignment  adjustment  measure, 
and  change  the  alignment  result  by  map-
ping f to e. 

This  modification  guarantees  that  the  number 
of alignment does not change; the algorithm is 
designed  to  minimize  the  risk  by  maintaining 
the  fertility  of  a  word  estimated  by  the  IBM 
Model.  Figure  2  illustrates  the  result  before 
and  after  the  alignment  modification.  Incor-
rectly  links  from  e1  and  e3  are  deleted  and 
missing links from e2 and e4 are generated dur-
ing this alignment modification. 

The  alignment  modification  through  the  re-
flection  of  POS  alignment  tendency  is  per-
formed  on  both  e-to-f  and  f-to-e  bidirectional 
word  alignments.  The  bidirectional  word 
alignment results are then symmetrized. 

4  Experiments 

In  this  paper,  we  attempt  to  reflect  the  POS 
alignment  tendency  in  improving  the  word 
alignment  performance.  This  section  provides 
the  experimental  setup  and  the  results  that 
demonstrate  whether  the  proposed  approach 
can improve the statistical word alignment per-

formance. 

We  collected  bilingual  texts  from  major  bi-
lingual news broadcasting sites. 500K sentence 
pairs  are  collected  and  refined  manually  to 
construct  correct  parallel  sentences  pairs.  The 
same number of monolingual sentences is also 
used  from  the  same  sites  to  train  Korean  lan-
guage. We also prepared a subset of the bilin-
gual text with the size of 50K to show that the 
proposed  model  is  very  effective  when  the 
training set is small. 

In  order  to  evaluate  the  performance  of 
word alignment, we  additionally constructed a 
reference  set  with  400  sentence  pairs.  The 
evaluation is performed using precision, recall, 
and  F-score.  We  use  the  GIZA++  toolkit  for 
word  alignment  as  well as  four  heuristic  sym-
metrizations:  intersection,  union,  grow-diag-
final, and grow-diag (Och, 2000).  

4.1  Word Alignment 

We now evaluate the effectiveness of the pro-
posed  word  alignment  method.  Table  1  and  2 
report the experimental results by adding POS 
information  to  the  parallel  corpus.  “Lexical” 
denotes  the  result  of  conventional  word  align-
ment produced by GIZA++. No pre-processing 
or  post-processing  is  applied  in  this  result. 
“Lemma/POS” is the result of word alignment 
with  the  pre-processing  introduced  Lee  et  al. 
(2006).  Compared  to  the  result,  lemmatized 
lexical  and  POS  tags  are  proven  to  be  useful 
information for word alignment. “Lemma/POS” 
consistently  outperforms  “Lexical”  despite  the 
symmetrization heuristics in terms of precision, 
recall  and  F-score.  We  expect  this  improve-
ment  is  benefited  from  the  alleviated  data 
sparseness  by  using  lemmatized  lexical  and 
POS tags rather than using the lexical itself. 

 

 
 

626

 

Alignment heuristic 

Lexical 

Lemma/POS 

Lemma/POS 

+ POS alignment 

tendency 

Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 

Precision  Recall 
50.8% 
81.2% 
80.9% 
67.2% 
55.3% 
83.3% 
83.0% 
71.6% 
63.5% 
85.1% 
84.9% 
77.0% 

94.0% 
53.2% 
54.6% 
60.9% 
95.8% 
58.1% 
59.7% 
67.0% 
96.1% 
67.4% 
69.8% 
80.0% 

Table 1. The performance of word alignment using small training set (50k pairs) 

 

Experimental Setup 

Alignment heuristic 

Lexical 

Lemma/POS 

Lemma/POS 

+ POS alignment 

tendency 

Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 

Precision  Recall 
64.9% 
87.4% 
87.1% 
79.2% 
66.2% 
89.0% 
88.8% 
80.5% 
69.3% 
86.7% 
86.4% 
81.5% 

96.8% 
66.6% 
67.8% 
74.4% 
97.3% 
70.7% 
72.1% 
78.8% 
97.2% 
73.9% 
75.6% 
85.2% 

F-score 
66.0% 
64.3% 
65.2% 
63.9% 
70.1% 
68.4% 
69.5% 
69.2% 
76.5% 
75.2% 
76.6% 
78.5% 

F-score 
77.7% 
75.6% 
76.2% 
76.7% 
78.8% 
78.8% 
79.6% 
79.7% 
80.9% 
79.8% 
80.7% 
83.4% 

Table 2. The performance of word alignment using a large training set (500k pairs) 

 

Experimental Setup 

Symmetrization Heuristic 

BLEU(50k) 

BLEU (500k) 

Lexical 

Lemma/POS 

Factored Model 
(Lemma, POS) 

Lemma/POS 

+ POS alignment 

tendency 

Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 
Intersection 
Union 
Grow-diag-final 
Grow-diag 
Table  3. The performance of translation 

20.1% 
18.6% 
19.9% 
20.2% 
20.3% 
18.5% 
20.1% 
20.4% 
20.5% 
18.1% 
20.3% 
20.9% 
21.8% 
19.5% 
21.3% 
20.8% 

 

 

29.2% 
27.2% 
27.7% 
29.4% 
26.4% 
27.8% 
29.2% 
30.8% 
30.0% 
27.5% 
28.2% 
31.1% 
29.3% 
27.2% 
28.4% 
29.1% 

 
 

 
 

627

 
 

Figure 3. Average recall of word alignment pairs 
according to the number of their co-occurrence 

Since  lemmatized  lexical  and  POS  tags  are 
shown to be useful, our post-processing meth-
od is applied to “Lemma/POS”. 

The  experimental  results  show  that  the  pro-
posed  method  consistently  improves  word 
alignment  in terms  of  F-score.  It  is  interesting 
that  the  proposed  method  improves  the  recall 
of  the  intersection  result  and  the  precision  of 
the  union  result.  Thus,  the  proposed  method 
achieves the best alignment performance. 

As can be seen in Table 1 and 2, our method 
consistently improves the performance of word 
alignment despite the size of training data. In a 
small data set, the improvement of our method 
is  much  higher  than  that  in  a  large  set.  This 
implies  that  our  method  is  more  helpful  when 
the training data set is insufficient.  

We  investigate  whether  the  proposed  meth-
od actually alleviates the data sparseness prob-
lem by analyzing the aligned word pairs of low 
co-occurrence  frequency.  There  are  multiple 
word  pairs  that  share  the  same  number  of  co-
occurrence  in  the  corpus.  For  example,  let  us 
that  “report-bogoha”,  “newspaper-
assume 
sinmun”  and  “China-jungguk”  pairs  are  co-
occurred  1,000  times.  We  can  calculate  the 
mean  of  their  individual  recalls.  We  refer  to 
this  new  measurement  as  average  recall.  The 
average  recalls  of  these  pairs  are  relatively 
higher  than  those  of  pairs  with  low  co-
occurrence  frequency  such  as  “food-jinji”  and 
“center-chojeom”  pairs.  These  pairs  are  diffi-
cult  to  be  linked,  because  the  word  alignment 
model suffers from data sparseness  when esti-
mating their translation probability.  

Figure 3 shows the average recall according 
to  the  number  of  co-occurrence.  We  can  ob-

 
 

serve  that  the  word  alignment  model  tends  to 
link word pairs more correctly if they are more 
frequently  co-occurred.  Both  “Lemma/POS” 
and our method consistently show higher aver-
age  recall  throughout  all  frequencies,  and  the 
proposed method shows the best performance. 
It  is  also  notable  that  the  both  “Lemma/POS” 
and  our  method  achieve  much  more  improve-
ment  for  low  co-occurrence  frequencies  (e.g., 
11~40). This implies that the proposed method 
incorporates POS information more effectively 
than  the  previous  method,  since  the  proposed 
 
method achieves much higher average recall. 

 

4.2  Statistical Machine Translation 

Next,  we  examine  the  effect  of  the  improve-
ment  of  the  word  alignment  on the  translation 
quality.  For  this,  we  built  some  SMT  systems 
with  the  word  alignment  results.  We  use  the 
Moses  toolkit  for  translation  (Koehn  et  al., 
2007).  Moses  is  an  implementation  of  phrase-
based statistical machine translation model that 
has  shown  a  state-of-the-art  performance  in 
various  evaluation  sets.  We  also  perform  the 
evaluation of the Factored model (Koehn et al., 
2007) using Moses.  

To  investigate  how  the  improved  word 
alignment  affect  the  quality  of  machine  trans-
lation, we calculate the BLEU score for trans-
lation  results  with  different  word  alignment 
settings  as  shown  in  Table  3.  First  of  all,  we 
can  easily  conclude  that  the  quality  of  the 
translation is strongly dominated by the size of 
the  training  data.  We  can  also  find  that  the 
quality  of  the  translation  is  correlated  to  the 
performance of the word alignment. 

For  a  small  test  set,  the  proposed  method 
achieved  the  best  performance  in  terms  of 
BLEU (21.8%). For a larger test set, however, 
the  proposed  method  could  not  improve  the 
performance of the translation with better word 
alignment.  It  is  not  feasible  to  investigate  the 
factors that affect this deterioration, since Mo-
ses  is  a  black  box  module  to  our  system.  The 
training  of  the  phrase-based  SMT  model  in-
volves the extraction of phrases, and the result 
of word alignment is reflected within this pro-
cess. When the training data is small, the num-
ber  of  extracted  phrases  is  also  apparently 
small. However, abundant phrases are extract-
ed from a large amount of training data. In this 
case,  we  hypothesize  that  the  most  plausible 

628

 
 

Rank 

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 

translation 
bob/NNP 
rice/NN 
eat/VB 
meal/NN 
food/NN 
bob/NN 
feed/VB 
cook/VB 
living/NN 
dinner/NN 

IBM Model 

    (   )  #co-occur 

0.348 
0.192 
0.107 
0.075 
0.043 
0.038 
0.010 
0.010 
0.008 
0.008 

83 
73 
57 
43 
29 
10 
7 
9 
4 
10 

POS Alignment Tendency 

translation 
bob/NNP 
rice/NN 
meal/NN 
food/NN 
eat/VB 
bob/NN 
living/NN 
dinner/NN 
bread/NN 
breakfast/NN 

     (    ) 

#co-occur 

0.214 
0.136 
0.078 
0.062 
0.061 
0.059 
0.045 
0.044 
0.044 
0.043 

83 
73 
43 
29 
57 
10 
4 
10 
9 
6 

Table 4. Top 10 translations for Korean word “bap” (food). 

phrases are already obtained, and the effect of 
more accurate word alignment seems insignifi-
cant.  More  thorough  analysis  of  this  is  re-
mained as future work. 

4.3  Acquisition of Bilingual Dictionary 

One  of  the  most  applications  of  word  align-
ment is the construction of bilingual dictionar-
ies. By using word alignment, we can collect a 
(ranked)  list  of  bilingual  word  pairs.  Table  4 
reports  the  top  10  translations  (the  most  ac-
ceptable  target  words  to  align)  for  Korean 
word  “bap”  (food).  The  table  contains  the 
probabilities estimated by the IBM Models, the 
adjusted  scores,  and 
the  number  of  co-
occurrence, respectively. Italicized translations 
are  in  fact  incorrect  translations.  Highlighted 
ones  are  new  translation  candidates  that  are 
correct.  As  can  be  seen  in  the  table,  the  pro-
posed approach shows a positive effect of rais-
ing  new  and  better  candidates  for  translation. 
For  example,  “bread”  and  “breakfast”  have 
come  up  to  the  top  10  translations.  This 
demonstrates  that  the  low  co-occurrences  of 
“bap”  with  “bread”  and  “breakfast”  are  not 
suitably handled by alignments solely based on 
lexicals.  However,  the  proposed  approach 
ranks them at higher positions by reflecting the 
alignment tendency of POSs. 

5  Conclusion 

In  this  paper,  we  propose  a  new  method  for 
incorporating  the  POS  alignment  tendency  to 
improve  traditional  word  alignment  model  in 
post  processing  step.  Experimental  results 
show that the proposed method helps to allevi-
ate  the  data  sparseness  problem  especially 

when the training data is insufficient. 

It  is  still  difficult  to  conclude  that  better 
word alignment always leads  to better transla-
tion.  We  plan  on  investigating  the  effective-
ness of the proposed method using other trans-
lation  system,  such  as  Hiero  (Chiang  et  al., 
2005). We also plan to incorporate our method 
into  other  effective  models,  such  as  Factored 
translation model. 

References 

David Chiang et al., 2005. The Hiero machine 
translation system: Extensions, evaluation, and 
analysis. In Proc. of HLT-EMLP:779–786, Oct. 
 
Franz Josef Och. 2000. Giza++: Training of statis-
tical translation models. Available at http://www-

i6.informatik.rwthaachen.de/∼och/software/GIZA+
+.html. 
 
Franz Josef Och & Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29 (1):19-51. 
 
G. Sanchis and J.A. Sánchez. Vocabulary exten- 
sion via POS information for SMT. In Mixing 
Approaches to Machine Translation, 2008. 
 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae 
Lee. Improving Phrase-based Korean-English Sta-
tistical Machine Translation. INTERSPEECH 2006. 
 
Kuzman Ganchev, Joao V. Graca and Ben Taskar. 
2008. Better Alignments = Better Translations? 
Proceedings of ACL-08: HLT: 986–993. 
 
Peter F. Brown et al.,1993. The Mathematics of 
Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics 9(2): 263-311 
 

 
 

629

 
 

Philipp Koehn and Hieu Hoang. Factored Transla-
tion Models. EMNLP 2007. 
 
Phillipp Koehn et al., 2007. Moses: Open source 
toolkit for statistical machine translation.In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, demonstation ses-
sion. 

 
 

