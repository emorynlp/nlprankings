



















































Source-side Syntactic Reordering Patterns with Functional Words for Improved Phrase-based SMT


Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 19–27,
COLING 2010, Beijing, August 2010.

Source-side Syntactic Reordering Patterns with Functional Words for
Improved Phrase-based SMT

Jie Jiang, Jinhua Du, Andy Way
CNGL, School of Computing, Dublin City University 
{jjiang,jdu,away}@computing.dcu.ie

Abstract

Inspired by previous source-side syntactic
reordering methods for SMT, this paper
focuses on using automatically learned
syntactic reordering patterns with func-
tional words which indicate structural re-
orderings between the source and target
language. This approach takes advan-
tage of phrase alignments and source-side
parse trees for pattern extraction, and then
filters out those patterns without func-
tional words. Word lattices transformed
by the generated patterns are fed into PB-
SMT systems to incorporate potential re-
orderings from the inputs. Experiments
are carried out on a medium-sized cor-
pus for a Chinese–English SMT task. The
proposed method outperforms the base-
line system by 1.38% relative on a ran-
domly selected testset and 10.45% rela-
tive on the NIST 2008 testset in terms
of BLEU score. Furthermore, a system
with just 61.88% of the patterns filtered
by functional words obtains a comparable
performance with the unfiltered one on the
randomly selected testset, and achieves
1.74% relative improvements on the NIST
2008 testset.

1 Introduction

Previous work has shown that the problem of
structural differences between language pairs in
SMT can be alleviated by source-side syntactic
reordering. Taking account for the integration
with SMT systems, these methods can be divided
into two different kinds of approaches (Elming,

2008): the deterministic reordering and the non-
deterministic reordering approach.

To carry out the deterministic approach, syntac-
tic reordering is performed uniformly on the train-
ing, devset and testset before being fed into the
SMT systems, so that only the reordered source
sentences are dealt with while building during
the SMT system. In this case, most work is fo-
cused on methods to extract and to apply syntac-
tic reordering patterns which come from manually
created rules (Collins et al., 2005; Wang et al.,
2007a), or via an automatic extraction process tak-
ing advantage of parse trees (Collins et al., 2005;
Habash, 2007). Because reordered source sen-
tence cannot be undone by the SMT decoders (Al-
Onaizan et al., 2006), which implies a systematic
error for this approach, classifiers (Chang et al.,
2009b; Du & Way, 2010) are utilized to obtain
high-performance reordering for some specialized
syntactic structures (e.g. DE construction in Chi-
nese).

On the other hand, the non-deterministic ap-
proach leaves the decisions to the decoders to
choose appropriate source-side reorderings. This
is more flexible because both the original and
reordered source sentences are presented in the
inputs. Word lattices generated from syntactic
structures for N-gram-based SMT is presented
in (Crego et al., 2007). In (Zhang et al., 2007a;
Zhang et al., 2007b), chunks and POS tags are
used to extract reordering rules, while the gener-
ated word lattices are weighted by language mod-
els and reordering models. Rules created from a
syntactic parser are also utilized to form weighted
n-best lists which are fed into the decoder (Li et
al., 2007). Furthermore, (Elming, 2008; Elm-

19



ing, 2009) uses syntactic rules to score the output
word order, both on English–Danish and English–
Arabic tasks. Syntactic reordering information is
also considered as an extra feature to improve PB-
SMT in (Chang et al., 2009b) for the Chinese–
English task. These results confirmed the effec-
tiveness of syntactic reorderings.

However, for the particular case of Chinese
source inputs, although the DE construction has
been addressed for both PBSMT and HPBSMT
systems in (Chang et al., 2009b; Du & Way,
2010), as indicated by (Wang et al., 2007a), there
are still lots of unexamined structures that im-
ply source-side reordering, especially in the non-
deterministic approach. As specified in (Xue,
2005), these include the bei-construction, ba-
construction, three kinds of de-construction (in-
cluding DE construction) and general preposition
constructions. Such structures are referred with
functional words in this paper, and all the con-
structions can be identified by their correspond-
ing tags in the Penn Chinese TreeBank. It is in-
teresting to investigate these functional words for
the syntactic reordering task since most of them
tend to produce structural reordering between the
source and target sentences.

Another related work is to filter the bilingual
phrase pairs with closed-class words (Sánchez-
Martı́nez, 2009). By taking account of the word
alignments and word types, the filtering process
reduces the phrase tables by up to a third, but still
provide a system with competitive performance
compared to the baseline. Similarly, our idea is to
use special type of words for the filtering purpose
on the syntactic reordering patterns.

In this paper, our objective is to exploit
these functional words for source-side syntac-
tic reordering of Chinese–English SMT in the
non-deterministic approach. Our assumption is
that syntactic reordering patterns with functional
words are the most effective ones, and others can
be pruned for both speed and performance.

To validate this assumption, three systems are
compared in this paper: a baseline PBSMT sys-
tem, a syntactic reordering system with all pat-
terns extracted from a corpus, and a syntactic re-
ordering system with patterns filtered with func-
tional words. To accomplish this, firstly the lat-

tice scoring approach (Jiang et al., 2010) is uti-
lized to discover non-monotonic phrase align-
ments, and then syntactic reordering patterns are
extracted from source-side parse trees. After that,
functional word tags specified in (Xue, 2005) are
adopted to perform pattern filtering. Finally, both
the unfiltered pattern set and the filtered one are
used to transform inputs into word lattices to
present potential reorderings for improving PB-
SMT system. A comparison between the three
systems is carried out to examine the performance
of syntactic reordering as well as the usefulness of
functional words for pattern filtering.

The rest of this paper is organized as follows:
in section 2 we describe the extraction process of
syntactic reordering patterns, including the lattice
scoring approach and the extraction procedures.
Then section 3 presents the filtering process used
to obtain patterns with functional words. After
that, section 4 shows the generation of word lat-
tices with patterns, and experimental setup and re-
sults included related discussion are presented in
section 5. Finally, we give our conclusion and av-
enues for future work in section 6.

2 Syntactic reordering patterns
extraction

Instead of top-down approaches such as (Wang
et al., 2007a; Chang et al., 2009a), we use a
bottom-up approach similar to (Xia et al., 2004;
Crego et al., 2007) to extract syntactic reordering
patterns from non-monotonic phrase alignments
and source-side parse trees. The following steps
are carried out to extract syntactic reordering pat-
terns: 1) the lattice scoring approach proposed
in (Jiang et al., 2010) is used to obtain phrase
alignments from the training corpus; 2) reorder-
ing regions from the non-monotonic phrase align-
ments are used to identify minimum treelets for
pattern extraction; and 3) the treelets are trans-
formed into syntactic reordering patterns which
are then weighted by their occurrences in the
training corpus. Details of each of these steps are
presented in the rest of this section.

2.1 Lattice scoring for phrase alignments

The lattice scoring approach is proposed in (Jiang
et al., 2010) for the SMT data cleaning task.

20



To clean the training corpus, word alignments
are used to obtain approximate decoding results,
which are then used to calculate BLEU (Papineni
et al., 2002) scores to filter out low-scoring sen-
tences pairs. The following steps are taken in
the lattice scoring approach: 1) train an initial
PBSMT model; 2) collect anchor pairs contain-
ing source and target phrase positions from word
alignments generated in the training phase; 3)
build source-side lattices from the anchor pairs
and the translation model; 4) search on the source-
side lattices to obtain approximate decoding re-
sults; 5) calculate BLEU scores for the purpose of
data cleaning.

Note that the source-side lattices in step 3 come
from anchor pairs, so each edge in the lattices con-
tain both the source and target phrase positions.
Thus the outputs of step 4 contain phrase align-
ments on the training corpus. These phrase align-
ments are used to identify non-monotonic areas
for the extraction of reordering patterns.

2.2 Reordering patterns

Non-monotonic regions of the phrase alignments
are examined as potential source-side reorderings.
By taking a bottom-up approach, the reordering
regions are identified and mapped to minimum
treelets on the source parse trees. After that, syn-
tactic reordering patterns are transformed from
these minimum treelets.

In this paper, reordering regions A and B indi-
cating swapping operations on the source side are
only considered as potential source-side reorder-
ings. Thus, given reordering regions AB, this im-
plies (1):

AB ⇒ BA (1)
on the source-side word sequences. Referring to
the phrase alignment extraction in the last section,
each non-monotonic phrase alignment produces
one reordering region. Furthermore, for each re-
ordering region identified, all of its sub-areas in-
dicating non-monotonic alignments are also at-
tempted to produce more reordering regions.

To represent the reordering region using syn-
tactic structure, given the extracted reordering re-
gions AB, the following steps are taken to map
them onto the source-side parse trees, and to gen-
erate corresponding patterns:

1. Generate a parse tree for each of the source
sentences. The Berkeley parser (Petrov,
2006) is used in this paper. To obtain sim-
pler tree structures, right-binarization is per-
formed on the parse trees, while tags gener-
ated from binarization are not distinguished
from the original ones (e.g. @V P and V P
are the same).

2. Map reordering regions AB onto the parse
trees. Denote NA as the set of leaf nodes in
region A and NB for region B. The mapping
is carried out on the parse tree to find a mini-
mum treelet T , which satisfies the following
two criteria: 1) there must exist a path from
each node in NA ∪ NB to the root node of
T ; 2) each leaf node of T can only be the
ancestor of nodes in NA or NB (or none of
them).

3. Traverse T in pre-order to obtain syntactic
reordering pattern P . Label all the leaf nodes
of T with A or B as reorder options, which
indicate that the descendants of nodes with
label A are supposed to be swapped with
those with label B.

Instead of using subtrees, we use treelets to
refer the located parse tree substructures, since
treelets do not necessarily go down to leaf nodes.

Since phrase alignments cannot always be per-
fectly matched with parse trees, we also expand
AB to the right and/or the left side with a limited
number of words to find a minimum treelet. In
this situation, a minimum number of ancestors of
expanded tree nodes are kept in T but they are as-
signed the same labels as those from which they
have been expanded. In this case, the expanded
tree nodes are considered as the context nodes of
syntactic reordering patterns.

Figure 1 illustrates the extraction process. Note
the symbol @ indicates the right-binarization sym-
bols (e.g. @V P in the figure). In the figure, tree
T (surrounded by dashed lines) is the minimum
treelet mapped from the reordering region AB.
Leaf node NP is labeled by A, V P is labeled by
B, and the context node P is also labeled by A.
Leaf nodes labeled A or B are collected into node
sequences LA or LB to indicate the reordering op-

21



A B

T

Figure 1: Reordering pattern extraction

erations. Thus the syntactic reordering pattern P
is obtained from T as in (2):

P = {V P (PP (P NP ) V P )|O = {LA, LB}}
(2)

where the first part of P is the V P with its tree
structure, and the second part O indicates the re-
ordering scheme, which implies that source words
corresponding with descendants of LA are sup-
posed to be swapped with those of LB .

2.3 Pattern weights estimation

We use preo to represent the chance of reordering
when a treelet is located by a pattern on the parse
tree. It is estimated by the number of reorderings
for each of the occurrences of the pattern as in (3):

preo(P ) =
count{reorderings of P}
count{observation of P} (3)

By contrast, one syntactic pattern P usually con-
tains several reordering schemes (specified in for-
mula (2)), each of them weighted as in (4):

w(O,P ) =
count{reorderings of O in P}

count{reorderings of P}
(4)

Generally, a syntactic reordering pattern is ex-
pressed as in (5):

P = {tree | preo | O1, w1, · · · , On, wn} (5)
where tree is the tree structures of the pattern,
preo is the reordering probability, Oi and wi are
the reordering schemes and weights (1 ≤ i ≤ n).

3 Patterns with functional words

Some of the patterns extracted may not benefit
the final system since the extraction process is
controlled by phrase alignments rather than syn-
tactic knowledge. Inspired by the study of DE
constructions (Chang et al., 2009a; Du & Way,
2010), we assume that syntactic reorderings are
indicated by functional words for the Chinese–
English task. To incorporate the knowledge of
functional words into the extracted patterns, in-
stead of directly specifying the syntactic struc-
ture from the linguistic aspects, we use functional
word tags to filter the extracted patterns. In this
case, we assume that all patterns containing func-
tional words tend to produce meaningful syntactic
reorderings. Thus the filtered patterns carry the re-
ordering information from the phrase alignments
as well as the linguistic knowledge. Thus the
noise produced in phrase alignments and the size
of pattern set can be reduced, so that the speed and
the performance of the system can be improved.

The functional word tags used in this paper are
shown in Table 1, which come from (Xue, 2005).
We choose them as functional words because nor-
mally they imply word reorders between Chinese
and English sentence pairs.

Tag Description
BA ba-construction

DEC de (1st kind) in a relative-clause
DEG associative de (1st kind)
DER de (2nd kind) in V-de const. & V-de-R
DEV de (3rd kind) before VP
LB bei in long bei-construction
P preposition excluding bei and ba

SB bei in short bei-construction

Table 1: Syntactic reordering tags for functional
words

Note that there are three kinds of de-
constructions, but only the first kind is the DE
construction in (Chang et al., 2009a; Du & Way,
2010). After the filtering process, both the unfil-
tered pattern set and the filtered one are used to
build different syntactic reordering PBSMT sys-
tems for comparison purpose.

22



4 Word lattice construction

Both the devset and testset are transformed into
word lattices by the extracted patterns to incor-
porate potential reorderings. Figure 2 illustrates
this process: treelet T ′ is matched with a pat-
tern, then its leaf nodes {a1, · · · am} ∈ LA (span-
ning {w1, · · · , wp}) are swapped with leaf nodes
{b1, · · · , bn} ∈ LB (spanning {v1, · · · , vq}) on
the generated paths in the word lattice.

T’

a1

am b1

bn

... ...

... ...

w1 w2 ... wp v1 v2 vq...

w1 w2 ... wp v1 v2 vq...

w2...
wpv1

v2 ...

... ...

vq w1

Sub parse tree 

matched with 

a pattern

Source side 

sentence

Generated 

lattice

Figure 2: Incorporating potential reorderings into
lattices

We sort the matched patterns by preo in formula
(5), and only apply a pre-defined number of re-
orderings for each sentence. For each lattice node,
if we denote E0 as the edge from the original sen-
tence, while patterns {P1, · · · , Pi, · · · , Pk} are ap-
plied to this node, then E0 is weighted as in (6):

w(E0) = α+
k∑

i=1

{(1− α)
k

∗ {1− preo(Pi)}}

(6)
where preo(Pi) is the pattern weight in formula
(3), and α is the base probability to avoid E0 be-
ing equal to zero. Suppose {Es, · · · , Es+r−1} are
generated by r reordering schemes of Pi, then Ej
is weighted as in (7):

w(Ej) =
(1 − α)

k
∗preo(Pi)∗

ws−j+1(Pi)∑r
t=1 wt(Pi)

(7)

where wt(Pi) is the reordering scheme in formula
(5), and s <= j < s + r. Reordering patterns
with the same root lattice node share equal proba-
bilities in formula (6) and (7).

5 Experiments and results

We conducted our experiments on a medium-sized
corpus FBIS (a multilingual paragraph-aligned
corpus with LDC resource number LDC2003E14)
for the Chinese–English SMT task. The Cham-
pollion aligner (Ma, 2006) is utilized to perform
sentence alignment. A total number of 256,911
sentence pairs are obtained, while 2,000 pairs for
devset and 2,000 pairs for testset are randomly se-
lected, which we call FBIS set. The rest of the
data is used as the training corpus.

The baseline system is Moses (Koehn et
al., 2007), and GIZA++1 is used to perform
word alignment. Minimum error rate training
(MERT) (Och, 2003) is carried out for tuning. A
5-gram language model built via SRILM2 is used
for all the experiments in this paper.

Experiments results are reported on two differ-
ent sets: the FBIS set and the NIST set. For the
NIST set, the NIST 2005 testset (1,082 sentences)
is used as the devset, and the NIST 2008 test-
set (1,357 sentences) is used as the testset. The
FBIS set contains only one reference translation
for both devset and testset, while NIST set has
four references.

5.1 Pattern extraction and filtering with
functional words

The lattice scoring approach is carried out with
the same baseline system as specified above to
produce the phrase alignments. The initial PB-
SMT system in the lattice scoring approach is
tuned with the FBIS devset to obtain the weights.
As specified in section 2.1, phrase alignments are
generated in the step 4 of the lattice scoring ap-
proach.

From the generated phrase alignments and
source-side parse trees of the training corpus,
we obtain 48,285 syntactic reordering patterns
(57,861 reordering schemes) with an average
number of 11.02 non-terminals. For computa-
tional efficiency, any patterns with number of non-
terminal less than 3 and more than 9 are pruned.
This procedure leaves 18,169 syntactic reordering
patterns (22,850 reordering schemes) with a aver-

1http://fjoch.com/GIZA++.html
2http://www.speech.sri.com/projects/srilm/

23



age number of 7.6 non-terminals. This pattern set
is used to built the syntactic reordering PBSMT
system without pattern filtering, which here after
we call the ‘unfiltered system’.

Using the tags specified in Table 1, the ex-
tracted syntactic reordering patterns without func-
tional words are filtered out, while only 6,926 syn-
tactic reordering patterns (with 9,572 reordering
schemes) are retained. Thus the pattern set are
reduced by 61.88%, and over half of them are
pruned by the functional word tags. The filtered
pattern set is used to build the syntactic reorder-
ing PBSMT system with pattern filtering, which
we refer as the ‘filtered system’.

Type Tag Patterns Percent
ba-const. BA 222 3.20%

bei-const.
LB 97

2.79%
SB 96

de-const. (1st)
DEC 1662

60.11%
DEG 2501

de-const. (2nd) DER 52 0.75%
de-const. (3rd) DEV 178 2.57%

preposition
P 2591 37.41%

excl. ba & bei

Table 2: Statistics on the number of patterns for
each type of functional word

Statistics on the patterns with respect to func-
tional word types are shown in Table 2. The num-
ber of patterns for each functional word in the fil-
tered pattern set are illustrated, and percentages of
functional word types are also reported. Note that
some patterns contain more than one kind of func-
tional word, so that the percentages of functional
word types do not sum to one.

As demonstrated in Table 2, the first kind of de-
construction takes up 60.11% of the filtered pat-
tern set, and is the main type of patterns used in
our experiment. This indicates that more than half
of the patterns are closely related to the DE con-
struction examined in (Chang et al., 2009b; Du
& Way, 2010). However, the general preposi-
tion construction (excluding bei and ba) accounts
for 37.41% of the filtered patterns, which implies
that it is also a major source of syntactic reorder-
ing. By contrast, other constructions have much
smaller amount of percentages, so have a minor

impact on our experiments.

5.2 Word lattice construction

As specified in section 4, for both unfiltered and
the filtered systems, both the devset and testset
are converted into word lattices with the unfiltered
and filtered syntactic reordering patterns respec-
tively. To avoid a dramatic increase in size of the
lattices, the following constraints are applied: for
each source sentence, the maximum number of re-
ordering schemes is 30, and the maximum span of
a pattern is 30.

For the lattice construction, the base probabil-
ity in (6) and (7) is set to 0.05. The two syntac-
tic reordering PBSMT systems also incorporate
the built-in reordering models (distance-based and
lexical reordering) of Moses, and their weights in
the log-linear model are tuned with respect to the
devsets.

The effects of the pattern filtering by functional
words are also reported in Table 3. For both the
FBIS and NIST sets, the average number of nodes
in word lattices are illustrated before and after pat-
tern filtering. From the table, it is clear that the
pattern filtering procedure dramatically reduces
the input size for the PBSMT system. The reduc-
tion is up to 37.99% for the NIST testset.

Data set Unfiltered Filtered Reduced
FBIS dev 183.13 131.38 28.26%
FBIS test 183.68 136.56 25.65%
NIST dev 175.78 115.89 34.07%
NIST test 149.13 92.48 37.99%

Table 3: Comparison of the average number of
nodes in word lattices

5.3 Results on FBIS set

Three systems are compared on the FBIS set:
the baseline PBSMT system, and the syntactic
reordering systems with and without pattern fil-
tering. Since the built-in reordering models of
Moses are enabled, several values of the distor-
tion limit (DL) parameter are chosen to validate
consistency. The evaluation results on the FBIS
set are shown in Table 4.

As shown in Table 4, the syntactic reordering
systems with and without pattern filtering outper-

24



System DL BLEU NIST METE

Baseline

0 22.32 6.45 52.51
6 23.67 6.63 54.07

10 24.52 6.66 54.04
12 24.57 6.69 54.31

Unfiltered

0 23.92 6.60 54.30
6 24.57 6.68 54.64

10 24.98 6.71 54.67
12 24.84 6.69 54.65

Filtered

0 23.71 6.60 54.11
6 24.65 6.68 54.61

10 24.87 6.71 54.84
12 24.91 6.7 54.51

Table 4: Results on FBIS testset (DL = distortion
limit, METE=METEOR)

form the baseline system for each of the distortion
limit parameters in terms of the BLEU, NIST and
METEOR scores (scores in bold face). By con-
trast, the filtered systems has a comparable perfor-
mance with the unfiltered system: for some of the
distortion limits, the filtered systems even outper-
forms the unfiltered system (scores in bold face,
e.g. BLEU and NIST for DL=12, METEOR for
DL=10).

The best performance of the baseline system
is obtained with distortion limit 12 (underlined);
the best performance of the unfiltered system is
achieved with distortion limit 10 (underlined);
while for the filtered system, the best BLEU score
is accomplished with distortion limit 12 (under-
lined), and the best NIST and METEOR scores
are shown with distortion limit 10 (underlined).
Thus the unfiltered system outperforms the base-
line by 0.41 (1.67% relative) BLEU points, 0.02
(0.30% relative) NIST points and 0.36 (0.66%
relative) METEOR points. By contrast, the fil-
tered system outperforms the baseline by 0.34
(1.38% relative) BLEU points, 0.02 (0.30% rel-
ative) NIST points and 0.53 (0.98% relative) ME-
TEOR points.

Compared with the unfiltered system, pattern
filtering with functional words degrades perfor-
mance by 0.07 (0.28% relative) in term of BLEU,
but improves the system by 0.17 (0.31% rela-
tive) in term of METEOR, while the two systems
achieved the same best NIST score.

These results indicates that the filtered system
has a comparable performance with the unfiltered
one on the FBIS set, while both of them outper-
form the baseline.

5.4 Results on NIST set

The evaluation results on the NIST set are illus-
trated in Table 5.

System DL BLEU NIST METE

Baseline

0 14.43 5.75 45.03
6 15.61 5.88 45.75

10 15.73 5.78 45.27
12 15.89 6.16 45.88

Unfiltered

0 16.77 6.54 47.16
6 17.25 6.67 47.65

10 17.15 6.64 47.78
12 16.88 6.56 47.17

Filtered

0 16.79 6.64 47.67
6 17.55 6.71 48.06

10 17.51 6.72 48.15
12 17.37 6.72 48.08

Table 5: Results on NIST testset (DL = distortion
limit, METE=METEOR)

From Table 5, the unfiltered system outper-
forms the baseline system for each of the distor-
tion limits in terms of the BLEU, NIST and ME-
TEOR scores (scores in bold face). By contrast,
the filtered system also outperform the unfiltered
system for each of the distortion limits in terms of
the three evaluation methods (scores in bold face).

The best performance of the baseline system
is obtained with distortion limit 12 (underlined),
while the best performance of the unfiltered sys-
tem is obtained with distortion limit 6 for BLEU
and NIST, and 10 for METEOR (underlined). For
the filtered system, the best BLEU score is shown
with distortion limit 6, and the best NIST and ME-
TEOR scores are accomplished with distortion
limit 10 (underlined). Thus the unfiltered system
outperforms the baseline by 1.36 (8.56% relative)
BLEU points, 0.51 (8.28% relative) NIST points
and 1.90 (4.14% relative) METEOR points. By
contrast, the filtered system outperforms the base-
line by 1.66 (10.45% relative) BLEU points, 0.56
(9.52% relative) NIST points and 2.27 (4.95% rel-
ative) METEOR points.

25



Compared with the unfiltered system, patterns
with functional words boost the performance by
0.30 (1.74% relative) in term of BLEU, 0.05
(0.75% relative) in term of NIST, and 0.37 (0.77%
relative) in term of METEOR.

These results demonstrate that the pattern filter-
ing improves the syntactic reordering system on
the NIST set, while both of them significantly out-
perform the baseline.

5.5 Discussion

Experiments in the previous sections demonstrate
that: 1) the two syntactic reordering systems im-
prove the PBSMT system by providing potential
reorderings obtained from phrase alignments and
parse trees; 2) patterns with functional words play
a major role in the syntactic reordering process,
and filtering the patterns with functional words
maintains or even improves the system perfor-
mance for Chinese–English SMT task. Further-
more, as shown in the previous section, pattern
filtering prunes the whole pattern set by 61.88%
and also reduces the sizes of word lattices by up
to 37.99%, thus the whole syntactic reordering
procedure for the original inputs as well as the
tuning/decoding steps are sped up dramatically,
which make the proposed methods more useful in
the real world, especially for online SMT systems.

From the statistics on the filtered pattern set
in Table 2, we also argue that the first kind
of de-construction and general preposition (ex-
cluding bei and ba) are the main sources of
Chinese–English syntactic reordering. Previous
work (Chang et al., 2009b; Du & Way, 2010)
showed the advantages of dealing with the DE
construction. In our experiments too, even though
all the patterns are automatically extracted from
phrase alignments, these two constructions still
dominate the filtered pattern set. This result con-
firms the effectiveness of previous work on DE
construction, and also highlights the importance
of the general preposition construction in this task.

6 Conclusion and future work

Syntactic reordering patterns with functional
words are examined in this paper. The aim is to
exploit these functional words within the syntactic
reordering patterns extracted from phrase align-

ments and parse trees. Three systems are com-
pared: a baseline PBSMT system, a syntactic re-
ordering system with all patterns extracted from a
corpus and a syntactic reordering system with pat-
terns filtered with functional words. Evaluation
results on a medium-sized corpus showed that the
two syntactic reordering systems consistently out-
perform the baseline system. The pattern filtering
with functional words prunes 61.88% of patterns,
but still maintains a comparable performance with
the unfiltered one on the randomly select testset,
and even obtains 1.74% relative improvement on
the NIST 2008 testset.

In future work, the structures of patterns con-
taining functional words will be investigated to
obtain fine-grained analysis on such words in this
task. Furthermore, experiments on larger corpora
as well as on other language pairs will also be car-
ried out to validation our method.

Acknowledgements

This research is supported by Science Foundation
Ireland (Grant 07/CE/I1142) as part of the Centre
for Next Generation Localisation (www.cngl.ie) at
Dublin City University. Thanks to Yanjun Ma for
the sentence-aligned FBIS corpus.

References

Yaser Al-Onaizan and Kishore Papineni 2006. Dis-
tortion models for statistical machine translation.
Coling-ACL 2006: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pages 529-536, Sydney,
Australia.

Pi-Chuan Chang, Dan Jurafsky, and Christopher
D.Manning 2009a. Disambiguating DE for
Chinese–English machine translation. Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 215-223, Athens, Greece.

Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009b. Discriminative
reordering with Chinese grammatical features. Pro-
ceedings of SSST-3: Third Workshop on Syntax and
Structure in Statistical Translation, pages 51-59,
Boulder, CO.

Michael Collins, Philipp Koehn, and Ivona Kučerová.
2005. Clause restructuring for statistical machine
translation. ACL-2005: 43rd Annual meeting of the

26



Association for Computational Linguistics, pages
531-540, University of Michigan, Ann Arbor, MI.

Josep M. Crego, and José B. Mariño. 2007. Syntax-
enhanced N-gram-based SMT. MT Summit XI,
pages 111-118, Copenhagen, Denmark.

Jinhua Du and Andy Way. 2010. The Impact of
Source-Side Syntactic Reordering on Hierarchical
Phrase-based SMT. EAMT 2010: 14th Annual Con-
ference of the European Association for Machine
Translation, Saint-Raphaël, France.

Jakob Elming. 2008. Syntactic reordering integrated
with phrase-based SMT. Coling 2008: 22nd In-
ternational Conference on Computational Linguis-
tics, Proceedings of the conference, pages 209-216,
Manchester, UK.

Jakob Elming, and Nizar Habash. 2009. Syntac-
tic reordering for English-Arabic phrase-based ma-
chine translation. Proceedings of the EACL 2009
Workhop on Computational Approaches to Semitic
Languages, pages 69-77, Athens, Greece.

Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. MT Summit XI, pages
215-222, Copenhagen, Denmark.

Jie Jiang, Andy Way, Julie Carson-Berndsen. 2010.
Lattice Score-Based Data Cleaning For Phrase-
Based Statistical Machine Translation. EAMT
2010: 14th Annual Conference of the European As-
sociation for Machine Translation, Saint-Raphaël,
France.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. ACL 2007: proceedings of demo and
poster sessions, pp. 177-180, Prague, Czech Repub-
lic.

Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. ACL 2007: proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 720-727, Prague, Czech
Republic.

Xiaoyi Ma. 2006. Champollion: A Robust Paral-
lel Text Sentence Aligner. LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation, pp.489-492, Genova, Italy.

Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. ACL-2003:

41st Annual meeting of the Association for Compu-
tational Linguistics, pp. 160-167, Sapporo, Japan.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method For Automatic
Evaluation of Machine Translation. ACL-2002:
40th Annual meeting of the Association for Compu-
tational Linguistics, pp.311-318, Philadelphia, PA.

Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. Coling-ACL 2006:
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 433-440, Sydney, Australia.

Felipe Sánchez-Martı́nez and Andy Way. 2009.
Marker-based filtering of bilingual phrase pairs for
SMT. EAMT-2009: Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation, pages 144-151, Barcelona,
Spain.

Chao Wang, Michael Collins, and Philipp Koehn.
2007a. Chinese syntactic reordering for statistical
machine translation. EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
737-745, Prague, Czech Republic.

Fei Xia, and Michael McCord 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. Coling 2004: 20th International
Conference on Computational Linguistics, pages
508-514, University of Geneva, Switzerland.

Nianwen Xue, Fei Xia, Fu-dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2), pages 207-238.

Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP, pages 333-341, Suntec, Singa-
pore.

Yuqi Zhang, Richard Zens, and Hermann Ney 2007a.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statisti-
cal machine translation. SSST, NAACL-HLT-2007
AMTA Workshop on Syntax and Structure in Statis-
tical Translation, pages 1-8, Rochester, NY.

Yuqi Zhang, Richard Zens, and Hermann Ney 2007b.
Improved chunk-level reordering for statistical ma-
chine translation. IWSLT 2007: International Work-
shop on Spoken Language Translation, pages 21-28,
Trento, Italy.

27


