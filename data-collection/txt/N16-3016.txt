



















































SODA:Service Oriented Domain Adaptation Architecture for Microblog Categorization


Proceedings of NAACL-HLT 2016 (Demonstrations), pages 77–81,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

SODA : Service Oriented Domain Adaptation Architecture
for Microblog Categorization

Himanshu S. Bhatt, Sandipan Dandapat, Peddamuthu Balaji, Shourya Roy,
Sharmistha∗∗and Deepali Semwal∗

Xerox Research Centre India, Bangalore, INDIA
{Himanshu.Bhatt, Sandipan.Dandapat, Balaji.Peddamuthu2, Shourya.Roy}@xerox.com

∗{sharmistha@ssl.serc.iisc.in, semwaldeepali@gmail.com}

Abstract

We demonstrate SODA (Service Oriented Do-
main Adaptation) for efficient and scalable
cross-domain microblog categorization which
works on the principle of transfer learning.
It is developed on a novel similarity-based
iterative domain adaptation algorithm while
extended with features such as active learn-
ing and interactive GUI to be used by busi-
ness professionals. SODA demonstrates ef-
ficient classification accuracy on new collec-
tions while minimizing and sometimes elimi-
nating the need for expensive data labeling ef-
forts. SODA also implements an active learn-
ing (AL) technique to select informative in-
stances from the new collection to seek anno-
tations, if a small amount of labeled data is
required by the adaptation algorithm.

1 Introduction

Online social media, such as Twitter.com, have be-
come the de facto standard for sharing information,
thoughts, ideas, personal feelings, daily happenings
etc. which essentially led research and development
in the field of social media analytics to flourish. So-
cial media analytics provide actionable insights to
business by analyzing huge amount of user gener-
ated content (UGC) (Sriram et al., 2010; Jo and Oh,
2011; He et al., 2012; Si et al., 2013; Nakov et al.,
2013). Sentiment categorization, one of the common
social media analytics task, segregates a collection
of UGC into different buckets with positive, neg-
ative or neutral orientation (Liu and Zhang, 2012;

∗Work done at Xerox Research Centre India

Thelwall et al., 2011; Bollen et al., 2009). This in-
formation is used to aggregate statistics and identify
trends which are helpful for many applications viz.
Customer Care, Product Marketing, User Studies.

Supervised machine learning (ML) techniques
such as text categorization have played a key en-
abler role to classify microblogs into sentiment cat-
egories (Pang and Lee, 2008; Tan et al., 2009; Go et
al., 2009; Fernández et al., 2014). These are trained
on a fraction of annotated data as per client pro-
vided label set e.g. {positive, negative, and neu-
tral} for a product/service/domain 1. One of the
obstacles towards rapid adoption of such systems is
requirement of labeled tweets for developing ML-
based models as it requires extensive human labeling
efforts. Additionally, need of manual labeling slows
down the process of categorization on high velocity
social media which requires fast analytic insights.
From our conversations with business professionals,
we derived the need of a practical solution which
would help them scale up across hundreds of col-
lections and domains without the overhead of anno-
tating data and building models from scratch every
time for a new collection.

In this paper, we demonstrate Service Oriented
Domain Adaptation (SODA) which offers social
media analytics as-a-service to the users. Specif-
ically, it provides sentiment categorization as-a-
service that allows users to efficiently analyze com-
ments from any new collection without the over-

1We use the word collection to describe tweets pertaining
to a product/service/domain. For example tweets pertaining to
Apple iPhone Maps and Samsung S3 Battery are two different
collections. Tweets and comments are used interchangeably.

77



Figure 1: Overall architecture of SODA.

head of manual annotations or re-training models.
It thus enables faster wide-scale analysis within and
across different domains/industries such as telecom,
healthcare, finance etc. SODA is based on an iter-
ative ensemble based adaptation technique (Bhatt et
al., 2015) which gradually transfers knowledge from
the source to the new target collection while being
cognizant of similarity between the two collections.
It has been extensively evaluated by business profes-
sionals in a user-trial and on a benchmark dataset.

2 SODA Features
Figure 1 illustrates the architecture of SODA com-
prising three primary modules, 1) similarity, 2) do-
main adaptation, and 3) active learning. The first
two modules use unlabeled data from the new col-
lection while the optional third module helps in cre-
ating labeled data for enhanced classification perfor-
mance. These modules are explained below.

2.1 Similarity

In social media analytics, especially for senti-
ment categorization, there exist numerous collec-
tions about different products or services where la-
beled data is available and thus can be used to adapt
to a new unlabeled collection. Given a target col-
lection, the key question is to identify the best pos-
sible source collection to adapt from. The similar-
ity module in SODA identifies the best adaptable
source collection based on the similarity between
the source and target collections. This is based on
the observations from existing literature (Bhatt et al.,
2015; Blitzer et al., 2007) which suggest that if the
source and target collections are similar, the adapta-
tion performance tends to be better than if the two
collections are dissimilar. The similarity module in
SODA is capable of computing different kinds of

lexical, syntactic, and semantic similarities between
unlabeled target and labeled source collections. For
this demonstration on sentiment categorization from
social media data, it measures cosine similarity be-
tween the comments in each collection and com-
putes sim as the similarity score.

2.2 Domain Adaptation

The heart of SODA is the adaptation module that
works on two principles, generalization and adapta-
tion. During generalization, it learns shared com-
mon representation (Blitzer et al., 2007; Ji et al.,
2011; Pan et al., 2010) which minimizes the diver-
gence between two collections. We leverage one
of the widely used structural correspondence learn-
ing (SCL) approach (Blitzer et al., 2007) to com-
pute shared representations. The idea adhered here
is that a model learned on the shared feature rep-
resentation using labeled data from the source col-
lection will also generalize well on the target col-
lection. Towards this, we learn a model (CS) on the
shared feature representation from the source collec-
tion, referred to as “source classifier”. CS is then
used to predict labels for the pool of unlabeled in-
stances from the target collection, referred to as Pu,
using the shared representations. All instances in Pu
which are predicted with a confidence (α1) higher
than a predefined threshold (θ1) are moved to the
pool of pseudo-labeled target instances, referred to
as Ps. We now learn a target domain model CT on
Ps using the target specific representation, referred
to as “target classifier”.
CT captures a separate view of the target in-

stances than the shared representation and hence
brings in discriminating target specific information
which is useful for categorization in target collec-
tion. For further adaptation, the source (CS) and
target (CT ) classifiers are combined in a weighted
ensemble (E) with ws and wt as the corresponding
weights and iterate over the remaining unlabeled in-
stances in Pu. In each iteration, the ensemble pro-
cesses the remaining instances and iteratively adds
confidently predicted instances to Ps which are used
to re-train/update CT . This iterative process contin-
ues till all instances in Pu are confidently labeled or
a maximum number of iterations is reached. Trans-
fer occurs within the ensemble where the source
classifier progressively facilitates the learning of tar-

78



Algorithm 1 Adaptation Algorithm
Input: Cs, Q, Ct, Ps, & Pu.
Iterate: l = 0 : till Pu = {φ}
Process: Construct ensemble E → wslCs + wtlCt. Initialize wsl &
wtl tos 0.5.
for i = 1 to n (size of Pu) do

Predict labels: E(Qxi, xi)→ ŷi; confidence of prediction: αi.
if αi > θ2 then

Move ith instance from Pu to Ps with pseudo label ŷi.
end if.

end for.
Re-train Ct & update wsl and w

t
l as:

ws
(l+1)

=
(sim∗ws

l
∗I(Cs))

(sim∗ws
l
∗I(Cs)+(1−sim)∗wtl∗I(Ct))

wt
(l+1)

=
((1−sim)∗wt

l
∗I(Ct))

(sim∗ws
l
∗I(Cs)+(1−sim)∗wtl∗I(Ct))

end iterate.
Output: Final Ct and updated weights ws and wt

get classifier by providing pseudo labeled instances.
The weights of the individual classifiers are updated,
as a function of error (I(·)) and the similarity (sim)
between the collections, which gradually shift the
emphasis from source to the target classifier. Finally,
the ensemble is used to predict labels for future un-
seen instances in the target collection. Algorithm 1
summarizes our approach (refer (Bhatt et al., 2015)
for more details).

2.3 Active Learning
SODA also implements an active learning module
to allow users to annotate a few selected informative
comments from the target collection. These com-
ments are selected using cross entropy difference
(CED) (Axelrod et al., 2011) such that the difference
with source collection and the similarity with target
collection is maximized. It selects comment(s) from
target collection that have low CED score i.e. com-
ments that have high entropy with respect to source
HS(·) and low entropy with respect to target collec-
tion HT (·) as in Equation (1).

CED(s) = HT (s)−HS(s) (1)

Note, this active learning module is optional and
should be used when the adaptation performance
with unlabeled instances is not satisfactory. More
and more instances can be annotated in multiple
rounds till a satisfactory performance is achieved.
These annotated instances are used to build a
stronger target classifier for the ensemble based
adaptation algorithm.

Figure 2: Interactive GUI of service oriented domain
adaptation (SODA) (best viewed in color).

3 Design and Internals

Figure 2(a) illustrates the interactive user interface
(UI) of SODA where one can select a new target
collection for the analysis task (i.e. sentiment cat-
egorization). For a new target collection, it identi-
fies relevant adaptable source collections based on
their similarity. One can select any of the candidate
source collections (selected collection highlighted
in Figure 2(a)) and adapt. Figure 2(b) shows the
performance report along with the predicted com-
ments from the target collection. User evaluates the
adaptation performance in unlabeled target collec-
tion by analyzing the predicted comments and de-
cides whether to annotate additional comments in

79



Figure 3: The effect of labeled comments on the per-
formance while adapting from Coll-1→ Coll-6.
target collection? If yes, Figure 2(c) lists a few in-
formative comments selected using the active learn-
ing module to seek annotations. One can mark these
comments as positive, negative or neutral and subse-
quently adapt using these labeled instances from the
target collection. Figure 2(b) also shows the adap-
tation performance with a few labeled instances in
the target collection. One can continue annotating
more instances in the target collection until satis-
factory performance is achieved. For more detailed
demonstration, please refer to the video.2

The interactive UI of SODA is developed using
Ruby on Rails framework. All collections are man-
aged in MySQL server. All three modules in SODA
fetch data from the server and write the output back
to the server. All modules work in real time enabling
the system to be highly responsive to the user. The
application is hosted on Amazon AWS as RESTful
web services using Java Jersey (Tomcat server) that
act as a bridge between the UI and back end.

4 User Trial & Experimental Results

To evaluate the overall experience, a user trial was
conducted where several business professionals pro-
vided feedback on SODA. The objective was to eval-
uate the overall usability, reduction in required ef-
forts, and the performance on the new target collec-
tions. The overall evaluation rated SODA 5 on us-
ability and 4 for reduction in efforts (1 being worst
& 5 being the best). Table 1 reports the classifica-
tion accuracy of SODA with few labeled comments
from the target collection (ranging from 0 to 100). It
also reports the performance of the in-domain clas-
sifier which is trained and tested on data from the
same collection. Coll-1 to Coll-8 refer to collections
pertaining to marketing & sales, comcast support,

2https://www.youtube.com/watch?v=
zKnP5QEHVAE

Table 1: User-trial results on social media data.

Source Target Accuracy with annotations In-domain0 25 50 75 100
Coll-1 Coll-6 70 74 77 79 80 71.7
Col-2 Coll-8 73 45 71 73 79 74.5
Coll-3 Coll-5 87 94 94 93 95 92.0
Coll-4 Coll-7 83 84 94 96 96 85.7
Table 2: Results on the Amazon review dataset.

Target Source BL SCL SODA In-domain

B
E 61.5 76.1 78.9

80.4K 68.4 66.2 74.8
D 60.1 77.6 80.0

E
B 68.2 77.8 80.3

84.4D 61.5 74.1 76.4
K 76.2 83.1 85.2

K
B 71.7 78.8 80.1

87.7D 60.3 79.4 82.0
E 73.2 83.8 87.9

D
B 63.5 74.2 81.9

82.4E 62.3 74.8 80.1
K 67.4 76.5 78.8

DirectTV support, ASUS, Johnson & Johnson CSAT,
Apple iPhone6, and HUWAEI respectively. Figure
3 compares the effect of adding labeled comments
in batches of 25 comments at-a-time. When there
is no labeled data in the target collection, in-domain
classifier can not be applied while SODA still yields
good classification accuracy. Moreover, SODA con-
sistently performs better than the in-domain classi-
fier with same amount of labeled data.

We also evaluated the performance of domain
adaptation (DA) module of SODA on the Ama-
zon review dataset (Blitzer et al., 2007) which is a
benchmark dataset for sentiment categorization. It
has 4 domains, namely, books(B), dvds(D), elec-
tronics(E), and kitchen(K) each with 2000 reviews
divided equally into positive and negative reviews.
Table 2 shows that DA module of SODA outper-
forms 1) a widely used domain adaptation technique
, namely, structural correspondence learning (SCL)
(Blitzer et al., 2007; Blitzer et al., 2006), 2) the base-
line (BL) where a classifier trained on one domain
is applied on another domain, and 3) the in-domain
classifier. Note that in Table 2, the performance of
DA module of SODA is reported when it does not
use any labeled instances from the target domain.

5 Conclusion
We demonstrated SODA for efficient microblog cat-
egorization on new social media collections with
minimum (or sometimes no) need of manual anno-
tations; thus, enabling faster and efficient analytics.

80



References

Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 355–
362.

Himanshu Sharad Bhatt, Deepali Semwal, and Shourya
Roy. 2015. An iterative similarity based adapta-
tion technique for cross-domain text classification. In
Proceedings of Conference on Computational Natural
Language Learning.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of Conference on
Empirical Methods in Natural Language Processing,
pages 120–128.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boomboxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of Association for Computational Lin-
guistics, pages 187–205.

Johan Bollen, Alberto Pepe, and Huina Mao. 2009.
Modeling public mood and emotion: Twitter sentiment
and socio-economic phenomena. CoRR.

Javi Fernández, Yoan Gutiérrez, José M Gómez, and
Patricio Martınez-Barco. 2014. Gplsi: Supervised
sentiment analysis in twitter using skipgrams. In Pro-
ceedings of International Workshop on Semantic Eval-
uation (SemEval), pages 294–299.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1:12.

Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai Wong.
2012. Tracking sentiment and topic dynamics from
social media. In Proceedings of International AAAI
Conference on Weblogs and Social Media.

Yang-Sheng Ji, Jia-Jun Chen, Gang Niu, Lin Shang, and
Xin-Yu Dai. 2011. Transfer learning via multi-view
principal component analysis. Journal of Computer
Science and Technology, 26(1):81–98.

Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of International Conference on Web Search
and Data Mining, pages 815–824.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415–463.

Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings International Conference on World Wide
Web, pages 751–760.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi
Li, and Xiaotie Deng. 2013. Exploiting topic based
twitter sentiment for stock prediction. In Proceedings
of Association for Computational Linguistics, volume
2013, pages 24–29.

Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-
hatosmanoglu, and Murat Demirbas. 2010. Short text
classification in twitter to improve information filter-
ing. In Proceedings of International ACM SIGIR con-
ference on Research and Development in Information
Retrieval, pages 841–842.

Songbo Tan, Xueqi Cheng, Yuefen Wang, and Hongbo
Xu. 2009. Adapting naive bayes to domain adaptation
for sentiment analysis. In Advances in Information Re-
trieval, pages 337–349.

Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.
2011. Sentiment in twitter events. Journal of the
American Society for Information Science and Tech-
nology, 62(2):406–418.

81


