



















































Template Kernels for Dependency Parsing


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1422–1427,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Template Kernels for Dependency Parsing

Hillel Taub-Tabib
Hebrew University
Jerusalem, Israel
{hillel.t,yoav.goldberg}@gmail.com

Yoav Goldberg
Bar-Ilan University
Ramat-Gan, Israel

Amir Globerson
Hebrew University
Jerusalem, Israel

gamir@cs.huji.ac.il

Abstract

A common approach to dependency parsing
is scoring a parse via a linear function of a
set of indicator features. These features are
typically manually constructed from templates
that are applied to parts of the parse tree. The
templates define which properties of a part
should combine to create features. Existing
approaches consider only a small subset of
the possible combinations, due to statistical
and computational efficiency considerations.
In this work we present a novel kernel which
facilitates efficient parsing with feature rep-
resentations corresponding to a much larger
set of combinations. We integrate the kernel
into a parse reranking system and demonstrate
its effectiveness on four languages from the
CoNLL-X shared task.1

1 Introduction
Dependency parsing is the task of labeling a sen-
tence x with a syntactic dependency tree y ∈ Y (x),
where Y (x) denotes the space of valid trees over x.
Each word in x is represented as a list of linguis-
tic properties (e.g. word form, part of speech, base
form, gender, number, etc.). In the graph based ap-
proach (McDonald et al., 2005b) parsing is cast as a
structured linear prediction problem:

hv(x) = argmax
y∈Y(x)

vT · Φ (x, y) (1)

where Φ (x, y) ∈ Rd is a feature representation de-
fined over a sentence and its parse tree, and v ∈ Rd
is a vector of parameters.

To construct an effective representation, Φ (x, y)
is typically decomposed into local representations

1See https://bitbucket.org/hillel/templatekernels for imple-
mentation.

over parts p of the tree y:

Φ (x, y) =
∑
p∈y

φ (x, p)

Standard decompositions include different types of
parts: arcs, sibling arcs, grandparent arcs, etc. Fea-
ture templates are then applied to the parts to con-
struct the local representations. The templates de-
termine how the linguistic properties of the words
in each part should combine to create features (see
Section 2).

Substantial effort has been dedicated to the man-
ual construction of feature templates (McDonald et
al., 2005b; Carreras, 2007; Koo and Collins, 2010).
Still, for both computational and statistical reasons,
existing templates consider only a small subset of
the possible combinations of properties. From a
computational perspective, solving Eq. 1 involves
applying the templates to y and calculating a dot
product in the effective dimension of Φ. The use
of many templates thus quickly leads to computa-
tional infeasibility (the dimensionality of v, as well
as the number of non-zero features in Φ, become
very large). From a statistical perspective, the use
of a large number of feature templates can lead to
overfitting.

Several recent works have proposed solutions to
the above problem. Lei et al., (2014) represented
the space of all possible property combinations in
an arc-factored model as a third order tensor and
learned the parameter matrix for the tensor under
a low rank assumption. In the context of transi-
tion parsers, Chen and Manning (2014) have im-
plemented a neural network that uses dense repre-
sentations of words and parts of speech as its in-
put and implicitly considers combinations in its in-
ner layers. Earlier work on transition-based depen-
dency parsing used SVM classifiers with 2nd order
polynomial kernels to achieve similar effects (Hall

1422



Figure 1: Feature template over the second order con-
secutive siblings part type. The part type contains slots
for the head (h), sibling (s) and modifier (m) words, as
well as for the two edges (e1 and e2). Each slot is asso-
ciated with a set of properties. The directed path skips
over the edge properties and defines the partial template
<h-cpos=?; s-cpos=?; m-gender=?>.

et al., 2006). While training greedy transition-based
parsers such as the ones used in (Chen and Manning,
2014) and (Hall et al., 2006) amounts to training a
multiclass classifier, the graph-based parsing frame-
work explored in (Lei et al., 2014) and in the present
work is a more involved structured-learning task.

In this paper we present a kernel based approach
to automated feature generation in the context of
graph-based parsing. Compared to tensors and neu-
ral networks, kernel methods have the attractive
properties of a convex objective and well understood
generalization bounds (Shawe-Taylor and Cristian-
ini, 2004). We introduce a kernel that allows us to
learn the parameters for a representation similar to
the tensor representation in (Lei et al., 2014) but
without the low rank assumption, and without ex-
plicitly instantiating the exponentially many possi-
ble features.

In contrast to previous works on parsing with ker-
nels (Collins and Duffy, 2002), in which the ker-
nels are defined over trees and count the number
of shared subtrees, our focus is on feature combi-
nations. In that sense our work is more closely re-
lated to work on tree kernels for relation extraction
(Zelenko et al., 2003; Culotta and Sorensen, 2004;
Reichartz et al., 2010; Sun and Han, 2014), but the
kernel we propose is designed to generate combi-
nations of properties within selected part types and
does not involve the all-subtrees representation.

2 Template Kernels

For simplicity, we begin with the case where parts
p correspond to head modifier pairs (h,m) (i.e. all
parts belong to the ”arc” part type). The features in
φ(x, p) can then depend on any property of h,m and
the sentence x. We denote properties related to h us-
ing the prefix h- (e.g., h-pos corresponds to the part-
of-speech of the head), and similarly for m-. We
also use e- to denote properties related to the triplets
h,m, x (e.g., the surface distance between h,m in x
is denoted by e-dist).

Templates defined over the ”arc” part type will
then combine different properties of h,m and e,
to create features. e.g. the template <h-form=?;
e-dist=?; m-form=?,m-pos=?>, when applied to
a dependency arc, may yield the feature: <h-
form=dog;e-dist=1;m-form=black,m-pos=JJ>.

More generally, a parse tree part can be seen as
ordered lists of slots that contain properties (differ-
ent part types will contain different lists of slots).
The feature templates defined over them select one
property from each slot (possibly skipping some
slots to produce partial templates). A template can
thus be thought of as a directed path between the
properties it selects in the different slots. Clearly, the
number of possible templates in a given part type is
exponential in the number of its slots. Figure 1 de-
picts the process for sibling parts.

As discussed in Section 1, manually constructed
feature templates consider only a small subset of the
combinations of properties (i.e. a small number of
“good” paths is manually identified and selected).
Our goal is to introduce a kernel that allows us to
represent all possible paths for a part type in poly-
nomial time.

Formally, let Φ (x, y) =
∑
p∈y

φ (x, p) be a feature

representation which associates a feature with any
distinct combination of properties in any of the tree
parts in the training set. For a given part p, the effec-
tive dimensionality of φ (x, p) is thus O (ms) where
s is the number of slots in p, and m is the maximal
number of properties in a slot.

Explicitly representing Φ (x, y) is therefore often
impractical. However, the well known “kernel trick”
(Shawe-Taylor and Cristianini, 2004) implies that
linear classifiers depend only on dot products be-
tween feature vectors and not on the feature vectors

1423



themselves. In the context of reranking (see Sec-
tion 3), it means we can learn classifiers if we can
calculate dot products K (y1, y2) = ΦT (x1, y1) ·
Φ (x2, y2) for two sentences and candidate parses.2

We first note that such dot products can be ex-
pressed as sum of dot products over parts:

K (y1, y2) =
∑
p∈y1

∑
p′∈y2

k
(
p, p′

)
where k (p, p′) = φ (x1, p) · φ (x2, p).

To calculate k (p, p′) we’ll assume for simplic-
ity that p and p′ are of the same type (otherwise
k(p, p′) = 0). Let pij and p′ij be the values of the
i’th property in the j’th slot in p, p′ (e.g., for a sec-
ond order sibling part as in Figure 1, p1,4 will cor-
respond to the label of the edge e2 in p) , and let
Cp↔p′ ∈ {0, 1}m×s be a binary matrix comparing p
and p′ such that

[
Cp↔p′

]
ij

= 1 when pij = p′ij and
0 otherwise. Simple algebra yields that:

k
(
p, p′

)
=
∏
j

~1T · [Cp↔p′]:,j
That is, calculating k (p, p′) amounts to multiply-
ing the sums of the columns in C.3 The runtime of
k (p, p′) is then O (m× s) which means the overall
runtime of K (y1, y2) is O (|y1| × |y2| × |s| × |m|),
where |y1| , |y2| are the number of parts in y1 and y2.

Finally, note that adding 1 to one of the column
counts ofC corresponds to a slot that can be skipped
to produce a partial template (this simulates a wild
card property that is always on).

3 Kernel Reranker
We next show how to use the template kernels within
a reranker. In the reranking approach (Collins and
Koo, 2005; Charniak and Johnson, 2005), a base
parser produces a list of k-best candidate parses for
an input sentence and a separately trained reranking
model is used to select the best one.

2For brevity we’ll omit x from the kernel parameters and use
K (y1, y2) instead of K ((x1, y1), (x2, y2)).

3We omit the proof, but intuitively, the product of column
sums is equal to the number of 1 valued paths between elements
in the different columns of C. Each such path corresponds to a
path in p and p′ where all the properties have identical values.
i.e. it corresponds to a feature that is active in both φ(x1, p) and
φ(x2, p

′) and thus contributes 1 towards the dot product.

Features: Our feature vector will have two parts.
One, Φg(x, y) ∈ Rd1 , consists of features obtained
from manually constructed templates. The other,
Φk(x, y) ∈ Rd2 , corresponds to our kernel features.
We will not evaluate or store it, but rather use the
kernel trick for implicitly learning with it, as ex-
plained below. The score of a candidate parse y
for sentence x is calculated via the following linear
function:

Φ (x, y) = [Φg (x, y) ,Φk (x, y)]
hv (x, y) = v · Φ (x, y) (2)

Learning For learning we use the passive-
aggressive algorithm (Crammer et al., 2006; Mc-
Donald et al., 2005a), and adapt it to use with ker-
nels. Formally, let S = {(xi,K (xi))}ni=1 be a train-
ing set of size n such that K (xi) = {yi1, . . . , yik}
is the set of k-best candidate trees produced for the
sentence xi. Assume that yi1 is the optimal tree in
terms of Hamming distance to the gold tree.

A key observation to make is that the v generated
by the PA algorithm will depend on two parameters.
One is a weight vector w ∈ Rd1 , in the manually
constructed Φg feature space. The other is a set of
weights αij with i = 1, . . . , n and j = 1, . . . , k cor-
responding to the jth candidate for the ith sample.4

The score is then given by:

fw,α(x, y) = v ·Φ (x, y) = w ·Φg (x, y)+fα (x, y)
where:

fα (x, y) =
∑
i,j

αij · (K (yi1, y)−K (yij , y))

We can now rewrite the updates of the PA algo-
rithm using w, α, as described in Alg 1.5

4 Implementation
The classifier depends on parameters αij , which are
updated using the PA algorithm. In the worst case,
all nk of these may be non-zero. For large datasets,
this may slow down both learning and prediction.

4This follows from tracing the steps of PA and noting their
dependence on dot products.

5The denominator in line 5 is equal to
‖Φg (xi, yij)− Φg (xi, yi1)‖2+K (yij , yij)−2K (yij , yi1)+
K (yi1, yi1) so it can be calculated efficiently using the kernel.
‖yi1 − yij‖1 is the hamming distance between yi1 and yij .
The updates for ᾱ are equivalent to averaging over all alphas in
iterations 1, ..., T . We use this form to save space.

1424



Below we discuss implementation techniques to mit-
igate this problem. To facilitate the discussion we
rewrite the dot-product computation as follows:

fα (x, y) =
∑
p′∈y

f̂α
(
x, p′

)
(3)

where:

f̂α
(
x, p′

)
=
∑
i,j

αij

∑
p∈yi1

k
(
p, p′
)−∑

p∈yij
k
(
p, p′
)

Reducing Prediction Runtime From Equation 3
we note several facts. First, prediction involves cal-
culating k (p, p′) for every combination of a part
p from the support instances (i.e., those for which
αij > 0) and part p′ from the instances in the k-best
list. Our implementation thus maintains its support
as a set of parts rather than a set of instances.

Second, parts that appear in both yi1 and yij do
not affect the result of fα (x, y) since they can-
cel each other out. Our implementation thus only
updates the support set with parts that belong ex-
clusively to either yi1 or yij . This improves per-
formance significantly since the number of non-
overlapping parts in yi1 and yij is typically much
smaller than the total number of parts therein.

Another important performance gain is obtained
by caching the results of f̂α (x, p′) when calculating
fα (x, y) for the different instances in the k-best list.
This avoids recalculating the summation for parts
that occur multiple times in the k-best list. Once
again, this amounts to a considerable gain, as the
number of distinct parts in the k-best list is much
smaller than the total number of parts therein.

Reducing Training Runtime We greatly improve
training speed by caching the results of fα (xi, yij)
between training iterations so that on each repeat-
ing invocation of the function, only the support parts
added since the previous iteration need to be consid-
ered. Since the predictions of the learning algorithm
become increasingly more accurate, the number of
added support parts decreases sharply between iter-
ations6, and so does the runtime. In practice, all iter-
ations from the 3rd onwards have negligible runtime
compared to the first and second iterations. This
technique allows us to comfortably train the kernel

6On correct predictions, τt at line 5 of Alg 1 is 0, so no
update is taking place and no support parts are added.

Algorithm 1 PA Algorithm for Template Kernels
Input: S = {(xi,K (xi))}ni=1, NumIters, Ag-

gressiveness parameter C
1: ∀i, j αij ← 0, ᾱij ← 0;T ← n×NumIters
2: for t = 1 to T do
3: i← t mod n
4: j ← argmax

j: yij∈K(xi)
fw,α (xi, yij)

5: τt←min
{
C,

fw,α(x,yij)−fw,α(x,yi1)+‖yi1−yij‖1
‖Φ(xi,yij)−Φ(xi,yi1)‖2

}
6: αij ← αij + τt
7: ᾱij ← ᾱij + τt (T − t+ 1)
8: w(t+1)←w(t)+τt (Φg(xi, yi1)−Φg(xi, yij))
9: end for

10: ∀i, j, ᾱij ← ᾱijT , w̄ = 1T
T∑
t=1

w(t)

Output: predictor: argmax
y∈K(x)

(fw̄,ᾱ (x, y))

predictor on large datasets.

5 Experimental Setup
Datasets We test our system on 4 languages from
the CoNLL 2006 shared task, all with rich mor-
phological features.7 The properties provided for
each word in these datasets are its form, part of
speech (pos), coarse part of speech (cpos), lemma
and morph features (number, gender, person, etc.
around 10-20 feats in total). We use 20-fold jack-
knifing to create the k-best lists for the reranker
(Collins and Duffy, 2002).

Base Parser The base parser used in experiments
was the sampling parser of Lei et al. (2014), aug-
mented to produce the k-best trees encountered dur-
ing sampling. The parser was set to use feature tem-
plates over third order part types, but its tensor com-
ponent and global templates were deactivated.

Features The manual features Φg were based on
first to third order templates from Lei et al. (2014).
For the kernel features Φk we annotated the nodes
and edges in each tree with the properties in Table 1.
We used a first order template kernel to train a model
using all the the possible combinations of head, edge
and modifier properties. Our kernel also produces all
the property combinations of the head and modifier
words (disregarding the edge properties).

7Our property combination approach is less relevant for tree-
banks that do not specify morphological properties. This is the

1425



Node Unigram properties:
form
pos
cpos
∀i morphi

form−1
pos−1
cpos−1

form+1
pos+1
cpos+1

Node Bigram properties: Edge prop:
pos−1, pos
pos, pos+1
pos, form
∀i pos, morphi

label
len, dist
always on

Table 1: Linguistic properties for nodes and edges.

Results For each language we train a Kernel
Reranker by running Alg 1 for 10 iterations over the
training set, using k-best lists of size 25 and C set to
infinity. As baseline, we train a Base Reranker in the
same setup but with kernel features turned off. Table
2 shows the results for the two systems. Even though
they use the same feature set, the base-reranker lags
behind the base-parser. We attribute this to the fact
that the reranker explores a much smaller fraction
of the search space, and that the gold parse tree
may not be available to it in either train or test
time. However, the kernel-reranker significantly im-
proves over the base-reranker. In Bulgarian and
Danish, the kernel-reranker outperforms the base-
parser. This is not the case for Slovene and Ara-
bic, which we attribute to the low oracle accuracy of
the k-best lists in these languages. As is common in
reranking (Jagarlamudi and Daumé III, 2012), our
final system incorporates the scores assigned to sen-
tences by the base parser: i.e. scorefinal (x, y) =
βscorebase (x, y) + scorereranker (x, y). β is tuned
per language on a development set.8 Our final sys-
tem outperforms the base parser, as well as Tur-
boParser (Martins et al., 2013), a parser based on
manually constructed feature templates over up to
third order parts. The system lags slightly behind
the sampling parser of Zhang et al. (2014) which ad-
ditionally uses global features (not used by our sys-
tem) and a tensor component for property combi-
nations. Another important difference between the
systems is that our search is severely restricted by
the use of a reranker. It is likely that using our ker-
nel in a graph-based parser will further improve its

reason we did not select the English treebank.
8To obtain a development set we further split the reranker

training sets into tuning training and a development sets (90/10).
We then tune β per language on the respective development sets
by selecting the best value from a list of {0, 0.05, . . . , 3}

Arabic Slovene Danish Bulgarian
Base Parser 80.15 86.13 90.76 92.98
Base Reranker 79.46 84.61 90.36 92.27
Kernel Reranker 79.48 85.25 91.04 93.28
Final System 80.19 86.44 91.56 93.4
Turbo Parser 79.64 86.01 91.48 93.1
Zhang et al. 80.24 86.72 91.86 93.72

Table 2: System Performance (UAS excluding punctua-
tion). TurboParser is (Martins et al., 2013), Zhang et al.
is (Zhang et al., 2014)

Arabic Slovene Danish Bulgarian
Sentences 1,460 1,534 5,190 12,823
Avg. Sent Len 37 19 18 15
Support Parts 15,466 10,101 31,627 58,842
Training Time 6m 7m 31m 57m
tokens/sec 551 432 223 99

Table 3: Runtime statistics, measured on a standard Mac-
book Pro 2.8 GHz Core i7 using 8 threads.

accuracy.

Performance Table 3 lists the performance met-
rics of our system on the four evaluation treebanks.
While training times are reasonable even for large
datasets, the increase in support size causes predic-
tion to become slow for medium and large training
sets. The number of support instances is a gen-
eral problem with kernel methods. It has been ad-
dressed using techniques like feature maps (Rahimi
and Recht, 2007; Lu et al., 2014) and bounded on-
line algorithms (Dekel et al., 2008; Zhao et al.,
2012). The application of these techniques to tem-
plate kernels is a topic for future research.

6 Conclusions
We present a kernel approach to graph based de-
pendency parsing. The proposed method facilitates
globally optimal parameter estimation in a high di-
mensional feature space, corresponding to the full
set of property combinations. We implemented our
solution as part of a parse reranking system, demon-
strating state of the art results. Future work will fo-
cus on performance improvements, using the kernel
on higher order parts, and integrating the kernel di-
rectly into a graph based dependency parser.

Acknowledgments This work is supported by the
US-Israel Binational Science Foundation (BSF, Grant No
2012330) and by the Intel Collaborative Research Insti-
tute for Computational Intelligence (ICRI-CI). We thank
the NAACL HLT reviewers for their comments.

1426



References
Xavier Carreras. 2007. Experiments with a higher-order

projective dependency parser. In EMNLP-CoNLL,
pages 957–961.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173–180.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), volume 1, pages 740–750.

Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263–270. Associa-
tion for Computational Linguistics.

Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–70.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. The Journal of Machine Learn-
ing Research, 7:551–585.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 423. Association for Com-
putational Linguistics.

Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based perceptron on
a budget. SIAM Journal on Computing, 37(5):1342–
1372.

Johan Hall, Joakim Nivre, and Jens Nilsson. 2006. Dis-
criminative classifiers for deterministic dependency
parsing. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 316–323.

Jagadeesh Jagarlamudi and Hal Daumé III. 2012. Low-
dimensional discriminative reranking. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 699–709.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1–11. Association for Computa-
tional Linguistics.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the 52nd

Annual Meeting of the Association for Computational
Linguistics, volume 1, pages 1381–1391.

Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri
Garakani, Dong Guo, Aurélien Bellet, Linxi Fan,
Michael Collins, Brian Kingsbury, Michael Picheny,
et al. 2014. How to scale up kernel methods
to be as good as deep neural nets. arXiv preprint
arXiv:1411.4000.

André FT Martins, Miguel Almeida, and Noah A Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics, pages 617–622.

Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
91–98.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523–530. Association for Computational Lin-
guistics.

Ali Rahimi and Benjamin Recht. 2007. Random features
for large-scale kernel machines. In Advances in neural
information processing systems, pages 1177–1184.

Frank Reichartz, Hannes Korte, and Gerhard Paass.
2010. Semantic relation extraction with kernels over
typed dependency trees. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 773–782.

John Shawe-Taylor and Nello Cristianini. 2004. Kernel
methods for pattern analysis. Cambridge university
press.

Le Sun and Xianpei Han. 2014. A feature-enriched tree
kernel for relation extraction. In The 52nd Annual
Meeting of the Association for Computational Linguis-
tics, volume 2, pages 61–67.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning
Research, 3:1083–1106.

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1013–1024.

Peilin Zhao, Jialei Wang, Pengcheng Wu, Rong Jin, and
Steven CH Hoi. 2012. Fast bounded online gradi-
ent descent algorithms for scalable kernel-based online
learning. arXiv preprint arXiv:1206.4633.

1427


