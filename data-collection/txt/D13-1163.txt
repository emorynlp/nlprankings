










































Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1563–1573,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Lexical Chain Based Cohesion Models for
Document-Level Statistical Machine Translation

Deyi Xiong1, Yang Ding2, Min Zhang1∗ and Chew Lim Tan2
1School of Computer Science and Technology, Soochow University, Suzhou, China 215006

{dyxiong, minzhang}@suda.edu.cn
2School of Computing, National University of Singapore, Singapore 117417

{a0082379, tancl}@comp.nus.edu.sg

Abstract

Lexical chains provide a representation of the
lexical cohesion structure of a text. In this pa-
per, we propose two lexical chain based co-
hesion models to incorporate lexical cohesion
into document-level statistical machine trans-
lation: 1) a count cohesion model that rewards
a hypothesis whenever a chain word occurs in
the hypothesis, 2) and a probability cohesion
model that further takes chain word transla-
tion probabilities into account. We compute
lexical chains for each source document to be
translated and generate target lexical chains
based on the computed source chains via max-
imum entropy classifiers. We then use the
generated target chains to provide constraints
for word selection in document-level machine
translation through the two proposed lexical
chain based cohesion models. We verify the
effectiveness of the two models using a hier-
archical phrase-based translation system. Ex-
periments on large-scale training data show
that they can substantially improve translation
quality in terms of BLEU and that the prob-
ability cohesion model outperforms previous
models based on lexical cohesion devices.

1 Introduction

Given a source document, traditionally most statisti-
cal machine translation (SMT) systems translate the
document sentence by sentence. In such a transla-
tion scheme, sentences are translated independent
of any other sentences. However, a text is normally
written cohesively, in which sentences are connected

∗Corresponding author

to each other via syntactic and lexical devices. This
linguistic phenomenon is called as textual cohesion
(Halliday and Hasan, 1976).

Cohesion is a surface-level property of well-
formed texts. It deals with five categories of rela-
tionships between text units, namely co-reference,
ellipsis, substitution, conjunction and lexical cohe-
sion that is realized via semantically related words.
The former four cohesion relations can be grouped
as grammatical cohesion. Generally speaking,
grammatical cohesion is less common and harder
to identify than lexical cohesion (Barzilay and El-
hadad, 1997).

As most SMT systems translate a text in a
sentence-by-sentence fashion, they tend to build less
lexical cohesion than human translators (Wong and
Kit, 2012). We therefore study lexical cohesion for
document-level translation. We use lexical chains
(Morris and Hirst, 1991) to capture lexical cohe-
sion in a text. Lexical chains are connected graphs
that represent the lexical cohesion structure of a text.
They have been successfully used for information
retrieval (Stairmand, 1996), document summariza-
tion (Barzilay and Elhadad, 1997) and so on. In this
paper, we investigate how lexical chains can be used
to incorporate lexical cohesion into document-level
translation.

Our basic assumption is that the lexical chains of
a target document are direct correspondences of the
lexical chains of its counterpart source document.
This assumption is reasonable as the target docu-
ment translation should be faithful to the source doc-
ument in terms of both text meaning and structure.
Based on this assumption, we propose a framework

1563



to incorporate lexical cohesion into target document
translation via lexical chains, which works as fol-
lows.

• Compute lexical chains for each source docu-
ment that is to be translated;

• Project the computed source lexical chains onto
the corresponding target document by translat-
ing source chain words into target chain words
using maximum entropy classifiers;

• Incorporate lexical cohesion into the target doc-
ument translation via cohesion models built on
the projected target lexical chains .

We build two lexical chain based cohesion mod-
els. The first model is a count model that rewards a
hypothesis whenever a word in the projected target
lexical chains occur in the hypothesis. As a source
chain word may be translated into many different
target words, we further extend the count model to
a second cohesion model: a probability model that
takes chain word translation probabilities into ac-
count.

We test the two lexical chain based cohesion mod-
els on a hierarchical phrase-based SMT system that
is trained with large-scale Chinese-English bilin-
gual data. Experiment results show that our lexi-
cal chain based cohesion models can achieve sub-
stantial improvements over the baseline. Further-
more, the probability cohesion model is better than
the count model and it also outperforms previous
cohesion models based on lexical cohesion devices
(Xiong et al., 2013).

To the best of our knowledge, this is the first at-
tempt to explore lexical chains for statistical ma-
chine translation. The remainder of this paper is or-
ganized as follows. Section 2 discusses related work
and highlights the differences between our method
and previous work. Section 3 briefly introduces
lexical chains and algorithms that compute lexical
chains. Section 4 elaborates the proposed lexical
chain based framework, including details on source
lexical chain computation, target lexical chain gen-
eration and the two lexical chain based cohesion
models. Section 5 presents our large-scale experi-
ments and results. Finally, we conclude with future
directions in Section 6.

2 Related Work

Recent years have witnessed growing research in-
terests in document-level statistical machine trans-
lation. Such research efforts can be roughly di-
vided into two groups: 1) general document-level
machine translation that does not explore or ex-
plores very little linguistic discourse information;
2) linguistically-motivated document-level machine
translation that incorporates discourse information
such as cohesion and coherence into SMT. Recent
studies (Guillou, 2013; Beigman Klebanov and
Flor, 2013) show that this discourse information is
very important for document-level machine transla-
tion.

General Document-Level Machine Translation
Tiedemann (2010) propose cache-based language
and translation models for document-level machine
translation. These models are built on recently trans-
lated sentences. Following this cache-based ap-
proach, Gong et al. (2011) further introduce two
additional caches. They use a static cache to store
bilingual phrases extracted from documents in train-
ing data that are similar to the document being trans-
lated. They also adopt a topic cache with target
language topic words. Xiao et al. (2011) study
the translation consistency issue in document-level
machine translation. They use a hard constraint to
consistently translate ambiguous source words into
the most frequent translation options. Ture et al.
(2012) soften this consistency constraint by integrat-
ing three counting features into decoder.

Using Lexical Cohesion Devices in Document-
Level SMT Lexical cohesion devices are seman-
tically related words, including word repetition,
synonyms/near-synonyms, hyponyms and so on.
They are also the cohesion-building elements in lex-
ical chains.

Wong and Kit (2012) use lexical cohesion device
based metrics to improve machine translation evalu-
ation at the document level. These metrics measure
the proportion of content words that are used as lex-
ical cohesion devices in machine-generated transla-
tions. Hardmeier et al. (2012) propose a document-
wide phrase-based decoder and integrate a semantic
language model into the decoder. They argue that
their semantic language model can capture lexical
cohesion by exploring n-grams that cross sentence

1564



boundaries.
Most recently Xiong et al. (2013) integrate

three categories of lexical cohesion devices into
document-level machine translation. They define
three cohesion models based on lexical cohesion de-
vices: a direct reward model, a conditional probabil-
ity model and a mutual information trigger model.
The latter two models measure the strength of lexical
cohesion relation between two lexical items. They
are incorporated into SMT to calculate how appro-
priately lexical cohesion devices are used in doc-
ument translation. As lexical chains capture lexi-
cal cohesion relations among sequences of related
words rather than those only between two words, ex-
periments in Section 5 show that our lexical chain
based probability cohesion model is better than the
lexical cohesion device based trigger model, which
is the best among the three cohesion models pro-
posed by Xiong et al. (2013).

Modeling Coherence in Document-Level SMT
In discourse analysis, cohesion is often studied to-
gether with coherence which is another dimension
of the linguistic structure of a text (Barzilay and
Elhadad, 1997). Cohesion is related to the sur-
face structure of a text while coherence is concerned
with the underlying meaning connectedness in a text
(Vasconcellos, 1989). Compared with cohesion, co-
herence is not easy to be detected. Even so, various
models have been proposed to explore coherence for
document summarization and generation (Barzilay
and Lapata, 2008; Louis and Nenkova, 2012). Fol-
lowing this line, Xiong and Zhang (2013) integrate
a topic-based coherence model into document-level
machine translation, where coherence is defined as a
continuous sentence topic transition.

Our lexical chain based cohesion models are also
related to previous work on using word and phrase
sense disambiguation for lexical choice in SMT
(Carpuat and Wu, 2007b; Carpuat and Wu, 2007a;
Chan et al., 2007). The difference is that we use
document-wide lexical chains to build our cohesion
models rather than sentence-level context features.
In our framework, lexical choice is performed to
make the selected words consistent with the lexical
cohesion structure of a document.

Carpuat (2009) explores the principle of one sense
per discourse (Gale et al., 1992) in the context of
SMT and imposes the constraint of one translation

per discourse on document translation. We also
use the one sense per discourse principle to perform
word sense disambiguation on the source side in our
lexical chaining algorithm (See Section 4.1).

3 Background: Lexical Chain and Chain
Computation

Lexical chains are sequences of semantically related
words (Morris and Hirst, 1991). They represent the
lexical cohesion structure of a text. Figure 2 displays
six lexical chains computed from the Chinese news
article shown in Figure 1. Words in these lexical
chains have lexical cohesion relations such as rep-
etition, synonym, which may range over the entire
text. For example, in the lexical chain LC1 of Fig-
ure 2, the same word “dWgu_” (Germany) repeats
9 times. In the lexical chain LC3, the two words
“z`ngcSi” (president) and “zhdx[” (chairman) are
synonym words. Generally, a text can have many
different lexical chains, each of which represents a
thread of cohesion through the text.

Several lexical chaining algorithms have been
proposed to compute lexical chains from texts. Nor-
mally they need an ontology to obtain semantic re-
lations between words. Word sense disambiguation
(WSD) is also used to determine the sense of each
word in a text. Generally a lexical chain compu-
tation algorithm completes the following three sub-
tasks:

• Building a representation of a text with a set
of candidate words and assigning semantic re-
lations between the candidate words according
to the ontology;

• Choosing the right sense for each candidate
word via WSD;

• Building chains over the semantically related
and disambiguated candidate words.

These three sub-tasks can be done separately or si-
multaneously.

Morris and Hirst (Morris and Hirst, 1991) de-
fine the first lexical chain computation algorithm
that adopts a greedy strategy to immediately disam-
biguate a word at its first occurrence. This algo-
rithm runs in linear time but suffers from inaccu-
rate disambiguation. Barzilay and Elhadad (Barzi-
lay and Elhadad, 1997) significantly improve WSD

1565



dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma c[zh[
dWgu_ diUnx]n g^ngsZ xuRnbe , qiSn jiRnsh]hu] zhdx[ qZsh[Yr su] de xZlYXr jiRng dRnrYn gRi
g^ngsZ de l[nsh[ z`ngcSi , wWiqZ lie gY yuY , zh[dUo su`ma de j]rYn rWnxuTn jiVrYn wWizh\"
( fTxZnshY b^ Sng diUn ) dWgu_ diUnx]n g^ngsZ z`ngcSi su`ma jZntiRn c[qe tR de zh[we , tR
shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx]n g^ngsZ jiRnsh]hu] de ch^ngfYn x]nrYn ,
c[zh[ sh] tR wWiyZ de xuTnzW"
t_uzZrWn huRny[ng zhYxiUng xuRnbe , dWgu_ diUnx]n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc
gdpiUo jiRoy] sh]chTng shUng zhTng bTifYnzhZsh[yZ y\shUng"
su`ma zUi dWgu_ diUnx]n g^ngsZ b^ Sng z`ngbe zhUokRi jiRnsh]hu] tYbiW hu]y] zh^ng fRbiTo
yZ xiUng shVngm[ng , tR shu^ : 7 w` y\ yUoqic jiRnsh]hu] jiXchc w` de zh[we"8
y_uyc liTng gY yuY hau jiRng jdx[ng dUxuTn , dUn liSnhWzhYngfd zUi m[n diUo zh^ng shVngwUng
luahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh] xZn dZ sh[ , dWgu_ diUnx]n
g^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch[ tR"
dWgu_ diUnx]n gdjiU haulSi hu[ wXn , y\ sh[yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTng
bTifYnzhZbRdiTnwds]"
dWgu_ cSizhYngbe huRny[ng su`ma c[zh[ de juWd]ng"

Figure 1: An example of a Chinese news article (written in pinyin).

LC1: {dWgu_, dWgu_, b^, dWgu_, dWgu_, dWgu_,
dWgu_, b^, dWgu_, dWgu_, dWgu_}
LC2:{jiRnsh]hu], fTxZnshY, jiRnsh]hu], z`ngbe,
jiRnsh]hu], jiRnsh]hu]}
LC3: {z`ngcSi, zhdx[, z`ngcSi, z`ngl\}
LC4: {c[zh[, c[qe, c[zh[, c[zh[}
LC5: {zhTng, xiUcua, shUngzhTng}
LC6: {xuRnbe, xuRnbe, fRbiTo}

Figure 2: Six lexical chains from the example in Figure
1.

accuracy by processing all possible combinations of
word senses in a text to disambiguate words. Un-
fortunately, their algorithm runs slowly in quadratic
time. Galley and Mckeown (2003) present an algo-
rithm that are better than the former two algorithms
both in terms of running efficiency and WSD accu-
racy. They separate the WSD sub-task from the task
of lexical chain building and impose a “one sense
per discourse” constraint in the WSD step.

4 Translating Documents Using Lexical
Chains

In this section, we describe how we incorporate lex-
ical cohesion into document-level machine transla-
tion using lexical chains. We divide the lexical chain
based document-level machine translation process

into three steps: (1) computing lexical chains for
source documents with a source language ontology,
(2) generating target lexical chains from the com-
puted source lexical chains, and finally (3) incorpo-
rating lexical cohesion encoded in the generated tar-
get lexical chains into document-level translation via
lexical chain based cohesion models. The remainder
of this section will elaborate these three steps.

4.1 Source Lexical Chains Computation

We follow the chain computation algorithm intro-
duced by Galley and McKeown (2003) to build lex-
ical chains on source (Chinese) documents. In the
algorithm, the chaining process includes three steps:
choosing candidate words to build a disambiguation
graph (Galley and McKeown, 2003) for each doc-
ument, disambiguating the candidate words and fi-
nally building lexical chains over the disambiguated
candidate words.

The disambiguation graph can be considered as
a representation of all possible interpretations of its
corresponding text. In the graph, nodes are candi-
date words with different senses and edges between
word senses are weighted according to their seman-
tic relations, such as synonym, hypernym and so on.
We use an extended version of a Chinese thesaurus
Tongyici Cilin (Cilin for short) to define word senses
and semantic relations between senses. The ex-

1566



level 1

level 2

level 3

level 4

level 5

Figure 3: The architecture of the extended Cilin. For sim-
plicity, we only draw a binary tree to represent the hier-
archical structure of Cilin. This doesn’t mean that each
semantic class at level i has only two sub-classes at level
i+ 1. Actually, they have multiple sub-classes.

tended Cilin contains 77,343 Chinese words, which
are organized in a hierarchical structure containing
5 levels as shown in Figure 3. In the 5th level, each
node represents an atomic concept which consists of
a set of synonyms. These atomic concepts are just
like synsets in WordNet. We use them to represent
senses of words in the disambiguation graph.

We select nouns, verbs, abbreviations and idioms
as candidate words for the disambiguation graph.
These words are identified by a Chinese part-to-
speech tagger LTP (Che et al., 2010) in a preprocess-
ing step. In order to build the disambiguation graph,
we first build an array indexed by the atomic con-
cepts of Cilin, then insert a copy of each candidate
word into its all concept (sense) entries in the array.
After that, we create all semantic links among senses
of different candidate words in the disambiguation
graph following Galley and McKeown (2003).

In the second step, we use the principle of one
sense per discourse to perform WSD for each can-
didate word in the disambiguation graph. We sum
the weights of all semantic links under the different
senses of the candidate word in question. The sense
with the highest sum of weights is considered as the
most probable sense for this word. We then assign
this sense to all occurrences of the word in the doc-
ument by adopting the constraint of one sense per
discourse.

Once all candidate words are disambiguated, we
can build lexical chains over these words by remov-
ing all semantic links that connect those unselected

word senses. The six lexical chains shown in Fig-
ure 2 are computed from the Chinese document in
Figure 1 exactly following the algorithm of Galley
and McKeown (2003). The only difference is that
we use Cilin rather than WordNet as the ontology.

4.2 Target Lexical Chains Generation
Since a faithful target document translation should
follow the same cohesion structure as that in its cor-
responding source document, we generate target lex-
ical chains from the computed source lexical chains.
Given a source lexical chain LCs = {sji} where the
ith chain word sji is from the jth sentence of the
source document Ds, we generate a target lexical
chain LCt = {tji} using maximum entropy (Max-
Ent) classifiers. Particularly, we translate a word sji
in the source lexical chain into a target word tji in
the target lexical chain using a corresponding Max-
Ent classifier as follows1.

P (tji |C(s
j
i )) =

exp(
∑

k θkfk(t
j
i , C(s

j
i )))∑

t exp(
∑

k θkfk(t, C(s
j
i )))

(1)

where fk are binary features, θk are weights of these
features, and C(sji ) is the surrounding context of
chain word sji .

We train one MaxEnt classifier per unique source
chain word. For each classifier, we define two
groups of binary features: 1) the preceding and
succeeding two words of sji in the jth sentence
({w−2, w−1, sji , w+1, w+2}); 2) the preceding and
succeeding one word of sji in the lexical chain LCs
({spi−1, s

j
i , s

q
i+1}). All features are in the following

binary form.

f(tji , C(s
j
i )) =

{
1, if tji = ♣ and C(s

j
i ).♥ = ♠

0, else
(2)

where the symbol ♣ is a placeholder for a possible
target word, the symbol♥ indicates a contextual ele-
ment for the chain word sji (e.g., the preceding word
in the jth sentence or the succeeding word in the
lexical chain LCs), and the symbol ♠ represents the
value of ♥.

Given a source document Ds and its N lexical
chains {LCks }Nk=1 computed from the document as

1We collect training instances from word-aligned bilingual
data to train the MaxEnt classifier.

1567



described in Section 4.1, we can generate the N
target lexical chains {LCkt }Nk=1 using our MaxEnt
classifiers. Each target word tji in the target lexi-
cal chain LCkt is the translation of its corresponding
source word sji in the source lexical chain LC

k
s with

the highest probability P (tji |C(s
j
i )) according to Eq.

(1).
As we know, the MaxEnt classifier can gen-

erate multiple translations for each source word.
In order to incorporate these multiple chain word
translations, we can generate a super target lexi-
cal chain �LCt from a source lexical chain LCs,
where � is a pre-defined threshold used to se-
lect multiple translations. For example, given a
source lexical chain LCs = {a, b, c}, we can
have the corresponding super target lexical chain
�LCt = {{a1t , a2t ...}, {b1t , b2t ...}, {c1t , c2t ...}}, where
xit is the translation of x with a translation probabil-
ity P (xit|C(x)) ≥ � according to Eq. (1). Integrat-
ing multiple translations for each source chain word,
we can reduce the error propagation of the MaxEnt
classifier to some extent. Our experiments also con-
firm that the super target lexical chains with multi-
ple translation options for each chain word are better
than the target lexical chains with only one transla-
tion per chain word. Therefore we build our cohe-
sion models based on the super target lexical chains,
which will be described in the next section.

4.3 Lexical Chain Based Cohesion Models

Once we generate the super target lexical chains
{�LCkt }Nk=1 for the target document Dt, we can use
them to provide constraints for the target document
translation. Our key interest is to make the target
document translation TDt as cohesive as possible.
We therefore propose lexical chain based cohesion
models to measure the cohesion of the target docu-
ment translation. The basic idea is to reward a trans-
lation hypothesis if a word from the super target lexi-
cal chains occurs in the hypothesis. According to the
difference in the reward strategy, we have two cohe-
sion models: a count cohesion model and a proba-
bility cohesion model.

Count Cohesion Model Mc(TDt , {�LCkt }Nk=1):
This model rewards a translation hypothesis of the
jth sentence in the document whenever a lexical
chain word tji occurs in the hypothesis. The model

maintains a counter and accumulates the counter
when necessary. It is factorized into the sentence
cohesion metric Mc(Tj , {�LCkt }Nk=1), where Tj is
the translation of the jth sentence in the target docu-
ment. Mc(Tj , {�LCkt }Nk=1) is formulated as follows.

Mc(Tj , {�LCkt }Nk=1) =
∏
w∈Tj

∏
tji∈C

eδ(w,t
j
i ) (3)

where C represents {�LCkt }Nk=1, and the δ function
is defined as follows.

δ(w, tji ) =

{
1, if tji = w
0, otherwise

(4)

Probability Cohesion Model
Mp(TDt , {�LCkt }Nk=1): This model rewards a
translation hypothesis according to the translation
probability of a chain word that occurs in the
hypothesis. The translation probability is computed
by Eq. (1). The model is also factorized into the
sentence cohesion metric Mp(Tj , {�LCkt }Nk=1)
which is formulated as follows.

Mp(Tj ,{�LCkt }Mk=1) =∏
w∈Tj

∏
tji∈C

eδ(w,t
j
i ) × P (tji |C(s

j
i ))

(5)

where P (tji |C(s
j
i ) is the translation probability com-

puted according to Eq. (1).

4.4 Decoding
The proposed lexical chain based cohesion models
are integrated into the log-linear translation frame-
work of SMT as a cohesion feature. Before translat-
ing a source document, we compute lexical chains
for the source document as described in Section 4.1.
We then generate the super target lexical chains. In
order to efficiently calculate our lexical chain based
cohesion models, we reorganize words in the super
target lexical chains into vectors. We associate each
source sentence Sj a vector to store target lexical
chain words that are to occur in the corresponding
target sentence Tj .

Although we still translate a source document
sentence by sentence, we capture the global cohe-
sion structure of the document via lexical chains and
use the lexical chain based cohesion models to con-
strain word selection in document translation. Fig-
ure 4 shows the architecture of an SMT system with
the lexical chain based cohesion model.

1568



Figure 4: Architecture of an SMT system with the lexical
chain based cohesion model.

5 Experiments

In this section, we conducted a series of experiments
to validate the effectiveness of the proposed lexical
chain based cohesion models for Chinese-to-English
document-level machine translation. We used a hier-
archical phrased-based SMT system (Chiang, 2007)
trained on large-scale data. In particular, we aim at:

• Measuring the impact of the threshold � on the
probability cohesion model and selecting the
best threshold on a development test set.

• Investigating the effect of the two lexical-chain
based cohesion models.

• Comparing our lexical chain based cohesion
models against the previous lexical cohesion
device based models (Xiong et al., 2013).

5.1 Setup

We collected our bilingual training data from
LDC, which includes the corpus LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004E12,
LDC2004T07, LDC2004T08 (Only Hong Kong
News), LDC2005T06 and LDC2005T10. The
collected bilingual training data contains 3.8M
sentence pairs with 96.9M Chinese words and
109.5M English words. We trained a 4-gram
language model on the Xinhua portion of the
English Gigaword corpus (306 million words) via
the SRILM toolkit (Stolcke, 2002) with Kneser-Ney
smoothing.

Training MT05 MT06 MT08
#Doc 103,236 100 79 109
#Sent 2.80M 1,082 1,664 1,357
#Chain 3.52M 1700 2172 1693
#AvgC 35.72 17 27.49 15.53
#AvgW 14.81 5.89 6.89 5.63

Table 1: Statistics of the training, development and test
sets, which show the number of documents (#Doc) and
sentences (#Sent), the number of lexical chains extracted
from the source documents (#Chain), the average number
of lexical chains per document (#AvgC) and the average
number of words per lexical chain (#AvgW).

In order to build the lexical chain based cohesion
models, we selected corpora with document bound-
aries explicitly provided from the bilingual training
data together with the whole Hong Kong parallel
text corpus as the cohesion model training data2. We
show the statistics of these selected corpora in Table
1. They contain 103,236 documents and 2.80M sen-
tences. Averagely, each document consists of 28.4
sentences. From the source documents of the se-
lected corpora, we extract 3.52M lexical chains. On
average, there are 35.72 lexical chains per document
and 14.81 words per lexical chain.

We used the off-the-shelf MaxEnt toolkit3 to train
one MaxEnt classifier per unique source lexical
chain word (61,121 different source chain words in
total). We performed 100 iterations of the L-BFGS
algorithm implemented in the training toolkit for
each chain word with both Gaussian prior and event
cutoff set to 1 to avoid overfitting. After event cutoff,
we have an average of 17.75 different classes (target
translations) per source chain word.

We used the NIST MT05 as the tuning set for the
minimum error rate training (MERT) [Och, 2003],
the NIST MT06 as the development test set and the
MT08 as the final test set. The numbers of doc-
uments/sentences in the NIST MT05, MT06 and
MT08 are 100/1082, 79/1664 and 109/1357 respec-
tively. They contain 17, 27.49 and 15.53 lexical
chains per document respectively.

We used the case-insensitive BLEU-4 (Papineni
2The training data includes LDC2003E14, LDC2004T07,

LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong
Hansards/Laws/News).

3Available at: http://homepages.inf.ed.ac.uk/lzhang10/
maxent toolkit.html

1569



� MT06
0.05 30.53
0.1 31.64
0.2 31.45
0.3 30.73
0.4 31.01

Table 2: BLEU scores of the probability cohesion
model Mp(TDt , {�LCkt }Nk=1) with different values for
the threshold �.

et al., 2002) as our evaluation metric. As MERT is
normally instable, we ran the tuning process three
times for all our experiments and presented the av-
erage BLEU scores on the three MERT runs as sug-
gested by Clark et al (2011).

5.2 Setting the Threshold �

As the two lexical chain based cohesion models are
built on the super target lexical chains that are asso-
ciated with a parameter �, we need to tune the thresh-
old parameter � on the development test set NIST
MT06. We conducted a group of experiments using
the probability cohesion model defined in Eq. (5)
to find the best threshold. Experiment results are
shown in Table 2.

If we set the threshold too small (e.g., 0.05), the
super target lexical chains may contain too many
noisy words that are not the translations of source
lexical chain words, which may jeopardise the qual-
ity of the super target lexical chains. The cohesion
model built on these noisy super target lexical chains
may select incorrect words rather than the proper
lexical chain words. On the other hand, if we set the
threshold too large (e.g., 0.3 or 0.4), we may take
the risk of not selecting the appropriate chain word
translations into the super target lexical chains. It
seems that the best threshold is 0.1 as we obtained
the highest BLEU score 31.64 on the NIST MT06
with this threshold. Therefore we set the threshold �
to 0.1 in all experiments thereafter.

5.3 Effect of the Count and Probability
Cohesion Model

After we found the best threshold, we carried out ex-
periments to test the effect of the two lexical chain
based cohesion models: the count and probability
cohesion model. We compared them against the

System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexChainCount(top 1) 30.46 23.52 26.99
LexChainCount 30.79 23.34 27.07
LexChainProb 31.64 24.54 28.09

Table 3: Effects of the lexical chain based count and
probability cohesion models. LexChainCount: the count
model defined in Eq. (3). LexChainProb: the probability
model defined in Eq. (5).

baseline system that does not integrate any lexical
chain information. We also compared the count co-
hesion model (LexChainCount(top1)) built on the
target lexical chains where each target chain word is
the best translation of its corresponding source lex-
ical chain word according to Eq. (1). Experiment
results are shown in Table 3.

From Table 3, we can observe that

• Our lexical chain based cohesion models are
able to substantially improve the translation
quality in terms of BLEU score. We achieve
an average improvement of up to 1.21 BLEU
points over the baseline on the two test sets
MT06 and MT08.

• The count cohesion model built on the super
target lexical chains is better than that based
on the target lexical chains only with top one
translations (27.07 vs. 26.99). This shows
the advantage of the super target lexical chains
{�LCkt }Nk=1 over the standard target lexical chi-
ans {LCkt }Nk=1.

• Finally, the probability cohesion model is much
better than the count cohesion model (28.09
vs. 27.07). This suggests that we should take
into account chain word translation probabili-
ties when we reward hypotheses where target
lexical chain words occur.

5.4 Lexical Chains vs. Lexical Cohesion
Devices

As we have mentioned in Section 2, lexical cohe-
sion devices can be also used to build lexical cohe-
sion models to capture lexical cohesion relations in a
text. We therefore want to compare our lexical chain
based cohesion models with the lexical cohesion de-
vice based cohesion models.

1570



System MT06 MT08 Avg
Baseline 30.43 23.32 26.88
LexDeviceTrigger 31.35 24.11 27.73
LexChainProb 31.64 24.54 28.09

Table 4: The lexical chain based probability cohesion
model (LexChainProb) vs. the lexical cohesion device
based trigger model (LexDeviceTrigger).

We re-implemented the mutual information trig-
ger model that is the best lexical cohesion model
based on lexical cohesion devices among the three
models proposed by Xiong et al. (2013). The mu-
tual information trigger model measures the associ-
ation strength of two lexical cohesion items x and y
in a lexical cohesion relation xRy. In the model, it
is required that x occurs in a sentence preceding the
sentence where y occurs and that the two items have
a lexical cohesion relation such as word repetition,
synonym. The model treats x as the trigger and y as
the triggered item. The mutual information between
the trigger x and the triggered item y estimates how
possible y will occur given x is mentioned in a text.

The comparison results are reported in Table 4.
Our lexical chain based probability cohesion model
outperforms the lexical cohesion device based trig-
ger model by 0.36 BLEU points. The reason for this
superiority of our cohesion model over the trigger
model may be that the former model captures lex-
ical cohesion relations among sequences of words
through lexical chains while the latter model cap-
tures lexical cohesion relations only between two re-
lated words.

6 Conclusions

We have presented two lexical chain based cohesion
models that incorporate the lexical cohesion struc-
ture of a text into document-level machine transla-
tion. We project the lexical chains of a source docu-
ment to the corresponding target document by trans-
lating each word in each source lexical chain into
their counterparts via MaxEnt classifiers. The pro-
jected target lexical chains provide a representation
of the lexical cohesion structure of the target doc-
ument that is to be generated. We build two co-
hesion models based on the projected target lexi-
cal chains: a count model that rewards a hypothesis
according to the time of occurrence of target lexi-

cal chain words in the hypothesis and a probability
model that further takes translation probabilities into
account when rewarding hypotheses. These two co-
hesion models are used to constrain word selection
for document translation so that the generated doc-
ument is consistent with the projected lexical cohe-
sion structure.

We have integrated the two proposed cohesion
models into a hierarchical phrase-based SMT sys-
tem. Experiment results on large-scale data validate
that

• The lexical chain based cohesion models are
able to substantially improve translation qual-
ity in terms of BLEU.

• The probability cohesion model is better than
the count cohesion model.

• The lexical chain based probability cohesion
model is better than the previous mutual infor-
mation trigger model that adopts lexical cohe-
sion devices to capture lexical cohesion rela-
tions between two related words.

As we mentioned in Section 2, cohesion is closely
connected to coherence. It provides a surface indi-
cator for coherence identification (Barzilay and El-
hadad, 1997). In the future, we would like to use
lexical chains to identify coherence and incorporate
both cohesion and coherence into document-level
machine translation.

References
Regina Barzilay and Michael Elhadad. 1997. Using lex-

ical chains for text summarization. In In Proceedings
of the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10–17.

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.

Beata Beigman Klebanov and Michael Flor. 2013. As-
sociative texture is lost in translation. In Proceedings
of the Workshop on Discourse in Machine Translation,
pages 27–32, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In Pro-
ceedings of the 11th Conference on Theoretical and

1571



Methodological Issues in Machine Translation, pages
43–52.

Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 61–72, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.

Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19–27, Boulder, Colorado, June.
Association for Computational Linguistics.

Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33–40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
a chinese language technology platform. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, COLING ’10,
pages 13–16, Stroudsburg, PA, USA. Association for
Computational Linguistics.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176–181, Port-
land, Oregon, USA, June.

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, Harriman, NY, February.

Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
Proceedings of the 18th international joint conference
on Artificial intelligence, IJCAI’03, pages 1486–1488,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.

Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 909–919, Edinburgh, Scotland, UK., July.

Liane Guillou. 2013. Analysing lexical consistency in
translation. In Proceedings of the Workshop on Dis-
course in Machine Translation, pages 10–18, Sofia,

Bulgaria, August. Association for Computational Lin-
guistics.

M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion in
English. London: Longman.

Christian Hardmeier, Joakim Nivre, and Jörg Tiedemann.
2012. Document-wide decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1179–1190, Jeju Island,
Korea, July.

Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157–1168, Jeju
Island, Korea, July. Association for Computational
Linguistics.

Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator of
the structure of text. Comput. Linguist., 17(1):21–48,
March.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July.

M.A. Stairmand. 1996. A computational analysis of
lexical cohesion with applications in information re-
trieval. UMIST.

Andreas Stolcke. 2002. Srilm–an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, Colorado, USA, September.

Jörg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 8–15, Uppsala, Sweden, July.

Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012.
Encouraging consistent translation choices. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 417–
426, Montréal, Canada, June.

Muriel Vasconcellos. 1989. Cohesion and coherence in
the presentation of machine translation products. In
James E.Alatis, editor, Geogetown University Round
Table on Languages and Linguistics 1989, pages 89–
105, Washington, D.C. Georgetown University Press.

Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In Proceedings of the

1572



2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1060–1068, Jeju Island,
Korea, July.

Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Proceedings of the 2011 MT sum-
mit XIII, pages 131–138, Xiamen, China, September.

Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
Proceedings of the Twenty-Seventh AAAI Conference
on Artificial Intelligence (AAAI-13), Bellevue, Wash-
ington, USA, July.

Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third International Joint Conference on
Artificial Intelligence (IJCAI-13), Beijing, China, Au-
gust.

1573


