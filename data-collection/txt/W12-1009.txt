










































Linguistically-Adapted Structural Query Annotation for Digital Libraries in the Social Sciences


Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55–64,
Avignon, France, 24 April 2012. c©2012 Association for Computational Linguistics

Linguistically-Adapted Structural Query Annotation for Digital Li-
braries in the Social Sciences 

 Carol ine Brun Vassilina Nikoulina Nikolaos Lagos 

Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan France 

{firstname.lastname}@xrce.xerox.com 
 

Abstrac t 

Query processing is an essential part of a  
range of applications in the social sciences 
and cultural heritage domain. However, out-
of-the-box natural language processing tools 
originally developed for full phrase analysis 
are inappropriate for query analysis. In this  
paper, we propose an approach to solving 
this problem by adapting a complete and in-
tegrated chain of NLP tools, to make it  suit-
able for queries analysis. Using as a case 
study the automatic translation of queries 
posed to the Europeana library, we demon-
strate that adapted linguistic processing can 
lead to improvements in translation quality. 

1 Introduction 

Query processing tools are essential components 
of digital libraries and content aggregators. Their 
operation varies from simple stop word removal 
and stemming to advanced parsing, that treats 
queries as a collection of phrases rather than sin-
gle terms (Mothe and Tanguy, 2007). They are 
used in a range of applications, from information 
retrieval (via search engines that provide access 
to the digital collections) to query analysis.  

Current query processing solutions tend to 
use out-of-the-box Natural Language Processing 
(NLP) tools that were originally developed for 
full phrase analysis, being inappropriate for 
query analysis. 

Correct query annotation and interpretation is 
even more important in the cultural heritage or 
social sciences domain, as a lot of the content 
can be in multimedia form and only metadata 
(most of the times in the form of tags) is exploit-
able by traditional text-oriented information re-
trieval and analysis techniques. 

Furthermore, as recent studies of user query-
ing behavior mention, queries in these domains 
are not only very short but are also quite specific 

in terms of content: they refer to artist names, 
titles, dates, and objects (Koolen and Kamps, 
2010; Ireson and Oomen, 2007). Take the exam-
ple of a query like “coupe apollon” (“bowl apol-
lon”). While in standard analysis “coupe” would 
be identified as a verb (“couper”, i.e. “to cut”), in 
the context of a query it should be actually 
tagged as a noun, which refers to an object. Such 
a difference may lead to different preprocessing 
and worse retrieval. 

In this paper, we propose an approach to solv-
ing this problem by adapting a complete and in-
tegrated chain of NLP tools, based on the Xerox 
Incremental Parser (XIP), to make it suitable for 
queries’ analysis. The adaptation includes recapi-
talization, adapted Part of Speech (PoS) tagging, 
adapted chunking and Named Entities (NE) rec-
ognition. We claim that several heuristics espe-
cially important for queries’ analysis, such as 
favoring nominal interpretations, result in im-
proved linguistic structures, which can have an 
impact in a wide range of further applications 
(e.g. information retrieval, query translation, in-
formation extraction, query reformulation etc.). 

2 Prior art 

The problem of adapted query processing, often 
referred to as structural query annotation, in-
cludes capitalization, NEs detection, PoS tagging 
and query segmentation. Most of the existing 
works treat each of these steps independently and 
address only one of the above issues. 

Many works address the problem of query 
segmentation. According to Tan and Peng 
(2008), query segmentation is a problem which is 
close to the chunking problem, but the chunking 
problem is directly linked to the PoS tagging re-
sults, which are often noisy for the queries. Thus, 
most of the works on query segmentation are 
based on the statistical interaction between a pair 
of query words to identify the border between the 
segments in the query (Jones et al., 2006; Guo et 

55



al., 2008). Tan and Peng (2008) propose a gen-
erative language model enriched with Wikipedia 
to identify “concepts” rather than simply “fre-
quency-based” patterns. The segmentation pro-
posed by Bergsma and Wang (2007) is closer to 
the notion of NP chunking. They propose a ma-
chine-learned query segmentation system trained 
on manually annotated set of 500 AOL queries. 
However, in this work PoS tagging is used as one 
of the features in query segmentation and is done 
with a generic PoS tagger, non adapted for que-
ries. 

PoS tagging is an important part of query 
processing and used in many information ana-
lytics tasks (query reformulation, query segmen-
tation, etc.). However very few works address 
query-oriented PoS tagging. Allan and Raghavan 
(2002) consider that PoS tagging might be am-
biguous for short queries and propose to interact 
with the user for disambiguation. Barr et al. 
(2008) produce a set of manually annotated que-
ries, and then train a Brill tagger on this set in 
order to create an adapted PoS tagger for search 
queries. 

A notable work is the one by Bendersky et al. 
(2010), which addresses the capitalization, PoS 
tagging and query segmentation in the same pa-
per. However, this approach proposes for each of 
the above steps a probabilistic model that relies 
on the document corpus rather on the query it-
self. Such an approach is not applicable for most 
digital content providers who would reluctantly 
give access to their document collection. More-
over, the query expansion, which is the central 
idea of the described approach, is not possible for 
most of digital libraries that are organized in a 
database. Secondly, Bendersky et al. (2010) pro-
poses adapting each processing step independ-
ently. Although this is not mentioned in the 
paper, these three steps can be applied in a se-
quence, where PoS tagging can profit from the 
recapitalization, and chunking from the PoS tag-
ging step. However, once the recapitalization is 
done, it can not be changed in the following 
steps. This work doesn’t address the adaptation 
of the NE recognition component, as we do, and 
which might change the final chunking and PoS 
tagging in certain cases. 

In our approach, part of the recapitalization is 
done during the PoS tagging, in interaction with 
the NE recognition, which allows us to consider 
these two steps as interleaved. Moreover, the 
linguistic processing we propose is generic: cor-
pus-independent (at least most of its parts except 

for NE recognition) and doesn’t require access to 
the document collection. 

3 Data 

This work is based on search logs from Euro-
peana 1

4 Motivation 

. These are real users’ queries, where 
Named Entities are often lowercased and the 
structures are very different from normal phrase 
structure. Thus, this data is well adapted to dem-
onstrate the impact of adapted linguistic process-
ing. 

We show the importance of the adapted linguistic 
query processing using as example the task of 
query translation, a real need for today’s digital 
content providers operating in a multilingual en-
vironment. We took a sample of Europeana que-
ries and translated them with different MT 
systems: in-house (purely statistical) or available 
online (rule-based). Some examples of problem-
atic translations are shown in the Table 1. 

 
 Input query Automatic 

Translation 
Human 
translation 

 French-English 
1 
 

journal pano-
rama paris  

newspaper 
panorama bets 

newspaper 
panorama 
paris   

2 saint jean de 
luz 

saint jean of 
luz 

saint jean 
de luz 

3 vie et mort 
de l’image 

life and died of 
the image 

life and 
death of 
image 

4 langue et 
réalité 

and the reality 
of language 

language 
and reality 

 English-French 
5 maps europe trace l’Europe cartes de 

l’Europe 
6 17th century 

saw 
Du 17ème 
siècle a vu 

scie du 
17ème 
siècle 

7 chopin 
george sand 

george sable 
chopin soit 

chopin 
george 
sand 

Table 1: Examples of the problematic query 
translations 

 

                                                                 
1 A portal that acts as an interface to millions of digitized 
records, allowing users to explore Europe’s cultural heri-
tage. For more information please visit 
http://www.europeana.eu/portal/ 

56



Although in general, the errors done by statis-
tical and rule-based models are pretty different, 
there are some common errors done in the case 
of the query translation. Both models, being de-
signed for full-sentence translation, find the 
query structure very unnatural and tend to repro-
duce the full sentence in the output (ex. 1, 3, 4, 5, 
6). The errors may come either from a wrong 
PoS tagging (for rule-based systems), or from the 
wrong word order (statistical-based systems), or 
from the choice of the wrong translation (both 
types of systems). 

One might think that the word order problem 
is not crucial for queries, because most of the IR 
models use the bag of words models, which ig-
nore the order of words. However, it might mat-
ter in some cases: for example, if and/or are 
interpreted as a logical operator, it is important to 
place them correctly in the sentence (examples3, 
4).  

Errors also may happen when translating NEs 
(ex.1, 2, 7). The case information, which is often 
missing in the real-life queries, helps to deal with 
the NEs translation. 

The examples mentioned above illustrate that 
adapted query processing is important for a task 
such as query translation, both in the case of 
rule-based and empirical models. Although the 
empirical models can be adapted if an appropri-
ately sized corpus exists, such a corpus is not 
always available.  

Thus we propose adapting the linguistic proc-
essing prior to query translation (which is further 
integrated in the SMT model). We demonstrate 
the feasibility and impact of our approach based 
on the difference in translation quality but the 
adaptations can be useful in a number of other 
tasks involving query processing (e.g. question 
answering, query logs analysis, etc.). 

5 Linguistic Processing Adaptation 

As said before, queries have specific linguistic 
properties that make their analysis difficult for 
standard NLP tools. This section describes the 
approach we have designed to improve query 
chunking. Following a study of the corpus of 
query logs, we rely on the specific linguistic 
properties of the queries to adapt different steps 
of linguistic analysis, from preprocessing to 
chunking.  

These adaptations consist in the following 
very general processes, for both English and 
French: 

Recapitalization: we recapitalize, in a preproc-
essing step, some uncapitalized words in queries 
that can be proper nouns when they start with a 
capital letter. 
Part of Speech disambiguation
• the part of speech tagging favors nominal 

interpretation (whereas standard part of 
speech taggers are designed to find a verb in 
the input, as PoS tagging generally applies 
on complete sentences); 

:  

• the recapitalization information transmitted 
from the previous step is used to change the 
PoS interpretation in some contexts. 

Chunking
• considering that a full NE is a chunk, which 

is not the case in standard text processing, 
where a NE can perfectly be just a part of a 
chunk; 

:  the chunking is improved by: 

• grouping coordinated NEs of the same type; 

• performing PP and AP attachment with the 
closest antecedent that is morphologically 
compatible 

These processes are very general and may ap-
ply to queries in different application domains, 
with maybe some domain-dependent adaptations 
(for example, NEs may change across domains). 

These adaptations have been implemented 
within the XIP engine, for the French and Eng-
lish grammars. The XIP framework allows inte-
grating the adaptations of different steps of query 
processing into a unified framework, where the 
changes from one step can influence the result of 
the next step: the information performed at a 
given step is transmitted to the next step by XIP 
through linguistic features. 

5.1 Preprocessing 
Queries are often written with misspelling errors, 
in particular for accents and capital letters of 
NEs. See the following query examples extracted 
from our query log corpus: 
 

lafont Robert (French query) 
henry de forge et jean mauclère 
(French query) 
muse prado madrid (French query) 
carpaccio queen cornaro (English 
query) 
man ray (English query) 
 

This might be quite a problem for linguistic 
treatments, like PoS tagging and of course NE 

57



recognition, which often use capital letter infor-
mation as a triggering feature.  

Recapitalizing these words at the preprocess-
ing step of a linguistic analysis, i.e. during the 
morphological analysis, is technically relatively 
easy, however it would be an important generator 
of spurious ambiguities in the context of full sen-
tence parsing (standard context of linguistic pars-
ing). Indeed, considering that all lower case 
words that can be proper nouns with a capital 
letter should also have capitalized interpretation, 
such as price, jean, read, us, bush, lay, etc., in 
English or pierre, médecin, … in French) would 
be problematic for a PoS tagger as well as for a 
NE recognizer. That’s why it is not performed in 
a standard analysis context, considering also that 
misspelling errors are not frequent in “standard” 
texts. In the case of queries however, they are 
frequent, and since queries are far shorter in av-
erage than full sentences the tagging can be 
adapted to this context (see next section), we can 
afford to perform recapitalization using the fol-
lowing methodology, combining lexical informa-
tion and contextual rules: 

1. The preprocessing lexicon integrates all 
words starting with a lower case letter 
which can be first name (henry, jean, 
isaac …), family and celebrity name 
(chirac, picasso...) and  place names 
(paris, saint pétersbourg, …) when capi-
talized. 

2. When an unknown word starting with a 
lower case letter is preceded by a first 
name and eventually by a particle (de, 
van, von …), it is analyzed as a last 
name, in order to be able to trigger stan-
dard NE recognition. This is one exam-
ple of interleaving of the processes: here 
part-of-speech interpretation is condi-
tioned by the recapitalization steps which 
transmits information about recapitaliza-
tion (via features within XIP) that trig-
gers query-specific pos disambiguation 
rules. 

The recapitalization (1) has been imple-
mented within the preprocessing components of 
XIP within finite state transducers (see (Kart-
tunen, 2000)). The second point (2) is done di-
rectly within XIP in the part-of-speech tagging 
process, with a contextual rule.  For example, the 
analysis of the input query “jean mauclère” gets 
the following structure and dependency output 
with the standard French grammar. 

 

Query: jean mauclère 
NMOD(jean, mauclère) 
0>GROUP[NP[jean] AP[mauclère]] 

 
Because jean is a common noun and mauclère 

is an unknown word which has been guessed as 
an adjective by the lexical guesser. 

It gets the following analysis with the pre-
processing adaptations described above: 
 

NMOD(jean,mauclère) 
PERSON_HUM(jean mauclère) 
FIRSTNAME(jean,jean mauclère) 
LASTNAME(mauclère,jean mauclère) 
0>GROUP[NP[NOUN[jean mauclère]]] 

 
Because jean has been recognized as a first 

name and consequently the unknown word after 
has been inferred has a proper noun (last name) 
by the pos tagging contextual rule; the recapitali-
zation process and part-of-speech interpretation 
are therefore interleaved. 

5.2 Part of speech disambiguation 
In the context of query analysis, part-of-speech 
tagging has to be adapted also, since standard 
part-of-speech disambiguation strategies aim 
generally at disambiguating in the context of full 
sentences. But queries are very different from 
full sentences: they are mostly nominal with 
sometimes infinitive, past participial, or gerun-
dive insertions, e.g.: 

 
statuettes hommes jouant avec un 
chien (French query) 
coupe apollon (French query) 
architecture musique (French 
query) 
statue haut relief grecque du 5 
siecle (French query)  
david playing harp fpr saul (Eng-
lish query) 
stained glass angel (English 
query) 

 
Standard techniques for part-of-speech tag-

ging include rule based methods and statistical 
methods, mainly based on hidden Markov mod-
els (see for example (Chanod and Tapanainen, 
1995)). In this case, it would be possible to re-
compute the probabilities on a corpus of queries 
manually annotated. However, the correction of 
part-of-speech tags in the context of queries is 
easy to develop with a small set of rules. We fo-
cus on English and French, and in queries, the 
main problems come from the ambiguity be-

58



tween noun and verbs, which has to be solved 
differently than in the context of a standard sen-
tence.   

The approach we adopt to correct the tagging 
with the main following contextual rules: 
• If there is a noun/verb ambiguity: 

• If the ambiguity is on the first word of 
the query (e.g. “coupe apollon”, “oil

• If the ambiguity is on the second word of 
the query, prefer the noun interpretation 
if the query starts with an adjective or a 
noun (e.g. in “young 

 
flask”), select the noun interpretation; 

people

• Select noun interpretation if there is no 
person agreement with one of the previ-
ous nouns (e.g. “les frères 

 social com-
petences”, select the noun interpretation 
for people, instead of verb) 

bissons

• For a verb which is neither at the past 
participle form nor the infinitive form, 
select the noun interpretation if it is not 
followed by a determiner (e.g. “tremble-
ment 

”, 
frères belongs to the 3rd person but bis-
sons to the 1st one of the verb “bisser”) 

terre

• Choose the noun interpretation if the 
word is followed by a conjunction and a 
noun or preceded by a noun and a con-
junction (e.g. in “gauguin 

 lisbonne”, terre is disambigu-
ated as a noun”)) 

moon and 
earth”, choose the noun interpretation 
for moon, instead of verb2

• In case of ambiguity between adjective and 
past participle verb, select the adjective in-
terpretation if the word is followed by a 
noun (e.g.  “stained glass angel”, stained is 
disambiguated as an adjective instead of a 
past participle verb) 

). 

5.3 Chunking 
The goal of chunking is to assign a partial 

structure to a sentence and focuses on easy to 
parse pieces in order to avoid ambiguity and re-
cursion. In the very specific context of query 
analysis, and once again since queries have spe-
cific linguistic properties (they are not sentences 
but mostly nominal sequences), chunking can be 
improved along several heuristics. We propose 
here some adaptations to improve query chunk-

                                                                 
2To moon about 

ing to deal with AP and PP attachment, and co-
ordination, using also NE information to guide 
the chunking strategy. 

AP and PP attachment 
In standard cases of chunking, AP and PP at-
tachment is not considered, because of attach-
ment ambiguity problems that cannot be solved 
at this stage of linguistic analysis.  

Considering the shortness of queries and the 
fact that they are mostly nominal, some of these 
attachments can be solved however in this con-
text. 

For the adjectival attachment in French, we 
attach the post modifier adjectival phrases to the 
first previous noun with which there is agreement 
in number and gender. For example, the chunk-
ing structure for the query “Bibliothèque eu-
ropeenne numerique” is: 
 
NP[ [Bibliothèque AP[europeenne] 
AP[numerique] ] 
 
while it is  
 
NP[Bibliothèque] AP[europeenne] 
AP[numerique] 
 
with our standard French grammar.  

For PP attachment, we simply consider that 
the PP attaches systematically to the previous 
noun. For example, the chunking structure for 
“The history of the University of Oxford” is: 
 
NP[the history PP[of the University 
PP[of Oxford] ] ]  
 
instead of: 

 
NP[The history] PP[of the Univer-
sity] PP[of  Oxford ] 

Coordination 
Some cases of coordination, usually very com-
plex, can be solved in the query context, in par-
ticular when NEs are involved. For both English 
and French, we attach coordinates when they 
belong to the same entity type (person conj per-
son, date conj date, place conj place, etc.), for 
example, “vase achilles et priam” :  
 
NP[vase] NP[Achille et Priam]  
 
instead of: 

 
NP[vase] NP[Achille] et NP[Priam] 

59



 
We also attach coordinates when the second 

is introduced by a reflexive pronoun, such as in: 
“[Le laboureur et ses enfants] La Fontaine” and 
attach coordinates within a PP when they are in-
troduced by the preposition “entre” in French 
and “between” in English. 

Use of NE information to guide the chunking 
strategy 
We also use information about NEs present in 
the queries to guide the query chunking strategy. 
In standard analysis, NEs are generally part of 
larger chunking units. In queries, however, be-
cause of their strong semantic, they can be iso-
lated as separate chunking units. We have 
adapted our chunking strategy using this infor-
mation: when the parser detects a NE (including 
a date), it chunks it as a separate NP. The follow-
ing examples show the chunking results for this 
adapted strategy versus the analysis of standard 
grammar: 
 
• “Anglo Saxon 11th century” (English) 
Adapted chunking:  
NP[Anglo Saxon] NP[ 11th century] 
 
Standard chunking:  
NP[Anglo Saxon 11th century ] 

 
• “Alexandre le Grand Persepolis”  (French) 
Adapted chunking:  
NP[Alexandre le Grand] NP[Perspolis] 
 
Standard chunking: 
NP[Alexandre le Grand Perspolis] 
 
The whole process is illustrated in Figure 1.  
 

When applying the full chain on an example 
query like “gauguin moon and earth”, we have 
the following steps and result: 
Preprocessing: gauguin is recognized as Gauguin 
(proper noun of celebrity); 
Part of speech tagging:  moon is disambiguated 
as a noun instead of a verb); 
Chunking:

So we get the following structure:  

 moon and earth are grouped together 
in a coordination chunk, gauguin is a NE 
chunked separately.  

 
NP[Gauguin] NP[moon and earth] 
 
and gauguin is recognized as a person name, 

instead of  

 
SC 3 [NP[gauguin] FV 4

 
Input query 

Query 
Preprocessing 

Query POS 
Disambiguation 

Query  
Chunking 

Adapted lexical 
Resources combined 
with contextual rules 

Improved query structure 

Adapted strategy for 
POS tagging 
(Contextual rules) 

Adapted Chunking 
strategy: contextual 
rules + named entities  

[moon]] and 
NP[earth],  

 
gauguin remaining unknown,  with the standard 
English grammar. 
 

Fig 1: Linguistic processing adaptation for que-
ries 

 

5.4 Examples of query structures 
The following table shows the differences of 
query structures obtained with the standard lin-
guistic processing and with the adapted linguistic 
processing.  
 

1. Albert Camus la peste 

Standard LP: NP {albert}  AP {camus}  
NP {la peste}   

Adapted LP: NP {albert camus}  NP {la 
peste}   

2. dieux ou héros grec 

Standard LP: NP {dieux}  COORD {ou}  
NP {héros}  AP {grec}   

                                                                 
3 SC: chunk tag for sentential clause 
4 FV: finite verb chunk 

60



Adapted LP: NP {dieux}  COORD {ou}  
NP {héros grec}   

3. pierre bergé 

Standard LP: NP {pierre}  VERB {bergé}   

Adapted LP: NP {pierre bergé} 
Table 2:  Some examples of query structure produced 
by standard and adapted linguistic processing. 
 

The evaluation of this customization is done 
indirectly through query translation, and is de-
scribed in the next section 

6 Experiments 

6.1 Experimental settings 
In our experiments we tried to enrich our base-
line SMT system with an adapted linguistic proc-
essing in order to improve the query translation. 
These experiments have double goal. First, to 
show that the adapted linguistic processing al-
lows to improve query translation compared to a 
standard linguistic processing, and second, to 
show that enriching an SMT model with a lin-
guistic processing (adapted) is helpful for the 
translation.  

We use an open source toolkit Moses (trained 
on Europarl) as a baseline model for query trans-
lations. Based on the examples from the section 
5, we choose to integrate the chunking and NE 
information in the translation. We integrate this 
knowledge in the following way: 
• Chunking: We check whether the query 

matches one of the following patterns: “NP1 
and NP2”, “NP1 or NP2”, “NP1 NP2”, 
“NP1, NP2”, etc. If it is the case, the NPs are 
translated independently.  Thus, we make 
sure that the output query will preserve the 
logical structure, if “and/or” are treated as 
logical operators. Also, translating NPs inde-
pendently might result at different (hopefully 
better) lexical choices.  

• Named entities: We introduce XML tags for 
person names where we propose a possible 
translation. During the translation process the 
proposed translation competes with the pos-
sible translations from a bi-phrase library. 
The translation maximizing internal transla-
tion score is chosen. In these experiments we 
propose not to translate an NE at all, how-

ever in more general case we could imagine 
having an adapted NE dictionary.  

6.2 Evaluation 
We have translated the totality of available 

Europeana French logs to English (8870 distinct 
queries), with the following translation models:  

• Moses trained on Europarl (Baseline 
MT) 

• Baseline MT model enriched with lin-
guistic processing (as defined in 6.1) 
based on basic grammar (Baseline MT + 
basic grammar) 

• Baseline MT enriched with linguistic 
processing based on adapted grammar 
(Baseline MT + adapted grammar) 

Our approach brings two new aspects com-
pared to simple SMT system. First, an SMT sys-
tem is enriched with linguistic processing as 
opposed to system without linguistic processing 
(baseline system), second: usage of an adapted 
linguistic processing as opposed to standard lin-
guistic processing. Thus, we evaluate: 

1. The impact of linguistic processing on 
the final query translations; 

2. The impact of grammar adaptation 
(adapted linguistic processing) in the 
context of query translation. 

First, we measure the overall impact of each 
of the two aspects mentioned above. Table 3 re-
flects the general impact of linguistic enrichment 
and grammar adaptation on query structure and 
translation. 

First, we note that the linguistic processing as 
defined in 6.1 won’t be applied to all queries. 
Thus, we count an amount of queries out of our 
test set to which this processing can actually be 
applied. This corresponds to the first line of the 
Table 3 (26% of all queries). 

Second, we compare the queries translation 
with and without linguistic processing. This is 
shown in the second line of the Table 3: the 
amount of queries for which the linguistic proc-
essing lead to different translation (25% of que-
ries for which the linguistic processing was 
applied). 

The second part of the table shows the differ-
ence between the standard linguistic processing 
and an adapted linguistic processing. First, we 
check how many queries get different structure 
after grammar adaptation (Section 5) (~42%) and 
second, we check how many of these queries 

61



actually get different translation (~16% queries 
with new structure obtained after adaptation get 
different translations).  

These numbers show that the linguistic 
knowledge that we integrated into the SMT 
framework may impact a limited portion of que-
ries’ translations. However, we believe that this 
is due, to some extent, to the way the linguistic 
knowledge was integrated in SMT, which ex-
plores only a small portion of the actual linguis-
tic information that is available. We carried out 
these experiments as a proof of concept for the 
adapted linguistic processing, but we believe that 
a deeper integration of the linguistic knowledge 
into the SMT framework will lead to more sig-
nificant results. For example, integrating such an 
adapted linguistic processing in a rule-based MT 
system will be straightforward and beneficial, 
since the linguistic information is explored di-
rectly by a translation model (e.g. in the example 
6 in Table 1 tagging "saw" as a noun will defi-
nitely lead to a better translation). 

Next, we define 2 evaluation tasks, where the 
goal of each task is to compare 2 translation 
models. We compare:  

1. Baseline MT versus linguistically en-
riched translation model (Baseline MT+adapted 
adapted linguistic processing). This task evalu-
ates the impact of linguistic enrichment in the 
query translation task with SMT.  

2. Translation model using standard lin-
guistic processing versus translation model using 
adapted linguistic processing. This task evalu-
ates the impact of the adapted linguistic process-
ing in the query translation task.  

For each evaluation task we have randomly 
selected a sample of 200 translations (excluding 
previously the identical translations for the 2 
models compared) and we perform a pairwise 
evaluation for each evaluation task. Thus, for the 
first evaluation task, a baseline translation (per-
formed by standard Moses without linguistic 
processing) is compared to the translation done 
by Moses + adapted linguistic processing. In the 
second evaluation task, the translation performed 
by Moses + standard linguistic processing is 
compared to the translation performed by Moses 
+ adapted linguistic processing. 

The evaluation has been performed by 3 
evaluators. However, no overlapping evaluations 
have been performed to calculate intra-evaluators 
agreement. We could observe, however, the simi-
lar tendency for improvement in each on the 
evaluated sample (similar to the one shown in the 
Table 2).  

We evaluate the overall translation perform-
ance, independently of the task in which the 
translations are going to be used afterwards (text  

Table 3: Impact of linguistic processing and 
grammar adaptation for query translation 

 
understanding, text analytics, cross-lingual in-
formation retrieval etc.) 

The difference between slight improvements 
and important improvements as in the examples 
below has been done during the evaluation. 

 
src1: max weber 
t1:max mr weber 
t2:max weber (slight improvement)

   
src2: albert camus la peste 
t1:albert camus fever 
t2:albert camus the plague (impor-

tant improvement) 
 
Thus, each pair of translations (t1, t2) re-

ceives a score from the scale [-2, 2] which can 
be: 

• 2, if t2 is much better than t1,  

• 1, if t2 is better than t1,  

• 0, if t2 is equivalent to t1,  

• -1, if t1 is better than t2,  

• -2, if t1 is much better than t2,  

Linguistic enrichment 

Nb of  queries to which the adapted 
linguistic processing was applied 
before translation.  

2311 
(26% of 
8870) 

Nb of translations which differ 
between baseline Moses and Moses 
with adapted linguistic processing.  

582  

(25% of 
2311) 

Grammar adaptation 

Nb of queries which get different 
structures between standard linguistic 
processing and adapted linguistic 
processing. 

3756  
(42% of 
8870) 

Nb of translations which differ 
between Moses+standard linguistic 
processing and Moses+adapted 
linguistic processing 

638   

(16 %  of 
3756) 

62



Table 4 presents the results of translation 
evaluation. 

 
Note, that a part of slight decreases can be 

corrected by introducing an adapted named enti-
ties dictionary to the translation system. For ex-
ample, for the source query “romeo et juliette”, 
keeping NEs untranslated results at the following 
translation: “romeo and juliette”, which is con-
sidered as a slight decrease in comparison to a 
baseline translation: “romeo and juliet”. Creating 
an adapted NEs dictionary, either by crawling 
Wikipedia, or other parallel resources, might be 
helpful for such cases. 

Often, the cases of significantly better transla-
tions could potentially lead to the better retrieval. 
For example, a better lexical choice (don juan 
moliere vs. donation juan moliere, the plague vs. 
fever) often judged as significant improvement 
may lead to a better retrieval. 

Based on this observation one may hope that 
the adapted linguistic processing can indeed be 
useful in the query translation task in CLIR con-
text, but also in general query analysis context. 

7 Conclusion 

Queries posed to digital library search engines in 
the cultural heritage and social sciences domain 
tend to be very short, referring mostly to artist 
names, objects, titles, and dates. As we have il-
lustrated with several examples, taken from the 
logs of the Europeana portal, standard NLP 
analysis is not well adapted to treat that domain. 
In this work we have proposed adapting a com-
plete chain of linguistic processing tools for 
query processing, instead of using out-of-the-box 
tools designed to analyze full sentences. 

Focusing on the cultural heritage domain, we 
translated queries from the Europeana portal us-
ing a state-of-the-art machine translation system 
and evaluated translation quality before and after 
applying the adaptations. The impact of the lin-
guistic adaptations is quite significant, as in 42% 
of the queries the resulting structure changes. 
Subsequently, 16% of the query translations are 
also different. The positive impact of the adapted 
linguistic processing on the translation quality is 
evident, as for 99 queries the translation (out of 
200 sample evaluated) is improved when com-
pared to having no linguistic processing. We ob-
serve also that 78 queries are better translated 
after adapting the linguistic processing compo-
nents. 

Our results show that customizing the linguis-
tic processing of queries can lead to important  

 
improvements in translation (and eventually to 
multilingual information retrieval and data min-
ing). A lot of the differences are related to the 
ability of properly identifying and treating do-
main-specific named entities. We plan to further 
research this aspect in future works. 
 

Acknowledge ments  

This research was supported by the European 
Union’s ICT Policy Support Programme as part 
of the Competitiveness and Innovation Frame-
work Programme, CIP ICT-PSP under grant 
agreement nr 250430 (Project GALATEAS). 

References 
Bin Tan and Fuchun Peng. 2008. Unsupervised query 

segmentation using generative language models 
and wikipedia. In Proceedings of the 17th interna-
tional conference on World Wide Web (WWW 
'08). ACM, New York, NY, USA, 347-356. 

Cory Barr, Rosie Jones, Moira Regelson. 2008. The 
Linguistic Structure of EnglishWeb-Search Que-
ries, Proceedings of ENMLP'08, pp 1021–1030, 
Octobre 2008, Honolulu. 

James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In  
Proceedings of the 25th annual international ACM 
SIGIR conference on Research and development in  
informat ion retrieval (SIGIR '02). ACM, New 
York, NY, USA, 307-314. 

Jeann-Pierre Chanod, Pasi Tapanainen. 1995.  Tag-
ging French - comparing a statistical and a con-
straint-based method. Proc. From Texts To Tags: 

 Impor
tant 
++ 

 Total 
nb+ 

Impo
rtant 

- - 

Total  
nb - 

Overall 
impact   

Moses<
Moses+
adapted  

35 87 4 19 99 

Moses+
basic<
Moses+
adapted  

28 66 2 12 80 

Table 4: Translation evaluation. Total nb+ (-): total 
number of improvements (decreases), not distinguish-
ing whether it is slight or important; important ++ (--): 
the number of important improvements (decreases). 
Overall impact = (Total nb+) + (Importan++ ) – (Total 
nb-) – (Important --) 

63



Issues In Multilingual Language Analysis, EACL 
SIGDAT workshop. Dublin, 1995. 

Jiafeng Guo, Gu Xu, Hang Li, Xueqi Cheng. 2008. A 
Unified and Discriminative Model for Query Re-
finement. Proc. SIGIR’08, July 20–24, 2008, Sin-
gapore. 

Josiane Mothe and Ludovic Tanguy. 2007. Linguistic 
Analysis of Users' Queries: towards an adaptive In-
formation Retrieval System. International Confer-
ence on Signal-Image Technology & Internet–
Based Systems, Shangai, China, 2007. 
http://halshs.archives-ouvertes.fr/halshs-
00287776/fr/ [Last accessed March 3, 2011]  

Lauri Karttunen. 2000. Applications of Finite-State 
Transducers in Natural Language Processing. Pro-
ceedings of CIAA-2000. Lecture Notes in Com-
puter Science. Springer Verlag. 

Marijn Koolen and Jaap Kamps. 2010. Searching cul-
tural heritage data: does structure help expert  
searchers?. In Adaptivity, Personalizat ion and Fu-
sion of Heterogeneous Information (RIAO '10). Le 
centre des hautes etudes internationals 
d’informat ique documentaire, Paris, France, 152-
155. 

Michael Bendersky, W. Bruce Croft and David A. 
Smith. 2010. Structural Annotation of Search Que-
ries Using Pseudo-Relevance Feedback. Proceed-
ings of CIKM'10, October 26-29, 2010, Toronto, 
Ontario, Canada 

Neil Ireson and Johan Oomen. 2007. Capturing e-
Culture: Metadata in MultiMatch., J. In Proc. 
DELOS-MultiMatch workshop, February 2007, 
Tirrenia, Italy. 

Rosie Jones, Ben jamin Rey, Omid Madani, and Wiley 
Greiner. 2006. Generating query substitutions. In 
Proceedings of the 15th international conference on 
World Wide Web (WWW '06). ACM, New York, 
NY, USA, 387-396. 

Shane Bergsma and Qin Iris Wang. 2007. Learning 
Noun Phrase Query Segmentation, Proceedings of 
the 2007 Jo int Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pp. 819–826, Prague, 
June 2007. 

64


