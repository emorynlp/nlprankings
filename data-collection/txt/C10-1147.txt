Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1308–1316,

Beijing, August 2010

1308

Maximum Metric Score Training for Coreference Resolution

Shanheng Zhao and Hwee Tou Ng

Department of Computer Science
National University of Singapore

{zhaosh,nght}@comp.nus.edu.sg

Abstract

A large body of prior research on coref-
erence resolution recasts the problem as
a two-class classiﬁcation problem. How-
ever, standard supervised machine learn-
ing algorithms that minimize classiﬁca-
tion errors on the training instances do not
always lead to maximizing the F-measure
of the chosen evaluation metric for coref-
erence resolution. In this paper, we pro-
pose a novel approach comprising the use
of instance weighting and beam search to
maximize the evaluation metric score on
the training corpus during training. Ex-
perimental results show that this approach
achieves signiﬁcant improvement over the
state-of-the-art. We report results on stan-
dard benchmark corpora (two MUC cor-
pora and three ACE corpora), when evalu-
ated using the link-based MUC metric and
the mention-based B-CUBED metric.

1

Introduction

Coreference resolution refers to the process of
determining whether two or more noun phrases
(NPs) in a text refer to the same entity. Suc-
cessful coreference resolution beneﬁts many nat-
ural language processing tasks. In the literature,
most prior work on coreference resolution recasts
the problem as a two-class classiﬁcation problem.
Machine learning-based classiﬁers are applied to
determine whether a candidate anaphor and a po-
tential antecedent are coreferential (Soon et al.,
2001; Ng and Cardie, 2002b).

A large body of prior research on corefer-
ence resolution follows the same process: dur-

ing training, they apply standard supervised ma-
chine learning algorithms to minimize the number
of misclassiﬁed training instances; during testing,
they maximize either the local or the global proba-
bility of the coreferential relation assignments ac-
cording to the speciﬁc chosen resolution method.

However, minimizing the number of misclas-
siﬁed training instances during training does not
guarantee maximizing the F-measure of the cho-
sen evaluation metric for coreference resolution.
First of all, coreference is a rare relation. There
are far fewer positive training instances than neg-
ative ones. Simply minimizing the number of mis-
classiﬁed training instances is suboptimal and fa-
vors negative training instances. Secondly, evalu-
ation metrics for coreference resolution are based
on global assignments. Not all errors have the
same impact on the metric score. Furthermore, the
extracted training instances are not equally easy to
be classiﬁed.

In this paper, we propose a novel approach
comprising the use of instance weighting and
beam search to address the above issues. Our pro-
posed maximum metric score training (MMST)
approach performs maximization of the chosen
evaluation metric score on the training corpus dur-
ing training. It iteratively assigns higher weights
to the hard-to-classify training instances. The out-
put of training is a standard classiﬁer. Hence,
during testing, MMST is faster than approaches
which optimize the assignment of coreferential re-
lations during testing. Experimental results show
that MMST achieves signiﬁcant improvements
over the baselines. Unlike most of the previous
work, we report improved results over the state-
of-the-art on all ﬁve standard benchmark corpora

1309

(two MUC corpora and three ACE corpora), with
both the link-based MUC metric and the mention-
based B-CUBED metric.

The rest of this paper is organized as follows.
We ﬁrst review the related work and the evaluation
metrics for coreference resolution in Section 2 and
3, respectively. Section 4 describes the proposed
MMST algorithm. Experimental results and re-
lated discussions are given in Section 5. Finally,
we conclude in Section 6.

2 Related Work

Soon et al. (2001) proposed a training and test-
ing framework for coreference resolution. Dur-
ing training, a positive training instance is formed
by a pair of markables, i.e., the anaphor (a noun
phrase) and its closest antecedent (another noun
phrase). Each markable (noun phrase) between
the two, together with the anaphor, form a neg-
ative training instance. A classiﬁer is trained on
all training instances, using a standard supervised
learning algorithm. During testing, all preceding
markables of a candidate anaphor are considered
as potential antecedents, and are tested in a back-
to-front manner. The process stops if either an an-
tecedent is found or the beginning of the text is
reached. This framework has been widely used in
the community of coreference resolution.

Recent work boosted the performance of coref-
erence resolution by exploiting ﬁne-tuned feature
sets under the above framework, or adopting al-
ternative resolution methods during testing (Ng
and Cardie, 2002b; Yang et al., 2003; Denis and
Baldridge, 2007; Versley et al., 2008).

Ng (2005) proposed a ranking model to maxi-
mize F-measure during testing. In the approach, n
different coreference outputs for each test text are
generated, by varying four components in a coref-
erence resolution system, i.e., the learning algo-
rithm, the instance creation method, the feature
set, and the clustering algorithm. An SVM-based
ranker then picks the output that is likely to have
the highest F-measure. However, this approach
is time-consuming during testing, as F-measure
maximization is performed during testing. This
limits its usage on a very large corpus.

In the community of machine learning, re-
searchers have proposed approaches for learning

a model to optimize a chosen evaluation met-
ric other than classiﬁcation accuracy on all train-
ing instances. Joachims (2005) suggested the use
of support vector machines to optimize nonlinear
evaluation metrics. However, the approach does
not differentiate between the errors in the same
category in the contingency table. Furthermore, it
does not take into account inter-instance relation
(e.g., transitivity), which the evaluation metric for
coreference resolution cares about.

Daume III (2006) proposed a structured learn-
ing framework for coreference resolution to ap-
proximately optimize the ACE metric. Our pro-
posed approach differs in two aspects. First, we
directly optimize the evaluation metric itself, and
not by approximation. Second, unlike the incre-
mental local loss in Daume III (2006), we evaluate
the metric score globally.

In contrast

to Ng (2005), Ng and Cardie
(2002a) proposed a rule-induction system with
rule pruning. However, their approach is speciﬁc
to rule induction, and is not applicable to other
supervised learning classiﬁers. Ng (2004) varied
different components of coreference resolution,
choosing the combination of components that re-
sults in a classiﬁer with the highest F-measure on
a held-out development set during training.
In
contrast, our proposed approach employs instance
weighting and beam search to maximize the F-
measure of the evaluation metric during training.
Our approach is general and applicable to any su-
pervised learning classiﬁers.

Recently, Wick and McCallum (2009) pro-
posed a partition-wise model for coreference reso-
lution to maximize a chosen evaluation metric us-
ing the Metropolis-Hastings algorithm (Metropo-
lis et al., 1953; Hastings, 1970). However, they
found that training on classiﬁcation accuracy, in
most cases, outperformed training on the corefer-
ence evaluation metrics. Furthermore, similar to
Ng (2005), their approach requires the generation
of multiple coreference assignments during test-
ing.

Vemulapalli et al. (2009) proposed a document-
level boosting technique for coreference resolu-
tion by re-weighting the documents that have
the lowest F-measures. By combining multiple
classiﬁers generated in multiple iterations, they

1310

achieved a CEAF score slightly better than the
baseline. Different from them, our approach
works at the instance level, and we output a sin-
gle classiﬁer.

3 Coreference Evaluation Metrics

In this section, we review two commonly used
evaluation metrics for coreference resolution.

First, we introduce the terminology. The gold
standard annotation and the output by a coref-
erence resolution system are called key and re-
sponse, respectively. In both the key and the re-
sponse, a coreference chain is formed by a set of
coreferential mentions. A mention (or markable)
is a noun phrase which satisﬁes the markable def-
inition in an individual corpus. A link refers to a
pair of coreferential mentions. If a mention has no
links to other mentions, it is called a singleton.

3.1 The MUC Evaluation Metric
Vilain et al.
(1995) introduced the link-based
MUC evaluation metric for the MUC-6 and MUC-
7 coreference tasks. Let Si be an equivalence
class generated by the key (i.e., Si is a corefer-
ence chain), and p(Si) be a partition of Si relative
to the response. Recall is the number of correctly
identiﬁed links over the number of links in the key.

Recall = P(|Si| − |p(Si)|)
P(|Si| − 1)

Precision, on the other hand, is deﬁned in the op-
posite way by switching the role of key and re-
sponse. F-measure is a trade-off between recall
and precision.

F =

2 · Recall · P recision
Recall + P recision

3.2 The B-CUBED Evaluation Metric
introduced the
Bagga and Baldwin (1998)
mention-based B-CUBED metric.
The B-
CUBED metric measures the accuracy of coref-
erence resolution based on individual mentions.
Hence, it also gives credit to the identiﬁcation of
singletons, which the MUC metric does not. Re-
call is computed as

Recall =

1

N Xd∈DXm∈d

|Om|
|Sm|

where D, d, and m are the set of documents, a
document, and a mention, respectively. Sm is the
equivalence class generated by the key that con-
tains m, while Om is the overlap of Sm and the
equivalence class generated by the response that
contains m. N is the total number of mentions in
D. The precision, again, is computed by switch-
ing the role of key and response. F-measure is
computed in the same way as the MUC metric.

4 Maximum Metric Score Training

Before explaining the algorithm, we describe our
coreference clustering method used during test-
ing. It is the same as most prior work in the lit-
erature, including Soon et al. (2001) and Ng and
Cardie (2002b). The individual classiﬁcation de-
cisions made by the coreference classiﬁer do not
guarantee that transitivity of coreferential NPs is
obeyed. So it can happen that the pair A and B,
and the pair B and C are both classiﬁed as coref-
erential, but the pair A and C is not classiﬁed
as coreferential by the classiﬁer. After all coref-
erential markable pairs are found (no matter by
closest-ﬁrst, best-ﬁrst, or resolving-all strategies
as in different prior work), all coreferential pairs
are clustered together to form the coreference out-
put. By doing so, transitivity is kept: a markable is
in a coreference chain if and only if it is classiﬁed
to be coreferential to at least one other markable
in the chain.

and n
mr

Instance Weighting

4.1
Suppose there are mk and mr coreferential links
in the key and the response, respectively, and a
coreference resolution system successfully pre-
dicts n correct links. The recall and the preci-
sion are then n
, respectively. The learnt
mk
classiﬁer predicts false positive and false negative
instances during testing. For a false positive in-
stance, if we could successfully predict it as neg-
ative, the recall is unchanged, but the precision
n
will be
mr−1, which is higher than the original
precision n
. For a false negative instance, it
mr
is more subtle.
If the two markables in the in-
stance are determined to be in the same corefer-
ence chain by the clustering algorithm, it does not
matter whether we predict this instance as posi-
tive or negative, i.e., this false negative does not

1311

change the F-measure of the evaluation metric at
all. If the two markables are not in the same coref-
erence chain under the clustering, in case that we
can predict it as positive, the recall will be n+1
,
mk
which is higher than the original recall n
, and
mk
the precision will be n+1
mr+1, which is higher than
the original precision n
In both
mr
cases, the F-measure improves. If we can instruct
the learning algorithm to pay more attention to
these false positive and false negative instances
and to predict them correctly by assigning them
more weight, we should be able to improve the
F-measure.

, as n < mr.

In the literature, besides the training instance
extraction methods proposed by Soon et al.
(2001) and Ng and Cardie (2002b) as discussed
in Section 2, McCarthy and Lehnert (1995) used
all possible pairs of training instances. We also
use all pairs of training instances in our approach
to keep as much information as possible. Initially
all the pairs are equally weighted. We then itera-
tively assign more weights to the hard-to-classify
pairs. The iterative process is conducted by a
beam search algorithm.

4.2 Beam Search
Our proposed MMST algorithm searches for a set
of weights to assign to training instances such
that the classiﬁer trained on the weighted training
instances gives the maximum coreference metric
score when evaluated on the training instances.
Beam search is used to limit the search. Each
search state corresponds to a set of weighted train-
ing instances, a classiﬁer trained on the weighted
training instances minimizing misclassiﬁcations,
and the F-measure of the classiﬁer when evalu-
ated on the weighted training instances using the
chosen coreference evaluation metric. The root
of the search tree is the initial search state where
all the training instances have identical weights of
one. Each search state s can expand into two dif-
ferent children search states sl and sr. sl (sr) cor-
responds to assigning higher weights to the false
positive (negative) training instances in s. The
search space thus forms a binary search tree.

Figure 1 shows an example of a binary search
tree. Initially, the tree has only one node: the root
(node 1 in the ﬁgure). In each iteration, the algo-

1

3

5

6

7

10

11

2

9

4

8

Figure 1: An example of a binary search tree

rithm expands all the leaf nodes in the beam. For
example, in the ﬁrst iteration, node 1 is expanded
to generate node 2 and 3, which corresponds to
adding weights to false positive and false nega-
tive training instances, respectively. An expanded
node always has two children in the binary search
tree. All the nodes are then sorted in descending
order of F-measure. Only the top M nodes are
kept, and the remaining nodes are discarded. The
discarded nodes can either be leaf nodes or non-
leaf nodes. For example, if node 5 is discarded
because of low F-measure, it will not be expanded
to generate children in the binary search tree. The
iterative algorithm stops when all the nodes in the
beam are non-leaf nodes, i.e., all the nodes in the
beam have been expanded.

Figure 2 gives the formal description of the
proposed maximum metric score training algo-
rithm.
In the algorithm, assume that we have
N texts T1, T2, . . ., TN in the training data set.
mki and mkj are the ith and jth markable in
the text Tk, respectively. Hence, for all i <
j, (mki, mkj, wkij) is a training instance for the
markable pair (mki, mkj), in which wkij is the
weight of the instance. Let Lkij and L′kij be the
true and predicted label of the pair (mki, mkj),
respectively. Let W , C, F , and E be the set of
weights {wkij|1 ≤ k ≤ N, i < j}, the classiﬁer,
the F-measure, and a boolean indicator of whether
the search state has been expanded, respectively.
Finally, M is the beam size, and δ controls how
much we update the weights in each iteration.

Since we train the model on all possible pairs,
during testing we also test if a potential anaphor is
coreferential to each preceding antecedent.

1312

INPUT: T1, T2, . . . , TN
OUTPUT: classiﬁer C
wkij ← 1, for all 1 ≤ k ≤ N and i < j
C ← train({(mki, mkj , wkij )|1 ≤ k ≤ N, i < j})
F ← resolve and evaluate T1, . . . , TN with C
E ← f alse
BEAM← {(W, C, F, E)}
repeat

BEAM′ ← {}
for all (W, C, F, E) in BEAM do

if E=false then

BEAM′ ← BEAM′S{(W, C, F, true)}

predict all L′kij with C (1 ≤ k ≤ N, i < j)
cluster into coreference chains based on L′kij
W ′ ← W
for all 1 ≤ k ≤ N, i < j do

if Lkij = f alse and L′kij = true then

w′kij ← w′kij + δ

end if
end for
C′ ← train({(mki, mkj , w′kij )|1 ≤ k ≤ N, i < j})
F ′ ← resolve and evaluate T1, . . . , TN with C′
W ′′ ← W
for all 1 ≤ k ≤ N, i < j do

BEAM′ ← BEAM′S{(W ′, C′, F ′, f alse)}

if Lkij = true and L′kij = f alse and
Chain(mki) 6= Chain(mkj ) then
w′′kij ← w′′kij + δ
end if
end for
C′′ ← train({(mki, mkj , w′′kij )|1 ≤ k ≤ N, i < j})
F ′′ ← resolve and evaluate T1, . . . , TN with C′′

BEAM′ ← BEAM′S{(W ′′, C′′, F ′′, f alse)}

end if
end for
BEAM← BEAM′
sort BEAM in descending order of F , keep top M elements

until for all E of all elements in BEAM, E = true
return C, from the top element (W, C, F, E) of BEAM

Figure 2: The maximum metric score training
(MMST) algorithm

5 Experiments

5.1 Experimental Setup
In the experiments, we used all the ﬁve commonly
used evaluation corpora for coreference resolu-
tion, namely the two MUC corpora (MUC6 and
MUC7) and the three ACE corpora (BNEWS,
NPAPER, and NWIRE). The MUC6 and the
MUC7 corpora were deﬁned in the DARPA Mes-
sage Understanding Conference (MUC-6, 1995;
MUC-7, 1998). The dry-run texts were used as the
training data sets. In both corpora, each training
data set contains 30 texts. The test data sets for
MUC6 and MUC7 consist of the 30 and 20 for-
mal evaluation texts, respectively. The ACE cor-
pora were deﬁned in NIST Automatic Content Ex-
traction phase 2 (ACE-2) (NIST, 2002). The three
data sets are from different news sources: broad-
cast news (BNEWS), newspaper (NPAPER), and

newswire (NWIRE). Each of the three data sets
contains two portions: training and development
test. They were used as our training set and test
set, respectively. The BNEWS, NPAPER, and
NWIRE data sets contain 216, 76, and 130 train-
ing texts, and 51, 17, and 29 test texts, respec-
tively.

Unlike some previous work on coreference res-
olution that assumes that the gold standard mark-
ables are known, we work directly on raw text in-
put. Versley et al.
(2008) presented the BART
package1, an open source coreference resolution
toolkit, that accepts raw text input and reported
state-of-the-art MUC F-measures on the three
ACE corpora. BART uses an extended feature set
and tree kernel support vector machines (SVM)
under the Soon et al. (2001) training and testing
framework. We used the BART package in our ex-
periments, and implemented the proposed MMST
algorithm on top of it. In our experiments reported
in this paper, the features we used are identical to
the features output by the preprocessing code of
BART reported in Versley et al.
(2008), except
that we did not use their tree-valued and string-
valued features (see the next subsection for de-
tails).

Since we use automatically extracted mark-
ables, it is possible that some extracted markables
and the gold standard markables are unmatched,
or twinless as deﬁned in Stoyanov et al. (2009).
How to use the B-CUBED metric for evaluating
twinless markables has been explored recently. In
this paper, we adopt the B3all variation proposed
by Stoyanov et al. (2009), which retains all twin-
less markables. We also experimented with their
B30 variation, which gave similar results. Note
that no matter which variant of the B-CUBED
metric is used, it is a fair comparison as long as
the baseline and our proposed MMST algorithm
are compared against each other using the same
B-CUBED variant.

5.2 The Baseline Systems
We include state-of-the-art coreference resolution
systems in the literature for comparison. Since
we use the BART package in our experiments,

1http://www.sfs.uni-tuebingen.de/

˜versley/BART/

1313

we include the results of the original BART sys-
tem (with its extended feature set and SVM-light-
TK (Moschitti, 2006), as reported in Versley et al.
(2008)) as the ﬁrst system for comparison. Vers-
ley et al. (2008) reported only the results on the
three ACE data sets with the MUC evaluation met-
ric. Since we used all the ﬁve data sets in our
experiments, for fair comparison, we also include
the MUC results reported in Ng (2004). To the
best of our knowledge, Ng (2004) was the only
prior work which reported MUC metric scores on
all the ﬁve data sets. The MUC metric scores of
Versley et al. (2008) and Ng (2004) are listed in
the row “Versley et al. 08” and “Ng 04”, respec-
tively, in Table 1. For the B-CUBED metric, we
include Ng (2005) for comparison, although it is
unclear how Ng (2005) interpreted the B-CUBED
metric. The scores are listed in the row “Ng 05”
in Table 2.

Tree kernel SVM learning is time-consuming.
To reduce the training time needed, instead of us-
ing SVM-light-TK, we used a much faster learn-
ing algorithm, J48, which is the WEKA imple-
mentation of the C4.5 decision tree learning algo-
rithm. (Quinlan, 1993; Witten and Frank, 2005).
As tree-valued features and string-valued features
cannot be used with J48, in our experiments we
excluded them from the extended feature set that
BART used to produce state-of-the-art MUC F-
measures on the three ACE corpora. All our re-
sults in this paper were obtained using this re-
duced feature set and J48 decision tree learn-
ing. However, given sufﬁcient computational re-
sources, our proposed approach is able to apply to
any supervised machine learning algorithms.

Our baselines that follow the Soon et al. (2001)
framework, using the reduced feature set and J48
decision tree learning, are shown in the row “SNL-
Style Baseline” in Table 1 and 2. The results
suggest that our baseline system is comparable
to the state of the art. Although in Table 1, the
performance of the SNL-style baseline is slightly
lower than Versley et al. (2008) on the three ACE
corpora, the computational time needed has been
greatly reduced.

Our MMST algorithm trains and tests on all
pairs of markables. To show the effectiveness of
weight updating of MMST, we built another base-

line which trains and tests on all pairs. The per-
formance of this system is shown in the row “All-
Style Baseline” in Table 1 and 2.

5.3 Results Using Maximum Metric Score

Training

Next, we show the results of using the proposed
maximum metric score training algorithm. From
the description of the algorithm, it can be seen that
there are two parameters in the algorithm. One
parameter is M, the size of the beam. The other
parameter is δ, which controls how much we in-
crease the weight of a training instance in each
iteration.

Since the best M and δ for the MUC evaluation
metric were not known, we used held-out develop-
ment sets to tune the parameters. Speciﬁcally, we
trained classiﬁers with different combinations of
M and δ on a development training set, and eval-
uated their performances on a development test
set. In our experiments, the development training
set contained 2/3 of the texts in the training set
of each individual corpus, while the development
test set contained the remaining 1/3 of the texts.
After having picked the best M and δ values, we
trained a classiﬁer on the entire training set with
the chosen parameters. The learnt classiﬁer was
then applied to the test set.

e
r
u
s
a
e
m
−
F

68

66

64

62

60

58

56

54

52

2

MMST
SNL−Style Baseline
All−Style Baseline

4

6

8

10

12

14

16

18

20

M

Figure 3: Tuning M on the held-out development
set

To limit the search space, we tuned the two
parameters sequentially. First, we ﬁxed δ =
1, which is equivalent to duplicating each train-
ing instance once in J48, and evaluated M =
. . . , 20. After having chosen the best
2, 4, 6,
M that corresponded to the maximum F-measure,
we ﬁxed the value of M, and evaluated δ =
. . . , 2.0. Take MUC6 as an exam-
0.1, 0.2, 0.3,

1314

MUC7

BNEWS

NPAPER

NWIRE

Model

MUC6
P

F

R

R

Versley et al. 08

–

Ng 04

75.8 61.4 67.9
SNL-Style Baseline 67.0 49.2 56.7
56.9 69.2 62.5
All-Style Baseline
73.3 59.9 65.9∗∗†† 66.8 59.8 63.1∗∗† 70.5 61.9 65.9∗∗† 69.9 64.0 66.8† 64.7 64.7 64.7∗∗†
M = 6, δ = 1.0 M = 6, δ = 0.7 M = 6, δ = 1.8 M = 6, δ = 0.9 M = 14, δ = 0.7

64.2 60.2 62.1
63.0 54.2 58.3
51.5 73.4 60.6

MMST

P

R
F
60.7 65.4 63.0
63.1 67.8 65.4
57.4 64.3 60.7
53.0 76.7 62.7

P

F

P

R

R
F
64.1 67.7 65.8 60.4 65.2 62.7
73.5 63.3 68.0 53.1 60.6 56.6
61.6 67.3 64.3 58.6 66.1 62.1
56.3 75.4 64.4 53.0 74.5 61.9

F

P
–

Table 1: Results for the two MUC and three ACE corpora with MUC evaluation metric

MUC6
P

MUC7

BNEWS

NPAPER

NWIRE

F

R

Model
Ng 05

R
59.3 75.4 66.4
SNL-Style Baseline 57.8 74.4 65.1
65.8 75.9 70.5
64.8 80.1 71.7
51.6 86.3 64.6
All-Style Baseline
62.7 81.5 70.9∗∗†† 61.8 73.6 67.2†† 61.6 83.7 71.0∗∗ 63.1 76.2 69.1∗∗ 64.3 81.0 71.7
M = 6, δ = 1.0 M = 8, δ = 0.8 M = 6, δ = 0.9 M = 14, δ = 0.5 M = 6, δ = 0.1

R
F
62.8 71.2 66.7
61.8 70.4 65.8
63.9 74.0 68.6

R
F
57.0 77.1 65.6
62.0 74.7 67.8
61.6 83.7 71.0

57.6 76.5 65.7
49.1 90.1 63.6

MMST

P
–

R

F

P

P

P

F

–

Table 2: Results for the two MUC and three ACE corpora with B3 evaluation metric

MMST
SNL−Style Baseline
All−Style Baseline

68

66

64

62

60

58

56

54

e
r
u
s
a
e
m
−
F

52

0

0.2

0.4

0.6

0.8

1

delta

1.2

1.4

1.6

1.8

2

Figure 4: Tuning δ on the held-out development
set

ple. The results of tuning M on MUC6 are shown
in Figure 3. The maximum F-measure is obtained
when M = 4 and M = 6. On all the different M
values we have tried, MMST outperforms both the
SNL-style baseline and the All-style baseline on
the development test set. We then ﬁxed M = 6,
and evaluated different δ values. The results are
shown in Figure 4. The best F-measure was ob-
tained when δ = 1.0. Again, on all the different
δ values we have tried, MMST outperforms both
baselines on the development test set.

The rows “MMST” in Table 1 and 2 show the
performance of MMST on the test sets, with the
tuned parameters indicated. In our experiments,
the statistical signiﬁcance test was conducted as
in Chinchor (1995). ∗ and ∗∗ stand for p < 0.05
and p < 0.01 over the SNL-style baseline, respec-
tively. † and †† stand for p < 0.05 and p < 0.01
over the All-style baseline, respectively.

For the MUC metric, when compared to the
All-style baseline, MMST gains 3.4, 2.5, 3.2, 2.4,
and 2.8 improvement in F-measure on MUC6,
MUC7, BNEWS, NPAPER, and NWIRE, respec-
tively. The experimental results clearly show that
MMST gains not only consistent, but also sta-
tistically signiﬁcant improvement over both the
SNL-style baseline and the All-style baseline in all
combinations (ﬁve data sets and two baselines) on
the MUC metric, except that it is not signiﬁcant
(p = 0.06) over the SNL-style baseline in NPA-
PER. As for the B-CUBED metric, MMST gains
signiﬁcant improvement in F-measure on MUC6
and MUC7 data sets, while its performance on
the three ACE data sets are comparable to the All-
style baseline.

5.4 Discussion

To see how MMST actually updates the weight,
we use the MUC metric as an example. Under the
experimental settings, it takes 6 – 9 iterations for
MMST to stop on the ﬁve data sets. The number
of explored states in the binary search tree, includ-
ing the root, is 33, 39, 25, 29, and 75 on MUC6,
MUC7, BNEWS, NPAPER, and NWIRE, respec-
tively. It is instructive to ﬁnd out the ﬁnal weight
of each instance. Take MUC6 as an example, the
number of positive instances with weight 1, 2, 3,
and 4 are 5,204, 1,568, 1,379, and 1,844, respec-
tively, while the number of negative instances with
weight 1 and 2 are 503,141 and 1,755, respec-

1315

tively. Counting the weighted number of instances
(e.g., an instance with weight 2 is equivalent to 2
instances), we have 19,853 positive and 506,651
negative training instances. This changes the ratio
of the positive instances from 1.9% to 3.8%. As a
by-product, MMST reduces data skewness, while
using all possible NP pairs for training to keep as
much information as possible.

The change of weights of the training instances
is equivalent to the change of distribution of the
training instances. This effectively changes the
classiﬁcation hypothesis to the one that tends to
yield higher evaluation metric score. Take the fol-
lowing sentence in the MUC6 data set as an ex-
ample:

In a news release, the company said the new
name more accurately reﬂects its focus on high-
technology communications, including business
and entertainment software,
interactive media
and wireless data and voice transmission.

In the above example, the pronoun its is coref-
erential to the antecedent NP the company. The
baseline classiﬁer gives a probability of 0.02 that
the two NPs are coreferential. The pair is clas-
siﬁed wrongly and none of the other pairs in the
article can link the two NPs together through clus-
tering. However, with MMST, this probability in-
creases to 0.54, which leads to the correct classi-
ﬁcation. This is because the baseline classiﬁer is
not good at predicting in the case when the sec-
ond markable is a pronoun. In the above exam-
ple, its can have another candidate antecedent the
new name. There are far more negative training
instances than positive ones for this case. In fact,
in the induced decision tree by the baseline, the
leaf node corresponding to the pair the company
– its has 7,782 training instances, out of which
only 175 are positive. With MMST, however,
these numbers decrease to 83 and 45, respectively.
MMST also promotes the Anaphor Is Pronoun
feature to a higher level in the decision tree. Al-
though we use decision tree to illustrate the work-
ing of the algorithm, MMST is not limited to tree
learning, and can make use of any learning algo-
rithms that are able to take advantage of instance
weighting.

It can also be seen that with the B-CUBED
metric, MMST gains improvement on MUC6 and

MUC7, but not on the three ACE corpora. How-
ever, the results of MMST on the three ACE cor-
pora with the B-CUBED evaluation metric are at
least comparable with the All-style baseline. This
is because we always pick the classiﬁer which cor-
responds to the maximum evaluation metric score
on the training set and the classiﬁer correspond-
ing to the All-style baseline is one of the candi-
dates. In addition, our MMST approach improves
upon state-of-the-art results (Ng, 2004; Ng, 2005;
Versley et al., 2008) on most of the ﬁve standard
benchmark corpora (two MUC corpora and three
ACE corpora), with both the link-based MUC
metric and the mention-based B-CUBED metric.
the F-
measure maximization during training, and is very
fast during testing, since the output of the MMST
algorithm is a standard classiﬁer. For example,
on the MUC6 data set with the MUC evaluation
metric, it took 1.6 hours and 31 seconds for train-
ing and testing, respectively, on an Intel Xeon
2.33GHz machine.

Finally, our approach performs all

6 Conclusion

In this paper, we present a novel maximum met-
ric score training approach comprising the use of
instance weighting and beam search to maximize
the chosen coreference metric score on the train-
ing corpus during training. Experimental results
show that the approach achieves signiﬁcant im-
provement over the baseline systems. The pro-
posed approach improves upon state-of-the-art re-
sults on most of the ﬁve standard benchmark cor-
pora (two MUC corpora and three ACE corpora),
with both the link-based MUC metric and the
mention-based B-CUBED metric.

Acknowledgments

We thank Yannick Versley for providing us
the BART package and the preprocessed data.
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.

1316

References

Bagga, Amit and Breck Baldwin. 1998. Algorithms
In Proceedings of

for scoring coreference chains.
the LREC1998, pages 563–566.

Chinchor, Nancy. 1995. Statistical signiﬁcance of
In Proceedings of the MUC-6,

MUC-6 results.
pages 39–43.

Daume III, Hal. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
University of Southern California.

Denis, Pascal and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. In Proceedings of the
NAACL-HLT2007, pages 236–243.

Hastings, W. K. 1970. Monte Carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57(1):97–109.

Joachims, Thorsten. 2005. A support vector method
for multivariate performance measures. In Proceed-
ings of the ICML2005, pages 377–384.

McCarthy, Joseph F. and Wendy G. Lehnert. 1995.
Using decision trees for coreference resolution. In
Proceedings of the IJCAI1995, pages 1050–1055.

Metropolis, Nicholas, Arianna W. Rosenbluth, Mar-
shall N. Rosenbluth, Augusta H. Teller, and Edward
Teller. 1953. Equation of state calculations by fast
computing machines. Journal of Chemical Physics,
21(6):1087–1092.

Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of the EACL2006, pages 113–120.

MUC-6. 1995. Coreference task deﬁnition (v2.3, 8
Sep 95). In Proceedings of the MUC-6, pages 335–
344.

MUC-7. 1998. Coreference task deﬁnition (v3.0, 13

Jul 97). In Proceedings of the MUC-7.

Ng, Vincent and Claire Cardie. 2002a. Combining
sample selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceedings
of the EMNLP2002, pages 55–62.

Ng, Vincent and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL2002, pages 104–111.

Ng, Vincent. 2004. Improving Machine Learning Ap-
proaches to Noun Phrase Coreference Resolution.
Ph.D. thesis, Cornell University.

Ng, Vincent. 2005. Machine learning for coreference
resolution: From local classiﬁcation to global rank-
ing.
In Proceedings of the ACL2005, pages 157–
164.

NIST.

2002.

The ACE 2002 evaluation plan.

ftp://jaguar.ncsl.nist.gov/ace/
doc/ACE-EvalPlan-2002-v06.pdf.

Quinlan, J. Ross. 1993. C4.5: Programs for Machine

Learning. Morgan Kaufmann.

Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.

Stoyanov, Veselin, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the ACL-IJCNLP2009,
pages 656–664.

Vemulapalli, Smita, Xiaoqiang Luo, John F. Pitrelli,
and Imed Zitouni. 2009. Classiﬁer combination
techniques applied to coreference resolution.
In
Proceedings of the NAACL-HLT2009 Student Re-
search Workshop and Doctoral Consortium, pages
1–6.

Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolu-
tion.
In Proceedings of the ACL2008:HLT Demo
Session, pages 9–12.

Vilain, Marc, John Burger, John Aberdeen, Dennis
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the MUC-6, pages 45–52.

Wick, Michael and Andrew McCallum. 2009. Ad-
vances in learning and inference for partition-
wise models of coreference resolution. Techni-
cal Report UM-CS-2009-028, University of Mas-
sachusets, Amherst, USA.

Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
The Morgan Kaufmann Series in Data Management
Systems. Morgan Kaufmann Publishers, second edi-
tion.

Yang, Xiaofeng, Guodong Zhou,

Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In Proceedings
of the ACL2003, pages 176–183.

