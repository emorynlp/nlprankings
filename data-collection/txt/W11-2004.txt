










































A Two-Stage Domain Selection Framework for Extensible Multi-Domain Spoken Dialogue Systems


Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18–29,
Portland, Oregon, June 17-18, 2011. c©2011 Association for Computational Linguistics

A Two-Stage Domain Selection Framework for Extensible Multi-Domain

Spoken Dialogue Systems

Mikio Nakano

Honda Research Institute Japan

Wako, Saitama, Japan

nakano@jp.honda-ri.com

Shun Sato

Tokyo Denki University

Hatoyama, Saitama, Japan

rela.relakuma@gmail.com

Kazunori Komatani

Nagoya University

Nagoya, Aichi, Japan

komatani@nuee.nagoya-u.ac.jp

Kyoko Matsuyama∗

Kyoto University

Kyoto, Kyoto, Japan

matuyama@kuis.kyoto-u.ac.jp

Kotaro Funakoshi

Honda Research Institute Japan

Wako, Saitama, Japan

funakoshi@jp.honda-ri.com

Hiroshi G. Okuno

Kyoto University

Kyoto, Kyoto, Japan

okuno@i.kyoto-u.ac.jp

Abstract

This paper describes a general and effective

domain selection framework for multi-domain

spoken dialogue systems that employ dis-

tributed domain experts. The framework con-

sists of two processes: deciding if the current

domain continues and estimating the probabil-

ities for selecting other domains. If the current

domain does not continue, the domain with

the highest activation probability is selected.

Since those processes for each domain expert

can be designed independently from other ex-

perts and can use a large variety of informa-

tion, the framework achieves both extensibil-

ity and robustness against speech recognition

errors. The results of an experiment using

a corpus of dialogues between humans and

a multi-domain dialogue system demonstrate

the viability of the proposed framework.

1 Introduction

As spoken dialogue interfaces are becoming more

widely utilized, they will be expected to be able to

engage in dialogues in a wide variety of topics. Par-

ticularly, spoken dialogue interfaces for office robots

(Asoh et al., 1999) and multimodal kiosk systems

(Gustafson and Bell, 2000) are expected to deal with

people’s various requests, unlike automated call cen-

ter systems that are dedicated to specific tasks.

One effective methodology to build such a sys-

tem is to integrate systems in small domains by

employing distributed multi-domain system archi-

tecture. This architecture has distributed modules

∗Currently with Panasonic Corporation.

that independently manage their own dialogue state

and knowledge for speech understanding and ut-

terance generation (e.g., Lin et al. (1999)). From

an engineering viewpoint, such architecture has an

advantage in that each domain expert can be de-

signed independently and that it is easy to add new

domains. It enables each domain expert to em-

ploy a dialogue strategy very different from those

for other domains. For example, the strategy may

be frame-based mixed-initiative, finite-state-based

system-initiative, or plan-based dialogue manage-

ment (McTear, 2004).

One of the crucial issues with distributed multi-

domain spoken dialogue systems is how to select an

appropriate domain for each user utterance so that

the system can appropriately understand it and an-

swer it. So far several methods have been proposed

but none of them satisfy two basic requirements at

the same time: the ability to be used with a variety

of domain experts (extensibility) and being robust

against ASR (Automatic Speech Recognition) errors

(robustness). We suspect that this is one of the

main reasons why not many multi-domain spoken

dialogue systems have been developed even though

their utility is widely recognized.

This paper presents a new general framework for

domain selection that satisfies the above two require-

ments. In our framework, each expert needs to have

two additional submodules: one for estimating the

probability that it is newly activated, and one for de-

ciding domain continuation when it is already acti-

vated. Since these submodules can be designed in-

dependently from those of other experts, there is no

restriction on designing experts in our framework,

18



and thus extensibility is achieved. Robustness is also

achieved because those submodules can be designed

so that they can utilize domain-dependent informa-

tion, including information on speech understanding

and dialogue history, without detracting from ex-

tensibility. Especially the submodule for deciding

domain continuation has the ability to utilize dia-

logue history to avoid erroneous domain shifts that

often occur in previous approaches. Note that we do

not focus on classifying each utterance without con-

textual information (e.g., Chu-Carroll and Carpenter

(1999)). Rather, we try to estimate the user inten-

tion with regard to continuing and shifting domains

in the course of dialogues.

In what follows, Section 2 explains the distributed

multi-domain spoken dialogue system architecture

and requirements for domain selection. Section 3

discusses previous work, and Section 4 presents our

proposed framework. Section 5 describes an exam-

ple implementation and its evaluation results, and

Section 6 concludes the paper.

2 Domain Selection in Multi-Domain

Spoken Dialogue Systems

2.1 Distributed Architecture

In distributed multi-domain spoken dialogue archi-

tecture (Figure 1), distributed modules indepen-

dently manage their own dialogue state and knowl-

edge for speech understanding and utterance gener-

ation (Lin et al., 1999; Salonen et al., 2004; Pakucs,

2003; Nakano et al., 2008). Although those modules

are referred to with various names in that literature,

we call them domain experts in this paper. In this

architecture, when an input utterance is received, its

ASR results are sent to domain experts. They try to

understand the ASR results using their own knowl-

edge for understanding. The domain selector gathers

information from those experts and decides which

expert should deal with the utterance and then de-

cide on the system utterances. In this paper, the do-

main expert engaging in understanding user utter-

ances and deciding system utterances is called acti-

vated.

2.2 Example Systems

So far many multi-domain spoken dialogue sys-

tems based on distributed architecture have been

user 
utterance

information 
for domain selection

domain selector
activate/

deactivate

system 
utterance
(from the 
activated

expert)

speech 
understanding

utterance 
generation

domain 
expert 1

domain expert 2

domain expert 3

dialogue
history

Figure 1: Distributed multi-domain spoken dialogue sys-

tem architecture.

built and have demonstrated their ability to engage

in dialogues in a variety of domains. For exam-

ple, several systems integrated information provid-

ing and database searches in multiple domains (Lin

et al., 1999; Komatani et al., 2006; O’Neill et al.,

2004; Gustafson and Bell, 2000). Some other sys-

tems integrated domain experts that employ very

different dialogue strategies. Lee et al. (2009) and

Nakano et al. (2006) integrated task-oriented and

non-task-oriented dialogue managements. Nakano

et al. (2008) integrated domain experts for not only

dialogues but also tasks requiring physical actions.

Below we explain an example system that we

used to collect dialogue data for the domain se-

lection experiment described in Section 5. It is a

Japanese system that can provide information on

UNESCO World Heritage Sites using speech, slides,

and Microsoft Agent1 gestures. It employs the fol-

lowing ten domain experts:

A question answering (QA) expert: It has a

database consisting of question-answer pairs on

World Heritage Sites (Narimatsu et al., 2010). Each

question-answer pair consists of a set of example

question sentences and answers to them. The an-

swers consist of one or more utterances, plus slides.

Keyphrase-based matching is used to select an ex-

ample question that matches the input utterance.

Eight interactive presentation (IP) experts: Each

of them explains in detail a famous World Her-

itage Site and adds a detailed explanation or skips

some explanation according to the user’s interrup-

tions (Nishimura et al., 2007). Patterns of interrup-

tion utterances are described in the expert’s knowl-

edge base.

1http://www.microsoft.com/MSAgent/

19



utterance expert

S1: Hello. I can answer questions about fa-

mous World Heritage Sites in Italy, Ger-

many, Greece, and other areas. I can also

explain some World Heritage Sites in de-

tail. Please ask me anything.

QA

U1: Show me the list of the World Heritage

Sites in Italy.

QA

S2: (show a slide of the list) I can tell you about

the World Heritage Sites in these areas in

Italy.

QA

U2: Can you show me the list of World Her-

itage Sites you can explain in detail?

QA

S3: (show a slide of the list) I can explain these

in detail.

QA

U3: Can you explain Maritime Greenwich? RU

S4: Maritime Greenwich, right? RU

U4: Yes. RU

S5: (show a slide on Greenwich) Okay. I’ll ex-

plain Maritime Greenwich, a historic port

town known for Greenwich Mean Time.

IP1

U5: What is Greenwich Mean Time? IP1

S6: Greenwich Mean Time is the mean solar

time at the Royal Greenwich Observatory.

IP1

S means system utterance and U means user utterance.

IP1 is one of the IP experts.

Figure 2: An example dialogue with the system for data

collection (translation from Japanese).

A request understanding (RU) expert: It under-

stands the user’s request to start one of the interac-

tive presentations explained above, and engages in

a dialogue to confirm the request. When the under-

standing finishes, the understood request is sent to

a module called task planner (Nakano et al., 2008;

Nakano et al., 2011). The task planner then activates

another expert to perform the requested presentation

(S5 in Figure 2).

Figure 2 shows an example dialogue between a

human and this system. Note that user utterances

are relatively short and include words related to spe-

cific World Heritage Sites or area names. If those

words are misrecognized, domain selection is diffi-

cult unless dialogue context information is used.

This figure also indicates the domain experts that

understood each user utterance and selected each

system utterance. The domain expert that should

deal with a user utterance is decided based on the set

of user utterances that the expert is designed to deal

with. The domains of utterances U1 and U3 are dif-

ferent because the QA expert has knowledge for un-

derstanding U1 and the RU expert has knowledge for

understanding U3. Thus, in this study, the domain of

each utterance is determined based on the design of

the experts employed in the system. If none of the

experts can deal with an utterance, it is considered

as an out-of-domain utterance. Sometimes the cor-

rect domain needs to be determined using contextual

information. For example, utterance U4 “Yes” can

appear in all domains, but, since this is a reply to S4,

its domain is RU.

This definition of domain is different from that of

domain (or topic) recognition and adaptation stud-

ies in text, monologue, and human-human conver-

sation processing, in which reference domains are

annotated based on human perspectives rather than

system perspectives. From a human perspective, all

user utterances in Figure 2 may be in “World Her-

itage Site” domain. However, it is not always easy

to build domain experts according to such domain

definitions, because different dialogue tasks in one

such domain may require different dialogue strate-

gies (such as question answering and request under-

standing).

2.3 Requirements for Domain Selection

We pursue a method for domain selection that can

be used in distributed architecture. Such a method

must satisfy the following two requirements.

Extensibility It must not detract from the extensi-

bility of distributed architecture, that is, any kind of

expert must be able to be incorporated, and each ex-

pert must be able to be designed independently from

other experts. This requires the interface between

each domain expert and the domain selector to be as

simple as possible.

Robustness It needs to be robust against ASR er-

rors; that is, the system needs to be able to avoid

erroneous domain transition caused by ASR errors.

3 Previous Work

So far various methods for domain selection have

been proposed, but, as far as we know, no method

satisfies both extensibility and robustness. Isobe et

al. (2003) estimate a score for each domain from the

20



ASR result and select the domain with the highest

score (hereafter referred to as RECSCORE). Since

each domain expert has only to output a numeric

score, it satisfies extensibility. However, because

this method does not take into account dialogue con-

text, it tends to erroneously shift domains when the

score of some experts becomes high by chance. For

example, if U4:“Yes” in Figure 2 is recognized as

“Italy” with a high recognition score in the QA ex-

pert, the domain erroneously shifts to QA and the

system explains about World Heritage Sites in Italy.

Thus this method is not robust.

To avoid erroneous domain shifts, Lin et al.

(1999) give preference to the preceding domain

(the domain in which the previous system utterance

was made) by adding a certain value to the score

of the preceding domain (hereafter called REC-

SCORE+BIAS ). However, to what extent the do-

main tends to continue varies depending on the dia-

logue context. For example, if a dialogue task in one

domain finishes (e.g., when an IP expert finishes its

presentation and says “This is the end of the presen-

tation. Do you have any questions?”), the domain is

likely to shift. So, adding a fixed score does not al-

ways work. O’Neill et al.’s (2004) system does not

change the dialogue domain until it finishes a task

in the domain, but it cannot recover from erroneous

domain shifts.

To achieve robustness against ASR errors, several

domain selection methods based on a classifier that

uses features concerning dialogue history as well as

ones concerning speech understanding results have

been developed (Komatani et al., 2006; Ikeda et al.,

2008; Lee et al., 2009). These studies, however, use

some features available only in some specific type

of domain experts, such as features concerning slot-

filling, so they cannot be used with other kinds of

domain experts. That is, these methods do not sat-

isfy extensibility.

Methods that use classifiers based on word (and

n-gram) frequencies have been developed for utter-

ance classification (e.g., Chu-Carroll and Carpenter

(1999)), topic estimation for ASR of speech cor-

pora (e.g., Hsu and Glass (2006) and Heidel and

Lee (2007)) and human-human dialogues (Lane and

Kawahara, 2005). These methods can be applied to

domain selection in multi-domain spoken dialogue

systems. However, since they require training data

in the same set of domains as the target system, it

detracts from extensibility. In addition, they are not

robust because they cannot utilize a variety of di-

alogue and understanding related features. Word

frequencies are not always effective when two do-

mains share words as in our system described in Sec-

tion 2.2.

4 Proposed Framework

4.1 Basic Idea

To achieve extensibility, we need to restrict the infor-

mation that each expert sends to the domain selector

to a simple one such as numeric scores. Although

RECSCORE and RECSCORE+BIAS satisfy this, they

would not achieve high accuracy as explained above.

One possible extension to those methods to im-

prove accuracy is to use not only recognition scores

but also various expert-dependent features such as

ones concerning dialogue history and speech under-

standing. Each expert first estimates the probability

that the input utterance is in its domain using such

features, and then the expert with the highest proba-

bility is selected (hereafter called MAXPROB). This

method retains extensibility because the domain se-

lector does not directly use those expert-dependent

features. However, it suffers from the same prob-

lem as RECSCORE and RECSCORE+BIAS; if one of

the experts other than the preceding domain’s expert

outputs a high probability by mistake, the domain

shifts regardless of the dialogue state in the preced-

ing domain’s expert.

We focus attention on the fact that the domain

does not often shift. Our idea is to decide if the do-

main continues or not by using information available

in the preceding domain’s expert. This prevents er-

roneous domain shifts when the utterance is consid-

ered not to change the domain. When it is decided

that the currently active domain does not continue,

each remaining expert estimates the probability of

being newly activated using information available in

the expert, and the expert whose probability is the

highest is selected as the new domain expert.

We further refine this idea in two ways. One is by

taking into account how likely the input utterance is

to activate one of the other domain experts. We pro-

pose to use the maximum value of probabilities for

other experts’ activation (maximum activation prob-

21



user 
utterance

expert for the preceding domain

experts for other domains

maximum
probability

select the
preceding 

domain

select the 
domain with 
maximum
activation 
probability

features

Stage 1

Stage 2

.
.
.

domain 
continuation

decision maker

select expert with 
maximum
activation
probability

speech 
understanding

features activation probability 
estimator

speech 
understanding

features activation probability 
estimator

speech 
understanding

decision is to
continue?

yes

no

out-of-
domain

handle as an 
out-of-domain 

utterance

dialogue
history

dialogue
history

dialogue
history

Figure 3: Two-stage domain selection framework.

ability) in the decision regarding domain continua-

tion. Since the maximum activation probability is

just a numeric score, this does not spoil extensibil-

ity. Unlike RECSCORE and RECSCORE+BIAS, in

our method, even if the maximum activation prob-

ability is very high, the preceding domain’s expert

can decide to continue or not to continue based on

its internal state. This makes it possible to retain ro-

bustness.

The other refinement is to explicitly deal with ut-

terances that are not in any domains (out-of-domain

(OOD) utterances). They include fillers and mur-

murs. They should be treated separately, because

they appear context-independently. So we make the

expert detect OOD utterances when deciding do-

main continuation. That is, it performs three-fold

classification, continue, not-continue, and OOD.

4.2 Two-Stage Domain Selection Framework

This idea can be summarized as a domain selection

framework which consists of two stages (Figure 3).

It assumes that each domain expert has two submod-

ules: activation probability estimator and a domain

continuation decision maker, which use information

available in the expert itself.

When a new input utterance is received, at Stage

1, the activation probability estimators of all non-

activated experts estimate probabilities and send

them to the domain selector. Then at Stage 2, the

domain selector sends their maximum value to the

expert of the preceding domain and asks it to decide

whether it continues to deal with the new input utter-

ances or does not continue, or it deals with the utter-

ance as out-of-domain. If it decides not to continue,

the domain selector selects the expert that outputs

the highest probability at Stage 1.

The reason we use the term “framework” is that

it does not specify the details of the algorithm and

features used in each domain expert’s submodules

for domain selection. It rather specifies the inter-

faces of those submodules. Note that RECSCORE,

RECSCORE+BIAS, and MAXPROB can be consid-

ered as one of the implementations of this frame-

work. This framework, however, allows developers

to use a wider variety of features and gives flexibility

in designing those submodules.

5 Example Implementation and

Evaluation

Since the proposed framework is an extension of

the previous methods, if the activation probability

estimator and domain continuation decision maker

for each expert are designed well and trained using

enough data, it should outperform previous methods

that satisfy extensibility. We believe that this theo-

retical consideration and an experimental result us-

ing a human-system dialogue corpus show the via-

bility of the framework. Below we explain our im-

plementation and an experiment.

5.1 Data

For the implementation and evaluation, we used a

corpus of dialogues between human users and the

World Heritage Site information system described

in Section 2.2. Domain selection of this system was

performed using hand-crafted rules.

35 participants (17 males and 18 females) whose

ages range from 19 to 57 were asked to engage in

22



domain preceding training training test

domain data A data B data

RU RU 134 169 145

QA 51 102 59

IP 21 16 23

subtotal 206 287 227

QA RU 46 55 51

QA 783 870 888

IP 59 87 66

subtotal 888 1,012 1,005

IP RU 2 1 3

QA 7 11 18

IP 311 305 277

subtotal 320 317 298

OOD RU 24 19 39

QA 168 155 183

IP 66 68 113

subtotal 258 242 335

total 1,672 1,858 1,865

Table 1: Number of utterances in each domain in the

training and test data.

conversation with the system four times. Each ses-

sion lasted eight minutes. For each utterance, the

correct domain or an OOD label was manually an-

notated. We also annotated its preceding domain,

i.e., the domain in which the previous system utter-

ance was made. It can be different from the previous

user utterance’s domain because of the system’s er-

roneous domain selection. Utterances including re-

quests in two domains at the same time should be

given an OOD label but there are no such utterances.

We used data from 23 participants (3,530 utterances)

for training and those from the remaining 12 par-

ticipants (1,865 utterances) for testing. We further

split the training data into training data A (1,672

utterances) and B (1,858 utterances) to train each

of the two submodules. Each training data set in-

cludes data from two sessions for each participant.

Table 1 shows detailed numbers of utterances in the

data sets.

5.2 Implementation

5.2.1 Expert Classes

Among the ten experts, eight IP (Interactive Pre-

sentation) experts have the same dialogue strategy

and most of the predicted user utterance patterns. In

addition, the number of training utterances for each

expert class QA IP RU

LM for ASR trigram trigram finite-state

grammar

language keyphrase keyphrase finite-state

understanding -based -based transducer

vocabulary 1,140 407 79

size (word)

phone error 10.95 19.47 23.60

rate (%)

Table 2: Speech understanding in each expert.

IP expert’s domain is small. We therefore used all

training utterances in the IP domains to build a com-

mon ASR language model (LM), a common acti-

vation probability estimator, and a common domain

continuation decision maker for all IP experts. Here-

after we call the set of IP experts the IP expert class.

The RU (Request Understanding) expert and the QA

(Question Answer) expert are themselves also expert

classes.

5.2.2 Speech Understanding

For all experts, we used the Julius speech recog-

nizer and the acoustic model in the Japanese model

repository (Kawahara et al., 2004).2 Features of

speech understanding in each expert class are shown

in Table 2. Compared to the system used for data

collection, LMs are enhanced based on the training

data. We obtained the ASR performance on the ut-

terances in each domain in the test data in terms of

phone error rates. This is because Japanese has no

standard word boundaries so it is not easy to cor-

rectly compute word error rates. The poor perfor-

mance of ASR for IP is mainly due to the small

amount of training utterances for LM and that for

RU is mainly due to out-of-grammar utterances.

5.2.3 Stage 1

For Stage 1, we used logistic regression to es-

timate the probability that a non-activated expert

would be activated by a user utterance. Features for

logistic regression include those concerning speech

recognition and understanding results as well as dia-

logue history (see Table 5 for the full list of features).

These features are expert-dependent. This makes it

possible to estimate how the input utterance is suit-

2Multiple LMs can be used at the same time with Julius.

23



able to the dialogue context more precisely than us-

ing just features available in any kind of expert.

To train the activation probability estimators, we

fitted logistic regression coefficients using Weka

data mining toolkit ver.3.6.2 (Witten and Frank,

2005)3 and training data A. In the training for each

expert class, we used utterances whose preceding

domain was not that of the class because activation

probabilities are estimated only for such utterances

during domain selection. If the utterance is in a do-

main of the expert class, it is assigned an activate la-

bel and otherwise not-activate. Next, we performed

feature selection to avoid overfitting. We used back-

ward stepwise selection so that the weighted (by the

sizes of activate and not-activate labels) average of

the F1 scores for training set B could be maximized.

Table 6 lists the remaining features and their sig-

nificances in terms of the F1 score obtained when

each feature is removed. Then, we duplicated the

activate-labeled utterances in the training data A so

that the ratio of activate-labeled utterances to not-

activate-labeled utterances became 1 to 3. This is

because the training data include a larger number of

not-activate-labeled utterances and thus the results

would be biased. The ratio was decided by trial and

error so that the weighted average of the F1 scores

for training data B becomes high.

5.2.4 Stage 2

For Stage 2, we used multi-class support vector

machines (SVMs)4 to decide if the activated expert

should continue to be activated, should not continue,

or should regard the input utterance as OOD. We

used the same set of features as Stage 1 as well

as the maximum activation probability obtained at

Stage 1. The training data for the SVM of each ex-

pert class is the set of utterances in training data B

whose preceding domain is in that expert class, be-

cause domain continuation is decided only for such

utterances during domain selection. They are la-

beled continue, not-continue, or OOD. Next, we

performed backward stepwise feature selection so

that the weighted average of F1 scores for continue,

not-continue, and OOD utterance detection on train-

ing data A could be maximized. Remaining fea-

3Multinominal logistic regression model with a ridge esti-

mator with Weka’s default values.
4Weka’s SMO with the linear kernel and its default values.

tures are listed in Table 7. The maximum activa-

tion probability was found to be significant in all ex-

pert classes. This suggests our two-stage framework

that uses maximum activation probability is viable.

Then, we duplicated utterances with not-continue la-

bel and OOD label in the training data so that the

ratio of continue, not-continue, and OOD utterances

became 3:1:1. This is because the number of utter-

ances with the continue label is far greater than oth-

ers. The ratio was experimentally decided by trial

and error so that the weighted average of F1 scores

on training data A becomes high.

5.3 Evaluation

5.3.1 Compared Methods

We compared the full implementation described in

Section 5.2 (FULLIMPL hereafter) with the follow-

ing four methods which satisfy extensibility. Note

that the first three methods were mentioned in Sec-

tion 4.

RECSCORE: This chooses the expert class whose

recognition score is the maximum (Isobe et al.,

2003). We used the ASR acoustic score normalized

by the duration of the utterance. If the IP expert class

was chosen, the IP expert that had been most re-

cently activated was chosen, because, in this system,

domain shifts to other IP experts never occur due to

the system constraints and the user did not try to do

it. If none of the experts had a higher score than a

fixed threshold, it recognized the utterance as OOD.

The threshold was experimentally determined using

the training data so that the weighted (by the sizes

of OOD and non-OOD utterances) average of the

F1 scores of OOD/non-OOD classification is max-

imized.

RECSCORE+BIAS: This is the same as REC-

SCORE except that a fixed value (bias) is added to

the score used in RECSCORE for the expert of the

preceding domain. This is basically the same as Lin

et al.’s (1999) method but we use a different recog-

nition score since the recognition score they used

cannot be used in our system due to the difference

of speech understanding methods. The most appro-

priate bias for each expert class was decided using

the training data so that the weighted average of the

F1 scores could be maximized. OOD detection was

done in the same way as RECSCORE.

24



method class recall prec- F1 weighted

ision ave. F1

RECSCORE cont. 0.763 0.867 0.812

shift 0.559 0.239 0.335

OOD 0.501 0.848 0.630 0.789

RECSCORE cont. 0.917 0.824 0.868

+BIAS shift 0.400 0.421 0.410

OOD 0.501 0.848 0.630 0.838

MAXPROB cont. 0.925 0.843 0.882

shift 0.282 0.264 0.273

OOD 0.275 0.477 0.348 0.832

NOACTIV cont. 0.875 0.890 0.882

PROB shift 0.464 0.385 0.421

OOD 0.785 0.843 0.813 0.849

FULLIMPL cont. 0.902 0.907 0.904

shift 0.591 0.565 0.578

OOD 0.824 0.829 0.826 0.883

CLASSIFIER cont. 0.956 0.881 0.917

(reference) shift 0.545 0.759 0.635

OOD 0.755 0.885 0.815 0.899

Table 3: Evaluation results (“cont.” means “continue.”).

MAXPROB: The activation probabilities for all ex-

perts were obtained using logistic regression and the

expert whose probability was the maximum was se-

lected. IP experts that had never been activated were

excluded because they cannot be activated due to

system constraint. For logistic regression, in addi-

tion to the features used in FULLIMPL, the previous

domain was used as a feature so that domain conti-

nuity was taken into account. Feature selection was

also performed. The probability that the utterance is

OOD was estimated in the same way using the fea-

tures concerning speech understanding. If the maxi-

mum probability of OOD detection was greater than

the maximum activation probability, then the utter-

ance was considered to be OOD.

NOACTIVPROB: This is the same as FULLIMPL

except that Stage 2 does not use the result of Stage

1, i.e., maximum activation probability.

5.3.2 Evaluation Results

To evaluate the domain selection, we focused on

domain shifts rather than the selected domain. We

classified the domain selection results into domain

continuations, domain shifts, and OOD utterance

detection. As the evaluation metric, we used the

weighted average of F1 scores for those classes.

Here the weight is the ratio of those classes of cor-

rect labels. Note that shifting to an incorrect do-

main is counted as a false positive when calculat-

ing precision for domain shifts. Table 3 shows the

results. In addition, the confusion matrices for the

three best methods are shown in Table 4. We found

FULLIMPL outperforms the other four methods. We

also found that the differences between the results of

the compared methods are all statistically significant

(p < .01) by two-tailed binomial tests.

For reference, we also evaluated a classifier-based

method that uses features from all the experts. Note

that this method does not satisfy extensibility be-

cause it requires training data in the same set of do-

mains as the target system. We evaluated this just

for estimating how well our proposed method works

while satisfying extensibility. It classifies each ut-

terance into one of four categories: the QA expert’s

domain, the RU expert’s domain, the most recently

activated IP expert’s domain, and OOD. If no IP ex-

pert has been activated before the utterance, three-

fold classification was performed. The training and

test data were split depending on whether one of the

IP experts has been activated before, and training

and testing were separately conducted. The training

data A was used for training SVM classifiers. Then

feature selection was performed using the training

data B. The performance of this method is shown

as CLASSIFIER in Tables 3 and 4. Although this

method outperforms FULLIMPL, FULLIMPL’s per-

formance is close to this method. This shows that

our method does not degrade its performance very

much even though it satisfies extensibility.

5.3.3 Discussion

One of the reasons why FULLIMPL outperforms

other methods is that its precision for domain shifts

is relatively higher than the other methods. This

suggests it can avoid erroneous domain shifts, thus

the proposed two-stage framework is more robust.

RECSCORE+BIAS performed relatively well despite

it used only limited features. We guess this is be-

cause adding preferences to the preceding domain

was effective since domain shifts are rare in these

data. Its low F1 score for OOD utterances suggests

using just recognition scores is insufficient to detect

them. The comparison of FULLIMPL with NOAC-

TIVPROB shows the effectiveness of using maxi-

mum activation probability in the second stage.

The F1 score for domain shifts is low even with

25



RECSCORE+BIAS:
estimated result

correct cont. correct wrong OOD total

shift shift

continue 1,201 - 82 27 1,310

shift 115 88 14 3 220

OOD 142 - 25 168 335

total 1,458 88 121 198 1,865

NOACTIVPROB:
estimated result

correct cont. correct wrong OOD total

shift shift

continue 1,146 - 123 41 1,310

shift 92 102 18 8 220

OOD 50 - 22 263 335

total 1,288 102 163 312 1,865

FULLIMPL:
estimated result

correct cont. correct wrong OOD total

shift shift

continue 1,181 - 77 52 1,310

shift 70 130 15 5 220

OOD 51 - 8 276 335

total 1,302 130 100 333 1,865

CLASSIFIER (reference):

estimated result

correct cont. correct wrong OOD total

shift shift

continue 1,252 - 30 28 1310

shift 92 120 3 5 220

OOD 77 - 5 253 335

total 1,421 120 38 286 1,865

Table 4: Confusion matrices for the domain shifts.

FULLIMPL, although it is higher than those with

other methods. One typical reason for this is that

when one keyword in the ASR result of an utter-

ance to shift the domain is also in the vocabulary of

the preceding domain’s expert, the selection tends to

continue the previous domain by mistake. For ex-

ample, an utterance “tell me about other World Her-

itage Sites” to shift from an IP domain to the QA

domain is sometimes misclassified as an IP domain

utterance, because “World Heritage Sites” is also in

IP domains’ vocabulary. We think this is because

the training data do not include a sufficient amount

of utterances that shift domains, and that a larger

amount of training data would solve this problem.

6 Concluding Remarks

This paper presented a novel general framework for

domain selection in extensible multi-domain spoken

dialogue systems. This framework makes it possi-

ble to build a robust domain selector because of its

flexibility in exploiting features and taking into ac-

count domain continuity. An experiment with data

collected with an example multi-domain system sup-

ported the viability of the proposed framework. We

believe that this framework will promote the devel-

opment of multi-domain spoken dialogue systems

and conversational robots/agents.

Among future work is to investigate how accurate

the activation probability estimator and the domain

continuation decision maker in each domain expert

should be for achieving a reasonable accuracy in do-

main selection. We also plan to conduct experiments

with systems that have a larger number of domain

experts to verify the scalability of this framework.

In addition, we will explore a way to estimate the

confidence of the domain selection to reduce erro-

neous domain selections.

Acknowledgments

The authors would like to thank Hiroshi Tsujino,

Yuji Hasegawa, and Hiromi Narimatsu for their sup-

port for this research.

References

Hideki Asoh, Toshihiro Matsui, John Fry, Futoshi Asano,

and Satoru Hayamizu. 1999. A spoken dialog system

for a mobile office robot. In Proc. 6th Eurospeech,

pages 1139–1142.

Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-

based natural language call routing. Computational

Linguistics, 25(3):361–388.

Joakim Gustafson and Linda Bell. 2000. Speech tech-

nology on trial: Experiences from the August system.

Natural Language Engineering, 6(3&4):273–286.

Aaron Heidel and Lin-shan Lee. 2007. Robust topic in-

ference for latent semantic language model adaptation.

In Proc. ASRU-07, pages 177–182.

Bo-June (Paul) Hsu and James Glass. 2006. Style and

topic language model adaptation using HMM-LDA.

In Proc. EMNLP ’06, pages 373–381,.

Satoshi Ikeda, Kazunori Komatani, Tetsuya Ogata, and

Hiroshi G. Okuno. 2008. Extensibility verification

26



of robust domain selection against out-of-grammar ut-

terances in multi-domain spoken dialogue system. In

Proc. Interspeech-2008 (ICSLP), pages 487–490.

T. Isobe, S. Hayakawa, H. Murao, T. Mizutani,

K. Takeda, and F. Itakura. 2003. A study on do-

main recognition of spoken dialogue systems. In Proc.

Eurospeech-2003, pages 1889–1892.

Tatsuya Kawahara, Akinobu Lee, Kazuya Takeda, Kat-

sunobu Itou, and Kiyohiro Shikano. 2004. Recent

progress of open-source LVCSR engine Julius and

Japanese model repository. In Proc. Interspeech-2004

(ICSLP), pages 3069–3072.

Kazunori Komatani, Naoyuki Kanda, Mikio Nakano,

Kazuhiro Nakadai, Hiroshi Tsujino, Tetsuya Ogata,

and Hiroshi G. Okuno. 2006. Multi-domain spo-

ken dialogue system with extensibility and robustness

against speech recognition errors. In Proc. 7th SIGdial

Workshop, pages 9–17.

Ian R. Lane and Tatsuya Kawahara. 2005. Incorporating

dialogue context and topic clustering in out-of-domain

detection. In Proc. ICASSP-2005, pages 1045–1048.

Cheongjae Lee, Sangkeun Jung, Seokhwan Kim, and

Gary Geunbae Lee. 2009. Example-based dialog

modeling for practical multi-domain dialog system.

Speech Communication, 51(5):466–484.

Bor-shen Lin, Hsin-ming Wang, and Lin-shan Lee. 1999.

A distributed architecture for cooperative spoken dia-

logue agents with coherent dialogue state and history.

In Proc. ASRU-99.

Michael F. McTear. 2004. Spoken Dialogue Technology.

Springer.

Mikio Nakano, Atsushi Hoshino, Johane Takeuchi,

Yuji Hasegawa, Toyotaka Torii, Kazuhiro Nakadai,

Kazuhiko Kato, and Hiroshi Tsujino. 2006. A robot

that can engage in both task-oriented and non-task-

oriented dialogues. In Proc. Humanoids-2006, pages

404–411.

Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and

Hiroshi Tsujino. 2008. A framework for building con-

versational agents based on a multi-expert model. In

Proc. 9th SIGdial Workshop, pages 88–91.

Mikio Nakano, Yuji Hasegawa, Kotaro Funakoshi, Jo-

hane Takeuchi, Toyotaka Torii, Kazuhiro Nakadai,

Naoyuki Kanda, Kazunori Komatani, Hiroshi G.

Okuno, and Hiroshi Tsujino. 2011. A multi-expert

model for dialogue and behavior control of conversa-

tional robots and agents. Knowledge-Based Systems,

24(2):248–256.

Hiromi Narimatsu, Mikio Nakano, and Kotaro Fu-

nakoshi. 2010. A classifier-based approach to

supporting the augmentation of the question-answer

database for spoken dialogue systems. In Proc. 2nd

IWSDS, pages 182–187.

Yoshitaka Nishimura, Shinichiro Minotsu, Hiroshi Dohi,

Mitsuru Ishizuka, Mikio Nakano, Kotaro Funakoshi,

Johane Takeuchi, Yuji Hasegawa, and Hiroshi Tsujino.

2007. A markup language for describing interactive

humanoid robot presentations. In Proc. IUI’07, pages

333–336.

Ian O’Neill, Philip Hanna, Xingkun Liu, and Michael

McTear. 2004. Cross domain dialogue modelling:

an object-based approach. In Proc. Interspeech-2004

(ICSLP), pages 205–208.

Botond Pakucs. 2003. Towards dynamic multi-domain

dialogue processing. In Proc. Eurospeech-2003, pages

741–744.

Esa-Pekka Salonen, Mikko Hartikainen, Markku Tu-

runen, Jaakko Hakulinen, and J. Adam Funk. 2004.

Flexible dialogue management using distributed and

dynamic dialogue control. In Proc. Interspeech-2004

(ICSLP), pages 197–200.

Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-

tical machine learning tools and techniques, 2nd Edi-

tion. Morgan Kaufmann, San Francisco.

27



expert Features

class

all Fi,r1 If SRRi,1 is obtained or not
classes Fi,r2 If SRRi,1 contains a filler or not
i = ru, Fi,r3 min (CMs of words in SRRi,1)
ip, qa Fi,r4 avg (CMs of words in SRRi,1)

Fi,r5 (acoustic score of SRRi,1) / duration
Fi,r6 LM score of SRRi,1
Fi,r7 # of words in SRRi,1
Fi,r8 # of words in SRRi,all
Fi,r9 (Fi,r5 - (acoustic score of SRRlv,1))

/ duration

RU Fru,r10 If SRRru,1 is an affirmative response
Fru,r11 If SRRru,1 is a denial response
Fru,r12 # of ASR results with LMru
Fru,r13 If SRRru,1 contains the name of a

World Heritage Site

Fru,r14 max (CMs of words comprising the
name of a World Heritage Site)

Fru,r15 ave (CMs of words comprising the
name of a World Heritage Site)

Fru,h1 If SRRru,1 is an affirmative response
(Stage 2 only)

Fru,h2 # of turns since this expert is acti-
vated

Fru,h3 # of denial responses recognized
since this expert is activated

Fru,h4 Fru,h4/Fru,h3
Fru,h5 If the previous system utterance is a

confirmation request to a user request

for starting a presentation

Fru,h6 If the previous system utterance is
an utterance to react to a non-

understandable user utterance

Fru,h7 If the system has made a confirma-
tion request to a user request for start-

ing a presentation since this expert

was activated

Fru,h8 If the system has made an utterance
to react to a non-understandable user

utterance since this expert was acti-

vated

Fru,h9 If the system has made a confirma-
tion request to a user request for start-

ing a presentation before

Fru,h10 If the system has made an utterance
to react to a non-understandable user

utterance before

expert Features

class

IP Fip,r10 If the SRRip,1 is out of database
Fip,r11

P

j
((# of keyphrases in SRRip,j) / (# of words in

SRRip,j) ) / (# of ASR results)

Fip,r12 mini( # of keyphrasei in SRRip,all / (# of ASR re-
sults))

Fip,r13 maxi( # of keyphrasei in SRRip,all / (# of ASR re-
sults))

Fip,r14 avg( CM of keyphrasei in SRRip,1)
Fip,r15 mini ( CM of keyphrasei in SRRip,1)
Fip,r16 maxi ( CM of keyphrasei in SRRip,1)
Fip,h1 If this expert has been activated before
Fip,h2 Same as Fru,h2
Fip,h3 If the previous system utterance is the final utter-

ance of the presentation

Fip,h4 If the previous system utterance is an utterance to
react to a user interruption

Fip,h5 Same as Fru,h6
Fip,h6 If the system has made the final utterance of the pre-

sentation since this expert was activated

Fip,h7 If the system has made an utterance to react to a user
interruption since this expert was activated

Fip,h8 Same as Fru,h8
Fip,h9 If the system has made the final utterance of the pre-

sentation before

Fip,h10 If the system has made an utterance to react to a user
interruption before

Fip,h11 Same as Fru,h10
QA Fqa,r10 Same as Fip,r12

Fqa,r11 Same as Fip,r13
Fqa,r12 Same as Fip,r14
Fqa,r13 Same as Fip,r15
Fqa,r14 Same as Fip,r16
Fqa,r15 Same as Fip,r17
Fqa,r16 If SRRqa,1 is an acknowledgment
Fqa,h1 Same as Fru,h1
Fqa,h2 Same as Fru,h2
Fqa,h3 Same as Fru,h3
Fqa,h4 Fqa,h4/Fqa,h3
Fqa,h5 If the previous system utterance is the final utter-

ance of an answer

Fqa,h6 Same as Fru,h6
Fqa,h7 If the system has made the final utterance of an an-

swer since this expert was activated

Fqa,h8 Same as Fru,h8
Fqa,h9 If the system has made the final utterance of an an-

swer before

Fqa,h10 Same as Fru,h10

SRRi,j means j-th speech recognition result with the language model (LM) for expert class i. SRRi,all means all the recognition
results in the n-best list. Fi,rx are speech understanding related features and Fi,hx are dialogue history related features. SRRlv,j
is an ASR result with a large-vocabulary (60,250 words) statistical model (Kawahara et al., 2004), which we used for utterance

verification. CM means confidence measure.

Table 5: Features used in the experiment.

28



expert class

(F1 score

obtained

after feature

selection)

remaining features (F1 score obtained

when each feature is removed)

RU(0.948) Fru,r9 (0.922), Fru,h8 (0.939), Fru,r5
(0.940), Fru,r14 (0.941), Fru,r2
(0.944), Fru,h9 (0.944), Fru,h5
(0.944), Fru,r13 (0.945), Fru,h10
(0.945), Fru,r10 (0.946), Fru,r8
(0.946), Fru,r7 (0.946)

IP(0.837) Fip,r7 (0.771), Fip,r6 (0.772), Fip,h9
(0.781), Fip,h7 (0.781), Fip,h11
(0.786), Fip,r4 (0.79), Fip,r2 (0.799),

Fip,r16 (0.809), Fip,r5 (0.809), Fip,r3
(0.809), Fip,h4 (0.809), Fip,r9 (0.814),

Fip,r15 (0.833), Fip,r12 (0.834), Fip,r13
(0.835), Fip,h10 (0.836)

QA(0.836) Fqa,r14 (0.813), Fqa,r7 (0.817),

Fqa,r16 (0.817), Fqa,r10 (0.818),

Fqa,h6 (0.820), Fqa,r6 (0.822), Fqa,r3
(0.831), Fqa,r5 (0.832)

Table 6: Features that remained after feature selection at

Stage 1 and their significances in terms of the F1 score

obtained when each feature is removed.

expert class

(F1 score

obtained

after feature

selection)

remaining features (F1 score obtained

when each feature is removed)

RU(0.773) Fru,r3 (0.728), Fru,a (0.737), Fru,h5
(0.743), Fru,h1 (0.751), Fru,r9 (0.754),

Fru,h10 (0.757), Fru,h8 (0.757),

Fru,r5 (0.758), Fru,r2 (0.759), Fru,r13
(0.762), Fru,r14 (0.763), Fru,h9
(0.767), Fru,r15 (0.768), Fru,r10
(0.768), Fru,h3 (0.772)

IP(0.827) Fip,h5 (0.808), Fip,r5 (0.809), Fip,r4
(0.810), Fip,r6 (0.811), Fip,a (0.812),

Fip,h4 (0.812), Fip,r13 (0.813), Fip,h3
(0.817), Fip,r15 (0.818), Fip,r3 (0.818),

Fip,h10 (0.819), Fip,r12 (0.820),

Fip,h7 (0.821), Fip,r11 (0.822), Fip,r10
(0.822), Fip,h8 (0.822), Fip,h6 (0.822),

Fip,r2 (0.824), Fip,r8 (0.824), Fip,h9
(0.824), Fip,h2 (0.825)

QA(0.873) Fqa,a (0.838), Fqa,r5 (0.857), Fqa,h1
(0.859), Fqa,r3 (0.862), Fqa,r6 (0.865),

Fqa,h8 (0.867), Fqa,r7 (0.868), Fqa,r15
(0.870), Fqa,r8 (0.870), Fqa,h7 (0.870),

Fqa,r12 (0.871), Fqa,r2 (0.871), Fqa,r16
(0.871), Fqa,h4 (0.871), Fqa,h3 (0.871),

Fqa,r11 (0.872), Fqa,h6 (0.872), Fqa,h5
(0.872)

Table 7: Features that remained after feature selection at

Stage 2 and their significances in terms of the F1 score

obtained when each feature is removed. Fru,a, Fip,a, and

Fqa,a are the maximum activation probabilities obtained

at Stage 1.

29


