










































Grounding Strategic Conversation: Using Negotiation Dialogues to Predict Trades in a Win-Lose Game


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 357–368,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Grounding Strategic Conversation:
Using negotiation dialogues to predict trades in a win-lose game

Anaı̈s Cadilhac
IRIT

Univ. Toulouse, France
cadilhac@irit.fr

Nicholas Asher
IRIT, CNRS

Toulouse, France
asher@irit.fr

Farah Benamara
IRIT

Univ. Toulouse, France
benamara@irit.fr

Alex Lascarides
School of Informatics
Univ. Edinburgh, UK
alex@inf.ed.ac.uk

Abstract

This paper describes a method that predicts
which trades players execute during a win-
lose game. Our method uses data collected
from chat negotiations of the game The Set-
tlers of Catan and exploits the conversation
to construct dynamically a partial model of
each player’s preferences. This in turn yields
equilibrium trading moves via principles from
game theory. We compare our method against
four baselines and show that tracking how
preferences evolve through the dialogue and
reasoning about equilibrium moves are both
crucial to success.

1 Introduction

Rational agents act so as to maximise their expected
utilities—an optimal trade off between what they
prefer and what they believe they can achieve (Sav-
age, 1954). Solving a game problem involves find-
ing equilibrium strategies: an optimal action for
each player that maximises his expected utility, as-
suming that the other players perform their speci-
fied action (Shoham and Leyton-Brown, 2009). Cal-
culating equilibria thus requires knowledge of the
other players’ preferences but almost all bargaining
games occur under the handicap of imperfect infor-
mation about this (Osborne and Rubinstein, 1994).
Players therefore try to extract their opponents’ pref-
erences from what they say, likewise revealing their
own preferences in their own utterances. These
elicited preferences guide an agent’s decisions, like
choosing to make such and such a bargain with such
and such a person. Tracking preferences through

dialogue is thus crucial for analyzing the agents’
strategic reasoning in real game scenarios.

In this paper, we design a model that maps what
people say in a win-lose game into a prediction of
exactly which players, if any, trade with each other,
and exactly what resources they exchange. We use
both statistics and logic: we use a corpus of nego-
tiation dialogues to learn classifiers that map each
utterance to its speech act and to other acts perti-
nent to bargaining; and we develop a symbolic al-
gorithm that, from the classifiers’ output, dynami-
cally constructs a model of each player’s preferences
as the conversation proceeds (for instance, the pref-
erence to receive a certain resource, or to accept a
certain trade). This preference model uses CP-nets
(Boutilier et al., 2004), a representation of prefer-
ences for which algorithms for computing equilib-
rium strategies exist. We adapt those algorithms to
predict the trades executed in the game.

The algorithm for construcing CP-nets uses only
the output of our classifiers, which in turn rely en-
tirely on shallow features in the raw text and robust
parsers. Together they provide an end to end model,
from raw text to a prediction of which trade, if any,
occurred. We evaluate the various components of
this (pipeline) algorithm separately, as well as the
end to end model.

Our study exploits a corpus of negotiation dia-
logues from an online version of the win lose game
The Settlers of Catan. Sections 2 and 3 describe
the corpus and its annotation. Section 4 introduces
our method for constructing the agents’ preferences
from the dialogues. We use this in Section 5 to pre-
dict whether a trade is executed as a result of the

357



players’ negotiations, and if so we predict who took
part in the trade, and what they exchanged. Our
method shows promising results, beating baselines
that don’t adequately track or reason about prefer-
ences. We compare our model to related work in
Section 6 and point to future work in Section 7.

2 The game

The Settlers of Catan (www.catan.com) is a win-
lose game that involves negotiations over restricted
resources. Each player (three or more) acquires re-
sources (of 5 types: ore, wood, wheat, clay, sheep),
which they use in different combinations to build
roads, settlements and cities, which in turn earns
them points towards winning. The first player to
10 points wins. Players acquire resources in sev-
eral ways, in particular through agreed trades with
other players. Some methods (e.g., robbing) are hid-
den from view, so players lack complete information
about their opponents’ resources.

Our corpus contains conversations of humans
playing an online version of Settlers (Afantenos et
al., 2012). Players must converse in a chat inter-
face to carry out trades. Each game contains several
dozen self-contained bargaining dialogues. Our ex-
periments use 10 Settlers games, consisting of more
than 2000 individual dialogue turns (see Section 3).

Table 1 is a sample dialogue from the corpus. The
sentences in the corpus have a relatively simple syn-
tax, though many also exhibit long distance depen-
dencies. However, these conversations are pragmat-
ically complex. They exhibit complex anaphoric de-
pendences (e.g., utterance ID 4 in Table 1). Other
pragmatic inferences, which are dependent on rea-
soning about intentions, speech acts and discourse
structure, are also ubiquitous. For example, the
question Have you got any ore? implies an offer for
the speaker to receive ore in exchange for something
from someone unspecified, and its response I’ve got
wheat not only implies a willingness to exchange
wheat for something, but as a response to the ques-
tion it also implies a refusal to give any ore.

More generally, a dialogue turn in our corpus can
express an offer, a counteroffer, an acceptance or re-
jection of an offer, or a commentary on the above
or on moves in the game. All except the last pro-
vide clues about preferences: e.g., which players

a speaker wants to execute a trade with; or what
resources to exchange. For instance, the utterance
Anybody have any sheep for wheat? conveys sev-
eral preferences. First, it conveys the speaker’s pref-
erence to trade with someone unspecified. Other
informative but underspecified preferences include:
the speaker’s preference to acquire some sheep over
alternatives; and in a context where she receives
sheep, a preference to give away some of her wheat
over the alternatives. Crucially, it does not convey
a preference to give away wheat in a context where
she receives nothing or something other than sheep.

In line with a non-cooperative bargaining game,
the preferences and offers that a speaker reveals are
less specific than an executable trade requires, where
the trading partners and the type of resources offered
and received must all be defined. Such general dia-
logue moves are essentially information seeking—
evidence that humans playing Settlers have imper-
fect information about their opponents’ preferences.
In fact, many offers to trade result in no trade be-
ing agreed to and executed. While observed negoti-
ation failure would be puzzling in a bargaining game
with perfect information (Osborne and Rubinstein,
1994), it occurs relatively frequently in Settlers.

3 Annotation

We have a multi-layered dialogue annotation
scheme that includes: (1) a pre-annotation that seg-
ments the dialogue into turns which are further seg-
mented into Elementary Discourse Units (EDUs)
with the author of each turn automatically given;
(2) a characterization of each EDU in terms of ba-
sic speech acts (assertion, question, request) as well
as dialogue acts that are specific to bargaining (of-
fers, counteroffers, etc.); and (3) associated infor-
mation about the givable and/or receivable resources
that EDUs express.

Two annotators received training on 77 dialogues,
totaling 699 EDUs. They then both annotated the
remaining dialogues independently (2741 EDUs and
511 dialogues in total). Kappas for inter-annotator
agreement are given below.

3.1 Dialogue act annotation (Kappa=0.79)

Each turn logs what a player enters in the chat win-
dow and also aspects of the game state at the time:

358



ID Dialogue Act Text Speaker Addressee Resource
1 Offer i need clay, any1 have? Rainbow All Receivable (clay, ?)
2 Refusal Nope, sorry inca Rainbow
3 Refusal Not at the moment, unfortunately. ariachiba Rainbow
4 Refusal need mine sorry Kittles Rainbow Not givable (Anaphoric, ?)

Anaphora Link:(mine , clay )
5 Offer no one has ore to giv? Rainbow All Receivable (ore, ?)
6 Accept oh yeah me Kittles Rainbow
7 Counteroffer ore for wheat again? Kittles Rainbow Givable (ore, ?) Receivable (wheat, ?)
8 Accept ya Rainbow Kittles
9 Accept ok Kittles Rainbow

Table 1: Example of an annotated negotiation dialogue.

his resources, the state of the game board and a time
stamp. The pre-annotation divides each turn into
EDUs. The annotators then have to specify the dia-
logue act of each EDU: Offer, Counteroffer, Accept
or Refusal (of an offer addressed to the emitter), and
Other. Other labels units that either comment on
strategic moves in the game or are not directly perti-
nent to bargaining. Annotators also specify the ad-
dressee of the EDU and its surface type: Question,
Request or Assertion.

3.2 Resource type annotation (Kappa=0.80)

Annotators also specify for each EDU and its dia-
logue act an associated feature structure, which cap-
tures (partial) information that the EDU expresses
about the type and quantity of resources that are of
the following four attributes: Givable, Not Givable,
Receivable or Not Receivable. These attributes can
take Boolean combinations of resources as values
via two operators AND and OR, that respectively
stand for conjunction (the agent expresses two pref-
erences and he prefers to achieve one of them if he
cannot have both, such as I need clay and wood) and
disjunction (free choice) of preferences (e.g., I can
give you clay or wood). We allow attributes to have
unknown values: the annotation tool inserts a ? in
these cases. We also insist that the annotators re-
solve anaphoric dependencies when specifying val-
ues to attributes, as shown in EDU (4) in Table 1.

4 Dialogue act and resource prediction

Predicting the executed trades from the dialogues
starts with three sub-tasks: automatically identify-
ing each EDU’s dialogue act; detecting the EDU’s
resources; and specifying the attributes of those re-
sources (i.e., Givable, Receivable, etc.).

4.1 Identifying dialogue acts

As is well established, one EDU’s dialogue act
depends on previous dialogue acts (Stolcke et al.,
2000). In our corpus, Accept or Reject frequently
follow Offer and Counteroffer. Since labeling is se-
quential, we use Conditional Random Fields (CRFs)
to learn dialogue acts. CRFs have been shown to
yield better results in dialogue act classification on
online chat than HMM-SVN and Naive Bayes (Kim
et al., 2012).

We use three types of features: lexical, syntactic
and semantic. And we exploit them as unigrams and
bigrams: unigrams associate the value of the feature
with the current output class (level 0); bigrams take
account of the value of the feature associated with
a combination of the current output class and pre-
vious output class (level -1). 6 features were used
exclusively as unigrams: the EDU’s position in the
dialogue, its first and last words, its subject lemma,
a boolean feature to indicate if the current speaker is
the one that initiates the dialogue and the position of
the speaker’s first turn in the dialogue.

We have 15 unigram and bigram features (at lev-
els 0 and -1), as well as templates that combine
feature values for the two levels. These include
14 boolean features that indicate if the EDU con-
tains: bargaining verbs (e.g. trade, offer), refer-
ences to another player (e.g. you), resource tokens
as encoded in a task dedicated lexicon (e.g. wheat,
clay), quantifiers (e.g. one, none), anaphoric pro-
nouns, occurrences of “for” prepositional phrases
(e.g. wheat for clay), acceptance words (e.g. OK),
negation words, emoticons, opinion words (from
(Benamara et al., 2011)), words of politeness, ex-
clamation marks, questions, and finally whether the
EDU’s speaker has talked previously in the dialogue.

359



The last feature gives the EDU speaker lemma. In
addition, 3 unigram and bigram booleans indicate
whether the current EDU contains the most frequent
tokens, couple of tokens and syntactic patterns in our
corpus. Finally, we use 2 composed bigram features
that encode whether the EDU contains an accep-
tance or refusal word, given that the previous EDU
is a question.

To assign sequential tags of dialogue acts within
a negotiation dialogue, we use the CRF++ tool
(crfpp.googlecode.com). Our data consists of
2741 EDUs in 511 dialogues. Each EDU is asso-
ciated with a dialogue act resulting in 410 Offer,
197 Counteroffer, 179 Accept, 398 Refusal and 1557
Other. We use 10-fold cross-validation to evalu-
ate our model, computing precision, recall and F-
score for each class and global accuracy from the
total number of true positives, false positives, false
negatives and true negatives obtained by summing
over all fold decisions. The results (in percent) are
given in Table 2 (MaF is the average of F-scores
of all the classes). Our model significantly out-
performs the frequency-based baseline (MaF=14.5;
Accuracy=56.8), with the best F-score achieved for
Other. The least good results are for the two least
frequent classes in our data. In addition to the fre-
quency problem, the lower score for Counteroffer is
mainly due to the model confusing it with Offer. Er-
rors in the Accept class were often due to misspelling
or to chat style conversation; e.g., kk, yup.

Dialogue act Precision Recall F-score
Other 87.4 93.1 90.1
Offer 80.0 81.0 80.5

Counterof. 64.8 53.3 58.5
Accept 65.1 53.1 58.5
Refusal 81.7 73.9 77.6

Macro-averaged F-score (MaF) 73.0
Accuracy 83.0

Table 2: Results for dialogue act classification.

4.2 Finding resource text spans

Since the resource vocabulary in The Settlers of
Catan is a closed set composed of words denoting
specific resources (e.g., clay, wood) and their syn-
onyms (brick), we use a simple rule to detect them:
a noun phrase (NP) is a resource text span if and

only if it contains a lemma from our resource lexi-
con. A closed set resource vocabulary is common to
many different types of negotiation dialogues. We
used the Stanford parser (Klein and Manning, 2003)
to obtain the NPs: there are 4361 NPs, where (by the
gold standard annotations) 21% are resources and
79% are not. We obtain an F-score of 96.9% and ac-
curacy of 97.9%, clearly beating both the frequency
and random baselines for this task.

4.3 Recognizing the type of resources

Recall that each resource within an EDU can be the
value of four types of attributes: Givable, Receiv-
able, Not Givable or Not Receivable (cf. Section
3.2). We predict these attributes using CRFs with the
following features. 8 features are used as unigram at
the current and the previous EDU level: the speaker,
the EDU’s subject, the dialogue act, and (if present)
the lemma of a bargaining verb, and 4 boolean fea-
tures indicate if the EDU contains an opinion word,
a reference to another speaker, if the resource comes
after a “for” and if it contains a refusal word. These
features also serve as bigrams at the current EDU
level. Additionally, we have a set of unigram and
bigram boolean features that indicate if the current
EDU contains the most frequent verbs in the corpus.
And finally, we use a feature that encodes the com-
bination subject/bargaining verb in the current EDU.

We used CRF++ to implement our classifier. Our
corpus data consists of 1077 Resources, split into
510 Receivable, 432 Givable, 116 Not Givable and
19 Not Receivable. We use again 10-fold cross-
validation to evaluate our model and compute the
results by summing over all fold decisions. We
present them (in percent) in Table 3. They beat
the frequency-based baseline (MaF=16.1; Accu-
racy=47.4), although performance on the Not Re-
ceivable class is poor probably due to its low fre-
quency in the data.

Ambiguities make this task challenging. For in-
stance, anyone wheat for clay? can mean that the
speaker wants to receive wheat and give clay or the
opposite, and resolving which meaning is intended
involves reasoning not only with the previous and/or
the following EDU, but also sometimes EDUs with
long distance attachments, which are not supported
by our classifier and require a full discourse parser.

360



Res. type Precision Recall F-score
Receivable 66.8 71.4 69.0

Givable 62.6 59.7 61.1
Not Giv. 88.1 89.7 88.9
Not Rec. 0 0 0
Macro-averaged F-score (MaF) 54.8

Accuracy 67.4

Table 3: Results for resource type classification.

5 Predicting Players’ Strategic Actions

We aim to capture the evolution of commitments to
certain preferences as the dialogue proceeds so as
to predict the agents’ bargaining behavior. In other
words, we wish to predict which of the 61 possi-
ble trade actions is executed at the end of each dia-
logue. The possible trades vary over which partner
the player whose turn it is trades with (3 options in a
4 player game), the resources exchanged (assuming
each partner gives one type of resource and receives
another type yields 5×4 = 20 possibilities), or there
is no trade; i.e., (3 × 20) + 1 = 61 possible actions
in the hypothesis space (we predict the types of re-
sources that are exchanged, but not their quantity).

We predict the executed action by identifying the
equilibrium trade entailed by the model of the play-
ers’ preferences, which in turn we construct dynam-
ically from the output of the classifiers in Section 4.
We use the attributes of resources in the EDUs (Giv-
able, etc.) to identify the preference that a speaker
conveys in the EDU, and we use the dialogue acts
(Offer, Accept, etc.) to update a model of the pref-
erences expressed so far in the dialogue with this
new preference (see Section 5.2). Our model of
preferences consists of a set of partial CP-nets, one
for each player (see Section 5.1 for details). The
resulting CP-nets are then used to infer the exe-
cuted trading action (if any) automatically, via well-
understood principles from game theory for identi-
fying rational behavior (Bonzon, 2007).

5.1 CP-Nets

Following Cadilhac et al. (2011), we use CP-nets
(Boutilier et al., 2004) to model preferences and
their dependencies. CP-nets are compatible with the
kind of partial information about preferences that ut-
terances reveal, and inference with CP-nets is com-

putationally efficient.
Just as Bayesian nets are a graphical model that

exploits probabilistic conditional independence to
provide a compact representation of a joint probabil-
ity distribution (Pearl, 1988), CP-nets are a graphi-
cal model that exploits conditional preferential in-
dependence to provide a compact representation of
the preference order over all outcomes. The CP-
net structures the decision maker’s preferences un-
der a ceteris paribus assumption: outcomes are com-
pared, other things being equal.

More formally, let V be a finite set of variables
whose combination of values determine all out-
comes O. Then a preference relation � over O is
a reflexive and transitive binary relation with strict
preference � defined as: o � o′ and o′ 6� o. Indif-
ference, written o ∼ o′, means o � o′ and o′ � o.
Definition 1 defines conditional preference indepen-
dence and Definition 2 defines CP-nets: the graphi-
cal component G of a CP-net specifies for each vari-
able X ∈ V its parent variables Pa(X) that affect
the agent’s preferences over the values of X , such
thatX is conditionally preferentially independent of
V \ ({X} ∪ Pa(X)) given Pa(X).
Definition 1 Let V be a set of variables, each vari-
able Xi with a domain D(Xi). Let {X,Y, Z} be
a partition of V . X is conditionally preferentially
independent of Y givenZ if and only if ∀z ∈ D(Z),
∀x1, x2 ∈ D(X) and ∀y1, y2 ∈ D(Y ), x1y1z �
x2y1z iff x1y2z � x2y2z.
Definition 2 NV = 〈G, T 〉 is a CP-net on variables
V , where G is a directed graph over V , and T is a
set of Conditional Preference Tables (CPTs). That
is, T = {CPT(Xj): Xj ∈ V }, where CPT(Xj)
specifies for each combination p of values of the par-
ent variables Pa(Xj) either p : xj � xj , p : xj �
xj or p : xj ∼ xj where the ¯̄ symbol sets the vari-
able to false.

We discuss below how a CP-net predicts rational
action, but first we describe how CP-nets are con-
structed from the dialogues. In the Settlers cor-
pus, preferences involve a quadruplet (o, a, <r,q>)
where: o is the preference owner, a is the ad-
dressee, r is the resource and q is its quantity. So
each variable in the CP-nets we construct is such a
quadruplet, and for each variable the possibles val-
ues are Givable (Giv), Not Givable (Giv), Receiv-

361



able (Rcv) and Not Receivable (Rcv).
For example, the utterance Anyone want to give

me a wheat for a clay? expresses two prefer-
ences: one for receiving wheat, represented by the
variable Pw = (A,All,<wheat,1>); and given this
preference, another for giving clay, represented by
Pc = (A,All,<clay,1>) (where A is the name of the
speaker). The corresponding CP-Net is Figure 1.

Pw

Pc

CPT(Pw) = Rcv � Rcv

CPT(Pc) = Rcv Pw : Giv � Giv

Figure 1: An example CP-net

5.2 Modeling players’ preferences
As stated above, we first automatically acquire a CP-
net from each EDU by using the EDU’s dialogue act
and the attributes (Givable, etc.) of its resources.
We then apply the rules presented in (Cadilhac et al.,
2011) to dynamically construct a preference model
of the dialogue overall: this uses an equivalence
between their coherence relations and our dialogue
acts. Our CP-nets reasoning model handles uncer-
tain information and noise because it use as input
only the outputs of the statistical models described
in Section 4, and these prior models handle uncer-
tain information and noise. The symbolic rules for
constructing CP-nets have complete coverage over
any possible combination of classes that are output
by the statistical models, and so they are robust. We
give our rules below where πi stands for EDU ID i.

Offers. Because an Offer may specify or refine an
existing preference or offer, we must model how the
preferences expressed in an EDU that’s an Offer up-
dates the prior declared preferences. So, while our
annotations treat Offer as a property of EDUs, we
treat them here as binary relations: Offer(π1, π2),
where the second term, π2, is the actual EDU whose
dialogue act is Offer and π1 is the set of EDUs oc-
curring between π2 and the last EDU uttered by
the same speaker. Offers then have a similar effect
on the CP-net as the coherence relation Elaboration
presented in (Cadilhac et al., 2011). That is, to auto-
matically update the CP-net constructed so far with
a current EDU that’s an Offer, the two step rule for
Offer(π1, π2) is:

1. to update the speaker’s CP-net according to the
preferences expressed in π1, and

2. if π2 expresses preferences, to enrich the CP-
net with these new preferences so that each
variable in π2 depends on each variable in π1.

Counteroffers. They specify or modify the terms
of a previous Offer or Counteroffer. Their purpose
is to give new information to refine the negotiation.
Like Offers they must also receive a contextually de-
pendent interpretation. The rule is quite similar to
that for Offer; however, Counteroffer can modify or
correct elements in a previously introduced offer. So
for Counteroffer(π1, π2), the rule is :

1. to partially update the speaker’s CP-net accord-
ing to the preferences expressed in π1 which do
not have the same resource type (Givable, Re-
ceivable) than the ones in π2.

2. same as step 2 Offer rule.

Accepts and Refusals. As they are answers to
Offers and Counteroffers, they behave like question
answer pairs (QAPs) presented in (Cadilhac et al.,
2011). Because we are not doing full discourse pars-
ing, we once again approximate its effects by mak-
ing Accepts and Refusals respond to the set of EDUs
between the current EDU and the speaker’s last turn.

Accepts are positive responses to Offers or Coun-
teroffers and are de facto similar to QAP(π1, π2)
where π2 is Yes. Thus, the rule is, as for Offer, to
update and enrich the CP-net.

Refusals are instead negative responses and be-
have like QAP(π1, π2) where π2 is No. For
Refusal(π1, π2), there is no update of the prefer-
ences expressed in π1. Instead, we enrich the CP-net
with the Non Givable and Non Receivable informa-
tion obtained from the negation of the preferences
expressed in the previous Offer or Counteroffer. We
then enrich the CP-net based on any new preferences
expressed in π2. If there is a conflict between the
value of a variable to be updated and the current
value in the CP-net, we apply the Correction rule:
all occurrences of the old value are replaced by the
new value in π2.

Other. This category pertains to content that does
not directly relate to trading in the game, and so we
choose to ignore resources expressed in the EDUs
with this dialogue act.

At the end of the negotiation dialogue, to predict
exactly what trade is executed (if any), the method

362



checks if there are complete and reciprocal prefer-
ences expressed in the CP-nets that respectively rep-
resent the declared preferences of two agents A and
B. This is done in two steps. First, we use the logic
of CP-nets to determine each agent’s best outcome
bestOA and bestOB from their respective CP-nets
(we’ll discuss how shortly). Secondly, we compare
these best outcomes: if they correspond to the same
trade, we predict that this trade was executed; if
not, we predict no trade is executed. Specifically,
bestOA (resp. bestOB) corresponds to a prefer-
ence for receiving a resource r1 from an agent B
(or from all the agents indifferently) and for giving
a resource r2 to this (or these) agent(s). We predict
that A gives B r2 and B gives A r1 if and only if:
bestOA = Rcv(A, B, r1) ∧ Giv(A, B, r2) and
bestOB = Rcv(B, A, r2) ∧Giv(B, A, r1).

The first step—computing each agent’s best out-
come from his CP-net—can be found in linear time
using the forward sweep algorithm (Boutilier et al.,
2004): sweep through the CP-net’s graph from top to
bottom, instantiating each variable with its preferred
value, given the values that are (already) assigned to
its parents. This algorithm is sound with respect to
the semantics of CP-nets.

Example. We apply this method for constructing
CP-nets and determining the executed trade to the
negotiation dialogue presented in Table 1.
π1 The EDU is an Offer, so Rainbow’s CP-net is

updated according to π1’s content.
CPT(R,All,<clay,?>) = Rcv � Rcv
π2 It’s a Refusal, so we update inca’s CP-net with

the negation of the preferences expressed in Rain-
bow’s offer.
CPT(I,R,<clay,?>) = Giv � Giv

π3 Idem for ariachiba.
CPT(A,R,<clay,?>) = Giv � Giv

π4 Idem for Kittles where the preferences ex-
pressed in this EDU are redundant with the negation
of the preferences in Rainbow’s offer.
CPT(K,R,<clay,?>) = Giv � Giv
π5 It’s an Offer, so Rainbow’s CP-net is first up-

dated according to previous EDUs (π2 to π4 until his
last speaking), then according to the content of π5.
CPT(R,All,<clay,?>) = Rcv � Rcv (inactive)
CPT(R,I,<clay,?>) = Rcv � Rcv
CPT(R,A,<clay,?>) = Rcv � Rcv

CPT(R,K,<clay,?>) = Rcv � Rcv
CPT(R,All,<ore,?>) = Rcv(R,I,<clay,?>) ∧ Rcv(R,A,
<clay,?>) ∧ Rcv(R,K,<clay,?>): Rcv � Rcv
The introduction of the preference to receive ore
conflicts with the prior one for receiving clay. So
the method adds to the associated CPT the label “in-
active” to indicate that this is older and should be
ignored if the preference about ore is satisfied.

π6 The EDU is an Accept, so Kittles’s CP-net is
updated according to previous EDUs (only π5).1

CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv � Giv
π7 The EDU is a Counteroffer. Since she is the

last speaker, her CP-net gets updated only according
to the content of the current EDU, to obtain:
CPT(K,R,<ore,?>) = Giv(K,R,<clay,?>): Giv � Giv
CPT(K,R,<wheat,?>) = Giv(K,R,<clay,?>) ∧
Giv(K,R, <ore,?>) : Rcv � Rcv

π8 The EDU is an Accept, so Rainbow’s CP-net
is updated according to previous EDUs (π6 and π7):
CPT(R,K,<ore,?>) = Rcv(R,I,<clay,?>) ∧ Rcv(R,A,
<clay,?>) ∧ Rcv(R,K,<clay,?>) : Rcv � Rcv
CPT(R,K,<wheat,?>) =Rcv(R,I,<clay,?>) ∧Rcv(R,A,
<clay,?>) ∧ Rcv(R,K,<clay,?>) ∧ Rcv(R,K,<ore,?>) :
Giv � Giv

π9 It’s an Accept with nothing new to update.
At the end of the dialogue, these agents’ CP-nets

(correctly) predict that Kittles gave ore to Rainbow
in exchange for wheat.

5.3 Evaluation and results

We compare our model against four baselines. Since
none of these baselines support reasoning about
equilibrium moves, they all rely on the presence of
an Accept act to predict there was a trade, and its
absence to predict there wasn’t. The baselines dif-
fer, however, in how they identify the trading part-
ners and resources in an executed trade. The first
baseline predicts a trade according to the first Of-
fer and the last person to Accept, and if the Offer
doesn’t specify one of the resources then it is chosen
randomly (similar random choices complete all par-
tial predictions in all the models we consider here):
e.g., for Table 1 this would predict that Kittles gave
clay to Rainbow (which is incorrect) in exchange for

1Due to lack of space, in the following CP-nets, we do not
copy the inactive CPTs and CPTs about Not Givable or Not Re-
ceivable resources.

363



something that’s chosen randomly (which will prob-
ably be incorrect). The second baseline uses the
last Offer and the last person to Accept: e.g., for
Table 1 this predicts that Kittles gave ore to Rain-
bow (correct) for something random (probably in-
correct). The third baseline uses the last Offer or
Counteroffer, whichever is latest, and the last per-
son to Accept: e.g., for Table 1 this correctly pre-
dicts that Kittles gave ore to Rainbow in exchange
for wheat. And the fourth baseline, uses default
unification between the prior Offers or Counterof-
fers and the current one to resolve any of the cur-
rent offer’s elided parts and to replace specific val-
ues in prior offers with conflicting specific values in
the current offer (Ehlen and Johnston, 2013). One
then takes the executed trade to be the result of this
unification process at the point where the last Accept
occurs. This makes the same predictions as the third
baseline for Table 1, but outperforms it in the corpus
example (1) by predicting the correct and complete
trade (i.e., Rainbow gave Kittles sheep for wheat,
rather than for something random):

(1) Rainbow: i need clay ore or wheat
Kittles: i got wheat
Rainbow: i cn giv sheep
Kittles: ok

We performed the evaluation on the data pre-
sented in Sections 3 and 4: 254 dialogues in total
since we ignore dialogues that contain only Others.
90 of these dialogues end with a trade being exe-
cuted and 2 of them end with 2 trades. A random
baseline would give 1.6% accuracy (given the 61
possible trading actions) and a frequency baseline
(always choose no trade) gives 64.1% accuracy.

Table 4 presents the accuracy figures for all the
models when calculated from the gold standard la-
bels rather than the classifiers’ predicted labels from
Section 4, so that we can compare the models in
isolation of the classifiers’ errors. McNemar’s test
shows that our model significantly outperforms all
the baselines (p < 0.05). A predicted trade counts as
correct only if it specifies the right participants and
the correct type of resources offered and received
(we ignore their quantity). True Positives (TP) are
thus examples where the model correctly predicts
not only that a trade happened, but also the correct
partners and resources; Wrong Positives (WP), on

the other hand, constitute a correct prediction that
there was a trade but errors on the partners and/or
resources involved (so WPs undermine accuracy).
True Negatives (TN) are examples where the model
correctly predicts there was no trade (so TPs and
TNs contribute to accuracy). False Positives (FP)
and False Negatives (FN) are respectively incorrect
predictions that there was a trade, or that there was
no trade.

While Table 4 does not reflect this, the first three
baselines tend to predict incomplete information
about the trade even when what they do predict is
correct: that is, they predict the correct addressee
and the owner but resort to random choice for a re-
source that’s missing from the Offer or Counterof-
fer that predicts which trade occurred. For the first
baseline 34 examples are like this; for the second
and third baselines it’s 32. In contrast, this prob-
lem occurs only once with the fourth baseline, and
all the trades predicted by our method are complete,
making random choice unnecessary. Moreover, the
first three baselines often make incorrect predictions
about the addressee or resources exchanged because
in contrast to our model and the fourth baseline, they
don’t track how potential trades evolve through a se-
quence of offers and counteroffers.

Even though the fourth baseline, which uses de-
fault unification to track the content of the current
offer, is smart and gives good results, it has statis-
tically significant lower accuracy than our model.
One major problem with the fourth baseline is that,
in contrast to our model, it does not track each
player’s attitude towards the current offer. Instead,
like all our baselines, it relies on the presence of an
Accept act to predict that there’s a trade.2 But sev-
eral corpus examples are like (2), in which a trade
is executed but there’s no Accept act, thus yielding a
False Negative (FN) for all four baselines:

(2) Joel: anyone have sheep or wheat
Cardlinger: neither :(
Joel: will give clay or ore
Euan: not just now
Jon: got a wheat for a clay
(Joel gives clay to Jon and receives wheat)

2We tried a baseline that doesn’t rely on the presence of an
Accept act, but rather predicts a trade whenever default unifica-
tion yields a complete offer. It performed worse than the fourth
baseline.

364



So overall, our analysis shows that using CP-nets
significantly outperforms all baselines that don’t
model how preferences evolve in the dialogue, and
error analysis yields evidence that our model outper-
forms the fourth baseline because our model sup-
ports reasoning about player preferences, rational
behavior and equilibrium strategies.

1st baseline: first Offer/last Accept
TP FP FN TN WP Accuracy
24 14 30 150 38 68.0

2nd baseline: last Offer/last Accept
TP FP FN TN WP Accuracy
29 6 32 158 31 73.0

3rd baseline: last (Counter)Offer/last Accept
TP FP FN TN WP Accuracy
39 4 23 160 30 77.7

4th baseline: default unification
TP FP FN TN WP Accuracy
64 4 23 160 5 87.5

Our method
TP FP FN TN WP Accuracy
75 4 15 160 2 91.8

Table 4: Results for trade prediction. TP, FP, FN, TN
and WP are the True and False Positives, False and True
Negatives and Wrong Positives.

Table 5 presents the results for the end to end
evaluation, where trade predictions are made from
the classifiers’ output from Section 4 rather than the
gold standard labels. As expected, performance de-
creases due to the classifiers’ errors, mainly on the
type of resources (Givable, etc.). But our method
still significantly outperforms all the baselines with
an accuracy of 73.4% when the baselines obtain val-
ues between 60.9% and 68.4%.

4th baseline: default unification
TP FP FN TN WP Accuracy
23 12 37 152 32 68.4

Our method
TP FP FN TN WP Accuracy
34 10 43 154 15 73.4

Table 5: Results for the end to end trade prediction.

6 Related Work

6.1 Dialogue act modeling

Most work on dialogue act modeling focuses on spo-
ken dialogue (Stolcke et al., 2000; Fernández et al.,
2005; Keizer et al., 2002). But live chats introduce
specific complications (Kim et al., 2012): ill-formed
data, abbreviations and acronyms, emotional indi-
cators and entanglement (especially for multi-party
chat). Among related work in this emerging field,
Joty et al. (2011) use unsupervised learning to model
dialogue acts in Twitter, Ivanovic (2008) and Kim et
al. (2010) analyze one-to-one online chat in a cus-
tomer service domain, and Wu et al. (2002) and Kim
et al. (2012) predict dialogue acts in a multi-party
setting. We used a similar classifier to predict dia-
logue acts as the one reported in (Kim et al., 2012)
and evaluation yields similar results.

This paper proposes an approach to dialogue act
identification in online chat that aims to predict
strategic actions like bargaining. Compared to (Sid-
ner, 1994) and DAMSL (Core and Allen, 1997), our
domain level annotation is much more detailed: we
not only predict moves like Accept but also features
like the Givable and Receivable resources. Our gen-
eral speech act typology of EDUs lacks intentional
descriptions of speech acts, however. This reflects
a conscious choice to specify the semantics of each
act purely by the public commitments made to offer
or to receive goods.

6.2 Preference extraction

While preference extraction from non-linguistic ac-
tions is well studied (Chen and Pu, 2004; Fürnkranz
and Hüllermeier, 2011), their extraction from spon-
taneous conversation has received little attention. To
our knowledge, the only existing work is (Asher
et al., 2010; Cadilhac et al., 2011; Cadilhac et al.,
2012) which we build on. Cadilhac et al. (2011)
compute CP-nets from coherence relations, found in
the annotation of the Verbmobil corpus (Baldridge
and Lascarides, 2005). Here we adapt their algo-
rithm from coherence relations to unary dialogue
acts. Further, while they assume that preferences are
given, here we apply versions of the NLP techniques
from Cadilhac et al (2012) to estimate the prefer-
ences of EDUs automatically. And we go further
than any of these works by using the elicited pref-

365



erences to infer the domain-level actions that result
from information exchanged in the conversation.

In this respect, our work relates to models for
grounding language, where semantic parsing tech-
niques are used to automatically map linguistic in-
structions to domain-level actions (Artzi and Zettle-
moyer, 2013; Kim and Mooney, 2013). Our do-
main of application is more challenging, however:
to our knowledge, this is the first attempt to map
non-cooperative dialogues into predictions about
domain-level actions. We can tackle these strategic
scenarios because we exploit a logic of preferences
as part of our model, yielding inferences about ratio-
nal action even when agents’ preferences conflict.

Compared to previous work, our task is new. Our
aim is not to predict what dialogue act to perform
next, but what non verbal action should be per-
formed, mapping dialogue acts to non verbal ac-
tions. The difference between our work and other
work on grounding is that we are grounding non-
cooperative dialogue rather than instructions in a co-
operative setting. There is no prior work of which
we’re aware that maps a non-cooperative dialogue
into a prediction about which joint non-verbal ac-
tion the agents will do as a result of what they’ve
learned about their opponent through conversation.
Furthermore, both the CP-net and the fourth base-
line, whose accuracy is quite high (making it a hard
baseline to beat), use the dialogue history as they in-
crementally build up the preference model.

6.3 Predicting strategic actions
Modeling player behavior in real-time strategy
games is a growing research area in AI. These mod-
els can be used to identify common strategic states,
discover new strategies as they emerge or predict
an opponents future actions and so help players to
optimize their choices. For example, Schadd et
al. (2007) develop a hierarchical opponent model in
the game Spring, Dereszynski et al. (2011) reason
about strategic behavior in StarCraft using hidden
Markov models and Amato and Shani (2010) use re-
inforcement learning to acquire a policy for switch-
ing among high-level strategies in Civilization IV.

In comparison, we propose a novel approach for
predicting strategic action based on the symbolically
formalized preferences that each agent commits to in
spontaneous conversation. Our approach thus deals

with imperfect information by exploiting the agents’
declared preferences. By predicting what bargain (if
any) will take place, we are able to verify the cor-
rectness of our preference descriptions. Our task is
a subtask of learning a strategy over an entire game
space, but our approach yields good predictive re-
sults on relatively little data—an advantage of ex-
ploiting CP-nets and the symbolic rules that guide
their evolution from observable evidence.

7 Conclusion

We have proposed a linguistic approach to strategy
prediction in spontaneous conversation, exploiting
dialogue acts to build a partial model of the agents’
declared preferences. Our method tracks how pref-
erences evolve during the dialogue, which we use to
infer their bargaining behavior, i.e. what resources,
if any, are exchanged, and by whom.

We based our study on a corpus collected using an
online version of The Settlers of Catan. Negotiations
in this game mirror complex real life negotiations
and provide a fruitful arena to study strategic con-
versation. Evaluation shows that our approach pro-
vides more accurate and complete information about
trades than baselines that don’t track how an offer
evolves through the dialogue, and we also argued
that game-theoretic reasoning about rational behav-
ior has advantages over relying on the presence or
absence of an Accept act to make predictions.

Our approach, however, does not exploit dis-
course structure, which is needed to properly handle
long distance dependencies of offers on prior mate-
rial. We will exploit this in future work to improve
our results. We also plan to investigate other aspects
of strategic reasoning on a larger dataset.

We have proposed a method that relies on a typol-
ogy of dialogue acts that is domain sensitive. How-
ever, in other work we have shown how to adapt
our algorithms to several domains (Cadilhac et al.,
2012). In future work, we plan to link our prefer-
ence extraction algorithms to an automatically ac-
quired discourse structure for a given text. This will
provide a domain independent means for extracting
preferences from dialogue.

Acknowledgments

This work is supported by ERC grant 269427 STAC.

366



References

Stergos Afantenos, Nicholas Asher, Farah Benamara,
Anaı̈s Cadilhac, Cédric Dégremont, Pascal Denis,
Markus Guhe, Simon Keizer, Alex Lascarides, Oliver
Lemon, Philippe Muller, Soumya Paul, Verena Rieser,
and Laure Vieu. 2012. Developing a corpus of strate-
gic conversation in the settlers of catan. In Pro-
ceedings of the 1st Workshop on Games and NLP
(GAMNLP-12).

Christopher Amato and Guy Shani. 2010. High-
level reinforcement learning in strategy games. In
Proceedings of the 9th International Conference on
Autonomous Agents and Multiagent Systems (AA-
MAS’10), pages 75–82.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1:49–62.

Nicholas Asher, Elise Bonzon, and Alex Lascarides.
2010. Extracting and modelling preferences from dia-
logue. In IPMU, pages 542–553.

Jason Baldridge and Alex Lascarides. 2005. Annotating
discourse structures for robust semantic interpretation.
In Proceedings of the 6th IWCS.

Farah Benamara, Baptiste Chardon, Yannick Mathieu,
and Vladimir Popescu. 2011. Towards context-based
subjectivity analysis. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1180–1188, Chiang Mai, Thailand.

Elise Bonzon. 2007. Modélisation des interactions en-
tre agents rationnels : les jeux booléens. PhD thesis,
Université Paul Sabatier, Toulouse.

Craig Boutilier, Craig Brafman, Carmel Domshlak, Hol-
ger H. Hoos, and David Poole. 2004. Cp-nets: A tool
for representing and reasoning with conditional ceteris
paribus preference statements. Journal of Artificial In-
telligence Research, 21:135–191.

Anaı̈s Cadilhac, Nicholas Asher, Farah Benamara, and
Alex Lascarides. 2011. Commitments to preferences
in dialogue. In Proceedings of SIGDIAL, pages 204–
215. ACL.

Anaı̈s Cadilhac, Nicholas Asher, Farah Benamara,
Vladimir Popescu, and Mohamadou Seck. 2012. Pref-
erence extraction from negotiation dialogues. In Eu-
ropean Conference on Artificial Intelligence (ECAI),
pages 211–216. IOS Press.

Li Chen and Pearl Pu. 2004. Survey of preference elici-
tation methods. Technical report.

Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the DAMSL annotation scheme. In Work-
ing Notes of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines.

Ethan W. Dereszynski, Jesse Hostetler, Alan Fern,
Thomas G. Dietterich, Thao-Trang Hoang, and Mark
Udarbe. 2011. Learning probabilistic behavior mod-
els in real-time strategy games. In AIIDE.

Patrick Ehlen and Michael Johnston. 2013. A multi-
modal dialogue interface for mobile local search. In
IUI Companion, pages 63–64.

Raquel Fernández, Jonathan Ginzburg, and Shalom Lap-
pin. 2005. Using machine learning for non-sentential
utterance classification. In Proceedings of the 6th SIG-
dial Workshop on Discourse and Dialogue, pages 77–
86.

Johannes Fürnkranz and Eyke Hüllermeier, editors.
2011. Preference Learning. Springer.

Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts. In
Masters thesis, The University of Melbourne.

Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in asyn-
chronous conversations. In Proceedings of the 22nd
International Joint Conference on Artificial Intelli-
gence, pages 1807–1813.

Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with bayesian net-
works for dutch dialogues. In Proceedings of the 3rd
SIGdial Workshop on Discourse and Dialogue, pages
88–94. Association for Computational Linguistics.

Joohyun Kim and Raymond J. Mooney. 2013. Adapting
discriminative reranking to grounded language learn-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (ACL-
2013), Sofia, Bulgaria.

Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin.
2010. Classifying dialogue acts in 1-to-1 live chats.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 862–
871.

Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats. In 26th Pacific Asia Conference on Lan-
guage,Information and Computation, pages 463–472.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423–430.

Martin Osborne and Ariel Rubinstein. 1994. A Course in
Game Theory. MIT Press.

Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kauffmann.

Leonard Savage. 1954. The Foundations of Statistics.
John Wiley.

367



Frederik Schadd, Sander Bakkes, and Pieter Spronck.
2007. Opponent modeling in real-time strategy games.
In Games and Simulation GAMEON, pages 61–68.

Yoav Shoham and Kevin Leyton-Brown. 2009. Multia-
gent Systems: Algorithmic, Game-Theoretic and Logi-
cal Foundations. Cambridge University Press.

Candace Sidner. 1994. An artificial discourse language
for collaborative negotiation. In AAAI, volume 1,
pages 814–819. MIT Press, Cambridge.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol V. Ess-dykema, and Marie
Meteer. 2000. Dialogue act modeling for automatic
tagging and recognition of conversational speech. In
Computational Linguistics, pages 26:339–373.

Tianhao Wu, Faisal M. Khan, Todd A. Fisher, Lori A.
Shuler, and William M. Pottenger. 2002. Posting
act tagging using transformation-based learning. In
Foundations of Data Mining and knowledge Discov-
ery, pages 319–331. Springer.

368


