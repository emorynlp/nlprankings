Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 662–670,

Beijing, August 2010

662

Adaptive Development Data Selection for Log-linear Model

in Statistical Machine Translation

Mu Li

Microsoft Research Asia
muli@microsoft.com

Yinggong Zhao∗
Nanjing University

zhaoyg@nlp.nju.edu.cn

Dongdong Zhang

Microsoft Research Asia

Ming Zhou

Microsoft Research Asia

dozhang@microsoft.com

mingzhou@microsoft.com

Abstract

This paper addresses the problem of dy-
namic model parameter selection for log-
linear model based statistical machine
translation (SMT) systems. In this work,
we propose a principled method for this
task by transforming it to a test data de-
pendent development set selection prob-
lem. We present two algorithms for au-
tomatic development set construction, and
evaluated our method on several NIST
data sets for the Chinese-English trans-
lation task. Experimental results show
that our method can effectively adapt
log-linear model parameters to different
test data, and consistently achieves good
translation performance compared with
conventional methods that use a ﬁxed
model parameter setting across different
data sets.

1

Introduction

In recent years, log-linear model (Och and Ney,
2002) has been a mainstream method to formu-
late statistical models for machine translation. Us-
ing this formulation, various kinds of relevant
properties and data statistics used in the transla-
tion process, either on the monolingual-side or on
the bilingual-side, are encoded and used as real-
valued feature functions, thus it provides an ef-
fective mathematical framework to accommodate
a large variety of SMT formalisms with different
computational linguistic motivations.

∗This work was done while the author was visiting Mi-

crosoft Research Asia.

Formally, in a log-linear SMT model, given a
source sentence f, we are to ﬁnd a translation e∗
with largest posterior probability among all possi-
ble translations:

e∗ = argmax

e

Pr(e|f )

and the posterior probability distribution Pr(e|f )
is directly approximated by a log-linear formula-
tion:

(1)

=

Pr(e|f ) = pλ(e|f )
exp(PM
Pe0 exp(PM

m=1 λmhm(e, f ))

m=1 λmhm(e0, f ))

in which hm’s are feature functions and λ =
(λ1, . . . , λM ) are model parameters
(feature
weights).

For a successful practical

log-linear SMT
model, it is usually a combined result of the sev-
eral efforts:

• Construction of well-motivated SMT models
• Accurate estimation of feature functions
• Appropriate scaling of log-linear model fea-

tures (feature weight tuning).

In this paper, we focus on the last mentioned
issue – parameter tuning for log-linear model.
In general, log-linear model parameters are opti-
mized on a held-out development data set. Us-
ing this method, similarly to many machine learn-
ing tasks, the model parameters are solely tuned
based on the development data, and the optimal-
ity of obtained model on unseen test data relies
on the assumption that both development and test
data observe identical probabilistic distribution,

663

which often does not hold for real-world data. The
goal of this paper is to investigate novel meth-
ods for test data dependent model parameter se-
lection. We begin with discussing the principle
of parameter learning for log-linear SMT models,
and explain the rationale of task transformation
from parameter selection to development data se-
lection. We describe two algorithms for automatic
development set construction, and evaluated our
method on several NIST MT evaluation data sets.
Experimental results show that our method can ef-
fectively adapt log-linear model parameters to dif-
ferent test data and achieves consistent good trans-
lation performance compared with conventional
methods that use a group of ﬁxed model param-
eters across different data sets.

2 Model Learning for SMT with

Log-linear Models

Model learning refers to the task to estimate a
group of suitable log-linear model parameters
λ = (λ1, . . . , λM ) for use in Equation 1, which is
often formulated as an optimization problem that
ﬁnds the parameters maximizing certain goodness
of the translations generated by the learnt model
on a development corpus D. The goodness can be
measured with either the translations’ likelihood
or speciﬁc machine translation evaluation metrics
such as TER or BLEU.

More speciﬁcally, let e∗ be the most probable
translation of D with respect to model parameters
λ, and E(e∗, λ, D) be a score function indicating
the goodness of translation e∗, then a parameter
estimation algorithm will try to ﬁnd the λ which
satisﬁes:

λ∗ = argmax

E(e∗, λ, D)

(2)

λ

Note when the goodness scoring function E(·)
is speciﬁed, the parameter learning criterion in
Equation 2 indicates that the derivation of model
parameters λ∗ only depends on development data
D, and does not require any knowledge of test
data T . The underlying rationale for this rule is
that if the test data T observes the same distribu-
tion as D, λ∗ will be optimal for both of them.

On the other side, however, when there are mis-
matches between development and test data, the

translation performance on test data will be sub-
optimal, which is very common for real-world
data. Due to the difference between data sets, gen-
erally there is no such λ∗ that is optimal for multi-
ple data sets at the same time. Table 1 shows some
empirical evidences when two data sets are mutu-
ally used as development and test data. In this set-
ting, we used a hierarchical phrase based decoder
and 2 years’ evaluation data of NIST Chinese-
to-English machine translation task (for the year
2008 only the newswire subset was used because
we want to limit both data sets within the same do-
main to show that data mismatch also exists even
if there is no domain difference), and report re-
sults using BLEU scores. Model parameters were
tuned using the MERT algorithm (Och, 2003) op-
timized for BLEU metric.

Dev data MT05 MT08-nw
0.402
MT05
MT08-nw 0.372

0.306
0.343

Table 1: Translation performance of cross devel-
opment/test on two NIST evaluation data sets.

In our work, we present a solution to this prob-
lem by using test data dependent model parame-
ters for test data translation. As discussed above,
since model parameters are solely determined by
development data D, selection of log-linear model
parameters is basically equivalent to selecting a
set of development data D.

However, automatic development data selection
in current SMT research remains a relatively open
issue. Manual selection based on human experi-
ence and observation is still a common practice.

3 Adaptive Model Parameter Selection
An important heuristic behind manual develop-
ment data selection is to use the dataset which is
as similar to test set as possible in order to work
around the data mismatch problem to maximal
extent. There are also empirical evidences sup-
porting this heuristics. For instance, it is gener-
ally perceived that data set MT03 is more similar
to MT05, while MT06-nw is closer to MT08-nw.
Table 2 shows experimental results using model
parameters induced from MT03 and MT06-nw as

664

development sets with the same settings as in Ta-
ble 1. As expected, MT06-nw is far more suitable
than MT03 as the development data for MT08-
nw; yet for test set MT05, the situation is just the
opposite.

Dev data MT05 MT08-nw
0.397
MT03
MT06-nw 0.381

0.306
0.337

Table 2: Translation performance on different test
sets of using different development sets.

In this work, this heuristic is further exploited
for automatic development data selection when
there is no prior knowledge of the test data avail-
able. In the following discussion, we assume the
availability of a set of candidate source sentences
together with translation references that are qual-
iﬁed for the log-linear model parameter learning
task. Let DF be the full candidate set, given a test
set T , the task of selecting a set of development
data which can optimize the translation quality on
T can be transformed to searching for a suitable
subset of DF which is most similar to T :

D∗ = argmax
D⊆DF

Sim(D, T )

To achieve this goal, we need to address the fol-
lowing key issues:

• How to deﬁne and compute Sim(D, T ), the

similarity between different data sets;

• How to extract development data sets from a

full candidate set for unseen test data.

3.1 Dataset Similarity
Computing document similarity is a classical task
in many research areas such as information re-
trieval and document classiﬁcation. However, typ-
ical methods for computing document similarity
may not be suitable for our purpose. The reasons
are two-fold:

1. The sizes of both development and test data
are small in usual circumstances, and using
similarity measures such as cosine or dice
coefﬁcient based on term vectors will suffer
from severe data sparseness problems. As a

result, the obtained similarity measure will
not be statistically reliable.

2. More importantly, what we care about here
is not the surface string similarity. Instead,
we need a method to measure how similar
two data sets are from the view of a log-linear
SMT model.

Next we start with discussing the similarity
between sentences. Given a source sentence
f, we denote its possible translation space with
H(f ).
In a log-linear SMT model, every trans-
lation e ∈ H(f ) is essentially a feature vector
h(e) = (h1, . . . , hM ). Accordingly, the similar-
ity between two sentences f1 and f2 should be de-
ﬁned on the feature space of the model in use. Let
V (f ) = {h(e) : e ∈ H(f )} be the set of feature
vectors for all translations in H(f ), we have
Sim(f1, f2) = Sim(cid:18)V (f1), V (f2)(cid:19)

(3)

Because it is not practical to compute Equation
3 directly by enumerating all translations in H(f1)
and H(f2) due to the huge search space in SMT
tasks, we need to resort to some approximations.
A viable solution to this is that if we can use a
single feature vector ˜h(f ) to represent V (f ), then
Equation 3 can be simply computed using existing
vector similarity measures.

One reasonable method to derive ˜h(f ) is to use
a feature vector based on the average principle –
each dimension of the vector is set to the expec-
tation of its corresponding feature value over all
translations:

˜h(f ) = Xe∈H(f )

P (e|f )h(e)

(4)

An alternative and much simpler way to com-
pute ˜h(f ) is to employ the max principle in which
we just use the feature vector of the best transla-
tion in H(f ):

˜h(f ) = h(e∗)

(5)

where e∗ = argmaxe P (e|f ).
Note that in both Equation 4 and Equation 5
we make use of e’s posterior probability P (e|f ).

665

Since the true distribution is unknown, a pre-
learnt model M has to be used to assign approxi-
mate probabilities to translations, which indicates
that the obtained similarity depends on a speciﬁc
model. As a convention, we use SimM(f1, f2) to
denote the similarity between f1 and f2 based on
M, and call M the reference model of the com-
puted similarity. To avoid unexpected bias caused
by a single reference model, multiple reference
models can be simultaneously used, and the simi-
larity is deﬁned to be the maximum of all model-
dependent similarity values:

Sim(f1, f2) = max
M

SimM(f1, f2)

(6)

where M belongs to {M1, . . . ,Mn}, which is
the set of reference models under consideration.
To generalize this method to data set level, we
compute the vector ˜h(S) for a data set S =
(f1, . . . , f|S|) as follows:
|S|Xi=1

˜h(S) =

˜h(fi)

(7)

3.2 Development Sets Pre-construction
In the following, we sketch a method for automat-
ically building a set of development data based on
the full candidate set DF before seeing any test
data.

Theoretically, a subset of DF containing ran-
domly sampled sentences from DF will not meet
our requirement well because it is very probable
that it will observe a distribution similar to DF .
What we expect is that the pre-built development
sets can approximate as many as possible typi-
cal data distributions that can be estimated from
subsets of DF . Our solution is based on the as-
sumption that DF can be depicted by some mix-
ture models, hence we can use classical cluster-
ing methods such as k-means to partition DF into
subsets with different distributions.

Let SF be the set of extracted development data
from DF . The construction of SDF proceeds as
following:

1. Train a log-linear model MF using DF as

development data;

2. Compute a feature vector ˜h(d)1 for each sen-
tence d ∈ DF using MF as reference model;
3. Cluster sentences in DF using ˜h(d)/|d| as

feature vectors;

4. Add obtained sentence clusters to SDF as

candidate development sets.

In the third step, since the feature vector ˜h(d)
is deﬁned at sentence level, it is averaged by the
number of words in d so that it is irrelevant to the
length of a sentence. Considering the outputs of
unsupervised data clustering methods are usually
sensitive to initial conditions, we include in SDF
sentence clusters based on different initialization
conﬁgurations to remove related random effects.
An initialization conﬁguration for sentence clus-
tering in our work includes starting point for each
cluster and total number of clusters. In fact, the
inclusion of more sentence clusters increases the
diversity of the resulted SDF as well.

At decoding time, when a test set T is pre-
sented, we compute the similarity between T and
each development set D ∈ SDF , and choose the
one with largest similarity score as the develop-
ment set for T :

D∗ = argmax
D∈SDF

Sim(T, D)

(8)

When a single reference model is used to com-
pute Sim(T, D), MF is a natural choice. In the
multi-model setting as shown in Equation 6, mod-
els learnt from the development sets in SDF can
serve this purpose.

Note in this method model learning is not re-
quired for every new test set because the model
parameters for each development set in SDF can
also be pre-learnt and ready to be used for decod-
ing.

3.3 Dynamic Development Set Construction
In the previous method, test data T is only in-
volved in the process of choosing a development
set from a list of candidates but not in process of
development set construction. Next we present a

1Throughout this paper, a development sentence d gener-
ally refers to the source part of it if there is no extra explana-
tion.

666

method for building a development set on demand
based on test data T .

Let DF = (d1, . . . , dn) be the data set con-
taining all candidate sentences for development
data selection. The method is iterative process in
which development data and learnt model are al-
ternatively updated. Detailed steps are illustrated
as follows:

1. Let i = 0, D0 = DF ;
2. Train a model Mi based on Di;
3. For each dk ∈ DF , compute the similarity
score SimMi(T, dk) between T and dk based
on model Mi;

4. Select top n candidate sentences with highest

similarity scores from DF to form Di+1;

5. Repeat step 2 to step 4 until the similarity be-
tween T and latest selected development data
converges (the increase in similarity measure
is less than a speciﬁed threshold compared to
last round) or the speciﬁed iteration limit is
reached.

In step 4, Di+1 is greedily extracted from DF ,
and there is no guarantee that SimMi(T, Di+1)
will increase or decrease after a new sentence is
added to Di+1. Thereby the number of selected
sentences n needs to be empirically determined.
If n is too small, neither the selected data nor the
learnt model parameters will be statistically reli-
able; while if n is too large, we may have to in-
clude some sentences that are not suitable for test
data in the development data, and miss the oppor-
tunity to extract the most desirable development
set.

One drawback of this method is the relatively
high computational cost because it requires multi-
ple parameter training passes when any test set is
presented to the system for translation.

4 Experiments
4.1 Data
Experiments were conducted on the data sets
used for NIST Chinese-English machine transla-
tion evaluation tasks. MT03 and MT06 data sets,

which contain 919 and 1,664 sentences respec-
tively, were used for development data in vari-
ous settings. MT04, MT05 and MT08 data sets
were used for test purpose. In some settings, we
also used a test set MT0x, which containing 1,000
sentences randomly sampled from the above 3
data sets. All the translation performance results
were measured in terms of case-insensitive BLEU
scores.

For all experiments, all parallel corpora avail-
able to the constrained track of NIST 2008
Chinese-English MT evaluation task were used
for translation model training, which consist of
around 5.1M bilingual sentence pairs. GIZA++
was used for word alignment in both directions,
which was further reﬁned with the intersec-diag-
grow heuristics.

We used a 5-gram language model which was
trained from the Xinhua portion of English Giga-
word corpus version 3.0 from LDC and the En-
glish part of parallel corpora.

4.2 Machine Translation System
We used an in-house implementation of the hierar-
chical phrase-based decoder as described in Chi-
ang (2005).
In addtion to the standard features
used in Chiang (2005), we also used a lexicon fea-
ture indicating how many word paris in the trans-
lation found in a conventional Chinese-English
lexicon. Phrasal rules were extracted from all the
parallel data, but hierarchical rules were only ex-
tracted from the FBIS part of the parallel data
which contains around 128,000 sentence pairs.
For all the development data, feature weights of
the decoder were tuned using the MERT algorithm
(Och, 2003).

4.3 Results of Development Data

Pre-construction

In the following we ﬁrst present some overall re-
sults using the method of development data pre-
construction, then dive into more detailed settings
of the experiments.

Table 3 shows the results using 3 different data
sets for log-linear model parameter tuning. El-
ements in the ﬁrst column indicate the data sets
used for parameter tuning, and other columns con-
tain evaluation results on different test sets. In the

667

Tuning set

MT04

MT05

MT08

MT0x

MT03
MT06

MT03+MT06
Oracle cluster
Self-training

0.399 / 0.392
0.381 / 0.388
0.391 / 0.401

0.395 / 0.390
0.382 / 0.391
0.392 / 0.397

0.241 / 0.258
0.275 / 0.283
0.265 / 0.281

0.319 / 0.322
0.343 / 0.342
0.336 / 0.345

0.401
0.406

0.398
0.402

0.293
0.298

0.345
0.351

Table 3: Translation performance using different methods and data sets for parameter tuning.

third row of the table, MT03+MT06 means com-
bining the data sets of MT03 and MT06 together
to form a larger tuning set. The ﬁrst number in
each cell denotes the BLEU score using the tuning
set as standard development set D, and the second
for using the tuning set as a candidate set DF .

For all experiment settings in the table, we
used cosine value between feature vectors to mea-
sure similarity between data sets, and feature vec-
tors were computed according to Equation 5 and
Equation 7 using a reference model which is
trained on the corresponding candidate set DF as
development set.2 We adopted the k-means algo-
rithm for data clustering with the number of clus-
ters iterating from 2 to 5. In each iteration, we ran
4 passes of clustering using different initial values.
Therefore, in total there are 56 sentence clusters
generated in each SDF .3

From the table it can be seen that given
the same set of sentences (MT03, MT06 and
MT03+MT06), when they are used as the can-
didate set DF for the development set pre-
construction method, the translation performance
is generally better than when they are just used as
development sets as a whole. Using MT03 data
set as DF is an exception: there is slight perfor-
mance drop on test sets MT04 and MT05, but it
also helps reduce the performance see-saw prob-
lem on different test sets as shown in Table 1.
Meanwhile, in the other two settings of DF , we
observed signiﬁcant BLEU score increase on all
test sets but MT0x (on which the performance al-
most kept unchanged). In addition, the fact that
using MT03+MT06 as DF achieves best (or al-
2For example, in all the experiments in the row of MT03
as DF , we use the same reference model trained with MT03
as development set.

3Sometimes some clusters are empty or contain too few

sentences, so the actual number may be smaller.

most best) performance on all test sets implies that
it should be a better choice to include as diverse
data as possible in DF .

We also appended two oracle BLEU numbers
for each test set in Table 3 for reference. One is
denoted with oracle cluster, which is the high-
est possible BLEU that can be achieved on the
test set when the development set must be cho-
sen from the sentence clusters in SM T 03+M T 06.
The other is labeled as self-training, which is the
BLEU score that can be obtained when the test
data itself is used as development data. This num-
ber can serve as actual performance upper bound
on the test set.

Next we investigated the impact of using dif-
ferent ways to compute feature vectors presented
in Section 3.1. We re-ran some previous exper-
iments on test sets MT04, MT05 and MT08 us-
ing MT03+MT06 as DF . Most settings were kept
unchanged except that the feature vector of each
sentence was computed according to Equation 4.
A 20-best translation list was used to approximate
H(f ). The results are shown in Table 4.

Test set
MT04
MT05
MT08

average max
0.401
0.397
0.397
0.393
0.286
0.281

Table 4: Translation performance when using av-
eraged feature values for similarity computation.

The numbers in the second column are based
on Equation 4. Numbers based on Equation 5 are
also listed in the third column for comparison. In
all the experiment settings we did not observe con-
sistent or signiﬁcant advantage when using Equa-
tion 4 over using Equation 5. Since Equation 5

668

is much simpler, it is a good decision to use it in
practice. So did we conduct all following experi-
ments based on Equation 5.

We are also interested in the correlation be-
tween two measures: the similarity between de-
velopment and test data and the actual translation
performance on test data.

First we would like to echo the motivating ex-
periment presented in Section 3. Table 5 shows
the similarity between the data sets used in the ex-
periment with MMT03+MT06 as reference model.
Obviously the results in Table 2 and Table 5 ﬁt
each other very well.

Dev data MT05 MT08-nw
MT03
0.99012
0.99728

0.99988
MT06-nw 0.99004

Table 5: Similarity between NIST data sets.

Figure 1 shows the results of a set of more com-
prehensive experiments on MT05 data set con-
cerning the similarity between development and
test sets.

using Equation 6, and the reference model set con-
tains all models learnt from the development sets
in SM T 03+M T 06. The other two settings use refer-
ence models learnt from MT06 and MT03+MT06
data sets respectively.

We can observe from the ﬁgure that the corre-
lation between BLEU scores and data set similar-
ity can only be identiﬁed on macro scales for all
the three similarity settings. Although using data
similarity may not be able to select the perfect de-
velopment data set from SDF , by picking a devel-
opment set with highest similarity score, we can
usually (almost always) get good enough BLEU
scores in our experiments.

4.4 Results of Development Data Dynamic

Generation

We ran two sets of experiments for the method of
development data dynamic construction.

The ﬁrst one was designed to investigate how
the size of extracted development data affects the
translation performance. Using MT05 and MT08
as test sets and MT03+MT06 as DF , we ran ex-
periments for the algorithm presented in Section
3.3 with n = 200 to n = 1, 000.
In this ex-
periment we did not observe signiﬁcant enough
changes in BLEU scores – the difference between
the highest and lowest numbers is generally less
than 0.005.

The second one aimed at examining how BLEU
numbers changes when the extracted development
data were iteratively updated. Figure 2 shows one
set of results on test sets MT05 and MT08 using
MT03+MT06 data set as DF and n set to 400.

Figure 1: Correlation between similarity and
BLEU on MT05 data set

In the ﬁgure, every data line shows how BLEU
score changes when different pre-built develop-
ment set in SM T 03+M T 06 is used for model learn-
ing. The data points in each line are sorted by the
rank of similarity between the development set in
use and the MT05 data set. We also compared re-
sults based on 3 reference model settings. In the
ﬁrst one (multiple), the similarity was computed

Figure 2: BLEU score as function of iteration in
dynamic development data extraction.

The similarity usually converged after 2 to 3 it-

U
E
L
B

 0.4

 0.38

 0.36

 0.34

 0.32

 0.3

 0.28

Multiple
MT03+MT06
MT06

 10

 20

 30

 40

 50

 60

Rank of development set

U
E
L
B
 
5
0
T
M

 0.405
 0.4
 0.395
 0.39
 0.385
 0.38
 0.375
 0.37
 0.365

MT05
MT08

 1

 2

 3

 4

 5

 6

 7

Iteration

 0.305
 0.3
 0.295
 0.29
 0.285
 0.28
 0.275
 0.27
 0.265

U
E
L
B
 
8
0
T
M

669

erations, which is consistent with trend of BLEU
scores on test sets. However, in all our experimen-
tal settings, we did not observe any results signif-
icantly better than using the development set pre-
construction method.

5 Discussions
Some of the previous work related to building
adaptive SMT systems were discussed in the do-
main adaptation context, in which one fundamen-
tal idea is to estimate a more suitable domain-
speciﬁc translation model or language model.
When the target domain is already known, adding
a small amount of domain data (both monolingual
and bilingual) to the existing training corpora has
been shown to be very effective in practice. But
model adaptation is required in more scenarios
other than explicitly deﬁned domains. As shown
by the results in Table 2, even for the data from
the same domain, distribution mismatch can also
be a problem.

There are also considerable efforts made to deal
with the unknown distribution of text to be trans-
lated, and the research topics were still focused on
translation and language model adaptation. Typ-
ical methods used in this direction include dy-
namic data selection (L¨u et al., 2007; Zhao et al.,
2004; Hildebrand et al., 1995) and data weighting
(Foster and Kuhn, 2007; Matsoukas et al., 2009).
All the mentioned methods use information re-
trieval techniques to identify relevant training data
from the entire training corpora.

Our work presented here also makes no as-
sumption about the distribution of test data, but
it differs from the previous methods signiﬁcantly
from a log-linear model’s perspective. Adjust-
ing translation and language models based on test
data can be viewed as adaptation of feature val-
ues, while our method is essentially adaptation of
feature weights. This difference makes these two
kinds of methods complementary to each other —
it is possible to make further improvement by us-
ing both of them in one task.

To our knowledge, there is no dedicated discus-
sion on principled methods to perform develop-
In L¨u
ment data selection in previous research.
et al.
(2007), log-linear model parameters can
also be adjusted at decoding time. But in their

approach, the adjustment was based on heuristic
rules and re-weighted training data distribution.
In addition, compared with training data selection,
the computational cost of development data selec-
tion is much smaller.

From machine learning perspective, both pro-
posed methods can be viewed as certain form
of transductive learning applied to the SMT task
(Uefﬁng et al., 2007). But our methods do
not rely on surface similarities between training
and training/development sentences, and develop-
ment/test sentences are not used to re-train SMT
sub-models.

6 Conclusions and Future Work

In this paper, we addressed the data mismatch is-
sue between training and decoding time of log-
linear SMT models, and presented principled
methods for dynamically inferring test data de-
pendent model parameters with development set
selection. We describe two algorithms for this
task, development set pre-construction and dy-
namic construction, and evaluated our method
on the NIST data sets for the Chinese-English
translation task. Experimental results show that
our methods are capable of consistently achiev-
ing good translation performance on multiple
test sets with different data distributions without
manual tweaking of log-linear model parameters.
Though theoretically using the dynamic construc-
tion method could bring better results, the pre-
construction method performs comparably well in
our experimental settings. Considering the fact
that the pre-consruction method is computation-
ally cheaper, it should be a better choice in prac-
tice.

In the future, we are interested in two direc-
tions. One is to explore the possibility to perform
data clustering on test set as well and choosing
suitable model parameters for each cluster sepa-
rately. The other involves dynamic SMT model
selection – for example, some parts of the test
data ﬁt the phrase-based model better while other
parts can be better translated using a syntax-based
model.

670

Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING. Geneva, Switzerland.

References
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of
the Association
for Computational Linguistic (ACL). Ann Arbor,
Michigan.

the 43th Annual Meeting of

George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second
ACL Workshop on Statistical Machine Translation..
Prague, Czech Republic.

Michel Galley, Mark Hopkins, Kevin Knight and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of the Human Language Technology Conf.
(HLT-NAACL). Boston, Massachusetts.

Almut Hildebrand, Matthias Eck, Stephan Vogel, and
Alex Waibel. 1995. Adaptation of the Transla-
tion Model for Statistical Machine translation Based
on Information Retrieval. In Proc. of EAMT. Bu-
dapest, Hungary.

Philipp Koehn, Franz Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL).
Edmonton, Canada.

Yang Liu, Qun Liu, and Shouxun Lin. 2007. Tree-
to-string alignment template for statistical machine
translation.
In Proc. of the 45th Annual Meet-
ing of the Association for Computational Linguistic
(ACL). Prague, Czech Republic.

Yajuan L¨u, Jin Huang and Qun Liu. 2007.

Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of the Conference on Empirical Methods in Natural
Language Processing. Prague, Czech Republic.

Spyros Matsoukas, Antti-Veikko I. Rosti and Bing
Zhang. 2009. Discriminative Corpus Weight Es-
timation for Machine Translation.
In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Singapore.

Franz Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistic (ACL). Philadelphia, PA.

Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistic (ACL). Sapporo, Japan.

Nicola Uefﬁng, Gholamreza Haffari and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguis-
tics. Prague, Czech Republic.

