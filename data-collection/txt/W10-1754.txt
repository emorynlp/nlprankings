










































TESLA: Translation Evaluation of Sentences with Linear-Programming-Based Analysis


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354–359,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

TESLA: Translation Evaluation of Sentences with
Linear-programming-based Analysis

Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore

2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg

Abstract

We present TESLA-M and TESLA, two
novel automatic machine translation eval-
uation metrics with state-of-the-art perfor-
mances. TESLA-M builds on the suc-
cess of METEOR and MaxSim, but em-
ploys a more expressive linear program-
ming framework. TESLA further exploits
parallel texts to build a shallow seman-
tic representation. We evaluate both on
the WMT 2009 shared evaluation task and
show that they outperform all participating
systems in most tasks.

1 Introduction

In recent years, many machine translation (MT)
evaluation metrics have been proposed, exploiting
varying amounts of linguistic resources.

Heavyweight linguistic approaches including
RTE (Pado et al., 2009) and ULC (Giménez and
Màrquez, 2008) performed the best in the WMT
2009 shared evaluation task. They exploit an ex-
tensive array of linguistic features such as parsing,
semantic role labeling, textual entailment, and dis-
course representation, which may also limit their
practical applications.

Lightweight linguistic approaches such as ME-
TEOR (Banerjee and Lavie, 2005), MaxSim
(Chan and Ng, 2008), wpF and wpBleu (Popović
and Ney, 2009) exploit a limited range of linguis-
tic information that is relatively cheap to acquire
and to compute, including lemmatization, part-of-
speech (POS) tagging, and synonym dictionaries.

Non-linguistic approaches include BLEU (Pap-
ineni et al., 2002) and its variants, TER (Snover et
al., 2006), among others. They operate purely at
the surface word level and no linguistic resources
are required. Although still very popular with MT
researchers, they have generally shown inferior
performances than the linguistic approaches.

We believe that the lightweight linguistic ap-
proaches are a good compromise given the current
state of computational linguistics research and re-
sources. In this paper, we devise TESLA-M and
TESLA, two lightweight approaches to MT eval-
uation. Specifically: (1) the core features are F-
measures derived by matching bags of N-grams;
(2) both recall and precision are considered, with
more emphasis on recall; and (3) WordNet syn-
onyms feature prominently.

The main novelty of TESLA-M compared to
METEOR and MaxSim is that we match the N-
grams under a very expressive linear programming
framework, which allows us to assign weights to
the N-grams. This is in contrast to the greedy ap-
proach of METEOR, and the more restrictive max-
imum bipartite matching formulation of MaxSim.

In addition, we present a heavier version
TESLA, which combines the features using a lin-
ear model trained on development data, making
it easy to exploit features not on the same scale,
and leaving open the possibility of domain adapta-
tion. It also exploits parallel texts of the target lan-
guage with other languages as a shallow semantic
representation, which allows us to model phrase
synonyms and idioms. In contrast, METEOR and
MaxSim are capable of processing only word syn-
onyms from WordNet.

The rest of this paper is organized as follows.
Section 2 gives a high level overview of the eval-
uation task. Sections 3 and 4 describe TESLA-M
and TESLA, respectively. Section 5 presents ex-
perimental results in the setting of the WMT 2009
shared evaluation task. Finally, Section 6 con-
cludes the paper.

2 Overview

We consider the task of evaluating machine trans-
lation systems in the direction of translating the
source language to the target language. Given a
reference translation and a system translation, the

354



goal of an automatic machine translation evalua-
tion algorithm such as TESLA(-M) is to output a
score predicting the quality of the system transla-
tion. Neither TESLA-M nor TESLA requires the
source text, but as additional linguistic resources,
TESLA makes use of phrase tables generated from
parallel texts of the target language and other lan-
guages, which we refer to as pivot languages. The
source language may or may not be one of the
pivot languages.

3 TESLA-M

This section describes TESLA-M, the lighter
one among the two metrics. At the highest
level, TESLA-M is the arithmetic average of F-
measures between bags of N-grams (BNGs). A
BNG is a multiset of weighted N-grams. Math-
ematically, a BNG B consists of tuples (bi, bWi ),
where each bi is an N-gram and bWi is a posi-
tive real number representing its weight. In the
simplest case, a BNG contains every N-gram in a
translated sentence, and the weights are just the
counts of the respective N-grams. However, to
emphasize the content words over the function
words, we discount the weight of an N-gram by
a factor of 0.1 for every function word in the N-
gram. We decide whether a word is a function
word based on its POS tag.

In TESLA-M, the BNGs are extracted in the tar-
get language, so we call them bags of target lan-
guage N-grams (BTNGs).

3.1 Similarity functions
To match two BNGs, we first need a similarity
measure between N-grams. In this section, we
define the similarity measures used in our exper-
iments.

We adopt the similarity measure from MaxSim
as sms. For unigrams x and y,

• If lemma(x) = lemma(y), then sms = 1.

• Otherwise, let

a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))

where I(·) is the indicator function, then
sms = (a + b)/2.

The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than En-
glish, a synonym dictionary is used instead.

We define two other similarity functions be-
tween unigrams:

slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))

All the three unigram similarity functions general-
ize to N-grams in the same way. For two N-grams
x = x1,2,...,n and y = y1,2,...,n,

s(x, y) =

{
0 if ∃i, s(xi, yi) = 0
1
n

∑n
i=1 s(x

i, yi) otherwise

3.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input the following:

1. Two BNGs, X and Y . The ith entry in X
is xi and has weight xWi (analogously for yj
and yWj ).

2. A similarity measure, s, that gives a similar-
ity score between any two entries in the range
of 0 to 1.

Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall simi-
larity. As translations often contain one-to-many
or many-to-many alignments, we allow one entry
to split its weight among multiple alignments. An
example matching problem is shown in Figure 1a,
where the weight of each node is shown, along
with the similarity for each edge. Edges with a
similarity of zero are not shown. The solution to
the matching problem is shown in Figure 1b, and
the overall similarity is 0.5 × 1.0 + 0.5 × 0.6 +
1.0× 0.2 + 1.0× 0.1 = 1.1.

Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges

w(xi, yj) ∀i, j

We maximize∑
i,j

s(xi, yj)w(xi, yj)

subject to

w(xi, yj) ≥ 0 ∀i, j∑
j

w(xi, yj) ≤ xWi ∀i∑
i

w(xi, yj) ≤ yWj ∀j

1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.

355



w=1.0 w=0.8 w=0.2 w=0.1

w=1.0 w=0.8 w=0.1

w=0.2

s=0.5 s=1.0s=0.5 s=1.0

(a) The matching problem

w=1.0 w=0.8 w=0.2 w=0.1

w=1.0 w=0.8 w=0.1

w=0.2

w=1.0 w=0.2w=0.6 w=0.1

(b) The solution

Figure 1: A BNG matching problem

The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have

Precision =
S∑
j y

W
j

Recall =
S∑
i x

W
i

The F-measure is derived from the precision and
the recall:

F =
Precision× Recall

α× Precision + (1− α)× Recall

In this work, we set α = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.

3.3 Scoring
The TESLA-M sentence-level score for a refer-
ence and a system translation is the arithmetic av-
erage of the BTNG F-measures for unigrams, bi-
grams, and trigrams based on similarity functions
sms and spos. We thus have 3× 2 = 6 features for
TESLA-M.

We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.

3.4 Reduction
When every xWi and y

W
j is 1, the linear program-

ming problem proposed above reduces to weighted
bipartite matching. This is a well known result;
see for example, Cormen et al. (2001) for details.

This is the formalism of MaxSim, which precludes
the use of fractional weights.

If the similarity function is binary-valued
and transitive, such as slem and spos, then
we can use a much simpler and faster greedy
matching procedure: the best match is simply∑

g min(
∑

xi=g
xWi ,

∑
yi=g

yWi ).

4 TESLA

Unlike the simple arithmetic average used in
TESLA-M, TESLA uses a general linear com-
bination of three types of features: BTNG F-
measures as in TESLA-M, F-measures between
bags of N-grams in each of the pivot languages,
called bags of pivot language N-grams (BPNGs),
and normalized language model scores of the sys-
tem translation, defined as 1n log P , where n is
the length of the translation, and P the language
model probability. The method of training the lin-
ear model depends on the development data. In
the case of WMT, the development data is in the
form of manual rankings, so we train SVM rank

(Joachims, 2006) on these instances to build the
linear model. In other scenarios, some form of re-
gression can be more appropriate.

The rest of this section focuses on the genera-
tion of the BPNGs. Their matching is done in the
same way as described for BTNGs in the previous
section.

4.1 Phrase level semantic representation

Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al., 2006; Haghighi et al., 2009).

We observe that the distribution of aligned
phrases in a pivot language can serve as a se-
mantic representation of a target language phrase.
That is, if two target language phrases are often
aligned to the same pivot language phrase, then
they can be inferred to be similar in meaning.
Similar observations have been made by previous
researchers (Bannard and Callison-Burch, 2005;
Callison-Burch et al., 2006; Snover et al., 2009).

We note here two differences from WordNet
synonyms: (1) the relationship is not restricted to
the word level only, and (2) the relationship is not
binary. The degree of similarity can be measured
by the percentage of overlap between the seman-
tic representations. For example, at the word level,

356



the phrases good morning and hello are unrelated
even with a synonym dictionary, but they both very
often align to the same French phrase bonjour, and
we conclude they are semantically related to a high
degree.

4.2 Segmenting a sentence into phrases

To extend the concept of this semantic represen-
tation of phrases to sentences, we segment a sen-
tence in the target language into phrases. Given a
phrase table, we can approximate the probability
of a phrase p by:

Pr(p) =
N(p)∑
p′ N(p′)

(1)

where N(·) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:

Pr(p1, p2, . . . , pn|S) =
1

Z(S)

n∏
i=1

Pr(pi) (2)

where Z(S) is a normalizing constant. The seg-
mentation of S that maximizes the probability can
be determined efficiently using a dynamic pro-
gramming algorithm. The formula has a strong
preference for longer phrases, as every Pr(p) is
a small fraction. To deal with out-of-vocabulary
(OOV) words, we allow any single word w to be
considered a phrase, and if N(w) = 0, we set
N(w) = 0.5 instead.

4.3 BPNGs as sentence level semantic
representation

Simply merging the phrase-level semantic rep-
resentation is insufficient to produce a sensible
sentence-level semantic representation. As an ex-
ample, we consider two target language (English)
sentences segmented as follows:

1. ||| Hello , ||| Querrien ||| . |||

2. ||| Morning , sir . |||

A naive comparison of the bags of aligned pivot
language (French) phrases would likely conclude
that the two sentences are completely unrelated,
as the bags of aligned phrases are likely to be
completely disjoint. We tackle this problem by
constructing a confusion network representation
of the aligned phrases, as shown in Figures 2 and

Bonjour , / 0.9

Salut , / 0.1

Querrien / 1.0 . / 1.0

Figure 2: A confusion network as a semantic rep-
resentation

Bonjour , monsieur . / 1.0

Figure 3: A degenerate confusion network as a se-
mantic representation

3. A confusion network is a compact representa-
tion of a potentially exponentially large number of
weighted and likely malformed French sentences.
We can collect the N-gram statistics of this ensem-
ble of French sentences efficiently from the confu-
sion network representation. For example, the tri-
gram Bonjour , Querrien 2 would receive a weight
of 0.9 × 1.0 = 0.9 in Figure 2. As with BTNGs,
we discount the weight of an N-gram by a factor
of 0.1 for every function word in the N-gram, so
as to place more emphasis on the content words.

The collection of all such N-grams and their
corresponding weights forms the BPNG of a sen-
tence. The reference and system BPNGs are then
matched using the algorithm outlined in Section
3.2.

4.4 Scoring
The TESLA sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, (2) BPNG F-measures for un-
igrams, bigrams, and trigrams based on similar-
ity functions slem and spos for each pivot lan-
guage, and (3) normalized language model scores.
In this work, we use two language models. We
thus have 3 × 2 features from the BTNGs, 3 ×
2 × #pivot languages features from the BPNGs, and
2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.

5 Experiments

5.1 Setup
We test our metrics in the setting of the WMT
2009 evaluation task (Callison-Burch et al., 2009).
The manual judgments from WMT 2008 are used

2Note that the N-gram can span more than one segment.

357



as the development data and the metric is evalu-
ated on WMT 2009 manual judgments with re-
spect to two criteria: sentence level consistency
and system level correlation.

The sentence level consistency is defined as the
percentage of correctly predicted pairs among all
the manually judged pairs. Pairs judged as ties
by humans are excluded from the evaluation. The
system level correlation is defined as the average
Spearman’s rank correlation coefficient across all
translation tracks.

5.2 Pre-processing
We POS tag and lemmatize the texts using the fol-
lowing tools: for English, OpenNLP POS-tagger3

and WordNet lemmatizer; for French and German,
TreeTagger4; for Spanish, the FreeLing toolkit
(Atserias et al., 2006); and for Czech, the Morce
morphological tagger5.

For German, we additionally perform noun
compound splitting. For each noun, we choose the
split that maximizes the geometric mean of the fre-
quency counts of its parts, following the method in
(Koehn and Knight, 2003):

max
n,p1,p2,...,pn

[
n∏

i=1

N(pi)

] 1
n

The resulting compound split sentence is then POS
tagged and lemmatized.

Finally, we remove all non-alphanumeric tokens
from the text in all languages. To generate the lan-
guage model features, we train SRILM (Stolcke,
2002) trigram models with modified Kneser-Ney
discounting on the supplied monolingual Europarl
and news commentary texts.

We build phrase tables from the supplied news
commentary bitexts. Word alignments are pro-
duced by the Berkeley aligner. The widely used
phrase extraction heuristic in (Koehn et al., 2003)
is used to extract phrase pairs and phrases of up to
4 words are collected.

5.3 Into-English task
For each of the BNG features, we generate three
scores, for unigrams, bigrams, and trigrams re-
spectively. For BPNGs, we generate one such
triple for each of the four pivot languages supplied,
namely Czech, French, German, and Spanish.

3opennlp.sourceforge.net
4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5ufal.mff.cuni.cz/morce/index.php

System
correlation

Sentence
consistency

TESLA 0.8993 0.6324
TESLA-M 0.8718 0.6097

ulc 0.83 0.63
maxsim 0.80 0.62

meteor-0.6 0.72 0.50

Table 1: Into-English task on WMT 2009 data

Table 1 compares the scores of TESLA and
TESLA-M against three participants in WMT
2009 under identical settings6: ULC (a heavy-
weight linguistic approach with the best per-
formance in WMT 2009), MaxSim, and ME-
TEOR. The results show that TESLA outperforms
all these systems by a substantial margin, and
TESLA-M is very competitive too.

5.4 Out-of-English task

A synonym dictionary is required for target lan-
guages other than English. We use the freely avail-
able Wiktionary dictionary7 for each language.
For Spanish, we additionally use the Spanish
WordNet, a component of FreeLing.

Only one pivot language (English) is used for
the BPNG. For the English-Czech task, we only
have one language model instead of two, as the
Europarl language model is not available.

Tables 2 and 3 show the sentence-level consis-
tency and system-level correlation respectively of
TESLA and TESLA-M against the best reported
results in WMT 2009 under identical setting. The
results show that both TESLA and TESLA-M
give very competitive performances. Interestingly,
TESLA and TESLA-M obtain similar scores in the
out-of-English task. This could be because we use
only one pivot language (English), compared to
four in the into-English task. We plan to inves-
tigate this phenomenon in our future work.

6 Conclusion

This paper describes TESLA-M and TESLA. Our
main contributions are: (1) we generalize the
bipartite matching formalism of MaxSim into a
more expressive linear programming framework;

6The original WMT09 report contained erroneous results.
The scores here are the corrected results released after publi-
cation.

7www.wiktionary.org

358



en-fr en-de en-es en-cz Overall

TESLA 0.6828 0.5734 0.5940 0.5519 0.5796

TESLA-M 0.6390 0.5890 0.5927 0.5656 0.5847

wcd6p4er 0.67 0.58 0.61 0.59 0.60

wpF 0.66 0.60 0.61 n/a 0.61

terp 0.62 0.50 0.54 0.31 0.43

Table 2: Out-of-English task sentence-level con-
sistency on WMT 2009 data

en-fr en-de en-es en-cz Overall

TESLA 0.8529 0.7857 0.7272 0.3141 0.6700

TESLA-M 0.9294 0.8571 0.7909 0.0857 0.6657

wcd6p4er -0.89 0.54 -0.45 -0.1 -0.22

wpF 0.90 -0.06 0.58 n/a n/a

terp -0.89 0.03 -0.58 -0.40 -0.46

Table 3: Out-of-English task system-level correla-
tion on WMT 2009 data

(2) we exploit parallel texts to create a shallow se-
mantic representation of the sentences; and (3) we
show that they outperform all participants in most
WMT 2009 shared evaluation tasks.

Acknowledgments

This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.

References
J. Atserias, B. Casas, E. Comelles, M. González,

L. Padró, and M. Padró. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source NLP
library. In Proceedings of LREC.

S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or
Summarization.

C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of ACL.

C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of HLT-NAACL.

C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009

Workshop on Statistical Machine Translation. In
Proceedings of WMT.

Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proceedings of ACL.

T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein,
2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.

C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.

J. Giménez and L. Màrquez. 2008. A smorgasbord of
features for automatic MT evaluation. In Proceed-
ings of the Third WMT.

A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG mod-
els. In Proceedings of ACL-IJCNLP.

T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of KDD.

P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of EACL.

P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACL.

P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.

F.J. Och and N. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).

S. Pado, M. Galley, D. Jurafsky, and C.D. Man-
ning. 2009. Robust machine translation evaluation
with entailment features. In Proceedings of ACL-
IJCNLP.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.

M. Popović and H. Ney. 2009. Syntax-oriented eval-
uation measures for machine translation output. In
Proceedings of WMT.

M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.

M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of WMT.

A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP.

359


