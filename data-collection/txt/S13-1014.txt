










































DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103–108, Atlanta, Georgia, June 13-14, 2013. c©2013 Association for Computational Linguistics

DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level

Semantic Similarity Estimation

Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,

Shrikanth Narayanan1

1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece

malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,

shri@sipi.usc.edu

Abstract

This paper describes our submission for the

*SEM shared task of Semantic Textual Sim-

ilarity. We estimate the semantic similarity

between two sentences using regression mod-

els with features: 1) n-gram hit rates (lexical

matches) between sentences, 2) lexical seman-

tic similarity between non-matching words, 3)

string similarity metrics, 4) affective content

similarity and 5) sentence length. Domain

adaptation is applied in the form of indepen-

dent models and a model selection strategy

achieving a mean correlation of 0.47.

1 Introduction

Text semantic similarity estimation has been an ac-

tive research area, thanks to a variety of potential ap-

plications and the wide availability of data afforded

by the world wide web. Semantic textual similar-

ity (STS) estimates can be used for information ex-

traction (Szpektor and Dagan, 2008), question an-

swering (Harabagiu and Hickl, 2006) and machine

translation (Mirkin et al., 2009). Term-level simi-

larity has been successfully applied to problems like

grammar induction (Meng and Siu, 2002) and affec-

tive text categorization (Malandrakis et al., 2011). In

this work, we built on previous research and our sub-

mission to SemEval’2012 (Malandrakis et al., 2012)

to create a sentence-level STS model for the shared

task of *SEM 2013 (Agirre et al., 2013).

Semantic similarity between words has been

well researched, with a variety of knowledge-based

(Miller, 1990; Budanitsky and Hirst, 2006) and

corpus-based (Baroni and Lenci, 2010; Iosif and

Potamianos, 2010) metrics proposed. Moving to

sentences increases the complexity exponentially

and as a result has led to measurements of simi-

larity at various levels: lexical (Malakasiotis and

Androutsopoulos, 2007), syntactic (Malakasiotis,

2009; Zanzotto et al., 2009), and semantic (Rinaldi

et al., 2003; Bos and Markert, 2005). Machine trans-

lation evaluation metrics can be used to estimate lex-

ical level similarity (Finch et al., 2005; Perez and

Alfonseca, 2005), including BLEU (Papineni et al.,

2002), a metric using word n-gram hit rates. The pi-

lot task of sentence STS in SemEval 2012 (Agirre et

al., 2012) showed a similar trend towards multi-level

similarity, with the top performing systems utilizing

large amounts of partial similarity metrics and do-

main adaptation (the use of separate models for each

input domain) (Bär et al., 2012; Šarić et al., 2012).

Our approach is originally motivated by BLEU

and primarily utilizes “hard” and “soft” n-gram hit

rates to estimate similarity. Compared to last year,

we utilize different alignment strategies (to decide

which n-grams should be compared with which).

We also include string similarities (at the token and

character level) and similarity of affective content,

expressed through the difference in sentence arousal

and valence ratings. Finally we added domain adap-

tation: the creation of separate models per domain

and a strategy to select the most appropriate model.

2 Model

Our model is based upon that submitted for the same

task in 2012 (Malandrakis et al., 2012). To esti-

mate semantic similarity metrics we use a super-

vised model with features extracted using corpus-

103



based word-level similarity metrics. To combine

these metrics into a sentence-level similarity score

we use a modification of BLEU (Papineni et al.,

2002) that utilizes word-level semantic similarities,

string level comparisons and comparisons of affec-

tive content, detailed below.

2.1 Word level semantic similarity

Co-occurrence-based. The semantic similarity be-

tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,

1990): I(i, j) = log p̂(i,j)
p̂(i)p̂(j) , where p̂(i) and p̂(j) are

the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence

is denoted by p̂(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al., 2012)

we employed a modification of the pointwise mutual

information based on the maximum sense similar-

ity assumption (Resnik, 1995) and the minimization

of the respective error in similarity estimation. In

particular, exponential weights α were introduced in
order to reduce the overestimation of denominator

probabilities. The modified metric Ia(i, j), is de-
fined as:

Ia(i, j)=
1

2

[

log
p̂(i, j)

p̂α(i)p̂(j)
+ log

p̂(i, j)

p̂(i)p̂α(j)

]

. (1)

The weight α was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word

sense coverage in the semantic neighborhood of

each word. The Ia(i, j) metric using the estimated
value of α = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets

(Rubenstein and Goodenough, 1965; Miller and

Charles, 1998; Finkelstein et al., 2002).

Context-based: The fundamental assumption

behind context-based metrics is that similarity

of context implies similarity of meaning (Harris,

1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) =

vi.vj
||vi|| ||vj ||

.

The elements of feature vectors can be weighted

according various schemes [(Iosif and Potamianos,

2010)], while, here we use a binary scheme.

Network-based: The aforementioned similarity

metrics were used for the definition of a semantic

network (Iosif and Potamianos, 2013; Iosif et al.,

2013). A number of similarity metrics were pro-

posed under either the attributional similarity (Tur-

ney, 2006) or the maximum sense similarity (Resnik,

1995) assumptions of lexical semantics1.

2.2 Sentence level similarities

To utilize word-level semantic similarities in the

sentence-level task we use a modified version of

BLEU (Papineni et al., 2002). The model works in

two passes: the first pass identifies exact matches

(similar to baseline BLEU), the second pass com-

pares non-matched terms using semantic similarity.

Non-matched terms from the hypothesis sentence

are compared with all terms of the reference sen-

tence (regardless of whether they were matched dur-

ing the first pass). In the case of bigram and higher

order terms, the process is applied recursively: the

bigrams are decomposed into two words and the

similarity between them is estimated by applying the

same method to the words. All word similarity met-

rics used are peak-to-peak normalized in the [0,1]

range, so they serve as a “degree-of-match”. The se-

mantic similarity scores from term pairs are summed

(just like n-gram hits) to obtain a BLEU-like hit-rate.

Alignment is performed via maximum similarity:

we iterate on the hypothesis n-grams, left-to-right,

and compare each with the most similar n-gram in

the reference. The features produced by this process

are “soft” hit-rates (for 1-, 2-, 3-, 4-grams)2. We also

use the “hard” hit rates produced by baseline BLEU

as features of the final model.

2.3 String similarities

We use the following string-based similarity fea-

tures: 1) Longest Common Subsequence Similarity

(LCSS) (Lin and Och, 2004) based on the Longest

Common Subsequence (LCS) character-based dy-

1The network-based metrics were applied only during the

training phase of the shared task, due to time limitations. They

exhibited almost identical performance as the metric defined by

(1), which was used in the test runs.
2Note that the features are computed twice on each sentence

pair and then averaged.

104



namic programming algorithm. LCSS represents the

length of the longest string (or strings) that is a sub-

string (or are substrings) of two or more strings. 2)

Skip bigram co-occurrence measures the overlap of

skip-bigrams between two sentences or phrases. A

skip-bigram is defined as any pair of words in the

sentence order, allowing for arbitrary gaps between

words (Lin and Och, 2004). 3) Containment is de-

fined as the percentage of a sentence that is con-

tained in another sentence. It is a number between

0 and 1, where 1 means the hypothesis sentence is

fully contained in the reference sentence (Broder,

1997). We express containment as the amount of n-

grams of a sentence contained in another. The con-

tainment metric is not symmetric and is calculated

as: c(X,Y ) = |S(X) ∩ S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.

2.4 Affective similarity

We used the method proposed in (Malandrakis et al.,

2011) to estimate affective features. Continuous (va-

lence and arousal) ratings in [−1, 1] of any term are
represented as a linear combination of a function of

its semantic similarities to a set of seed words and

the affective ratings of these words, as follows:

v̂(wj) = a0 +

N
∑

i=1

ai v(wi) dij , (2)

where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English

Words (ANEW) (Bradley and Lang, 1999) corpus.

Using this model we generate affective ratings for

every content word (noun, verb, adjective or adverb)

of every sentence. We assume that these can ad-

equately describe the affective content of the sen-

tences. To create an “affective similarity metric” we

use the difference of means of the word affective rat-

ings between two sentences.

d̂affect = 2− |µ(v̂(s1))− µ(v̂(s2))| (3)

where µ(v̂(si)) the mean of content word ratings in-
cluded in sentence i.

2.5 Fusion

The aforementioned features are combined using

one of two possible models. The first model is a

Multiple Linear Regression (MLR) model

D̂L = a0 +
k

∑

n=1

an fk, (4)

where D̂L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.

The second model is motivated by an assumption

of cognitive scaling of similarity scores: we expect

that the perception of hit rates is non-linearly af-

fected by the length of the sentences. We call this the

hierarchical fusion scheme. It is a combination of

(overlapping) MLR models, each matching a range

of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ≤ l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ∈ [1, l1] are decoded
with DL1, sentences with length l ∈ (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,

we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 =∞.

Domain adaptation is employed, by creating sep-

arate models per domain (training data source). Be-

yond that, we also create a unified model, trained

on all data to be used as a fallback if an appropriate

model can not be decided upon during evaluation.

3 Experimental Procedure and Results

Initially all sentences are pre-processed by the

CoreNLP (Finkel et al., 2005; Toutanova et al.,

2003) suite of tools, a process that includes named

entity recognition, normalization, part of speech tag-

ging, lemmatization and stemming. We evaluated

multiple types of preprocessing per unsupervised

metric and chose different ones depending on the

metric. Word-level semantic similarities, used for

soft comparisons and affective feature extraction,

were computed over a corpus of 116 million web

snippets collected by posing one query for every

word in the Aspell spellchecker (asp, ) vocabulary to

the Yahoo! search engine. Word-level emotional rat-

ings in continuous valence and arousal scales were

produced by a model trained on the ANEW dataset

105



and using contextual similarities. Finally, string sim-

ilarities were calculated over the original unmodified

sentences.

Next, results are reported in terms of correla-

tion between the generated scores and the ground

truth, for each corpus in the shared task, as well as

their weighted mean. Feature selection is applied

to the large candidate feature set using a wrapper-

based backward selection approach on the train-

ing data.The final feature set contains 15 features:

soft hit rates calculated over content word 1- to 4-

grams (4 features), soft hit rates calculated over un-

igrams per part-of-speech, for adjectives, nouns, ad-

verbs, verbs (4 features), BLEU unigram hit rates

for all words and content words (2 features), skip

and containment similarities, containment normal-

ized by sum of sentence lengths or product of sen-

tence lengths (3 features) and affective similarities

for arousal and valence (2 features).

Domain adaptation methods are the only dif-

ference between the three submitted runs. For all

three runs we train one linear model per training set

and a fallback model. For the first run, dubbed lin-

ear, the fallback model is linear and model selection

during evaluation is performed by file name, there-

fore results for the OnWN set are produced by a

model trained with OnWN data, while the rest are

produced by the fallback model. The second run,

dubbed length, uses a hierarchical fallback model

and model selection is performed by file name. The

third run, dubbed adapt, uses the same models as

the first run and each test set is assigned to a model

(i.e., the fallback model is never used). The test set -

model (training) mapping for this run is: OnWN →
OnWN, headlines → SMTnews, SMT → Europarl
and FNWN→ OnWN.

Table 1: Correlation performance for the linear model us-

ing lexical (L), string (S) and affect (A) features

Feature headl. OnWN FNWN SMT mean

L 0.68 0.51 0.23 0.25 0.46

L+S 0.69 0.49 0.23 0.26 0.46

L+S+A 0.69 0.51 0.27 0.28 0.47

Results are shown in Tables 1 and 2. Results for

the linear run using subsets of the final feature set

are shown in Table 1. Lexical features (hit rates) are

obviously the most valuable features. String similar-

ities provided us with an improvement in the train-

Table 2: Correlation performance on the evaluation set.

Run headl. OnWN FNWN SMT mean

linear 0.69 0.51 0.27 0.28 0.47

length 0.65 0.51 0.25 0.28 0.46

adapt 0.62 0.51 0.33 0.30 0.46

ing set which is not reflected in the test set. Af-

fect proved valuable, particularly in the most diffi-

cult sets of FNWN and SMT.

Results for the three submission runs are shown

in Table 2. Our best run was the simplest one, using

a purely linear model and effectively no adaptation.

Adding a more aggressive adaptation strategy im-

proved results in the FNWN and SMT sets, so there

is definitely some potential, however the improve-

ment observed is nowhere near that observed in the

training data or the same task of SemEval 2012. We

have to question whether this improvement is an ar-

tifact of the rating distributions of these two sets

(SMT contains virtually only high ratings, FNWN

contains virtually only low ratings): such wild mis-

matches in priors among training and test sets can

be mitigated using more elaborate machine learning

algorithms (rather than employing better semantic

similarity features or algorithms). Overall the sys-

tem performs well in the two sets containing large

similarity rating ranges.

4 Conclusions

We have improved over our previous model of sen-

tence semantic similarity. The inclusion of string-

based similarities and more so of affective content

measures proved significant, but domain adaptation

provided mixed results. While expanding the model

to include more layers of similarity estimates is

clearly a step in the right direction, further work is

required to include even more layers. Using syntac-

tic information and more levels of abstraction (e.g.

concepts) are obvious next steps.

5 Acknowledgements

The first four authors have been partially funded

by the PortDial project (Language Resources for

Portable Multilingual Spoken Dialog Systems) sup-

ported by the EU Seventh Framework Programme

(FP7), grant number 296170.

106



References

E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.

2012. Semeval-2012 task 6: A pilot on semantic tex-

tual similarity. In Proc. SemEval, pages 385–393.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-

Agirre, and Weiwei Guo. 2013. *sem 2013 shared

task: Semantic textual similarity, including a pilot on

typed-similarity. In Proc. *SEM.

Gnu aspell. http://www.aspell.net.

D. Bär, C. Biemann, I. Gurevych, and T. Zesch. 2012.

Ukp: Computing semantic textual similarity by com-

bining multiple content similarity measures. In Proc.

SemEval, pages 435–440.

M. Baroni and A. Lenci. 2010. Distributional mem-

ory: A general framework for corpus-based semantics.

Computational Linguistics, 36(4):673–721.

J. Bos and K. Markert. 2005. Recognising textual en-

tailment with logical inference. In Proceedings of the

Human Language Technology Conference and Confer-

ence on Empirical Methods in Natural Language Pro-

cessing, page 628635.

M. Bradley and P. Lang. 1999. Affective norms for En-

glish words (ANEW): Stimuli, instruction manual and

affective ratings. Technical report C-1. The Center for

Research in Psychophysiology, University of Florida.

Andrei Z. Broder. 1997. On the resemblance and con-

tainment of documents. In In Compression and Com-

plexity of Sequences (SEQUENCES97, pages 21–29.

IEEE Computer Society.

A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-

based measures of semantic distance. Computational

Linguistics, 32:13–47.

K. W. Church and P. Hanks. 1990. Word association

norms, mutual information, and lexicography. Com-

putational Linguistics, 16(1):22–29.

A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-

chine translation evaluation techniques to determine

sentence-level semantic equivalence. In Proceedings

of the 3rd International Workshop on Paraphrasing,

page 1724.

J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-

corporating non-local information into information ex-

traction systems by gibbs sampling. In Proceedings of

the 43rd Annual Meeting on Association for Computa-

tional Linguistics, pages 363–370.

L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,

Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-

ing search in context: The concept revisited. ACM

Transactions on Information Systems, 20(1):116–131.

S. Harabagiu and A. Hickl. 2006. Methods for Us-

ing Textual Entailment in Open-Domain Question An-

swering. In Proceedings of the 21st International Con-

ference on Computational Linguistics and 44th Annual

Meeting of the Association for Computational Linguis-

tics, pages 905–912.

Z. Harris. 1954. Distributional structure. Word,

10(23):146–162.

E. Iosif and A. Potamianos. 2010. Unsupervised seman-

tic similarity computation between terms using web

documents. IEEE Transactions on Knowledge and

Data Engineering, 22(11):1637–1647.

E. Iosif and A. Potamianos. 2012. Semsim: Resources

for normalized semantic similarity computation using

lexical networks. In Proc. Eighth International Con-

ference on Language Resources and Evaluation, pages

3499–3504.

Elias Iosif and Alexandros Potamianos. 2013. Similarity

Computation Using Semantic Networks Created From

Web-Harvested Data. Natural Language Engineering,

(submitted).

E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-

vanou. 2013. Semantic similarity computation for ab-

stract and concrete nouns using network-based distri-

butional semantic models. In 10th International Con-

ference on Computational Semantics (IWCS), pages

328–334.

Chin-Yew Lin and Franz Josef Och. 2004. Automatic

evaluation of machine translation quality using longest

common subsequence and skip-bigram statistics. In

Proceedings of the 42nd Annual Meeting on Associa-

tion for Computational Linguistics, ACL ’04, Strouds-

burg, PA, USA. Association for Computational Lin-

guistics.

P. Malakasiotis and I. Androutsopoulos. 2007. Learn-

ing textual entailment using svms and string similar-

ity measures. In Proceedings of of the ACL-PASCAL

Workshop on Textual Entailment and Paraphrasing,

pages 42–47.

P. Malakasiotis. 2009. Paraphrase recognition using ma-

chine learning to combine similarity measures. In Pro-

ceedings of the 47th Annual Meeting of ACL and the

4th Int. Joint Conference on Natural Language Pro-

cessing of AFNLP, pages 42–47.

N. Malandrakis, A. Potamianos, E. Iosif, and

S. Narayanan. 2011. Kernel models for affec-

tive lexicon creation. In Proc. Interspeech, pages

2977–2980.

N. Malandrakis, E. Iosif, and A. Potamianos. 2012.

DeepPurple: Estimating sentence semantic similarity

using n-gram regression models and web snippets. In

Proc. Sixth International Workshop on Semantic Eval-

uation (SemEval) – The First Joint Conference on

Lexical and Computational Semantics (*SEM), pages

565–570.

H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-

tion of semantic structures for understanding domain-

107



specific natural language queries. IEEE Transactions

on Knowledge and Data Engineering, 14(1):172–181.

G. Miller and W. Charles. 1998. Contextual correlates

of semantic similarity. Language and Cognitive Pro-

cesses, 6(1):1–28.

G. Miller. 1990. Wordnet: An on-line lexical database.

International Journal of Lexicography, 3(4):235–312.

S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-

man, and S. Idan. 2009. Source-language entailment

modeling for translating unknown terms. In Proceed-

ings of the 47th Annual Meeting of ACL and the 4th Int.

Joint Conference on Natural Language Processing of

AFNLP, pages 791–799.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.

Bleu: a method for automatic evaluation of ma-

chine translation. In Proceedings of the 40th Annual

Meeting on Association for Computational Linguis-

tics, pages 311–318.

D. Perez and E. Alfonseca. 2005. Application of the

bleu algorithm for recognizing textual entailments. In

Proceedings of the PASCAL Challenges Worshop on

Recognising Textual Entailment.

P. Resnik. 1995. Using information content to evalu-

ate semantic similarity in a taxanomy. In Proc. of In-

ternational Joint Conference for Artificial Intelligence,

pages 448–453.

F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and

D. Molla. 2003. Exploiting paraphrases in a question

answering system. In Proceedings of the 2nd Interna-

tional Workshop on Paraphrasing, pages 25–32.

H. Rubenstein and J. B. Goodenough. 1965. Contextual

correlates of synonymy. Communications of the ACM,

8(10):627–633.

I. Szpektor and I. Dagan. 2008. Learning entailment

rules for unary templates. In Proceedings of the 22nd

International Conference on Computational Linguis-

tics, pages 849–856.

K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.

2003. Feature-rich part-of-speech tagging with a

cyclic dependency network. In Proceedings of Con-

ference of the North American Chapter of the Associ-

ation for Computational Linguistics on Human Lan-

guage Technology, pages 173–180.

P. Turney. 2006. Similarity of semantic relations. Com-

putational Linguistics, 32(3):379–416.

F. Šarić, G. Glavaš, M. Karan, J. Šnajder, and B. Dal-

belo Bašić. 2012. Takelab: Systems for measuring

semantic text similarity. In Proc. SemEval, pages 441–

448.

F. Zanzotto, M. Pennacchiotti, and A. Moschitti.

2009. A machine-learning approach to textual en-

tailment recognition. Natural Language Engineering,

15(4):551582.

108


