










































Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -


Proceedings of the ACL 2010 Conference Short Papers, pages 189–193,
Uppsala, Sweden, 11-16 July 2010. c©2010 Association for Computational Linguistics

Tree-Based Deterministic Dependency Parsing
— An Application to Nivre’s Method —

Kotaro Kitagawa Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology,

The University of Tokyo
kitagawa@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp

Abstract

Nivre’s method was improved by en-
hancing deterministic dependency parsing
through application of a tree-based model.
The model considers all words necessary
for selection of parsing actions by includ-
ing words in the form of trees. It chooses
the most probable head candidate from
among the trees and uses this candidate to
select a parsing action.

In an evaluation experiment using the
Penn Treebank (WSJ section), the pro-
posed model achieved higher accuracy
than did previous deterministic models.
Although the proposed model’s worst-case
time complexity is O(n2), the experimen-
tal results demonstrated an average pars-
ing time not much slower than O(n).

1 Introduction

Deterministic parsing methods achieve both effec-
tive time complexity and accuracy not far from
those of the most accurate methods. One such
deterministic method is Nivre’s method, an incre-
mental parsing method whose time complexity is
linear in the number of words (Nivre, 2003). Still,
deterministic methods can be improved. As a spe-
cific example, Nivre’s model greedily decides the
parsing action only from two words and their lo-
cally relational words, which can lead to errors.

In the field of Japanese dependency parsing,
Iwatate et al. (2008) proposed a tournament model
that takes all head candidates into account in judg-
ing dependency relations. This method assumes
backward parsing because the Japanese depen-
dency structure has a head-final constraint, so that
any word’s head is located to its right.

Here, we propose a tree-based model, applica-
ble to any projective language, which can be con-
sidered as a kind of generalization of Iwatate’s

idea. Instead of selecting a parsing action for
two words, as in Nivre’s model, our tree-based
model first chooses the most probable head can-
didate from among the trees through a tournament
and then decides the parsing action between two
trees.

Global-optimization parsing methods are an-
other common approach (Eisner, 1996; McDon-
ald et al., 2005). Koo et al. (2008) studied
semi-supervised learning with this approach. Hy-
brid systems have improved parsing by integrat-
ing outputs obtained from different parsing mod-
els (Zhang and Clark, 2008).

Our proposal can be situated among global-
optimization parsing methods as follows. The pro-
posed tree-based model is deterministic but takes a
step towards global optimization by widening the
search space to include all necessary words con-
nected by previously judged head-dependent rela-
tions, thus achieving a higher accuracy yet largely
retaining the speed of deterministic parsing.

2 Deterministic Dependency Parsing

2.1 Dependency Parsing

A dependency parser receives an input sentence
x = w1, w2, . . . , wn and computes a dependency
graph G = (W,A). The set of nodes W =
{w0, w1, . . . , wn} corresponds to the words of a
sentence, and the node w0 is the root of G. A is
the set of arcs (wi, wj), each of which represents a
dependency relation where wi is the head and wj
is the dependent.

In this paper, we assume that the resulting de-
pendency graph for a sentence is well-formed and
projective (Nivre, 2008). G is well-formed if and
only if it satisfies the following three conditions of
being single-headed, acyclic, and rooted.

2.2 Nivre’s Method

An incremental dependency parsing algorithm
was first proposed by (Covington, 2001). After

189



Table 1: Transitions for Nivre’s method and the proposed method.
Transition Precondition

Nivre’s
Method

Left-Arc (σ|wi, wj |β, A) ⇒ (σ,wj |β, A ∪ {(wj , wi)}) i ̸= 0 ∧ ¬∃wk (wk, wi) ∈ A
Right-Arc (σ|wi, wj |β, A) ⇒ (σ|wi|wj , β, A ∪ {(wi, wj)})
Reduce (σ|wi, β, A) ⇒ (σ, β, A) ∃wk (wk, wi) ∈ A
Shift (σ,wj |β, A) ⇒ (σ|wj , β, A)

Proposed
Method

Left-Arc (σ|ti, tj |β,A) ⇒ (σ, tj |β,A ∪ {(wj , wi)}) i ̸= 0
Right-Arc (σ|ti, tj |β,A) ⇒ (σ|ti, β, A ∪ {(mphc(ti, tj), wj)})
Shift (σ, tj |β, A) ⇒ (σ|tj , β, A)

studies taking data-driven approaches, by (Kudo
and Matsumoto, 2002), (Yamada and Matsumoto,
2003), and (Nivre, 2003), the deterministic incre-
mental parser was generalized to a state transition
system in (Nivre, 2008).

Nivre’s method applying an arc-eager algorithm
works by using a stack of words denoted as σ, for
a buffer β initially containing the sentence x. Pars-
ing is formulated as a quadruple (S, Ts, sinit, St),
where each component is defined as follows:

• S is a set of states, each of which is denoted
as (σ, β,A) ∈ S.

• Ts is a set of transitions, and each element of
Ts is a function ts : S → S.

• sinit = ([w0], [w1, . . . , wn], ϕ) is the initial
state.

• St is a set of terminal states.

Syntactic analysis generates a sequence of optimal
transitions ts provided by an oracle o : S → Ts,
applied to a target consisting of the stack’s top ele-
ment wi and the first element wj in the buffer. The
oracle is constructed as a classifier trained on tree-
bank data. Each transition is defined in the upper
block of Table 1 and explained as follows:

Left-Arc Make wj the head of wi and pop wi,
where wi is located at the stack top (denoted
as σ|wi), when the buffer head is wj (denoted
as wj |β).

Right-Arc Make wi the head of wj , and push wj .
Reduce Pop wi, located at the stack top.
Shift Push the word wj , located at the buffer head,

onto the stack top.

The method explained thus far has the following
drawbacks.

Locality of Parsing Action Selection

The dependency relations are greedily determined,
so when the transition Right-Arc adds a depen-
dency arc (wi, wj), a more probable head of wj
located in the stack is disregarded as a candidate.

Features Used for Selecting Reduce

The features used in (Nivre and Scholz, 2004) to
define a state transition are basically obtained from
the two target words wi and wj , and their related
words. These words are not sufficient to select Re-
duce, because this action means that wj has no de-
pendency relation with any word in the stack.

Preconditions

When the classifier selects a transition, the result-
ing graph satisfies well-formedness and projectiv-
ity only under the preconditions listed in Table 1.
Even though the parsing seems to be formulated as
a four-class classifier problem, it is in fact formed
of two types of three-class classifiers.

Solving these problems and selecting a more
suitable dependency relation requires a parser that
considers more global dependency relations.

3 Tree-Based Parsing Applied to Nivre’s
Method

3.1 Overall Procedure

Tree-based parsing uses trees as the procedural el-
ements instead of words. This allows enhance-
ment of previously proposed deterministic mod-
els such as (Covington, 2001; Yamada and Mat-
sumoto, 2003). In this paper, we show the applica-
tion of tree-based parsing to Nivre’s method. The
parser is formulated as a state transition system
(S, Ts, sinit, St), similarly to Nivre’s parser, but σ
and β for a state s = (σ, β,A) ∈ S denote a stack
of trees and a buffer of trees, respectively. A tree
ti ∈ T is defined as the tree rooted by the word wi,
and the initial state is sinit = ([t0], [t1, . . . , tn], ϕ),
which is formed from the input sentence x.

The state transitions Ts are decided through the
following two steps.

1. Select the most probable head candidate
(MPHC): For the tree ti located at the stack
top, search for and select the MPHC for wj ,
which is the root word of tj located at the
buffer head. This procedure is denoted as a

190



He

watched

most probable

head candidate

birdsHe

the

with the
head candidates

across

river

the

birds

w j
telescope

Figure 1: Example of a tournament.

function mphc(ti, tj), and its details are ex-
plained in §3.2.

2. Select a transition: Choose a transition,
by using an oracle, from among the follow-
ing three possibilities (explained in detail in
§3.3):

Left-Arc Make wj the head of wi and pop
ti, where ti is at the stack top (denoted
as σ|ti, with the tail being σ), when the
buffer head is tj (denoted as tj |β).

Right-Arc Make the MPHC the head of wj ,
and pop the MPHC.

Shift Push the tree tj located at the buffer
head onto the stack top.

These transitions correspond to three possibilities
for the relation between ti and tj : (1) a word of ti
is a dependent of a word of tj ; (2) a word of tj is a
dependent of a word of ti; or (3) the two trees are
not related.

The formulations of these transitions in the
lower block of Table 1 correspond to Nivre’s tran-
sitions of the same name, except that here a tran-
sition is applied to a tree. This enhancement from
words to trees allows removal of both the Reduce
transition and certain preconditions.

3.2 Selection of Most Probable Head
Candidate

By using mphc(ti, tj), a word located far from wj
(the head of tj) can be selected as the head can-
didate in ti. This selection process decreases the
number of errors resulting from greedy decision
considering only a few candidates.

Various procedures can be considered for im-
plementing mphc(ti, tj). One way is to apply the
tournament procedure to the words in ti. The tour-
nament procedure was originally introduced for
parsing methods in Japanese by (Iwatate et al.,

The biped

was

sold

separately by

robot

his company

ti t j

mphc ),( ji tt

Right-Arc

The biped

was

sold

separately by

robot

his company

ti t j

Figure 2: Example of the transition Right.

2008). Since the Japanese language has the head-
final property, the tournament model itself consti-
tutes parsing, whereas for parsing a general pro-
jective language, the tournament model can only
be used as part of a parsing algorithm.

Figure 1 shows a tournament for the example
of “with,” where the word “watched” finally wins.
Although only the words on the left-hand side of
tree tj are searched, this does not mean that the
tree-based method considers only one side of a de-
pendency relation. For example, when we apply
the tree-based parsing to Yamada’s method, the
search problems on both sides are solved.

To implement mphc(ti, tj), a binary classifier
is built to judge which of two given words is more
appropriate as the head for another input word.
This classifier concerns three words, namely, the
two words l (left) and r (right) in ti, whose ap-
propriateness as the head is compared for the de-
pendent wj . All word pairs of l and r in ti are
compared repeatedly in a “tournament,” and the
survivor is regarded as the MPHC of wj .

The classifier is generated through learning of
training examples for all ti and wj pairs, each
of which generates examples comparing the true
head and other (inappropriate) heads in ti. Ta-
ble 2 lists the features used in the classifier. Here,
lex(X) and pos(X) mean the surface form and part
of speech of X , respectively. X left means the
dependents of X located on the left-hand side of
X , while Xright means those on the right. Also,
Xhead means the head of X . The feature design
concerns three additional words occurring after
wj , as well, denoted as wj+1, wj+2, wj+3.

3.3 Transition Selection

A transition is selected by a three-class classifier
after deciding the MPHC, as explained in §3.1.
Table 1 lists the three transitions and one precon-

191



Table 2: Features used for a tournament.

pos(l), lex(l)
pos(lhead), pos(lleft), pos(lright)
pos(r), lex(r)
pos(rhead), pos(rleft), pos(rright)

pos(wj), lex(wj), pos(w
left
j )

pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2)
pos(wj+3), lex(wj+3)

Table 3: Features used for a state transition.

pos(wi), lex(wi)
pos(wlefti ), pos(w

right
i ), lex(w

left
i ), lex(w

right
i )

pos(MPHC), lex(MPHC)
pos(MPHChead), pos(MPHCleft), pos(MPHCright)
lex(MPHChead), lex(MPHCleft), lex(MPHCright)

pos(wj), lex(wj), pos(w
left
j ), lex(w

left
j )

pos(wj+1), lex(wj+1), pos(wj+2), lex(wj+2), pos(wj+3), lex(wj+3)

dition. The transition Shift indicates that the tar-
get trees ti and tj have no dependency relations.
The transition Right-Arc indicates generation of
the dependent-head relation between wj and the
result of mphc(ti, tj), i.e., the MPHC for wj . Fig-
ure 2 shows an example of this transition. The
transition Left-Arc indicates generation of the de-
pendency relation in which wj is the head of wi.
While Right-Arc requires searching for the MPHC
in ti, this is not the case for Left-Arc1.

The key to obtaining an accurate tree-based
parsing model is to extend the search space while
at the same time providing ways to narrow down
the space and find important information, such as
the MPHC, for proper judgment of transitions.

The three-class classifier is constructed as fol-
lows. The dependency relation between the target
trees is represented by the three words wi, MPHC,
and wj . Therefore, the features are designed to in-
corporate these words, their relational words, and
the three words next to wj . Table 3 lists the exact
set of features used in this work. Since this transi-
tion selection procedure presumes selection of the
MPHC, the result of mphc(ti, tj) is also incorpo-
rated among the features.

4 Evaluation

4.1 Data and Experimental Setting

In our experimental evaluation, we used Yamada’s
head rule to extract unlabeled dependencies from
the Wall Street Journal section of a Penn Treebank.
Sections 2-21 were used as the training data, and
section 23 was used as the test data. This test data

1The head word of wi can only be wj without searching
within tj , because the relations between the other words in tj
and wi have already been inferred from the decisions made
within previous transitions. If tj has a child wk that could
become the head of wi under projectivity, this wk must be
located between wi and wj . The fact that wk’s head is wj
means that there were two phases before ti and tj (i.e., wi
and wj) became the target:

• ti and tk became the target, and Shift was selected.
• tk and tj became the target, and Left-Arc was selected.

The first phase precisely indicates that wi and wk are unre-
lated.

was used in several other previous works, enabling
mutual comparison with the methods reported in
those works.

The SVMlight package2 was used to build the
support vector machine classifiers. The binary
classifier for MPHC selection and the three-class
classifier for transition selection were built using a
cubic polynomial kernel. The parsing speed was
evaluated on a Core2Duo (2.53 GHz) machine.

4.2 Parsing Accuracy

We measured the ratio of words assigned correct
heads to all words (accuracy), and the ratio of sen-
tences with completely correct dependency graphs
to all sentences (complete match). In the evalua-
tion, we consistently excluded punctuation marks.

Table 4 compares our results for the proposed
method with those reported in some previous
works using equivalent training and test data.
The first column lists the four previous methods
and our method, while the second through fourth
columns list the accuracy, complete match accu-
racy, and time complexity, respectively, for each
method. Here, we obtained the scores for the pre-
vious works from the corresponding articles listed
in the first column. Note that every method used
different features, which depend on the method.

The proposed method achieved higher accuracy
than did the previous deterministic models. Al-
though the accuracy of our method did not reach
that of (McDonald and Pereira, 2006), the scores
were competitive even though our method is de-
terministic. These results show the capability of
the tree-based approach in effectively extending
the search space.

4.3 Parsing Time

Such extension of the search space also concerns
the speed of the method. Here, we compare its
computational time with that of Nivre’s method.
We re-implemented Nivre’s method to use SVMs
with cubic polynomial kernel, similarly to our

2http://svmlight.joachims.org/

192



Table 4: Dependency parsing performance.

Accuracy Complete Time Global vs. Learning
match complexity deterministic method

McDonald & Pereira (2006) 91.5 42.1 O(n3) global MIRA
McDonald et al. (2005) 90.9 37.5 O(n3) global MIRA

Yamada & Matsumoto (2003) 90.4 38.4 O(n2) deterministic support vector machine
Goldberg & Elhadad (2010) 89.7 37.5 O(n log n) deterministic structured perceptron

Nivre (2004) 87.1 30.4 O(n) deterministic memory based learning
Proposed method 91.3 41.7 O(n2) deterministic support vector machine

10 20 30 40 50

0
10

20
30

40
50

60

Nivre’s Method

length of input sentence

pa
rs

in
g 

tim
e 

 [s
ec

]

10 20 30 40 50

0
10

20
30

40
50

60

Proposed Method

length of input sentence

pa
rs

in
g 

tim
e 

 [s
ec

]

Figure 3: Parsing time for sentences.

method. Figure 3 shows plots of the parsing times
for all sentences in the test data. The average pars-
ing time for our method was 8.9 sec, whereas that
for Nivre’s method was 7.9 sec.

Although the worst-case time complexity for
Nivre’s method is O(n) and that for our method is
O(n2), worst-case situations (e.g., all words hav-
ing heads on their left) did not appear frequently.
This can be seen from the sparse appearance of the
upper bound in the second figure.

5 Conclusion

We have proposed a tree-based model that decides
head-dependency relations between trees instead
of between words. This extends the search space
to obtain the best head for a word within a deter-
ministic model. The tree-based idea is potentially
applicable to various previous parsing methods; in
this paper, we have applied it to enhance Nivre’s
method.

Our tree-based model outperformed various de-
terministic parsing methods reported previously.
Although the worst-case time complexity of our
method is O(n2), the average parsing time is not
much slower than O(n).

References
Xavier Carreras. 2007. Experiments with a higher-order

projective dependency parse. Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL, pp. 957-961.

Michael A. Covington. 2001. A fundamental algorithm for
dependency parsing. Proceedings of ACM, pp. 95-102.

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. Proceedings of
COLING, pp. 340-345.

Yoav Goldberg and Michael Elhadad. 2010. An Efficient Al-
gorithm for Easy-First Non-Directional Dependency Pars-
ing. Proceedings of NAACL.

Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto.
2008. Japanese dependency parsing using a tournament
model. Proceedings of COLING, pp. 361–368.

Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. Proceed-
ings of ACL, pp. 595–603.

Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking Proceedings of
CoNLL, pp. 63–69.

Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. Proceedings of ACL, pp. 91–98.

Ryan McDonald and Fernando Pereira. 2006. Online learn-
ing of approximate dependency parsing algorithms. Pro-
ceedings of the EACL, pp. 81–88.

Joakim Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of IWPT, pp. 149–160.

Joakim Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics, vol.
34, num. 4, pp. 513–553.

Joakim Nivre and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. Proceedings of COLING,
pp. 64–70.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. Pro-
ceedings of IWPT, pp. 195–206.

Yue Zhang and Stephen Clark. 2008. A tale of two parsers:
investigating and combining graph-based and transition-
based dependency parsing using beamsearch. Proceed-
ings of EMNLP, pp. 562–571.

193


