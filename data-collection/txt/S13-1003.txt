










































Coarse to Fine Grained Sense Disambiguation in Wikipedia


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 22–31, Atlanta, Georgia, June 13-14, 2013. c©2013 Association for Computational Linguistics

Coarse to Fine Grained Sense Disambiguation in Wikipedia

Hui Shen

School of EECS

Ohio University

Athens, OH 45701, USA

hui.shen.1@ohio.edu

Razvan Bunescu

School of EECS

Ohio University

Athens, OH 45701, USA

bunescu@ohio.edu

Rada Mihalcea

Department of CSE

University of North Texas

Denton, TX 76203, USA

rada@cs.unt.edu

Abstract

Wikipedia articles are annotated by volunteer

contributors with numerous links that connect

words and phrases to relevant titles. Links

to general senses of a word are used concur-

rently with links to more specific senses, with-

out being distinguished explicitly. We present

an approach to training coarse to fine grained

sense disambiguation systems in the presence

of such annotation inconsistencies. Experi-

mental results show that accounting for anno-

tation ambiguity in Wikipedia links leads to

significant improvements in disambiguation.

1 Introduction and Motivation

The vast amount of world knowledge available in

Wikipedia has been shown to benefit many types

of text processing tasks, such as coreference res-

olution (Ponzetto and Strube, 2006; Haghighi and

Klein, 2009; Bryl et al., 2010; Rahman and Ng,

2011), information retrieval (Milne, 2007; Li et al.,

2007; Potthast et al., 2008; Cimiano et al., 2009),

or question answering (Ahn et al., 2004; Kaisser,

2008; Ferrucci et al., 2010). In particular, the user

contributed link structure of Wikipedia has been

shown to provide useful supervision for training

named entity disambiguation (Bunescu and Pasca,

2006; Cucerzan, 2007) and word sense disambigua-

tion (Mihalcea, 2007; Ponzetto and Navigli, 2010)

systems. Articles in Wikipedia often contain men-

tions of concepts or entities that already have a cor-

responding article. When contributing authors men-

tion an existing Wikipedia entity inside an article,

they are required to link at least its first mention to

the corresponding article, by using links or piped

links. Consider, for example, the following Wiki

source annotations: The [[capital city|capital]] of

Georgia is [[Atlanta]]. The bracketed strings iden-

tify the title of the Wikipedia articles that describe

the corresponding named entities. If the editor wants

a different string displayed in the rendered text, then

the alternative string is included in a piped link, af-

ter the title string. Based on these Wiki processing

rules, the text that is rendered for the aforementioned

example is: The capital of Georgia is Atlanta.

Since many words and names mentioned in

Wikipedia articles are inherently ambiguous, their

corresponding links can be seen as a useful source

of supervision for training named entity and word

sense disambiguation systems. For example,

Wikipedia contains articles that describe possible

senses of the word “capital”, such as CAPITAL CITY,

CAPITAL (ECONOMICS), FINANCIAL CAPITAL, or

HUMAN CAPITAL, to name only a few. When dis-

ambiguating a word or a phrase in Wikipedia, a con-

tributor uses the context to determine the appropriate

Wikipedia title to include in the link. In the exam-

ple above, the editor of the article determined that

the word “capital” was mentioned with the political

center meaning, consequently it was mapped to the

article CAPITAL CITY through a piped link.

In order to useWikipedia links for training aWSD

system for a given word, one needs first to define a

sense repository that specifies the possible meanings

for that word, and then use the Wikipedia links to

create training examples for each sense in the repos-

itory. This approach might be implemented using

the following sequence of steps:

22



In global climate models, the state and properties of the [[atmosphere]] are specified at a number of discrete locations

General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A → A(S)→ AE

The principal natural phenomena that contribute gases to the [[Atmosphere of Earth|atmosphere]] are emissions from volcanoes
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A → A(S)→ AE

An aerogravity assist is a spacecraft maneuver designed to change velocity when arriving at a body with an [[atmosphere]]

General = ATMOSPHERE; Specific = ATMOSPHERE ⊲ generic; Label = A → A(G)

Assuming the planet’s [[atmosphere]] is close to equilibrium, it is predicted that 55 Cancri d is covered with water clouds

General = ATMOSPHERE; Specific = ATMOSPHERE OF CANCRI ⊲ missing; A→ A(G)

Figure 1: Coarse and fine grained sense annotations in Wikipedia (bold). The proposed hierarchical Label (right).

A(S) = ATMOSPHERE (S), A(G) = ATMOSPHERE (G), A = ATMOSPHERE, AE = ATMOSPHERE OF EARTH.

1. Collect all Wikipedia titles that are linked from

the ambiguous anchor word.

2. Create a repository of senses from all titles that

have sufficient support in Wikipedia i.e., titles

that are referenced at least a predefined min-

imum number of times using the ambiguous

word as anchor.

3. Use the links extracted for each sense in the

repository as labeled examples for that sense

and train a WSD model to distinguish between

alternative senses of the ambiguous word.

Taking the word “atmosphere” as an example, the

first step would result in a wide array of titles,

ranging from the general ATMOSPHERE and its in-

stantiations ATMOSPHERE OF EARTH or ATMO-

SPHERE OF MARS, to titles as diverse as ATMO-

SPHERE (UNIT), MOOD (PSYCHOLOGY), or AT-

MOSPHERE (MUSIC GROUP). In the second step,

the most frequent titles for the anchor word “at-

mosphere” would be assembled into a repository R
= {ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,

STELLAR ATMOSPHERE, ATMOSPHERE (UNIT),

ATMOSPHERE (MUSIC GROUP)}. The classifier
trained in the third step would use features ex-

tracted from the context to discriminate between

word senses.

This Wikipedia-based approach to creating train-

ing data for word sense disambiguation has a ma-

jor shortcoming. Many of the training examples ex-

tracted for the title ATMOSPHERE could very well

belong to more specific titles such as ATMOSPHERE

OF EARTH or ATMOSPHERE OF MARS. Whenever

the word “atmosphere” is used in a context with the

sense of “a layer of gases that may surround a ma-

terial body of sufficient mass, and that is held in

place by the gravity of the body,” the contributor

has the option of adding a link either to the title AT-

MOSPHERE that describes this general sense of the

word, or to the title of an article that describes the

atmosphere of the actual celestial body that is re-

ferred in that particular context, as shown in the first

2 examples in Figure 1. As shown in bold in Fig-

ure 1, different occurrences of the same word may

be tagged with either a general or a specific link, an

ambiguity that is pervasive in Wikipedia for words

like ”atmosphere” that have general senses that sub-

sume multiple, popular specific senses. There does

not seem to be a clear, general rule underlying the

decision to tag a word or a phrase with a general

or specific sense link in Wikipedia. We hypothesize

that, in some cases, editors may be unaware that an

article exists in Wikipedia for the actual reference

of a word or for a more specific sense of the word,

and therefore they end up using a link to an article

describing the general sense of the word. There is

also the possibility that more specific articles are in-

troduced only in newer versions of Wikipedia, and

thus earlier annotations were not aware of these re-

cent articles. Furthermore, since annotating words

with the most specific sense available in Wikipedia

may require substantial cognitive effort, editors may

often choose to link to a general sense of the word, a

choice that is still correct, yet less informative than

the more specific sense.

2 Annotation Inconsistencies in Wikipedia

In order to get a sense of the potential magnitude

of the general vs. specific sense annotation ambi-

guity, we extracted all Wikipedia link annotations

23



for the words “atmosphere”, “president”, “game”,

“dollar”, “diamond” and “Corinth”, and created

a special subset from those that were labeled by

Wikipedia editors with the general sense links AT-

MOSPHERE, PRESIDENT, GAME, DOLLAR, DIA-

MOND, and CORINTH, respectively. Then, for each

of the 7,079 links in this set, we used the context

to manually determine the corresponding more spe-

cific title, whenever such a title exists in Wikipedia.

The statistics in Tables 1 and 2 show a significant

overlap between the general and specific sense cate-

gories. For example, out of the 932 links from “at-

mosphere” to ATMOSPHERE that were extracted in

total, 518 were actually about the ATMOSPHERE OF

EARTH, but the user linked them to the more general

sense category ATMOSPHERE. On the other hand,

there are 345 links to ATMOSPHERE OF EARTH that

were explicitly made by the user. We manually as-

signed general links (G) whenever the word is used

with a generic sense, or when the reference is not

available in the repository of titles collected for that

word because either the more specific title does not

exist in Wikipedia or the specific title exists, but it

does not have sufficient support – at least 20 linked

anchors – in Wikipedia. We grouped the more spe-

cific links for any given sense into a special cate-

gory suffixed with (S), to distinguish them from the

general links (generic use, or missing reference) that

were grouped into the category suffixed with (G).

For many ambiguous words, the annotation in-

consistencies appear when the word has senses

that are in a subsumption relationship: the ATMO-

SPHERE OF EARTH is an instance of ATMOSPHERE,

whereas a STELLAR ATMOSPHERE is a particular

type of ATMOSPHERE. Subsumed senses can be

identified automatically using the category graph in

Wikipedia. The word “Corinth” is an interesting

case: the subsumption relationship between AN-

CIENT CORINTH and CORINTH appears because of

a temporal constraint. Furthermore, in the case of

the word “diamond”, the annotation inconsistencies

are not caused by a subsumption relation between

senses. Instead of linking to the DIAMOND (GEM-

STONE) sense, Wikipedia contributors often link to

the related DIAMOND sense indicating the mineral

used in the gemstone.

A supervised learning algorithm that uses the ex-

tracted links for training aWSD classification model

atmosphere Size

ATMOSPHERE 932

Atmosphere (S) 559

Atmosphere of Earth 518

Atmosphere of Mars 19

Atmosphere of Venus 9

Stellar Atmosphere 13

Atmosphere (G) 373

ATMOSPHERE OF EARTH 345

ATMOSPHERE OF MARS 37

ATMOSPHERE OF VENUS 26

STELLAR ATMOSPHERE 29

ATMOSPHERE (UNIT) 96

ATMOSPHERE (MUSIC GROUP) 104

president Size

PRESIDENT 3534

President (S) 989

Chancellor (education) 326

President of the United States 534

President of the Philippines 42

President of Pakistan 27

President of France 22

President of India 21

President of Russia 17

President (G) 2545

CHANCELLOR (EDUCATION) 210

PRESIDENT OF THE UNITED STATES 5941

PRESIDENT OF THE PHILIPPINES 549

PRESIDENT OF PAKISTAN 192

PRESIDENT OF FRANCE 151

PRESIDENT OF INDIA 86

PRESIDENT OF RUSSIA 101

Table 1: Wiki (CAPS) and manual (italics) annotations.

to distinguish between categories in the sense repos-

itory assumes implicitly that the categories, and

hence their training examples, are mutually disjoint.

This assumption is clearly violated for words like

“atmosphere,” consequently the learned model will

have a poor performance on distinguishing between

the overlapping categories. Alternatively, we can

say that sense categories like ATMOSPHERE are ill

defined, since their supporting dataset contains ex-

amples that could also belong to more specific sense

categories such as ATMOSPHERE OF EARTH.

We see two possible solutions to the problem of

inconsistent link annotations. In one solution, spe-

cific senses are grouped together with the subsuming

general sense, such that all categories in the result-

ing repository become disjoint. For “atmosphere”,

the general category ATMOSPHERE would be aug-

mented to contain all the links previously annotated

24



dollar Size

DOLLAR 379

Dollar (S) 231

United States dollar 228

Canadian dollar 3

Australian dollar 1

Dollar (G) 147

UNITED STATES DOLLAR 3516

CANADIAN DOLLAR 420

AUSTRALIAN DOLLAR 124

DOLLAR SIGN 290

DOLLAR (BAND) 30

DOLLAR, CLACKMANNANSHIRE 30

game Size

GAME 819

Game (S) 99

Video game 55

PC game 44

Game (G) 720

VIDEO GAME 312

PC GAME 24

GAME (FOOD) 232

GAME (RAPPER) 154

diamond Size

DIAMOND 716

Diamond (S) 221

Diamond (gemstone) 221

Diamond (G) 495

DIAMOND (GEMSTONE) 71

BASEBALL FIELD 36

MUSIC RECORDING SALES CERT. 36

Corinth Size

CORINTH 699

Corinth (S) 409

Ancient Corinth 409

Corinth (G) 290

ANCIENT CORINTH 92

CORINTH, MISSISSIPPI 72

Table 2: Wiki (CAPS) and manual (italics) annotations.

as ATMOSPHERE, ATMOSPHERE OF EARTH, AT-

MOSPHERE OF MARS, ATMOSPHERE OF VENUS,

or STELLAR ATMOSPHERE. This solution is

straightforward to implement, however it has the

disadvantage that the resulting WSD model will

never link words to more specific titles in Wikipedia

like ATMOSPHERE OF MARS.

Another solution is to reorganize the original

sense repository into a hierarchical classification

scheme such that sense categories at each classifi-

cation level become mutually disjoint. The resulting

WSD system has the advantage that it can make fine

grained sense distinctions for an ambiguous word,

despite the annotation inconsistencies present in the

training data. The rest of this paper describes a feasi-

ble implementation for this second solution that does

not require any manual annotation beyond the links

that are already provided by Wikipedia volunteers.

3 Learning for Coarse to Fine Grained

Sense Disambiguation

Figure 2 shows our proposed hierarchical classifica-

tion scheme for disambiguation, using “atmosphere”

as the ambiguous word. Shaded leaf nodes show

the final categories in the sense repository for each

word, whereas the doted elliptical frames on the

second level in the hierarchy denote artificial cate-

gories introduced to enable a finer grained classifi-

cation into more specific senses. Thick dotted ar-

rows illustrate the classification decisions that are

made in order to obtain a fine grained disambigua-

tion of the word. Thus, the word “atmosphere”

is first classified to have the general sense ATMO-

SPHERE, i.e. “a layer of gases that may surround a

material body of sufficient mass, and that is held in

place by the gravity of the body”. In the first so-

lution, the disambiguation process would stop here

and output the general sense ATMOSPHERE. In the

second solution, the disambiguation process contin-

ues and further classifies the word to be a reference

to ATMOSPHERE OF EARTH. To get to this final

classification, the process passes through an inter-

mediate binary classification level where it deter-

mines whether the word has a more specific sense

covered in Wikipedia, corresponding to the artificial

category ATMOSPHERE (S). If the answer is no, the

system stops the disambiguation process and out-

puts the general sense category ATMOSPHERE. This

basic sense hierarchy can be replicated depending

on the existence of even finer sense distinctions in

Wikipedia. For example, Wikipedia articles describ-

ing atmospheres of particular stars could be used to

further refine STELLAR ATMOSPHERE with two ad-

ditional levels of the type Level 2 and Level 3. Over-

all, the proposed disambiguation scheme could be

used to relabel the ATMOSPHERE links in Wikipedia

with more specific, and therefore more informative,

senses such as ATMOSPHERE OF EARTH. In gen-

eral, the Wikipedia category graph could be used

to automatically create hierarchical structures for re-

25



Figure 2: Hierarchical disambiguation scheme, from coarse to fine grained senses.

lated senses of the same word.

Training word sense classifiers for Levels 1 and 3

is straightforward. For Level 1, Wikipedia links that

are annotated by users as ATMOSPHERE, ATMO-

SPHERE OF EARTH, ATMOSPHERE OF MARS, AT-

MOSPHERE OF VENUS, or STELLAR ATMOSPHERE

are collected as training examples for the general

sense category ATMOSPHERE. Similarly, links that

are annotated as ATMOSPHERE (UNIT) and ATMO-

SPHERE (MUSIC GROUP) will be used as training

examples for the two categories, respectively. A

multiclass classifier is then trained to distinguish be-

tween the three categories at this level. For Level 3,

a multiclass classifiers is trained on Wikipedia links

collected for each of the 4 specific senses.

For the binary classifier at Level 2, we could

use as training examples for the category ATMO-

SPHERE (G) all Wikipedia links that were anno-

tated as ATMOSPHERE, whereas for the category

ATMOSPHERE (S) we could use as training exam-

ples all Wikipedia links that were annotated specif-

ically as ATMOSPHERE OF EARTH, ATMOSPHERE

OF MARS, ATMOSPHERE OF VENUS, or STELLAR

ATMOSPHERE. A traditional binary classification

SVM could be trained on this dataset to distinguish

between the two categories. We call this approach

Naive SVM, since it does not account for the fact that

a significant number of the links that are annotated

by Wikipedia contributors as ATMOSPHERE should

actually belong to the ATMOSPHERE (S) category –

about 60% of them, according to Table 1. Instead,

we propose treating all ATMOSPHERE links as unla-

beled examples. If we consider the specific links in

ATMOSPHERE (S) to be positive examples, then the

problem becomes one of learning with positive and

unlabeled examples.

3.1 Learning with positive and unlabeled

examples

This general type of semi-supervised learning has

been studied before in the context of tasks such

as text classification and information retrieval (Lee

and Liu, 2003; Liu et al., 2003), or bioinformat-

ics (Elkan and Noto, 2008; Noto et al., 2008). In

this setting, the training data consists of positive ex-

amples x ∈ P and unlabeled examples x ∈ U .
Following the notation of Elkan and Noto (2008),

we define s(x) = 1 if the example is positive and
s(x) = −1 if the example is unlabeled. The true
label of an example is y(x) = 1 if the example
is positive and y(x) = −1 if the example is neg-
ative. Thus, x ∈ P ⇒ s(x) = y(x) = 1 and
x ∈ U ⇒ s(x) = −1 i.e., the true label y(x) of an
unlabeled example is unknown. For the experiments

reported in this paper, we use our implementation

of two state-of-the-art approaches to Learning with

Positive and Unlabeled (LPU) examples: the Biased

SVM formulation of Lee and Liu (2003) and the

Weighted Samples SVM formulation of Elkan and

Noto (2008). The original version of Biased SVM

was designed to maximize the product between pre-

cision and recall. In the next section we describe a

26



modification to the Biased SVM approach that can

be used to maximize accuracy, a measure that is of-

ten used to evaluate WSD performance.

3.1.1 The Biased SVM

In the Biased SVM formulation (Lee and Liu,

2003; Liu et al., 2003), all unlabeled examples are

considered to be negative and the decision function

f(x) = wTφ(x) + b is learned using the standard
soft-margin SVM formulation shown in Figure 3.

minimize:
1

2
‖w‖2 + CP

∑

x∈P

ξx + CU
∑

x∈U

ξx

subject to: s(x)
(

w
Tφ(x) + b

)

≥ 1− ξx

ξx ≥ 0, ∀x ∈ P ∪ U

Figure 3: Biased SVM optimization problem.

The capacity parameters CP and CU control how
much we penalize errors on positive examples vs. er-

rors on unlabeled examples. Since not all unlabeled

examples are negative, one would want to select ca-

pacity parameters satisfying CP > CU , such that
false negative errors are penalized more than false

positive errors. In order to find the best capacity pa-

rameters to use during training, the Biased SVM ap-

proach runs a grid search on a separate development

dataset. This search is aimed at finding values for

the parameters CP and CU that maximize pr, the
product between precision p = p(y = 1|f = 1) and
recall r = p(f = 1|y = 1). Lee and Liu (2003)
show that maximizing the pr criterion is equivalent
with maximizing the objective r2/p(f = 1), where
both r = p(f = 1|y = 1) and p(f = 1) can be es-
timated using the trained decision function f(x) on
the development dataset.

Maximizing the pr criterion in the original Biased
SVM formulation was motivated by the need to opti-

mize the F measure in information retrieval settings,
where F = 2pr(p+ r). In the rest of this section we
show that classification accuracy can be maximized

using only positive and unlabeled examples, an im-

portant result for problems where classification ac-

curacy is the target performance measure.

The accuracy of a binary decision function f(x)
is, by definition, acc = p(f = 1|y = 1) + p(f =

−1|y = −1). Since the recall is r = p(f = 1|y =
1), the accuracy can be re-written as:

acc = r + 1− p(f = 1|y = −1) (1)

Using Bayes’ rule twice, the false positive term
p(f = 1|y = −1) can be re-written as:

p(f = 1|y = −1) =
p(f = 1)p(y = −1|f = 1)

p(y = −1)

=
p(f = 1)

p(y = −1)
× (1− p(y = 1|f = 1))

=
p(f = 1)

p(y = −1)
−

p(f = 1)

p(y = −1)
×

p(y = 1)p(f = 1|y = 1)

p(f = 1)

=
p(f = 1)− p(y = 1)× r

p(y = −1)
(2)

Plugging identity 2 in Equation 1 leads to:

acc = 1 + r +
r × p(y = 1)− p(f = 1)

p(y = −1)

= 1 +
r − p(f = 1)

p(y = −1)
(3)

Since p(y = −1) can be assimilated with a con-
stant, Equation 3 implies that maximizing accu-

racy is equivalent with maximizing the criterion

r − p(f = 1), where both the recall r and p(f = 1)
can be estimated on the positive and unlabeled ex-

amples from a separate development dataset.

In conclusion, one can use the original Biased

SVM formulation to maximize r2/p(f = 1), which
has been shown by Lee and Liu (2003) to maximize

pr, a criterion that has a similar behavior with the
F-measure used in retrieval applications. Alterna-

tively, if the target performance measure is accuracy,

we can choose instead to maximize r − p(f = 1),
which we have shown above to correspond to accu-

racy maximization.

3.1.2 The Weighted Samples SVM

Elkan and Noto (2008) introduced two ap-

proaches for learning with positive and unlabeled

data. Both approaches are based on the assumption

that labeled examples {x|s(x) = 1} are selected at
random from the positive examples {x|y(x) = 1}
i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their
best performing approach uses the positive and unla-

beled examples to train two distinct classifiers. First,

the dataset P ∪ U is split into a training set and a
validation set, and a classifier g(x) is trained on the

27



labeling s to approximate the label distribution i.e.
g(x) = p(s = 1|x). The validation set is then used
to estimate p(s = 1|y = 1) as follows:

p(s=1|y=1) = p(s=1|x, y=1) =
1

|P |

∑

x∈P

g(x) (4)

The second and final classifier f(x) is trained on a
dataset of weighted examples that are sampled from

the original training set as follows:

– Each positive example x ∈ P is copied as a
positive example in the new training set with

weight p(y = 1|x, s = 1) = 1.
– Each unlabeled example x ∈ U is duplicated
into two training examples in the new dataset:

a positive example with weight p(y = 1|x, s =
0) and a negative example with weight p(y =
−1|x, s = 0) = 1− p(y = 1|x, s = 0).

Elkan and Noto (2008) show that the weights above
can be derived as:

p(y=1|x, s=0) =
1−p(s=1|y=1)

p(s=1|y=1)
×

p(s=1|x)

1−p(s=1|x)
(5)

The output of the first classifier g(x) is used to
approximate the probability p(s = 1|x), whereas
p(s = 1|y = 1) is estimated using Equation 4.
The two classifiers g and f are trained using

SVMs and a linear kernel. Platt scaling is used with

the first classifier to obtain the probability estimates

g(x) = p(s = 1|x), which are then converted into
weights following Equations 4 and 5, and used dur-

ing the training of the second classifier.

4 Experimental Evaluation

We ran disambiguation experiments on the 6 am-

biguous words atmosphere, president, dollar, game,

diamond andCorinth. The correspondingWikipedia

sense repositories have been summarized in Tables 1

and 2. All WSD classifiers used the same set of stan-

dard WSD features (Ng and Lee, 1996; Stevenson

and Wilks, 2001), such as words and their part-of-

speech tags in a window of 3 words around the am-

biguous keyword, the unigram and bigram content

words that are within 2 sentences of the current sen-

tence, the syntactic governor of the keyword, and

its chains of syntactic dependencies of lengths up to

two. Furthermore, for each example, a Wikipedia

specific feature was computed as the cosine similar-

ity between the context of the ambiguous word and

the text of the article for the target sense or reference.

The Level1 and Level3 classifiers were trained us-

ing the SVMmulti component of the SVMlight pack-

age.1 TheWSD classifiers were evaluated in a 4-fold

cross validation scenario in which 50% of the data

was used for training, 25% for tuning the capacity

parameter C, and 25% for testing. The final accu-
racy numbers, shown in Table 3, were computed by

averaging the results over the 4 folds. Since the word

president has only one sense on Level1, no classifier

needed to be trained for this case. Similarly, words

diamond andCorinth have only one sense on Level3.

atmosphere president dollar

Level1 93.1% — 94.1%

Level3 85.6% 82.2% 90.8%

game diamond Corinth

Level1 82.9% 95.5% 92.7%

Level3 92.9% — —

Table 3: Disambiguation accuracy at Levels 1 & 3.

The evaluation of the binary classifiers at the sec-

ond level follows the same 4-fold cross validation

scheme that was used for Level1 and Level3. The

manual labels for specific senses and references in

the unlabeled datasets are always ignored during

training and tuning and used only during testing.

We compare the Naive SVM, Biased SVM, and

Weighted SVM in the two evaluation settings, using

for all of them the same train/development/test splits

of the data and the same features. We emphasize

that our manual labels are used only for testing pur-

poses – the manual labels are ignored during train-

ing and tuning, when the data is assumed to contain

only positive and unlabeled examples. We imple-

mented the Biased SVM approach on top of the bi-

nary SVMlight package. TheCP andCU parameters
of the Biased SVM were tuned through the c and j
parameters of SVMlight (c = CU and j = CP /CU ).
Eventually, all three methods use the development

data for tuning the c and j parameters of the SVM.
However, whereas the Naive SVM tunes these pa-

rameters to optimize the accuracy with respect to the

noisy label s(x), the Biased SVM tunes the same pa-
rameters to maximize an estimate of the accuracy or

1http://svmlight.joachims.org

28



F-measure with respect to the true label y(x). The
Weighted SVM approach was implemented on top

of the LibSVM2 package. Even though the original

Weighted SVM method of Elkan and Noto (2008)

does not specify tuning any parameters, we noticed

it gave better results when the capacity c and weight
j parameters were tuned for the first classifier g(x).

Table 4 shows the accuracy results of the three

methods for Level2, whereas Table 5 shows the F-

measure results. The Biased SVM outperforms the

Naive SVM on all the words, in terms of both ac-

curacy and F-measure. The most dramatic increases

are seen for the words atmosphere, game, diamond,

and Corinth. For these words, the number of pos-

itive examples is significantly smaller compared to

the total number of positive and unlabeled examples.

Thus, the percentage of positive examples relative to

the total number of positive and unlabeled examples

is 31.9% for atmosphere, 29.1% for game, 9.0% for

diamond, and 11.6% for Corinth. The positive to to-

tal ratio is however significantly larger for the other

two words: 67.2% for president and 91.5% for dol-

lar. When the number of positive examples is large,

the false negative noise from the unlabeled dataset

in the Naive SVM approach will be relatively small,

hence the good performance of Naive SVM in these

cases. To check whether this is the case, we have

also run experiments where we used only half of

the available positive examples for the word presi-

dent and one tenth of the positive examples for the

word dollar, such that the positive datasets became

comparable in size with the unlabeled datasets. The

results for these experiments are shown in Tables 4

and 5 in the rows labeled presidentS and dollarS . As

expected, the difference between the performance of

Naive SVM and Biased SVM gets larger on these

smaller datasets, especially for the word dollar.

The Weighted SVM outperforms the Naive SVM

on five out of the six words, the exception being the

word president. Comparatively, the Biased SVM

has a more stable behavior and overall results in a

more substantial improvement over the Naive SVM.

Based on these initial results, we see the Biased

SVM as the method of choice for learning with pos-

itive and unlabeled examples in the task of coarse to

fine grained sense disambiguation in Wikipedia.

2http://www.csie.ntu.edu.tw/˜cjlin/libsvm

Word NaiveSVM BiasedSVM WeightedSVM

atmosphere 39.9% 79.6% 75.0%

president 91.9% 92.5% 89.5%

dollar 96.0% 97.0% 97.1%

game 83.8% 87.1% 84.6%

diamond 70.2% 74.5% 75.1%

Corinth 46.2% 75.1% 51.9%

presidentS 88.1% 90.6% 87.4%

dollarS 70.3% 84.9% 70.6%

Table 4: Disambiguation accuracy at Level2.

Word NaiveSVM BiasedSVM WeightedSVM

atmosphere 30.5% 86.0% 83.2%

president 94.4% 95.0% 92.8%

dollar 97.9% 98.4% 98.5%

game 75.1% 81.8% 77.5%

diamond 8.6% 53.5% 46.3%

Corinth 15.3% 81.2% 68.0%

presidentS 90.0% 92.4% 89.5%

dollarS 77.9% 91.2% 78.2%

Table 5: Disambiguation F-measure at Level2.

In a final set of experiments, we compared the

traditional flat classification approach and our pro-

posed hierarchical classifier in terms of their over-

all disambiguation accuracy. In these experiments,

the sense repository contains all the leaf nodes as

distinct sense categories. For example, the word

atmosphere would correspond to the sense repos-

itory R = {ATMOSPHERE (G), ATMOSPHERE OF
EARTH, ATMOSPHERE OF MARS, ATMOSPHERE

OF VENUS, STELLAR ATMOSPHERE, ATMO-

SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.
The overall accuracy results are shown in Table 6

and confirm the utility of using the LPU framework

in the hierarchical model, which outperforms the tra-

ditional flat model, especially on words with low ra-

tio of positive to unlabeled examples.

atmosphere president dollar

Flat 52.4% 89.4% 90.0%

Hierarchical 79.7% 91.0% 90.1%

game diamond Corinth

Flat 83.6% 65.7% 42.6%

Hierarchical 87.2% 76.8% 72.1%

Table 6: Flat vs. Hierarchical disambiguation accuracy.

29



5 Future Work

Annotation inconsistencies in Wikipedia were cir-

cumvented by adapting two existing approaches that

use only positive and unlabeled data to train binary

classifiers. This binary classification constraint led

to the introduction of the artificial specific (S) cat-

egory on Level2 in our disambiguation framework.

In future work, we plan to investigate a direct exten-

sion of learning with positive and unlabeled data to

the case of multiclass classification, which will re-

duce the number of classification levels from 3 to 2.

We also plan to investigate the use of unsupervised

techniques in order to incorporate less popular refer-

ences of a word in the hierarchical classification.

Conclusion

We presented an approach to training coarse to fine

grained sense disambiguation systems that treats

annotation inconsistencies in Wikipedia under the

framework of learning with positive and unlabeled

examples. Furthermore, we showed that the true ac-

curacy of a decision function can be optimized us-

ing only positive and unlabeled examples. For test-

ing purposes, we manually annotated 7,079 links be-

longing to six ambiguous words 3. Experimental

results demonstrate that accounting for annotation

ambiguity in Wikipedia links leads to consistent im-

provements in disambiguation accuracy. The man-

ual annotations were only used for testing and were

ignored during training and development. Conse-

quently, the proposed framework of learning with

positive and unlabeled examples for sense disam-

biguation could be applied on the entire Wikipedia

without any manual annotations. By augmenting

general sense links with links to more specific ar-

ticles, such an application could have a significant

impact on Wikipedia itself.

Acknowledgments

This work was supported in part by the Na-

tional Science Foundation IIS awards #1018613 and

#1018590, and an allocation of computing time from

the Ohio Supercomputer Center.

3Data and code will be made publicly available.

References

D. Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Ri-

jke, and S. Schlobach. 2004. Using Wikipedia at the

TREC QA track. In Proceedings of the 13th Text Re-

trieval Conference (TREC 2004).

Volha Bryl, Claudio Giuliano, Luciano Serafini, and

Kateryna Tymoshenko. 2010. Using background

knowledge to support coreference resolution. In Pro-

ceedings of the 2010 conference on ECAI 2010: 19th

European Conference on Artificial Intelligence, pages

759–764, Amsterdam, The Netherlands.

Razvan Bunescu and Marius Pasca. 2006. Using ency-

clopedic knowledge for named entity disambiguation.

In Proceesings of the 11th Conference of the European

Chapter of the Association for Computational Linguis-

tics (EACL-06), pages 9–16, Trento, Italy.

Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp

Sorg, and Steffen Staab. 2009. Explicit versus la-

tent concept models for cross-language information re-

trieval. In International Joint Conference on Artificial

Intelligence (IJCAI-09, pages 1513–1518, Pasadena,

CA, july.

S. Cucerzan. 2007. Large-scale named entity disam-

biguation based on Wikipedia data. In Proceedings of

the Conference on Empirical Methods in Natural Lan-

guage Processing, pages 708–716.

Charles Elkan and Keith Noto. 2008. Learning clas-

sifiers from only positive and unlabeled data. In

Proceedings of the 14th ACM SIGKDD international

conference on Knowledge discovery and data mining,

KDD ’08, pages 213–220.

David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,

James Fan, David Gondek, Aditya Kalyanpur, Adam

Lally, J. William Murdock, Eric Nyberg, John M.

Prager, Nico Schlaefer, and Christopher A. Welty.

2010. Building watson: An overview of the deepqa

project. AI Magazine, 31(3):59–79.

Aria Haghighi and Dan Klein. 2009. Simple coreference

resolution with rich syntactic and semantic features.

In Proceedings of the 2009 Conference on Empiri-

cal Methods in Natural Language Processing, pages

1152–1161, Singapore, August.

M. Kaisser. 2008. The QuALiM question answering

demo: Supplementing answers with paragraphs drawn

from Wikipedia. In Proceedings of the ACL-08 Hu-

man Language Technology Demo Session, pages 32–

35, Columbus, Ohio.

Wee Sun Lee and Bing Liu. 2003. Learning with pos-

itive and unlabeled examples using weighted logistic

regression. In Proceedings of the Twentieth Interna-

tional Conference on Machine Learning (ICML, pages

448–455, Washington, DC, August.

30



Y. Li, R. Luk, E. Ho, and K. Chung. 2007. Improv-

ing weak ad-hoc queries using Wikipedia as external

corpus. In Proceedings of the 30th Annual Interna-

tional ACM SIGIR Conference on Research and De-

velopment in Information Retrieval, pages 797–798,

Amsterdam, Netherlands.

Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and

Philip S. Yu. 2003. Building text classifiers using pos-

itive and unlabeled examples. In Proceedings of the

Third IEEE International Conference on Data Mining,

ICDM ’03, pages 179–186, Washington, DC, USA.

R. Mihalcea. 2007. Using Wikipedia for automatic word

sense disambiguation. In Human Language Technolo-

gies 2007: The Conference of the North American

Chapter of the Association for Computational Linguis-

tics, pages 196–203, Rochester, New York, April.

D. Milne. 2007. Computing semantic relatedness using

Wikipedia link structure. In Proceedings of the New

Zealand Computer Science Research Student Confer-

ence, Hamilton, New Zealand.

Hwee Tou Ng and H. B. Lee. 1996. Integrating multiple

knowledge sources to disambiguate word sense: An

exemplar-based approach. In Proceedings of the 34th

Annual Meeting of the Association for Computational

Linguistics (ACL-96), pages 40–47, Santa Cruz, CA.

Keith Noto, Milton H. Saier, Jr., and Charles Elkan.

2008. Learning to find relevant biological articles

without negative training examples. In Proceedings of

the 21st Australasian Joint Conference on Artificial In-

telligence: Advances in Artificial Intelligence, AI ’08,

pages 202–213.

Simone Paolo Ponzetto and Roberto Navigli. 2010.

Knowledge-rich word sense disambiguation rivaling

supervised systems. In Proceedings of the 48th Annual

Meeting of the Association for Computational Linguis-

tics, pages 1522–1531, Stroudsburg, PA, USA. Asso-

ciation for Computational Linguistics.

Simone Paolo Ponzetto and Michael Strube. 2006. Ex-

ploiting semantic role labeling, wordnet and wikipedia

for coreference resolution. In Proceedings of the Hu-

man Language Technology Conference of the North

American Chapter of the Association of Computa-

tional Linguistics, pages 192–199.

M. Potthast, B. Stein, and M. A. Anderka. 2008.

Wikipedia-based multilingual retrieval model. In Pro-

ceedings of the 30th European Conference on IR Re-

search, Glasgow.

Altaf Rahman and Vincent Ng. 2011. Coreference res-

olution with world knowledge. In Proceedings of the

49th Annual Meeting of the Association for Compu-

tational Linguistics: Human Language Technologies -

Volume 1, pages 814–824, Stroudsburg, PA, USA. As-

sociation for Computational Linguistics.

Mark Stevenson and YorickWilks. 2001. The interaction

of knowledge sources in word sense disambiguation.

Computational Linguistics, 27(3):321–349, Septem-

ber.

31


