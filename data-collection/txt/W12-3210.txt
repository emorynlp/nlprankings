










































Towards an ACL Anthology Corpus with Logical Document Structure. An Overview of the ACL 2012 Contributed Task


Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88–97,
Jeju, Republic of Korea, 10 July 2012. c©2012 Association for Computational Linguistics

Towards an ACL Anthology Corpus with Logical Document Structure
An Overview of the ACL 2012 Contributed Task

Ulrich Schäfer
DFKI Language Technology Lab

Campus D 3 1
D-66123 Saarbrücken, Germany
ulrich.schaefer@dfki.de

Jonathon Read, Stephan Oepen
Department of Informatics

Universitetet i Oslo
0316 Oslo, Norway

{jread |oe}@ifi.uio.no

Abstract

The ACL 2012 Contributed Task is a com-
munity effort aiming to provide the full ACL
Anthology as a high-quality corpus with rich
markup, following the TEI P5 guidelines—
a new resource dubbed the ACL Anthology
Corpus (AAC). The goal of the task is three-
fold: (a) to provide a shared resource for ex-
perimentation on scientific text; (b) to serve
as a basis for advanced search over the ACL
Anthology, based on textual content and cita-
tions; and, by combining the aforementioned
goals, (c) to present a showcase of the benefits
of natural language processing to a broader au-
dience. The Contributed Task extends the cur-
rent Anthology Reference Corpus (ARC) both
in size, quality, and by aiming to provide tools
that allow the corpus to be automatically ex-
tended with new content—be they scanned or
born-digital.

1 Introduction—Motivation

The collection of the Association for Computational
Linguistics (ACL) Anthology began in 2002, with
3,100 scanned and born-digital1 PDF papers. Since
then, the ACL Anthology has become the open ac-
cess collection2 of scientific papers in the area of
Computational Linguistics and Language Technol-
ogy. It contains conference and workshop proceed-
ings and the journal Computational Linguistics (for-
merly the American Journal of Computational Lin-
guistics). As of Spring 2012, the ACL Anthol-

1The term born-digital means natively digital, i.e. prepared
electronically using typesetting systems like LATEX, OpenOffice,
and the like—as opposed to digitized (or scanned) documents.

2http://aclweb.org/anthology

ogy comprises approximately 23,000 papers from 46
years.

Bird et al. (2008) started collecting not only the
PDF documents, but also providing the textual con-
tent of the Anthology as a corpus, the ACL Anthol-
ogy Reference Corpus3 (ACL-ARC). This text ver-
sion was generated fully automatically and in differ-
ent formats (see Section 2.2 below), using off-the-
shelf tools and yielding somewhat variable quality.

The main goal was to provide a reference cor-
pus with fixed releases that researchers could use
and refer to for comparison. In addition, the vision
was formulated that manually corrected ground-
truth subsets could be compiled. This is accom-
plished so far for citation links from paper to paper
inside the Anthology for a controlled subset. The
focus thus was laid on bibliographic and bibliomet-
ric research and resulted in the ACL Anthology Net-
work (Radev et al., 2009) as a public, manually cor-
rected citation database.

What is currently missing is an easy-to-process
XML variant that contains high-quality running text
and logical markup from the layout, such as section
headings, captions, footnotes, italics etc. In prin-
ciple this could be derived from LATEX source files,
but unfortunately, these are not available, and fur-
thermore a considerable amount of papers have been
typeset with various other word processing software.

Here is where the ACL 2012 Contributed Task
starts: The idea is to combine OCR and PDFBox-
like born-digital text extraction methods and re-
assign font and logical structure information as part
of a rich XML format. The method would rely on
OCR exclusively only in cases where no born-digital

3http://acl-arc.comp.nus.edu.sg

88



PDFs are available—in case of the ACL Anthology
mostly papers published before the year 2000. Cur-
rent results and status updates will always be acces-
sible through the following address:�



�
	http://www.delph-in.net/aac/

We note that manually annotating the ACL An-
thology is not viable. In a feasibility study we took
a set of five eight-page papers. After extracting
the text using PDFBox4 we manually corrected the
output and annotated it with basic document struc-
ture and cross-references; this took 16 person-hours,
which would suggest a rough estimate of some 25
person-years to manually correct and annotate the
current ACL Anthology. Furthermore, the ACL An-
thology grows substantially every year, requiring a
sustained effort.

2 State of Affairs to Date

In the following, we briefly review the current status
of the ACL Anthology and some of its derivatives.

2.1 ACL Anthology

Papers in the current Anthology are in PDF format,
either as scanned bitmaps or digitally typeset with
LATEX or word processing software. Older scanned
papers were often created using type writers, and
sometimes even contained hand-drawn graphics.

2.2 Anthology Reference Corpus (ACL-ARC)

In addition to the PDF documents, the ACL-ARC
also contains (per page and per paper)

• bitmap files (in the PNG file format)

• plain text in ‘normal’ reading order

• formatted text (in two columns for most of the
papers)

• XML raw layout format containing position in-
formation for each word, grouped in lines, with
font information, but no running text variant.

The latter three have been generated using OCR
software (OmniPage) operating on the bitmap files.

4http://pdfbox.apache.org

However, OCR methods tend to introduce charac-
ter and layout recognition errors, from both scanned
and born-digital documents.

The born-digital subset of the ACL-ARC (mostly
papers that appeared in 2000 or later) also contains
PDFBox plain text output. However, this is not
available for approximately 4% of the born-digital
PDFs due to unusual font encodings. Note though,
that extracting text from PDFs in normal reading
order is not a trivial task (Berg et al., 2012), and
many errors exist. Furthermore, the plain text is
not dehyphenated, necessitating a language model
or lexicon-based lookup for post-processing.

2.3 ACL Anthology Network

The ACL Anthology Network (Radev et al., 2009)
is based on the ACL-ARC text outputs. It addition-
ally contains manually-corrected citation graphs, au-
thor and affiliation data for most of the Anthology
(papers until 2009).

2.4 Publications with the ACL Anthology as a
Corpus

We did a little survey in the ACL Anthology of pa-
pers reporting on having used the ACL Anthology as
corpus/dataset. The aim here is to get an overview
and distribution of the different NLP research tasks
that have been pursued using the ACL Anthology as
dataset. There are probably other papers outside the
Anthology itself, but these have not been looked at.

The pioneers working with the Anthology as cor-
pus are Ritchie et al. (2006a, 2006b). They did work
related to citations which also forms the largest topic
cluster of papers applying or using Anthology data.

Later papers on citation analysis, summarization,
classification, etc. are Qazvinian et al. (2010), Abu-
Jbara & Radev (2011), Qazvinian & Radev (2010),
Qazvinian & Radev (2008), Mohammad et al.
(2009), Athar (2011), Schäfer & Kasterka (2010),
and Dong & Schäfer (2011).

Text summarization research is performed in
Qazvinian & Radev (2011) and Agarwal et al.
(2011a, 2011b).

The HOO (“Help our own”) text correction shared
task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-
zovskaya et al., 2011; Dahlmeier et al., 2011) aims
at developing automated tools and techniques that

89



assist authors, e.g. non-native speakers of English,
in writing (better) scientific publications.

Classification/Clustering related publications are
Muthukrishnan et al. (2011) and Mao et al. (2010).

Keyword extraction and topic models based on
Anthology data are addressed in Johri et al. (2011),
Johri et al. (2010), Gupta & Manning (2011), Hall
et al. (2008), Tu et al. (2010) and Daudaravičius
(2012). Reiplinger et al. (2012) use the ACL An-
thology to acquire and refine extraction patterns for
the identification of glossary sentences.

In this workshop several authors have used the
ACL Anthology to analyze the history of compu-
tational linguistics. Radev & Abu-Jbara (2012) ex-
amine research trends through the citing sentences
in the ACL Anthology Network. Anderson et al.
(2012) use the ACL Anthology to perform a people-
centered analysis of the history of computational
linguistics, tracking authors over topical subfields,
identifying epochs and analyzing the evolution of
subfields. Sim et al. (2012) use a citation analysis to
identify the changing factions within the field. Vo-
gel & Jurafsky (2012) use topic models to explore
the research topics of men and women in the ACL
Anthology Network. Gupta & Rosso (2012) look
for evidence of text reuse in the ACL Anthology.

Most of these and related works would benefit
from section (heading) information, and partly the
approaches already used ad hoc solutions to gather
this information from the existing plain text ver-
sions. Rich text markup (e.g. italics, tables) could
also be used for linguistic, multilingual example ex-
traction in the spirit of the ODIN project (Xia &
Lewis, 2008; Xia et al., 2009).

3 Target Text Encoding

To select encoding elements we adopt the TEI P5
Guidelines (TEI Consortium, 2012). The TEI en-
coding scheme was developed with the intention of
being applicable to all types of natural language, and
facilitating the exchange of textual data among re-
searchers across discipline. The guidelines are im-
plemented in XML; we currently use inline markup,
but stand-off annotations have also been applied
(Bański & Przepiórkowski, 2009).

We use a subset of the TEI P5 Guidelines as
not all elements were deemed necessary. This pro-

cess was made easier through Roma5, an online
tool that assists in the development of TEI valida-
tors. We note that, while we initially use a simpli-
fied version, the schemas are readily extensible. For
instance, Przepiórkowski (2009) demonstrates how
constituent and dependency information can be en-
coded following the guidelines, in a manner which
is similar to other prominent standards.

A TEI corpus is typically encoded as a sin-
gle XML document, with several text elements,
which in turn contain front (for abstracts), body
and back elements (for acknowledgements and bib-
liographies). Then, sections are encoded using div
elements (with xml:ids), which contain a heading
(head) and are divided into paragraphs (p). We
aim for accountability when translating between for-
mats; for example, the del element records deletions
(such as dehyphenation at line breaks).

An example of a TEI version of an ACL Anthol-
ogy paper is depicted in Figure 1 on the next page.

4 An Overview of the Contributed Task

The goal of the ACL 2012 Contributed Task is to
provide a high-quality version of the textual content
of the ACL Anthology as a corpus. Its rich text
XML markup will contain information on logical
document structure such as section headings, foot-
notes, table and figure captions, bibliographic ref-
erences, italics/emphasized text portions, non-latin
scripts, etc.

The initial source are the PDF documents of the
Anthology, processed with different text extraction
methods and tools that output XML/HTML. The in-
put to the task itself then consists of two XML for-
mats:

• PaperXML from the ACL Anthology Search-
bench6 (Schäfer et al., 2011) provided
by DFKI Saarbrücken, of all approximately
22,500 papers currently in the Anthology (ex-
cept ROCLING which are mostly in Chi-
nese). These were obtained by running a com-
mercial OCR program and applying logical
markup postprocessing and conversion to XML
(Schäfer & Weitz, 2012).

5http://www.tei-c.org/Roma/
6http://aclasb.dfki.de

90



<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0"

xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en">
<teiHeader>

<fileDesc>
<titleStmt>

<title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title>
<author>

Yusuke Miyao† Rune Sætre† Kenji Sagae† Takuya Matsuzaki† Jun’ichi Tsujii†‡*
†Department of Computer Science, University of Tokyo, Japan
‡School of Computer Science, University of Manchester, UK
National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp

</author>
</titleStmt>
<publicationStmt>

<publisher>Association for Computational Linguistics</publisher>
<pubPlace> Columbus, Ohio, USA</pubPlace>
<date>June 2008</date>

</publicationStmt>
<sourceDesc> [. . . ] </sourceDesc>

</fileDesc>
<encodingDesc> [. . . ] </encodingDesc>

</teiHeader>
<text>

<front>
<div type="abs">

<head>Abstract</head>
<p> [. . . ] </p>

</div>
</front>
<body>

<div xml:id="SE1">
<head>Introduction</head>
<p>

Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame<del type="lb">-</del>
works (<ref target="#BI6">Charniak, 2000</ref>;
[. . . ]

</p>
</div>

</body>
<back>

<div type="ack">
<head>Acknowledgements</head>
<p> [. . . ] </p>

</div>
<div type="bib">

<head>References</head>
<listBibl>

<bibl xml:id="BI1">
D. M. Bikel. 2004. Intricacies of Collins’ parsing model.
<hi rend="italic">Computational Linguistics</hi>, 30(4):479–511.

</bibl>
[. . . ]

</listBibl>
<pb n="54"/>

</div>
</back>

</text>
</TEI>

Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006. Some elements are
truncated ([. . . ]) for brevity.

91



• TEI P5 XML generated by PDFExtract. For pa-
pers from after 1999, an additional high-quality
extraction step took place, applying state-of
the art word boundary and layout recognition
methods directly to the native, logical PDF
structure (Berg et al., 2012). As no charac-
ter recognition errors occur, this will form the
master format for textual content if available.

Because both versions are not perfect, a large, ini-
tial part of the Contributed Task requires automat-
ically adding missing or correcting markup, using
information from OCR where necessary (e.g. for ta-
bles). Hence, for most papers from after 1999 (cur-
rently approx. 70% of the papers), the Contributed
Task can make use of both representations simulta-
neously.

The role of paperXML in the Contributed Task is
to serve as fall-back source (1) for older, scanned
papers (mostly published before the year 2000), for
which born-digital PDF sources are not available,
or (2) for born-digital PDF papers on which the
PDFExtract method failed, or (3) for document parts
where PDFExtract does not output useful markup
such as currently for tables, cf. Section 4.2 below.

A big advantage of PDFExtract is its ability to ex-
tract the full Unicode character range without char-
acter recognition errors, while the OCR-based ex-
traction methods in our setup are basically limited
to Latin1 characters to avoid higher recognition er-
ror rates.

We proposed the following eight areas as possible
subtasks towards our goal.

4.1 Subtask 1: Footnotes
The first task addresses identification of footnotes,
assigning footnote numbers and text, and generating
markup for them in TEI P5 style. For example:
We first determine lexical heads of nonterminal

nodes by using Bikel's implementation of

Collins' head detection algorithm

<note place="foot" n="9">

<hi rend="monospace">http://www.cis.upenn.edu/

~dbikel/software.html</hi>

</note>

(<ref target="#BI1">Bikel, 2004</ref>;

<ref target="#BI11">Collins, 1997</ref>).

Footnotes are handled to some extent in PDFEx-
tract and paperXML, but the results require refine-
ment.

4.2 Subtask 2: Tables

Task 2 identifies figure/table references in running
text and links them to their captions. The latter
will also have to be distinguished from running text.
Furthermore, tables will have to be identified and
transformed into HTML style table markup. This
is currently not generated by PDFExtract, but the
OCR tool used for paperXML generation quite re-
liably recognizes tables and transforms tables into
HTML. Thus, a preliminary solution would be to in-
sert missing table content in PDFExtract output from
the OCR results. In the long run, implementing table
handling in PDFExtract would be desirable.

<ref target="#TA3">Table 3</ref> shows the

time for parsing the entire AImed corpus,...

<figure xml:id="TA3">

<head>Table 3: Parsing time (sec.)</head>

<!-- TEI table content markup here -->

</figure>

4.3 Subtask 3: Bibliographic Markup

The purpose of this task is to identify citations in
text and link them to the bibliographic references
listed at the end of each paper. In TEI markup, bibli-
ographies are contained in listBibl elements. The
contents of listBibl can range from formatted text
to moderately-structured entries (biblStruct) and
fully-structured entries (biblFull). For example:

We follow the PPI extraction method of

<ref target="#BI39">Sætre et al. (2007)</ref>,

which is based on SVMs ...

<div type="bib">

<head>References</head>

<listBibl>

<bibl xml:id="BI39">

R. Sætre, K. Sagae, and J. Tsujii. 2007.

Syntactic features for protein-protein

interaction extraction. In

<hi rend="italic">LBM 2007 short papers</hi>.

</bibl>

</listBibl>

</div>

A citation extraction and linking tool that is
known to deliver good results on ACL Anthology
papers (and even comes with CRF models trained
on this corpus) is ParsCit (Councill et al., 2008). In
this volume, Nhat & Bysani (2012) provide an im-
plementation for this task using ParsCit and discuss
possible further improvements.

92



4.4 Subtask 4: De-hyphenation

Both paperXML and PDFExtract output contain soft
hyphenation indicators at places where the original
paper contained a line break with hyphenation. In
paperXML, they are represented by the Unicode soft
hyphen character (in contrast to normal dashes that
also occur). PDFExtract marks hyphenation from
the original text using a special element. How-
ever, both tools make errors: In some cases, the hy-
phens are in fact hard hyphens. The idea of this
task is to combine both sources and possibly ad-
ditional information, as in general the OCR pro-
gram used for paperXML more aggressively pro-
poses de-hyphenation than PDFExtract. Hyphen-
ation in names often persists in paperXML and
therefore remains a problem that will have to be ad-
dressed as well. For example:
In this paper, we present a comparative

eval<del type="lb">-</del>uation of syntactic

parsers and their output

represen<del type="lb">-</del>tations based on

different frameworks:

4.5 Subtask 5: Remove Garbage such as
Leftovers from Figures

In both paperXML and PDFExtract output, text
remains from figures, illustrations and diagrams.
This occurs more frequently in paperXML than in
PDFExtract output because text in bitmap figures
undergoes OCR as well. The goal of this subtask
is to recognize and remove such text.

Bitmaps in born-digital PDFs are embedded ob-
jects for PDFExtract and thus can be detected and
encoded within TEI P5 markup and ignored in the
text extraction process:
<figure xml:id="FI3">

<graphic url="P08-1006/FI3.png" />

<head>

Figure 3: Predicate argument structure

</head>

</figure>

4.6 Subtask 6: Generate TEI P5 Markup for
Scanned Papers from paperXML

Due to the nature of the extraction process, PDFEx-
tract output is not available for older, scanned pa-
pers. These are mostly papers from before 2000, but
also e.g. EACL 2003 papers. On the other hand, pa-
perXML versions exist for almost all papers of the

ACL Anthology, generated from OCR output. They
still need to be transformed to TEI P5, e.g. using
XSLT. The paperXML format and transformation to
TEI P5 is discussed in Schäfer & Weitz (2012) in
this volume.

4.7 Subtask 7: Add Sentence Splitting Markup
Having a standard for sentence splitting with unique
sentence IDs per paper to which everyone can refer
to later could be important. The aim of this task is to
add sentence segmentation to the target markup. It
should be based on an open source tokenizer such as
JTok, a customizable open source tool7 that was also
used for the ACL Anthology Searchbench semantic
index pre-processing, or the Stanford Tokenizer8.
<p><s>PPI extraction is an NLP task to identify

protein pairs that are mentioned as interacting

in biomedical papers.</s> <s>Because the number

of biomedical papers is growing rapidly, it is

impossible for biomedical researchers to read

all papers relevant to their research; thus,

there is an emerging need for reliable IE

technologies, such as PPI identification.

</s></p>

4.8 Subtask 8: Math Formulae
Many papers in the Computational Linguistics area,
especially those dealing with statistical natural lan-
guage processing, contain mathematical formulae.
Neither paperXML nor PDFExtract currently pro-
vide a means to deal with these.

A math formula recognition is a complex task, in-
serting MathML9 formula markup from an external
tool (formula OCR, e.g. from InftyReader10) could
be a viable solution.

For example, the following could become the tar-
get format of MathML embedded in TEI P5, for
∃δ > 0 3 f (x) < 1:
<mrow>

<mo> there exists </mo>

<mrow>

<mrow>

<mi> &#916; <!--GREEK SMALL DELTA--></mi>

<mo> &gt; </mo>

<mn> 0 </mn>

7http://heartofgold.opendfki.de/repos/trunk/

jtok; LPGL license
8http://nlp.stanford.edu/software/tokenizer.

shtml; GPL V2 license
9http://www.w3.org/TR/MathML/

10http://sciaccess.net/en/InftyReader/

93



</mrow>

<mo> such that </mo>

<mrow>

<mrow>

<mi> f </mi>

<mo> &#2061; <!--FUNCTION APPL.--></mo>

<mrow>

<mo> ( </mo>

<mi> x </mi>

<mo> ) </mo>

</mrow>

</mrow>

<mo> &lt; </mo>

<mn> 1 </mn>

</mrow>

</mrow>

</mrow>

An alternative way would be to implement math
formula recognition directly in PDFExtract using
methods known from math OCR, similar to the page
layout recognition approach.

5 Discussion—Outlook

Through the ACL 2012 Contributed Task, we have
taken a (small, some might say) step further towards
the goal of a high-quality, rich-text version of the
ACL Anthology as a corpus—making available both
the original text and logical document structure.

Although many of the subtasks sketched above
did not find volunteers in this round, the Contributed
Task, in our view, is an on-going, long-term com-
munity endeavor. Results to date, if nothing else,
confirm the general suitability of (a) using TEI P5
markup as a shared target representation and (b) ex-
ploiting the complementarity of OCR-based tech-
niques (Schäfer & Weitz, 2012), on the one hand,
and direct interpretation of born-digital PDF files
(Berg et al., 2012), on the other hand. Combin-
ing these approaches has the potential to solve the
venerable challenges that stem from inhomogeneous
sources in the ACL Anthology—e.g. scanned, older
papers and digital newer papers, generated from a
broad variety of typesetting tools.

However, as of mid-2012 there still is no ready-to-
use, high-quality corpus that could serve as a shared
starting point for the range of Anthology-based NLP
activities sketched in Section 1 above. In fact, we
remain slightly ambivalent about our recommenda-
tions for utilizing the current state of affairs and ex-
pected next steps—as we would like to avoid much

work getting underway with a version of the corpus
that we know is unsatisfactory. Further, obviously,
versioning and well-defined release cycles will be a
prerequisite to making the corpus useful for compa-
rable research, as discussed by Bird et al. (2008).

In a nutshell, we see two possible avenues for-
ward. For the ACL 2012 Contributed Task, we col-
lected various views on the corpus data (as well as
some of the source code used in its production) in a
unified SVN repository. Following the open-source,
crowd-sourcing philosophy, one option would be to
make this repository openly available to all inter-
ested parties for future development, possibly aug-
menting it with support infrastructure like, for ex-
ample, a mailing list and shared wiki.

At the same time, our experience from the past
months suggests that it is hard to reach sufficient
momentum and critical mass to make substantial
progress towards our long-term goals, while con-
tributions are limited to loosely organized volun-
teer work. A possibility we believe might overcome
these limitations would be an attempt at formaliz-
ing work in this spirit further, for example through a
funded project (with endorsement and maybe finan-
cial support from organizations like the ACL, ICCL,
AFNLP, ELRA, or LDC).

A potential, but not seriously contemplated ‘busi-
ness model’ for the ACL Anthology Corpus could be
that only groups providing also improved versions
of the corpus would get access to it. This would
contradict the community spirit and other demands,
viz. that all code should be made publicly available
(as open source) that is used to produce the rich-text
XML for new papers added to the Anthology. To de-
cide on the way forward, we will solicit comments
and expressions of interest during ACL 2012, in-
cluding of course from the R50 workshop audience
and participants in the Contributed Task. Current
results and status updates will always be accessible
through the following address:�



�
	http://www.delph-in.net/aac/

The ACL publication process for conferences and
workshops already today supports automated collec-
tion of metadata and uniform layout/branding. For
future high-quality collections of papers in the area
of Computational Linguistics, the ACL could think

94



about providing extended macro packages for con-
ferences and journals that generate rich text and doc-
ument structure preserving (TEI P5) XML versions
as a side effect, in addition to PDF generation. Tech-
nically, it should be possible in both LATEX and (for
sure) in word processors such as OpenOffice or MS
Word. It would help reducing errors induced by
the tedious PDF-to-XML extraction this Contributed
Task dealt with.

Finally, we do think that it will well be possible to
apply the Contributed Task ideas and machinery to
scientific publications in other areas, including the
envisaged NLP research and existing NLP applica-
tions for search, terminology extraction, summariza-
tion, citation analysis, and more.

6 Acknowledgments

The authors would like to thank the ACL, the work-
shop organizer Rafael Banchs, the task contributors
for their pioneering work, and the NUS group for
their support. We are indebted to Rebecca Dridan
for helpful feedback on this work.

The work of the first author has been funded
by the German Federal Ministry of Education and
Research, projects TAKE (FKZ 01IW08003) and
Deependance (FKZ 01IW11003). The second and
third authors are supported by the Norwegian Re-
search Council through the VerdIKT programme.

References

Abu-Jbara, A., & Radev, D. (2011). Coherent
citation-based summarization of scientific papers.
In Proceedings of the 49th annual meeting of the
association for computational linguistics: Human
language techologies (pp. 500–509). Portland,
OR.

Agarwal, N., Reddy, R. S., Gvr, K., & Rosé, C. P.
(2011a). Scisumm: A multi-document summa-
rization system for scientific articles. In Proceed-
ings of the ACL-HLT 2011 system demonstrations
(pp. 115–120). Portland, OR.

Agarwal, N., Reddy, R. S., Gvr, K., & Rosé, C. P.
(2011b). Towards multi-document summarization
of scientific articles: Making interesting compar-
isons with SciSumm. In Proceedings of the work-
shop on automatic summarization for different

genres, media, and languages (pp. 8–15). Port-
land, OR.

Anderson, A., McFarland, D., & Jurafsky, D.
(2012). Towards a computational history of the
ACL:1980–2008. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.

Athar, A. (2011). Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of the ACL 2011 student session (pp. 81–87).
Portland, OR.

Bański, P., & Przepiórkowski, A. (2009). Stand-off
TEI annotation: the case of the National Corpus
of Polish. In Proceedings of the third linguistic
annotation workshop (pp. 64–67). Suntec, Singa-
pore.

Berg, Ø. R., Oepen, S., & Read, J. (2012). To-
wards high-quality text stream extraction from
PDF. Technical background to the ACL 2012
Contributed Task. In Proceedings of the ACL-
2012 main conference workshop on Rediscover-
ing 50 Years of Discoveries. Jeju, Republic of
Korea.

Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,
Kan, M.-Y., Lee, D., Powley, B., Radev, D., &
Tan, Y. F. (2008). The ACL Anthology Reference
Corpus: A reference dataset for bibliographic re-
search in computational linguistics. In Proceed-
ings of the sixth international conference on lan-
guage resources and evaluation (LREC-08). Mar-
rakech, Morocco.

Councill, I. G., Giles, C. L., & Kan, M.-Y. (2008).
ParsCit: An open-source CRF reference string
parsing package. In Proceedings of LREC-2008
(pp. 661–667). Marrakesh, Morocco.

Dahlmeier, D., Ng, H. T., & Tran, T. P. (2011). NUS
at the HOO 2011 pilot shared task. In Proceedings
of the generation challenges session at the 13th
european workshop on natural language genera-
tion (pp. 257–259). Nancy, France.

Dale, R., & Kilgarriff, A. (2010). Helping Our Own:
Text massaging for computational linguistics as a
new shared task. In Proceedings of the 6th inter-
national natural language generation conference.
Trim, Co. Meath, Ireland.

95



Daudaravičius, V. (2012). Applying collocation seg-
mentation to the ACL Anthology Reference Cor-
pus. In Proceedings of the ACL-2012 main con-
ference workshop: Rediscovering 50 years of dis-
coveries. Jeju, Republic of Korea.

Dong, C., & Schäfer, U. (2011). Ensemble-style
self-training on citation classification. In Pro-
ceedings of 5th international joint conference on
natural language processing (pp. 623–631). Chi-
ang Mai, Thailand.

Gupta, P., & Rosso, P. (2012). Text reuse with
ACL: (upward) trends. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.

Gupta, S., & Manning, C. (2011). Analyzing the
dynamics of research by extracting key aspects of
scientific papers. In Proceedings of 5th interna-
tional joint conference on natural language pro-
cessing (pp. 1–9). Chiang Mai, Thailand.

Hall, D., Jurafsky, D., & Manning, C. D. (2008).
Studying the history of ideas using topic models.
In Proceedings of the 2008 conference on empir-
ical methods in natural language processing (pp.
363–371). Honolulu, Hawaii.

Johri, N., Ramage, D., McFarland, D., & Jurafsky,
D. (2011). A study of academic collaborations
in computational linguistics using a latent mix-
ture of authors model. In Proceedings of the 5th
ACL-HLT workshop on language technology for
cultural heritage, social sciences, and humanities
(pp. 124–132). Portland, OR.

Johri, N., Roth, D., & Tu, Y. (2010). Experts’
retrieval with multiword-enhanced author topic
model. In Proceedings of the NAACL HLT 2010
workshop on semantic search (pp. 10–18). Los
Angeles, California.

Mao, Y., Balasubramanian, K., & Lebanon, G.
(2010). Dimensionality reduction for text using
domain knowledge. In COLING 2010: Posters
(pp. 801–809). Beijing, China.

Mohammad, S., Dorr, B., Egan, M., Hassan, A.,
Muthukrishan, P., Qazvinian, V., Radev, D., & Za-
jic, D. (2009). Using citations to generate surveys

of scientific paradigms. In Proceedings of human
language technologies: The 2009 annual confer-
ence of the north american chapter of the associa-
tion for computational linguistics (pp. 584–592).
Boulder, Colorado.

Muthukrishnan, P., Radev, D., & Mei, Q. (2011). Si-
multaneous similarity learning and feature-weight
learning for document clustering. In Proceedings
of textgraphs-6: Graph-based methods for natu-
ral language processing (pp. 42–50). Portland,
OR.

Nhat, H. D. H., & Bysani, P. (2012). Linking ci-
tations to their bibliographic references. In Pro-
ceedings of the ACL-2012 main conference work-
shop: Rediscovering 50 years of discoveries. Jeju,
Republic of Korea.

Przepiórkowski, A. (2009). TEI P5 as an XML stan-
dard for treebank encoding. In Proceedings of the
eighth international workshop on treebanks and
linguistic theories (pp. 149–160). Milano, Italy.

Qazvinian, V., & Radev, D. R. (2008). Scientific
paper summarization using citation summary net-
works. In Proceedings of the 22nd international
conference on computational linguistics (COL-
ING 2008) (pp. 689–696). Manchester, UK.

Qazvinian, V., & Radev, D. R. (2010). Identi-
fying non-explicit citing sentences for citation-
based summarization. In Proceedings of the 48th
annual meeting of the association for computa-
tional linguistics (pp. 555–564). Uppsala, Swe-
den.

Qazvinian, V., & Radev, D. R. (2011). Learning
from collective human behavior to introduce di-
versity in lexical choice. In Proceedings of the
49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies (pp. 1098–1108). Portland, OR.

Qazvinian, V., Radev, D. R., & Ozgur, A. (2010).
Citation summarization through keyphrase ex-
traction. In Proceedings of the 23rd international
conference on computational linguistics (COL-
ING 2010) (pp. 895–903). Beijing, China.

Radev, D., & Abu-Jbara, A. (2012). Rediscovering
ACL discoveries through the lens of ACL Anthol-
ogy Network citing sentences. In Proceedings of

96



the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.

Radev, D., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network corpus. In
Proceedings of the 2009 workshop on text and
citation analysis for scholarly digital libraries.
Morristown, NJ, USA.

Radev, D. R., Muthukrishnan, P., & Qazvinian, V.
(2009). The ACL Anthology Network. In Pro-
ceedings of the 2009 workshop on text and cita-
tion analysis for scholarly digital libraries (pp.
54–61). Suntec City, Singapore.

Reiplinger, M., Schäfer, U., & Wolska, M. (2012).
Extracting glossary sentences from scholarly ar-
ticles: A comparative evaluation of pattern boot-
strapping and deep analysis. In Proceedings of the
ACL-2012 main conference workshop: Rediscov-
ering 50 years of discoveries. Jeju, Republic of
Korea.

Ritchie, A., Teufel, S., & Robertson, S. (2006a).
Creating a test collection for citation-based IR ex-
periments. In Proceedings of the human language
technology conference of the NAACL, main con-
ference (pp. 391–398). New York City.

Ritchie, A., Teufel, S., & Robertson, S. (2006b).
How to find better index terms through cita-
tions. In Proceedings of the workshop on how can
computational linguistics improve information re-
trieval? (pp. 25–32). Sydney, Australia.

Rozovskaya, A., Sammons, M., Gioja, J., & Roth,
D. (2011). University of illinois system in HOO
text correction shared task. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 263–266). Nancy, France.

Schäfer, U., & Kasterka, U. (2010). Scientific au-
thoring support: A tool to navigate in typed cita-
tion graphs. In Proceedings of the NAACL HLT
2010 workshop on computational linguistics and
writing: Writing processes and authoring aids
(pp. 7–14). Los Angeles, CA.

Schäfer, U., Kiefer, B., Spurk, C., Steffen, J., &
Wang, R. (2011). The ACL Anthology Search-

bench. In Proceedings of the ACL-HLT 2011 sys-
tem demonstrations (pp. 7–13). Portland, OR.

Schäfer, U., & Weitz, B. (2012). Combining OCR
outputs for logical document structure markup.
Technical background to the ACL 2012 Con-
tributed Task. In Proceedings of the ACL-2012
main conference workshop on Rediscovering 50
Years of Discoveries. Jeju, Republic of Korea.

Sim, Y., Smith, N. A., & Smith, D. A. (2012).
Discovering factions in the computational linguis-
tics community. In Proceedings of the ACL-
2012 main conference workshop: Rediscovering
50 years of discoveries. Jeju, Republic of Korea.

TEI Consortium. (2012, February). TEI P5: Guide-
lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)

Tu, Y., Johri, N., Roth, D., & Hockenmaier, J.
(2010). Citation author topic model in expert
search. In COLING 2010: Posters (pp. 1265–
1273). Beijing, China.

Vogel, A., & Jurafsky, D. (2012). He said, she said:
Gender in the ACL anthology. In Proceedings of
the ACL-2012 main conference workshop: Redis-
covering 50 years of discoveries. Jeju, Republic
of Korea.

Xia, F., Lewis, W., & Poon, H. (2009). Language
ID in the context of harvesting language data off
the web. In Proceedings of the 12th conference
of the european chapter of the ACL (EACL 2009)
(pp. 870–878). Athens, Greece.

Xia, F., & Lewis, W. D. (2008). Repurposing the-
oretical linguistic data for tool development and
search. In Proceedings of the third international
joint conference on natural language processing:
Volume-i (pp. 529–536). Hyderabad, India.

Zesch, T. (2011). Helping Our Own 2011: UKP
lab system description. In Proceedings of the
generation challenges session at the 13th euro-
pean workshop on natural language generation
(pp. 260–262). Nancy, France.

97


