



















































Transducer Disambiguation with Sparse Topological Features


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2275–2280,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Transducer Disambiguation with Sparse Topological Features

Gonzalo Iglesias† Adrià de Gispert†‡ Bill Byrne†‡

†SDL Research, Cambridge, U.K.
{giglesias|agispert|bbyrne}@sdl.com

‡Department of Engineering, University of Cambridge, U.K.

Abstract

We describe a simple and efficient al-
gorithm to disambiguate non-functional
weighted finite state transducers (WFSTs),
i.e. to generate a new WFST that con-
tains a unique, best-scoring path for each
hypothesis in the input labels along with
the best output labels. The algorithm uses
topological features combined with a trop-
ical sparse tuple vector semiring. We em-
pirically show that our algorithm is more
efficient than previous work in a PoS-
tagging disambiguation task. We use our
method to rescore very large translation
lattices with a bilingual neural network
language model, obtaining gains in line
with the literature.

1 Introduction

Weighted finite-state transducers (WFSTs), or lat-
tices, are used in speech and language process-
ing to compactly represent and manipulate a large
number of strings. Applying a finite-state opera-
tion (eg. PoS tagging) to a lattice via composition
produces a WFST that maps input (eg. words)
onto output strings (eg. PoS tags) and preserves
the arc-level alignment between each input and
output symbol (eg. each arc is labeled with a
word-tag pair and has a weight). Typically, the
result of such operation is a WFST that is am-
biguous because it contains multiple paths with the
same input string, and non-functional because it
contains multiple output strings for a given input
string (Mohri, 2009).

Disambiguating such WFSTs is the task of cre-
ating a WFST that encodes only the best-scoring
path of each input string, while still maintaining
the arc-level mapping between input and output
symbols. This is a non-trivial task1, and so far only

1Unless one enumerates all the possible input strings in

one algorithm has been described (Shafran et al.,
2011); the main steps are:

(a) Map the WFST into an equivalent weighted
finite-state automata (WFSA) using weights
that contain both the WFST weight and out-
put symbols (using a special semiring)

(b) Apply WFSA determinization under this
semiring to ensure that only one unique path
per input string survives

(c) Expand the result back to an WFST that pre-
serves arc-level alignments

We present a new disambiguation algorithm
that can efficiently accomplish this. In Section 2
we describe how the tropical sparse tuple vec-
tor semiring can keep track of individual arcs in
the original WFST as topological features during
the mapping step (a). This allows us to describe
in Section 3 an efficient expansion algorithm for
step (c). We show in Section 4 empirical evidence
that our algorithm is more efficient than Shafran et
al. (2011) in their same PoS-tagging task. We also
show how our method can be applied in rescor-
ing translation lattices under a bilingual neural-
network model (Devlin et al., 2014), obtaining
BLEU score gains consistent with the literature.
Section 5 reviews related work and concludes.

2 Semiring Definitions

A WFST T = (Σ,∆, Q, I, F,E, ρ) over a semir-
ing (K,⊕,⊗, 0, 1) has input and output alphabets
Σ and ∆, a set of states Q, the initial state I ∈ Q,
a set of final states F ⊂ Q, a set of transitions
(edges) E ⊂ (Q × Σ ×∆ × K × Q), and a final
state function ρ : F → K. We focus on extensions
to the tropical semiring (R±∞,min,+,∞, 0).
the lattice, searches for the best output string for each in-
put string, and converts the resulting sequences back into a
WFST, which is clearly inefficient.

2275



Let e = (p[e], i[e], o[e], w[e], n[e]) be an edge
in E. A path π = e1...en is a sequence of edges
such that n[ej ] = p[ej+1], 1 ≤ j < n. w[π] =⊗

ej∈π w[ej ] ; p[π] = p[e1], n[π] = n[en]. A path
is accepting if p[π] = I and n[π] ∈ F . The weight
associated by T to a set of paths Π is T (Π) =⊕

π∈Πw[π]⊗ ρ(n[π]).
2.1 Tropical Sparse Vector Semiring
Let f̄ [ei] = fi ∈ RN be the unweighted feature
vector associated with edge ei, and let ᾱ ∈ RN be
a global feature weight vector. The tropical weight
is then found as wi = w[ei] = ᾱ · f̄i =

∑
k αkfi,k.

Given a fixed ᾱ, we define the operators for the
tropical vector semiring as: f̄i ⊕α f̄j = min(ᾱ ·
f̄i, ᾱ · f̄j) and f̄i⊗α f̄j =

∑
k αk(fi,k + fj,k). The

tropical weights are maintained correctly by the
vector semiring as f̄i ⊕α f̄j = wi ⊕ wj and f̄i ⊗α
f̄j = wi⊗wj . Finally, we define the element-wise
times operator as: f̄i ∗ f̄j = f̄m, where fm,k =
fi,k+fj,k, ∀k. It follows thatwi⊗wj = α·(f̄i∗f̄j).

When dealing with high-dimensional feature
vectors which have few non-zero elements, it
is convenient in practice (for computational effi-
ciency) to use a sparse representation for vectors:
f̄ = [(i, fi), i : fi 6= 0]. That is, f̄ is comprised
of a sparse set of tuples (i, fi), where i is a feature
index and fi is its value; e.g. [(2, f2)] is short for
[0, f2, 0, 0] if N = 4.

The semiring that operates on sparse feature
vectors, which we call tropical sparse tuple vec-
tor semiring2, uses conceptually identical opera-
tors as the non-sparse version defined above, so it
also maintains the tropical weights w correctly.

3 A Disambiguation Algorithm

We now describe how we use the semiring de-
scribed in Section 2 for steps (a) and (b), and de-
scribe an expansion algorithm for step (c) that effi-
ciently converts the output of determinization into
an unambiguous WFST with arc-level alignments.

WFSA with Sparse Topological Features
Let T be a tropical-weight WFST with K edges.
T is topologically sorted so that if an edge ek pre-
ceeds an edge ek′ on a path, then k < k′. We now
use tropical sparse tuple vector weights to create
a WFSA A that maintains (in its weights) pointers
to specific edges in T . These ’pointers’ are sparse
topological features.

2We implement this semiring as an extension to the sparse
tuple weights of the OpenFst library (Allauzen et al., 2007).

For each edge ek = (pk, ik, ok, wk, nk) of T ,
we create an edge e′k = (pk, ik, ik, f̄k, nk) in A,
where f̄k = [wk, 0, . . . , 0, 1, 0, . . . , 0]; the 1 is in
the kth position. In other words, fk,0 is the tropical
weight of the kth edge in T and fk,k = 1 indicates
that this tropical weight belongs to edge k in T . In
sparse notation, f̄k = [(0, wk), (k, 1)].
For example, this non-deterministic transducer T :

a : A/2

1

b : B/1
3

b : V/5
4

a : Z/3

2
5

c : D/4

is mapped to acceptorA with topological features:

a/[(0, 2), (1, 1)]
b/[(0, 1), (3, 1)]

b/[(0, 5), (4, 1)]

a/[(0, 3), (2, 1)] c/[(0, 4), (5, 1)]

Given α = [1, 0, . . . , 0], operations on A yield the
same path weights as in the usual tropical semir-
ing.

WFSA determinization
We now apply the standard determinization algo-
rithm to A, which yields AD:

a/[(0, 2), (1, 1)]
b/[(0, 1), (3, 1)]

c/[(0, 5), (1,−1), (2, 1), (5, 1)]

This now accepts only one best-scoring path
for each input string, and the weights ’point’ to
the sequence of arcs traversed by the relevant
path in T . In turn, this reveals the best output
string for the given input string. For example,
the path-level features associated with ‘a b’ are
[(0, 3), (1, 1), (3, 1)], indicating a path π = e1e3
with tropical weight 3 through T (and hence out-
put string ‘AB’).

The topology of AD is compact because multi-
ple input strings may share arcs while still encod-
ing different output strings in their weights. This
is achieved by ‘cancelling’ topological features in
subsequent arcs and ‘replacing’ them by new ones
as one traverses the path. For example, the string
‘a c’ initially has feature (1, 1), but this gets can-
celled later in the path by (1,−1), and replaced by
[(2, 1), (5, 1)], indicating a path π = e2e5 through
T with output string ‘ZD’.

2276



EXPANDTFEA(Ar = (Σ, ∆, Q, I, F, E))
1 I ′ ← (I,0)
2 ENQUEUE(S, I ′)
3 while |S| do
4 (q, f̄)← HEAD(S)
5 DEQUEUE(S)
6 if q ∈ F then
7 F ′ ← F ′ ∪ {(q, f̄))}
8 for each e ∈ E(q) do
9 (w′, t′, f̄ ′)← POPTFEA(f̄ , e)

10 q′ ← (n[e], f̄ ′)
11 e′ ← ((q, f̄), i[e], i[e], [(0, w′), (t′, 1)], q′)
12 Q′ ← Q′ ∪ {q′}
13 E′ ← E′ ∪ {e′}
14 ENQUEUE(S, (n[e], f̄ ′))
15 return Br = (Σ, ∆, Q′, I ′, F ′, E′)

Figure 1: Expansion and topological feature repo-
sitioning algorithm for step (c).

Expansion Algorithm

We now describe an expansion algorithm to con-
vertAD into an unambiguous WFST T ′ that main-
tains the arc-level input-output alignments of the
original transducer T . In our example, T ′ should
be identical to T except for edge 4, which is re-
moved.

Due to the WFSA determinization algorithm,
we observe empirically that the cancelling features
in AD tend to appear in a path after the feature it-
self. This allows us to define an algorithm that tra-
versesAD in reverse (from its final states to its ini-
tial state) and creates an equivalent acceptor with
the topology of T ′.

The algorithm is described in Figure 1. It per-
forms a forward pass through Ar (the reverse of
AD). The intuition is that, for each arc, we cre-
ate a new arc where we ‘pop’ the highest topo-
logical feature (as it will not be cancelled later)
and its tropical weight. The new states encode the
original state q and the residual features that have
not been ‘popped’ yet. For each edge E(q), the
auxiliary POPTFEA(f̄ , e) returns a (w′, t′, f̄ ′) tu-
ple, where w′ is the tropical weight obtained as
f̄ ⊗α f̄ [e] (which is equivalent to f0 + f [e]0 given
our ᾱ); t′ is the index of the highest topological
feature of f̄ ∗ f̄ [e]; and f̄ ′ is the vector of resid-
ual topological features after excluding f0 and ft′ ,
that is, f̄ ∗ f̄ [e] ∗ [(0,−w′), (t′,−1)]. For exam-
ple, POPTFEA([(0, 5), (1,−1), (2, 1), (6, 1)]) re-
turns (5, 6, [(1,−1), (2, 1)]); if w has only one
topological feature, the residual is 0. The residual
in all final states of Br will be 0 (no topological
features still to be popped).

Graphically, in our running example Ar is:

1 2 3
a/[(0, 2), (1, 1)]

b/[(0, 1), (3, 1)]

c/[(0, 5), (1,−1), (2, 1), (5, 1)]

and the output is Br:

(1,0)

(2,0)

(3,0)

(2, [(1,−1), (2, 1)])

b/[(0, 1), (3, 1)] a/[(0, 2), (1, 1)]

c/[(0, 5), (5, 1)] a/[(0, 2), (2, 1)]

Reversing Br yields an acceptor B (still in
the sparse tuple vector semiring) which has the
same topology as our goal T ′ and can be trivially
mapped to T ′ in linear time: each arc takes the
tropical weight via ᾱ and has only one topological
feature which points to the arc in T containing the
required output symbol.

Two-pass Expansion
As mentioned earlier, our algorithm relies on ‘can-
celling’ topological features appearing after the
feature they cancel in a given path. In general,
consider T a weighted transducer andA it’s equiv-
alent automaton with sparse topological features,
as described here. Ap is the result of applying
standard WFST operations, such as determiniza-
tion, minimization or shortest path. Assume as a
final step that the weights have been pushed to-
wards the final states. It is worth noting the prop-
erty that: two topological features in a path ac-
cepted by A will never get reordered in Ap, al-
though they can appear together on the same edge,
as shown in our running example. Indeed, if Ap

contains only one single path, all the topological
features would appear on the final state.

Let us define a function dA(e) as the minimum
number of edges on any path in A from the start
state to n[e] through edge e.

Consider all edges ei in A and ep in Ap, with
f [ei]i = 1 and f [ep]i 6= 0, i.e. we are interested
in the topological feature contribution on ep due
to the edge ei in A. If dA(ei) ≤ dAp(ep) is al-
ways satisfied, then EXPANDTFEA will yield the
correct answer because the residual at each state,
together with the the weight of the current edge,
contains all the necessary information to pop the
next correct topological feature.

However, many deterministic WFSAs will not
exhibit this behaviour (eg. minimised WFSAs),
even after pushing the weights towards the final
states. For example see this acceptor A:

2277



Figure 2: Number of succesfully disambiguated transducers over time for PoS tagged lattices (left) and
HiFST lattices (right).

0

1

3

2

e1 = y/[(0, 1), (1, 1)] e3 = x/[(0, 2), (3, 1)]

e2 = z/[(0, 3), (2, 1)] e4 = x/[(0, 4), (4, 1)]

And Ap is a minimised version of A:

0 1 2

ep3 = x/[(0, 3), (1, 1)(3, 1)]
ep1 = y

ep2 = z/[(0, 4), (1,−1), (3,−1), (2, 1), (4, 1)]

As dA(e3) = dA(e4) = 2 and dAp(e
p
2) = 1,

the distance test fails for both topo-
logical features (3,−1)(4, 1). Running
Br=EXPANDTFEA((Ap)r) will not cancel
feature (3, 1) along the path ‘z x’ and will pop
(4, 1) instead, storing the remaining none-0
residual in a final state of Br.

As mentioned before, two topological features
along the same path in A will not reorder in Ap.
In this example, as (4, 1) appears in edge ep2, fea-
ture (2, 1) must also appear in this edge (or on an
earlier edge, in a more complicated machine). In
general, any remaining topological features along
the path back to the start state of Ap will all be
popped after their correct edges in Br. All edges
inBr pass the distance test compared toAr, the re-
versed form of A: for all edges ei with f [ei]i = 1
inAr and eq inBr such that f [eq]i 6= 0, dAr(ei) ≤
dBr(eq). Edges in these machines are now reverse
sorted, i.e. if an edge ek precedes an edge ek′ on a
path, then k′ < k.

We can perform a second pass with the same al-
gorithm over B, with the only minor modification

that t′ is now the index of the lowest topological
feature of f̄ ∗ f̄ [e]. This expands the acceptor cor-
rectly. Because correct expansions yield 0 residu-
als on the final states, the algorithm can be trivially
modified to trigger the second pass automatically
if the residual on any final state is not 0.

4 Experiments

We evaluate our algorithm, henceforth called topo-
logical, in two ways: we empirically contrast
disambiguation times against previous work, and
then apply it to rescore translation lattices with
bilingual neural network models.

4.1 PoS Transducer Disambiguation

We apply our algorithm to the 4,664 NIST English
CTS RT Dev04 set PoS tagged lattices used by
Sproat el al. (2014); these were generated with a
speech recognizer similar to (Soltau et al., 2005)
and tagged with a WFST-based HMM tagger. The
average number of states is 493. We contrast with
the lexicographic tropical categorial semiring im-
plementation of Shafran et al. (2011), henceforth
referred to as the categorial method.

Figure 2 (left) shows the number of disam-
biguated WFSTs as processing time increases.
The topological algorithm proves much faster (and
we observe no memory footprint differences). In
50ms it disambiguates 3540 transducers, as op-
posed to the 2771 completed by the categorial pro-
cedure; the slowest WFST to disambiguate takes
230 seconds in the categorial procedure and 60
seconds in our method. Using sparse topological
features with our semiring disambiguates all WF-

2278



STs faster in 99.8% of cases.

4.2 Neural Network Bilingual Rescoring
We use the disambiguation algorithm to apply the
bilingual neural network language model (BiLM)
of Devlin et al. (2014) to the output lattices of the
CUED OpenMT12 Arabic-English hierarchical
phrase-based translation system3 using HiFST (de
Gispert et al., 2010). We use a development set
mt0205tune (2075 sentences) and a validation set
mt0205test (2040 sentences) from the NIST MT02
through MT05 evaluation sets.

The edges in these WFSTs are of the form
t:i/w, where t is the target word, i is the source
sentence position t aligns to, and w contains the
translation and language model score. HiFST
outputs these WFSTs by using a standard hi-
ero grammar (Chiang, 2007) augmented with tar-
get side heuristic alignments or affiliations to the
source (Devlin et al., 2014).

In a rule over source and target words

X →< s1X s2 s3, t1X t2 > / 2, 1

the feature ‘2, 1’ indicates that the target word t1
is aligned to source word s2 and that t2 aligns to
s1. As rules are applied in translation, this infor-
mation can be used to link target words to absolute
positions within the source sentence.

Allowing for admissible pruning, all possible
affiliation sequences under the grammar for ev-
ery translation are available in the WFSTs; dis-
ambiguation keeps the best affiliation sequence
for each translation hypothesis, which allows the
rescoring of very large lattices with the BiLM
model.

This disambiguation task involves much bigger
lattices than the POS-tagging task: the average
number of states of the HiFST lattices is 38,200.
Figure 2 (right) shows the number of mt0205tune
disambiguated WFSTs over time compared to the
categorial method. As with the PoS disambigua-
tion task, the topological method is always much
faster than the categorial one. After 10 seconds,
our method has disambiguated 1953 lattices out of
2075, whereas the categorial method has only fin-
ished 1405. The slowest WFST to disambiguate
takes 6700 seconds with the categorial procedure,
which compares to 1000 seconds in our case.

The BiLM model is trained with
NPLM (Vaswani et al., 2013) with a context

3See http://www.nist.gov/itl/iad/mig/openmt12results.cfm.

system mt0205tune mt0205test
baseline 52.2 51.9
+BiLM 53.0 52.9

Table 1: Translation scores in lower-case BLEU.

of 3 source and 4 target words. Lattice rescoring
with this model requires a special variation of the
standard WFST composition which looks at both
input and output labels on a transducer arc; we use
KenLM (Heafield, 2011) to retrieve neural net-
work scores for on-the-fly composition. We retune
the parameters with Lattice MERT (Macherey
et al., 2008) . Results are shown in Table 1.
Acknowledging the task differences with respect
to (Devlin et al., 2014), we find BLEU gains
consistent with rescoring results reported in their
Table 5.

5 Conclusions and Related Work

We have described a tagging disambiguation algo-
rithm that supports non-functional WFSTs, which
cannot be handled directly by neither WFST deter-
minization (Mohri, 1997) nor WFST disambigua-
tion (Mohri, 2012). We show it is faster than
the implementation with a lexicographic-tropical-
categorial semiring described by Shafran et al.
(2011) and describe a use case in a practical
rescoring task of an MT system with bilingual
neural networks that yield 1.0 BLEU gain.

Povey et al. (2012) also use a special semir-
ing that allows to map non-functional WFSTs into
WFSAs by inserting the tag into a string weight.
However, in contrast to our implementation and
that of Shafran et al (2011), no expansion into an
WFST with aligned input/output is described.

Lexicographic semirings, used for PoS tagging
disambiguation (Shafran et al., 2011), have been
also shown to be useful in other tasks (Sproat et
al., 2014), such as optimized epsilon encoding for
backoff language models (Roark et al., 2011), and
hierarchical phrase-based decoding with Push-
down Automata (Allauzen et al., 2014).

The tools for disambiguation and WFST com-
position with bilingual models, along with a tu-
torial to replicate Section 4.2, are all available at
http://ucam-smt.github.io.

Acknowledgments

We thank the authors of (Sproat et al., 2014) for
generously sharing their PoS-tagging experiments.

2279



References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-

jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. In Proceedings of CIAA.

Cyril Allauzen, Bill Byrne, Adrià de Gispert, Gonzalo
Iglesias, and Michael Riley. 2014. Pushdown Au-
tomata in Statistical Machine Translation. Compu-
tational Linguistics, 40(3):687–723.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Adrià de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite-state transducers and shallow-n grammars.
Computational Linguistics, 36(3):505–533.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In Proceedings of
ACL.

Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of
EMNLP.

Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Trans-
lation. In Proceedings of EMNLP.

Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23:269–311.

Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, Monographs in Theoretical Computer Sci-
ence. An EATCS Series, pages 213–254. Springer
Berlin Heidelberg.

Mehryar Mohri. 2012. A Disambiguation Algorithm
for Finite Automata and Functional Transducers. In
Proceedings of CIAA, volume 7381, pages 265–277.

Daniel Povey, Mirko Hannemann, Gilles Boulianne,
Lukas Burget, Arnab Ghoshal, Milos Janda, Mar-
tin Karafiat, Stefan Kombrink, Petr Motlıcek, Yan-
min Qian, Korbinian Riedhammer, Karel Vesely,
and Ngoc Thang Vu. 2012. Generating Exact Lat-
tices in the WFST Framework. In Proceedings of
ICASSP.

Brian Roark, Richard Sproat, and Izhak Shafran. 2011.
Lexicographic Semirings for Exact Automata En-
coding of Sequence Models. In Proceedings of
ACL-HLT.

Izhak Shafran, Richard Sproat, Mahsa Yarmohammadi,
and Brian Roark. 2011. Efficient Determinization
of Tagged Word Lattices using Categorial and Lexi-
cographic Semirings. In Proceedings of ASRU.

Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, and Geoffrey Zweig. 2005.
The IBM 2004 Conversational Telephony System
for Rich Transcription. In Proceedings of ICASSP.

Richard Sproat, Mahsa Yarmohammadi, Izhak Shafran,
and Brian Roark. 2014. Applications of Lex-
icographic Semirings to Problems in Speech and
Language Processing. Computational Linguistics,
40(4):733–761.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-Scale
Neural Language Models Improves Translation. In
Proceedings of EMNLP.

2280


