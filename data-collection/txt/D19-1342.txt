
























































Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3426‚Äì3431,
Hong Kong, China, November 3‚Äì7, 2019. c¬©2019 Association for Computational Linguistics

3426

Recognizing Conflict Opinions in Aspect-level Sentiment Classification
with Dual Attention Networks

Xingwei Tan, Yi Cai‚àó and Changxi Zhu
School of Software Engineering, South China University of Technology

Guangzhou, China
setanxw5@mail.scut.edu.cn

‚àóycai@scut.edu.cn

Abstract

Aspect-level sentiment classification, which is
a fine-grained sentiment analysis task, has re-
ceived lots of attention these years. There is a
phenomenon that people express both positive
and negative sentiments towards an aspect at
the same time. Such opinions with conflict-
ing sentiments, however, are ignored by ex-
isting studies, which design models based on
the absence of them. We argue that the exclu-
sion of conflict opinions is problematic, for the
reason that it represents an important style of
human thinking ‚Äì dialectic thinking. If a real-
world sentiment classification system ignores
the existence of conflict opinions when it is de-
signed, it will incorrectly mixed conflict opin-
ions into other sentiment polarity categories in
action. Existing models have problems when
recognizing conflicting opinions, such as data
sparsity. In this paper, we propose a multi-
label classification model with dual attention
mechanism to address these problems.

1 Introduction

Aspect-level sentiment classification is a fine-
grained sentiment analysis task (Pang et al., 2008).
Given aspect categories or target entities in the
text, this task aims at inferring the sentiment po-
larity (e.g., positive, negative, or neutral) of the
aspects. A recent study (Kenyon-Dean et al.,
2018) on sentiment analysis dataset construction
brings attention to sentiments beyond the scope of
positive, negative, and neutral categories, indicat-
ing that they will benefit the real-world sentiment
analysis systems. We find that there is a similar
type of sentiment which is labeled as conflict in
SemEval dataset for aspect-level sentiment classi-
fication (Pontiki et al., 2014). Conflict label ap-
plies when both positive and negative sentiment
is expressed about an aspect. For example, the

‚àóCorresponding author

sentence ‚ÄúThe atmosphere is attractive, but a lit-
tle uncomfortable.‚Äù expresses conflict sentiment
towards ambience aspect. However, most of ex-
isting studies ignore conflict opinions, for the rea-
son that they are sparse in the datasets (Tang et al.,
2016b; He et al., 2018). In this work, we argue that
the exclusion of conflict opinions is problematic.

The conflict opinions are the production of
an important style of human thinking ‚Äì dialectic
thinking. Dialectic thinking, unlike the thinking
constrained by the laws of formal logic, shows
a degree of tolerance of contradiction (Peng and
Nisbett, 1999). If a real-world sentiment classi-
fication system ignores the existence of conflict
opinions when it is designed, its predicted positive
opinions may contain negative expressions and its
predicted negative opinions may contain positive
expressions. Considering the existence of senti-
ment expressions of both sites, it is unreasonable
to put conflict opinions into either positive or neg-
ative sentiment classes.

There are problems when including conflict
opinions in aspect-level sentiment classification.
Based on our observation, existing methods have
significant lower accuracy on recognizing conflict
sentiment than recognizing other sentiments. We
analyse that the reasons are as follows: 1) Conflict
opinions are sparse in dataset, which is difficult for
a model to learn its class-specific features during
training; 2) Existing models have difficulty in rec-
ognizing the complex expressions of conflict sen-
timent, which are the combination of positive and
negative sentiment expressions.

In this paper, we propose a method to address
the two problems. Firstly, we model the task as
a multi-label classification problem. In contrast
to mutually exclusive 4-class (positive, negative,
neutral, and conflict) classification, our model pre-
dicts two labels for each aspect: 1) Whether posi-
tive sentiment is expressed towards the aspect; 2)



3427

Whether negative sentiment is expressed towards
the aspect. Then, we transform the 2-label pre-
dicted results into 4-class labels by rules (e.g., if
both positive and negative expressions exist, then
apply conflict). During training, the 4-class train-
ing labels in dataset are transformed into 2-label
targets. In this way, we utilize the relation be-
tween conflict label and positive/negative labels,
which makes the model learn to recognize con-
flict opinions from abundant positive and nega-
tive data. It is also based on the fact that neural
network is good at recognizing positive or nega-
tive sentiments, which are indicated by compact
and concrete expressions. On the contrary, we
find that the expressions of conflict opinions usu-
ally are lengthy and implicit. Faced with conflict
opinions, existing attention-based models (Wang
et al., 2016) (with single attention) usually attend
on a portion of a conflict expression and ignore
the other portion, which make its predicted result
prone to be positive or negative. Instead, we use
two independent attentions in the proposed model
to focus on positive or negative expressions sepa-
rately.

Our contributions are summarized as follows:
1) To the best of our knowledge, this is the first
work that discuss the necessity of including con-
flict opinions in aspect-level sentiment classifica-
tion task; 2) We propose a model that can recog-
nize the complex expressions of conflict opinions
and deal with sparsity problem of conflict data; 3)
The experiment results on SemEval dataset indi-
cate that our model performs well on recognizing
conflict opinions and outperforms all baselines.

2 Related Work

Sentiment analysis is a frequently studied topic
in artificial intelligence (Cambria, 2016). Differ-
ent from previous work about sentiment analysis,
Kenyon-Dean et al. (2018) states that there are
opinions cannot be properly annotated as positive,
negative, or neutral sentiment polarities. Their ex-
periment shows that existing classifiers have sig-
nificant lower accuracy on recognizing sentiments
in these opinions. Their work is about sentence-
level sentiment analysis and does not propose a
model. Our work is different from theirs since we
propose a model capable of recognizing conflict
opinions in existing aspect-level sentiment classi-
fication dataset.

In recent years, lots of efforts have been made

to imporve aspect-based sentiment analysis (Wang
et al., 2014; Zhao et al., 2015). Traditional meth-
ods in aspect-level sentiment classification lever-
aged sentiment lexicons (Mohammad et al., 2013),
while recent work tends to use neural network to
generate text representations automatically. TD-
LSTM and TC-LSTM (Tang et al., 2016a) en-
code contextual information with respect to the
target. Methods based on memory network (Tang
et al., 2016b; Zhu and Qian, 2018) instead gener-
ate aspect-related representation. Attention-based
recurrent neural network (Wang et al., 2016; Chen
et al., 2017; Cheng et al., 2017) selectively at-
tends to aspect-related regions and calculates the
weighted sum of the hidden states of these re-
gions. Xue et al. (2018) instead utilize gated con-
volutional neural network to extract aspect-related
representations, which increase computational ef-
ficiency. Our work differs from these works since
we address two problems of recognizing conflict
opinion which they ignore.

3 Model: Dual Attention-based GRU

3.1 Network Architecture

The architecture of the proposed Dual Atteniton-
based GRU (D-AT-GRU) is illustrated as Fig. 1.
Given a sentence with L words and an aspect
(or aspect term) as inputs, the model first trans-
forms them into word embeddings [w1, ..., wL]
and aspect embedding va. Then, it extracts aspect-
related text features through recurrent neural net-
work and dual attention mechanism. Lastly, it
produces 2-label predictions through classification
layers and transforms them into 4-class labels.

Aspect embedding. We use an embedding ma-
trix VA ‚àà R|A|√óda to represent aspects, where da
is the dimension of aspect embedding and |A| is
the number of aspects in dataset. Each row in VA
is a representation vector va for aspect a ‚àà A.
The aspect embeddings are initialized randomly
and learned as parameters.

Text feature extraction. We use Gated Re-
current Unit (GRU) (Cho et al., 2014) to extract
contextual information of words in our model.
Long short-term memory (LSTM) (Hochreiter
and Schmidhuber, 1997) is an alternative choice,
which is often used in previous models (Wang
et al., 2016; Chen et al., 2017; He et al., 2018).
However, we find that GRU performs better than
LSTM in our model. Thus, we choose GRU. Let
matrix H = [h1, ..., hL] ‚àà RL√ód denotes the hid-



3428

ùë§1 ùë§2 ùë§3 ùë§ùêø

GRU GRU GRU GRU‚Ä¶
‚Ñé1 ‚Ñé2 ‚Ñé3 ‚Ñéùêø

ùë£ùëé ùë£ùëé ùë£ùëé ùë£ùëé

ùõºùëù ùõºùëõ

ùëüùëù

ùëüùëõ

Word Embedding

Hidden State

Aspect Embedding

Attention

‚®Ç
‚®Ç

ùë£ùëé

ùë£ùëé

ùë¶ùëù

ùë¶ùëõ

Unrolled GRU Cell

Figure 1: The network architecture of D-AT-GRU.

den states produced by GRU.
Attention mechanism. Based on the hid-

den states H and the embedding va of given
aspect, attention mechanism gives importance
weights which indicate aspect-related context re-
gion. Given that a conflict opinion consist of pos-
itive and negative expressions, attention usually
fails to capture the full expression and only at-
tends to a portion of it. Affected by previous pos-
itive or negative training samples, attention tends
to think that just a portion of the conflict expres-
sion is enough to determine sentiment label and
omit the other portion which also contains sen-
timent words. We therefore propose to use two
attentions separately: one is responsible for find-
ing positive expression, the other is responsible for
finding negative expression. The positive attention
is calculated by:

gpi = u
T
p tanh(Wp[hi; va] + bp), (1)

Œ±pi =
exp(gpi )‚àëL
j=1 exp(g

p
j )
, (2)

where Wp ‚àà Rd√ó2d, up ‚àà Rd and bp ‚àà Rd are
parameters. The negative attention is calculated
through following functions:

gni = u
T
n tanh(Wn[hi; va] + bn), (3)

Œ±ni =
exp(gni )‚àëL
j=1 exp(g

n
j )
, (4)

where Wn ‚àà Rd√ó2d, un ‚àà Rd and bn ‚àà Rd are
parameters.

Regularization term. For the reason that pos-
itive sentiment and negative sentiment usually are
expressed in different regions, the two attentions
should not overlap. Thus, we need to add a regu-
larization term to make the two sets of attention
scores differentiate from each other. We com-
pare KL-divergence (Kullback and Leibler, 1951)
and orthogonal regularization (He et al., 2017) in
experiments. We observed no consistent differ-
ence in performance regarding to sentiment anal-
ysis between these two terms. But the time cost
of computing KL-divergence is 2.7 times as much
as computing orthogonal regularization for each
batch. Therefore, we utilize orthogonal regular-
ization (He et al., 2017):

Ro = ||MTM ‚àí I||2, (5)

where M = [Œ±p;Œ±n],M ‚àà R2√óL contains atten-
tion scores.

Text representation. According to the posi-
tive and negative attention scores, corresponding
aspect-related text representations are calculated
through the weighted sum of hidden states H:

vp =

L‚àë
i=1

Œ±pi hi, (6)

vn =
L‚àë
i=1

Œ±ni hi. (7)

Classification layer. An expression may have
various meanings when describing different as-
pects. Therefore, we take aspect information into



3429

account in classification layer by concatenating
text representations with aspect embeddings.

yp = œÉ(Wpy[vp; va] + bpy), (8)

yn = œÉ(Wny[vn; va] + bny), (9)

where yp is predicted positive sentiment distribu-
tion, yn is predicted negative sentiment distribu-
tion; Wpy,Wny ‚àà R2d and bpy, bny ‚àà R1 are pa-
rameters.

3.2 Objection Function

The model can be trained in an end-to-end way
by backpropagation, where the objective function
is the sum of the binary cross-entropy losses of
positive and negative sentiments:

Lp = ‚àíyÃÇplog(yp)‚àí (1‚àí yÃÇp)log(1‚àí yp), (10)
Ln = ‚àíyÃÇnlog(yn)‚àí (1‚àí yÃÇn)log(1‚àí yn), (11)

where yÃÇp, yÃÇn are target distributions. During train-
ing, 4-class sentiment labels are transformed into
2-label targets (e.g., conflict is transformed into
(yÃÇp, yÃÇn) = (1, 1)). The goal of training is to mini-
mize the following function:

L = Lp + Ln + ŒªRo, (12)

where Œª is a hyperparameter used to balance the
weight of orthogonal regularization. We set Œª to 1
in our experiment.

3.3 Label Transformation

Given the predicted distributions yp, yn, we trans-
form them into 4-class labels. If yp > p, yn >
p, the predicted label will be conflict. If yp ‚â§
p, yn ‚â§ p, the predicted label will be neutral. If
yp ‚â§ p, yn > p, the predicted label will be nega-
tive. If yp > p, yn ‚â§ p, the predicted label will be
positive. In this paper, we directly use p = 0.5 to
minimize human interference, although it can be
tuned manually as a hyperparameter.

4 Experiment

4.1 Dataset and Preparation

We experiment on SemEval 2014 datasets (Pon-
tiki et al., 2014) (statistics shown in Table 1) to
evaluate the proposed model. For the reason that
there are too few conflict test instances regarding
aspect term to show significance, we choose to ex-
periment on aspect category data. Our model is

compared with the state-of-the-art methods in as-
pect category sentiment classification, which are
adapted to 4-class classification.

In our experiment1, dimension sizes d, da, dw
are all set to 300. Word embeddings are initial-
ized with 300-dimension pre-trained GloVe vec-
tors (Pennington et al., 2014). All the other pa-
rameters are initialized by sampling from a normal
distribution N (0, 0.01). We use Adagrad (Duchi
et al., 2011) with a batch size of 10 samples, ini-
tial learning rate of 0.01, and maximum epochs of
30. We use development set chosen by Tay et al.
(2018). The patience value of early-stopping is 3.

positive negative neutral conflict

Train 1759 704 442 187
Dev 420 135 58 8
Test 657 222 94 52

Table 1: Statistics of datasets.

4.2 Result and Analysis

Model Overall Conflict

AT-LSTM 77.13% 11.54%
ATAE-LSTM 78.00% 23.08%
GCAE 78.30% 25.00%
AT-GRU 77.22% 19.23%
AT-GRU 2-label 77.02% 26.92%
D-AT-GRU w/o orthogonal 77.22% 26.92%
D-AT-GRU 78.50% 40.38%

Table 2: Experiment results on SemEval 2014 dataset.

Table 2 shows that D-AT-GRU model outper-
forms all baseline methods. Given that AT-LSTM
(Wang et al., 2016) has strong correlation to our
base model (AT-GRU), their work can be cate-
gorized as a baseline to our model. In contrast,
the results prove that the additional components
are helpful to recognize conflict opinions. We
also compare our model to the recently proposed
GCAE (Xue and Li, 2018), which is based on
gated CNN. D-AT-GRU performs competitively
with GCAE overall and significantly better on
conflict category.

We also compare with ablated versions of D-
AT-GRU: 1) Replace the LSTM in AT-LSTM with

1Code available: https://github.com/tanxw17/DATGRU



3430

the atmosphere is attractive , but a little uncomfortable .

the atmosphere is attractive , but a little uncomfortable .

Negative attention

Positive attention

the atmosphere is attractive , but a little uncomfortable .Attention

D-AT-GRU

AT-LSTM

Attention score: high low

Figure 2: The visualizations of attention.

GRU and directly do 4-class classification (AT-
GRU); 2) Use AT-GRU with two separate classifi-
cation layers to do 2-label classification (AT-GRU
2-label); 3) Remove orthogonal regularization (D-
AT-GRU w/o orthogonal). Their performance suf-
fers from the ablations, which proves that every
component is essential for the proposed model.

We test KL-divergence (Kullback and Leibler,
1951) and orthogonal regularization (He et al.,
2017) as regularization terms to make positive at-
tention and negative attention differentiate from
each other. The experiments were conducted on
a NVIDIA 1080 Ti. The results are shown in Ta-
ble 3.

Regularizer Overall Conflict Time

Orthogonal 78.50% 40.38% 0.15 ms
KL-divergence 78.40% 40.38% 0.40 ms

Table 3: Experiment results about different regulariza-
tion terms.

4.3 Case Study

Figure 2 is a test case in our experiment. AT-
LSTM attends to ‚Äúuncomfortable‚Äù and ignores
the existence of ‚Äúattractive‚Äù, which causes it to
incorrectly predict negative label. With our D-
AT-GRU model, the negative attention finds ‚Äúun-
comfortable‚Äù in the second part of the sentence,
which makes the model predict that there is nega-
tive sentiment; the positive attention instead gives
the highest score to ‚Äúattractive‚Äù, which leads the
model to predict that there is positive sentiment.
Given the existence of both positive and negative
sentiments, D-AT-GRU correctly predict conflict
label. This shows that the dual attention mech-
anism of D-AT-GRU model can more accurately
capture the complete expression of a conflict opin-
ion.

5 Conclusion

In this paper, we present a method to recognize
conflict opinions in aspect-level sentiment classi-
fication task. By transforming the problem into
recognizing simpler sentiments, we alleviate the
sparsity problem of conflict data. Our model uti-
lize dual attention mechanism and orthogonal reg-
ularization, which are capable of recognizing the
complex expressions of conflict opinions. Our ex-
periment on SemEval dataset demonstrates the ef-
fectiveness of the proposed model.

Acknowledgments

This work was supported by the Fundamental
Research Funds for the Central Universities,
SCUT (No. 2017ZD048, D2182480), the Science
and Technology Planning Project of Guangdong
Province (No.2017B050506004), the Science
and Technology Program of Guangzhou (No.
201704030076,201802010027,201902010046).
The research described in this paper has been
supported by a collaborative research grant from
the Hong Kong Research Grants Council (project
no. C1031-18G).

References
Erik Cambria. 2016. Affective computing and senti-

ment analysis. IEEE Intelligent Systems, 31:102‚Äì
107.

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 452‚Äì461.

Jiajun Cheng, Shenglin Zhao, Jiani Zhang, Irwin King,
Xin Zhang, and Hui Wang. 2017. Aspect-level sen-
timent classification with heat (hierarchical atten-
tion) network. In Proceedings of the 2017 ACM
on Conference on Information and Knowledge Man-
agement, pages 97‚Äì106. ACM.



3431

Kyunghyun Cho, Bart van Merrienboer, aglar GuÃàlehre,
Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using
rnn encoder-decoder for statistical machine transla-
tion. In EMNLP.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121‚Äì2159.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2017. An unsupervised neural attention
model for aspect extraction. In ACL.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018. Effective attention modeling for
aspect-level sentiment classification. In COLING.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.

Kian Kenyon-Dean, Eisha Ahmed, Scott Fujimoto,
Jeremy Georges-Filteau, Christopher Glasz, Barleen
Kaur, Auguste Lalande, Shruti Bhanderi, Robert
Belfer, Nirmal Kanagasabai, et al. 2018. Sentiment
analysis: Its complicated! In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 1886‚Äì1895.

Solomon Kullback and Richard A Leibler. 1951. On
information and sufficiency. The annals of mathe-
matical statistics, 22(1):79‚Äì86.

Saif Mohammad, Svetlana Kiritchenko, and Xiao-
Dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. In
SemEval@NAACL-HLT.

Bo Pang, Lillian Lee, et al. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R¬© in In-
formation Retrieval, 2(1‚Äì2):1‚Äì135.

Kaiping Peng and Richard E Nisbett. 1999. Culture,
dialectics, and reasoning about contradiction. Amer-
ican psychologist, 54(9):741.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532‚Äì1543.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27‚Äì35.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent sen-
timent classification. In Proceedings of COLING

2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, pages 3298‚Äì
3307.

Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect
level sentiment classification with deep memory net-
work. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 214‚Äì224.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018.
Learning to attend via word-aspect associative fu-
sion for aspect-based sentiment analysis. In AAAI.

Tao Wang, Yi Cai, Ho fung Leung, Raymond Y. K.
Lau, Qing Li, and Huaqing Min. 2014. Prod-
uct aspect extraction supervised with online domain
knowledge. Knowl.-Based Syst., 71:86‚Äì100.

Yequan Wang, Minlie Huang, Li Zhao, et al. 2016.
Attention-based lstm for aspect-level sentiment clas-
sification. In Proceedings of the 2016 conference on
empirical methods in natural language processing,
pages 606‚Äì615.

Wei Xue and Tao Li. 2018. Aspect based sentiment
analysis with gated convolutional networks. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 2514‚Äì2523.

Y. Zhao, B. Qin, and T. Liu. 2015. Creating a fine-
grained corpus for chinese sentiment analysis. IEEE
Intelligent Systems, 30(1):36‚Äì43.

Peisong Zhu and Tieyun Qian. 2018. Enhanced aspect
level sentiment classification with auxiliary mem-
ory. In Proceedings of the 27th International Con-
ference on Computational Linguistics, pages 1077‚Äì
1087.

https://doi.org/10.1109/MIS.2014.33
https://doi.org/10.1109/MIS.2014.33

