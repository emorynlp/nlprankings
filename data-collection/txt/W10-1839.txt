










































A Hybrid Model for Annotating Named Entity Training Corpora


Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 243–246,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

A Hybrid Model for Annotating Named Entity Training Corpora 

 

 

Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Hannah Copperman 

Microsoft 

475 Brannan St. Suite 330 

San Francisco, CA 94107, USA 

{Robert.Voyer, Valerie.Nygaard, Will.Fitzgerald,  

Hannah.Copperman}@microsoft.com 

 

  

 

Abstract 

In this paper, we present a two-phase, hybrid 

model for generating training data for Named 

Entity Recognition systems. In the first phase, 

a trained annotator labels all named entities in 

a text irrespective of type. In the second phase, 

naïve crowdsourcing workers complete binary 

judgment tasks to indicate the type(s) of each 

entity. Decomposing the data generation task 

in this way results in a flexible, reusable cor-

pus that accommodates changes to entity type 

taxonomies. In addition, it makes efficient use 

of precious trained annotator resources by lev-

eraging highly available and cost effective 

crowdsourcing worker pools in a way that 

does not sacrifice quality. 

Keywords: annotation scheme design, annota-

tion tools and systems, corpus annotation, an-

notation for machine learning 

1 Background 

The task of Named Entity Recognition (NER) is 

fundamental to many Natural Language Pro-

cessing pipelines. Named entity recognizers are 

most commonly built as machine learned sys-

tems that require annotated training data. Manual 

annotation of named entities is an expensive pro-

cess, and as a result, much recent work has been 

done to acquire training corpora automatically 

from the web. Automatic training corpus acquisi-

tion usually requires the existence of one or more 

first-pass classifiers to identify documents that 

correspond to a predetermined entity ontology. 

Using this sort of approach requires an additional 

set of training data for the initial classifier. More 

importantly, the quality of our training corpus is 

limited by the accuracy of any preliminary clas-

sifiers. Each automatic step in the process corre-

sponds to increased error in the resulting system. 

It is not unusual for NE annotation schemas to 

change as the intended application of NER sys-

tems evolves over time – an issue that is rarely 

mentioned in the literature. Extending named 

entity ontologies when using an automated ap-

proach like the one outlined in (Nothman, 2008), 

for example, requires non-trivial modifications 

and extensions to an existing system and may 

render obsolete any previously collected data. 

Our NER system serves a dual purpose; its 

primary function is to aid our deep natural lan-

guage parser by identifying single and multiword 

named entities (NE) in Wikipedia articles. In ad-

dition to rendering these phrases as opaque units, 

the same classifier categorizes these entities as 

belonging to one of four classes: person, loca-

tion, organization, and miscellaneous. These 

class labels serve as additional features that are 

passed downstream and facilitate parsing. Once 

identified and labeled, we then add correspond-

ing entries to our semantic index for improved 

ranking and retrieval. 

We scoped each type in the repertoire men-

tioned above in an attempt to most effectively 

support our parser and the end-to-end retrieval 

task. While this taxonomy resembles the one 

used in the 7th Message Understanding Confer-

ence (MUC-7) NER shared task (Chinchor, 

1998), our specification is in fact slightly nu-

anced. For example, the organization and loca-

tion classes used in our production system are 

much more limited, disallowing governmental 

committees, subcommittees, and other organiza-

tions that fall under the MUC-7 definition of or-

ganization. Indeed, the determination of types to 

tag and the definitions of these types is very 

much dependent upon the application for which a 

given NER system is being designed. Accurate 

243



training and evaluation of NER systems therefore 

requires application-specific corpora. 

Previously, we collected training documents 

for our system with a more automated two-pass 

system. In the first pass, we used a set of prede-

fined heuristic rules – based on sequences of 

part-of-speech (POS) tags and common NE pat-

terns – to identify overlapping candidate spans in 

the source data. These candidates were then up-

loaded as tasks to Amazon Mechanical Turk 

(AMT), in which users were asked to determine 

if the selected entity was one of 5 specified 

types. We used majority vote to choose the best 

decision. Candidates with no majority vote were 

resubmitted for additional Turker input. 

There were a few drawbacks with this system. 

First and foremost, while the heuristics to identi-

fy candidate spans were designed to deliver high 

recall, it was impossible to have perfect cover-

age. This imposed an upper bound on the cover-

age of the system learning from this data. Recall 

would inevitably decline if we extended our NE 

taxonomy to include less formulaic types such as 

titles and band names, for example. One could 

imagine injecting additional layers of automatic 

candidate generators into the system to improve 

recall, each of which would incur additional 

overhead in judgment cost or complexity. The 

next issue was quality; many workers tried to 

scam the system, and others didn’t quite under-

stand the task, specifically when it came to dif-

ferentiating types. The need to address these is-

sues is what led us to our current annotation 

model. 

2 Objective 

As the search application supported by our NER 

system evolved, it became clear both that we 

would need to be able to support additional name 

types and that there was a demand for a lighter 

weight system to identify (especially multiword) 

NE spans without the need to specify the type. 

The underlying technology at the core of our ex-

isting NER software is well suited for such clas-

sification tasks. The central hurdle to extending 

our system in this way is acquiring a suitable 

training corpus. Consider the following list of 

potential classifiers: 

 

1. A single type system capable of identify-
ing product names 

2. A targeted system for identifying only 
movie titles and person names 

3. A generic NE span tagger for tagging all 
named entities 

4. A generic-span tagger that tags all mul-
tiword named entities 

 

Given that manual annotation is an extremely 

costly task, we consider optimization of our cor-

pora for reuse while maintaining quality in all 

supported systems to be a primary goal. Second-

ly, although throughput is important – it is often 

said that quantity trumps quality in machine 

learned systems – the quality of the data is very 

highly correlated with the accuracy of the sys-

tems in question. At the scale of our typical train-

ing corpus – one to ten thousand documents – the 

quality of the data has a significant impact. 

3 Methodology  

In general, decomposing multifaceted annotation 

tasks into their fundamental decision points re-

duces the cognitive load on annotators, increas-

ing annotator throughput while ultimately im-

proving the quality of the marked-up data 

(Medero et al., 2006). Identifying named entities 

can be decomposed into two tasks: identifying 

the span of the entity and determining its type(s). 

Based on our experience, the first of these tasks 

requires much more training than the second. 

The corner cases that arise in determining if any 

arbitrary sequence of tokens is a named entity 

make this first task significantly more complex 

than determining if a given name is, for example, 

a person name. Decomposing the task into span 

identification and type judgment has two distinct 

advantages:  

 The span-identification task can be given 
to more highly trained annotators who are 

following a specification, while the rela-

tively simpler task can be distributed to 

naïve/crowdsource judges. 

 The task given to the trained annotators 
goes much more quickly, increasing their 

throughput. 

In a round of pilot tasks, our Corpus Devel-

opment team performed dual-annotation and 

complete adjudication on a small sample of 100 

documents. We used the output of these tasks to 

help identify areas of inconsistency in annotator 

behavior as well as vagueness in the specifica-

tion. This initial round provided helpful feed-

back, which we used both to refine the task spec-

ification and to help inform the intuitions of our 

annotators.

244



 
Figure 1: A NE type task in the Crowdflower interface

 

After these initial tasks, inter-annotator agree-

ment was estimated at 91%, which can be taken 

to be a reasonable upper bound for our automat-

ed system. 

In our current process, the data is first marked 

up by a trained annotator and then checked over 

by a second trained annotator, and finally under-

goes automatic post-processing to catch common 

errors. Thus, our first step in addressing the issue 

of poor data quality is to remove the step of au-

tomated NE candidate generation and to shift 

part of the cognitive load of the task from un-

trained workers to expert annotators.  

After span-tagged data has been published by 

our Corpus Development team, in order to get 

typed NE annotations for our existing system, we 

then submit candidate spans along with a two 

additional sentences of context to workers on 

AMT. Workers are presented with assignments 

that require simple binary decisions (Figure 1). Is 

the selected entity of type X – yes or no? Each 

unit is presented to at least 5 different workers. 

We follow this procedure for all labeled spans in 

our tagged corpus. This entire process can be 

completed for all of the types that we’re interest-

ed in – person, location, organization, product, 

title, etc. Extending this system to cover arbitrary 

additional types requires simply that we create a 

new task template and instructions for workers. 

Instead of putting these tasks directly onto 

AMT, we chose to leverage Crowdflower for its 

added quality control. Crowdflower is a 

crowdsourcing service built on top of AMT that 

associates a trust level with workers based on 

their performance on gold data and uses these 

trust levels to determine the correctness of work-

er responses. It provides functionality for retriev-

ing aggregated reports, in which responses are 

aggregated not based on simple majority voting, 

but rather by users’ trust levels. Our early exper-

iments with this service indicate that it does in 

fact improve the quality of the output data. An 

added bonus of their technology is that we can 

associate confidence levels with the labels pro-

duced by workers in their system. 

This entire process yields several different an-

notated versions of the same corpus: an un-typed 

named entity training corpus, along with an addi-

tional corpus for each named entity type. Ideally, 

each NE span submitted to workers will come 

back as belonging to zero or one classes. How do 

we reconcile the fact that our existing system 

requires a single label per token, when some to-

kens may in fact fall under multiple categories? 

Merging the type labels produced by Turkers 

(with the help of Crowdflower) is an interesting 

problem in itself. Ultimately, we arrived at a sys-

tem that allows us to remove type labels that do 

not meet a confidence threshold, while also bias-

ing certain types over others based on their diffi-

culty. Interestingly, agreement rates among 

crowdsourcing workers can provide useful in-

sight into the difficulty of labeling some types 

over others, potentially indicating which types 

are less precisely scoped. We consistently saw 

inter-judge agreement rates in the 92%–97% 

range for person names and locations, while 

agreement on the less well-defined category of 

organizations often yielded agreement rates clos-

er to 85%. 

4 Initial Results 

As a first level comparison of how the new ap-

proach affects the overall accuracy of our sys-

tem, we trained two named entity recognizers. 

The first system was trained on a subset of the 

training data collected using the old approach. 

System 2 was trained on a subset of documents 

collected using the new approach. Both systems 

are trained using only a single type – person 

names. For the former, we randomly selected 

200 docs from our previous canonical training 

set, with the guiding principle that we should 

have roughly the same number of sentences as 

exist in our new training corpus (~7400 sentenc-

es). Both systems were evaluated against one of 

our standard, blind measurement sets, hand-

245



annotated with personal names. The results in 

table 1 indicate the strict phrase-level precision, 

recall, and F-score. 

It bears mentioning that many NER systems 

report token-level accuracy or F-score using a 

flexible phrase-level metric that gives partial 

credit if either the type classification is correct or 

the span boundaries are correct. Naturally, these 

metrics result in higher accuracy numbers when 

compared to the strict phrase-level metric that we 

use. Our evaluation tool gives credit to instances 

where both boundaries and type are correct. In-

correct instances incur at least 2 penalties, count-

ing as at least 1 false positive and 1 false nega-

tive, depending on the nature of the error. We 

optimize our system for high precision. 

 

System P R F-score 

Old system 89.7 70.3 78.9 

New system 91.6 72.1 80.7 

 
Table 1: Strict phrase-level precision, recall, and 

F-score. 

 

Our other target application is a generic entity 

tagger. For this experiment we trained on our 

complete set of 817 training documents (14,297 

sentences) where documents are tagged for all 

named entities and types are not labeled. We 

evaluated the resulting system on a blind 100-

document measurement set in which generic NE 

spans have been manually labeled by our Corpus 

Development team. These results are included in 

Table 2. 

 

System P R F-score 

Generic span 80.3 85.7 82.9 

 
Table 2: Strict phrase-level precision, recall and F-

score for generic span tagging. 

5 Conclusions 

The results indicate that our new approach 

does indeed produce higher quality training data. 

An improvement of 1.8 F-score points is relative-

ly significant, particularly given the size of the 

training set used in this experiment. It is worth 

noting that our previous canonical training set 

underwent a round of manual editing after it was 

discovered that there were significant quality 

issues. The system trained on the curated data 

showed marked improvement over previous ver-

sions. Given this, we could expect to see a great-

er disparity between the two systems if we used 

the output of our previous training data collec-

tion system as is. 

The generic named entity tagger requires sig-

nificantly fewer features than type-aware sys-

tems, allowing us to improve F-score while also 

improving runtime performance. We expect to be 

able to improve precision to acceptable produc-

tion levels (>90%) while maintaining F-score 

with a bit more feature engineering, making this 

system comparable to other state-of-the-art sys-

tems. 

To extend and improve these initial experi-

ments, we would like to use identical documents 

for both single-type systems, compare perfor-

mance on additional NE types, and analyze the 

learning curve of both systems as we increase the 

size of the training corpus. 

References  

Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 

2003. Automatic acquisition of named entity 

tagged corpus from world wide web. The Com-

panion Volume to the Proceedings of 41st An-

nual Meeting of the Association for Computa-

tional Linguistics, pages 165-168. 

Nancy Chinchor. 1998. Overview of MUC-7. Pro-

ceedings of the 7th Message Understanding 

Conference. 

Julie Medero, Kazuaki Maeda, Stephanie Strassel, and 

Christopher Walker. 2006. An Efficient Approach 

to Gold-Standard Annotation: Decision Points for 

Complex Tasks. Proceedings of the Fifth Inter-

national Conference on Language Resources 

and Evaluation. 

Joel Nothman, Tara Murphy, and James R. Curran. 

2009. Analyzing Wikipedia and Gold-Standard 

Corpora for NER Training. Proceedings of the 

12th Conference of the European Chapter of 

the Association for Computational Linguistics, 
pages 612-620. 

Joel Nothman, James R. Curran, and Tara Murphy. 

2008. Transforming Wikipedia into named entity 

training data. Proceedings of the Australian 

Language Technology Workshop, pages 124–
132. 

Lee Ratinov and Dan Roth. 2009. Design challenges 

and misconceptions in named entity recognition. 

Proceedings of the Thirteenth Conference on 

Computational Natural Language Learning 
(CoNLL-2009), pages 147-155. 

246


