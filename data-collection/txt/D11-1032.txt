



















































Large-Scale Cognate Recovery


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344–354,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Large-Scale Cognate Recovery

David Hall and Dan Klein
Computer Science Division

University of California at Berkeley
{dlwh,klein}@cs.berkeley.edu

Abstract

We present a system for the large scale in-
duction of cognate groups. Our model ex-
plains the evolution of cognates as a sequence
of mutations and innovations along a phy-
logeny. On the task of identifying cognates
from over 21,000 words in 218 different lan-
guages from the Oceanic language family, our
model achieves a cluster purity score over
91%, while maintaining pairwise recall over
62%.

1 Introduction

The critical first step in the reconstruction of an
ancient language is the recovery of related cog-
nate words in its descendants. Unfortunately, this
process has largely been a manual, linguistically-
intensive undertaking for any sizable number of de-
scendant languages. The traditional approach used
by linguists—the comparative method—iterates be-
tween positing putative cognates and then identify-
ing regular sound laws that explain correspondences
between those words (Bloomfield, 1938).

Successful computational approaches have been
developed for large-scale reconstruction of phyloge-
nies (Ringe et al., 2002; Daumé III and Campbell,
2007; Daumé III, 2009; Nerbonne, 2010) and an-
cestral word forms of known cognate sets (Oakes,
2000; Bouchard-Côté et al., 2007; Bouchard-Côté
et al., 2009), enabling linguists to explore deep his-
torical relationships in an automated fashion. How-
ever, computational approaches thus far have not
been able to offer the same kind of scale for iden-
tifying cognates. Previous work in cognate identi-
fication has largely focused on identifying cognates
in pairs of languages (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak,

2001; Mulloni, 2007), with a few recent exceptions
that can find sets in a handful of languages (Bergsma
and Kondrak, 2007; Hall and Klein, 2010).

While it may seem surprising that cognate de-
tection has not successfully scaled to large num-
bers of languages, the task poses challenges not
seen in reconstruction and phylogeny inference. For
instance, morphological innovations and irregular
sound changes can completely obscure relationships
between words in different languages. However, in
the case of reconstruction, an unexplainable word is
simply that: one can still correctly reconstruct its an-
cestor using words from related languages.

In this paper, we present a system that uses two
generative models for large-scale cognate identi-
fication. Both models describe the evolution of
words along a phylogeny according to automatically
learned sound laws in the form of parametric edit
distances. The first is an adaptation of the genera-
tive model of Hall and Klein (2010), and the other
is a new generative model called PARSIM with con-
nections to parsimony methods in computational bi-
ology (Cavalli-Sforza and Edwards, 1965; Fitch,
1971). Our model supports simple, tractable infer-
ence via message passing, at the expense of being
unable to model some cognacy relationships. To
help correct this deficiency, we also describe an ag-
glomerative inference procedure for the model of
Hall and Klein (2010). By using the output of our
system as input to this system, we can find cognate
groups that PARSIM alone cannot recover.

We apply these models to identifying cognate
groups from two language families using the Aus-
tronesian Basic Vocabulary Database (Greenhill et
al., 2008), a catalog of words from about 40% of
the Austronesian languages. We focus on data from
two subfamilies of Austronesian: Formosan and

344



Oceanic. The datasets are by far the largest on
which automated cognate recovery has ever been at-
tempted, with 18 and 271 languages respectively.
On the larger Oceanic data, our model can achieve
cluster purity scores of 91.8%, while maintaining
pairwise recall of 62.1%. We also analyze the mis-
takes of our system, where we find that some of the
erroneous cognate groups our system finds may not
be errors at all. Instead, they may be previously
unknown cognacy relationships that were not anno-
tated in the data.

2 Background

Before we present our model, we first describe ba-
sic facts of the Austronesian language family, along
with a description of the Austronesian Basic Vocab-
ulary Database, which forms the dataset that we use
for our experiments. For far more detailed coverage
of the Austronesian languages, we direct the inter-
ested reader to Blust (2009)’s comprehensive mono-
graph.

2.1 The Austronesian Language Family
The Austronesian language family is one of the
largest in the world, comprising about one-fifth of
the world’s languages. Geographically, it stretches
from its homeland on Formosa (Taiwan) to Mada-
gascar in the west, and as far as Hawai’i and (at one
point) the Easter Islands to the east. Until the ad-
vent of European colonialism spread Indo-European
languages to every continent, Austronesian was the
most widespread of all language families.

Linguistically, the language family is as diverse
as it is large, but a few regularities hold. From
a phonological perspective, two features stand out.
First, the phoneme inventories of these languages
are typically small. For example, it is well-known
that Hawaiian has only 13 phonemes. Moreover, the
phonotactics of these languages are often restrictive.
Sticking with the same example, Hawaiian only al-
lows (C)V syllables: consonants clusters are forbid-
den, and no syllable may end with a consonant.

2.2 The Austronesian Basic Vocabulary
Database

The Austronesian Basic Vocabulary Database
(ABVD) (Greenhill et al., 2008) is an ambitious, on-
going effort to catalog the lexicons and basic facts

about all of the languages in the Austronesian lan-
guage family. It also contains manual reconstruc-
tions for select ancestor languages produced by lin-
guists.

The sample we use—from Bouchard-Côté et al.
(2009)—contains about 50,000 words across 471
languages spanning all the major divisions of Aus-
tronesian. These words are grouped into cognate
groups and arranged by gloss. For instance, there are
37 distinct cognate groups for the gloss “tail.” One
of these groups includes the words /ekor/, /ingko/,
/iNkot/, /kiikiPu/, and /PiPina/, among others. Most
of these words have been transcribed into the Inter-
national Phonetic Alphabet, though it appears that
some words are transcribed using the Roman alpha-
bet. For instance, the second word in the example is
likely /iNko/, which is a much more likely sequence
than what is transcribed.

In this sample, there are 6307 such cognate
groups and 210 distinct glosses. The data is
somewhat sparse: fewer than 50% of the possible
gloss/language pairs are present. Moreover, there is
some amount of homoplasy—that is, languages with
a word from more than one cognate group for a given
gloss.

Finally, it is important to note that the ABVD is
still a work in progress: they have data from only
50% of extant Austronesian languages.

2.3 Subfamilies of Austronesian
In this paper we focus on two branches of the Aus-
tronesian language family, one as a development set
and one as a test set. For our development set, we
use the Formosan branch. The languages in this
group are exclusively found on the Austronesian
homeland of Formosa. The family encompasses a
substantial portion of the linguistic diversity of Aus-
tronesian: Blust (2009) argues that Formosan con-
tains 9 of the 10 first-order splits of the Austrone-
sian family. Formosan’s diversity is surprising since
it contains a mere 18 languages. Thus, Formosan is
a smaller development set that nevertheless is repre-
sentative of larger families.

For our final test set, we use the Oceanic sub-
family, which includes almost 50% of the languages
in the Austronesian family, meaning that it repre-
sents around 10% of all languages in the world.
Oceanic also represents a large fraction of the ge-

345



ographic diversity of Austronesian, stretching from
New Zealand in the south to Hawai’i in the north.
Our sample includes 21863 words from 218 lan-
guages in the Oceanic family.

3 Models

In this section we describe two models, one based on
Hall and Klein (2010)—which we call HK10—and
another new model that shares some connection to
parsimony methods in computational biology, which
we call PARSIM. Both are generative models that
describe the evolution of words w` from a set of lan-
guages {`} in a cognate group g along a fixed phy-
logeny T .1 Each cognate group and word is also
associated with a gloss or meaning m, which we as-
sume to be fixed.2 In both models, words evolve
according to regular sound laws ϕ`, which are spe-
cific to each language. Also, both models will make
use of a language model λ, which is used for gen-
erating words that are not dependent on the word in
the parent language. (We leave ϕ` and λ as abstract
parameters for now. We will describe them in sub-
sequent sections.)

3.1 HK10
The first model we describe is a small modification
of the phylogenetic model of Hall and Klein (2010).
In HK10, there is an unknown number of cognate
groups G where each cognate group g consists of a
set of words {wg,`}. In each cognate group, words
evolve along a phylogeny, where each word in a lan-
guage is the result of that word evolving from its
parent according to regular sound laws. To model
the fact that not all languages have a cognate in
each group, each language in the tree has an asso-
ciated “survival” variable Sg,`, where a word may
be lost on that branch (and its descendants) instead
of evolving. Once the words are generated, they are
then “permuted” so that the cognacy relationships

1Both of these models therefore are insensitive to geo-
graphic and historical factors that cannot be easily approxi-
mated by this tree. See Nichols (1992) for an excellent dis-
cussion of these factors.

2One could easily envision allowing the meaning of a word
to change as well. Modeling this semantic drift has been consid-
ered by Kondrak (2001). In the ABVD, however, any semantic
drift has already been elided, since the database has coarsened
glosses to the extent that there is no meaningful way to model
semantic drift given our data.

M

G

WFor

WAta

φ φ φ

SFor

SAta

SPai SSquSCiu

wpt wes

L  
π

wPaiwPaiwPaiwPaiwPaiwPaiWPaiWPai

Su
rv
iv
al

Evolution

Pe
rm
ut
at
io
n

(a)

IAta

IPai ISquICiu

WAta

WSquWCiuWPai

(b)
WFor

φ

φ

φ

φ

Figure 1: Plate diagrams for (a) HK10 (Hall and Klein,
2010) and (b) PARSIM, our new parsimony model, for
a small set of languages. In HK10, words are generated
following a phylogenetic tree according to sound laws ϕ,
and then “scrambled” with a permutation π so that the
original cognate groups are lost. In PARSIM, all words
for each of the M glosses are generated in a single tree,
with innovations I starting new cognate groups. The
languages depicted are Formosan (For), Paiwan (Pai),
Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic
(Squ).

346



are obscured. The task of inference then is to re-
cover the original cognate groups.

The generative process for their model is as fol-
lows:

• For each cognate group g, choose a root word
Wroot ∼ p(W |λ), a language model over
words.

• For each language ` in a pre-order traversal of
the phylogeny:

1. Choose S` ∼ Bernoulli(β`), indicating
whether or not the word survives.

2. If the word survives, choose W` ∼
p(W |ϕ`,Wpar(`)).

3. Otherwise, stop generating words in that
language and its descendants.

• For each language, choose a random permuta-
tion π of the observed data, and rearrange the
cognates according to this permutation.

We reproduce the graphical model for HK10 for a
small phylogeny in Figure 1a.

Inference in this model is intractable; to perform
inference exactly, one has to reason over all parti-
tions of the data into cognate groups. To address
this problem, Hall and Klein (2010) propose an it-
erative bipartite matching scheme where one lan-
guage is held out from the others, and then words
are assigned to the remaining groups to maximize
the probability of the attachment. That is, for some
language ` and fixed assignments π−` for the other
languages, they seek an assignment π` that maxi-
mizes:

π∗ = argmax
π

∑

g

log p(w(`,π`(g))|ϕ, π,w−`)

Unfortunately, while this approach was effective
with only a few languages (they tested on three), this
algorithm cannot scale to the eighteen languages in
Formosan, let alone the hundreds of languages in
Oceanic. Therefore, we make two simple modifi-
cations. First, we restrict the cognate assignments
to stay within a gloss. Thus, there are many fewer
potential matchings to consider. Second, we use an
agglomerative inference procedure, which greedily
merges cognate groups that result in the greatest gain

in likelihood. That is, for all pairs of cognate groups
ga with words wa and gb with words wb, we com-
pute the score:

log p(wa∪b|ϕ)− log p(wa|ϕ)− log p(wb|ϕ)

This score is the difference between the log proba-
bility of generating two cognate groups jointly and
generating them separately. We then merge the two
that generate the highest gain in likelihood. Like
the iterative bipartite matching algorithm described
above, this algorithm is not exact. However, it is
O(n2 log n) (where n is the size of the largest gloss,
which for Oceanic is 153), while the bipartite match-
ing algorithm is O(n3) (Kuhn, 1955).

Actually, the original HK10 is doubly intractable.
They use weighted automata to represent distribu-
tions over strings, but these automata—particularly
if they are non-deterministic—make inference in
any non-trivial graphical model intractable. We dis-
cuss this issue in more detail in Section 6.

3.2 A Parsimony-Inspired Model

We now describe a new model called PARSIM that
supports exact inference tractably, though it sacri-
fices some of the expressive power of HK10. In
our model, each language has at most one word for
each gloss, and this one word changes from one
language to its children according to some edge-
specific Markov process. These changes may either
be mutations, which merely change the surface form
of the word, or innovations, which start a new word
in a new cognate group that is unrelated to the previ-
ous word. Mutations take the form of a conditional
edit operation that models insertions, substitutions,
and deletions that correspond to regular (and, with
lower probability, irregular) sound changes that are
likely to occur between a language and its parent.
Innovations, on the other hand, are generated from a
language model independent of the parent’s word.

Specifically, our generative process takes the fol-
lowing form:

• For each gloss m, choose a root word Wroot ∼
λ, a language model over words.

• For each language ` in a pre-order traversal of
the phylogeny:

347



malapzo mappo
Ciuli Squliq

purrok
PaiwanRukai

pouroukou

Figure 2: A small example of how PARSIM works.
Listed here are the words for “ten” in four languages
from the Formosan family, along with the tree that ex-
plains them. The dashed line indicates an innovation on
the branch.

1. Choose I` ∼ Bernoulli(β`), indicating
whether or not the word is an innovation
or a mutation.

2. If it is a mutation, choose W` ∼
p(W |ϕ`,Wpar(`)).

3. Otherwise, choose W` ∼ λ.

We also depict our model as a plate diagram for a
small phylogeny in Figure 1b.

Because there is only one tree per gloss, there
is no assignment problem to consider, which is the
main source of the intractability of HK10. Instead,
pieces of the phylogeny are simply “cut” into sub-
trees whenever an innovation occurs. Thus, message
passing can be used to perform inference.

As an example of how our process works, con-
sider Figure 2. The Formosan word for “ten”
probably resembled either /purrok/ or /pouroukou/.
There was an innovation in Ciuli and Squliq’s an-
cestor Atayalic that produced a new word for ten.
This word then mutated separately into the words
/malapzo/ and /mappo/, respectively.

4 Relation to Parsimony

PARSIM is related to the parsimony principle
from computational biology (Cavalli-Sforza and Ed-
wards, 1965; Fitch, 1971), where it is used to search
for phylogenies. When using parsimony, a phy-
logeny is scored according to the derivation that re-
quires the fewest number of changes of state, where
a state is typically thought of as a gene or some other
trait in a species. These genes are typically called
“characters” in the computational biology literature,

and two species would have the same value for a
character if they share the same property that that
state represents.

When inducing phylogenies of languages, a natu-
ral choice for characters are glosses from a restricted
vocabulary like a Swadesh list, and two words are
represented as the same value for a character if they
are cognate (Ringe et al., 2002). Other features can
be used (Daumé III and Campbell, 2007; Daumé III,
2009), but they are not relevant to our discussion.

Consider the small example in Figure 3a with just
four languages. Here, cognacy is encoded using
characters. In this example, at least two changes of
state are required to explain the data: both C and B
must have evolved from A. Therefore, the parsimony
score for this tree is two.

Of course, there is no reason why all changes
should be equally likely. For instance, it might be
extremely likely that B changes into both A and
C, but that A never changes into B or C, and so
weighted variants of parsimony might be neces-
sary (Sankoff and Cedergren, 1983).

With this in mind, PARSIM can be thought of a
weighted variant of parsimony, with two differences.
First, the characters do not indicate ahead of time
which words are related. Instead, the characters are
the words themselves. Second, the transitions be-
tween different states (words) are not uniform. In-
stead, they are weighted by the log probability of
one word changing into another, including both mu-
tations and innovations.

Thus, the task of inference in PARSIM is to find
the most “parsimonious” explanation for the words
we have observed, which is the same as finding the
most likely derivation. Because the distances be-
tween words (that is, the transition probabilities)
are not known ahead of time, they must instead be
learned, which we discuss in Section 7.3

5 Limitations of the Parsimony Model

Potentially, our parsimony model sacrifices a cer-
tain amount of power to make inference tractable.
Specifically, it cannot model homoplasy, the pres-
ence of more than one word in a language for a given

3It is worth noting that we are not the first to point out a
connection between parsimony and likelihood. Indeed, many
authors in the computational biology literature have formally
demonstrated a connection (Farris, 1973; Felsenstein, 1973).

348



A B C A

A

A

A

(a)

A B B A

{A,B} {A,B}

(b)
{A,B}

A B B A

A

(c)
A

A

A A

A

(d)
A

A

B B

B

B

B

Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and
C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There
are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages
contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown
as dashed lines). (d) The HK10 model can recover this relationship, but this power makes the model intractable.

gloss. Homoplasy can arise for a variety of reasons
in phylogenetic models of cognates, and we describe
some in this section.

Consider the example illustrated in Figure 3b,
where the two central languages share a cognate, as
do the two outer languages. This is the canonical ex-
ample of homoplasy, and PARSIM cannot correctly
recover this grouping. Instead, it can at best only se-
lect group A or group B as the value for the parent,
and leave the other group fragmented as two innova-
tions, as in Figure 3c. On the other hand, HK10 can
recover this relationship (Figure 3d), but this power
is precisely what makes it intractable.

There are two reasons this kind of homoplasy
could arise. The first is that there were indeed two
words in the parent language for this gloss, or that
there were two words with similar meanings and
the two meanings drifted together. Second, the tree
could be an inadequate model of the evolution in
this case. For instance, there could have been a cer-
tain amount of borrowing between two of these lan-
guages, or there was not a single coherent parent lan-
guage, but rather a language continuum that cannot
be explained by any tree.

However, homoplasy seems to be relatively un-
common (though not unheard of) in the Oceanic and
Formosan families. Where it does appear, our model
should simply fail to get one of the cognate groups,
instead explaining all of them via innovation. To
repair this shortcoming, we can simply run the ag-
glomerative clustering procedure for the model of
Hall and Klein (2010), starting from the groups that
PARSIM has recovered. Using this procedure, we
can hopefully recover many of the under-groupings

caused by homoplasy.

6 Inference and Scale

6.1 Inference

In this section we describe the basics of infer-
ence in the PARSIM model. We have a nearly
tree-structured graphical model (Figure 1); it is
not a tree only because of the innovation param-
eters. Therefore, we apply the common trick of
grouping variables to form a tree. Specifically, we
group each word variable W` with its innovation
parameter I`. The distribution of interest is then
p(W`, I`|Wpar(`), φ`, β`), and the primary operation
is summing out messages µ from the children of a
language and sending a new message to its parent:

µ`(wpar(`)) =
∑

w`

p(w`|·)
∏

`′ ∈ child(`)

µ`′(w`)

p(w`|·) = p(w`|I` = 0, wpar(`), φ`)p(I` = 0|β`)
+ p(w`|I` = 1, φ`)P (I` = 1|β`)

(1)

The first term involves computing the probability of
the word mutating from its parent, and the second
involves the probability of the child word from a lan-
guage model. We describe the parameters and pro-
cedures for these operations in 7.1.

6.2 Scale

Even though inference by message-passing in our
model is tractable, we needed to make certain con-
cessions to make inference acceptably fast. These
choices mainly affect how we represent distributions
over strings.

349



First, we need to model distributions and mes-
sages over words on the internal nodes of a phy-
logeny. The natural choice in this scenario is to use
weighted finite automata (Mohri et al., 1996). Au-
tomata have been used to successfully model distri-
butions of strings for inferring morphology (Dreyer
and Eisner, 2009) as well as cognate detection (Hall
and Klein, 2010). Even in models that would be
tractable with “ordinary” messages, inference with
automata quickly becomes intractable, because the
size of the automata grow exponentially with the
number of messages passed. Therefore, approxima-
tions must be used. Dreyer and Eisner (2009) used
a mixture of a k-best list and a unigram language
model, while Hall and Klein (2010) used an approx-
imation procedure that projected complex automata
to simple, tractable automata using a modified KL
divergence.

While either approach could be used here in prin-
ciple, we found that automata machinery was simply
too slow for our application. Instead, we exploit the
intuition that we do not need to accurately recon-
struct the word for any ancestral language. More-
over, it is inefficient to keep track of probabilities for
all strings. Therefore, we only track scores for words
that actually exist in a given gloss, which means that
internal nodes only have mass on those words. That
is, if a gloss has 10 distinct words across all the lan-
guages in our dataset, we pass messages that only
contain information about those 10 words.

Now, this representation—while more efficient
than the automata representations—results in infer-
ence that is still quadratic in the number of words
in a gloss, since we have distributions of the form
p(w`|wpar(`), φ`). Intuitively, it is unlikely that a
word from one distant branch of tree resembles a
word in another branch. Therefore, rather than score
all of these unlikely words, we use a beam where we
only factor in words whose score is at most a fac-
tor of e−10 less than the maximum score. Our initial
experiments found that using a beam provides large
savings in time with little impact on prediction qual-
ity.

7 Learning

PARSIM has three kinds of parameters that we need
to learn: the mutation parameters ϕ`, the innovation

probabilities β`, and the global language model λ
for generating new words. We learn these parame-
ters via Expectation Maximization (Dempster et al.,
1977), iterating between computing expected counts
and adjusting parameters to maximize the posterior
probability of the parameters. In this section, we de-
scribe those parameters.

7.1 Sound Laws
The core piece of our system is learning the sound
laws associated with each edge. Since the founda-
tion of historical linguists with the neogrammari-
ans, linguists have argued for the regularity of sound
change at the phonemic level (Schleicher, 1861;
Bloomfield, 1938). That is to say, if in some lan-
guage a /t/ changes to a /d/ in some word, it is al-
most certain that it will change in every other place
that has the same surrounding context.

In practice, of course, sound change is not entirely
regular, and complex extralinguistic events can lead
to sound changes that are irregular. For example,
in some cultures in which Oceanic languages are
spoken, the name of the chief is taboo: one cannot
speak his name, nor say any word that sounds too
much like his name. Speakers of these languages
do find ways around this prohibition, often resulting
in sound changes that cannot be explained by sound
laws alone (Keesing and Fifi’i, 1969).

Nevertheless, we find it useful to model sound
change as a largely regular if stochastic process.
We employ a sound change model whose expressive
power is equivalent to that of Hall and Klein (2010),
though with a different parameterization. We model
the evolution of a word w` to its child w`′ as a
sequence of unigram edits that include insertions,
deletions, and substitutions. Specifically, we use a
standard three-state pair hidden Markov model that
is closely related to the classic alignment algorithm
of Needleman and Wunsch (1970) (Durbin et al.,
2006).

The three states in this HMM correspond to
matches/substitutions, insertions, and deletions. The
transitions are set up such that insertions and dele-
tions cannot be interleaved. This prevents spurious
equivalent alignments, which would cause the model
to assign unnecessarily higher probability to transi-
tions with many insertions and deletions.

Actually learning these parameters involves learn-
350



ing the transition probabilities of this HMM (which
model the overall probability of insertion and dele-
tion) as well as the emission probabilities (which
model the particular edits). Because there are rel-
atively few words for each language (96 on average
in Oceanic), we found it important to tie together
the parameters for the various languages, in contrast
to Hall and Klein (2010) who did not. In our maxi-
mization step, we fit a joint log-linear model for each
language, using features that are both specific to a
language and shared across languages. Our features
included indicators on each substitution, insertion,
and deletion operation, along with an indicator for
the outcome of each edit operation. This last fea-
ture reflects the propensity of a particular phoneme
to appear in a given language at all, no matter what
its ancestral phoneme was. This parameterization
is similar to the one used in the reconstruction sys-
tem of Bouchard-Côté et al. (2009), except that they
used edit operations that conditioned on the context
of the surrounding word, which is crucial when try-
ing to accurately reconstruct ancestral word forms.
To encourage parameter sharing, we used an `2 reg-
ularization penalty.

7.2 Innovation Parameters

The innovation parameters β` are parameters for
simple Bernoulli distribution that govern the propen-
sity for a language to start a new word. These pa-
rameters can be learned separately, though due to
data sparsity, we found it better to use a tied param-
eterization as with the sound laws. Specifically, we
fit a log linear model whose features are indicators
on the specific language, as well as a global inno-
vation parameter that is shared across all languages.
As with the sound laws, we used an `2 regularization
penalty to encourage the use of the global innovation
parameter.

7.3 Language Model

Finally, we have a single language model λ that is
also shared across all languages. λ is a simple bi-
gram language model over characters in the Interna-
tional Phonetic Alphabet. λ is used when generating
new words either via innovation or from the root of
the tree.

In principle, we could of course have language
models specific to each language, but because there

Formosan
System Prec Recall F1 Purity

Agg. HK10 77.6 83.2 80.0 84.7
PARSIM 87.8 71.0 78.5 94.6

Combination 85.2 81.3 83.2 92.3
Oceanic

System Prec Recall F1 Purity
PARSIM 84.4 62.1 71.5 91.8

Combination 76.0 73.8 74.9 85.5

Table 1: Results on the Formosan and Oceanic fami-
lies. PARSIM is the new parsimony model in this pa-
per, Agg. HK10 is our agglomerative variant of Hall and
Klein (2010) and Combination uses PARSIM’s output to
seed the agglomerative matcher. For the agglomerative
systems, we report the point with maximal F1 score, but
we also show precision/recall curves. (See Figure 4.)

are so few words per language, we found that
branch-specific language models caused the model
to prefer to innovate at almost every node since the
language models could essentially memorize the rel-
atively small vocabularies of these languages.

8 Experiments

8.1 Cognate Recovery

We ran both PARSIM and our agglomerative ver-
sion of HK10 on the Formosan datasets. For PAR-
SIM, we initialized the mutation parameters ϕ to a
model that preferred matches to insertions, substi-
tutions and deletions by a factor of e3, innovation
parameters to 0.5, and the language model to a uni-
form distribution over characters. For the agglomer-
ative HK10, we initialized its parameters to the val-
ues found by our model.4

Based on our observations about homoplasy, we
also considered a combined system where we ran
PARSIM, and then seeded the agglomerative cluster-
ing algorithm with the clusters found by PARSIM.

For evaluation, we report a few metrics. First,
we report cluster purity, which is a kind of pre-
cision measure for clusterings. Specifically, each
cluster is assigned to the cognate group that is the
most common cognate word in that group, and then
purity is computed as the fraction of words that

4Attempts to learn parameters directly with the agglomera-
tive clustering algorithm were not effective.

351



0.6 

0.7 

0.8 

0.9 

1 

0.4 0.5 0.6 0.7 0.8 0.9 1 

Pr
ec

is
io

n 

Recall 

Combined System 
PARSIM 
Agg. HK10 

Figure 4: Precision/Recall curves for our systems. The
Combined System starts from PARSIM’s output, so it
has fewer points to plot, and starts from a point with
lower precision. As PARSIM outputs only one result, it
is starred.

are in a cluster whose gold cognate group matches
the cognate group of the cluster. For gold parti-
tions G = {G1, G2, . . . , Gg} and found partitions
F = {F1, F2, . . . , Ff}, we have: purity(G,F ) =
1
N

∑
f maxg |Gg∩Ff |. We also report pairwise pre-

cision and recall computed over pairs of words.5 Fi-
nally, because agglomerative clustering does not de-
fine a natural “stopping point” other than when the
likelihood gain decreases to 0—which did not per-
form well in our initial tests—we will report both
a precision/recall curve, as well the maximum pair-
wise F1 obtained by the agglomerative HK10 and
the combined system.

The results are in Table 1. On Formosan, PAR-
SIM has much higher precision and purity than our
agglomerative version of HK10 at its highest point,
though its recall and F1 suffer somewhat. Of course,
the comparison is not quite fair, since we have se-
lected the best possible point for HK10.

However, our combination of the two systems
does even better. By feeding our high-precision re-
sults into the agglomerative system and sacrificing
just a little precision, our combined system achieves
much higher F1 scores than either of the systems
alone.

Next, we also examined precision and recall
curves for the two agglomerative systems on For-

5The main difference between precision and purity is that
pairwise precision is inherently quadratic, meaning that it pe-
nalizes mistakes in large groups much more heavily than mis-
takes in small groups.

mosan, which we have plotted in Figure 4, along
with the one point output by PARSIM.

We then ran PARSIM and the combined system
on the much larger Oceanic dataset. Performance
on all metrics decreased somewhat, but this is to be
expected since there is so much more data. As with
Formosan, PARSIM has higher precision than the
combined system, but it has much lower recall.

8.2 Reconstruction
We also wanted to see how well our cognates could
be used to actually reconstruct the ancestral forms of
words. To do so, we ran a version of Bouchard-Côté
et al. (2009)’s reconstruction system using both the
cognate groups PARSIM found in the Oceanic lan-
guage family and the gold cognate groups provided
by the ABVD. We then evaluated the average Leven-
shtein distance of the reconstruction for each word
to the reconstruction of that word’s Proto-Oceanic
ancestor provided by linguists. Our evaluation dif-
fers from Bouchard-Côté et al. (2009) in that they
averaged over cognate groups, which does not make
sense for our task because there are different cognate
groups. Instead, we average over per-modern-word
reconstruction error.

Using this metric, reconstructions using our sys-
tem’s cognates are an average of 2.47 edit opera-
tions from the gold reconstruction, while with gold
cognates the error is 2.19 on average. This repre-
sents an error increase of 12.8%. To see if there
was some pattern to these errors, we also plotted the
fraction of words with each Levenshtein distance for
these reconstructions in Figure 5. While the plots are
similar, the automatic cognates exhibit a longer tail.
Thus, even with automatic cognates, the reconstruc-
tion system can reconstruct words faithfully in many
cases, but in a few instances our system fails.

9 Analysis

We now consider some of the errors made by our
system. Broadly, there are two kinds of mistakes
in a model like ours: those affecting precision and
those affecting recall.

9.1 Precision
Many of our precision errors seem to be due to
our somewhat limited model of sound change. For
instance, the language Pazeh has two words for

352



0	  

0.05	  

0.1	  

0.15	  

0.2	  

0.25	  

0.3	  

0.35	  

0.4	  

0	   1	   2	   3	   4	   5	   6	   7	   8	   9	  

Fr
ac

tio
n 

of
 W

or
ds

 

Levenshtein Distance 

Automatic Cognates 

Gold Cognates 

Figure 5: Percentage of words with varying levels of
Levenshtein distance from the gold reconstruction. Gold
Cognates were hand-annotated by linguists, while Auto-
matic Cognates were found by our system.

“to sleep:” /mudamai/ and /mid@m/. Somewhat
surprisingly the former word is cognate with Pai-
wan /qmereN/ and Saisiat /maPr@m/ while the lat-
ter is not. Our system, however, makes the mistake
of grouping /mid@m/ with the Paiwan and Saisiat
words. Our system has inferred that the insertions of
/u/ and /ai/ (which are required to bring /mudamai/
into alignment with the Saisiat and Paiwan words)
are less likely than substituting a few vowels and the
consonant /r/ for /d/ (which are required to align
/mid@m/). Perhaps a more sophisticated model of
sound change could correctly learn this relationship.

However, a preliminary inspection of the data
seems to indicate that not all of our precision errors
are actually errors, but rather places where the data
is insufficiently annotated (and indeed, the ABVD is
still a work in progress). For instance, consider the
words for “meat/flesh” in the Formosan languages:
Squliq /hiP/, Bunun /titiP/, Paiwan /seti/, Kavalan
/PisiP/, CentralAmi /titi/, Our system groups all of
these words except for Squliq /hiP/. However, de-
spite these words’ similarity, there are actually three
cognate groups here. One includes Squliq /hiP/ and
Kavalan /PisiP/, another includes just Paiwan /seti/,
and the third includes Bunun /titiP/ and CentralAmi
/titi/. Crucially, these cognate groups do not fol-
low the phylogeny closely. Thus, either there was a
significant amount of borrowing between these lan-
guages, or there was a striking amount of homoplasy
in Proto-Formosan, or these words are in fact mostly
cognate. While a more thorough, linguistically-
informed analysis is needed to ensure that these are
actually cognates, we believe that our system, in

conjunction with a trained Austronesian specialist,
could potentially find many more cognate groups,
speeding up the process of completing the ABVD.

9.2 Recall

Our system can also fail to group words that should
be grouped. One recurring problem seems to
be reduplication, which is a fairly common phe-
nomenon in Austronesian languages. For instance,
there is a cognate group for “to eat” that includes
Bunun /maun/, Thao /kman/, Favorlang /man/, and
Sediq /manakamakan/, among others. Our system
correctly finds this group, with the exception of
/manakamakan/, which is clearly the result of redu-
plication. Reduplication cannot be modeled using
mere sound laws, and so a more complex transition
model is needed to correctly identify these kinds of
changes.

10 Conclusion

We have presented a new system for automatically
finding cognates across many languages. Our sys-
tem is comprised of two parts. The first, PAR-
SIM, is a new high-precision generative model with
tractable inference. The second, HK10, is a mod-
ification of Hall and Klein (2010) that makes their
approximate inference more efficient. We discuss
certain trade-offs needed to make both models scale,
and demonstrated its performance on the Formosan
and Oceanic language families.

References
Shane Bergsma and Greg Kondrak. 2007. Multilingual

cognate identification using integer linear program-
ming. In RANLP Workshop on Acquisition and Man-
agement of Multilingual Lexicons, Borovets, Bulgaria,
September.

Leonard Bloomfield. 1938. Language. Holt, New York.
R. A. Blust. 2009. The Austronesian languages. Aus-

tralian National University.
Alexandre Bouchard-Côté, Percy Liang, Thomas Grif-

fiths, and Dan Klein. 2007. A probabilistic approach
to diachronic phonology. In EMNLP.

Alexandre Bouchard-Côté, Thomas L. Griffiths, and Dan
Klein. 2009. Improved reconstruction of protolan-
guage word forms. In NAACL, pages 65–73.

L. L. Cavalli-Sforza and A. W. F. Edwards. 1965. Analy-
sis of human evolution. In S. J. Geerts Genetics Today,

353



editor, Proceedings of XIth International Congress of
Genetics, 1963, Vol, page 923–933. 3, 3.

Hal Daumé III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational Lin-
guistics (ACL).

Hal Daumé III. 2009. Non-parametric Bayesian areal
linguistics. In NAACL.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1–38.

Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In EMNLP, Singapore, Au-
gust.

R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological sequence analysis. eleventh edition.

James S. Farris. 1973. On Comparing the Shapes of
Taxonomic Trees. Systematic Zoology, 22(1):50–54,
March.

J. Felsenstein. 1973. Maximum likelihood and mini-
mum steps methods for estimating evolutionnary trees
from data on discrete characters. Systematic Zoology,
23:240–249.

W. M. Fitch. 1971. Toward defining the course of evo-
lution: minimal change for a specific tree topology.
Systematic Zoology, 20:406–416.

S.J. Greenhill, R. Blust, and R.D. Gray. 2008. The
Austronesian basic vocabulary database: from bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271–283.

David Hall and Dan Klein. 2010. Finding cognates using
phylogenies. In Association for Computational Lin-
guistics (ACL).

Robert M. Keesing and Jonathan Fifi’i. 1969. Kwaio
word tabooing in its cultural context. Journal of the
Polynesian Society, 78(2):154–177.

Grzegorz Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In NAACL.

Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83–97.

John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguistics,
20(3):381–417.

Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
NAACL.

Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop. John Wiley and Sons.

Andrea Mulloni. 2007. Automatic prediction of cognate
orthography using support vector machines. In ACL,
pages 25–30.

Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
Molecular Biology, 48(3):443 – 453.

John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.

J. Nichols. 1992. Linguistic diversity in space and time.
University of Chicago Press.

Michael P. Oakes. 2000. Computer estimation of vocab-
ulary in a protolanguage from word lists in four daugh-
ter languages. Quantitative Linguistics, 7(3):233–243.

Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100(1):59–129.

D. Sankoff and R. J. Cedergren, 1983. Simultaneuous
comparison of three or more sequences related by a
tree, page 253–263. Addison-Wesley, Reading, MA.

August Schleicher. 1861. A Compendium of the Com-
parative Grammar of the Indo-European, Sanskrit,
Greek and Latin Languages.

354


