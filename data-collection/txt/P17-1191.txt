



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2089–2098
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1191

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2089–2098
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1191

Ontology-Aware Token Embeddings for Prepositional Phrase Attachment

Pradeep Dasigi1 Waleed Ammar2 Chris Dyer1,3 Eduard Hovy1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA, USA

2Allen Institute for Artificial Intelligence, Seattle WA, USA
3DeepMind, London, UK

pdasigi@cs.cmu.edu, wammar@allenai.org,
cdyer@cs.cmu.edu, hovy@cmu.edu

Abstract

Type-level word embeddings use the same
set of parameters to represent all instances
of a word regardless of its context, ignor-
ing the inherent lexical ambiguity in lan-
guage. Instead, we embed semantic con-
cepts (or synsets) as defined in WordNet
and represent a word token in a partic-
ular context by estimating a distribution
over relevant semantic concepts. We use
the new, context-sensitive embeddings in a
model for predicting prepositional phrase
(PP) attachments and jointly learn the con-
cept embeddings and model parameters.
We show that using context-sensitive em-
beddings improves the accuracy of the
PP attachment model by 5.4% absolute
points, which amounts to a 34.4% relative
reduction in errors.

1 Introduction

Type-level word embeddings map a word type
(i.e., a surface form) to a dense vector of real num-
bers such that similar word types have similar em-
beddings. When pre-trained on a large corpus of
unlabeled text, they provide an effective mecha-
nism for generalizing statistical models to words
which do not appear in the labeled training data
for a downstream task.

In accordance with standard terminology, we
make the following distinction between types and
tokens in this paper: By word types, we mean the
surface form of the word, whereas by tokens we
mean the instantiation of the surface form in a con-
text. For example, the same word type ‘pool’ oc-
curs as two different tokens in the sentences “He
sat by the pool,” and “He played a game of pool.”

Most word embedding models define a single
vector for each word type. However, a fundamen-

tal flaw in this design is their inability to distin-
guish between different meanings and abstractions
of the same word. In the two sentences shown
above, the word ‘pool’ has different meanings, but
the same representation is typically used for both
of them. Similarly, the fact that ‘pool’ and ‘lake’
are both kinds of water bodies is not explicitly in-
corporated in most type-level embeddings. Fur-
thermore, it has become a standard practice to tune
pre-trained word embeddings as model parameters
during training for an NLP task (e.g., Chen and
Manning, 2014; Lample et al., 2016), potentially
allowing the parameters of a frequent word in the
labeled training data to drift away from related but
rare words in the embedding space.

Previous work partially addresses these prob-
lems by estimating concept embeddings in Word-
Net (e.g., Rothe and Schütze, 2015), or improv-
ing word representations using information from
knowledge graphs (e.g., Faruqui et al., 2015).
However, it is still not clear how to use a lexical
ontology to derive context-sensitive token embed-
dings.

In this work, we represent a word token in
a given context by estimating a context-sensitive
probability distribution over relevant concepts in
WordNet (Miller, 1995) and use the expected
value (i.e., weighted sum) of the concept embed-
dings as the token representation (see §2). We
take a task-centric approach towards doing this,
and learn the token representations jointly with the
task-specific parameters. In addition to providing
context-sensitive token embeddings, the proposed
method implicitly regularizes the embeddings of
related words by forcing related words to share
similar concept embeddings. As a result, the rep-
resentation of a rare word which does not appear
in the training data for a downstream task benefits
from all the updates to related words which share
one or more concept embeddings.

2089

https://doi.org/10.18653/v1/P17-1191
https://doi.org/10.18653/v1/P17-1191


Figure 1: An example grounding for the word
‘pool’. Solid arrows represent possible senses and
dashed arrows represent hypernym relations. Note
that the same set of concepts are used to ground
the word ‘pool’ regardless of its context. Other
WordNet senses for ‘pool’ were removed from the
figure for simplicity.

Our approach to context-sensitive embeddings
assumes the availability of a lexical ontology.
While this work relies on WordNet, and we ex-
ploit the order of senses given by WordNet, our
model is, in principle applicable to any ontology,
with appropriate modifications. In this work, we
do not assume the inputs are sense tagged. We use
the proposed embeddings to predict prepositional
phrase (PP) attachments (see §3), a challenging
problem which emphasizes the selectional prefer-
ences between words in the PP and each of the
candidate head words. Our empirical results and
detailed analysis (see §4) show that the proposed
embeddings effectively use WordNet to improve
the accuracy of PP attachment predictions.

2 WordNet-Grounded Context-Sensitive
Token Embeddings

In this section, we focus on defining our context-
sensitive token embeddings. We first describe
our grounding of word types using WordNet con-
cepts. Then, we describe our model of context-
sensitive token-level embeddings as a weighted
sum of WordNet concept embeddings.

2.1 WordNet Grounding

We use WordNet to map each word type to a set of
synsets, including possible generalizations or ab-

stractions. Among the labeled relations defined in
WordNet between different synsets, we focus on
the hypernymy relation to help model generaliza-
tion and selectional preferences between words,
which is especially important for predicting PP at-
tachments (Resnik, 1993). To ground a word type,
we identify the set of (direct and indirect) hyper-
nyms of the WordNet senses of that word. A sim-
plified grounding of the word ‘pool’ is illustrated
in Figure 1. This grounding is key to our model of
token embeddings, to be described in the follow-
ing subsections.

2.2 Context-Sensitive Token Embeddings
Our goal is to define a context-sensitive model of
token embeddings which can be used as a drop-
in replacement for traditional type-level word em-
beddings.

Notation. Let Senses(w) be the list of synsets
defined as possible word senses of a given word
type w in WordNet, and Hypernyms(s) be the list
of hypernyms for a synset s.1 For example, ac-
cording to Figure 1:

Senses(pool) = [pond.n.01, pool.n.09], and

Hypernyms(pond.n.01) = [pond.n.01, lake.n.01,

body of water.n.01, entity.n.01]

Each WordNet synset s is associated with a set
of parameters vs ∈ Rn which represent its em-
bedding. This parameterization is similar to that
of Rothe and Schütze (2015).

Embedding model. Given a sequence of tokens
t and their corresponding word types w, let ui ∈
Rn be the embedding of the word token ti at po-
sition i. Unlike most embedding models, the to-
ken embeddings ui are not parameters. Rather, ui
is computed as the expected value of concept em-
beddings used to ground the word type wi corre-
sponding to the token ti:

ui =
∑

s∈Senses(wi)

∑

s′∈Hypernyms(s)
p(s, s′ | t,w, i) vs′

(1)

such that
∑

s∈Senses(wi)

∑

s′∈Hypernyms(s)
p(s, s′ | t,w, i) = 1

1For notational convenience, we assume that s ∈
Hypernyms(s).

2090



Figure 2: Steps for computing the context-
sensitive token embedding for the word ‘pool’, as
described in §2.2.

The distribution which governs the expectation
over synset embeddings factorizes into two com-
ponents:

p(s, s′ | t,w, i) ∝λwi exp−λwi rank(s,wi)×
MLP([vs′ ; context(i, t)]) (2)

The first component, λwi exp
−λwi rank(s,wi), is a

sense prior which reflects the prominence of each
word sense for a given word type. Here, we
exploit2 the fact that WordNet senses are or-
dered in descending order of their frequencies, ob-
tained from sense tagged corpora, and parameter-
ize the sense prior like an exponential distribution.
rank(s, wi) denotes the rank of sense s for the
word type wi, thus rank(s, wi) = 0 corresponds
to s being the first sense of wi. The scalar pa-
rameter (λwi) controls the decay of the probability
mass, which is learned along with the other pa-
rameters in the model. Note that sense priors are
defined for each word type (wi), and are shared
across all tokens which have the same word type.

MLP([vs′ ; context(i, t)]), the second compo-
nent, is what makes the token representations
context-sensitive. It scores each concept in the
WordNet grounding of wi by feeding the concate-
nation of the concept embedding and a dense vec-

2Note that for ontologies where such information is not
available, our method is still applicable but without this com-
ponent. We show the effect of using a uniform sense prior in
§4.2.

tor that summarizes the textual context into a mul-
tilayer perceptron (MLP) with two tanh layers fol-
lowed by a softmax layer. This component is in-
spired by the soft attention often used in neural
machine translation (Bahdanau et al., 2014).3 The
definition of the context function is dependent on
the encoder used to encode the context. We de-
scribe a specific instantiation of this function in
§3.

To summarize, Figure 2 illustrates how to com-
pute the embedding of a word token ti = ‘pool’ in
a given context:

1. compute a summary of the context
context(i, t),

2. enumerate related concepts for ti,

3. compute p(s, s′ | t,w, i) for each pair (s, s′),
and

4. compute ui = E[vs′ ].

In the following section, we describe our model
for predicting PP attachments, including our defi-
nition for context.

3 PP Attachment

Disambiguating PP attachments is an important
and challenging NLP problem. Since modeling
hypernymy and selectional preferences is criti-
cal for successful prediction of PP attachments
(Resnik, 1993), it is a good fit for evaluating our
WordNet-grounded context-sensitive embeddings.

Figure 3, reproduced from Belinkov et al.
(2014), illustrates an example of the PP attach-
ment prediction problem. The accuracy of a
competitive English dependency parser at predict-
ing the head word of an ambiguous prepositional
phrase is 88.5%, significantly lower than the over-
all unlabeled attachment accuracy of the same
parser (94.2%).4

This section formally defines the problem of PP
attachment disambiguation, describes our baseline
model, then shows how to integrate the token-level
embeddings in the model.

3.1 Problem Definition
We follow Belinkov et al. (2014)’s definition of the
PP attachment problem. Given a preposition p and

3Although soft attention mechanism is typically used to
explicitly represent the importance of each item in a se-
quence, it can also be applied to non-sequential items.

4See Table 2 in §4 for detailed results.

2091



Figure 3: Two sentences illustrating the impor-
tance of lexicalization in PP attachment decisions.
In the top sentence, the PP ‘with butter’ attaches
to the noun ‘spaghetti’. In the bottom sentence,
the PP ‘with chopsticks’ attaches to the verb ‘ate’.
Note: This figure and caption have been repro-
duced from Belinkov et al. (2014).

its direct dependent d in the prepositional phrase
(PP), our goal is to predict the correct head word
for the PP among an ordered list of candidate head
words h. Each example in the train, validation,
and test sets consists of an input tuple 〈h, p, d〉
and an output index k to identify the correct head
among the candidates in h. Note that the order of
words that form each 〈h, p, d〉 is the same as that
in the corresponding original sentence.

3.2 Model Definition
Both our proposed and baseline models for PP
attachment use bidirectional RNN with LSTM
cells (bi-LSTM) to encode the sequence t =
〈h1, . . . , hK , p, d〉.

We score each candidate head by feeding the
concatenation of the output bi-LSTM vectors for
the head hk, the preposition p and the direct de-
pendent d through an MLP, with a fully connected
tanh layer to obtain a non-linear projection of
the concatenation, followed by a fully-connected
softmax layer:

p(hkis head) = MLPattach([lstm out(hk);

lstm out(p);

lstm out(d)]) (3)

To train the model, we use cross-entropy loss at
the output layer for each candidate head in the

training set. At test time, we predict the candidate
head with the highest probability according to the
model in Eq. 3, i.e.,

k̂ = argmax
k

p(hkis head = 1). (4)

This model is inspired by the Head-Prep-Child-
Ternary model of Belinkov et al. (2014). The main
difference is that we replace the input features for
each token with the output bi-RNN vectors.

We now describe the difference between the
proposed and the baseline models. Generally, let
lstm in(ti) and lstm out(ti) represent the input and
output vectors of the bi-LSTM for each token ti ∈
t in the sequence. The outputs at each timestep
are obtained by concatenating those of the forward
and backward LSTMs.

Baseline model. In the baseline model, we use
type-level word embeddings to represent the input
vector lstm in(ti) for a token ti in the sequence.
The word embedding parameters are initialized
with pre-trained vectors, then tuned along with the
parameters of the bi-LSTM and MLPattach. We call
this model LSTM-PP.

Proposed model. In the proposed model, we use
token level word embedding as described in §2
as the input to the bi-LSTM, i.e., lstm in(ti) =
ui. The context used for the attention compo-
nent is simply the hidden state from the previous
timestep. However, since we use a bi-LSTM, the
model essentially has two RNNs, and accordingly
we have two context vectors, and associated at-
tentions. That is, contextf (i, t) = lstm in(ti−1)
for the forward RNN and contextb(i, t) =
lstm in(ti+1) for the backward RNN. Conse-
quently, each token gets two representations, one
from each RNN. The synset embedding param-
eters are initialized with pre-trained vectors and
tuned along with the sense decay (λw) and MLP
parameters from Eq. 2, the parameters of the bi-
LSTM and those of MLPattach. We call this model
OntoLSTM-PP.

4 Experiments

Dataset and evaluation. We used the English
PP attachment dataset created and made available
by Belinkov et al. (2014). The training and test
splits contain 33,359 and 1951 labeled examples
respectively. As explained in §3.1, the input for
each example is 1) an ordered list of candidate
head words, 2) the preposition, and 3) the direct

2092



dependent of the preposition. The head words are
either nouns or verbs and the dependent is always a
noun. All examples in this dataset have at least two
candidate head words. As discussed in Belinkov
et al. (2014), this dataset is a more realistic PP at-
tachment task than the RRR dataset (Ratnaparkhi
et al., 1994). The RRR dataset is a binary classifi-
cation task with exactly two head word candidates
in all examples. The context for each example in
the RRR dataset is also limited which defeats the
purpose of our context-sensitive embeddings.

Model specifications and hyperparameters.
For efficient implementation, we use mini-batch
updates with the same number of senses and hy-
pernyms for all examples, padding zeros and trun-
cating senses and hypernyms as needed. For each
word type, we use a maximum of S senses and
H indirect hypernyms from WordNet. In our ini-
tial experiments on a held-out development set
(10% of the training data), we found that values
greater than S = 3 and H = 5 did not im-
prove performance. We also used the develop-
ment set to tune the number of layers in MLPattach
separately for the OntoLSTM-PP and LSTM-PP,
and the number of layers in the attention MLP in
OntoLSTM-PP. When a synset has multiple hy-
pernym paths, we use the shortest one. Finally,
words types which do not appear in WordNet are
assumed to have one unique sense per word type
with no hypernyms. Since the POS tag for each
word is included in the dataset, we exclude Word-
Net synsets which are incompatible with the POS
tag. The synset embedding parameters are initial-
ized using the synset vectors obtained by running
AutoExtend (Rothe and Schütze, 2015) on 100-
dimensional GloVe (Pennington et al., 2014) vec-
tors for WordNet 3.1. We refer to this embedding
as GloVe-extended. Representation for the OOV
word types in LSTM-PP and OOV synset types in
OntoLSTM-PP were randomly drawn from a uni-
form 100-d distribution. Initial sense prior param-
eters (λw) were also drawn from a uniform 1-d dis-
tribution.

Baselines. In our experiments, we compare our
proposed model, OntoLSTM-PP with three base-
lines – LSTM-PP initialized with GloVe em-
bedding, LSTM-PP initialized with GloVe vec-
tors retrofitted to WordNet using the approach of
Faruqui et al. (2015) (henceforth referred to as
GloVe-retro), and finally the best performing stan-

dalone PP attachment system from Belinkov et al.
(2014), referred to as HPCD (full) in the paper.
HPCD (full) is a neural network model that learns
to compose the vector representations of each of
the candidate heads with those of the preposition
and the dependent, and predict attachments. The
input representations are enriched using syntactic
context information, POS, WordNet and VerbNet
(Kipper et al., 2008) information and the distance
of the head word from the PP is explicitly encoded
in composition architecture. In contrast, we do not
use syntactic context, VerbNet and distance infor-
mation, and do not explicitly encode POS infor-
mation.

4.1 PP Attachment Results

Table 1 shows that our proposed token level em-
bedding scheme OntoLSTM-PP outperforms the
better variant of our baseline LSTM-PP (with
GloVe-retro intialization) by an absolute accuracy
difference of 4.9%, or a relative error reduction
of 32%. OntoLSTM-PP also outperforms HPCD
(full), the previous best result on this dataset.

Initializing the word embeddings with GloVe-
retro (which uses WordNet as described in Faruqui
et al. (2015)) instead of GloVe amounts to a small
improvement, compared to the improvements ob-
tained using OntoLSTM-PP. This result illustrates
that our approach of dynamically choosing a con-
text sensitive distribution over synsets is a more
effective way of making use of WordNet.

Effect on dependency parsing. Following Be-
linkov et al. (2014), we used RBG parser (Lei
et al., 2014), and modified it by adding a binary
feature indicating the PP attachment predictions
from our model.

We compare four ways to compute the addi-
tional binary features: 1) the predictions of the
best standalone system HPCD (full) in Belinkov
et al. (2014), 2) the predictions of our baseline
model LSTM-PP, 3) the predictions of our im-
proved model OntoLSTM-PP, and 4) the gold la-
bels Oracle PP.

Table 2 shows the effect of using the PP attach-
ment predictions as features within a dependency
parser. We note there is a relatively small differ-
ence in unlabeled attachment accuracy for all de-
pendencies (not only PP attachments), even when
gold PP attachments are used as additional fea-
tures to the parser. However, when gold PP attach-
ment are used, we note a large potential improve-

2093



System Initialization Embedding Resources Test Acc.
HPCD (full) Syntactic-SG Type WordNet, VerbNet 88.7
LSTM-PP GloVe Type - 84.3
LSTM-PP GloVe-retro Type WordNet 84.8
OntoLSTM-PP GloVe-extended Token WordNet 89.7

Table 1: Results on Belinkov et al. (2014)’s PPA test set. HPCD (full) is from the original paper, and
it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted (Faruqui et al., 2015) to WordNet
3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend (Rothe and
Schütze, 2015) on GloVe.

System Full UAS PPA Acc.
RBG 94.17 88.51
RBG + HPCD (full) 94.19 89.59
RBG + LSTM-PP 94.14 86.35
RBG + OntoLSTM-PP 94.30 90.11
RBG + Oracle PP 94.60 98.97

Table 2: Results from RBG dependency parser
with features coming from various PP attachment
predictors and oracle attachments.

ment of 10.46 points in PP attachment accuracies
(between the PPA accuracy for RBG and RBG +
Oracle PP), which confirms that adding PP pre-
dictions as features is an effective approach. Our
proposed model RBG + OntoLSTM-PP recovers
15% of this potential improvement, while RBG +
HPCD (full) recovers 10%, which illustrates that
PP attachment remains a difficult problem with
plenty of room for improvements even when us-
ing a dedicated model to predict PP attachments
and using its predictions in a dependency parser.

We also note that, although we use the same
predictions of the HPCD (full) model in Belinkov
et al. (2014)5, we report different results than Be-
linkov et al. (2014). For example, the unlabeled
attachment score (UAS) of the baselines RBG and
RBG + HPCD (full) are 94.17 and 94.19, respec-
tively, in Table 2, compared to 93.96 and 94.05,
respectively, in Belinkov et al. (2014). This is due
to the use of different versions of the RBG parser.6

4.2 Analysis

In this subsection, we analyze different aspects of
our model in order to develop a better understand-

5The authors kindly provided their predictions for 1942
test examples (out of 1951 examples in the full test set). In
Table 2, we use the same subset of 1942 test examples and
will include a link to the subset in the final draft.

6We use the latest commit (SHA: e07f74) on the GitHub
repository of the RGB parser.

ing of its behavior.

Effect of context sensitivity and sense priors.
We now show some results that indicate the rela-
tive strengths of two components of our context-
sensitive token embedding model. The second
row in Table 3 shows the test accuracy of a sys-
tem trained without sense priors (that is, making
p(s|wi) from Eq. 1 a uniform distribution), and
the third row shows the effect of making the to-
ken representations context-insensitive by giving a
similar attention score to all related concepts, es-
sentially making them type level representations,
but still grounded in WordNet. As it can be seen,
removing context sensitivity has an adverse effect
on the results. This illustrates the importance of
the sense priors and the attention mechanism.

It is interesting that, even without sense priors
and attention, the results with WordNet grounding
is still higher than that of the two LSTM-PP sys-
tems in Table 1. This result illustrates the regu-
larization behavior of sharing concept embeddings
across multiple words, which is especially impor-
tant for rare words.

Effect of training data size. Since OntoLSTM-
PP uses external information, the gap between the
model and LSTM-PP is expected to be more pro-
nounced when the training data sizes are smaller.
To test this hypothesis, we trained the two mod-
els with different amounts of training data and
measured their accuracies on the test set. The
plot is shown in Figure 4. As expected, the gap
tends to be larger at smaller data sizes. Surpris-
ingly, even with 2000 sentences in the training data
set, OntoLSTM-PP outperforms LSTM-PP trained
with the full data set. When both the models are
trained with the full dataset, LSTM-PP reaches a
training accuracy of 95.3%, whereas OntoLSTM-
PP reaches 93.5%. The fact that LSTM-PP is over-
fitting the training data more, indicates the regular-

2094



Figure 4: Effect of training data size on test accu-
racies of OntoLSTM-PP and LSTM-PP.

Model PPA Acc.
full 89.7
- sense priors 88.4
- attention 87.5

Table 3: Effect of removing sense priors and con-
text sensitivity (attention) from the model.

ization capability of OntoLSTM-PP.

Qualitative analysis. To better understand the
effect of WordNet grounding, we took a sample of
100 sentences from the test set whose PP attach-
ments were correctly predicted by OntoLSTM-
PP but not by LSTM-PP. A common pattern ob-
served was that those sentences contained words
not seen frequently in the training data. Figure 5
shows two such cases. In both cases, the weights
assigned by OntoLSTM-PP to infrequent words
are also shown. The word types soapsuds and
buoyancy do not occur in the training data, but
OntoLSTM-PP was able to leverage the parame-
ters learned for the synsets that contributed to their
token representations. Another important observa-
tion is that the word type buoyancy has four senses
in WordNet (we consider the first three), none of
which is the metaphorical sense that is applicable
to markets as shown in the example here. Select-
ing a combination of relevant hypernyms from var-
ious senses may have helped OntoLSTM-PP make
the right prediction. This shows the value of us-
ing hypernymy information from WordNet. More-
over, this indicates the strength of the hybrid na-
ture of the model, that lets it augment ontological
information with distributional information.

Parameter space. We note that the vocabulary
sizes in OntoLSTM-PP and LSTM-PP are compa-
rable as the synset types are shared across word
types. In our experiments with the full PP at-
tachment dataset, we learned embeddings for 18k
synset types with OntoLSTM-PP and 11k word
types with LSTM-PP. Since the biggest contribu-
tion to the parameter space comes from the em-
bedding layer, the complexities of both the models
are comparable.

5 Related Work

This work is related to various lines of research
within the NLP community: dealing with syn-
onymy and homonymy in word representations
both in the context of distributed embeddings
and more traditional vector spaces; hybrid models
of distributional and knowledge based semantics;
and selectional preferences and their relation with
syntactic and semantic relations.

The need for going beyond a single vector
per word-type has been well established for a
while, and many efforts were focused on building
multi-prototype vector space models of meaning
(Reisinger and Mooney, 2010; Huang et al., 2012;
Chen et al., 2014; Jauhar et al., 2015; Neelakantan
et al., 2015; Arora et al., 2016, etc.). However, the
target of all these approaches is obtaining multi-
sense word vector spaces, either by incorporating
sense tagged information or other kinds of exter-
nal context. The number of vectors learned is still
fixed, based on the preset number of senses. In
contrast, our focus is on learning a context depen-
dent distribution over those concept representa-
tions. Other work not necessarily related to multi-
sense vectors, but still related to our work includes
Belanger and Kakade (2015)’s work which pro-
posed a Gaussian linear dynamical system for es-
timating token-level word embeddings, and Vil-
nis and McCallum (2015)’s work which proposed
mapping each word type to a density instead of
a point in a space to account for uncertainty in
meaning. These approaches do not make use of
lexical ontologies and are not amenable for joint
training with a downstream NLP task.

Related to the idea of concept embeddings is
Rothe and Schütze (2015) who estimated Word-
Net synset representations, given pre-trained type-
level word embeddings. In contrast, our work fo-
cuses on estimating token-level word embeddings
as context sensitive distributions of concept em-

2095



Figure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP
does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of in-
frequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTM-
PP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets.

beddings.

There is a large body of work that tried to im-
prove word embeddings using external resources.
Yu and Dredze (2014) extended the CBOW model
(Mikolov et al., 2013) by adding an extra term in
the training objective for generating words con-
ditioned on similar words according to a lexi-
con. Jauhar et al. (2015) extended the skipgram
model (Mikolov et al., 2013) by representing word
senses as latent variables in the generation pro-
cess, and used a structured prior based on the on-
tology. Faruqui et al. (2015) used belief propa-
gation to update pre-trained word embeddings on
a graph that encodes lexical relationships in the
ontology. Similarly, Johansson and Pina (2015)
improved word embeddings by representing each
sense of the word in a way that reflects the topol-
ogy of the semantic network they belong to, and
then representing the words as convex combina-
tions of their senses. In contrast to previous work
that was aimed at improving type level word repre-
sentations, we propose an approach for obtaining
context-sensitive embeddings at the token level,
while jointly optimizing the model parameters for
the NLP task of interest.

Resnik (1993) showed the applicability of se-
mantic classes and selectional preferences to re-
solving syntactic ambiguity. Zapirain et al. (2013)
applied models of selectional preferences auto-

matically learned from WordNet and distributional
information, to the problem of semantic role la-
beling. Resnik (1993); Brill and Resnik (1994);
Agirre (2008) and others have used WordNet in-
formation towards improving prepositional phrase
attachment predictions.

6 Conclusion

In this paper, we proposed a grounding of lexical
items which acknowledges the semantic ambigu-
ity of word types using WordNet and a method
to learn a context-sensitive distribution over their
representations. We also showed how to integrate
the proposed representation with recurrent neural
networks for disambiguating prepositional phrase
attachments, showing that the proposed WordNet-
grounded context-sensitive token embeddings out-
performs standard type-level embeddings for pre-
dicting PP attachments. We provided a detailed
qualitative and quantitative analysis of the pro-
posed model.

Implementation and code availability. The
models are implemented using Keras (Chol-
let, 2015), and the functionality is avail-
able at https://github.com/pdasigi/
onto-lstm in the form of Keras layers to make
it easier to use the proposed embedding model in
other NLP problems.

2096



Future work. This approach may be extended
to other NLP tasks that can benefit from using
encoders that can access WordNet information.
WordNet also has some drawbacks, and may not
always have sufficient coverage given the task at
hand. As we have shown in §4.2, our model can
deal with missing WordNet information by aug-
menting it with distributional information. More-
over, the methods described in this paper can be
extended to other kinds of structured knowledge
sources like Freebase which may be more suitable
for tasks like question answering.

Acknowledgements

The first author is supported by a fellowship from
the Allen Institute for Artificial Intelligence. We
would like to thank Matt Gardner, Jayant Krishna-
murthy, Julia Hockenmaier, Oren Etzioni, Hector
Liu, Filip Ilievski, and anonymous reviewers for
their comments.

References
Eneko Agirre. 2008. Improving parsing and pp attach-

ment performance with sense information. In ACL.
Citeseer.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016. Linear algebraic struc-
ture of word senses, with applications to polysemy.
arXiv preprint arXiv:1601.03764 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473.

David Belanger and Sham M. Kakade. 2015. A linear
dynamical system model for text. In ICML.

Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir
Globerson. 2014. Exploring compositional architec-
tures and word vector representations for preposi-
tional phrase attachment. Transactions of the Asso-
ciation for Computational Linguistics 2:561–572.

Eric Brill and Philip Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment disam-
biguation. In Proceedings of the 15th conference on
Computational linguistics-Volume 2. Association for
Computational Linguistics, pages 1198–1204.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP. pages 740–750.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In EMNLP. pages 1025–1035.

François Chollet. 2015. Keras. https://github.
com/fchollet/keras.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard H. Hovy, and Noah A. Smith.
2015. Retrofitting word vectors to semantic lexi-
cons. In NAACL.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1. Association for Com-
putational Linguistics, pages 873–882.

Sujay Kumar Jauhar, Chris Dyer, and Eduard H. Hovy.
2015. Ontologically grounded multi-sense represen-
tation learning for semantic vector space models. In
NAACL.

Richard Johansson and Luis Nieto Pina. 2015. Em-
bedding a semantic network in a word space. In
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics–Human Language Technologies.
Citeseer.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
english verbs. Language Resources and Evaluation
42(1):21–40.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL.

Tao Lei, Yuan Zhang, Regina Barzilay, and Tommi
Jaakkola. 2014. Low-rank tensors for scoring de-
pendency structures. In ACL. Association for Com-
putational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed repre-
sentations of words and phrases and their composi-
tionality. CoRR abs/1310.4546.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.

Arvind Neelakantan, Jeevan Shankar, Alexandre
Passos, and Andrew McCallum. 2015. Effi-
cient non-parametric estimation of multiple embed-
dings per word in vector space. arXiv preprint
arXiv:1504.06654 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the workshop
on Human Language Technology.

2097



Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In HLT-ACL.

Philip Resnik. 1993. Semantic classes and syntactic
ambiguity. In Proceedings of the workshop on Hu-
man Language Technology. Association for Compu-
tational Linguistics.

Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In ACL.

Luke Vilnis and Andrew McCallum. 2015. Word rep-
resentations via gaussian embedding. In ICLR.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In ACL.

Benat Zapirain, Eneko Agirre, Lluis Marquez, and Mi-
hai Surdeanu. 2013. Selectional preferences for se-
mantic role classification. Computational Linguis-
tics .

2098


	Ontology-Aware Token Embeddings for Prepositional Phrase Attachment

