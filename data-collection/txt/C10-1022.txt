Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188–196,

Beijing, August 2010

188

A Twin-Candidate Based Approach for Event Pronoun Resolution us-

ing Composite Kernel  

Chen Bin1 

Su Jian2 

Tan Chew Lim1 

1National University of Singapore 

2Institute for Inforcomm Research, A-STAR 

{chenbin,tancl}@comp.nus.edu.sg 

sujian@i2r.a-star.edu.sg 

 

Abstract 

Event  Anaphora  Resolution  is  an  important 
task  for  cascaded  event  template  extraction 
and other NLP study. In this paper, we provide 
a  first  systematic  study  of  resolving  pronouns 
to  their  event  verb  antecedents  for  general 
purpose.  First,  we  explore  various  positional, 
lexical  and  syntactic  features  useful  for  the 
event  pronoun  resolution.  We  further  explore 
tree  kernel  to  model  structural  information 
embedded  in  syntactic  parses.  A  composite 
kernel  is  then  used  to  combine  the  above  di-
verse information. In addition, we employed a 
twin-candidate  based  preferences 
learning 
model to capture the pair wise candidates’ pre-
ference  knowledge.  Besides  we  also  look  into 
the  incorporation  of  the  negative  training  in-
stances  with  anaphoric  pronouns  whose  ante-
cedents are not verbs. Although these negative 
training  instances  are  not  used  in  previous 
study on anaphora resolution, our study shows 
that  they  are  very  useful  for  the  final  resolu-
tion  through  random  sampling  strategy.  Our 
experiments  demonstrate  that  it’s  meaningful 
to  keep  certain  training  data  as  development 
data to help SVM select a more accurate hyper 
plane which provides significant improvement 
over the default setting with all training data. 

1  Introduction 
Anaphora resolution, the task of resolving a giv-
en  text  expression  to  its  referred  expression  in 
prior  texts,  is  important  for  intelligent  text 
processing  systems.  Most  previous  works  on 
anaphora  resolution  mainly  aims  at  object  ana-
phora  in  which  both  the  anaphor  and  its  antece-
dent are mentions of the same real world objects 
In contrast, an event anaphora as first defined 
in  (Asher,  1993) is  an anaphoric reference  to  an 
event,  fact,  and  proposition  which  is  representa-
tive  of  eventuality  and  abstract  entity.  Consider 
the following example: 

This  was  an  all-white,  all-Christian  community 
that  all  the  sudden  was  taken  over  --  not  taken 
over, that's a very bad choice of words, but  [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one  of  the  town's  two  meat-packing  plants  13 
years ago. 

The  anaphor  [It]2  in  the  above  example  refers 
back to an event, “all-white and all-Christian city 
of Postville is diluted by different ethnic groups.” 
Here,  we  take  the  main  verb  of  the  event,  [in-
vaded]1  as  the  representation  of  this  event  and 
the antecedent for pronoun [It]2.  

According  to  (Asher,  1993),  antecedents  of 
event  pronoun  include  both  gerunds  (e.g.  de-
struction) and inflectional verbs (e.g. destroying). 
In  our  study,  we  focus  on  the  inflectional  verb 
representation,  as  the  gerund  representation  is 
studied  in  the  conventional  anaphora  resolution. 
For  the  rest  of  this  paper,  “event  pronouns”  are 
pronouns  whose  antecedents  are  event  verbs 
while “non-event  anaphoric  pronouns”  are  those 
with antecedents other than event verbs. 

in  other  natural 

 Entity  anaphora  resolution  provides  critical 
links  for  cascaded  event  template  extraction.  It 
also provides useful information for further infe-
rence  needed 
language 
processing  tasks  such  as  discourse  relation  and 
entailment.  Event  anaphora  (both  pronouns  and 
noun  phrases)  contributes  a  significant  propor-
tion  in  anaphora  corpora,  such  as  OntoNotes. 
19.97% of its total number of entity chains con-
tains event verb mentions. 

In  (Asher,  1993)  chapter  6,  a  method  to  re-
solve  references  to  abstract  entities  using  dis-
course  representation  theory  is  discussed.  How-
ever,  no  computation  system  was  proposed  for 
entity  anaphora  resolution.  (Byron,  2002)  pro-
posed  semantic  filtering  as  a  complement  to  sa-
lience  calculations  to  resolve  event  pronoun  tar-
geted by us. This knowledge deep approach only 

189

works  for  much  focused  domain  like  trains spo-
ken  dialogue  with  handcraft  knowledge  of  rele-
vant events for only limited number of verbs in-
volved.  Clearly, this approach is not suitable for 
general  event  pronoun  resolution  say  in  news 
articles.  Besides,  there’s  also  no  specific  perfor-
mance  report  on  event  pronoun  resolution,  thus 
it’s  not  clear  how  effective  their  approach  is. 
(Müller, 2007) proposed pronoun resolution sys-
tem  using  a  set  of  hand-crafted  constraints  such 
as “argumenthood” and “right-frontier condition” 
together with logistic regression model based on 
corpus  counts.  The  event  pronouns  are  resolved 
together  with  object  pronouns.  This  explorative 
work produced an 11.94% F-score for event pro-
noun resolution which demonstrated the difficul-
ty  of  event  anaphora  resolution.  In  (Pradhan, 
et.al,  2007),  a  general  anaphora  resolution  sys-
tem  is  applied  to  OntoNotes  corpus.  However, 
their  set  of  features  is  designed  for  object  ana-
phora  resolution.  There  is  no  specific  perfor-
mance  reported  on  event  anaphora.  We  suspect 
the  event  pronouns  are  not  correctly  resolved  in 
general as most of these features are irrelevant to 
event pronoun resolution.  

In  this  paper,  we  provide  the  first  systematic 
study  on  pronominal  references  to  event  antece-
dents.  First,  we  explore  various  positional,  lexi-
cal  and  syntactic  features  useful  for  event  pro-
noun  resolution,  which  turns  out  quite  different 
from  conventional  pronoun  resolution  except 
sentence  distance  information.  These  have  been 
used  together  with  syntactic  structural  informa-
tion  using  a  composite  kernel.  Furthermore,  we 
also  consider  candidates’  preferences  informa-
tion using twin-candidate model. 

Besides we further look into the incorporation 
of  negative  instances  from  non-event  anaphoric 
pronoun, although these instances are not used in 
previous study on co-reference or anaphora reso-
lution as they make training instances extremely 
unbalanced.  Our  study  shows  that  they  can  be 
very  useful  for  the  final  resolution  after  random 
sampling strategy.  

We further demonstrate that it’s meaningful to 
keep certain training data as development data to 
help  SVM  select  a  more  accurate  hyper-plane 
which  provide  significant  improvement  over  the 
default setting with all training data.  

The rest of this paper is organized as follows.  
Section  2  introduces  the  framework  for  event 

pronoun  resolution,  the  considerations  on  train-
ing instance, the various features useful for event 
pronoun  resolution  and  SVM  classifier  with  ad-
justment  of  hyper-plane.  Twin-candidate  model 
is  further  introduced  to  capture  the  preferences 
among  candidates.  Section  3  presents  in  details 
the  structural  syntactic  feature  and  the  kernel 
functions to incorporate such a feature in the res-
olution. Section 4 presents the experiment results 
and  some  discussion.  Section  5  concludes  the 
paper. 

2  The Resolution Framework 
Our event-anaphora resolution system adopts the 
common  learning-based  model  for  object  ana-
phora  resolution,  as  employed  by  (Soon  et  al., 
2001) and (Ng and Cardie, 2002a). 

2.1  Training and Testing instance 
In  the  learning  framework,  training  or  testing 
instance  of  the  resolution  system  has  a  form  of 
                 where         is  the  ith  candi-
date  of  the  antecedent  of  anaphor     .  An  in-
stance is labeled as positive if        is the ante-
cedent  of       ,  or  negative  if        is  not  the 
antecedent  of        .  An  instance  is  associated 
with  a  feature  vector  which  records  different 
properties and relations between     and       . 
The features used in our system will be discussed 
later in this paper.  

During  training,  for  each  event  pronoun,  we 
consider  the  preceding  verbs  in  its  current  and 
previous  two  sentences  as  its  antecedent  candi-
dates. A positive instance is formed by pairing an 
anaphor with its correct antecedent. And a set of 
negative  instances  is  formed  by  pairing  an  ana-
phor  with  its  candidates  other  than  the  correct 
antecedent.  In  addition,  more  negative  instances 
are  generated  from  non-event  anaphoric  pro-
nouns. Such an instance is created by pairing up 
a  non-event  anaphoric  pronoun  with each  of  the 
verbs within the pronoun’s sentence and previous 
two  sentences.  This  set  of  instances  from  non-
event anaphoric pronouns is employed to provide 
extra  power  on  ruling  out  non-event  anaphoric 
pronouns  during  resolution.  This  is  inspired  by 
the fact that event pronouns are only 14.7% of all 
the pronouns in the OntoNotes corpus. Based on 
these generated training instances, we can train a 
binary  classifier  using  any  discriminative  learn-
ing algorithm. 

190

The  natural  distribution  of  textual  data  is  of-
ten imbalanced. Classes with fewer examples are 
under-represented  and  classifiers  often  perform 
far below satisfactory. In our study, this becomes 
a  significant  issue  as  positive  class  (event  ana-
phoric)  is  the  minority  class  in  pronoun  resolu-
tion  task.  Thus  we  utilize  a  random  down  sam-
pling method to reduce majority class samples to 
an  equivalent level  with  the  minority  class  sam-
ples  which  is  described  in  (Kubat  and  Matwin, 
1997)  and  (Estabrooks  et  al,  2004).  In  (Ng  and 
Cardie, 2002b), they proposed a negative sample 
selection  scheme  which  included  only  negative 
instances  found  in  between  an  anaphor  and  its 
antecedent. However, in our event pronoun reso-
lution, we are distinguishing the event-anaphoric 
from  non-event  anaphoric  which  is  different 
from (Ng and Cardie, 2002b). 

2.2  Feature Space 
In  a  conventional  pronoun  resolution,  a  set  of 
syntactic  and  semantic  knowledge  has  been  re-
ported as in (Strube and Müller, 2003; Yang et al, 
2004;2005a;2006).  These  features  include  num-
ber agreement, gender agreement and many oth-
ers. However, most of these features are not use-
ful for our task, as our antecedents are inflection-
al  verbs  instead  of  noun  phrases.  Thus  we  have 
conducted  a  study  on  effectiveness  of  potential 
positional,  lexical  and  syntactic  features.  The 
lexical  knowledge  is  mainly  collected  from  cor-
pus  statistics.  The  syntactic  features  are  mainly 
from intuitions. These features are purposely en-
gineered  to  be  highly  correlated  with  positive 
instances.  Therefore  such  kind  of  features  will 
contribute to a high precision classifier.  
  Sentence Distance 
This  feature  measures  the  sentence  distance  be-
tween  an  anaphor  and  its  antecedent  candidate 
under  the  assumptions  that  a  candidate  in  the 
closer sentence to the anaphor is preferred to be 
the antecedent. 

  Word Distance  
This feature measures the word distance between 
an  anaphor  and  its  antecedent  candidate.  It  is 
mainly  to  distinguish  verbs  from  the  same  sen-
tence. 
  Surrounding Words and POS Tags 
The intuition behind this set of features is to find 
potential surface words that occur most frequent-
ly  with  the  positive  instances.  Since  most  of 

verbs occurred in front of pronoun, we have built 
a frequency table from the preceding 5 words of 
the  verb  to  succeeding  5  surface  words  of  the 
pronoun.  After  the  frequency  table  is  built,  we 
select  those  words  with  confidence 1 >  70%  as 
features. Similar to Surrounding Words, we have 
built  a  frequency  table  to  select  indicative  sur-
rounding POS tags which occurs most frequently 
with positive instances. 

  Co-occurrences of Surrounding Words 
The intuition behind this set of features is to cap-
ture  potential  surface  patterns  such  as  “It 
caused…”  and  “It  leads  to”.  These  patterns  are 
associated  with  strong  indication  that  pronoun 
“it”  is  an  event  pronoun.  The  range  for  the  co-
occurrences  is  from  preceding  5  words  to  suc-
ceeding  5  words.  All  possible  combinations  of 
word  positions  are  used  for  a  co-occurrence 
words  pattern.  For  example  “it  leads  to”  will 
generate a pattern as “S1_S2_lead_to” where S1 
and S2 mean succeeding position 1 and 2. Simi-
lar  to  previous  surrounding  words,  we  will  con-
duct  corpus  statistics  analysis  and  select  co-
occurrence  patterns  with  a  confidence  greater 
than 70%. Following the same process, we have 
examined co-occurrence patterns for surrounding 
POS tags.  
  Subject/Object Features 
This  set  of  features  aims  to  capture  the  relative 
position of the pronoun in a sentence. It denotes 
the preference of pronoun’s position at the clause 
level.  There  are  4  features  in  this  category  as 
listed below. 
Subject of Main Clause 
This feature indicates whether a pronoun is at the 
subject position of a main clause. 
Subject of Sub-clause 
This feature indicates whether a pronoun is at the 
subject position of a sub-clause. 
Object of Main Clause 
This feature indicates whether a pronoun is at the 
object position of a main clause. 
Object of Sub-clause 
This feature indicates whether a pronoun is at the 
object position of a sub-clause. 

  Verb of Main/Sub Clause 
Similar  to  the  Subject/Object  features  of  pro-
noun, the following two features capture the rela-

                                                 
1                 

                                          

                     

 

191

tive  position  of  a  verb  in  a  sentence.  It  encodes 
the  preference  of  verb  position  between  main 
verbs in main/sub clauses. 
Main Verb in Main Clause 
This  feature  indicates  whether  a  verb  is  a  main 
verb in a main clause. 
Main Verb in Sub-clause 
This  feature  indicates  whether  a  verb  is  a  main 
verb in a sub-clause. 

2.3  Support Vector Machine 
In  theory,  any  discriminative  learning  algorithm 
is applicable to learn a classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels 
to  incorporate  the  structure  feature.  One  advan-
tage  of  SVM  is  that  we  can  use  tree  kernel  ap-
proach to capture syntactic parse tree information 
in a particular high-dimension space. 

Suppose  a  training  set    consists  of  labeled 
vectors           ,  where     is  the  feature  vector 
of a training instance and    is its class label. The 
classifier learned by SVM is: 

                             

  

   

where     is  the  learned  parameter  for  a  support 
vector   .  An  instance   is  classified  as  positive 
if         . Otherwise,   is negative. 
 Adjust Hyper-plane with Development Data 
Previous  works  on  pronoun  resolution  such  as 
(Yang  et  al,  2006)  used  the  default  setting  for 
hyper-plane  which  sets           .  And  an  in-
stance  is  positive  if          and  negative  oth-
erwise.  In  our  study,  we  look  into  a  method  of 
adjusting the hyper-plane’s position using devel-
opment  data  to  improve  the  classifier’s  perfor-
mance.  

Considering a default model setting for SVM 
as shown in Figure 2(for illustration purpose, we 
use a 2-D example). 

Figure 2: 2-D SVM Illustration 

 

The objective of SVM learning process is to find 
a  set  of  weight  vector   which  maximizes  the 
margin  (defined  as   
   )  with  constraints  defined 

by support vectors. The separating hyper-plane is 
given by               as bold line in the center. 
The margin is the region between the two dotted 
lines  (bounded  by                 and         
      ). The  margin  is  a  space  without any  in-
formation  from  training  instances.  The  actual 
hyper-plane  may  fall  in  any  place  within  the 
margin.  It  does  not  necessarily  occur  in  the. 
However,  the  hyper-plane  is  used  to  separate 
positive and negative instances during classifica-
tion process without consideration of the margin. 
Thus if an instance falls in the margin, SVM can 
only  decide  class  label  from  hyper-plane  which 
may cause misclassification in the margin. 

 Based on the previous discussion, we propose 
an adjustment of the  hyper-plane using  develop-
ment  data.  For  simplicity,  we  adjust  the  hyper-
plane  function  value  instead  of  modeling  the 
function  itself.  The  hyper-plane  function  value 
will be further referred as a threshold  . The fol-
lowing  is  a  modified  version  of  a  learned  SVM 
classifier. 

           

                                

   

     

  

                               

   

     

where   is  the  threshold,     is  the  learned  para-
meter  for  a  feature    and    is  its  class  label.  A 
set of development data is used to adjust the hy-
per-plane  function  threshold   in  order  to  max-
imize the accuracy of the learned SVM classifier 
on  development  data.  The  adjustment  of  hyper-
plane is defined as: 

                                 

  

   

where         is an indicator function which out-
put 1 if        is same sign as   and 0 otherwise. 
Thereafter,  the learned threshold    is applied to 
the testing set. 

3  Incorporating  Structural  Syntactic  In-

formation 

A  parse  tree  that  covers  a  pronoun  and  its  ante-
cedent  candidate  could  provide  us  much  syntac-
tic information related to the pair which is expli-
citly or implicitly represented in the tree. There-
fore,  by  comparing  the  common  sub-structures 
between two trees we can find out to what degree 
two  trees  contain  similar  syntactic  information, 
which can be done using a convolution tree ker-
nel. The  value returned from  tree  kernel  reflects 
similarity between two instances in syntax. Such 

192

syntactic similarity can be further combined with 
other  knowledge  to  compute  overall  similarity 
between two instances, through a composite ker-
nel. Normally, parsing is done  at sentence level. 
However, in many cases a pronoun and its ante-
cedent  candidate  do  not  occur  in  the  same  sen-
tence.  To  present  their  syntactic  properties  and 
relations in a single tree structure, we construct a 
syntax  tree  for  an  entire  text,  by  attaching  the 
parse trees of all its sentences to an upper node. 
Having obtained the parse tree of a text, we shall 
consider how to select the appropriate portion of 
the  tree  as  the  structured  feature  for  a  given  in-
stance.  As  each  instance  is  related  to  a  pronoun 
and  a  candidate,  the  structured  feature  at  least 
should be able to cover both of these two expres-
sions. 

3.1  Structural Syntactic Feature 
Generally,  the  more  substructure  of  the  tree  is 
included,  the  more  syntactic  information  would 
be provided, but at the same time the more noisy 
information  that  comes  from  parsing  errors 
would likely be introduced. In our study, we ex-
amine three possible structured features that con-
tain different substructures of the parse tree: 

 

  Minimum Expansion Tree 
This feature records the minimal structure cover-
ing both pronoun and its candidate in parse tree. 
It only includes the nodes occurring in the short-
est path connecting the pronoun and its candidate, 
via  the  nearest  commonly  commanding  node.  
When the pronoun and candidate are from differ-
ent sentences, we will find a path through pseudo 
“TOP” node which links all the parse trees. Con-
sidering the example given in section 1,  
This  was  an  all-white,  all-Christian  community 
that  all  the  sudden  was  taken  over  --  not  taken 
over, that's a very bad choice of words, but  [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one  of  the  town's  two  meat-packing  plants  13 
years ago. 
The minimum expansion structural feature of the 
instance  {invaded,  it}  is  annotated  with  bold 
lines and shaded nodes in figure 1.  
  Simple Expansion Tree 
Minimum-Expansion  could,  to some  degree,  de-
scribe  the  syntactic  relationships  between  the 
candidate and pronoun. However, it is incapable 
of  capturing  the  syntactic  properties  of  the  can-

didate or the pronoun, because the tree structure 
surrounding the expression is not taken into con-
sideration. To incorporate such information, fea-
ture  Simple-Expansion  not  only  contains  all  the 
nodes in Minimum-Expansion, but also includes 
the first-level children of these nodes2 except the 
punctuations.  The  simple-expansion  structural 
feature  of  instance  {invaded,  it}  is  annotated  in 
figure 2. In the left sentence’s tree, the node “NP” 
for  “perhaps  different  groups”  is  terminated  to 
provide a clue that we have a noun phrase at the 
object position of the candidate verb. 

Figure 1: Minimum-Expansion Tree 

Figure 2: Simple Expansion Tree 

Figure 3: Full-Expansion Tree 

 

 

 

  Full Expansion Tree 
This  feature  focuses  on  the  whole  tree  structure 
between  the  candidate  and  pronoun.  It  not  only 
includes  all  the  nodes  in  Simple-Expansion,  but 
also the nodes (beneath the nearest commanding 
parent)  that  cover  the  words  between  the  candi-

                                                 
2 If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences 
before the candidate or after the pronoun. 

TOP

S

S

NP

VP

NP

S

VP

VP

PP

NP

ADVP

DT

VBD

CC

VBN

IN

This

was

…...

but

invaded

by

,

,

RB

JJ

NNS

perhaps

different

groups

.

.

NP

VP

SBAR

S

WHADVP

NP

VP

PRP VBD WRB DT

It

began when a

…...

…...

.

.

TOP

S

S

NP

VP

NP

S

VP

VP

PP

NP

ADVP

DT

VBD

CC

VBN

IN

This

was

…...

but

invaded

by

,

,

RB

JJ

NNS

perhaps

different

groups

.

.

NP

VP

SBAR

S

WHADVP

NP

VP

PRP VBD WRB DT

It

began when a

…...

…...

.

.

TOP

S

S

NP

VP

NP

S

VP

VP

PP

NP

ADVP

DT

VBD

CC

VBN

IN

This

was

…...

but

invaded

by

,

,

RB

JJ

NNS

perhaps

different

groups

.

.

NP

VP

SBAR

S

WHADVP

NP

VP

PRP VBD WRB DT

It

began when a

…...

…...

.

.

193

date  and  the  pronoun3.  Such  a  feature  keeps  the 
most information related to the pronoun and can-
didate pair. Figure 3 shows the structure for fea-
ture full-expansion for instance {invaded, it}. As 
illustrated, the “NP” node for “perhaps different 
groups” is further expanded to the POS level. All 
its child nodes are included in the full-expansion 
tree except the surface words. 

3.2  Convolution Parse Tree Kernel and Com-

posite Kernel 

To  calculate  the  similarity  between  two  struc-
tured features, we use the convolution tree kernel 
that  is  defined  by  Collins  and  Duffy  (2002)  and 
Moschitti  (2004).  Given  two  trees,  the  kernel 
will  enumerate  all  their  sub-trees  and  use  the 
number  of  common  sub-trees  as  the  measure  of 
similarity between two trees. The above tree ker-
nel only aims for the structured feature. We also 
need  a  composite  kernel  to  combine  the  struc-
tured  feature  and  the  flat  features  from  section 
2.2. In our study we define the composite kernel 
as follows: 

               

             
               

 

             
 
               

where         is  the  convolution  tree  kernel  de-
fined  for  the  structured  feature,  and       is  the 
kernel  applied  on  the  flat  features.  Both  kernels 
are divided by their respective length4
 for norma-
lization.  The  new  composite  kernel      ,  de-
fined  as  the  sum  of  normalized       and      , 
will  return  a  value  close  to  1  only  if  both  the 
structured features and the flat features have high 
similarity under their respective kernels. 

3.3  Twin-Candidate Framework using Rank-

ing SVM Model 

In  a  ranking  SVM  kernel  as  described  in  (Mo-
schitti  et  al,  2006)  for  Semantic  Role  Labeling, 
two argument annotations (as argument trees) are 
presented  to  the  ranking  SVM  model  to  decide 
which one is better.  In our case, we present two 
syntactic  trees  from  two  candidates  to  the  rank-
ing  SVM  model. The  idea is  inspired  by  (Yang, 
et.al,  2005b;2008).  The  intuition  behind  the 
twin-candidate  model  is  to  capture  the  informa-
tion  of  how  much  one  candidate  is  more  pre-

                                                 
3 We will not expand the nodes denoting the sentences other 
than where the pronoun and the candidate occur. 
4  The  length  of  a  kernel     is  defined  as               
                       

ferred than  another.  The  candidate  wins  most  of 
the  pair  wise  comparisons  is  selected  as  antece-
dent. 

The  feature  vector  for  each  training  instance 
has  a  form  of                        .  An  in-
stance  is  positive  if       is  a  better  antecedent 
choice  than        .  Otherwise,  it  is  a  negative 
instance. For each feature vector, both tree struc-
tural  features  and  flat  features  are  used.    Thus 
each  feature  vector  has  a  form  of      
                 where     and     are  trees  of  candi-
date i and j respectively,    and    are flat feature 
vectors of candidate i and j respectively.  

In  the  training  instances  generation,  we  only 
generate  those  instances  with  one  candidate  is 
the  correct  antecedent.  This  follows  the  same 
strategy  used  in  (Yang  et  al,  2008)  for  object 
anaphora resolution. 

 
 

In  the  resolution  process,  a  list  of  m  candi-
dates is extracted from a three sentences window. 
A total of  
  instances are generated by pairing-
up  the  m  candidates  pair-wisely.  We  used  a 
Round-Robin  scoring  scheme  for  antecedent  se-
lection.  Suppose  a  SVM  output  for  an  instance 
                      is 1, we will give a score 
1 for        and -1 for        and vice versa. At 
last,  the  candidate  with  the  highest  score  is  se-
lected  as  antecedent.  In  order  to  handle  a  non-
event anaphoric pronoun, we have set a threshold 
to  distinguish  event  anaphoric  from  non-event 
anaphoric.  A  pronoun  is  considered  as  event 
anaphoric  if  its  score  is  above  the  threshold.  In 
our  experiments,  we  kept  a  set  of  development 
data to find out the threshold in an empirical way. 

4  Experiments and Discussions 
4.1  Experimental Setup 
OntoNotes  Release  2.0  English  corpus  as  in 
(Hovy  et  al,  2006)  is  used  in  our  study,  which 
contains  300k  words  of  English  newswire  data 
(from the Wall Street Journal) and 200k words of 
English  broadcast  news  data  (from  ABC,  CNN, 
NBC,  Public  Radio  International  and  Voice  of 
America).  Table 1 shows the distribution of var-
ious  entities.  We  focused  on  the  resolution  of 
502  event  pronouns  encountered  in  the  corpus. 
The  resolution  system  has  to  handle  both  the 
event  pronoun  identification  and  antecedent  se-
lection tasks. To illustrate the difficulty of event 
pronoun  resolution,  14.7%  of  all  pronoun  men-
tions  are  event  anaphoric  and  only  31.5%  of 

194

event  pronoun  can  be  resolved  using  “most  re-
cent  verb”  heuristics.  Therefore  a  most-recent-
verb baseline will yield an f-score 4.63%. 

To conduct event pronoun resolution, an input 
raw  text  was  preprocessed  automatically  by  a 
pipeline  of  NLP  components.  The  noun  phrase 
identification and the predicate-argument extrac-
tion  were  done  based  on  Stanford  Parser  (Klein 
and  Manning,  2003a;b)  with  F-score  of  86.32% 
on Penn Treebank corpus.  

Non-Event Anaphora:        4952   80.03% 
Event NP:        733   59.35% 

Event  
Anaphora: 
1235  
19.97% 

Event  
Pronoun: 
502   40.65% 

It:       29.0% 
This:   16.9% 
That:  54.1% 

Table  1:  The  distribution  of  various  types  of  6187 

anaphora in OntoNotes 2.0 

For  each  pronoun  encountered  during  resolu-
tion,  all  the  inflectional  verbs  within  the  current 
and  previous  two  sentences  are  taken  as  candi-
dates.  For  the  current  sentence,  we  take  only 
those verbs in front of the pronoun. On average, 
each  event  pronoun  has  6.93  candidates.  Non-
event anaphoric pronouns will generate 7.3 nega-
tive instances on average.  

4.2  Experiment Results and Discussion 
In this section, we will present our experimental 
results with discussions. The performance meas-
ures  we  used  are  precision,  recall  and  F-score. 
All  the  experiments  are  done  with  a  10-folds 
cross validation. In each fold of experiments, the 
whole corpus is divided into 10 equal sized por-
tions.  One  of  them  is  selected  as  testing  corpus 
while  the  remaining  9  are  used  for  training.  In 
experiments  with  development  data,  1  of  the  9 
training portions is kept for development purpose. 
In  case  of  statistical  significance  test  for  differ-
ences is needed, a two-tailed, paired-sample Stu-
dent’s t-Test is performed at 0.05 level of signi-
ficance. 

In the first set of experiments, we are aiming 
to  investigate  the  effectiveness  of  each  single 
knowledge  source.  Table  2  reports  the  perfor-
mance  of  each  individual  experiment.  The  flat 
feature set yields a baseline system with 40.6% f-
score. By using each tree structure along, we can 
only  achieve  a  performance  of  44.4%  f-score 
using  the  minimum-expansion  tree.  Therefore, 
we  will  further  investigate  the  different  ways  of 
combining flat and syntactic structure knowledge 
to improve resolution performances. 

 

Flat 

Min-Exp 

Simple-Exp 

Full-Exp 

Precision  Recall 
0.406 
0.596 
0.512 
0.476 

0.406 
0.355 
0.347 
0.323 

F-score 

0.406 
0.444 
0.414 
0.385 

Table 2: Contribution from Single Knowledge Source 
The second set of experiments is conducted to 
verify the performances of various tree structures 
combined  with  flat  features.  The  performances 
are  reported  in  table  3.  Each  experiment  is  re-
ported with two performances. The upper one is 
done with default hyper-plane setting. The lower 
one is  done  using  the  hyper-plane  adjustment as 
we discussed in section 2.3. 

 

Min-Exp + 

Flat 

Simple-Exp 

+Flat 

Full-Exp + 

Flat 

Precision  Recall 
0.512 
(0.446) 
0.534 
(0.492) 
0.526 
(0.496) 

0.433 
(0.727) 
0.423 
(0.652) 
0.416 
(0.638) 

F-score 

0.469 
(0.553) 
0.472 
(0.561) 
0.465 
(0.558) 

Table 3: Comparison of Different Tree Structure +Flat 
As  table  3  shows,  minimum-expansion  gives 
highest  precision  in  both  experiment  settings. 
Minimum-expansion emphasizes syntactic struc-
tures  linking  the  anaphor  and  antecedent.  Al-
though using only the syntactic path may lose the 
contextual  information,  but  it  also  prune  out  the 
potential  noise  within  the  contextual  structures. 
In  contrast,  the  full-expansion  gives  the  highest 
recall. This is probably  due  to  the  widest  know-
ledge  coverage  provides  by  the  full-expansion 
syntactic  tree.  As  a  trade-off,  the  precision  of 
full-expansion  is  the  lowest  in  the  experiments. 
One  reason  for  this  may  be  due  to  OntoNotes 
corpus  is  from  broadcasting  news  domain.  Its 
texts  are  less-formally  structured.  Another  type 
of  noise  is  that  a  narrator  of  news  may  read  an 
abnormally  long  sentence.  It  should  appear  as 
several  separate  sentences  in  a  news  article. 
However,  in  broadcasting  news,  these  sentences 
maybe simply joined by conjunction word “and”. 
Thus  a  very  nasty  and  noisy  structure  is  created 
from  it.  Comparing  the  three  knowledge  source, 
simple-expansion  achieves  moderate  precision 
and  recall  which  results  in  the  highest  f-score. 
From this, we can draw a conclusion that simple-
expansion achieves a balance between the indica-
tive structural information and introduced noises. 
In  the  next  set  of  experiments,  we  will  com-
pare  different  setting  for  training  instances  gen-
eration.  A  typical  setting  contains  no  negative 

195

instances  generated  from  non-event  anaphoric 
pronoun. This is not an issue for object pronoun 
resolution as majority of pronouns in an article is 
anaphoric.  However  in  our  case,  the  event  pro-
noun  consists  of  only  14.7%  of  the  total  pro-
nouns  in  OntoNotes.  Thus  we  incorporate  the 
instances  from  non-event  pronouns  to  improve 
the  precision  of  the  classifier.  However,  if  we 
include all the negative instances from non-event 
anaphoric  pronouns,  the  positive  instances  will 
be  overwhelmed  by  the  negative  instances.  A 
down  sampling  is  applied  to  the  training  in-
stances  to  create  a  more  balanced  class  distribu-
tion.  Table  4  reports  various  training  settings 
using simple-expansion tree structure.  

Simple-Exp Tree 

Precision  Recall  F-score 

Without Non-
event Negative 

0.423 

0.534 

0.472 

0.526 
Incl. All Negative 
0.549 
Balanced Negative 
0.561 
Development Data 
Table 4: Comparison of Training Setup, Simple-Exp 

0.410 
0.506 
0.492 

0.733 
0.599 
0.652 

In table 4, the first line is experiment without 
any negative instances from non-event pronouns. 
The second line is the performance with all nega-
tive  instances  from  non-event  pronouns.  Third 
line is performance using a balanced training set 
using down sampling. The last line is experiment 
using  hyper-plane  adjustment.  The  first  line 
gives the highest recall measure because it has no 
discriminative knowledge on non-event anaphor-
ic  pronoun.  The  second  line  yields  the  highest 
precision  which  complies  with  our  claim  that 
including  negative  instances  from  non-event 
pronouns will improve precision of the classifier 
because  more  discriminative  power  is  given  by 
non-event pronoun instances. The balanced train-
ing  set  achieves  a  better  f-score  comparing  to 
models  with  no/all  negative  instances.  This  is 
because  balanced  training  set  provides  a  better 
weighted  positive/negative  instances  which  im-
plies  a  balanced  positive/negative  knowledge 
representation.  As  a  result  of  that,  we  achieve  a 
better  balanced  f-score.  In  (Ng  and  Cardie, 
2002b),  they  concluded  that  only  the  negative 
instances in between the anaphor and antecedent 
are  useful  in  the  resolution.  It  is  same  as  our 
strategy  without  negative  instances  from  non-
event  anaphoric  pronouns.  However,  our  study 
showed  an  improvement  by  adding  in  negative 
instances from  non-event  anaphoric pronouns as 

showed  in  table  4.  This  is  probably  due  to  our 
random  sampling  strategy  over  the  negative  in-
stances  near  to  the  event  anaphoric  instances.  It 
empowers  the  system  with  more  discriminative 
power. The best performance is given by the hy-
per-plane  adaptation  model.  Although  the  num-
ber  of  training  instances  is  further  reduced  for 
development data, we can have an adjustment of 
the hyper-plane which is more fit to dataset.  

In the last set of experiments, we will present 
the  performance  from  the  twin-candidates  based 
approach in table 5. The first line is the best per-
formance from single candidate system with hy-
per-plane  adaptation.  The  second  line  is  perfor-
mance using the twin-candidates approach. 

Precision  Recall  F-score 

Simple-Exp Tree 
Single Candidate 
Twin-Candidates 
Table 5: Single vs. Twin Candidates, Simple-Exp 

0.492 
0.540 

0.652 
0.626 

0.561 
0.579 

Comparing to the single candidate model, the 
recall  is  significantly  improved  with  a  small 
trade-off in precision. The difference in results is 
statistically significant using t-test at 5% level of 
significance.  It  reinforced  our  intuition  that  pre-
ferences between two candidates are contributive 
information sources in co-reference resolution.  

5  Conclusion and Future Work 
The  purpose  of  this  paper  is  to  conduct  a  syste-
matic study of the event pronoun resolution. We 
propose a resolution system utilizing a set of flat 
positional,  lexical  and  syntactic  feature  and 
structural  syntactic  feature.  The  state-of-arts 
convolution tree kernel is used to extract indica-
tive  structural  syntactic  knowledge.  A  twin-
candidates preference learning based approach is 
incorporated  to  reinforce  the  resolution  system 
with candidates’ preferences knowledge. Last but 
not least, we also proposed a study of the various 
incorporations  of  negative  training  instances, 
specially  using  random  sampling  to  handle  the 
imbalanced  data.  Development  data  is  also  used 
to  select  more  accurate  hyper-plane  in  SVM  for 
better determination. 

To further our research work, we plan to  em-
ploy  more  semantic  information  into  the  system 
such as semantic role labels and verb frames.  

Acknowledgment 
We  would  like  to  thank  Professor  Massimo  Poesio 
from University of Trento for the initial discussion of 
this work. 

196

References  
N. Asher. 1993. Reference to Abstract Objects in Dis-

course. Kluwer Academic Publisher. 1993. 

V.  Vapnik.  1995.  The  Nature  of  Statistical  Learning 

Theory. Springer.1995. 

M. Kubat and S. Matwin, 1997. Addressing the curse 
of  imbalanced  data  set:  One  sided  sampling.  In 
Proceedings  of  the  Fourteenth  International  Con-
ference on Machine Learning,1997. pg179–186. 
T. Joachims. 1999. Making large-scale svm learning 

practical.  In  Advances  in  Kernel  Methods  -  Sup-
port Vector Learning. MIT Press.1999. 

W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing  approach  to  coreference  resolution  of  noun 
phrases.  In  Computational  Linguistics,  Vol:27(4), 
pg521– 544. 

D.  Byron.  2002.  Resolving  Pronominal  Reference  to 
Abstract  Entities,  in  Proceedings  of  the  40th  An-
nual Meeting of the Association for Computational 
Linguistics (ACL’02). July 2002. , USA  

M.  Collins  and  N.  Duffy.  2002.  New  ranking  algo-
rithms  for  parsing  and  tagging:  Kernels  over  dis-
crete  structures,  and  the  voted  perceptron.  In  Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL’02). July 
2002. , USA 

V.  Ng  and  C.  Cardie.  2002a.  Improving  machine 
learning  approaches  to  coreference  resolution.  In 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL’02). 
July 2002. , USA. pg104–111. 

V.  Ng,  and  C.  Cardie.  2002b.  Identifying  anaphoric 
and  non-anaphoric  noun  phrases  to  improve  core-
ference  resolution.  In  Proceedings  of  the  19th  In-
ternational Conference on Computational Linguis-
tics (COLING02). (2002) 

M. Strube and C. Müller. 2003.  A Machine Learning 
Approach to Pronoun Resolution in Spoken Dialo-
gue. . In Proceedings of the 41st Annual Meeting of 
the  Association  for  Computational  Linguistics 
(ACL’03), 2003 

D.  Klein  and  C.  Manning.  2003a.  Fast  Exact  Infe-
rence with a Factored Model for Natural Language 
Parsing.  In  Advances 
in  Neural  Information 
Processing  Systems  15  (NIPS  2002),  Cambridge, 
MA: MIT Press, pp. 3-10. 

D.  Klein  and  C.Manning.  2003b.  Accurate  Unlexica-
lized  Parsing.  In  Proceedings  of  the  41st  Annual 
Meeting of the Association for Computational Lin-
guistics (ACL’03), 2003.  pg423-430. 

X.  Yang,  G.  Zhou,  J.  Su,  and  C.Tan.  2003.  Corefe-
rence Resolution Using Competition Learning Ap-
proach. In Proceedings of the 41st Annual Meeting 
of  the  Association  for  Computational  Linguistics 
(ACL’03), 2003. pg176–183. 

A.  Moschitti.  2004.  A  study  on  convolution  kernels 
for  shallow  semantic  parsing.  In  Proceedings  of 
the  42nd  Annual  Meeting  of  the  Association  for 
Computational Linguistics (ACL’04), pg335–342. 
A. Estabrooks, T. Jo, and N. Japkowicz. 2004. A mul-
tiple  resampling  method  for  learning  from  imba-
lanced  data  sets.  In  Computational  Intelligence  
Vol:20(1). pg18–36. 

X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improving 
pronoun  resolution  by  incorporating  coreferential 
information  of  candidates.  In  Proceedings  of  42th 
Annual  Meeting  of  the  Association  for  Computa-
tional Linguistics, 2004. pg127–134. 

X. Yang, J. Su and C.Tan. 2005a. Improving Pronoun 
Resolution  Using  Statistics-Based  Semantic  Com-
patibility  Information.  In  Proceedings  of  Proceed-
ings of the 43rd Annual Meeting of the Association 
for  Computational  Linguistics  (ACL’05).  June 
2005.  

X. Yang, J. Su and C.Tan. 2005b. A Twin-Candidates 
Model  for  Coreference  Resolution  with  Non-
Anaphoric  Identification  Capability.  In  Proceed-
ings of IJCNLP-2005. Pp. 719-730, 2005 

E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. 
Weischedel. 2006. OntoNotes: The 90\% Solution. 
In  Proceedings  of  the  Human  Language  Technol-
ogy Conference of the NAACL, 2006 

X.  Yang,  J.  Su  and  C.Tan.  2006.  Kernel-Based  Pro-
noun  Resolution  with  Structured  Syntactic  Know-
ledge.  In  Proceedings  of  the  44th  Annual  Meeting 
of  the  Association  for  Computational  Linguistics 
(ACL’06). July 2006. Australia. 

A. Moschitti, Making tree kernels practical for natural 
language  learning.  In  Proceedings  EACL  2006, 
Trento, Italy, 2006. 

C.  Müller.  2007.  Resolving  it,  this,  and  that  in  unre-
stricted  multi-party  dialog.  In  Proceedings  of  the 
45th  Annual  Meeting  of  the  Association  for  Com-
putational Linguistics (ACL’07). 2007.  Czech Re-
public. pg816–823. 

X. Yang, J. Su and C.Tan. 2008. A Twin-Candidates 
Model for Learning-Based Coreference Resolution. 
In  Computational  Linguistics,  Vol:34(3).  pg327-
356. 

S.  Pradhan,  L.  Ramshaw,  R.  Weischedel,  J.  Mac-
Bride, and L. Micciulla. 2007. Unrestricted Corefe-
rence:  Identifying  Entities  and  Events  in  Onto-
Notes.  In  Proceedings  of  the  IEEE  International 
Conference  on  Semantic  Computing  (ICSC),  Sep. 
2007. 

