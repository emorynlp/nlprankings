



















































Distributional Semantic Concept Models for Entity Relation Discovery


Proceedings of NAACL-HLT 2015, pages 49‚Äì55,
Denver, Colorado, May 31 ‚Äì June 5, 2015. c¬©2015 Association for Computational Linguistics

Distributional Semantic Concept Models for Entity Relation Discovery 

 
 

Jay Urbain Glenn Bushee Paul Knudson George Kowalski Brad Taylor 
Milwaukee School of  

Engineering 
CTSI, MCW 

Clinical Translational  
Science Institute 

Medical College of WI 

Clinical Translational  
Science Institute 

Medical College of 
WI 

Clinical Translational  
Science Institute 

Medical College of WI 

Clinical Translational  
Science Institute 

Medical College of WI 

1025 N. Broadway Ave. 8701 Watertown Plank  8701 Watertown 
Plank  

8701 Watertown Plank  8701 Watertown Plank  

Milwaukee, WI, USA Milwaukee, WI, USA Milwaukee, WI, USA Milwaukee, WI, USA Milwaukee, WI, USA 
urbain@msoe.edu gbushee@mcw.edu knudson@mcw.edu gkowalski@mcw.edu btaylor@mcw.edu 

 
  

 

Abstract 

We present an ad hoc concept modeling approach using 
distributional semantic models to identify fine-grained 
entities and their relations in an online search setting. Con-
cepts are generated from user-defined seed terms, distribu-
tional evidence, and a relational model over concept 
distributions. A dimensional indexing model is used for 
efficient aggregation of distributional, syntactic, and rela-
tional evidence. The proposed semi-supervised model al-
lows concepts to be defined and related at varying levels of 
granularity and scope. Qualitative evaluations on medical 
records, intelligence documents, and open domain web 
data demonstrate the efficacy of our approach. 

1 Introduction 

Knowledge discovery could be facilitated with the 
ability to define concepts ad hoc, and from these con-
cepts identify semantically related named entities and 
entity relations. In an online search setting, identifica-
tion of specific named entities may not be available, 
or may not have the granularity to support specific 
information needs. Attempting to provide models for 
all possible entity and relation types is computational-
ly intractable, so there is a need for a more flexible, 
fine-grained, user-driven approach.  

These needs are in contrast to named entities 
identified by models defined in advance from labeled 
training data, knowledge bases, or embedded in a set 
of rules. Entities identified from these models may be 
too general, e.g., person versus terrorist, or disease 
versus diabetes; or domain specific, e.g., protein type 
in a dietary versus a molecular biology sense. This 
can be an impediment to search and discovery since 

many discoveries are serendipitous in nature and are 
found by identifying linkages between more special-
ized concepts within and across domains. Using a 
flexible dimensional index for efficient aggregation 
of distributional statistics and a distributional rela-
tional model over concept distributions, we propose a 
new, more flexible approach for creating fine-
grained, user-driven concept models for identification 
of semantically related entity relations. 
 First, we present an information-seeking sce-
nario to motivate our approach. This is followed by a 
presentation of our proposed distributional semantic 
concept model and qualitative results.  
 

1.1 Ad hoc information seeking scenario 

Interactive knowledge discovery can be modeled us-
ing a dual representation of concepts and relations 
(Bollegala, et al., 2010). Concepts can be defined by the 
relations they participate in, and by their lexical and 
semantic similarity. Relations can be defined by their 
participating concepts, and by semantically similar 
relations. In the following scenario, we are interested 
in identifying relations between Alzheimer‚Äôs disease 
(AD) and other diseases. We‚Äôve heard of studies link-
ing Type 2 Diabetes Mellitus (T2DM) with AD, so we 
start with the query ‚ÄúDiabetes related to Alzheimers.‚Äù  
The system extracts candidate entity instances and 
relations from the query (Table 1). 
 

49



Query: Diabetes related to Alzheimers 
Concepts:  
     Diabetes - id: /en/diabetes_mellitus, type: 
/medicine/disease  
     Alzheimers - id: /en/alzheimers_disease, type: 
/medicine/disease 
Relations - (concept1, relation 1, 2,‚Ä¶, concept2):  
     diabetes; related to; alzheimers -> disease; related to; 
disease 

 
Table 1. Parsed query with semantically related enti-
ties. 

 
A structured representation of the query is generated 
that integrates syntactic and lexical evidence with 
distributional semantic concept models of each can-
didate entity. Sentences semantically relevant to the 
query are retrieved and rank ordered. A sample 
search result for ‚ÄúDiabetes related to Alzheimer‚Äôs‚Äù 
with extracted concepts and relations are shown in 
Table 2(a).  Table 2(b) shows a entity-relation-entity 
graph of the query and a retrieved sentence. 
 

a) Retrieved 
Sentence with 
concepts & 
relational 
dependencies 

Diabetes is a risk factor for vascular 
dementia. 
Dependency relations: (concept1; relation 
1, 2,...;concept2) 
diabetes; ; risk_factor 
risk_factor; for; vascular_dementia 

diabetes; risk_factor_for; vascu-
lar_dementia 

b) Concept-
relation 
graph: Query 
+ Sentence 

 
c) Semantic 
similarity 
graph: query: 
(vascular de-
mentia; risk 
factor; *). 

 
  
Table 2. (a) Concept-relation search result for query: Dia-
betes related to Alzheimer‚Äôs. (b) Graph of query and sen-
tence result. (c) Concept-relation graph search results for 
query: (vascular dementia; risk factor; *). 

The search result and query provide a rela-
tional lattice linking diabetes, vascular dementia, and 
Alzheimer‚Äôs with risk factors. From analyzing the 
results of the query, the user may be interested in 
identifying other concepts related to risk factors and 
vascular dementia. For example, the user may expand 
the scope of the search space by querying for any 
concept related as a risk factor to vascular dementia.  

A dimensional index is used for efficiently 
aggregating distributional statistics and relating evi-
dence of concepts and relations within the search in-
dex with information from the query. Table 2(c) 
shows the results using a force-directed graph. The 
user can now identify new concepts participating in 
some form of risk factor relation. From these results, 
other relations for one or more concepts or any com-
bination of concept relation could be explored. Table 
4 lists the ranked retrieval process. 

 
1. The user presents a natural language query. 
2. The NLP engine parses the query, extracts candidate 

entities, dependency relations, syntax, and textual con-
text. 

3. A structured query is generated from the evidence 
extracted by the NLP engine. 

4. A distributional semantic model is generated for each 
entity within the query from the dimensional index. 

5. Word and phrase search within the context of individ-
ual sentences and documents. 

6. Query model (4) applied to the top ranking sentences 
from (step 5).  

7. User can provide relevance feedback to the system. 

Iterate over search results. 

 
Table 4. Ranked retrieval process (top) and architec-
ture (bottom). 

2 Dimensional Indexing 

A dimensional indexing model (Kimball, 1996; Gray, 
et al. 1997) is used for efficient search and aggrega-
tion of distributional statistics. The model represents 
a Vector Space Model (VSM) of distributional statis-
tics for defining concepts, and a data warehousing 
style (dimensional data model) inverted index of 
words, phrases, named entities, relations, and sen-
tences. The grain of the index is the individual word 
with attributes for position, part-of-speech, and 
phrase. Semantic concepts are defined over word dis-
tributions from the index. An entity-relation-entity 
index is also created during indexing to link candidate 

50



entity instances (noun phrases) with their shortest 
path dependency relation within sentences. The same 
NLP is used for query processing, and sentence pars-
ing during indexing.  
 Importantly, the dimensional index facilitates 
efficient OLAP style SQL queries for aggregating 
distributional statistics, and for executing relational 
queries over concepts. The index also supports aggre-
gation over word, phrase, entity, relation, sentence, or 
document. A variation on this indexing approach has 
been scaled to several hundred Gigabytes for chemi-
cal patent retrieval (Urbain, et al. 2009). Indexes can 
be created from local collections and integrated with 
indexes created from online web search results. 

3 Distributional Semantic Model 

Distributional semantics quantifies and categorizes 
semantic similarities between linguistic terms based 
on their distributional properties in large samples of 
text. The central assumption is that the context sur-
rounding a given word provides important infor-
mation about its meaning (Church et al., 1989, 1991; 
Firth, 1968; Harris, 1954; Turney and Pantel, 2010). 
VSMs provide a mechanism for representing term, 
concept, relation, or sentence meaning by using dis-
tributional statistics. The semantic properties of 
words are captured in a multi-dimensional space by 
vectors that are constructed from large bodies of text 
by observing the distributional patterns of co-
occurrence with their neighboring words. These vec-
tors can then be used as measures of text similarity 
between words, phrases, abstract concepts, entities, 
relations, or snips of arbitrary text.  

We base our distributional measures of se-
mantic similarity using pointwise mutual information  
(PMI). PMI measures the pointwise mutual infor-
mation between two objects as the log ratio of the 
joint probability of two objects co-occurring relative 
to the probability of those objects occurring inde-
pendently. PMI using information retrieval (PMI-IR) 
was suggested by Turney (2001) as an unsupervised 
measure for the evaluation of the semantic similarity 
of words (Eq. 1). Turney defined words as words co-
occurring if they co-occurred within a 10-word win-
dow. 

ùëÉùëÄùêº ùë§1,ùë§2 = ùëôùëúùëî2
!(!1,!2)
! !1 !(!2)

     (1) 

Multiple evaluations have demonstrated the 
effectiveness of PMI on semantic similarity bench-
marks (Mihalecea, 2006; Eneko, 2012). We are also 
attracted to its simplicity and efficiency for generat-

ing distributional concept models online within our 
dimensional data model.  Tables 6 and 7 show the 
PMI of words for the concepts Diabetes and CHF 
(Congestive Heart Failure). The distribution of se-
mantically similar words (shown in lexically 
stemmed form) for each disease can be used to infer 
the underlying concepts Diabetes and CHF respec-
tively. 
 

Concept Stem term PMI 
diabet mellitu 4.12 
diabet depend 3.52 
diabet type 2.67 
diabet retinopathi 2.14 
diabet insulin 2.13 
diabet nephropathi 2.02 
diabet noninsulin 1.84 
diabet hyperlipidemia 1.76 
diabet esrd 1.54 
diabet adult 1.52 
diabet glaucoma 1.42 
diabet hypercholesterolemia 1.10 

 
Table 6. PMI of words for Diabetes. 

 

Concept Stem term PMI 
chf exacerb 2.34 
chf ef 1.5 
chf drainag 1.4 
chf leukocytosi 0.71 
chf lvh 0.47 
chf treat 0.34 
chf secondari 0.33 
chf etiolog 0.31 
chf cad 0.29 
chf diuresi 0.27 
chf evid 0.25 
chf pleural 0.21 
 
Table 7. PMI of words for CHF. 

 
Mihalecea, et al. (2006) extended semantic 

similarity measurements to two arbitrary text seg-
ments. Given a measurement for the semantic similar-
ity of two unordered (bag of words) text segments 
and a measurement for term specificity, the semantic 
similarity of two text segments C1 and C2 can be de-
fined using a model that combines the semantic simi-
larities of each text segment in turn with respect to 
the other text segment. We extended the original bag-
of-words text-to-text measurement to include phrases 

51



(candidate entities and their relation dependencies). 
Using PMI as the underlying measure of semantic 
similarity, we developed the following 2nd order 

PMI-based model for measuring the semantic similar-
ity between concepts C1, C2. (Eq.  2). 

 
 

 

ùëÜùëíùëöùëÜùëñùëö ùê∂1,ùê∂2 =
1
2

ùëÉùëÄùêº ùê∂1,ùë§ ‚àó ùëñùëëùëì ùë§ + ùëÉùëÄùêº ùê∂2,ùë§ ‚àó ùëñùëëùëì ùë§!‚àà(!1 ¬†‚à©!2 ¬†)
ùëñùëëùëì ùë§!‚àà(!1 ¬†‚à©!2 ¬†)

 ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†(2) 

 

 
Concept1 Concept2 Co-term ùë∑ùë¥ùë∞ ùë™1,ùíò ‚àó ùíäùíÖùíá ùíò  ùë∑ùë¥ùë∞ ùë™2,ùíò ‚àó ùíäùíÖùíá ùíò  Average  
afghanistan pakistan india 6.00 6.66 6.33 
afghanistan pakistan iran 6.10 6.04 6.07 
afghanistan pakistan china 6.15 5.94 6.05 
afghanistan pakistan franc 6.03 5.94 5.99 
afghanistan pakistan russia 5.63 6.04 5.83 
afghanistan pakistan tajikistan 5.48 6.10 5.79 
afghanistan pakistan arabia 4.93 5.88 5.41 
afghanistan pakistan soviet 5.42 5.09 5.25 
afghanistan pakistan britain 5.63 4.48 5.06 

 
Table 7. Semantic similarity (SemSim) between concepts Afghanistan and Pakistan 
 
 

ùëÖùëíùëôùê∑ùëíùëùùëÜùëñùëö ùëÖ1,ùëÖ2 =‚àù ùëÅùêºùëÖùê∑ùêπ ùë§!‚àà !1 ¬†‚à©!2 ¬† + (1‚àí‚àù) ùëÜùëíùëöùëÜùëñùëö ùëír1i, ùëír2i!!!!       (3) 

 

ùêøùëíùë•ùëÜùëñùëö ùëÜ1, ùëÜ2 =‚àù 1 1!‚àà !1 ¬†‚à©!2 ¬† +‚àù 2ùêµùëÄ25 ùëÜ1, ùëÜ2 +‚àù 3ùêµùëÄ25 ùê∑1,ùê∑2                     (4) 

Where ‚àù 1 > ¬†‚àù 2 > ¬†‚àù 3. 
 

ùê¥ùëîùëîùëÜùëñùëö ùê∂ùëÖ1,ùê∂ùëÖ2 =‚àù 1ùëÜùëíùëöùëÜùëñùëö(ùê∂1,ùê∂2)+‚àù 2ùëÖùëíùëôùê∑ùëíùëùùëÜùëñùëö ùëÖ1,ùëÖ2 +‚àù 3ùêøùëíùë•ùëÜùëñùëö ùëÜ1, ùëÜ2 +‚àù 4ùëÉùëÖùëÜùëñùëö ùëÜ1, ùëÜ2  (5) 

Where ‚àù 1 > ¬†‚àù 2 > ¬†‚àù 3 ¬† > ¬†‚àù 4. 
 

Concept instances used in Eq. 2 may be any text 
segment. PMI is calculated over the inner product 
(relational join) of all mutually co-occurring words 
between C1 and C2 is weighted by their respective 
semantic similarity (SemSim) and their normalized 
inverse document frequency (NIDF). This meas-
urement is completely unsupervised and can be 
used to compare any ordered or non-ordered text 
segment across any domain. To demonstrate the 
open domain capability of the semantic similarity 
measurement, we list the top co-occurring 
PMI*IDF measurements for Afghanistan and Paki-
stan in a post 9/11 intelligence document collec-
tion Table 7.  

For reference we provide information retrieval 
measurements for relational dependency similarity 

(Eq.3), lexical similarity (Eq. 4) using Robertson‚Äôs 
BM25 (2000), and an aggregate similarity meas-
urement integrating semantic, relational dependen-
cy, and lexical similarity (Eq. 5). 

3.1 Learning Semantic Concepts 

Figure 1 illustrates the following process for defin-
ing semantic concepts.  

 
1) Users provide seed terms to bootstrap learning 

of a semantic concept. In this case, the user de-
fines the semantic concept CAD, and seed 
terms CAD and coronary artery disease. Note: 
Seed terms may be any combination of individ-
ual words or phrases. 

 

52



Figure 1. Learning semantic concepts 
 
2) Concept terms can come from different con-

ceptual areas to meet specific information re-
trieval needs. For example, terms from finance 
and terrorism, or terms identifying medical co-
morbidities such as coronary artery disease 
and diabetes.  Additional terms can also be 
added for increased specificity. 
 

A vector-space model of a concept‚Äôs distribution is 
generated from 2nd order probabilistic likelihood of 
co-occurring terms (PMI) (Figure 2): 
 

 
 
Figure 2. Distributional concept model for CAD  

 
Qualitative review of concept terms demon-
strates the accuracy of this approach.  To 
properly evaluate the sematic model, we 
should be able to take the model and predict 
relevant named entities.  
 

3) From the concept model CAD, we can predict 
the likelihood of semantic relatedness of can-
didate entities (Figure 3).  Note: Candidate en-
tities are noun phrases identified during 
indexing or query processing. 

 

 
 
Figure 3. Named entitities predicted for concept 
CAD 

 
4) From the semantic concept model, CAD, we 

can predict the likelihood of generating sen-
tences by using this model for sentence infor-
mation retrieval (Figure 4). 

53



 
 
Table 4. Sentence retrieved from the semantic concept model, CAD

3.2 Distributional relational model 

A distributional relational model can be defined 
over semantic concept distributions. For example, 
we may be interested in searching the intersection 
of concepts Terrorist and Yemen. So we could de-
fine a relational natural join operation (‚ãà) over 
Terrorist and Yemen concept distributions to iden-
tify semantically related terms at the intersection of 
Terrorist and Yemen. From this result set we could 
predict the most semantically related entities, rela-
tions, or sentences  

We may also be interested in major cities 
in Afghanistan and Pakistan, i.e., what are the most 
prominent semantically similar attributes of major 
cities in Afghanistan Pakistan? In this case, we 
could formulate a query using relational addition 
(‚Äò+‚Äô) or UNION. Alternatively, we could use rela-
tional subtraction (‚Äò-‚Äò).  For example, what is spe-
cific to COPD (Chronic Obstructive Pulmonary 
Disorder) that is not shared by CAD (Coronary 
Artery Disease)? 

Defining relational operators for addition 
and subtraction over distributions requires some 
thought. Given matching terms in separate distri-
butions, how are distributions coalesced? Our ap-
proach for defining distributional operators are 
summarized below:  

 
‚Ä¢ Natural join (‚ãà) ‚Äì set intersection. Only main-

tain matching terms in each distribution. 

‚Ä¢ Boolean addition: set UNION. Set semantic 
similarity coefficient (SSC) to the arithmetic 
mean of matching terms. 

‚Ä¢ Boolean subtraction: set SUBTRACTION.  
Remove terms from second operand distribu-
tion from first distribution. 

‚Ä¢ Distributional addition: set UNION. Set se-
mantic similarity coefficient (SSC) to sum of 
matching terms, maximum 1. 

‚Ä¢ Distributional subtraction: set 
SUBTRACTION.  Subtract SSC of matching 
terms in second operand distribution from first 
operand distribution, minimum 0. 

 
Relational query operations are defined as a first-
order relational algebra and can be of arbitrary 
complexity. Query expressions are recursively 
parsed into a postfix expression: 

 
Expression (Given):  

((Karachi+Islamabad+Lahore)-
Pakistan)+Afghanistan 

Parse (Output):  
[ADD, Afghanistan, SUBTRACT, Pakistan, 
ADD, Lahore, ADD, Islamabad, Karachi] 

 
The postfix expression is tranlated to a series of 
SQL statements, which are executed against con-
cept distribution tables. The result set of the query 
defines a new concept that can in turn be used as 
any other distributional concept to predict entities, 
relations, or sentences. 

 
 

  

54



4 Conclusion 

We have presented an ad hoc concept modeling ap-
proach using distributional semantic models to identify 
and relate fine-grained entities in an online search set-
ting. We have also presented, a novel distributional rela-
tional model for relating semantically similar concepts. 
The distributional concept and relational models provide 
a framework for future research. For example, quantita-
tively determining the most effective concept distribu-
tion models and distributional relational operators. What 
are the best architectures for scaling ad hoc distribution-
al semantics? 

Acknowledgments 
This publication and project was supported by the 
National Center for Advancing Translational Sci-
ences, National Institutes of Health, through Grant 
Number 8UL1TR000055. Its contents are solely 
the responsibility of the authors and do not neces-
sarily represent the official views of the NIH. 
 
This material is based on past research sponsored 
by the Air Force Research Laboratory and Air 
Force Office of Science and Research Visiting 
Faculty Research and Summer Faculty Fellowship 
Programs (2010-2011) agreement number 
(13.20.02.B4488), and current research being 
sponsored by the Air Force Research Laboratory 
under agreement number (FA8750-12-1-
0031).  The U.S. Government is authorized to re-
produce and distribute reprints for Governmental 
purposes notwithstanding any copyright notation 
thereon. 

References  
 
Bollegala, D.T., Yutaka M., and Mitsuru I. (2010. Rela-

tional duality: Unsupervised extraction of semantic 
relations between entities on the web. Proceedings of 
the 19th international conference on World wide 
web. ACM. 

Church, K.W., Hanks, P. 1989. Word Association 
Norms, Mutual Information and Lexicography. Pro-
ceedings of the 27th Annual Conference of the Asso-
ciation of Computational Linguistics, 76-83.  

Church, K., Gale, W., Hanks, P., Hindle, D. 1991. Us-
ing Statistics in Lexical Analysis. In: Uri Zernik 
(ed.), Lexical Acquisition: Exploiting On-Line Re-
sources to Build a Lexicon. New Jersey: Lawrence 
Erlbaum 115-164. 

Copi, I. 1998. Introduction to Logic (1998). Prentice 
Hall College Div, 

Eneko, A., et al. 2012. Semeval-2012 task 6: A pilot on 
semantic textual similarity. Proceedings of the First 
Joint Conference on Lexical and Computational Se-
mantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings 
of the Sixth International Workshop on Semantic 
Evaluation. Association for Computational Linguis-
tics. 

Finkel, J., Grenager, T., and Manning, C. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. Proceedings of 
the 43nd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005), pp. 363-370.   

Firth, J.R. A synopsis of linguistic theory 1930-1955. 
In Studies in Linguistic Analysis, 1968. Oxford: 
Philological Society. (1957). Reprinted in F.R. Palm-
er (ed.), Selected Papers of J.R. Firth 1952-1959, 
London: Longman. 

Kimball, R. 1996. Data Warehouse Toolkit: Practical 
Techniques for Building Dimensional Data Ware-
houses. Ralph, John Wiley.  

Gray, J., et al. 1997. Data Cube: A Relational Aggrega-
tion Operator Generalizing Group-By, Cross-Tab, 
and Sub-Totals. Data Mining and Knowledge Dis-
covery, Vol. 1, Issue 1. 

Harris, Z. Distributional structure. 1954. Word 10 (23): 
146‚Äì162. 

Mihalcea, R., Corley, C., and Strapparava, C. 2006. 
Corpus-based and knowledge-based measures of text 
semantic similarity. AAAI Press. 775-780. 

S. Robertson and S. Walker. Okapi/Keenbow at TREC-
8,‚ÄùNIST Special Publication 500-246, 2000. 

Sahlgren, M.  The Distributional Hypothesis. 2008. Ri-
vista di Linguistica 20 (1): 33‚Äì53.  

Turney, P. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. 

Turney, P., and Pantel, P. 2010. From frequency to 
meaning: Vector space models of semantics. Journal 
of artificial intelligence research 37.1 141-188. 

Urbain, J., Frieder, O., and Goharian. N. 2009. assage 
relevance models for genomics search, BMC Bioin-
formatica, 10 (Suppl 3): S3. 

Urbain, J., and Frieder. O. 2010, Exploring contextual 
models in chemical patent search. Advances in Mul-
tidisciplinary Retrieval. Springer Berlin Heidelberg. 
60-69. 

 
 
 
 
 

55


