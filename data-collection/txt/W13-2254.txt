



















































MEANT at WMT 2013: A Tunable, Accurate yet Inexpensive Semantic Frame Based MT Evaluation Metric


Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422–428,
Sofia, Bulgaria, August 8-9, 2013 c©2013 Association for Computational Linguistics

MEANT at WMT 2013: A tunable, accurate yet inexpensive
semantic frame based MT evaluation metric

Chi-kiu LO and Dekai WU
HKUST

Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology

{jackielo|dekai}@cs.ust.hk

Abstract

The linguistically transparentMEANT and
UMEANT metrics are tunable, simple
yet highly effective, fully automatic ap-
proximation to the human HMEANT MT
evaluation metric which measures seman-
tic frame similarity between MT output
and reference translations. In this pa-
per, we describe HKUST’s submission
to the WMT 2013 metrics evaluation
task, MEANT and UMEANT. MEANT
is optimized by tuning a small number
of weights—one for each semantic role
label—so as to maximize correlation with
human adequacy judgment on a devel-
opment set. UMEANT is an unsuper-
vised version where weights for each se-
mantic role label are estimated via an in-
expensive unsupervised approach, as op-
posed to MEANT’s supervised method re-
lying on more expensive grid search. In
this paper, we present a battery of exper-
iments for optimizing MEANT on differ-
ent development sets to determine the set
of weights that maximize MEANT’s accu-
racy and stability. Evaluated on test sets
from the WMT 2012/2011 metrics evalua-
tion, bothMEANT and UMEANT achieve
competitive correlations with human judg-
ments using nothing more than a monolin-
gual corpus and an automatic shallow se-
mantic parser.

1 Introduction

We evaluate in the context of WMT 2013 the
MEANT (Lo et al., 2012) and UMEANT (Lo
andWu, 2012) semantic machine translation (MT)
evaluation metrics—tunable, simple yet highly ef-
fective, fully-automatic semantic frame based ob-
jective functions that score the degree of similarity

between the MT output and the reference transla-
tions via semantic role labels (SRL). Recent stud-
ies (Lo et al., 2013; Lo and Wu, 2013) show that
tuningMT systems againstMEANTmore robustly
improves translation adequacy, compared to tun-
ing against BLEU or TER.
In the past decade, the progress of machine

translation (MT) research is predominantly driven
by the fast and cheap n-gram based MT eval-
uation metrics, such as BLEU (Papineni et al.,
2002), which assume that a good translation is one
that shares the same lexical choices as the ref-
erence translation. Despite enforcing fluency, it
has been established that these metrics do not en-
force translation utility adequately and often fail to
preserve meaning closely (Callison-Burch et al.,
2006; Koehn and Monz, 2006). Unlike BLEU,
or other n-gram based MT evaluation metrics,
MEANT adopts at outset the principle that a good
translation is one from which the human readers
may successfully understand at least the central
meaning of the input sentence as captured by the
basic event structure—“who did what to whom,
when, where and why”(Pradhan et al., 2004).
Lo et al. (2012) show that MEANT correlates

better with human adequacy judgment than other
commonly used automatic MT evaluation metrics,
such as BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005), CDER (Leusch et al., 2006), WER (Nießen
et al., 2000), and TER (Snover et al., 2006). Re-
cent studies (Lo et al., 2013; Lo andWu, 2013) also
show that tuning MT system against MEANT pro-
duces more robustly adequate translations on both
formal news text genre and informal web forum
or public speech genre compared to tuning against
BLEU or TER. These studies show thatMEANT is
a tunable and highly-accurate MT evaluation met-
ric that drives MT system development towards
higher utility.
As described in Lo and Wu (2011a), the pa-

422



rameters in MEANT, i.e. the weight for each se-
mantic role label, could be estimated using simple
grid search to optimize the correlation with human
adequacy judgments. Later, Lo and Wu (2012)
described an unsupervised approach for estimat-
ing the parameters of MEANT using relative fre-
quency of each semantic role label in the reference
translations under the situation when the human
judgments for the development set are unavailable.
In this paper, we refer the version of MEANT us-
ing the unsupervised approach of weight estima-
tion as UMEANT.
In this paper, we present a battery of exper-

iments for optimizing MEANT on different de-
velopment sets to determine the set of weights
that maximizes MEANT’s accuracy and stability.
Evaluated on the test sets ofWMT 2012/2011 met-
rics evaluation, MEANT and UMEANT achieve
a competitive correlation score with human judg-
ments by nothing more than a monolingual corpus
and an automatic shallow semantic parser.

2 Related work

2.1 Lexical similarity based metrics

N-gram or edit distance based metrics such as
BLEU (Papineni et al., 2002), NIST (Dodding-
ton, 2002), METEOR (Banerjee and Lavie, 2005),
CDER (Leusch et al., 2006), WER (Nießen et
al., 2000), and TER (Snover et al., 2006) do not
correctly reflect the similarity of the basic event
structure— “who did what to whom, when, where
and why”— of the input sentence. In fact, a
number of large scale meta-evaluations (Callison-
Burch et al., 2006; Koehn and Monz, 2006) report
cases where BLEU strongly disagrees with human
judgments of translation adequacy.
Although AMBER (Chen et al., 2012) shows a

high correlation with human adequacy judgment
(Callison-Burch et al., 2012) and claims to pre-
serve the simplicity of BLEU, the modifications it
incurred on BLEU through four different n-gram
matching strategies and several different penalties
makes it very hard to interpret and indicate what
errors the MT systems are making.

2.2 Linguistic feature based metrics

ULC (Giménez and Màrquez, 2007, 2008) is
an automatic metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Giménez

and Màrquez, 2007; Callison-Burch et al., 2008;
Giménez and Màrquez, 2008) but no work has
been done towards tuning an SMT system using a
pure form of ULC perhaps due to its expensive run
time. Lambert et al. (2006) did tune on QUEEN,
a simplified version of ULC that discards the se-
mantic features of ULC and is based on pure lexi-
cal similarity. Therefore, QUEEN suffers from the
problem of failing to reflect translation adequacy
similar to other n-gram based metrics.
Similarly, SPEDE (Wang andManning, 2012) is

an integrated probabilistic FSM and probabilistic
PDA model that predicts the edit sequence needed
for the MT output to match the reference. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps; con-
tain several dozens of parameters to tune and em-
ploy expensive linguistic resources, like WordNet
and paraphrase table. Like ULC, these matrices
are not useful in the MT system development cy-
cle for tuning due to expensive running time. The
metrics themselves are also expensive in training
and tuning due to the large number of parameters
to be estimated. Although ROSE (Song and Cohn,
2011) is a weighted linear model of shallow lin-
guistic features which is cheaper in run time but it
still contains several dozens of weights that need to
be tuned which affects the portability of the metric
for evaluating translations across domains.
Rios et al. (2011) introduced TINE, an auto-

matic recall-oriented evaluationmetric which aims
to preserve the basic event structure, but no work
has been done toward tuning an SMT system
against it. TINE performs comparably to BLEU
and worse than METEOR on correlation with hu-
man adequacy judgment.

3 MEANT and UMEANT

MEANT (Lo et al., 2012), which is the weighted
f-measure over the matched semantic role labels
of the automatically aligned semantic frames and
role fillers, outperforms BLEU, NIST, METEOR,
WER, CDER and TER. Recent studies (Lo et al.,
2013; Lo andWu, 2013) also show that tuning MT
system against MEANT produces more robustly
adequate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech. Pre-

423



Figure 1: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow se-
mantic parser. There are no semantic frames for MT3 since there is no predicate.

cisely, MEANT is computed as follows:

1. Apply an automatic shallow semantic parser
on both the references and MT output. (Fig-
ure 1 shows examples of automatic shallow
semantic parses on both reference and MT
output.)

2. Applymaximumweighted bipartite matching
algorithm to align the semantic frames be-
tween the references and MT output by the
lexical similarity of the predicates.

3. For each pair of aligned semantic frames,

(a) Lexical similarity scores determine the
similarity of the semantic role fillers.

(b) Apply maximum weighted bipartite
matching algorithm to align the seman-
tic role fillers between the reference and
MT output according to their lexical
similarity.

4. Compute the weighted f-measure over the
matching role labels of these aligned predi-
cates and role fillers.

Mi,j ≡ total # ARG j of aligned frame i in MT
Ri,j ≡ total # ARG j of aligned frame i in REF

Si,pred ≡ similarity of predicate in aligned frame i
Si,j ≡ similarity of ARG j in aligned frame i

wpred ≡ weight of similarity of predicates
wj ≡ weight of similarity of ARG j

mi ≡ #tokens filled in aligned frame i of MTtotal #tokens in MT
ri ≡ #tokens filled in aligned frame i of REFtotal #tokens in REF

precision =

∑
i mi

wpredSi,pred+
∑

j wjSi,j

wpred+
∑

j wjMi,j∑
i mi

recall =

∑
i ri

wpredSi,pred+
∑

j wjSi,j

wpred+
∑

j wjRi,j∑
i ri

where mi and ri are the weights for frame, i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi,j and Ri,j are the to-
tal counts of argument of type j in frame i in the
MT and REF respectively. Si,pred and Si,j are the
lexical similarities of the predicates and role fillers
of the arguments of type j between the reference
translations and the MT output. wpred and wj are
the weights of the lexical similarities of the predi-
cates and role fillers of the arguments of typej be-
tween the reference translations and the MT out-
put. There are in total 12 weights for the set of

424



semantic role labels in MEANT as defined in Lo
and Wu (2011b).
For MEANT, wpred and wj are determined us-

ing supervised estimation via a simple grid search
to optimize the correlation with human adequacy
judgments (Lo and Wu, 2011a). For UMEANT,
wpred and wj are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the reference translations when the hu-
man judgments on adequacy of the development
set were unavailable (Lo and Wu, 2012).
In this experiment, we use a MEANT /

UMEANT implementation along the lines de-
scribed in Lo et al. (2012) and Tumuluru et al.
(2012) but we incorporate a variant of the aggre-
gation function proposed in Mihalcea et al. (2006)
for phrasal similarity of role fillers as it normal-
izes the phrase length better than geometric mean
as described in Tumuluru et al. (2012). In case
there is no semantic frame in the sentence, we treat
the whole sentence as a phrase and calculate the
phrasal similarity, like the role fillers in step 3.1,
as the MEANT score.

4 Experimental setup

We tune the 12 weights for the set of semantic role
labels in MEANT using grid search to maximize
the correlationwith human judgment on 6 develop-
ment sets. Following the protocol inWMT12 met-
rics evaluation task (Callison-Burch et al., 2012),
we use Kendall’s correlation coefficient for the
sentence-level correlation with human judgments.
The GALE development set consists of 40 sen-

tences randomly drawn from the DARPA GALE
P2.5 Chinese-English evaluation set along with the
outputs from 3 participating MT systems and the
corresponding human adequacy judgments. The
WMT12-A development set consists of 800 sen-
tences randomly drawn from the Czech-English
test set in WMT12 metrics evaluation task along
with the output from 5 participating systems and
the corresponding human judgments. Similarly,
each of theWMT12-B,WMT12-C andWMT12-D
development sets consists of 800 randomly drawn
sentences from the WMT12 metrics evaluation
test set on German-English, Spanish-English and
French-English respectively. The WMT12-E de-
velopment set consists of 800 sentences out of
which 200 sentences were randomly drawn from
each of WMT12-A, WMT12-B, WMT12-C and
WMT12-D data set.

We evaluated MEANT and UMEANT on 3
groups of test sets. The first group is the original
(without partition) test data for each language pair
(translated in English) in WMT12. This group of
test sets is used for comparing MEANT’s perfor-
mance with the reported results from other partic-
ipants of WMT12. The second group is the held
out subset of the test data for each language pair in
WMT12. The third group is the original set of test
data for each language pair in WMT11. The lat-
ter 2 groups are used for determining which set of
tuned weights maximize the accuracy and stability
of MEANT.

5 Results

Table 1 shows that the best and the worst sentence-
level correlations reported in Callison-Burch et al.
(2012) on the original WMT12 test sets (without
partitioning) for translations into English, together
the sentence-level correlation of MEANT tuned
on different development sets and UMEANT. The
grey boxes mark the results of experiments in
which there was an overlap between parts of the
development data and the test data. A study of the
values for the 12 weights associated with the se-
mantic role labels show that a general trend of the
importance of different labels in MEANT: ”who”
is always the most important; ”did”, ”what”,
”where”, ”why”, ”extent”, ”modal” and ”other”
are quite important too; ”when”, ”manner” and
”negation” fluctuate where they are quite impor-
tant in some development sets but not quite im-
portant in some development sets; ”whom” is usu-
ally not important. Given the fact that MEANT
employs significantly less expensive linguistic re-
sources and less sophisticated machine learning al-
gorithm in tuning the parameters, the performance
of MEANT is very competitive with other partici-
pants last year.
Table 2 shows the sentence-level correlation on

the WMT12 held-out test sets and the original
WMT11 test sets of MEANT tuned on different
development sets and UMEANT together with the
average sentence-level correlation on all test sets.
The results show that MEANT tuning onWMT12-
C development set achieve the highest sentence-
level correlation with human judgments on aver-
age. UMEANT, the unsupervised wight estimated
version of MEANT, achieves a very competitive
correlation score when compared with MEANT
tuned on different development sets. As a result,

425



Table 1: The best and the worst sentence-level correlation reported in Callison-Burch et al. (2012) on the
original WMT12 test sets (without partitioning) for translations into English together the sentence-level
correlation of MEANT tuned on different development sets and UMEANT. The grey box marked results
of experiments in which parts of the development data and the test data are overlapped.

WMT12 cz-en WMT12 de-en WMT12 es-en WMT12 fr-en
Best reported 0.21 0.28 0.26 0.26
MEANT (GALE) 0.13 0.16 0.15 0.15
MEANT (WMT12-A) 0.12 0.17 0.16 0.15
MEANT (WMT12-B) 0.11 0.18 0.15 0.14
MEANT (WMT12-C) 0.12 0.17 0.17 0.15
MEANT (WMT12-D) 0.12 0.17 0.16 0.16
MEANT (WMT12-E) 0.12 0.17 0.17 0.15
UMEANT 0.12 0.17 0.16 0.14
Worst reported 0.06 0.08 0.08 0.07

Table 2: Sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test sets
of MEANT tuned on different development sets and UMEANT together with the average sentence-level
correlation on all test sets.

WMT12 held-out WMT11 Average
cz-en de-en es-en fr-en cz-en de-en es-en fr-en -

MEANT (GALE) 0.0657 0.1251 0.1762 0.1719 0.3460 0.1123 0.2416 0.1913 0.1788
MEANT (WMT12-A) 0.0652 0.1117 0.1663 0.1540 0.3764 0.1101 0.2314 0.1944 0.1762
MEANT (WMT12-B) 0.0458 0.1294 0.1556 0.1548 0.3992 0.1479 0.2571 0.2037 0.1867
MEANT (WMT12-C) 0.0746 0.1278 0.1833 0.1592 0.3764 0.1324 0.2674 0.1882 0.1887
MEANT (WMT12-D) 0.0628 0.1164 0.1826 0.1655 0.3802 0.1168 0.2339 0.1975 0.1820
MEANT (WMT12-E) 0.0496 0.1353 0.1791 0.1619 0.3840 0.1101 0.2596 0.1851 0.1831
UMEANT 0.0477 0.1333 0.1606 0.1548 0.3764 0.1257 0.2828 0.1913 0.1841

we submitted two metrics to WMT 2013 metrics
evaluation task. One is MEANT with weights
learned from tuning on WMT12-C development
sets and the other submission is UMEANT.

6 Conclusion

In this paper, we have evaluated in the context of
WMT2013 the MEANT and UMEANT metrics,
which are tunable, accurate yet inexpensive fully
automatic machine translation evaluation metrics
that measure similarity between theMT output and
the reference via semantic frames. Recent stud-
ies show that tuning MT system against MEANT
produces more robustly adequate translations than
the common practice of tuning against BLEU or
TER across different data genres, such as formal
newswire text, informal web forum text and infor-
mal public speech. The weight for each seman-
tic role label in MEANT is estimated by maximiz-
ing the correlation with human adequacy judgment
on a development set. UMEANT is a version of
MEANT in which weight for each semantic role
label is estimated in an unsupervised fashion us-
ing the relative frequency of the semantic role la-
bels in the reference. We present the experimen-
tal results for determining the set of weights that

maximize MEANT’s accuracy and stability by op-
timizing MEANT on different development sets.
We disagree with the notion “a good evaluation

metric is not necessarily a good tuning metric, and
vice versa” (Chen et al., 2012). Instead, we be-
lieve that a good evaluation metric should be one
that is a good objective function to drive the devel-
opment of MT systems towards higher utility. In
other words, a good evaluation metric should cor-
relate well with human adequacy judgment and at
the same time, be inexpensive in running time so as
to fit into the MT pipeline to improve MT quality.
Our results shows that MEANT is a good evalu-
ation/tuning metric because it achieves a competi-
tive correlation scorewith human judgments by us-
ing less expensive linguistic resources and training
algorithms making it possible to tune MT system
against MEANT to improve MT quality.

7 Acknowledgment

This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-

426



ment no. 287658; and by the Hong Kong Re-
search Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.

References

Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65–
72, Ann Arbor, Michigan, June 2005.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249–256, 2006.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136–158, 2007.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70–106,
2008.

Chris Callison-Burch, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Spe-
cia. Findings of the 2012 Workshop on Statisti-
cal Machine Translation. In Proceedings of the
7th Workshop on Statistical Machine Transla-
tion (WMT 2012), pages 10–51, 2012.

Julio Castillo and Paula Estrella. Semantic Textual
Similarity for MT evaluation. In Proceedings of
the 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), pages 52–58, 2012.

Boxing Chen, Roland Kuhn, and George Foster.
Improving AMBER, an MT Evaluation Metric.
In Proceedings of the 7th Workshop on Statis-
tical Machine Translation (WMT 2012), pages
59–63, 2012.

George Doddington. Automatic evaluation of
machine translation quality using n-gram co-

occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138–145,
San Diego, California, 2002.

Jesús Giménez and Lluís Màrquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256–264, Prague, Czech Republic,
June 2007.

Jesús Giménez and LluísMàrquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195–198, Colum-
bus, Ohio, June 2008.

Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102–121, 2006.

Patrik Lambert, Jesús Giménez, Marta R Costa-
jussá, Enrique Amigó, Rafael E Banchs, Lluıs
Márquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246–249. IEEE, 2006.

Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.

Chi-kiu Lo and Dekai Wu. MEANT: An Inexpen-
sive, High-Accuracy, Semi-Automatic Metric
for Evaluating Translation Utility based on Se-
mantic Roles. In Proceedings of the Joint con-
ference of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics : Human
Language Technologies (ACL-HLT-11), 2011.

Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelli-
gence (IJCAI-11), 2011.

Chi-kiu Lo and Dekai Wu. Unsupervised vs.
supervised weight estimation for semantic MT
evaluation metrics. In Proceedings of the 6th
Workshop on Syntax and Structure in Statistical
Translation (SSST-6), 2012.

427



Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic seman-
tic metrics? In Proceedings of the 14th Machine
Translation Summit (MTSummit-XIV), 2013.

Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.

Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL-13), 2013.

Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In Pro-
ceedings of the national conference on artificial
intelligence, volume 21, page 775. Menlo Park,
CA; Cambridge, MA; London; AAAI Press;
MIT Press; 1999, 2006.

Sonja Nießen, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–
318, Philadelphia, Pennsylvania, July 2002.

Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.

Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess MT adequacy. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation (WMT-2011), pages 116–122,
2011.

Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference

of the Association for Machine Translation in
the Americas (AMTA-06), pages 223–231, Cam-
bridge, Massachusetts, August 2006.

Xingyi Song and Trevor Cohn. Regression and
Ranking based Optimisation for Sentence Level
Machine Translation Evaluation. In Proceed-
ings of the 6th Workshop on Statistical Machine
Translation (WMT 2011), pages 123–129, 2011.

Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring
the lexical similarity of semantic role fillers
for automatic semantic mt evaluation. In Pro-
ceeding of the 26th Pacific Asia Conference
on Language, Information, and Computation
(PACLIC-26), 2012.

Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic Edit Distance Metrics for
MT Evaluation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation (WMT
2012), pages 76–83, 2012.

428


