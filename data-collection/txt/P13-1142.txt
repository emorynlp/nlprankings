



















































BRAINSUP: Brainstorming Support for Creative Sentence Generation


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1446–1455,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

BRAINSUP: Brainstorming Support for Creative Sentence Generation

Gözde Özbal
FBK-irst

Trento, Italy
gozbalde@gmail.com

Daniele Pighin
Google Inc.

Zürich, Switzerland
daniele.pighin@gmail.com

Carlo Strapparava
FBK-irst

Trento, Italy
strappa@fbk.eu

Abstract

We present BRAINSUP, an extensible
framework for the generation of creative
sentences in which users are able to
force several words to appear in the sen-
tences and to control the generation pro-
cess across several semantic dimensions,
namely emotions, colors, domain related-
ness and phonetic properties. We evalu-
ate its performance on a creative sentence
generation task, showing its capability of
generating well-formed, catchy and effec-
tive sentences that have all the good qual-
ities of slogans produced by human copy-
writers.

1 Introduction
A variety of real-world scenarios involve talented
and knowledgable people in a time-consuming
process to write creative, original sentences gen-
erated according to well-defined requisites. For
instance, to advertise a new product it could be
desirable to have its name appearing in a punchy
sentence together with some keywords relevant for
marketing, e.g. “fresh”, or “thirst” for the adver-
tisement of a drink. Besides, it could be interesting
to characterize the sentence with respect to a spe-
cific color, like “blue” to convey the idea of fresh-
ness, or to a color more related to the brand of the
company, e.g. “red” for a new Ferrari. Moreover,
making the slogan evoke “joy” or “satisfaction”
could make the advertisement even more catchy
for customers. On the other hand, there are many
examples of provocative slogans in which copy-
writers try to impress their readers by suscitating
strong negative feelings, as in the case of anti-
smoke campaigns (e.g., “there are cooler ways to
die than smoking” or “cancer cures smoking”), or
the famous beer motto “Guinness is not good for

you”. As another scenario, creative sentence gen-
eration is also a useful teaching device. For ex-
ample, the keyword or linkword method used for
second language learning links the translation of
a foreign (target) word to one or more keywords
in the native language which are phonologically
or lexically similar to the target word (Sagarra and
Alba, 2006). To illustrate, for teaching the Ital-
ian word “tenda”, which means “curtain” in En-
glish, the learners are asked to imagine “rubbing
a tender part of their leg with a curtain”. These
words should co-occur in the same sentence, but
constructing such sentences by hand can be a dif-
ficult and very time-consuming process. Özbal
and Strapparava (2011), who attempted to auto-
mate the process, conclude that the inability to re-
trieve from the web a good sentence for all cases
is a major bottleneck.

Although state of the art computational mod-
els of creativity often produce remarkable results,
e.g., Manurung et al. (2008), Greene et al. (2010),
Guerini et al. (2011), Colton et al. (2012) just to
name a few, to our best knowledge there is no at-
tempt to develop an unified framework for the gen-
eration of creative sentences in which users can
control all the variables involved in the creative
process to achieve the desired effect.

In this paper, we advocate the use of syntactic
information to generate creative utterances by de-
scribing a methodology that accounts for lexical
and phonetic constraints and multiple semantic di-
mensions at the same time. We present BRAIN-
SUP, an extensible framework for creative sen-
tence generation in which users can control all the
parameters of the creative process, thus generat-
ing sentences that can be used for practical ap-
plications. First, users can define a set of key-
words which must appear in the final sentence.
Second, they can slant the output towards a spe-

1446



Domain Keywords BRAINSUP output examples

coffee waking,
cup

Between waking and doing there
is a wondrous cup.

coke drink, ex-
haustion

The physical exhaustion wants
the dark drink.

health day, juice,
sunshine

With juice and cereal the normal
day becomes a summer sunshine.

beauty kiss,lips
Passionate kiss, perfect lips. –
Lips and eyes want the kiss.

mascara drama,
lash

Lash your drama to the stage. –
A mighty drama, a biting lash.

pickle crunch, bite Crunch your bite to the top. –
Crunch of a savage byte. – A
large byte may crunch a little at-
tention.

soap
skin,
love,
touch

A touch of love is worth a fortune
of skin. – The touch of froth is
the skin of love. – A skin of water
is worth a touch of love.

Table 1: A selection of sentences automatically
generated by BRAINSUP for specific domains.

cific emotion, color or domain. At the same time,
they can require a sentence to include desired pho-
netic properties, such as rhymes, alliteration or
plosives. The combination of these features al-
lows for the generation of potentially catchy and
memorable sentences by establishing connections
between linguistic, emotional (LaBar and Cabeza,
2006), echoic and visual (Borman et al., 2005)
memory, as exemplified by the system outputs
showcased in Table 1. Other creative dimensions
can easily be plugged in, due to the inherently
modular structure of the system.

BRAINSUP supports the creative process by
greedily exploring a huge solution space to pro-
duce completely novel utterances responding to
user requisites. It exploits syntactic constraints to
dramatically cut the size of the search space, thus
making it possible to focus on the creative aspects
of sentence generation.

2 Related work

Research in creative language generation has
bloomed in recent years. In this section, we pro-
vide a necessarily succint overview of a selection
of the studies that most heavily inspired and influ-
enced the development of BRAINSUP.

Humor generators are a notable class of sys-
tems exploring new venues in computational cre-
ativity (Binsted and Ritchie, 1997; McKay, 2002;
Manurung et al., 2008). Valitutti et al. (2009)
present an interactive system which generates hu-
morous puns obtained through variation of famil-

iar expressions with word substitution. The varia-
tion takes place considering the phonetic distance
and semantic constraints such as semantic similar-
ity, semantic domain opposition and affective po-
larity difference. Possibly closer to slogan genera-
tion, Guerini et al. (2011) slant existing textual ex-
pressions to obtain more positively or negatively
valenced versions using WordNet (Miller, 1995)
semantic relations and SentiWordNet (Esuli and
Sebastiani, 2006) annotations. Stock and Strap-
parava (2006) generate acronyms based on lexical
substitution via semantic field opposition, rhyme,
rythm and semantic relations. The model is lim-
ited to the generation of noun phrases.

Poetry generation systems face similar chal-
lenges to BRAINSUP as they struggle to combine
semantic, lexical and phonetic features in a unified
framework. Greene et al. (2010) describe a model
for poetry generation in which users can control
meter and rhyme scheme. Generation is modeled
as a cascade of weighted Finite State Transduc-
ers that only accept strings conforming to the de-
sired rhyming scheme. Toivanen et al. (2012) at-
tempt to generate novel poems by replacing words
in existing poetry with morphologically compat-
ible words that are semantically related to a tar-
get domain. Content control and the inclusion of
phonetic features are left as future work and syn-
tactic information is not taken into account. The
Electronic Text Composition project1 is a corpus
based approach to poetry generation which recur-
sively combines automatically generated linguistic
constituents into grammatical sentences. Colton et
al. (2012) propose another data-driven approach to
poetry generation based on simile transformation.
The mood and theme of the poems are influenced
by daily news. Constraints about phonetic proper-
ties of the selected words or their frequencies can
be enforced during retrieval. Unlike these exam-
ples, BRAINSUP makes heavy use of syntactic in-
formation to enforce well-formed sentences and to
constraint the search for a solution, and provides
an extensible framework in which various forms
of linguistic creativity can easily be incorporated.

Several slogan generators are available on the
web2, but their capabilities are very limited as they
can only replace single words or word sequences
within existing slogan. This often results in syn-
tactically incorrect outputs. Furthermore, they do
not allow for other forms of user control.

1http://slought.org/content/11199
2E.g.: http://www.procato.com/slogan+

generator, http://www.sloganizer.net/en/,
http://www.sloganmania.com/index.htm.

1447



3 Architecture of BRAINSUP

To effectively support the creative process with
useful suggestions, we must be able to generate
sentences conforming to the user needs. First of
all, users can select the target words that need to
appear in the sentence. In the context of second
language learning, these might be the words that a
learner must associate in order to expand her vo-
cabulary. For slogan generation, the target words
could be the key features of a product, or target-
defining keywords that copywriters want to explic-
itly mention. On top of that, a user can character-
ize the generated sentences according to several
dimensions, namely: 1) a specific semantic do-
main, e.g.: “sports” or “blankets”; 2) a specific
emotion, e.g., “joy”, “anger” or just “negative”; 3)
a specific color, e.g., “red” or “blue”; 4) a com-
bination of phonetic properties of the words that
will appear in the sentence, i.e., rhymes, allitera-
tions and plosives. More formally, the user input
is a tuple: U = 〈t,d, c, e, p,w〉 , where t is the
set of target words, d is a set of words defining the
target domain, c and p are, respectively, the color
and the emotion towards which the user wants to
slant the sentence, p represents the desired pho-
netic features, and w is a set of weights that control
the influence of each dimension on the generative
process, as detailed in Section 3.3. For target and
domain words, users can explicitly select one or
more POSes to be considered, e.g., “drink/verb”
or “drink/verb,noun”.

The sentence generation process is based on
morpho-syntactic patterns which we automati-
cally discover from a corpus of dependency parsed
sentences P . These patterns represent very gen-
eral skeletons of well-formed sentences that we
employ to generate creative sentences by only
focusing on the lexical aspects of the process.
Candidate fillers for each empty position (slot)
in the patterns are chosen according to the lexi-
cal and syntactic constraints enforced by the de-
pendency relations in the patterns. These con-
straints are learned from relation-head-modifier
co-occurrence counts estimated from a depen-
dency treebank L. A beam search in the space of
all possible lexicalizations of a syntactic pattern
promotes the words with the highest likelihood of
satisfying the user specification.

Algorithm 1 provides a high-level description of
the creative sentence generation process. Here, Θ
is a set of meta-parameters that affect search com-
plexity and running time of the algorithm, such
as the minimum/maximum number of solutions to

Algorithm 1 SentenceGeneration(U,Θ,P,L): U is the
user specification, Θ is a set of meta-parameters; P and L are
two dependency treebanks.
O ← ∅
for all p ∈ CompatiblePatternsΘ(U,P) do

while NotEnoughSolutionsΘ(O) do
O ← O ∪ FillInPatternΘ(U, p,L)

return SelectBestSolutionsΘ(O)

DT NNS VBD DT JJ NN IN DT NN
The * * a * * in the *

det nsubj

dobj

det

amod

prep

pobj

det

Figure 1: Example of a syntactic pattern. A “*”
represents an empty slot to be filled with a filler.

be generated, the maximum number of patterns to
consider, or the maximum size of the generated
sentences. CompatiblePatterns(·) finds the most
frequent syntactic patterns in P that are compat-
ible with the user specification, as explained in
Section 3.1; FillInPattern(·) carries out the beam
search, and returns the best solutions generated for
each pattern p given U . The algorithm terminates
when at least a minimum number of solutions have
been generated, or when all the compatible pat-
terns have been exhausted. Finally, only the best
among the generated solutions are shown to the
user. More details about the search in the solution
space are provided in Section 3.2.

3.1 Pattern selection
We generate creative sentences starting from
morpho-syntactic patterns which have been au-
tomatically learned from a large corpus P . The
choice of the corpus from which the patterns
are extracted constitutes the first element of the
creative sentence generation process, as differ-
ent choices will generate sentences with different
styles. For example, a corpus of slogans or punch-
lines can result in short, catchy and memorable
sentences, whereas a corpus of simplified English
would be a better choice to learn a second lan-
guage or to address low reading level audiences.

A pattern is the syntactic skeleton of a class
of sentences observed in P . Within a pattern, a
second element of creativity involves the selec-
tion of original combinations of words (fillers) that
do not violate the grammaticality of the sentence.
The patterns that we employ are automatic de-
pendency trees from which all content-words have
been removed, as exemplified in Figure 1. After
selecting the target corpus, we parse all the sen-
tences with the Stanford Parser (Klein and Man-

1448



ning, 2003) and produce the patterns by stripping
away all content words from the parses. Then,
for each pattern we count how many times it has
been observed in the corpus. Additionally, we
keep track of what kind of empty slots, i.e., empty
positions, are available in each pattern. For ex-
ample, the pattern in Figure 1 can accommodate
up to two singular nouns (NN), one plural noun
(NNS), one adjective (JJ) and one verb in the past
tense (VBD). This information is needed to se-
lect the patterns which are compatible with the
target words t in the user specification U . For
example, this pattern is not compatible with t =
[heading/VBG, edge/NN] as the pattern does not
have an empty slot for a gerundive verb, while it
satisfies t = [heading/NN, edge/NN] as it can
accommodate the two singular nouns. While re-
trieving patterns, we also need to enforce that a
pattern be not completely filled just by adding the
target words t, as under these conditions there
would be no room to achieve any kind of creative
effect. Therefore, we also require that the pat-
terns retrieved by CompatiblePatterns(·) have
more empty slots than the size of t. The mini-
mum and maximum number of excess slots in the
pattern are two other meta-parameters controlled
by Θ. CompatiblePatterns(·) returns compati-
ble patterns ordered by their frequency, i.e. when
generating solutions the first patterns that are ex-
plored are the most frequently observed ones. In
this way, we achieve the following two objectives:
1) we compensate for the unavoidable errors intro-
duced by the automatic parser, as frequently ob-
served parses are less likely to be the result of
an erroneous interpretation of a sentence; and 2)
we generate sentences that are most likely to be
catchy and memorable, being based on syntactic
constructs that are used more frequently. To avoid
always selecting the same patterns for the same
kinds of inputs, we add a small random compo-
nent (also controlled by Θ) to the pattern sorting
algorithm, thus allowing for sentences to be gen-
erated also from non-top ranked patterns.

3.2 Searching the solution space

With the compatible patterns selected, we can ini-
tiate a beam search in the space of all possible
lexicalizations of the patterns, i.e., the space of
all sentences that can be generated by respect-
ing the syntactic constraints encoded by each pat-
tern. The process starts with a syntactic pattern
p containing only stop words, syntactic relations
and morphologic constraints (i.e., part-of-speech

DT NNS VBD DT JJ NN IN DT NN
The fires X a * smoke in the *

det nsubj

dobj

det

amod

prep

pobj

det

Figure 2: A partially lexicalized sentence with a
highlighted empty slot marked with X. The rele-
vant dependencies to fill in the slot are shown in
boldface.

tags) for the empty slots. The search advances to-
wards a complete solution by selecting an empty
slot to fill and trying to place candidate fillers in
the selected position. Each partially lexicalized
solution is scored by a battery of scoring func-
tions that compete to generate creative sentences
respecting the user specificationU , as explained in
Section 3.3. The most promising solutions are ex-
tended by filling another slot, until completely lex-
icalized sentences, i.e., sentences without empty
slots, are generated.

To limit the number of words that can occupy
a given position in a sentence, we define a set of
operators that return a list of candidate fillers for
a slot solely based on syntactic clues. To achieve
that, we analyze a large corpus of parsed sentences
L3 and store counts of observed head-relation-
modifier (〈h, r,m〉) dependency relations. Let
τr(h) be an operator that, when applied to a head
word h in a relation r, returns the set of words in
L which have been observed as modifiers for h in
r with a specific POS. To simplify the notation,
we assume that the relation r also carries along
the POS of the head and modifier slots. As an
example, with respect to the tree depicted in Fig-
ure 2, τamod(smoke) would return all the words
with POS equal to “JJ” that have been observed as
adjective modifiers for the singular noun “smoke”.
We will refer to τr(·) as the dependency operator
for r. For every τr(·), we also define an inverse
dependency operator τ−1r (·), which returns the list
of the possible heads in r when applied to a mod-
ifier word m. For instance, with respect to Fig-
ure 2, τ−1nsubj(fires) would return the set of verbs in
the past tense of which “fires” as a plural noun can
be a subject.

While filling in a given slot X , the dependency
operators can be combined to obtain a list of words
which are likely to occupy that position given the
syntactic constraints induced by the structure of
the pattern. Let W = ∪i{wi} be the set of words
which are directly connected to the empty slot by

3Distinct from the corpus used for pattern selection, P .

1449



a dependency relation. Each word wi implies a
constraint that candidate fillers for X must satisfy.
If wi is the head of X , then a direct operator is
used to retrieve a list of fillers that satisfy the ith

constraint. Conversely, if wi is a modifier of X ,
an inverse operator is employed.

As an example, let us consider the partially
completed sentence shown in Figure 2 having
an empty slot marked with X . Here, the word
“smoke” is a modifier for X , to which it is con-
nected by a dobj relation. Therefore, we can ex-
ploit τ−1dobj(smoke) to obtain a ranked list of words
that can occupy X according to this constraint.
Similarly, the τ−1nsubj(fires) operator can be used to
retrieve a list of verbs in the past tense that ac-
cept “fires” as nsubj modifier. Finally τ−1prep(in)
can further restrict our options to verbs that ac-
cepts complements introduced by the preposition
“in”. For example, the words “generated”, “pro-
duced”, “caused” or “formed” would be good can-
didates to fill in the slot considering all the pre-
vious constraints. More formally, we can de-
fine the set of candidate fillers for a slot X , CX ,
as: CX = τ−1rhX,X(hX) ∩ (

⋂
wi|wi∈MX τrwi,X(wi)),

where rwi,X is the type of relation between wi and
X , MX is the set of modifiers of X and hX is the
syntactic head of X .4

Concerning the order in which slots are filled,
we start from those that have the highest num-
ber of dependencies (both head or modifiers) that
have been already instantiated in the sentence, i.e.,
we start from the slots that are connected to the
highest number of non-empty slots. In doing so
we maximize the constraints that we can rely on
when inserting a new word, and eventually gener-
ate more reliable outputs.

3.3 Filler selection and solution scoring
We have devised a set of feature functions that ac-
count for different aspects of the creative sentence
generation process. By changing the weight w of
the feature functions in U , users can control the
extent to which each creativity component will af-
fect the sentence generation process, and tune the
output of the system to better match their needs.
As explained in the remainder of this section, fea-
ture functions are responsible for ranking the can-
didate slot fillers to be used during sentence gen-
eration and for selecting the best solutions to be

4An empty slot does not generate constraints for X . In
addition, there might be cases in which it is not possible to
find a filler that satisfies all the constraints at the same time.
In such cases, all the fillers that satisfy the maximum number
of constraints are considered.

Algorithm 2 RankCandidates(U, f , c1, c2, s,X): c1
and c2 are two candidate fillers for the slot X in the sentence
s = [s0, . . . sn]; f is the set of feature functions; U is the user
specification.

sc1 ← s, sc2 ← s, sc1 [X]← c1, sc2 [X]← c2
for all f ∈ SortFeatureFunctionsΘ(U, f) do

if f(sc1 , U) > f(sc2 , U) then return c1 � c2
else if f(sc1 , U) < f(sc2 , U) then

return c1 ≺ c2
return c1 ≡ c2

shown to the users.
Algorithm 2 details the process of ranking can-

didate fillers. To compare two candidates c1 and c2
for the slot X in the sentence s, we first generate
two sentences sc1 and sc2 in which the empty slot
X is occupied by c1 and c2, respectively. Then, we
sort the feature functions based on their weights
in descending order, and in turn we apply them
to score the two sentences. As soon as we find
a scorer for which one sentence is better than the
other, we can take a decision about the ranking of
the fillers. This approach makes it possible to es-
tablish a strict order of precedence among feature
functions and to select fillers that have a highest
chance of maximizing the user satisfaction.

Concerning the scoring of partial solutions and
complete sentences, we adopt a simple linear com-
bination of scoring functions. Let s be a (partial)
sentence, f = [f0, . . . , fk] be the vector of scor-
ing functions and w = [w0, . . . , wk] the associ-
ated vector of weights in U . The overall score of s
is calculated as score(s, U) =

∑k
i=0wifi(s, U) .

Solutions that do not contain all the required target
words are discarded and not shown to the user.

Currently, the model employs the following 12
feature functions:

Chromatic and emotional connota-
tion. The chromatic connotation of a
sentence s = [s0, . . . , sn] is computed as
f(s, U) =

∑
si

(sim(si, c) −
∑

cj 6=c sim(si, cj)),
where c is the user selected target color and
sim(si, cj) is the degree of association between
the word si and the color cj as calculated by
Mohammad (2011). All the words in the sentence
which have an association with the target color
c give a positive contribution, while those that
are associated with a color ci 6= c contribute
negatively. Emotional connotation works exactly
in the same way, but in this case word-emotion
associations are taken from (Mohammad and
Turney, 2010).

Domain relatedness. This feature function uses
an LSA (Deerwester et al., 1990) vector space

1450



model to measure the similarity between the words
in the sentence and the target domain d speci-
fied by the user. It is calculated as: f(s, U) =∑

di
v(di)·

∑
si

v(si)

‖∑di v(di)‖·‖
∑

si
v(si)‖ where v(·) returns the rep-

resentation of a word in the vector space.

Semantic cohesion. This feature behaves ex-
actly like domain relatedness, with the only dif-
ference that it measures the similarity between the
words in the sentence and the target words t.

Target-words scorer. This feature function
simply counts what fraction of the target
words t is present in a partial solution:
f(s, U) = (

∑
si|si∈t 1)/|t|. The target word scorer

takes care of enforcing the presence of the target
words in the sentences. Letting beam search find
the best placement for the target words comes at
no extra cost and results in a simple and elegant
model.

Phonetic features (plosives, alliteration and
rhyme). All the phonetic features are based on
the phonetic representation of English words of
the Carnegie Mellon University pronouncing dic-
tionary (Lenzo, 1998). The plosives feature is cal-
culated as the ratio between the number of plo-
sive sounds in a sentence and the overall num-
ber of phonemes. For the alliteration scorer, we
store the phonetic representation of each word in
s in a trie (i.e., prefix tree), and count how many
times each node ni of the trie (corresponding to a
phoneme) is traversed. Let ci be the value of the
counts for ni. The alliteration score is then cal-
culated as f(s, U) = (

∑
i|ci>1 ci)/

∑
i ci. More

simply put, we count how many of the phonetic
prefixes of the words in the sentence are repeated,
and then we normalize this value by the total num-
ber of phonemes in s. The rhyme feature works
exactly in the same way, with the only difference
that we invert the phonetic representation of each
word before adding it to the TRIE. Thus, we give
higher scores to sentences in which several words
share the same phonetic ending.

Variety scorer. This feature function promotes
sentences that contain as many different words as
possible. It is calculated as the number of distinct
words in the sentence over the size of the sentence.

Unusual-words scorer. To increase the ability
of the model to generate sentences containing non-
trivial word associations, we may want to prefer
solutions in which relatively uncommon words are
employed. Inversely, we may want to lower lex-

ical complexity to generate sentences more ap-
propriate for certain education or reading levels.
We define ci as the number of times each word
si ∈ s is observed in a corpus V . Accord-
ingly, the value of this feature is calculated as:
f(s, U) = (1/|s|)(∑si 1/ci).
N-gram likelihood. This is simply the likeli-
hood of a sentence estimated by an n-gram lan-
guage model, to enforce the generation of well-
formed word sequences. When a solution is not
complete, in the computation we include only the
sequences of contiguous words (i.e., not inter-
rupted by empty slots) having length greater than
or equal to the order of the n-gram model.

Dependency likelihood. This feature is re-
lated to the dependency operators introduced
in Section 3.2 and it enforces sentences in
which dependency chains are well formed. We
estimate the probability of a modifier word
m and its head h to be in the relation r
as pr(h,m) = cr(h,m)/(

∑
hi

∑
mi

cr(hi,mi)),
where cr(·) is the number of times that m
depends on h in the dependency treebank
L and hi,mi are all the head/modifier pairs
observed in L. The dependency-likelihood
of a sentence s can then be calculated as
f(s, U) = exp(

∑
〈h,m,r〉∈r(s) log pr(h,m)), r(s)

being the set of dependency relations in s.

4 Evaluation

We evaluated our model on a creative sentence
generation task. The objective of the evaluation
is twofold: we wanted to demonstrate 1) the effec-
tiveness of our approach for creative sentence gen-
eration, in general, and 2) the potential of BRAIN-
SUP to support the brainstorming process behind
slogan generation. To this end, the annotation tem-
plate included one question asking the annotators
to rate the quality of the generated sentences as
slogans.

Five experienced annotators were asked to rate
432 creative sentences according to the follow-
ing criteria, namely: 1) Catchiness: is the sen-
tence attractive, catchy or memorable? [Yes/No]
2) Humor: is the sentence witty or humorous?
[Yes/No]; 3) Relatedness: is the sentence seman-
tically related to the target domain? [Yes/No]; 4)
Correctness: is the sentence grammatically cor-
rect? [Ungrammatical/Slightly disfluent/Fluent];
5) Success: could the sentence be a good slogan
for the target domain? [As it is/With minor edit-
ing/No]. In these last two cases, the annotators

1451



were instructed to select the middle option only
in cases where the gap with a correct/successful
sentence could be filled just by performing minor
editing. The annotation form had no default val-
ues, and the annotators did not know how the eval-
uated sentences were generated, or whether they
were the outcome of one or more systems.

We started by collecting slogans from an on-
line repository of slogans5. Then, we randomly
selected a subset of these slogans and for each of
them we generated an input specification U for the
system. We used the commercial domain of the
advertised product as the target domain d. Two
or three content words appearing in each slogan
were randomly selected as the target words t. We
did so to simulate the brainstorming phase behind
the slogan generation process, where copywriters
start with a set of relevant keywords to come up
with a catchy slogan. In all cases, we set the tar-
get emotion to “positive” as we could not estab-
lish a generally valid criteria to associate a spe-
cific emotion to a product. Concerning chromatic
slanting, for target domains having a strong chro-
matic correlation we allowed the system to slant
the generated sentences accordingly. In the other
cases, a random color association was selected. In
this manner, we produced 10 tuples 〈t,d, c, e, p〉.
Then, from each tuple we produced 5 complete
user specifications by enabling or disabling differ-
ent feature function combinations6. The four com-
binations of features are: base: Target-word scorer
+ N-gram likelihood + Dependency likelihood +
Variety scorer + Unusual-words scorer + Seman-
tic cohesion; base+D: all the scorers in base +
Domain relatedness; base+D+C: all the scorers in
base+D + Chromatic connotation; base+D+E: all
the scorers in base+D + Emotional connotation;
base+D+P: all the scorers in base+D + Phonetic
features. For each of the resulting 50 input config-
urations, we generated up to 10 creative sentences.
As the system could not generate exactly 10 solu-
tions in all the cases, we ended up with a set of
432 items to annotate. The weights of the feature
functions were set heuristically, due to the lack
of an annotated dataset suitable to learn an opti-

5http://www.tvacres.com/advertising_
slogans.htm

6An alternative strategy to keep the annotation effort un-
der control would have been to generate fewer sentences from
a larger number of inputs. We adopted the former setting
since we regarded it as more similar to a brainstorming ses-
sion, where the system proposes different alternatives to in-
spire human operators. Forcing BRAINSUP to only output
one or two sentences would have limited its ability to explore
and suggest potentially valuable outputs.

MC Cat. Hum. Corr. Rel. Succ. RND2 RND3

2 - - 16.67 - 22.22 - 37.04
3 47.45 39.58 43.52 13.66 44.21 62.50 49.38
4 33.10 37.73 32.18 21.99 22.22 31.25 12.35
5 19.44 22.69 07.64 64.35 11.34 06.25 01.23

Table 2: Majority classes (%) for the five dimen-
sions of the annotation.

mal weight configuration. We started by assign-
ing the highest weight to the Target Word scorer
(i.e., 1.0), followed by the Variety and Unusual
Word scorers (0.99), the Phonetic Features, Chro-
matic/Emotional Connotation and Semantic Co-
hesion scorers (0.98) and finally the Domain, N -
gram and Dependency Likelihood scorers (0.97).
These settings allow us to enforce an order of
precedence among the scorers during slot-filling,
while giving them virtually equal relevance for so-
lution ranking.

As discussed in Section 3 we use two differ-
ent treebanks to learn the syntactic patterns (P)
and the dependency operators (L). For these ex-
periments, patterns were learned from a corpus
of 16,000 proverbs (Mihalcea and Strapparava,
2006), which offers a good selection of short sen-
tences with a good potential to be used for slo-
gan generation. This choice seemed to be a good
compromise as, to our best knowledge, there is
no published slogan dataset with an adequate size.
Besides, using existing slogans might have legal
implications that we might not be aware of. De-
pendency operators were learned by dependency
parsing the British National Corpus7. To reduce
the amount of noise introduced by the automatic
parses, we only considered sentences having less
than 20 words. Furthermore, we only considered
sentences in which all the content words are listed
in WordNet (Miller, 1995) with the observed part
of speech.8 The LSA space used for the semantic
feature functions was also learned on BNC data,
but in this case no filtering was applied.

4.1 Results

To measure the agreement among the annota-
tors, similarly to Mohammad (2011) and Ozbal
and Strapparava (2012) we calculated the majority
class for each dimension of the annotation task. A

7http://www.natcorp.ox.ac.uk/
8Since the CMU pronouncing dictionary used by the pho-

netic scorers is based on the American pronunciation of
words, we actually pre-processed the whole BNC by replac-
ing all British-English words with their American-English
counterparts. To this end, we used the mapping available at
http://wordlist.sourceforge.net/.

1452



Cat. Rel. Hum. Succ. Corr.

Yes 67.59 93.98 12.73 32.41 64.35
Partly - - - 23.15 31.71
No 32.41 06.02 87.27 44.44 03.94

Table 3: Majority decisions (%) for each annota-
tion dimension.

majority class greater than or equal to 3 means that
the absolute majority of the 5 annotators agreed
on the same decision9. Table 2 shows the ob-
served agreement for each dimension. The column
labeled RND2 (RND3) shows the random agree-
ment for a given number of annotators and a binary
(ternary) decision. For example, all five annotators
(MC=5) agreed on the annotation of the catchiness
of the slogans in 19.44% of the cases. The random
chance of agreement for 5 annotators on the binary
decision problem is 6.25%. The figures for MC ≥
4 are generally high, confirming a good agreement
among the annotators. The agreement on the relat-
edness of the slogans is especially high, with all 5
annotators taking the same decision in almost two
cases out of three, i.e., 64.35%.

Table 3 lists the distribution of answers for each
dimension in the cases where a decision can be
taken by majority vote. The generated slogans
are found to be catchy in more than 2/3 of the
cases, (i.e., 67.59%), completely successful in 1/3
of the cases (32.41%) and completely correct in
2/3 of the cases (64.35%). These figures demon-
strate that BRAINSUP is very effective in gener-
ating grammatical utterances that have all the ap-
pealing properties of a successful slogan. As for
humor, the sentences are found to have this prop-
erty in only 12.73% of cases. Even though the
figure is not very high, we should also consider
that BRAINSUP is not explicitly trying to gener-
ate amusing utterances. Concerning success, we
should point out that in 23.15% of the cases the
annotators have found that the generated slogans
have the potential to be turned into successful ones
only with minor editing. This is a very important
piece of result, as it corroborates our claim that
BRAINSUP can indeed be a valuable tool for copy-
writing, even when it does not manage to output a
perfectly good sentence. Similar conclusions can
be drawn concerning the correctness of the output,
as in almost one third of the cases the slogans are

9For the binary decisions (i.e., catchiness, relatedness and
humor), at least 3 annotators out of 5 must necessarily agree
on the same option.

only affected by minor disfluencies.
The relatedness figure is especially high, as in

almost 94% of the cases the majority of annota-
tors found the slogans to be pertinent to the tar-
get domain. This result is not surprising, as all
the slogans are generated by considering keywords
that already exist in real slogans for the same do-
main. Anyhow, this is exactly the kind of setting in
which we expect BRAINSUP to be employed, i.e.,
to support creative sentence generation starting
from a good set of relevant keywords. Nonethe-
less, it is very encouraging to observe that the gen-
eration process does not deteriorate the positive
impact of the input keywords.

We would also like to mention that in 63 cases
(14.58%) the majority of the annotators have la-
beled the slogans favorably across all 5 dimen-
sions. The examples listed in Table 1 are selected
from this set. It is interesting to observe how
the word associations established by BRAINSUP
can result in pertinent yet unintentional rhetori-
cal devices such as metaphors (“a summer sun-
shine”), puns (“lash your drama”) and personifica-
tions (“lips and eyes want”). Some examples show
the effect of the phonetic features, e.g. plosives in
“passionate kiss, perfect lips”, alliteration in “the
dark drink” and rhyming in “lips and eyes want
the kiss”. In some cases, the output of BRAINSUP
seems to be governed by mysterious philosophical
reasoning, as in the delicate examples generated
for “soap”.

For comparison, Table 4 lists a selection of
the examples that have been labeled as unsuc-
cessful by the majority of raters. In some cases,
BRAINSUP is improperly selecting attributes that
highlight undesirable properties in the target do-
main, e.g., “A pleasant tasting, a heady wine”. To
avoid similar errors, it would be necessary to rea-
son about the valence of an attribute for a spe-
cific domain. In other cases, the N -gram and the
Dependency Likelihood features may introduce
phrases which are very cohesive but unrelated to
the rest of the sentence, e.g., “Unscrupulous doc-
tors smoke armored units”. Many of these errors
could be solved by increasing the weight of the
Semantic Cohesion and Domain Relatedness scor-
ers. In other cases, such as “A sixth calorie may
taste an own good” or “A same sunshine is fewer
than a juice of day”, more sophisticated reason-
ing about syntactic and semantic relations in the
output might be necessary in order to enforce the
generation of sound and grammatical sentences.

We could not find a significant correlation be-

1453



Domain Keywords BRAINSUP output examples

pleasure wine, tast-
ing

A pleasant tasting, a heady wine.
– A fruity tasting may drink a
sparkling wine.

healthy day, juice,
sunshine

Drink juice of your sunshine, and
your weight will choose day of
you. – A same sunshine is fewer
than a juice of day.

cigarette doctors,
smoke

Unscrupulous doctors smoke ar-
mored units. – Doctors smoke no
arrow.

mascara drama,
lash

The such drama is the lash.

soap skin, love,
touch

The touch of skin is the love of
cacophony. – You love an own
skin for a first touch.

coke calorie,
taste, good

A sixth calorie may taste an own
good.

coffee waking,
cup

You cannot cup hands without
waking some fats.

Table 4: Unsuccessful BRAINSUP outputs.

tween the input variables (e.g., presence or ab-
sence of phonetic features or chromatic slanting)
and the outcome of the annotation, i.e. the sys-
tem by and large produces correct, catchy, related
and (at least potentially) successful outputs regard-
less of the specific input configurations. In this re-
spect, it should be noted that we did not carry out
any kind of optimization of the feature weights,
which might be needed to obtain more heavily
characterized sentences. Furthermore, to better
appreciate the contribution of the individual fea-
tures, comparative experiments in which the users
evaluate the system before and after triggering a
feature function might be necessary. Concern-
ing the correlation among output dimensions, we
only observed relatively high Spearman correla-
tion between correctness and relatedness (0.65),
and catchiness and success (0.68).

5 Conclusion

We have presented BRAINSUP, a novel system
for creative sentence generation that allows users
to control many aspects of the creativity process,
from the presence of specific target words in the
output, to the selection of a target domain, and
to the injection of phonetic and semantic proper-
ties in the generated sentences. BRAINSUP makes
heavy use of dependency parsed data and statistics
collected from dependency treebanks to ensure the
grammaticality of the generated sentences, and to
trim the search space while seeking the sentences
that maximize the user satisfaction.

The system has been designed as a support-
ing tool for a variety of real-world applications,
from advertisement to entertainment and educa-
tion, where at the very least it can be a valu-
able support for time-consuming and knowledge-
intensive sentence generation needs. To demon-
strate this point, we carried out an evaluation on a
creative sentence generation benchmark showing
that BRAINSUP can effectively produce catchy,
memorable and successful sentences that have the
potential to inspire the work of copywriters.

To our best knowledge, this is the first system-
atic attempt to build an extensible framework that
allows for multi-dimensional creativity while at
the same time relying on syntactic constraints to
enforce grammaticality. In this regard, our ap-
proach is dual with respect to previous work based
on lexical substitution, which suffers from limited
expressivity and creativity latitude. In addition, by
acquiring the lexicon and the sentence structure
from two distinct corpora, we can guarantee that
the sentences that we generate have never been
observed. We believe that our contribution con-
stitutes a valid starting point for other researchers
to deal with unexplored dimensions of creativity.

As future work, we plan to use machine learn-
ing techniques to estimate optimal weights for the
feature functions in different use cases. We would
also like to consider syntactic clues while reason-
ing about semantic properties of the sentence, e.g.,
color and emotion associations, instead on relying
solely on lexical semantics. Concerning the exten-
sion of the capabilities of BRAINSUP, we want to
include common-sense knowledge and reasoning
to profit from more sophisticated semantic rela-
tions and to inject humor on demand. Further tun-
ing of BRAINSUP to build a dedicated system for
slogan generation is also part of our future plans.
After these improvements, we would like to con-
duct a more focused evaluation on slogan genera-
tion involving human copywriters and domain ex-
perts in an interactive setting.

We would like to conclude this paper with a pearl
of BRAINSUP’s wisdom:

It is wiser to believe in science
than in everlasting love.

Acknowledgments

Gözde Özbal and Carlo Strapparava were partially
supported by the PerTe project (Trento RISE).

1454



References
Kim Binsted and Graeme Ritchie. 1997. Computa-

tional rules for generating punning riddles. Humor -
International Journal of Humor Research, 10(1):25–
76, January.

Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Pic-net: Pictorial representations for illustrated se-
mantic networks. In Proceedings of the AAAI Spring
Symposium on Knowledge Collection from Volun-
teer Contributors.

Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of
the 3rd International Conference on Computational
Creativity, pages 95–102.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
Of The American Society for Information Science,
41(6):391–407.

Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC’06), pages 417–422.

Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry
with applications to generation and translation. In
EMNLP, pages 524–533.

Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2011. Slanting existing text with valentino. In Pro-
ceedings of the 16th international conference on In-
telligent user interfaces, IUI ’11, pages 439–440,
New York, NY, USA. ACM.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Kevin S. LaBar and Roberto Cabeza. 2006. Cognitive
neuroscience of emotional memory. Nature reviews.
Neuroscience, 7(1):54–64, January.

Kevin Lenzo. 1998. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.

Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O’Mara, and Rolf Black. 2008.
The Construction of a Pun Generator for Language
Skills Development. Applied Artificial Intelligence,
22(9):841–869, October.

J McKay. 2002. Generation of idiom-based witticisms
to aid second language learning. In Twente Work-
shop on Language Technology 20, pages 70–74.

R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126–142, May.

George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39–41.

Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ’10, pages 26–
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Saif Mohammad. 2011. Even the abstract have color:
Consensus in word-colour associations. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 368–373, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.

Gözde Özbal and Carlo Strapparava. 2011. Autom-
atized Memory Techniques for Vocabulary Acquisi-
tion in a Second Language. In Alexander Verbraeck,
Markus Helfert, José Cordeiro, and Boris Shishkov,
editors, CSEDU, pages 79–87. SciTePress.

Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 703–711, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228–243.

Oliviero Stock and Carlo Strapparava. 2006. Laughing
with hahacronym, a computational humor system.
In proceedings of the 21st national conference on
Artificial intelligence - Volume 2, pages 1675–1678.
AAAI Press.

J. M. Toivanen, H. Toivonen, A. Valitutti, and O. Gross.
2012. Corpus-based Generation of Content and
Form in Poetry. In International Conference on
Computational Creativity, pages 175–179.

A. Valitutti, C. Strapparava, , and O. Stock. 2009.
Graphlaugh: a tool for the interactive generation of
humorous puns. In Proceedings of ACII-2009, Third
Conference on Affective Computing and Intelligent
Interaction, Demo track.

1455


