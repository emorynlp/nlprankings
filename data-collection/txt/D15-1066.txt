



















































LDTM: A Latent Document Type Model for Cumulative Citation Recommendation


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 561–566,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

LDTM: A Latent Document Type Model for Cumulative Citation
Recommendation

Jingang Wang1∗, Dandan Song1†, Zhiwei Zhang2, Lejian Liao1, Luo Si2, Chin-Yew Lin3
1School of Computer Science, Beijing Institute of Technology

2Dept. of Computer Science, Purdue University
3Knowledge Mining Group, Microsoft Research
{bitwjg, sdd, liaolj}@bit.edu.cn
{zhan1187, lsi}@purdue.edu

cyl@microsoft.com

Abstract

This paper studies Cumulative Citation
Recommendation (CCR) - given an entity
in Knowledge Bases, how to effectively
detect its potential citations from volume
text streams. Most previous approaches
treated all kinds of features indifferently to
build a global relevance model, in which
the prior knowledge embedded in docu-
ments cannot be exploited adequately. To
address this problem, we propose a la-
tent document type discriminative model
by introducing a latent layer to capture the
correlations between documents and their
underlying types. The model can better
adjust to different types of documents and
yield flexible performance when dealing
with a broad range of document types. An
extensive set of experiments has been con-
ducted on TREC-KBA-2013 dataset, and
the results demonstrate that this model can
yield a significant performance gain in rec-
ommendation quality as compared to the
state-of-the-art.

1 Introduction

Knowledge Bases (KBs), like Wikipedia, are
playing increasingly important roles in numerous
entity-based information retrieval tasks. Neverthe-
less, most KBs are hard to be up-to-date due to
their manual maintenances by human editors. As
reported in (Frank et al., 2012), there exists a me-
dian time lag of 356 days between the day a news
article is published and the time that the news is
cited in a Wikipedia article dedicated to the entity
concerned by the news. The time lag would be
reduced if relevant documents could be automati-
cally detected as soon as they are published online

∗This work was partially performed when the first author
was visiting Purdue University and Microsoft Research Asia.

†Corresponding Author

and then recommended to the editors. This task is
studied as Cumulative Citation Recommendation
(CCR). Formally, given a set of KB entities, CCR
is to filter relevant documents from a stream cor-
pus and evaluate their citation-worthiness to the
target entities.

A variety of supervised approaches (e.g., clas-
sification, learning to rank) have been employed
and achieved promising results (Wang et al., 2013;
Balog and Ramampiaro, 2013; Balog et al., 2013).
Nevertheless, most of them leverage all features
indiscriminately to build a global relevance model,
which leads to unsatisfactory performance. The
documents can offer some prior knowledge, which
is named as type in this paper. The type is the prior
knowledge embedded in the document that im-
pacts on the probability of its being recommended
to KBs. For instance, when dealing with a docu-
ment on “music” topic, we would like to have less
weights put on a politician entity because this doc-
ument is not likely to related to it, but more often
related to musicians or musical bands. Besides,
the source of a document impacts on the recom-
mendation strategies too. A document from news
agencies is more reliable and citable than the one
from social websites even if they state an identi-
cal story about the target KB entity. Hence we
consider two kinds of document features to model
the prior type knowledge: (1) topic-based features,
and (2) source-based features.

This paper proposes a latent document type dis-
criminative mixture model for CCR. We introduce
an intermediate latent layer to model latent docu-
ment types and define a joint distribution over the
document-entity pairs and latent document-types
on the observation data. The aim is to achieve a
discriminative mixture model that is expected to
outperform the global relevance model.

To the best of our knowledge, this is the first
research work that leverages prior knowledge em-
bedded in documents to improve CCR perfor-

561



mance. An extensive set of experiments conducted
on TREC-KBA-2013 dataset has demonstrated the
effectiveness of the proposed mixture model.

2 Discriminative Models for CCR

Given a set of KB entities E = {eu|u =
1, · · · ,M} and a document collection D =
{dv|v = 1, · · · , N}, our objective is to es-
timate the conditional probability of relevance
P (r|e, d) with respect to an entity-document pair
(e, d). Each (e, d) is represented as a feature
vector f(e, d) = (f1(e, d), · · · , fK(e, d)), where
K is the dimension of the entity-document fea-
ture vector. Moreover, to model the hidden
document type, each document is represented
as an document-type feature vector g(d) =
(g1(d), · · · , gL(d)), where L indicates the dimen-
sion of the document-type feature vector.

2.1 Global Model
This paper utilizes logistic regression to estimate
the conditional probability P (r|e, d), where r(r ∈
{1,−1}) is a binary label indicating the relevance
of an entity-document pair (e, d). The value of r is
1 if the document d is related to the entity e, oth-
erwise r = −1. Formally, the parametric form
of P (r=1|e, d) is expressed as P (r=1|e, d) =
δ(
∑K

i=1ωifi(e, d)), where δ(x) is the standard lo-
gistic function, ωi is the combination parame-
ter for the ith feature. It is easy to derive that
for different values of r, the only difference in
P (r|e, d) is the sign within the logistic function.
Therefore, we adopt the general representation of
P (r|e, d)=δ(r∑Ki=1 ωifi(e, d)). This model is
denoted as GM in this paper. Several previous ap-
proaches can be deemed as global models adopt-
ing different classification functions such as deci-
sion trees (Wang et al., 2013) and Support Vector
Machine (SVM) (Bonnefoy et al., 2013).

2.2 Latent Document Type Model
In GM, a fixed set of combination weights (i.e.,
ω) are learned to optimize the overall performance
for all entity-document pairs. However, the best
combination strategy for a given pair is not al-
ways the best for the others since both the docu-
ments and entities are heterogeneous. Therefore,
we may benefit from developing a document type
dependent model in which we choose the com-
bination strategy individually for each document
type to optimize the performance for specific doc-
ument types. Since it is not feasible to determine

the proper combination strategy for each docu-
ment type, we need to classify documents into one
of several types. The combination strategy is then
tuned to optimize average performance for docu-
ments within the same type.

We propose a latent document type model
(LDTM) by introducing an intermediate layer
to capture the underlying type information in
documents. A latent variable z is utilized to
indicate which type the combination weights ωz
are drawn from. The choice of z is determined
by the document d. The joint probability of
relevance r and the latent variable z is represented
as P (r, z|e, d;α, ω)=P (z|d;α)P (r|e, d, z;ω),
where P (z|d;α) is the mixing coefficient, denot-
ing the probability of choosing the hidden type
z given document d, and α is the corresponding
parameter. P (r|e, d, z;ω) denotes the discrimi-
native component which takes a logistic function.
By marginalizing out z, we obtain

P (r|e, d;α, ω) =
Nz∑
z

P (z|d;α)δ
(
r

K∑
i=1

ωzifi(e, d)
) (1)

where ωzi is the weight for the ith entry in the
feature vector under the hidden variable z. We
adopt a soft-max function 1Zd exp(

∑L
j=1 α

z
jgj(d))

to model P (z|d;α), and Zd is the normalization
factor that scaled the exponential function to be
a probability distribution. In this representation,
each document d is denoted by a bag of document
type features (g1(d), · · · , gL(d)). By plugging the
soft-max function into Equation (1), we have

P (r|e, d;α, ω)=
1
Zd

Nz∑
z=1

exp
(Lz∑
j=1

αzjgj(d)
)
δ

(
r

K∑
i=1

ωzifi(e, d)
)

(2)

Suppose entity-document pairs in training set are
represented as T ={(du, ev)}, and R={ruv} de-
notes the corresponding relevance judgment of
(du, ev), where u = 1, · · · ,M and v = 1, · · · , N .
Assume training instances in T are independently
generated, the conditional likelihood of training
data is written as

P (R|T ) =
M∏
u=1

N∏
v=1

P (ruv|eu, dv) (3)

562



2.3 Parameter Estimation
The parameters (i.e., ω and α) can be estimated
by maximizing the data log-likelihood L(ω, α),
which is the form of logarithm of Equation (3).
A typical parameter estimation method is to use
Expectation-Maximization (EM) algorithm by it-
erating E-step and M-step continuously until con-
vergence. The E-step is derived by computing the
posterior probability of z given du and ev, which
is denoted as P (z|du, ev).

P (z|du, ev) =

exp
(∑Lz

j=1αzjgj(du)
)
δ

(
ruv
∑K

i=1ωzifi(du, ev)
)

∑
zexp

(∑Lz
j=1αzjgj(du)

)
δ

(
ruv
∑K

i=1ωzifi(du, ev)
)

(4)

In M-step, we can obtain the following parame-
ter update rules.

ω∗z =

arg max
ωz

∑
uv

P (z|du, ev)log
(
δ
( K∑
i=1

ωzifi(du, ev)
))

α∗z = arg maxαz∑
u

(∑
v

P (z|du, ev)
)

log
(

1
Zdu

exp
( Lz∑
j=1

αzjgj(du)
))
(5)

To optimize Equation (5), we employ the
minFunc toolkit1 using Quasi-Newton strategy.
We adopt Akaike Information Criteria (AIC) to
determine the number of latent variables (Fang et
al., 2010), which is calculated as 2m − 2L(ω, α),
wherem is the number of parameters in the model.

LDTM holds two advantages over GM. (1) The
combination parameters vary across various docu-
ment types and hence lead to a gain of flexibility;
(2) It offers probabilistic semantics for the latent
document types and thus documents can be asso-
ciated with multiple types.

3 Features

This section presents the two types of fea-
tures used in the discriminative models. Entity-
document features (i.e., f(e, d)) are used in the
discriminative components of GM and LDTM. In

1http://www.cs.ubc.ca/˜schmidtm/
Software/minFunc.html

addition, LDTM requires document-type features
(i.e., g(e)) to learn the mixing coefficients in the
mixture component.

Since our goal is not to develop new entity-
document features, we adopt the identical entity-
document feature set proposed in our previous
work (Wang et al., 2013; Wang et al., 2015a; Wang
et al., 2015b), which has been proved effective.

In terms of document-type features, we consider
two kinds of prior knowledge embedded in doc-
uments to model the correlations between docu-
ments and their latent types.

Topic-based features One prior knowledge to
model a document’s latent type is its intrinsic top-
ics. As we have claimed, documents with one
or more obvious topics are more likely to be rec-
ommended to KB than those without any explicit
topic. We capture the underlying topics in docu-
ments with word co-occurrences. After removing
stop words, we represent each document as a fea-
ture vector with the bag-of-words model, where
word weights are determined by TF-IDF scheme.

Source-based features The source of a docu-
ment is another prior knowledge to evaluate the
probability of the document’s being recommended
to KBs. We leverage a “bag-of-sources” model to
represent each document as source-based feature
vector, and term weights are determined by binary
occurrence scheme. Please note that the sources
are organized hierarchically. For example, main-
stream news is a sub-source of news.

4 Experiments

4.1 Dataset
We utilize TREC-KBA-2013 dataset2 as our ex-
perimental dataset. The dataset is composed
of a temporally stream corpus and a target KB
entity set. The stream corpus contains nearly
1 billion documents crawled from 10 sources:
news, mainstream news, social, weblog, link-
ing, arxiv, classified, reviews, forum and meme-
tracker3. The corpus has been split with doc-
uments from October 2011 to February 2012 as
training instances and the remainder for evalua-
tion. We adopt the same training/test range setting
in our experiments. The entity set is composed of
121 Wikipedia entities and 20 Twitter entities.

2http://trec-kba.org/
kba-stream-corpus-2013.shtml

3http://www.memetracker.org/

563



Each entity-document pair is labeled as one of
the 4 relevance levels: (i) Vital, timely informa-
tion about the entity’s current state, actions, or sit-
uation. This would motivate a change to an al-
ready up-to-date KB article. (ii) Useful, possibly
citable but not timely, e.g., background biography,
secondary source information. (iii) Neural, in-
formative but not citable, e.g., tertiary source like
Wikipedia article itself. and (iv) Garbage, no in-
formation about the target entity could be learned
from the document, e.g., spam. Annotation details
of the dataset are presented in Table 1.

Range Vital Useful Neutral Garbage
Train 2011.10 ∼ 2012.02 1696 2121 1030 1702
Test 2012.03 ∼ 2013.02 5630 11579 3379 10543

Table 1: Annotation details of TREC-KBA-2013
dataset.

4.2 Evaluation Scenarios
According to different granularity settings, we
evaluate the proposed models in two scenarios:
(i) Vital Only. Only vital entity-document pairs
are treated as positive instances. (ii) Vital + Use-
ful. Both vital and useful entity-document pairs
are treated as positive instances.

4.3 Comparison Methods
We conduct extensive comparisons with the fol-
lowing methods.

• Global Model (GM). The global discrimina-
tive model introduced in section 2.1.

• Source-based Latent Document Type Model
(src LDTM). A variant of LDTM that uti-
lizes source-based features as document-type
features.

• Topic-based Latent Document Type Model
(topic LDTM). A variant of LDTM that uti-
lizes topic-based features as document-type
features.

• Combination Latent Document Type Model
(combine LDTM). This approach utilizes
source-based and topic-based features to-
gether as document-type features. In our ex-
perimental setting, we simply union the two
feature vectors together into an integral fea-
ture vector.

For reference, we also include three top-ranked ap-
proaches in TREC-KBA-2013 track.

• BIT-MSRA (Wang et al., 2013). A global
random forests classification method, the first
place approach in TREC-KBA-2013 track.

• UDEL (Liu et al., 2013). An entity-centric
query expansion approach, the second place
approach in TREC-KBA-2013 track.

• Official Baseline (Frank et al., 2013). A
strong baseline in which human annotators
go through target entities and came up with a
list of keywords for filtering vital documents.

4.4 Results and Discussion
Improving precision is harder than improving re-
call for CCR (Frank et al., 2013). Therefore,
we care more about recommendation quality of
CCR. Precision and overall accuracy are adopted
as metrics to evaluate different approaches. All
the metrics are computed in the test pool of all
entity-document pairs. The results are reported
in Table 2. In comparison to the baselines listed

Methods
Vital Only Vital + Useful
P Accu P Accu

Official Baseline .171 .175 .540 .532
BIT-MSRA .214 .445 .589 .615

UDEL .169 .259 .573 .579
GM .218 .587 .604 .565

src LDTM .273 .763 .626 .607
topic LDTM .293 .755 .643 .609

combine LDTM .299 .751 .633 .611

Table 2: Overall results of evaluated methods.
Best scores are typeset boldface.

in the 2nd block of Table 2, our mixture models
achieve higher or competitive precision and ac-
curacy in both scenarios considerably. Compared
with the official baseline, our best mixture model
improves precision about 28%. In both scenarios,
the variants of LDTM outperform GM on preci-
sion and accuracy, which validates our motivations
that (i) introducing document latent types in mix-
ture model can enhance the recommendation qual-
ity, and (ii) source-based and topic-based features
can capture the hidden type information of docu-
ments.

Moreover, topic LDTM generally performs bet-
ter than src LDTM in both scenarios, which meets
our expectation because topic-based features have
far more dimensions than source-based features.
However, even if source-based feature vector
holds a few dimensions (10 in our experiments),

564



src LDT improves the precision on the basis of
GM. Thus, the precision can be enhanced further
if we can develop more valuable features to repre-
sent the underlying document types. The combi-
nation variant of LDTM achieve the best precision
in Vital Only scenario and the best accuracy in
Vital + Useful scenario. The naı̈ve combination
strategy of two types of features can improve the
performance but not stable, so we need find better
combination strategies.

For all variant of the LDTM, the number of
latent types determined by AIC are reported in
Table 3. The optimal number of latent types in
Vital + Useful is more than that in Vital Only.
This reveals that the types of Vital documents for
entities have more restrictions than Useful docu-
ments, either by topics or by sources. In addition,
the optimal number of latent topics is more than
that of latent sources, which also follows our in-
tuition that topic-based features holding more di-
mensions than source-based features. Since we
employ a naı̈ve combination strategy for the two
types of features, the number of latent types of
combine LDTM is more close to topic LDTM,
which possesses more features than src LDTM.

Model Vital Vital + Useful
src LDTM 6 7

topic LDTM 9 15
combine LDTM 14 15

Table 3: Number of latent types determined by
AIC.

5 Related Work

There are three kinds of approaches developed
for CCR in previous work: query expansion (Liu
et al., 2013; Wang et al., 2013), classification
such as SVM (Kjersten and McNamee, 2012)
and Random Forest classifier (Bonnefoy et al.,
2013; Balog et al., 2013), and learning to rank
approaches (Wang et al., 2013; Balog and Ra-
mampiaro, 2013). Transfer learning is utilized
to transfer the keyword importance learned from
training pairs to query pairs (Zhou and Chang,
2013).

However, some highly supervised methods re-
quire training instances for each entity to build
a relevance model, limiting their scalabilities. A
compromised solution is to build a global dis-
criminative model with all features indifferently.

We spotlight document-type features and study the
impacts of them in discriminative mixture models.
Mixture model has been applied and proved effec-
tive in multiple information retrieval tasks, such
as expert search (Fang et al., 2010) and federated
search (Hong and Si, 2012). By learning flexible
combination weights for different types of training
instances, mixture model can outperform global
models with fixed weights for all instances.

6 Conclusion

Cumulative Citation Recommendation (CCR) is
an important task to automatically detect citation-
worthy documents from volume text streams for
knowledge base entities. We study CCR as a
classification problem and propose a latent docu-
ment type model (LDTM) through introducing a
latent layer in a discriminative model to capture
the correlations between documents and their in-
trinsic types. Two variants of LDTM are imple-
mented by modeling the latent types with doc-
ument source-based and topic-based features re-
spectively. Experimental results on TREC-KBA-
2013 dataset demonstrate that our mixture model
can improve CCR performance significantly, espe-
cially on precision and accuracy, revealing the ad-
vantage of LDTM in enhancing recommendation
quality of citation-worthy documents.

For future work, we wish to explore more use-
ful document-type features and apply more proper
combination strategies to improve the latent docu-
ment type model.

Acknowledgement

The authors would like to thank Fei Sun, Qifan
Wang and Chen Shao for their valuable sugges-
tions and the anonymous reviewers for their help-
ful comments. This work is funded by the Na-
tional Program on Key Basic Research Project
(973 Program, Grant No. 2013CB329600),
National Natural Science Foundation of China
(NSFC, Grant Nos. 61472040 and 60873237), and
Beijing Higher Education Young Elite Teacher
Project (Grant No. YETP1198).

References

Krisztian Balog and Heri Ramampiaro. 2013. Cu-
mulative citation recommendation: classification vs.
ranking. In SIGIR, pages 941–944. ACM.

565



Krisztian Balog, Heri Ramampiaro, Naimdjon Takhi-
rov, and Kjetil Nørvåg. 2013. Multi-step classifica-
tion approaches to cumulative citation recommenda-
tion. In OAIR, pages 121–128.

Ludovic Bonnefoy, Vincent Bouvier, and Patrice Bel-
lot. 2013. A weakly-supervised detection of entity
central documents in a stream. In SIGIR, pages 769–
772.

Yi Fang, Luo Si, and Aditya P. Mathur. 2010. Discrim-
inative models of integrating document evidence and
document-candidate associations for expert search.
In SIGIR, pages 683–690. ACM.

J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu,
C. Zhang, C. Re, and I. Soboroff. 2012. Building an
Entity-Centric Stream Filtering Test Collection for
TREC 2012. In TREC.

John Frank, Steve J. Bauer, Max Kleiman-Weiner,
Daniel A. Roberts, Nilesh Triouraneni, Ce Zhang,
and Christopher Rè. 2013. Evaluating stream filter-
ing for entity profile updates for trec 2013. In TREC.

Dzung Hong and Luo Si. 2012. Mixture model with
multiple centralized retrieval algorithms for result
merging in federated search. In SIGIR, pages 821–
830. ACM.

Brain Kjersten and Paul McNamee. 2012. The hltcoe
approach to the trec 2012 kba track. In TREC.

Xitong Liu, Jeffrey Darko, and Hui Fang. 2013. A re-
lated entity based approach for knowledge base ac-
celeration. In TREC.

Jingang Wang, Dandan Song, Chin-Yew Lin, and
Lejian Liao. 2013. Bit and msra at trec kba ccr
track 2013. TREC.

Jingang Wang, Lejian Liao, Dandan Song, Lerong Ma,
Chin-Yew Lin, and Yong Rui. 2015a. Resort-
ing relevance evidences to cumulative citation rec-
ommendation for knowledge base acceleration. In
Web-Age Information Management, volume 9098 of
Lecture Notes in Computer Science, pages 169–180.
Springer International Publishing.

Jingang Wang, Dandan Song, Qifan Wang, Zhiwei
Zhang, Luo Si, Lejian Liao, and Chin-Yew Lin.
2015b. An entity class-depedent discriminative mix-
ture model for cumulative citation recommendation.
In SIGIR. ACM.

Mianwei Zhou and Kevin Chen-Chuan Chang. 2013.
Entity-centric document filtering: boosting feature
mapping through meta-features. In CIKM, pages
119–128. ACM.

566


