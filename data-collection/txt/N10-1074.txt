










































Learning Words and Their Meanings from Unsegmented Child-directed Speech


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 501–509,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Learning Words and Their Meanings from Unsegmented Child-directed
Speech

Bevan K. Jones & Mark Johnson
Dept of Cognitive and Linguistic Sciences

Brown University
Providence, RI 02912, USA

{Bevan Jones,Mark Johnson}@Brown.edu

Michael C. Frank
Dept of Brain and Cognitive Science

Massachusetts Institute of Technology
Cambridge, MA 02139, USA

mcfrank@mit.edu

Abstract

Most work on language acquisition treats
word segmentation—the identification of lin-
guistic segments from continuous speech—
and word learning—the mapping of those seg-
ments to meanings—as separate problems.
These two abilities develop in parallel, how-
ever, raising the question of whether they
might interact. To explore the question, we
present a new Bayesian segmentation model
that incorporates aspects of word learning and
compare it to a model that ignores word mean-
ings. The model that learns word meanings
proposes more adult-like segmentations for
the meaning-bearing words. This result sug-
gests that the non-linguistic context may sup-
ply important information for learning word
segmentations as well as word meanings.

1 Introduction

Acquiring a language entails mastering many learn-
ing tasks simultaneously, including identifying
where words begin and end in continuous speech
and learning meanings for those words. It is com-
mon to treat these tasks as separate, sequential pro-
cesses, where segmentation is a prerequisite to word
learning but otherwise there are few if any depen-
dencies. The earliest evidence of segmentation,
however, is for words bordering a child’s own name
(Bortfeld et al., 2005). In addition, infants begin
learning their first words before they achieve adult-
level competence in segmentation. These two pieces
of evidence raise the question of whether the tasks of
meaning learning and segmentation might mutually
inform one another.

To explore this question we present a joint model
that simultaneously identifies word boundaries and
attempts to associate meanings with words. In do-
ing so we make two contributions. First, by model-
ing the two levels of structure in parallel we simu-
late a more realistic situation. Second, a joint model
allows us to explore possible synergies and interac-
tions. We find evidence that our joint model per-
forms better on a segmentation task than an alterna-
tive model that does not learn word meanings.

The picture in Figure 1 depicts a language learn-
ing situation from our corpus (originally from Fer-
nald and Morikawa, 1993; recoded in Frank et al.,
2009) where a mother talks while playing with var-
ious toys. Setting down the dog and picking up the
hand puppet of a pig, she asks, “Is that the pig?”
Starting out, a young learner not only does not know
that the word “pig” refers to the puppet but does not
even know that “pig” is a word at all. Our model
simulates the learning task, taking as input the un-
segmented phonemic representation of the speech
along with the set of objects in the non-linguistic
context as shown in Figure 1 (a), and infers both a
segmentation and a word-object mapping as in Fig-
ure 1 (b).

One can formulate the word learning task as
that of finding a reasonably small set of reusable
word-meaning pairs consistent with the underlying
communicative intent. Infant directed speech often
refers to objects in the immediate environment, and
early word learning seems to involve associating fre-
quently co-occurring word-object pairs (Akhtar and
Montague, 1999; Markman, 1990). Several compu-
tational models are based on this idea that a word

501



Figure 1: (a) The input to our system for the utterance
“Is that the pig?” consists of an unsegmented sequence
of phonemes and the set of objects representing the non-
linguistic context. These objects were manually iden-
tified by inspecting the associated video, a frame from
which is shown above. (b) The gold-standard segmenta-
tion and word-object assignments of the same utterance,
against which the output of our system is evaluated (all
words except “pIg” are mapped to a special “null” object,
as explained in the text).

that frequently occurs in the presence of an object
and not so frequently in its absence is likely to re-
fer to that object (Frank et al., 2009a; Siskind, 1996;
Yu and Ballard, 2007). Importantly, all these models
assume words are pre-segmented in the input.

While the word segmentation task relates less
clearly to the communicative content, it can be for-
mulated according to a similar objective, that of at-
tempting to explain the sound sequences in the input
in terms of some reasonably small set of reusable
units, or words. Computational models have suc-
cessfully addressed the problem in much this way
(Johnson and Goldwater, 2009; Goldwater et al.,
2009; Brent, 1999), and the general approach is con-
sistent with experimental observations that humans
are sensitive to statistics of sound sequences (Saffran
et al., 1996; Frank et al., 2007).

The two tasks can be integrated in a relatively
seamless way, since, as we have just formulated
them, they have a common objective, that of finding
a minimal, consistent set of reusable units. However,
the two deal with different types of information with

different dependencies. The basic idea is that learn-
ing a vocabulary that both meets the constraints of
the word-learning task and is consistent with the ob-
jective of the segmentation task can yield a better
segmentation. That is, we hope to find a synergy in
the joint inference of meaning and segmentation.

Note that to the best of our knowledge there is
very little computational work that combines word
form and word meaning learning (Frank et al. 2006
takes a first step but their model is applicable only
to small artificial languages). Frank et al. (2009a)
and Regier (2003) review pure word learning mod-
els and, in addition to the papers we have already
cited, Brent (1999) presents a fairly comprehensive
review of previous pure segmentation models. How-
ever, none of the models reviewed make any attempt
to jointly address the two problems. Similarly, in the
behavioral literature on development, we are aware
of only one segmentation study (Graf-Estes et al.,
2007) that involves non-linguistic context, though
this study treats the two tasks sequentially rather
than jointly.

We now describe our model and inference proce-
dure and follow with evaluation and discussion.

2 Model Definition

Cross-situational meaning learning in our joint word
learning and segmenting model is inspired by the
model of Frank et al. (2009a). Our model can
be viewed as a variant of the Latent Dirichlet Al-
location (LDA) topic model of Blei et al. (2003),
where topics are drawn from the objects in the non-
linguistic context. The model associates each utter-
ance with a single referent object, the topic, and ev-
ery word in the utterance is either generated from a
distribution over words associated with that object
or else from a distribution associated with a special
“null” object shared by all utterances. Note that in
this paper we use “topic” to denote the referent ob-
ject of an utterance, otherwise we depart from topic
modeling convention and use the term “object” in-
stead.

Segmentation is based on the unigram model pro-
posed by Brent (1999) and reformulated by Goldwa-
ter et al. (2009) in terms of a Dirichlet process. Since
both LDA and the unigram segmenter are based on
unigram distributions it is relatively straightforward

502



Figure 2: Topical Unigram Model: Oj is the set of objects
in the non-linguistic context of the jth utterance, zj is the
utterance topic, wji is the ith word of the utterance, xji is
the category of the word (referring or non-referring), and
the other variables are distribution parameters.

to integrate the two to simultaneously infer word
boundaries and word-object associations.

Figure 2 illustrates a slightly simplified form of
the model, and the the relevant distributions are as
follows:

z|O ∼ Uniform(O)

Gz|z, α0, α1, P0 ∼

{

DP(α1, P0) if z 6= 0

DP(α0, P0) otherwise

π ∼ Beta(1, 1)

x|π ∼ Bernoulli(π)

w|G, z, x ∼

{

Gz if x = 1

G0 if x = 0

Note that Uniform(O) denotes a discrete uniform
distribution over the elements of the set O. P0 is
described later.

Briefly, each utterance has a single topic zj , drawn
from the objects in the non-linguistic context Oj ,
and then for each word wji we first flip a coin xji
to determine if it refers to the topic or not. Then, de-
pending on xji the word is either drawn from a dis-
tribution specific to the topic (xji = 1) or from a dis-
tribution associated with the “null” object (xji = 0).
In slightly greater detail but still glossing over the

details of how the multinomial parameters are gen-
erated, the generative story proceeds as follows:

1. For each utterance, indexed by j

2. (a) Pick a single topic zj uniformly from the set
of objects in the environment Oj

(b) For each word wji of the utterance

(c) i. Determine if it refers to zj or not by set-
ting xji to 1 (referring) with probability π,
and to 0 (non-referring) otherwise.

ii. if xji is 1, draw wji from the topic specific
distribution over words Gzj .

iii. otherwise, draw wji from G0, the distribu-
tion over words associated with the “null”
object.

This generative story is a simplification since it
does not describe how we model utterance bound-
aries. It is important for segmentation purposes
to explicitly model utterance boundaries since, un-
like utterance-internal word boundaries, we as-
sume utterance boundaries are observed. Thus,
the story is complicated by the fact that there is
a chance each time we generate a word that we
also generate an utterance boundary. The choice of
whether to terminate the utterance or not is captured
by a Bernoulli(γ) random variable $ji indicating
whether the ith word was the last word of the jth

utterance.

γ ∼ Beta(1, 1)

$|γ ∼ Bernoulli(γ)

The Gz multinomial parameters are generated
from a Dirichlet process with base distribution over
words, P0, which describes how new word types
are generated from their constituent phonemes.
Phonemes are generated sequentially, i.i.d. uni-
formly from m phonemic types. In addition, there
is a probability p# of generating a word boundary.

P0(w) = (1− p#)
|w|−1p#

1

m|w|

The concentration parameters α0 and α1 also play
a critical role in the generation of words and word
types. Any given word has a certain probability
of either being produced from the set of previously
seen word types, or from an entirely new one. The

503



greater the concentration parameter, the more likely
the model is to appeal to the base distribution P0 to
introduce a new word type.

Like Frank et al. (2009a), we distinguish between
two coarse grammatical categories, referring and
non-referring. Referring words are generated by the
topic, while non-referring words are drawn from G0,
a distribution associated with the “null” object. The
distinction ensures sparse word-object maps that
obey the principle of mutual exclusion. Otherwise
all words in the utterance would be associated with
the topic object, resulting in a very large set of words
for each object that is very likely to overlap with the
words for other objects. As a further bias toward
a small lexicon, we employ different concentration
parameters (α0 and α1) for the non-referring and re-
ferring words, using a much smaller value for the
referring words. Intuitively, there should be a rela-
tively small prior probability of introducing a new
word-object pair, corresponding to a small α1 value.
On the other hand, most other words don’t refer to
the topic object (or any other object for that matter),
corresponding to a much larger α0 value.

Note that this topical unigram model is a straight-
forward generalization of the unigram segmentation
model (Goldwater et al., 2009) to the case of multi-
ple topics. In fact, if all words were assumed to refer
to the same object (or to no object at all) the models
would be identical.

Unlike LDA, each “document” has only one topic,
which is necessitated by the fact that in our model
documents correspond single utterances. The ut-
terances in our corpus of child directed speech are
often only four or five words long, whereas the
general LDA model assumes documents are much
larger. Thus, there may not be enough words to in-
fer a useful utterance specific distribution over top-
ics. Consequently, rather than inferring a separate
topic distribution for each utterance, we simply as-
sume a uniform distribution over objects in the non-
linguistic context. In effect, we rely entirely on the
non-linguistic context and word-object associations
to infer topics. Though necessitated by data sparsity
issues, we also note that it is very rare in our cor-
pus for utterances to refer to more than one object in
the non-linguistic context, so the choice of a single
topic may also be a more accurate model. In fact,
even with multi-sentence documents, LDA may per-

form better if only one topic is assumed per sentence
(Gruber et al., 2007).

3 Inference

We use a collapsed Gibbs sampling procedure, in-
tegrating over all possible Gz , π, and γ values and
then iteratively sample values for each variable con-
ditioned on the current state of all other variables.
We visit each utterance once per iteration, sample a
topic, and then visit each possible word boundary lo-
cation to sample the boundary and word categories
simultaneously according to their joint probability.

A single topic is sampled for each utterance, con-
ditioned on the words and their current determina-
tions as referring or non-referring. Since zj is drawn
from a uniform distribution, this probability is sim-
ply proportionate to the conditional probability of
the words given zj and the xji variables.

P (zj |wj,xj,h
−j) ∝

Γ(
∑Wj

w n
(h−)
w,zj + α1P0(w))

Γ(
∑Wj

w n
(h)
w,zj + α1P0(w))

·

Wj
∏

w

Γ(n
(h)
w,zj + α1P0(w))

Γ(n
(h−)
w,zj + α1P0(w))

Here, P (zj |wj,xj,h−j) is the probability of topic
zj given the current hypothesis h for all variables ex-

cluding those for the current utterance. Also, n(h
−j)

w,zj

is the count of occurrences of word type w that refer
to topic zj among the current variable assignments,
and Wj is the set of word types appearing in utter-
ance j. The vectors of word and category variables
in utterance j are represented as wj and xj, respec-
tively. Note that only referring words have any bear-
ing on the appropriate selection of zj and so all fac-
tors involving only non-referring words are absorbed
by the constant of proportionality.

The word categories can be sampled conditioned
on the current word boundary states according to the

following conditional probability, where n(h
−ji)

xji is
the number of words categorized according to label

504



xji over the entire corpus excluding word wji.

P (xji|wji, zj ,h
−ji) ∝ P (wji|zj , xji,h

−ji)

·P (xji|h
−ji)

=
n

(h−ji)
wji,xjizj + αxjiP0(wji)

n
(h−ji)
•,xjizj + αxji

·
n

(h−ji)
xji + 1

n
(h−ji)
• + 2

(1)

In practice, however, we actually sample the word
category variables jointly with the boundary states,
using a scheme similar to that outlined in Gold-
water et al. (2009). We visit each possible word
boundary location (any point between two consec-
utive phonemes) and compute probabilities for the
hypotheses for which the phonemic environment
makes up either one word or two. As illustrated be-
low there are two sets of cases: those where we treat
the segment as a single word, and those where we
treat it as two words.

x1 x2 x3
. . .#w1#. . . vs. . . .#w2#w3#. . .

↑ ↑

The probabilities of the hypotheses can be derived
by application of equation 1. Since the x variables
can each describe two possible events, there are a to-
tal of six different cases to consider for each bound-
ary assignment: two cases without and four with a
word boundary.

The probability of each of the two cases without
a word boundary can be computed as follows:

P (w1, x1|z,h
−) =

n
(h−)
w1,x1z + αx1P0(w1)

n
(h−)
•,x1z + αx1

·
n

(h−)
x1 + 1

n
(h−)
• + 2

·
n

(h−)
$1

+ 1

n
(h−)
• + 2

Here h− signifies the current hypothesis for all
variables excluding those for the current segment

and n(h
−)

$1
is the count for h− of either utterance fi-

nal words if w1 is utterance final or non-utterance
final words if w1 is also not utterance final.

In the four cases with a word boundary, we have
two words and two categories to sample.

P (w2, x2, w3, x3|z,h
−) =

n
(h−)
w2,x2z + αx2P0(w2)

n
(h−)
•,x2z + αx2

·
n

(h−)
x2 + 1

n
(h−)
• + 2

·
n

(h−)
$2=0

+ 1

n
(h−)
• + 2

·
n

(h−)
w3,x3z + δx2(x3)δw2(w3) + αx3P0(w3)

n
(h−)
•,x3z + δx2(x3) + αx3

·
n

(h−)
x3 + δx2(x3) + 1

n
(h−)
• + 3

·
n

(h−)
$3

+ δ$2($3) + 1

n
(h−)
• + 3

Here δx(y) is 1 if x = y and 0 otherwise.

4 Results & Model Comparisons

4.1 Corpus

Our training corpus (Fernald and Morikawa, 1993;
Frank et al., 2009b) consists of about 22,000 words
and 5,600 utterances. Video recordings consisting
of mother-child play over pairs of toys were ortho-
graphically transcribed, and each utterance was an-
notated with the set of objects present in the non-
linguistic context. The object referred to by the ut-
terance, if any, was noted, as described in Frank et al.
(2009b). We used the VoxForge dictionary to map
orthographic words to phoneme sequences in a pro-
cess similar to that described in Brent (1999).

Figure 1 (a) presents an example of the coding
of phonemic transcription and non-linguistic context
for a single utterance. The input to the system con-
sists solely of the phonemic transcription and the ob-
jects in the non-linguistic context.

4.2 Evaluation

We ran the sampler ten times for 100,000 iterations
with parameter settings of α1 = 0.01, α0 = 20, and
p# = 0.5, keeping only the final sample for evalu-
ation. We defined the word-object pairs for a sam-
ple as the words in the referring category that were
paired at least once with a particular topic. These
pairs were then compared against a gold standard
set of word-object pairs, while segmentation perfor-
mance was evaluated by comparing the final bound-
ary assignments against the gold standard segmenta-
tion.

505



4.2.1 Word Learning

To explore the contribution of word boundaries
to the joint word learning and segmenting task, we
compare our full joint model against a variant that
only infers topics, using the gold standard segmen-
tation as input. In this way we also reproduce the
usual assumption of a sequential relationship be-
tween segmentation and word learning and test the
necessity of the simplifying assumption. The re-
sults are shown in Table 2. We compare them with
three different metric types: topic accuracy; preci-
sion, recall, and F-score of the word-object pairs;
and Kullback-Liebler (KL) divergence.

First, treating utterances with no referring words
as though they have no topic, we compute the ac-
curacy of the inferred topics. Note that we don’t
report accuracy for the the variant with no non-
linguistic context, since in this case the objects are
interchangeable, and we have a problem identifying
which cluster corresponds to which topic. Table 2
shows that the joint segmentation and word learning
model gets the topic right for 81% of the utterances.
The variant that assumes pre-segmented input does
comparably well with an accuracy of 79%. Surpris-
ingly, it seems that knowing the gold segmentation
doesn’t add very much, at least for the topic infer-
ence task.

To evaluate how well we discovered the word-
object map, we manually compiled a list of all the
nouns in the corpus that named one of the 30 ob-
jects. We used this set of nouns, cross-referenced
with their topic objects, as a gold standard set of
word-object pairs. By counting the co-occurrences,
we also compute a gold standard probability distri-
bution for the words given the topic, P (w|z, x = 1).

Precision, recall and F-score are computed as per
Frank et al. (2009a). In particular, precision is the
fraction of gold pairs among the sampled set and re-
call is the fraction of sampled pairs among the gold
standard pairs.

p =
|Sampled ∩ Gold|

|Sampled|
, r =

|Sampled ∩ Gold|
|Gold|

KL divergence is a way of measuring the differ-
ence between distributions. Small numbers gener-
ally indicate a close match and is zero only when
the two are equal. Using the empirical distribution

Object Words
BOX thebox box
BRUSH brush
BUNNY rabbit Rosie
BUS bus
CAR car thecar
CHEESE cheese
DOG thedoggy doggy
DOLL doll thedoll yeah benice
DOUGH dough
ERNIE Ernie

Table 1: Subset of an inferred word-object mapping. For
clarity, the proposed words have been converted to stan-
dard English orthography.

p r f KL acc
Joint 0.21 0.45 0.28 2.78 0.81
Gold Seg 0.21 0.60 0.31 1.82 0.79

Table 2: Word Learning Performance. Comparing
precision, recall, and F-score of word-object pairs,
DKL(P (w, z)||Q(w, z)), and accuracy of utterance top-
ics for the full joint model and a variant that only infers
meanings given a gold standard segmentation.

over gold topics P (z), we use the standard formula
for KL divergence to compare the gold standard dis-
tribution P against the inferred distribution Q. I.e.,
we compute DKL(P (w, z)||Q(w, z)).

The model learns fairly meaningful word-object
associations; results are shown in Table 2. As in the
case of topic accuracy, the joint and word learning
only variants perform similarly, this time with some-
what better performance for the easier task with an
F-score and KL divergence of 0.31 and 1.82 vs. 0.28
and 2.78 for the joint task.

Table 1 illustrates the sort of word-object pairs
the model discovers. As can be seen, many of the
errors are due to the segmentation, usually under-
segmentation errors where it segments two words as
one. This is a general problem with the unigram seg-
menter on which our model is based (Goldwater et
al., 2009). Yet, even though these segmentation er-
rors are also counted as word learning errors, they
are often still meaningful in the sense that the true
referring word is a subsequence.

So, word segmentation has an impact on word
learning. Yet, the joint model still tends to uncover
reasonable meanings. The next question is whether
these meanings have an impact on the segmentation.

506



NoCon Random Joint
Referring Nouns 0.36 0.35 0.50
Neighbors 0.33 0.33 0.37
Utt Final Nouns 0.36 0.36 0.52
Entire Corpus 0.53 0.53 0.54

Table 3: Segmentation performance. F-score for three
subsets and the full corpus for three variants: the model
without non-linguistic context, the model with random
topics, and the full joint model.

4.2.2 Word Segmentation

To measure the impact of word learning on seg-
mentation, we again compare the model on the full
joint task against two other variants: one where top-
ics are randomly selected, and one that ignores the
non-linguistic context. For the random topics vari-
ant, we choose each topic during initialization ac-
cording to the empirical distribution over gold topics
and treat these topic assignments as observed vari-
ables for subsequent iterations. The variant that ig-
nores non-linguistic context draws topics uniformly
from the entire set of objects ever discussed in the
corpus, another test of the contribution of the non-
linguistic context to segmentation. We report token
F-score, computed as per Goldwater et al. (2009),
where any segment proposed by the model is a true
positive only if it matches the gold segmentation and
is a false positive otherwise. Any segment in the
gold data not found by the model is a false negative.

Table 3 shows the segmentation performance for
various subsets as well as for the entire corpus. Be-
cause we are primarily interested in synergies be-
tween word learning and segmentation, we focus on
the words most directly impacted by the meanings:
gold standard referring nouns and their neighboring
words.

The model behaves the same with randomized
topics as without context; it learns none of the gold
standard pairs (no matter how we identify clusters
with topics for the contextless case). On all subsets,
the full joint model outperforms the other two vari-
ants. In particular, the greatest gain is for the refer-
ring nouns, with a 21% reduction in error. Also, sim-
ilar to the findings of Bortfeld et al. (2005) regarding
6 month olds’ abilities to segment words adjoining a
familiar name, we also find that neighboring words
benefit from sharing a word boundary with a learned

word.
The model performs exceptionally well on utter-

ance final referring nouns, with a 24% reduction
in error. This may explain certain psycholinguistic
observations. Frank et al. (2006) performed an ar-
tificial language experiment with humans subjects
demonstrating that adults were able to learn words
at the same time as they learned to segment the lan-
guage. However, subjects did much better on a word
learning task when the meaning bearing words were
consistently placed at the end of utterances. There
are several possible reasons why this might have
been the case. For instance, it is common in English
for the object noun to occur at the end of the sen-
tence, and since the subjects were all English speak-
ers, they may have found it easier to learn an artifi-
cial language with a similar pattern. However, our
model predicts another simple possibility: the seg-
mentation task is easier at the end because one of
the two word boundaries is already known (the ut-
terance boundary itself).

4.3 Discussion

The word learning model generally prefers a very
sparse word-to-object map. This is enforced by us-
ing a concentration parameter α1 that is quite small
relative to the α0 parameter, and it biases the model
so that the distributions over referring words are
very different from that over non-referring words. A
small concentration parameter biases the estimator
to prefer a small set of word types. In contrast, the
relatively large concentration parameter for the non-
referring words tends to result in most of the words
receiving highest probability as non-referring words.
The model thus categorizes words accordingly. It is
in part due to this tendency towards sparse word-
object maps that the model enforces mutual exclu-
sivity, a phenomenon well documented among natu-
ral word learners (Markman, 1990).

Aside from contributing to mutual exclusivity
and specialization among the topical word distri-
butions, the small concentration parameter also has
important implications for the segmentation task.
A very small value for α1 discourages the learner
from acquiring more word types for each mean-
ing than absolutely necessary, thereby forcing the
segmenter to use fewer types to explain the se-
quence of phonemes. A model without any notion

507



of meaning cannot maintain separate distributions
for different topics, and must in some sense treat all
words as non-referring. A segmenting model with-
out meanings cannot share the word learner’s reluc-
tance to propose new meaning-bearing word types
and might propose three separate types for “your
book”, “a book”, and “the book”. However, with
a small enough prior on new referring word types,
the word learner that discovers a common refer-
ent for all three sequences and, preferring fewer re-
ferring word types, is more likely to discover the
common subsequence “book”. With a single word-
object pair (“book”, BOOK), the word learner could
explain reference for all three sequences instead of
using the three separate pairs (“yourbook”, BOOK),
(“abook”, BOOK), and (“thebook”, BOOK).

While relying on non-linguistic context helps seg-
ment the meaning-bearing words, the overall im-
provement is small in our current corpus. One rea-
son for this small improvement was that only 9%
of the tokens in the corpus were referring words.
In corpora containing a larger variety of objects –
and in cases where sub- and super-ordinate labels
like “eyes” and “ears” are coded – this percentage is
likely to be much higher, leading to a greater boost
in overall segmentation performance.

We should acknowledge that the decisions en-
tailed in enriching the annotations are neither triv-
ial nor without theoretic implication, however. It is
not immediately obvious how to represent the non-
linguistic correlates of verbs, for instance. Devel-
opmentally, verbs are typically acquired much later
than nouns, and it has been argued that this may be
due to the difficulty of producing a cognitive rep-
resentation of the associated meaning (Gentner and
Boroditsky, 2001). Even among concrete nouns, not
all are equal. Children tend to have a bias toward
whole objects when mapping novel words to their
non-linguistic counterparts (Markman, 1990). De-
cisions about more sophisticated encoding of non-
linguistic information may thus require more knowl-
edge about children’s representations of the world
around them

5 Conclusion and Future Work

We find (1) that it is possible to jointly infer both
meanings and a segmentation in a fully unsupervised

way and (2) that doing so improves the segmenta-
tion performance of our model. In particular, we
found that although the word learning side suffered
from segmentation errors, and performed worse than
a model that learned from a gold standard segmen-
tation, the loss was only slight. On the other hand,
segmentation performance for the meaning bearing
words improved a great deal. The first result sug-
gests that is not necessary to assume fully segmented
input in order to learn word meanings, and that the
segmentation and word learning tasks can be effec-
tively modeled in parallel, allowing us to explore po-
tential developmental interactions. The second re-
sult suggests that synergies do actually exist and ar-
gue not only that we can model the two as parallel
processes, but that doing so could prove fruitful.

Our model is relatively simple both in terms of
word learning and in terms of word segmentation.
For instance, social cues and shared attention, or dis-
course effects, might all play a role (Frank et al.,
2009b). Shared features or other relationships can
also potentially impact how quickly one might gen-
eralize a label to multiple instances (Tenenbaum and
Xu, 2000). There are many ways to elaborate on the
word learning task, with additional potential syner-
gistic implications.

We might also elaborate the linguistic structures
we incorporate into the word learning model. For
instance, Johnson (2008) explores synergies in syl-
lable and morphological structures in word segmen-
tation. Aspects of linguistic structure, such as mor-
phology, may contribute to word meaning learning
beyond its contribution to word segmentation per-
formance.

Acknowledgments

This research was funded by NSF awards 0544127
and 0631667 to Mark Johnson and by NSF DDRIG
0746251 to Michael C. Frank. We would also like
to thank Anne Fernald for providing the corpus and
Maeve Cullinane for help in coding it.

References

Nameera Akhtar and Lisa Montague. 1999. Early lexi-
cal acquisition: The role of cross-situational learning.
First Language, 19(57 Pt 3):347–358.

508



David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.

Heather Bortfeld, James L. Morgan, Roberta Michnick
Golinkoff, and Karen Rathbun. 2005. Mommy
and me: Familiar names help launch babies into
speechstream segmentation. Psychological Science,
16(4):298–304.

Michael R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discovery.
Machine Learning, 34:71–105.

Anne Fernald and Hiromi Morikawa. 1993. Common
themes and cultural variations in japanese and ameri-
can mothers’ speech to infants. In Child Development,
number 3, pages 637–656, June.

Michael C. Frank, Vikash Mansinghka, Edward Gibson,
and Joshua B. Tenenbaum. 2006. Word segmentation
as word learning: Integrating stress and meaning with
distributional cues. In Proceedings of the 31st Annual
Boston University Conference on Language Develop-
ment.

Michael C. Frank, Sharon Goldwater, Vikash Mans-
inghka, Tom Griffiths, and Joshua Tenenbaum. 2007.
Modeling human performance in statistical word seg-
mentation. Proceedings of the 29th Annual Meeting of
the Cognitive Science Society, pages 281–286.

Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009a. Using speakers’ referential inten-
tions to model early cross-situational word learning.
Psychological Science, 5:578–585.

Michael C. Frank, Noah D. Goodman, Joshua B. Tenen-
baum, and Anne Fernald. 2009b. Continuity of dis-
course provides information for word learning.

Dedre Gentner and Lera Boroditsky. 2001. Individua-
tion, relativity, and early word learning. Language,
culture, & cognition, 3:215–56.

Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.

Katharine Graf-Estes, Julia L. Evans, Martha W. Alibali,
and Jenny R. Saffran. 2007. Can infants map meaning
to newly segmented words? statistical segmentation
and word learning. Psychological Science, 18(3):254–
260.

Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Artificial Intelligence
and Statistics (AISTATS), March.

Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-

can Chapter of the Association for Computational Lin-
guistics, pages 317–325, Boulder, Colorado, June. As-
sociation for Computational Linguistics.

Mark Johnson. 2008. Using adaptor grammars to identi-
fying synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics, Columbus, Ohio. Association for Computational
Linguistics.

Ellen M. Markman. 1990. Constraints children place on
word learning. Cognitive Science, 14:57–77.

Terry Regier. 2003. Emergent constraints on word-
learning: A computational review. Trends in Cognitive
Sciences, 7:263–268.

Jenny R. Saffran, Elissa L. Newport, and Richard N.
Aslin. 1996. Word segmentation: The role of dis-
tributional cues. Journal of memory and Language,
35:606–621.

Jeffrey M. Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):39–91.

Joshua B. Tenenbaum and Fei Xu. 2000. Word learn-
ing as bayesian inference. In Proceedings of the 22nd
Annual Conference of the Cognitive Science Society,
pages 517–522.

Chen Yu and Dana H. Ballard. 2007. A unified model of
early word learning: Integrating statistical and social
cues. Neurocomputing, 70(13-15):2149–2165.

509


