










































Corpus-Driven Terminology Development: Populating Swedish SNOMED CT with Synonyms Extracted from Electronic Health Records


Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 36–44,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Corpus-Driven Terminology Development: Populating Swedish
SNOMED CT with Synonyms Extracted from Electronic Health Records

Aron Henriksson1, Maria Skeppstedt1, Maria Kvist1,2, Martin Duneld1, Mike Conway3

1Department of Computer and Systems Sciences (DSV), Stockholm University, Sweden
2Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institute, Sweden

3Division of Biomedical Informatics, University of California San Diego, USA

Abstract

The various ways in which one can re-
fer to the same clinical concept needs to
be accounted for in a semantic resource
such as SNOMED CT. Developing termi-
nological resources manually is, however,
prohibitively expensive and likely to re-
sult in low coverage, especially given the
high variability of language use in clinical
text. To support this process, distributional
methods can be employed in conjunction
with a large corpus of electronic health
records to extract synonym candidates for
clinical terms. In this paper, we exem-
plify the potential of our proposed method
using the Swedish version of SNOMED
CT, which currently lacks synonyms. A
medical expert inspects two thousand term
pairs generated by two semantic spaces –
one of which models multiword terms in
addition to single words – for one hundred
preferred terms of the semantic types dis-
order and finding.

1 Introduction

In recent years, the adoption of standardized ter-
minologies for the representation of clinical con-
cepts – and their textual instantiations – has en-
abled meaning-based retrieval of information from
electronic health records (EHRs). By identify-
ing and linking key facts in health records, the
ever-growing stores of clinical documentation now
available to us can more readily be processed
and, ultimately, leveraged to improve the qual-
ity of care. SNOMED CT1 has emerged as the
de facto international terminology for represent-
ing clinical concepts in EHRs and is today used
in more than fifty countries, despite only being

1http://www.ihtsdo.org/snomed-ct/

available in a handful of languages2. Translations
into several other languages are, however, under
way3. This translation effort is essential for more
widespread integration of SNOMED CT in EHR
systems globally.

Translating a comprehensive4 terminology such
as SNOMED CT to an additional language is,
however, a massive and expensive undertaking. A
substantial part of this process involves enrich-
ing the terminology with synonyms in the tar-
get language. SNOMED CT has, for instance,
recently been translated into Swedish; however,
the Swedish version does not as yet contain syn-
onyms. Methods and tools that can accelerate the
language porting process in general and the syn-
onym identification task in particular are clearly
needed, not only to lower costs but also to in-
crease the coverage of SNOMED CT in clinical
text. Methods that can account for real-world lan-
guage use in the clinical setting, then, as well as to
changes over time, are particularly valuable.

This paper evaluates a semi-automatic method
for the extraction of synonyms of SNOMED CT
preferred terms using models of distributional se-
mantics to induce semantic spaces from a large
corpus of clinical text. In contrast to most ap-
proaches that exploit the notion of distributional
similarity for synonym extraction, this method ad-
dresses the key problem of identifying synonymy
between terms of varying length: a simple solution
is proposed that effectively incorporates the notion
of paraphrasing in a distributional framework. The
semantic spaces – and, by extension, the method –
are evaluated for their ability to extract synonyms
of SNOMED CT terms of the semantic types dis-
order and finding in Swedish.

2SNOMED CT is currently available in US English, UK
English, Spanish, Danish and Swedish.

3http://www.ihtsdo.org/snomed-ct/
snomed-ct0/different-languages/

4SNOMED CT contains more than 300,000 active con-
cepts and over a million relations.

36



2 Background

Synonymy is an aspect of semantics that con-
cerns the fact that concepts can be instantiated
using multiple linguistic expressions, or, viewed
conversely, that multiple linguistic expressions
can refer to the same concept. As synonymous
expressions do not necessarily consist of single
words, we sometimes speak of paraphrasing rather
than synonymy (Androutsopoulos and Malakasio-
tis, 2010). This variability of language use needs
to be accounted for in order to build high-quality
natural language processing (NLP) and text min-
ing systems. This is typically achieved by us-
ing thesauri or encoding textual instantiations of
concepts in a semantic resource, e.g. an ontol-
ogy. Creating such resources manually is, how-
ever, prohibitively expensive and likely to lead
to low coverage, especially in the clinical genre
where language use variability is exceptionally
high (Meystre et al., 2008).

2.1 Synonym Extraction

As a result, the task of extracting synonyms –
and other semantic relations – has long been a
central challenge in the NLP research commu-
nity, not least in the biomedical (Cohen and Hersh,
2005) and clinical (Meystre et al., 2008) do-
mains. A wide range of techniques has been pro-
posed for relation extraction in general and syn-
onym extraction in particular – lexico-syntactic
patterns (Hearst, 1992), distributional semantics
(Dumais and Landauer, 1997) and graph-based
models (Blondel et al., 2004) – from a variety
of sources, including dictionaries (Blondel et al.,
2004), linked data such as Wikipedia (Nakayama
et al., 2007), as well as both monolingual (Hindle,
1990) and multilingual (van der Plas and Tiede-
mann, 2006) corpora. In recent years, ensemble
methods have been applied to obtain better perfor-
mance on the synonym extraction task, combin-
ing models from different families (Peirsman and
Geeraerts, 2009), with different parameter settings
(Henriksson et al., 2012) and induced from differ-
ent data sources (Wu and Zhou, 2003).

In the context of biomedicine, the goal has of-
ten been to extract synonyms of gene and pro-
tein names from the biomedical literature (Yu and
Agichtein, 2003; Cohen et al., 2005; McCrae and
Collier, 2008). In the clinical domain, Conway
and Chapman (2012) used a rule-based approach
to generate potential synonyms from the BioPor-

tal ontology web service, verifying candidate syn-
onyms against a large clinical corpus. Zeng et
al. (2012) used three query expansion methods
for information retrieval of clinical documents and
found that a model of distributional semantics –
LDA-based topic modeling – generated the best
synonyms. Henriksson et al. (2012) combined
models of distributional semantics – random in-
dexing and random permutation – to extract syn-
onym candidates for Swedish MeSH5 terms and
possible abbreviation-definition pairs. In the con-
text of SNOMED CT, distributional methods have
been applied to capture synonymous relations be-
tween terms of varying length: 16-24% of English
SNOMED CT synonyms present in a large clini-
cal corpus were successfully identified in a list of
twenty suggestions (Henriksson et al., 2013).

2.2 Distributional Semantics

Models of distributional semantics (see Cohen and
Widdows (2009) for an overview of methods and
their application in the biomedical domain) were
initially motivated by the inability of the vector
space model to account for synonymy, which had
a negative impact on recall in information retrieval
systems (Deerwester et al., 1990). The theoretical
foundation underpinning such models of seman-
tics is the distributional hypothesis (Harris, 1954),
according to which words with similar meanings
tend to appear in similar contexts. By exploiting
the availability of large corpora, the meaning of
terms can be modeled based on their distribution
in different contexts. An estimate of the semantic
relatedness between terms can then be quantified,
thereby, in some sense, rendering semantics com-
putable.

An obvious application of distributional seman-
tics is the extraction of semantic relations between
terms, such as synonymy, hyp(o/er)nymy and co-
hyponymy (Panchenko, 2013). As synonyms are
interchangeable in some contexts – and thus have
similar distributional profiles – synonymy is cer-
tainly a semantic relation that should be captured.
However, since hyp(o/er)nyms and co-hyponyms
– in fact, even antonyms – are also likely to have
similar distributional profiles, such semantic rela-
tions will be extracted too.

Many models of distributional semantics dif-
fer in how context vectors, representing term

5Medical Subject Headings (http://www.nlm.nih.
gov/mesh).

37



meaning, are constructed. They are typically de-
rived from a term-context matrix that contains
the (weighted, normalized) frequency with which
terms occur in different contexts. Partly due
to the intractability of working with such high-
dimensional data, it is projected into a lower-
dimensional (semantic) space, while approxi-
mately preserving the relative distances between
data points. Methods that rely on computa-
tionally expensive dimensionality reduction tech-
niques suffer from scalability issues.

Random Indexing
Random indexing (RI) (Kanerva et al., 2000) is
a scalable and computationally efficient alterna-
tive in which explicit dimensionality reduction is
avoided: a lower dimensionality d is instead cho-
sen a priori as a model parameter and the d-
dimensional context vectors are then constructed
incrementally. Each unique term in the corpus is
assigned a static index vector, consisting of ze-
ros and a small number of randomly placed 1s
and -1s6. Each term is also assigned an initially
empty context vector, which is incrementally up-
dated by adding the index vectors of the surround-
ing words within a sliding window, weighted by
their distance to the target term. The semantic re-
latedness between two terms is then estimated by
calculating, for instance, the cosine similarity be-
tween their context vectors.

Random Permutation
Random permutation (RP) (Sahlgren et al., 2008)
is a modification of RI that attempts to take into
account term order information by simply permut-
ing (i.e. shifting) the index vectors according to
their direction and distance from the target term
before they are added to the context vector. RP
has been shown to outperform RI on the synonym
part of the TOEFL7 test.

Model Parameters
The model parameters need to be configured for
the task that the semantic space is to be used
for. For instance, with a document-level con-
text definition, syntagmatic relations are mod-
eled, i.e. terms that belong to the same topic
(<car, motor, race>), whereas, with a sliding win-
dow context definition, paradigmatic relations are

6By generating sparse vectors of a sufficiently high di-
mensionality in this way, the context representations will be
nearly orthogonal.

7Test Of English as a Foreign Language

modeled (<car, automobile, vehicle>) (Sahlgren,
2006). Synonymy is an instance of a paradigmatic
relation.

The dimensionality has also been shown to be
potentially very important, especially when the
size of the vocabulary and the number of contexts8

are large (Henriksson and Hassel, 2013).

3 Materials and Methods

The task of semi-automatically identifying syn-
onyms of SNOMED CT preferred terms is here
approached by, first, statistically identifying mul-
tiword terms in the data and treating them as com-
pounds; then, performing a distributional analysis
of a preprocessed clinical corpus to induce a se-
mantic term space; and, finally, extracting the se-
mantically most similar terms for each preferred
term of interest.

The experimental setup can be broken down
into the following steps: (1) data preparation, (2)
term recognition, (3) model parameter tuning and
(4) evaluation. Semantic spaces are induced with
different parameter configurations on two dataset
variants: one with unigram terms only and one that
also includes multiword terms. The model param-
eters are tuned using MeSH, which contains syn-
onyms for Swedish. The best parameter settings
for each of the two dataset variants are then em-
ployed in the final evaluation, where a medical ex-
pert inspects one hundred term lists extracted for
SNOMED CT preferred terms belonging to the se-
mantic types disorder and finding.

3.1 Data Preparation

The data used to induce the semantic spaces is ex-
tracted from the Stockholm EPR Corpus (Dalia-
nis et al., 2009), which contains Swedish health
records from the Karolinska University Hospital
in Stockholm9. The subset (∼33 million tokens)
used in these experiments comprises all forms of
text-based records – i.e., clinical notes – from
a large variety of clinical practices. The docu-
ments in the corpus are initially preprocessed by
simply lowercasing tokens and removing punctu-
ation and digits. Lemmatization is not performed,
as we want to be able to capture morphological

8The vocabulary size and the number of contexts are
equivalent when employing a window context definition.

9This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikprövningsnämnden i Stock-
holm), permission number 2012/834-31/5.

38



variants of terms; stop-word filtering is not per-
formed, as traditional stop words – for instance,
high-frequency function words – could potentially
be constituents of multiword terms.

3.2 Term Recognition

Multiword terms are extracted statistically from
the corpus using the C-value statistic (Frantzi and
Ananiadou, 1996; Frantzi et al., 2000). This tech-
nique has been used successfully for term recog-
nition in the biomedical domain, largely due to
its ability to handle nested terms (Zhang et al.,
2008). Using the C-value statistic for term recog-
nition first requires a list of candidate terms, for
which the C-value can then be calculated. Here,
this is simply produced by extracting n-grams –
unigrams, bigrams and trigrams – from the corpus
with TEXT-NSP (Banerjee and Pedersen, 2003).
The statistic is based on term frequency and term
length (number of words); if a candidate term is
part of a longer candidate term (as will be the case
for practically all unigram and bigram terms), the
number and frequency of those longer terms are
also taken into account (Figure 1).

In order to improve the quality of the extracted
terms, a number of filtering rules is applied to the
generated term list: terms that begin and/or end
with certain words, e.g. prepositions and articles,
are removed. The term list – ranked according to
C-value – is further modified by giving priority to
terms of particular interest, e.g. SNOMED CT dis-
order and finding preferred terms: these are moved
to the top of the list, regardless of their C-value.
As a result, the statistical foundation on which the
distributional method bases its semantic represen-
tation will effectively be strengthened.

The term list is then used to perform exact string
matching on the entire corpus: multiword terms
with a higher C-value than their constituents are
concatenated. We thereby treat multiword terms as
separate (term) types with distinct distributions in
the data, different from those of their constituents.

3.3 Model Parameter Tuning

Term spaces with different parameter configu-
rations are induced from the two dataset vari-
ants: one containing only unigram terms (Uni-
gram Word Spaces) and one containing also mul-
tiword terms (Multiword Term Spaces). The fol-
lowing model parameters are tuned:

• Distributional Model: Random indexing (RI)
vs. Random permutation (RP)

• Context Window Size: 2+2, 4+4, 8+8 sur-
rounding terms (left+right of the target term)

• Dimensionality: 1000, 2000, 3000

As the Swedish version of SNOMED CT cur-
rently does not contain synonyms, it cannot be
used to perform the parameter tuning automat-
ically. This is instead done with the Swedish
version of MeSH, which is one of the very few
standard terminologies that contains synonyms for
medical terms in Swedish. However, as the op-
timal parameter configurations for capturing syn-
onymy are not necessarily identical for all seman-
tic types, the parameter tuning is performed by
evaluating the semantic spaces for their ability to
identify synonyms of MeSH terms that belong to
the categories Disease or Syndrome and Sign or
Symptom. These particular categories are simply
chosen as they, to a reasonable extent, seem to
correspond to the SNOMED CT semantic types
studied in this paper, namely Disorder and Find-
ing. Only synonym pairs that appear at least fifty
times in each of the dataset variants are included
(155 for Unigram Word Spaces and 123 for Mul-
tiword Term Spaces), as the statistical foundation
for terms that only occur rarely in the data may
not be sufficiently solid. In these Multiword Term
Spaces, the MeSH terms – but not the synonyms
– are given precedence in the term list. A term
is provided as input to a semantic space and the
twenty semantically most similar terms are out-
put, provided that they also appear at least fifty
times in the data. Recall Top 20 is calculated for
each input term: what proportion of the MeSH
synonyms are identified in a list of twenty sugges-
tions? Since each synonym pair must appear at
least fifty times in the corresponding dataset vari-
ant, it should be duly noted that the optimization
sets will not be identical, which in turn means that
the results of the Unigram Word Spaces and the
Multiword Term Spaces are not directly compara-
ble. The optimal parameter configuration, then,
may be different when also multiword terms are
modeled.

3.4 Evaluation
The optimal parameter configuration for each
dataset variant is employed in the final evaluation.
In this Multiword Term Space, the SNOMED CT

39



C-value(a) =
{

log2 |a| · f(a) if a is not nested
log2 |a| · (f(a)− 1P (Ta)

∑
b�Ta f(b)) otherwise

a = candidate term Ta = set of extracted candidate terms that contain a
b = longer candidate terms P (Ta) = number of candidate terms in Ta
f(a) = term frequency of a f(b) = term frequency of longer candidate term b
|a| = length of candidate term (number of words)

Figure 1: C-Value Formula. The formula for calculating C-value of candidate terms.

preferred terms of interest, rather than the MeSH
terms, are prioritized in the term list. The seman-
tic spaces – and, in effect, the method – are pri-
marily evaluated for their ability to identify syn-
onyms of SNOMED CT preferred terms, in this
case of concepts that belong to the semantic types
disorder and finding. The need to identify syn-
onyms for these semantic types is clear, as it has
been shown that the coverage of SNOMED CT
for mentions of disorders (38%) and, in particu-
lar, findings (23%) in Swedish clinical text is low
(Skeppstedt et al., 2012). Since the Swedish ver-
sion of SNOMED CT currently lacks synonyms,
the evaluation reasonably needs to be manual, as
there is no reference standard. One option, then,
could be to choose a random sample of preferred
terms to use in the evaluation. A potential draw-
back of such a(n) (unguided) selection is that many
concepts in the English version of SNOMED CT
do not have any synonymous terms, which might
lead to evaluators spending valuable time looking
for something which does not exist. An alterna-
tive approach, which is assumed here, is to inspect
concepts that have many synonyms in the English
version of SNOMED CT. The fact that some con-
cepts have many textual instantiations in one lan-
guage does not necessarily imply that they also
have many textual instantiations in another lan-
guage. This, however, seems to be the case when
comparing the English and Swedish versions of
MeSH: terms10 that have the most synonyms in the
English version tend to have at least one synonym
in the Swedish version to a larger extent than a ran-
dom selection of terms (60% and 62% of the terms
in the Swedish version have at least one synonym
when looking at the top 100 and top 50 terms with
the most synonyms in the English version, com-
pared to 41% overall in the Swedish version).

For the two dataset variants, we thus select 25
SNOMED CT preferred terms for each semantic

10These calculations are based on MeSH terms that belong
to the categories Disease or Syndrome and Sign or Symptom.

type – disorder and finding – that (1) have the most
synonyms in the English version and (2) occur at
least fifty times in the data. In total, fifty terms
are input to the Unigram Word Space and another
fifty terms (potentially with some overlap) are in-
put to the Multiword Term Space. A medical ex-
pert inspects the twenty semantically most simi-
lar terms for each input term. Synonymy is here
the primary semantic relation of interest, but the
semantic spaces are also evaluated for their abil-
ity, or tendency, to extract other semantic term re-
lations: hypernyms or hyponyms, co-hyponyms,
antonyms, as well as disorder-finding relations.

4 Results

The term recognition and concatenation of mul-
tiword terms naturally affect some properties of
the dataset variants, such as the vocabulary size
(number of types) and the type-token ratio. The
Unigram Word Space contains 381,553 types and
an average of 86.54 tokens/type, while the Mul-
tiword Term Space contains 2,223,953 types and
an average of 9.72 tokens/type. This, in turn, may
have an effect on which parameter configuration is
‘optimal’ for the synonym extraction task. In fact,
this seems to be the case when tuning the parame-
ters for the two dataset variants. For the Unigram
Word Spaces, random indexing with a sliding con-
text window of 8+8 terms and a dimensionality of
2000 seems to work best, whereas for the Mul-
tiword Term Spaces, random permutation with a
sliding window context of 4+4 terms and a dimen-
sionality of 3000 works better (Table 1).

When these parameter configurations are ap-
plied to the SNOMED CT terms, a total of 40 syn-
onyms are extracted by the Unigram Word Space
and 33 synonyms by the Multiword Term Space
(Table 2). On average, 0.80 and 0.66 synonyms
are extracted per preferred term, respectively. The
number of identified synonyms per input term
varies significantly: for some, none; for others, up
to ten. Other semantic relations are also extracted

40



Unigram Word Spaces Multiword Term Spaces
RI RP RI RP

Sliding Window→ 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8
1000 dimensions 0.43 0.47 0.48 0.41 0.45 0.42 0.21 0.25 0.26 0.25 0.26 0.24
2000 dimensions 0.43 0.48 0.49 0.48 0.48 0.43 0.21 0.24 0.25 0.25 0.25 0.24
3000 dimensions 0.44 0.47 0.48 0.46 0.45 0.43 0.22 0.24 0.24 0.23 0.27 0.25

Table 1: Model Parameter Tuning. Results, reported as recall top 20, for MeSH synonyms that appear
at least 50 times in each of the dataset variants (unigram vs. multiword). Random indexing (RI) and
Random permutation (RP) term spaces were built with different context window sizes (2+2, 4+4, 8+8
surrounding terms) and dimensionality (1000, 2000, 3000).

by the semantic spaces: mainly co-hyponyms,
but also hypernyms and hyponyms, antonyms and
disorder-finding relations. The Unigram Word
Space extracts, on average, 0.52 hypernyms or hy-
ponyms, 1.8 co-hyponyms, 0.1 antonyms and 0.34
disorder-finding relations. The Multiword Term
Space extracts, on average, 0.16 hypernyms or
hyponyms, 1.1 co-hyponyms, 0.14 antonyms and
0.66 disorder-finding relations. In general, more
of the above semantic relations are extracted by
the Unigram Word Space than by the Multiword
Term Space (178 vs. 136). It is, however, inter-
esting to note that almost twice as many disorder-
finding relations are extracted by the latter com-
pared to the former. Of course, none of the re-
lations extracted by the Unigram Word Space in-
volve a multiword term; on the other hand, more
than half (around 57%) of the relations extracted
by the Multiword Term Space involve at least one
multiword term.

Both semantic spaces identify more synonyms
of preferred terms that belong to the semantic type
finding than disorder (in total 56 vs. 39). The same
holds true for hyp(er/o)nyms and co-hypnoyms;
however, the converse is true for antonyms and
disorder-finding relations.

5 Discussion

The results demonstrate that it is indeed possible
to extract synonyms of medical terms by perform-
ing a distributional analysis of a large corpus of
clinical text – unigram-unigram relations, as well
as unigram-multiword and multiword-unigram re-
lations. It is also clear, however, that other se-
mantically related terms share distributional pro-
files to a similar degree as synonymous terms. The
predominance of the other semantic relations, ex-
cept for antonymy, in the term lists can reason-
ably be explained by the simple fact that there

exist more hypernyms, hyponyms, co-hyponyms
and disorder-finding relations than synonyms (or
antonyms).

It is also evident that more semantic relations,
and indeed more synonyms, are extracted by the
Unigram Word Space than the Multiword Term
Space. Again, it is important to underline that the
results cannot be compared without due qualifica-
tion since the evaluation sets are not identical: the
Unigram Word Space does not contain any mul-
tiword terms, for instance. The ability to model
multiword terms in a distributional framework and
to handle semantic composition – i.e., how mean-
ing is, and sometimes is not, composed by the
meaning of its constituents – has long been an en-
deavor in the NLP research community (Sag et al.,
2002; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Mitchell, 2011). Treating
multiword terms as compound tokens is a simple
and rather straightforward approach, which also
makes intuitive sense: rather than treat individ-
ual words as clearly delineated bearers of mean-
ing, identify semantic units – regardless of term
length – and model their distributional profiles.
Unfortunately, there are problems with this ap-
proach. First, the attendant increase in vocabu-
lary size entails a lower tokens-type ratio, which
in turn means that the statistical foundation for
terms will weaken. In this case, the average token-
type ratio decreased from 86.54 to 9.72. This ap-
proach therefore requires access to a sufficiently
large corpus. Second, the inflation in vocabulary
size entails a corresponding increase in the num-
ber of vectors in the semantic space. This not only
requires more memory; to ensure that the crucial
near-orthogonality property11 of RI-based models
is maintained, the dimensionality has to be suffi-

11Random indexing assumes that the index vectors – rep-
resenting distinct contexts – are nearly orthogonal.

41



Unigram Word Space Multiword Term Space
DISORDER FINDING DISORDER FINDING

Synonyms
sum 18 22 16 17

average 0.72 0.88 0.64 0.68
≥ 1 / preferred term 12 12 8 6

involves mwe - - 10 13
Hyp(er/o)nyms

sum 12 14 4 4
average 0.48 0.56 0.16 0.16

≥ 1 / preferred term 6 8 4 3
involves mwe - - 3 3

Co-hyponyms
sum 34 56 22 33

average 1.36 2.24 0.88 1.32
≥ 1 / preferred term 14 17 10 13

involves mwe - - 19 15
Antonyms

sum 3 2 4 3
average 0.12 0.08 0.16 0.12

≥ 1 / preferred term 3 2 3 3
involves mwe - - 0 1

Disorder-Finding
sum 11 6 28 5

average 0.44 0.24 1.12 0.2
≥ 1 / preferred term 6 5 12 5

involves mwe - - 11 2

Table 2: Evaluation Results. The types of semantic relations extracted among the twenty most se-
mantically similar terms of 25 DISORDER and 25 FINDING SNOMED CT preferred terms from each
semantic space. Sum is the total number of identified relevant terms. Average is the average number of
relevant terms per preferred term. ≥ 1 / preferred term is the number of preferred terms for which at
least one relevant term is identified. Involves mwe is the number of relevant relations where either the
preferred term or the relevant term is a multiword expression.

ciently large in relation to the number of contexts
(represented by index vectors). In the Multiword
Term Space the vocabulary size is over two million
(compared to less than 400,000 in the Unigram
Word Space). A dimensionality of 3000 is likely
insufficient to ensure that each term type has an
initial distinct and uncorrelated representation. In
the evaluation, there were several examples where
two groups of terms – semantically homogenous
within each group, but semantically heterogenous
across groups – co-existed in the same term list:
these ‘topics’ had seemingly collapsed into the
same subspace. Despite these problems, it should
be recognized that the Multiword Term Space is, in
fact, able to retrieve 23 synonymous relations that
involve at least one multiword term. The Unigram

Word Space cannot retrieve any such relations.
The ability to extract high-quality terms would

seem to be an important prerequisite for this ap-
proach to modeling multiword terms in a distribu-
tional framework. However, despite employing a
rather simple means of extracting terms – without
using any syntactic information – the terms that
actually appeared in the lists of semantically re-
lated terms were mostly reasonable. This perhaps
indicates that the term recognition task does not
need to be perfect: terms of interest, of course,
need to be identified, but some noise in the form
of bad terms might be acceptable. A weakness
of the term recognition part is, however, that too
many terms were identified, which in turn led to
the aforementioned inflation in vocabulary size.

42



Limiting the number of multiword terms in the ini-
tial term list – for instance by extracting syntactic
phrases as candidate terms – could provide a pos-
sible solution to this problem.

Overall, more synonyms were identified for the
semantic type finding than for disorder. One pos-
sible explanation for this could be that there are
more ways of describing a finding than a disorder
– not all semantic types can be assumed to have
the same number of synonyms. The same holds
true for all other semantic relations except for dis-
order-finding, where disorders generated a much
larger number of distributionally similar findings
than vice versa. This could perhaps also be ex-
plained by the possible higher number of syn-
onyms for finding than disorder.

When this method was evaluated using the
English version of SNOMED CT, 16-24% of
known synonyms were identified (Henriksson et
al., 2013). In this case, however, we extracted
synonym candidates for terms that may or may
not have synonyms. This is thus a scenario that
more closely resembles how this method would
actually be used in a real-life setting to populate
a terminology with synonyms. Although the com-
parison with MeSH showed that terms with many
synonyms in English also tend to have at least one
synonym in Swedish, approximately 40% of them
did not have any synonyms. It is thus not certain
that the terms used in this evaluation all have at
least one synonym, which was also noted by the
evaluator in this study.

6 Conclusions

In this study, we have demonstrated a method
that could potentially be used to expedite the lan-
guage porting process of terminologies such as
SNOMED CT. With access to a large corpus of
clinical text in the target language and an initial
set of terms, this language-independent method is
able to extract and present candidate synonyms to
the lexicographer, thereby providing valuable sup-
port for semi-automatic terminology development.
A means to model multiword terms in a distri-
butional framework is an important feature of the
method and is crucial for the synonym extraction
task.

Acknowledgments

This work was partly supported by the Swedish
Foundation for Strategic Research through the

project High-Performance Data Mining for Drug
Effect Detection (ref. no. IIS11-0053) at Stock-
holm University, Sweden, and partly funded by
the Stockholm University Academic Initiative
through the Interlock project. Finally, we would
like to thank the reviewers for their constructive
feedback.

References
Ion Androutsopoulos and Prodromos Malakasiotis.

2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135–187.

Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation, and Use of the Ngram Statis-
tic Package. In Proceedings of CICLing, pages 370–
381.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Representing
Adjective-Noun Constructions in Semantic Space.
In Proceedings of EMNLP, pages 1183–1193.

Vincent D. Blondel, Anahı́ Gajardo, Maureen Hey-
mans, Pierre Senellart, and Paul Van Dooren.
2004. A Measure of Similarity between Graph Ver-
tices: Applications to Synonym Extraction and Web
Searching. SIAM Review, 46(4):647–666.

Aaron M. Cohen and William R. Hersh. 2005. A Sur-
vey of Current Work in Biomedical Text Mining.
Briefings in Bioinformatics, 6(1):57–71.

Trevor Cohen and Dominic Widdows. 2009. Empirical
Distributional Semantics: Methods and Biomedical
Applications. J Biomed Inform, 42(2):390–405.

AM Cohen, WR Hersh, C Dubay, and K Spackman.
2005. Using co-occurrence network structure to
extract synonymous gene and protein names from
medline abstracts. BMC Bioinformatics, 6(1):103.

Mike Conway and Wendy W. Chapman. 2012. Dis-
covering Lexical Instantiations of Clinical Con-
cepts using Web Services, WordNet and Corpus Re-
sources. In AMIA Fall Symposium, page 1604.

Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus:
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR, pages 243–249.

Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Susan T. Dumais and Thomas K. Landauer. 1997. A
Solution to Plato’s Problem: The Latent Semantic

43



Analysis Theory of Acquisition, Induction and Rep-
resentation of Knowledge. Psychological Review,
104(2):211–240.

Katerina Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of
COLING, pages 41–46.

Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic Recognition of Multi-
Word Terms: The C-value/NC-value Method. In-
ternational Journal on Digital Libraries, 3(2):115–
130.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of EMNLP, pages 1394–1404.

Zellig S. Harris. 1954. Distributional Structure. Word,
10:146–162.

Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings
of COLING, pages 539–545.

Aron Henriksson and Martin Hassel. 2013. Optimiz-
ing the Dimensionality of Clinical Term Spaces for
Improved Diagnosis Coding Support. In Proceed-
ings of Louhi.

Aron Henriksson, Hans Moen, Maria Skeppstedt, Ann-
Marie Eklund, and Vidas Daudaravicius. 2012.
Synonym Extraction of Medical Terms from Clini-
cal Text Using Combinations of Word Space Mod-
els. In Proceedings of SMBM, pages 10–17.

Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying Syn-
onymy between SNOMED Clinical Terms of Vary-
ing Length Using Distributional Analysis of Elec-
tronic Health Records. In AMIA Annual Symposium
(submitted).

Donald Hindle. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings of
ACL, pages 268–275.

Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random Indexing of Text Samples for Latent
Semantic Analysis. In Proceedings CogSci, page
1036.

John McCrae and Nigel Collier. 2008. Synonym
Set Extraction from the Biomedical Literature by
Lexical Pattern Discovery. BMC Bioinformatics,
9(1):159.

Stéphane M. Meystre, Guergana K. Savova, Karin C.
Kipper-Schuler, John F. Hurdle, et al. 2008. Ex-
tracting Information from Textual Documents in the
Electronic Health Record: A Review of Recent Re-
search. Yearb Med Inform, 35:128–44.

Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, University of
Edinburgh.

Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2007. Wikipedia Mining for an Association Web
Thesaurus Construction. In Proceedings of WISE,
pages 322–334.

Alexander Panchenko. 2013. Similarity Measures for
Semantic Relation Extraction. Ph.D. thesis, PhD
thesis, Université catholique de Louvain & Bauman
Moscow State Technical University.

Yves Peirsman and Dirk Geeraerts. 2009. Predicting
Strong Associations on the Basis of Corpus Data. In
Proceedings of EACL, pages 648–656.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of CICLing, pages 1–15.

Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a Means to Encode Order
in Word Space. In Proceedings of CogSci, pages
1300–1305.

Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words in
High-Dimensional Vector Spaces. Ph.D. thesis, PhD
thesis, Stockholm University.

Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based Entity Recognition and Coverage
of SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of LREC, pages 1250–1257.

Lonneke van der Plas and Jörg Tiedemann. 2006.
Finding Synonyms Using Automatic Word Align-
ment and Measures of Distributional Similarity. In
Proceedings of COLING/ACL, pages 866–873.

Hua Wu and Ming Zhou. 2003. Optimizing Syn-
onym Extraction Using Monolingual and Bilingual
Resources. In Proceedings of the Second Interna-
tional Workshop on Paraphrasing, pages 72–79.

Hong Yu and Eugene Agichtein. 2003. Extracting
Synonymous Gene and Protein Terms from Biolog-
ical Literature. Bioinformatics, 19(suppl 1):i340–
i349.

Qing T Zeng, Doug Redd, Thomas Rindflesch, and
Jonathan Nebeker. 2012. Synonym, Topic Model
and Predicate-Based Query Expansion for Retriev-
ing Clinical Documents. In Proceedings AMIA An-
nual Symposium, pages 1050–9.

Ziqi Zhang, José Iria, Christopher Brewster, and Fabio
Ciravegna. 2008. A Comparative Evaluation of
Term Recognition Algorithms. In Proceedings of
LREC.

44


