



















































SEMILAR: The Semantic Similarity Toolkit


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 163–168,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

SEMILAR: The Semantic Similarity Toolkit 

 

Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu 

Department of Computer Science 

The University of Memphis 

Memphis, TN 38152 

{vrus,rbanjade,mclinten,nbnraula,dstfnscu}@memphis.edu 

 

 

Abstract 

We present in this paper SEMILAR, the SE-

Mantic simILARity toolkit. SEMILAR im-

plements a number of algorithms for assessing 

the semantic similarity between two texts. It is 

available as a Java library and as a Java 

standalone ap-plication offering GUI-based 

access to the implemented semantic similarity 

methods. Furthermore, it offers facilities for 

manual se-mantic similarity annotation by ex-

perts through its component SEMILAT (a 

SEMantic simILarity Annotation Tool). 

1 Introduction 

We present in this paper the design and im-

plementation of SEMILAR, the SEMantic 

simILARity toolkit. SEMILAR 

(www.semanticsimilarity.org) includes im-

plementations of a number of algorithms pro-

posed over the last decade or so to address 

various instances of the general problem of 

text-to-text semantic similarity. Semantic sim-

ilarity is an approach to language understand-

ing that is widely used in real applications. It 

is a practical alternative to the true under-

standing approach, which is intractable as it 

requires world knowledge, a yet to-be-solved 

problem in Artificial Intelligence. 

Text A: York had no problem with MTA’s in-

sisting the decision to shift funds had been within 

its legal rights. 

Text B: York had no problem with MTA’s say-

ing the decision to shift funds was within its 

powers. 

 

Given such two texts, the paraphrase identifi-

cation task is about automatically assessing 

whether Text A is a paraphrase of, i.e. has the 

same meaning as, Text B. The example above is 

a positive instance, meaning that Text A is a par-

aphrase of Text B and vice versa. 

The importance of semantic similarity in Nat-

ural Language Processing (NLP) is highlighted 

by the diversity of datasets and shared task eval-

uation campaigns (STECs) that have been pro-

posed over the last decade (Dolan, Quirk, and 

Brockett, 2004; McCarthy & McNamara, 2008; 

Agirre et al., 2012). These datasets include in-

stances from various applications.  Indeed, there 

is a need to identify and quantify semantic rela-

tions between texts in many applications. For 

instance, paraphrase identification, an instance of 

the semantic similarity problem, is an important 

step in a number of applications including Natu-

ral Language Generation, Question Answering, 

and dialogue-based Intelligent Tutoring Systems. 

In Natural Language Generation, paraphrases are 

a method to increase diversity of generated text 

(Iordanskaja et al. 1991). In Question Answer-

ing, multiple answers that are paraphrases of 

each other could be considered as evidence for 

the correctness of the answer (Ibrahim et al. 

2003). In Intelligent Tutoring Sys-tems (Rus et 

al., 2009; Lintean et al., 2010; Lintean, 2011), 

paraphrase identification is useful to assess 

whether students’ articulated answers to deep 

questions (e.g. conceptual physics questions) are 

similar-to/paraphrases-of ideal answers. 

Generally, the problem of semantic similarity 

between two texts, denoted text A and text B, is 

defined as quantifying and identifying the pres-

ence of semantic relations between the two texts, 

e.g. to what extent text A has the same meaning 

as or is a paraphrase of text B (paraphrase rela-

tion; Dolan, Quirk, and Brockett, 2004). Other 

semantic relations that have been investigated 

systematically in the recent past are entailment, 

i.e. to what extent text A entails or logically in-

fers text B (Dagan, Glickman, & Magnini, 2004), 

and elaboration, i.e. is text B is an elaboration of 

text A? (McCarthy & McNamara, 2008). 

163



Semantic similarity can be broadly construed 

between texts of any size. Depending on the 

granularity of the texts, we can talk about the 

following fundamental text-to-text similarity 

problems: word-to-word similarity, phrase-to-

phrase similarity, sentence-to-sentence similari-

ty, paragraph-to-paragraph similarity, or docu-

ment-to-document similarity. Mixed combina-

tions are also possible such as assessing the simi-

larity of a word to a sentence or a sentence to a 

paragraph. For instance, in summarization it 

might be useful to assess how well a sentence 

summarizes an entire paragraph. 

2 Motivation 

The problem of word-to-word similarity has been 

extensively studied over the past decades and a 

word-to-word similarity library (WordNet Simi-

larity) has been developed by Pedersen and col-

leagues (Pedersen, Patwardhan, & Michelizzi, 

2004). 

Methods to assess the semantic similarity of 

larger texts, in particular sentences, have been 

proposed over the last decade (Corley and 

Mihalcea, 2005; Fernando & Stevenson, 2008; 

Rus, Lintean, Graesser, & McNamara 2009). 

Androutsopoulos & Malakasiotis (2010) com-

piled a survey of methods for paraphrasing and 

entailment semantic relation identification at sen-

tence level. Despite all the proposed methods to 

assess semantic similarity between two texts, no 

semantic similarity library or toolkit, similar to 

the WordNet library for word-to-word similarity, 

exists for larger texts. Given the importance of 

semantic similarity, there is an acute need for 

such a library and toolkit. The developed SEMI-

LAR library and toolkit presented here fulfill this 

need. 

In particular, the development of the semantic 

similarity toolkit SEMILAR has been motivated 

by the need for an integrated environment that 

would provide:  

 

 easy access to implementations of various 
semantic similarity approaches from the 

same user-friendly interface and/or library. 

 easy access to semantic similarity methods 
that work at different levels of text granulari-

ty: word-to-word, sentence-to-sentence, par-

agraph-to-paragraph, document-to-

document, or a combination (SEMILAR in-

tegrates word-to-word similarity measures). 

 authoring methods for semantic similarity. 

 a common environment for that allows sys-
tematic and fair comparison of semantic sim-

ilarity methods. 

 facilities to manually annotate texts with se-
mantic similarity relations using a graphical 

user interface that make such annotations 

easier for experts (this component is called 

SEMILAT component - a SEMantic similari-

ty Annotation Tool). 

 

SEMILAR is thus a one-stop-shop for investi-

gating, annotating, and authoring methods for the 

semantic similarity of texts of any level of granu-

larity. 

3 SEMILAR: The Semantic Similarity 
Toolkit 

The authors of the SEMILAR toolkit (see Figure 

1) have been involved in assessing the semantic 

Figure 1. Snapshot of SEMILAR. The Data View tab is shown. 

164



similarity of texts for more than a decade. During 

this time, they have conducted a careful require-

ments analysis for an integrated software toolkit 

that would integrate various methods for seman-

tic similarity assessment. The result of this effort 

is the prototype presented here. We briefly pre-

sent the components of SEMILAR next and then 

describe in more detail the core component of 

SEMILAR, i.e. the set of semantic similarity 

methods that are currently available. It should be 

noted that we are continuously adding new se-

mantic similarity methods and features to SEMI-

LAR. 

The SEMILAR toolkit includes the following 

components: project management; data view-

browsing-visualization; preprocessing (e.g., col-

location identification, part-of-speech tagging, 

phrase or dependency parsing, etc.), semantic 

similarity methods (word-level and sentence-

level), classification components for qualitative 

decision making with respect to textual semantic 

relations (naïve Bayes, Decision Trees, Support 

Vector Machines, and Neural Network), kernel-

based methods (sequence kernels, word sequence 

kernels, and tree kernels; as of this writing, we 

are still implementing several other tree kernel 

methods); debugging and testing facilities for 

model selection; and annotation components (al-

lows domain expert to manually annotate texts 

with semantic relations using GUI-based facili-

ties; Rus et al., 2012). For space reasons, we only 

detail next the main algorithms in the core com-

ponent, i.e. the major text-to-text similarity algo-

rithms currently available in SEMILAR. 

4 The Semantic Similarity Methods 
Available in SEMILAR 

The core component of SEMILAR is a set of 

text-to-text semantic similarity methods. We 

have implemented methods that handle both uni-

directional similarity measures as well as bidirec-

tional similarity measures. For instance, the se-

mantic relation of entailment between two texts 

is unidirectional (a text T logically entails a hy-

pothesis text H but H does not entail T) while the 

paraphrase relation is bidirectional (text A has 

same meaning as text B and vice versa). 

Lexical Overlap. Given two texts, the sim-

plest method to assess their semantic similarity is 

to compute lexical overlap, i.e. how many words 

they have in common. There are many lexical 

overlap variations. Indeed, a closer look at lexi-

cal overlap reveals a number of parameters that 

turns the simple lexical overlap problem into a 

large space of possibilities. The parameters in-

clude preprocessing options (collocation detec-

tion, punctuation, stopword removal, etc.), filter-

ing options (all words, content words, etc.), 

weighting schemes (global vs. local weighting, 

binary weighting, etc.), and normalization factors 

(largest text, weighted average, etc.). A total of 

3,456 variants of lexical overlap can be generat-

ed by different parameter settings in SEMILAR. 

Lintean (2011) has shown that performance on 

lexical overlap methods on the tasks of para-

phrase identification and textual entailment tasks 

can vary significantly depending on the selected 

parameters. Some lexical overlap variations lead 

to performance results rivaling more sophisticat-

ed, state-of-the-art methods. 

It should be noted that the overlap category of 

methods can be extended to include N-gram 

overlap methods (see the N-gram overlap meth-

ods proposed by the Machine Translation com-

munity such as BLEU and METEOR). SEMI-

LAR offers bigram and unigram overlap methods 

including the BLEU and METEOR scores. 

A natural approach to text-to-text similarity 

methods is to rely on word-to-word similarity 

measures. Many of the methods presented next 

compute the similarity of larger texts using indi-

vidual word similarities. 

Mihalcea, Corley, & Strappavara (2006; 

MCS) proposed a greedy method based on word-

to-word similarity measures. For each word in 

text A (or B) the maximum similarity score to 

any word in the other text B (or A) is used. An 

idf-weighted average is then computed as shown 

in the equation below. 

 

)

}
2

{
)(

}
2

{
)}(*)1,(max{

}
1

{
)(

}
1

{
)}(*)2,(max{

(
2

1
)2,1(
















Tw
widf

Tw
widfTwSim

Tw
widf

Tw
widfTwSim

TTsim

 

 

The word-to-word similarity function sim(w, 

T) in the equation above can be instantiated to 

any word-to-word similarity measure (e.g. 

WordNet similarities or Latent Semantic Analy-

sis). The vast majority of word-to-word similari-

ty measures that rely on WordNet are concept-to-

concept measures and to be able to use them one 

must map words in the input texts onto concepts 

in WordNet, i.e. word sense disambiguation 

(WSD) is needed. As of this writing, SEMILAR 

addresses the issue in two simple ways: (1) se-

165



lecting the most frequent sense for each word, 

which is sense #1 in WordNet, and (2) using all 

the senses for each word and then take the max-

imum (or average) of the relatedness scores for 

each pair of word senses. We label the former 

method as ONE (sense one), whereas the latter is 

labeled as ALL-MAX or ALL-AVG (all senses 

maximum score or all senses average score, re-

spectively). Furthermore, most WordNet-based 

measures only work within a part-of-speech cat-

egory, e.g. only between nouns. 

Other types of word-to-word measures, such 

as those based on Latent Semantic Analysis or 

Latent Dirichlet Allocation, do not have a word-

sense disambiguation challenge.  

Rus and Lintean (2012; Rus-Lintean-

Optimal Matching or ROM) proposed an opti-

mal solution for text-to-text similarity based on 

word-to-word similarity measures. The optimal 

lexical matching is based on the optimal assign-

ment problem, a fundamental combinatorial op-

timization problem which consists of finding a 

maximum weight matching in a weighted bipar-

tite graph.  

Given a weighted complete bipartite graph 

, where edge  has weight 

, the optimal assignment problem is to 

find a matching M from X to Y with maximum 

weight. 

A typical application is about assigning a 

group of workers, e.g. words in text A in our 

case, to a set of jobs (words in text B in our case) 

based on the expertise level, measured by 

, of each worker at each job. By adding 

dummy workers or jobs we may assume that X 

and Y have the same size, n, and can be viewed 

as   and Y = . 

In the semantic similarity case, the weight  

is the word-to-word similarity between a word x 

in text A and a word y in text B.  

The assignment problem can also be stated as 

finding a permutation  of {1, 2, 3, … , n} for 

which  is maximum. Such an 

assignment is called optimum assignment. The 

Kuhn-Munkres algorithm (Kuhn, 1955) can find 

a solution to the optimum assignment problem in 

polynomial time. 

Rus and colleagues (Rus et al., 2009; Rus & 

Graesser, 2006; Rus-Syntax-Negation or RSN) 
used a lexical overlap component combined with 

syntactic overlap and negation handling to com-

pute an unidirectional subsumption score be-

tween two sentences, T (Text) and H (Hypothe-

sis), in entailment recognition and student input 

assessment in Intelligent Tutoring Systems. Each 

text is regarded as a graph with words as 

nodes/vertices and syntactic dependencies as 

edges. The subsumption score reflects how much 

a text is subsumed or contained by another. The 

equation below provides the overall subsumption 

score, which can be averaged both ways to com-

pute a similarity score, as opposed to just the 

subsumption score, between the two texts.  

 

2

)
_#

)1(1(
)

||

),(max

||

),(max

(),(

relneg

hE

eHh
E

tEhEmatcheTtE

hV

vHh
V

tVhVmatchvTtV

HTsubsump





 




 







 

The lexical component can be used by itself 

(given a weight of 1 with the syntactic compo-

nent given a weight of 0) in which case the simi-

larity between the two texts is just a composi-

tional extension of word-to-word similarity 

measures. The match function in the equation 

can be any word-to-word similarity measure in-

cluding simple word match, WordNet similarity 

measures, LSA, or LDA-based similarity 

measures. 

Fernando and Stevenson (FST; 2008) pro-

posed a method in which similarities among all 

pairs of words are taken into account for compu-

ting the similarity of two texts. Each text is rep-

resented as a binary vector (1 – the word occurs 

in the text; 0 – the word does not occur in the 

text). They use a similarity matrix operator W 

that contains word-to-word similarities between 

any two words. 

||||

),( 







ba

T
bWa

basim  

Each element wij represents the word-level 

semantic similarity between word ai in text A 

and word bj in text B. Any word-to-word seman-

tic similarity measure can be used. 

Lintean and Rus (2010; weighted-LSA or 

wLSA) extensively studied methods for semantic 

similarity based on Latent Semantic Analysis 

(LSA; Landauer et al., 2006). LSA represents 

words as vectors in a 300-500 dimensional LSA 

space. An LSA vector for larger texts can be de-

rived by vector algebra, e.g. by summing up the 

individual words’ vectors. The similarity of two 

texts A and B can be computed using the cosine 

(normalized dot product) of their LSA vectors. 

Alternatively, the individual word vectors can be 

166



combined through weighted sums. Lintean and 

Rus (2010) experimented with a combination of 

3 local weights and 3 global weights. All these 

versions of LSA-based text-to-text similarity 

measures are available in SEMILAR. 

SEMILAR also includes a set of similarity 

measures based on the unsupervised method La-

tent Dirichlet Allocation (LDA; Blei, Ng, & 

Jordnan, 2003; Rus, Banjade, & Niraula, 

2013). LDA is a probabilistic generative model 

in which documents are viewed as distributions 

over a set of topics (θd - text d’s distribution over 

topics) and topics are distributions over words (φt 

– topic t’s distribution over words). That is, each 

word in a document is generated from a distribu-

tion over words that is specific to each topic. 

A first LDA-based semantic similarity meas-

ure among words would then be defined as a dot-

product between the corresponding vectors rep-

resenting the contributions of each word to a top-

ic (φt(w) – represents the probability of word w 

in topic t). It should be noted that the contribu-

tions of each word to the topics does not consti-

tute a distribution, i.e. the sum of contributions is 

not 1. Assuming the number of topics T, then a 

simple word-to-word measure is defined by the 

formula below. 

 

  
 

 

 

More global text-to-text similarity measures could 

be defined in several ways as detailed next.  
Because in LDA a document is a distribution 

over topics, the similarity of two texts needs to 

be computed in terms of similarity of distribu-

tions. The Kullback-Leibler (KL) divergence 

defines a distance, or how dissimilar, two distri-

butions p and q are as in the formula below. 

 

 

 

 

 

If we replace p with θd (text/document d’s dis-

tribution over topics) and q with θc 

(text/document c’s distribution over topics) we 

obtain the KL distance between two documents 

(documents d and c in our example). The KL 

distance has two major problems. In case qi is 

zero KL is not defined. Then, KL is not symmet-

ric. The Information Radius measure (IR) solves 

these problems by considering the average of pi 

and qi as below. Also, the IR can be transformed 

into a symmetric similarity measure as in the fol-

lowing (Dagan, Lee, & Pereira, 1997): 

 
 
 
The Hellinger and Manhattan distances be-

tween two distributions are two other options 
that avoid the shortcomings of the KL distance. 
Both are options are implemented in SEMILAR. 

LDA similarity measures between two docu-

ments or texts c and d can also include similarity 

of topics. That is, the text-to-text similarity is 

obtained multiplying the similarities between the 

distribution over topics (θd and θc) and distribu-

tion over words (φt1 and φt2). The similarity of 

topics can be computed using the same methods 

illustrated above as the topics are distributions 

over words (for all the details see Rus, Banjade, 

& Niraula, 2013). 

The last semantic similarity method presented 

in this paper is based on the Quadratic Assign-

ment Problem (QAP). The QAP method aims at 

finding an optimal assignment from words in text 

A to words in text B, based on individual word-

to-word similarity measures, while simultaneous-

ly maximizing the match between the syntactic 

dependencies of the words. 

The Koopmans-Beckmann (1957) formulation 

of the QAP problem best fits this purpose. The 

goal of the original QAP formulation, in the do-

main of economic activity, was to minimize the 

objective function QAP shown below where ma-

trix F describes the flow between any two facili-

ties, matrix D indicates the distances between 

locations, and matrix B provides the cost of lo-

cating facilities to specific locations. F, D, and B 

are symmetric and non-negative. 

 











n

i

n

i ii
b

n

j ji
djifBDFQAP

1 1 )(,1 )()(
,),,(min   

 

The fi,j term denotes the flow between facili-

ties i and j which are placed at locations π(i) and 

π(j), respectively. The distance between these 

locations is dπ(i)π(j). In our case, F and D describe 

dependencies between words in one sentence 

while B captures the word-to-word similarity 

between words in opposite sentences. Also, we 

have weighted each term in the above formula-

tion and instead of minimizing the sum we are 

maximizing it resulting in the formulation below.  
 











n

i

n

i ii
b

n

j ji
djifBDFQAP

1 1 )(,
)1(

1 )()(
,),,(max   

 





T

t

tt vwvwwwLDA
1

)()(),(2 

),(
10),(

dcIR
qpSIM








T

i i

i
i

q

p
pqpKL

1

log),(

167



5 Discussion and Conclusions 

The above methods were experimented with on 
various datasets for paraphrase, entailment, and 
elaboration. For paraphrase identification, the 
QAP method provides best accuracy results 
(=77.6%) on the test subset of the Microsoft Re-
search Paraphrase corpus, one of the largest par-
aphrase datasets. 
 Due to space constraints, we have not de-
scribed all the features available in SEMILAR. 
For a complete list of features, latest news, refer-
ences, and updates of the SEMILAR toolkit 
along with downloadable resources including 
software and data files, the reader can visit this 
link: www.semanticsimilarity.org. 
 

Acknowledgments 

This research was supported in part by Institute 

for Education Sciences under award 

R305A100875. Any opinions, findings, and con-

clusions or recommendations expressed in this 

material are solely the authors’ and do not neces-

sarily reflect the views of the sponsoring agency. 

References  

Androutsopoulos, I. & Malakasiotis, P. 2010. A sur-

vey of paraphrasing and textual entailment meth-

ods. Journal of Artificial Intelligence Research, 

38:135-187. 

Agirre, E., Cer, D., Diab, M., & Gonzalez-Agirre, A. 

(2012). SemEval-2012 Task 6: A Pilot on Semantic 

Textual Similarity, First Joint Conference on Lexi-

cal and Computational Semantics (*SEM), Mon-

treal, Canada, June 7-8, 2012. 

Blei, D.M., Ng, A.Y., & Jordan, M.I. 2003. Latent 

dirichlet allocation, The Journal of Machine Learn-

ing Research 3, 993-1022. 

Corley, C., & Mihalcea, R. (2005). Measuring the 

Semantic Similarity of Texts. In Proceedings of the 

ACL Workshop on Empirical Modeling of Seman-

tic Equivalence and Entailment. Ann Arbor, MI. 

Dagan, I., Glickman, O., & Magnini, B. (2004). The 

PASCAL Recognising textual entailment Chal-

lenge. In Quinorero-Candela, J.; Dagan, I.; Magni-

ni, B.; d'Alche-Buc, F. (Eds.), Machine Learning 

Challenges. Lecture Notes in Computer Science, 

Vol. 3944, pp. 177-190, Springer, 2006. 

Dolan, B., Quirk, C., & Brockett, C. (2004). Unsuper-

vised construction of large paraphrase corpora: Ex-

ploiting massively parallel news sources. In Pro-

ceedings of the 20
th

 International Conference on 

Computational Linguistics (COLING-2004), Gene-

va, Switzerland. 

Fernando, S. & Stevenson, M. (2008). A semantic 

similarity approach to paraphrase detec-

tion, Computational Linguistics UK (CLUK 2008) 

11th Annual Research Colloquium. 

Lintean, M., Moldovan, C., Rus, V., & McNamara D. 

(2010). The Role of Local and Global Weighting in 

Assessing the Semantic Similarity of Texts Using 

Latent Semantic Analysis. Proceedings of the 23rd 

International Florida Artificial Intelligence Re-

search Society Conference. Daytona Beach, FL. 

Lintean, M. (2011). Measuring Semantic Similarity: 

Representations and Methods, PhD Thesis, De-

partment of Computer Science, The University of 

Memphis, 2011.  

Ibrahim, A., Katz, B., & Lin, J. (2003). Extracting 

structural paraphrases from aligned monolingual 

corpora In Proceedings of the Second International 

Workshop on Paraphrasing, (ACL 2003). 

Iordanskaja, L., Kittredge, R., & Polgere, A. (1991). 

Natural Language Generation in Artificial Intelli-

gence and Computational Linguistics. Lexical se-

lection and paraphrase in a meaning-text genera-

tion model, Kluwer Academic. 

McCarthy, P.M. & McNamara, D.S. (2008). User-

Language Paraphrase Corpus Challenge 

https://umdrive.memphis.edu/pmmccrth/public/Par

aphraseCorpus/Paraphrase site.htm. Retrieved 

2/20/2010 online, 2009. 

Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). 

WordNet::Similarity - Measuring the Relatedness 

of Concepts, In the Proceedings of the Nineteenth 

National Conference on Artificial Intelligence 

(AAAI-04), pp. 1024-1025, July 25-29, 2004, San 

Jose, CA (Intelligent Systems Demonstration). 

Rus, V., Lintean M., Graesser, A.C., & McNamara, 

D.S. (2009). Assessing Student Paraphrases Using 

Lexical Semantics and Word Weighting. In Pro-

ceedings of the 14
th

 International Conference on 

Artificial Intelligence in Education, Brighton, UK. 

Rus, V., Lintean, M., Moldovan, C., Baggett, W., 

Niraula, N., Morgan, B. (2012). The SIMILAR 

Corpus: A Resource to Foster the Qualitative Un-

derstanding of Semantic Similarity of Texts, In 

Semantic Relations II: Enhancing Resources and 

Applications, The 8th Language Resources and 

Evaluation Conference (LREC 2012), May 23-25, 

Instanbul, Turkey. 

Rus, V., Banjade, R., & Niraula, N. (2013). Similarity 

Measures based on Latent Dirichlet Allocation, 

The 14
th

 International Conference on Intelligent 

Text Procesing and Computational Linguistics, 

March 24-30, 2013, Samos, Greece. 

 

168


