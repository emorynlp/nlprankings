




















































Interpretable Structure Induction via Sparse Attention


Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 365–367
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

365

Interpretable Structure Induction Via Sparse Attention

Ben Peters† Vlad Niculae† and André F.T. Martins†‡

†Instituto de Telecomunicações, Lisbon, Portugal
‡Unbabel, Lisbon, Portugal

benzurdopeters@gmail.com, vlad@vene.ro, andre.martins@unbabel.com.

1 Introduction

Neural network methods are experiencing wide

adoption in NLP, thanks to their empirical per-

formance on many tasks. Modern neural ar-

chitectures go way beyond simple feedforward

and recurrent models: they are complex pipelines

that perform soft, differentiable computation in-

stead of discrete logic. Inspired by pioneering

work by, e.g. Kohonen et al. (1981); Das et al.

(1992); Schmidhuber (1992), such modern dif-

ferentiable architectures include neural memories

(Sukhbaatar et al., 2015) and attention mecha-

nisms (Bahdanau et al., 2015). The price of such

soft computing is the introduction of dense depen-

dencies, which make it hard to disentangle the pat-

terns that trigger a prediction. Our recent work on

sparse and structured latent computation (Mar-

tins and Astudillo, 2016; Niculae and Blondel,

2017; Niculae et al., 2018; Malaviya et al., 2018)

presents a promising avenue for enhancing inter-

pretability of such neural pipelines. Through this

extended abstract, we aim to discuss and explore

the potential and impact of our methods.

The principle of parsimony suggests that sim-

pler explanations are more plausible and inter-

pretable. Our perspective is similar to prior

work on regularizing model weights (Hastie et al.,

2015), but with a twist: instead of model sparsity

that tells us which “static” groups of variables are

relevant for a task, we now have a “dynamic” form

of sparsity that tells us, for a particular input ob-

ject, where we should attend to produce a decision.

• sparsity: shrinking probabilities to zero to

prune entire parts of the input when explaining

a prediction (Martins and Astudillo, 2016);

• regularization: injecting prior assumptions,

such as that neighbouring words should be fused

together (Niculae and Blondel, 2017);

• constraints: constraining probabilities within

lower and upper bounds, to prevent words

from receiving too much or too little attention

(Malaviya et al., 2018);

• structure: learning latent structure predictors

(e.g. aligners or parsers), to induce a compact

representation as a small, interpretable set of

global structures (Niculae et al., 2018).

2 Attention Mechanisms

The key background for our work is the concept

of attention. Attention mechanisms and mem-

ory networks are able to “point” to relevant items

(e.g. words or pixels) that determine the final pre-

diction, approximating a discrete choice (argmax)

with a soft, differentiable one (softmax). Let H =
[h1, . . . ,hL] ∈ R

D×L be a matrix whose columns

are vectors encoding the L different choices (for
example, words in a sentence). An attention mech-

anism maps a H and a control state s to a proba-

bility distribution p ∈ △L over the L choices.1

This can be split into (i) generating scores for

each choice, e.g., zi = v
⊤tanh(Whi + Us)

for i ∈ {1, . . . , L} and (ii) mapping the scores
to a probability distribution. Common attention

uses (Bahdanau et al., 2015; Luong et al., 2015)

p = softmax(z), i.e., pi = exp(zi)/
∑

j exp(zj).
Since softmax is strictly positive, this leads to

dense probability distributions. However, putting

nonzero weight on every choice is not ideal for in-

terpretability (Fig. 1, center); instead, we explore

sparse selection, identifying a small set of choices

responsible for a prediction. Niculae and Blondel

(2017) proposed the general family

ΠΩ(z) = argmax
p∈△L

z⊤p− Ω(p), (1)

recovering softmax for Ω(p) = −
∑

j pj log pj .

1We denote by △L = {p ∈ RL |
∑

L

i=1
pi = 1, pi ≥

0, ∀i} the (L− 1)-dimensional probability simplex.

mailto:benzurdopeters@gmail.com
mailto:vlad@vene.ro
mailto:andre.martins@unbabel.com


366

rus
sia
n

de
fen
se

mi
nis
ter
iva
no
v

cal
led
sun

da
y forthe

cre
ati
on of ajoin

t
fro
nt for

com
ba
tin
g

glo
ba
l

ter
ror
ism

.

russian
defense
minister

calls
for

joint
front

against
terrorism
<EOS>

fusedmax

rus
sia
n

de
fen
se

mi
nis
ter
iva
no
v

cal
led
sun

da
y forthe

cre
ati
on of ajoin

t
fro
nt for

com
ba
tin
g

glo
ba
l

ter
ror
ism

.

softmax

rus
sia
n

de
fen
se

mi
nis
ter
iva
no
v

cal
led
sun

da
y forthe

cre
ati
on of ajoin

t
fro
nt for

com
ba
tin
g

glo
ba
l

ter
ror
ism

.

sparsemax

Figure 1: Attention weights for a sequence-to-sequence sentence compression instance. Traditional softmax attention (middle)
yields dense weights, which are less interpretable than the sparse weights from sparsemax (right) or fusedmax (left); the latter
further enhances interpretability by clustering probabilities of adjacent words. Image courtesy of Niculae and Blondel (2017).

Sparse attention. Martins and Astudillo (2016)

proposed sparsemax, which replaces softmax with

a Euclidean projection, remaining differentiable

while also yielding sparse probabilities. This can

be obtained by setting Ω = 1
2
‖ · ‖2

2
in Eqn 1. The

resulting probabilities are substantially more inter-

pretable, as the contribution of irrelevant words is

now shrunk to exactly 0 (Fig. 1, right).

Regularized attention. Parsimony goes beyond

sparsity: prior assumptions may encourage se-

lecting groups or clusters with equal proba-

bility. Niculae and Blondel (2017) propose

two linguistically-motivated regularized attention

mechanisms: fusedmax, which tends to group ad-

jacent words together, and oscarmax, which may

cluster non-adjacent words, suitable for languages

with flexible word order. Such mechanisms can

select interpretable segments (Fig. 1, left).

Constrained attention. Some forms of parsi-

mony must be strictly enforced using constraints,

rather than simply encouraged via regulariza-

tion. One such constraint is to add an upper

bound to the cumulative attention an input vari-

able may receive. This can be done using con-

strained softmax (Martins and Kreutzer, 2017)

or its sparse analogue, constrained sparsemax

(Malaviya et al., 2018). Constraining attention

weights can be interpreted as specifying the fertil-

ity (Brown et al., 1993) of the alignments between

the source and target, in machine translation.

3 Structured Attention

In this section, we consider combinatorial repre-

sentations. Across application domains, but es-

pecially in NLP, many objects of interest can be

represented by such structures: syntactic and de-

pendency trees, sequential labellings, alignments.

apoliceofficerwatchesasituationclosely

a

gentleman

overlooking

a

neighborhood

situation

.

a

po
lic

e

of
fic

er

w
at

ch
es a

si
tu

at
io

n

cl
os

el
y .

a

gentleman

overlooking

a

neighborhood

situation

.

(a) sequence

a

police

officer

watches

a

situation

closely

.

a

po
lic

e

of
fic

er

w
at

ch
es a

si
tu

at
io

n

cl
os

el
y .

a

gentleman

erlooking

a

neighborhood

situation

.

(b) matching

Figure 2: Structured alignment on SNLI (Niculae et al.,
2018). The premise is on the y-axis, the hypothesis on the
x-axis. Sequential alignment encourages monotonic align-
ments, matching induces a single symmetrical alignment.

Allowing hidden layers to output structured repre-

sentations can be valuable for modelling perspec-

tive but also for interpretability: discrete structures

provide organized representations, in contrast to

unstructured vectors of neuron activations.

SparseMAP (Niculae et al., 2018) allows han-

dling discrete structures within end-to-end differ-

entiable neural networks, able to automatically se-

lect only a few global structures. On natural lan-

guage inference, for a word-to-word alignment

joint attention mechanism, SparseMAP can induce

structured alignments as illustrated in Fig. 2.

4 Conclusion

Building upon the principle of parsimony, we pro-

pose sparse, regularized, constrained and struc-

tured hidden layers. We seek to discuss the poten-

tials of these strategies with an expert community

on black-box interpretability.



367

Acknowledgments

This work was supported by the European Re-

search Council (ERC StG DeepSPIN 758969) and

by the Fundação para a Ciência e Tecnologia

through contract UID/EEA/50008/2013.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.

Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.
Learning context-free grammars: Capabilities and
limitations of a recurrent neural network with an ex-
ternal stack memory. In Proceedings of The Four-
teenth Annual Conference of Cognitive Science So-
ciety. Indiana University, page 14.

Trevor Hastie, Robert Tibshirani, and Martin Wain-
wright. 2015. Statistical Learning with Spar-
sity: The Lasso and Generalizations. Chapman &
Hall/CRC.

Teuvo Kohonen, Erkki Oja, and Pekka Lehtio. 1981.
Storage and processing of information in distributed
associative memory systems. Parallel Models of As-
sociative Memory, pages 129–167.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proc. of EMNLP.

Chaitanya Malaviya, Pedro Ferreira, and André F. T.
Martins. 2018. Sparse and constrained attention for
neural machine translation. In Proc. of ACL.

André F. T. Martins and Ramón Astudillo. 2016. From
softmax to sparsemax: A sparse model of attention
and multi-label classification. In Proc. of ICML.

André FT Martins and Julia Kreutzer. 2017. Learn-
ing what’s easy: Fully differentiable neural easy-first
taggers. In Proc. of EMNLP, pages 349–362.

Vlad Niculae and Mathieu Blondel. 2017. A regular-
ized framework for sparse and structured neural at-
tention. In Proc. of NIPS.

Vlad Niculae, André F. T. Martins, Mathieu Blondel,
and Claire Cardie. 2018. SparseMAP: Differen-
tiable sparse structured inference. In Proc. of ICML.

Jürgen Schmidhuber. 1992. Learning to control fast-
weight memories: An alternative to dynamic recur-
rent networks. Neural Computation, 4(1):131–139.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Proc. of NIPS.

https://web.stanford.edu/~hastie/StatLearnSparsity/
https://web.stanford.edu/~hastie/StatLearnSparsity/
https://arxiv.org/abs/1705.07704
https://arxiv.org/abs/1705.07704
https://arxiv.org/abs/1705.07704
https://arxiv.org/abs/1802.04223
https://arxiv.org/abs/1802.04223

