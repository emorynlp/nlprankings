



















































Detecting Deceptive Groups Using Conversations and Network Analysis


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 857â€“866,

Beijing, China, July 26-31, 2015. cÂ©2015 Association for Computational Linguistics

Detecting Deceptive Groups Using Conversations and Network Analysis

Dian Yu1, Yulia Tyshchuk2, Heng Ji1, William Wallace2

1Computer Science Department, Rensselaer Polytechnic Institute
2Department of Industrial and Systems Engineering, Rensselaer Polytechnic Institute

1,2{yud2,tyshcy,jih,wallaw}@rpi.edu

Abstract

Deception detection has been formulated
as a supervised binary classification prob-
lem on single documents. However, in
daily life, millions of fraud cases involve
detailed conversations between deceivers
and victims. Deceivers may dynamically
adjust their deceptive statements accord-
ing to the reactions of victims. In addition,
people may form groups and collaborate
to deceive others. In this paper, we seek to
identify deceptive groups from their con-
versations. We propose a novel subgroup
detection method that combines linguis-
tic signals and signed network analysis for
dynamic clustering. A social-elimination
game called Killer Game is introduced as a
case study1. Experimental results demon-
strate that our approach significantly out-
performs human voting and state-of-the-
art subgroup detection methods at dynam-
ically differentiating the deceptive groups
from truth-tellers.

1 Introduction

Deception generally entails messages and infor-
mation intentionally transmitted to create a false
conclusion (Buller et al., 1994). Deception detec-
tion is an important task for a wide range of ap-
plications including law enforcement, intelligence
gathering, and financial fraud. Most of the previ-
ous work (e.g., (Ott et al., 2011; Feng et al., 2012))
focused on content analysis of a single document
in isolation (e.g., a product review). The promot-
ers of a product may post fake complimentary re-
views, while their competitors may hire people to
write fake negative reviews (Ott et al., 2011).

1The data set is publicly available for research purposes
at: http://nlp.cs.rpi.edu/data/killer.zip

However, when we want to detect deception
from text or voice conversations, the deception be-
havior may be affected by the following factors be-
yond textual statements.

1. Dynamic. Recent research in social science
suggests that deception communication is dy-
namic and involves interactions among peo-
ple (e.g., (Buller and Burgoon, 1996)). Addi-
tionally, the research postulates that humanâ€™s
capacity to learn by observation enables him
to acquire large, integrated units of behav-
ior by example (Bandura, 1971). Therefore,
a personâ€™s behavior concerning deception or
truth-telling can change constantly, while he
learns from othersâ€™ statements during conver-
sations.

2. Global. People may form groups for purpose
of deception. Research in social psychology
has shown that an individualâ€™s object-related
behavior may be affected by the attitudes of
other people due to group dynamics (Fried-
kin, 2010).

Recent studies typically have been conducted
over â€œstaticâ€ written or oral deceptive statements.
There is no obligatory requirement for communi-
cation between the author and the readers of these
statements (Yancheva and Rudzicz, 2013). As a
result, a victim of deception tends to trust the sto-
ry mainly based on the statement he reads (Ott et
al., 2011). However, in daily life, millions of fraud
cases involve detailed conversations between de-
ceivers and victims. A deceiver may make a state-
ment, which is partially true in order to deceive
or mislead victims and adjust his deceptive strate-
gies based on the reactions of victims (Zhou et al.,
2004). Therefore, it is more challenging to identity
a deceiver in an interactive process of deception.

Most deception detection research addressed in-
dividual deceivers, but deceivers often act in pairs
or larger groups (Vrij et al., 2010). The interac-

857



Identify a playerâ€™s 
attitude toward other 

players based on  
his statement during 

each round 

Clustering 
â‘  

â‘¡ 

â‘¢ 

 
 
 
 

Subgroups 

â‘  

â‘¡ 

â‘¢ 

 
 
 

Signed Network  
(each round) 

1 
1 

-1 
-1 

-1 
-1 

â‘  

â‘¡ 
â‘¢ 

 
 
 
 
 
 

Player Attitude Profile  
(each round) 

 

â‘  

â‘¡ 

â‘¢ 

â‘  â‘¡ â‘¢ 

1  âˆ’1 1 

âˆ’1 1 âˆ’1 

1 âˆ’1 1 

Partition 

 
 
 
 

Subgroups 

â‘  

â‘¡ 

â‘¢ 

Cluster  
Ensembles 

 
 
 
 

Subgroups 

â‘  

â‘¡ 

â‘¢ 

Figure 1: Deceptive group detection for a single round.

tions within a deceptive group have been ignored.
For example, a product review from a deceiver
may be supported by his teammates so that his
deceptive comments can be read by more poten-
tial buyers. In this case, we can identify a decep-
tive group based on their collaborations and com-
mon characteristics, which is more promising than
the typical methods of classifying individual state-
ments as deceptive or trustworthy.

In order to identify deceptive groups by analyz-
ing the evolution of a personâ€™s deception strategy
during his interactions with victims and the inter-
actions within the deceptive group from conver-
sations, we use a social-elimination game called
Killer Game which contains the ground-truth of
subgroups.

The killer game has many variants that involve
different roles and skills. We choose a classical
version played by three roles/teams: detectives, c-
itizens, and killers. The role of each player (game
participant) is randomly assigned by a third-party
game judge. Every killer/detective is given the i-
dentities of his teammates. There are two alter-
nating phases of the game: â€œnightâ€, when killer-
s may covertly â€œmurderâ€ a player and detectives
may learn one playerâ€™s role; and â€œdayâ€, when sur-
viving players are informed of who was killed last
â€œnightâ€ and then asked to speculate about the roles
of other surviving players. Before a â€œdayâ€ ends,
every surviving player should vote for a suspect.
The candidate with the most votes is eliminated. A
playerâ€™s identity is not exposed after his â€œdeathâ€.
The game continues until all killers have been e-
liminated or all detectives have been killed. The
killers are treated as deceivers, and citizens and
detectives as truth-tellers.

In this paper, we present an unsupervised ap-
proach for differentiating the deceptive groups

from truth-tellers in a game. During each round,
we use Natural Language Processing (NLP) tech-
niques to identify a playerâ€™s attitude toward other
players (Section 2), which are used to construc-
t a vector of attitudes for each surviving player
(Section 3.1) and a signed social network repre-
sentation (Section 3.2) for the discussions. Then
we use a clustering algorithm to cluster the atti-
tude vector space and obtain results for each round
(Section 3.1). We also implement a greedy op-
timization algorithm to partition the singed net-
work based on the attitude clustering result (Sec-
tion 3.2). Finally, we apply a pairwise-similarity
approach that makes use of the predicted co-
occurrence relations between players to combine
all results from each round (Section 3.3). Figure 1
provides an overview of our system pipeline.

The major novel contributions of this paper are
as follows.

â€¢ This is the first study to investigate conversa-
tions and deceptive groups for computerized
deception detection.
â€¢ The proposed clustering technique is shown

to be successful in separating deceptive
groups from truth-tellers.
â€¢ The method can be applied to dynamically

detect subgroups in a network with discus-
sants who tend to change their opinions.

2 Attitude Identification

In this section, we describe how we take a playerâ€™s
statement in a single round as input to extract his
attitudes toward other players and represent them
by an attitude 3-tuple (speaker, target, polarity)
list. For this work, the polarity of attitudes (Bal-
ahur et al., 2009) can be positive (1), negative (-1)
or neutral (0). A game log from a single round

858



will be used as our illustrative example, as shown
in Figure 2.

as shown in Figure 2.

(1), negative (-1) or neutral (0). A game log
from a single round will be used as our illustrative
example, as shown in Figure 2.

tuple list is: [(16, 16, +1), (16, 11, -1), (16, 2, -1),
(16, 1, 0), (16, 3, 0), . . . , (16, 15, 0)].

target, polarity)) list. For this work, the polarity
of attitudes can be positive (1), negative (-1) and
neutral (0). A game log from a single round,
as shown in Figure 2, will be used as our walk-
through example. For instance, given Player 16â€™s
statement, its attitude tuple list is: [(16, 16, +1),
(16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3, 0), . . . ,
(16, 15, 0)].

target, polarity)) list. For this work, the polarity
of attitudes can be positive (1), negative (-1) and
neutral (0). A game log from a single round,
as shown in Figure 2, will be used as our walk-
through example. For instance, given Player 16â€™s
statement, its attitude tuple list is: [(16, 16, +1),
(16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3, 0), . . . ,
(16, 15, 0)].

represents them by an attitude 3-tuple ((speaker,
target, polarity)) list. For this work, the polarity
of attitudes can be positive, negative and neutral.
A game log from a single round, as shown in
Figure 2, will be used as our walk-through exam-
ple. For instance, given Player 16â€™s statement, its
attitude tuple list is: [(16, 16, +1), (16, 11, -1), (16,
2, -1), (16, 1, 0), (16, 3, 0), . . . , (16, 15, 0)].

Combining Group Conversations and Signed Network Analysis for
Deception Detection

Abstract

In previous studies, deception detection was
formulated as a traditional binary classification
problem. For example, given a product review, su-
pervised learning methods could predict whether
it was deceptive or genuine with high accuracy.
However, in a daily life, millions of fraud cases
involve detailed conversations between criminals
and victims. Deceivers may adjust their decep-
tive statements according to the reactions of their
victims rather than simply making a single state-
ment. In addition, people may form a group and
collaborate to deceive others. In this paper, we
aim to identify a deceptive group from dynamic
conversations. We propose a novel subgroup de-
tection method that combines content and signed
network analyses for dynamic clustering. A social
elimination game called Killer Game is introduced
as a case study. Experimental results demonstrate
that our approach significantly outperforms hu-
man voting and state-of-the-art subgroup detection
methods at dynamically differentiating the decep-
tive group from the people telling truths.

1 Introduction

Deception generally entails messages and infor-
mation intentionally transmitted to create a false
conclusion (Buller et al., 1994). Deception detec-
tion is an important task for a wide range of ap-
plications including law enforcement, intelligence
gathering, and financial fraud. Most of the previ-
ous work (e.g., (Ott et al., 2011; Feng et al., 2012))
focused on a content analysis of a single document
in isolation (e.g., hotel reviews). However, when
we aim to detect deceptions from conversations,
the deception behavior may be affected by other
factors beyond textual statements. These factors
can be both

1. Dynamic. The interpersonal deception
theory (Buller and Burgoon, 1996) suggests

that deception communication is not static
but rather dynamic and involves interaction
between people. The social learning
theory (Bandura, 1971) states that humanâ€™s
capacity to learn by observation enables
him/her to acquire large, integrated units of
behavior by example. Therefore, a personâ€™s
behavior on deception or truth-telling can
change constantly, while s/he learns from
othersâ€™ statements during conversation and

2. Global. In some scenarios, people may form
groups for deception. The social influence
network theory (Friedkin, 2010) states that
individualsâ€™ object-related behaviors may be
affected by the attitudes of other persons due
to the group dynamics.

There are numerous ways to categorize decep-
tion. According to the distance between partici-
pants, we can classify deception into two types.
The first type is a face-to-face deception, where
cues include the deceiverâ€™s body movement, pulse,
facial expressions, etc.. The other type is based
on text or voice messages. For example, in
product reviews, promoters of a product may post
fake complimentary reviews, while their com-
petitors may hire people to write fake negative
reviews (Ott et al., 2011).

Recent studies have been typically conducted
over â€œstaticâ€ written or oral deceptive statements.
There is no obligatory requirement for communi-
cation between the author and readers for these
statements (Yancheva and Rudzicz, 2013). As a
result, a person being deceived tends to trust the
story mainly based on the statement she reads (Ott
et al., 2011). However, in daily life, millions
of fraud cases involve detailed conversations be-
tween criminals and victims. Deceivers may make
a statement, which is partially true in order to de-
ceive or mislead victims and adjust their deceptive
strategies based on the reactions of victims (Zhou
et al., 2004). Therefore, it is more challenging to

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since I played as a
killer. Iâ€™m a citizen. I donâ€™t want to comment on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a citizen. I will vote for
2. 6 sounds very strange and I found 6 very suspicious. I will follow the
detective 5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2. 10, 13, 5, 2 vote for 7. 9,
6 vote for 11. 2 is out.

2.1 Target and Attitude Word Identification
We start by identifying targets and attitudes
from the conversations. In the killer game, a
target is represented by his/her player ID. We
create a domain-specific vocabulary by applying
word segmentation and part-of-speech (POS)
tagging (Zhang et al., 2003) to identify game
terms from the gameâ€™s website 1 and related
discussion forums. We collected 41 terms in
total. There are two kinds of game terms:
positive and negative. Positive attitude words
include â€œcitizenâ€, â€œgood personâ€, â€œgood person
certified by the detectivesâ€, â€œdetectiveâ€, etc..
Negative attitude words include â€œkillerâ€, â€œkiller
verified by the detectivesâ€, â€œa killer who claimed
himself/herself to be a detectiveâ€, etc.. We assign
the polarity score +1, -1 to positive and negative
terms respectively.

2.2 Attitude-Target Pairing
Then we associate each attitude term with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude term from
a playerâ€™s statement during each round.

We propose a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude terms in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude terms with the speaker. We reverse the
polarity of an attitude word if it appears in a
negation context. For each attitude-target pair, we
check the POS-tag sequence between them. If
there exists an attitude term, a belief-oriented verb
such as â€œthinkâ€, â€œbelieveâ€, â€œfeelâ€, or more than
two verbs, we will discard this pair because the
statement is too subjective.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity
score is set to 0.

3 Clustering

In this section, we introduce a method to construct
an attitude profile for each player and a signed
network based on the attitude tuple list in Section 2
and combine them to handle a dynamic network
with discussants telling lies and truths, which has
not been explored previously.

1e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since I played as a
killer. Iâ€™m a citizen. I donâ€™t want to comment on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a citizen. I will vote for
2. 6 sounds very weird and I found 6 very suspicious. I will follow the
detective 5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2. 10, 13, 5, 2 vote for 7. 9,
6 vote for 11. 2 is out.

2.1 Target and Attitude Word Identification
We start by identifying targets and attitudes
from the conversations. In the killer game, a
target is represented by his/her player ID. We
create a domain-specific vocabulary by applying
word segmentation and part-of-speech (POS)
tagging (Zhang et al., 2003) to identify game
terms from the gameâ€™s website 1 and related
discussion forums. We collected 41 terms in
total. There are two kinds of game terms:
positive and negative. Positive attitude words
include â€œcitizenâ€, â€œgood personâ€, â€œgood person
certified by the detectivesâ€, â€œdetectiveâ€, etc..
Negative attitude words include â€œkillerâ€, â€œkiller
verified by the detectivesâ€, â€œa killer who claimed
himself/herself to be a detectiveâ€, etc.. We assign
the polarity score +1, -1 to positive and negative
terms respectively.

2.2 Attitude-Target Pairing
Then we associate each attitude term with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude term from
a playerâ€™s statement during each round.

We propose a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude terms in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude terms with the speaker. We reverse the
polarity of an attitude word if it appears in a
negation context. For each attitude-target pair, we
check the POS-tag sequence between them. If
there exists an attitude term, a belief-oriented verb
such as â€œthinkâ€, â€œbelieveâ€, â€œfeelâ€, or more than
two verbs, we will discard this pair because the
statement is too subjective.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity
score is set to 0.

3 Clustering

In this section, we introduce a method to construct
an attitude profile for each player and a signed
network based on the attitude tuple list in Section 2
and combine them to handle a dynamic network
with discussants telling lies and truths, which has
not been explored previously.

1e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since I played as a
killer. Iâ€™m a citizen. I donâ€™t want to comment on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a citizen. I will vote for
2. 6 sounds very weird and I found 6 very suspicious. I will follow the
detective 5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 Â· Â· Â· 10, 13, 5, 2 vote for
7 Â· Â· Â· 9, 6 vote for 11 Â· Â· Â· 2 is out.

2.1 Target and Attitude Word Identification
We start by identifying targets and attitudes
from the conversations. In the killer game, a
target is represented by his/her player ID. We
create a domain-specific vocabulary by applying
word segmentation and part-of-speech (POS)
tagging (Zhang et al., 2003) to identify game
terms from the gameâ€™s website 1 and related
discussion forums. We collected 41 terms in
total. There are two kinds of game terms:
positive and negative. Positive attitude words
include â€œcitizenâ€, â€œgood personâ€, â€œgood person
certified by the detectivesâ€, â€œdetectiveâ€, etc..
Negative attitude words include â€œkillerâ€, â€œkiller
verified by the detectivesâ€, â€œa killer who claimed
himself/herself to be a detectiveâ€, etc.. We assign
the polarity score +1, -1 to positive and negative
terms respectively.

2.2 Attitude-Target Pairing
Then we associate each attitude term with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude term from
a playerâ€™s statement during each round.

We propose a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude terms in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude terms with the speaker. We reverse the
polarity of an attitude word if it appears in a
negation context. For each attitude-target pair, we
check the POS-tag sequence between them. If
there exists an attitude term, a belief-oriented verb
such as â€œthinkâ€, â€œbelieveâ€, â€œfeelâ€, or more than
two verbs, we will discard this pair because the
statement is too subjective.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity
score is set to 0.

3 Clustering

In this section, we introduce a method to construct
an attitude profile for each player and a signed
network based on the attitude tuple list in Section 2
and combine them to handle a dynamic network
with discussants telling lies and truths, which has
not been explored previously.

1e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since I played as a
killer. Iâ€™m a citizen. 11 is suspicious and I donâ€™t want to comment on
16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a citizen. I will vote for
2. 6 sounds very weird and I found 6 very suspicious. I will follow the
detective 5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 Â· Â· Â· 10, 13, 5, 2 vote for
7 Â· Â· Â· 9, 6 vote for 11 Â· Â· Â· 2 is out.

2.1 Target and Attitude Word Identification
We start by identifying targets and attitudes from
the conversations. In the killer game, a target is
represented by his/her player ID and game terms
are regarded as attitude words. We collected

41 terms in total from the gameâ€™s website 1 and
related discussion forum. ICTCLAS (Zhang et
al., 2003) is used for word segmentation and part-
of-speech (POS) tagging (Zhang et al., 2003).
There are two kinds of game terms: positive and
negative. Positive terms include â€œcitizenâ€, â€œgood
personâ€, â€œgood person certified by the detectivesâ€,
â€œdetectiveâ€, etc.. Negative terms include â€œkillerâ€,
â€œkiller verified by the detectivesâ€, â€œa killer who
claimed himself/herself to be a detectiveâ€, etc..
We assign the polarity score +1, -1 to positive and
negative terms respectively.

2.2 Attitude-Target Pairing

Then we associate each attitude term with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude term from
a playerâ€™s statement during each round.

We develop a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude terms in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude terms with the speaker. We reverse the
polarity of an attitude word if it appears in a
negation context.

For each attitude-target pair, we check the POS
tag sequence between them. If there exists an at-
titude term, a belief-oriented verb such as â€œthinkâ€,
â€œbelieveâ€, â€œfeelâ€, or more than two verbs in the
sequence, we will discard this pair. In our task, we
assume that a target and an attitude term can form
a pair if the target is dominated by the term. (Yu
et al., 2015) showed that we can judge if a word
is dominated by any other word by analyzing the
POS tag sequence between them.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude term used
when they are mentioned, the attitude polarity
score is set to 0.

3 Clustering

In this section, we introduce a method to construct
an attitude profile for each player and a signed net-
work based on the attitude tuple list in Section 2,
and combine them to handle a dynamic network
with discussants telling lies and truths.

1e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since
I played as a killer. Iâ€™m a citizen. 11 is suspicious and
I donâ€™t want to comment on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a
citizen. I will vote for 2. 6 sounds very weird and
I found 6 very suspicious. I will follow the detective
5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a
citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 Â· Â· Â· 10, 13, 5, 2 vote for
7 Â· Â· Â· 9, 6 vote for 11 Â· Â· Â· 2 is out.

2.1 Target and Attitude Word Identification

We start by identifying targets and attitude words
from conversations. In the killer game, a target

is represented by his/her unique ID1 and game
terms are regarded as attitude words. We collected
41 terms in total from the gameâ€™s website 2 and
related discussion forum. ICTCLAS (Zhang et
al., 2003) is used for word segmentation and part-
of-speech (POS) tagging. There are two kinds
of game terms: positive and negative. Positive
terms include â€œcitizenâ€, â€œgood personâ€, â€œgood
person certified by the detectivesâ€, â€œdetectiveâ€,
etc.. Negative terms include â€œkillerâ€, â€œkiller ver-
ified by the detectivesâ€, â€œa killer who claimed
himself/herself to be a detectiveâ€, etc.. We assign
the polarity score +1, -1 to positive and negative
terms respectively.

2.2 Attitude-Target Pairing

Then we associate each attitude word with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude word from
a playerâ€™s statement during each round.

We develop a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude words in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude words with the speaker. We reverse the
polarity of an attitude word if it appears in a
negation context.

Previous methods pair a target and an atti-
tude word if they satisfy at least one dependency
rules (e.g.,(Somasundaran and Wiebe, 2009)). We
check the POS tag sequence between them. For
each attitude-target pair, if there exists an attitude
word, a belief-oriented verb such as â€œthinkâ€, â€œbe-
lieveâ€, â€œfeelâ€, or more than two verbs in the se-
quence, we will discard this pair. The assumption
is that POS tag sequences can be used to roughly
summarize dependency rules when statements are
relatively short.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity
score is set to 0. For instance, given Player 16â€™s
statement in Figure 2, its attitude tuple list is: [(16,
16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,
0), . . . , (16, 15, 0)].

1Each player has a game ID, assigned by the online game
system based on when s/he entered the game room.

2e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: citizen; D: detective; K: killer
System: First Round.
System: 15 was killed last night. 15, please leave your last words.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since
I played as a killer. Iâ€™m a citizen. 11 is suspicious and
I donâ€™t want to comment on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a
citizen. I will vote for 2. 6 sounds very weird and
I found 6 very suspicious. I will follow the detective
5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a
citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 Â· Â· Â· 10, 13, 5, 2 vote for
7 Â· Â· Â· 9, 6 vote for 11 Â· Â· Â· 2 is out.

2.1 Target and Attitude Word Identification

We start by identifying targets and attitude words
from conversations. In the killer game, a target
is represented by his/her unique ID1 and game
terms are regarded as attitude words. We collected
41 terms in total from the gameâ€™s website 2 and
related discussion forum. ICTCLAS (Zhang et
al., 2003) is used for word segmentation and part-
of-speech (POS) tagging. There are two kinds
of game terms: positive and negative. Positive
terms include â€œcitizenâ€, â€œgood personâ€, â€œgood
person certified by the detectivesâ€, â€œdetectiveâ€,
etc.. Negative terms include â€œkillerâ€, â€œkiller
verified by the detectivesâ€, â€œa killer who claimed
himself/herself to be a detectiveâ€, etc.. We assign
the polarity score +1, -1 to positive and negative
terms respectively.

2.2 Attitude-Target Pairing

Then we associate each attitude word with its
corresponding target. We remove interrogative
and exclamatory sentences and only keep the
sentences that include at least one attitude word
from a playerâ€™s statement during each round.

We develop a rule-based approach for attitude-
target pairing: if there is at least one ID in the
sentence, we associate all attitude words in that
sentence with it. Otherwise, if â€Iâ€ is the only
subject or there are no subjects at all, we associate
attitude words with the ID of the speaker. We
reverse the polarity of an attitude word if it appears
in a negation context.

Previous methods pair a target and an attitude
word if they satisfy at least one dependency rules
(e.g.,(Somasundaran and Wiebe, 2009)). We
check the POS tag sequence between them. For
each attitude-target pair, if there exists an attitude
word, a belief-oriented verb such as â€œthinkâ€,
â€œbelieveâ€, â€œfeelâ€, or more than two verbs in
the sequence, we will discard this pair. The
assumption is that POS tag sequences can be used
to roughly summarize dependency rules when
statements are relatively short.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity
score is set to 0. For instance, given Player 16â€™s
statement in Figure 2, its attitude tuple list is: [(16,

1Each player has a game ID, assigned by the online game
system based on when s/he entered the game room.

2e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (1st round).

C: CITIZEN; D: DETECTIVE; K: KILLER

System: First Round.
System: 15 was killed last night. 15, please leave your last words.
15(C): Iâ€™m a citizen. Over.
16(K): Iâ€™m a good person. 11 and 2 are suspicious.
1(K): Iâ€™m a good person. It has been a long time since I played as a
killer. Iâ€™m a citizen. 11 is suspicious and I donâ€™t want to comment
on 16â€™s statement.
2(C): Iâ€™m a detective. 6 was proved as a killer last night. Over.
3(C): I donâ€™t know 2â€™s identity. Itâ€™s hard to judge 16â€™s statement. 1
seems to be a good person. Iâ€™m a citizen.
4(C): Citizen. I cannot find a killer. I trust 2 since 2 sounds a good
person. 16 is suspicious. I regard 16 as a killer. Iâ€™m 2â€™s teammate.
5(D): Iâ€™m a detective. I verify 2â€™s identity and 2 is a killer. 13 is good.
6(C): Why do you want to attack 2? I donâ€™t understand. 14 is
suspicious.
7(K): Itâ€™s hard to define 6â€™s identity. 4 may be a citizen. I will vote
for 2. 6 sounds very weird and I found 6 very suspicious. I will
follow the detective 5 to vote for 2.
8(C): We should calm down. 7 seems to be a bad person.
9(C): 1 and 7 seem to be killers. There is no evidence to support 2 as a
detective. 3 is a citizen. 4 is possibly a detective. 6 is also good.
10(D): I agree with you. 7 must be a killer. 2 and 7 should debate.
11(C): I donâ€™t know 2 but I think 2 is good. 3 is good. There should be
one or two killers among 1, 4 and 7.
12(K): 11 sounds like a killer. 2 is a killer. Iâ€™m a citizen. Vote for 2.
13(D): 15 is a citizen. 16 is logically good. I think 1, 8, 9, 10 are OK. I
donâ€™t think 2 is a killer. I doubt 7â€™s intention. Please vote for 7.
14(D): 10, 13, 16 are good. I donâ€™t think 7 must be a killer. 2 is
obviously bad. Iâ€™m a citizen.
System: 16, 11, 14, 7, 1, 3, 8, 12, 4 vote for 2 Â· Â· Â· 10, 13, 5, 2 vote for
7 Â· Â· Â· 9, 6 vote for 11 Â· Â· Â· 2 is out.

2.1 Target and Attitude Word Identification
We start by identifying targets and attitude words
from conversations. In the killer game, a target is

represented by his/her unique ID1 and game terms
are regarded as attitude words. We collected 41
terms in total from the gameâ€™s website 2 and re-
lated discussion forum. ICTCLAS (Zhang et al.,
2003) is used for word segmentation and part-of-
speech (POS) tagging. There are two kinds of
game terms: positive and negative. Positive terms
include â€œcitizenâ€, â€œgood personâ€, â€œgood person
certified by the detectivesâ€, â€œdetectiveâ€, etc.. Neg-
ative terms include â€œkillerâ€, â€œkiller verified by the
detectivesâ€, â€œa killer who claimed himself/herself
to be a detectiveâ€, etc.. We assign the polarity s-
core +1, -1 to positive and negative terms respec-
tively.

2.2 Attitude-Target Pairing

Then we associate each attitude word with it-
s corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude word from
a playerâ€™s statement during each round.

We develop a rule-based approach for attitude-
target pairing: if there is at least one ID in the sen-
tence, we associate all attitude words in that sen-
tence with it. Otherwise, if â€Iâ€ is the only subject
or there are no subjects at all, we associate atti-
tude words with the ID of the speaker. We reverse
the polarity of an attitude word if it appears in a
negation context.

Previous methods pair a target and an atti-
tude word if they satisfy at least one dependency
rules (e.g.,(Somasundaran and Wiebe, 2009)). We
check the POS tag sequence between them. For
each attitude-target pair, if there exists an attitude
word, a belief-oriented verb such as â€œthinkâ€, â€œbe-
lieveâ€, â€œfeelâ€, or more than two verbs in the se-
quence, we will discard this pair. The assumption
is that POS tag sequences can be used to summa-
rize dependency rules when statements are rela-
tively short.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity s-
core is set to 0. For instance, given Player 16â€™s s-
tatement in Figure 2, its attitude tuple list is: [(16,
16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,
0), . . . , (16, 15, 0)].

1Each player has a game ID, assigned by the online game
system based on when s/he entered the game room.

2e.g., http://www.3j3f.com/how/

Figure 2: Killer game sample log (the 1st round).

2.1 Target and Attitude Word Identification

We start by identifying targets and attitude word-
s from conversations. In the killer game, a target
is represented by his unique ID2 and game terms
are regarded as attitude words. We collected 41
terms in total from the gameâ€™s website 3 and re-
lated discussion forum posts. ICTCLAS (Zhang
et al., 2003) is used for word segmentation and
part-of-speech (POS) tagging. There are two kind-
s of game terms: positive and negative. Posi-
tive terms include â€œcitizenâ€, â€œgood personâ€, â€œgood
person certified by the detectivesâ€ and â€œdetectiveâ€.
Negative terms include â€œkillerâ€, â€œkiller verified by
the detectivesâ€ and â€œa killer who claimed him-
self/herself to be a detectiveâ€. We assign the po-
larity score +1, -1 to positive and negative terms
respectively.

2Each player has a game ID, assigned by the online game
system based on when he entered the game room.

3e.g., http://www.3j3f.com/how/

2.2 Attitude-Target Pairing

Then we associate each attitude word with it-
s corresponding target. We remove interrogative
and exclamatory sentences and only keep the sen-
tences that include at least one attitude word from
a playerâ€™s statement during each round.

We develop a rule-based approach for attitude-
target pairing: if there is at least one ID in the sen-
tence, we associate all attitude words in that sen-
tence with it. Otherwise, if â€œIâ€ is the only subject
or there are no subjects at all, we associate atti-
tude words with the ID of the speaker. We reverse
the polarity of an attitude word if it appears in a
negation context.

Previous methods pair a target and an attitude
word if they satisfy at least one dependency rule
(e.g., (Somasundaran and Wiebe, 2009)). We
check the POS tag sequence between them. For
each attitude-target pair, if there exists an attitude
word, a belief-oriented verb such as â€œthinkâ€, â€œbe-
lieveâ€, â€œfeelâ€, or more than two verbs in the se-
quence, we will discard this pair. The assumption
is that POS tag sequences can be used to summa-
rize dependency rules when statements are rela-
tively short.

For those targets, the speaker didnâ€™t mention
or there is no positive/negative attitude word used
when they are mentioned, the attitude polarity s-
core is set to 0. For instance, given Player 16â€™s s-
tatement in Figure 2, its attitude tuple list is: [(16,
16, +1), (16, 11, -1), (16, 2, -1), (16, 1, 0), (16, 3,
0), . . . , (16, 15, 0)].

3 Clustering

Since the statements in conversations are relatively
short and concise, it is difficult to identify which
one is deceptive, even using deep linguistic fea-
tures such as the language style.

In this section, we introduce a method to con-
struct an attitude profile for each player and a
signed network based on the attitude tuple list in
Section 2, and combine them to analyze a dynam-
ic network with discussants telling lies and truths.

3.1 Clustering based on Attitude Profile

We use a vector containing numerical values to
represent each playerâ€™s attitude toward identified
targets in each round. The values correspond to
the polarity scores in a playerâ€™s attitude tuple list.
For example, the polarity score of player 16â€™s atti-
tude toward target 11 is âˆ’1 as shown in Figure 2.

859



We call this vector as the discussant attitude pro-
file (DAP) following (Abu-Jbara et al., 2012a).

Suppose there are n players who participate in
a single game. Since a playerâ€™s identity is not ex-
posed to the public after his death4, people can still
analyze the identity of a â€œdeadâ€ player. Therefore,
the number of possibly mentioned targets in each
round equals to n. Given all the statements from
m surviving players in a single round, each play-
erâ€™s DAP has n+ 1 dimensions including his vote
and thus we can have a m Ã— (n + 1) attitude ma-
trixA whereAij represents the attitude polarity of
i toward j we got from Section 2. Ai(n+1) repre-
sents iâ€™s vote.

In a certain round, given a set of m surviving
players X = {x1, x2, Â· Â· Â· , xm} to be clustered
and their respective DAPs, we can modify the Eu-
clidean metric to compute the differences in atti-
tudes and get an mÃ—m distance matrix M :

Mij =

âˆšâˆšâˆšâˆš nâˆ‘
k=1

(Aik âˆ’Ajk)2 + (2âˆ’ 2Î´Ai(n+1),Aj(n+1))2

(1)

The Kronecker delta function Î´ is:

Î´ij =

{
1 i = j
0 i 6= j (2)

We use this function to compare the votes of t-
wo players separately because a playerâ€™s vote can
be inconsistent with his previous statements. We
assume that there is a larger distance between two
players when they vote for different suspects.

A common assumption in previous research was
that a member is more likely to show a positive
attitude toward other members in the same group,
and a negative attitude toward the opposing group-
s (Abu-Jbara et al., 2012a). However, a deceiver
may pretend to be innocent by supporting those
truth-tellers and attacking his teammates, whose i-
dentities have already been exposed. Therefore,
it is not enough to judge the relationship between
two players by simply measuring the distance be-
tween their DAPs.

In addition to comparing DAPs between player-
s i and j, we also consider the attitudes of other
players toward i and j, as well as their attitudes

4Each round, the player killed by killers and the player
with the most votes are out.

toward each other. We modify Mij as follows and
show it in Figure 3:

M
â€²
ij = Mij +

âˆšâˆšâˆšâˆš mâˆ‘
k=1

(Aki âˆ’Akj)2 + (h(Aij) + h(Aji))2

(3)

where the function h detects the negative atti-
tudes. h(x) = 0 if x â‰¥ 0 and h(x) = âˆ’1 other-
wise.

We perform hierarchical clustering on the con-
densed distance matrix ofM and use the complete
linkage method to compute the distance between
two clusters (Voorhees, 1986). We set the num-
ber of clusters as 3 since there are three natural
groups in the game. We focus on separating de-
ceivers (killers) from truth-tellers (citizens and de-
tectives).

ğ‘– ğ‘— 

ğ‘– 

ğ‘— 

compare  ğ‘– 
and ğ‘—â€²s DAPs 

Figure 3: Computation of the distance between
player i and j based on the attitude matrix.

3.2 Signed Network Partition

When we computed the distance between two
players in Section 3.1, we did not consider the net-
work structure among all the players. For exam-
ple, if A supports C, B supports D and C and D
dislike each other, A and B may belong to differ-
ent groups. Thus, we propose to capture the in-
teractions in the social network to further improve
the attitude-profile-based clustering result.

We can easily convert the attitude matrix A into
a signed network by adding a directed edge iâ†’ j
between i and j if Aij 6= 0. We denote a directed
graph corresponding to a signed network as G =
(V, S,N,W ), where V is the set of nodes, S is the
set of positive edges,N is the set of negative edges
and W : (V Ã— V ) â†’ {âˆ’1, 1} is a function that
maps every directed edge to a value, W (i, j) =
Aij .

We use a greedy optimization algorithm (Dor-
eian and Mrvar, 1996) to find partitions. A criteri-
on function for an optimal partitioning procedure

860



is constructed such that positive links are dense
within groups and negative links are dense be-
tween groups. For any potential partition C, we
seek to minimize the following error function:

E(C) =
âˆ‘
CâˆˆC

[(1âˆ’ Î³)
âˆ‘
iâˆˆC
j /âˆˆC

W (i, j)Si,j âˆ’ Î³
âˆ‘

i,jâˆˆC
W (i, j)Ni,j ]

(4)

where Î³ âˆˆ [0, 1] controls the balance of the
penalty difference between putting a positive edge
across and a negative edge within a group. We re-
gard these two types of errors as equally important
and set Î³ = 0.5 for our experiments.

Initially, we use the clustering result in Sec-
tion 3.1 to partition nodes into three differen-
t groups and an error function, E, is evaluated for
that cluster. Every cluster has a set of neighbor
clusters in the cluster space. A neighbor cluster
is obtained by moving a node from one group to
another, or exchanging two nodes in two different
groups. E is evaluated for all the neighbor clusters
of the current cluster and the one with the lowest
value is set as the new cluster. The algorithm is
repeated until it finds a minimal solution5. We set
the upper limit for the number of subgroups to 3.

3.3 Cluster Ensembles

The relationships between players are dynamic
throughout the game. For example, a killer tends
to hide his identity and pretends to be friendly to
others at later stages in order to survive. Thus, it
is insufficient to rely on a single roundâ€™s discus-
sion to cluster players. In addition, for each single
round, we also need to combine the clustering re-
sults from the attitude profiles of the players and
the signed network.

In a game with information gathered from up
to r rounds, let P = {P1, P2, Â· Â· Â· , Pr} be the set
of r clusterings (partitionings) based on attitude
profiles and P

â€²
= {P â€²1, P

â€²
2, Â· Â· Â· , P

â€²
r} be the set of

r clusterings based on the signed network.
Using the co-occurrence relations between

players, we can generate a n Ã— n pairwise simi-
larity matrix T based on the information of all r
rounds:

T rij =
Î» Â· voteij + (1âˆ’ Î») Â· voteâ€²ij

rij
(5)

5Since our graphs are small, we search through all parti-
tions. We repeated 1000 times in our experiment.

where voteij , vote
â€²
ij are the number of times

that player i and j are assigned to the same cluster
in P and P

â€²
respectively. rij denotes the number

of rounds when both of them survived (rij â‰¤ r).
T rij âˆˆ [0, 1]. We assign a higher weight to the re-
sult of P1 and set Î» = 2/3 in our experiments.

Given the input in Figure 2, x3 and x4 are as-
signed to the same cluster in P1 (vote34 = 1) and
in P

â€²
1 (vote

â€²
34 = 1) respectively as shown in Fig-

ure 4. x3 and x4 co-occurred in the first round
(r34 = 1). T 134 = (2/3Ã— 1 + 1/3Ã— 1)/1 = 1.

ğ‘¥16   

ğ‘·ğŸ   

ğ‘·â€²ğŸ   

Round 1 

ğ‘¥2   

ğ‘¥11   

ğ‘¥1   

ğ‘¥7   

ğ‘¥12    
ğ‘¥14    

ğ‘¥3 

ğ‘¥15   
ğ‘¥4   

ğ‘¥5   

ğ‘¥13   

ğ‘¥10  

ğ‘¥8   
ğ‘¥9   

ğ‘¥6   

ğ‘¥2   

ğ‘¥11   

ğ‘¥14   

ğ‘¥3 

ğ‘¥15   
ğ‘¥4   

ğ‘¥5   

ğ‘¥13   

ğ‘¥10   

ğ‘¥8   
ğ‘¥9   

ğ‘¥6   

ğ‘¥16    

ğ‘¥1   

ğ‘¥7   ğ‘¥12   

KILLER CITIZEN OR DETECTIVE 

Figure 4: Example of cluster ensemble for a single
round.

We apply hierarchical clustering (Voorhees,
1986) to the similarity matrix above to obtain the
final global clustering results.

4 Experiments

4.1 Dataset Construction

We recorded 10 games from 3J3F6, one of the
most popular Chinese online killer game web-
sites 7. A screenshot of the game system inter-
face is shown in Figure 5. There are 16 partic-
ipating players per game: 4 detectives, 4 killer-
s and 8 citizens. Each player occupies a posi-
tion in 1 . All the surviving players can express
their attitudes via a voice channel using 2 , while
detectives and killers can also communicate with
teammates in their respective private team chan-
nels 3 via texts. The system provides real-time
updates on the game progress, voting results, and
so on using the public channel 4 . We manually
transcribed speech and stored the text information
in the public channel, which contains the voting
and death information. The average game length

6http://www.3j3f.com
7All data sets and resources will be made available for

research purposes upon the acceptance of the paper.

861



Game
#

Purity (%) Entropy
D N H eD eD +N D N H eD eD +N

1 68.8 75.0 75.0 68.8 75.0 0.48 0.50 0.78 0.63 0.50
2 75.0 68.8 68.8 43.8 81.3 0.71 0.69 0.81 0.73 0.43
3 43.8 81.3 56.3 75.0 75.0 0.77 0.67 0.81 0.72 0.72
4 75.0 62.5 75.0 93.8 93.8 0.78 0.68 0.74 0.28 0.28
5 62.5 75.0 81.3 75.0 75.0 0.61 0.50 0.61 0.72 0.72
6 81.3 81.3 75.0 81.3 81.3 0.64 0.38 0.74 0.60 0.60
7 81.3 75.0 81.3 81.3 87.5 0.65 0.70 0.68 0.51 0.51
8 87.5 75.0 75.0 93.8 93.8 0.41 0.73 0.78 0.23 0.23
9 75.0 43.8 75.0 81.3 87.5 0.76 0.80 0.78 0.67 0.49

10 62.5 75.0 87.5 81.3 81.3 0.78 0.60 0.51 0.61 0.67
Average 71.3 71.3 75.0 77.5 83.2 0.66 0.62 0.72 0.57 0.51

Table 1: Results on subgroup detection. D refers to DAPC, N refers to Network, H refers to Human Voting, and eD
refers to extended DAPC.

is about 76.3 minutes and there are on average 5
rounds and 411 sentences per game. Note that our
method is language-independent and could easily
be adapted to other languages.

Current Speaker: 14 

TEAM CHANNEL 

PUBLIC CHANNEL  

START END 

OUT 

1 

2 

3 

4 

Figure 5: Screenshot of the online killer game in-
terface.

4.2 Evaluation Metrics
We use two metrics to evaluate the clustering ac-
curacy: Purity and Entropy. Purity (Manning et
al., 2008) is a metric in which each cluster is as-
signed to the class with the majority vote in the
cluster, and then the accuracy of this assignmen-
t is measured by dividing the number of correctly
assigned instances by the total number of instances
N . More formally:

purity(â„¦, C) =
1
N

âˆ‘
k

maxj |wk âˆ© cj | (6)

where â„¦ = {w1, w2, Â· Â· Â· , wk} is the set of clusters
and C = {c1, c2, Â· Â· Â· , cj} is the set of classes. wk
is interpreted as the set of instances in wk and cj
is the set of instances in cj . The purity increases
as the quality of clustering improves.

Entropy (Steinbach et al., 2000) measures the
uniformity of a cluster. The entropy for all clusters

is defined by the weighted sum of the entropy of
each cluster:

Entropy = âˆ’
jâˆ‘ nj

n

iâˆ‘
P (i, j)Ã— log2P (i, j)

(7)

where P (i, j) is the probability of finding an el-
ement from the category i in the cluster j, nj is
the number of items in cluster j and n is the total
number of items in the distribution. The entropy
decreases as the quality of clustering improves.

4.3 Overall Performance

We compare our approach with two state-of-the-
art subgroup detection methods and human perfor-
mance as follows:

1. DAPC: In Section 3.1, we introduced our im-
plementation of the discussant attitude profile
clustering (DAPC) method proposed in (Abu-
Jbara et al., 2012a). In the original DAPC
method, for each opinion target, there are 3
dimensions in the feature vector, correspond-
ing to (1) the number of positive expression-
s, (2) negative expressions toward the tar-
get from the online posts and (3) the num-
ber of times the discussant mentioned the tar-
get. For our experiment, we only keep one
dimension representing the discussantâ€™s atti-
tude (positive, negative, neutral) toward the
target since a discussant attitude remains the
same in his statement within a single round.

2. Network: We also implemented the signed
network partition method for subgroup detec-
tion proposed by (Hassan et al., 2012). To
determine the number of subgroups t, we set
an upper limit of t = 3 in order to minimize
the optimization function.

862



3. Human Voting: We also compare our meth-
ods with human voting results. There are two
subgroups based on the voting results. The
players with the highest votes each round be-
long to one subgroup and the rest of the play-
ers are in the other subgroup.

Table 1 shows the overall performance of vari-
ous methods on subgroup detection and Figure 6
depicts the average performance. We can see that
our method significantly outperforms two baseline
methods and human voting. The human perfor-
mance is not satisfying, which indicates itâ€™s very
challenging even for a human to identify a deceiv-
er whose deceptive statement is mixed with plenty
of truthful opinions (Xu and Zhao, 2012).

1 Hu
man_V

oting BL_DA
PC

BL_Ne
twork EDAP

C

EDAP
C+Ne

twork

50

55

60

65

70

75

80

85

%

Method

 Purity
 Entropy

Figure 6: An overview of the average performance
of all the methods.

By extending the DAPC method (EDPAC), we
can estimate the distance between two players
more accurately by considering the attitudes of
other players toward them and their attitudes to-
ward each other. Given the log in Figure 2 as in-
put, players 5 (detective) and 7 (killer) are clus-
tered into one group when DAPC is applied s-
ince they donâ€™t have conflicting views on the i-
dentities of other players. However, 5 voted for
7 and is supported by more players compared with
7, which indicates that they are less likely to be
teammates. We can successfully separate them af-
ter re-computing the distance between them.

Adding network information provided 5.7%
further gain in Purity. In some cases, the perfor-
mance remains the same when EDAPC clustering
result is already optimal with the minimum value
of the criterion function.

4.4 Dynamic Subgroup Detection

As shown in Figure 7, the performance of our
approach improves as the game proceeds. Play-
ers seldom maintain their opinions throughout a
game. Figure 2 shows that most killers (16,1,12)
insisted that citizen 11 should be a killer except 7.
As a response to the group pressure (Asch, 1951),
7 changed his opinion and stated that 11 could be
a killer in the following round.

In reality, a discussant who participates in an
online discussion tends to change his opinion-
s about a target as he learns more information,
which shows both the necessity and importance of
the dynamic detection of subgroups. Our method
can be applied to detect subgroups dynamically by
grouping posts into multiple discussion â€œroundsâ€
based on their timestamps.

1 

Purity Entropy

50

60

70

80
%
 1st round
 1st + 2nd rounds
 all rounds

Figure 7: Average performance based on different
rounds.

5 Related Work

5.1 Opinion Analysis

Our work on mining a playerâ€™s attitude toward oth-
er players is related to opinion mining. Attitudes
and opinions are related and can be regarded as
the same in our task. Compared with the previ-
ous work (e.g.,(Qiu et al., 2011; Kim and Hovy,
2006)), the opinion words and targets in our task
are relatively easier to recognize due to the sim-
plicity of statements. Some recent work (e.g., (So-
masundaran and Wiebe, 2009; Abu-Jbara et al.,
2012a)) developed syntactic rules to pair an opin-
ion word and a target if they satisfy at least one
specific dependency rule. We use POS tag se-
quences to efficiently help us filter out irrelevant
pairs.

863



5.2 Deception Detection

Most of the previous computational work for
deception detection used supervised/semi-
supervised classification methods (Li et al.,
2013b). Besides lexical and syntactical fea-
tures (Ott et al., 2011; Feng et al., 2012; Yancheva
and Rudzicz, 2013), Feng and Hirst (2013) pro-
posed using profile compatibility to distinguish
fake and genuine reviews. Xu and Zhao (2012)
used deep linguistic features such as text genre
to detect deceptive opinion spams. Banerjee et
al. (2014) used extended linguistic signals such
as keystroke patterns. Li et al. (2013a) used topic
models to detect the difference between deceptive
and truthful topic-word distribution. Researchers
have began to realize the importance of analyzing
computer-mediated communication in deception
detection. Zhou and Sung (2008) conducted
an empirical study on deception cues using the
killer game as a task scenario and obtained many
interesting findings (e.g., deceivers send fewer
messages than truth-tellers).

Our work is most related to the work of Chit-
taranjan and Hung (2010) on detecting deceptive
roles in the Werewolf Game which is another vari-
ant of the killer game. They created a Werewolf
data set by audio-visual recording 8 games played
by 2 groups of people face-to-face and extract-
ed audio features and interaction features for their
experiments. However, we should note that non
face-to-face deception detection emphasizes ver-
bal and linguistic cues over less controllable non-
verbal communication cues (Walther, 1996).

5.3 Subgroup Detection

In online discussions, people usually split into
subgroups based on various topics. The member
of a subgroup is more likely to show positive at-
titude to the members of the same subgroup, and
negative attitude to the members of opposing sub-
groups (Abu-Jbara et al., 2012a). Previous work
also studied subgroup detection in social media
sites. Abu-Jbara et al. (2012a) constructed a dis-
cussant attitude profile (DAP) for each discussant
and then used clustering techniques to cluster their
attitudes. Hassan et al. (2012; 2012b; 2013) pro-
posed various methods to automatically construct
a signed social network representation of discus-
sions and then identify subgroups by partitioning
their signed networks. Qiu et al. (2013) applied
collaborative filtering through Probabilistic Matrix

Factorization (PMF) to generalize and improve ex-
tracted opinion matrices.

An underlying assumption of the previous work
was that a participant will not tell lies nor hide his
own stance. Moreover, their work did not take in-
to account that a personâ€™s attitude or stance will
change as he learns more by reading the com-
ments from others and acquiring more background
knowledge (Bandura, 1971). Our contribution is
that we extend the DAP method and combine it
with the signed network partition in order to clus-
ter the hidden group members. We also develop a
novel cluster ensemble approach in order to ana-
lyze the dynamic network.

6 Conclusions and Future Work

Using the killer game as a case study, we present
an effective clustering method to detect subgroups
from dynamic conversations with lies and truth-
s. This is the first work to utilize the dynam-
ics of group conversations for deception detec-
tion. Experiments demonstrated that truth-tellers
and deceptive groups are separable and the pro-
posed method significantly outperforms baseline
approaches and human voting.

Our work builds a pathway to future work in
deception detection in content-rich dynamic envi-
ronments such as electronic commerce and repeat-
ed interrogation which will require sophisticated
content and network analysis. In real-life suspects
may be interrogated about particular events on nu-
merous occasions. Our method can potentially be
modified to find criminals who act in groups based
on their statements. Other applications of this re-
search include law enforcement, financial fraud,
fraudulent ad campaigns and social engineering.

This study focuses on analyzing the verbal con-
tent in conversations. It will be interesting to study
non-verbal features such as blink rate, gaze aver-
sion and pauses (Granhag and StroÌˆmwall, 2002)
when people play this game face-to-face and com-
bine the non-verbal and verbal features for decep-
tion detection. In addition, it is worth exploring
the impact of cross-cultural analysis in detecting
deception. When attempting to detect deceit in
people of other ethnic origin than themselves, peo-
ple perform even worse in terms of lie detection
accuracy than when judging people of their own
ethnic origin (Vrij, 2000). For the future work,
we aim to use automatic prediction of deceivers to
help truth-tellers win games more easily.

864



Acknowledgement

This work was supported by the U.S. DARPA
DEFT Program No. FA8750-13-2-0041, ARL
NS-CTA No. W911NF-09-2-0053, NSF Award-
s IIS-0953149 and IIS-1523198, AFRL DREAM
project, gift awards from IBM, Google, Disney
and Bosch. The views and conclusions contained
in this document are those of the authors and
should not be interpreted as representing the of-
ficial policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.

References
A. Abu-Jbara, M. Diab, P. Dasigi, and D. Radev.

2012a. Subgroup detection in ideological discus-
sions. In Proc. Annual Meeting of the Association
for Computational Linguistics (ACL 2012).

A. Abu-Jbara, A. Hassan, and D. Radev. 2012b. Atti-
tudeminer: mining attitude from online discussions.
In Proc. North American Chapter of the Association
for Computational Linguistics - Human Language
Technologies (NAACL HLT 2012).

A. Abu-Jbara, B. King, M. Diab, and D. Radev. 2013.
Identifying opinion subgroups in arabic online dis-
cussions. In Proc. Association for Computational
Linguistics (ACL 2013).

S. Asch. 1951. Effects of group pressure upon the
modification and distortion of judgments. Groups,
leadership, and men. S.

A. Balahur, R. Steinberger, E. Goot, B. Pouliquen, and
M. Kabadjov. 2009. Opinion mining on newspaper
quotations. In IEEE/WIC/ACM International Joint
Conferences on Web Intelligence and Intelligent A-
gent Technologies (WI-IAT 2009).

A. Bandura. 1971. Social Learning Theory. General
Learning Corporation.

R. Banerjee, S. Feng, J. Kang, and Y. Choi. 2014.
Keystroke patterns as prosody in digital writings: A
case study with deceptive reviews and essays. In
Proc. Empirical Methods on Natural Language Pro-
cessing (EMNLP 2014).

D. Buller and J. Burgoon. 1996. Interpersonal decep-
tion theory. Communication theory.

David B Buller, Judee K Burgoon, JA Daly, and
JM Wiemann. 1994. Deception: Strategic and
nonstrategic communication. Strategic interperson-
al communication.

G. Chittaranjan and H. Hung. 2010. Are you awere-
wolf? detecting deceptive roles and outcomes in a
conversational role-playing game. In IEEE Interna-
tional Conference on Acoustics Speech and Signal
Processing (ICASSP 2010).

P. Doreian and A. Mrvar. 1996. A partitioning ap-
proach to structural balance. Social networks.

V. Feng and G. Hirst. 2013. Detecting deceptive opin-
ions with profile compatibility. In Proc. Internation-
al Joint Conference on Natural Language Process-
ing (IJCNLP 2013).

S. Feng, R. Banerjee, and Y. Choi. 2012. Syntactic
stylometry for deception detection. In Proc. Associ-
ation for Computational Linguistics (ACL 2012).

N. E. Friedkin. 2010. The attitude-behavior linkage in
behavioral cascades. Social Psychology Quarterly.

P. Granhag and L. StroÌˆmwall. 2002. Repeated inter-
rogations: verbal and non-verbal cues to deception.
Applied Cognitive Psychology.

A. Hassan, A. Abu-Jbara, and D. Radev. 2012. De-
tecting subgroups in online discussions by model-
ing positive and negative relations among partici-
pants. In Proc. Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012).

S. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news me-
dia text. In Proc. ACL-COLING 2006 Workshop on
Sentiment and Subjectivity in Text.

J. Li, C. Cardie, and S. Li. 2013a. Topicspam: a
topic-model based approach for spam detection. In
Proc. Association for Computational Linguistics (A-
CL 2013).

J. Li, M. Ott, and C. Cardie. 2013b. Identifying ma-
nipulated offerings on review portals. In Proc. Em-
pirical Methods on Natural Language Processing
(EMNLP 2013).

C. Manning, P. Raghavan, and H. SchuÌˆtze. 2008. In-
troduction to information retrieval. Cambridge uni-
versity press Cambridge.

M. Ott, Y. Choi, C. Cardie, and J. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. In Proc. Association for Computation-
al Linguistics (ACL 2011).

G. Qiu, B. Liu, J. Bu, and C. Chen. 2011. Opinion
word expansion and target extraction through double
propagation. Computational linguistics.

M. Qiu, L. Yang, and J. Jiang. 2013. Mining us-
er relations from online discussions using sentimen-
t analysis and probabilistic matrix factorization. In
Proc. North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL HLT 2013).

865



S. Somasundaran and J. Wiebe. 2009. Recognizing s-
tances in online debates. In Proc. Joint Conference
of the Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP.

M. Steinbach, G. Karypis, V. Kumar, et al. 2000. A
comparison of document clustering techniques. In
Proc. KDD 2000 workshop on text mining.

E. Voorhees. 1986. Implementing agglomerative hi-
erarchic clustering algorithms for use in document
retrieval. Information Processing & Management.

A. Vrij, P. Granhag, and S. Porter. 2010. Pitfalls and
opportunities in nonverbal and verbal lie detection.
Psychological Science in the Public Interest.

A. Vrij. 2000. Detecting lies and deceit: The psychol-
ogy of lying and implications for professional prac-
tice. Wiley.

J. Walther. 1996. Computer-mediated communication
impersonal, interpersonal, and hyperpersonal inter-
action. Communication research.

Q. Xu and H. Zhao. 2012. Using deep linguistic
features for finding deceptive opinion spam. In
Proc. International Conference on Computational
Linguistics (COLING 2012).

M. Yancheva and F. Rudzicz. 2013. Automatic de-
tection of deception in child-produced speech using
syntactic complexity features. In Proc. Association
for Computational Linguistics (ACL 2013).

H. Zhang, H. Yu, D. Xiong, and Q. Liu. 2003. Hhmm-
based chinese lexical analyzer ictclas. In Proc.
SIGHAN 2003 workshop on Chinese language pro-
cessing.

L. Zhou and Y. Sung. 2008. Cues to deception in on-
line chinese groups. In Proc. Hawaii International
Conference on System Sciences (HICSS 2008).

L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communications. Group decision and ne-
gotiation.

866


