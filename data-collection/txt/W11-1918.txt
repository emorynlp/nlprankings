










































UBIU: A Robust System for Resolving Unrestricted Coreference


Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 112–116,
Portland, Oregon, 23-24 June 2011. c©2011 Association for Computational Linguistics

UBIU: A Robust System for Resolving Unrestricted Coreference

Desislava Zhekova
University of Bremen

zhekova@uni-bremen.de

Sandra Kübler
Indiana University

skuebler@indiana.edu

Abstract

In this paper, we discuss the application
of UBIU to the CONLL-2011 shared task
on “Modeling Unrestricted Coreference” in
OntoNotes. The shared task concentrates on
the detection of coreference not only in noun
phrases but also involving verbs. The infor-
mation provided for the closed track included
WordNet as well as corpus generated number
and gender information. Our system shows
no improvement when using WordNet infor-
mation, and the number information proved
less reliable than the information in the part
of speech tags.

1 Introduction

Coreference Resolution is the process of identify-
ing the linguistic expressions in a discourse that re-
fer to the same real world entity and to divide those
expressions into equivalence classes that represent
each discourse entity. For this task, a deeper knowl-
edge of the discourse is often required. However,
such knowledge is difficult to acquire. For this rea-
son, many systems use superficial information such
as string match. The CoNLL shared task on ”Model-
ing Unrestricted Coreference in OntoNotes” (Prad-
han et al., 2011) presents challenges that go be-
yond previous definitions of the task. On the one
hand, mention extraction is part of the task while
many previous approaches assumed gold standard
mentions. On the other hand, coreference is not
restricted to noun phrases, verbs are also included.
Thus, in Sales of passenger cars grew 22%. The
strong growth followed year-to-year increases., the
verb grew has an identity relation with the noun
phrase The strong growth.

The system that we used for the shared task is
the memory-based machine learning system UBIU
(Zhekova and Kübler, 2010). We describe the most
important components of the system in section 2.
The system was originally developed for robust,
multilingual coreference resolution, and thus had to
be adapted to this shared task. We investigate the
quality of our mention extraction in section 2.1 and
the quality of the features used in the classifier in
section 2.2. In section 3, we present UBIU’s results
on the development set, and in section 4, UBIU’s
final results in the shared task.

2 UBIU

UBIU (Zhekova and Kübler, 2010) was developed
as a multilingual coreference resolution system. A
robust approach is necessary to make the system ap-
plicable for a variety of languages. For this rea-
son, we use a machine learning approach to clas-
sify mention pairs. We use TiMBL (Daelemans et
al., 2007), a memory-based learner (MBL) that la-
bels the feature vectors from the test set based on
the k nearest neighbors in the training instances.
We chose TiMBL since MBL has been shown to
work well with small training sets. A non-exhaustive
parameter optimization on the development set led
us to use the IB1 algorithm, similarity is computed
based on weighted overlap, the relevance weights
are computed using gain ratio and the number of
nearest neighbors is set to k = 3 (for a description
of the algorithm and parameters cf. (Daelemans et
al., 2007)). The classifier is preceded by a mention
extractor, which identifies possible mentions, and a
feature extractor. The latter creates a feature vec-
tor for each possible pair of a potentially coreferring

112



mention and all possible antecedents in a context of
3 sentences. Another important step is to separate
singleton mentions from coreferent ones since only
the latter are annotated in OntoNotes. Our markable
extractor overgenerates in that it extracts all possi-
ble mentions, and only after classification, the sys-
tem can decide which mentions are singletons. We
investigate the performance of the mention and fea-
ture extraction modules in more detail below.

2.1 Mention Extraction

UBIU’s mention extractor uses part-of-speech
(POS), syntactic, and lemma information provided
in the OntoNotes data set to detect mentions. The
module defines a mention for each noun phrase,
based on syntactic information, as well as for all
possessive pronouns and all proper nouns, based on
their POS tags. Since for the shared task, verbs are
also potentially coreferent, we included a mention
for each of the verbs with a predicate lemma. An ex-
ample of the output of the mention extraction mod-
ule is shown in table 1. Each mention is numbered
with an individual number and thus still represents a
distinct entity. Since singleton mentions are not an-
notated in the OntoNotes data set, mentions without
coreference relations after classification need to be
removed from the answer set, which can only be per-
formed after coreference resolution when all coref-
erent pairs are identified. For this reason, the mark-
able extractor is bound to overgenerate. The latter
can clearly be seen when the mention extraction out-
put is compared to the provided gold mentions (cf.
the last column in table 1).

We conducted a simple experiment on the devel-
opment data in order to gain insight into the per-
formance of the mention extraction module. Using
the scorer provided by the shared task, we evaluated
the output of the module, without performing coref-
erence resolution and without removing singleton
mentions. This led to a recall of 96.55 % and a preci-
sion of 18.55%, resulting in an F-score of 31.12. The
high recall shows that the system is very reliable in
finding mentions with the correct boundaries. How-
ever, since we do not remove any singletons, UBIU
overgenerates and thus the system identified a con-
siderable number of singletons, too. Nevertheless,
the fact that UBIU identified 96.55% of all mentions
shows that the performance of the mention extrac-

# Word POS Parse bit ME output Gold
0 Devastating VBG (TOP(NP(NP* (1)|(2|(3 -
1 Critique NN *) 3) -
2 of IN (PP* - -
3 the DT (NP* (4 (32
4 Arab JJ * - -
5 World NN *)) 4) 32)
6 by IN (PP* - -
7 One CD (NP(NP*) (5|(6) -
8 of IN (PP* - -
9 Its PRP$ (NP* (7)|(8 (32)
10 Own JJ *)))))) 8)|5)|2) -

Table 1: The output of the mention extractor for a sample
sentence.

tion module is close to optimal.

2.2 Feature Extraction
Feature extraction is the second important subtask
for the UBIU pipeline. Since mentions are repre-
sented by their syntactic head, the feature extractor
uses a heuristic that selects the rightmost noun in a
noun phrase. However, since postmodifying prepo-
sitional phrases may be present in the mention, the
noun may not be followed by a preposition. For each
mention, a feature vector is created for all of its pre-
ceding mentions in a window of 3 sentences. Af-
ter classification, a filter can optionally be applied
to filter out mention pairs that disagree in number,
and another filter deletes all mentions that were not
assigned an antecedent in classification. Note that
the number information was derived from the POS
tags and not from the number/gender data provided
by the shared task since the POS information proved
more reliable in our system.

Initially, UBIU was developed to use a wide set of
features (Zhekova and Kübler, 2010), which consti-
tutes a subset of the features described by Rahman
and Ng (2009). For the CONLL-2011 shared task,
we investigated the importance of various additional
features that can be included in the feature set used
by the memory-based classifier. Thus, we conducted
experiments with a base set and an extended feature
set, which makes use of lexical semantic features.

Base Feature Set Since the original feature set in
Zhekova and Kübler (2010) contained information
that is not easily accessible in the OntoNotes data
set (such as grammatical functions), we had to re-
strict the feature set to information that can be de-
rived solely from POS annotations. Further infor-

113



# Feature Description
1 mj - the antecedent
2 mk - the mention to be resolved
3 Y if mj is a pronoun; else N
4 number - S(ingular) or P(lural)
5 Y if mk is a pronoun; else N
6 C if the mentions are the same string; else I
7 C if one mention is a substring of the other; else I
8 C if both mentions are pronominal and are the same

string; else I
9 C if the two mentions are both non-pronominal and

are the same string; else I
10 C if both mentions are pronominal and are either the

same pronoun or different only w.r.t. case;
NA if at least one of them is not pronominal; else I

11 C if the mentions agree in number; I if they disagree;
NA if the number for one or
both mentions cannot be determined

12 C if both mentions are pronouns; I if neither are
pronouns; else NA

13 C if both mentions are proper nouns; I if neither are
proper nouns; else NA

14 sentence distance between the mentions

Table 2: The pool of features used in the base feature set.

mation as sentence distance, word overlap etc. was
included as well. The list of used features is shown
in table 2.

Extended Feature Set Since WordNet informa-
tion was provided for the closed setting of the
CONLL-2011 shared task, we also used an ex-
tended feature set, including all features from the
base set along with additional features derived from
WordNet. The latter features are shown in table 3.

2.3 Singletons
In section 2.1, we explained that singletons need to
be removed after classification. However, this leads
to a drastic decrease in system performance for two
reasons. First, if a system does not identify a coref-
erence link, the singleton mentions will be removed
from the coreference chains, and consequently, the
system is penalized for the missing link as well as
for the missing mentions. If singletons are included,
the system will still receive partial credit for them
from all metrics but MUC. For this reason, we in-
vestigated filtered and non-filtered results in combi-
nation with the base and the extended feature sets.

3 Results on the Development Set

The results of our experiment on the development
set are shown in table 4. Since the official scores
of the shared task are based on an average of MUC,

# Feature Description
15 C if both are nouns and mk is hyponym of mj ; I if both

are nouns but mk is not a hyponym of mj ; NA otherwise
16 C if both are nouns and mj is hyponym of mk; I if both

are nouns but mj is not a hyponym of mk; NA otherwise
17 C if both are nouns and mk is a partial holonym of mj ;

I if both are nouns but mk is not a partial holonym of mj ;
NA otherwise

18 C if both are nouns and mj is a partial holonym of mk;
I if both are nouns but mj is not a partial holonym of mk;
NA otherwise

19 C if both are nouns and mk is a partial meronym of mj ;
I if both are nouns but mk is not a partial meronym of mj ;
NA otherwise

20 C if both are nouns and mj is a partial meronym of mk;
I if both are nouns but mj is not a partial meronym of mk;
NA otherwise

21 C if both are verbs and mk entails mj ; I if both are
verbs but mk does not entail mj ; NA otherwise

22 C if both are verbs and mj entails mk; I if both are
verbs but mj does not entail mk; NA otherwise

23 C if both are verbs and mk is a hypernym of mj ;
I if both are verbs but mk is not a hypernym of mj ;
NA otherwise

24 C if both are verbs and mj is a hypernym of mk;
I if both are verbs but mj is not a hypernym of mk;
NA otherwise

25 C if both are verbs and mk is a troponym of mj ;
I if both are verbs but mk is not a troponym of mj ;
NA otherwise

26 C if both are verbs and mj is a troponym of mk;
I if both are verbs but mj is not a troponym of mk;
NA otherwise

Table 3: The features extracted from WordNet.

B3, and CEAFE, we report these measures and their
average. All the results in this section are based on
automatically annotated linguistic information. The
first part of the table shows the results for the base
feature set (UBIUB), the second part for the ex-
tended feature set (UBIUE). We also report results
if we keep all singletons (& Sing.) and if we filter
out coreferent pairs that do not agree in number (&
Filt.). The results show that keeping the singletons
results in lower accuracies on the mention and the
coreference level. Only recall on the mention level
profits from the presence of singletons. Filtering for
number agreement with the base set has a detrimen-
tal effect on mention recall but increases mention
precision so that there is an increase in F-score of
1%. However, on the coreference level, the effect is
negligible. For the extended feature set, filtering re-
sults in a decrease of approximately 2.0% in mention
precision, which also translates into lower corefer-
ence scores. We also conducted an experiment in
which we filter before classification (& Filt. BC),
following a more standard approach. The reasoning

114



IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1

UBIUB 62.71 38.66 47.83 30.59 24.65 27.30 67.06 62.65 64.78 34.19 40.16 36.94 43.01
UBIUB & Sing. 95.11 18.27 30.66 30.59 24.58 27.26 67.10 62.56 64.75 34.14 40.18 36.92 42.97
UBIUB & Filt. 61.30 40.58 48.83 29.10 25.77 27.33 64.88 64.63 64.76 35.38 38.74 36.98 43.02
UBIUB & Filt. BC 61.33 40.49 48.77 28.96 25.54 27.14 64.95 64.48 64.71 35.23 38.71 36.89 42.91
UBIUE 62.72 39.09 48.16 30.63 24.94 27.49 66.72 62.76 64.68 34.19 39.90 36.82 43.00
UBIUE & Sing. 95.11 18.27 30.66 29.87 20.96 24.64 69.13 57.71 62.91 32.28 42.24 36.59 41.38
UBIUE & Filt. 63.01 36.62 46.32 28.65 21.05 24.27 68.10 58.72 63.06 32.91 41.53 36.72 41.35
Gold ME 100 100 100 38.83 82.97 52.90 39.99 92.33 55.81 66.73 26.75 38.19 48.97

Table 4: UBIU system results on the development set.

is that the training set for the classifier is biased to-
wards not assuming coreference since the majority
of mention pairs does not have a coreference rela-
tion. Thus filtering out non-agreeing mention pairs
before classification reduces not only the number of
test mention pairs to be classified but also the num-
ber of training pairs. However, in our system, this
approach leads to minimally lower results, which is
why we decided not to pursue this route. We also
experimented with instance sampling in order to re-
duce the bias towards non-coreference in the training
set. This also did not improve results.

Contrary to our expectation, using ontological in-
formation does not improve results. Only on the
mention level, we see a minimal gain in precision.
But this does not translate into any improvement on
the coreference level. Using filtering in combination
with the extended feature set results in a more pro-
nounced deterioration than with the base set.

The last row of table 4 (Gold ME) shows re-
sults when the system has access to the gold stan-
dard mentions. The MUC and B3 results show that
the classifier reaches an extremely high precision
(82.97% and 92.33%), from which we conclude that
the coreference links that our system finds are re-
liable, but it is also too conservative in assuming
coreference relations. For the future, we need to
investigate undersampling the negative examples in
the training set and more efficient methods for filter-
ing out singletons.

4 Final Results

In the following, we present the UBIU system re-
sults in two separate settings: using the test set with
automatically extracted mentions (section 4.1) and
using a test set with gold standard mentions, includ-
ing singletons (section 4.2). An overview of all sys-

tems participating in the CONLL-2011 shared task
and their results is provided by Pradhan et al. (2011).

4.1 Automatic Mention Identification

The final results of UBIU for the test set without
gold standard mentions are shown in the first part
of table 5. They are separated into results for the
coreference resolution module based on automati-
cally annotated linguistic information and the gold
annotations. Again, we report results for both the
base feature set (UBIUB) and the extended feature
set using WordNet features (UBIUE). A comparison
of the system results on the test and the development
set in the UBIUB setting shows that the average F-
score is considerably lower for the test set, 40.46 vs.
43.01 although the quality of the mentions remains
constant with an F-score of 48.14 on the test set and
47.83 on the development set.

The results based on the two data sets show that
UBIU’s performance improves when the system has
access to gold standard linguistic annotations. How-
ever, the difference between the results is in the area
of 2%. The improvement is due to gains of 3-5%
in precision for MUC and B3, which are counter-
acted by smaller losses in recall. In contrast, CEAFE
shows a loss in precision and a similar gain in recall,
resulting in a minimal increase in F-score.

A comparison of the results for the experiments
with the base set as opposed to the extended set in
5 shows that the extended feature set using Word-
Net information is detrimental to the final results av-
eraged over all metrics while it led to a slight im-
provement on the mention level. Our assumption
is that while in general, the ontological information
is useful, the additional information may be a mix-
ture of relevant and irrelevant information. Mihalcea
(2002) showed for word sense disambiguation that

115



IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1

Automatic Mention Identification

auto UBIUB 67.27 37.48 48.14 28.75 20.61 24.01 67.17 56.81 61.55 31.67 41.22 35.82 40.46UBIUE 67.49 37.60 48.29 28.87 20.66 24.08 67.14 56.67 61.46 31.57 41.21 35.75 40.43

gold UBIUB 65.92 40.56 50.22 31.05 25.57 28.04 64.94 62.23 63.56 33.53 39.08 36.09 42.56UBIUE 66.11 40.37 50.13 30.84 25.14 27.70 65.07 61.83 63.41 33.23 39.05 35.91 42.34
Gold Mention Boundaries

auto UBIUB 67.57 58.66 62.80 34.14 40.43 37.02 54.24 71.09 61.53 39.65 33.73 36.45 45.00UBIUE 69.19 57.27 62.67 33.48 37.15 35.22 55.47 68.23 61.20 38.29 34.65 36.38 44.27

gold UBIUB 67.64 58.75 62.88 34.37 40.68 37.26 54.28 71.18 61.59 39.69 33.76 36.49 45.11UBIUE 67.72 58.66 62.87 34.18 40.40 37.03 54.30 71.04 61.55 39.64 33.78 36.47 45.02

Table 5: Final system results for the coreference resolution module on automatically extracted mentions on the gold
standard mentions for the base and extended feature sets.

memory-based learning is extremely sensitive to ir-
relevant features. For the future, we are planning
to investigate this problem by applying forward-
backward feature selection, as proposed by Mihal-
cea (2002) and Dinu and Kübler (2007).

4.2 Gold Mention Boundaries

UBIU was also evaluated in the experimental set-
ting in which gold mention boundaries were pro-
vided in the test set, including for singletons. The
results of the setting using both feature sets are re-
ported in the second part of table 5. The results show
that overall the use of gold standard mentions re-
sults in an increase of the average F-score of approx.
4.5%. Where mention quality and MUC are con-
cerned, gold standard mentions have a significant
positive influence on the average F-score. For B3

and CEAFE, however, there is no significant change
in scores. The increase in performance is most no-
ticeable in mention identification, for which the F-
score increases from 48.14 to 62.80. But this im-
provement has a smaller effect on the overall coref-
erence system performance leading to a 5% increase
of results. In contrast to the gold mention results in
the development set, we see lower precision values
in the test set. This is due to the fact that the test set
contains singletons. Detecting singletons reliably is
a difficult problem that needs further investigation.

5 Conclusion and Future Work

In the current paper, we presented the results of
UBIU in the CONLL-2011 shared task. We showed
that for a robust system for coreference resolution
such as UBIU, automatically annotated linguistic
data is sufficient for mention-pair based coreference

resolution. We also showed that ontological infor-
mation as well as filtering non-agreeing mention
pairs leads to an insignificant improvement of the
overall coreference system performance. The treat-
ment of singletons in the data remains a topic that
requires further investigation.

References
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and

Antal van den Bosch. 2007. TiMBL: Tilburg memory
based learner – version 6.1 – reference guide. Techni-
cal Report ILK 07-07, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.

Georgiana Dinu and Sandra Kübler. 2007. Sometimes
less is more: Romanian word sense disambiguation
revisited. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing, RANLP 2007, Borovets, Bulgaria.

Rada Mihalcea. 2002. Instance based learning with
automatic feature selection applied to word sense
disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics,
COLING’02, Taipeh, Taiwan.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL), Portland, Oregon.

Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP
2009, Singapore.

Desislava Zhekova and Sandra Kübler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation (SemEval), pages 96–99, Up-
psala, Sweden.

116


