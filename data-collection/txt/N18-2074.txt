



















































Self-Attention with Relative Position Representations


Proceedings of NAACL-HLT 2018, pages 464–468
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Self-Attention with Relative Position Representations

Peter Shaw
Google

petershaw@google.com

Jakob Uszkoreit
Google Brain

usz@google.com

Ashish Vaswani
Google Brain

avaswani@google.com

Abstract

Relying entirely on an attention mechanism,
the Transformer introduced by Vaswani et
al. (2017) achieves state-of-the-art results for
machine translation. In contrast to recurrent
and convolutional neural networks, it does
not explicitly model relative or absolute po-
sition information in its structure. Instead,
it requires adding representations of abso-
lute positions to its inputs. In this work
we present an alternative approach, extend-
ing the self-attention mechanism to efficiently
consider representations of the relative posi-
tions, or distances between sequence elements.
On the WMT 2014 English-to-German and
English-to-French translation tasks, this ap-
proach yields improvements of 1.3 BLEU and
0.3 BLEU over absolute position representa-
tions, respectively. Notably, we observe that
combining relative and absolute position rep-
resentations yields no further improvement in
translation quality. We describe an efficient
implementation of our method and cast it as an
instance of relation-aware self-attention mech-
anisms that can generalize to arbitrary graph-
labeled inputs.

1 Introduction

Recent approaches to sequence to sequence learn-
ing typically leverage recurrence (Sutskever et al.,
2014), convolution (Gehring et al., 2017; Kalch-
brenner et al., 2016), attention (Vaswani et al.,
2017), or a combination of recurrence and atten-
tion (Bahdanau et al., 2014; Cho et al., 2014; Lu-
ong et al., 2015; Wu et al., 2016) as basic building
blocks. These approaches incorporate information
about the sequential position of elements differ-
ently.

Recurrent neural networks (RNNs) typically
compute a hidden state ht, as a function of their
input at time t and a previous hidden state ht−1,
capturing relative and absolute positions along the

time dimension directly through their sequential
structure. Non-recurrent models do not necessar-
ily consider input elements sequentially and may
hence require explicitly encoding position infor-
mation to be able to use sequence order.

One common approach is to use position encod-
ings which are combined with input elements to
expose position information to the model. These
position encodings can be a deterministic func-
tion of position (Sukhbaatar et al., 2015; Vaswani
et al., 2017) or learned representations. Convolu-
tional neural networks inherently capture relative
positions within the kernel size of each convolu-
tion. They have been shown to still benefit from
position encodings (Gehring et al., 2017), how-
ever.

For the Transformer, which employs neither
convolution nor recurrence, incorporating explicit
representations of position information is an espe-
cially important consideration since the model is
otherwise entirely invariant to sequence ordering.
Attention-based models have therefore used posi-
tion encodings or biased attention weights based
on distance (Parikh et al., 2016).

In this work we present an efficient way of
incorporating relative position representations in
the self-attention mechanism of the Transformer.
Even when entirely replacing its absolute position
encodings, we demonstrate significant improve-
ments in translation quality on two machine trans-
lation tasks.

Our approach can be cast as a special case of ex-
tending the self-attention mechanism of the Trans-
former to considering arbitrary relations between
any two elements of the input, a direction we plan
to explore in future work on modeling labeled, di-
rected graphs.

464



2 Background

2.1 Transformer
The Transformer (Vaswani et al., 2017) em-
ploys an encoder-decoder structure, consisting of
stacked encoder and decoder layers. Encoder
layers consist of two sublayers: self-attention
followed by a position-wise feed-forward layer.
Decoder layers consist of three sublayers: self-
attention followed by encoder-decoder attention,
followed by a position-wise feed-forward layer.
It uses residual connections around each of the
sublayers, followed by layer normalization (Ba
et al., 2016). The decoder uses masking in its self-
attention to prevent a given output position from
incorporating information about future output po-
sitions during training.

Position encodings based on sinusoids of vary-
ing frequency are added to encoder and decoder
input elements prior to the first layer. In contrast
to learned, absolute position representations, the
authors hypothesized that sinusoidal position en-
codings would help the model to generalize to se-
quence lengths unseen during training by allowing
it to learn to attend also by relative position. This
property is shared by our relative position repre-
sentations which, in contrast to absolute position
representations, are invariant to the total sequence
length.

Residual connections help propagate position
information to higher layers.

2.2 Self-Attention
Self-attention sublayers employ h attention heads.
To form the sublayer output, results from each
head are concatenated and a parameterized linear
transformation is applied.

Each attention head operates on an input se-
quence, x = (x1, . . . , xn) of n elements where
xi ∈ Rdx , and computes a new sequence z =
(z1, . . . , zn) of the same length where zi ∈ Rdz .

Each output element, zi, is computed as
weighted sum of a linearly transformed input el-
ements:

zi =

n∑

j=1

αij(xjW
V ) (1)

Each weight coefficient, αij , is computed using
a softmax function:

αij =
exp eij∑n
k=1 exp eik

And eij is computed using a compatibility func-
tion that compares two input elements:

eij =
(xiW

Q)(xjW
K)T√

dz
(2)

Scaled dot product was chosen for the compat-
ibility function, which enables efficient computa-
tion. Linear transformations of the inputs add suf-
ficient expressive power.
WQ, WK , W V ∈ Rdx×dz are parameter matri-

ces. These parameter matrices are unique per layer
and attention head.

3 Proposed Architecture

3.1 Relation-aware Self-Attention

We propose an extension to self-attention to con-
sider the pairwise relationships between input ele-
ments. In this sense, we model the input as a la-
beled, directed, fully-connected graph.

The edge between input elements xi and xj is
represented by vectors aVij , a

K
ij ∈ Rda . The mo-

tivation for learning two distinct edge represen-
tations is that aVij and a

K
ij are suitable for use in

eq. (3) and eq. (4), respectively, without requiring
additional linear transformations. These represen-
tations can be shared across attention heads. We
use da = dz .

We modify eq. (1) to propagate edge informa-
tion to the sublayer output:

zi =

n∑

j=1

αij(xjW
V + aVij) (3)

This extension is presumably important for
tasks where information about the edge types se-
lected by a given attention head is useful to down-
stream encoder or decoder layers. However, as ex-
plored in 4.3, this may not be necessary for ma-
chine translation.

We also, importantly, modify eq. (2) to consider
edges when determining compatibility:

eij =
xiW

Q(xjW
K + aKij )

T

√
dz

(4)

The primary motivation for using simple addi-
tion to incorporate edge representations in eq. (3)
and eq. (4) is to enable an efficient implementation
described in 3.3.

465



x1 x2 x3 … xnx4

aV2,1=w
V
-1

aK2,1=w
K
-1

aV2,4=w
V
2

aK2,4=w
K
2

aV4,n=w
V
k

aK4,n=w
K
k

Figure 1: Example edges representing relative posi-
tions, or the distance between elements. We learn rep-
resentations for each relative position within a clipping
distance k. The figure assumes 2 <= k <= n − 4.
Note that not all edges are shown.

3.2 Relative Position Representations

For linear sequences, edges can capture infor-
mation about the relative position differences be-
tween input elements. The maximum relative po-
sition we consider is clipped to a maximum abso-
lute value of k. We hypothesized that precise rel-
ative position information is not useful beyond a
certain distance. Clipping the maximum distance
also enables the model to generalize to sequence
lengths not seen during training. Therefore, we
consider 2k + 1 unique edge labels.

aKij = w
K
clip(j−i,k)

aVij = w
V
clip(j−i,k)

clip(x, k) = max(−k,min(k, x))

We then learn relative position representations
wK = (wK−k, . . . , w

K
k ) and w

V = (wV−k, . . . , w
V
k )

where wKi , w
V
i ∈ Rda .

3.3 Efficient Implementation

There are practical space complexity concerns
when considering edges between input elements,
as noted by Veličković et al. (2017), which consid-
ers unlabeled graph inputs to an attention model.

For a sequence of length n and h attention
heads, we reduce the space complexity of storing
relative position representations from O(hn2da)
to O(n2da) by sharing them across each heads.
Additionally, relative position representations can
be shared across sequences. Therefore, the over-
all self-attention space complexity increases from
O(bhndz) to O(bhndz + n2da). Given da = dz ,
the size of the relative increase depends on nbh .

The Transformer computes self-attention effi-
ciently for all sequences, heads, and positions in

a batch using parallel matrix multiplication opera-
tions (Vaswani et al., 2017). Without relative posi-
tion representations, each eij can be computed us-
ing bh parallel multiplications of n×dz and dz×n
matrices. Each matrix multiplication computes eij
for all sequence positions, for a particular head
and sequence. For any sequence and head, this
requires sharing the same representation for each
position across all compatibility function applica-
tions (dot products) with other positions.

When we consider relative positions the repre-
sentations differ with different pairs of positions.
This prevents us from computing all eij for all
pairs of positions in a single matrix multiplication.
We also want to avoid broadcasting relative po-
sition representations. However, both issues can
be resolved by splitting the computation of eq. (4)
into two terms:

eij =
xiW

Q(xjW
K)T + xiW

Q(aKij )
T

√
dz

(5)

The first term is identical to eq. (2), and can be
computed as described above. For the second term
involving relative position representations, tensor
reshaping can be used to compute n parallel multi-
plications of bh×dz and dz×nmatrices. Each ma-
trix multiplication computes contributions to eij
for all heads and batches, corresponding to a par-
ticular sequence position. Further reshaping al-
lows adding the two terms. The same approach
can be used to efficiently compute eq. (3).

For our machine translation experiments, the re-
sult was a modest 7% decrease in steps per sec-
ond, but we were able to maintain the same model
and batch sizes on P100 GPUs as Vaswani et
al. (2017).

4 Experiments

4.1 Experimental Setup

We use the tensor2tensor 1 library for training and
evaluating our model.

We evaluated our model on the WMT 2014
machine translation task, using the WMT 2014
English-German dataset consisting of approxi-
mately 4.5M sentence pairs and the 2014 WMT
English-French dataset consisting of approxi-
mately 36M sentence pairs.

1The tensor2tensor library is available at https://
github.com/tensorflow/tensor2tensor.

466



Model Position Information EN-DE BLEU EN-FR BLEU
Transformer (base) Absolute Position Representations 26.5 38.2
Transformer (base) Relative Position Representations 26.8 38.7
Transformer (big) Absolute Position Representations 27.9 41.2
Transformer (big) Relative Position Representations 29.2 41.5

Table 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans-
lation tasks, using newstest2014 test set.

For all experiments, we split tokens into a
32,768 word-piece vocabulary (Wu et al., 2016).
We batched sentence pairs by approximate length,
and limited input and output tokens per batch to
4096 per GPU. Each resulting training batch con-
tained approximately 25,000 source and 25,000
target tokens.

We used the Adam optimizer (Kingma and Ba,
2014) with β1 = 0.9, β2 = 0.98, and � = 10−9.
We used the same warmup and decay strategy for
learning rate as Vaswani et al. (2017), with 4,000
warmup steps. During training, we employed la-
bel smoothing of value �ls = 0.1 (Szegedy et al.,
2016). For evaluation, we used beam search with
a beam size of 4 and length penalty α = 0.6 (Wu
et al., 2016).

For our base model, we used 6 encoder and de-
coder layers, dx = 512, dz = 64, 8 attention
heads, 1024 feed forward inner-layer dimensions,
and Pdropout = 0.1. When using relative posi-
tion encodings, we used clipping distance k = 16,
and used unique edge representations per layer and
head. We trained for 100,000 steps on 8 K40
GPUs, and did not use checkpoint averaging.

For our big model, we used 6 encoder and de-
coder layers, dx = 1024, dz = 64, 16 attention
heads, 4096 feed forward inner-layer dimensions,
and Pdropout = 0.3 for EN-DE and Pdropout = 0.1
for EN-FR. When using relative position encod-
ings, we used k = 8, and used unique edge repre-
sentations per layer. We trained for 300,000 steps
on 8 P100 GPUs, and averaged the last 20 check-
points, saved at 10 minute intervals.

4.2 Machine Translation

We compared our model using only relative po-
sition representations to the baseline Transformer
(Vaswani et al., 2017) with sinusoidal position en-
codings. We generated baseline results to iso-
late the impact of relative position representations
from any other changes to the underlying library
and experimental configuration.

For English-to-German our approach improved
performance over our baseline by 0.3 and 1.3
BLEU for the base and big configurations, respec-
tively. For English-to-French it improved by 0.5
and 0.3 BLEU for the base and big configurations,
respectively. In our experiments we did not ob-
serve any benefit from including sinusoidal posi-
tion encodings in addition to relative position rep-
resentations. The results are shown in Table 1.

4.3 Model Variations
We performed several experiments modifying var-
ious aspects of our model. All of our experi-
ments in this section use the base model configura-
tion without any absolute position representations.
BLEU scores are calculated on the WMT English-
to-German task using the development set, new-
stest2013.

We evaluated the effect of varying the clipping
distance, k, of the maximum absolute relative po-
sition difference. Notably, for k ≥ 2, there does
not appear to be much variation in BLEU scores.
However, as we use multiple encoder layers, pre-
cise relative position information may be able to
propagate beyond the clipping distance. The re-
sults are shown in Table 2.

k EN-DE BLEU
0 12.5
1 25.5
2 25.8
4 25.9
16 25.8
64 25.9
256 25.8

Table 2: Experimental results for varying the clipping
distance, k.

We also evaluated the impact of ablating each of
the two relative position representations defined in
section 3.1, aVij in eq. (3) and a

K
ij in eq. (4). Includ-

ing relative position representations solely when
determining compatibility between elements may

467



be sufficient, but further work is needed to deter-
mine whether this is true for other tasks. The re-
sults are shown in Table 3.

aVij a
K
ij EN-DE BLEU

Yes Yes 25.8
No Yes 25.8
Yes No 25.3
No No 12.5

Table 3: Experimental results for ablating relative po-
sition representations aVij and a

K
ij .

5 Conclusions

In this paper we presented an extension to self-
attention that can be used to incorporate rela-
tive position information for sequences, which im-
proves performance for machine translation.

For future work, we plan to extend this mecha-
nism to consider arbitrary directed, labeled graph
inputs to the Transformer. We are also inter-
ested in nonlinear compatibility functions to com-
bine input representations and edge representa-
tions. For both of these extensions, a key consid-
eration will be determining efficient implementa-
tions.

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-

ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450 .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Convolu-
tional sequence to sequence learning. arXiv preprint
arXiv:1705.03122 .

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. arXiv preprint arXiv:1610.10099 .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025 .

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Empirical
Methods in Natural Language Processing.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems. pages
2440–2448.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pages 2818–2826.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems. pages 6000–6010.

Petar Veličković, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 .

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .

468


