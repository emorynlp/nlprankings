










































Phone set selection for HMM-based dialect speech synthesis


Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 65–69,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Phone set selection for HMM-based dialect speech synthesis

Michael Pucher
Telecommunications

Research Center (FTW)
Vienna, Austria

pucher@ftw.at

Nadja Kerschhofer-Puhalo
Acoustics Research

Institute (ARI)
Vienna, Austria

nadja.kerschhofer@oeaw.ac.at

Dietmar Schabus
Telecommunications

Research Center (FTW)
Vienna, Austria

schabus@ftw.at

Abstract

This paper describes a method for selecting an
appropriate phone set in dialect speech synthe-
sis for a so far undescribed dialect by applying
hidden Markov model (HMM) based training
and clustering methods. In this pilot study we
show how a phone set derived from the pho-
netic surface can be optimized given a small
amount of dialect speech training data.

1 Introduction

In acoustic modeling for dialect speech synthesis we
are confronted with two closely related major prob-
lems1, (1) to find an appropriate phone set for syn-
thesis and (2) to design a recording script with suf-
ficient phonetic and prosodic coverage. In HMM-
based synthesis, we can use the training process of
the voices itself to analyze the used phone set and to
try to optimize it for synthesis.

2 Corpus and phone set design

Goiserian, the dialect of Bad Goisern in the most
southern part of Upper Austria, is a local dialect
of the Middle Bavarian/Southern Bavarian transition
zone. The target variety for speech synthesis de-
scribed here demonstrates the typical problems re-
lated to scarcity of data. While several varieties of
the central and northern part of Upper Austria are
quite well described, detailed descriptions of the va-
rieties in this region do not exist. Lacking a lexi-
con, a phonological description, orthographic rules

1Apart from additional problems that have to do with text
analysis, orthography, and grapheme-to-phoneme conversion.

or a transcription system, a speech corpus and an
appropriate phone set have to be created. Our cur-
rent project aims at audio-visual dialect synthesis,
which is based on a systematic description of speech
data collected from spontaneous speech, word lists
and translated sentences by 10 speakers of the same
dialect. Although it would be ideal to use con-
versational speech data for dialect speech synthe-
sis (Campbell, 2006) we decided to use a hybrid ap-
proach for our full corpus where we plan to collect
a set of prompts from conversational dialect speech,
which will be realized by the dialect speakers.

The speech data for the first preliminary study
presented here consists of 150 sentences and col-
loquial phrases spoken in Goiserian by a female
speaker who can be described as a conservative
speaker of the original basic dialect of the region.
The prompts were translated spontaneously by the
speaker from Standard German into Goiserian and
contain typical phonetic and phonological character-
istics of local Bavarian varieties in multiple occur-
rences.

3 Voice building

The data was originally recorded at 96kHz, 24 bit
and was downsampled to 16kHz, 16 bit for synthesis
and voice building. A preliminary phone set (PS1)
was created on the basis of a fine phonetic transcrip-
tion including sub-phonemic details (e.g. nasaliza-
tion of vowels before nasals “VN”). Phones occur-
ring less than twice were substituted prior to voice
training with phonetically similar phones or repre-
sentatives of the same phoneme. This leaves us with
a set of 72 phones (see Table 1 and 2).

65



The TRA voice was trained with a HMM-
based speaker-dependent system. Given the limited
amount of training data (150 prompts) and to be able
to analyze the decision trees we only used the cur-
rent, 2 preceding, and 2 succeeding phones as fea-
tures.

HTK IPA # HTK IPA #
s s 207 t t 204
d d 179 n n 171
m m 115 k k 98
h h 84 g g 79
v v 79 f f 62
r r 61 S S 49
N n

"
42 l l 41

b b 31 ts ţ 27
ng N 19 p p 17
w B 14 L l

"
12

X x 11 c c 10
RX X 9 j j 7
R R 6 ks ks 3
pf pf 3

Table 1: Consonants (27) in phone set PS1 for training
(72 phones) (Blue = not in PS2).

Based on a phonetic transcription of the training
corpus, flat-start forced alignment with HTK was
carried out. Stops are split into two parts, one for
the closure and one for plosion plus burst. Ad-
ditionally, we applied forced alignment using pro-
nunciation variants2, which is the preferred method
when building a voice for dialect synthesis using a
larger corpus (Pucher, 2010). With this method it
is not necessary to have a phonetic transcription of
the recordings. Given our small corpus, this method
produced several errors ([tsvoa] / [tsvai], [tsum] /
[tsun] etc.) which led us to use the standard align-
ment method from a transcription of the corpus. Af-
ter the transcription we had to correct severe align-
ment errors. These errors are simple to find since
several segments within the utterance are affected.

From this corpus we selected 5 prompts contain-
ing only phonemes that appear at least more than 3
times in the rest of the corpus. This leaves us with
a training corpus of 145 prompts and a 5 prompt

2In a previous project on Viennese dialect synthesis, 33% of
the lexicon entries are pronunciation variants.

HTK IPA # HTK IPA #
a a 138 aa a: 10
A 6 80 AA 6: 3
AN 6̃ 80 Ai 6i 3
AuN 6̃u 7
e e 100 ee e: 9
ei ei 22 eiN ẽi 10
E E 20 EE E: 11
EN Ẽ 4 EiN Ẽi 6
i i 175 ii i: 7
iN ĩ 6
o o 45 oo o: 3
ou ou 4 Ou O 4
u u 20 U U 15
UN Ũ 3
q ø 9 qY øY 3
QY œY 4
y y 9 yy y: 3
Y Y 4
eV @ 11 aV 5 89
ai ai 24 aiN ãi 9
au au 24 ea e5 7
eaN ẽ5 4 ia i5 30
oa o5 16 oaN õ5 9
Oi Oi 6 oi oi 26
ua u5 21 ui ui 6

Table 2: Vowels (33) and diphtongs (12) in phone set PS1
for training (72 phones) (Blue = not in PS2, Red = not in
PS2 and PS3, green = not in PS3).

test set. For the subjective evaluation, the entire re-
synthesized corpus was used to show us how well
the used phone set covers the data.

The 145 prompts were then used for training
a speaker-dependent HMM-based synthetic voice.
Figure 1 shows the architecture of the HMM-based
speaker dependent system (Zen, 2005). For synthe-
sis we used the full-context label files of the corpus
without duration information. By that text analysis
is not necessary for synthesis. Our implicit assump-
tion is that the letter-to-sound rules and text analysis
produce exactly the string of phones from the tran-
scription. In this way we can evaluate the acoustic
modeling part separately, independently from text
analysis.

66



Excitation
parameter
extraction

Spectral
parameter
extraction

Training of MSD-HSMM

Parameter generation
from MSD-HSMMText analysis

Excitation 
generation

Synthesis
filter

Speech signal

Labels

Single speaker
speech database

Training

Synthesis

Spectral parametersExcitation parameters

Labels

Context-dependent
multi-stream MSD-HSMMs

TEXT

SYNTHESIZED
SPEECH

Figure 1: HMM-based speaker dependent speech synthe-
sis system.

4 Voice analysis

To be able to analyze the decision trees we used
phone features only. The HMM-based voice con-
sists of a mel-cepstrum, duration, F0, and an aperi-
odicity model. In a first step we defined the phones
that are not used for modeling, or are used for a cer-
tain model only.

Figure 3 shows those phones that are not used for
clustering of the different models. This may be due
to their rare occurrence in the data (3-4 times) or due
to possible inconsistencies in their phonetic realiza-
tion. The F0 model is not shown since all phonemes
were used in the F0 tree in some context.

To define other possible phone sets we decided
to substitute the phones only occurring in the F0
model but not in the other 3 models, namely the
mel-cepstrum, duration, and the aperiodicity model.
We therefore merged “Ai”, “AuN”, “EN”, “ks”, “L”,
“Ou”, “qY”, “yy” with their phonetically most sim-
ilar equivalents (e.g. syllabic “L” with “l”, “ks”
with “k”+“s”, or a nasalized “EN” or “AuN” before
nasals with the non-nasal phone) and thus obtained
a new smaller phone set (PS2), which was used for
training a second voice model.

Another possible set of phones (PS 3) is defined
by merging long (VV) and short (V) vowels of the
same quality, namely “ii”, “yy”, “ee”, “EE”, “aa”,
“AA”, “oo” with their short counterpart. From a lin-
guistic point of view, the phonemic status of vowel

C-sil

C-s

no

mcep_s4_1

yes

C-A

no

L-sil

yes

C-t

no

mcep_s4_2

yes

mcep_s4_35

no

mcep_s4_34

yes

C-n

no

mcep_s4_3

yes

C-m

no

mcep_s4_4

yes

C-e

no

mcep_s4_5

yes

C-a

no

L-h

yes

C-AN

no

mcep_s4_6

yes

mcep_s4_37

no

mcep_s4_36

yes

C-k

no

mcep_s4_7

yes

C-d

no

mcep_s4_8

yes

C-g

no

mcep_s4_9

yes

C-b

no

mcep_s4_10

yes

C-N

no

mcep_s4_11

yes

C-f

no

mcep_s4_12

yes

C-i

no

mcep_s4_13

yes

C-r

no

mcep_s4_14

yes

C-h

no

mcep_s4_15

yes

C-X

no

mcep_s4_16

yes

C-c

no

mcep_s4_17

yes

C-oa

no

mcep_s4_18

yes

C-ua

no

mcep_s4_19

yes

C-oaN

no

mcep_s4_20

yes

C-EE

no

mcep_s4_21

yes

C-ia

no

mcep_s4_22

yes

C-ei

no

mcep_s4_23

yes

C-ng

no

mcep_s4_24

yes

C-au

no

mcep_s4_25

yes

C-ai

no

mcep_s4_26

yes

C-aV

no

mcep_s4_27

yes

C-v

no

mcep_s4_28

yes

C-ea

no

mcep_s4_29

yes

C-aa

no

mcep_s4_30

yes

C-ee

no

mcep_s4_31

yes

C-oi

no

mcep_s4_32

yes

R-n

no

mcep_s4_33

yes

C-y

no

C-Oi

yes

C-eV

no

mcep_s4_38

yes

mcep_s4_40

no

mcep_s4_39

yes

mcep_s4_42

no

mcep_s4_41

yes

Figure 2: Part of the mel-cepstrum clustering tree for the
3rd state of the HMM.

duration as a primary feature in Austrian German
is a controversial issue. While differences in length
do exist at the phonetic surface, these differences are
not necessarily of phonemic relevance (Moosmüller,
2007; Scheutz, 1985). We obtain thus a third phone
set (PS3) by merging long and short vowels.

Model # #C #L #LL #R #RR
Mel-cep. 42 38 2 0 1 0
Aperiod. 36 31 0 3 0 1
F0 196 54 37 38 30 36
Duration 83 32 14 9 14 13

Table 3: Number of models and questions in mel-
cepstrum, aperiodicity, F0, and duration model for central
HMM state.

4.1 Mel-cepstrum and aperiodicity model
The mel-cepstrum model contains a separate model
for each phone that is used in the cepstral clus-
tering. In Figure 2 this is shown with the model
“mcep s4 32”, which is used in case that the cur-
rent phone is an “ee” (C-ee) and with the model
“mcep s4 33”, which is used in case that the cur-
rent phone is an “oi”. These two models are special
models which only cover certain phones. The only
effect of the clustering is that some phones are not
modeled separately, resulting in an unbalanced tree.

However there is one instance of context cluster-
67



MEL-CEP DURATION

APERIODICITY

aa ea ii iN ou

Ai AuN EN ks 
L Ou qY yy

AA ai aiN au
eaN ee EE ei EiN j
ng oa oaN oi Oi q 
QY R u ua w X y  

E eiN

oo pf RX
U ui UN Y

p 

Figure 3: Phones that were not used for clustering in the
trees for mel-cepstrum, duration, and aperiodicity in any
context (current, 2 preceding, and 2 succeeding phones)
and any of the 5 states.

ing in the central state of the mel-cepstrum HMMs.
If the right phone is an “n” (R-n) there are two dif-
ferent models used (“mcep s4 39”, “mcep s4 40”),
depending on whether the current phone is an “Oi”
(C-Oi) or not (Figure 2).

All phones that are not modeled through a sepa-
rate model are modeled by the model at the end of
the tree (model “mcep s4 42”).

The aperiodicity model is very similar to the mel-
cepstrum model, as can be seen in Table 3 and Fig-
ure 3.

4.2 F0 and duration model

The F0 model uses all phones as shown in Figure 3
and is the most complex model in terms of context
questions as can be seen from Table 3.

The duration model contains the lowest number of
phone related questions as shown by Figure 3 but is
still more complex than the spectrum related models
in terms of context-dependent questions as shown
in Table 3. Similarly to the F0 model, it is rather
difficult to analyze this model directly.

5 Voice evaluation

After the analysis of the voice that was trained with
our basic phoneset PS1 we defined two new phone-
sets PS2 and PS3. These phonesets were used to
train additional voice models for the same speaker.

With these voice models, we synthesized our small
set of 5 test sentences. To evaluate the suitabil-
ity of the phonesets for the training data, we re-
synthesized the training corpus of 145 prompts.

In a pair-wise comparison test of the 150 prompts
we evaluated the three voice models in a subjective
listening test with three expert listeners. The experts
listened to a set of prompts, each prompt synthesized
with two different voice models. They were asked
to compare them and to decide which prompt they
would prefer in terms of overall quality, or whether
they would rate them as “equally good”.

PS1 PS2 PS3
56 102 105

Table 4: Number of winning comparisons per phone set
(PS1-PS3).

Table 4 illustrates that both approaches to reduce
and redefine the phoneset (PS2, PS3) improved the
overall quality estimation considerably compared to
the initial phoneset PS1.

6 Conclusion

One major challenge for speech synthesis of so far
undescribed varieties is the lack of an appropriate
phoneset and sufficient training data. We met this
challenge by deriving a phoneset directly from the
phonetic surface of a very restricted corpus of natu-
ral speech. This phone set was used for voice train-
ing. Based on the outcome of the first voice training
we reconsidered the choice of phones and created
new phone sets following 2 approaches: (1) remov-
ing phones that are not used in the clustering, and
(2) a linguistically motivated choice of phone sub-
stitutions based on clustering results. Both methods
yielded a considerable improvement of voice qual-
ity. Thus, HMM-based machine learning methods
and supervised optimization can be used for the def-
inition of the phoneset of an unkown dialect. Our
future work will elaborate this method with dialect
speech training corpora of different size to show
whether it can be applied to adaptive methods in-
volving multiple-speaker training. The considera-
tion of inter- and intra-speaker variation and style
shifting will be a crucial question for further study.

68



References
Nick Campbell. 2006. Conversational speech synthesis

and the need for some laughter. IEEE Transactions
on Speech and Audio Processing, 14(4), pages 1171-
1178.

Michael Pucher, Friedrich Neubarth, Volker Strom,
Sylvia Moosmüller, Gregor Hofer, Christian Kranzler,
Gudrun Schuchmann and Dietmar Schabus. 2010. Re-
sources for speech synthesis of Viennese varieties. In
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC), Valletta,
Malta.

Sylvia Moosmüller. 2007. Vowels in Standard Aus-
trian German. An Acoustic-Phonetic and Phonologi-
cal Analysis. Habilitationsschrift, Vienna.

Hannes Scheutz. 1985. Strukturen der Lautveränderung.
Braumüller, Vienna.

Heiga Zen and Tomoki Toda. 2005. An Overview
of Nitech HMM-based Speech Synthesis System for
Blizzard Challenge 2005. In Proceedings of the 9th
European Conference on Speech Communication and
Technology (INTERSPEECH), Lisboa, Portugal.

69


