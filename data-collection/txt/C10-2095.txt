828

Coling 2010: Poster Volume, pages 828–836,

Beijing, August 2010

A Power Mean Based Algorithm for Combining Multiple

Alignment Tables

Sameer Maskey, Steven J. Rennie, Bowen Zhou

IBM T.J. Watson Research Center

{smaskey, sjrennie, zhou}@us.ibm.com

Abstract

Most existing techniques for combining
multiple alignment
tables can combine
only two alignment tables at a time, and
are based on heuristics (Och and Ney,
2003), (Koehn et al., 2003).
In this pa-
per, we propose a novel mathematical for-
mulation for combining an arbitrary num-
ber of alignment tables using their power
mean. The method frames the combi-
nation task as an optimization problem,
and ﬁnds the optimal alignment lying be-
tween the intersection and union of multi-
ple alignment tables by optimizing the pa-
rameter p: the afﬁnely extended real num-
ber deﬁning the order of the power mean
function. The combination approach pro-
duces better alignment tables in terms of
both F-measure and BLEU scores.

1

Introduction

Machine Translation (MT) systems are trained on
bi-text parallel corpora. One of the ﬁrst steps
involved in training a MT system is obtaining
alignments between words of source and target
languages. This is typically done using some
form of Expectation Maximization (EM) algo-
rithm (Brown et al., 1993), (Och and Ney, 2003),
(Vogel et al., 1996). These unsupervised algo-
rithms provide alignment links between english
words ei and the foreign words fj for a given e−f
sentence pair. The alignment pairs are then used
to extract phrases tables (Koehn et al., 2003), hi-
erarchical rules (Chiang, 2005), or tree-to-string
mappings (Yamada and Knight, 2001). Thus, the

accuracy of these alignment links has a signiﬁcant
impact in overall MT accuracy.

One of the commonly used techniques to im-
prove the alignment accuracy is combining align-
ment tables obtained for source to target (e2f) and
target to source (f 2e) directions (Och and Ney,
2003). This combining technique involves obtain-
ing two sets of alignment tables A1 and A2 for the
same sentence pair e − f, and producing a new
set based on union A∪ = A1 ∪ A2 or intersec-
tion A∩ = A1 ∩ A2 or some optimal combination
Ao such that it is subset of A1 ∪ A2 but a super-
set of A1 ∩ A2. How to ﬁnd this optimal Ao is a
key question. A∪ has high precision but low re-
call producing fewer alignments and A∩ has high
recall but low precision.

2 Related Work

Most existing methods for alignment combina-
tion (symmetrization) rely on heuristics to iden-
tify reliable links (Och and Ney, 2003), (Koehn
et al., 2003). The method proposed in (Och and
Ney, 2003), for example, interpolates the intersec-
tion and union of two asymmetric alignment ta-
bles by adding links that are adjacent to intersec-
tion links, and connect at least one previously un-
aligned word. Another example is the method in
(Koehn et al., 2003), which adds links to the inter-
section of two alignment tables that are the diago-
nal neighbors of existing links, optionally requir-
ing that any added links connect two previously
unaligned words.

Other methods try to combine the tables dur-
ing alignment training.
In (Liang et al., 2006),
asymmetric models are jointly trained to maxi-
mize the similarity of their alignments, by opti-

829

mizing an EM-like objective function based on
agreement heuristics. In (Ayan et al., 2004), the
authors present a technique for combining align-
ments based on various linguistic resources such
as parts of speech, dependency parses, or bilingual
dictionaries, and use machine learning techniques
to do alignment combination. One of the main dis-
advantages of (Ayan et al., 2004)’s method, how-
ever, is that the algorithm is a supervised learning
method, and so requires human-annotated data.
Recently, (Xiang et al., 2010) proposed a method
that can handle multiple alignments with soft links
which are deﬁned by conﬁdence scores of align-
ment links.
(Matusov et al., 2004) on the other
hand, frame symmetrization as ﬁnding a set with
minimal cost using use a graph based algorithm
where costs are associated with local alignment
probabilities.

In summary, most existing alignment combina-
tion methods try to ﬁnd an optimal alignment set
Ao that lies between A∩ and A∪ using heuristics.
The main problems with methods based on heuris-
tics are:

1. they may not generalize well across language

pairs

2. they typically do not have any parameters to

optimize

3. most methods can combine only 2 align-

ments at a time

4. most approaches are ad-hoc and are not

mathematically well deﬁned

In this paper we address these issues by propos-
ing a novel mathematical formulation for com-
bining an arbitrary number of alignment tables.
The method frames the combination task as an op-
timization problem, and ﬁnds the optimal align-
ment lying between the intersection and union of
multiple alignment tables by optimizing the pa-
rameter p of the power mean function.

3 Alignment combination using the

power mean

Given an english-foreign sentence pair (eI
1, f J
1 )
the alignment problem is to determine the pres-
ence of absence of alignment links aij between

the words ei and fj, where i ≤ I and j ≤ J. In
this paper we will use the convention that when
aij = 1, words ei and fj are linked, otherwise
aij = 0. Let us deﬁne the alignment tables we ob-
tain for two translation directions as A1 and A2,
respectively. The union of these two alignment
tables A∪ contain all of the links in A1 and A2,
and the intersection A∩ contain only the common
links. Deﬁnitions 1 and 2 below deﬁne A∪ and
A∩ more formally. Our goal is to ﬁnd an align-
ment set Ao such that |A∩| ≤ |Ao| ≤ |A∪| that
maximizes some objective function. We now de-
scribe the power mean (PM) and show how the
PM can represent both the union and intersection
of alignment tables using the same formula.

The power mean:

The power mean is deﬁned by equation 1 below,
where p is a real number in (−∞,∞) and an is a
positive real number.

Sp(a1, a2, ..., an) = (

1
p

ap
k)

1
n

nXk=1

(1)

The power mean, also known as the generalized
mean, has several interesting properties that are
relevant to our alignment combination problem.
In particular, the power mean is equivalent to the
geometric mean G when p → 0 as shown in equa-
tion 2 below:

G(a1, a2, ..., an) = (

1
n

ai)

nYi=1

1
p

(2)

1
n

ap
k)

= lim
(
p→0

nXk=1
The power mean, furthermore, is equivalent to the
maximum function M when p → ∞:
nXk=1

M (a1, a2, ..., an) = max(a1, a2, ..., an)

= lim
(
p→∞

ap
k)

1
n

(3)

1
p

Importantly,
the PM Sp is a non-decreasing
function of p.
This means that Sp is lower
bounded by G and upper-bounded by M for p ∈
[0, ∞]:

G < Sp < M,

0 < p < ∞.

(4)

830

Figure 1: The power-mean is a principled way to interpolate between the extremes of union and inter-
section when combining multiple alignment tables.

They key insight underpinning our mathematical
formulation of the alignment combination prob-
lem is that the geometric mean of multiple align-
ment tables is equivalent to their intersection,
while the maximum of multiple alignment tables
is equivalent to their union.

Let Aq be an alignment with elements aq

ij such
that aq
ij = 1 if words ei and fj are linked, and
aq
ij = 0 otherwise. The union and intersection of
a set of n alignment tables can then be formally
deﬁned as follows:
Deﬁnition 1:

The union of alignments
ij = 1

Deﬁnition 2: The intersection of alignments
ij = 1

A1, A2, ..., An is a set A∪ with a∪ij = 1 if aq
for any q ∈ {1, 2, ..., n}.
A1, A2, ..., An is a set A∩ with a∩ij = 1 if aq
for all q ∈ {1, 2, ..., n}.
Figure 1 depicts a simple example of the align-
ment combination problem for the common case
of alignment symmetrization. Two alignments ta-
bles, Ae→f and Af→e (one-to-many alignments),
need to be combined.
The result of taking
the union A∪ and intersection A∩ of the ta-

bles is shown. A∪ can be computed by taking
the element-wise maximum of Ae→f and Af→e,
which in turn is equal to the power mean Ap of
the elements of these tables in the limit as p → ∞.
The intersection of the two tables, A∩, can simi-
larly be computed by taking the geometric mean
of the elements of Ae→f and Af→e, which is
equal to the power mean Ap of the elements of
these tables in the limit as p → 0. For p ∈ (0,∞),
equation 4 implies that Ap has elements with val-
ues between A∩ and A∪. We now provide formal
proofs for these results when combining an arbi-
trary number of alignment tables.

3.1 The intersection of alignment tables

A1..An is equivalent to their
element-wise geometric mean
G(A1, A2, ..., An), as deﬁned in (2).

P roof : Let A∩ be the intersection of all Aq
where q ∈ {1, 2, .., n}. As per our deﬁnition of
intersection ∩ between alignment tables, A∩ con-
tains links where aq
Let Ag be the set that contains the elements

ij = 1 ∀ q.

831

Then ag
of G(A1, A2, ..., An).
ij
metric mean of the elements aq
{1, 2, .., n}, as deﬁned in equation 2, that is, ag
(Qn
n . This product is equal to 1 iff aq
1∀ q and zero otherwise, since aq
Hence Ag = A∩. Q.E.D.
3.2 The union of alignment tables A1..An is

is the geo-
ij where q ∈
ij =
ij =
ij ∈ {0, 1}∀ q.

q=1 ag
ij)

1

equivalent to their element-wise
maximum M (A1, A2, ..., An), as deﬁned
in (3).

P roof : Let A∪ be the union of all Aq for q ∈
{1, 2, .., n}. As per our deﬁnition of the union be-
tween alignments A∪ has links where aq
ij = 1 for
some q.

Let Am be the set that contain the elements of
M (A1, A2, ..., An). Let am
ij be the maximum of
the elements aq
ij where q ∈ {1, 2, .., n}, as deﬁned
in equation (3). The max function is equal to 1
iff aq
ij = 1 for some q and zero otherwise, since
aq
ij ∈ {0, 1}∀ q. Hence Am = A∪. Q.E.D.
3.3 The element-wise power mean

Sp(A1, A2, ..., An) of alignment tables
A1..An has entries that are
lower-bounded by the intersection of
these tables, and upper-bounded by their
union for p ∈ [0, ∞].

P roof : We have already shown that the union
and intersection of a set of alignment tables are
equivalent to the maximum and geometric mean
of these tables, respectively. Therefore given that
the result in equation 4 is true (we will not prove it
here), the relation holds. In this sense, the power
mean can be used to interpolate between the in-
tersection and union of multiple alignment tables.
Q.E.D.

4 Data
We evaluate the proposed method using an
English-Pashto translation task, as deﬁned by the
DARPA TransTac program. The training data for
this task consists of slightly more than 100K par-
allel sentences. The Transtac task was designed to
evaluate speech-to-speech translation systems, so
all training sentences are conversational in nature.
The sentence length of these utterances varies
greatly, ranging from a single word to more than

Method F-measure
I
H
GDF
PM
PMn
U

0.5979
0.6891
0.6712
0.6984
0.7276
0.6589

Table 1: F-measure Based on Various Alignment
Combination Methods

50 words. 2026 sentences were randomly sampled
from this training data to prepare held out devel-
opment set. The held out Transtac test set consists
of 1019 parallel sentences.

5 Experiments and Discussion
We have shown in the previous sections that union
and intersection of alignments can be mathemat-
ically formulated using the power mean. Since
both combination operations can be represented
with the same mathematical expression, we can
search the combination space “between” the in-
tersection and union of alignment tables by op-
timizing p w.r.t. any chosen objective function.
In these experiments, we deﬁne the optimal align-
ment as the one that maximizes the objective func-
tion f ({aijt},{ˆaijt}, p), where f is standard F-
measure, {ˆaijt} is the set of all estimated align-
ment entries on some dataset, {aijt} is the set of
all corresponding human-annotated alignment en-
tries, and p is the order of the power mean func-
tion.
Instead of attempting to optimize the F-
measure using heuristics, we can now optimize it
by ﬁnding the appropriate power order p using any
suitable numerical optimization algorithm. In our
experiments we used the general simplex algo-
rithm of amoeba search (Nelder and Mead, 1965),
which attempts to ﬁnd the optimal set of parame-
ters by evolving a simplex of evaluated points in
the direction that the F-measure is increasing.

In order to test our alignment combination for-
mulation empirically we performed experiments
on English-Pashto language with data described in
Section 4. We ﬁrst trained two sets of alignments,
the e2f and f2e directions, based on GIZA++
(Och and Ney, 2003) algorithm. We then com-
bined these alignments by performing intersec-

832

tion (I) and union (U). We obtained F-measure of
0.5979 for intersection (I), 0.6589 for union (U).
For intersection the F-measure is lower presum-
ably because many alignments are not shared by
the input alignment tables so the number of links
is under-estimated. We then also re-produced the
two commonly used combination heuristic meth-
ods that are based on growing the alignment di-
agonally (GDF) (Koehn et al., 2003), and adding
links based on reﬁned heuristics (H) (Och and
Ney, 2003), respectively. We obtained F-measure
of 0.6891 for H, and 0.6712 for GDF as shown in
Table 1.

We then used our power mean formulation for
combination to maximize the F-measure function
with the aforementioned simplex algorithm for
tuning the power parameter p, where F-measure
is computed with respect to the hand aligned de-
velopment data, which contains 150 sentences.
This hand aligned development set is different
than the development set for training MT models.
While doing so we also optimized table weights

Wq ∈ (0, 1),Pq Wq = 1, which were applied to

the alignment tables before combining them using
the PM. The Wq allow the algorithm to weight the
two directions differently. We found that the F-
measure function had many local minima so the
simplex algorithm was initialized at several val-
ues of p and {Wq} to ﬁnd the globally optimal
F-measure.
After obtaining power mean outputs for the
alignment entries,
they need to be converted
into binary valued alignment
is,
ij) needs to be converted into a bi-
Sp(a1
nary table. There are many ways to do this con-
version such as simple thresholding or keeping
best N% of the links. In our experiments we used
the following simple selection method, which ap-
pears to perform better than thresholding. First we
sorted links by PM value and then added the links
from the top of the sorted list such that ei and fj
are linked if ei−1 and ei+1 are connected to fj, or
fj−1 and fj+1 is linked to ei, or both ei and fj are
not connected. After tuning power mean parame-
ter and the alignment weights the best parameter
gave an F-measure of 0.6984 which is higher than
commonly used GDF by 2.272% and H by 0.93%
absolute respectively. We observe in Figure 2 that

ij, ...an

ij, a2

links,

that

even though PM has higher F-measure compared
with GDF it has signiﬁcantly fewer number of
alignment links suggesting that PM has improved
precision on the ﬁnding the alignment links. The
presented PM based alignment combination can
be tuned to optimize any chosen objective, so it is
not surprising that we can improve upon previous
results based on heuristics.

One of the main advantages of the combining
alignment tables using the PM is that our state-
ments are valid for any number of input tables,
whereas most heuristic approaches can only pro-
cess two alignment tables at a time. The presented
power mean algorithm, in contrast, can be used
to combine any number of alignments in a sin-
gle step, which, importantly, makes it possible to
jointly optimize all of the parameters of the com-
bination process.

In the second set of experiments the PM ap-
proach, which we call PMn, is applied simultane-
ously to more than two alignments. We obtained
four more sets of alignments from the Berke-
ley aligner (BA) (Liang et al., 2006), the HMM
aligner (HA) (Vogel et al., 1996), the alignment
based on partial words (PA), and alignment based
on dependency based reordering (DA) (Xu et al.,
2009). Alignment I was obtained by using Berke-
ley aligner as an off-the-shelf alignment tool. We
built the HMM aligner based on (Vogel et al.,
1996) and use the HMM aligner for producing
Alignment II. Producing different sets of align-
ments using different algorithms could be useful
because some alignments that are pruned by one
algorithm may be kept by another giving us a big-
ger pool of possible links to chose from.

We produced Alignment III based on partial
words. Pashto is morphologically rich language
with many preﬁxes and sufﬁxes. In lack of a mor-
phological segmenter it has been suggested that
keeping only ﬁrst ‘n’ characters of a word can ef-
fectively reduce the vocabulary size and may pro-
duce better alignments. (Chiang et al., 2009) used
partial words for alignment training in English and
Urdu. We trained such alignments using using
GIZA++ on parallel data with partial words for
Pashto sentences.

The fourth type of alignment we produced,
Alignment IV, was motivated by the (Xu et al.,

833

section and union, but also that combining more
than two alignments is useful. We note that PMn
performed 3.85% absolute better than H (Och and
Ney, 2003), and 5.64% better than GDF heuris-
tics.

In the above experiments the parameters of
the power mean combination method were tuned
on development data to optimize alignment F-
measure, and the performance of several align-
ment combination techniques were compared in
terms of F-measure. However, it is not clear how
correlated alignment F-measures are with BLEU
scores, as explained in (Fraser and Marcu, 2007).
While there is no mathematical problem with
optimizing the parameters of the presented PM-
based combination algorithm w.r.t. BLEU scores,
computationally it is not practical to do so because
each iteration would require a complete training
phase. To further evaluate the quality of the align-
ments methods being compared in this paper, we
built several MT models based on them and com-
pared the resulting BLEU scores.

E2F Dev
0.1064
I
H
0.1028
GDF 0.1256
PM 0.1214
0.1378
PMn
U
0.1062

Test
0.0941
0.0894
0.1091
0.1094
0.1209
0.0897

Table 2: E2F BLEU: PM Alignment Combination
Based MT Model Comparision

We built a standard phrase-based translation
system (Koehn et al., 2003) that utilizes a stack-
based decoder based on an A∗ search. Based on
the combined alignments, we extracted phrase ta-
bles with a maximum phrase length of 6 for En-
glish and 8 for Pashto, respectively. We then
trained the lexicalized reordering model that pro-
duced distortion costs based on the number of
words that are skipped on the target side,
in
a manner similar to (Al-Onaizan and Papineni,
2006). Our training sentences are a compilation
of sentences from various domains collected by
DARPA, and hence we were able to build interpo-
lated language model which weights the domains
differently. We built an interpolated LM for both

Figure 2: Number of Alignments Links for Dif-
ferent Combination Types

2009).
(Xu et al., 2009) showed that transla-
tion between subject-verb-object (English) and
subject-object-verb (Pashto) languages can be im-
proved by reordering the source side of the par-
allel data. They obtained dependency tree of the
source side and used high level human gener-
ated rules to reorder source side using precedence-
based movement of dependency subtrees. The
rules were particularly useful in reordering of
verbs that moved to the end of the sentence. Mak-
ing the ordering of source and target side more
similar may produce better alignments for lan-
guage pairs which differ in verb ordering, as many
alignment algorithms penalize or fail to consider
alignments that link words that differ greatly in
sentence position. A Pashto language expert was
hired to produce similar precedence-based rules
for the English-Pashto language pair. Using the
rules and algorithm described in (Xu et al., 2009)
we reordered all of the source side and used
GIZA++ to align the sentences.

The four additional alignment sets just de-
scribed, including our baseline alignment, Align-
ment V, were combined using the presented PMn
combination algorithm, where n signiﬁes the
number of tables being combined. As seen on
Table 1, we obtained an F-measure of 0.7276
which is 12.97% absolute better than intersection
and 6.87% better than union. Furthermore PMn,
which in these experiments utilizes 5 alignments,
is better than PM by 2.92% absolute. This is an
encouraging result because this not only shows
that we are ﬁnding better alignments than inter-

834

English and Pashto, but for English we had signif-
icantly more monolingual sentences (1.4 million
in total) compared to slightly more than 100K sen-
tences for Pashto. We tuned our MT model using
minimum error rate (Och, 2003) training.

F2E Dev
I
0.1145
H
0.1262
GDF 0.1115
PM 0.1201
PMn
0.1198
0.1111
U

Test
0.1101
0.1193
0.1204
0.1155
0.1196
0.1155

Table 3: F2E BLEU : PM Alignment Combina-
tion Based MT Model Comparision

We built ﬁve different MT models based on
Intersection (I), Union (U), (Koehn et al., 2003)
Grow Diagonal Final (GDF), (Och and Ney, 2003)
H reﬁned heuristics and Power Mean (PMn) align-
ment sets where n = 5. We obtained BLEU (Pa-
pineni et al., 2002) scores for E2F direction as
shown in Table 2. As expected MT model based
on I alignment has the low BLEU score of 0.1064
on the dev set and 0.0941 on the test set on E2F
direction. Intersection, though, has higher preci-
sion, but throws away many alignments, so the
overall number of alignments is too small to pro-
duce a good phrase translation table. Similarly
the U alignment also has low scores (0.1062 and
0.0897) on the dev and test sets, respectively. The
best scores for E2F direction for both dev and test
set is obtained using the model based on PMn al-
gorithm. We obtained BLEU scores of 0.1378 on
the dev set and 0.1209 on the test set which is bet-
ter than all heuristic based methods. It is better
by 1.22 absolute BLEU score on the dev set and
1.18 on a test compared to commonly used GDF
(Koehn et al., 2003) heuristics. The above BLEU
scores were all computed based on 1 reference.
Note that for the e2f direction PM, which com-
bines only 2 alignments, is not worse than any of
the heuristic based methods. Also note that the
difference in the BLEU score of PM and PMn is
quite large, which indicates that combining more
than two alignments using the power mean leads
to substantial gains in performance.

Although we saw signiﬁcant gains on E2F di-

Type PT Size (100K)

I
H

GDF
PM
PMn
U

182.17
30.73
27.65
60.87
25.67
24.54

Table 4: E2F Phrase Table Size

rection we did not see similar gains on F2E di-
rection unfortunately. Matching our expectation
Intersection (I) produced the worse results with
BLEU scores of 0.1145 and 0.1101 on the dev
and test set respectively, as shown in Table 3. Our
PMn algorithm obtained BLEU score of 0.1198
on the dev set and 0.1196 on test set which is
better by 0.83 absolute in dev set over GDF. On
the test set though performance between PMn and
GDF is only slightly different with 0.1196 for
PMn and 0.1204 for GDF. The standard deviation
on test set BLEU scores for F2E direction is only
0.0042 which is one third of the standard devia-
tion in E2F direction at 0.013 signifying that the
alignment seems to make less difference in F2E
direction for our models. One possible explana-
tion for such results is that the Pashto LM for the
E2F direction is trained on a small set of sen-
tences available from training corpus while En-
glish LM for F2E direction was trained on 1.4 mil-
lion sentences. Therefore the English LM, which
is trained on signiﬁcantly more data, is probably
more robust to translation model errors.

Type PT Size (100K)

I
H

GDF
PM
PMn
U

139.98
56.76
22.96
47.50
21.24
20.33

Table 5: F2E Phrase Table Size

Note that different alignments lead to differ-
ent phrase table (PT) sizes (Figure 2). The inter-
section (I) method has the least number of align-
ment links, and tends to produce the largest phrase
tables, because there are less restrictions on the

835

phrases to be extracted. The Union (U) method,
on the other hand, tends to produce the least num-
ber of phrases, because the phrase extraction al-
gorithm has more constraints to satisfy. We ob-
serve that PT produced by intersection is signiﬁ-
cantly larger than others as seen in Tables 4 and
5. The PT size produced by PMn as shown in
Table 4 is between I and U and is signiﬁcantly
smaller than the other heuristic based methods. It
is 7.1% smaller than GDF heuristic based phrase
table. Similarly in F2E direction as well (Table
5) we see the similar trend where PMn PT size
is smaller than GDF by 4.2%. The decrease in
phrase table size and increase in BLEU scores for
most of the dev and test sets show that our PM
based combined alignments are helping to pro-
duce better MT models.

6 Conclusion and Future Work
We have presented a mathematical formulation for
combining alignment tables based on their power
mean. The presented framework allows us to ﬁnd
the optimal alignment between intersection and
union by ﬁnding the best power mean parameter
between 0 and ∞, which correspond to intersec-
tion and union operations, respectively. We eval-
uated the proposed method empirically by com-
puting BLEU scores in English-Pashto transla-
tion task and also by computing an F-measure
with respect to human alignments. We showed
that the approach is more effective than intersec-
tion, union, the heuristics of (Och and Ney, 2003),
and the grow diagonal ﬁnal (GDF) algorithm of
(Koehn et al., 2003). We also showed that our al-
gorithm is not limited to two tables, which makes
it possible to jointly optimize the combination of
multiple alignment tables to further increase per-
formance.

In future work we would like to address two
particular issues. First, in this work we converted
power mean outputs to binary alignment links by
simple selection process. We are currently investi-
gating ways to integrate the binary constraint into
the PM-based optimization algorithm. Second,
we do not have to limit ourselves to alignments ta-
bles that are binary. PM based algorithm can com-
bine alignments that are not binary, which makes
it easier to integrate other sources of information

such as posterior probability of word translation
into the alignment combination framework.

7 Acknowledgment
This work is partially supported by the DARPA
TRANSTAC program under the contract number
of NBCH2030007. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reﬂect the views of DARPA.

References
Al-Onaizan, Yaser and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL.

Ayan, Necip, Bonnie J. Dorr,

, and Nizar Habash.
2004. Multi-align: Combining linguistic and statis-
tical techniques to improve alignments for adaptable
mt. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas.

Brown, P., V. Della Pietra, S. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational
Linguistics, 19(2):263–311.

Chiang, David, Kevin Knight, and Samad Echihabi.
2009. In Presentation at NIST MT 2009 Workshop,
August.

Chiang, David. 2005. A hierarchical phrase-based
In Pro-

model for statistical machine translation.
ceedings of ACL.

Fraser, Alexander and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Comput. Linguist., 33(3):293–303.

Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.

Liang, Percy, Ben Taskar, and Dan Klein.

2006.

Alignment by agreement. In Proceedings of ACL.

Matusov, Evgeny, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation.
In Proceedings of COLING,
page 219, Morristown, NJ, USA.

Nelder, JA and R Mead. 1965. A simplex method for
function minimization. The Computer Journal 7:
308-313.

Och, F. J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.

836

Och, Franz J. 2003. Minimum error rate training in

statistical machine. In Proceedings of ACL.

Papineni, Kishore, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In In Proceedings of
ACL, pages 311–318.

Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in statis-
tical translation. In COLING 96: The 16th Int. Conf.
on Computational Linguistics, pages 836–841.

Xiang, Bing, Yonggang Deng, and Bowen Zhou. 2010.
Diversify and combine: Improving word alignment
for machine translation on low-resource languages.
In Proceedings of ACL.

Xu, Peng, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In NAACL,
pages 245–253, Morristown, NJ, USA.

Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL, pages 523–530, Toulouse, France, July.
ACL.

