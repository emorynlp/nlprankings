



















































Finding Good Conversations Online: The Yahoo News Annotated Comments Corpus


Proceedings of the 11th Linguistic Annotation Workshop, pages 13–23,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Finding Good Conversations Online:
The Yahoo News Annotated Comments Corpus

Courtney Napoles,1 Joel Tetreault,2 Enrica Rosato,3 Brian Provenzale,4 and Aasish Pappu3
1 Johns Hopkins University, napoles@cs.jhu.edu
2 Grammarly, joel.tetreault@grammarly.com
3 Yahoo, {aasishkp|enricar}@yahoo-inc.com

4 Accellion, bprovenzale@gmail.com

Abstract

This work presents a dataset and annota-
tion scheme for the new task of identifying
“good” conversations that occur online,
which we call ERICs: Engaging, Respect-
ful, and/or Informative Conversations. We
develop a taxonomy to reflect features of
entire threads and individual comments
which we believe contribute to identify-
ing ERICs; code a novel dataset of Ya-
hoo News comment threads (2.4k threads
and 10k comments) and 1k threads from
the Internet Argument Corpus; and ana-
lyze the features characteristic of ERICs.
This is one of the largest annotated corpora
of online human dialogues, with the most
detailed set of annotations. It will be valu-
able for identifying ERICs and other as-
pects of argumentation, dialogue, and dis-
course.

1 Introduction

Automatically curating online comments has been
a large focus in recent NLP and social media work,
as popular news outlets can receive millions of
comments on their articles each month (Warzel,
2012). Comment threads often range from vacu-
ous to hateful, but good discussions do occur on-
line, with people expressing different viewpoints
and attempting to inform, convince, or better un-
derstand the other side, but they can get lost among
the multitude of unconstructive comments. We
hypothesize that identifying and promoting these
types of conversations (ERICs) will cultivate a
more civil and constructive atmosphere in online
communities and potentially encourage participa-
tion from more users.

ERICs are characterized by:

• A respectful exchange of ideas, opinions, and/or
information in response to a given topic(s).
• Opinions expressed as an attempt to elicit a di-

alogue or persuade.
• Comments that seek to contribute some new in-

formation or perspective on the relevant topic.
ERICs have no single identifying attribute: for in-
stance, an exchange where communicants are in
total agreement throughout can be an ERIC, as
can an exchange with heated disagreement. Fig-
ures 1 and 2 contain two threads that are charac-
terized by continual disagreement, but one is an
ERIC and the other is not. We have developed a
new coding scheme to label ERICs and identify
six dimensions of comments and three dimensions
of threads that are frequently seen in the comments
section. Many of these labels are for characteris-
tics of online conversations not captured by tra-
ditional argumentation or dialogue features. Some
of the labels we collect have been annotated in pre-
vious work (§2), but this is the first time they are
aggregated in a single corpus at the dialogue level.

In this paper, we present the Yahoo News
Annotated Comments Corpus (YNACC), which
contains 2.4k threads and 10k comments from
the comments sections of Yahoo News articles.
We additionally collect annotations on 1k threads
from the Internet Argument Corpus (Abbott et al.,
2016), representing another domain of online de-
bates. We contrast annotations of Yahoo and IAC
threads, explore ways in which threads perceived
to be ERICs differ in this two venues, and identify
some unanticipated characteristics of ERICs.

This is the first exploration of how charac-
teristics of individual comments contribute to
the dialogue-level classification of an exchange.
YNACC will facilitate research to understand
ERICS and other aspects of dialogue. The cor-
pus and annotations will be available at https:
//github.com/cnap/ynacc.

13



Legend 
Agreement: Agree ! , Disagree " , Adjunct opinion✋
Audience: Broadcast $ , Reply %
Persuasiveness: Persuasive & , Not persuasive '

Sentiment: Mixed ( , Neutral ) , Negative ☹ , Positive +
Topic: Off-topic with article , , off-topic with conv. -
Tone:                         .

Controversial

Controversial

Sarcastic

Sarcastic

$ 	' 	☹

% ' 	☹" 	

% 	& 	)✋

% 	& 	)✋

% ' 	☹☹" 	

% 	& 	☹☹" 	

when a country has to use force to keep it's businesses behind a wall. . . 
something is very wrong. will the next step be forcing the talented and wealthy 
to remain? this strategy did not work well for the soviet union.

A

your solution is?B

@B, lower the govt imposed costs and businesses will stay voluntarily.C

just because a company was started in us, given large tax breakes in the us and 
makes most of its profits in the us does not mean it owes loyalty right? they 
have to appease the shareholders who want more value so lower your cost of 
business by lowering taxes while still getting all the perks is one way of doing it.

D

@D - in your world who eventually pays the taxes that our gov't charges 
business?

C

@C lowering corporate taxes does not equate to more jobs, its only equates to 
corporations making more money. did you think they take their profits and 
make high paying jobs with them? lol wake up!

B

Headline: Allergan CEO: Feds blindsided us on Pfizer deal

Figure 1: An ERIC that is labeled argumentative, positive/respectful, and having continual disagreement.

Controversial
Mean! 	" 	☹

Controversial$ 	% 	&' 	

$ " 	☹' 	
Controversial
Informative$ 	% 	(' 	

Informative$ 	% 	(' 	

Controversial$ 	% 	☹' 	

$ " 	( 	)' 	

$ " 	( 	)' 	

$ " 	( 	)' 	

$ " 	(' 	 	)

quit your whining you are in america assimilate into american society. or go 
back where you came from.

E

american society is that of immigrants and the freedom to practice whatever 
religion you wish. you anti american?

F

@F, you may be an immigrant, but i'm notG

the only reason you are an american is because of immigrants.F

that can be said of all humans. humans migrated from africa. everyone in 
germany is an immigrant.

G

then any statement about they need to "go back" is irrelevant and wrong. thanks 
for proving my point.

F

floridians tell new yorkers to go back. you have no point.G

just because someone says something doesnt make it valid. your point has no 
point.

F

just because someone says something doesnt make it valid. nothing you say is 
valid.

G

that's your opinion. but it's not valid. my factual statement is.F

Headline: 'The Daily Show' Nailed How Islamophobia Hurts the Sikh Community Too

Figure 2: A non-ERIC that is labeled argumentative and off-topic with continual disagreement.

2 Related work

Recent work has focused on the analysis of user-
generated text in various online venues, includ-
ing labeling certain qualities of individual com-
ments, comment pairs, or the roles of individual
commenters. The largest and most extensively an-
notated corpus predating this work is the Internet
Argument Corpus (IAC), which contains approxi-
mately 480k comments in 16.5k threads from on-

line forums in which users debate contentious is-
sues. The IAC has been coded for for topic (3k
threads), stance (2k authors), and agreement, sar-
casm, and hostility (10k comment pairs) (Abbott
et al., 2016; Walker et al., 2012). Comments from
online news articles are annotated in the SEN-
SEI corpus, which contains human-authored sum-
maries of 1.8k comments posted on Guardian ar-
ticles (Barker et al., 2016). Participants described

14



each comment with short, free-form text labels
and then wrote a 150–250-word comment sum-
mary with these labels. Barker et al. (2016) recog-
nized that comments have diverse qualities, many
of which are coded in this work (§3), but did not
explicitly collect labels of them.

Previous works present a survey of how edi-
tors and readers perceive the quality of comments
posted in online news publications (Diakopoulos
and Naaman, 2011) and review the criteria profes-
sional editors use to curate comments (Diakopou-
los, 2015). The latter identifies 15 criteria for cu-
rating user-generated responses, from online and
radio comments to letters to the editor. Our anno-
tation scheme overlaps with those criteria but also
diverges as we wish for the labels to reflect the
nature of all comments posted on online articles
instead of just the qualities sought in editorially
curated comments. ERICs can take many forms
and may not reflect the formal tone or intent that
editors in traditional news outlets seek.

Our coding scheme intersects with attributes
examined in several different areas of research.
Some of the most recent and relevant discourse
corpora from online sources related to this work
include the following: Concepts related to persua-
siveness have been studied, including annotations
for “convincing-ness” in debate forums (Habernal
and Gurevych, 2016), influencers in discussions
from blogs and Wikipedia (Biran et al., 2012),
and user relations as a proxy of persuasion in red-
dit (Tan et al., 2016; Wei et al., 2016). Polite-
ness was labeled and identified in Stack Exchange
and Wikipedia discussions (Danescu-Niculescu-
Mizil et al., 2013). Some previous work focused
on detecting agreement has considered blog and
Wikipedia discussions (Andreas et al., 2012) and
debate forums (Skeppstedt et al., 2016). Sar-
casm has been identified in a corpus of microblogs
identified with the hashtag #sarcasm on Twitter
(González-Ibánez et al., 2011; Davidov et al.,
2010) and in online forums (Oraby et al., 2016).
Sentiment has been studied widely, often in the
context of reviews (Pang and Lee, 2005), and
in the context of user-generated exchanges, posi-
tive and negative attitudes have been identified in
Usenet discussions (Hassan et al., 2010). Other
qualities of user-generated text that are not cov-
ered in this work but have been investigated be-
fore include metaphor (Jang et al., 2014) and toler-
ance (Mukherjee et al., 2013) in online discussion

threads, “dogmatism” of reddit users (Fast and
Horvitz, 2016), and argumentation units in discus-
sions related to technology (Ghosh et al., 2014).

3 Annotation scheme

This section outlines our coding scheme for identi-
fying ERICs, with labels for comment threads and
each comment contained therein.

Starting with the annotation categories from
the IAC and the curation criteria of Diakopoulos
(2015), we have adapted these schemes and iden-
tified new characteristics that have broad coverage
over 100 comment threads (§4) that we manually
examined.

Annotations are made at the thread-level and
the comment-level. Thread-level annotations cap-
ture the qualities of a thread on the whole, while
comment-level annotations reflect the characteris-
tics of each comment. The labels for each dimen-
sion are described below. Only one label per di-
mension is allowed unless otherwise specified.

3.1 Thread labels
Agreement The overall agreement present in a
thread.
• Agreement throughout
• Continual disagreement
• Agreement → disagreement: Begins with

agreement which turns into disagreement.
• Disagreement → agreement: Starts with dis-

agreement that converges into agreement.
Constructiveness A binary label indicating
when a conversation is an ERIC, or has a clear
exchange of ideas, opinions, and/or information
done so somewhat respectfully.1

• Constructive
• Not constructive

Type The overall type or tone of the conversa-
tion, describing the majority of comments. Two
labels can be chosen if conversations exhibit more
than one dominant feature.
• Argumentative: Contains a lot of “back and

forth” between participants that does not nec-
essarily reach a conclusion.
• Flamewar: Contains insults, users “yelling” at

each other, and no information exchanged.
1Note that this definition of constructive differs from that

of Niculae and Danescu-Niculescu-Mizil (2016), who use the
term to denote discrete progress made towards identifying a
point on a map. Our definition draws from the more tradi-
tional meaning when used in the context of conversations as
“intended to be useful or helpful” (Macmillan, 2017).

15



• Off-Topic/digression: Comments are com-
pletely irrelevant to the article or each other,
or the conversation starts on topic but veers off
into another direction.
• Personal stories: Participants exchange per-

sonal anecdotes.
• Positive/respectful: Consists primarily of com-

ments expressing opinions in a respectful, po-
tentially empathetic manner.
• Snarky/humorous: Participants engage with

each other using humor rather than argue or
sympathize. May be on- or off-topic.

3.2 Comment labels

Agreement Agreement expressed with explicit
phrasing (e.g., I disagree...) or implicitly,
such as in Figure 2. Annotating the target of
(dis)agreement is left to future work due to the
number of other codes the annotators need to at-
tend to. Multiple labels can be chosen per com-
ment, since a comment can express agreement
with one statement and disagreement with another.
• Agreement with another commenter
• Disagreement with another commenter
• Adjunct opinion: Contains a perspective that

has not yet been articulated in the thread.
Audience The target audience of a comment.
• Reply to specific commenter: Can be ex-

plicit (i.e., @HANDLE) or implicit (not di-
rectly naming the commenter). The target of
a reply is not coded.
• Broadcast message: Is not directed to a spe-

cific person(s).
Persuasiveness A binary label indicating
whether a comment contains persuasive language
or an intent to persuade.
• Persuasive
• Not persuasive

Sentiment The overall sentiment of a comment,
considering how the user feels with respect to what
information they are trying to convey.
• Negative
• Neutral
• Positive
• Mixed: Contains both positive and negative

sentiments.
Tone These qualities describe the overall tone of
a comment, and more than one can apply.
• Controversial: Puts forward a strong opinion

that will most likely cause disagreement.
• Funny: Expresses or intends to express humor.

• Informative: Contributes new information to
the discussion.
• Mean: The purpose of the comment is to be

rude, mean, or hateful.
• Sarcastic: Uses sarcasm with either intent to

humor (overlaps with Funny) or offend.
• Sympathetic: A warm, friendly comment that

expresses positive emotion or sympathy.
Topic The topic addressed in a comment, and
more than one label can be chosen. Comments are
on-topic unless either Off-topic label is selected.
• Off-topic with the article
• Off-topic with the conversation: A digression

from the conversation.
• Personal story: Describes the user’s personal

experience with the topic.

4 Corpus collection

With the taxonomy described above, we coded
comments from two separate domains: online
news articles and debate forums.

Threads from online news articles YNACC
contains threads from the “comments section” of
Yahoo News articles from April 2016.2 Yahoo fil-
ters comments containing hate speech (Nobata et
al., 2016) and abusive language using a combina-
tion of manual review and automatic algorithms,
and these comments are not included in our cor-
pus. From the remaining comments, we identified
threads, which contain an initial comment and at
least one comment posted in reply. Yahoo threads
have a single-level of embedding, meaning that
users can only post replies under a top-level com-
ment. In total, we collected 521,608 comments in
137,620 threads on 4,714 articles on topics includ-
ing finance, sports, entertainment, and lifestyle.
We also collected the following metadata for each
comment: unique user ID, time posted, headline,
URL, category, and the number of thumbs up and
thumbs down received. We included comments
posted on a thread regardless of how much time
had elapsed since the initial comment because the
vast majority of comments were posted in close se-
quence: 48% in the first hour after an initial com-
ment, 67% within the first three hours, and 92%
within the first 24 hours.

We randomly selected 2,300 threads to anno-
tate, oversampling longer threads since the aver-

2Excluding comments labeled non-English by LangID, a
high-accuracy tool for identifying languages in multiple do-
mains (Lui and Baldwin, 2012)

16



IAC Yahoo
# Threads 1,000 2,400
# Comments 16,555 9,160
Thread length 29± 55 4± 3
Comment length 568± 583 232± 538
Trained 0 1,400 threads

9,160 comments
Untrained 1,000 threads 1,300 threads

Table 1: Description of the threads and comments
annotated in this work and and the number coded
by trained and untrained annotators. Thread length
is in comments, comment length in characters.

age Yahoo thread has only 3.8 comments. The dis-
tribution of thread lengths is 20% with 2–4 com-
ments, 60% 5–8, and 20% 9–15. For a held-out
test set, we collected an additional 100 threads
from Yahoo articles posted in July 2016, with the
same length distribution. Those threads are not in-
cluded in the analysis performed herein.

Threads from web debate forums To test this
annotation scheme on a different domain, we also
code online debates from the IAC 2.0 (Abbott et
al., 2016). IAC threads are categorically differ-
ent from Yahoo ones in terms of their stated pur-
pose (debate on a particular topic) and length. The
mean IAC thread has 29 comments and each com-
ment has 102 tokens, compared to Yahoo threads
which have 4 comments with 51 tokens each. Be-
cause significant attention is demanded to code the
numerous attributes, we only consider IAC threads
with 15 comments or fewer for annotation, but do
not limit the comment length. In total, we se-
lected 1,000 IAC thread to annotate, specifically:
474 threads from 4forums that were coded in the
IAC, all 23 threads from CreateDebate, and 503
randomly selected threads from ConvinceMe.

4.1 Annotation

The corpus was coded by two groups of anno-
tators: professional trained editors and untrained
crowdsourced workers. Three separate annota-
tors coded each thread. The trained editors were
paid contractors who received two 30–45-minute
training sessions, editorial guidelines (2,000-word
document), and two sample annotated threads.
The training sessions were recorded and available
to the annotators during annotation, as were the
guidelines. They could communicate their ques-
tions to the trainers, who were two authors of this
paper, and receive feedback during the training
and annotation phases.

Because training is expensive and time consum-
ing, we also collected annotations from untrained
coders on Amazon Mechanical Turk (AMT). To
simplify the task for AMT, we only solicited
thread-level labels, paying $0.75 per thread. For
quality assurance, only workers located in the
United States or Canada with a minimum HIT
acceptance rate of 95% could participate, and
the annotations were spot-checked by the authors.
Trained annotators coded 1,300 Yahoo threads
and the 100-thread test set on the comment- and
thread-levels; untrained annotators coded thread-
level labels of 1,300 Yahoo threads (300 of which
overlapped with the trained annotations) and 1,000
IAC threads (Table 1). In total, 26 trained and 495
untrained annotators worked on this task.

4.2 Confidence
To assess the difficulty of the task, we also col-
lected a rating for each thread from the trained an-
notators describing how confident they were with
their judgments of each thread and the comments
it comprises. Ratings were made on a 5-level Lik-
ert scale, with 1 being not at all confident and
5 fully confident. The levels of confidence were
high (3.9 ± 0.7), indicating that coders were able
to distinguish the thread and comment codes with
relative ease.

4.3 Agreement levels
We measure inter-annotator agreement with Krip-
pendorff’s alpha (Krippendorff, 2004) and find
that, over all labels, there are substantial levels of
agreement within groups of annotators: α = 0.79
for trained annotators and α = 0.71 and 72 for un-
trained annotators on the Yahoo and IAC threads,
respectively. However, there is lower agreement
on thread labels than comment labels (Table 2).
The agreement of thread type is 25% higher for
the Yahoo threads than the IAC (0.62–0.64 com-
pared to 0.48). The less subjective comment la-
bels (i.e., agreement, audience, and topic) have
higher agreement than persuasiveness, sentiment,
and tone. While some of the labels have only
moderate agreement (0.5 < α < 0.6), we find
these results satisfactory as the agreement levels
are higher than those reported for similarly sub-
jective discourse annotation tasks (e.g., Walker et
al. (2012)).

To evaluate the untrained annotators, we com-
pare the thread-level annotations made on 300 Ya-
hoo threads by both trained and untrained coders,

17



Yahoo IAC
Thread label Trained Untrained Untrained
Agreement 0.52 0.50 0.53
Constructive 0.48 0.52 0.63
Type 0.62 0.64 0.48
Comment label
Agreement 0.80 – –
Audience 0.74 – –
Persuasiveness 0.48 – –
Sentiment 0.50 – –
Tone 0.63 – –
Topic 0.82 – –

Table 2: Agreement levels found for each label
category within trained and untrained groups of
annotators, measured by Krippendorff’s alpha.

Category Label Matches
Constructive class – 0.61
Agreement – 0.62
Thread type Overall 0.81

Argumentative 0.72
Flamewar 0.80
Off-topic 0.82
Personal stories 0.94
Respectful 0.81
Snarky/humorous 0.85

Table 3: Percentage of threads (out of 300) for
which the majority label of the trained annotators
matched that of the untrained annotators.

by taking the majority label per item from each
group of annotators and calculating the percent
of exact matches (Table 3). When classifying
the thread type, multiple labels are allowed for
each thread, so we convert each option into a
boolean and analyze them separately. Only 8% of
the threads have no majority constructive label in
the trained and/or untrained annotations, and 20%
have no majority agreement label. Within both an-
notation groups, there are majority labels on all of
the thread type labels. The category with the low-
est agreement is constructive class with only 61%
of the majority labels matching, followed closely
by agreement (only 62% matching). A very high
percent of the thread type labels (81%). The strong
agreement levels between trained and untrained
annotators suggest that crowdsourcing is reliable
for coding thread-level characteristics.

5 Annotation analysis

To understand what makes a thread constructive,
we explore the following research questions:

1. How does the overall thread categorization
differ between ERICs and non-ERICs? (§5.1)

2. What types of comments make up ERICs

compared to non-ERICs? (§5.2)
3. Are social signals related to whether a thread

is an ERIC? (§5.3)

5.1 Thread-level annotations

Before examining what types of threads are
ERICs, we first compare the threads coded by
different sets of annotators (trained or untrained)
and from different sources (IAC or Yahoo). We
measure the significance of annotation group for
each label with a test of equal proportions for bi-
nary categories (constructiveness and each thread
type) and a chi-squared test of independence for
the agreement label. Overall, annotations by the
trained and untrained annotators on Yahoo threads
are very similar, with significant differences only
between some of the thread type labels (Fig-
ure 3). We posit that the discrepancies between the
trained and untrained annotators is due to the for-
mer’s training sessions and ability to communicate
with the authors, which could have swayed anno-
tators to make inferences into the coding scheme
that were not overtly stated in the instructions.

The differences between Yahoo and IAC
threads are more pronounced. The only label for
which there is no significant difference is per-
sonal stories (p = 0.41, between the IAC and
trained Yahoo labels). All other IAC labels are
significantly different from both trained and un-
trained Yahoo labels (p < 0.001). ERICs are more
prevalent in the IAC, with 70% of threads labeled
constructive, compared to roughly half of Yahoo
threads. On the whole, threads from the IAC
are more concordant and positive than from Ya-
hoo: they have more agreement and less disagree-
ment, more than twice as many positive/respectful
threads, and fewer than half the flamewars.

For Yahoo threads, there is no significant dif-
ference between trained and untrained coders for
constructiveness (p = 0.11) and the argumenta-
tive thread type (p = 0.07; all other thread types
are significant with p < 10−5). There is no signif-
icant difference between the agreement labels, ei-
ther (p = 1.00). Untrained coders are more likely
than trained to classify threads using emotional la-
bels like snarky, flamewar, and positive/respectful,
while trained annotators more frequently recog-
nize off-topic threads. These differences should
be taken into consideration for evaluating the IAC
codes, and for future efforts collecting subjective
annotations through crowdsourcing.

18



0.00 0.25 0.50 0.75 1.00

snarky/humorous

positive/respectful

personal stories

off-topic/digression

flamewar

argumentative

not constructive

constructive

disagreement→agreement
agreement→disagreement

continual disagreement

agreement throughout

Ag
ree

me
nt

Co
ns

tru
cti

ve
ne

ss

Ty
pe

IAC, untrained Yahoo, untrained Yahoo, trained

Figure 3: % threads assigned labels by annotator
type (trained, untrained) and source (Yahoo, IAC).

We measure the strength of relationships be-
tween labels with the phi coefficient (Figure 4).
There is a positive association between ERICs and
all agreement labels in both Yahoo (trained) and
IAC threads, which indicates that concord is not
necessary for threads to be constructive. The ex-
ample in Figure 1 is a constructive thread that is
argumentative and contains disagreement. Thread
types associated with non-ERICs are flamewars,
off-topic digressions, and snarky/humorous ex-
changes, which is consistent across data sources.
The labels from untrained annotators show a
stronger correlation between flamewars and not
constructive compared to the trained annotators,
but the former also identified more flamewars.
Some correlations are expected: across all anno-
tating groups, there is a positive correlation be-
tween threads labeled with agreement throughout
and positive/respectful, and disagreement through-
out is correlated with argumentative (Figures 1
and 2) and, to a lesser degree, flamewar.

The greatest difference between the IAC and
Yahoo are the thread types associated with ERICs.
In the IAC, the positive/respectful label has a
much stronger positive relationship with construc-
tive than the trained Yahoo labels, but this could
be due to the difference between trained and un-
trained coders. Argumentative has a positive cor-
relation with constructive in the Yahoo threads,
but a weak negative relationship is found in the
IAC. In both domains, threads characterized as off-
topic, snarky, or flamewars are more likely to be

non-ERICs. Threads with some level of agree-
ment characterized as positive/respectful are com-
monly ERICs. A two-tailed z-test shows a sig-
nificant difference between the number of ERICs
and non-ERICs in Yahoo articles in the Arts &
Entertainment, Finance, and Lifestyle categories
(p < 0.005; Figure 5).

5.2 Comment annotations

We next consider the codes assigned by trained an-
notators to Yahoo comments (Figure 6). The ma-
jority of comments are not persuasive, reply to a
previous comment, express disagreement, or have
negative sentiment. More than three times as many
comments express disagreement than agreement,
and comments are labeled negative seven times as
frequently as positive. Approximately half of the
comments express disagreement or a negative sen-
timent. Very few comments are funny, positive,
sympathetic, or contain a personal story (< 10%).
Encouragingly, only 6% of comments are off-topic
with the conversation, suggesting that participants
are attuned to and respectful of the topic. Only
20% of comments are informative, indicating that
participants infrequently introduce new informa-
tion to complement the article or discussion.

The only strong correlations are between the bi-
nary labels, but the moderate correlations provide
insight into the Yahoo threads (Figure 7). Some
relationships accord with intuition. For instance,
participants tend to go off-topic with the article
when they are responding to others and not dur-
ing broadcast messages; comments expressing dis-
agreement with a commenter are frequently posted
in a reply to a commenter; comments express-
ing agreement tend to be sympathetic and have
positive sentiment; and mean comments correlate
with negative sentiment. Commenters in this do-
main also express disagreement without partic-
ular nastiness, since there is no correlation be-
tween disagreement and mean or sarcastic com-
ments. The informative label is moderately cor-
related with persuasiveness, suggesting that com-
ments containing facts and new information are
more convincing than those without.

The correlation between comment and thread
labels is shown in Figure 7. Many of the rela-
tionships are unsurprising, like off-topic threads
tend to have off-topic comments, personal-story
threads have personal-story comments; thread
agreement levels correlate with comment-level

19



ag
re

em
en

tt
hr

ou
gh

ou
t

co
nt

in
ua

ld
is

ag
re

em
en

t
ag

re
em

en
t→

di
sa

gr
ee

m
en

t
di

sa
gr

ee
m

en
t→

ag
re

em
en

t
co

ns
tr

uc
tiv

e
no

tc
on

st
ru

ct
iv

e
ar

gu
m

en
ta

tiv
e

fla
m

ew
ar

of
f-

to
pi

c/
di

gr
es

si
on

pe
rs

on
al

st
or

ie
s

po
si

tiv
e/

re
sp

ec
tf

ul
sn

ar
ky

/h
um

or
ou

s

snarky/humorous
positive/respectful

personal stories
off-topic/digression

flamewar
argumentative

not constructive
constructive

disagreement→agreement
agreement→disagreement

continual disagreement
agreement throughout

Ag
ree

me
nt

Con
stru

ctiv
ene

ss

Typ
e

Ag
ree

me
nt

Con
stru

ctiv
ene

ss Typ
e

Yahoo, trained

ag
re

em
en

tt
hr

ou
gh

ou
t

co
nt

in
ua

ld
is

ag
re

em
en

t
ag

re
em

en
t→

di
sa

gr
ee

m
en

t
di

sa
gr

ee
m

en
t→

ag
re

em
en

t
co

ns
tr

uc
tiv

e
no

tc
on

st
ru

ct
iv

e
ar

gu
m

en
ta

tiv
e

fla
m

ew
ar

of
f-

to
pi

c/
di

gr
es

si
on

pe
rs

on
al

st
or

ie
s

po
si

tiv
e/

re
sp

ec
tf

ul
sn

ar
ky

/h
um

or
ou

s

Ag
ree

me
nt

Con
stru

ctiv
ene

ss Typ
e

Yahoo, untrained

ag
re

em
en

tt
hr

ou
gh

ou
t

co
nt

in
ua

ld
is

ag
re

em
en

t
ag

re
em

en
t→

di
sa

gr
ee

m
en

t
di

sa
gr

ee
m

en
t→

ag
re

em
en

t
co

ns
tr

uc
tiv

e
no

tc
on

st
ru

ct
iv

e
ar

gu
m

en
ta

tiv
e

fla
m

ew
ar

of
f-

to
pi

c/
di

gr
es

si
on

pe
rs

on
al

st
or

ie
s

po
si

tiv
e/

re
sp

ec
tf

ul
sn

ar
ky

/h
um

or
ou

s

Ag
ree

me
nt

Con
stru

ctiv
ene

ss Typ
e

φ
IAC, untrained

−1

0

1

Figure 4: Correlation between thread labels, measured by the phi coefficient (φ).

0 100 200 300
Sports

Society & Culture
Science & Technology

Politics
Lifestyle
Finance

Current Events
Celebrities

Arts & Entertainment

ERICs Non-ERICs

Figure 5: Number of threads by article category.

0.00 0.25 0.50 0.75 1.00
personal story

off-topic with conversation
off-topic with article

sympathetic
sarcastic

mean
informative

funny
controversial

positive
neutral

negative
mixed

persuasive
not persuasive

reply to a specific commenter
broadcast message

disagreement with commenter
agreement with commenter

adjunct opinion

Top
ic

Ton
e

Sen
tim

ent

Per
sua

sion
Au

die
nce

Ag
ree

me
nt

Figure 6: % Yahoo comments assigned each label.

agreements; and flamewars are correlated with
mean comments.

In accord with our definition of ERICs, con-
structiveness is positively correlated with informa-
tive and persuasive comments and negatively cor-
related with negative and mean comments. From
these correlations one can infer that argumenta-

tive threads are generally respectful because, while
they are strongly correlated with comments that
are controversial or express disagreement or a
mixed sentiment, there is no correlation with mean
and very little with negative sentiment. More sur-
prising is the positive correlation between contro-
versial comments and constructive threads. Con-
troversial comments are more associated with
ERICs, not non-ERICs, even though the contro-
versial label also positively correlates with flame-
wars, which are negatively correlated with con-
structiveness. The examples in Figures 1–2 both
have controversial comments expressing disagree-
ment, but comments in the second half of the non-
ERIC veer off-topic and are not persuasive, where
the ERIC stays on-topic and persuasive.

5.3 The relationship with social signals

Previous work has taken social signals to be a
proxy for thread quality, using some function of
the total number of votes received by comments
within a thread (e.g., Lee et al. (2014)). Because
earlier research has indicated that user votes are
not completely independent or objective (Sipos et
al., 2014; Danescu-Niculescu-Mizil et al., 2009),
we take the use of votes as a proxy for quality
skeptically ad perform our own exploration of the
relationship between social signals and the pres-
ence of ERICs. On Yahoo, users reacted to com-
ments with a thumbs up or thumbs down and we
collected the total number of such reactions for
each comment in our corpus. First, we com-
pare the total number of thumbs up (TU) and
thumbs down (TD) received by comments in a

20



ad
ju

nc
to

pi
ni

on
ag

re
em

en
tw

ith
co

m
m

en
te

r
di

sa
gr

ee
m

en
tw

ith
co

m
m

en
te

r
br

oa
dc

as
tm

es
sa

ge
re

pl
y

to
a

sp
ec

ifi
c

co
m

m
en

te
r

no
tp

er
su

as
iv

e
pe

rs
ua

si
ve

m
ix

ed
ne

ga
tiv

e
ne

ut
ra

l
po

si
tiv

e
co

nt
ro

ve
rs

ia
l

fu
nn

y
in

fo
rm

at
iv

e
m

ea
n

sa
rc

as
tic

sy
m

pa
th

et
ic

of
f-

to
pi

c
w

ith
ar

tic
le

of
f-

to
pi

c
w

ith
co

nv
er

sa
tio

n
pe

rs
on

al
st

or
y

personal story
off-topic with conversation

off-topic with article
sympathetic

sarcastic
mean

informative
funny

controversial
positive
neutral

negative
mixed

persuasive
not persuasive

reply to a specific commenter
broadcast message

disagreement with commenter
agreement with commenter

adjunct opinion

Ag
ree

me
nt

Au
die

nce

Per
sua

sion

Sen
tim

ent Ton
e

Top
ic

C
om

m
en

tl
ab

el
s

Top
ic

Ton
e

Sen
tim

ent
Per

sua
sionA

udi
enc

eAg
ree

me
nt

Comment labels

ag
re

em
en

tt
hr

ou
gh

ou
t

co
nt

in
ua

ld
is

ag
re

em
en

t
ag

re
em

en
t→

di
sa

gr
ee

m
en

t
di

sa
gr

ee
m

en
t→

ag
re

em
en

t
co

ns
tr

uc
tiv

e
no

tc
on

st
ru

ct
iv

e
ar

gu
m

en
ta

tiv
e

fla
m

ew
ar

of
f-

to
pi

c/
di

gr
es

si
on

pe
rs

on
al

st
or

ie
s

po
si

tiv
e/

re
sp

ec
tf

ul
sn

ar
ky

/h
um

or
ou

s

personal story
off-topic with conversation

off-topic with article
sympathetic

sarcastic
mean

informative
funny

controversial
positive
neutral

negative
mixed

persuasive
not persuasive

reply to a specific commenter
broadcast message

disagreement with commenter
agreement with commenter

adjunct opinion

Ag
ree

me
nt

Con
stru

ctiv
ene

ss Typ
e

Th
re

ad
la

be
ls

Top
ic

Ton
e

Sen
tim

ent
Per

sua
sionA

udi
enc

eAg
ree

me
nt

Comment labels
φ

−1

0

1

Figure 7: Correlation between comment labels (left) and comment labels and thread labels (right).

thread to the coded labels to determine whether
there are any relationships between social signals
and threads qualities. We calculate the relation-
ship between labels in each category with TU and
TD with Pearson’s coefficient for the binary la-
bels and a one-way ANOVA for the agreement cat-
egory. The strongest correlation is between TD
and untrained annotators’ perception of flamewars
(r = 0.21), and there is a very weak to no correla-
tion (positive or negative) between the other labels
and TU, TD, or TU−TD. There is moderate cor-
relation between TU and TD (r = 0.46), suggest-
ing that threads that elicit reactions tend to receive
both thumbs up and down.

The correlation between TU and TD received
by each comment is weaker (r = 0.23). Com-
paring the comment labels to the TU and TD re-
ceived by each comment also show little corre-
lation. Comments that reply to a specific com-
menter are negatively correlated with TU, TD, and
TU−TD (r = 0.30, -0.25, and -0.22, respectively).
The only other label with a non-negligible corre-
lation is disagreement with a commenter, which
negatively correlates with TU (r = −0.21). There
is no correlation between social signal and the
presence of ERICs or non-ERICs. These results
support the findings of previous work and indicate
that thumbs up or thumbs down alone (and, pre-
sumably, up/down votes) are inappropriate proxies
for quality measurements of comments or threads

in this domain.

6 Conclusion

We have developed a coding scheme for label-
ing “good” online conversations (ERICs) and cre-
ated the Yahoo News Annotated Comments Cor-
pus, a new corpus of 2.4k coded comment threads
posted in response to Yahoo News articles. Ad-
ditionally, we have annotated 1k debate threads
from the IAC. These annotations reflect several
different characteristics of comments and threads,
and we have explored their relationships with each
other. ERICs are characterized by argumentative,
respectful exchanges containing persuasive, infor-
mative, and/or sympathetic comments. They tend
to stay on topic with the original article and not
to contain funny, mean, or sarcastic comments.
We found differences between the distribution of
annotations made by trained and untrained anno-
tators, but high levels of agreement within each
group, suggesting that crowdsourcing annotations
for this task is reliable. YNACC will be a valu-
able resource for researchers in multiple areas of
discourse analysis.

Acknowledgments

We are grateful to Danielle Lottridge, Smaranda
Muresan, and Amanda Stent for their valuable in-
put. We also wish to thank the anonymous review-
ers for their feedback.

21



References
Rob Abbott, Brian Ecker, Pranav Anand, and Marilyn

Walker. 2016. Internet Argument Corpus 2.0: An
SQL schema for dialogic social media and the cor-
pora to go with it. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016), pages 4445–4452, Paris,
France, May. European Language Resources Asso-
ciation (ELRA).

Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagree-
ment in threaded discussion. In Proceedings of
the Eight International Conference on Language
Resources and Evaluation (LREC’12), pages 818–
822, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA).

Emma Barker, Monica Lestari Paramita, Ahmet
Aker, Emina Kurtic, Mark Hepple, and Robert
Gaizauskas. 2016. The SENSEI annotated corpus:
Human summaries of reader comment conversations
in on-line news. In Proceedings of the 17th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 42–52, Los Angeles, Septem-
ber. Association for Computational Linguistics.

Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37–45, Montréal, Canada, June.
Association for Computational Linguistics.

Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study
on amazon.com helpfulness votes. In Proceedings
of the 18th International Conference on World Wide
Web, WWW ’09, pages 141–150, New York. ACM.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
250–259, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcasm in Twitter
and Amazon. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 107–116, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.

Nicholas Diakopoulos and Mor Naaman. 2011. To-
wards quality discourse in online news comments.
In Proceedings of the ACM 2011 Conference on
Computer Supported Cooperative Work, CSCW ’11,
pages 133–142, New York. ACM.

Nicholas Diakopoulos. 2015. Picking the NYT picks:
Editorial criteria and automation in the curation of

online news comments. ISOJ Journal, 5(1):147–
166.

Ethan Fast and Eric Horvitz. 2016. Identifying dog-
matism in social media: Signals and models. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pages
690–699, Austin, Texas, November. Association for
Computational Linguistics.

Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,
Mark Aakhus, and Matthew Mitsui. 2014. Ana-
lyzing argumentative discourse units in online in-
teractions. In Proceedings of the First Workshop
on Argumentation Mining, pages 39–48, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.

Roberto González-Ibánez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in Twit-
ter: A closer look. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers-Volume 2, pages 581–586. Association for
Computational Linguistics.

Ivan Habernal and Iryna Gurevych. 2016. Which ar-
gument is more convincing? Analyzing and pre-
dicting convincingness of web arguments using bidi-
rectional LSTM. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1589–
1599, Berlin, Germany, August. Association for
Computational Linguistics.

Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What’s with the attitude? Identi-
fying sentences with attitude in online discussions.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1245–1255, Cambridge, MA, October. Association
for Computational Linguistics.

Hyeju Jang, Mario Piergallini, Miaomiao Wen, and
Carolyn Rose. 2014. Conversational metaphors in
use: Exploring the contrast between technical and
everyday notions of metaphor. In Proceedings of
the Second Workshop on Metaphor in NLP, pages
1–10, Baltimore, MD, June. Association for Com-
putational Linguistics.

Klaus Krippendorff. 2004. Content analysis: An in-
troduction to its methodology. Sage Publications,
Thousand Oaks, CA, 2nd edition.

Jung-Tae Lee, Min-Chul Yang, and Hae-Chang Rim.
2014. Discovering high-quality threaded discus-
sions in online forums. Journal of Computer Sci-
ence and Technology, 29(3):519–531.

Macmillan Publishers Ltd. 2009. The online
English dictionary: Definition of constructive.
http://www.macmillandictionary.com/
dictionary/american/constructive.
Accessed January 20, 2017.

22



Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 25–30, Jeju Island, Korea, July. Association
for Computational Linguistics.

Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analy-
sis of tolerance in online discussions. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1680–1690, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Vlad Niculae and Cristian Danescu-Niculescu-Mizil.
2016. Conversational markers of constructive dis-
cussions. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 568–578, San Diego, Califor-
nia, June. Association for Computational Linguis-
tics.

Chikashi Nobata, Joel Tetreault, Achint Thomas,
Yashar Mehdad, and Yi Chang. 2016. Abu-
sive language detection in online user content. In
Proceedings of the 25th International Conference
on World Wide Web, pages 145–153. International
World Wide Web Conferences Steering Committee.

Shereen Oraby, Vrindavan Harrison, Lena Reed,
Ernesto Hernandez, Ellen Riloff, and Marilyn
Walker. 2016. Creating and characterizing a di-
verse corpus of sarcasm in dialogue. In Proceed-
ings of the 17th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue, pages 31–41,
Los Angeles, September. Association for Computa-
tional Linguistics.

Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
the 43rd annual meeting on association for compu-
tational linguistics, pages 115–124. Association for
Computational Linguistics.

Ruben Sipos, Arpita Ghosh, and Thorsten Joachims.
2014. Was this review helpful to you?: It depends!
Context and voting patterns in online content. In
Proceedings of the 23rd International Conference on
World Wide Web, pages 337–348. ACM.

Maria Skeppstedt, Magnus Kerren, Carita Sahlgren,
and Andreas Paradis. 2016. Unshared task: (Dis)
agreement in online debates. In 3rd Workshop
on Argument Mining (ArgMining’16), Berlin, Ger-
many, August 7-12, 2016, pages 154–159. Associa-
tion for Computational Linguistics.

Chenhao Tan, Vlad Niculae, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2016. Win-
ning arguments: Interaction dynamics and persua-
sion strategies in good-faith online discussions. In
Proceedings of the 25th International Conference
on World Wide Web, pages 613–624. International
World Wide Web Conferences Steering Committee.

Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob
Abbott, and Joseph King. 2012. A corpus for
research on deliberation and debate. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istan-
bul, Turkey, May. European Language Resources
Association (ELRA).

Charlie Warzel. 2012. Everything in moderation.
Adweek, June 18. http://www.adweek.com/
digital/everything-moderation-
141163/. Accessed February 20, 2017.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this
post persuasive? Ranking argumentative comments
in online forum. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 195–200,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

23


