










































ETS: An Error Tolerable System for Coreference Resolution


Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76–80,
Portland, Oregon, 23-24 June 2011. c©2011 Association for Computational Linguistics

ETS: An Error Tolerable System for Coreference Resolution

Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lü
Key Lab. of Intelligent Information Processing

Institute of Computing Technology
Chinese Academy of Sciences

P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn

Abstract

This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al., 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.

1 Introduction

Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al. (2007) have ever used this corpus
for similar unrestricted coreference task.

Our approach to this year’s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development

1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/

set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al., 2008) for
detailed definitions.

Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al. (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.

As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.

2 Mention Detection

The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training

76



Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. “my daughter” is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.

instances using gold boundaries provided by official
data.

The first stage of our system consists of following
three successive steps:

• Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.

• Extracting constituents with the same tags as
the last step from packed forest.

• Extracting Named Entity recognized by given
data.

It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention “my daughter” has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline’s 97%.

Since the requirement of this year’s task is
to model unrestricted coreference, intuitively, we

should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.

3 Determining Coreference

This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.’s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj−1.

We use the following features to train our classi-
fier.
Features in Soon et al.’s work (Soon et al., 2001)
Lexical features

IS PREFIX: whether the string of one mention is
prefix of the other;

IS SUFFIX: whether the string of one mention is
suffix of the other;

ACRONYM: whether one mention is the acronym
of the other;
Distance features

SENT DIST: distance between the sentences con-
taining the two mentions;

MEN DIST: number of mentions between two
mentions;
Grammatical features

IJ PRONOUN: whether both mentions are pro-
noun;

I NESTED: whether mention i is nested in an-
other mention;

J NESTED: whether mention j is nested in an-
other mention;
Syntax features

HEAD: whether the heads of two mentions have
the same string;

HEAD POS: whether the heads of two mentions
have the same POS;

HEA POS PAIRS: pairs of POS of the two men-
tions’ heads;

77



Semantic features
WNDIST: distance between two mentions in

WordNet;
I ARG0: whether mention i has the semantic role

of Arg0;
J ARG0: whether mention j has the semantic role

of Arg0;
IJ ARGS: whether two mentions have the seman-

tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-

rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.

3.1 Creation of Entities

This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al., 2004) and propose
to use beam search to find global optimal partition.

Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
“closed” formula B(n) = 1e

∑∞
k=0

kn

k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.

Formally, the task is to optimize the following ob-
jective,

ŷ = arg max
ϕ∈P

∑
e∈ϕ

Prob(e) (1)

where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),

Prob(e) =
∑

i∈e,j∈e
pos(mi, mj)

+
∑

i∈e,j /∈e

neg(mi, mj)
(2)

where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.

Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:

• [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]

• [AB][C][D], [ABD][C],[AB][CD]

• [A][BC][D], [AD][BC], [A][BCD]

• [ABC][D], [ABCD]

The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A, D) +
neg(B, D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al. (2004) for detailed description, since our ap-
proach is almost similar to theirs.

4 Experiments

4.1 Data Preparation

The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and

78



Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%

baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%

sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%

Table 1: Experimental results on development set (F score).

Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%

Table 2: Experimental results on development set with different training division (F score).

take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.

4.2 Experimental Results

We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.

Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-

3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-

tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.

inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.

To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:

• sys1: all training corpus + WSJ development
corpus

• sys2: WSJ training corpus + WSJ development
corpus

Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.

5 Conclusion

In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.

6 Acknowledgement

The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.

79



References
E.T. Bell. 1934. Exponential numbers. The American

Mathematical Monthly, 41(7):411–419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-

best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173–180.
Association for Computational Linguistics.

Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.

Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594, Columbus, Ohio, June.

Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.

X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135–es. As-
sociation for Computational Linguistics.

H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192–199.
Citeseer.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396–1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.

Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.

W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521–
544.

80


