710

Coling 2010: Poster Volume, pages 710–718,

Beijing, August 2010

Fast-Champollion: A Fast and Robust

Sentence Alignment Algorithm

Peng Li and Maosong Sun

Department of Computer Science and Technology

State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
pengli09@gmail.com, sms@tsinghua.edu.cn

Ping Xue

The Boeing Company

ping.xue@boeing.com

Abstract

Sentence-level aligned parallel texts are
important resources for a number of nat-
ural language processing (NLP) tasks and
applications such as statistical machine
translation and cross-language informa-
tion retrieval. With the rapid growth
of online parallel texts, efﬁcient and ro-
bust sentence alignment algorithms be-
come increasingly important.
In this
paper, we propose a fast and robust
sentence alignment algorithm, i.e., Fast-
Champollion, which employs a combi-
nation of both length-based and lexicon-
based algorithm. By optimizing the pro-
cess of splitting the input bilingual texts
into small fragments for alignment, Fast-
Champollion, as our extensive experi-
ments show, is 4.0 to 5.1 times as fast
as the current baseline methods such as
Champollion (Ma, 2006) on short texts
and achieves about 39.4 times as fast on
long texts, and Fast-Champollion is as ro-
bust as Champollion.

Introduction

1
Sentence level aligned parallel corpora are very
important resources for NLP tasks including ma-
chine translation, cross-language information re-
trieval and so on. These tasks typically require
support by large aligned corpora. In general, the
more aligned text we have, the better result we
achieve. Although there is a huge amount of bilin-
gual text on the Internet, most of them are either
only aligned at article level or even not aligned
at all. Sentence alignment is a process mapping

sentences in the source text to their correspond-
ing units in the translated text. Manual sentence
alignment operation is both expensive and time-
consuming, and thus automated sentence align-
ment techniques are necessary. A sentence align-
ment algorithm for practical use should be (1)
fast enough to process large corpora, (2) robust
enough to tackle noise commonly present in the
real data, and (3) effective enough to make as few
mistakes as possible.

Various sentence alignment algorithms have
been proposed, which generally fall into three
types:
length-based, lexicon-based, and the hy-
brid of the above two types. Length-based algo-
rithms align sentences according to their length
(measured by character or word). The ﬁrst length-
based algorithm was proposed in (Brown et al.,
1991). This algorithm is fast and has a good per-
formance if there is minimal noise (e.g., sentence
or paragraph omission) in the input bilingual texts.
As this algorithm does not use any lexical infor-
mation, it is not robust. Lexicon-based algorithms
are usually more robust than the length-based al-
gorithm, because they use the lexical information
from source and translation lexicons instead of
solely sentence length to determine the transla-
tion relationship between sentences in the source
text and the target text. However, lexicon-based
algorithms are slower than length-based sentence
alignment algorithms, because they require much
more expensive computation. Typical lexicon-
based algorithms include (Ma, 2006; Chen, 1993;
Utsuro et al., 1994; Melamed, 1996). Sentence
length and lexical information are also combined
to achieve more efﬁcient algorithms in two ways.
One way is to use both sentence length and lex-

711

ical information together to determine whether
two sentences should be directly aligned or not
(Simard et al., 1993; Wu, 1994). The other way is
to produce a rough alignment based on sentence
length (and possibly some lexical information at
the same time), and then build more precise align-
ment by using more effective lexicon-based algo-
rithms (Moore, 2002; Varga et al., 2005). But both
of the two ways suffer from high computational
cost and are not fast enough for processing large
corpora.

Lexical information is necessary for improving
robustness of a sentence alignment algorithm, but
use of lexical information will introduce higher
computational cost and cause a lower speed. A
common fact is that the shorter the text is, the
less combination possibilities it would introduce
and the less computational cost it would need. So
if we can ﬁrst split the input bilingual texts into
small aligned fragments reliably with a reasonable
amount of computational cost, and then further
align these fragments one by one, we can speed
up these algorithms remarkably. This is the main
idea of our algorithm Fast-Champollion.

The rest of this paper is organized as fol-
lows: Section 2 presents formal deﬁnitions of sen-
tence alignment problem, and brieﬂy reviews the
length-based sentence alignment algorithm and
Champollion algorithm; Section 3 proposes the
Fast-Champollion algorithm. Section 4 shows the
experiment results; and Section 5 is the conclu-
sion.

2 Deﬁnitions and Related Work

2.1 Deﬁnitions and Key Points
A segment is one or more consecutive sen-
tence(s). A fragment consists of one segment of
the source text (denoted by S) and one segment of
the target text (denoted by T ), and a fragment can
be further divided into one or more beads. A bead
represents a group of one or more sentences in
the source text and the corresponding sentence(s)
in the target text, denoted by Ai = (SAi; TAi) =
(Sai−1+1, Sai−1+2,··· , Sai; Tbi−1+1, Tbi−1+2,··· ,
Tbi), where Si and Tj are the ith and jth sentence
of S and T respectively.

In practice, we rarely encounter crossing align-

ment, e.g., sentences Si and Sj of the source lan-
guage are aligned to the sentences Tj and Ti of
the target language respectively. But much more
effort has to be taken for an algorithm to process
crossing alignment well. So we do not consider
crossing alignment here.

In addition, only a few type of beads are fre-
quently observed in the real world. As it can save
signiﬁcantly in terms of computational cost and it
would not do signiﬁcant harm to algorithm with-
out considering rare bead types, a common prac-
tice for designing sentence alignment algorithms
is to only consider the frequently observed types
of beads. Following this practice, we only con-
sider beads of 1-to-0, 0-to-1, 1-to-1, 1-to-2, 2-to-
1, 1-to-3, 3-to-1, 1-to-4, 4-to-1 and 2-to-2 types in
our algorithm, where n-to-m means the bead con-
sists of n sentence(s) of the source language and
m sentence(s) of the target language.

2.2 Length-based Sentence Alignment

Algorithm

Length-based sentence alignment algorithm was
ﬁrst proposed in (Brown et al., 1991). This algo-
rithm captures the idea that long or short sentences
tend to be translated into long or short sentences.
A probability is produced for each bead based on
the sentence length, and a dynamic programming
algorithm is used to search for the alignment with
the highest probability, which is treated as the best
alignment.

This algorithm is fast and can produce good
alignment when the input bilingual texts do not
contain too much noise, but it is not robust, be-
cause it only uses the sentence length information.
When there is too much noise in the input bilin-
gual texts, sentence length information will be no
longer reliable.

2.3 Champollion Aligner
Champollion aligner was proposed in (Ma, 2006).
It borrows the idea of tf-idf value, which is widely
used in information retrieval, to weight term1 pair
similarity. Greater weight is assigned to the less
frequent translation term pairs, because these term

1Here terms are not limited to linguistic words, but also

can be tokens like “QX6800”

712

pairs have much stronger evidence for two seg-
ments to be aligned. For any two segments, a sim-
ilarity is assigned based on the term pair weight,
sentence number and sentence length. And the dy-
namic programming algorithm is used to search
for the alignment with the greatest total similarity.
This alignment is treated as the best alignment.

Champollion aligner can produce good align-
ment even on noisy input as reported in (Ma,
2006).
Its simplicity and robustness make it a
good candidate for practical use. But this aligner
is slow. Because its time complexity is O(n2) and
it has to look up the dictionary multiple times in
each step of the dynamic programming algorithm,
which needs higher computational cost.

3 Fast-Champollion Algorithm

In this section we propose a new sentence align-
ment algorithm: Fast-Champollion.
Its basis
is splitting the input bilingual texts into small
aligned fragments and then further aligning them
one by one to reduce running time while maintain-
ing Champollion-equivalent (or better) alignment
quality; it takes the advantages of both length-
based and lexicon-based algorithms to the maxi-
mum extent. The outline of the algorithm is that
ﬁrst the length-based splitting module is used to
split the input bilingual texts into aligned frag-
ments, and then the components of each of these
fragments will be identiﬁed and aligned by a
Champollion-based algorithm. The details are de-
scribed in the following sections.

3.1 Length-based Splitting Module

Although length-based sentence alignment algo-
rithm is not robust enough, it can produce rough
alignment very fast with a certain number of re-
liably translated beads. Length-based splitting
module is designed to select these reliably trans-
lated beads to be used for delimiting and splitting
the input bilingual texts into fragments. These
beads will be referred to as anchor beads in the
remaining sections.

There are four steps in this module as described

below in detail.

Step 1: decide whether to skip step 2-4 or not
When there is too much noise in the input bilin-
gual texts, the percentage of reliably translated
beads in the alignment produced by the length-
based algorithm will be very low. In this case, we
will skip step 2 through 4.

An evidence for such a situation is that the
difference between the sentence numbers of the
source and target language is too big. Suppose
NS and NT are the number of sentences of the
source and target language respectively. We spec-
ify r = |NS − NT|/ min{NS, NT} as a measure
of the difference, where min means minimum. If
r is bigger than a threshold, we say the difference
is too big. In our experiments, the threshold is set
as 0.4 empirically.

Step 2: align the input texts using
length-based algorithm

In this step, length-based sentence alignment
algorithm is used to align the input bilingual texts.
Brown, et al. (1991) models the process of sen-
tence alignment as two steps. First, a bead is gen-
erated according to a ﬁxed probability distribution
over bead types, and then sentence length in the
bead is generated according to this model: for the
0-to-1 and 1-to-0 type of beads, it is assumed that
the sentence lengths are distributed according to
a probability distribution estimated from the data.
For other type of beads, the lengths of sentences of
the source language are generated independently
from the probability distribution for the 0-to-1 and
1-to-0 type of beads, and the total length of sen-
tences of the target language is generated accord-
ing to a probability distribution conditioned on the
total length of sentences of the source language.
For a bead Ai = (SAi; TAi), lSAi and lTAi are
the total lengths of sentences in SAi and TAi re-
spectively, which are measured by word2. Brown,
et al. (1991) assumed this conditioned probability
distribution is

P rob(lTAi|lSAi) = α exp(cid:18)−

2σ2 (cid:19) ,
(λi − µ)2

where λi = log(lTAi/lSAi) and α is a normal-
ization factor. Moore (2002) assumed the condi-
2For Chinese, word segmentation should be done ﬁrst to

identify words.

713

tioned probability distribution is

,

exp (−lSAir) (lSAir)lTAi

lTAi!

P rob(lTAi|lSAi) =
where r is the ratio of the mean length of sen-
tences of the target language to the mean length
of sentences of the source language. We tested the
two models on our development corpus and the re-
sult shows that the ﬁrst model performs better, so
we choose the ﬁrst one.

Step 3: determine the anchor beads

In this step, the reliably translated beads in
the alignment produced by the length-based algo-
rithm in Step 2 will be selected as anchor beads.

The length-based algorithm can generate a
probability for each bead it produces. So a triv-
ial way is to choose the beads with a probability
above certain threshold as anchor beads. But as
pointed out before, when there is too much noise,
the alignment produced by the length-based algo-
rithm is no longer reliable, and so is it with the
probability. A fact is that if we select a non-
translated bead as an anchor bead, we will split
the input bilingual texts into wrong fragments and
may cause many errors. So we have to make de-
cision conservatively in this step and we decide to
use lexical information instead of the probability
to determine the anchor beads.

For a bead Ai = (SAi; TAi), the proportion of
translation term-pairs is a good measure for de-
termine whether this bead is reliably translated
or not. In addition, use of local information will
also be greatly helpful. To explain the use of “lo-
cal information”, let’s deﬁne the ﬁngerprint of a
sentence ﬁrst. Suppose we have a sequence of
sentences S1, S2,··· , Sm, and W (Si)is the set of
distinct words in Si, then the ﬁngerprint of Si is

f (Si) = W (Si) − W (Si−1) − W (Si+1),

and specially

f (S1) = W (S1) − W (S2),

As you can see, the ﬁngerprint of a sentence is the
set of words in the sentence that do not appear in
the adjacent sentence(s), and thus can distinguish
this sentence from its neighbors. So ﬁngerprint
is also a good measure. By combining these two
measures together, we can select out more reliably
translated beads.

For a word w, we use dD(w) to denote the set
of all its translations in a bilingual dictionary D,
and use tD(w) to denote the union of {w} and
dD(w), i.e., tD(w) = {w} ∪ dD(w). Given two
sets of words A and B. We say a word w of A is
translated by B if either one of its translations in
the dictionary D or the word itself appears in B,
i.e., tD(w)∩ B 6= ∅. The set of all the words of A
that are translated by B is:

hD(A, B) = {w ∈ A and tD(w) ∩ B 6= ∅}.

Then the proportion of terms in A that are trans-
lated by B is

rD(A, B) = |hD(A, B)|

|A|

.

We specify the proportion of translation term
to be

pairs in a bead, denoted as arD(Ai),
min{rD(W (SAi), W (TAi)), rD(W (TAi), W (SAi))},
where W (SAi) and W (TAi) are the sets of dis-
tinct words in SAi and TAi respectively. Also we
specify the proportion of translation term-pairs
in the ﬁngerprint, denoted as f rD(Ai),
to be
min{rD(f (SAi), f (TAi)), rD(f (TAi), f (SAi))}.
Given thresholds T Har and T Hf r, a bead is
selected as an anchor bead when arD(Ai) and
f rD(Ai) are not smaller than T Har and T Hf r
respectively. We will show that Fast-Champollion
algorithm is not sensitive to T Har and T Hf r to
some extent in Section 4.2.

Step 4: split the input bilingual texts

The anchor beads determined in Step 3 are used
to split the input texts into fragments. The ending
location of each anchor bead is regarded as a split-
ting point, resulting in two fragments.

f (Sm) = W (Sm) − W (Sm−1).

The ﬁngerprints of SAi and TAi, denoted by
f (SAi) and f (TAi), are the unions of all the ﬁn-
gerprints of sentences in SAi and TAi respectively.

3.2 Aligning Fragments with Champollion

Aligner

The similarity function used by Champollion
aligner is deﬁned as follows. Given two (source

714

and target language) groups of sentences in a
fragment, denoted by GS=S1, S2,··· ,Sm and
GT =T1, T2,··· ,Tn, suppose there are k pairs
of
translated terms in GS and GT denoted
by (ws1, wt1),(ws2, wt2),··· ,(wsk, wtk), where
wsi is in GS and wti is in GT . For each pair of
the translated terms (wsi, wti), deﬁne idtf (wsi)
to be

T otal # of terms in the whole document

# occurrences of wsi in GS

,

and deﬁne

stf (wsi, wti) = min{stf (wsi), stf (wti)},

where stf (wsi) and stf (wti) are the frequency
of wsi and wti in GS and GT respectively. The
similarity between GS and GT is deﬁned as

kPi=1

log (idtf (wsi) × stf (wsi, wti))
×alignment penalty
×length penalty,

where alignment penalty is 1 for 1-to-1 align-
ment type of beads and a number between 0 and 1
for other type of beads, length penalty is a func-
tion of the total sentence lengths of GS and GT .

The reason for choosing Champollion aligner
instead of other algorithms will be given in Sec-
tion 4.2. And another question is how idtf values
should be calculated. idtf is used to estimate how
widely a term is used. An intuition is that idtf
will work better if the texts are longer, because
if the texts are short, most words will have a low
frequency and will seem to only appear locally. In
Fast-Champollion, we calculate idtf according to
the whole document instead of each fragment. In
this way, a better performance is achieved.

3.3 Parameter Estimation
A development corpus is used to estimate the pa-
rameters needed by Fast-Champollion.

For the length-based algorithm, there are ﬁve
parameters that need to be estimated. The ﬁrst one
is the probability distribution over bead types. The
ratio of different types of beads in the develop-
ment corpus is used as the basis for the estimation.
The second and third parameters are the proba-
bility distributions over the sentence length of the

source language and the target language. These
distributions are estimated as the distributions ob-
served from the input bilingual texts. That is to
say, these two distributions will not be the same
for different bilingual input texts. The forth and
ﬁfth are µ and σ. They are estimated as the mean
and variance of λi over the development corpus.
For Champollion aligner, alignment penalty
and length penalty need to be determined. Be-
cause the Perl version of Champollion aligner3
is well developed, we borrow the two deﬁnitions
from it directly.

4 Experiments
4.1 Datasets and Evaluation Metrics
We have two English-Chinese parallel corpora,
one for the development purpose and one for the
testing purpose. Both of the two corpora are col-
lected from the Internet and are manually aligned.
The development corpus has 2,004 beads.
Given the space constraint, detailed information
about the development corpus is omitted here.

The testing corpus contains 26 English-Chinese
bilingual articles collected from the Internet, in-
cluding news reports, novels, science articles,
television documentary subtitles and the record of
government meetings. There are 9,130 English
sentences and 9,052 Chinese sentences in these ar-
ticles4. The number of different type of beads in
the golden standard answer is shown in Table 1.

Type
1:1

1:2 2:1
1:3 3:1
1:4 4:1

2:2

1:0 0:1
others
total

Number
7275
846
77
16
32
482
17
8745

Percentage(%)
83.19
9.67
0.88
0.18
0.37
5.51
0.19
100.00

Table 1: Types of beads in the golden standard

Both the Fast-Champollion algorithm and the
Champollion aligner need a bilingual dictionary
and we supply the same bidirectional dictionary to

3http://champollion.sourceforge.net
4The deﬁnition of “sentence” is slightly different from the
common sense here. We also treat semicolon and colon as the
end of a sentence.

715

them in the following evaluations. This dictionary
contains 45,439 pair of English-Chinese transla-
tion terms.

We use four commonly used measures for eval-
uating the performance of a sentence alignment
algorithm, which are the running time,
Precision = |GB ∩ P B|

,

|P B|
Recall = |GB ∩ P B|

,

|GB|

and

F1-measure =

2 × P recision × Recall
P recision + Recall

,

where GB is the set of beads in the golden stan-
dard, and P B is the set of beads produced by the
algorithm.

All the following experiments are taken on a PC

with an Intel QX6800 CPU and 8GB memory.

4.2 Algorithm Design Issues
Why Choose Champollion?

We compared Champollion aligner with two
other sentence alignment algorithms which also
make use of lexical information. And the result
is shown in Table 2. “Moore-1-to-1” and “Moore-
all” are corresponding to the algorithm proposed
in (Moore, 2002). The difference between them is
how Recall is calculated. Moore’s algorithm can
only output 1-to-1 type of beads. For “Moore-1-
to-1”, we only consider beads of 1-to-1 type in
the golden standard when calculating Recall, but
all types of beads are considered for “Moore-all”.
The result suggests that ignoring the beads that are
not of 1-to-1 type does have much negative effect
on the overall performance of Moore’s algorithm.
Our goal is to design a general purpose sentence
alignment algorithm that can process frequently
observed types of beads. So Moore’s algorithm is
not a good choice. Hunalign refers to the hunalign
algorithm proposed in (Varga et al., 2005). The re-
sources provided to Champollion aligner and hu-
nalign algorithm are the same in the test, but hu-
nalign algorithm’s performance is much lower. So
hunalign algorithm is not a good choice either.
Champollion algorithm is simple and has a high
overall performance. So it is a better choice for
us.

Aligner

Champollion
Moore-1-to-1

Moore-all
Hunalign

Precision
0.9456
0.9529
0.9529
0.8813

Recall
0.9546
0.9436
0.7680
0.9037

F1-measure
0.9501
0.9482
0.8505
0.8923

Table 2: The performance of different aligners on
the development corpus

The Effect of T Har and T Hf r

T Har and T Hf r are two thresholds for select-
ing anchor beads in Step 3 of length-based split-
ting module. In order to investigate the effect of
these two thresholds on the performance of Fast-
Champollion, we run Fast-Champollion on the de-
velopment corpus with different T Har and T Hf r.
Both T Har and T Hf r vary from 0 to 1 with step
0.05. And the running time and F1-measure are
shown in Figure 1 and Figure 2 respectively.

Figure 1: The running time corresponding to dif-
ferent T Har and T Hf r

Figure 2: The F1-measure corresponding to dif-
ferent T Har and T Hf r

)
s
(
e
m

i
t
 
g
n
i
n
n
u
R

200

150

100

50

0
1

0.5

T

H
fr

0

0

0.5

T H a r

1

1

e
r
u
s
a
e
m
−
1
F

0.95

0.9

0.85

0.8
1

0.5

T

H

fr

0

0

0.5

T H a r

1

716

From Figure 1 and Figure 2, we see that for a
large range of the possible values of T Har and
T Hf r, the running time of Fast-Champollion in-
creases slowly while F1-measure are nearly the
same. In other words, Fast-Champollion are not
sensitive to T Har and T Hf r to some extent. So
making choice for the exact values of T Har and
T Hf r becomes simple. And we use 0.5 for both
of them in the following experiments.

4.3 Performance of Fast-Champollion
We use three baselines in the following evalua-
tions. One is an implementation of the length-
based algorithm in Java, one is a re-implemented
Champollion aligner in Java according to the Perl
version, and the last one is Fast-Champollion-
Recal. Fast-Champollion-Recal is the same as
Fast-Champollion except that it calculates idtf
values according to the fragments themselves in-
dependently instead of the whole document, and
the Java versions of the length-based algorithm
and Champollion aligner are used for evaluation.
Performance on Texts from the Internet

Table 3 shows the performance of Fast-
Champollion and the baselines on the testing cor-
pus. The result shows that Fast-Champollion
achieves slightly better performance than Fast-
Champollion-Recal. The running time of Cham-
pollion is about 2.6 times longer than Fast-
Champollion with lower Precision, Recall and
F1-measure.
It should be pointed out that Fast-
Champollion achieves better Precision, Recall and
F1-measure than Champollion does because the
splitting process may split the regions hard to
align into different fragments and reduces the
chance for making mistakes. Because of the noise
in the corpus, the F1-measure of the length-based
algorithm is low. This result suggests that Fast-
Champollion is fast, robust and effective enough
for aligning texts from the Internet.
Robustness of Fast-Champollion

In order to make a more precise investigation
on the robustness of Fast-Champollion against
noise, we made the following evaluation. First
we manually removed all the 1-to-0 and 0-to-
1 type of beads from the testing corpus to pro-
duce a clean corpus. This corpus contains 8,263

beads. Then we added 8263×n% 1-to-0 or 0-to-
1 type of beads to this corpus at arbitrary posi-
tions to produce a series of noisy corpora, with
n having the values of 5,10,...,100. Finally we
ran Fast-Champollion algorithm and the baselines
on these corpora respectively and the results are
shown in Figure 3 and Figure 4, which indi-
cate that for Fast-Champollion, when n increases
1, Precision drops 0.0021, Recall drops 0.0038
and F1-measure drops 0.0030 on average, which
are very similar to those of Champollion, but
Fast-Champollion is 4.0 to 5.1 times as fast as
Champollion. This evaluation proves that Fast-
Champollion is robust against noise and is a more
reasonable choice for practical use.

Figure 3: Running Time of Fast-Champollion,
Fast-Champollion-Recal, Champollion and the
length-based algorithm

Figure 4: Precision (P), Recall (R) and F1-
measure (F1) of Fast-Champollion (FC), Fast-
Champollion-Recall (FCR), Champollion (C) and
the length-base algorithm (L)

Performance on Long Texts

to test

In order

the scalability of Fast-
Champollion algorithm, we evaluated it on long
texts. We merged all the articles in the testing cor-

300

250

200

150

100

50

)
s
(
e
m

i
t
 

g
n
i
n
n
u
R

 

0
0

20

 

Time of Fast−Champollion
Time of Fast−Champllion−Recal
Time of Champollion
Time of length−based aligner

40

60

80

100

Noisy Level

1

0.8

0.6

0.4

0.2

 

X: 100

Y: 0.5427

 

0
0

20

40

60
Noisy Level

80

100

F1 of FC
F1 of FCR
F1 of C
F1 of L
P of FC
P of FCR
P of C
P of L
R of FC
R of FCR
R of C
R of L

717

Aligner

Fast-Champollion

Fast-Champollion-Recal

Champollion
Length-based

Precision
0.9458
0.9470
0.9450
0.8154

Recall
0.9408
0.9373
0.9385
0.7878

F1-measure
0.9433
0.9421
0.9417
0.8013

Running time(s)
48.0
45.4
173.5
11.3

Table 3: Performance on texts from the Internet

Aligner

Fast-Champollion

Fast-Champollion-Recall

Champollion
Length-based

Precision
0.9457
0.9456
0.9464
0.8031

Recall
0.9418
0.9362
0.9412
0.7729

F1-measure
0.9437
0.9409
0.9438
0.7877

Running time(s)
51.5
50.7
2029.0
23.8

Table 4: Performance on long text

pus into a single long “article”. Its length is com-
parable to that of the novel of Wuthering Heights.
Table 4 shows the evaluation results on this long
article. Fast-Champollion is about 39.4 times as
fast as Champollion with slightly lower Precision,
Recall and F1-measure, and is just about 1.2 times
slower than the length-based algorithm, which has
much lower Precision, Recall and F1-measure. So
Fast-Champollion is also applicable for long text,
and has a signiﬁcantly higher speed.

4.4 Evaluation of the Length-based Splitting

Module

The reason for Fast-Champollion can achieve rel-
atively high speed is that the length-based split-
ting module can split the bilingual input texts into
many small fragments reliably. We investigate the
fragments produced by the length-based splitting
module when aligning the long article used in Sec-
tion 4.3. The length-based splitting module splits
the long article at 1,993 places, and 1,972 seg-
ments are correct. The numbers of Chinese and
English segments with no more than 30 Chinese
and English sentences are shown in Figure 5. As
there are only 27 and 29 segments with more than
30 sentences for Chinese and English respectively,
we omit them in the ﬁgure. We can conclude that
although the length-based splitting module is sim-
ple, it is efﬁcient and reliable.

5 Conclusion and Future Work
In this paper we propose a new sentence align-
ment algorithm Fast-Champollion. It reduces the
running time by ﬁrst splitting the bilingual input
texts into small aligned fragments and then further
aligning them one by one. The evaluations show

Figure 5: Numbers of Chinese/English segments
with no more than 30 Chinese/English sentences

that Fast-Champollion is fast, robust and effective
enough for practical use, especially for aligning
large amount of bilingual texts or long bilingual
texts.

Fast-Champollion needs a dictionary for align-
ing sentences, and shares the same problem of
Champollion aligner as indicated in (Ma, 2006),
that is the precision and recall will drop as the
size of the dictionary decreases. So how to build
bilingual dictionaries automatically is an impor-
tant task for improving the performance of Fast-
Champollion in practice, and is a critical problem
for applying Fast-Champollion on language pairs
without a ready to use dictionary.

Acknowledgement

research is

This
supported by the Boeing-
Tsinghua Joint Research Project “Robust Chi-
nese Word Segmentation and High Performance
English-Chinese Bilingual Text Alignment”.

800

600

400

200

r
e
b
m
u
N

 

0
0

English Segment
Chinese Segment

5

10

15

20

25

Number of Sentences Contained in a Segment

 

30

718

References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora.
In
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 169–
176, Berkeley, California, USA, June. Association
for Computational Linguistics.

Chen, Stanley F. 1993. Aligning sentences in bilin-
gual corpora using lexical information. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 9–16, Colum-
bus, Ohio, USA, June. Association for Computa-
tional Linguistics.

Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In Proceedings of LREC-
2006: Fifth International Conference on Language
Resources and Evaluation, pages 489–492.

Melamed, I. Dan. 1996. A geometric approach to
mapping bitext correspondence. In Proceedings of
the First Conference on Empirical Methods in Nat-
ural Language Processing, pages 1–12.

Moore, Robert C. 2002. Fast and accurate sentence
alignment of bilingual corpora.
In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, pages 135–144,
London, UK. Springer-Verlag.

Simard, Michel, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilingual
corpora. In Proceedings of the 1993 Conference of
the Centre for Advanced Studies on Collaborative
Research, pages 1071–1082. IBM Press.

Utsuro, Takehito, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statis-
tics.
In Proceedings of the 15th Conference on
Computational Linguistics, pages 1076–1082, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.

Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
Nagy V. 2005. Parallel corpora for medium den-
sity languages. In Proceedings of the RANLP 2005,
pages 590–596.

Wu, Dekai. 1994. Aligning a parallel english-chinese
corpus statistically with lexical criteria. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 80–87, Las
Cruces, New Mexico, USA, June. Association for
Computational Linguistics.

