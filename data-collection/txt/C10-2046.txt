400

Coling 2010: Poster Volume, pages 400–408,

Beijing, August 2010

Learning to Model Domain-Speciﬁc Utterance Sequences for Extractive

Summarization of Contact Center Dialogues

Ryuichiro Higashinaka†, Yasuhiro Minami‡, Hitoshi Nishikawa†,

Kohji Dohsaka‡, Toyomi Meguro‡, Satoshi Takahashi†, Genichiro Kikui†

† NTT Cyber Space Laboratories, NTT Corporation

‡ NTT Communication Science Laboratories, NTT Corporation

higashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jp

nishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp

{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jp
Abstract

This paper proposes a novel extractive
summarization method for contact cen-
ter dialogues. We use a particular
type of hidden Markov model (HMM)
called Class Speaker HMM (CSHMM),
which processes operator/caller utterance
sequences of multiple domains simulta-
neously to model domain-speciﬁc utter-
ance sequences and common (domain-
wide) sequences at the same time. We
applied the CSHMM to call summariza-
tion of transcripts in six different con-
tact center domains and found that our
method signiﬁcantly outperforms compet-
itive baselines based on the maximum
coverage of important words using integer
linear programming.

Introduction

1
In modern business, contact centers are becom-
ing more and more important for improving cus-
tomer satisfaction. Such contact centers typically
have quality analysts who mine calls to gain in-
sight into how to improve business productivity
(Takeuchi et al., 2007; Subramaniam et al., 2009).
To enable them to handle the massive number of
calls, automatic summarization has been utilized
and shown to successfully reduce costs (Byrd et
al., 2008). However, one of the problems in cur-
rent call summarization is that a domain ontology
is required for understanding operator/caller utter-
ances, which makes it difﬁcult to port one summa-
rization system from domain to domain.

This paper describes a novel automatic sum-
marization method for contact center dialogues
without the costly process of creating domain on-

tologies. More speciﬁcally, given contact center
dialogues categorized into multiple domains, we
create a particular type of hidden Markov model
(HMM) called Class Speaker HMM (CSHMM)
to model operator/caller utterance sequences. The
CSHMM learns to distinguish sequences of indi-
vidual domains and common sequences in all do-
mains at the same time. This approach makes it
possible to accurately distinguish utterances spe-
ciﬁc to a certain domain and thereby has the po-
tential to generate accurate extractive summaries.
In Section 2, we review recent work on auto-
matic summarization, including its application to
contact center dialogues.
In Section 3, we de-
scribe the CSHMM. In Section 4, we describe
our automatic summarization method in detail. In
Section 5, we describe the experiment we per-
formed to verify our method and present the re-
sults. In Section 6, we summarize and mention
future work.

2 Related Work
There is an abundance of research in automatic
summarization. It has been successfully applied to
single documents (Mani, 2001) as well as to mul-
tiple documents (Radev et al., 2004), and various
summarization methods, such as the conventional
LEAD method, machine-learning based sentence
selection (Kupiec et al., 1995; Osborne, 2002),
and integer linear programming (ILP) based sen-
tence extraction (Gillick and Favre, 2009), have
been proposed. Recent years have seen work on
summarizing broadcast news speech (Hori and
Furui, 2003), multi-party meetings (Murray et al.,
2005), and contact center dialogues (Byrd et al.,
2008). However, despite the large amount of pre-
vious work, little work has tackled the automatic
summarization of multi-domain data.

401

In the past few decades, contact center dia-
logues have been an active research focus (Gorin
et al., 1997; Chu-Carroll and Carpenter, 1999).
Initially, the primary aim of such research was
to transfer calls from answering agents to oper-
ators as quickly as possible in the case of prob-
lematic situations. However, real-time processing
of calls requires a tremendous engineering effort,
especially when customer satisfaction is at stake,
which led to recent work on the ofﬂine process-
ing of calls, such as call mining (Takeuchi et al.,
2007) and call summarization (Byrd et al., 2008).
The work most related to ours is (Byrd et al.,
2008), which maps operator/caller utterances to
an ontology in the automotive domain by using
support vector machines (SVMs) and creates a
structured summary by heuristic rules that assign
the mapped utterances to appropriate summary
sections. Our work shares the same motivation
as theirs in that we want to make it easier for
quality analysts to analyze the massive number of
calls. However, we tackle the problem differently
in that we propose a new modeling of utterance se-
quences for extractive summarization that makes
it unnecessary to create heuristics rules by hand
and facilitates the porting of a summarization sys-
tem.

HMMs have been successfully applied to au-
tomatic summarization (Barzilay and Lee, 2004).
In their work, an HMM was used to model the
transition of content topics. The Viterbi decod-
ing (Rabiner, 1990) was performed to ﬁnd con-
tent topics that should be incorporated into a sum-
mary. Their approach is similar to ours in that
HMMs are utilized to model topic sequences, but
they did not use data of multiple domains in creat-
ing their model. In addition, their method requires
training data (original articles with their reference
summaries) in order to ﬁnd which content top-
ics should be included in a summary, whereas our
method requires only the raw sequences with their
domain labels.

3 Class Speaker HMM
A Class Speaker HMM (CSHMM) is an exten-
sion of Speaker HMM (SHMM), which has been
utilized to model two-party conversations (Me-
guro et al., 2009).
In an SHMM, there are two
states, and each state emits utterances of one of
the two conversational participants. The states are

Speaker HMM for Class 1

Speaker HMM for Class 2

1:speaker1

2:speaker2

3:speaker1

4:speaker2

Figure 1: Topology of an ergodic CSHMM. Num-
bers before ‘speaker1’ and ‘speaker2’ denote state
IDs.
connected ergodically and the emission/transition
probabilities are learned from training data by
using the EM-algorithm. Although Meguro et
al., (2009) used SHMMs to analyze the ﬂow of
listening-oriented dialogue, we extend their idea
to make it applicable to classiﬁcation tasks, such
as dialogue segmentation.

A CSHMM is simply a concatenation of
SHMMs, each of which is trained by using ut-
terance sequences of a particular dialogue class.
After such SHMMs are concatenated, the Viterbi
algorithm is used to decode an input utterance
sequence into class labels by estimating from
which class each utterance has most likely to have
been generated. Figure 1 illustrates the basic
topology of a CSHMM where two SHMMs are
concatenated ergodically. When the most likely
state sequence for an input utterance sequence is
<1,3,4,2>, we can convert these state IDs into
their corresponding classes; that is, <1,2,2,1>,
which becomes the result of utterance classiﬁca-
tion.

We have conceived three variations of CSHMM
as we describe below. They differ in how we treat
utterance sequences that appear commonly in all
classes and how we train the transition probabili-
ties between independently trained SHMMs.

3.1 Ergodic CSHMM
The most basic CSHMM is the ergodic CSHMM,
which is a simple concatenation of SHMMs in
an ergodic manner as shown in Fig. 1. For K
classes, K SHMMs are combined with the initial
and transition probabilities all set to equal. In this
CSHMM, the assignment of class labels solely de-
pends on the output distributions of each class.

3.2 Ergodic CSHMM with Common States
This type of CSHMM is the same as the ergodic
CSHMM except that it additionally has a SHMM
trained from all dialogues of all classes. There-

402

Speaker HMM for Class 1

1:speaker1

2:speaker2

Speaker HMM for All Classes (Class 0)

5:speaker1

6:speaker2

3:speaker1

4:speaker2

Speaker HMM for Class 2

Figure 2: CSHMM with common states.

Step 1

Train

Class

1

M1M1

Class

k

Train

MkMk

Retrain

Retrain

Class

K

Train

MKMK

Retrain

Step 2

M1…MK become 

less likely to

output common 

sequences

Copy

M0

Train

All 

Classes

Step 3

M1+0

Mk+0

MK+0

M0M0

M1M1

M0

MkMk

M0

MKMK

+

M0

Concatenate

Mconcat

If the fitting has 
converged for 
all Mk+0

END

AVG

Step 2’

Transition probabilities
of M0 are redistributed
between M0 and Mk

M1

Mk

MK

Split Mconcat into 
pairs again and 
retrain Mk+0

M1+0

Mk+0

MK+0

M0M0

M1M1

M0

MkMk

M0

MKMK

Figure 3: Three steps to create a CSHMM using
concatenated training.

fore, for K classes, this CSHMM has K + 1
SHMMs. Figure 2 shows the model topology.
This newly added SHMM works in a manner sim-
ilar to the background model (Reynolds et al.,
2000) representing sequences that are common
to all classes. By having these common states,
common utterance sequences can be classiﬁed as
‘common’, making it possible to avoid forcefully
classifying common utterance sequences into one
of the given classes.

Detecting common sequences is especially
helpful when several classes overlap in nature. For
example, most dialogues commonly start and end
with greetings, and many calls at contact centers
commonly contain exchanges in which the opera-
tor requests personal information about the caller
for conﬁrmation. Regarding the model topology
in Fig. 2, if the most likely state sequence by
the Viterbi decoding is <1,4,5,6,3,2>, we obtain

a class label sequence <1,2,0,0,2,1> where the
third and fourth utterances are classiﬁed as ‘zero’,
meaning that they do not belong to any class.

3.3 CSHMM using Concatenated Training
The CSHMMs presented so far have two prob-
lems: one is that the order of utterances of differ-
ent classes cannot be taken into account because
of the equal transition probabilities. As a result,
the very merit of HMMs, their ability to model
time series data, is lost. The other is that the out-
put distributions of common states may be overly
broad because they are the averaged distributions
over all classes; that is, the best path determined
by the Viterbi decoding may not go through the
common states at all.

Our solution to these problems is to apply con-
catenated training (Lee, 1989), which has been
successfully used in speech recognition to model
phoneme sequences in an unsupervised manner.
The procedure for concatenated training is illus-
trated in Fig. 3 and has three steps.
step 1 Let Mk (Mk ∈ M, 1 ≤ k ≤ K) be the
SHMM trained using dialogues Dk where
Dk = {∀dj|c(dj) = k}, and M0 be the
SHMM trained using all dialogues; i.e., D.
Here, K means the total number of classes
and c(dj) the class assigned to a dialogue dj.
step 2 Connect each Mk ∈ M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ∀dj ∈ Dk where
c(dj) = k.
step 3 Merge all models Mk+0 (1 ≤ k ≤ K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the ﬁtting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l (cid:5)= k) are
summed and equally distributed between the
copied M0’s self-loop and transitions to the
states in Mk.

In concatenated training, the transition and output
probabilities can be optimized between M0 and

403

INPUT: A dialogue in Domain k

Utterance sequence

Extract content words

as utterance features

Feature sequence

Topic Model

L S A / L D A
A s s i g n  
l a b e l s

t o p i c

Assign
topic labels

Topic label sequence

Viterbi decoding

Contact
Center

Dialogues

Domain 1

…

Domain K

Model topic label 
sequences

HMM for Domain 1 HMM for Domain K

…..

HMM for All Domains

Class Speaker
HMM

Domain label sequence

OUTPUT: summary

Select utterances labeled with Domain k

Figure 4: Overview of our summarization
method.

Mk, meaning that the output probabilities of utter-
ance sequences that are common and also found
in Mk can be moved from Mk to M0. This makes
the distribution of Mk sharp (not broad/uniform),
making it likely to output only the utterances rep-
resentative of a class k. As regards M0, its distri-
bution of output probabilities can also be sharp-
ened for utterances that occur commonly in all
classes. This sharpening of distributions is likely
to be helpful for class discrimination.

4 Summarization Method
We apply CSHMMs to extractive summarization
of contact center dialogues because such dia-
logues are two-party, can be categorized into mul-
tiple classes by their call domains (e.g., inquiry
types), and are likely contain many overlapping
exchanges between an operator and a caller across
domains, such as greetings, the conﬁrmation of
personal information, and other cliches in busi-
ness (e.g., name exchanges, thanking/apologizing
phrases, etc.), making them the ideal target for
CSHMMs.

In our method, summarization is performed by
decoding a sequence of utterances of a domain
DM k into domain labels and selecting those ut-
terances that have domain labels DM k. This
makes it possible to extract utterances that are
characteristic of DM k in relation to other do-
mains. Our assumption is that extracting charac-
teristic sequences of a given domain provides a
good summary for that domain because such se-
quences should contain important information ne-
cessitated by the domain.

Figure 4 outlines our extractive summarization
process. The process consists of a training phase
and a decoding phase as described below.

Training phase: Let D (d1 . . . dN) be the entire
set of contact center dialogues, DM k (DM k ∈
DM, 1 ≤ k ≤ K) the domain assigned to do-
main k, and Udi,1 . . . Udi,H the utterances in di.
Here, H is the number of utterances in di. From
D, we create two models: a topic model (T M)
and a CSHMM.

The topic model is used to assign a single topic
to each utterance so as to facilitate the training
of the CSHMM by reducing the dimensions of
the feature space. The same approach has been
taken in (Barzilay and Lee, 2004). The topic
model can be created by such techniques as prob-
abilistic latent semantic analysis (PLSA) (ˇSingliar
and Hauskrecht, 2006) and latent Dirichlet allo-
cation (LDA) (Tam and Schultz, 2005). PLSA
models the latent topics of the documents and its
Baysian extension is LDA, which also models the
co-occurrence of topics using the Dirichlet prior.
We ﬁrst derive features Fd1 . . . FdN for the dia-
logues. Here, we assume a bag-of-words repre-
sentation for the features; therefore, Fdi is repre-
sented as {< w1, c1 > . . . < wV , cV >}, where
V means the total number of content words in the
vocabulary and < wi, ci > denotes that a content
word wi appears ci times in a dialogue. Note that
we derive the features for dialogues, not for utter-
ances, because utterances in dialogue can be very
short, often consisting of only one or two words
and thus making it hard to calculate the word co-
occurrence required for creating a topic model.
From the features, we build a topic model that in-
cludes P(z|w), where w is a word and z is a topic.
Using the topic model, we can assign a single
topic label to every utterance in D by ﬁnding its
z (cid:2)w∈words(Udi) P(z|w).
likely topic; i.e., argmax

After labeling all utterances in D with topic la-
bels, we train a CSHMM that learns characteristic
topic label sequences in each domain as well as
common topic label sequences across domains.

Decoding phase: Let dj be the input dialogue,
DM (dj) (∈ DM) the table for obtaining the do-
main label of dj, and Udj,1 . . . Udj ,Hdj
the utter-
ances in dj, where Hdj is the number of the utter-
ances. We use T M to map the utterances to topic

404

Domain # Tasks Sentences Characters
289.93
259.53
328.55
326.20
354.07
322.22
314.46

8.93
7.20
9.85
10.07
9.40
8.44
9.01

FIN
ISP
LGU
MO
PC
TEL
ALL

15
15
20
15
15
18
98

Table 1: Scenario statistics: the number of tasks
and averaged number of sentences/characters in a
task scenario in the six domains.

and convert them into do-
labels Tdj,1 . . . Tdj,Hdj
us-
main label sequences DMdj ,1 . . . DMdj ,Hdj
ing the trained CSHMM by the Viterbi decoding.
Then, we select Udj,h (1 ≤ h ≤ Hdj) whose cor-
responding domain label DMdj ,h equals DM (dj)
and output the selected utterances in the order of
appearance in the original dialogue as a summary.

5 Experiment
We performed an experiment to verify our sum-
marization method. We ﬁrst collected simulated
contact center dialogues using human subjects.
Then, we compared our method with baseline sys-
tems. Finally, we analyzed the created summaries
to investigate what had been learned by our CSH-
MMs.

5.1 Dialogue Data
Since we do not have access to actual contact cen-
ter data, we recruited human subjects to collect
simulated contact center dialogues. A total of 90
participants (49 males and 41 females) took the
roles of operator or a caller and talked over tele-
phones in separate rooms. The callers were given
realistic scenarios that included their motivation
for a call as well as detailed instructions about
what to ask. The operators, who had experience
of working at contact centers, were given manuals
containing the knowledge of the domain and ex-
plaining how to answer questions in speciﬁc sce-
narios.

The dialogues took place in six different do-
mains: Finance (FIN), Internet Service Provider
(ISP), Local Government Unit (LGU), Mail Or-
der (MO), PC support (PC), and Telecommuni-
cation (TEL). In each domain, there were 15–20
tasks. Table 1 shows the statistics of the task sce-
narios used by the callers. We cannot describe the
details of each domain for lack of space, but ex-

MO task No. 3: It is becoming a good season for the
Japanese Nabe (pan) cuisine. You own a Nabe restau-
rant and it is going well. When you were searching on
the Internet, thinking of creating a new dish, you saw
that drop-shipped Shimonoseki puffer ﬁsh was on sale.
Since you thought the puffer ﬁsh cuisine would become
hot in the coming season, you decided to order it as a
trial.
. . . You ordered a puffer ﬁsh set on the Internet,
but you have not received the conﬁrmation email that
you were supposed to receive.
. . . You decided to call
the contact center to make an inquiry, ask them whether
the order has been successful, and request them to send
you the conﬁrmation email.

Figure 5: Task scenario in the MO domain. The
scenario was originally in Japanese and was trans-
lated by the authors.

amples of the tasks for FIN are inquiries about in-
surance, notiﬁcations of the loss of credit cards,
and applications for ﬁnance loans, and those for
ISP are inquiries about fees for Internet access, re-
quests to forward emails, and reissuance of pass-
words. Figure 5 shows one of the task scenarios
in the MO domain.

We collected data on two separate occasions us-
ing identical scenarios but different participants,
which gave us two sets of dialogue data. We used
the former for training our summarization sys-
tem and the latter for testing. We only use the
transcriptions in this paper so as to avoid partic-
ular problems of speech. All dialogues were in
Japanese. Tables 2 and 3 show the statistics of the
training data and the test data, respectively. As
can be seen from the tables, each dialogue is quite
long, which attests to the complexity of the tasks.

5.2 Training our Summarization System
For training our system, we ﬁrst created a topic
model using LDA. We performed a morphological
analysis using ChaSen1 to extract content words
from each dialogue and made its bag-of-words
features. We deﬁned content words as nouns,
verbs, adjectives, unknown words, and interjec-
tions (e.g., “yes”, “no”, “thank you”, and “sorry”).
We included interjections because they occur very
frequently in dialogues and often possess impor-
tant content, such as agreement and refusal, in
transactional communication. We use this deﬁni-
tion of content words throughout the paper.

Then, using an LDA software package2, we
built a topic model. We tentatively set the number

1http://chasen-legacy.sourceforge.jp/
2http://chasen.org/˜daiti-m/dist/lda/

405

Utterances/Dial.

Characters/Utt.

Domain # dial. OPE CAL Both OPE CAL Both
59 75.73 72.69 148.42 17.44 7.54 12.59
64 55.09 53.17 108.27 20.11 8.03 14.18
76 58.28 50.55 108.83 12.83 8.55 10.84
70 66.39 58.74 125.13 15.09 7.43 11.49
56 89.34 77.80 167.14 15.48 6.53 11.31
66 75.58 63.97 139.55 12.74 8.24 10.67
391 69.21 61.96 131.17 15.40 7.69 11.76

FIN
ISP
LGU
MO
PC
TEL
ALL

Table 2: Training data statistics: Averaged num-
ber of utterances per dialogue and characters per
utterance for each domain. OPE and CAL denote
operator and caller, respectively. See Section 5.1
for the full domain names.

Utterances/Dial.

Characters/Utt.

Domain # dial. OPE CAL Both OPE CAL Both
60 73.97 61.05 135.02 14.53 7.50 11.35
59 76.08 61.24 137.32 15.43 6.94 11.65
56 66.55 51.59 118.14 14.54 7.53 11.48
47 75.53 64.87 140.40 10.53 6.79 8.80
44 124.02 94.16 218.18 14.23 7.79 11.45
41 93.71 68.54 162.24 13.94 7.85 11.37
307 83.07 65.69 148.76 13.98 7.41 11.08

FIN
ISP
LGU
MO
PC
TEL
ALL

Table 3: Test data statistics.

of topics to 100. Using this topic model, we la-
beled all utterances in the training data using these
100 topic labels.

We trained seven different CSHMMs in all: one
ergodic CSHMM (ergodic0), three variants of er-
godic CSHMMs with common states (ergodic1,
ergodic2, ergodic3), and three variants of CSH-
MMs with concatenated training (concat1, con-
cat2, concat3). The difference within the variants
is in the number of common states. The numbers
0–3 after ‘ergodic’ and ‘concat’ indicate the num-
ber of SHMMs containing common states. For
example, ergodic3 has nine SHMMs (six SHMMs
for the six domains plus three SHMMs contain-
ing common states). Since more states would
enable more minute modeling of sequences, we
made such variants in the hope that common se-
quences could be more accurately modeled. We
also wanted to examine the possibility of creat-
ing sharp output distributions in common states
without the concatenated training by such minute
modeling. These seven CSHMMs make seven dif-
ferent summarization systems.

5.3 Baselines
Baseline-1: BL-TF We prepared two baseline
systems for comparison. One is a simple sum-

marizer based on the maximum coverage of high
term frequency (TF) content words. We call
this baseline BL-TF. This baseline summarizes a
dialogue by maximizing the following objective
function:

max (cid:3)zi∈Z

weight(wi) · zi

where ‘weight’ returns the importance of a con-
tent word wi and zi is a binary value indicating
whether to include wi in the summary. Here,
‘weight’ returns the count of wi in a given dia-
logue. The maximization is done using ILP (we
used an off-the-shelf solver lp solve3) with the
following three constraints:

xi, zi ∈ {0, 1}
(cid:3)xi∈X
lixi ≤ K
mijxi ≥ zj

(∀zj ∈ Z)

(cid:3)i

where xi is a binary value that indicates whether
to include the i-th utterance in the summary, li is
the length of the i-th utterance, K is the maximum
number of characters to include in a summary, and
mij is a binary value that indicates whether wi is
included in the j-th utterance. The last constraint
means that if a certain utterance is included in the
summary, all words in that utterance have to be
included in the summary.

Baseline-2: BL-DD Although BL-TF should be
a very competitive baseline because it uses the
state-of-the-art formulation as noted in (Gillick
and Favre, 2009), having only this baseline is
rather unfair because it does not make use of the
training data, whereas our proposed method uses
them. Therefore, we made another baseline that
learns domain-speciﬁc dictionaries (DDs) from
the training data and incorporates them into the
weights of content words of the objective function
of BL-TF. We call this baseline BL-DD. In this
baseline, the weight of a content word wi in a do-
main DM k is

weight(wi,DM k) =

log(P(wi|DM k))

log(P(wi|DM\DM k))

3http://lpsolve.sourceforge.net/5.5/

406

PROPOSED

(Same-length) BL-TF

(Same-length) BL-DD

Compression Rate

ergodic0 ergodic1 ergodic2 ergodic3 concat1
Metric
F
0.177
precision 0.145
0.294
recall
0.171
F
precision 0.132
0.294
recall
F
0.189
precision 0.155
0.287
recall
0.42

0.187∗e0e1
0.161∗
0.280∗
0.168
0.135
0.270
0.189
0.162
0.273
0.37

0.177
0.145
0.294
0.171
0.132
0.294
0.189
0.155
0.287
0.42

0.177
0.145
0.294
0.171
0.132
0.294
0.189
0.155
0.287
0.42

0.177
0.145
0.294
0.171
0.132
0.294
0.189
0.155
0.287
0.42

0.191∗+
0.259∗+
0.164
0.140
0.241
0.187
0.170
0.250
0.30

0.195∗+
0.259∗+
0.163
0.140
0.240
0.187
0.172
0.248
0.30

concat2

concat3

e2e3 0.198∗+e0e1

e2e3c1 0.199∗+e0e1

e2e3c1

Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf. Table 3) in the test
set for the proposed methods and baselines BL-TF and BL-DD conﬁgured to output the same-length
summaries as the proposed systems. The averaged compression rate for each proposed system is shown
at the bottom. The columns (ergodic0–concat3) indicate our methods as well as the character lengths
used by the baselines. Asterisks, ‘+’, e0–e3, and c1–c3 indicate our systems’ statistical signiﬁcance by
the Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0–3, and concat1–3, respectively.
Statistical tests for the precision and recall were only performed between the proposed systems and
their same-length baseline counterparts. Bold font indicates the best score in each row.

where P(wi|DM k) denotes the occurrence prob-
in the dialogues of DM k, and
ability of wi
P(wi|DM\DM k) the occurrence probability of
wi in all domains except for DM k. This log like-
lihood ratio estimates how much a word is char-
acteristic of a given domain. Incorporating such
weights would make a very competitive baseline.

5.4 Evaluation Procedure
We made our seven proposed systems and two
baselines (BL-TF and BL-DD) output extractive
summaries for the test data. Since one of the
shortcomings of our proposed method is its inabil-
ity to set the compression rate, we made our sys-
tems output summaries ﬁrst and made the baseline
systems output their summaries within the charac-
ter lengths of our systems’ summaries.

We used scenario texts (See Fig. 5) as reference
data; that is, a dialogue dealing with a certain task
is evaluated using the scenario text for that task.
As an evaluation criterion, we used the F-measure
(F1) to evaluate the retrieval accuracy on the ba-
sis of the recall and precision of retrieved content
words. We used the scenarios as references be-
cause they contain the basic content exchanged
between an operator and a caller, the retrieval ac-
curacy of which should be important for quality
analysts.

We could have used ROUGE (Lin and Hovy,
2003), but we did not because ROUGE does not
correlate well with human judgments in conversa-

tional data (Liu and Liu, 2008). Another beneﬁt of
using the F-measure is that summaries of varying
lengths can be compared.

5.5 Results
Table 4 shows the evaluation results for the pro-
posed systems and the baselines. It can be seen
that concat3 shows the best performance in F-
measure among all systems, having a statistically
better performance over all systems except for
concat2. The CSHMMs with concatenated train-
ing were all better than ergodic0–3. Here, the per-
formance (and output) of ergodic0–3 was exactly
the same. This happened because of the broad dis-
tributions in their common states; no paths went
through the common states and all paths went
through the SHMMs of the six domains instead.

The evaluation results in Table 4 may be rather
in favor of our systems because the summarization
lengths were set by the proposed systems. There-
fore, we performed another experiment to inves-
tigate the performance of the baselines with vary-
ing compression rates and compared their perfor-
mance with the proposed systems in F-measure.
We found that the best performance was achieved
by BL-DD when the compression rate was 0.4
with the F-measure of 0.191, which concat3 sig-
niﬁcantly outperformed by the Wilcoxon signed-
rank test (p<0.01). Note that the performance
shown in Table 4 may seem low. However, we
found that the maximum recall
is 0.355 (cal-

407

CAL1 When I order a product from you, I get a conﬁr-

CAL2
CAL3

mation email
Puffer ﬁsh
Sets I have ordered, but I haven’t received
the conﬁrmation email

OPE1 Order
OPE2

OPE3

I will make a conﬁrmation whether you have
ordered
Ten sets of Shimonoseki puffer ﬁsh by drop-
ship
“Yoriai” (name of the product)
OPE4
Two kilos of bony parts of tiger puffer ﬁsh
OPE5
Baked ﬁns for ﬁn sake
OPE6
600 milliliter of puffer ﬁsh soy sauce
OPE7
OPE8 And, grated radish and red pepper
OPE9 Your desired delivery date is the 13th of Febru-

ary

This is q in alphabet right?

CAL4 Yes, all in small cases
CAL5
CAL6 Hyphen g
CAL7 You mean that the order was successful
OPE10 Yes, it was Nomura at JDS call center

Figure 6: Example output of concat3 for MO task
No. 3 (cf Fig. 5). The utterances were translated
by the authors. The compression rate for this dia-
logue was 0.24.

culated by using summaries with no compres-
sion). This means that the maximum F-measure
we could attain is lower than 0.524 (when the pre-
cision is ideal with 1). This is because of the dif-
ferences between the scenarios and the actual di-
alogues. We want to pursue ways to improve our
evaluation methodology in the future.

Despite such issues in evaluation, from the re-
sults, we conclude that our extractive summa-
rization method is effective and that having the
common states and training CSHMMs with con-
catenated training are useful in modeling domain-
speciﬁc sequences of contact center dialogues.

5.6 Example of System Output
Figure 6 shows an example output of concat3 for
the scenario MO task No. 3 (cf. Fig. 5). Bold font
indicates utterances that were NOT included in the
summary of concat3’s same-length-BF-DD coun-
terpart.
It is clear that sequences related to the
MO domain were successfully extracted. When
we look at the summary of BF-DD, we see such
utterances as “Can I have your address from the
postcode” and “Finally, can I have your email ad-
dress”, which are obvious cliches in contact center
dialogues. This indicates the usefulness of com-
mon states for ignoring such common exchanges.

6 Summary and Future Work
This paper proposed a novel extractive sum-
marization method for contact center dialogues.
We devised a particular type of HMM called
CSHMM, which processes operator/caller utter-
ance sequences of multiple domains simulta-
neously to model domain-speciﬁc utterance se-
quences and common sequences at the same time.
We trained a CSHMM using the transcripts of
simulated contact center dialogues and veriﬁed its
effectiveness for the summarization of calls.

There still remain several limitations in our ap-
proach. One is its inability to change the com-
pression rate, which we aim to solve in the next
step using the forward-backward algorithm (Ra-
biner and Juang, 1986). This algorithm can cal-
culate the posterior probability of each state at
each time frame given an input dialogue sequence,
enabling us to extract top-N domain-speciﬁc se-
quences. We also need to ﬁnd the appropriate
topic number for the topic model. In our imple-
mentation, we used a tentative value of 100, which
may not be appropriate. In addition, we believe
the topic model and the CSHMM can be uniﬁed
because these models are fundamentally similar,
especially when LDA is employed. Model topolo-
gies may also have to be reconsidered.
In our
CSHMM with concatenated training, the states in
domain-speciﬁc SHMMs are only connected to
the common states, which may be inappropriate
because there could be a case where a domain
changes from one to another without having a
common sequence. Applying CSHMMs to speech
and other NLP tasks is another challenge. As a
near-term goal, we aim to apply our method to the
summarization of meetings, where we will need to
extend our CSHMMs to deal with more than two
participants. Finally, we also want to build a con-
tact center dialogue agent by extending the CSH-
MMs to partially observable Markov decision pro-
cesses (POMDPs) (Williams and Young, 2007) by
following the recent work on building POMDPs
from dialogue data in the dynamic Bayesian net-
work (DBN) framework (Minami et al., 2009).

Acknowledgments
We thank the members of the Spoken Dialog
System Group, especially Noboru Miyazaki and
Satoshi Kobashikawa, for their effort in dialogue
data collection.

408

References
Barzilay, Regina and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to gener-
ation and summarization. In Proceedings of the Human
Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 113–120.

Byrd, Roy J., Mary S. Neff, Wilfried Teiken, Youngja
Park, Keh-Shin F. Cheng, Stephen C. Gates, and Karthik
Visweswariah. 2008. Semi-automated logging of contact
center telephone calls.
In Proceeding of the 17th ACM
conference on Information and knowledge management
(CIKM), pages 133–142.

Chu-Carroll, Jennifer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational Lin-
guistics, 25(3):361–388.

Gillick, Dan and Benoit Favre. 2009. A scalable global
model for summarization.
In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18.

Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113–127.

Hori, Chiori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions on
Multimedia, 5(3):368–378.

Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A
trainable document summarizer.
In Proceedings of the
18th annual international ACM SIGIR conference on Re-
search and development in information retrieval (SIGIR),
pages 68–73.

Lee, Kai-Fu. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.

Lin, Chin-Yew and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics on Human Language Technology (NAACL-HLT),
pages 71–78.

Liu, Feifan and Yang Liu.

2008. Correlation between
ROUGE and human evaluation of extractive meeting sum-
maries.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on Human
Language Technologies (HLT), pages 201–204.

Mani, Inderjeet. 2001. Automatic summarization.

Benjamins Publishing Company.

John

Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,
Yasuhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proceedings of the SIGDIAL 2009 conference, pages
124–127.

Minami, Yasuhiro, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes.
In
Proceedings of the 1st international workshop on spoken
dialogue systems technology (IWSDS), pages 254–263.

Murray, Gabriel, Steve Renals, and Jean Carletta. 2005. Ex-
tractive summarization of meeting recordings.
In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology (EUROSPEECH), pages
593–596.

Osborne, Miles. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Workshop
on Automatic Summarization, pages 1–8.

Rabiner, Lawrence R. and Biing-Hwang Juang. 1986. An
introduction to hidden Markov models. IEEE ASSP Mag-
azine, 3(1):4–16.

Rabiner, Lawrence R. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267–296.

Radev, Dragomir R., Hongyan Jing, Małgorzata Sty´s, and
Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Information Processing & Manage-
ment, 40(6):919–938.

Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.
Dunn. 2000. Speaker veriﬁcation using adapted Gaussian
mixture models. Digital Signal Processing, 10(1-3):19 –
41.

Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-
bal, Shantanu Godbole, and Mukesh K. Mohania. 2009.
Business intelligence from voice of customer.
In Pro-
ceedings of the 2009 IEEE International Conference on
Data Engineering (ICDE), pages 1391–1402.

Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-
sukawa, Shourya Roy, and Sreeram Balakrishnan. 2007.
A conversation-mining system for gathering insights to
improve agent productivity. In Proceedings of the IEEE
International Conference on E-Commerce Technology
and the IEEE International Conference on Enterprise
Computing, E-Commerce, and E-Services, pages 465–
468.

Tam, Yik-Cheung and Tanja Schultz.

2005. Dynamic
language model adaptation using variational Bayes in-
ference.
In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), pages 5–8.

ˇSingliar, Tomas and Milos Hauskrecht. 2006. Noisy-OR
component analysis and its application to link analy-
sis. The Journal of Machine Learning Research, 7:2189–
2213.

Williams, Jason D. and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393–422.

