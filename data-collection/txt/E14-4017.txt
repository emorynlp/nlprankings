



















































Painless Semi-Supervised Morphological Segmentation using Conditional Random Fields


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84–89,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Painless Semi-Supervised Morphological Segmentation using Conditional
Random Fields

Teemu Ruokolainena Oskar Kohonenb Sami Virpiojab Mikko Kurimoa
a Department of Signal Processing and Acoustics, Aalto University

b Department of Information and Computer Science, Aalto University
firstname.lastname@aalto.fi

Abstract

We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, that is the surface
forms of morphemes. We extend a re-
cent segmentation approach based on con-
ditional random fields from purely super-
vised to semi-supervised learning by ex-
ploiting available unsupervised segmenta-
tion techniques. We integrate the unsu-
pervised techniques into the conditional
random field model via feature set aug-
mentation. Experiments on three di-
verse languages show that this straight-
forward semi-supervised extension greatly
improves the segmentation accuracy of the
purely supervised CRFs in a computation-
ally efficient manner.

1 Introduction

We discuss data-driven morphological segmenta-
tion, in which word forms are segmented into
morphs, the surface forms of morphemes. This
type of morphological analysis can be useful for
alleviating language model sparsity inherent to
morphologically rich languages (Hirsimäki et al.,
2006; Creutz et al., 2007; Turunen and Kurimo,
2011; Luong et al., 2013). Particularly, we focus
on a low-resource learning setting, in which only
a small amount of annotated word forms are avail-
able for model training, while unannotated word
forms are available in abundance.

We study morphological segmentation using
conditional random fields (CRFs), a discrimina-
tive model for sequential tagging and segmenta-
tion (Lafferty et al., 2001). Recently, Ruoko-
lainen et al. (2013) showed that the CRFs can
yield competitive segmentation accuracy com-
pared to more complex, previous state-of-the-
art techniques. While CRFs yielded generally

the highest accuracy compared to their reference
methods (Poon et al., 2009; Kohonen et al., 2010),
on the smallest considered annotated data sets of
100 word forms, they were outperformed by the
semi-supervised Morfessor algorithm (Kohonen et
al., 2010). However, Ruokolainen et al. (2013)
trained the CRFs solely on the annotated data,
without any use of the available unannotated data.

In this work, we extend the CRF-based ap-
proach to leverage unannotated data in a straight-
forward and computationally efficient manner via
feature set augmentation, utilizing predictions of
unsupervised segmentation algorithms. Experi-
ments on three diverse languages show that the
semi-supervised extension substantially improves
the segmentation accuracy of the CRFs. The ex-
tension also provides higher accuracies on all the
considered data set sizes and languages compared
to the semi-supervised Morfessor (Kohonen et al.,
2010).

In addition to feature set augmentation, there
exists numerous approaches for semi-supervised
CRF model estimation, exemplified by minimum
entropy regularization (Jiao et al., 2006), gen-
eralized expectations criteria (Mann and McCal-
lum, 2008), and posterior regularization (He et al.,
2013). In this work, we employ the feature-based
approach due to its simplicity and the availabil-
ity of useful unsupervised segmentation methods.
Varying feature set augmentation approaches have
been successfully applied in several related tasks,
such as Chinese word segmentation (Wang et al.,
2011; Sun and Xu, 2011) and chunking (Turian et
al., 2010).

The paper is organized as follows. In Section 2,
we describe the CRF-based morphological seg-
mentation approach following (Ruokolainen et al.,
2013), and then show how to extend this approach
to leverage unannotated data in an efficient man-
ner. Our experimental setup and results are dis-
cussed in Sections 3 and 4, respectively. Finally,

84



we present conclusions on the work in Section 5.

2 Methods

2.1 Supervised Morphological Segmentation
using CRFs

We present the morphological segmentation task
as a sequential labeling problem by assigning each
character to one of three classes, namely {be-
ginning of a multi-character morph (B), middle
of a multi-character morph (M), single character
morph (S)}. We then perform the sequential label-
ing using linear-chain CRFs (Lafferty et al., 2001).

Formally, the linear-chain CRF model distribu-
tion for label sequence y = (y1, y2, . . . , yT ) and
a word form x = (x1, x2, . . . , xT ) is written as a
conditional probability

p (y |x;w) ∝
T∏

t=2

exp
(
w · φ(yt−1, yt, x, t)

)
,

(1)
where t indexes the character positions,w denotes
the model parameter vector, and φ the vector-
valued feature extracting function. The model pa-
rameters w are estimated discrimatively based on
a training set of exemplar input-output pairs (x, y)
using, for example, the averaged perceptron algo-
rithm (Collins, 2002). Subsequent to estimation,
the CRF model segments test word forms using
the Viterbi algorithm (Lafferty et al., 2001).

We next describe the feature set
{φi(yt−1, yt, x, t)}|φ|i=1 by defining emission
and transition features. Denoting the label set {B,
M, S} as Y , the emission feature set is defined as
{χm(x, t)1(yt = y′t) |m ∈ 1..M ,∀y′t ∈ Y} ,

(2)
where the indicator function 1(yt = y′t) returns
one if and only if yt = y′t and zero otherwise, that
is

1(yt = y′t) =
{

1 if yt = y′t
0 otherwise , (3)

and {χm(x, t)}Mm=1 is the set of functions describ-
ing the character position t. Following Ruoko-
lainen et al. (2013), we employ binary functions
that describe the position t of word x using all left
and right substrings up to a maximum length δ.
The maximum substring length δmax is considered
a hyper-parameter to be adjusted using a develop-
ment set. While the emission features associate
the input to labels, the transition feature set

{1(yt−1 = y′t−1)1(yt = y′t) | y′t, y′t−1 ∈ Y} (4)

captures the dependencies between adjacent labels
as irrespective of the input x.

2.2 Leveraging Unannotated Data
In order to utilize unannotated data, we explore a
straightforward approach based on feature set aug-
mentation. We exploit predictions of unsupervised
segmentation algorithms by defining variants of
the features described in Section 2.1. The idea is
to compensate the weaknesses of the CRF model
trained on the small annotated data set using the
strengths of the unsupervised methods that learn
from large amounts of unannotated data.

For example, consider utilizing predictions of
the unsupervised Morfessor algorithm (Creutz and
Lagus, 2007) in the CRF model. In order to ac-
complish this, we first learn the Morfessor model
from the unannotated training data, and then ap-
ply the learned model on the word forms in the
annotated training set. Assuming the annotated
training data includes the English word drivers,
the Morfessor algorithm might, for instance, re-
turn a (partially correct) segmentation driv + ers.
We present this segmentation by defining a func-
tion υ(t), which returns 0 or 1, if the position t is
in the middle of a segment or in the beginning of a
segment, respectively, as in

t 1 2 3 4 5 6 7
xt d r i v e r s

υ(t) 1 0 0 0 1 0 0

Now, given a set of U functions {υu(t)}Uu=1, we
define variants of the emission features in (2) as

{υu(x, t)χm(x, t)1(yt = y′t) |
∀u ∈ 1..U ,∀m ∈ 1..M ,∀y′t ∈ Y} . (5)

By adding the expanded features of form (5), the
CRF model learns to associate the output of the
unsupervised algorithms in relation to the sur-
rounding substring context. Similarly, an ex-
panded transition feature is written as

{υu(x, t)1(yt−1 = y′t−1)1(yt = y′t) |
∀u ∈ 1..U ,∀y′t, y′t−1 ∈ Y} . (6)

After defining the augmented feature set, the
CRF model parameters can be estimated in a stan-
dard manner on the small, annotated training data
set. Subsequent to CRF training, the Morfessor
model is applied on the test instances in order to
allow the feature set augmentation and standard
decoding with the estimated CRF model. We ex-
pect the Morfessor features to specifically improve

85



segmentation of compound words (for example,
brain+storm), which are modeled with high ac-
curacy by the unsupervised Morfessor algorithm
(Creutz and Lagus, 2007), but can not be learned
from the small number of annotated examples
available for the supervised CRF training.

As another example of a means to augment the
feature set, we make use of the fact that the output
of the unsupervised algorithms does not have to be
binary (zeros and ones). To this end, we employ
the classic letter successor variety (LSV) scores
presented originally by (Harris, 1955).1 The LSV
scores utilize the insight that the predictability of
successive letters should be high within morph
segments, and low at the boundaries. Conse-
quently, a high variety of letters following a prefix
indicates a high probability of a boundary. We use
a variant of the LSV values presented by Çöltekin
(2010), in which we first normalize the scores by
the average score at each position t, and subse-
qently logarithmize the normalized value. While
LSV score tracks predictability given prefixes, the
same idea can be utilized for suffixes, providing
the letter predecessor variety (LPV). Subsequent
to augmenting the feature set using the functions
LSV (t) and LPV (t), the CRF model learns to
associate high successor and predecessor values
(low predictability) to high probability of a seg-
ment boundary. Appealingly, the Harris features
can be obtained in a computationally inexpensive
manner, as they merely require counting statistics
from the unannotated data.

The feature set augmentation approach de-
scribed above is computationally efficient, if the
computational overhead from the unsupervised
methods is small. This is because the CRF param-
eter estimation is still based on the small amount
of labeled examples as described in Section 2.1,
while the number of features incorporated in the
CRF model (equal to the number of parameters)
grows linearly in the number of exploited unsu-
pervised algorithms.

3 Experimental Setup

3.1 Data

We perform the experiments on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al., 2009; Ku-

1We also experimented on modifying the output of the
Morfessor algorithm from binary to probabilistic, but these
soft cues provided no consistent advantage over the standard
binary output.

English Finnish Turkish
Train (unann.) 384,903 2,206,719 617,298
Train (ann.) 1,000 1,000 1,000
Devel. 694 835 763
Test 10,000 10,000 10,000

Table 1: Number of word types in the Morpho
Challenge data set.

rimo et al., 2010) consisting of manually prepared
morphological segmentations in English, Finnish
and Turkish. We follow the experiment setup, in-
cluding data partitions and evaluation metrics, de-
scribed by Ruokolainen et al. (2013). Table 1
shows the total number of instances available for
model estimation and testing.

3.2 CRF Feature Extraction and Training

The substring features included in the CRF model
are described in Section 2.1. We include all sub-
strings which occur in the training data. The Mor-
fessor and Harris (successor and predecessor va-
riety) features employed by the semi-supervised
extension are described in Section 2.2. We ex-
perimented on two variants of the Morfessor al-
gorithm, namely, the Morfessor Baseline (Creutz
and Lagus, 2002) and Morfessor Categories-MAP
(Creutz and Lagus, 2005), CatMAP for short. The
Baseline models were trained on word types and
the perplexity thresholds of the CatMAP models
were set equivalently to the reference runs in Mor-
pho Challenge 2010 (English: 450, Finnish: 250,
Turkish: 100); otherwise the default parameters
were used. The Harris features do not require any
hyper-parameters.

The CRF model (supervised and semi-
supervised) is trained using the averaged
perceptron algorithm (Collins, 2002). The num-
ber of passes over the training set made by the
perceptron algorithm, and the maximum length of
substring features are optimized on the held-out
development sets.

The experiments are run on a standard desktop
computer using a Python-based single-threaded
CRF implementation. For Morfessor Baseline, we
use the recently published implementation by Vir-
pioja et al. (2013). For Morfessor CatMAP, we
used the Perl implementation by Creutz and La-
gus (2005).

86



3.3 Reference Methods
We compare our method’s performance with
the fully supervised CRF model and the semi-
supervised Morfessor algorithm (Kohonen et al.,
2010). For semi-supervised Morfessor, we use the
Python implementation by Virpioja et al. (2013).

4 Results

Segmentation accuracies for all languages are pre-
sented in Table 2. The columns titled Train (ann.)
and Train (unann.) denote the number of anno-
tated and unannotated training instances utilized
by the method, respectively. To summarize, the
semi-supervised CRF extension greatly improved
the segmentation accuracy of the purely super-
vised CRFs, and also provided higher accuracies
compared to the semi-supervised Morfessor algo-
rithm2.

Appealingly, the semi-supervised CRF exten-
sion already provided consistent improvement
over the supervised CRFs, when utilizing the com-
putationally inexpensive Harris features. Addi-
tional gains were then obtained using the Morfes-
sor features. On all languages, highest accuracies
were obtained using a combination of Harris and
CatMAP features.

Running the CRF parameter estimation (includ-
ing hyper-parameters) consumed typically up to a
few minutes. Computing statistics for the Harris
features also took up roughly a few minutes on
all languages. Learning the unsupervised Mor-
fessor algorithm consumed 3, 47, and 20 min-
utes for English, Finnish, and Turkish, respec-
tively. Meanwhile, CatMAP model estimation
was considerably slower, consuming roughly 10,
50, and 7 hours for English, Finnish and Turkish,
respectively. Training and decoding with semi-
supervised Morfessor took 21, 111, and 47 hours
for English, Finnish and Turkish, respectively.

5 Conclusions

We extended a recent morphological segmenta-
tion approach based on CRFs from purely super-
vised to semi-supervised learning. We accom-
plished this in an efficient manner using feature set
augmentation and available unsupervised segmen-
tation techniques. Experiments on three diverse

2The improvements over the supervised CRFs and semi-
supervised Morfessor were statistically significant (confi-
dence level 0.95) according to the standard 1-sided Wilcoxon
signed-rank test performed on 10 randomly divided, non-
overlapping subsets of the complete test sets.

Method Train (ann.) Train (unann.) F1

English
CRF 100 0 78.8
S-MORF. 100 384,903 83.7
CRF (Harris) 100 384,903 80.9
CRF (BL+Harris) 100 384,903 82.6
CRF (CM+Harris) 100 384,903 84.4

CRF 1,000 0 85.9
S-MORF. 1,000 384,903 84.3
CRF (Harris) 1,000 384,903 87.6
CRF (BL+Harris) 1,000 384,903 87.9
CRF (CM+Harris) 1,000 384,903 88.4

Finnish
CRF 100 0 65.5
S-MORF. 100 2,206,719 70.4
CRF (Harris) 100 2,206,719 78.9
CRF (BL+Harris) 100 2,206,719 79.3
CRF (CM+Harris) 100 2,206,719 82.0

CRF 1,000 0 83.8
S-MORF. 1,000 2,206,719 76.4
CRF (Harris) 1,000 2,206,719 88.3
CRF (BL+Harris) 1,000 2,206,719 88.9
CRF (CM+Harris) 1,000 2,206,719 89.4

Turkish
CRF 100 0 77.7
S-MORF. 100 617,298 78.2
CRF (Harris) 100 617,298 82.6
CRF (BL+Harris) 100 617,298 84.9
CRF (CM+Harris) 100 617,298 85.5

CRF 1,000 0 88.6
S-MORF. 1,000 617,298 87.0
CRF (Harris) 1,000 617,298 90.1
CRF (BL+Harris) 1,000 617,298 91.7
CRF (CM+Harris) 1,000 617,298 91.8

Table 2: Results on test data. CRF (BL+Harris)
denotes semi-supervised CRF extension using
Morfessor Baseline and Harris features, while
CRF (CM+Harris) denotes CRF extension em-
ploying Morfessor CatMAP and Harris features.

languages showed that this straightforward semi-
supervised extension greatly improves the seg-
mentation accuracy of the supervised CRFs, while
being computationally efficient. The extension
also outperformed the semi-supervised Morfessor
algorithm on all data set sizes and languages.

Acknowledgements

This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012–2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).

87



References
Michael Collins. 2002. Discriminative training meth-

ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1–8. Association for Computational
Linguistics.

Çagrı Çöltekin. 2010. Improving successor variety
for morphological segmentation. In Proceedings of
the 20th Meeting of Computational Linguistics in the
Netherlands.

Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 21–30,
Philadelphia, PA, USA, July. Association for Com-
putational Linguistics.

Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Timo Honkela, Ville Könönen,
Matti Pöllä, and Olli Simula, editors, Proceedings of
AKRR’05, International and Interdisciplinary Con-
ference on Adaptive Knowledge Representation and
Reasoning, pages 106–113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of
Computer and Information Science.

Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3:1–3:34, January.

Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraçlar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):3:1–3:29, December.

Zellig Harris. 1955. From phoneme to morpheme.
Language, 31(2):190–222.

Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 38–46,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkkönen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515–541,
October.

Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved

sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 209–216. Association for Computational Lin-
guistics.

Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology, pages 78–86, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Mikko Kurimo, Sami Virpioja, Ville Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.

Mikko Kurimo, Sami Virpioja, and Ville Turunen.
2010. Overview and results of Morpho Chal-
lenge 2010. In Proceedings of the Morpho Chal-
lenge 2010 Workshop, pages 7–24, Espoo, Finland,
September. Aalto University School of Science and
Technology, Department of Information and Com-
puter Science. Technical Report TKK-ICS-R37.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Carla E. Brodley and Andrea Po-
horeckyj Danyluk, editors, Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282–289, Williamstown, MA, USA. Mor-
gan Kaufmann.

Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 29–37. Association for Computa-
tional Linguistics, August.

Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings
of ACL-08: HLT, pages 870–878. Association for
Computational Linguistics.

Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 209–217.
Association for Computational Linguistics.

Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of

88



the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 29–37. As-
sociation for Computational Linguistics, August.

Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979. As-
sociation for Computational Linguistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.

Ville Turunen and Mikko Kurimo. 2011. Speech re-
trieval from unsegmented Finnish audio using statis-
tical morpheme-like units for segmentation, recog-
nition, and retrieval. ACM Transactions on Speech
and Language Processing, 8(1):1:1–1:25, October.

Sami Virpioja, Peter Smit, Stig-Arne Grönroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python im-
plementation and extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication se-
ries SCIENCE + TECHNOLOGY, Department of
Signal Processing and Acoustics, Aalto University.

Yiou Wang, Yoshimasa Tsuruoka Jun’ichi Kazama,
Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,
and Kentaro Torisawa. 2011. Improving Chinese
word segmentation and POS tagging with semi-
supervised methods using large auto-analyzed data.
In IJCNLP, pages 309–317.

89


