










































MayoClinicNLP-CORE: Semantic representations for textual similarity


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 148–154, Atlanta, Georgia, June 13-14, 2013. c©2013 Association for Computational Linguistics

MayoClinicNLP–CORE: Semantic representations for textual similarity

Stephen Wu

Mayo Clinic

Rochester, MN 55905

wu.stephen@mayo.edu

Dongqing Zhu & Ben Carterette

University of Delaware

Newark, DE 19716

{zhu,carteret}@cis.udel.edu

Hongfang Liu

Mayo Clinic

Rochester, MN 55905

liu.hongfang@mayo.edu

Abstract

The Semantic Textual Similarity (STS) task

examines semantic similarity at a sentence-

level. We explored three representations of

semantics (implicit or explicit): named enti-

ties, semantic vectors, and structured vectorial

semantics. From a DKPro baseline, we also

performed feature selection and used source-

specific linear regression models to combine

our features. Our systems placed 5th, 6th, and

8th among 90 submitted systems.

1 Introduction

The Semantic Textual Similarity (STS) task (Agirre

et al., 2012; Agirre et al., 2013) examines semantic

similarity at a sentence-level. While much work has

compared the semantics of terms, concepts, or doc-

uments, this space has been relatively unexplored.

The 2013 STS task provided sentence pairs and a

0–5 human rating of their similarity, with training

data from 5 sources and test data from 4 sources.

We sought to explore and evaluate the usefulness

of several semantic representations that have had

recent significance in research or practice. First,

information extraction (IE) methods often implic-

itly consider named entities as ad hoc semantic rep-

resentations, for example, in the clinical domain.

Therefore, we sought to evaluate similarity based on

named entity-based features. Second, in many appli-

cations, an effective means of incorporating distri-

butional semantics is Random Indexing (RI). Thus

we consider three different representations possi-

ble within Random Indexing (Kanerva et al., 2000;

Sahlgren, 2005). Finally, because compositional

distributional semantics is an important research

topic (Mitchell and Lapata, 2008; Erk and Padó,

2008), we sought to evaluate a principled compo-

sition strategy: structured vectorial semantics (Wu

and Schuler, 2011).

The remainder of this paper proceeds as follows.

Section 2 overviews our similarity metrics, and Sec-

tion 3 overviews the systems that were defined on

these metrics. Competition results and additional

analyses are in Section 4. We end with discussion

on the results in Section 5.

2 Similarity measures

Because we expect semantic similarity to be multi-

layered, we expect that we will need many similar-

ity measures to approximate human similarity judg-

ments. Rather than reinvent the wheel, we have cho-

sen to introduce features that complement existing

successful feature sets. We utilized 17 features from

DKPro Similarity and 21 features from TakeLab,

i.e., the two top-performing systems in the 2012 STS

task, as a solid baseline.

These are summarized in Table 1. We introduce 3

categories of new similarity metrics, 9 metrics in all.

2.1 Named entity measures

Named entity recognition provides a common ap-

proximation of semantic content for the informa-

tion extraction perspective. We define three simple

similarity metrics based on named entities. First,

we computed the named entity overlap (exact string

matches) between the two sentences, where NEk
was the set of named entities found in sentence

Sk. This is the harmonic mean of how closely S1

148



Table 1: Full feature pool in MayoClinicNLP systems. The proposed MayoClinicNLP metrics are meant to comple-

ment DKPro (Bär et al., 2012) and TakeLab (Šarić et al., 2012) metrics.
DKPro metrics (17) TakeLab metrics (21) Custom MayoClinicNLP metrics (9)

n-grams/WordNGramContainmentMeasure 1 stopword-filtered t ngram/UnigramOverlap

n-grams/WordNGramContainmentMeasure 2 stopword-filtered t ngram/BigramOverlap

n-grams/WordNGramJaccardMeasure 1 t ngram/TrigramOverlap

n-grams/WordNGramJaccardMeasure 2 stopword-filtered t ngram/ContentUnigramOverlap

n-grams/WordNGramJaccardMeasure 3 t ngram/ContentBigramOverlap

n-grams/WordNGramJaccardMeasure 4 t ngram/ContentTrigramOverlap

n-grams/WordNGramJaccardMeasure 4 stopword-filtered

t words/WeightedWordOverlap custom/StanfordNerMeasure overlap.txt

t words/GreedyLemmaAligningOverlap custom/StanfordNerMeasure aligngst.txt

t words/WordNetAugmentedWordOverlap custom/StanfordNerMeasure alignlcs.txt

esa/ESA Wiktionary t vec/LSAWordSimilarity NYT custom/SVSePhrSimilarityMeasure.txt

esa/ESA WordNet t vec/LSAWordSimilarity weighted NYT custom/SVSeTopSimilarityMeasure.txt

t vec/LSAWordSimilarity weighted Wiki custom/SemanticVectorsSimilarityMeasure d200 wr0.txt

custom/SemanticVectorsSimilarityMeasure d200 wr6b.txt

custom/SemanticVectorsSimilarityMeasure d200 wr6d.txt

custom/SemanticVectorsSimilarityMeasure d200 wr6p.txt

n-grams/CharacterNGramMeasure 2 t other/RelativeLengthDifference

n-grams/CharacterNGramMeasure 3 t other/RelativeInfoContentDifference

n-grams/CharacterNGramMeasure 4 t other/NumbersSize

string/GreedyStringTiling 3 t other/NumbersOverlap

string/LongestCommonSubsequenceComparator t other/NumbersSubset

string/LongestCommonSubsequenceNormComparator t other/SentenceSize

string/LongestCommonSubstringComparator t other/CaseMatches

t other/StocksSize

t other/StocksOverlap

matches S2, and how closely S2 matches S1:

simneo(S1, S2) = 2 ⋅
∣NE1 ∩NE2∣
∣NE1∣ + ∣NE2∣

(1)

Additionally, we relax the constraint of requiring

exact string matches between the two sentences by

using the longest common subsequence (Allison and

Dix, 1986) and greedy string tiling (Wise, 1996) al-

gorithms. These metrics give similarities between

two strings, rather than two sets of strings as we

have with NE1 and NE2. Thus, we follow previ-

ous work in greedily aligning these named entities

(Lavie and Denkowski, 2009; Šarić et al., 2012) into

pairs. Namely, we compare each pair (nei,1, nej,2)
of named entity strings in NE1 and NE2. The

highest-scoring pair is entered into a set of pairs, P .

Then, the next highest pair is added to P if neither

named entity is already in P , and discarded other-

wise; this continues until there are no more named

entities in either NE1 or NE2.

We then define two named entity aligning mea-

sures that use the longest common subsequence

(LCS) and greedy string tiling (GST) fuzzy string

matching algorithms:

simnea(S1, S2) =

∑
(ne1,ne2)∈P

f(ne1, ne2)

max (∣NE1∣, ∣NE2∣)
(2)

where f(⋅) is either the LCS or GST algorithm.
In our experiments, we performed named entity

recognition with the Stanford NER tool using the

standard English model (Finkel et al., 2005). Also,

we used UKP’s existing implementation of LCS and

GST (Šarić et al., 2012) for the latter two measures.

2.2 Random indexing measures

Random indexing (Kanerva et al., 2000; Sahlgren,

2005) is another distributional semantics framework

for representing terms as vectors. Similar to LSA

(Deerwester et al., 1990), an index is created that

represents each term as a semantic vector. But

in random indexing, each term is represented by

an elemental vector et with a small number of

randomly-generated non-zero components. The in-

tuition for this means of dimensionality reduction is

that these randomly-generated elemental vectors are

like quasi-orthogonal bases in a traditional geomet-

ric semantic space, rather than, e.g., 300 fully or-

thogonal dimensions from singular value decompo-

sition (Landauer and Dumais, 1997). For a standard

model with random indexing, a contextual term vec-

tor ct,std is the the sum of the elemental vectors cor-

responding to tokens in the document. All contexts

for a particular term are summed and normalized to

produce a final term vector vt,std.

Other notions of context can be incorporated into

149



this model. Local co-occurrence context can be ac-

counted for in a basic sliding-window model by con-

sidering words within some window radius r (in-

stead of a whole document). Each instance of the

term t will have a contextual vector ct,win = et−r +
⋯ + et−1 + et+1 +⋯ + et+r; context vectors for each
instance (in a large corpus) would again be added

and normalized to create the overall vector vt,win.

A directional model doubles the dimensionality of

the vector and considers left- and right-context sepa-

rately (half the indices for left-context, half for right-

context), using a permutation to achieve one of the

two contexts. A permutated positional model uses a

position-specific permutation function to encode the

relative word positions (rather than just left- or right-

context) separately. Again, vt would be summed

and normalized over all instances of ct.

Sentence vectors from any of these 4 Random

Indexing-based models (standard, windowed, direc-

tional, positional) are just the sum of the vectors for

each term vS = ∑t∈S vt. We define 4 separate simi-
larity metrics for STS as:

simRI(S1, S2) = cos(vS1,vS2) (3)

We used the semantic vectors package (Widdows

and Ferraro, 2008; Widdows and Cohen, 2010) in

the default configuration for the standard model. For

the windowed, directional, and positional models,

we used a 6-word window radius with 200 dimen-

sions and a seed length of 5. All models were

trained on the raw text of the Penn Treebank Wall

Street Journal corpus and a 100,075-article subset of

Wikipedia.

2.3 Semantic vectorial semantics measures

Structured vectorial semantics (SVS) composes dis-

tributional semantic representations in syntactic

context (Wu and Schuler, 2011). Similarity met-

rics defined with SVS inherently explore the quali-

ties of a fully interactive syntax–semantics interface.

While previous work evaluated the syntactic contri-

butions of this model, the STS task allows us to eval-

uate the phrase-level semantic validity of the model.

We summarize SVS here as bottom-up vector com-

position and parsing, then continue on to define the

associated similarity metrics.

Each token in a sentence is modeled generatively

as a vector eγ of latent referents iγ in syntactic con-

text cγ ; each element in the vector is defined as:

eγ[iγ] = P(xγ ∣ lciγ), for preterm γ (4)

where lγ is a constant for preterminals.

We write SVS vector composition between two

word (or phrase) vectors in linear algebra form,1 as-

suming that we are composing the semantics of two

children eα and eβ in a binary syntactic tree into

their parent eγ :

eγ =M⊙ (Lγ×α ⋅ eα)⊙ (Lγ×β ⋅ eβ) ⋅ 1 (5)

M is a diagonal matrix that encapsulates probabilis-

tic syntactic information; the L matrices are linear

transformations that capture how semantically rele-

vant child vectors are to the resulting vector (e.g.,

Lγ×α defines the the relevance of eα to eγ). These

matrices are defined such that the resulting eγ is a

semantic vector of consistent P(xγ ∣ lciγ) probabil-
ities. Further detail is in our previous work (Wu,

2010; Wu and Schuler, 2011).

Similarity metrics can be defined in the SVS

space by comparing the distributions of the com-

posed eγ vectors — i.e., our similarity metric is

a comparison of the vector semantics at different

phrasal nodes. We define two measures, one cor-

responding to the top node c△ (e.g., with a syntactic

constituent c△ = ‘S’), and one corresponding to the
left and right largest child nodes (e.g.,, c∠ = ‘NP’
and c ∠= ‘VP’ for a canonical subject–verb–object
sentence in English).

simsvs-top(S1, S2) = cos(e△(S1),e△(S2)) (6)
simsvs-phr(S1, S2) =max(

avgsim(e∠(S1),e∠(S2);e ∠(S1),e ∠(S2)),

avgsim(e∠(S1),e ∠(S2);e ∠(S1),e∠(S2))) (7)

where avgsim() is the harmonic mean of the co-
sine similarities between the two pairs of arguments.

Top-level similarity comparisons in (6) amounts to

comparing the semantics of a whole sentence. The

phrasal similarity function simsvs-phr(S1, S2) in (7)
thus seeks to semantically align the two largest sub-

trees, and weight them. Compared to simsvs-top,

1We define the operator ⊙ as point-by-point multiplication

of two diagonal matrices and 1 as a column vector of ones, col-

lapsing a diagonal matrix onto a column vector.

150



the phrasal similarity function simsvs-phr(S1, S2) as-
sumes there might be some information captured in

the child nodes that could be lost in the final compo-

sition to the top node.

In our experiments, we used the parser described

in Wu and Schuler (2011) with 1,000 headwords

and 10 relational clusters, trained on the Wall Street

Journal treebank.

3 Feature combination framework

The similarity metrics of Section 2 were calculated

for each of the sentence pairs in the training set, and

later the test set. In combining these metrics, we ex-

tended a DKPro Similarity baseline (3.1) with fea-

ture selection (3.2) and source-specific models and

classification (3.3).

3.1 Linear regression via DKPro Similarity

For our baseline (MayoClinicNLPr1wtCDT), we

used the UIMA-based DKPro Similarity system

from STS 2012 (Bär et al., 2012). Aside from the

large number of sound similarity measures, this pro-

vided linear regression through the WEKA package

(Hall et al., 2009) to combine all of the disparate

similarity metrics into a single one, and some pre-

processing. Regression weights were determined on

the whole training set for each source.

3.2 Feature selection

Not every feature was included in the final linear re-

gression models. To determine the best of the 47

(DKPro–17, TakeLab–21, MayoClinicNLP–9) fea-

tures, we performed a full forward-search on the

space of similarity measures. In forward-search, we

perform 10-fold cross-validation on the training set

for each measure, and pick the best one; in the next

round, that best metric is retained, and the remaining

metrics are considered for addition. Rounds con-

tinue until all the features are exhausted, though a

stopping-point is noted when performance no longer

increases.

3.3 Subdomain source models and

classification

There were 5 sources of data in the training set:

paraphrase sentence pairs (MSRpar), sentence pairs

from video descriptions (MSRvid), MT evaluation

sentence pairs (MTnews and MTeuroparl) and gloss

pairs (OnWN). In our submitted runs, we trained

a separate, feature-selected model based on cross-

validation for each of these data sources. In train-

ing data on cross-validation tests, training domain-

specific models outperformed training a single con-

glomerate model.

In the test data, there were 4 sources, with 2

appearing in training data (OnWN, SMT) and 2

that were novel (FrameNet/Wordnet sense defini-

tions (FNWN), European news headlines (head-

lines)). We examined two different strategies for ap-

plying the 5-source trained models on these 4 test

sets. Both of these strategies rely on a multiclass

random forest classifier, which we trained on the 47

similarity metrics.

First, for each sentence pair, we considered the

final similarity score to be a weighted combination

of the similarity score from each of the 5 source-

specific similarity models. The combination weights

were determined by utilizing the classifier’s confi-

dence scores. Second, the final similarity was cho-

sen as the single source-specific similarity score cor-

responding to the classifier’s output class.

4 Evaluation

The MayoClinicNLP team submitted three systems

to the STS-Core task. We also include here a post-

hoc run that was considered as a possible submis-

sion.

r1wtCDT This run used the 47 metrics from

DKPro, TakeLab, and MayoClinicNLP as a

feature pool for feature selection. Source-

specific similarity metrics were combined with

classifier-confidence-score weights.

r2CDT Same feature pool as run 1. Best-match (as

determined by classifier) source-specific simi-

larity metric was used rather than a weighted

combination.

r3wtCD TakeLab features were removed from the

feature pool (before feature selection). Same

source combination as run 1.

r4ALL Post-hoc run using all 47 metrics, but train-

ing a single linear regression model rather than

source-specific models.

151



Table 2: Performance comparison.
TEAM NAME headlines rank OnWN rank FNWN rank SMT rank mean rank

UMBC EBIQUITY-ParingWords 0.7642 0.7529 0.5818 0.3804 0.6181 1

UMBC EBIQUITY-galactus 0.7428 0.7053 0.5444 0.3705 0.5927 2

deft-baseline 0.6532 0.8431 0.5083 0.3265 0.5795 3

MayoClinicNLP-r4ALL 0.7275 0.7618 0.4359 0.3048 0.5707

UMBC EBIQUITY-saiyan 0.7838 0.5593 0.5815 0.3563 0.5683 4

MayoClinicNLP-r3wtCD 0.6440 43 0.8295 2 0.3202 47 0.3561 17 0.5671 5

MayoClinicNLP-r1wtCDT 0.6584 33 0.7775 4 0.3735 26 0.3605 13 0.5649 6

CLaC-RUN2 0.6921 0.7366 0.3793 0.3375 0.5587 7

MayoClinicNLP-r2CDT 0.6827 23 0.6612 20 0.396 17 0.3946 5 0.5572 8

NTNU-RUN1 0.7279 0.5952 0.3215 0.4015 0.5519 9

CLaC-RUN1 0.6774 0.7667 0.3793 0.3068 0.5511 10

4.1 Competition performance

Table 2 shows the top 10 runs of 90 submitted in

the STS-Core task are shown, with our three sys-

tems placing 5th, 6th, and 8th. Additionally, we can

see that run 4 would have placed 4th. Notice that

there are significant source-specific differences be-

tween the runs. For example, while run 4 is better

overall, runs 1–3 outperform it on all but the head-

lines and FNWN datasets, i.e., the test datasets that

were not present in the training data. Thus, it is

clear that the source-specific models are beneficial

when the training data is in-domain, but a combined

model is more beneficial when no such training data

is available.

4.2 Feature selection analysis

0 10 20 30 40

0
.6

0
0

.6
5

0
.7

0
0

.7
5

0
.8

0
0

.8
5

0
.9

0

Step

P
e

a
rs

o
n

’s
 C

o
rr

e
la

ti
o

n
 C

o
e

ff
ic

ie
n

t

MSRpar

MSRvid

SMTeuroparl

OnWN

SMTnews

ALL

Figure 1: Performance curve of feature selection for

r1wtCDT, r2CDT, and r4ALL

Due to the source-specific variability among the

runs, it is important to know whether the forward-

search feature selection performed as expected. For

source specific models (runs 1 and 3) and a com-

bined model (run 4), Figure 1 shows the 10-fold

cross-validation scores on the training set as the next

feature is added to the model. As we would ex-

pect, there is an initial growth region where the first

features truly complement one another and improve

performance significantly. A plateau is reached for

each of the models, and some (e.g., SMTnews) even

decay if too many noisy features are added.

The feature selection curves are as expected. Be-

cause the plateau regions are large, feature selection

could be cut off at about 10 features, with gains in

efficiency and perhaps little effect on accuracy.

The resulting selected features for some of the

trained models are shown in Table 3.

4.3 Contribution of MayoClinicNLP metrics

We determined whether including MayoClinicNLP

features was any benefit over a feature-selected

DKPro baseline. Table 4 analyzes this question

by adding each of our measures in turn to a base-

line feature-selected DKPro (dkselected). Note that

this baseline was extremely effective; it would have

ranked 4th in the STS competition, outperforming

our run 4. Thus, metrics that improve this baseline

must truly be complementary metrics. Here, we see

that only the phrasal SVS measure is able to improve

performance overall, largely by its contributions to

the most difficult categories, FNWN and SMT. In

fact, that system (dkselected + SVSePhrSimilari-

tyMeasure) represents the best-performing run of

any that was produced in our framework.

152



Table 3: Top retained features for several linear regression models.
OnWN - r1wtCDT and r2CDT (15 shown/19 selected) SMTnews - r1wtCDT and r2CDT (15 shown/17 selected) All - r4ALL (29 shown/29 selected)

t ngram/ContentUnigramOverlap t other/RelativeInfoContentDifference t vec/LSAWordSimilarity weighted NYT

t other/RelativeInfoContentDifference n-grams/CharacterNGramMeasure 2 n-grams/CharacterNGramMeasure 2

t vec/LSAWordSimilarity weighted NYT t other/CaseMatches string/LongestCommonSubstringComparator

esa/ESA Wiktionary string/GreedyStringTiling 3 t other/NumbersOverlap

t ngram/ContentBigramOverlap custom/RandomIndexingMeasure d200 wr6p t words/WordNetAugmentedWordOverlap

n-grams/CharacterNGramMeasure 2 custom/StanfordNerMeasure overlap n-grams/WordNGramJaccardMeasure 1

t words/WordNetAugmentedWordOverlap t vec/LSAWordSimilarity weighted NYT n-grams/CharacterNGramMeasure 3

t ngram/BigramOverlap t other/SentenceSize t other/SentenceSize

string/GreedyStringTiling 3 custom/RandomIndexingMeasure d200 wr0 t other/RelativeInfoContentDifference

string/LongestCommonSubsequenceNormComparator custom/SVSePhrSimilarityMeasure t ngram/ContentBigramOverlap

custom/RandomIndexingMeasure d200 wr0 esa/ESA Wiktionary n-grams/WordNGramJaccardMeasure 4

custom/StanfordNerMeasure aligngst string/LongestCommonSubstringComparator t other/NumbersSize

custom/StanfordNerMeasure alignlcs t other/NumbersSize t other/NumbersSubset

custom/StanfordNerMeasure overlap n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/SVSePhrSimilarityMeasure

custom/SVSePhrSimilarityMeasure custom/SVSeTopSimilarityMeasure custom/SemanticVectorsSimilarityMeasure d200 wr6p

esa/ESA WordNet

OnWN - r3wtCD (7 shown/7 selected) SMTnews - r3wtCD (15 shown/23 selected) esa/ESA Wiktionary

esa/ESA Wiktionary string/GreedyStringTiling 3 string/LongestCommonSubsequenceComparator

string/LongestCommonSubsequenceComparator custom/StanfordNerMeasure overlap string/LongestCommonSubsequenceNormComparator

string/GreedyStringTiling 3 n-grams/CharacterNGramMeasure 2 n-grams/WordNGramContainmentMeasure 1 stopword-filtered

string/LongestCommonSubsequenceNormComparator custom/RandomIndexingMeasure d200 wr6p word-sim/MCS06 Resnik WordNet

string/LongestCommonSubstringComparator n-grams/CharacterNGramMeasure 3 t ngram/ContentUnigramOverlap

word-sim/MCS06 Resnik WordNet string/LongestCommonSubsequenceComparator n-grams/WordNGramContainmentMeasure 2 stopword-filtered

n-grams/WordNGramContainmentMeasure 2 stopword-filtered custom/StanfordNerMeasure aligngst n-grams/WordNGramJaccardMeasure 2 stopword-filtered

custom/SVSePhrSimilarityMeasure t ngram/UnigramOverlap

esa/ESA Wiktionary t ngram/BigramOverlap

esa/ESA WordNet t other/StocksSize

n-grams/WordNGramContainmentMeasure 2 stopword-filtered t words/GreedyLemmaAligningOverlap

n-grams/WordNGramJaccardMeasure 1 t other/StocksOverlap

string/LongestCommonSubstringComparator

custom/RandomIndexingMeasure d200 wr6d

custom/RandomIndexingMeasure d200 wr0

Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all

sources.

headlines OnWN FNWN SMT mean

dkselected 0.70331 0.79752 0.38358 0.31744 0.571319

dkselected + SVSePhrSimilarityMeasure 0.70178 0.79644 0.38685 0.32332 0.572774

dkselected + RandomIndexingMeasure d200 wr0 0.70054 0.79752 0.38432 0.31615 0.570028

dkselected + SVSeTopSimilarityMeasure 0.69873 0.79522 0.38815 0.31723 0.569533

dkselected + RandomIndexingMeasure d200 wr6d 0.69944 0.79836 0.38416 0.31397 0.569131

dkselected + RandomIndexingMeasure d200 wr6b 0.69992 0.79788 0.38435 0.31328 0.568957

dkselected + RandomIndexingMeasure d200 wr6p 0.69878 0.79848 0.37876 0.31436 0.568617

dkselected + StanfordNerMeasure aligngst 0.69446 0.79502 0.38703 0.31497 0.567212

dkselected + StanfordNerMeasure overlap 0.69468 0.79509 0.38703 0.31466 0.567200

dkselected + StanfordNerMeasure alignlcs 0.69451 0.79486 0.38657 0.31394 0.566807

(dk + all custom) selected 0.70311 0.79887 0.37477 0.31665 0.570586

Also, we see some source-specific behavior. None

of our introduced measures are able to improve the

headlines similarities. However, random indexing

improves OnWN scores, several strategies improve

the FNWN metric, and simsvs-phr is the only viable

performance improvement on the SMT corpus.

5 Discussion

Mayo Clinic’s submissions to Semantic Textual

Similarity 2013 performed well, placing 5th, 6th,

and 8th among 90 submitted systems. We intro-

duced similarity metrics that used different means

to do compositional distributional semantics along

with some named entity-based measures, finding

some improvement especially for phrasal similar-

ity from structured vectorial semantics. Through-

out, we utilized forward-search feature selection,

which enhanced the performance of the models. We

also used source-based linear regression models and

considered unseen sources as mixtures of existing

sources; we found that in-domain data is neces-

sary for smaller, source-based models to outperform

larger, conglomerate models.

Acknowledgments

Thanks to the developers of the UKP DKPro sys-

tem and the TakeLab system for making their code

available. Also, thanks to James Masanz for initial

implementations of some similarity measures.

153



References

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor

Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot

on semantic textual similarity. In Proceedings of the

First Joint Conference on Lexical and Computational

Semantics-Volume 1: Proceedings of the main confer-

ence and the shared task, and Volume 2: Proceedings

of the Sixth International Workshop on Semantic Eval-

uation, pages 385–393. Association for Computational

Linguistics.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-

Agirre, and Weiwei Guo. 2013. *sem 2013 shared

task: Semantic textual similarity, including a pilot on

typed-similarity. In *SEM 2013: The Second Joint

Conference on Lexical and Computational Semantics.

Association for Computational Linguistics.

Lloyd Allison and Trevor I Dix. 1986. A bit-string

longest-common-subsequence algorithm. Information

Processing Letters, 23(5):305–310.

Daniel Bär, Chris Biemann, Iryna Gurevych, and Torsten

Zesch. 2012. Ukp: Computing semantic textual sim-

ilarity by combining multiple content similarity mea-

sures. In Proceedings of the First Joint Conference

on Lexical and Computational Semantics-Volume 1:

Proceedings of the main conference and the shared

task, and Volume 2: Proceedings of the Sixth Interna-

tional Workshop on Semantic Evaluation, pages 435–

440. Association for Computational Linguistics.

Scott Deerwester, Susan Dumais, George W. Furnas,

Thomas K. Landauer, and Richard Harshman. 1990.

Indexing by latent semantic analysis. Journal of the

American Society for Information Science, 41(6):391–

407.

Katrin Erk and Sebastian Padó. 2008. A structured vec-

tor space model for word meaning in context. In Pro-

ceedings of EMNLP 2008.

Jenny Rose Finkel, Trond Grenager, and Christopher

Manning. 2005. Incorporating non-local information

into information extraction systems by gibbs sampling.

In Proceedings of the 43rd Annual Meeting on Associ-

ation for Computational Linguistics, pages 363–370.

Association for Computational Linguistics.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard

Pfahringer, Peter Reutemann, and Ian H. Witten.

2009. The weka data mining software: an update.

SIGKDD Explor. Newsl., 11(1):10–18, November.

Pentti Kanerva, Jan Kristofersson, and Anders Holst.

2000. Random indexing of text samples for latent se-

mantic analysis. In Proceedings of the 22nd annual

conference of the cognitive science society, volume

1036. Citeseer.

T.K. Landauer and S.T. Dumais. 1997. A Solution to

Plato’s Problem: The Latent Semantic Analysis The-

ory of Acquisition, Induction, and Representation of

Knowledge. Psychological Review, 104:211–240.

Alon Lavie and Michael J Denkowski. 2009. The meteor

metric for automatic evaluation of machine translation.

Machine translation, 23(2-3):105–115.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based

models of semantic composition. In Proceedings of

ACL-08: HLT, pages 236–244, Columbus, OH.

M. Sahlgren. 2005. An introduction to random index-

ing. In Methods and Applications of Semantic Index-

ing Workshop at the 7th International Conference on

Terminology and Knowledge Engineering, TKE, vol-

ume 5.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,

and Bojana Dalbelo Bašić. 2012. Takelab: Sys-

tems for measuring semantic text similarity. In Pro-

ceedings of the Sixth International Workshop on Se-

mantic Evaluation (SemEval 2012), pages 441–448,

Montréal, Canada, 7-8 June. Association for Compu-

tational Linguistics.

Dominic Widdows and Trevor Cohen. 2010. The seman-

tic vectors package: New algorithms and public tools

for distributional semantics. In Semantic Computing

(ICSC), 2010 IEEE Fourth International Conference

on, pages 9–15. IEEE.

D. Widdows and K. Ferraro. 2008. Semantic vec-

tors: a scalable open source package and online tech-

nology management application. Proceedings of the

Sixth International Language Resources and Evalua-

tion (LREC’08), pages 1183–1190.

Michael J Wise. 1996. Yap3: Improved detection of sim-

ilarities in computer program and other texts. In ACM

SIGCSE Bulletin, volume 28, pages 130–134. ACM.

Stephen Wu and William Schuler. 2011. Structured com-

position of semantic vectors. In Proceedings of the In-

ternational Conference on Computational Semantics.

Stephen Tze-Inn Wu. 2010. Vectorial Representations

of Meaning for a Computational Model of Language

Comprehension. Ph.D. thesis, Department of Com-

puter Science and Engineering, University of Min-

nesota.

154


