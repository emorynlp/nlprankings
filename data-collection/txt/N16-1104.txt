



















































Effective Crowd Annotation for Relation Extraction


Proceedings of NAACL-HLT 2016, pages 897–906,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Effective Crowd Annotation for Relation Extraction

Angli Liu, Stephen Soderland, Jonathan Bragg,
Christopher H. Lin, Xiao Ling, and Daniel S. Weld

Turing Center, Department of Computer Science and Engineering
Box 352350

University of Washington
Seattle, WA 98195, USA

{anglil, soderlan, jbragg, chrislin, xiaoling, weld} at cs.washington.edu

Abstract

Can crowdsourced annotation of training data
boost performance for relation extraction over
methods based solely on distant supervision?
While crowdsourcing has been shown effec-
tive for many NLP tasks, previous researchers
found only minimal improvement when ap-
plying the method to relation extraction. This
paper demonstrates that a much larger boost
is possible, e.g., raising F1 from 0.40 to 0.60.
Furthermore, the gains are due to a sim-
ple, generalizable technique, Gated Instruc-
tion, which combines an interactive tutorial,
feedback to correct errors during training, and
improved screening.

1 Introduction

Relation extraction (RE) is the task of identifying
instances of relations, such as nationality (person,
country) or place of birth (person, location), in pas-
sages of natural text. Since RE enables a broad range
of applications — including question answering and
knowledge base population — it has attracted atten-
tion from many researchers. Many approaches to RE
use supervised machine learning, e.g., (Soderland et
al., 1995; Califf and Mooney, 1997; Lafferty et al.,
2001), but these methods require a large, human-
annotated training corpus that may be unavailable.

In response, researchers developed methods for
distant supervision (DS) in which a knowledge base
such as Wikipedia or Freebase is used to automati-
cally tag training examples from a text corpus (Wu
and Weld, 2007; Mintz et al., 2009). Indeed, virtu-
ally all entries to recent TAC KBP relation extraction

0.0 0.2 0.4 0.6 0.8 1.0

Distantly Supervised (DS) Data

0.0

0.2

0.4

0.6

0.8

1.0

D
S
 D

a
ta

 P
lu

s 
2

0
k 

G
I 
C

ro
w

d
so

u
rc

e
d
 E

x
a
m

p
le

s
precision nationality

precision born

precision lived

precision died

precision traveled

recall nationality

recall born

recall lived

recall died

recall traveled

F1 nationality

F1 born

F1 lived

F1 died

F1 traveled

Figure 1: Adding 20K crowdsourced instances, acquired using
Gated Instruction, to 700K examples from distant supervision
raises precision, recall, and F1 for nearly all relations and raises
overall F1 from 0.40 to 0.60 with MIML-RE learning.

competitions use distant supervision (Ji and Grish-
man, 2011). However, distant supervision provides
noisy training data with many false positives, and
this limits the precision of the resulting extractors
(see Section 2). A natural assumption is that human-
annotated training data, either alone or in conjunc-
tion with distant supervision, would give better pre-
cision. In particular, Snow et al. (2008) showed that,
for many NLP tasks, crowdsourced data is as good
as or better than that annotated by experts.

It is quite surprising, therefore, that researchers
who have applied crowdsourced annotation to rela-
tion extraction argue the opposite, that crowdsourc-
ing provides only minor improvement:

897



• Zhang et al. (2012) conclude that “Human feed-
back has relatively small impact on precision and
recall.” Instead, they advise applying distant su-
pervision to vastly more data.
• Pershina et al. (2014) assert “Simply taking the

union of the hand-labeled data and the corpus la-
beled by distant supervision is not effective since
hand-labeled data will be swamped by a larger
amount of distantly labeled data.” Instead, they
introduce a complex feature-creation approach
which improves the F1-score of MIML-RE, a
state-of-the-art extractor (Surdeanu et al., 2012),
just 4%, from 0.28 to 0.32 on a set of 41 TAC
KBP relations.
• Angeli et al. (2014) explored a novel active

learning method to control crowdsourcing, but
found no improvement from adding the crowd-
sourced training to distant supervision using the
default settings of MIML-RE, and only a 0.04
improvement in F1 when they initialized MIML-
RE using the crowdsourced training.

This paper reports quite a different result, show-
ing up to a 0.20 boost to F1. By carefully de-
signing a quality-controlled crowdsourcing work-
flow that uses Gated Instruction (GI), we are able
to create much more accurate annotations than those
produced by previous crowdsourcing methods. GI
(summarized in Figure 2) includes an interactive tu-
torial to train workers, providing immediate feed-
back to correct mistakes during training. Workers
are then screened by their accuracy on gold-standard
questions while doing the annotation. We show that
GI generates much better training data than crowd-
sourcing used by other researchers, and that this
leads to dramatically improved extractors.

Adding GI-crowdsourced annotations of the ex-
ample sentences selected by Angeli et al.’s active
learning method provides a much larger boost to the
performance of the learned extractors than when
their traditional crowdsourcing methods are used.
In fact, the improvement due to our crowdsourcing
method substantially outweighs the benefits of An-
geli et al.’s active learning strategy as well. In total,
this paper makes the following contributions:
• We present the design of the Gated Instruction

crowdsourcing workflow with worker training
and screening that ensures high-precision anno-

tations for relation extraction training data in the
presence of unreliable workers.

• We demonstrate that Gated Instruction increases
the annotation quality of crowdsourced training
data, raising precision from 0.50 to 0.77 and
recall from 0.70 to 0.78, compared to Angeli
et al.’s crowdsourced tagging of the same sen-
tences. We make the data available for future
research (Section 4.1).

• Augmenting distant supervision with 10K of
Angeli et al.’s training examples annotated using
Gated Instruction boosts F1 from 0.40 to 0.47,
compared to 0.43, the result from using Angeli
et al.’s crowdsourced annotations.

• We demonstrate that improved crowdsourcing
has a greater effect than Angeli et al.’s active
learning approach. Adding 10K randomly se-
lected sentences, labeled using Gated Instruc-
tion, to distantly supervised data raises F1 by
6 points, compared to the 3 point gain from
adding Angeli et al.’s crowdsourced labels on
their active-learning sample.

• In contradiction to Zhang et al.’s prior claims, we
show that increasing amounts of crowdsourced
data can dramatically improve extractor perfor-
mance. When we augmented distant supervision
with 20K instances using Gated Instruction, we
show that F1 is raised from 0.40 to 0.60.

• Gated Instruction may also reduce the cost of
crowdsourcing. We show that with the high
quality Gated Instruction annotations, a single
annotation is more effective than majority vote
over multiple annotators.

Our results provide a clear lesson for future re-
searchers hoping to use crowdsourced data for NLP
tasks. Extreme care must be exercised in the details
of the workflow design to ensure quality data and
useful results.

2 Background and Related Work

Distant supervision (DS) is a method for training
extractors that obviates the need for human-labeled
training data by heuristically matching facts from a
background knowledge base (KB) to a large textual
corpus. Originally developed to extract biological
relations (Craven and Kumlien, 1999), DS was later
extended to extract relations from Wikipedia in-

898



foboxes (Wu and Weld, 2007) and Freebase (Mintz
et al., 2009). Specifically, distant supervision uses
the KB to find pairs of entities E1 and E2 for which
a relation R holds. Distant supervision then makes
that assumption that any sentence that contains a
mention of both E1 and E2 is a positive training in-
stance for R(E1, E2).

Unfortunately, this assumption leads to a large
proportion of false positive training instances. For
example, Freebase asserts that Nicolas Sarkozy was
born in Paris, but nearly all sentences in a news
corpus that mention Sarkozy and Paris do not give
evidence for a place of birth relation. To address
this shortcoming, there have been attempts to model
the relation dependencies as multi-instance multi-
class (Bunescu and Mooney, 2007; Riedel et al.,
2010) leading to state-of-the art extraction learners
MultiR (Hoffmann et al., 2011) and MIML-RE (Sur-
deanu et al., 2012).

Additionally, other techniques developed to study
the relation extraction problem have achieved cer-
tain success, including universal schemas (Riedel
et al., 2013), and deep learning (Nguyen and Gr-
ishman, 2014). Despite these technical innova-
tions, the best systems at the TAC-KBP evaluation1

still require substantial human effort, typically hand-
written extraction rules (Surdeanu and Ji, 2014).

Recently researchers have explored the idea of
augmenting distant supervision with a small amount
of crowdsourced annotated data in an effort to im-
prove relation extraction performance (Angeli et al.,
2014; Zhang et al., 2012; Pershina et al., 2014).

Zhang et al. (2012) studied how the size of the
crowdsourcing training corpus and distant supervi-
sion corpus affect the performance of the relation
extractor. They considered the 20 TAC KBP re-
lations that had a corresponding Freebase relation.
They added up to 20K instances of crowd data to
1.8M DS instances using sparse logistic regression,
tuning the relative weight of crowdsourced and DS
training. However, they saw only a marginal im-
provement from F1 0.20 to 0.22 when adding crowd-
sourced training to DS training, and conclude that
human feedback has little impact.

Angeli et al. (2014) also investigated meth-
ods for infusing distant supervision with crowd-
sourced annotations in the Stanford TAC-KBP sys-

1http://www.nist.gov/tac/

tem. They experimented with several methods, in-
cluding adding a random sample of annotated sen-
tences to the training mix, and using active learning
to select which sentences should be annotated by
humans. Their best results were what they termed
“Sample JS,” training a committee of MIML-RE
classifiers and then sampling the sentences to be
crowdsourced weighted by the divergence of clas-
sifications.

Surprisingly, they found that the simple approach
of just adding crowdsourced data to the training mix
hurt extractor performance slightly. They conclude
that the most important use for crowdsourced an-
notations is as a way to initialize MIML-RE, miti-
gating the problem of local minima during learning.
When they initialized MIML-RE with 10K Sam-
ple JS crowdsourced instances and then trained on
a combination of Sample JS crowdsourced and DS
instances, this raised F1 from 0.34 to 0.38.

Pershina et al. (2014) also exploited a small set
of highly informative hand-labeled training data to
improve distant supervision. Rather than crowd-
sourcing, they used the set of 2,500 labeled instances
from the KBP 2012 assessment data. They state that
“Simply taking the union of the hand-labeled data
and the corpus labeled by distant supervision is not
effective since hand-labeled data will be swamped
by a larger amount of distantly labeled data.” Instead
they use the hand-annotated data to learn guidelines
that are included in a graphical prediction model
that extends MIML-RE, trained using distant super-
vision. This raised F1 from 0.28 to 0.32 over a com-
parison system without the learned guidelines.

Gormley et al. (2010) filtered crowdsourced
workers by agreement with gold questions and by
noting which workers took fewer than three seconds
per question. They reported good inter-annotator
agreement, but did not build a relation extractor from
their data.

Both Zhang et al. and Angeli et al. used tradi-
tional methods to ensure the quality of their crowd-
sourced data. Zhang et al. replicated each ques-
tion three times and included a gold question (i.e.,
one with a known answer) in each set of five ques-
tions. They only used answers from workers who
answered at least 80% of the gold-standard ques-
tions correctly.

899



Angeli et al. included two gold-standard ques-
tions in every set of 15. They discarded sets in
which both controls were answered incorrectly, and
additionally discarded all submissions from workers
who failed the controls on more than one third of
their submissions. They collected five annotations
for each example, and used the majority vote as the
ground truth in their training. They did not report the
resulting quality of their crowdsourced annotations,
but did release their data, allowing us to measure its
precision and recall (see Section 4.1).

We argue that all these systems would have got-
ten better performance by focusing attention on
the quality of their crowdsourced annotation. We
demonstrate that by improving the crowdsourcing
workflow, we achieve a higher F1 score, both with
the crowdsourced training alone and in combination
with distant supervision.

Our work adds to the existing large body of work
that shows that crowdsourcing can be and is an ef-
fective and efficient method for training machine
learning algorithms. Snow et al. (2008) showed
that multiple workers can simulate an expert worker
in a variety of natural language tasks. Many re-
searchers (e.g., (Dawid and Skene, 1979; Whitehill
et al., 2009)) have designed methods to aggregate
crowd labels in order to reduce noise, and Sheng et
al. (2008) showed that paying multiple crowd work-
ers to relabel examples, as opposed to labeling new
ones, can increase the accuracy of a classifier.

The effectiveness of crowdsourcing is dependent
on a number of human factors. Several researchers
have studied how worker retention is affected by
payment schemes (Mao et al., 2013), recruitment
techniques (Ipeirotis and Gabrilovich, 2014), or at-
tention diversions (Dai et al., 2015). Ipeirotis and
Gabrilovich show that volunteer workers may pro-
vide higher quality work. By contrast, we show that
paid workers, too, can produce high quality work
through careful attention to worker training and test-
ing.

3 Gated Instruction Crowdsourcing

We used Amazon Mechanical Turk for our crowd-
sourcing, but designed our own website to imple-
ment the Gated Instruction (GI) protocol, rather than
use the platform Amazon provides directly. This al-
lowed us greater control over the UI and the worker

Gated Instruction Crowdsourcing Protocol 
 
Phase I: Interactive tutorial 

1.  Give a clear definition of each relation and tagging criteria. 
2.  Worker annotates practice sentences that illustrate each 

relation. 
3.  Give immediate feedback after each practice sentence. 

Phase II: Screening questions 
1.  Worker annotates representative set of 5 gold questions. 
2.  Give feedback to worker on each question. 
3.  Eliminate workers who fail a majority of these questions. 

Phase III: Batches of questions (with continued screening) 
1.  Include gold questions without feedback. 
2.  Sets of 5 gold questions in batches (20 questions) with 

exponentially decreasing frequency. 
3.  Eliminate workers with accuracy lower than 80% on last 10 

gold questions. 
 
General Principles 

1.  Accept only workers with AMT reputation above threshold. 
2.  Provide a link to definitions of relations during the task. 
3.  Worker may not proceed before correcting mistakes shown 

in feedback. 
4.  Give feedback on how much earned so far and performance 

on gold questions after each batch. 
5.  Remind of a bonus from completing all 10 batches. 

Figure 2: Architecture of the Gated Instruction protocol.

experience. The primary benefit of GI is worker
training, which is necessary across platforms, so we
expect to see comparable results on other platforms,
such as CrowdFlower.

The ideas behind Gated Instruction are summa-
rized in Figure 2. The workflow proceeds in three
phases: tutorial, weed-out, and work (described be-
low) with a focus on well-known user interface prin-
ciples (rapid feedback and availability of extra help).
While conceptually simple, we show this approach
has a much bigger effect on the resulting learned
NLP system than a more complex graphical model.

3.1 Interactive Tutorial Design

The most important step in crowdsourcing is ensur-
ing that workers understand the task. To this end we
required workers to complete an interactive tutorial
to learn the criteria for the relations to be annotated.

Since we wanted to test our extractor against of-
ficial answers for the TAC-KBP Slot Filling evalu-
ation, our tutorial taught workers to follow the offi-
cial KBP guidelines. These guidelines require tag-
ging only relations directly stated in the sentence,
and discourage plausible inferences. For example, if
a sentence states only that a person works in a city,
then annotating a place of residence relation with

900



Figure 3: Tutorial page that teaches guidelines for nationality and lived in. The worker answers practice sentences with immediate
feedback that teach each relation.

that city is counted as an error, even if it is proba-
ble that the person lives there.

Figure 3 shows a page from the tutorial that
explains annotation guidelines for nationality and
lived in (i.e., place of residence). This figure shows
the first page of the tutorial — as more relations are
taught, those relations are added to the question. The
real questions are asked in the same format later on
for consistency. The worker can click on a link to
see the relation definitions at any time during the tu-
torial or while doing the actual task. If workers make
a mistake during the tutorial, they are given immedi-
ate feedback along with an explanation for the cor-
rect answer. The workers cannot proceed without
correcting all errors on all problems in the tutorial.

3.2 Adaptive Worker Screening

After examining worker mistakes in a preliminary
experiment, we manually selected a set of gold ques-
tions (i.e., questions with unambiguous, known an-
swers) that workers are likely to get wrong if they
don’t clearly understand the annotation criteria. The
gold questions are grouped into sets of 5 questions
that represent all relations being annotated. The first
5 questions (the screening phase) are used to elimi-
nate spammers and careless workers early on. These
questions look no different from normal questions,
but we give feedback to workers with the right an-
swers if workers give wrong answers to any of these
questions. If a worker fails a majority of such ques-
tions, the worker is disqualified from the task.

We then place additional sets of gold questions
among real test questions without feedback in order
to spot-check workers’ responses. In our experience,
workers who start out with high accuracy maintain
that accuracy throughout the entire session. There-
fore, we place the gold questions in exponentially
decreasing frequency among the batches of 20 ques-
tions (5 gold questions in batches 2, 4, 8, etc.), and
allow only workers who maintain at least 80% ac-
curacy on the most recent 10 gold questions to con-
tinue with the task. Our task was not large enough
to attract problems of collusion, but more lucrative
or long-running tasks may require continual gen-
eration of new gold questions in order to combat
sharing of answers among workers (Oleson et al.,
2011). Techniques such as expectation maximiza-
tion (Dawid and Skene, 1979) can be used to pro-
duce new gold questions based on worker answers.

3.3 Motivational Feedback

We want workers to stay motivated, so our crowd-
sourcing system also provides feedback to work-
ers. In particular, workers receive adaptive per-batch
message feedback at the end of each batch of ques-
tions (every 20 questions) about how well they did
on the gold questions in the past batches, how much
they have earned so far, and a reminder of the bonus
for finishing all 10 batches. We paid workers $0.50
for each batch of 20 questions with a bonus of $1.00
for finishing 10 batches.

901



4 Experimental Results

In this section, we address the following questions:

• Does Gated Instruction produce training data
with higher precision and recall than other re-
search in crowdsourcing for relation extrac-
tion?

• Does higher quality crowdsourced training data
result in higher extractor performance when
adding crowdsourcing to distant supervision?

• How does the boost in extractor performance
on random training instances labeled with
Gated Instruction compare to that with in-
stances labeled using traditional crowdsourcing
techniques selected with active learning?

• How does extractor performance increase with
larger amounts of Gated Instruction training
data?

• What’s the most cost-effective way to aggre-
gate worker votes? Are multiple annotations
needed, given high quality crowdsourcing?

4.1 Quality of Gated Instruction Training
We took the best training set of 10,000 instances
from Angeli et al.’s 2014 system that selected train-
ing instances using active learning (their Sample JS
data). In order to focus on the effect of crowdsourc-
ing, we restricted our attention to four distinct re-
lations between person and location that were used
by previous researchers: nationality, place of birth,
place of residence, and place of death2. We then
sent these sentences to crowdsourced workers using
the Gated Instruction protocol.

To evaluate the crowdsourced training data qual-
ity, we hand-tagged the crowdsourced annotations
from both our Gated Instruction system and Angeli
et al.’s work on 200 random instances. Annotations
were considered correct if they followed the TAC-
KBP annotation guidelines. Two authors tagged the
sample with 87% agreement and then reconciled
opinions to agree on consensus labels.

The training precision, recall, and F1 are shown
in Figure 4. In this and all other experiments, aggre-
gate statistics are macro-averaged across relations.
We also include the training quality from Zhang et

2We collapsed the KBP relations per:city of *,
per:stateorprovince of *, and per:country of * into a sin-
gle relation place of *.

              Precision                 Recall                 F1
0.0

0.2

0.4

0.6

0.8

1.0

0.15

0.64

0.24

0.50

0.70

0.58

0.77 0.78 0.77

Zhang Angeli Gated Instruction

Figure 4: The training data produced by Gated Instruction has
much higher precision and somewhat higher recall than that of
Angeli et al. or Zhang et al.

al., although this is on a different set of sentences
and only for place of birth, place of residence, and
place of death.

Our Gated Instruction protocol gives higher F1 for
the training set of each of the four relations we com-
pared with Angeli’s crowdsourcing on the same sen-
tences. Our overall F1 was 0.77, compared to 0.58
for Angeli et al. and 0.24 for Zhang et al. The differ-
ence in precision is most dramatic, with our system
achieving 0.77 compared to 0.50 and 0.15.

Worker agreement with GI was surprisingly high.
Two workers agreed on between 78% to 97% of the
instances, depending on the relation. The average
agreement was 88%. The data is available for re-
search purposes.3

4.2 Integrating Crowdsourced Data with the
Relation Extraction Pipeline

The pipeline of our relation extraction system is as
follows. First we collected sentences of training data
from the TAC-KBP newswire corpus that contain a
person and a location according to the Stanford NER
tagger (Finkel et al., 2005). We represent them us-
ing the features described by Mintz et al. (2009).
These features include NER tags of the two argu-
ments, the dependency path between two designated
arguments, the sequence of tokens between the ar-

3https://www.cs.washington.edu/ai/gated_
instructions/naacl_data.zip

902



                      Precision                       Recall                       F1
0.0

0.2

0.4

0.6

0.8

1.0

0.40 0.41 0.400.39

0.47
0.430.43

0.50
0.46

0.43

0.51
0.47

DS only

DS plus Angeli's tagging of SampleJS

DS plus GI tagging of 10k random instances

DS plus GI tagging of SampleJS

Figure 5: Adding 10K instances with Gated Instruction to
700K DS instances boosts F1 more than that of the origi-
nal Sample JS annotations. Furthermore, GI applied to 10K
randomly-selected instances outperforms active learning with
traditional annotation.

guments, and the order of the arguments in the sen-
tence.

We then split the data into 700K used for distant
supervision and much smaller sets for crowdsourc-
ing and for a held-out test set. For the experiments
presented, unless otherwise noted, we used a variant
of majority vote to create a training set. We obtained
annotations from two workers for each example sen-
tence and kept the instances where both agreed as
our training data.

Finally, we ran a learning algorithm on the distant
supervision training data, the crowdsourced training
data, and a combination of the two. The results were
evaluated on the hand-labeled test set.

4.3 Effect of Data Quality on Extractor
Performance

We now study how the higher quality training data
from our crowdsourcing protocol affects extractor
performance, when it is added to a large amount of
distantly-supervised data.

We compared adding the 10K crowdsourced in-
stances from the previous experiment to 700K in-
stances from distant supervision, where the crowd-
sourced data had tags from either Gated Instruction
or the original crowdsourcing from Angeli et al. We
compare only with Angeli et al. as we did not have

annotations from Zhang et al. for the same training
sentences.

We experimented using three learning algorithms:
logistic regression, MultiR, and MIML-RE. We
found that logistic regression gives the best results
when applied to the crowdsourced training alone.
With logistic regression, training on the 10K Sam-
ple JS instances gave F1 of 0.31 with Angeli et al.’s
crowdsourced labels and 0.40 with Gated Instruc-
tion. Logistic regression is not a good fit for dis-
tant supervision — we had F1 of 0.34 from logistic
regression trained on DS only.

MultiR and MIML-RE gave the best results for
combining crowdsourcing with distant supervision.
Each of these multi-instance multi-class learners had
similar results, so we present results only for MIML-
RE in the remainder of our experiments, as it is the
learning algorithm used by other researchers.

We included no special mechanisms to prevent
distant supervision data from swamping the smaller
amount of crowdsourced data. MIML-RE has a
built-in mechanism to combine supervised and dis-
tant supervision. It automatically builds a classifier
from the supervised instances, uses this to initialize
the distant supervision instance labels, and locks the
supervised labels. With MultiR, we put the crowd-
sourced instances in separate singleton “bags” of
training instances, since MultiR always takes at least
one instance in each bag as truth.

As Angeli et al. found, it is important to use the
crowdsourced training to initialize MIML-RE. With
the default initialization, Angeli et al. report no gain
in F1. We found a small gain in F1 even with the
default initialization, but larger gains with crowd-
sourced initialization, which we use for the follow-
ing experiments.

To see how much of the boost over distant su-
pervision comes from the active learning that went
into Angeli et al.’s sample JS training, we also used
Gated Instruction on a randomly selected set of 10K
newswire instances from the TAC KBP 2010 corpus
(LDC2010E12) that contained at least one NER tag
for person and one for location.

As Figure 5 shows, adding the Sample JS training
with Gated Instruction crowdsourcing had a positive
impact on performance, increasing precision from
0.40 to 0.43, recall from 0.41 to 0.51, and F1 from
0.40 to 0.47. With the original crowdsourced tag-

903



              Precision                 Recall                 F1
0.0

0.2

0.4

0.6

0.8

1.0

0.40 0.41 0.40
0.43

0.50
0.46

0.56

0.64
0.60

DS only

DS plus GI tagging of 10k random instances

DS plus GI tagging of 20k random instances

Figure 6: Adding 20K instances with Gated Instruction to DS
gives a large boost to both precision and recall, raising F1 from
0.40 to 0.60.

ging from Angeli et al., adding the crowdsourced
instances actually caused a small drop in precision,
a smaller gain in recall than Gated Instruction, and
F1 of 0.43 — substantially less than achieved with
labels from Gated Instruction.

Furthermore, in an apples-to-oranges comparison,
we found that our improved crowdsourcing proto-
col had a much bigger impact than Angeli et al.’s
active learning mechanism. Adding 10K randomly
selected newswire instances tagged with Gated In-
struction gave higher precision (0.43), recall (0.51),
and F1 (0.46) than adding instances selected by ac-
tive learning (Sample JS) when labeled using Angeli
et al.’s protocol. In fact Gated Instruction gave dou-
ble the improvement (6 points gain in F1 vs. 3). Of
course, both of these numbers are small — bigger
gains come from using the techniques together, and
especially from using more crowdsourced data.

4.4 Effect of Data Quantity on Extractor
Performance

Zhang et al. reported negligible improvement in F1
from adding 20K instances with their crowdsourc-
ing to distant supervision, and Angeli et al. reported
a gain of 0.04 F1 from adding 10K instances with
active learning and their crowdsourcing.

As Figure 6 shows, Gated Instruction can raise
F1 from 0.40 to 0.60 over distant supervision alone
from adding 20K random newswire instances. This
experiment uses all five relations that we crowd-

Figure 7: With high quality crowdsourcing, the simple policy
of requesting a single annotation performs better than majority-
vote of 3, 5, 7, 9, or 11 annotations (holding the annotation
budget constant), since the increase in the number of data points
outweighs the reduction in noise.

sourced, adding travel to to the relations from Fig-
ure 5 that we had in common with Angeli et al. The
results for DS only and 10K random instances are
not significantly different from those in Figure 5 in
which travel to was omitted.

4.5 Comparison between Ways to Aggregate
Annotations

In this section we explore the cost-effectiveness of
alternate methods of creating training from Gated
Instruction annotations. We compare a policy of us-
ing the majority vote of two out of three, or three out
of five workers, and so forth, as opposed to solic-
iting a single annotation for each training sentence
(unilabeling). Lin et al. (2014) show that in many
settings, unilabeling is better because some classi-
fiers are able to learn more accurately with a larger,
noisier training set than a smaller, cleaner one.

With a given budget, single annotation gives three
times as many training instances as the policy that
uses three votes and five times as many as the policy
that requires five votes, and so forth. Is the quality
of data produced by Gated Instruction high enough
to rely on just one annotation per instance?

We randomly select 2K examples from the 20K
newswire instances and use Gated Instruction to ac-
quire labels from 10 workers for each sentence. Fig-
ure 7 shows that when training a logistic regression
classifier with high quality crowdsourcing data, a
single annotation is, indeed, more cost effective than

904



using a simple majority of three, five, or more anno-
tations (given a fixed budget). The learning curves
in Figure 7 use uncertainty sampling (US) to select
examples from the 2000 available with the curves la-
beled US 1/1 for single votes, US 2/3 for two out of
three, and so forth.

This is not to say that a single vote is always the
best policy. It is another example of the impact of
GI’s high quality annotation. In the same domain
of relation extraction, Lin et al. (2016) also show
that with a more intelligent and dynamic relabeling
policy, relabeling certain examples can still help.

5 Conclusion

This paper describes the design of Gated Instruc-
tion, a crowdsourcing protocol that produces high
quality training data. GI uses an interactive tuto-
rial to teach the annotation task, provides feedback
during training so workers understand their errors,
refuses to let workers annotate new sentences until
they have demonstrated competence, and adaptively
screens low-accuracy workers with a schedule of test
questions. While we demonstrate GI for the task of
relation extraction, the method is general and may
improve annotation for many other NLP tasks.

Higher quality training data produces higher ex-
tractor performance for a variety of learning algo-
rithms: logistic regression, MultiR, and MIML-RE.
Contrary to past claims, augmenting distant supervi-
sion with a relatively small amount of high-quality
crowdsourced training data gives a sizeable boost in
performance. Adding 10K instances that Angeli et
al. selected by active learning, annotated with Gated
Instruction, raised F1 from 0.40 to 0.47 — substan-
tially higher than the 0.43 F1 provided by Angeli et
al.’s annotations. We also find that Gated Instruction
is more effective than a complicated active learning
strategy. Adding 10K randomly selected instances
raises F1 to 0.46, and adding 20K random instances
gave F1 of 0.60.

Our experimental results yield two main takeaway
messages. First, we show that in contrast to prior
work, adding crowdsourced training data substan-
tially improves the performance of the resulting ex-
tractor as long as care is taken to ensure high quality
crowdsourced annotations. We haven’t yet experi-
mented beyond person-location relations, but we be-
lieve that Gated Instruction is generalizable, partic-

ularly where there are clear criteria to be taught. We
believe that Gated Instruction can greatly improve
training data for other NLP tasks beside relation ex-
traction as well.

Second, we provide practical and easily insti-
tuted guidelines for a novel crowdsourcing proto-
col, Gated Instruction, as an effective method for ac-
quiring high-quality training data. It’s important to
break complex annotation guidelines into small, di-
gestible chunks and to use tests (gates) to ensure that
the worker reads and understands each chunk of the
instructions before work begins. Without these ex-
tra checks, many poor workers pass subsequent gold
tests by accident, polluting results.

Acknowledgment

This work was supported by NSF grant IIS-1420667, ONR
grant N00014-15-1-2774, DARPA contract FA8750-13-2-0019,
the WRF/Cable Professorship, and a gift from Google. We are
very grateful to Chris Re and Ce Zhang who generously sup-
plied their annotated data. Likewise, Masha Pershina and Ralph
Grishman allowed us an early look at their paper and gave us
their data. Similarly, Gabor Angeli and Chris Manning not only
provided their data, but provided many insights about the best
ways to combine crowd and distantly supervised annotations.
Without this cooperation it would have been impossible to have
done our work. We appreciate Natalie Hawkins, who helped
prepare the data used in our study. We further thank Shih-Wen
Huang and Congle Zhang for their suggestions on the design of
the crowdsourcing system and the experiments.

References

Gabor Angeli, Julie Tibshirani, Jean Y. Wu, and Christo-
pher D. Manning. 2014. Combining distant and par-
tial supervision for relation extraction. In EMNLP.

Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.

M. Califf and R. Mooney. 1997. Relational learning
of pattern-match rules for information extraction. In
Workshop in Natural Language Learning, Conf. Assoc.
Computational Linguistics.

Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In ISMB.

Peng Dai, Jeffrey M Rzeszotarski, Praveen Paritosh, and
Ed H Chi. 2015. And Now for Something Completely
Different : Improving Crowdsourcing Workflows with
Micro-Diversions. In CSCW.

905



A.P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Applied Statistics, 28(1):20–28.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
363–370. Association for Computational Linguistics.

Matthew R. Gormley, Adam Gerber, Mary Harper, and
Mark Dredze. 2010. Non-expert correction of auto-
matically generated relation annotations. In Proceed-
ings of NAACL and HLT 2010.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of ACL. Associ-
ation for Computational Linguistics.

Panagiotis G. Ipeirotis and Evgeniy Gabrilovich. 2014.
Quizz: targeted crowdsourcing with a billion (poten-
tial) users. In WWW ’14: Proceedings of the 23rd In-
ternational Conference on the World Wide Web.

Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 1148–
1158, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, ICML ’01.

Christopher H. Lin, Mausam, and Daniel S. Weld. 2014.
To re(label), or not to re(label). In HCOMP.

Christopher H. Lin, Mausam, and Daniel S. Weld. 2016.
Reactive learning: Active learning with relabeling. In
AAAI.

Andrew Mao, Yiling Chen, Eric Horvitz, Megan E
Schwamb, Chris J Lintott, and Arfon M Smith. 2013.
Volunteering Versus Work for Pay: Incentives and
Tradeoffs in Crowdsourcing. In HCOMP.

Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of ACL. Association
for Computational Linguistics.

Thien Huu Nguyen and Ralph Grishman. 2014. Employ-
ing word representations and regularization for domain
adaptation of relation extraction. In Proceedings of the
52nd Annual Meeting of the Association for Computa-
tional Linguistics, volume 2, pages 68–74.

David Oleson, Alexander Sorokin, Greg P Laughlin,
Vaughn Hester, John Le, and Lukas Biewald. 2011.

Programmatic gold: Targeted and scalable quality as-
surance in crowdsourcing. In Human Computation
Workshop, page 11.

Maria Pershina, Bonan Min, Wei Xu, and Ralph Grish-
man. 2014. Infusion of labeled data into distant super-
vision for relation extraction. In Proceedings of ACL.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148–163.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. NAACL
HLT 2013, pages 74–84.

Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeiro-
tis. 2008. Get another label? improving data qual-
ity and data mining using multiple, noisy labelers. In
Proceedings of the Fourteenth ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254–
263. Association for Computational Linguistics.

S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictionary.
In Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, pages 1314–21.

Mihai Surdeanu and Heng Ji. 2014. Overview of the
English slot filling track at the TAC2014 knowledge
base population evaluation.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of EMNLP, pages 455–465. Association for Computa-
tional Linguistics.

Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R
Movellan, and Paul L Ruvolo. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In NIPS’09.

F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Portugal.

Ce Zhang, Feng Niu, Christopher Ré, and Jude Shavlik.
2012. Big data versus the crowd: Looking for relation-
ships in all the right places. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 825–834.
Association for Computational Linguistics.

906


