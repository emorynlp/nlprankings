










































An Empirical Study on the Effect of Morphological and Lexical Features in Persian Dependency Parsing


Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 97–107,
Seattle, Washington, USA, 18 October 2013. c©2013 Association for Computational Linguistics

An Empirical Study on the Effect of Morphological and Lexical Features in

Persian Dependency Parsing

Mojtaba Khallash, Ali Hadian and Behrouz Minaei-Bidgoli

Department of Computer Engineering

Iran University of Science and Technology

{khallash,hadian}@comp.iust.ac.ir, b minaei@iust.ac.ir

Abstract

This paper investigates the impact of dif-

ferent morphological and lexical information

on data-driven dependency parsing of Per-

sian, a morphologically rich language. We

explore two state-of-the-art parsers, namely

MSTParser and MaltParser, on the recently re-

leased Persian dependency treebank and es-

tablish some baselines for dependency pars-

ing performance. Three sets of issues are

addressed in our experiments: effects of us-

ing gold and automatically derived features,

finding the best features for the parser, and

a suitable way to alleviate the data sparsity

problem. The final accuracy is 87.91% and

88.37% labeled attachment scores for Malt-

Parser and MSTParser, respectively.

1 Introduction

Researchers have paid a lot of attention to data-

driven dependency parsing in recent years (Bohnet

and Kuhn, 2012; Bohnet and Nivre, 2012; Balles-

teros and Nivre, 2013). This approach is language-

independent and is solely dependent on the availabil-

ity of annotated corpora. Using data-driven parsers

for some languages requires careful selection of fea-

tures and tuning of the parameters to reach maxi-

mum performance. Difficulty of dependency pars-

ing in each language depends on having either free

word order or morphological information. Lan-

guages with free word order have a high degree

of freedom in arranging the words of a sentence.

Consequently, they usually have a high percentage

of non-projective structures. Morphology is deter-

mined by large inventory of word forms (Tsarfaty et

al., 2010).

According to the results from CoNLL shared

task 2007, languages are classified to three classes,

namely low, medium and high accuracy languages.

Among them, low-accuracy languages have high de-

gree of free word order along with inflection (Nivre

et al., 2007a). Languages which are more challeng-

ing in parsing are called morphologically rich lan-

guages (MRLs). In MRLs, multiple levels of infor-

mation, concerning syntactic units and relations, are

expressed at the word-level (Tsarfaty et al., 2010).

Free word order can be handled by non-projective

parsing algorithms via either post-processing the

output of a strictly projective parser (Nivre and

Nilsson, 2005), combining adjacent (Nivre, 2009)

or non-adjacent sub-structures (McDonald et al.,

2005). Nevertheless, there is no general solution

for resolving rich morphology issue and hence many

researcher focus on features of a specific language.

Most data-driven dependency parsers do not use any

information that is specific to the language being

parsed, but it is shown that using language specific

features has a crucial role in improving the overall

parsing accuracy (Ambati et al., 2010a).

Persian is an Indo-European language that is writ-

ten in Perso-Arabic script (written from right to

left). The canonical word order of Persian is SOV

97



(subject-object-verb), but there are a lot of frequent

exceptions in word order that turn this language into

a free word order language (Shamsfard, 2011). This

language has a high degree of free word order and

complex inflections. As an example of rich mor-

phology, there are more than 100 conjugates and

2800 declensions for some lemmas in Persian (Ra-

sooli et al., 2011).

Dependency treebank for Persian (Rasooli et al.,

2013) language has newly become available. Due to

the lack of deep research on dependency parsing in

Persian, we establish some baselines for dependency

parsing performance. We also conduct a set of ex-

periments in order to estimate the effect of errors in

morphological disambiguation on the parsers. We

show that with two simple changes to the input data,

performance of the two parsers can be improved for

both gold (manually annotated) and predicted data.

The remainder of the paper is organized as fol-

lows. Section 2 presents a brief overview of recent

studies on parsing morphologically rich languages.

In section 3, we introduce available morphological

features annotated in our experiments. Section 4 de-

scribes the experimental setup, including corpus and

parsers we use, and presents our experiments. Ex-

perimental evaluation and analysis of parsing errors

are demonstrated in Section 5. Finally, we draw con-

clusions and suggest future work in Section 6.

2 Related work

Many studies have been done on using morpholog-

ical features for parsing morphologically rich lan-

guages, (e.g. Bengoetxea and Gojenola (2010),

Seeker and Kuhn (2013), etc.). Koo et al. (2008) in-

troduce cluster-based features that incorporate word

clusters derived from a large corpus of plain text, to

improve statistical dependency parsing for English

and Czech. Agirre et al. (2011) use lexical semantic

information derived from WordNet.

Marton et al. (2011) augment the baseline model

for Arabic with nine morphological features. They

show that using predicted features causes a substan-

tial drop in accuracy while it greatly improves per-

formance in the gold settings. They show that us-

ing noisy morphological information is worse than

using nothing at all. Same phenomenon is re-

ported for Hebrew (Goldberg and Elhadad, 2010),

except that using morphological-agreement feature

improves the accuracy of both gold and predicted

morphological information.

Another interesting research direction is to find

the most beneficial features for dependency parsing

for each language. Ambati et al. (2010b) explored

the pool of features for Hindi through a series of ex-

periments. In their setting, features are incremen-

tally selected to create the best parser feature set. In

Korean, Choi and Palmer (2011b) focus on feature

extraction and suggest a rule-based way of selecting

important morphemes to use only these as features

to build dependency parsing models.

For the Persian language, Seraji et al. (2012b) in-

vestigated state-of-the-art dependency parsing algo-

rithms on UPDT1 (Seraji et al., 2012a). They test

three feature settings, namely gold POS tags for both

the training and the test sets (GG), gold POS tags for

the training set and auto-generated POS tags for the

test set (GA), and auto-generated POS tags for both

the training and the test sets (AA). The best result

is obtained in GG setting with 68.68% and 63.60%

LAS, for MaltParser (Nivre et al., 2007b) and MST-

Parser (McDonald et al., 2005) respectively. Using

AA and GA settings show worse results than GG,

namely 2.29% and 3.66% drop in accuracy for Malt-

Parser, and 1.8% and 3.23% drop for MSTParser.

They only explore the effect of gold and non-gold

POS tags with a small treebank with about 1,300

sentences. We apply GG and AA settings in our ex-

periments on a larger treebank that contains richer

morphological information. We define pool of 10

morphological and lexical semantic features in or-

der to create the best feature set for the parser.

3 Features of Persian

In this section, among possible morphological and

semantic features that exist in Persian, we briefly re-

view a subset of them that is either annotated in Per-

sian dependency treebank (Rasooli et al., 2013) or is

available from other studies.

3.1 Features from Treebank

Table 1 represents the features available in the Per-

sian dependency treebank, along with possible val-

ues for each feature.

1Uppsala Persian Dependency Treebank

98



Feature Values

Attachment {NXT, PRV, ISO}

Animacy {animate, inanimate}

Number {singular, plural}

Person {1, 2, 3}

Comparison {positive, comparative, superlative}

TMA see Table 2

Table 1: Description of features in Treebank

In some special cases, we have to break a word

into smaller parts in order to capture the syntac-

tic relations between the elements of the sentence.

For example, the two-word sentence XQ» Õç'
 @Y ‘se-
dAyam kard’ (called me), consist of three mor-

phemes: @Y (calling), Õç'
 (me), and XQ» (to do)
that have NXT (attached to the next word), PRV

(attached to the previous word), and ISO (isolated

word) attachment, respectively.

Person and number play a role in constraining

syntactic structure. Verbs usually agree with sub-

ject in person and number (Shamsfard, 2011). This

agreement is useful feature to detect subject of sen-

tence. for example in “Y	J�J 	̄P Aë ém��'. ,Qå��” (hey boy,
the kids are gone) sentence, both boy and kids are

noun, but only kids has number agreement with verb.

Tense, mood, and aspect are not separately anno-

tated in the treebank, but they can be induced from

the TMA value. Table 2 shows the conversion ta-

ble which consists of 14 valid TMA values. There is

not a unique mapping from TMA to aspect, because

in some conditions there is interference between the

aspects. For example, in indicative imperfective per-

fect, the verb has perfect or continuous aspects.

3.2 Automatic Semantic Features

Word Clusters [WC] We use all the words of the

treebank as inputs to the modified version of Brown

clustering algorithm (Liang, 2005). In order to tune

the parameters for the two parsers, we tweak the

cluster count from 50 to 300 with steps of 50, and bit

strings from 4 to 14. Finally, we choose 300 clusters

and 6–bit strings for MaltParser and 150 clusters and

10–bit strings for MSTParser2.

2https://github.com/mojtaba-khallash/

word-clustering

TMA Meaning Mood Tense

HA Imperative Imp. Pres.

AY Indicative Future Ind. Fut.

GNES Indicative Imperfective Perfect Ind. Past

GBES Indicative Imperfective Pluperfect Ind. Past

GES Indicative Imperfective Preterit Ind. Past

GN Indicative Perfect Ind. Past

GB Indicative Pluperfect Ind. Past

H Indicative Present Ind. Pres.

GS Indicative Preterit Ind. Past

GBESE Subjunctive Imperfective Pluperfect Sub. Past

GESEL Subjunctive Imperfective Preterit Sub. Past

GBEL Subjunctive Pluperfect Sub. Past

HEL Subjunctive Present Sub. Pres.

GEL Subjunctive Preterit Sub. Past

Table 2: Tense/Mood/Aspect types in Persian verbs.

Imp., Ind., Sub., Fut., and Pres. stand for imperative, in-

dicative, subjunctive, future and present, respectively.

Semantic Verb Clustering [VC]: Semantic verb

cluster is a generalization over verbs according to

their semantic properties that capture large amounts

of verb meaning without defining details for each

verb. Aminian et al. (2013) clustered 1082 Persian

verbs into 43 (fine-grained) semantic classes using

spectral clustering. For each verb in the treebank,

we included the corresponding cluster ID if the verb

exists in the list of clustered verbs3.

Synset Identifier [SID]: FarsNet (Shamsfard et

al., 2010) is a lexical ontology for the Persian lan-

guage that contains approximately 10000 synsets.

For each word in the treebank, we look up for pos-

sible synsets in FarsNet. If any synset is found, we

add the ID of the first synset to our feature set. About

59% of words in the treebank were supplied with a

synset.

Semantic File [SF]: In English WordNet, each

synset belongs to a unique semantic file. There is

a total of 45 semantic files (1 for adverbs, 3 for

adjectives, 15 for verbs, and 26 for nouns), based

on syntactic and semantic categories (Agirre et al.,

2011). FarsNet has a mapping to those of WordNet

synsets. We use both synsetID and semantic files

as instances of fine-grained and coarse-grained se-

mantic representations, respectively. Thus, we can

3https://github.com/mojtaba-khallash/

verb-spectral-cluster

99



learn what level of granularity in semantic features

can help improve performance of the parser4.

4 Experiments

Corpus Persian dependency treebank version

1.0 (Rasooli et al., 2013) is a freely-available re-

source5 with about 30,000 sentences, and half a mil-

lion tokens, annotated with syntactic roles in addi-

tion to morpho-syntactic features. The annotation

employs 17 coarse-grained and 30 fine-grained POS

tags, 22 morphological feature values and 43 depen-

dency labels. 21.93% of the sentences and 2.47% of

the edges are non-projective.

Table 3 provides statistical properties of Persian

dependency treebank, compared to UPDT6. In Per-

sian dependency treebank, syntactic and/or morpho-

logical features are represented as key-value pairs

separated by vertical bars (‘|’), while in UPDT, they
are represented as a single atomic feature.

Treebank Persian DT UPDT

Tok 498081 151671

Sen 29982 6000

AvgSL 16.61 25.28

Lem yes no

CPoS 17 15

PoS 30 30

MSF 22 30

Dep 43 48

NPT 2.47% 0.17%

NPS 21.93% 2.73%

Table 3: Comparison of UPDT (Seraji et al., 2012a)

and Persian dependency treebank (Rasooli et al., 2013).

Tok = number of tokens; Sen = number of sentences;

AvgSL = Average sentence length; Lem = lemmatiza-

tion present; CPoS = number of coarse-grained part-

of-speech tags; PoS = number of (fine-grained) part-of-

speech tags; MSF = number of morphosyntactic features

(split into atoms); Dep = number of dependency types;

NPT = proportion of non-projective dependencies/tokens

(%); NPS = proportion of non-projective dependency

graphs/sentences (%)

The data is split into standard train, development

4https://github.com/mojtaba-khallash/

semantic-tagger
5http://www.dadegan.ir/en
6Freely available at http://stp.lingfil.uu.se/

˜mojgan/UPDT.html

and test sets by the ratio of 80-10-10 percent in the

CoNLL dependency format. Furthermore, the tree-

bank is released in two representations with little

changes in their annotations. A sample comparison

between the two annotations is shown in Figure 1.

In the first representation, which is manually anno-

tated, the accusative case marker @P /rA/ is supposed
to be the head of the object plus rA. In the second

representation, which is an automatic conversion of

the first one obtained by reverse ordering the man-

ual annotation, rA is not the head of the object word.

Instead, rA is regarded as the accusative case marker

for the direct object.

. ÐY	K @ñ 	k ú �æ 	®Ã é» @P úG. A
�J» root

. read said that acc. the book
PUNC V V SUBR POSTP N

PUNC

ROOT

OBJ

PREDEP

NCL

POSDEP

(a) First representation: Manually annotating accusative case

marker @P as object of the sentence

. ÐY	K @ñ 	k ú �æ 	®Ã é» @P úG. A
�J» root

. read said that acc. the book
PUNC V V SUBR POSTP N

PUNC

ROOT

OBJ

ACC-CASE

NCL

POSDEP

(b) Second representation: Automatic conversion of first rep-

resentation. The accusative case marker @P depends on original
object of the sentence.

Figure 1: Two representation of object-verb relation for

“I read the book that you mentioned.” (Rasooli et al.,

2013).

Evaluation metric The most commonly used

metrics for dependency parsing are unlabeled attach-

ment score (UAS), labeled attachment score (LAS)

and label accuracy (LA). UAS is the proportion of

words that are assigned the correct head, LAS is

the proportion of words that are assigned the correct

head and dependency type, and LA is the proportion

of words that are assigned the correct dependency

100



type. We use LAS as our evaluation metric and

take punctuation into account as for evaluating out

parsing results. We use McNemars statistical signif-

icance test as implemented by (Nilsson and Nivre,

2008), and denote p < 0.05 and p < 0.01 with +

and ++, respectively.

Parsers We use two off-the-shelf data-driven

parsers, namely MaltParser (Nivre et al., 2007b)

and MSTParser (McDonald et al., 2005), which are

the two state-of-the-art dependency parsers that rep-

resent dominant approaches in data-driven depen-

dency parsing.

MaltParser7 is based on a transition-based ap-

proach to dependency parsing. Transition-based ap-

proach is based on transition systems for deriving

dependency trees, that greedily searches for highest

scoring transitions and uses features extracted from

parse history to predict the next transition (Choi and

Palmer, 2011a). We use MaltParser 1.7.1 along with

nine different parsing algorithms. In order to se-

lect the best algorithm and tune the parameters of

MaltParser, we use MaltOptimizer (Ballesteros and

Nivre, 2012) on the whole of training data. Mal-

tOptimizer analyzes data in three-phase optimiza-

tion process: data analysis, parsing algorithm selec-

tion, and feature selection.

MSTParser8 is based on a graph-based approach

to dependency parsing. The algorithm searches

globally in a complete graph to extract a spanning

tree during derivations using dynamic programming.

We use MSTParser 0.5 which has two implementa-

tions of maximum spanning tree (MST) algorithm

with projective and non-projective models9.

Baseline Experiments We run three phases of

MaltOptimizer on the training set in order to find

the best parsing algorithm in MaltParser. The first

phase validates the data and gains 84.02% LAS with

the default settings. In the second phase, using

non-projective version of the Covington algorithm,

which has the best accuracy, and after parameter tun-

7http://www.maltparser.org/
8http://www.seas.upenn.edu/˜strctlrn/

MSTParser/MSTParser.html
9We developed an all-in-one dependency parsing tool-

box that integrates different dependency parsing algo-

rithms: https://github.com/mojtaba-khallash/

dependency-parsing-toolbox

ing, 85.86% LAS was obtained. In the third phase,

the feature model was optimized and by tuning the

regularization parameter of the multiclass SVM; it

led to 87.43% LAS. Finally, we trained the best

algorithm with optimized settings on training set

and parsed on development set, thereby we reached

87.70% LAS as the baseline of MaltParser.

We tested four parsing algorithms that exist in

MSTParser and as a result, non-projective algorithm

with a second-order feature decoder gave 88.04%

LAS, which shows the highest improvement. There-

fore, we selected that as our baseline for MSTParser.

The baselines are obtained on the first represen-

tation of the treebank. We found baselines for the

second representation of the treebank on the devel-

opment set. Results are compared in Table 4.

The first representation performs better than the

second one. This was expected before, since rA is a

constant word that is annotated as the object of a sen-

tence in the first representation. This helps parsers to

find the object in a sentence. Moreover, as shown in

Figure 1, rA is closer to the verb than the direct ob-

ject, hence it has more chance to select.

Representation Malt MST

First 87.70 88.04

Second 87.22 (-0.48) 87.03 (-1.01)

Table 4: Comparison of two representations of Persian

treebank

Results In our experiments, we use the first repre-

sentation of treebank with algorithms and new con-

figurations presented in previous paragraph. For all

experiments in this section, we use training and de-

velopment sets of the treebank. In order to study

the effects of morphology in dependency parsing of

Persian, we organize experiments into three types

of challenges which are presented by Tsarfaty et al.

(2010): architecture and setup, representation and

modeling, and estimation and smoothing.

Architecture and Setup When using dependency

parsing on real-world tasks, we usually face with

sentences that must be tokenized, lemmatized, and

tagged with part of speech and morphological infor-

mation to offer those information as input features

to the parsing algorithms. Bijankhan corpus (Bi-

jankhan, 2004) is the first manually tagged Persian

101



corpus that consists of morpho-syntactic and mini-

mal semantic annotation of words. It is commonly

used to train POS tagger, but its POS tagset is differ-

ent from tagset of the treebank that we use. Sarabi et

al. (2013) introduce PLP Toolkit which is a compre-

hensive Persian Language Processing (PLP) toolkit

that contains fundamental NLP tools such as to-

kenizer, POS tagger, lemmatizer and dependency

parser. They merged the POS tagset of 10 million

words from bijankhan corpus with Persian depen-

dency treebank in order to create a bigger corpus

with the same tagset. They choose the tagset of Per-

sian dependency treebank as the base setting and

convert Bijankhan tagset to them. They have 11

coarse-grained and 45 fine-grained POS tags. PLP

POS tagger can automatically recognize three mor-

phological features, namely number, person, and

TMA. TMA values of the PLP tool are not the same

as Persian dependency treebank. Despite 14 possi-

ble TMA values in dependency treebank (Table 2),

only four out of the 14 values exist in PLP (AY, GS,

H, and HA), because there is no other value in Bi-

jankhan tagset for verbs. The accuracy of PLP POS

tagger on the fine grained tagset is about 98.5%. We

use this tagger and apply it on our training, develop-

ment, and test data. Results from these experiments

are presented in Table 5.

POS tags type Malt MST

Gold 87.70 88.04

Predicted 86.98 (-0.72) 86.81 (-1.23)

Table 5: Effect of gold vs. predicted POS tags and mor-

phological information in dependency parsers for Per-

sian.

Representation and Modeling In our experi-

ment, we use ten features of morphological and se-

mantic information. Using a forward selection pro-

cedure, the best feature set for each parser can be

found. Beside morphological features which exist

in the treebank (Attachment [A], Person [P], Num-

ber [N], TMA), we add Tense [T] and Mood [M]

with a simple conversion table, shown in Table 2,

based on the value of TMA.

Table 6 shows the effect of each feature for Malt-

Parser and MSTParser parser. For the former, mood

with slight differences achieves the best result and

Feature Malt Feature MST

Baseline 87.70 Baseline 88.04

M 87.77 TMA 88.21+

TMA 87.77 M 88.17

T 87.73 P 88.09

SF 87.70 T 88.04

WC 87.69 N 88.04

VC 87.68 SID 88.03

SID 87.67 SF 88.03

A 87.67 WC 88.02

P 87.66 VC 87.98

N 87.65 A 87.93

Table 6: Effect of each feature on two parsers

for the latter, TMA has the highest accuracy than

other features. TMA and two derivate features,

namely T and M, stands at the top of this ranking,

and four semantic features are placed in the middle.

This means that our newly added features can help

to improve performance of each parser.

In the next steps, we incrementally add one fea-

ture to the best result from previous step. As shown

in Table 7, combination of M and SF obtains the

best result for MaltParser (87.81%), while for MST-

Parser, combination of TMA and WC is the best

(88.25%). In the second step, adding one seman-

tic feature gets the best result. By trying to continue

this approach, we do not see any improvement in the

accuracy for both parser10.

Feature Malt Feature MST

{M,SF} 87.81 {TMA,WC} 88.25
{M,T} 87.79 {TMA,SID} 88.21
{M,VC} 87.78 {TMA,N} 88.16
{M,TMA} 87.77 {TMA,P} 88.14
{M,N} 87.76 {TMA,M} 88.13
{M,WC} 87.75 {TMA,A} 88.11
{M,A} 87.75 {TMA,T} 88.11
{M,P} 87.73 {TMA,VC} 88.07
{M,SID} 87.69 {TMA,SF} 88.05

Table 7: Combinations of two features

10https://github.com/mojtaba-khallash/

treebank-transform

102



Estimation and Smoothing Using a few training

data, especially for languages with rich morphol-

ogy, lexical features may infrequently appear during

training. In MRLs like Persian, due to many feature

combination by the inflectional system, we face a

high rate of out-of-vocabulary. There are some ways

to cope with this problem:

• Replacing word forms by lemma: Lemma of
a word has less data sparsity than word form.

• Number Normalization This is the default ap-
proach in MSTParser, in which each number is

replaced by a constant. We apply this approach

for numbers written either in English or Persian

scripts.

• Word Clustering and Semantic File: The
cluster ID of a word or its semantic file can be

used instead of the original word form. These

are two ways to categorize words into a group

bigger than their lemma.

Table 8 illustrates the effect of each smoothing

method on the accuracy for parsing MaltParser and

MSTParser. For MaltParser, number normalization

is the only technique that improves the accuracy.

For MSTParser, replacing word forms by lemma and

number normalization improves the accuracy. In the

case of MSTParser, we apply each method sepa-

rately and simultaneously on the development set,

but replacing word forms by lemma gets the best im-

provement, and hence we use it in our final configu-

ration.

Smoothing Malt MST

Baseline 87.70 88.04

Replacing word forms by lemma 87.38 88.10

Number Normalization 87.71 88.09

Word Clustering 86.98 87.47

Semantic File 87.31 85.25

Table 8: Accuracy obtained after applying different

sparsity-reduction tricks.

5 Error Analysis

We use the best configurations from the previous

section on the training and test data, for gold an-

notation and an automatically derived one. Table 9

shows the final test results of the two parsers for Per-

sian. In addition to LAS, we also include UAS and

LA to facilitate comparisons in the future. Baseline

results are included in the table. In the case of Malt-

Parser, after applying new configurations on data,

we repeat the third phase of MaltOptimizer in order

to find the best feature template for the new training

data. It seems that the graph-based parser performs

better than transitions-based parsers in general. De-

spite a high overall parsing accuracy, only 1017 and

922 (33.91% and 30.74%) of sentences in the test

set (with 2999 sentences) are parsed without errors

by MaltParser and MSTParser, respectively. Malt-

Parser has lower overall accuracy compared to MST-

Parser, but the number of completely correct parsed

sentences for MaltParser is more than MSTParser.

In the case of predicted setting, as mentioned in sec-

tion 4, there are four values for TMA. This means

that we cannot create tense and mood from TMA.

For this reason, we force to use TMA in the final

configuration of both parsers in the predicted setting.

In order to evaluate parsing errors, we use the

same approach as (McDonald and Nivre, 2011) to

shows a set of linguistic and structural properties of

the baseline and our best setting for each parser11.

Length Factors Figure 2 shows the accuracy rel-

ative to the sentence length in test data. Since there

are very limited long sentences in our treebank,

the parser cannot predict longer sentences correctly.

Consequently, the two parsers tend to have lower ac-

curacies for longer sentences. Both parsers have the

same performance, but MSTParser tends to perform

better on shorter sentences, that is in contrast with

results showed by McDonald and Nivre (2011). We

compare each parser with its corresponding base-

lines. Both parsers in all lengths perform better than

their baselines. For MaltParser, improvements occur

for longer sentences while for MSTParser improve-

ments occur at smaller sentences. These results are

in contrast with the results reported by McDonald

and Nivre (2011).

Graph Factors Figure 3 shows the accuracy for

arcs relative to their distance to the artificial root

node12. The area under the curve of final MaltParser

11In our analysis, we use MaltEval (Nilsson and Nivre, 2008).
12Number of arcs in the reverse path from the modifier of the

arc to the root.

103



Parser Method LAS UAS LA

Malt

Baseline 87.68 (87.04) 90.41 (89.92) 90.03 (89.49)

Final 87.91++ (87.16)+ 90.58+ (90.05)++ 90.22+ (89.60)+

Diff. +0.23 (+0.12) +0.17 (+0.13) +0.19 (+0.11)

MST

Baseline 87.98 (86.82) 91.30 (90.27) 90.53 (89.90)

Final 88.37++ (86.97) 91.55++ (90.36) 90.86++ (90.05)

Diff. +0.39 (+0.15) +0.25 (+0.09) +0.33 (+0.15)

Table 9: Baseline and final results of gold (predicted) test data for MaltParser

1–10 11–20 21–30 31–40 41–50 >50
75

80

85

90

95

Sentence Length

D
ep

en
d
en

cy
A

cc
u
ra

cy

Final

Baseline

(a) Accuracy of MaltParser per sentence length

1–10 11–20 21–30 31–40 41–50 >50
75

80

85

90

95

Sentence Length

D
ep

en
d
en

cy
A

cc
u
ra

cy

Final

Baseline

(b) Accuracy of MSTParser per sentence length

Figure 2: Accuracy relative to sentence length. Both

parsers perform better than their baselines.

is less than baseline, but it is over baseline for MST-

Parser. F-score of MSTParser for shorter distance is

much better than the baseline and by increasing the

distance to root, F-score degrades to be less than the

baseline.

Linguistic Factors MaltParser and MSTParser

can find 90.22% and 90.86% of all labels correctly.

Figure 4 shows the F-score of some important de-

pendency labels in the test data. MaltParser only

improves subject and object categories, while MST-

Parser improves object, ROOT, and adverb cate-

1 2 3 4 5 6 >6
85

86

87

88

89

Distance to Root
F

-S
co

re

(a) Baseline ( ) and final ( ) accuracy of MaltParser

1 2 3 4 5 6 >6
86

87

88

89

Distance to Root

F
-S

co
re

(b) Baseline ( ) and final ( ) accuracy of MSTParser

Figure 3: Dependency arc F-score relative to the distance

to root

gories. If we only consider the final results, Malt-

Parser performs better for predicting subject and ob-

ject, while MSTParser performs better for predicting

ROOT and ezafe dependent (MOZ)13, and both have

the same accuracy for adverb.

Table 10 gives the accuracy of arcs for each de-

pendent part-of-speech. Final MSTParser performs

13Ezafe construction is referred to nouns or pronouns that im-

ply a possessed-possessor relation (like first name-last name).

The relation between the possessed and possessor is called

mozaf (MOZ) that its sign is a vowel /e/ that pronounced right

after the head noun (Dadegan Research Group, 2012).

104



SBJ OBJ ROOT MOZ ADV
0

20

40

60

80

100

Dependency Type

F
-S

co
re

Baseline

F inal

(a) Accuracy of MaltParser per dependency type

SBJ OBJ ROOT MOZ ADV
0

20

40

60

80

100

Dependency Type

F
-S

co
re

Baseline

F inal

(b) Accuracy of MSTParser per dependency type

Figure 4: Dependency label F-score relative to some de-

pendency types.

better than its baseline for all categories, except pro-

nouns and better than MaltParser for all categories,

except preposition. Final MaltParser, performs bet-

ter than its baseline in all categories, except preposi-

tion.

6 Conclusion

In this paper, we have investigated a number of is-

sues in data-driven dependency parsing of Persian.

Because there is no previous study on parsing the

POS
Malt MST

Baseline Final Baseline Final

Verb 89.96 90.09 90.96 91.86

Noun 89.67 90.13 90.15 90.23

Pronoun 92.56 92.94 93.53 93.43

Adjective 87.80 88.37 87.77 88.56

Adverb 80.80 82.37 82.61 83.94

Conjunction 86.03 86.40 86.58 87.36

Preposition 70.93 70.32 69.74 70.76

Table 10: Accuracy for each dependent part of speech

Persian dependency treebank (Rasooli et al., 2013),

we first have drawn the baseline for each parser, by

selecting best performing algorithm and tuning its

parameters. For MaltParser (Nivre et al., 2007b) dif-

ferent between best algorithm (non-projective ver-

sion of Covington) with default settings and after op-

timizing feature template by the third phase of Mal-

tOptimizer (Ballesteros and Nivre, 2012) is about

1.5 percent. This shows that the definition of fea-

ture template is a crucial aspect of transition-based

parsing.

Our first experiment shows the effect of using au-

tomatic annotation of POS tags and morphological

information. Our new configuration improves two

parsers in both gold and predicted setting, but the

improvement for MSTParser is higher than for Malt-

Parser. MSTParser has higher accuracy in the gold

setting, while MaltParser has better performance in

predicted setting. It might mean that MaltParser is

more robust against noisy information.

In the second experiment, we have explored the

best combination of morphological and lexical se-

mantic features for dependency parsing of Persian.

We find that the combination of one morphological

feature and one lexical semantic feature gets the best

combination for each parser. Our lexical semantic

features can be automatically produced for any word

and thus we need to predict one morphological fea-

ture for real-world settings.

Finally we have proposed two simple methods for

reducing data sparsity of each parser. After apply-

ing our solutions to three types of challenges, we

reached 87.91% and 88.37% LAS on the test set

(0.23% and 0.39% improvement over our baseline)

for MaltParser and MSTParser, respectively.

Note that all of the experiments we reported in

this paper use existing parsers as black boxes. We

only changed the input data to obtain the best pos-

sible performance given our data sets. We plan to

explore modifications of the underlying parsing al-

gorithms to better make use of morphological infor-

mation.

Acknowledgments

We would like to thank Mohammad-Sadegh Rasooli

and our anonymous reviewers for helpful feedback

and suggestions. We would also thank Zahra Sarabi

105



for providing us the data and information about the

PLP toolkit.

References

Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and

Joakim Nivre. 2011. Improving Dependency Pars-

ing with Semantic Classes. In Proceedings of the 49th

Annual Meeting of the Association for Computational

Linguistics (ACL ’11): shortpapers, pages 699–703.

Baharat Ram Ambati, Samar Husain, Sambhav Jain,

Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two

methods to incorporate local morphosyntactic fea-

tures in Hindi dependency parsing. In Proceedings

of NAACL HLT 2010 First workshop on Statistical

Parsing of Morphologically-Rich Languages (SPMRL

2010), pages 22–30.

Baharat Ram Ambati, Samar Husain, Joakim Nivre, and

Rajeev Sangal. 2010b. On the Role of Morphosyntac-

tic Features in Hindi Dependency Parsing. In Proceed-

ings of the NAACL HLT 2010 First Workshop on Sta-

tistical Parsing of Morphologically-Rich Languages,

pages 94–102.

Maryam Aminian, Mohammad Sadegh Rasooli, and Hos-

sein Sameti. 2013. Unsupervised Induction of Persian

Semantic Verb Classes Based on Syntactic Informa-

tion. In Language Processing and Intelligent Informa-

tion Systems, pages 112–124.

Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-

timizer: A System for MaltParser Optimization. In

Proceedings of the Eighth International Conference

on Language Resources and Evaluation (LREC 2012),

pages 23–27.

Miguel Ballesteros and Joakim Nivre. 2013. Going to

the Roots of Dependency Parsing. Computational Lin-

guistics, pages 5–13.

Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-

tion of Different Techniques to Dependency Parsing of

Basque. In Proceedings of the NAACL HLT 2010 First

Workshop on Statistical Parsing of Morphologically-

Rich Languages, pages 31–39.

Mahmood Bijankhan. 2004. The role of the corpus in

writing a grammar: An introduction to a software. Ira-

nian Journal of Linguistics.

Bernd Bohnet and Jonas Kuhn. 2012. The Best of

Both Worlds A Graph-based Completion Model for

Transition-based Parsers. In Proceedings of the 13th

Conference of the European Chapter of the Associa-

tion for Computational Linguistics, pages 77–87.

Bernd Bohnet and Joakim Nivre. 2012. A Transition-

Based System for Joint Part-of-Speech Tagging and

Labeled Non-Projective Dependency Parsing. In Pro-

ceedings of the 2012 Joint Conference on Empirical

Methods in Natural Language Processing and Compu-

tational Natural Language Learning (EMNLP-CoNLL

2012), pages 1455–1465.

Jinho D. Choi and Martha Palmer. 2011a. Getting the

Most out of Transition-based Dependency Parsing. In

Proceedings of the 49th Annual Meeting of the Associ-

ation for Computational Linguistics (ACL ’11): short-

papers, pages 687–692.

Jinho D. Choi and Martha Palmer. 2011b. Statistical De-

pendency Parsing in Korean: From Corpus Genera-

tion To Automatic Parsing. In Proceedings of the 2nd

Workshop on Statistical Parsing of Morphologically-

Rich Languages (SPMRL 2011), pages 1–11.

Dadegan Research Group. 2012. Persian Dependency

Treebank Annotation Manual and User Guide. Tech-

nical report, SCICT.

Yoav Goldberg and Michael Elhadad. 2010. Easy First

Dependency Parsing of Modern Hebrew. In Proceed-

ings of the NAACL HLT 2010 First Workshop on Sta-

tistical Parsing of Morphologically-Rich Languages,

pages 103–107.

Terry Koo, Xavier Carreras, and Michael Collins. 2008.

Simple Semi-supervised Dependency Parsing. In Pro-

ceedings of ACL-08: HLT, pages 595–603.

Percy Liang. 2005. Semi-Supervised Learning for Natu-

ral Language. Ph.D. thesis, Massachusetts Institute of

Technology.

Yuval Marton, Nizar Habash, and Owen Rambow. 2011.

Improving Arabic Dependency Parsing with Form-

based and Functional Morphological Features. In Pro-

ceedings of the 49th Annual Meeting of the Association

for Computational Linguistics (ACL ’11), pages 1586–

1596.

Ryan McDonald and Joakim Nivre. 2011. Analyzing

and Integrating Dependency Parsers. Computational

Linguistics, pages 197–230.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and

Jan Hajič. 2005. Non-projective Dependency Parsing

using Spanning Tree Algorithms. In Proceedings of

the conference on Human Language Technology and

Empirical Methods in Natural Language Processing,

pages 523–530.

Jens Nilsson and Joakim Nivre. 2008. MaltEval: An

Evaluation and Visualization Tool for Dependency

Parsing. In Proceedings of the Sixth International

Language Resources and Evaluation (LREC ’08).

Joakim Nivre and Jens Nilsson. 2005. Pseudo-Projective

Dependency Parsing. In Proceedings of the 43rd An-

nual Meeting of the Association for Computational

Linguistics (ACL ’05), pages 99–106.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDon-

ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.

2007a. The CoNLL 2007 Shared Task on Dependency

106



Parsing. In Proceedings of the CoNLL Shared Task

Session of EMNLP-CoNLL 2007, pages 915–932.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,

Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov,

and Erwin Marsi. 2007b. MaltParser: A language-

independent system for data-driven dependency pars-

ing. Natural Language Engineering, pages 95–135.

Joakim Nivre. 2009. Non-Projective Dependency Pars-

ing in Expected Linear Time. In Proceedings of

the Joint Conference of the 47th Annual Meeting of

the ACL and the 4th International Joint Conference

on Natural Language Processing (IJCNLP) of the

AFNLP, pages 351–359.

Mohammad Sadegh Rasooli, Omid Kashefi, and Behrouz

Minaei-Bidgoli. 2011. Effect of Adaptive Spell

Checking in Persian. In 7th International Conference

on Natural Language Processing andKnowledge En-

gineering (NLP-KE), pages 161–164.

Mohammad Sadegh Rasooli, Manouchehr Kouhestani,

and Amirsaeid Moloodi. 2013. Development of a Per-

sian Syntactic Dependency Treebank. In Proceedings

of the 2013 Conference of the North American Chap-

ter of the Association for Computational Linguistics:

Human Language Technologies, pages 306–314.

Zahra Sarabi, Hooman Mahyar, and Mojgan Farhoodi.

2013. PLP Toolkit: Persian Language Processing

Toolkit. In 3rd International eConference on Com-

puter and Knowledge Engineering (ICCKE 2013).

Wolfgang Seeker and Jonas Kuhn. 2013. Morphological

and Syntactic Case in Statistical Dependency Parsing.

Computational Linguistics, pages 23–55.

Mojgan Seraji, Beáta Megyesi, and Joakim Nivre. 2012a.

Bootstrapping a Persian Dependency Treebank. Lin-

guistic Issues in Language Technology, pages 1–10.

Mojgan Seraji, Beáta Megyesi, and Joakim Nivre. 2012b.

Dependency Parsers for Persian. In Proceedings of

10th Workshop on Asian Language Resources, COL-

ING 2012, 24th International Conference on Compu-

tational Linguistics.

Mehrnoush Shamsfard, Akbar Hesabi, Hakimeh Fadaei,

Niloofar Mansoory, Ali Famian, Somayeh Bagher-

beigi, Elham Fekri, Maliheh Monshizadeh, and

S. Mostafa Assi. 2010. Semi Automatic Development

Of FarsNet: The Persian Wordnet. In Proceedings of

5th Global WordNet Conference (GWA2010).

Mehrnoush Shamsfard. 2011. Challenges and Open

Problems in Persian Text processing. In The 5th Lan-

guage and Technology Conference (LTC 2011), pages

65–69.

Reut Tsarfaty, Djamé Seddah, Yoav Goldberg, Sandra

Kübler, Marie Candito, Jennifer Foster, Yannick Ver-

sley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-

tistical Parsing of Morphologically Rich Languages

(SPMRL) What, How and Whither. In Proceedings

of the NAACL HLT 2010 First Workshop on Statistical

Parsing of Morphologically-Rich Languages, pages 1–

12.

107


