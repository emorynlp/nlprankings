










































A Comparative Investigation of Morphological Language Modeling for the Languages of the European Union


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 386–395,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

A Comparative Investigation of Morphological Language Modeling for the
Languages of the European Union

Thomas Müller, Hinrich Schütze and Helmut Schmid
Institute for Natural Language Processing

University of Stuttgart, Germany
{muellets,schmid}@ims.uni-stuttgart.de

Abstract

We investigate a language model that com-
bines morphological and shape features with
a Kneser-Ney model and test it in a large
crosslingual study of European languages.
Even though the model is generic and we use
the same architecture and features for all lan-
guages, the model achieves reductions in per-
plexity for all 21 languages represented in the
Europarl corpus, ranging from 3% to 11%. We
show that almost all of this perplexity reduc-
tion can be achieved by identifying suffixes by
frequency.

1 Introduction

Language models are fundamental to many natural
language processing applications. In the most com-
mon approach, language models estimate the proba-
bility of the next word based on one or more equiv-
alence classes that the history of preceding words is
a member of. The inherent productivity of natural
language poses a problem in this regard because the
history may be rare or unseen or have unusual prop-
erties that make assignment to a predictive equiva-
lence class difficult.

In many languages, morphology is a key source
of productivity that gives rise to rare and unseen
histories. For example, even if a model can learn
that words like “large”, “dangerous” and “serious”
are likely to occur after the relatively frequent his-
tory “potentially”, this knowledge cannot be trans-
ferred to the rare history “hypothetically” without
some generalization mechanism like morphological
analysis.

Our primary goal in this paper is not to de-
velop optimized language models for individual lan-

guages. Instead, we investigate whether a simple
generic language model that uses shape and mor-
phological features can be made to work well across
a large number of languages. We find that this is
the case: we achieve considerable perplexity reduc-
tions for all 21 languages in the Europarl corpus.
We see this as evidence that morphological language
modeling should be considered as a standard part of
any language model, even for languages like English
that are often not viewed as a good application of
morphological modeling due to their morphological
simplicity.

To understand which factors are important for
good performance of the morphological compo-
nent of a language model, we perform an exten-
sive crosslingual analysis of our experimental re-
sults. We look at three parameters of the morpho-
logical model we propose: the frequency threshold
θ that divides words subject to morphological clus-
tering from those that are not; the number of suffixes
used φ; and three different morphological segmen-
tation algorithms. We also investigate the differen-
tial effect of morphological language modeling on
different word shapes: alphabetical words, punctua-
tion, numbers and other shapes.

Some prior work has used morphological models
that require careful linguistic analysis and language-
dependent adaptation. In this paper we show that
simple frequency analysis performs only slightly
worse than more sophisticated morphological anal-
ysis. This potentially removes a hurdle to using
morphological models in cases where sufficient re-
sources to do the extra work required for sophisti-
cated morphological analysis are not available.

The motivation for using morphology in lan-
guage modeling is similar to distributional clustering

386



(Brown et al., 1992). In both cases, we form equiv-
alence classes of words with similar distributional
behavior. In a preliminary experiment, we find that
morphological equivalence classes reduce perplex-
ity as much as traditional distributional classes – a
surprising result we intend to investigate in future
work.

The main contributions of this paper are as fol-
lows. We present a language model design and a
set of morphological and shape features that achieve
reductions in perplexity for all 21 languages rep-
resented in the Europarl corpus, ranging from 3%
to 11%, compared to a Kneser-Ney model. We
show that identifying suffixes by frequency is suf-
ficient for getting almost all of this perplexity reduc-
tion. More sophisticated morphological segmenta-
tion methods do not further increase perplexity or
just slightly. Finally, we show that there is one pa-
rameter that must be tuned for good performance for
most languages: the frequency threshold θ above
which a word is not subject to morphological gen-
eralization because it occurs frequently enough for
standard word n-gram language models to use it ef-
fectively for prediction.

The paper is organized as follows. In Section 2
we discuss related work. In Section 3 we describe
the morphological and shape features we use. Sec-
tion 4 introduces language model and experimental
setup. Section 5 discusses our results. Section 6
summarizes the contributions of this paper.

2 Related Work

Whittaker and Woodland (2000) apply language
modeling to morpheme sequences and investigate
data-driven segmentation methods. Creutz et al.
(2007) propose a similar method that improves
speech recognition for highly inflecting languages.
They use Morfessor (Creutz and Lagus, 2007) to
split words into morphemes. Both approaches are
essentially a simple form of a factored language
model (FLM) (Bilmes and Kirchhoff, 2003). In a
general FLM a number of different back-off paths
are combined by a back-off function to improve the
prediction after rare or unseen histories. Vergyri et
al. (2004) apply FLMs and morphological features
to Arabic speech recognition.

These papers and other prior work on using mor-

phology in language modeling have been language-
specific and have paid less attention to the ques-
tion as to how morphology can be useful across
languages and what generic methods are appropri-
ate for this goal. Previous work also has concen-
trated on traditional linguistic morphology whereas
we compare linguistically motivated morphologi-
cal segmentation with frequency-based segmenta-
tion and include shape features in our study.

Our initial plan for this paper was to use com-
plex language modeling frameworks that allow ex-
perimenters to include arbitrary features (including
morphological and shape features) in the model. In
particular, we looked at publicly available imple-
mentations of maximum entropy models (Rosen-
feld, 1996; Berger et al., 1996) and random forests
(Xu and Jelinek, 2004). However, we found that
these methods do not currently scale to running a
large set of experiments on a multi-gigabyte parallel
corpus of 21 languages. Similar considerations ap-
ply to other sophisticated language modeling tech-
niques like Pitman-Yor processes (Teh, 2006), re-
current neural networks (Mikolov et al., 2010) and
FLMs in their general, more powerful form. In ad-
dition, perplexity reductions of these complex mod-
els compared to simpler state-of-the-art models are
generally not large.

We therefore decided to conduct our study in the
framework of smoothed n-gram models, which cur-
rently are an order of magnitude faster and more
scalable. More specifically, we adopt a class-based
approach, where words are clustered based on mor-
phological and shape features. This approach has the
nice property that the number of features used to es-
timate the classes does not influence the time needed
to train the class language model, once the classes
have been found. This is an important consideration
in the context of the questions asked in this paper as
it allows us to use large numbers of features in our
experiments.

3 Modeling of morphology and shape

Our basic approach is to define a number of morpho-
logical and shape features and then assign all words
with identical feature values to one class. For the
morphological features, we investigate three differ-
ent automatic suffix identification algorithms: Re-

387



s, e, d, ed, n, g, ng, ing, y, t, es, r, a, l, on, er, ion,
ted, ly, tion, rs, al, o, ts, ns, le, i, ation, an, ers, m, nt,
ting, h, c, te, sed, ated, en, ty, ic, k, ent, st, ss, ons, se,
ity, ble, ne, ce, ess, ions, us, ry, re, ies, ve, p, ate, in,
tions, ia, red, able, is, ive, ness, lly, ring, ment, led,
ned, tes, as, ls, ding, ling, sing, ds, ded, ian, nce, ar,
ating, sm, ally, nts, de, nd, ism, or, ge, ist, ses, ning,
u, king, na, el

Figure 1: The 100 most frequent English suffixes in Eu-
roparl, ordered by frequency

ports (Keshava and Pitler, 2006), Morfessor (Creutz
and Lagus, 2007) and Frequency, where Frequency
simply selects the most frequent word-final letter se-
quences as suffixes. The 100 most frequent suffixes
found by Frequency for English are given in Fig-
ure 1.

We use the φ most frequent suffixes for all three
algorithms, where φ is a parameter. The focus of our
work is to evaluate the utility of these algorithms for
language modeling; we do not directly evaluate the
quality of the suffixes.

A word is segmented by identifying the longest of
the φ suffixes that it ends with. Thus, each word has
one suffix feature if it ends with one of the φ suffixes
and none otherwise.

In addition to suffix features, we define features
that capture shape properties: capitalization, special
characters and word length. If a word in the test set
has a combination of feature values that does not oc-
cur in the training set, then it is assigned to the class
whose features are most similar. We described the
similarity measure and details of the shape features
in prior work (Müller and Schütze, 2011). The shape
features are listed in Table 1.

4 Experimental Setup

Experiments are performed using srilm (Stolcke,
2002), in particular the Kneser-Ney (KN) and
generic class model implementations. Estimation of
optimal interpolation parameters is based on (Bahl
et al., 1991).

4.1 Baseline

Our baseline is a modified KN model (Chen and
Goodman, 1999).

4.2 Morphological class language model

We use a variation of the model proposed by Brown
et al. (1992) that we developed in prior work on En-
glish (Müller and Schütze, 2011). This model is a
class-based language model that groups words into
classes and replaces the word transition probability
by a class transition probability and a word emission
probability:

PC(wi|wi−1i−N+1) =
P (g(wi)|g(wi−1i−N+1)) · P (wi|g(wi))

where g(w) is the class of word w and we write
g(wi . . . wj) for g(wi) . . . g(wj).

Our approach targets rare and unseen histories.
We therefore exclude all frequent words from clus-
tering on the assumption that enough training data
is available for them. Thus, clustering of words is
restricted to those below a certain token frequency
threshold θ. As described above, we simply group
all words with identical feature values into one class.
Words with a training set frequency above θ are
added as singletons. The class transition probabil-
ity P (g(wi)|g(wi−1i−N+1)) is estimated using Witten-
Bell smoothing.1

The word emission probability is defined as fol-
lows:

P (w|c) =





1 , N(w) > θ
N(w)P
w∈cN(w)

− ²(c)|c|−1 , θ≥N(w)>0
²(c) , N(w) = 0

where c = g(w) is w’s class and N(w) is the fre-
quency of w in the training set. The class-dependent
out-of-vocabulary (OOV) rate ²(c) is estimated on
held-out data. Our final model PM interpolates PC
with a modified KN model:

PM (wi|wi−N+1i−1 ) =
λ(g(wi−1)) · PC(wi|wi−N+1i−1 )

+(1− λ(g(wi−1))) · PKN(wi|wi−N+1i−1 ) (1)

This model can be viewed as a generalization of
the simple interpolation αPC + (1− α)PW used by
Brown et al. (1992) (where PW is a word n-gram

1Witten-Bell smoothing outperformed modified Kneser-Ney
(KN) and Good-Turing (GT).

388



is capital(w) first character of w is an uppercase letter
is all capital(w) ∀ c ∈ w : c is an uppercase letter
capital character(w) ∃ c ∈ w : c is an uppercase letter
appears in lowercase(w) ¬capital character(w) ∨ w′ ∈ ΣT
special character(w) ∃ c ∈ w : c is not a letter or digit
digit(w) ∃ c ∈ w : c is a digit
is number(w) w ∈ L([+− ²][0− 9] (([., ][0− 9])|[0− 9]) ∗)

Table 1: Shape features as defined by Müller and Schütze (2011). ΣT is the vocabulary of the training corpus T , w′ is
obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regular
expression expr.

model and PC a class n-gram model). For the set-
ting θ = ∞ (clustering of all words), our model is
essentially a simple interpolation of a word n-gram
and a class n-gram model except that the interpola-
tion parameters are optimized for each class instead
of using the same interpolation parameter α for all
classes. We have found that θ =∞ is never optimal;
it is always beneficial to assign the most frequent
words to their own singleton classes.

Following Yuret and Biçici (2009), we evaluate
models on the task of predicting the next word from
a vocabulary that consists of all words that occur
more than once in the training corpus and the un-
known word UNK. Performing this evaluation for
KN is straightforward: we map all words with fre-
quency one in the training set to UNK and then com-
pute PKN(UNK |h) in testing.

In contrast, computing probability estimates for
PC is more complicated. We define the vocabulary
of the morphological model as the set of all words
found in the training corpus, including frequency-1
words, and one unknown word for each class. We
do this because – as we argued above – morpholog-
ical generalization is only expected to be useful for
rare words, so we are likely to get optimal perfor-
mance for PC if we include all words in clustering
and probability estimation, including hapax legom-
ena. Since our testing setup only evaluates on words
that occur more than once in the training set, we ide-
ally would want to compute the following estimate
when predicting the unknown word:

PC(UNKKN |h) =∑

{w:N(w)=1}
PC(w|h) +

∑
c

PC(UNKc |h) (2)

where we distinguish the unknown words of the
morphological classes from the unknown word used
in evaluation and by the KN model by giving the lat-
ter the subscript KN.

However, Eq. 2 cannot be computed efficiently
and we would not be able to compute it in practical
applications that require fast language models. For
this reason, we use the modified class model P ′C in
Eq. 1 that is defined as follows:

P ′C(w|h) =
{
PC(w|h) , N(w) ≥ 1
PC(UNKg(w) |h), N(w) = 0

P ′C and – by extension – PM are deficient. This
means that the evaluation of PM we present below
is pessimistic in the sense that the perplexity reduc-
tions would probably be higher if we were willing to
spend additional computational resources and com-
pute Eq. 2 in its full form.

4.3 Distributional class language model
The most frequently used type of class-based lan-
guage model is the distributional model introduced
by Brown et al. (1992). To understand the dif-
ferences between distributional and morphological
class language models, we compare our morpholog-
ical model PM with a distributional model PD that
has exactly the same form as PM; in particular, it
is defined by Equations (1) and (2). The only dif-
ference is that the classes are morphological for PM
and distributional for PD.

The exchange algorithm that was used by Brown
et al. (1992) has very long running times for large
corpora in standard implementations like srilm. It
is difficult to conduct the large number of cluster-
ings necessary for an extensive study like ours using
standard implementations.

389



Language T/T ² #Sentences
S bg Bulgarian .0183 .0094 181,415
S cs Czech .0185 .0097 369,881
S pl Polish .0189 .0096 358,747
S sk Slovak .0187 .0088 368,624
S sl Slovene .0156 .0090 365,455
G da Danish .0086 .0077 1,428,620
G de German .0091 .0073 1,391,324
G en English .0028 .0023 1,460,062
G nl Dutch .0061 .0048 1,457,629
G sv Swedish .0090 .0095 1,342,667
E el Greek .0081 .0079 851,636
R es Spanish .0040 .0031 1,429,276
R fr French .0029 .0024 1,460,062
R it Italian .0040 .0030 1,389,665
R pt Portuguese .0042 .0032 1,426,750
R ro Romanian .0142 .0079 178,284
U et Estonian .0329 .0198 375,698
U fi Finnish .0231 .0183 1,394,043
U hu Hungarian .0312 .0163 364,216
B lt Lithuanian .0265 .0147 365,437
B lv Latvian .0182 .0086 363,104

Table 2: Statistics for the 21 languages. S = Slavic, G
= Germanic, E = Greek, R = Romance, U = Uralic, B
= Baltic. Type/token ratio (T/T) and # sentences for the
training set and OOV rate ² for the validation set. The
two smallest and largest values in each column are bold.

We therefore induce the distributional classes
as clusters in a whole-context distributional vector
space model (Schütze and Walsh, 2011), a model
similar to the ones described by Schütze (1992)
and Turney and Pantel (2010) except that dimension
words are immediate left and right neighbors (as op-
posed to neighbors within a window or specific types
of governors or dependents). Schütze and Walsh
(2011) present experimental evidence that suggests
that the resulting classes are competitive with Brown
classes.

4.4 Corpus

Our experiments are performed on the Europarl cor-
pus (Koehn, 2005), a parallel corpus of proceed-
ings of the European Parliament in 21 languages.
The languages are members of the following fam-
ilies: Baltic languages (Latvian, Lithuanian), Ger-
manic languages (Danish, Dutch, English, Ger-

man, Swedish), Romance languages (French, Ital-
ian, Portuguese, Romanian, Spanish), Slavic lan-
guages (Bulgarian, Czech, Polish, Slovak, Slovene),
Uralic languages (Estonian, Finnish, Hungarian)
and Greek. We only use the part of the corpus that
can be aligned to English sentences. All 21 corpora
are divided into training set (80%), validation set
(10%) and test set (10%). The training set is used for
morphological and distributional clustering and esti-
mation of class and KN models. The validation set
is used to estimate the OOV rates ² and the optimal
parameters λ, θ and φ. Table 2 gives basic statistics
about the corpus. The sizes of the corpora of lan-
guages whose countries have joined the European
community more recently are smaller than for coun-
tries who have been members for several decades.

We see that English and French have the lowest
type/token ratios and OOV rates; and the Uralic lan-
guages (Estonian, Finnish, Hungarian) and Lithua-
nian the highest. The Slavic languages have higher
values than the Germanic languages, which in turn
have higher values than the Romance languages ex-
cept for Romanian. Type/token ratio and OOV
rate are one indicator of how much improvement
we would expect from a language model with
a morphological component compared to a non-
morphological language model.2

5 Results and Discussion

We performed all our experiments with an n-gram
order of 4; this was the order for which the KN
model performs best for all languages on the vali-
dation set.

5.1 Morphological model

Using grid search, we first determined on the vali-
dation set the optimal combination of three param-
eters: (i) θ ∈ {100, 200, 500, 1000, 2000, 5000},
(ii) φ ∈ {50, 100, 200, 500} and (iii) segmentation
method. Recall that we only cluster words whose
frequency is below θ and only consider the φ most

2The tokenization of the Europarl corpus has a preference
for splitting tokens in unclear cases. OOV rates would be higher
for more conservative tokenization strategies.

4A two-tailed paired t-test on the improvements by language
shows that the morphological model significantly outperforms
the distributional model with p=0.0027. A test on the Germanic,
Romance and Greek languages yields p=0.19.

390



PPKN θ∗M φ
∗ M∗ PPC PPM ∆M θ∗D PPWC PPD ∆D

S bg 74 200 50 f 103 69 0.07 500 141 71 0.04
S cs 141 500 100 f 217 129 0.08 1000 298 134 0.04
S pl 148 500 100 m 241 134 0.09 1000 349 141 0.05
S sk 123 500 200 f 186 111 0.10 1000 261 116 0.06
S sl 118 500 100 m 177 107 0.09 1000 232 111 0.06
G da 69 1000 100 r 89 65 0.05 2000 103 65 0.05
G de 100 2000 50 m 146 94 0.06 2000 150 94 0.06
G en 55 2000 50 f 73 53 0.03 5000 87 53 0.04
G nl 70 2000 50 r 100 67 0.04 5000 114 67 0.05
G sv 98 1000 50 m 132 92 0.06 2000 154 92 0.06
E el 80 1000 100 f 108 73 0.08 2000 134 74 0.07
R es 57 2000 100 m 77 54 0.05 5000 93 54 0.05
R fr 45 1000 50 f 56 43 0.04 5000 71 42 0.05
R it 69 2000 100 m 101 66 0.04 2000 100 66 0.05
R pt 62 2000 50 m 88 59 0.05 2000 87 59 0.05
R ro 76 500 100 m 121 70 0.07 1000 147 71 0.07
U et 256 500 100 m 422 230 0.10 1000 668 248 0.03
U fi 271 1000 500 f 410 240 0.11 2000 706 261 0.04
U hu 151 200 200 m 222 136 0.09 1000 360 145 0.03
B lt 175 500 200 m 278 161 0.08 1000 426 169 0.03
B lv 154 500 200 f 237 142 0.08 1000 322 147 0.05

Table 3: Perplexities on the test set for N = 4. S = Slavic, G = Germanic, E = Greek, R = Romance, U =
Uralic, B = Baltic. θ∗x, φ

∗ and M∗ denote frequency threshold, suffix count and segmentation method optimal on the
validation set. The letters f, m and r stand for the frequency-based method, Morfessor and Reports. PPKN, PPC,
PPM, PPWC, PPD are the perplexities of KN, morphological class model, interpolated morphological class model,
distributional class model and interpolated distributional class model, respectively. ∆x denotes relative improvement:
(PPKN−PPx)/PPKN. Bold numbers denote maxima and minima in the respective column.4

frequent suffixes. An experiment with the optimal
configuration was then run on the test set. The re-
sults are shown in Table 3. The KN perplexities vary
between 45 for French and 271 for Finnish.

The main result is that the morphological model
PM consistently achieves better performance than
KN (columns PPM and ∆M), in particular for
Slavic, Uralic and Baltic languages and Greek. Im-
provements range from 0.03 for English to 0.11 for
Finnish.

Column θ∗M gives the threshold that is optimal for
the validation set. Values range from 200 to 2000.
Column φ∗ gives the optimal number of suffixes. It
ranges from 50 to 500. The morphologically com-
plex language Finnish seems to benefit from more
suffixes than morphologically simple languages like
Dutch, English and German, but there are a few lan-
guages that do not fit this generalization, e.g., Esto-

nian for which 100 suffixes are optimal.
The optimal morphological segmenter is given in

column M∗: f = Frequency, r = Reports, m = Mor-
fessor. The most sophisticated segmenter, Morfes-
sor is optimal for about half of the 21 languages, but
Frequency does surprisingly well. Reports is opti-
mal for two languages, Danish and Dutch. In gen-
eral, Morfessor seems to have an advantage for com-
plex morphologies, but is beaten by Frequency for
Finnish and Latvian.

5.2 Distributional model

Columns PPD and ∆D show the performance of the
distributional class language model. As one would
perhaps expect, the morphological model is superior
to the distributional model for morphologically com-
plex languages like Estonian, Finnish and Hungar-
ian. These languages have many suffixes that have

391



∆θ+ −∆θ− θ+ θ− ∆φ+ −∆φ− φ+ φ− ∆M+ −∆M− M+ M−
S bg 0.03 200 5000 0.01 50 500 f m
S cs 0.03 500 5000 100 500 f r
S pl 0.03 500 5000 0.01 100 500 m r
S sk 0.02 500 5000 200 500 0.01 f r
S sl 0.03 500 5000 0.01 100 500 m r
G da 0.02 1000 100 100 50 r f
G de 0.02 2000 100 50 500 m f
G en 0.01 2000 100 50 500 f r
G nl 0.01 2000 100 50 500 r f
G sv 0.02 1000 100 50 500 m f
E el 0.02 1000 100 100 500 0.01 f r
R es 0.02 2000 100 100 500 m r
R fr 0.01 1000 100 50 500 f r
R it 0.01 2000 100 100 500 m r
R pt 0.02 2000 100 50 500 m r
R ro 0.03 500 5000 100 500 m r
U et 0.02 500 5000 0.01 100 50 0.01 m r
U fi 0.03 1000 100 0.03 500 50 0.02 f r
U hu 0.03 200 5000 0.01 200 50 m r
B lt 0.02 500 5000 200 50 m r
B lv 0.02 500 5000 200 500 f r

Table 4: Sensitivity of perplexity values to the parameters (on the validation set). S = Slavic, G = Germanic, E =
Greek, R = Romance, U = Uralic, B = Baltic. ∆x+ and ∆x− denote the relative improvement of PM over the KN
model when parameter x is set to the best (x+) and worst value (x−), respectively. The remaining parameters are set
to the optimal values of Table 3. Cells with differences of relative improvements that are smaller than 0.01 are left
empty.

high predictive power for the distributional contexts
in which a word can occur. A morphological model
can exploit this information even if a word with an
informative suffix did not occur in one of the lin-
guistically licensed contexts in the training set. For
a distributional model it is harder to learn this type
of generalization.

What is surprising about the comparative perfor-
mance of morphological and distributional models is
that there is no language for which the distributional
model outperforms the morphological model by a
wide margin. Perplexity reductions are lower than
or the same as those of the morphological model
in most cases, with only four exceptions – English,
French, Italian, and Dutch – where the distributional
model is better by one percentage point than the
morphological model (0.05 vs. 0.04 and 0.04 vs.
0.03).

Column θ∗D gives the frequency threshold for the

distributional model. The optimal threshold ranges
from 500 to 5000. This means that the distributional
model benefits from restricting clustering to less fre-
quent words – and behaves similarly to the morpho-
logical class model in that respect. We know of no
previous work that has conducted experiments on
frequency thresholds for distributional class models
and shown that they increase perplexity reductions.

5.3 Sensitivity analysis of parameters

Table 3 shows results for parameters that were opti-
mized on the validation set. We now want to analyze
how sensitive performance is to the three parame-
ters θ, φ and segmentation method. To this end, we
present in Table 4 the best and worst values of each
parameter and the difference in perplexity improve-
ment between the two.

Differences of perplexity improvement between
best and worst values of θM range between 0.01

392



and 0.03. The four languages with the smallest
difference 0.01 are morphologically simple (Dutch,
English, French, Italian). The languages with the
largest difference (0.03) are morphologically more
complex languages. In summary, the frequency
threshold θM has a comparatively strong influence
on perplexity reduction. The strength of the effect is
correlated with the morphological complexity of the
language.

In contrast to θ, the number of suffixes φ and
the segmentation method have negligible effect on
most languages. The perplexity reductions for dif-
ferent values of φ are 0.03 for Finnish, 0.01 for Bul-
garian, Estonian, Hungarian, Polish and Slovenian,
and smaller than 0.01 for the other languages. This
means that, with the exception of Finnish, we can
use a value of φ = 100 for all languages and be very
close to the optimal perplexity reduction – either be-
cause 100 is optimal or because perplexity reduction
is not sensitive to choice of φ. Finnish is the only
language that clearly benefits from a large number
of suffixes.

Surprisingly, the performance of the morphologi-
cal segmentation methods is very close for 17 of the
21 languages. For three of the four where there is
a difference in improvement of ≥ 0.01, Frequency
(f) performs best. This means that Frequency is a
good segmentation method for all languages, except
perhaps for Estonian.

5.4 Impact of shape
The basic question we are asking in this paper is
to what extent the sequence of characters a word
is composed of can be exploited for better predic-
tion in language modeling. In the final analysis in
Table 5 we look at four different types of character
sequences and their contributions to perplexity re-
duction. The four groups are alphabetic character
sequences (W), numbers (N), single special charac-
ters (P = punctuation), and other (O). Examples for
O would be “751st” and words containing special
characters like “O’Neill”. The parameters used are
the optimal ones of Table 3. Table 5 shows that the
impact of special characters on perplexity is similar
across languages: 0.04 ≤ ∆P ≤ 0.06. The same is
true for numbers: 0.23 ≤ ∆N ≤ 0.33, with two out-
liers that show a stronger effect of this class: Finnish
∆N = 0.38 and German ∆N = 0.40.

∆W ∆P ∆N ∆O
S bg 0.07 0.04 0.28 0.16
S cs 0.09 0.04 0.26 0.33
S pl 0.10 0.05 0.23 0.22
S sk 0.10 0.05 0.25 0.28
S sl 0.10 0.04 0.28 0.28
G da 0.05 0.05 0.31 0.18
G de 0.06 0.05 0.40 0.18
G en 0.03 0.04 0.33 0.14
G nl 0.04 0.05 0.31 0.26
G sv 0.06 0.05 0.31 0.35
E el 0.08 0.05 0.33 0.14
R es 0.05 0.04 0.26 0.14
R fr 0.04 0.04 0.29 0.01
R it 0.04 0.05 0.33 0.02
R pt 0.05 0.05 0.28 0.39
R ro 0.08 0.04 0.25 0.17
U et 0.11 0.05 0.26 0.26
U fi 0.12 0.06 0.38 0.36
U hu 0.10 0.04 0.32 0.23
B lt 0.08 0.06 0.27 0.05
B lv 0.08 0.05 0.26 0.19

Table 5: Relative improvements of PM on the valida-
tion set compared to KN for histories wi−1i−N+1 grouped
by the type of wi−1. The possible types are alphabetic
word (W), punctuation (P), number (N) and other (O).

The fact that special characters and numbers be-
have similarly across languages is encouraging as
one would expect less crosslinguistic variation for
these two classes of words.

In contrast, “true” words (those exclusively com-
posed of alphabetic characters) show more variation
from language to language: 0.03 ≤ ∆W ≤ 0.12.
The range of variation is not necessarily larger than
for numbers, but since most words are alphabetical
words, class W is responsible for most of the differ-
ence in perplexity reduction between different lan-
guages. As before we observe a negative correlation
between morphological complexity and perplexity
reduction; e.g., Dutch and English have small ∆W
and Estonian and Finnish large values.

We provide the values of ∆O for completeness.
The composition of this catch-all group varies con-
siderably from language to language. For exam-
ple, many words in this class are numbers with al-
phabetic suffixes like “2012-ben” in Hungarian and

393



words with apostrophes in French.

6 Summary

We have investigated an interpolation of a KN model
with a class language model whose classes are de-
fined by morphology and shape features. We tested
this model in a large crosslingual study of European
languages.

Even though the model is generic and we use
the same architecture and features for all languages,
the model achieves reductions in perplexity for all
21 languages represented in the Europarl corpus,
ranging from 3% to 11%, when compared to a KN
model. We found perplexity reductions across all
21 languages for histories ending with four different
types of word shapes: alphabetical words, special
characters, and numbers.

We looked at the sensitivity of perplexity reduc-
tions to three parameters of the model: θ, a thresh-
old that determines for which frequencies words are
given their own class; φ, the number of suffixes used
to determine class membership; and morphological
segmentation. We found that θ has a considerable
influence on the performance of the model and that
optimal values vary from language to language. This
parameter should be tuned when the model is used
in practice.

In contrast, the number of suffixes and the mor-
phological segmentation method only had a small
effect on perplexity reductions. This is a surprising
result since it means that simple identification of suf-
fixes by frequency and choosing a fixed number of
suffixes φ across languages is sufficient for getting
most of the perplexity reduction that is possible.

7 Future Work

A surprising result of our experiments was that the
perplexity reductions due to morphological classes
were generally better than those due to distributional
classes even though distributional classes are formed
directly based on the type of information that a lan-
guage model is evaluated on – the distribution of
words or which words are likely to occur in se-
quence. An intriguing question is to what extent the
effect of morphological and distributional classes is
additive. We ran an exploratory experiment with
a model that interpolates KN, morphological class

model and distributional class model. This model
only slightly outperformed the interpolation of KN
and morphological class model (column PPM in Ta-
ble 3). We would like to investigate in future work if
the information provided by the two types of classes
is indeed largely redundant or if a more sophisticated
combination would perform better than the simple
linear interpolation we have used here.

Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank the
anonymous reviewers for their valuable comments.

References

Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Eurospeech.

Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Comput. Linguist.

Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL-HLT.

Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.

Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM TSLP.

Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti
Puurula, Janne Pylkkönen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Saraçlar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM TSLP.

Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PASCAL
Morpho Challenge.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In ICSLP.

Thomas Müller and Hinrich Schütze. 2011. Improved
modeling of out-of-vocabulary words using morpho-
logical classes. In ACL.

394



Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Computer
Speech & Language.

Hinrich Schütze and Michael Walsh. 2011. Half-context
language models. Comput. Linguist.

Hinrich Schütze. 1992. Dimensions of meaning.
In ACM/IEEE Conference on Supercomputing, pages
787–796.

Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Interspeech.

Yee Whye Teh. 2006. A hierarchical bayesian language
model based on Pitman-Yor processes. In ACL.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
JAIR.

Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In ICSLP.

E.W.D. Whittaker and P.C. Woodland. 2000. Particle-
based language modelling. In ICSLP.

Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In EMNLP.

Deniz Yuret and Ergun Biçici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In ACL-IJCNLP.

395


