



















































Variable Bit Quantisation for LSH


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 753–758,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Variable Bit Quantisation for LSH

Sean Moran

School of Informatics

The University of Edinburgh

EH8 9AB, Edinburgh, UK

sean.moran@ed.ac.uk

Victor Lavrenko

School of Informatics

The University of Edinburgh

EH8 9AB, Edinburgh, UK

vlavrenk@inf.ed.ac.uk

Miles Osborne

School of Informatics

The University of Edinburgh

EH8 9AB, Edinburgh, UK

miles@inf.ed.ac.uk

Abstract

We introduce a scheme for optimally al-

locating a variable number of bits per

LSH hyperplane. Previous approaches as-

sign a constant number of bits per hyper-

plane. This neglects the fact that a subset

of hyperplanes may be more informative

than others. Our method, dubbed Variable

Bit Quantisation (VBQ), provides a data-

driven non-uniform bit allocation across

hyperplanes. Despite only using a fraction

of the available hyperplanes, VBQ outper-

forms uniform quantisation by up to 168%

for retrieval across standard text and image

datasets.

1 Introduction

The task of retrieving the nearest neighbours to a

given query document permeates the field of Nat-

ural Language Processing (NLP). Nearest neigh-

bour search has been used for applications as di-

verse as automatically detecting document transla-

tion pairs for the purposes of training a statistical

machine translation system (SMT) (Krstovski and

Smith, 2011), the large-scale generation of noun

similarity lists (Ravichandran et al., 2005) to an

unsupervised method for extracting domain spe-

cific lexical variants (Stephan Gouws and Metzle,

2011).

There are two broad approaches to nearest

neighbour based search: exact and approximate

techniques, which are differentiated by their abil-

ity to return completely correct nearest neighbours

(the exact approach) or have some possibility of

returning points that are not true nearest neigh-

bours (the approximate approach). Approximate

nearest neighbour (ANN) search using hashing

techniques has recently gained prominence within

NLP. The hashing-based approach maps the data

into a substantially more compact representation

referred to as a fingerprint, that is more efficient

for performing similarity computations. The re-

sulting compact binary representation radically re-

duces memory requirements while also permitting

fast sub-linear time retrieval of approximate near-

est neighbours.

Hashing-based ANN techniques generally com-

prise two main steps: a projection stage followed

by a quantisation stage. The projection stage

performs a neighbourhood preserving embedding,

mapping the input data into a lower-dimensional

representation. The quantisation stage subse-

quently reduces the cardinality of this represen-

tation by converting the real-valued projections

to binary. Quantisation is a lossy transformation

which can have a significant impact on the result-

ing quality of the binary encoding.

Previous work has quantised each projected di-

mension into a uniform number of bits (Indyk and

Motwani, 1998) (Kong and Li, 2012) (Kong et al.,

2012) (Moran et al., 2013). We demonstrate that

uniform allocation of bits is sub-optimal and pro-

pose a data-driven scheme for variable bit alloca-

tion. Our approach is distinct from previous work

in that it provides a general objective function for

bit allocation. VBQ makes no assumptions on the

data and, in addition to LSH, it applies to a broad

range of other projection functions.

2 Related Work

Locality sensitive hashing (LSH) (Indyk and Mot-

wani, 1998) is an example of an approximate

nearest neighbour search technique that has been

widely used within the field of NLP to preserve the

Cosine distances between documents (Charikar,

2002). LSH for cosine distance draws a large

number of random hyperplanes within the input

feature space, effectively dividing the space into

non-overlapping regions (or buckets). Each hy-

perplane contributes one bit to the encoding, the

value (0 or 1) of which is determined by comput-

753



[a]� �

�

�

�
�

�
�

�
�

�
�

��

����

��

[b]
� �

��������	�����

��������	�����

�� �� �� ��

�
� �� ��


�������������	�����	
����������
�	������
������
��	���	
���	
�����

��������������	�����	
�������
	�
�����	
�
	�
��
������	�������	����	


�
�

�
�

�
�

�
�

Figure 1: Left: Data points with identical shapes are 1-NN. Two hyperplanes h1, h2 are shown alongside
their associated normal vectors (n1, n2). Right top: Projection of points onto the normal vectors n1
and n2 of the hyperplanes (arrows denote projections). Right middle: Positioning of the points along
normal vector n2. Three quantisation thresholds (t1, t2, t3, and consequently 2 bits) can maintain the
neighbourhood structure. Right bottom: the high degree of mixing between the 1-NN means that this

hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely).

ing the dot product of a data-point (x) with the
normal vector to the hyperplane (ni): that is, if
x.ni < 0, i ∈ {1 . . . k}, then the i-th bit is set
to 0, and 1 otherwise. This encoding scheme is

known as single bit quantisation (SBQ). More re-

cent hashing work has sought to inject a degree

of data-dependency into the positioning of the hy-

perplanes, for example, by using the principal di-

rections of the data (Wang et al., 2012) (Weiss

et al., 2008) or by training a stack of restricted

Boltzmann machines (Salakhutdinov and Hinton,

2009).

Existing quantisation schemes for LSH allocate

either one bit per hyperplane (Indyk and Motwani,

1998) or multiple bits per hyperplane (Kong et al.,

2012) (Kong and Li, 2012) (Moran et al., 2013).

For example, (Kong et al., 2012) recently pro-

posed the Manhattan Hashing (MQ) quantisation

technique where each projected dimension is en-

coded with multiple bits of natural binary code

(NBC). The Manhattan distance between the NBC

encoded data points is then used for nearest neigh-

bour search. The authors demonstrated that MQ

could better preserve the neighbourhood structure

between the data points as compared to SBQ with

Hamming distance.

Other recent quantisation work has focused on

the setting of the quantisation thresholds: for ex-

ample (Kong and Li, 2012) suggested encoding

each dimension into two bits and using an adaptive

thresholding scheme to set the threshold positions.

Their technique dubbed, Double Bit Quantisation

(DBQ), attempts to avoid placing thresholds be-

tween data points with similar projected values. In

other work (Moran et al., 2013) demonstrated that

retrieval accuracy could be enhanced by using a

topological quantisation matrix to guide the quan-

tisation threshold placement along the projected

dimensions. This topological quantisation matrix

specified pairs of ǫ-nearest neighbours in the orig-
inal feature space. Their approach, Neighbour-

hood Preserving Quantisation (NPQ), was shown

to achieve significant increases in retrieval accu-

racy over SBQ,MQ and DBQ for the task of image

retrieval. In all of these cases the bit allocation is

uniform: each hyperplane is assigned an identical

number of bits.

3 Variable Bit Quantisation

Our proposed quantisation scheme, Variable Bit

Quantisation (VBQ), assigns a variable number of

bits to each hyperplane subject to a maximum up-

per limit on the total number of bits1. To do so,

VBQ computes an F-measure based directly on the

positioning of the quantisation thresholds along a

projected dimension. The higher the F-measure

for a given hyperplane, the better that hyperplane

is at preserving the neighbourhood structure be-

tween the data points, and the more bits the hyper-

plane should be afforded from the bit budget B.
Figure 1(a) illustrates the original 2-

dimensional feature space for a toy example.

1Referred to as the bit budget B, typically 32 or 64 bits.

754



The space is divided into 4 buckets by two

random LSH hyperplanes. The circles, diamonds,

squares and stars denote 1-nearest neighbours

(1-NN). Quantisation for LSH is performed by

projecting the data points onto the normal vectors

(n1, n2) to the hyperplanes (h1, h2). This leads

to two projected dimensions. Thresholding these

projected dimensions at zero, and determining

which side of zero a given data-point falls, yields

the bit encoding for a given data-point.

Figure 1(b) demonstrates our proposed quanti-

sation scheme. Similar to vanilla LSH, the data-

points are projected onto the normal vectors, to

yield two projected dimensions. This is illustrated

on the topmost diagram in Figure 1(b). VBQ dif-

fers in how these projected dimensions are thresh-

olded to yield the bit encoding: rather than one

threshold situated at zero, VBQ employs one or

more thresholds and positions these thresholds in

an adaptive manner based upon maximisation of

an F-measure. Using multiple thresholds enables

more than one bit to be assigned per hyperplane2.

Figure 1(b) (middle, bottom) depicts the F-

measure driven threshold optimisation along the

projected dimensions. We define as a positive

pair, those pairs of data points in the original fea-

ture space that are ǫ-nearest neighbours (ǫ-NN),
and a negative pair otherwise. In our toy exam-

ple, data points with the same shape symbol form

a positive pair, while points with different sym-

bols are negative pairs. Intuitively, the thresholds

should be positioned in such a way as to maxi-

mize the number of positive pairs that fall within

the same thresholded region, while also ensuring

the negative pairs fall into different regions.

This intuition can be captured by an F-measure

which counts the number of positive pairs that are

found within the same thresholded regions (true

positives, TP), the number of negative pairs found

within the same regions (false positives, FP), and

the number of positive pairs found in different re-

gions of the threshold partitioned dimension (false

negatives, FN). For n2, three thresholds are opti-

mal, given they perfectly preserve the neighbour-

hood structure. For n1, no thresholds can provide a

neighbourhood preserving quantisation and there-

fore it is better to discard the hyperplane h1. VBQ

uses random restarts to optimise the F-measure3.

The computed F-measure scores per hyper-

2b bits, requires 2b − 1 thresholds.
3More details on the computation of the F-measure per

hyperplane can be found in (Moran et al., 2013).

plane (h), per bit count (b) are an effective sig-
nal for bit allocation: more informative hyper-

planes tend to have higher F-measure, for higher

bit counts. VBQ applies a binary integer linear

program (BILP) on top of the F-measure scores

to obtain the bit allocation. To do so, the algo-

rithm collates the scores in a matrix F with ele-

ments Fb,h, where b ∈ {0, . . . , k} 4 indexes the
rows, with k being the maximum number of bits
allowable for any given hyperplane (set to 4 in this

work), and h ∈ {1 . . . , B} indexes the columns.
The BILP uses F to find the bit allocation that

maximises the cumulative F-measure across theB
hyperplanes (Equation 1).

max ‖F ◦ Z‖
subject to ‖Zh‖ = 1 h ∈ {1 . . . B}

‖Z ◦ D‖ ≤ B
Z is binary

(1)

‖.‖ denotes the Frobenius L1 norm, ◦ the
Hadamard product and D is a constraint matrix,

with Db,h = b, ensuring that the bit allocation
remains within the bit budget B. The BILP is
solved using the standard branch and bound op-

timization algorithm (Land and Doig, 1960). The

output from the BILP is an indicator matrix Z ∈
{0, 1}(k+1)×B whose columns specify the optimal
bit allocation for a given hyperplane i.e. Zb,h = 1
if the BILP decided to allocate b bits for hyper-
plane h, and zero otherwise. Example matrices for
the toy problem in Figure 1 are given hereunder (in

this example, k = 2 and B = 2).




F h1 h2

b0 0.25 0.25
b1 0.35 0.50
b2 0.40 1.00







D

0 0
1 1
2 2







Z

1 0
0 0
0 1




Notice how the indicator matrix Z specifies an

assignment of 0 bits for hyperplane h1 and 2 bits
for hyperplane h2 as this yields the highest cu-
mulative F-measure across hyperplanes while also

meeting the bit budget. VBQ is therefore a princi-

pled method to select a discriminative subset of

hyperplanes, and simultaneously allocate bits to

the remaining hyperplanes, given a fixed overall

bit budget B, while maximizing cumulative F-
measure.

4For 0 bits, we compute the F-measure without any
thresholds along the projected dimension.

755



Dataset CIFAR-10 TDT-2 Reuters-21578

SBQ MQ DBQ NPQ VBQ SBQ MQ DBQ VBQ SBQ MQ DBQ VBQ

SIKH 0.042 0.063 0.047 0.090 0.161 0.034 0.045 0.031 0.092 0.102 0.112 0.087 0.389

LSH 0.119 0.093 0.066 0.153 0.207 0.189 0.097 0.089 0.229 0.276 0.201 0.175 0.538

BLSI 0.038 0.135 0.111 0.155 0.231 0.283 0.210 0.087 0.396 0.100 0.030 0.030 0.156

SH 0.051 0.135 0.111 0.167 0.202 0.146 0.212 0.167 0.370 0.033 0.028 0.030 0.154

PCAH 0.036 0.137 0.107 0.153 0.219 0.281 0.208 0.094 0.374 0.095 0.034 0.027 0.154

Table 1: Area under the Precision Recall curve (AUPRC) for all five projection methods. Results are for

32 bits (images) and at 128 bits (text). The best overall score for each dataset is shown in bold face.

4 Experiments

4.1 Datasets

Our text datasets are Reuters-21578 and TDT-2.

The original Reuters-21578 corpus contains 21578

documents in 135 categories. We use theModApte

version and discard those documents with multi-

ple category labels. This leaves 8,293 documents

in 65 categories. The corpus contains 18,933 dis-

tinct terms. The TDT-2 corpus consists of 11,201

on-topic documents which are classified into 96

semantic categories. We remove those documents

appearing in two or more categories and keep only

the largest 30 categories. This leaves 9,394 docu-

ments in total with 36,771 distinct terms. Both text

datasets are TF-IDF and L2 norm weighted. To

demonstrate the generality of VBQ we also evalu-

ate on the CIFAR-10 image dataset (Krizhevsky,

2009), which consists of 60,000 images repre-

sented as 512 dimensional Gist descriptors (Oliva

and Torralba, 2001). All of the datasets are identi-

cal to those that have been used in previous ANN

hashing work (Zhang et al., 2010) (Kong and Li,

2012) and are publicly available on the Internet.

4.2 Projection Methods

VBQ is independent of the projection stage and

therefore can be used the quantise the projections

from a wide range of different projection func-

tions, including LSH. In our evaluation we take

a sample of the more popular data-independent

(LSH, SIKH) and data-dependent (SH, PCAH,

BLSI) projection functions used in recent hashing

work:

• SIKH: Shift-Invariant Kernel Hashing
(SIKH) uses random projections that approx-

imate shift invariant kernels (Raginsky and

Lazebnik, 2009). We follow previous work

and use a Gaussian kernel with a bandwidth

set to the average distance to the 50th nearest

neighbour (Kong et al., 2012) (Raginsky and

Lazebnik, 2009).

• LSH: Locality Sensitive Hashing uses a
Gaussian random matrix for projection (In-

dyk and Motwani, 1998) (Charikar, 2002).

• BLSI: Binarised Latent Semantic Indexing
(BLSI) forms projections through Singular

Value Decomposition (SVD) (Salakhutdinov

and Hinton, 2009).

• SH: Spectral Hashing (SH) uses the eigen-
functions computed along the principal com-

ponent directions of the data for projec-

tion (Weiss et al., 2008).

• PCAH: Principal Component Analysis
Hashing (PCAH) employs the eigenvectors

corresponding the the largest eigenvalues of

the covariance matrix for projection (Wang

et al., 2012).

4.3 Baselines

Single Bit Quantisation (SBQ) (Indyk and Mot-

wani, 1998), Manhattan Hashing (MQ) (Kong et

al., 2012), Double Bit Quantisation (DBQ) (Kong

and Li, 2012) and Neighbourhood Preserving

Quantisation (NPQ) (Moran et al., 2013). MQ,

DBQ and NPQ all assign 2 bits per hyperplane,

while SBQ assigns 1 bit per hyperplane. All meth-

ods, including VBQ, are constrained to be within

the allocated bit budget B. If a method assigns
more bits to one hyperplane, then it either dis-

cards, or assigns less bits to other hyperplanes.

4.4 Evaluation Protocol

We adopt the standard Hamming ranking evalua-

tion paradigm (Kong et al., 2012). We randomly

select 1000 query data points per run. Our re-

sults are averaged over 10 runs, and the average

reported. The ǫ-neighbours of each query point

756



[1]
� �� �� �� �� �� �� ��

�

�	��

�	�

�	��

�	�

�	��

�	�

�	��


�� � ���

��������	�
��

�
�

�
�

�

[3]
�� �� �� �� ��� ���

	

	
	�

	
�

	
��

	
�

	
��

	
�

	
��

	
�

	
��

	
�
�� � ��

��������	�
��

�
�

�
�

�

[5]
�� �� �� �� ��� ���

	

	
�

	
�

	
�

	
�

	
�

	
�

	
�

�� �� ��

��������	�
��

�
�

�
�

�

[2]
� ��� ��� ��� ��� ��� ��� ��	 ��
 ��� �

�

���

���

���

���

���

���

��	

��


���

�

�� �� ��

������

�
�
�
�
�	

�

�

[4]
� ��� ��� ��� ��� ��� ��� ��	 ��
 ��� �

�

���

���

���

���

���

���

��	

��


���

�

�� �� ��

������

�
�
�
�
�	

�

�

[6]
� ��� ��� ��� ��� ��� ��� ��	 ��
 ��� �

�

���

���

���

���

���

���

��	

��


���

�

�� �� ��

������

�
�
�
�
�	

�

�

Figure 2: [1] LSH AUPRC vs bits for CIFAR-10 [2] LSH Precision-Recall curve for CIFAR-10 [3]

LSH AUPRC vs bits for TDT-2 [4] LSH Precision-Recall curve for TDT-2 [5] LSH AUPRC vs bits for

Reuters-21578 [6] LSH Precision-Recall curve for Reuters-21578

form the ground truth for evaluation. The thresh-

old ǫ is computed by sampling 100 training data-
points at random from the training dataset and de-

termining the distance at which these points have

50 nearest neighbours on average. Positive pairs

and negative pairs for F-measure computation are

computed by thresholding the training dataset

Euclidean distance matrix by ǫ. We adopt the
Manhattan distance and multi-bit binary encoding

method as suggested in (Kong et al., 2012). The

F-measure we use for threshold optimisation is:

Fβ = (1+β
2)TP/((1+β2)TP +β2FN+FP ).

We select the parameter β on a held-out valida-
tion dataset. The area under the precision-recall

curve (AUPRC) is used to evaluate the quality of

retrieval.

4.5 Results

Table 1 presents our results. For LSH on text

(Reuters-21578) at 128 bits we find a substantial

95% gain in retrieval performance over uniformly

assigning 1 bit per hyperplane (SBQ) and a 168%

gain over uniformly assigning 2 bits per hyper-

plane (MQ). VBQ gain over SBQ at 128 bits is sta-

tistically significant based upon a paired Wilcoxon

signed rank test across 10 random train/test parti-

tions (p-value: ≤ 0.0054). This pattern is repeated
on TDT-2 (for 128 bits, SBQ vs VBQ: p-value

≤ 0.0054) and CIFAR-10 (for 32 bits, SBQ vs
VBQ: p-value: ≤ 0.0054). VBQ also reaps sub-
stantial gains for the Eigendecomposition based

projections (PCAH, SH, BLSI) effectively exploit-

ing the imbalanced variance across hyperplanes -

that is, those hyperplanes capturing higher propor-

tions of the variance in the data are allocated more

bits from the fixed bit budget. Figure 2 (top row)

illustrates that VBQ is effective across a range of

bit budgets. Figure 2 (bottom row) presents the

precision-recall (PR) curves at 32 bits (CIFAR-10)

and 128 bits (TDT-2, Reuters-21578). We confirm

our hypothesis that judicious allocation of variable

bits is significantly more effective than uniform al-

location.

5 Conclusions

Our proposed quantisation scheme computes a

non-uniform bit assignment across LSH hyper-

planes. The novelty of our approach is centred

upon a binary integer linear program driven by a

novel F-measure based objective function that de-

termines the most appropriate bit allocation: hy-

perplanes that better preserve the neighbourhood

structure of the input data points are awarded more

bits from a fixed bit budget. Our evaluation on

standard datasets demonstrated that VBQ can sub-

stantially enhance the retrieval accuracy of a se-

lection of popular hashing techniques across two

distinct modalities (text and images). In this paper

we concentrated on the hamming ranking based

scenario for hashing. In the future, we would like

to examine the performance of VBQ in the lookup

based hashing scenario where hash tables are used

for fast retrieval.

757



References

Moses Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC, pages
380–388.

Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the thirtieth annual
ACM symposium on Theory of computing, STOC
’98, pages 604–613, New York, NY, USA. ACM.

Weihao Kong and Wu-Jun Li. 2012. Double-bit quan-
tization for hashing. In AAAI.

Weihao Kong, Wu-Jun Li, andMinyi Guo. 2012. Man-
hattan hashing for large-scale image retrieval. SI-
GIR ’12, pages 45–54.

Alex Krizhevsky. 2009. Learning Multiple Layers of
Features from Tiny Images. Master’s thesis.

Kriste Krstovski and David A. Smith. 2011. A Mini-
mally Supervised Approach for Detecting and Rank-
ing Document Translation Pairs. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, Edinburgh, Scotland. Association for Compu-
tational Linguistics.

A. H. Land and A. G. Doig. 1960. An automatic
method of solving discrete programming problems.
Econometrica, 28:pp. 497–520.

Sean Moran, Victor Lavrenko, and Miles Osborne.
2013. Neighbourhood preserving quantisation for
lsh. In 36th Annual International ACM Conference
on Research and Development in Information Re-
trieval (SIGIR), Dublin, Ireland, 07/2013.

Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42(3):145–175.

Maxim Raginsky and Svetlana Lazebnik. 2009.
Locality-sensitive binary codes from shift-invariant
kernels. In NIPS ’09, pages 1509–1517.

Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. ACL ’05, pages 622–629. Association
for Computational Linguistics.

Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969–978.

Dirk Hovy Stephan Gouws and Donald Metzle. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, EMNLP ’11, page 8290.
Association for Computational Linguistics.

Jun Wang, S. Kumar, and Shih-Fu Chang. 2012. Semi-
supervised hashing for large-scale search. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 34(12):2393–2406.

Yair Weiss, Antonio B. Torralba, and Robert Fergus.
2008. Spectral hashing. In NIPS, pages 1753–1760.

Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu.
2010. Self-taught hashing for fast similarity search.
In SIGIR, pages 18–25.

758


