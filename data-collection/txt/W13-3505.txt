



















































Graph-Based Posterior Regularization for Semi-Supervised Structured Prediction


Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 38–46,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Graph-Based Posterior Regularization
for Semi-Supervised Structured Prediction

Luheng He Jennifer Gillenwater
Computer and Information Science

University of Pennsylvania
{luhe,jengi}@cis.upenn.edu

Ben Taskar
Computer Science and Engineering

University of Washington
taskar@cs.washington.edu

Abstract

We present a flexible formulation of semi-
supervised learning for structured mod-
els, which seamlessly incorporates graph-
based and more general supervision by ex-
tending the posterior regularization (PR)
framework. Our extension allows for any
regularizer that is a convex, differentiable
function of the appropriate marginals. We
show that surprisingly, non-linearity of
such regularization does not increase the
complexity of learning, provided we use
multiplicative updates of the structured ex-
ponentiated gradient algorithm. We il-
lustrate the extended framework by learn-
ing conditional random fields (CRFs) with
quadratic penalties arising from a graph
Laplacian. On sequential prediction tasks
of handwriting recognition and part-of-
speech (POS) tagging, our method makes
significant gains over strong baselines.

1 Introduction

Recent success of graph-based semi-supervised
learning builds on access to plentiful unsupervised
data and accurate similarity measures between
data examples (Zhu et al., 2003; Joachims, 2003;
Belkin et al., 2005; Zhu and Lafferty, 2005; Al-
tun et al., 2005; Zhu, 2005; Chapelle et al., 2006;
Subramanya and Bilmes, 2009; Subramanya et
al., 2010; Das and Petrov, 2011). Many ap-
proaches, such as Joachims (2003) and Subra-
manya and Bilmes (2009) use graph-based learn-
ing in the transductive setting, where unlabeled ex-
amples are classified without learning a parametric
predictive model. While predicted labels can then
be leveraged to learn such a model (e.g. a CRF),
this pipelined approach misses out on the benefits
of modeling sequential correlations during graph
propagation. In this work we seek to better inte-

grate graph propagation with estimation of a struc-
tured, parametric predictive model.

To do so, we build on the posterior regulariza-
tion (PR) framework of Ganchev et al. (2010). PR
is a principled means of providing weak super-
vision during structured model estimation. More
concretely, PR introduces a penalty whenever the
model’s posteriors over latent variables contra-
dict the specified weak supervision. Ganchev
et al. (2010) show how to efficiently optimize a
likelihood-plus-posterior-penalty type objective in
the case where the penalty is linear in the model’s
marginals. Yet, there are many forms of supervi-
sion that cannot be expressed as a linear function
of marginals. For example, graph Laplacian regu-
larization. In this work, we extend PR to allow for
penalties expressed as any convex, differentiable
function of the marginals and derive an efficient
optimization method for such penalties.

In our experiments, we explore graph Lapla-
cian posterior regularizers for two applications:
handwriting recognition and POS tagging. The
methods of Altun et al. (2005), Subramanya et al.
(2010), and Das and Petrov (2011) are the most
closely related to this work. Altun et al. (2005)
describes coupling a graph regularizer with a max-
margin objective for pitch accent prediction and
handwriting recognition tasks. Their method suf-
fers from scalability issues though; it relies on op-
timization in the dual, which requires inversion of
a matrix whose dimension grows with graph size.

The more recent work of Subramanya et al.
(2010) tackles the POS tagging task and pro-
vides a more scalable method. Their method
is a multi-step procedure that iterates two main
steps, graph propagation and likelihood optimiza-
tion, until convergence. Actually computing the
optimum for the graph propagation step would re-
quire a matrix inversion similar to that used by Al-
tun et al. (2005), but they skirt this issue by using
an heuristic update rule. Unfortunately though, no

38



guarantees for the quality of this update are es-
tablished. Das and Petrov (2011) proceed very
similarly, adapting the iterative procedure to in-
clude supervision from bi-text data, but applying
the same heuristic update rule.

The work we present here similarly avoids the
complexity of a large matrix inversion and iter-
ates steps related to graph propagation and likeli-
hood optimization. But in contrast to Subramanya
et al. (2010) and Das and Petrov (2011) it comes
with guarantees for the optimality of each step and
convergence of the overall procedure. Further, our
approach is based on optimizing a joint objective,
which affords easier analysis and extensions us-
ing other constraints or optimization methods. The
key enabling insight is a surprising factorization of
the non-linear regularizer, which can be exploited
using multiplicative updates.

2 Posterior regularization

We focus on the semi-supervised setting, showing
how to extend the discriminative, penalty-based
version of PR for a linear chain CRF. Our results
apply more generally though to the unsupervised
setting, the constraint-based versions of PR, and
other graphical models.

In the standard semi-supervised setting we are
given n data instances, {x1, . . . ,xn}, and labels
{y1, . . . ,yl} for the first l � n instances. For
simplicity of notation, we’ll assume each xi has
T components. Modeling this data with a linear
chain CRF, the standard conditional log-likelihood
objective with a Gaussian prior (variance∝ σ2) is:

L(θ) =
l∑

i=1

log pθ(y
i | xi)− ||θ||

2
2

2σ2
. (1)

Note that this discriminative objective does not at-
tempt to leverage the unlabeled data. Since pθ de-
composes according to the independence assump-
tions of a linear chain CRF, it can be expressed as:

pθ(y | x) =
exp

[∑T
t=1 θ

>f(yt, yt−1,x)
]

Zp(x)
(2)

where the Zp(x) is a normalizer:

Zp(x) =
∑

y′
exp

[
T∑

t=1

θ>f(y′t, y
′
t−1,x)

]
(3)

and the f are arbitrary feature functions. We as-
sume f(y1, y0,x) receives a special “start” marker

for y0. In what follows, we refer to functions
over the (yt, yt−1,x) as local factors, or p-factors;
pθ(y | x) decomposes as a product of p-factors.

Given this decomposition, L and its gradient
with respect to θ can be efficiently computed using
the forward-backward algorithm for linear chains.
This amounts to computing posterior marginals
for each p-factor (yt, yt−1,x). Following the gra-
dient suffices to find the global optimum of L,
since likelihood is concave, and the Gaussian prior
makes it strictly concave.

Penalty-based posterior regularization (PR)
modifies the likelihood objective by adding a
“penalty” term expressing prior knowledge about
the posteriors (Ganchev et al., 2010). To allow for
more efficient optimization, penalty terms are im-
posed on an auxiliary joint distribution q over the
labels instead of directly on pθ. Agreement be-
tween q and pθ is encouraged by a KL term:

KL(q ‖ pθ) =
n∑

i=1

KL(q(Y | xi) ‖ pθ(Y | xi))

where Y is a random variable that can take on any
possible labeling y, and q(Y |xi) is a an arbitrary
distribution over Y for each i1. The penalty term
itself is restricted to be an essentially linear func-
tion of the p-factor marginals of q(Y | xi). To
compactly express this, we first define some no-
tation. Let mi denote the p-factor marginals of
q(Y | xi). For first-order linear chain models,
if K is the total number of labels a y variable
can take on, then mi contains the marginals for
t ∈ {1, . . . , T} and all K2 possible (yt, yt−1) la-
bel pairs. That is, mi is a length O(TK2) vector
with entries:

mit,k,j =
∑

y

1(yt = k, yt−1 = j)q(y | xi) .

(4)
Stacking all these mi, we let m represent the
O(nTK2) vector [m1, . . . ,mn]. We further de-
fine a matrix A of constraint features. The product
Am is then the expectation of these features under
q. Finally we have, with a vector b of limits, the
following expression for the penalty term:

hlin(m) = ||max (Am− b,0)||β (5)

where ||·||β denotes an arbitrary norm. This ex-
pression will be non-zero if the expected value of

1We use a notation that is slightly different than, but
equivalent to, that of prior work, in order to facilitate our ex-
tensions later.

39



Am is larger than the limit b. The full posterior
regularizer is then:

R(θ, q) = KL(q ‖ pθ) + λhlin(m) , (6)

where λ is a hyperparameter that controls the
strength of the second term.

Running example: Consider the task of part-
of-speech (POS) tagging, where the y are tags
and the x are words. To encourage every sen-
tence to contain at least one verb, we can pe-
nalize if the expected number of verbs under
the q distribution is less than 1. Specifically,
if “verb” is represented by tag number v, for
sentence i we penalize unless:

1 ≤
T∑

t=1

K∑

yt−1=1

mit,v,yt−1 . (7)

In the notation of Equation (5), these penal-
ties correspond to: an n-row A matrix, where
row i has−1’s to select exactly the portion of
m from Equation (7), and a limit b = −1.
We briefly note here that generalized expec-

tation (Mann and McCallum, 2007; Mann and
McCallum, 2008) can be used to impose similar
penalties, but without the auxiliary q distribution.
Unfortunately though, this means the expectation
of the A features is with respect to pθ, so comput-
ing the gradient requires the covariance between
the constraint features in A and the model features
f , under θ. For a linear chain CRF, this means the
run time of forward-backward is squared, although
some optimizations are possible. PR’s use of the
auxiliary q allows us to optimize more efficiently
by splitting the problem into easier blocks.

The new objective that combines likelihood
with the PR penalty is: J (θ, q) = L(θ) −
R(θ, q). While optimizing L(θ) is easy, finding
maxθ,q J (θ, q) is NP-hard even for the simplest
models. To optimize J , Ganchev et al. (2010)
employ an expectation maximization (EM) based
method. At iteration t + 1, the algorithm updates
q and θ as follows:

E : qt+1 = argmin
q

R(θt, q) (8)

M : θt+1 = argmax
θ

L(θ) + (9)

δ
n∑

i=l+1

∑

y

qt+1(y | xi) log pθ(y | xi)

where δ here is a hyperparameter that trades off
between the labeled and unlabeled data. Though
not stated above, note that in the E-step minimiza-
tion over q(Y | xi) is constrained to the probabil-
ity simplex. Ganchev et al. (2010) show that this
E-step can be efficiently implemented, via pro-
jected gradient descent on the dual. The M-step
is similar to optimizing the original L, but with a
contribution from the unlabeled data that further
encourages q and pθ to agree. Thus, the M-step
can be implemented via the same gradient ascent
methods as used for L. As with standard EM,
this method monotonically increases J and thus
is guaranteed to converge to a local optimum.

In this work, we contemplate what other types
of posterior penalty terms besides hlin(m) are
possible. In the subsequent section, we show that
it is possible to extend the class of efficiently-
optimizable PR penalties to encompass all convex,
differentiable functions of the marginals.

3 Non-linear PR

Let h(m) denote an arbitrary convex, differen-
tiable function of the marginals of q. Replacing
R’s penalty term with h, we have:

R̃(θ, q) = KL(q ‖ pθ) + λh(m) (10)

Let J̃ represent the full objective with R̃. We
show that J̃ can be efficiently optimized.

Running example: Returning to our POS
tagging example, let’s consider one type of
non-linear convex penalty that might be use-
ful. Suppose our corpus has N unique
trigrams, and we construct a graph G =
(V,E,W ) where each vertex in V is a trigram
and each edge (a, b) ∈ E has a weight wab
that indicates the similarity of trigrams a and
b. To use the information from this graph to
inform our CRF, we can use the graph Lapla-
cian: L = D−W , where D is a diagonal de-
gree matrix with daa =

∑N
j=1waj . The form

of L is such that for every vector v ∈ RN :

v>Lv =
1

2

N∑

a=1

N∑

b=1

wab(va − vb)2 . (11)

The larger the disparity in v values of similar
vertices, the larger the value of v>Lv. The
matrix L is positive semi-definite, so v>Lv is

40



convex in v. If each entry va is a linear func-
tion of the vector of marginals m described
above, then v(m)>Lv(m) is convex in m.
Thus, for any linear v(m), we can use this
Laplacian expression as a PR penalty.

For example, we can define v(m) such that
h(m) applies a penalty if trigrams that are
similar according to the graph have different
expected taggings under the CRF model. To
state this more formally, let’s define a map-
pingB : ({1, . . . , n}, {1, . . . , T}) 7→ V from
words in the corpus to vertices in the graph:
B(i, t) = a implies word xit maps to vertex a.
Then, for a given tag k, we have the following
formula for the value of vertex a:

va,k = m̃a,k =

n∑
i=1

T∑
t=1

B(i,t)=a

K∑
yt−1=1

mit,k,yt−1

∑n
i=1

∑T
t=1 1(B(i, t) = a)

There are several issues to overcome in showing
that EM with these more general h(m) can still
be run efficiently and will still reach a local opti-
mum. First, we have to show that the optimal q
for the E-step minimization can still be compactly
representable as a product of p-factors.

3.1 Decomposition

Theorem 1. If h(m) is a convex, differen-
tiable function of q’s p-factor marginals, q∗ =
argminq R̃(θ, q) decomposes as a product of p-
factors.

Proof. Consider the E-step gradient of R̃(θ, q)
with respect to q. Using the shorthand qiy for
q(y | xi), the gradient is:

∂R̃
∂qiy

= log qiy + 1− log pθ(y | xi) + (12)

λ
∂h(m)

∂m

>∂m
∂qiy

.

Here, ∂m
∂qiy

is just a 0-1 vector indicating which of

the marginals from m apply to qiy. For example,
for yt = k and yt−1 = j, the marginal mit,k,j is
relevant. We can more simply write:

∂h(m)

∂m

>∂m
∂qiy

=

T∑

t=1

∂h(m)

∂mit,yt,yt−1
. (13)

Setting the gradient equal to zero and solving for
qiy, we see that it must take the following form:

qiy =

pθ(y | xi) exp
[
−λ

T∑
t=1

∂h(m)

∂mit,yt,yt−1

]

Zq(xi)
.

(14)
From this expression, it is clear that qiy is propor-
tional to a product of p-factors.

Running example: Recall the graph Lapla-
cian penalty, discussed above for a particular
tag k. Summing over all tags, the penalty is:

h(m) =
1

2

K∑

k=1

N∑

a=1

N∑

b=1

wab(m̃a,k − m̃b,k)2 .

The derivative ∂h(m)
∂mit,yt,yt−1

is then:

2

K∑

k=1

N∑

a=1

wa,B(i,t)(m̃B(i,t),k − m̃a,k) . (15)

In words: for a given k, this gradient is pos-
itive if node B(i, t) has larger probability of
taking tag k than its close neighbors. Moving
in the direction opposite the gradient encour-
ages similar taggings for similar trigrams.

Theorem 1 confirms that the optimal q will de-
compose as desired, but does not address whether
we can efficiently find this q. Previous PR work
optimized the E-step in the dual. But while the
dual is easy to compute in closed form for norms
or linear functions, for arbitrary convex functions
the dual is often non-trivial.

Running example: For the case of a
graph Laplacian regularizer, in the primal the
penalty takes the form of a quadratic pro-
gram: v>Lv. Unfortunately, the dual of a
quadratic program contains a matrix inverse,
L−1 (van de Panne and Whinston, 1964).
Taking a matrix inverse is expensive, which
makes optimization in the dual unattractive.

Since moving to the dual would be inefficient,
optimizing R̃ will require some form of gradient
descent on the qiy. However, the standard gradient
descent update:

qiy ← qiy − η
∂R̃
∂qiy

(16)

41



where η is the step size, does not result in a fea-
sible optimization scheme, for several reasons.
First, it is possible for the updated q to be outside
the probability simplex. To be sure it remains in
the simplex would require a projection step on the
full, exponential-size set of all qiy, for each exam-
ple xi. Second, the updated q may not be propor-
tional to a product of p-factors. To be concrete,
suppose the starting point is qiy = pθ(y | xi),
which does decompose as a product of p-factors.
Then after the first gradient update, we have:

qiy = pθ(y | xi)− η
(
1 + λ

T∑

t=1

∂h(m)

∂mit,yt,yt−1

)
.

Unfortunately, while pθ(y | xi) decomposes as a
product of p-factors, the other term decomposes
as a sum. Naturally, as we discuss in the following
section, multiplicative updates are more suitable.

3.2 Exponentiated Gradient
The exponentiated gradient descent (EGD) algo-
rithm was proposed by Kivinen and Warmuth
(1995), who illustrate its application to linear pre-
diction. More recently, Collins et al. (2005) and
Collins et al. (2008) extended EGD to exploit fac-
torization in structured models. The most impor-
tant aspect of EGD for us is that a variable’s up-
date formula takes a multiplicative rather than an
additive form. Specifically, the update for qiy is:

qiy ← qiy exp
[
−η ∂R̃

∂qiy

]
. (17)

Lemma 2. EGD update Equation (17) preserves
decomposition of q into p-factors.

Proof. Applying the multiplicative EGD update
formula to qiy, we see that its new value equals the
following product:

(qiy)
1−ηpθ(y | xi)η exp

[
−ηλ

T∑

t=1

∂h(m)

∂mit,yt,yt−1

]
,

up to a normalization constant. Since qiy and
pθ(y | xi) both decompose as a product of p-
factors and since the update term is another prod-
uct of p-factors, the updated expression is itself a
product of p-factors (up to normalization).

Note that normalization is not an issue with
the EGD updates. Since q retains its decompo-
sition, the normalization can be efficiently com-
puted using forward-backward. Thus, Lemma 2

moves us much closer to the goal of running EM
efficiently, though there remain several stumbling
blocks. First and foremost, we cannot afford to ac-
tually apply EGD to each qiy, as there are an expo-
nential number of them. Thankfully, we can show
these EGD updates are equivalent to following the
gradient on a much smaller set of values. In par-
ticular, letting F represent the dimension of m,
which for example is O(nTK2) for linear chains,
we have the following result.

Lemma 3. Given the gradient vector ∂h(m)∂m , one
step of EGD on R̃(θ, q) can be completed in time
O(F ), where F is the dimension ofm.

Proof. First, we re-express qiy in log-linear form.
Applying Lemma 2, we know that qiy is propor-
tional to a product of p-factors This means that
there must exist some factors r such that qiy can
be written:

qiy =
1

Zq(xi)
exp

[
T∑

t=1

ri,t(yt, yt−1)

]
. (18)

Re-expressing ∂R̃
∂qiy

given these r, we have:

∂R̃
∂qiy

= C +

T∑

t=1

[
ri,t(yt, yt−1)− (19)

θ>f(yt, yt−1,xi) + λ
∂h(m)

∂mit,yt,yt−1

]
,

whereC = 1−logZq(xi)+logZp(xi) is constant
with respect to y. This means that we can just
update the individual r factors as follows:

ri,t(yt, yt−1)← (1− η)ri,t(yt, yt−1) +

ηθ>f(yt, yt−1,xi)− ηλ
∂h(m)

∂mit,yt,yt−1
. (20)

Note that if we start from qiy = pθ(y | xi), then
the initial ri,t(yt, yt−1) are just θ>f(yt, yt−1,xi).
To conclude, since the number of r functions is
equal to the dimension ofm, the overall update is
linear in the number of marginals.

At this point, just one small issue remains: how
expensive is computing ∂h(m)∂m ? Work analyzing
the reverse mode of automatic differentiation in-
dicates that if computing a function h requires c
operations, then computing its gradient vector re-
quires no more than O(c) operations (Griewank,
1988). Thus, as long as our penalty function is

42



itself efficiently computable, the gradient vector
will be too. We conclude by observing that our
efficient algorithm converges to a local optimum.

Theorem 4. The above EGD-based EM algorithm
for optimizing J̃ (θ, q) converges to a local opti-
mum of this objective.

Proof. The M-step remains unchanged from stan-
dard PR EM, and as such is strictly convex in
θ. The E-step is strictly convex in q, since KL-
divergence is strictly convex and h(m) is convex.
Applying EGD, we know that we can efficiently
find the E-step optimum. Therefore, the EGD-
based EM algorithm efficiently implements coor-
dinate ascent on J̃ (θ, q), with each step monoton-
ically increasing J̃ :

J̃ (θt, qt) ≤ J̃ (θt, qt+1) ≤ J̃ (θt+1, qt+1) .

Hence, we have shown that it is possible to
efficiently use an arbitrary convex, differentiable
function of the marginals, h(m), as a PR penalty
function. In the following section, we apply one
such function — the graph Laplacian quadratic
from the running example — to several tasks.

4 Experiments

We evaluate the effect of a graph Laplacian PR
penalty on two different sequence prediction tasks:
part-of-speech (POS) tagging and handwriting
recognition. Our experiments are conducted in a
semi-supervised setting, where only a small num-
ber, l, of labeled sequences are available during
training. Both the l labeled sequences and the re-
mainder of the dataset (instances l + 1 through n)
are used to construct a graph Laplacian2. We train
a second-order CRF using the methods described
in Section 3 and report results for a test set con-
sisting of instances l + 1 through n.

4.1 Graph construction
For each task we define a symmetric similarity
function on the task’s vertices V , sim : V × V 7→
R, and build the graph based on its values. Specif-
ically, denoting the k nearest neighbors (NN) of
node u by Nk(u), we use the following mutual k-
NN criterion to decide which edges to include:

(u, v) ∈ E ⇐⇒ u ∈ Nk(v) ∧ v ∈ Nk(u) .
2While these particular experiments are transductive, our

method can easily be applied inductively as well.

Entries in the final edge weight matrix are: wuv =
1[(u, v) ∈ E]sim(u, v).

4.2 Part-of-speech tagging
We experiment on ten languages. Our English
(EN) data is from the Penn Treebank (Marcus et
al., 1993), Italian (IT) and Greek (EL) are from
CoNLL-2007 (Nivre et al., 2007), and the remain-
ing languages in Figure 1 (a): German (DE), Span-
ish (ES), Portuguese (PT), Danish (DA), Slovene
(SL), Swedish (SV), and Dutch (NL) are from
CoNLL-X (Buchholz and Marsi, 2006). We use
a universal tag set (Das et al., 2012) throughout.

For each language, we first construct a mu-
tual 60-NN graph3 on trigram types, excluding
trigrams whose center word is punctuation. Our
smallest graph (Slovene) contains 25,198 nodes
while the largest (English) has 611,730.

For the similarity function sim(u, v), we follow
the method used in (Subramanya et al., 2010) and
(Das and Petrov, 2011), but with a somewhat mod-
ified feature set. For instance, while (Subramanya
et al., 2010) uses suffixes of the trigram’s center
word, we find this type of feature is too easy for
unrelated trigrams to match, leading to a noisy
graph. Let a trigram and its left/right context be
denoted by the 5-gram (w0, w1, w2, w3, w4). Then
the features we use to build the graph are:

• Trigram features: w12, w13, w23, w2,
suffix(w3)w2, suffix(w1)w2

• Context features: w0134, w012, w023, w024,
w124, w234, w01, w02, w24, w34

where suffix indicates common suffixes collected
from Wiktionary data. For a given feature f and
trigram type t, the value of the feature is deter-
mined by pointwise mutual information (PMI):
log #(f∧t)#(f)#(t) . Then, for each pair of trigram types,
sim(u, v) is given by the cosine similarity of the
trigrams’ feature vectors.

For the second-order CRF, we use a fairly stan-
dard set of features:

• Emission features: 1(yt = k ∧ f(xt′)),
where k can be any POS tag and t′ ∈ {t, t −
1, t + 1}. The f(xt′) takes the form of a
function from the following set: one indica-
tor for each word, lowercased word, and suf-

3In preliminary experiments we tested graphs with 20, 40,
60, 80, and 100 NNs and found that beyond 60 NNs addi-
tional performance gains are small.

43



fix, and also is-capitalized, is-punctuation, is-
digit, contains-hyphen, and contains-period.

• Transition features: For any POS tags
k1, k2, k3, we have a feature 1(yt =
k1, yt−1 = k2, yt+1 = k3) and its backoffs
(indicators for one or two matching tags).

4.3 Handwriting recognition

The handwriting dataset we use was collected by
Kassel (1995) and filtered to 6,877 words (Taskar
et al., 2003). For each word, the first letter is re-
moved so that every remaining letter is one of the
English language’s 26 lowercase letters.

Again, we first build a mutual NN graph. In this
case, we use 20-NN, since our graph has fewer
nodes and a larger set of possible node identi-
ties (26 letters instead of 12 tags). Each node
in this graph is one letter from the dataset, for a
total of 52,152 nodes. As a first step, we com-
pute cosine similarity on the pixels of each pair of
nodes, and then consider only pairs with a similar-
ity greater than 0.3. Next, we apply the Fast Earth
Mover’s distance ÊMD(u, v) (Pele and Werman,
2009) with default parameters to compute the dis-
similarity of each pair of images. We convert these
into similarities via:

s(u, v) = exp

{
−ÊMD(u, v)

σ2EMD

}
(21)

where we set the variance σEMD = 10. The fi-
nal similarity function sim(u, v) is the weighted
combination of the similarity of the nodes (u, v)
and their left neighbors (ul, vl) and right neigh-
bors (ur, vr) from their respective words:

sim(u, v) = αs(u, v)+(1−α)(s(ul, vl)+s(ur, vr))

where we fix α = 0.8.
For the second-order CRF, the transition fea-

tures are same as for POS tagging, but with tags re-
placed by the English alphabet. The emission fea-
tures take a similar form, but with different mean-
ings for the f(xt′) indicator functions. Specifi-
cally, there is one indicator for each pixel loca-
tion, with value 1 if the pixel is turned on. As
there are many more emission than transition fea-
tures, we count the number of fired emission and
transition features, say fe and ft, then discount all
emission features, multiplying them by ftfe to bal-
ance the amount of supervision.

4.4 Baselines
We compare our posterior regularization (PR) re-
sults with three baselines. We also include results
for the first EM iteration of our PR method (PR1),
to show there is still significant optimization oc-
curring after the first iteration.

The first baseline is graph propagation (GP).
Specifically, we start from uniform posteriors for
all the unlabeled nodes in the graph, then for each
tag/letter k and each node v we apply the gradient
update:

qk,v ← qk,v − η
∑

u∈Nk(v)
wkuv(qk,v − qk,u) (22)

until convergence. We then select the tag/letter
with the largest probability as the prediction for
a node. If multiple tokens are mapped to a node,
then all receive the same prediction.

The second baseline incorporates both graph
propagation and sequence information. As a first
step, we run the GP baseline, then use the decod-
ing as additional labeled data to train a second-
order CRF (see GP→CRF results). The third base-
line is simply a second-order CRF, trained on the l
labeled examples.

4.5 Training details
For optimizing the CRF, we use L-BFGS (Bert-
sekas, 2004) and a Gaussian prior with σ = 100
(chosen by cross-validation on the labeled train-
ing examples). The final predictions are obtained
via posterior decoding. For PR, we run EM for
at most 20 iterations, which is enough for con-
vergence of the combined objective J̃ (θ, q). We
cross-validate the constraint strength parameter λ
over the following values: {0.1, 0.5, 1.0, 2.0}, ul-
timately selecting λ = 1 for the POS tagging task
and λ = 0.1 for the handwriting recognition task.

4.6 Results and analysis
POS tagging. For each language, we randomly
sample 1000 labeled examples and split them into
10 non-overlapping training sets of size l = 100.
Figure 1 (a) shows the average error and its stan-
dard deviation for these training sets. If for each
language we take the difference between the aver-
age error of PR and that of the best of the three
baselines, the min, average, and max improve-
ments are: 2.69%, 4.06%, and 5.35%. When
analyzing the results, we observed that one re-
gion where PR makes substantial gains over the

44



EN DE ES PT DA SL SV EL IT NL Avg
0

5

10

15

20

25

Language

P
O

S
 T

a
g

g
in

g
 E

rr
o

r

 

 

GP GP→ CRF CRF PR1 PR

(a)

0 100 200 300 400 500
0

5

10

15

20

# of Labeled Examples

P
o

rt
u

g
u

e
s
e

 T
a

g
g

in
g

 E
rr

o
r

 

 

GP GP→ CRF CRF PR1 PR

(b)

Figure 1: (a): POS results for 10 languages. Each bar in each group corresponds to the average POS
tagging error of one method; the left-to-right order of the methods is the same as in the legend. Whiskers
indicate standard deviations. The final set of bars is an average across all languages. See supplement for
a table with the exact numbers. (b): POS results on one language for a range of l.

CRF baseline is on unseen words (words that do
not occur in the set of l labeled examples). If
we measure performance only on such words, the
gain of PR over CRF is 6.7%. We also test with
l = {50, 100, 150, 200, 300, 400, 500} on one lan-
guage to illustrate how PR performs with different
amounts of supervision. Figure 1 (b) shows that
even when l = 500 our PR method is still able to
provide improvement over the best baseline.

Handwriting recognition. For this task, the
overall dataset contains 55 distinct word types.
Thus, we set l = 110 and sample 10 training
sets such that each contains 2 examples of each
of word. Note that due to the well-balanced train-
ing sets, baselines are fairly high here compared
to other similar work with this dataset. Table 1
shows there is an average improvement of 4.93%
over the best of the three baselines.

GP GP→CRF CRF PR1 PR
Mean 17.57 15.07 9.82 6.03 4.89

StdDev 0.30 0.35 0.48 0.20 0.42

Table 1: Handwriting recognition errors.

Even in a simpler setting closer to that of POS
tagging, where we just draw l = 100 samples ran-
domly, there are many cases where PR beats the
baselines. Figure 2 shows predictions from such
a setting and provides general intuition as to why
PR does well on handwriting recognition. For the
word ‘Wobble’ (with the first letter removed), the
CRF predicts ‘obble’ as ‘ovely’, because of it re-
lies heavily on sequential information; in our small
training set, bigrams ‘ov’ (2 times) and ‘ly’ (12
times) are more frequent than ‘ob’ (1 time) and

‘le’ (7 times). GP correctly predicts these letters
because the graph connects them to good neigh-
bors. However, GP mislabels ‘l’ as ‘i’, since most
of this letter’s neighbors are i’s. The coupling of
GP and CRF via PR links the neighbor informa-
tion with bigram information — ‘bl’ (5 times) is
more frequent than ‘bi’ in the training set — to
yield the correct labeling.

      l    t    l    l    l
      i    i    i    l    i
      l    l    l    l    l 

  CRF   b    b    b    b    m
  GP    b    b    b    b    b
  PR    b    b    b    b    b

CRF  o  v  e  l  y
GP   o  b  b  i  e
PR   o  b  b  l  e  

Figure 2: Predictions on the word ‘Wobble’ and
the 5-NNs of its first ‘b’ and ‘l’.

5 Conclusion

We have presented an efficient extension of the
posterior regularization (PR) framework to a more
general class of penalty functions. Encouraging
results using a graph Laplacian penalty suggest
potential applications to a much larger class of
weakly supervised problems.

Acknowledgements

J. Gillenwater was supported by a National Sci-
ence Foundation Graduate Research Fellowship.
L. He and B. Taskar were partially supported by
ONR Young Investigator Award N000141010746.

45



References
[Altun et al.2005] Y. Altun, D. McAllester, and

M. Belkin. 2005. Maximum Margin Semi-
Supervised Learning for Structured Variables. In
Proc. NIPS.

[Belkin et al.2005] M. Belkin, P. Niyogi, and V. Sind-
hwani. 2005. On Manifold Regularization. In Proc.
AISTATS.

[Bertsekas2004] D. Bertsekas. 2004. Nonlinear Pro-
gramming.

[Buchholz and Marsi2006] S. Buchholz and E. Marsi.
2006. CoNLL-X Shared Task on Multilingual De-
pendency Parsing. In Proc. CoNLL.

[Chapelle et al.2006] O. Chapelle, B. Schölkopf, and
A. Zien, editors. 2006. Semi-Supervised Learning.

[Collins et al.2005] M. Collins, P. Bartlett,
D. McAllester, and B. Taskar. 2005. Expo-
nentiated Gradient Algorithms for Large-Margin
Structured Classification. In Proc. NIPS.

[Collins et al.2008] M. Collins, A. Globerson, T. Koo,
and X. Carreras. 2008. Exponentiated Gradient Al-
gorithms for Conditional Random Fields and Max-
Margin Markov Networks. JMLR.

[Das and Petrov2011] D. Das and S. Petrov. 2011. Un-
supervised Part-of-Speech Tagging with Bilingual
Graph-Based Projections. In Proc. ACL.

[Das et al.2012] D. Das, S. Petrov, and R. McDonald.
2012. A Universal Part-of-Speech Tagset. In Proc.
LREC.

[Ganchev et al.2010] K. Ganchev, J. Graça, J. Gillen-
water, and B. Taskar. 2010. Posterior Regulariza-
tion for Structured Latent Variable Models. JMLR.

[Griewank1988] A. Griewank. 1988. On Automatic
Differentiation. Technical report, Argonne National
Laboratory.

[Joachims2003] T. Joachims. 2003. Transductive
Learning via Spectral Graph Partitioning. In Proc.
ICML.

[Kassel1995] R. Kassel. 1995. A Comparison of Ap-
proaches to On-line Handwritten Character Recog-
nition. Ph.D. thesis, Massachusetts Institute of
Technology.

[Kivinen and Warmuth1995] J. Kivinen and M. War-
muth. 1995. Additive Versus Exponentiated Gra-
dient Updates for Linear Prediction. In Proc. STOC.

[Mann and McCallum2007] G. Mann and A. McCal-
lum. 2007. Simple, Robust, Scalable Semi-
Supervised Learning via Expectation Regulariza-
tion. In Proc. ICML.

[Mann and McCallum2008] G. Mann and A. McCal-
lum. 2008. Generalized Expectation Criteria for
Semi-Supervised Learning of Conditional Random
Fields. In Proc. ACL.

[Marcus et al.1993] M. Marcus, M. Marcinkiewicz, and
B. Santorini. 1993. Building a Large Annotated
Corpus of English: Then Penn Treebank. Compu-
tational Linguistics.

[Nivre et al.2007] J. Nivre, J. Hall, S. Kübler, R. Mc-
Donald, J. Nilsson, S. Riedel, and D. Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Pars-
ing. In Proc. CoNLL.

[Pele and Werman2009] O. Pele and M. Werman.
2009. Fast and Robust Earth Mover’s Distances. In
Proc. ICCV.

[Subramanya and Bilmes2009] A. Subramanya and
J. Bilmes. 2009. Entropic Graph Regularization in
Non-Parametric Semi-Supervised Classification. In
Proc. NIPS.

[Subramanya et al.2010] A. Subramanya, S. Petrov, and
F. Pereira. 2010. Efficient Graph-Based Semi-
Supervised Learning of Structured Tagging Models.
In Proc. EMNLP.

[Taskar et al.2003] B. Taskar, C. Guestrin, and
D. Koller. 2003. Max Margin Markov Networks.
In Proc. NIPS.

[van de Panne and Whinston1964] C. van de Panne and
A. Whinston. 1964. The Simplex and the Dual
Method for Quadratic Programming. Operational
Research Quarterly.

[Zhu and Lafferty2005] X. Zhu and J. Lafferty. 2005.
Harmonic Mixtures: Combining Mixture Models
and Graph-Based Methods for Inductive and Scal-
able Semi-Supervised Learning. In Proc. ICML.

[Zhu et al.2003] X. Zhu, Z. Ghahramani, and J. Laf-
ferty. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. In Proc.
ICML.

[Zhu2005] X. Zhu. 2005. Semi-Supervised Learning
Literature Survey. Technical report, University of
Wisconsin-Madison.

46


