










































Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation

Chris Callison-Burch
Johns Hopkins University
ccb@cs.jhu.edu

Philipp Koehn
University of Edinburgh

pkoehn@inf.ed.ac.uk

Christof Monz
University of Amsterdam
c.monz@uva.nl

Kay Peterson and Mark Przybocki
National Institute of Standards and Technology

kay.peterson,mark.przybocki@nist.gov

Omar F. Zaidan
Johns Hopkins University
ozaidan@cs.jhu.edu

Abstract

This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon’s
Mechanical Turk.

1 Introduction

This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al., 2007; Callison-Burch et al.,
2008; Callison-Burch et al., 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al.,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The

1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.

performance on each of these shared task was de-
termined after a comprehensive human evaluation.

There were a number of differences between
this year’s workshop and last year’s workshop:

• Non-expert judgments – In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon’s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.

• Clearer results for system combination –
This year we excluded Google translations
from the systems used in system combina-
tion. In last year’s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.

• Fewer rule-based systems – This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.

The primary objectives of this workshop are to
evaluate the state of the art in machine transla-

17



tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.

2 Overview of the shared translation and
system combination tasks

The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.

2.1 Test data

The test data for this year’s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3

Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)

French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco

Dias (11)
English: BBC (5), Economist (2), Washington

Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-

gel (4)

The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.

2.2 Training data

As in past years we provided parallel corpora to
train translation models, monolingual corpora to

2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid

tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.

4http://www.ceet.eu/

train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.

2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al., 2007; Li et al.,
2009).

2.4 Submitted systems
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year’s shared task.

We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems’ language models.

2.5 System combination
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year’s system combina-
tion task, we provided two additional resources to
participants:

• Development set: We reserved 25 articles
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.

• n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.

Table 2 lists the 9 participants in the system
combination task.

3 Human evaluation

As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention

18



Europarl Training Corpus

Spanish↔ English French↔ English German↔ English
Sentences 1,650,152 1,683,156 1,540,549

Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464

News Commentary Training Corpus

Spanish↔ English French↔ English German↔ English Czech↔ English
Sentences 98,598 84,624 100,269 94,742

Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306

United Nations Training Corpus

Spanish↔ English French↔ English
Sentences 6,222,450 7,230,217

Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815

109 Word Parallel Corpus

French↔ English
Sentences 22,520,400

Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836

CzEng Training Corpus

Czech↔ English
Sentences 7,227,409

Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770

Europarl Language Model Data

English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217

Distinct words 99,206 178,934 127,689 328,628

News Language Model Data

English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201

Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376

News Test Set

English Spanish French German Czech
Sentences 2489

Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825

Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.

19



ID Participant
AALTO Aalto University, Finland (Virpioja et al., 2010)

CAMBRIDGE Cambridge University (Pino et al., 2010)
CMU Carnegie Mellon University’s Cunei system (Phillips, 2010)

CMU-STATXFER Carnegie Mellon University’s statistical transfer system (Hanneman et al., 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (Žabokrtský et al., 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)

DCU Dublin City University (Penkale et al., 2010)
DFKI Deutsches Forschungszentrum für Künstliche Intelligenz (Federmann et al., 2010)

EU European Parliament, Luxembourg (Jellinghaus et al., 2010)
EUROTRANS commercial MT provider from the Czech Republic

FBK Fondazione Bruno Kessler (Hardmeier et al., 2010)
GENEVA University of Geneva

HUICONG Shanghai Jiao Tong University (Cong et al., 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al., 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al., 2010)

LIMSI LIMSI (Allauzen et al., 2010)
LIU Linköping University (Stymne et al., 2010)

LIUM University of Le Mans (Lambert et al., 2010)
NRC National Research Council Canada (Larkin et al., 2010)

ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University

RALI RALI - Université de Montréal (Huet et al., 2010)
RWTH RWTH Aachen (Heger et al., 2010)

SFU Simon Fraser University (Sankaran et al., 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)

UEDIN University of Edinburgh (Koehn et al., 2010)
UMD University of Maryland (Eidelman et al., 2010)
UPC Universitat Politècnica de Catalunya (Henrı́quez Q. et al., 2010)

UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Politécnica de Valencia (Sanchis-Trilles et al., 2010)

UU-MS Uppsala University - Saers (Saers et al., 2010)

Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.

20



ID Participant
BBN-COMBO BBN system combination (Rosti et al., 2010)

CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)

DCU-COMBO Dublin City University system combination (Du et al., 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)

LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)

UPV-COMBO Universidad Politécnica de Valencia (González-Rubio et al., 2010)

Table 2: Participants in the system combination task.

Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905

Table 3: The number of items that were collected for each task during the manual evaluation. An item
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.

21



that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.

Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour’s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6

We asked people to evaluate the systems’ output
in two different ways:

• Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.

• Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.

The total number of judgments collected for the
different modes of annotation is given in Table 3.

In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.

3.1 Ranking translations of sentences

Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:

Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).

5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P (A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.

6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.

Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.

Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.

3.2 Inter- and Intra-annotator agreement in
the ranking task

We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor’s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.

Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two “local repeat” screens, the
system choices were also preserved.

Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator’s screens repeats of pre-
viously seen sets in the same batch ensured we

7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs’ pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.

22



INTER-ANNOTATOR AGREEMENT
P (A) K

With references 0.658 0.487
Without references 0.626 0.439
WMT ’09 0.549 0.323

INTRA-ANNOTATOR AGREEMENT
P (A) K

With references 0.755 0.633
Without references 0.734 0.601
WMT ’09 0.707 0.561

Table 4: Inter- and intra-annotator agreement for
the sentence ranking task. In this task, P (E) is
0.333.

had enough data to measure intra-annotator agree-
ment.

We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as

K =
P (A)− P (E)

1− P (E)

where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.

For inter-annotator agreement for the ranking
tasks we calculated P (A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed thatA > B, A = B, orA < B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.

Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0− .2 is slight, .2− .4
is fair, .4 − .6 is moderate, .6 − .8 is substantial
and the rest is almost perfect.

Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,

judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.

3.3 Editing machine translation output
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people’s understanding of the out-
put.

The instructions given to our judges were as fol-
lows:

Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select “No corrections
needed.” If you cannot understand the
sentence well enough to correct it, select
“Unable to correct.”

A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year’s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.

Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system’s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system’s output.

3.4 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:

23



Edit Machine Translation Outputs
Instructions:

You are shown several machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.

Your edited translations           The machine translations
   
The shortage of snow in mountain worries the hoteliers

Edited     No corrections needed     Unable to
correct         Reset

 

The shortage of snow in mountain
worries the hoteliers

   
The deserted tracks are not putting down problem only at the exploitants 
of skilift.

Edited     No corrections needed     Unable to
correct         Reset

 

The deserted tracks are not
putting down problem only at the
exploitants of skilift.

   
The lack of snow deters the people to reserving their stays at the ski in 
the hotels and pension.

Edited     No corrections needed     Unable to
correct         Reset

 

The lack of snow deters the people
to reserving their stays at the ski
in the hotels and pension.

   
Thereby, is always possible to track free bedrooms for all the dates in 
winter, including Christmas and Nouvel An.

Edited     No corrections needed     Unable to
correct         Reset

 

Thereby, is always possible to
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.

   
We have many of visit on our site

Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.

24



Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.

In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.

4 Translation task results

We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:

• Which systems produced the best translation
quality for each language pair?

• Did the system combinations produce better
translations than individual systems?

• Which of the systems that used only the pro-
vided training materials produced the best
translation quality?

Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p ≤ 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.

Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs

Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system’s output was under-
standable. Figure 3 gives the percentage of times
that each system’s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).

This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:

• There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.

• The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.

5 Shared evaluation task overview

In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a “Metrics for MAchine TRanslation” chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al.,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.

In this year’s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:

Aalto University of Science and Technology
(Dobrinkat et al., 2010)

• MT-NCD – A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with

25



French-English
551–755 judgments per system

System C? ≥others
LIUM •? Y 0.71
ONLINEB • N 0.71
NRC •? Y 0.66
CAMBRIDGE •? Y +GW 0.66
LIMSI ? Y +GW 0.65
UEDIN Y 0.65
RALI •? Y +GW 0.65
JHU Y 0.59
RWTH •? Y +GW 0.55
LIG Y 0.53
ONLINEA N 0.52
CMU-STATXFER Y 0.51
HUICONG Y 0.51
DFKI N 0.42
GENEVA Y 0.27
CU-ZEMAN Y 0.21

English-French
664–879 judgments per system

System C? ≥others
UEDIN •? Y 0.70
ONLINEB • N 0.68
RALI •? Y +GW 0.66
LIMSI •? Y +GW 0.66
RWTH •? Y +GW 0.63
CAMBRIDGE ? Y +GW 0.63
LIUM Y 0.63
NRC Y 0.62
ONLINEA N 0.55
JHU Y 0.53
DFKI N 0.40
GENEVA Y 0.35
EU N 0.32
CU-ZEMAN Y 0.26
KOC Y 0.26

Czech-English
788–868 judgments per system
System C? ≥others
ONLINEB • N 0.7
UEDIN ? Y 0.61
CMU Y 0.55
CU-BOJAR N 0.55
AALTO Y 0.43
ONLINEA N 0.37
CU-ZEMAN Y 0.22

German-English
723–879 judgments per system

System C? ≥others
ONLINEB • N 0.73
KIT •? Y +GW 0.72
UMD •? Y 0.68
UEDIN ? Y 0.66
FBK ? Y +GW 0.66
ONLINEA • N 0.63
RWTH Y +GW 0.62
LIU Y 0.59
UU-MS Y 0.55
JHU Y 0.53
LIMSI Y +GW 0.52
UPPSALA Y 0.51
DFKI N 0.50
HUICONG Y 0.47
CMU Y 0.46
AALTO Y 0.42
CU-ZEMAN Y 0.36
KOC Y 0.23

English-German
1284–1542 judgments per system

System C? ≥others
ONLINEB • N 0.70
DFKI • N 0.62
UEDIN •? Y 0.62
KIT ? Y 0.60
ONLINEA N 0.59
FBK ? Y 0.56
LIU Y 0.55
RWTH Y 0.51
LIMSI Y 0.51
UPPSALA Y 0.47
JHU Y 0.46
SFU Y 0.34
KOC Y 0.30
CU-ZEMAN Y 0.28

English-Czech
1375–1627 judgments per system

System C? ≥others
ONLINEB • N 0.70
CU-BOJAR • N 0.66
PC-TRANS • N 0.62
UEDIN •? Y 0.62
CU-TECTO Y 0.60
EUROTRANS N 0.54
CU-ZEMAN Y 0.50
SFU Y 0.45
ONLINEA N 0.44
POTSDAM Y 0.44
DCU N 0.38
KOC Y 0.33

Spanish-English
1448–1577 judgments per system

System C? ≥others
ONLINEB • N 0.70
UEDIN •? Y 0.69
CAMBRIDGE Y +GW 0.61
JHU Y 0.61
ONLINEA N 0.54
UPC ? Y 0.51
HUICONG Y 0.50
DFKI N 0.45
COLUMBIA Y 0.45
CU-ZEMAN Y 0.27

English-Spanish
540–722 judgments per system

System C? ≥others
ONLINEB • N 0.71
ONLINEA • N 0.69
UEDIN ? Y 0.61
DCU N 0.61
DFKI ? N 0.55
JHU ? Y 0.55
UPV ? Y 0.55
CAMBRIDGE ? Y +GW 0.54
UHC-UPV ? Y 0.54
SFU Y 0.40
CU-ZEMAN Y 0.23
KOC Y 0.19

Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.

C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC’s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).

• indicates a win in the category, meaning that no other system is statistically significantly better at p-level≤0.1 in pairwise
comparison.

? indicates a constrained win, no other constrained system is statistically better.

For all pairwise comparisons between systems, please check the appendix.

Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-
lations relative to each other)

26



French-English
589–716 judgments per combo

System ≥others
RWTH-COMBO • 0.77
CMU-HYP-COMBO • 0.77
DCU-COMBO • 0.72
LIUM ? 0.71
CMU-HEA-COMBO • 0.70
UPV-COMBO • 0.68
NRC 0.66
CAMBRIDGE 0.66
UEDIN ? 0.65
LIMSI ? 0.65
JHU-COMBO 0.65
RALI 0.65
LIUM-COMBO 0.64
BBN-COMBO 0.64
RWTH 0.55

English-French
740–829 judgments per combo

System ≥others
RWTH-COMBO • 0.75
CMU-HEA-COMBO • 0.74
UEDIN 0.70
KOC-COMBO • 0.68
UPV-COMBO 0.66
RALI ? 0.66
LIMSI 0.66
RWTH 0.63
CAMBRIDGE 0.63

Czech-English
766–843 judgments per combo

System ≥others
CMU-HEA-COMBO • 0.71
ONLINEB ? 0.7
BBN-COMBO • 0.70
RWTH-COMBO • 0.65
UPV-COMBO • 0.63
JHU-COMBO 0.62
UEDIN 0.61

German-English
743–835 judgments per combo

System ≥others
BBN-COMBO • 0.77
RWTH-COMBO • 0.75
CMU-HEA-COMBO 0.73
KIT ? 0.72
UMD ? 0.68
JHU-COMBO 0.67
UEDIN ? 0.66
FBK 0.66
CMU-HYP-COMBO 0.65
UPV-COMBO 0.64
RWTH 0.62
KOC-COMBO 0.59

English-German
1340–1469 judgments per combo

System ≥others
RWTH-COMBO • 0.65
DFKI ? 0.62
UEDIN ? 0.62
KIT ? 0.60
CMU-HEA-COMBO • 0.59
KOC-COMBO 0.59
FBK ? 0.56
UPV-COMBO 0.55

English-Czech
1405–1496 judgments per combo
System ≥others
DCU-COMBO • 0.75
ONLINEB ? 0.70
RWTH-COMBO 0.70
CMU-HEA-COMBO 0.69
UPV-COMBO 0.68
CU-BOJAR 0.66
KOC-COMBO 0.66
PC-TRANS 0.62
UEDIN 0.62

Spanish-English
1385–1535 judgments per combo

System ≥others
UEDIN ? 0.69
CMU-HEA-COMBO • 0.66
UPV-COMBO • 0.66
BBN-COMBO 0.62
JHU-COMBO 0.55
UPC 0.51

English-Spanish
516–673 judgments per combo

System ≥others
CMU-HEA-COMBO • 0.68
KOC-COMBO 0.62
UEDIN ? 0.61
UPV-COMBO 0.60
RWTH-COMBO 0.59
DFKI ? 0.55
JHU 0.55
UPV 0.55
CAMBRIDGE ? 0.54
UPV-NNLM ? 0.54

System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.

• indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level≤0.1 in pairwise comparison.

? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level≤0.1.

For all pairwise comparisons between systems, please check the appendix.

Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,

except in the Czech-English and English-Czech conditions, where ONLINEB was included.

Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)

27



System % Yes Yes count No count N/A count Total count *** en-cz ***
ref 0.97 63 2 0 65 en-cz
dcu-c 0.58 29 21 0 50 en-cz
onlineB 0.55 22 18 0 40 en-cz
rwth-c 0.49 56 59 0 115 en-cz
koc-c 0.45 29 36 0 65 en-cz
pc-trans 0.43 26 34 0 60 en-cz
upv-c 0.42 23 32 0 55 en-cz
cu-bojar 0.4 20 30 0 50 en-cz
eurotrans 0.4 18 27 0 45 en-cz
uedin 0.34 24 46 0 70 en-cz
cu-tecto 0.34 29 55 1 85 en-cz
cmu-hea-c 0.29 13 32 0 45 en-cz
sfu 0.24 14 44 0 58 en-cz
potsdam 0.24 13 42 0 55 en-cz
cu-zeman 0.21 15 55 0 70 en-cz
koc 0.21 21 79 0 100 en-cz
onlineA 0.2 13 52 0 65 en-cz
dcu 0.19 13 57 0 70 en-cz

0.1260077028

*** en-de ***
ref 0.94 47 3 0 50 en-de
onlineA 0.8 20 5 0 25 en-de
koc-c 0.68 17 8 0 25 en-de
uppsala 0.65 26 14 0 40 en-de
uedin 0.62 50 30 0 80 en-de
kit 0.62 37 23 0 60 en-de
upv-c 0.57 30 23 0 53 en-de
onlineB 0.52 21 19 0 40 en-de
dfki 0.52 13 12 0 25 en-de
koc 0.51 18 17 0 35 en-de
limsi 0.51 18 16 1 35 en-de
liu 0.51 28 27 0 55 en-de
rwth 0.5 15 15 0 30 en-de
rwth-c 0.49 22 23 0 45 en-de
jhu 0.48 12 13 0 25 en-de
cmu-hea-c 0.47 14 16 0 30 en-de
fbk 0.4 4 6 0 10 en-de
sfu 0.31 11 24 0 35 en-de
cu-zeman 0.19 10 40 3 53 en-de

0.1364453014

System % Yes Yes count No count N/A count Total count *** en-es ***
ref 0.83 48 10 0 58 en-es
onlineB 0.58 25 18 0 43 en-es
upv 0.5 20 20 0 40 en-es
rwth-c 0.46 13 15 0 28 en-es
dcu 0.42 16 22 0 38 en-es
koc 0.4 17 24 1 42 en-es
upv-nnlm 0.39 15 23 0 38 en-es
onlineA 0.38 11 18 0 29 en-es
jhu 0.38 17 27 1 45 en-es
koc-c 0.38 20 33 0 53 en-es
uedin 0.36 12 21 0 33 en-es
upb-c 0.32 13 27 0 40 en-es
cmu-hea-c 0.32 16 34 0 50 en-es
camb 0.3 12 27 1 40 en-es
dfki 0.29 7 17 0 24 en-es
cu-zeman 0.29 16 39 0 55 en-es
sfu 0.26 9 25 0 34 en-es

0.0845946216

System % Yes Yes count No count N/A count Total count *** en-fr ***
ref 0.91 64 4 2 70 en-fr
rwth-c 0.54 27 23 0 50 en-fr
onlineB 0.52 47 42 1 90 en-fr
upv-c 0.51 34 33 0 67 en-fr
koc-c 0.48 32 34 0 66 en-fr
uedin 0.48 30 32 1 63 en-fr
rali 0.47 21 24 0 45 en-fr
rwth 0.45 25 30 0 55 en-fr
lium 0.43 20 27 0 47 en-fr
camb 0.42 26 36 0 62 en-fr
onlineA 0.41 15 22 0 37 en-fr
limsi 0.37 26 44 0 70 en-fr
jhu 0.37 27 46 0 73 en-fr
nrc 0.36 13 23 0 36 en-fr
cmu-hea-c 0.32 22 47 0 69 en-fr
geneva 0.31 32 70 0 102 en-fr
eu 0.3 13 30 0 43 en-fr
dfki 0.28 16 42 0 58 en-fr
koc 0.21 12 44 1 57 en-fr
cu-zeman 0.17 11 52 0 63 en-fr

0.1045877454

System % Yes Yes count No count N/A count Total count *** cz-en ***
ref 1.00 33 0 0 33 cz-en
cu-bojar 0.6 3 2 0 5 cz-en
upv-c 0.43 15 20 0 35 cz-en
cmu-hea-c 0.35 14 26 0 40 cz-en
rwth-c 0.32 16 34 0 50 cz-en
onlineB 0.3 12 28 0 40 cz-en
bbn-c 0.28 17 43 0 60 cz-en
uedin 0.28 11 28 1 40 cz-en
aalto 0.27 8 22 0 30 cz-en
jhu-c 0.26 13 37 0 50 cz-en
onlineA 0.2 6 24 0 30 cz-en
cmu 0.17 5 25 0 30 cz-en
cu-zeman 0.09 4 40 1 45 cz-en

0.1292958787

System % Yes Yes count No count N/A count Total count *** de-en ***

ref 0.98 44 1 0 45 de-en
umd 0.8 8 2 0 10 de-en
bbn-c 0.67 10 5 0 15 de-en
onlineB 0.65 13 7 0 20 de-en
cmu-hea-c 0.52 12 11 0 23 de-en
jhu-c 0.51 18 17 0 35 de-en
upv-c 0.51 18 16 1 35 de-en
fbk 0.5 20 20 0 40 de-en
uppsala 0.5 20 19 1 40 de-en
limsi 0.46 30 34 1 65 de-en
kit 0.45 18 22 0 40 de-en
liu 0.44 19 24 0 43 de-en
uedin 0.44 11 14 0 25 de-en
dfki 0.4 12 18 0 30 de-en
onlineA 0.4 6 9 0 15 de-en
rwth 0.4 14 21 0 35 de-en
cmu-hyp-c 0.37 11 19 0 30 de-en
huicong 0.36 9 16 0 25 de-en
koc-c 0.36 9 14 2 25 de-en
rwth-c 0.36 10 18 0 28 de-en
koc 0.31 11 23 1 35 de-en
cu-zeman 0.3 12 28 0 40 de-en
uu-ms 0.26 13 37 0 50 de-en
jhu 0.26 9 26 0 35 de-en
cmu 0.24 6 19 0 25 de-en
aalto 0.07 1 14 0 15 de-en

0.1512635669

System % Yes Yes count No count N/A count Total count *** es-en ***
ref 0.98 39 0 1 40 es-en
onlineB 0.71 39 15 1 55 es-en
onlineA 0.64 32 18 0 50 es-en
upv-c 0.6 36 24 0 60 es-en
huicong 0.54 27 23 0 50 es-en
jhu 0.54 35 30 0 65 es-en
cmu-hea-c 0.52 26 23 1 50 es-en
bbn-c 0.51 36 33 1 70 es-en
uedin 0.51 33 30 2 65 es-en
jhu-c 0.47 28 31 1 60 es-en
dfki 0.46 16 18 1 35 es-en
upc 0.43 28 36 1 65 es-en
cu-zeman 0.4 18 26 1 45 es-en
camb 0.36 25 45 0 70 es-en
columbia 0.29 19 46 0 65 es-en

0.1104436607

System % Yes Yes count No count N/A count Total count *** fr-en ***
ref 0.91 32 3 0 35 fr-en
cmu-hyp-c 0.7 21 9 0 30 fr-en
uedin 0.58 23 17 0 40 fr-en
bbn-c 0.56 14 10 1 25 fr-en
rwth-c 0.53 16 14 0 30 fr-en
onlineB 0.51 28 27 0 55 fr-en
camb 0.5 20 19 1 40 fr-en
rali 0.48 31 34 0 65 fr-en
lium 0.46 23 27 0 50 fr-en
dcu-c 0.45 15 16 2 33 fr-en
lig 0.45 9 11 0 20 fr-en
cmu-statxfer 0.44 11 14 0 25 fr-en
nrc 0.43 15 20 0 35 fr-en
dfki 0.4 8 12 0 20 fr-en
jhu 0.4 10 14 1 25 fr-en
jhu-c 0.4 22 30 3 55 fr-en
upv-c 0.4 14 20 1 35 fr-en
lium-c 0.4 27 41 0 68 fr-en
cmu-hea-c 0.35 14 26 0 40 fr-en
limsi 0.35 14 26 0 40 fr-en
onlineA 0.33 20 40 0 60 fr-en
huicong 0.32 13 25 2 40 fr-en
cu-zeman 0.24 6 19 0 25 fr-en
geneva 0.24 6 19 0 25 fr-en
rwth 0.2 1 4 0 5 fr-en

0.1143475506

0

0.25

0.5

0.75

1

re
f

d
cu
-c

on
lin
eB

rw
th
-c

ko
c-
c

p
c-
tr
an
s

up
v-
c

cu
-b
oj
ar

eu
ro
tr
an
s

ue
d
in

cu
-t
ec
to

cm
u-
he
a-
c

sf
u

p
ot
sd
am

cu
-z
em

an ko
c

on
lin
eA d
cu

.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97

English-Czech

0

0.25

0.5

0.75

1

re
f

on
lin
eA

ko
c-
c

up
p
sa
la

ue
d
in ki
t

up
v-
c

on
lin
eB d
fk
i

ko
c

lim
si liu

rw
th

rw
th
-c jh
u

cm
u-
he
a-
c

fb
k

sf
u

cu
-z
em

an

.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94

English-German

0

0.25

0.5

0.75

1

re
f

on
lin
eB up
v

rw
th
-c

d
cu ko
c

up
v-
nn
lm

on
lin
eA jh
u

ko
c-
c

ue
d
in

up
b
-c

cm
u-
he
a-
c

ca
m
b

d
fk
i

cu
-z sf
u

.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83

English-Spanish

0

0.25

0.5

0.75

1

re
f

rw
th
-c

on
lin
eB

up
v-
c

ko
c-
c

ue
d
in ra
li

rw
th

liu
m

ca
m
b

on
lin
eA

lim
si

jh
u

nr
c

cm
u-
he
a-
c

ge
ne
va eu d
fk
i

ko
c

cu
-z
em

an

.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91

English-French

0

0.25

0.5

0.75

1

re
f

um
d

b
b
n-
c

on
lin
eB

cm
u-
he
a-
c

jh
u-
c

up
v-
c

fb
k

up
p
sa
la

lim
si ki
t

liu

ue
d
in

d
fk
i

on
lin
eA

rw
th

cm
u-
hy
p
-c

hu
ic
on
g

ko
c-
c

rw
th
-c

ko
c

cu
-z
em

an
uu
-m

s
jh
u

cm
u

aa
lto

.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98

German-English

0

0.25

0.5

0.75

1

re
f

cu
-b
oj
ar

up
v-
c

cm
u-
he
a-
c

rw
th
-c

on
lin
eB

b
b
n-
c

ue
d
in

aa
lto

jh
u-
c

on
lin
eA

cm
u

cu
-z
em

an

.09.17.2.26.27.28.28.3.32.35.43.61.0

Czech-English

0

0.25

0.5

0.75

1

re
f

on
lin
eB

on
lin
eA

up
v-
c

hu
ic
on
g

jh
u

cm
u-
he
a-
c

b
b
n-
c 45

jh
u-
c

d
fk
i

up
c

cu
-z
em

an
ca
m
b

co
lu
m
b
ia

.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98

Spanish-English

0

0.25

0.5

0.75

1

re
f

cm
u-
hy
p
-c

ue
d
in

b
b
n-
c

rw
th
-c

on
lin
eB

ca
m
b

ra
li

liu
m

d
cu
-c lig

cm
u-
st
at
xf
er nr
c

d
fk
i

jh
u

jh
u-
c

up
v-
c

liu
m
-c

cm
u-
he
a-
c

lim
si

on
lin
eA

hu
ic
on
g

cu
-z
em

an
ge
ne
va

rw
th

.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91

French-English

Figure 3: The percent of time that each system’s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system’s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.

28



a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.

• MT-mNCD – Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR’s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.

Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.

BabbleQuest International8

• Badger 2.0 full – Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.

• Badger 2.0 lite – The lite version uses default
gap, gap extension and substitution costs.

City University of Hong Kong (Wong and Kit,
2010)

• ATEC 2.1 – This version of ATEC extends
the measurement of word choice and word or-
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.

Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.

8http://www.babblequest.com/badger2

Carnegie Mellon University (Denkowski and
Lavie, 2010)

• METEOR-NEXT-adq – Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).

• METEOR-NEXT-hter – METEOR-NEXT
tuned to HTER.

• METEOR-NEXT-rank – METEOR-NEXT
tuned to human judgments of rank.

Columbia University9

• SEPIA – A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.

Charles University Prague (Bojar and Kos,
2010)

• SemPOS – Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.

9http://www1.ccls.columbia.edu/˜SEPIA/

29



• SemPOS-BLEU – A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.

Dublin City University (He et al., 2010)

• DCU-LFG – A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover’s paraphrase
database10.

University of Edinburgh (Birch and Osborne,
2010)

• LRKB4 – A novel metric which directly mea-
sures reordering success using Kendall’s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall’s tau permutation.

• LRHB4 – LRKB4, replacing Kendall’s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.

Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.

Harbin Institute of Technology, China

• I-letter-BLEU – Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.

• I-letter-recall – A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.

10Available at http://www.umiacs.umd.edu/
˜snover/terp/.

• SVM-RANK – Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.

National University of Singapore (Liu et al.,
2010)

• TESLA-M – Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.

• TESLA – Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.

Stanford University

• Stanford – A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.

Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.

University of Maryland11

• TER-plus (TERp) – An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.

11http://www.umiacs.umd.edu/˜snover/
terp

30



Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.

University Politècnica de Catalunya/University
de Barcelona (Comelles et al., 2010)

• DR – An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.

• DRdoc – Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.

• ULCh – An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).

University of Southern California, ISI

• BEwT-E – Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.

• Bkars – Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.

Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.

6 Evaluation task results

The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set along with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.

The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.

The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.

6.1 System Level Metric Scores

The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST’s mteval software12

and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.

Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.

As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.

12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz

31



cz-
en

fr-
en

de-
en

es-
en

avg

SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76

SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75

NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74

MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74

ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73

IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73

meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73

badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73

badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73

SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72

TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70

LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66

MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46

Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.

It is noticeable that system combinations are of-
ten among those achieving the highest scores.

6.2 System-Level Correlations

To assess the performance of the automatic met-
rics, we correlated the metrics’ scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system’s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman’s ρ rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.

Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-

en-
cz

en-
fr

en-
de

en-
es

avg

SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54

LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52

LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49

ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49

Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47

TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46

Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46

NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34

SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29

Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.

lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.

6.3 Segment-Level Metric Analysis

The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0–1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric’s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.

32



Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55

No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of “1” (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.

Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.

The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.

We plan to also analyze metric performance for
translation into English.

7 Feasibility of Using Non-Expert
Annotators in Future WMTs

In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon’s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the “expert” data).

7.1 Data collection

To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5× 8× 600 = 24,000 assignments
been completed, we would have obtained 24,000
× 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.

To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.

When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks’ lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.

13We suspect that newly registered workers on MTurk al-
ready start with an “approval rating” of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.

33



Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18

Table 9: The segment-level performance for metrics for the into-English direction.

en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed ≥ once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)

Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 × 5 × 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.

34



INTER-ANNOTATOR AGREEMENT
P (A) K K∗

With references 0.466 0.198 0.487
Without references 0.441 0.161 0.439

INTRA-ANNOTATOR AGREEMENT
P (A) K K∗

With references 0.539 0.309 0.633
Without references 0.538 0.307 0.601

Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K∗), taken from Table 4.

7.2 Quality of MTurk data

It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.

Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system’s output or tied with
it).

Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker’s labels are comparisons for

14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen’s worth of work had three sets.

which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4–0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.

7.3 Filtering MTurk data by agreement with
experts

We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker’s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P (E) is 0.333, Kexp(w)
ranges between−0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).

We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15

Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of “overfitting”
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.

In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert

15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.

35



-

20

40

60

80

100

120

140

160

0 100 200 300 400 500 600 700

# Workers Removed

M
T
u
rk

 D
a
ta

 R
e
m

a
in

in
g

(%
 o

f 
E

x
p
e
rt

 D
a
ta

)

-

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

0 100 200 300 400 500 600 700

# Workers Removed

A
g
re

e
m

e
n
t 
w

it
h
 E

x
p
e
rt

 D
a
ta

 (
k
a
p
p
a
)

82%

84%

86%

88%

90%

92%

94%

96%

98%

100%

0 100 200 300 400 500 600 700

# Workers Removed

R
e
fe

re
n
c
e
 P

re
fe

re
n
c
e
 R

a
te

0.10

0.15

0.20

0.25

0.30

0 100 200 300 400 500 600 700

# Workers Removed

In
te

r-
A

n
n
o
ta

to
r 

A
g
re

e
m

e
n
t 
(k

a
p
p
a
)

Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).

data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.

7.4 Feasibility of using only MTurk data

In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers’ competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.

We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)

We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (“Unfiltered”).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.

We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (“Voting”) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (“Kexp-filtered”) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method

36



(“RPR-filtered”) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.

The fourth and fifth methods (“Weighted by
Kexp” and “Weighted by K(RPR)”) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Kexp for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.

Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ‘built-in’ properties of that
worker’s own data, without resorting to making
comparisons with other workers or with experts.

8 Summary

As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.

The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants

Unfortunately, fewer rule-based systems partic-
ipated in this year’s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.

This was the first time that the WMT workshop
was held as a joint workshop with NIST’s Metric-
sMATR evaluation initiative. This joint effort was

very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.

This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon’s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.

As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16

Acknowledgments

This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.

References
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-

Kahlout, and Francois Yvon. 2010. Limsi’s statisti-
cal translation systems for wmt’10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29–34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.

Loı̈c Barrault. 2010. Many: Open source mt system
combination at wmt’10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation

16http://www.statmt.org/wmt09/results.
html

37



Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)

en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944

Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
references.) Kexp is the kappa coefficient of the worker’s agreement rate with experts, with P (A) = 0.33.
K(RPR) is the kappa coefficient of the worker’s RPR (see 7.2), with P (A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.

and MetricsMATR, pages 252–256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257–262, Up-
psala, Sweden, July. Association for Computational
Linguistics.

Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263–270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302–307, Uppsala, Sweden, July. Association for
Computational Linguistics.

Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35–41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.

Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.

(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.

Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.

Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon’s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.

Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308–313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42–46, Uppsala, Sweden, July. Association
for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved

38



evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314–
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Marcus Dobrinkat, Tero Tapiovaara, Jaakko Väyrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318–323, Uppsala, Sweden, July. Association for
Computational Linguistics.

Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271–276, Uppsala, Sweden, July. Association for
Computational Linguistics.

Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47–51, Uppsala, Sweden,
July. Association for Computational Linguistics.

Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52–56, Uppsala, Sweden, July. Association
for Computational Linguistics.

Jesús González-Rubio, Germán Sanchis-Trilles, Joan-
Andreu Sánchez, Jesús Andrés-Ferrer, Guillem
Gascó, Pascual Martı́nez-Gómez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277–
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57–62, Uppsala, Sweden, July.
Association for Computational Linguistics.

Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63–67, Uppsala, Sweden, July. Association
for Computational Linguistics.

Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in

wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324–328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68–
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.

Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74–78, Uppsala, Sweden,
July. Association for Computational Linguistics.

Carlos A. Henrı́quez Q., Marta Ruiz Costa-jussà, Vi-
das Daudaravicius, Rafael E. Banchs, and José B.
Mariño. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79–83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt’10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282–285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.

Stéphane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84–90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.

Michael Jellinghaus, Alexandros Poulis, and David
Kolovratnı́k. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91–95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.

Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.

39



Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96–101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102–107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159–174.

Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108–113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.

Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290–
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114–118, Uppsala, Sweden, July.
Association for Computational Linguistics.

Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329–
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation

and MetricsMATR, pages 286–289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119–123, Uppsala, Sweden, July. Association
for Computational Linguistics.

NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.

Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124–129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Aaron Phillips. 2010. The cunei machine translation
platform for wmt ’10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130–135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.

Juan Pino, Gonzalo Iglesias, Adrià de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136–141, Uppsala, Sweden,
July. Association for Computational Linguistics.

Marion Potet, Laurent Besacier, and Hervé Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142–147, Uppsala, Sweden, July. Association
for Computational Linguistics.

Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 “Met-
rics for MAchine TRanslation” challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.

Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296–
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148–152, Uppsala,

40



Sweden, July. Association for Computational Lin-
guistics.

Germán Sanchis-Trilles, Jesús Andrés-Ferrer, Guillem
Gascó, Jesús González-Rubio, Pascual Martı́nez-
Gómez, Martha-Alicia Rocha, Joan-Andreu
Sánchez, and Francisco Casacuberta. 2010.
Upv-prhlt english–spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153–157, Uppsala, Sweden, July. Association for
Computational Linguistics.

Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197–204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158–163, Uppsala, Sweden, July.
Association for Computational Linguistics.

Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164–169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.

Jörg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170–175, Uppsala, Sweden,
July. Association for Computational Linguistics.

Sami Virpioja, Jaakko Väyrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176–181, Uppsala, Sweden,
July. Association for Computational Linguistics.

Zdeněk Žabokrtský, Martin Popel, and David Mareček.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182–187, Up-
psala, Sweden, July. Association for Computational
Linguistics.

Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335–
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Francisco Zamora-Martinez and Germán Sanchis-
Trilles. 2010. Uch-upv english–spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188–192, Uppsala, Sweden, July.
Association for Computational Linguistics.

Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193–196, Uppsala, Sweden, July. Association
for Computational Linguistics.

41



A Pairwise system comparisons by human judges

Tables 13–20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables’ cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.

Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ≤ 0.10, † indicates statistical significance at p ≤ 0.05, and ‡ indicates statistical
significance at p ≤ 0.01, according to the Sign Test.

B Automatic scores

The tables on pages 33–32 give the automatic scores for each of the systems.

C Pairwise system comparisons for combined expert and non-expert data

Tables 21–20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.

The number of pairwise comparisons at the ? level of significance increases from 48 to 50, and the
number at the † level of significants increases from 79 to 80 (basically same number). However, the
‡ level of significance went up considerably, from 280 to 369. That’s a 31% increase. 75 of ‡ are
comparisons involving the reference, then the non-reference ‡ count went up from 205 to 294, a 43%
increase.

R
E

F

C
A

M
B

R
ID

G
E

C
M

U
-S

TA
T

X
F

E
R

C
U

-Z
E

M
A

N

D
F

K
I

G
E

N
E

V
A

H
U

IC
O

N
G

JH
U

L
IG

L
IM

S
I

L
IU

M

N
R

C

O
N

L
IN

E
A

O
N

L
IN

E
B

R
A

L
I

R
W

T
H

U
E

D
IN

B
B

N
-C

O
M

B
O

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

C
M

U
-H

Y
P

O
S

E
L

-C
O

M
B

O

D
C

U
-C

O
M

B
O

JH
U

-C
O

M
B

O

L
IU

M
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .00‡ .00‡ .00‡ .00‡ .00‡ .04‡ .03‡ .00‡ .00‡ .00‡ .00‡ .04‡ .00‡ .04‡ .00‡ .00‡ .00‡ .00‡ .05‡ .06‡ .03‡ .09‡ .04‡ .04‡

CAMBRIDGE .79‡ – .36 .16‡ .12‡ .23† .27 .43 .26† .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59? .44
CMU-STATXFER .84‡ .58 – .16‡ .48 .14‡ .19 .39 .33 .54 .54? .50† .36 .50 .70‡ .55? .50 .46 .58† .67† .50 .56† .48 .58‡ .52†

CU-ZEMAN 1.00‡ .77‡ .72‡ – .76‡ .37 .73‡ .74‡ .79‡ .77‡ .77‡ .81‡ .75‡ .94‡ .86‡ .77‡ .89‡ .67 .77‡ .79‡ .81‡ .81‡ .77‡ .96‡ .86‡

DFKI 1.00‡ .72‡ .45 .12‡ – .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61? .50 .68† .73‡ .70† .60 .59? .72‡ .71‡

GENEVA 1.00‡ .69† .76‡ .48 .56 – .47 .71† .79‡ .72† .79‡ .71† .68† .76‡ .83‡ .57 .86‡ .72‡ .71† .69† .76† .65‡ .88‡ .96‡ .70
HUICONG .86‡ .54 .29 .12‡ .26 .37 – .48 .31 .43 .63‡ .62† .53 .55 .53‡ .44 .50 .55 .52 .68‡ .52? .51 .52? .57 .53

JHU .83‡ .39 .42 .13‡ .33 .19† .3 – .3 .36 .56† .56? .47 .52 .46 .29 .36 .42 .42 .59† .50 .31 .43 .29 .37
LIG .97‡ .63† .36 .15‡ .37 .18‡ .40 .60 – .62? .57‡ .39 .35 .54† .46 .33 .34 .38 .54† .48? .42 .44 .50 .61? .56

LIMSI .96‡ .41 .23 .19‡ .31 .17† .32 .50 .28? – .35 .42 .21 .62‡ .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83‡ .33 .21? .13‡ .41 .05‡ .13‡ .15† .09‡ .3 – .39 .19 .36 .43 .26 .23† .28 .29 .45 .28 .26 .28 .33 .28

NRC .96‡ .3 .10† .10‡ .32 .24† .15† .22? .22 .33 .43 – .26 .58 .26 .24 .3 .50 .36 .45 .47† .23 .38 .36† .35
ONLINEA .96‡ .55 .57 .14‡ .42 .16† .42 .4 .39 .53 .52 .47 – .52? .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87‡ .37 .33 .03‡ .29 .12‡ .31 .26 .16† .12‡ .39 .35 .20? – .33 .38 .17† .36 .29 .21 .33 .3 .3 .32 .21‡

RALI .89‡ .45 .15‡ .06‡ .35 .04‡ .12‡ .42 .35 .46 .32 .42 .39 .52 – .32 .31 .26 .43 .41 .27 .43 .40 .63? .26
RWTH .91‡ .46 .21? .05‡ .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 – .39 .38 .39 .52 .46 .53† .52 .50‡ .25

UEDIN .96‡ .40 .33 .03‡ .28? .03‡ .28 .29 .49 .38 .61† .3 .32 .50† .34 .24 – .42 .33 .43 .48 .18? .13 .27 .38
BBN-C .90‡ .48 .46 .29 .39 .22‡ .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 – .28 .44† .33 .26 .62? .36 .28

CMU-HEA-C .89‡ .50 .23† .14‡ .30† .21† .26 .25 .17† .33 .43 .16 .36 .43 .26 .29 .24 .24 – .48 .27 .13 .25 .30 .15
CMU-HYP-C .81‡ .17 .19† .11‡ .19‡ .19† .14‡ .14† .19? .40 .23 .18 .29 .46 .35 .29 .21 .15† .17 – .26 .18 .07‡ .32 .21

DCU-C .88‡ .27 .25 .11‡ .22† .24† .20? .28 .21 .35 .50 .10† .31 .44 .27 .29 .22 .21 .2 .30 – .12? .26 .26 .08
JHU-C .86‡ .48 .16† .16‡ .33 .21‡ .35 .41 .32 .44 .39 .35 .39 .37 .26 .19† .50? .23 .32 .43 .40? – .36 .27 .39

LIUM-C .87‡ .41 .36 .13‡ .31? .08‡ .21? .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27? .25 .67‡ .26 .44 – .54‡ .48
RWTH-C .88‡ .18? .13‡ .04‡ .22‡ .04‡ .14 .24 .25? .3 .33 .05† .43 .50 .30? .13‡ .23 .14 .18 .21 .19 .23 .11‡ – .24

UPV-C .92‡ .25 .12† .10‡ .16‡ .3 .25 .34 .29 .31 .34 .29 .39 .65‡ .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 –
> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37

>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68

Table 13: Sentence-level ranking for the WMT10 French-English News Task

42



R
E

F

C
A

M
B

R
ID

G
E

C
U

-Z
E

M
A

N

D
F

K
I

E
U

G
E

N
E

V
A

JH
U

K
O

C

L
IM

S
I

L
IU

M

N
R

C

O
N

L
IN

E
A

O
N

L
IN

E
B

R
A

L
I

R
W

T
H

U
E

D
IN

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

K
O

C
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .08‡ .02‡ .00‡ .04‡ .08‡ .13‡ .06‡ .09‡ .09‡ .07‡ .16‡ .11‡ .12‡ .12‡ .12‡ .05‡ .07‡ .08‡ .09‡

CAMBRIDGE .82‡ – .16‡ .24† .15‡ .07‡ .35 .10‡ .42 .36 .43 .27 .67‡ .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98‡ .82‡ – .47 .54? .62‡ .71‡ .41 .79‡ .82‡ .70‡ .67‡ .85‡ .90‡ .75‡ .72‡ .92‡ .82‡ .88‡ .82‡

DFKI .95‡ .66† .31 – .46 .25? .78‡ .36 .59 .62? .75‡ .65† .45 .56? .75‡ .69‡ .71‡ .63? .57 .65†

EU .96‡ .78‡ .30? .41 – .55 .68‡ .16‡ .76‡ .72‡ .82‡ .67‡ .63‡ .86‡ .78‡ .78‡ .76‡ .76‡ .75‡ .71‡

GENEVA .86‡ .81‡ .23‡ .55? .34 – .65‡ .25‡ .65† .70‡ .69‡ .66‡ .77‡ .71‡ .70‡ .89‡ .75‡ .63† .84‡ .75‡

JHU .77‡ .42 .15‡ .22‡ .22‡ .22‡ – .06‡ .58? .47 .52† .49 .70‡ .61† .53 .64‡ .53? .65‡ .68‡ .50
KOC .85‡ .67‡ .4 .58 .55‡ .69‡ .82‡ – .76‡ .85‡ .81‡ .72‡ .86‡ .82‡ .86‡ .85‡ .77‡ .77‡ .74‡ .79‡

LIMSI .84‡ .23 .08‡ .29 .09‡ .30† .21? .08‡ – .33 .37 .17‡ .51 .40 .29 .45 .49 .40 .61‡ .28
LIUM .85‡ .39 .07‡ .32? .11‡ .21‡ .44 .07‡ .46 – .44 .4 .32 .44 .37 .64† .35 .40 .35 .42

NRC .91‡ .43 .15‡ .20‡ .11‡ .25‡ .21† .09‡ .31 .45 – .32 .48 .44 .49 .61† .52† .30 .58? .40
ONLINEA .80‡ .51 .21‡ .33† .23‡ .15‡ .41 .14‡ .60‡ .42 .54 – .52? .56? .36 .67‡ .61‡ .45 .50 .44
ONLINEB .87‡ .23‡ .08‡ .43 .23‡ .11‡ .12‡ .08‡ .27 .36 .43 .25? – .38 .31 .33 .52 .33? .46 .29

RALI .83‡ .38 .05‡ .27? .11‡ .15‡ .22† .10‡ .36 .44 .49 .31? .50 – .38 .44 .42 .37 .38 .34
RWTH .76‡ .33 .11‡ .12‡ .15‡ .17‡ .34 .05‡ .34 .44 .29 .42 .49 .40 – .56 .48 .44 .53‡ .50

UEDIN .84‡ .29 .20‡ .17‡ .12‡ .09‡ .19‡ .07‡ .33 .23† .24† .24‡ .56 .31 .3 – .36? .27 .51 .18†

CMU-HEAFIELD-COMBO .90‡ .23 .04‡ .23‡ .18‡ .12‡ .22? .11‡ .32 .41 .20† .23‡ .28 .31 .31 .11? – .29 .24 .3
KOC-COMBO .91‡ .26 .08‡ .31? .17‡ .28† .20‡ .07‡ .23 .26 .19 .36 .57? .37 .32 .32 .42 – .38 .34

RWTH-COMBO .85‡ .21? .02‡ .36 .16‡ .07‡ .12‡ .07‡ .16‡ .3 .30? .4 .34 .32 .06‡ .26 .35 .16 – .21?

UPV-COMBO .87‡ .38 .08‡ .30† .19‡ .19‡ .37 .11‡ .39 .24 .33 .37 .44 .27 .34 .46† .35 .28 .50? –
> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45

>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66

Table 14: Sentence-level ranking for the WMT10 English-French News Task

43



R
E

F

A
A

LT
O

C
M

U

C
U

-Z
E

M
A

N

D
F

K
I

F
B

K

H
U

IC
O

N
G

JH
U

K
IT

K
O

C

L
IM

S
I

L
IU

O
N

L
IN

E
A

O
N

L
IN

E
B

R
W

T
H

U
E

D
IN

U
M

D

U
P

P
S

A
L

A

U
U

-M
S

B
B

N
-C

O
M

B
O

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

C
M

U
-H

Y
P

O
S

E
L

-C
O

M
B

O

JH
U

-C
O

M
B

O

K
O

C
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .00‡ .03‡ .00‡ .06‡ .03‡ .00‡ .00‡ .05‡ .00‡ .00‡ .03‡ .06‡ .09‡ .06‡ .00‡ .09‡ .03‡ .03‡ .14‡ .03‡ .06‡ .03‡ .03‡ .06‡ .00‡

AALTO 1.00‡ – .50 .31 .60 .69‡ .39 .41 .71† .31 .45 .60‡ .59† .65‡ .66‡ .64‡ .81‡ .45 .41 .69† .72‡ .75† .55 .55‡ .76‡ .57†

CMU .93‡ .31 – .29 .49 .57‡ .38 .50 .74‡ .13‡ .44 .59‡ .57† .59? .60† .67† .59‡ .41 .50 .68‡ .67‡ .46 .64‡ .55? .67‡ .54?

CU-ZEMAN 1.00‡ .44 .56 – .58 .64‡ .17 .44 .75‡ .38 .50 .54† .76† .79‡ .73‡ .72‡ .72‡ .50? .73‡ .78‡ .80‡ .68‡ .72† .62† .68? .73‡

DFKI .92‡ .25 .32 .27 – .53 .36 .46 .65? .07‡ .50 .47 .47 .69‡ .56 .35 .55 .58 .47 .67† .61? .52 .47 .38 .67† .51
FBK .97‡ .20‡ .16‡ .14‡ .38 – .11‡ .31 .45 .10‡ .22? .36 .50 .57† .37 .43 .40 .12‡ .17† .48? .43 .35 .38 .22 .38 .39

HUICONG .93‡ .35 .28 .46 .43 .75‡ – .52 .69† .16† .39 .42 .64† .79‡ .31 .51† .78‡ .27 .41 .49 .74‡ .68‡ .60? .37 .68‡ .56†

JHU .86‡ .34 .29 .16 .43 .31 .26 – .61‡ .15‡ .35 .36 .45 .69‡ .52? .56? .64† .27 .36 .70‡ .53 .47 .66‡ .52 .68‡ .44
KIT .89‡ .21† .10‡ .14‡ .29? .33 .19† .14‡ – .03‡ .27 .21† .36 .46 .17‡ .29 .24 .25‡ .25‡ .48 .23? .31 .38 .2 .36 .12‡

KOC .96‡ .58 .77‡ .48 .70‡ .77‡ .58† .71‡ .97‡ – .77‡ .90‡ .72‡ .82‡ .76‡ .84‡ .81‡ .84‡ .66‡ .83‡ .87‡ .79‡ .77‡ .75‡ .93‡ .71‡

LIMSI 1.00‡ .23 .28 .35 .35 .53? .33 .45 .41 .19‡ – .49 .48 .63† .49 .63‡ .52 .36 .29 .73‡ .53? .45 .59‡ .29 .56† .59†

LIU .88‡ .12‡ .15‡ .16† .39 .21 .46 .36 .61† .00‡ .27 – .44 .63† .49 .45 .53 .27? .33 .67‡ .55? .46 .44 .32 .37 .55
ONLINEA .92‡ .15† .23† .24† .42 .34 .21† .35 .50 .10‡ .32 .36 – .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68‡ .18‡ .29? .17‡ .26‡ .24† .18‡ .23‡ .33 .18‡ .23† .27† .34 – .3 .15‡ .29 .24† .15‡ .44 .28 .33? .20† .21‡ .38 .3

RWTH .88‡ .17‡ .20† .20‡ .37 .49 .41 .23? .61‡ .16‡ .4 .3 .43 .56 – .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89‡ .14‡ .22† .13‡ .62 .34 .18† .22? .39 .03‡ .17‡ .3 .44 .67‡ .42 – .39 .15‡ .14‡ .52? .40 .36 .43 .26 .41 .38

UMD .91‡ .07‡ .14‡ .08‡ .36 .34 .11‡ .25† .48 .16‡ .24 .34 .52 .56 .41 .45 – .16‡ .21† .41 .28 .29 .43 .29 .25 .23
UPPSALA .97‡ .32 .34 .17? .36 .54‡ .23 .37 .70‡ .00‡ .41 .62? .56 .68† .57 .64‡ .59‡ – .2 .63‡ .69‡ .51‡ .60? .33 .69‡ .63‡

UU-MS .82‡ .22 .43 .14‡ .45 .51† .19 .21 .68‡ .14‡ .39 .52 .60 .64‡ .44 .53‡ .61† .28 – .36 .58‡ .52? .53? .30 .64‡ .44
BBN-C .86‡ .25† .10‡ .07‡ .27† .17? .23 .18‡ .35 .07‡ .15‡ .12‡ .32 .41 .3 .19? .22 .15‡ .27 – .39 .06† .23? .11‡ .21 .18†

CMU-HEA-C .87‡ .14‡ .15‡ .08‡ .29? .33 .04‡ .26 .53? .00‡ .20? .24? .44 .31 .46 .23 .53 .15‡ .13‡ .27 – .40 .2 .14‡ .22 .28
CMU-HYP-C .94‡ .25† .24 .14‡ .44 .3 .15‡ .26 .47 .08‡ .45 .31 .42 .67? .24 .36 .46 .14‡ .21? .50† .32 – .43 .28 .51? .42

JHU-C .97‡ .34 .11‡ .20† .29 .34 .29? .03‡ .38 .12‡ .07‡ .29 .55 .67† .34 .32 .23 .24? .24? .48? .40 .32 – .27 .37 .31
KOC-C .88‡ .00‡ .23? .21† .53 .44 .29 .22 .43 .08‡ .36 .50 .53 .63‡ .39 .37 .39 .28 .19 .64‡ .61‡ .38 .55 – .48? .46

RWTH-C .82‡ .09‡ .06‡ .29? .25† .25 .18‡ .18‡ .24 .03‡ .19† .26 .36 .54 .25 .26 .33 .06‡ .14‡ .29 .22 .23? .3 .17? – .13‡

UPV-C .97‡ .17† .21? .17‡ .36 .36 .23† .19 .67‡ .20‡ .18† .29 .41 .40 .40 .38 .48 .17‡ .31 .50† .43 .27 .27 .27 .65‡ –
> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41

>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64

Table 15: Sentence-level ranking for the WMT10 German-English News Task

R
E

F

C
U

-Z
E

M
A

N

D
F

K
I

F
B

K

JH
U

K
IT

K
O

C

L
IM

S
I

L
IU

O
N

L
IN

E
A

O
N

L
IN

E
B

R
W

T
H

S
F

U

U
E

D
IN

U
P

P
S

A
L

A

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

K
O

C
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .03‡ .06‡ .01‡ .02‡ .05‡ .00‡ .00‡ .01‡ .04‡ .03‡ .01‡ .01‡ .01‡ .02‡ .01‡ .01‡ .05‡ .06‡

CU-ZEMAN .97‡ – .85‡ .67‡ .62‡ .78‡ .58? .70‡ .64‡ .80‡ .85‡ .64‡ .52 .80‡ .61† .79‡ .69‡ .76‡ .73‡

DFKI .89‡ .14‡ – .36† .24‡ .38 .30‡ .27‡ .36? .36? .55 .35† .21‡ .41 .39 .46 .38? .47 .37?

FBK .97‡ .30‡ .59† – .35† .42 .12‡ .36 .48 .48 .64‡ .39 .29‡ .46 .30† .44 .46 .48 .38
JHU .98‡ .27‡ .72‡ .57† – .59‡ .30‡ .51 .53 .56? .65‡ .43 .39 .66‡ .45 .56 .61‡ .52 .47
KIT .92‡ .18‡ .55 .42 .29‡ – .23‡ .32 .32† .43 .53? .41 .27‡ .43 .23‡ .41 .41 .42 .37

KOC 1.00‡ .37? .64‡ .82‡ .62‡ .70‡ – .74‡ .74‡ .74‡ .82‡ .63‡ .48 .62† .65‡ .73‡ .67‡ .81‡ .71‡

LIMSI .95‡ .27‡ .68‡ .39 .45 .49 .17‡ – .49 .74‡ .70‡ .51 .28‡ .58‡ .32 .51 .53? .52† .31
LIU .95‡ .32‡ .59? .4 .36 .58† .21‡ .37 – .39 .74‡ .33? .23‡ .55† .36? .49 .42 .46 .38

ONLINEA .95‡ .16‡ .55? .4 .36? .45 .21‡ .23‡ .50 – .56† .38 .23‡ .41 .23‡ .48 .4 .50 .33†

ONLINEB .92‡ .12‡ .42 .26‡ .27‡ .33? .14‡ .23‡ .21‡ .32† – .24‡ .14‡ .39 .19‡ .29‡ .27‡ .36 .32‡

RWTH .98‡ .33‡ .61† .51 .47 .46 .30‡ .33 .52? .55 .71‡ – .33† .57? .45 .40 .51† .47 .46
SFU .98‡ .42 .77‡ .66‡ .51 .69‡ .48 .68‡ .69‡ .72‡ .77‡ .56† – .82‡ .53 .65‡ .69‡ .73‡ .62‡

UEDIN .94‡ .17‡ .51 .4 .31‡ .49 .34† .25‡ .30† .52 .52 .36? .10‡ – .33? .31 .42 .38 .22‡

UPPSALA .97‡ .36† .55 .51† .47 .70‡ .25‡ .46 .57? .67‡ .71‡ .41 .38 .54? – .53† .42 .58‡ .40
CMU-HEAFIELD-COMBO .96‡ .17‡ .49 .36 .36 .37 .21‡ .35 .49 .42 .64‡ .38 .28‡ .48 .28† – .35 .46 .35

KOC-COMBO .99‡ .27‡ .56? .32 .27‡ .32 .23‡ .32? .41 .55 .64‡ .30† .21‡ .37 .36 .41 – .34 .36
RWTH-COMBO .92‡ .17‡ .50 .34 .35 .41 .09‡ .25† .38 .4 .54 .38 .20‡ .42 .19‡ .28 .35 – .16‡

UPV-COMBO .93‡ .23‡ .58? .38 .36 .51 .23‡ .50 .49 .57† .60‡ .42 .28‡ .51‡ .3 .38 .46 .48‡ –
> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39

>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55

Table 16: Sentence-level ranking for the WMT10 English-German News Task

44



R
E

F

C
A

M
B

R
ID

G
E

C
O

L
U

M
B

IA

C
U

-Z
E

M
A

N

D
F

K
I

H
U

IC
O

N
G

JH
U

O
N

L
IN

E
A

O
N

L
IN

E
B

U
E

D
IN

U
P

C

B
B

N
-C

O
M

B
O

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

JH
U

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .00‡ .01‡ .01‡ .01‡ .00‡ .00‡ .00‡ .00‡ .00‡ .01‡ .02‡ .05‡ .01‡ .04‡

CAMBRIDGE .95‡ – .23‡ .14‡ .34? .31† .41 .34 .62‡ .45? .35 .40? .42 .22† .44
COLUMBIA .97‡ .58‡ – .25‡ .52 .45 .59‡ .53? .65‡ .60‡ .47 .56‡ .55‡ .45 .58‡

CU-ZEMAN .96‡ .71‡ .59‡ – .60‡ .68‡ .79‡ .66‡ .75‡ .80‡ .66‡ .79‡ .78‡ .69‡ .75‡

DFKI .97‡ .51? .37 .23‡ – .43 .59‡ .52† .66‡ .62‡ .48 .53† .55† .55† .64‡

HUICONG .95‡ .50† .34 .21‡ .41 – .45 .50 .66‡ .61‡ .39 .50? .59‡ .40 .52‡

JHU .98‡ .39 .22‡ .12‡ .30‡ .33 – .37 .56‡ .51‡ .34 .39 .34† .22‡ .34
ONLINEA .96‡ .46 .37? .23‡ .32† .38 .44 – .59‡ .53† .4 .50 .36 .30† .54‡

ONLINEB .88‡ .25‡ .21‡ .16‡ .23‡ .21‡ .27‡ .23‡ – .35 .24‡ .28‡ .34† .22‡ .36
UEDIN .96‡ .31? .28‡ .10‡ .25‡ .19‡ .25‡ .31† .48 – .23‡ .27† .31 .23‡ .2

UPC .94‡ .47 .4 .20‡ .41 .33 .43 .46 .66‡ .56‡ – .50? .52† .48? .49†

BBN-COMBO .95‡ .26? .31‡ .09‡ .32† .34? .33 .37 .54‡ .44† .33? – .35 .24‡ .34
CMU-HEAFIELD-COMBO .91‡ .39 .21‡ .08‡ .34† .22‡ .16† .42 .57† .45 .31† .31 – .14‡ .27

JHU-COMBO .95‡ .40† .32 .15‡ .36† .31 .44‡ .50† .66‡ .50‡ .32? .47‡ .43‡ – .43†

UPV-COMBO .92‡ .35 .28‡ .16‡ .27‡ .23‡ .38 .28‡ .47 .30 .28† .26 .35 .25† –
> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43

>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66

Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task

R
E

F

C
A

M
B

R
ID

G
E

C
U

-Z
E

M
A

N

D
C

U

D
F

K
I

JH
U

K
O

C

O
N

L
IN

E
A

O
N

L
IN

E
B

S
F

U

U
E

D
IN

U
P

V

U
C

H
-U

P
V

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

K
O

C
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .00‡ .02‡ .07‡ .15‡ .07‡ .02‡ .11‡ .14‡ .07‡ .07‡ .03‡ .06‡ .09‡ .06‡ .03‡ .07‡

CAMBRIDGE .91‡ – .28† .45 .38 .45 .11‡ .52 .61† .21? .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95‡ .70† – .79‡ .75‡ .85‡ .49 .83‡ .82‡ .74‡ .87‡ .67‡ .85‡ .81‡ .80‡ .70‡ .74‡

DCU .93‡ .32 .21‡ – .45 .32 .09‡ .70† .59 .24‡ .48 .38 .29 .32 .36 .24 .14‡

DFKI .80‡ .41 .15‡ .45 – .38 .12‡ .64† .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90‡ .37 .10‡ .52 .56 – .17‡ .67† .67‡ .26† .34 .3 .49 .54 .53† .47 .35
KOC .98‡ .87‡ .47 .88‡ .73‡ .76‡ – .76‡ .87‡ .67‡ .83‡ .86‡ .90‡ .87‡ .90‡ .86‡ .86‡

ONLINEA .82‡ .42 .08‡ .30† .18† .24† .20‡ – .49 .36 .25† .17‡ .25† .45 .30? .29 .18‡

ONLINEB .76‡ .26† .10‡ .32 .37 .22‡ .10‡ .34 – .21‡ .28 .24† .32 .33 .22‡ .19‡ .27?

SFU .91‡ .54? .19‡ .67‡ .51 .63† .27‡ .64 .72‡ – .74‡ .57? .68‡ .77‡ .71‡ .64‡ .46
UEDIN .91‡ .3 .08‡ .4 .38 .34 .14‡ .71† .49 .09‡ – .34 .4 .58 .33 .3 .31

UPV .94‡ .34 .07‡ .41 .53 .54 .07‡ .73‡ .61† .27? .45 – .37 .51 .44 .38 .48†

UCH-UPV .90‡ .55 .07‡ .58 .51 .41 .08‡ .69† .52 .24‡ .51 .46 – .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83‡ .29 .13‡ .37 .38 .35 .07‡ .48 .54 .08‡ .29 .26 .28 – .17† .21? .21

KOC-COMBO .88‡ .27 .15‡ .40 .42 .24† .03‡ .62? .60‡ .15‡ .41 .27 .34 .53† – .3 .40
RWTH-COMBO .92‡ .36 .21‡ .52 .33 .31 .10‡ .55 .65‡ .14‡ .37 .22 .41 .52? .48 – .31

UPV-COMBO .91‡ .32 .13‡ .69‡ .4 .32 .09‡ .76‡ .52? .36 .38 .19† .31 .45 .35 .28 –
> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40

>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60

Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task

45



R
E

F

A
A

LT
O

C
M

U

C
U

-B
O

JA
R

C
U

-Z
E

M
A

N

O
N

L
IN

E
A

O
N

L
IN

E
B

U
E

D
IN

B
B

N
-C

O
M

B
O

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

JH
U

-C
O

M
B

O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .04‡ .02‡ .03‡ .00‡ .02‡ .00‡ .03‡ .03‡ .04‡ .01‡ .04‡ .02‡

AALTO .88‡ – .49 .51 .22‡ .38 .64‡ .55† .57? .71‡ .64‡ .65‡ .59‡

CMU .97‡ .35 – .4 .14‡ .18‡ .59‡ .49† .45† .57‡ .50‡ .34 .43
CU-BOJAR .90‡ .33 .43 – .12‡ .20‡ .64‡ .45 .45 .54‡ .42 .42 .41

CU-ZEMAN .99‡ .60‡ .77‡ .75‡ – .56† .81‡ .78‡ .88‡ .79‡ .84‡ .84‡ .76‡

ONLINEA .92‡ .46 .68‡ .59‡ .28† – .65‡ .54‡ .72‡ .75‡ .58‡ .57‡ .66‡

ONLINEB .97‡ .27‡ .28‡ .21‡ .10‡ .17‡ – .25† .32 .22 .21† .32 .28
UEDIN .95‡ .28† .26† .38 .07‡ .22‡ .49† – .60‡ .52‡ .33 .31 .32

BBN-COMBO .92‡ .31? .20† .39 .08‡ .15‡ .41 .16‡ – .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90‡ .13‡ .23‡ .25‡ .07‡ .15‡ .31 .23‡ .34 – .18‡ .35 .28

JHU-COMBO .93‡ .20‡ .19‡ .33 .08‡ .25‡ .48† .39 .38 .52‡ – .37 .42
RWTH-COMBO .92‡ .18‡ .37 .38 .13‡ .25‡ .34 .28 .43 .40 .26 – .25

UPV-COMBO .96‡ .25‡ .36 .41 .11‡ .27‡ .45 .35 .37 .44 .31 .34 –
> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40

>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63

Table 19: Sentence-level ranking for the WMT10 Czech-English News Task

R
E

F

C
U

-B
O

JA
R

C
U

-T
E

C
T

O

C
U

-Z
E

M
A

N

D
C

U

E
U

R
O

T
R

A
N

S

K
O

C

O
N

L
IN

E
A

O
N

L
IN

E
B

P
C

-T
R

A
N

S

P
O

T
S

D
A

M

S
F

U

U
E

D
IN

C
M

U
-H

E
A

FI
E

L
D

-C
O

M
B

O

D
C

U
-C

O
M

B
O

K
O

C
-C

O
M

B
O

R
W

T
H

-C
O

M
B

O

U
P

V
-C

O
M

B
O

REF – .04‡ .04‡ .03‡ .01‡ .05‡ .03‡ .08‡ .04‡ .04‡ .03‡ .02‡ .02‡ .04‡ .08‡ .04‡ .07‡ .04‡

CU-BOJAR .87‡ – .46 .27‡ .12‡ .28‡ .16‡ .17‡ .44 .4 .11‡ .27‡ .41 .28 .52‡ .28 .42 .43
CU-TECTO .88‡ .36 – .30† .23‡ .38 .17‡ .28‡ .56† .44 .29† .27‡ .36 .45 .51† .4 .58† .35

CU-ZEMAN .91‡ .58‡ .51† – .38 .49 .19‡ .39 .62‡ .63‡ .36 .41 .48 .51‡ .58‡ .48† .54† .55‡

DCU .98‡ .73‡ .52‡ .43 – .59‡ .22‡ .47 .74‡ .63‡ .47† .53† .56‡ .77‡ .77‡ .62‡ .76‡ .71‡

EUROTRANS .88‡ .61‡ .47 .33 .30‡ – .10‡ .33 .51 .54† .25‡ .27‡ .49 .57‡ .59† .49 .57‡ .60‡

KOC .93‡ .69‡ .67‡ .54‡ .49‡ .77‡ – .54‡ .71‡ .70‡ .51‡ .55‡ .64‡ .72‡ .78‡ .65‡ .76‡ .78‡

ONLINEA .91‡ .62‡ .57‡ .51 .39 .44 .24‡ – .66‡ .62‡ .39 .43 .55‡ .60‡ .61‡ .59‡ .73‡ .61‡

ONLINEB .91‡ .31 .29† .27‡ .13‡ .33 .14‡ .19‡ – .44 .22‡ .09‡ .39 .19 .34 .24? .22† .39
PC-TRANS .88‡ .45 .43 .24‡ .26‡ .29† .21‡ .24‡ .49 – .22‡ .27‡ .37 .43 .55† .33† .49 .41
POTSDAM .88‡ .60‡ .51† .40 .27† .59‡ .25‡ .47 .63‡ .64‡ – .45 .52‡ .56‡ .69‡ .61‡ .70‡ .68‡

SFU .95‡ .52‡ .56‡ .4 .30† .61‡ .27‡ .39 .65‡ .64‡ .29 – .55‡ .54‡ .76‡ .53‡ .70‡ .60‡

UEDIN .94‡ .39 .44 .33 .23‡ .32 .20‡ .26‡ .32 .49 .25‡ .26‡ – .43 .57‡ .18 .46† .42
CMU-HEAFIELD-COMBO .91‡ .42 .39 .23‡ .10‡ .27‡ .14‡ .19‡ .23 .35 .24‡ .19‡ .28 – .48‡ .28 .34 .29

DCU-COMBO .84‡ .23‡ .27† .23‡ .03‡ .31† .10‡ .21‡ .42 .31† .15‡ .10‡ .16‡ .20‡ – .18‡ .27? .22‡

KOC-COMBO .91‡ .37 .49 .25† .10‡ .39 .17‡ .32‡ .42? .55† .17‡ .27‡ .26 .33 .41‡ – .32 .22
RWTH-COMBO .88‡ .29 .34† .28† .05‡ .26‡ .10‡ .17‡ .48† .43 .16‡ .15‡ .24† .33 .46? .36 – .29

UPV-COMBO .92‡ .37 .52 .22‡ .09‡ .25‡ .10‡ .19‡ .28 .47 .15‡ .25‡ .33 .24 .49‡ .34 .39 –
> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45

>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68

Table 20: Sentence-level ranking for the WMT10 English-Czech News Task

46



!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

""#)* +,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--

''567*8'* +,4+ +,/1 !"#$ !"#% !"#% !"#% !"#% !"#! !"## !"#& !"#' !"() !"'& !"*& !"%% !"#$ !"%)

78&69:67*8'* !"&' +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !"#& +,/+ !"() !"'& +,.; !"%% !"#$ +,-1

78& +,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !"'& +,.. +,-3 +,/- +,-;

7&6'*<"= +,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/. +,-;

7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3. +,2+ +,.+ +,/+ +,-3

<@&67*8'* +,;2 +,/4 !"#$ !"#% !"#% +,/. !"#% +,-1 +,/. +,// +,-0 +,21 !"'& +,./ +,-2 +,/. +,-4

*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2

*5#A5?C +,4+ !"#) !"#$ !"#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,.; +,-. +,/. +,-;

=D)@67*8'* +,;/ +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/. +,/; +,-0 !"() !"'& +,./ +,-2 +,// +,-1

&?EA5 +,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1

&FG67*8'* +,;. +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,./ +,-. +,// +,-1

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

""#)* +,-0 +,/2 -,04 !"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+

''567*8'* +,/- !"#& /,-; !"%+ !"!* +,33 !"** !"%+ !"*! !"#+ !"#+ !"(# &"'!

78&69:67*8'* +,/- +,/4 /,-. !"%+ !"!* +,+0 !"** +,-1 !"*! +,/4 +,/4 !"(# ;,01

78& +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;

7&6'*<"= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-

7&6>?8"5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0 +,3- /,-/

<@&67*8'* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/; +,2. 4,++

*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-. +,2/ +,/2 +,/2 +,31 ;,3.

*5#A5?C !"## !"#& /,-3 !"%+ !"!* !"'$ +,.2 +,-4 !"*! !"#+ !"#+ +,2- ;,3.

=D)@67*8'* +,/- +,/; /,-+ !"%+ !"!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;

&?EA5 +,/. !"#& /,-+ !"%+ !"!* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2. ;,;-

&FG67*8'* +,/- !"#& #"%& !"%+ !"!* +,+4 !"** +,-4 +,20 !"#+ !"#+ +,2- ;,00

="5H

IJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%

RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK

,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?(%@*D5,
SJ^C- SJ9C-

WOKMCSNY(G3."

RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)? BMN_(2,3

I=E*7 YS_@O(#?P?=(CSNY

K?8VTK(CSNY I_Y6S:a

MNKSB(R

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

'')*+,-', ./01 ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.

+"-'56789 ./00 ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1. ./1; %&'* ./2.

+-&*<=*+,-', ./>. ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.

+-&*?@A*+,-', %&(( ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.

+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.( %&-. ./20 ./1:

+&*F9-") ./E4 ./2E ./22 ./24 ./2. ./10 ./2E ./21 ./1: ./E1 %&/' %&,. %&.( ./23 ./11

7+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-. %&'% %&'* %&'/

7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1. ./22 ./10

89)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11

?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2. ./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>

I?&*+,-', ./02 ./0. %&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1. ./1: %&'* ./2.

I?& ./2; ./0. %&'( ./2> ./20 ./2E ./2: ./0. ./23 ./34 %&/* ./3: ./1> ./2> ./1;

#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4> ./30 ./12 ./20 ./1:

#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0. ./23 ./34 ./4> ./3: ./1> ./2> ./1:

#6&-*+,-', ./01 ./0. %&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;

#6&- ./>4 ./0. %&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.

)5+ ./00 ./0. %&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;

,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2. ./20 ./2: ./24 ./3. ./4> ./30 ./11 ./21 ./10

,)#6)9K ./>4 ./0. %&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;

5"#6 ./02 ./0. ./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1. ./1: ./2> ./1;

5LB?*+,-', %&(( ./0. %&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.

5LB? ./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0. ./23 ./34 ./4> ./3: ./10 %&'( ./1;

&976) ./02 ./0. ./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1. ./1; %&'* ./1;

&AH*+,-', ./0: ./0. %&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

'')*+,-', %&'* ./0. 2/:. ./24 %&%. ./.E %&.( ./2. %&.- %&+/ ./3. (&*/

+"-'56789 ./2> ./0. 2/:. ./24 %&%. ./.2 %&.( ./2. ./33 %&+/ ./3. >/02

+-&*<=*+,-', %&'* ./0. 2/:4 %&', %&%. ./.3 %&.( %&'/ ./31 %&+/ ./3. >/0;

+-&*?@A*+,-', %&'* ./0. 2/:1 %&', %&%. ./.3 %&.( %&'/ %&.- %&+/ ./3. >/0;

+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%. ./.3 ./31 ./1; ./34 ./2: ./E2 >/40

+&*F9-") ./24 ./21 2/3. ./1. ./.E ./.E ./E> ./1. ./E> ./22 ./4; 0/4E

7+&*+,-', %&'* %&+/ 2/:> ./2E %&%. ./.3 %&.( %&'/ %&.- %&+/ %&./ >/>:

7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.4

89)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>

?&6+,)8 ./22 ./2: 2/0. ./1: %&%. ./.3 ./33 ./1> ./34 ./2> ./E1 0/;;

I?&*+,-', ./2> ./0. 2/>> ./24 %&%. ./.4 ./30 ./2. ./33 ./0. ./E; >/04

I?& ./2> ./2; 2/>1 ./2. %&%. ./.: ./32 ./2. ./3E ./0. ./E: >/11

#68 ./22 ./2: 2/0E ./1: %&%. ./.2 ./31 ./1; ./34 ./2: ./E0 >/43

#6-%6 ./20 ./2; 2/>E ./24 %&%. ./.2 ./30 ./2. ./33 ./2; ./E> >/34

#6&-*+,-', ./2> ./0. 2/>> ./24 %&%. ./.3 %&.( %&'/ ./33 ./0. ./E; >/>.

#6&- %&'* %&+/ '&** %&', %&%. ./.0 %&.( %&'/ %&.- %&+/ ./E; >/2;

)5+ ./2> ./0. 2/>; ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E; >/23

,)#6)9J ./22 ./2: 2/21 ./2. %&%. ./.: ./33 ./1: ./34 ./2: ./E2 >/E;

,)#6)9K %&'* ./0. 2/:. %&', %&%. %&// %&.( %&'/ %&.- %&+/ ./E; >/>3

5"#6 ./2> ./0. 2/:. %&', %&%. ./.0 ./30 ./2. ./33 ./0. ./E: >/11

5LB?*+,-', %&'* %&+/ 2/:> %&', %&%. ./.4 %&.( %&'/ %&.- %&+/ ./3. >/>3

5LB? ./20 ./2; 2/>3 ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E: >/3;

&976) ./2> ./0. 2/:E ./24 %&%. ./.1 ./30 ./2. ./33 %&+/ ./E: >/11

&AH*+,-', %&'* %&+/ '&** %&', %&%. ./.3 %&.( ./2. %&.- %&+/ ./3. >/01

0123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/
5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVW

P(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57

MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4

KUVW(43" OPQRRVSA ]S ]57,+ WU\? KVLR*V K["5%

47



!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

""#)* +,-. +,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02

''567*8'* !"## !"$% +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. !"'( +,39 +,02 +,-2 !"$& !"&)

78&6:;67*8'* +,90 +,/4 +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& !"&)

78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3

78& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,04

7&6?@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-. +,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0. +,/+ +,02

ABC +,/+ +,/- +,/. +,-1 +,-4 +,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02

DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-. +,/. +,01

<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02

G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3

G<& +,/0 +,// +,/. +,/. +,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09

EC) +,9. +,/4 +,/- +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+

E*767*8'* +,/1 +,/2 +,/- +,/0 +,/. +,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+

E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/

#C8%C +,/. +,/9 +,/- +,/. +,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01

#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0. +,-3 +,/. +,01

*5#C5@H +,20 +,/9 +,/- +,/. +,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02

*5#C5@I +,90 !"$% !"$* !"$$ !"$& !"$) +,/9 !"*! !"$' !"'( !"(+ !"'# !"&# !"$& +,-3

JK)<67*8'* +,9/ +,/4 +,// +,/- !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& +,-3

JK)< +,2. +,/4 +,// +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3

&@AC5 +,22 +,/4 +,/- +,/0 +,/. +,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01

&8A +,24 +,/4 +,// +,/0 +,/. +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+

&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04

&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3

&&68% +,// +,// +,/. +,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

""#)* +,/+ +,/. /,+3 +,-3 +,+. +,+3 +,.1 +,-. +,./ +,/0 +,31 2,04

''567*8'* +,// +,/9 /,-2 +,-4 !"!' +,+- +,0/ +,-9 +,0+ +,/4 !")* #")+

78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!' +,+0 +,0- +,-2 +,0+ +,/4 !")* 9,31

78&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!' +,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+1

78& +,/+ +,/0 /,+4 +,-. +,+. +,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-9

7&6?@8"5 +,-4 +,/3 -,14 +,09 +,+. +,+. +,.2 +,01 +,.0 +,/3 +,32 /,93

ABC +,-1 +,/0 /,3. +,-/ +,+. +,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/

DE +,/- +,/9 /,-0 +,-9 !"!' +,+. +,00 +,-2 +,.4 +,/4 +,.. 2,29

<&C7*5F +,/+ +,/0 /,33 +,-. +,+. +,+. +,0+ +,-0 +,.2 +,/. +,31 2,30

G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!' +,+. +,0- +,-2 +,.1 +,/4 +,./ 9,30

G<& +,/3 +,/0 /,3. +,-. +,+. +,+/ +,03 +,-- +,.9 +,/- +,.3 2,90

EC) +,/- +,/4 /,/3 +,-4 !"!' +,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13

E*767*8'* +,/. +,// /,03 +,-/ +,+. +,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44

E*7 +,-0 +,-4 -,9. +,02 +,+. +,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01

#C8%C +,/0 +,/2 /,0/ +,-9 +,+. +,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90

#C& +,/0 +,/2 /,0- +,-2 +,+. +,+0 +,0. +,-2 +,.4 +,/2 +,.. 2,2+

*5#C5@H +,/0 +,// /,.- +,-2 +,+. +,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44

*5#C5@I !"$* !"*! $"#( !"$) !"!' !"(# !"'* !"&% !"') !"*( !")* 9,34

JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!' +,+. +,0/ +,-9 +,0+ +,/1 !")* 9,.+

JK)< +,/- +,/9 /,-- +,-9 !"!' +,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12

&@AC5 +,/- +,/4 /,/0 +,-1 !"!' +,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+

&8A +,// +,/4 /,/+ +,-4 !"!' +,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99

&>>%"#" +,/3 +,/- /,.3 +,-0 +,+. +,+. +,03 +,-- +,.9 +,/- +,.3 2,/3

&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!' +,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/

&&68% +,/+ +,/- /,34 +,-- +,+. +,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.

,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,
J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVW

P(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JA

MVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3

IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 !"#$

+"-'89:;< ./02 ./0. ./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$

+-&*>?*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 !"#$

+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3. ./30 ./35 ./31 ./7. ./2= ./73 ./6= ./3= ./64

+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63

:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=

B&9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64

CB&*+,-', ./33 ./0. ./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6. ./32 ./34 ./3.

CB& ./02 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65

,)#9)"D ./36 ./0. ./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60

,)#9)<E !"%! !"&' !"#( !"&) !"&$ !"#& !"&) !"&# !"#% !"'& !")! !"*% !"## ./35 ./3.

&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0. ./07 ./33 ./77 ./24 ./61 ./31 ./35 !"#$

&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65

&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./61 ./37 !"&! !"#$

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$

'')*+,-', ./35 ./02 3/51 ./31 !"!' ./.6 ./74 ./3. ./76 ./01 ./71 =/56

+"-'89:;< ./34 ./0. 3/43 ./32 !"!' ./.= ./7= ./3. ./77 ./02 ./72 =/==

+-&*>?*+,-', ./35 ./02 3/5. ./31 !"!' ./.6 ./7= ./3. ./76 ./01 ./72 =/4=

+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7. ./3= ./1= =/25

+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03

:A9 ./36 ./34 3/07 ./3. ./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.

B&9+,); ./3= ./35 3/=7 ./65 !"!' ./.3 ./76 ./64 ./72 ./35 ./10 =/7=

CB&*+,-', ./34 ./0. 3/47 ./32 !"!' ./.2 ./7= ./3. ./77 ./02 ./7. =/42

CB& ./34 ./0. 3/47 ./32 !"!' ./.4 ./70 ./65 ./77 ./02 ./15 =/=1

,)#9)"D ./3= ./35 3/=. ./31 !"!' ./22 ./70 ./3. ./71 ./02 ./14 =/==

,)#9)<E !"&$ !"&' &"!' !"#& !"!' !")# !"*! !"#' !"'& !"&* !"'' +"'(

&<:9) ./35 ./01 3/5= ./36 !"!' ./21 ./74 ./32 ./76 ./01 ./7. =/04

&F+ ./30 ./0. 3/=3 ./3. !"!' ./.7 ./73 ./64 ./71 ./0. ./10 =/13

&FG*+,-', ./35 ./01 3/5= ./37 !"!' ./.6 ./74 ./3. ./76 ./01 ./72 =/41

,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/
8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?] RO^E6 RO>E6M<-_`M M<-_`M(ERST

L(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:

HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2

ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%

48



!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)*&+,-+).*'. /012 /034 /035 /03/ /056 /076 !"#$ /076 /052 /038 /055

)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057
)&+;<);. /01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058

)&+><*"? /03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072
@)&+).*'. !"%$ !"$& !"$$ !"$' !"#( !")& !"#$ !")( !"$' !"$* !"#+

@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057
<&:.;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=

A.)+).*'. /011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053
A.) /077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=

.?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=

.?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053
E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=
E.;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=

:F;G+).*'. /06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053
%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057

&<@B? /01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055
&EI+).*'. /014 /034 /033 /03/ /056 /074 !"#$ /076 /052 /038 /053

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)*&+,-+).*'. !"#+ /056 504/ !"$$ +/0/= /0/3 /037 /086 301=

)&+'.9": /053 /051 506/ /037 ,!"!' /0/1 /038 /081 3056
)&+;<);. /057 /053 501= /037 ,!"!' /0/2 /03/ /087 30=3

)&+><*"? /058 /05= 5071 /03/ ,!"!' /0/3 /056 /08= 5021
@)&+).*'. /056 !"#( #"&! !"$$ ,!"!' !"'# !"$# !"'& $"&+

@)& /05/ /058 50=3 /056 ,!"!' /0/3 /051 /087 5023
<&:.;:"?% /05/ /05= 505= /038 ,!"!' /0/5 /051 /08/ 5055

A.)+).*'. /053 /051 506/ /037 ,!"!' /0/= /038 /086 3034
A.) /074 /05/ 50=5 /057 ,!"!' /0/7 /055 /08= 5037

.?#B?<C /058 /057 5054 /038 ,!"!' /0/1 /056 /08= 5013

.?#B?<D !"#+ !"#( 5046 /035 ,!"!' /08/ /037 /086 3031
E)+;:"?% /058 /057 503= /03/ ,!"!' /0/3 /056 /08/ 5031
E.;%@"* /05/ /05= 5074 /052 ,!"!' /0/7 /051 /08= 5065

:F;G+).*'. !"#+ /056 5048 !"$$ ,!"!' /0/5 /038 /084 304=
%H& /058 /057 5055 /03/ ,!"!' /0/5 /056 /088 5014

&<@B? /053 /051 5067 /035 ,!"!' /088 /03= /081 3057
&EI+).*'. !"#+ !"#( 5045 /035 ,!"!' /0/1 /037 /084 3044

:"?A

JKL;"?H.:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%

RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL

-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0
SK^D5 SK,D5

WOLMDSNY(I87"

RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08

J:@.) YS_GO(#<P<:(DSNY

L<*VTL(DSNY J_Y+S-a

MNLSC(R

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)"*'+,-./ 0123 0145 0146 !"#$ 0144 0164 0167 0148 0140

)*&9:;9)<*'< 0156 0148 !"## !"#$ !"#% !"&$ !"#' !"%! !"#(
)&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163

-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162
/& 013? 0143 014@ 0168 0165 0138 0166 0142 0162

./>/B" 0134 0144 014@ 0167 0168 0135 016? 0146 0166
CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168

E<)9)<*'< 0128 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(
E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162
#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140
#,&* 0123 0148 0146 0142 0144 0162 !"#' !"%! 014@
>+) 012? 0145 0146 0146 0143 0164 0167 0148 0167

<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167
<>#,>/G 0128 0148 0146 !"#$ 0144 0162 !"#' !"%! 014@

+"#, 0122 0148 0146 0142 0144 0162 0140 !"%! 014@
+HID9)<*'< !"$# !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(

+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@
&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%! 014@

&JB9)<*'< 0122 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)"*'+,-./ 0144 0145 4143 014? 9010? 0102 0148 01?8 5165

)*&9:;9)<*'< !"#$ 0147 4156 0146 *!"!' 0103 !"%! !"+! 514?
)&9=/*"> 0140 014? 4103 0160 90103 0103 014? 01@2 4163

-A, 014@ 0146 41?7 0162 9010? 0102 0146 01?0 21@2
/& 0140 0146 41?@ 0162 9010? 0103 0143 01@7 4176

./>/B" 0140 014? 41@0 0168 90103 0104 014? 01@8 2106
CD& 0144 0145 414@ 0140 9010? 0104 0145 01?6 2124

E<)9)<*'< !"#$ 0147 4152 0146 *!"!' 0100 !"%! 01?7 5143
E<) 0167 014@ 4102 0164 9010? 010? 014? 01?0 21@6
#,*%, 0142 0148 4125 0146 9010? 0102 0148 01?5 51?0
#,&* !"#$ 0147 4152 0146 *!"!' 0105 0147 01?7 5138
>+) 0142 0148 412@ 014? *!"!' 0102 0148 01?5 51??

<>#,>/F 0144 0145 4148 014? 9010? 0105 0148 01?4 2177
<>#,>/G !"#$ !"%! #",' 0146 9010? !"'& !"%! 01?8 513?

+"#, !"#$ 0147 4153 0146 *!"!' 0102 0147 01?8 513?
+HID9)<*'< !"#$ 0147 4157 !"## *!"!' 0106 !"%! !"+! 512?

+HID 0142 0148 4127 0143 *!"!' 0102 0147 01?8 51?8
&/-,> 0142 0148 4150 0146 *!"!' 0107 0147 01?8 51?6

&JB9)<*'< !"#$ 0147 4158 !"## *!"!' 0104 !"%! !"+! $"%#

-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1
+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTU

N(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-

KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@

GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%

49



!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)*&+,-+).*'. /012 /011 /034 /034 /035 /064 /074 !"#$ !"%&

)&+89*": /07; /035 !"&' /037 /03< /06/ /077 /036 /06<
=>? /057 /016 /035 /033 /033 /063 /073 /031 /066
@A /015 /011 /034 /031 /031 /065 /075 /031 /063
BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<
A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066

A.)+).*'. /012 /011 /034 /034 /035 /064 /075 /035 !"%&
A.) /06/ /01/ /031 /037 /037 /06< /077 /033 /067
#?*%? /01< /013 /035 /031 /033 /061 /071 /031 /066
#?& /011 /013 /034 /035 /031 /061 /071 /033 /066

.:#?:9E /012 /013 /035 /035 /031 /061 /071 /033 /067

.:#?:9F !"$! !"&( /03; !"#) !"#$ !"%* !"') !"#$ !"%&
GHDC+).*'. /051 /011 /034 /034 /035 /064 /074 /035 !"%&

GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063
%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;

&9=?: /057 /011 /034 /035 /031 /065 /075 /035 /063
&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063

&JK+).*'. /011 /011 /034 /034 !"#$ /064 /074 /031 /063

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)*&+C9"L9#=+).*'. /034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;

)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6
=>? /031 /034 3042 /035 !"!# !"+' /01/ /0<3 107;
@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1
BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042
A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;

A.)+).*'. /035 /034 3044 /031 /0/6 /0// /016 /0<4 102;
A.) /03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2
#?*%? /031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/
#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1

.:#?:9E /035 /03; 30;7 /035 !"!# /0/2 /017 /0<1 1051

.:#?:9F !"#* !"&! &"!& !"#) !"!# /07/ !"&& !"+* ("+)
GHDC+).*'. /035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5

GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5
%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7

&9=?: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023
&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043

&JK+).*'. /035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</

,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0
G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVW

P(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=

MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<

FUVW(<6" OPQRRVSJ ]S ]G=.) WU\C FVHR+V FA"G%

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120

)*&89:8);*'; 015< 0150 !"#$ 0124 012< 013< !"%& 012< 0127
)&8=/*"> 017? 0122 !"#$ 012@ 0120 013@ 0177 0127 013?

-)& 015@ 0150 !"#$ 012< 0126 0136 0172 012< 012@
-A, 0122 0126 0123 0123 0127 0137 017? 0123 0136
BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<

D;)8);*'; 0157 0150 !"#$ 012< 0126 0136 !"%& 012< 012@
D;) 01@4 0123 012? 012@ 0134 01?< 017@ 012? 0135

;>#,>/E 0154 0150 !"#$ 012< 0126 013< 0172 012< 012@
;>#,>/F !"$' !"&' !"#$ !"&! !"#( !")( !"%& !"&! !"#*

+GHC8);*'; 0124 0124 0125 0124 012< 013< !"%& 012< 012@
%I& 0130 012< 0122 0122 0123 0137 0173 012? 0132

&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@
&J'8);*'; 0150 0150 !"#$ 0124 012< 013< !"%& 012< 012@
&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134

&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120

!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$
)"*'+,-./ 0126 0124 2162 0132 !"!' 0100 0150 0174 6120

)*&89:8);*'; 0124 015@ 2140 013< !"!' 0100 0157 !"*% 6145
)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123

-)& 0124 015@ 2140 0136 !"!' 0100 015@ 01?0 6167
-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?
BC& 0126 0124 2167 0132 !"!' 0100 0150 017< 612<

D;)8);*'; 012< 0150 21<3 0135 !"!' 0100 015@ 01?0 6164
D;) 012@ 012? 21@4 01?5 0100 0100 012? 017@ 51?6

;>#,>/E 0124 0157 2146 013< !"!' 0100 0157 01?0 6152
;>#,>/F !"&! !"&* &"'% !")( !"!' 0100 0153 !"*% 61<6

+GHC8);*'; 0124 015@ 21<4 0136 !"!' 0100 0150 01?@ $"($
%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<

&/-,> 012< 015@ 21<5 0136 !"!' !"!' 015@ 01?0 6155
&J'8);*'; 012< 0150 21<< 0136 !"!' 0100 015@ 01?@ 614?
&JK8>>#* 0126 0124 216@ 0133 !"!' 0100 0124 017< 61?4

&JK 012< 0150 21<0 0132 !"!' 0100 0150 0174 613<

+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1
+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUV

O(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-

LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@

FTUV(@?" NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%

50



R
E

F

A
A

LT
O

C
M

U

C
U

-B
O

JA
R

C
U

-Z
E

M
A

N

O
N

L
IN

E
A

O
N

L
IN

E
B

U
E

D
IN

B
B

N
-C

C
M

U
-H

E
A

-C

JH
U

-C

R
W

T
H

-C

U
P

V
-C

REF – .03‡ .02‡ .03‡ .01‡ .03‡ .02‡ .05‡ .02‡ .06‡ .03‡ .05‡ .03‡

AALTO .93‡ – .54‡ .54‡ .23‡ .36 .58‡ .56‡ .65‡ .69‡ .64‡ .67‡ .62‡

CMU .94‡ .30‡ – .47 .14‡ .22‡ .52‡ .41 .50‡ .57‡ .45† .44 .38
CU-BOJAR .94‡ .26‡ .38 – .10‡ .22‡ .61‡ .47† .46 .55‡ .42 .49‡ .44

CU-ZEMAN .98‡ .58‡ .73‡ .77‡ – .55‡ .79‡ .71‡ .84‡ .80‡ .77‡ .79‡ .75‡

ONLINEA .94‡ .41 .61‡ .57‡ .23‡ – .68‡ .63‡ .71‡ .71‡ .63‡ .54‡ .61‡

ONLINEB .93‡ .30‡ .31‡ .26‡ .10‡ .17‡ – .32† .35 .31 .22‡ .29? .38
UEDIN .91‡ .27‡ .35 .34† .11‡ .18‡ .47† – .54‡ .50‡ .35 .29 .35
BBN-C .95‡ .21‡ .22‡ .36 .06‡ .17‡ .38 .26‡ – .32 .24‡ .31? .26‡

CMU-HEA-C .90‡ .17‡ .19‡ .23‡ .09‡ .18‡ .32 .27‡ .34 – .31† .31? .30‡

JHU-C .93‡ .19‡ .30† .35 .09‡ .24‡ .50‡ .34 .47‡ .45† – .41‡ .36
RWTH-C .91‡ .16‡ .35 .29‡ .12‡ .27‡ .41? .37 .42? .42? .23‡ – .24†

UPV-C .94‡ .24‡ .40 .36 .09‡ .28‡ .39 .32 .46‡ .47‡ .33 .36† ?
> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40

>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62

Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
non-expert Mechanical Turk judgments)

51



R
E

F

A
A

LT
O

C
M

U

C
U

-Z
E

M
A

N

D
F

K
I

F
B

K

H
U

IC
O

N
G

JH
U

K
IT

K
O

C

L
IM

S
I

L
IU

O
N

L
IN

E
A

O
N

L
IN

E
B

R
W

T
H

U
E

D
IN

U
M

D

U
P

P
S

A
L

A

U
U

-M
S

B
B

N
-C

C
M

U
-H

E
A

-C

C
M

U
-H

Y
P

O
-C

JH
U

-C

K
O

C
-C

R
W

T
H

-C

U
P

V
-C

REF – .00‡ .02‡ .00‡ .07‡ .04‡ .03‡ .00‡ .06‡ .04‡ .00‡ .02‡ .07‡ .07‡ .07‡ .02‡ .09‡ .03‡ .03‡ .10‡ .04‡ .04‡ .03‡ .02‡ .07‡ .06‡

AALTO 1.00‡ – .43 .39 .48 .60‡ .38 .41 .74‡ .18‡ .42 .57‡ .50† .63‡ .55‡ .68‡ .79‡ .42 .33 .71‡ .61‡ .66‡ .54 .51‡ .66‡ .56‡

CMU .95‡ .34 – .19‡ .45 .52† .38 .50 .63‡ .17‡ .51‡ .55‡ .56† .66‡ .55‡ .60‡ .56‡ .30 .40 .62‡ .64‡ .49‡ .58‡ .46 .64‡ .46†

CU-ZEMAN 1.00‡ .44 .64‡ – .43 .72‡ .31 .45† .69‡ .36 .55 .62‡ .75‡ .75‡ .78‡ .75‡ .75‡ .48? .56† .79‡ .82‡ .72‡ .68‡ .63‡ .67‡ .84‡

DFKI .92‡ .29 .33 .35 – .37 .40 .34 .59 .08‡ .42 .50 .49 .64‡ .35 .44 .44 .50 .41 .70‡ .61† .57 .46 .47 .62‡ .44
FBK .93‡ .26‡ .23† .17‡ .49 – .12‡ .30 .52† .08‡ .20‡ .45? .41 .62‡ .44 .44 .48? .18‡ .25† .53‡ .47 .38 .38 .22† .41 .51?

HUICONG .92‡ .34 .39 .37 .38 .71‡ – .53† .67‡ .18‡ .51† .47 .60‡ .65‡ .49? .55‡ .78‡ .35 .41 .56‡ .77‡ .74‡ .58‡ .41 .65‡ .57‡

JHU .92‡ .35 .30 .17† .52 .45 .25† – .58‡ .16‡ .43 .38 .57† .60‡ .54‡ .60‡ .70‡ .29 .25 .65‡ .75‡ .56‡ .62‡ .49? .66‡ .48†

KIT .90‡ .14‡ .16‡ .14‡ .35 .28† .19‡ .16‡ – .03‡ .29? .20‡ .35 .53? .21‡ .24† .30 .20‡ .22‡ .44 .29 .38 .35 .24 .40 .24†

KOC .95‡ .66‡ .71‡ .51 .75‡ .80‡ .58‡ .68‡ .93‡ – .75‡ .87‡ .72‡ .74‡ .74‡ .81‡ .81‡ .78‡ .66‡ .89‡ .85‡ .80‡ .80‡ .72‡ .91‡ .73‡

LIMSI .99‡ .26 .24‡ .32 .45 .61‡ .25† .38 .50? .10‡ – .50? .55? .69‡ .52? .57‡ .57‡ .29† .22‡ .60‡ .52† .42 .47† .37 .60‡ .56‡

LIU .87‡ .17‡ .20‡ .14‡ .34 .22? .31 .38 .66‡ .04‡ .27? – .51? .53† .52? .53? .51 .20‡ .33 .64‡ .59‡ .48† .48 .51 .37 .53?

ONLINEA .90‡ .25† .29† .18‡ .34 .43 .23‡ .28† .49 .08‡ .32? .30? – .44 .38 .40 .42 .32† .35? .39 .47 .51 .27‡ .35 .43 .40
ONLINEB .76‡ .22‡ .24‡ .14‡ .27‡ .27‡ .25‡ .25‡ .32? .22‡ .21‡ .28† .32 – .27† .21‡ .30† .23‡ .15‡ .41 .31 .40 .23‡ .16‡ .42 .29

RWTH .89‡ .22‡ .23‡ .13‡ .49 .35 .29? .21‡ .62‡ .15‡ .32? .29? .46 .57† – .39 .49 .25 .38 .41 .27 .34 .36 .27 .48? .22‡

UEDIN .91‡ .15‡ .20‡ .12‡ .49 .35 .24‡ .22‡ .49† .04‡ .22‡ .30? .46 .62‡ .43 – .39 .11‡ .15‡ .45 .33 .40 .45 .33 .34 .33
UMD .91‡ .12‡ .23‡ .06‡ .35 .29? .11‡ .16‡ .47 .14‡ .23‡ .35 .40 .55† .36 .47 – .16‡ .17‡ .44 .29† .27 .37 .26 .27 .24†

UPPSALA .94‡ .30 .41 .23? .35 .53‡ .26 .37 .66‡ .03‡ .54† .71‡ .57† .65‡ .45 .72‡ .67‡ – .25 .59‡ .69‡ .49‡ .63‡ .33 .60‡ .64‡

UU-MS .83‡ .28 .42 .24† .41 .49† .28 .42 .68‡ .10‡ .55‡ .48 .55? .63‡ .49 .56‡ .60‡ .32 – .52† .58‡ .61‡ .64‡ .46‡ .64‡ .50?

BBN-C .90‡ .15‡ .16‡ .10‡ .22‡ .17‡ .22‡ .18‡ .41 .06‡ .16‡ .21‡ .35 .45 .30 .26 .34 .13‡ .20† – .42† .14† .27 .11‡ .25 .21†

CMU-HEA-C .83‡ .20‡ .18‡ .07‡ .29† .32 .06‡ .10‡ .49 .05‡ .26† .21‡ .41 .33 .37 .43 .58† .10‡ .14‡ .18† – .33 .32 .11‡ .34 .24?

CMU-HYPO-C .96‡ .24‡ .20‡ .07‡ .37 .33 .12‡ .21‡ .40 .10‡ .41 .26† .40 .54 .25 .37 .44 .13‡ .17‡ .49† .31 – .34 .23? .51† .45
JHU-C .97‡ .33 .22‡ .18‡ .31 .30 .27‡ .18‡ .33 .12‡ .19† .33 .59‡ .60‡ .39 .32 .30 .19‡ .20‡ .44 .29 .34 – .21? .36 .23
KOC-C .93‡ .11‡ .31 .17‡ .41 .50† .25 .27? .44 .11‡ .42 .36 .47 .68‡ .43 .41 .40 .33 .18‡ .59‡ .57‡ .46? .47? – .52† .43

RWTH-C .87‡ .20‡ .10‡ .21‡ .25‡ .27 .15‡ .23‡ .24 .02‡ .20‡ .30 .34 .47 .27? .34 .36 .14‡ .20‡ .33 .26 .21† .24 .20† – .17‡

UPV-C .93‡ .14‡ .20† .10‡ .42 .29? .25‡ .25† .57† .20‡ .22‡ .33? .39 .45 .47‡ .40 .50† .24‡ .28? .44† .42? .27 .34 .28 .56‡ ?
> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42

>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64

Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
non-expert Mechanical Turk judgments)

R
E

F

C
A

M
B

R
ID

G
E

C
O

L
U

M
B

IA

C
U

-Z
E

M
A

N

D
F

K
I

H
U

IC
O

N
G

JH
U

O
N

L
IN

E
A

O
N

L
IN

E
B

U
E

D
IN

U
P

C

B
B

N
-C

C
M

U
-H

E
A

-C

JH
U

-C

U
P

V
-C

REF – .05‡ .01‡ .02‡ .03‡ .03‡ .01‡ .02‡ .04‡ .03‡ .04‡ .03‡ .07‡ .05‡ .04‡

CAMBRIDGE .90‡ – .24‡ .11‡ .35† .26‡ .43 .35 .50† .45† .33? .40 .46 .28? .41
COLUMBIA .97‡ .61‡ – .25‡ .47 .44 .61‡ .53‡ .62‡ .59‡ .48† .59‡ .57‡ .45† .57‡

CU-ZEMAN .92‡ .73‡ .59‡ – .62‡ .66‡ .71‡ .65‡ .75‡ .79‡ .58‡ .75‡ .78‡ .71‡ .72‡

DFKI .95‡ .50† .41 .21‡ – .46 .56‡ .52‡ .65‡ .62‡ .47 .52‡ .56‡ .52† .60‡

HUICONG .93‡ .57‡ .34 .21‡ .36 – .47† .43 .67‡ .58‡ .40 .51‡ .62‡ .46† .52‡

JHU .94‡ .39 .22‡ .16‡ .30‡ .32† – .41 .52‡ .47‡ .37 .41 .33† .28 .35
ONLINEA .92‡ .45 .35‡ .24‡ .34‡ .41 .41 – .60‡ .58‡ .38 .55‡ .46 .36 .57‡

ONLINEB .87‡ .34† .24‡ .15‡ .21‡ .19‡ .33‡ .25‡ – .34† .26‡ .34† .37? .24‡ .40
UEDIN .94‡ .33† .26‡ .12‡ .24‡ .22‡ .25‡ .25‡ .50† – .25‡ .28† .32? .25‡ .26

UPC .89‡ .45? .36† .23‡ .39 .37 .42 .48 .62‡ .57‡ – .54‡ .51‡ .50‡ .53‡

BBN-C .91‡ .33 .25‡ .11‡ .32‡ .30‡ .34 .31‡ .51† .41† .30‡ – .36 .26‡ .31
CMU-HEA-C .89‡ .37 .20‡ .10‡ .29‡ .23‡ .23† .35 .50? .44? .31‡ .34 – .23‡ .31

JHU-C .89‡ .39? .31† .17‡ .37† .33† .38 .42 .63‡ .47‡ .31‡ .42‡ .42‡ – .37†

UPV-C .91‡ .35 .30‡ .16‡ .29‡ .26‡ .32 .28‡ .44 .35 .27‡ .27 .30 .24† ?
> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43

>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66

Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)

52



R
E

F

C
A

M
B

R
ID

G
E

C
M

U
-S

TA
T

X
F

E
R

C
U

-Z
E

M
A

N

D
F

K
I

G
E

N
E

V
A

H
U

IC
O

N
G

JH
U

L
IG

L
IM

S
I

L
IU

M

N
R

C

O
N

L
IN

E
A

O
N

L
IN

E
B

R
A

L
I

R
W

T
H

U
E

D
IN

B
B

N
-C

C
M

U
-H

E
A

-C

C
M

U
-H

Y
P

O
-C

D
C

U
-C

JH
U

-C

L
IU

M
-C

R
W

T
H

-C

U
P

V
-C

REF – .02‡ .00‡ .00‡ .00‡ .00‡ .05‡ .02‡ .00‡ .00‡ .00‡ .02‡ .06‡ .02‡ .04‡ .02‡ .04‡ .03‡ .02‡ .05‡ .05‡ .04‡ .05‡ .06‡ .02‡

CAMBRIDGE .82‡ – .42 .16‡ .12‡ .35 .31 .45 .21‡ .47 .29 .38 .28† .54 .43 .33 .38 .28 .39 .45† .24 .25 .34 .54† .37
CMU-STATXFER .91‡ .50 – .17‡ .41 .17‡ .28 .44 .36 .48? .56‡ .57‡ .47 .56? .70‡ .49 .50 .47 .61‡ .68‡ .55† .50 .42 .52† .51†

CU-ZEMAN 1.00‡ .74‡ .71‡ – .74‡ .46 .67‡ .73‡ .73‡ .74‡ .75‡ .76‡ .75‡ .89‡ .78‡ .66‡ .83‡ .74‡ .87‡ .73‡ .80‡ .83‡ .77‡ .95‡ .82‡

DFKI 1.00‡ .77‡ .48 .17‡ – .27† .49 .52 .48 .64‡ .69‡ .67† .47 .62? .53 .47 .64‡ .60† .73‡ .72‡ .79‡ .58? .66‡ .73‡ .74‡

GENEVA .98‡ .58 .70‡ .44 .59† – .55? .67‡ .70‡ .70‡ .77‡ .73‡ .63‡ .81‡ .81‡ .69† .77‡ .73‡ .62† .66‡ .75‡ .60‡ .73‡ .88‡ .67†

HUICONG .89‡ .53 .34 .13‡ .34 .30? – .41 .36 .43 .70‡ .56‡ .57 .59† .56‡ .43 .55† .45 .51? .64‡ .48 .49 .49 .53† .57†

JHU .88‡ .36 .38 .11‡ .34 .25‡ .35 – .33? .46 .49? .48 .40 .50 .40 .34 .36 .39 .33 .59‡ .54? .41 .42 .40 .41
LIG .98‡ .65‡ .34 .18‡ .44 .26‡ .39 .56? – .60‡ .55‡ .51‡ .45 .54† .53 .39 .38 .52? .54† .53‡ .51? .53† .55 .51 .58†

LIMSI .98‡ .40 .24? .23‡ .23‡ .15‡ .29 .38 .25‡ – .28 .38 .27† .64‡ .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90‡ .40 .19‡ .12‡ .30‡ .11‡ .11‡ .26? .15‡ .36 – .36 .25† .37 .39 .26 .29 .24 .34 .49† .34 .33 .34 .31 .38

NRC .93‡ .31 .06‡ .15‡ .29† .23‡ .20‡ .32 .16‡ .38 .36 – .23† .53 .36 .24? .31 .44 .37 .47? .45? .29 .39 .38 .42
ONLINEA .92‡ .60† .47 .15‡ .44 .22‡ .32 .46 .34 .57† .52† .60† – .52? .34 .44 .57† .56 .51 .51 .64† .46 .51 .41 .60
ONLINEB .85‡ .35 .32? .09‡ .33? .10‡ .29† .31 .25† .17‡ .40 .34 .24? – .38 .32? .28 .39 .30 .42 .37 .41 .35 .32 .22‡

RALI .90‡ .31 .19‡ .10‡ .38 .10‡ .17‡ .47 .35 .38 .33 .38 .48 .48 – .29? .31 .29 .38 .40 .38 .34 .31 .57† .21†

RWTH .93‡ .43 .33 .12‡ .47 .26† .39 .40 .47 .35 .45 .49? .44 .53? .54? – .44? .42 .48 .51? .54? .48† .49 .50‡ .26
UEDIN .92‡ .42 .32 .10‡ .22‡ .10‡ .28† .30 .42 .30 .55 .36 .23† .43 .33 .20? – .41 .24 .52† .46 .25 .22 .27 .37
BBN-C .92‡ .49 .33 .24‡ .28† .18‡ .40 .39 .28? .45 .27 .27 .36 .39 .35 .35 .31 – .26 .45‡ .43 .26 .58‡ .36 .28

CMU-HEA-C .90‡ .41 .21‡ .06‡ .23‡ .29† .28? .27 .22† .39 .40 .22 .39 .43 .29 .30 .40 .28 – .43 .28 .15? .25 .26 .16
CMU-HYPO-C .84‡ .18† .20‡ .14‡ .20‡ .22‡ .21‡ .19‡ .16‡ .31 .22† .21? .36 .38 .34 .27? .22† .16‡ .24 – .36 .23 .10‡ .33 .24

DCU-C .92‡ .27 .24† .12‡ .17‡ .23‡ .30 .29? .24? .32 .43 .22? .28† .41 .23 .27? .28 .22 .23 .25 – .23 .23 .24 .17
JHU-C .88‡ .47 .26 .10‡ .33? .24‡ .36 .34 .24† .41 .39 .40 .42 .39 .34 .25† .42 .28 .37? .38 .39 – .37 .32 .38?

LIUM-C .90‡ .48 .42 .13‡ .25‡ .20‡ .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22‡ .34 .56‡ .33 .43 – .49‡ .44
RWTH-C .89‡ .22† .19† .03‡ .23‡ .12‡ .19† .23 .27 .30 .36 .19 .47 .54 .26† .16‡ .27 .19 .26 .28 .16 .22 .16‡ – .22

UPV-C .89‡ .27 .15† .10‡ .16‡ .29† .30† .31 .25† .36 .42 .24 .32 .64‡ .46† .34 .27 .44 .33 .44 .23 .17? .31 .24 ?
> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39

>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67

Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
non-expert Mechanical Turk judgments)

53


