










































Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems


Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 69–78,
Uppsala, Sweden, 11-16 July 2010. c©2010 Association for Computational Linguistics

Learning to Adapt to Unknown Users:
Referring Expression Generation in Spoken Dialogue Systems

Srinivasan Janarthanam
School of Informatics

University of Edinburgh
s.janarthanam@ed.ac.uk

Oliver Lemon
Interaction Lab

Mathematics and Computer Science (MACS)
Heriot-Watt University
o.lemon@hw.ac.uk

Abstract

We present a data-driven approach to learn
user-adaptive referring expression gener-
ation (REG) policies for spoken dialogue
systems. Referring expressions can be dif-
ficult to understand in technical domains
where users may not know the techni-
cal ‘jargon’ names of the domain entities.
In such cases, dialogue systems must be
able to model the user’s (lexical) domain
knowledge and use appropriate referring
expressions. We present a reinforcement
learning (RL) framework in which the sys-
tem learns REG policies which can adapt
to unknown users online. Furthermore,
unlike supervised learning methods which
require a large corpus of expert adaptive
behaviour to train on, we show that effec-
tive adaptive policies can be learned from
a small dialogue corpus of non-adaptive
human-machine interaction, by using a RL
framework and a statistical user simula-
tion. We show that in comparison to
adaptive hand-coded baseline policies, the
learned policy performs significantly bet-
ter, with an 18.6% average increase in
adaptation accuracy. The best learned pol-
icy also takes less dialogue time (average
1.07 min less) than the best hand-coded
policy. This is because the learned poli-
cies can adapt online to changing evidence
about the user’s domain expertise.

1 Introduction

We present a reinforcement learning (Sutton and
Barto, 1998) framework to learn user-adaptive re-
ferring expression generation policies from data-
driven user simulations. A user-adaptive REG pol-
icy allows the system to choose appropriate ex-
pressions to refer to domain entities in a dialogue

Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.

Table 1: Referring expression examples for 2 enti-
ties (from the corpus)

setting. For instance, in a technical support con-
versation, the system could choose to use more
technical terms with an expert user, or to use more
descriptive and general expressions with novice
users, and a mix of the two with intermediate users
of various sorts (see examples in Table 1).

In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user’s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Rule-based and super-
vised learning approaches to user adaptation in
SDS have been proposed earlier (Cawsey, 1993;
Akiba and Tanaka, 1994). However, such methods
require expensive resources such as domain ex-
perts to hand-code the rules, or a corpus of expert-
layperson interactions to train on. In contrast, we
present a corpus-driven framework using which
a user-adaptive REG policy can be learned using
RL from a small corpus of non-adaptive human-
machine interaction.

We show that these learned policies perform
better than simple hand-coded adaptive policies
in terms of accuracy of adaptation and dialogue

69



time. We also compared the performance of poli-
cies learned using a hand-coded rule-based simu-
lation and a data-driven statistical simulation and
show that data-driven simulations produce better
policies than rule-based ones.

In section 2, we present some of the related
work. Section 3 presents the dialogue data that
we used to train the user simulation. Section 4 and
section 5 describe the dialogue system framework
and the user simulation models. In section 6, we
present the training and in section 7, we present
the evaluation for different REG policies.

2 Related work

There are several ways in which natural language
generation (NLG) systems adapt to users. Some
of them adapt to a user’s goals, preferences, en-
vironment and so on. Our focus in this study
is restricted to the user’s lexical domain exper-
tise. Several NLG systems adapt to the user’s do-
main expertise at different levels of generation -
text planning (Paris, 1987), complexity of instruc-
tions (Dale, 1989), referring expressions (Reiter,
1991), and so on. Some dialogue systems, such
as COMET, have also incorporated NLG modules
that present appropriate levels of instruction to the
user (McKeown et al., 1993). However, in all the
above systems, the user’s knowledge is assumed to
be accurately represented in an initial user model
using which the system adapts its language. In
contrast to all these systems, our adaptive REG
policy knows nothing about the user when the con-
versation starts.

Rule-based and supervised learning approaches
have been proposed to learn and adapt during the
conversation dynamically. Such systems learned
from the user at the start and later adapted to the
domain knowledge of the users. However, they ei-
ther require expensive expert knowledge resources
to hand-code the inference rules (Cawsey, 1993) or
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these ex-
pensive resources. It is also not clear how super-
vised and rule-based approaches choose between
when to seek more information and when to adapt.
In this study, we show that using reinforcement
learning this decision is learned automatically.

Reinforcement Learning (RL) has been suc-

cessfully used for learning dialogue management
policies since (Levin et al., 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn information
presentation and recommendation strategies using
reinforcement learning (Rieser and Lemon, 2009;
Hernandez et al., 2003; Rieser and Lemon, 2010),
and joint optimisation of Dialogue Management
and NLG using hierarchical RL has been pro-
posed by (Lemon, 2010). In contrast, we present a
framework to learn to choose appropriate referring
expressions based on a user’s domain knowledge.
Earlier, we reported a proof-of-concept work us-
ing a hand-coded rule-based user simulation (Ja-
narthanam and Lemon, 2009c).

3 The Wizard-of-Oz Corpus

We use a corpus of technical support dialogues
collected from real human users using a Wizard-
of-Oz method (Janarthanam and Lemon, 2009b).
The corpus consists of 17 dialogues from users
who were instructed to physically set up a home
broadband connection using objects like a wire-
less modem, cables, filters, etc. They listened to
the instructions from the system and carried them
out using the domain objects laid in front of them.
The human ‘wizard’ played the role of only an in-
terpreter who would understand what the user said
and annotate it as a dialogue act. The set-up ex-
amined the effect of using three types of referring
expressions (jargon, descriptive, and tutorial), on
the users.

Out of the 17 dialogues, 6 used a jargon strat-
egy, 6 used a descriptive strategy, and 5 used a
tutorial strategy1. The task had reference to 13
domain entities, mentioned repeatedly in the di-
alogue. In total, there are 203 jargon, 202 descrip-
tive and 167 tutorial referring expressions. Inter-
estingly, users who weren’t acquainted with the
domain objects requested clarification on some of
the referring expressions used. The dialogue ex-
changes between the user and system were logged
in the form of dialogue acts and the system’s
choices of referring expressions. Each user’s
knowledge of domain entities was recorded both
before and after the task and each user’s interac-

1The tutorial strategy uses both jargon and descriptive ex-
pressions together.

70



tions with the environment were recorded. We use
the dialogue data, pre-task knowledge tests, and
the environment interaction data to train a user
simulation model. Pre and post-task test scores
were used to model the learning behaviour of the
users during the task (see section 5).

The corpus also recorded the time taken to com-
plete each dialogue task. We used these data to
build a regression model to calculate total dialogue
time for dialogue simulations. The strategies were
never mixed (with some jargon, some descriptive
and some tutorial expressions) within a single con-
versation. Therefore, please note that the strate-
gies used for data collection were not adaptive and
the human ‘wizard’ has no role in choosing which
referring expression to present to the user. Due to
this fact, no user score regarding adaptation was
collected. We therefore measure adaptation objec-
tively as explained in section 6.1.

4 The Dialogue System

In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.

Figure 1: System User Interaction (learning)

4.1 Dialogue Manager
The dialogue manager identifies the next instruc-
tion (dialogue act) to give to the user based on the

dialogue management policy πdm. Since, in this
study, we focus only on learning the REG policy,
the dialogue management is coded in the form of
a finite state machine. In this dialogue task, the
system provides two kinds of instructions - ob-
servation and manipulation. For observation in-
structions, users observe the environment and re-
port back to the system, and for the manipulation
instructions (such as plugging in a cable in to a
socket), they manipulate the domain entities in the
environment. When the user carries out an instruc-
tion, the system state is updated and the next in-
struction is given. Sometimes, users do not under-
stand the referring expressions used by the system
and then ask for clarification. In such cases, the
system provides clarification on the referring ex-
pression (provide clar), which is information to
enable the user to associate the expression with
the intended referent. The system action As,t (t
denoting turn, s denoting system) is therefore to
either give the user the next instruction or a clarifi-
cation. When the user responds in any other way,
the instruction is simply repeated. The dialogue
manager is also responsible for updating and man-
aging the system state Ss,t (see section 4.2). The
system interacts with the user by passing both the
system action As,t and the referring expressions
RECs,t (see section 4.3).

4.2 The dialogue state

The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
with a state where the system does not have any
idea about the user. As the conversation pro-
gresses, the dialogue manager records the evi-
dence presented to it by the user in terms of his
dialogue behaviour, such as asking for clarifica-
tion and interpreting jargon. Since the model is
updated according to the user’s behaviour, it may
be inaccurate if the user’s behaviour is itself uncer-
tain. So, when the user’s behaviour changes (for
instance, from novice to expert), this is reflected
in the user model during the conversation. Hence,
unlike previous studies mentioned in section 2, the
user model used in this system is not always an ac-
curate model of the user’s knowledge and reflects
a level of uncertainty about the user.

71



Each jargon referring expression x is repre-
sented by a three valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm. Initially each variable is set to
not sure. If the user responds to an instruction
containing the referring expression x with a clari-
fication request, then user knows x is set to no.
Similarly, if the user responds with appropriate in-
formation to the system’s instruction, the dialogue
manager sets user knows x is set to yes.

The dialogue manager updates the variables
concerning the referring expressions used in the
current system utterance appropriately after the
user’s response each turn. The user may have the
capacity to learn jargon. However, only the user’s
initial knowledge is recorded. This is based on the
assumption that an estimate of the user’s knowl-
edge helps to predict the user’s knowledge of the
rest of the referring expressions. Another issue
concerning the state space is its size. Since, there
are 13 entities and we only model the jargon ex-
pressions, the state space size is 313.

4.3 REG module
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between the
two types of referring expressions - jargon and de-
scriptive. For example, the domain entity broad-
band filter can be referred to using the jargon ex-
pression “broadband filter” or using the descrip-
tive expression “small white box”2. We call this
the act of choosing the REG action. The tutorial
strategy was not investigated here since the corpus
analysis showed tutorial utterances to be very time
consuming. In addition, they do not contribute to
the adaptive behaviour of the system.

The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG mod-
ule learns to associate dialogue states with opti-
mal REG actions. This is represented by a REG

2We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. “broadband filter”).

policy πreg : UMs,t → RECs,t, which maps
the states of the dialogue (user model) to optimal
REG actions. The referring expression choices
RECs,t is a set of pairs identifying the refer-
ent R and the type of expression T used in the
current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression “small white box”.

RECs,t = {(R1, T1), ..., (Rn, Tn)}
In the evaluation mode, a trained REG policy in-

teracts with unknown users. It consults the learned
policy πreg to choose the referring expressions
based on the current user model.

5 User Simulations

In this section, we present user simulation models
that simulate the dialogue behaviour of a real hu-
man user. These external simulation models are
different from internal user models used by the
dialogue system. In particular, our model is the
first to be sensitive to a system’s choices of refer-
ring expressions. The simulation has a statistical
distribution of in-built knowledge profiles that de-
termines the dialogue behaviour of the user being
simulated. If the user does not know a referring
expression, then he is more likely to request clar-
ification. If the user is able to interpret the refer-
ring expressions and identify the references then
he is more likely to follow the system’s instruc-
tion. This behaviour is simulated by the action se-
lection models described below.

Several user simulation models have been pro-
posed for use in reinforcement learning of dia-
logue policies (Georgila et al., 2005; Schatzmann
et al., 2006; Schatzmann et al., 2007; Ai and Lit-
man, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. Earlier, we
presented a two-tier simulation trained on data
precisely for REG policy learning (Janarthanam
and Lemon, 2009a). However, it is not suited for
training on small corpus like the one we have at
our disposal. In contrast to the earlier model, we
now condition the clarification requests on the ref-
erent class rather than the referent itself to handle
data sparsity problem.

The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a
user action Au,t (u denoting user). This can ei-
ther be a clarification request (cr) or an instruction

72



response (ir). We used two kinds of action selec-
tion models: corpus-driven statistical model and
hand-coded rule-based model.

5.1 Corpus-driven action selection model
In the corpus-driven model, the US produces a
clarification request cr based on the class of the
referent C(Ri), type of the referring expression
Ti, and the current domain knowledge of the user
for the referring expression DKu,t(Ri, Ti). Do-
main entities whose jargon expressions raised clar-
ification requests in the corpus were listed and
those that had more than the mean number of clar-
ification requests were classified as difficult
and others as easy entities (for example, “power
adaptor” is easy - all users understood this
expression, “broadband filter” is difficult).
Clarification requests are produced using the fol-
lowing model.

P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ∈ RECs,t

One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.

P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-

sponse which is determined by the system action
As,t. Instruction responses can be different in dif-
ferent conditions. For an observe and report in-
struction, the user issues a provide info action
and for a manipulation instruction, the user re-
sponds with an acknowledgement action and so
on.

P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-

pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. According to the data, clarification requests
are much more likely when jargon expressions
are used to refer to the referents that belong to
the difficult class and which the user doesn’t

livebox = 1 power adaptor = 1
wall phone socket = 1 broadband filter = 0
broadband cable = 0 ethernet cable = 1
lb power light = 1 lb power socket = 1
lb broadband light = 0 lb ethernet light = 0
lb adsl socket = 0 lb ethernet socket = 0
pc ethernet socket = 1

Table 2: Domain knowledge: an Intermediate
User

know about. When the system uses expressions
that the user knows, the user generally responds
to the instruction given by the system. These user
simulation models have been evaluated and found
to produce behaviour that is very similar to the
original corpus data, using the Kullback-Leibler
divergence metric (Cuayahuitl, 2009).

5.2 Rule-based action selection model

We also built a rule-based simulation using the
above models but where some of the parameters
were set manually instead of estimated from the
data. The purpose of this simulation is to in-
vestigate how learning with a data-driven statisti-
cal simulation compares to learning with a simple
hand-coded rule-based simulation. In this simula-
tion, the user always asks for a clarification when
he does not know a jargon expression (regardless
of the class of the referent) and never does this
when he knows it. This enforces a stricter, more
consistent behaviour for the different knowledge
patterns, which we hypothesise should be easier to
learn to adapt to, but may lead to less robust REG
policies.

5.3 User Domain knowledge

The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. The initial knowledge base
(DKu,initial) for an intermediate user is shown in
table 2. A novice user knows only “power adap-
tor”, and an expert knows all the jargon expres-
sions. We assume that users can interpret the de-
scriptive expressions and resolve their references.
Therefore, they are not explicitly represented. We
only code the user’s knowledge of jargon expres-
sions. This is represented by a boolean variable
for each domain entity.

73



Corpus data shows that users can learn jargon
expressions during the conversation. The user’s
domain knowledge DKu is modelled to be dy-
namic and is updated during the conversation.
Based on our data, we found that when presented
with clarification on a jargon expression, users al-
ways learned the jargon.

if As,t = provide clar(Ri, Ti)
DKu,t+1(Ri, Ti) ← 1

Users also learn when jargon expressions are re-
peatedly presented to them. Learning by repetition
follows the pattern of a learning curve - the greater
the number of repetitions #(Ri, Ti), the higher the
likelihood of learning. This is modelled stochas-
tically based on repetition using the parameter
#(Ri, Ti) as follows (where (Ri, Ti) ∈ RECs,t) .

P (DKu,t+1(Ri, Ti) ← 1|#(Ri, Ti))
The final state of the user’s domain knowl-

edge (DKu,final) may therefore be different from
the initial state (DKu,initial) due to the learn-
ing effect produced by the system’s use of jar-
gon expressions. In most studies done previously,
the user’s domain knowledge is considered to be
static. However in real conversation, we found that
the users nearly always learned jargon expressions
from the system’s utterances and clarifications.

6 Training

The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. As
shown in figure 1, the learning agent (REG mod-
ule) is given a reward at the end of every dialogue.
During the training session, the learning agent ex-
plores different ways to maximize the reward. In
this section, we discuss how to code the learning
agent’s goals as reward. We then discuss how the
reward function is used to train the learning agent.

6.1 Reward function

A reward function generates a numeric reward for
the learning agent’s actions. It gives high rewards
to the agent when the actions are favourable and
low rewards when they are not. In short, the re-
ward function is a representation of the goal of the
agent. It translates the agent’s actions into a scalar
value that can be maximized by choosing the right
action sequences.

We designed a reward function for the goal of
adapting to each user’s domain knowledge. We
present the Adaptation Accuracy score AA that
calculates how accurately the agent chose the ex-
pressions for each referent r, with respect to the
user’s knowledge. Appropriateness of an expres-
sion is based on the user’s knowledge of the ex-
pression. So, when the user knows the jargon ex-
pression for r, the appropriate expression to use is
jargon, and if s/he doesn’t know the jargon, an de-
scriptive expression is appropriate. Although the
user’s domain knowledge is dynamically chang-
ing due to learning, we base appropriateness on
the initial state, because our objective is to adapt to
the initial state of the user DKu,initial. However,
in reality, designers might want their system to ac-
count for user’s changing knowledge as well. We
calculate accuracy per referent RAr as the ratio
of number of appropriate expressions to the total
number of instances of the referent in the dialogue.
We then calculate the overall mean accuracy over
all referents as shown below.

RAr =
#(appropriate expressions(r))

#(instances(r))

AdaptationAccuracyAA = 1#(r)ΣrRAr

Note that this reward is computed at the end of
the dialogue (it is a ‘final’ reward), and is then
back-propagated along the action sequence that
led to that final state. Thus the reward can be com-
puted for each system REG action, without the
system having access to the user’s initial domain
knowledge while it is learning a policy.

Since the agent starts the conversation with
no knowledge about the user, it may try to use
more exploratory moves to learn about the user,
although they may be inappropriate. However,
by measuring accuracy to the initial user state,
the agent is encouraged to restrict its exploratory
moves and start predicting the user’s domain
knowledge as soon as possible. The system should
therefore ideally explore less and adapt more to
increase accuracy. The above reward function re-
turns 1 when the agent is completely accurate in
adapting to the user’s domain knowledge and it
returns 0 if the agent’s REC choices were com-
pletely inappropriate. Usually during learning, the
reward value lies between these two extremes and
the agent tries to maximize it to 1.

74



6.2 Learning

The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. Two types of simulations were
used as described above: Data-driven and Hand-
coded. Both user simulations were calibrated to
produce three types of users: Novice, Int2 (in-
termediate) and Expert, randomly but with equal
probability. Novice users knew just one jargon
expression, Int2 knew seven, and Expert users
knew all thirteen jargon expressions. There was
an underlying pattern in these knowledge profiles.
For example, Intermediate users were those who
knew the commonplace domain entities but not
those specific to broadband connection. For in-
stance, they knew “ethernet cable” and “pc ether-
net socket” but not “broadband filter” and “broad-
band cable”.

Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring expres-
sions are chosen, the system presents the user sim-
ulation with both the dialogue act and referring ex-
pression choices. The choice of referring expres-
sion affects the user’s dialogue behaviour which in
turn makes the dialogue manager update the user
model. For instance, choosing a jargon expres-
sion could evoke a clarification request from the
user, which in turn prompts the dialogue manager
to update the user model with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user’s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations

to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy.

Figure 2 shows how the agent learns using the
data-driven (Learned DS) and hand-coded simu-
lations (Learned HS) during training. It can be
seen in the figure 2 that towards the end the curve
plateaus signifying that learning has converged.

Figure 2: Learning curves - Training

7 Evaluation

In this section, we present the evaluation metrics
used, the baseline policies that were hand-coded
for comparison, and the results of evaluation.

7.1 Metrics

In addition to the adaptation accuracy mentioned
in section 6.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We calculate the time
taken (Time) for the user to complete the dialogue
task. This is calculated using a regression model
from the corpus based on number of words, turns,
and mean user response time. We also measure
the (normalised) learning gain (LG) produced by
using unknown jargon expressions. This is calcu-
lated using the pre and post scores from the user
domain knowledge (DKu) as follows.

Learning Gain LG = Post−Pre1−Pre

75



7.2 Baseline REG policies
In order to compare the performance of the learned
policy with hand-coded REG policies, three sim-
ple rule-based policies were built. These were
built in the absence of expert domain knowledge
and a expert-layperson corpus.

• Jargon: Uses jargon for all referents by de-
fault. Provides clarifications when requested.

• Descriptive: Uses descriptive expressions for
all referents by default.

• Switching: This policy starts with jargon
expressions and continues using them until
the user requests for clarification. It then
switches to descriptive expressions and con-
tinues to use them until the user complains.
In short, it switches between the two strate-
gies based on the user’s responses.

All the policies exploit the user model in sub-
sequent references after the user’s knowledge of
the expression has been set to either yes or no.
Therefore, although these policies are simple, they
do adapt to a certain extent, and are reasonable
baselines for comparison in the absence of expert
knowledge for building more sophisticated base-
lines.

7.3 Results
The policies were run under a testing condition
(where there is no policy learning or exploration)
using a data-driven simulation calibrated to simu-
late 5 different user types. In addition to the three
users - Novice, Expert and Int2, from the train-
ing simulations, two other intermediate users (Int1
and Int3) were added to examine how well each
policy handles unseen user types. The REG mod-
ule was operated in evaluation mode to produce
around 200 dialogues per policy distributed over
the 5 user groups.

Overall performance of the different policies in
terms of Adaptation Accuracy (AA), Time and
Learning Gain (LG) are given in Table 3. Fig-
ure 3 shows how each policy performs in terms of
accuracy on the 5 types of users.

We found that the Learned DS policy (i.e.
learned with the data-driven user simulation) is
the most accurate (Mean = 79.70, SD = 10.46)
in terms of adaptation to each user’s initial state
of domain knowledge. Also, it is the only pol-
icy that has more or less the same accuracy scores

Figure 3: Evaluation - Adaptation Accuracy

Policies AA Time T LG
Descriptive 46.15 7.44 0
Jargon 74.54 9.15* 0.97*

Switching 62.47 7.48 0.30
Learned HS 69.67 7.52 0.33
Learned DS 79.70* 8.08* 0.63*

* Significantly different from all oth-
ers (p < 0.05).

Table 3: Evaluation on 5 user types

over all different user types (see figure 3). It
should also be noted that the it generalised well
over user types (Int1 and Int3) which were un-
seen in training. Learned DS policy outperforms
all other policies: Learned HS (Mean = 69.67, SD
= 14.18), Switching (Mean = 62.47, SD = 14.18),
Jargon (Mean = 74.54, SD = 17.9) and Descrip-
tive (Mean = 46.15, SD = 33.29). The differences
between the accuracy (AA) of the Learned DS pol-
icy and all other policies were statistically signif-
icant with p < 0.05 (using a two-tailed paired t-
test). Although Learned HS policy is similar to
the Learned DS policy, as shown in the learning
curves in figure 2, it does not perform as well
when confronted with users types that it did not
encounter during training. The Switching policy,
on the other hand, quickly switches its strategy
(sometimes erroneously) based on the user’s clar-
ification requests but does not adapt appropriately
to evidence presented later during the conversa-
tion. Sometimes, this policy switches erroneously
because of the uncertain user behaviours. In con-
trast, learned policies continuously adapt to new
evidence. The Jargon policy performs better than

76



the Learned HS and Switching policies. This be-
cause the system can learn more about the user
by using more jargon expressions and then use
that knowledge for adaptation for known referents.
However, it is not possible for this policy to pre-
dict the user’s knowledge of unseen referents. The
Learned DS policy performs better than the Jargon
policy, because it is able to accurately predict the
user’s knowledge of referents unseen in the dia-
logue so far.

The learned policies are a little more time-
consuming than the Switching and Descriptive
policies but compared to the Jargon policy,
Learned DS takes 1.07 minutes less time. This is
because learned policies use a few jargon expres-
sions (giving rise to clarification requests) to learn
about the user. On the other hand, the Jargon pol-
icy produces more user learning gain because of
the use of more jargon expressions. Learned poli-
cies compensate on time and learning gain in order
to predict and adapt well to the users’ knowledge
patterns. This is because the training was opti-
mized for accuracy of adaptation and not for learn-
ing gain or time taken. The results show that using
our RL framework, REG policies can be learned
using data-driven simulations, and that such a pol-
icy can predict and adapt to a user’s knowledge
pattern more accurately than policies trained us-
ing hand-coded rule-based simulations and hand-
coded baseline policies.

7.4 Discussion

The learned policies explore the user’s expertise
and predict their knowledge patterns, in order to
better choose expressions for referents unseen in
the dialogue so far. The system learns to iden-
tify the patterns of knowledge in the users with
a little exploration (information seeking moves).
So, when it is provided with a piece of evidence
(e.g. the user knows “broadband filter”), it is able
to accurately estimate unknown facts (e.g. the user
might know “broadband cable”). Sometimes, its
choices are wrong due to incorrect estimation of
the user’s expertise (due to stochastic behaviour
of the users). In such cases, the incorrect adapta-
tion move can be considered to be an information
seeking move. This helps further adaptation us-
ing the new evidence. By continuously using this
“seek-predict-adapt” approach, the system adapts
dynamically to different users. Therefore, with
a little information seeking and better prediction,

the learned policies are able to better adapt to users
with different domain expertise.

In addition to adaptation, learned policies learn
to identify when to seek information from the user
to populate the user model (which is initially set
to not sure). It should be noted that the sys-
tem cannot adapt unless it has some information
about the user and therefore needs to decisively
seek information by using jargon expressions. If
it seeks information all the time, it is not adapting
to the user. The learned policies therefore learn to
trade-off between information seeking moves and
adaptive moves in order to maximize the overall
adaptation accuracy score.

8 Conclusion

In this study, we have shown that user-adaptive
REG policies can be learned from a small cor-
pus of non-adaptive dialogues between a dialogue
system and users with different domain knowl-
edge levels. We have shown that such adaptive
REG policies learned using a RL framework adapt
to unknown users better than simple hand-coded
policies built without much input from domain ex-
perts or from a corpus of expert-layperson adap-
tive dialogues. The learned, adaptive REG poli-
cies learn to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. Learned
policies start the conversation with information
seeking moves, learn a little about the user, and
start adapting dynamically as the conversation
progresses. We have also shown that a data-driven
statistical user simulation produces better policies
than a simple hand-coded rule-based simulation,
and that the learned policies generalise well to un-
seen users.

In future work, we will evaluate the learned
policies with real users to examine how well
they adapt, and examine how real users evalu-
ate them (subjectively) in comparison to baselines.
Whether the learned policies perform better or as
well as a hand-coded policy painstakingly crafted
by a domain expert (or learned using supervised
methods from an expert-layperson corpus) is an
interesting question that needs further exploration.
Also, it would also be interesting to make the
learned policy account for the user’s learning be-
haviour and adapt accordingly.

77



Acknowledgements

The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.

References
H. Ai and D. Litman. 2007. Knowledge consistent

user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.

T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.

A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145–204.

A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221–247.

H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.

H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.

R. Dale. 1989. Cooking up referring expressions. In
Proc. ACL-1989.

K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.

F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.

E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26–37.

S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial’09.

S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG’09.

S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG’09.

O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).

E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.

K. McKeown, J. Robin, and M. Tanenblatt. 1993. Tai-
loring Lexical Choice to the User’s Vocabulary in
Multimedia Explanation Generation. In Proc. ACL
1993.

C. L. Paris. 1987. The Use of Explicit User Models
in Text Generations: Tailoring to a User’s Level of
Expertise. Ph.D. thesis, Columbia University.

E. Reiter. 1991. Generating Descriptions that Exploit a
User’s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257–285. Academic
Press.

V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL’09.

V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).

J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97–126.

J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.

D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.

R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.

78


