



















































LTG at SemEval-2016 Task 11: Complex Word Identification with Classifier Ensembles


Proceedings of SemEval-2016, pages 996‚Äì1000,
San Diego, California, June 16-17, 2016. c¬©2016 Association for Computational Linguistics

LTG at SemEval-2016 Task 11: Complex Word Identification with Classifier
Ensembles

Shervin Malmasi1 Mark Dras1 Marcos Zampieri2,3
1Macquarie University, Sydney, NSW, Australia

2Saarland University, Germany
3German Research Center for Artificial Intelligence, Germany

{first.last}@mq.edu.au, marcos.zampieri@dfki.de

Abstract

We present the description of the LTG entry
in the SemEval-2016 Complex Word Identi-
fication (CWI) task, which aimed to develop
systems for identifying complex words in En-
glish sentences. Our entry focused on the use
of contextual language model features and the
application of ensemble classification meth-
ods. Both of our systems achieved good per-
formance, ranking in 2nd and 3rd place overall
in terms of F-Score.

1 Introduction

Complex Word Identification (CWI) is the task of
identifying complex words in texts using computa-
tional methods (Shardlow, 2013). The task is usu-
ally carried out as part of lexical and text simpli-
fication systems. Shardlow (2014) considers CWI
as the first processing step in lexical simplification
pipelines. Complex or difficult words should first be
identified so they can be later substituted by simpler
ones to improve text readability.

CWI has gained more importance in the last
decade as lexical and text simplification systems
have been developed or tailored for a number of pur-
poses. They have been applied to make texts more
accessible to language learners (Petersen and Os-
tendorf, 2007); other researchers have explored text
simplification strategies targeted at populations with
low literacy skills (Aluƒ±ÃÅsio et al., 2008). Finally, an-
other relevant application of text simplification are
people with dyslexia (Rello et al., 2013).

The SemEval 2016 Task 11: Complex Word Iden-
tification (CWI) provides an interesting opportunity

to evaluate methods and approaches for this task.
The organizers proposed a binary text classification
task in which participants were required label words
in English sentences as either complex (1) or simple
(0). The task organizers provided participants with a
training set containing sentences annotated with this
information, followed by an unlabeled test set for
evaluation. The assessment of whether words in a
sentence are complex or simple was performed by
human annotators required to label the data.1

2 Data

Based on the information available at the shared
task‚Äôs website2: ‚Äú400 annotators were presented
with several sentences and asked to select which
words they did not understand their meaning.‚Äù

The CWI task dataset was divided as follows:

‚Ä¢ Training set: 2,237 judgments by 20 annota-
tors over 200 sentences. A word is considered
complex if at least one of the 20 annotators as-
signed it as so.

‚Ä¢ Test set: 88,221 judgments made over 9,000
sentences (1 annotator per sentence).

1Here the term complex is used as a synonym for difficult.
Unlike the Morphology term complex (antonym of simplex)
that defines compound words or words composed of multiple
morphs (Adams, 2001).

2http://alt.qcri.org/semeval2016/task11/

996



3 Methodology

The primary focus of our team‚Äôs entry was the use of
judgements from different annotators to create train-
ing data. We looked at how adjusting the thresh-
old for inter-annotator agreement would affect the
results and whether the combination of data created
using different threshold values could improve per-
formance.

Initially, the training data released by the organiz-
ers was labeled in a way that a word was marked as
complex if any annotator judged it so. During the
course of the shared task the organizers released ad-
ditional information about the training data, chiefly
the individual judgements of the 20 annotators that
were used to derive the final labels for each word.

We attempted to use this data in our system. Dur-
ing development we noted that by increasing this
threshold to two, the performance of our system un-
der cross-validation improved by a small amount.
Accordingly, we pursued this direction as the main
focus of our experiments.

3.1 Classifiers

We utilize a decision tree classifier, which we found
to perform better than Support Vector Machine
(SVM) and Naƒ±Ãàve Bayes classifiers for this data.

3.2 Features

Our core set of features are based on estimating n-
gram probabilities using web-scale language mod-
els. More specifically, this data was sourced from
the Microsoft Web N-Gram Service3, although we
should note that this service has been deprecated
and replaced since the shared task.4 These language
models are trained on web-scale corpora collected
by Microsoft‚Äôs Bing search engine from crawling
English web pages.

Given a target word wt, we extract several prob-
ability estimates to use as classification features.
These estimates, which we describe below, use the
target word as well its preceding and following
words, as shown in Figure 1.

3http://weblm.research.microsoft.com/
4It has been replaced by Microsoft‚Äôs Project Oxford:

https://www.projectoxford.ai/weblm

3.2.1 Word Probability
This is an estimate of how likely the target word

is to occur in the language model:

P (wt)

Rarer words would be assigned lower values and
thus this feature can help quantify word frequency
for the classifier.

3.2.2 Conditional Probability
We calculate the bigram probability of wt:

P (wt | wt‚àí1)

Similarly, we estimate the trigram probability:

P (wt | wt‚àí1, wt‚àí2)

These values estimate the likelihood of the target
word occurring given the previous one or two words.
They can help quantify if the word is being used in
a common or less frequent context.

3.2.3 Joint Probability
We also use the following joint probability esti-

mates of the target word and its surrounding words:

P (wt‚àí1, wt‚àí2, wt)

P (wt‚àí1, wt)

P (wt‚àí1, wt, wt+1)

P (wt, wt+1)

P (wt, wt+1, wt+2)

The intuition underlying the use of all of these
n-gram language model features is that the under-
standing of certain words depends on the context
they appear in. A large number of English words
are polysemous and their classification, without tak-
ing into account the specific sense being use, could
lead to misclassifications. This can occur in scenar-
ios where a learner knows the most frequently used
sense of a polysemous word, but is confronted with
a different sense that they have not encountered be-
fore. Additionally, even if a known word is used in
an unusual context, it could be a cause of confusion
for learners.

997



This cavity is formed by the mantle skirt , a double fold of mantle which [...]

the mantle skirt a double

ùë§ùë§ùë°ùë°‚àí2 ùë§ùë§ùë°ùë°‚àí1 ùë§ùë§ùë°ùë° ùë§ùë§ùë°ùë°+1 ùë§ùë§ùë°ùë°+2

Figure 1: An example of the context extracted for a target word, which is ‚Äúskirt‚Äù in this example.

3.2.4 Word Length
Guided by the intuition that the most frequent

words in a language are usually shorter, we use the
length of a word as a classification feature.

4 Ensemble Classifiers

Classifier ensembles are a way of combining differ-
ent classifiers or experts with the goal of improving
accuracy through enhanced decision making. They
have been applied to a wide range of real-world
problems and shown to achieve better results com-
pared to single-classifier methods (Oza and Tumer,
2008). Through aggregating the outputs of multi-
ple classifiers in some way, their outputs are gener-
ally considered to be more robust. Ensemble meth-
ods continue to receive increasing attention from
researchers and remain a focus of much machine
learning research (WozÃÅniak et al., 2014; Kuncheva
and Rodrƒ±ÃÅguez, 2014).

Such ensemble-based systems often use a paral-
lel architecture, as illustrated in Figure 2, where the
classifiers are run independently and their outputs
are aggregated using a fusion method. For exam-
ple, Bagging (bootstrap aggregating) is a commonly
used method for ensemble generation (Breiman,
1996) that can create multiple base classifiers.

Input 

Classifier 1 

Classifier 2 
Combiner Decision 

‚Ä¶ 

Classifier N 

Ensemble Architecture 
2 

Figure 2: An example of parallel classifier ensemble architec-
ture where N independent classifiers provide predictions which

are then fused using an ensemble combination method.

It works by creating multiple bootstrap training
sets from the original training data and a separate
classifier is trained from each one of these sets. The
generated classifiers are said to be diverse because
each training set is created by sampling with re-
placement and contains a random subset of the orig-
inal data.

Other, more sophisticated, ensemble methods that
rely on meta-learning may employ a stacked archi-
tecture where the output from a first set of classifiers
is fed into a second level meta-classifier and so on.

The first part of creating an ensemble is generat-
ing the individual classifiers. Various methods for
creating these ensemble elements have been pro-
posed. These involve using different algorithms, pa-
rameters or feature types; applying different prepro-
cessing or feature scaling methods and varying (e.g.
distorting or resampling) the training data.

5 Systems

In this section we describe the two systems we cre-
ated and entered in the shared task.

5.1 System 1

Our first system was based on decision tree classi-
fier trained on data where the minimum threshold for
inter-annotator agreement was set to 3. Given that
the testing data was only annotated by a single rater,
we did not want to pick a value that was too high,
even though this could improve cross-validation per-
formance on the training data.

Additionally, we converted this setup to an en-
semble by creating 100 randomized decision tree
classifiers by using bagging, which we described
earlier. The decisions of these learners were fused
via plurality voting to yield the final label for an in-
stance.

998



Rank Team System Accuracy Precision Recall F-score G-score
1 PLUJAGH SEWDFF 0.922 0.289 0.453 0.353 0.608
2 LTG System2 0.889 0.220 0.541 0.312 0.672
3 LTG System1 0.933 0.300 0.321 0.310 0.478
4 MAZA B 0.912 0.243 0.420 0.308 0.575
5 HMC DecisionTree25 0.846 0.189 0.698 0.298 0.765
6 TALN RandomForest SIM.output 0.847 0.186 0.673 0.292 0.750
7 HMC RegressionTree05 0.838 0.182 0.705 0.290 0.766
8 MACSAAR RFC 0.825 0.168 0.694 0.270 0.754
9 TALN RandomForest WEI.output 0.812 0.164 0.736 0.268 0.772
10 UWB All 0.803 0.157 0.734 0.258 0.767

Table 1: The top 10 systems in task, ranked by their F-score.

5.2 System 2

For our second system we extended the threshold-
based approach to an ensemble of decision trees
trained on different data.

We created four individual classifiers, each
trained with a different minimum threshold5 rang-
ing between 1‚Äì4. The outputs of these classifiers, a
binary prediction, were then combined using a plu-
rality voting combiner. It should also be noted that
having an even number of base classifiers also intro-
duces the possibility of ties occurring.

6 Results

The top 10 task submissions, ranked by the F-score,
are shown in Table 1 with our systems highlighted.
Both of our systems achieved very competitive re-
sults, ranking in second and third place overall.

Our second system, an ensemble of classifiers
trained on distinct data derived using different lev-
els of inter-rater agreement, performed slightly bet-
ter than the first system. This could be interpreted
as this evidence the second approach is slightly
better, and we hypothesize that combining annota-
tions from different combinations of annotators may
help the classifier learn reliable models of the phe-
nomenon, since individual annotations (as well as
the original combined annotation) were noisy. How-
ever, determining this requires further experiments.
This is due to the fact that with four classifiers in the
ensemble, voting resulted in a tie for some 6% of the
testing data. These ties were broken arbitrarily, in-
troducing an element of stochasticity to our results.

In hindsight this does not appear to have been the

5Setting the threshold to 1 is equivalent to using the original
training data.

most intuitive or robust way of dealing with such ties
since the distribution of classes is not balanced. In
fact, this distribution is highly skewed, as we discuss
in the next section.

6.1 Conclusion

We developed two ensemble-based systems for this
task, both of which achieved competitive results in
the final rankings. Our results indicate that the use
of contextual features, as well as language models,
are promising for this task.

Analysis of the gold standard labels release af-
ter the task shows that only 4.7% of the 88k sam-
ples belonged to the positive class. This is a very
highly skewed distribution that can make it hard to
train effective classifiers. It also means that accuracy
cannot be used as the sole evaluation metric here; a
balanced measure of precision and recall like the F-
score is required. Alternatively, the balanced accu-
racy measure (Brodersen et al., 2010) could also be
used. Such a high data imbalance can result in train-
ing classifiers that are biased towards the majority
class. This bias can be more problematic if the distri-
bution of classes is different in the test set. Accord-
ingly, future work in this area could look at the use
of methods for dealing with unbalanced datasets (He
and Garcia, 2009). The application of such methods,
in conjunction with ensembles, could potentially re-
sult in greater performance.

Future work could attempt to integrate additional
language resources for this task. Analyzing the text
produced by learners could provide insight into the
limitations of learner vocabulary. Learner corpora,
widely used in the task of Native Language Identifi-
cation (Malmasi and Dras, 2014; Malmasi and Dras,
2015) could be useful here.

999



Acknowledgments

We would like to thank the CWI task organizers for
managing the organization of this event. We also
thank the anonymous reviewers for their insightful
comments.

References
Valerie Adams. 2001. Complex words in English. Rout-

ledge.
Sandra M Aluƒ±ÃÅsio, Lucia Specia, Thiago AS Pardo, Er-

ick G Maziero, and Renata PM Fortes. 2008. Towards
brazilian portuguese automatic text simplification sys-
tems. In Proceedings of DocEng.

Leo Breiman. 1996. Bagging predictors. In Machine
Learning, pages 123‚Äì140.

Kay H Brodersen, Cheng Soon Ong, Klaas E Stephan,
and Joachim M Buhmann. 2010. The balanced accu-
racy and its posterior distribution. In Proceedings of
ICPR.

Haibo He and Edwardo A Garcia. 2009. Learning from
Imbalanced Data. IEEE Transactions on Knowledge
and Data Engineering,, 21(9):1263‚Äì1284.

Ludmila I Kuncheva and Juan J Rodrƒ±ÃÅguez. 2014. A
weighted voting framework for classifiers ensembles.
Knowledge and Information Systems, 38(2):259‚Äì275.

Shervin Malmasi and Mark Dras. 2014. Chinese Native
Language Identification. In Proceedings of EACL.

Shervin Malmasi and Mark Dras. 2015. Multilingual
Native Language Identification. In Natural Language
Engineering.

Nikunj C Oza and Kagan Tumer. 2008. Classifier en-
sembles: Select real-world applications. Information
Fusion, 9(1):4‚Äì20.

Sarah E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis. In
Proceedings of SLaTE.

Luz Rello, Ricardo Baeza-Yates, Stefan Bott, and Hora-
cio Saggion. 2013. Simplify or help?: text simplifi-
cation strategies for people with dyslexia. In Proceed-
ings of W4A.

Matthew Shardlow. 2013. A comparison of techniques to
automatically identify complex words. In Proceedings
of the ACL Student Research Workshop.

Matthew Shardlow. 2014. A survey of automated text
simplification. International Journal of Advanced
Computer Science and Applications (IJACSA), (Spe-
cial Issue on Natural Language Processing).

Micha≈Ç WozÃÅniak, Manuel GranÃÉa, and Emilio Corchado.
2014. A survey of multiple classifier systems as hybrid
systems. Information Fusion, 16:3‚Äì17.

1000


