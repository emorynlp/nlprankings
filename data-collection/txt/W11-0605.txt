










































Top-Down Recognizers for MCFGs and MGs


Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 39–48,
Portland, Oregon, June 2011. c©2011 Association for Computational Linguistics

Top-down recognizers for MCFGs and MGs

Edward P. Stabler

stabler@ucla.edu

Abstract

This paper defines a normal form for MCFGs

that includes strongly equivalent representa-

tions of many MG variants, and presents

an incremental priority-queue-based TD rec-

ognizer for these MCFGs. After introduc-

ing MGs with overt phrasal movement, head

movement and simple adjunction are added

without change in the recognizer. The MG

representation can be used directly, so that

even rather sophisticated analyses of properly

non-CF languages can be defined very suc-

cinctly. As with the similar stack-based CF-

methods, finite memory suffices for the recog-

nition of infinite languages, and a fully con-

nected left context for probabilistic analysis is

available at every point.

1 Introduction

In the years after Joshi (1985) proposed that human

languages are weakly and strongly “mildly context

sensitive” (MCS), it was discovered that many in-

dependently proposed grammar formalisms define

exactly the same MCS languages. The languages

defined by Joshi’s tree adjoining grammars (TAGs)

are exactly the same as those defined by a ver-

sion of Steedman’s combinatory categorial gram-

mars, and the same as those defined by head wrap-

ping grammars (Vijay-Shanker and Weir, 1994). A

slightly larger class of languages is defined by an-

other variant of TAGs (set-local multicomponent),

by a version of Pollard’s generalized phrase struc-

ture grammars called multiple context free gram-

mars (MCFGs), and by a wide range of minimalist

grammar (MG) formalizations of Chomskian syn-

tax (Seki et al., 1991; Michaelis, 1998; Michaelis,

2001b; Harkema, 2001a; Stabler, 2011). These

remarkable convergences provide evidence from

across grammatical traditions that something like

these MCS proposals may be approximately right,

and so it is natural to consider psychological models

that fit with these proposals. With a range of per-

formance models for a range of MCS grammars, it

becomes possible to explore how grammatical de-

pendencies interact with other factors in the condi-

tioning of human linguistic performance.

For context free grammars (CFGs), perhaps the

simplest parsing model is top-down: beginning with

the prediction of a sentence, rules are applied to the

leftmost predicted category until a terminal element

is reached, which is then checked against the in-

put. This parsing method is of interest in psycho-

logical modeling not only because it uses the gram-

mar in a very transparent way, but also it is because

it is predictive in a way that may be similar to hu-

man parsing. At every point in analyzing a sentence

from left to right, the structure that has been con-

structed is fully connected: grammatical relation-

ships among the elements that have been heard have

been guessed, and there are no pieces of structure

which have not been integrated. Consequently, this

structure can be interpreted by a standard composi-

tional semantics and may be appropriate for “incre-

mental” models of sentence interpretation (cf. Had-

dock, 1989; Chambers et al., 2004; Shen and Joshi,

2005; Altmann and Mirković, 2009; Demberg and

Keller, 2009, Kato and Matsubara, 2009; Schuler,

2010). And like human parsing, when used with

39



backtracking or a beam search, TD memory de-

mands need not continually increase with sentence

length: a fixed bound on stack depth and on back-

track or beam depth suffices for infinitely many sen-

tences. Furthermore, TD parsing provides explicit,

relevant “left contexts” for probabilistic condition-

ing (Roark and Johnson, 1999; Roark, 2001; Roark,

2004). But it has not been clear until recently how

to apply this method to Chomskian syntax or any

of the other MCS grammar formalisms. There have

been some proposals along these lines, but they have

either been unnecessarily complex or applicable to

only a restricted to range of grammatical proposals

(Chesi, 2007; Mainguy, 2010).

This paper extends TD parsing to minimalist con-

text free grammars (MCFGs) in a certain normal

form and presents minimalist grammars (MGs) as a

succinct representation for some of those MCFGs.

With this extension, the TD parsing method han-

dles an infinite range of MCFGs that encompasses,

strongly and weakly, an infinite range of (many vari-

ants of) MGs in a very transparent and direct way.

The parsing method can be defined in complete de-

tail very easily, and, abstracting away from limita-

tions of time and memory, it is provably sound and

complete for all those grammars.

The TD recognizer for MCFGs is presented in §4,
generalizing and adapting ideas from earlier work

(Mainguy, 2010; Villemonte de la Clergerie, 2002).

Instead of using a stack memory, this recognizer

uses a “priority queue,” which just means that we

can access all the elements in memory, sorting them

into left-to-right order. Then it is easy to observe:

(§3.2) while the reference to MCFG is useful for
understanding the recognizer, an MG representation

can be used directly without explicitly computing

out its MCFG equivalent; (§5.1) the extensions for
head movement and simple adjunction allow the rec-

ognizer of §4 to apply without change; (§5.2) like its
stack-based CF counterpart, the MG recognizer re-

quires only finite memory to recognize certain infi-

nite subsets of languages – that is, memory demands

do not always strictly increase with sentence length;

and (§5.3) the TD recognizer provides, at every point
in processing the input, a fully connected left con-

text for interpretation and probabilistic condition-

ing, unlike LC and other familiar methods. Since

a very wide range of grammatical proposals can be

expressed in this formalism and parsed transparently

by this method, it is straightforward to compute fully

explicit and syntactically sophisticated parses of the

sorts of sentences used in psycholinguistic studies.

2 MCFGs

MCFGs are first defined by Seki et al. (1991),

but here it will be convenient to represent MCFGs

in a Prolog-like Horn clause notation, as in

Kanazawa (2009). In this notation, the familiar con-

text free rule for sentences would be written

S(x01x11) :- NP (x01),
V P (x11).

Reading :- as “if”, this formula says that a string

formed by concatenating any string x01 with string

x11 is an S, if x01 is an NP, and x11 is a VP. We

number the variables on the right side in such a way

as to indicate that each variable that appears on the

right side of any rule appears exactly once on the

right and once on the left. Lexical rules like

NP (Mary)
V P (sings),

have empty “right sides” and no variables in this no-

tation.

MCFGs allow categories to have multiple string

arguments, so that, for example, a VP with a wh-

phrase that is moving to another position could be

represented with two string arguments, one of which

holds the moving element. In general, each MCFG

rule for building an instance of category A from cat-

egories B0 . . . Bn (n ≥ 0) has the form,

A(t1, . . . , td(A)) :- B0(x01 , . . . , x0d(B0)),

. . . ,

Bn(xn1 , . . . , xnd(Bn)),

where each ti is an term (i.e. a sequence) over the (fi-

nite nonempty) vocabulary Σ and the variables that
appear on the right; no variable on the right occurs

more than once on the left (no copying); and the des-

ignated ‘start’ category S has ‘arity’ or ‘dimension’

d(S) = 1. For any such grammar, the language
L(G) is the set of strings s ∈ Σ∗ such that we can
derive S(s).

40



Here, we restrict attention to a normal form in

which (i) each MCFG rule is nondeleting in the

sense that every variable xij on the right occurs ex-

actly once on the left, and (ii) each rule is either lexi-

cal or nonlexical, where a lexical rule is one in which

n = 0 and d(A) = 1 and t1 ∈ Σ∪{ǫ}, and a nonlex-
ical rule is one in which n > 0 and each ti ∈ V ar

∗.

Clearly these additional restrictions do not affect the

expressive power of the grammars.

2.1 Example 1

Consider this MCFG for {aibjcidj | i, j > 0}, with 5
non-lexical rules, 4 lexical rules, and start category

S. We letter the rules for later reference:

a. S(x0x1x2x3) :- AC(x0, x2), BD(x1, x3)
b. AC(x0x2, x1x3) :- A(x0), C(x1), AC(x2, x3)
c. AC(x0, x1) :- A(x0), C(x1)
d. BD(x0x2, x1x3) :- B(x0),D(x1), BD(x2, x3)
e. BD(x0, x1) :- B(x0),D(x1)
f. A(a)
g. B(b)
h. C(c)
i. D(d)

With this grammar we can show that abbcdd has cat-

egory S with a derivation tree like this:

S(abbcdd)

AC(a, c)

A(a) C(c)

BD(bb, dd)

B(b) D(d) BD(b, d)

B(b) D(d)

See, for example, Kanazawa (2009) for a more de-

tailed discussion of MCFGs in this format.

3 MGs as MCFGs

Michaelis (1998; 2001a) shows that every MG has

a ‘strongly equivalent’ MCFG, in the sense that the

MG derivation trees are a relabeling of the MCFG

derivation trees. Here we present MGs as finite sets

of lexical rules that define MCFGs. MG categories

contain finite tuples of feature sequences, where the

features include categories like N,V,A,P,. . . , selec-

tors for those categories =N,=V,=A,=P,. . . , licensors

+case,+wh,. . . , and licensees -case,-wh,. . . . In our

MCFG representation, a category is a tuple

〈x, δ0, δ1, . . . , δj 〉

where (i) j ≥ 0, (ii) x = 1 if the element is lexical
and 0 otherwise, (iii) each δi is a nonempty feature

sequence, and (iv) the category has dimension j +1.
An MG is then given by a specified start category

and a finite set of lexical rules

〈 1, δ0 〉(a).

for some a ∈ Σ. The MG defines the language gen-
erated by its lexicon together with MCFG rules de-

termined by the lexicon, as follows. Let π2(Lex)
be the set of feature sequences δ0 contained in the

lexical rules, and let k be the number of differ-

ent types of licensees f that occur in the lexical

rules. For all 0 ≤ i, j ≤ k, all x, y ∈ {0, 1}, all
α, β, δi, γi ∈ suffix(π2(Lex)), and β 6= ǫ, we have
these ‘merge’ rules, broken as usual into the cases

where (i) we are merging into complement position

on the right, (ii) merging into specifier position on

the left, or (iii) merging with something that is mov-

ing:

〈 0, α, δ1, . . . , δj 〉(s0t0, t1, . . . , tj) :-
〈 1,=fα 〉(s0),
〈x, f, δ1, . . . , δj 〉(t0, . . . , tj)

〈 0, α, δ1, . . . , δi, γ1, . . . , γj 〉(t0s0, s1, . . . , si, t1, . . . , tj) :-
〈 0,=fα, δ1, . . . , δi, 〉(s0, . . . , si),
〈x, f, γ1, . . . , γj 〉(t0, . . . , tj)

〈 0, α, δ1, . . . , δi, β, γ1, . . . , γj 〉(s0, . . . , si, t0, . . . , tj) :-
〈x,=fα, δ1, . . . , δi, 〉(s0, . . . , si),
〈 y, fβ, γ1, . . . , γj 〉(t0, . . . , tj)

And we have these ‘move’ rules, broken as usual

into the cases where the moving element is landing,

when δi = -f ,

〈 0, α, δ1, . . . , δi−1, δi+1, . . . , δj 〉
(sis0, s1, . . . , si−1, si+1, . . . , sj) :-

〈 0,+fα, δ1, . . . , δj 〉(s0, . . . , sj),

and cases where the moving element must move

again, when δi = -fβ,

〈 0, α, δ1, . . . , δi−1, β, δi+1, . . . , δj 〉(s0, . . . , si) :-
〈 0,+fα, δ1, . . . , δj 〉(s0, . . . , si),

where none of δ1, . . . , δi−1, δi+1, . . . , δj begin with

-f . The language of the MG is the MCFL defined by

the lexicon and all instances of these 5 rule schemes

(always a finite set).

By varying the lexicon, MGs can define all the

MCFLs (Michaelis, 2001b; Harkema, 2001b), i.e.,

41



the set-local multi-component tree adjoining lan-

guages (MCTALs) (Weir, 1988; Seki et al., 1991).

TALs are a proper subset, defined by ‘well-nested

2-MCFGs’ (Seki et al., 1991; Kanazawa, 2009).

3.1 Example 2

Consider the following lexicon containing 7 items,

with the ‘complementizer’ start category C,

〈1, =D =D V〉(likes) 〈1, D〉(Mary)
〈1, =C =D V〉(knows) 〈1, D〉(John)
〈1, =V C〉(ǫ) 〈1, D -wh〉(who)
〈1, =V +wh C〉(ǫ)

Using the definition given just above, this deter-

mines an MG. This is a derivation tree for one of

the infinitely many expressions of category C:

〈0,C〉(Mary knows who John likes)

〈1,=V C〉(ǫ) 〈0,V〉(Mary knows who John likes)

〈0,=D V〉(knows who John likes)

〈1,=C =D V〉(knows) 〈0,C〉(who John likes)

〈0,+wh C,-wh〉(John likes,who)

〈1,=V +wh C〉(ǫ) 〈0,V,-wh〉(John likes,who)

〈0,=D V,-wh〉(likes,who)

〈1,=D =D V〉(likes) 〈1,D -wh〉(who)

〈1,D〉(John)

〈1,D〉(Mary)

If we relabel this tree so that each instance of merge

is labeled Merge or •, and each instance of move is
labeled Move or ◦, the result is the corresponding
MG derivation tree, usually depicted like this:

•

ǫ::=V C •

•

knows::=C =D V ◦

•

ǫ:=V +wh C •

•

likes::=D =D V who::D -wh

John::D

Mary::D

In fact, the latter tree fully specifies the MCFG

derivation above, because, in every MG derivation,

for every internal node, the categories of the children

determine which rule applies. This is easily verified

by checking the 5 schemes for non-lexical rules on

the previous page; the left side of each rule is a func-

tion of the right. Consequently the MCFG categories

at the internal nodes can be regarded as specifying

the states of a deterministic finite state bottom-up

tree recognizer for the MG derivation trees (Kobele

et al., 2007; Graf, 2011; Kobele, 2011).

3.2 MCFGs need not be computed

We did not explicitly present the nonlexical MCFG

rules used in the previous section §3.1, since they are
determined by the lexical rules. The first rule used

at the root of the derivation tree is, for example, an

instance of the first rule scheme in §3, namely:

〈 0, C 〉(s0t0) :- 〈 1,=V C 〉(s0), 〈 0, V 〉(t0).

Generating these non-lexical MCFG rules from the

MG lexicon is straightforward, and has been im-

plemented in (freely available) software by Guillau-

min (2004). But the definition given in §3 requires
that all feature sequences in all rules be suffixes

of lexical feature sequences, and notice that in any

derivation tree, like the one shown in §3.1, for exam-
ple, feature sequences increase along the left branch

from any node to the leaf which is its ‘head.’ Along

any such path, the feature sequences increase one

feature at a time until they reach the lexical leaf. So

in effect, if we are building the derivation top-down,

each step adds or ‘unchecks’ features in lexical se-

quences one at a time, and obviously the options for

doing this can be seen without compiling out all the

MCFG nonlexical rules.

4 The top-down recognizer

For any sequence s of elements of S, let |s|=the
length of s and nth(i, s) = a iff a ∈ S, and for
some u, v ∈ S∗, s = uav and |u| = i. Adapt-
ing basic ideas from earlier work (Mainguy, 2010;

Villemonte de la Clergerie, 2002) for TD recogni-

tion, we will instantiate variables not with strings but

with indices i ∈ N∗ to represent linear order of con-
stituents, to obtain indexed atoms A(i1, . . . , id(A)).

Consider any nonlexical rule α :- γ and any in-

dexed atom β where

α=A(t1, . . . , td(A))

β=A(i1, . . . , id(A))

γ=B0(x01 , . . . , x0d(B0)), . . . , Bn(xn1 , . . . , xnd(Bn)).

For each variable xij in γ, define

indexα,β(xij ) =

{

ik if tk = xij
ikp if |tk| > 1, xij = nth(p, tk).

42



Let indexα,β(γ) be the result of replacing each vari-
able xij in γ by indexα,β(xij ). Finally, let trim(γ)
map γ to itself except in the case when when every

index in γ begins with the same integer n, in which

case that initial n is deleted from every index.

Define a total order on the indices N∗ as follows.

For any α, β ∈ N∗,

α < β iff











α = ǫ 6= β, or

α = iα′, β = jβ′, i < j, or

α = iα′, β = iβ′, α′ < β′.

For any atom α, let µ(α) be the least index in α. So,
for example, µ(AB(31, 240)) = 240. And for any
indexed atoms α, β, let α < β iff µ(α) < µ(β). We
use this order it to sort categories into left-to-right

order in the ‘expand’ rule below.

We now define TD recognition in a deductive for-

mat. The state of the recognition sequence is given

by a (remaining input,priority queue) pair, where the

queue represents the memory of predicted elements,

sorted according to < so that they can be processed

from left to right. We have 1 initial axiom, which

predicts that input s will have start category S, where

S initially has index ǫ:

(s, S(ǫ))

The main work is done by the expand rule, which

pops atom α off the queue, leaving sequence Θ un-
derneath. Then, for any rule β :- γ with β of the

same category as α, we compute indexα,β(γ), ap-
pend the result and Θ, then sort and trim:

(s, αΘ)

(s, sort(trim(indexα,β(γ)Θ)))
β :- γ

(We could use ordered insertion instead of sorting,

and we could trim the indices much more aggres-

sively, but we stick to simple formulations here.) Fi-

nally, we have a scan rule, which scans input a if we

have predicted an A and our grammar tells us that

A(a). For all a ∈ (Σ ∪ ǫ), s ∈ Σ∗, n ∈ N∗:

(as, A(n) Θ)

(s,Θ)
A(a)

A string s is accepted if we can use these rules to get

from the start axiom to (ǫ,ǫ). This represents the fact

that we have consumed the whole input and there are

no outstanding predictions in memory.

4.1 Example 1, continued.

Here is the sequence of recognizer states that accepts

abbcdd, using the grammar presented in §2.1:

initial axiom:

init. (abbcdd, S(ǫ))

expand with rule a:

1. (abbcdd, AC(0,2),BD(1,3))

expand with rule c (note sort):

2. (abbcdd, A(0),BD(1,3),C(2))

scan with rule f:

3. (bbcdd, BD(1,3),C(2))

expand with rule d:

4. (bbcdd, B(10),BD(11,31),C(2),D(30))

scan with rule g:

5. (bcdd, BD(11,31),C(2),D(30))

expand with rule e:

6. (bcdd, B(11),C(2),D(30),D(31))

scan with rule g:

7. (cdd, C(2),D(30),D(31))

scan with rule h (note trim removes 3):

8. (dd, D(0),D(1))

scan with rule i:

9. (d, D(1))

scan with rule i:

10. (ǫ, ǫ)

The number of recognizer steps is always exactly

the number of nodes in the corresponding derivation

tree; compare this accepting sequence to the deriva-

tion tree shown in §2.1, for example.

5 Properties and extensions

5.1 Adding adjunction, head movement

Frey and Gärtner (2002) propose that adjunction be

added to MGs by (i) allowing another kind of select-

ing feature ≈f , which selects but does not ‘check
and delete’ the feature f of a phrase that it modi-

fies, where (ii) the head of the result is the selected,

‘modified’ phrase that it combines with, and (iii)

the selecting ‘modifier’ cannot have any constituents

moving out of it. We can implement these ideas by

adding a rule scheme like the following (compare

the first rule scheme in §3):

〈 0, fα, δ1, . . . , δj 〉(t0s0, t1, . . . , tj) :-
〈 y, fα, δ1, . . . , δj 〉(t0, . . . , tj),
〈x,≈f 〉(s0).

Note this rule ‘attaches’ the modifier on the right.

We could also allow left modifiers, but in the exam-

ples below will only use this one.

43



Some analyses of simple tensed sentences say that

tense affixes ‘hop’ onto the verb after the verb has

combined with its object. Affix hopping and head

movement are more challenging that adjunction, but

previous approaches can be adapted to the present

perspective by making two changes: (i) we keep the

head separate from other material in its phrase until

that phrase is merged with another phrase, so now

every non-lexical category A has d(A) ≥ 3 and (ii)
we add diacritics to the selection features to indi-

cate whether hopping or head movement should ap-

ply in the merge step. To indicate that a head A

selects category f we give A the feature =f , but

to indicate the the head of A should hop onto the

head of the selected constituent, we give A the fea-

ture f=>. Essentially this representation of MGs

with head movement and affix hopping as MCFGs is

immediate from the formalization in Stabler (2001)

and the automated translation by Guillaumin (2004).

The examples in this paper below will use only affix

hopping which is defined by the following modified

version of the first rule in §3:

〈 0, α, δ1, . . . , δj 〉(ǫ, ǫ, tsthshtc, t1, . . . , tj) :-
〈 1, f=>α 〉(sh),
〈x, f, δ1, . . . , δj 〉(ts, th, tc, t1, . . . , tj)

The first atom on the right side of this rule, the ‘se-

lector’, is a lexical head with string sh. The sec-

ond atom on the right of the rule has string com-

ponents ts, th, tc (these are the specifier, head, and

complement strings) together with j ≥ 0 moving el-
ements t1, . . . , tj . In the result on the left, we see

that the lexical selector sh is ‘hopped’ to the right of

the selected head th, where it is sandwiched between

the other concatenated parts of the selected phrase,

leaving ǫ in the head position. Since the usual start

category C now has 3 components, like every other

head, we begin with a special category S that serves

only to concatenate the 3 components of the matrix

complementizer phrase, by providing the recognizer

with this additional initializing rule:

S(ssshsc) :- 〈x, C 〉(ss, sh, sc).

The nature of adjunction is not quite clear, and

there is even less consensus about whether head

movement or affix hopping or both are needed in

grammars of human languages, but these illustrate

how easily the MCFG approach to MGs can be ex-

tended. Like many of the other MG variants, these

extensions do not change the class of languages that

can be defined (Stabler, 2011), and the recognizer

defined in §4 can handle them without change.

With head movement and adjunction we can,

for example, provide a roughly traditional analy-

sis of the famous example sentence from King and

Just (1991) shown in Figure 1. Note again that

the derivation tree in that figure has lexical items at

the leaves, and these completely determine the non-

lexical rules and the structure of the derivation. Var-

ious representations of the ‘derived trees’, like the

X-bar tree shown in this figure, are easily computed

from the derivation tree (Kobele et al., 2007). And

Figure 2 shows the recognizer steps accepting that

sentence. Plotting queue size versus recognizer step,

and simply overlaying the King and Just self-paced

reading times to see if they are roughly similar, we

see that, at least in sentences like these, readers go

more slowly when the queue gets large:

 0

 1

 2

 3

 4

 5

 6

 7

 0  5  10  15  20  25  30

TD queue size
King and Just reading times

Recent work has challenged the claim that reading

times are a function of the number of predictions in

memory, (e.g., Nakatani and Gibson, 2008, p.81) but

preliminary studies suggest that other performance

measures may correlate (Bachrach, 2008; Brennan

et al., 2010; VanWagenen et al., 2011). Exploring

these possibilities is beyond the scope this paper.

The present point is that any analysis expressible

in the MG formalism can be parsed transparently

with this approach, assessing its memory demands;

partially parallel beam search models for ambigu-

ity, used in natural language engineering, can also

be straightforwardly assessed.

44



•

ǫ::=T C ◦

•

-ed::V=> +ep T •

•

admit::=D =D V •

the::=N D error::N

•

the::=N D -ep •

reporter::N ◦

•

that::=T +wh ≈N ◦

•

-ed::V=> +ep T •

•

attack::=D =D V ∅::D -wh

•

the::=N D -ep senator::N

CP

C’

C TP

DP(2)

D’

D

the NP

NP

N

reporter

CP

DP(1)

∅

C’

C

that

TP

DP(0)

D’

D

the

NP

N’

N

senator

T’

T VP

DP

t(0)

V’

V

V

attack

T

-ed

DP

t(1)

T’

T VP

DP

t(2)

V’

V

V

admit

T

-ed

DP

D’

D

the

NP

N’

N

error

Figure 1: 28 node derivation tree and corresponding X-bar tree for King and Just (1991) example

5.2 Infinite languages with finite memory

Although memory use is not the main concern of

this paper, it is worth noting that, as in stack-based

CF models, memory demands do not necessarily in-

crease without bound as sentence length increases.

So for example, we can extend the naive grammar

of Figure 2 to accept this is the man that kiss -ed

the maid that milk -ed the cow that toss -ed the dog

that worry -ed the cat that chase -ed the rat, a sen-

tence with 6 clauses, and use no more memory at

any time than is needed for the 2 clause King and

Just example. Dynamic, chart-based parsing meth-

ods usually require more memory without bound as

sentence length grows, even when there is little or

no indeterminacy.

5.3 Connectedness

More directly relevant to incremental models is the

fact that the portions of the derivation traversed at

any point in TD recognition are all connected to each

other, their syntactic relations are established. As we

see in all our examples, the TD recognizer is always

traversing the derivation tree on paths connected to

the root; while the indexing and sorting ensures that

the leaves are scanned in the order of their appear-

ance in the derived X-bar tree. Left corner traver-

sals do not have this property. Consider a sentence

like the reporter poured the egg in the bowl over the

flour. In a syntax in the spirit of the one we see in

Figure 1, for example, in the bowl could be right ad-

joined to the direct object, and over the flour right

adjoined to VP. Let VP1 be the parent of over the

flour, and VP2 its sister. With LC, VP1 will be pre-

dicted right after the subject is completed. But the

verb is the left corner of VP2, and VP2 will not be

attached to VP1 – and so the subject and verb will

not be connected – until VP1 is completed. This de-

lay in the LC attachment of the subject to the verb

can be extended by adding additional right modifiers

to the direct object or the verb phrase, but the evi-

dence suggests that listeners make such connections

immediately upon hearing the words, as the TD rec-

ognizer does.

6 Future work

Standard methods for handling indeterminacy in

top-down CF parsers, when there are multiple ways

to expand a derivation top down, are easily adapted

to the MCFG and MG parsers proposed here. With

backtracking search, left recursion can cause non-

45



termination, but a probabilistic beam search can do

better. For α = (i,Θ) any recognizer state, let
step(α) be the (possibly empty) sequence of all the
next states that are licensed by the rules in §3 (al-
ways finitely many). A probabilistic beam search

uses the rules,

〈(s, S(ǫ))〉
init

αΘ

prune(sortC(step(α)Θ))
search,

popping a recognizer state α off the top of the queue

αΘ, appending step(α) and Θ, then sorting and
pruning the result. The sort in the search steps

is done according to the probability of each parser

state in context C , where the context may include a

history of previous recognizer steps – i.e. of each

derivation up to this point – but also possibly ex-

trasentential information of any sort. The pruning

rule acts to remove highly improbable analyses, and

success is achieved if a step puts (ǫ, ǫ) on top of
the queue. Roark shows that this ability to condi-

tion on material not in parser memory – indeed on

anything in the left context – can allow better esti-

mates of parse probability. On small experimental

grammars, we are finding that TD beam search per-

formance can be better than our chart parsers using

the same grammar. Further feasibility studies are in

1 init. (trttsa-a-te, S(ǫ))

1 init. (trttsa-a-te, 〈0,C〉(0,1,2))
2 1. (trttsa-a-te, 〈1,=TC〉(1),〈0,T〉(20,21,22))
1 2. (trttsa-a-te, 〈0,T〉(0,1,2))
1 3. (trttsa-a-te, 〈0,+epT,-ep〉(01,1,2,00))
2 4. (trttsa-a-te, 〈0,V,-ep〉(20,21,23,00),〈1,V=>+epT〉(22))
3 5. (trttsa-a-te, 〈0,D-ep〉(000,001,002),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
4 6. (trttsa-a-te, 〈1,=ND-ep〉(001),〈0,N〉(0020,0021,0022),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
3 7. (rttsa-a-te, 〈0,N〉(0020,0021,0022),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
4 8. (rttsa-a-te, 〈1,N〉(0021),〈0,≈N〉(00220,00221,00222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
3 9. (ttsa-a-te, 〈0,≈N〉(00220,00221,00222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
3 10. (ttsa-a-te, 〈0,+wh≈N ,-wh〉(002201,00221,00222,002200),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
4 11. (ttsa-a-te, 〈0,T,-wh〉(002220,002221,002222,002200),〈1,=T+wh≈N〉(00221),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
4 12. (ttsa-a-te, 〈0,+epT,-ep,-wh〉(0022201,002221,002222,0022200,002200),〈1,=T+wh≈N〉(00221),〈0,=DV〉(20,21,23),

〈1,V=>+epT〉(22))
5 13. (ttsa-a-te, 〈0,V,-ep,-wh〉(0022220,0022221,0022223,0022200,002200),〈1,=T+wh≈N〉(00221),〈1,V=>+epT〉(0022222),

〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
6 14. (ttsa-a-te, 〈0,=DV,-wh〉(0022220,0022221,0022223,002200),〈1,=T+wh≈N〉(00221),

〈0,D-ep〉(00222000,00222001,00222002),〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
7 15. (ttsa-a-te, 〈1,D-wh〉(002200),〈1,=T+wh≈N〉(00221),〈0,D-ep〉(00222000,00222001,00222002),〈1,=D=DV〉(0022221),

〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
6 16. (ttsa-a-te, 〈1,=T+wh≈N〉(00221),〈0,D-ep〉(00222000,00222001,00222002),〈1,=D=DV〉(0022221),

〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
5 17. (tsa-a-te, 〈0,D-ep〉(00222000,00222001,00222002),〈1,=D=DV〉(0022221),〈1,V=>+epT〉(0022222),

〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
6 18. (tsa-a-te, 〈1,=ND-ep〉(00222001),〈1,N〉(00222002),〈1,=D=DV〉(0022221),〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),

〈1,V=>+epT〉(22))
5 19. (sa-a-te, 〈1,N〉(00222002),〈1,=D=DV〉(0022221),〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
4 20. (a-a-te, 〈1,=D=DV〉(0022221),〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
3 21. (-a-te, 〈1,V=>+epT〉(0022222),〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
2 22. (a-te, 〈0,=DV〉(20,21,23),〈1,V=>+epT〉(22))
3 23. (a-te, 〈1,=D=DV〉(1),〈1,V=>+epT〉(2),〈0,D〉(30,31,32))
2 24. (-te, 〈1,V=>+epT〉(2),〈0,D〉(30,31,32))
1 25. (te, 〈0,D〉(30,31,32))
2 26. (te, 〈1,=ND〉(1),〈1,N〉(2))
1 27. (e, 〈1,N〉(2))
0 28. (ǫ, ǫ)

Figure 2: 28 step TD recognition of derivation in Figure 1, abbreviating input words by their initial characters. The

left column indicates queue size, plotted in §5.1.

46



progress.

The recognizer presented here simplifies Main-

guy’s (2010) top-down MG recognizer by generaliz-

ing it handle an MCFG normal form, so that a wide

range of MG extensions are immediately accommo-

dated. This is made easy when we adopt Kanazawa’s

Horn clause formulation of MCFGs where the order

of variables on the left side of the rules so visibly in-

dicates the surface order of string components. With

the Horn clause notation, the indexing can be string-

based and general rather than tree-based and tied to

particular assumptions about how the MGs work.

Transparently generalizing the operations of CF TD

recognizers, the indexing and operations here are

also slightly simpler than ‘thread automata’ (Ville-

monte de la Clergerie, 2002). Compare also the

indexing, sometimes more or less similar, in chart-

based recognizers of MCF and closely related sys-

tems (Burden and Ljunglöf, 2005; Harkema, 2001c;

Boullier, 1998; Kallmeyer, 2010).

Mainguy shows that when the probability of a

derivation is the product of the rule probabilities, as

usual, and when those rule probabilities are given by

a consistent probability assignment, a beam search

without pruning will always find a derivation if there

is one. When there is no derivation, though, an

unpruned search can fail to terminate; a pruning

rule can guarantee termination in such cases. Those

results extend to the MCFG recognizers proposed

here. Various applications have found it better to

use a beam search with top-down recognition of left-

or right-corner transforms of CF grammars (Roark,

2001; Roark, 2004; Schuler, 2010; Wu et al., 2010);

those transforms can (but need not always) disrupt

grammatical connectedness as noted in §5.3. Work
in progress explores the possibilities for such strate-

gies in incremental MCFG parsing. It would also

be interesting to generalize Hale’s (2011) “rational

parser” to these grammars.

Acknowledgments

Thanks to Thomas Mainguy, Sarah VanWagenen

and Éric Villemonte de la Clergerie for helpful dis-

cussions of this material.

References

Gerry T. M. Altmann and Jelena Mirković. 2009. Incre-

mentality and prediction in human sentence process-

ing. Cognitive Science, 33:583–809.

Asaf Bachrach. 2008. Imaging Neural Correlates of Syn-

tactic Complexity in a Naturalistic Context. Ph.D. the-

sis, Massachusetts Institute of Technology.

Pierre Boullier. 1998. Proposal for a natural lan-

guage processing syntactic backbone. Technical Re-

port 3242, Projet Atoll, INRIA, Rocquencourt.

Jonathan Brennan, Yuval Nir, Uri Hasson, Rafael

Malach, David J. Heeger, and Liinay Pylkkänen.

2010. Syntactic structure building in the anterior tem-

poral lobe during natural story listening. Forthcoming.

Håkan Burden and Peter Ljunglöf. 2005. Parsing linear

context-free rewriting systems. In Ninth International

Workshop on Parsing Technologies, IWPT’05.

Craig G. Chambers, Michael K. Tanenhaus, Kathleen M.

Eberhard, Hana Filip, and Greg N. Carlson. 2004.

Actions and affordances in syntactic ambiguity resolu-

tion. Journal of Experimental Psychology: Learning,

Memory and Cognition, 30(3):687–696.

Cristiano Chesi. 2007. An introduction to phase-based

minimalist grammars: Why move is top-down from

left-to-right. Technical report, Centro Interdepartmen-

tale di Studi Cognitivi sul Linguaggio.

Vera Demberg and Frank Keller. 2009. A computational

model of prediction in human parsing: Unifying local-

ity and surprisal effects. In Proceedings of the 29th

meeting of the Cognitive Science Society (CogSci-09),

Amsterdam.

Werner Frey and Hans-Martin Gärtner. 2002. On the

treatment of scrambling and adjunction in minimal-

ist grammars. In Proceedings, Formal Grammar’02,

Trento.

Thomas Graf. 2011. Closure properties of minimalist

derivation tree languages. In Logical Aspects of Com-

putational Linguistics, LACL’11, Forthcoming.

Matthieu Guillaumin. 2004. Conversions be-

tween mildly sensitive grammars. UCLA and

École Normale Supérieure. http://www.linguistics.

ucla.edu/people/stabler/epssw.htm.

Nicholas J. Haddock. 1989. Computational models of

incremental semantic interpretation. Language and

Cognitive Processes, 4((3/4)):337–368.

John T. Hale. 2011. What a rational parser would do.

Cognitive Science, 35(3):399–443.

Henk Harkema. 2001a. A characterization of minimalist

languages. In Proceedings, Logical Aspects of Com-

putational Linguistics, LACL’01, Port-aux-Rocs, Le

Croisic, France.

Henk Harkema. 2001b. A characterization of minimalist

languages. In Philippe de Groote, Glyn Morrill, and

47



Christian Retoré, editors, Logical Aspects of Compu-

tational Linguistics, Lecture Notes in Artificial Intelli-

gence, No. 2099, pages 193–211, NY. Springer.

Henk Harkema. 2001c. Parsing Minimalist Languages.

Ph.D. thesis, University of California, Los Angeles.

Aravind Joshi. 1985. How much context-sensitivity is

necessary for characterizing structural descriptions. In

D. Dowty, L. Karttunen, and A. Zwicky, editors, Natu-

ral Language Processing: Theoretical, Computational

and Psychological Perspectives, pages 206–250. Cam-

bridge University Press, NY.

Laura Kallmeyer. 2010. Parsing beyond context-free

grammars. Springer, NY.

Makoto Kanazawa. 2009. A pumping lemma for well-

nested multiple context free grammars. In 13th In-

ternational Conference on Developments in Language

Theory, DLT 2009.

Yoshihide Kato and Shigeki Matsubara. 2009. In-

cremental parsing with adjoining operation. IE-

ICE Transactions on Information and Systems,

E92.D(12):2306–2312.

Jonathan King and Marcel Adam Just. 1991. Individual

differences in syntactic processing: the role of working

memory. Journal of Memory and Language, 30:580–

602.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-

vati. 2007. An automata-theoretic approach to min-

imalism. In J. Rogers and S. Kepser, editors, Model

Theoretic Syntax at 10, ESSLLI’07.

Gregory M. Kobele. 2011. Minimalist tree languages are

closed under intersection with recognizable tree lan-

guages. In Logical Aspects of Computational Linguis-

tics, LACL’11, Forthcoming.

Thomas Mainguy. 2010. A probabilistic

top-down parser for minimalist grammars.

http://arxiv.org/abs/1010.1826v1.

Jens Michaelis. 1998. Derivational minimalism is mildly

context-sensitive. In Proceedings, Logical Aspects of

Computational Linguistics, LACL’98, pages 179–198,

NY. Springer.

Jens Michaelis. 2001a. On Formal Properties of Mini-

malist Grammars. Ph.D. thesis, Universität Potsdam.

Linguistics in Potsdam 13, Universitätsbibliothek,

Potsdam, Germany.

Jens Michaelis. 2001b. Transforming linear context

free rewriting systems into minimalist grammars. In

P. de Groote, G. Morrill, and C. Retoré, editors, Logi-

cal Aspects of Computational Linguistics, LNCS 2099,

pages 228–244, NY. Springer.

Kentaro Nakatani and Edward Gibson. 2008. Distin-

guishing theories of syntactic expectation cost in sen-

tence comprehension: Evidence from Japanese. Lin-

guistics, 46(1):63–86.

Brian Roark and Mark Johnson. 1999. Efficient proba-

bilistic top-down and left-corner parsing. In Proceed-

ings of the 37th Annual Meeting of the Association for

Computational Linguistics, pages 421–428.

Brian Roark. 2001. Probabilistic top-down parsing

and language modeling. Computational Linguistics,

27(2):249–276.

Brian Roark. 2004. Robust garden path parsing. Natural

Language Engineering, 10(1):1–24.

William Schuler. 2010. Incremental parsing in bounded

memory. In Proceedings of the 10th International

Workshop on Tree Adjoining Grammars and Related

Frameworks, TAG+10.

Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and

Tadao Kasami. 1991. On multiple context-free gram-

mars. Theoretical Computer Science, 88:191–229.

Libin Shen and Aravind Joshi. 2005. Incremental LTAG

parsing. In Proceedings, Human Language Technol-

ogy Conference and Conference on Empirical Meth-

ods in Human Language Processing.

Edward P. Stabler. 2001. Recognizing head movement.

In Philippe de Groote, Glyn Morrill, and Christian Re-

toré, editors, Logical Aspects of Computational Lin-

guistics, Lecture Notes in Artificial Intelligence, No.

2099, pages 254–260. Springer, NY.

Edward P. Stabler. 2011. Computational perspectives on

minimalism. In Cedric Boeckx, editor, Oxford Hand-

book of Linguistic Minimalism, pages 617–641. Ox-

ford University Press, Oxford.

Sarah VanWagenen, Jonathan Brennan, and Edward P.

Stabler. 2011. Evaluating parsing strategies in sen-

tence processing. In Proceedings of the CUNY Sen-

tence Processing Conference.

K. Vijay-Shanker and David Weir. 1994. The equiva-

lence of four extensions of context free grammar for-

malisms. Mathematical Systems Theory, 27:511–545.

Éric Villemonte de la Clergerie. 2002. Parsing MCS lan-

guages with thread automata. In Proceedings of the

6th International Workshop on Tree Adjoining Gram-

mars and Related Frameworks, TAG+6.

David Weir. 1988. Characterizing Mildly Context-

Sensitive Grammar Formalisms. Ph.D. thesis, Univer-

sity of Pennsylvania, Philadelphia.

Stephen Wu, Asaf Bachrach, Carlos Cardenas, and

William Schuler. 2010. Complexity metrics in an in-

cremental right-corner parser. In Proceedings of the

48th Annual Meeting of the Association for Computer

Linguistics, pages 1189–1198.

48


