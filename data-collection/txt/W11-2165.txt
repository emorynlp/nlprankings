










































Generative Models of Monolingual and Bilingual Gappy Patterns


Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512–522,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics

Generative Models of Monolingual and Bilingual Gappy Patterns

Kevin Gimpel Noah A. Smith
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA 15213, USA

{kgimpel,nasmith}@cs.cmu.edu

Abstract

A growing body of machine translation re-
search aims to exploit lexical patterns (e.g., n-
grams and phrase pairs) with gaps (Simard et
al., 2005; Chiang, 2005; Xiong et al., 2011).
Typically, these “gappy patterns” are discov-
ered using heuristics based on word align-
ments or local statistics such as mutual infor-
mation. In this paper, we develop generative
models of monolingual and parallel text that
build sentences using gappy patterns of arbi-
trary length and with arbitrarily many gaps.
We exploit Bayesian nonparametrics and col-
lapsed Gibbs sampling to discover salient pat-
terns in a corpus. We evaluate the patterns
qualitatively and also add them as features to
an MT system, reporting promising prelimi-
nary results.

1 Introduction

Beginning with the success of phrase-based transla-
tion models (Koehn et al., 2003), a trend arose of
modeling larger and increasingly complex structural
units in translation. One thread of work has focused
on the use of lexical patterns with gaps. Simard et
al. (2005) proposed using phrase pairs with gaps in a
phrase-based translation model, providing a heuris-
tic method to extract gappy phrase pairs from word-
aligned parallel corpora. The widely-used hierarchi-
cal phrase-based translation framework was intro-
duced by Chiang (2005) and also relies on a simple
heuristic for phrase pair extraction. On the mono-
lingual side, researchers have taken inspiration from
trigger-based language modeling for speech recog-
nition (Rosenfeld, 1996). Recently Xiong et al.
(2011) used monolingual trigger pairs to improve
handling of long-distance dependencies in machine
translation output.

All of this previous work used heuristics or local
statistical tests to extract patterns from corpora. In
this paper, we present probabilistic models that gen-
erate text using gappy patterns of arbitrary length
and with arbitrarily-many gaps. We exploit non-
parametric priors and use Bayesian inference to dis-
cover the most salient gappy patterns in monolin-
gual and parallel text. We first inspect these pat-
terns manually and discuss the categories of phe-
nomena that they capture. We also add them as
features in a discriminatively-trained phrase-based
MT system, using standard techniques to train their
weights (Arun and Koehn, 2007; Watanabe et al.,
2007) and incorporate them during decoding (Chi-
ang, 2007). We present experiments for Spanish-
English and Chinese-English translation, reporting
encouraging preliminary results.

2 Related Work

There is a rich history of trigger-based language
modeling in the speech recognition community, typ-
ically involving the use of statistical tests to discover
useful trigger-word pairs (Rosenfeld, 1996; Jelinek,
1997). Xiong et al. (2011) used Rosenfeld’s mutual
information procedure to discover trigger pairs and
added a single feature to a phrase-based MT system
that scores new words based on all potential trig-
gers from previous parts of the derivation. We are
not aware of prior work that uses generative model-
ing and Bayesian nonparametrics to discover these
same types of patterns automatically; doing so al-
lows us to discover larger patterns with more words
and gaps if they are warranted by the data.

In addition to the gappy phrase-based (Simard et
al., 2005) and hierarchical phrase-based (Chiang,
2005) models mentioned earlier, other researchers
have explored the use of bilingual gappy structures
for machine translation. Crego and Yvon (2009) and

512



π(  ) = .
π(  ) = baltic states

it provides either too little or too much .
it 's neither particularly complicated nor novel .

nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .

nato must either say " yes " or " no " to the baltic states .

π(  ) = either __ or

ππππππππππππ(  ) = either __ or

π(  ) = to the
π(  ) = " __ " __ " __ "π(  ) = must
π(  ) = yes __ no

π(  ) = sayπ(  ) = nato

Figure 1: A sentence from the news commentary cor-
pus, along with color assignments for the words and the
π function for each color.

Galley and Manning (2010) proposed ways of incor-
porating phrase pairs with gaps into standard left-to-
right decoding algorithms familiar to phrase-based
and N -gram-based MT; both used heuristics to ex-
tract phrase pairs. Bansal et al. (2011) presented a
model and training procedure for word alignment
that uses phrase pairs with gaps. They use a semi-
Markov model with an enlarged dynamic program-
ming state in order to represent alignment between
gappy phrases. Their model permits up to one gap
per phrase while our models permit an arbitrary
number.

3 Monolingual Pattern Models

We first present a model that generates a sentence as
a set of lexical items that we will refer to as gappy
patterns, or simply patterns. A pattern is defined as
a sequence containing elements of two types: words
and gaps. All patterns must obey the regular expres-
sion w+( w+)*, where w is a word and is a gap.
That is, patterns must begin and end with words and
may not contain consecutive gaps.

We assume that we have an n-word sentence
w1:n.1 We represent patterns in a sentence by as-
sociating each word with a color. To do so, we in-
troduce a vector of color assignment variables c1:n,
with one for each word. We represent a color Cj as
a set in terms of the ci variables: Cj = {i : ci = j}.
Each color corresponds to a pattern that is obtained
by concatenating its words from left to right in the
sentence, inserting gaps when necessary. We denote
the pattern for a color Cj by π(Cj); Figure 1 shows
examples of the correspondence between colors and
patterns.

The generative story for a single sentence follows:

1We use boldface lowercase letters to denote vectors (e.g.,
f ), denote entry i as fi, and denote the range from i to j as
f i:j .

1. Sample the number of words: n ∼ Poisson(β)
2. Sample the number of unique colors in the sen-

tence given n: m ∼ Uniform(1, n)
3. For each word index i = 1 . . . n, sample the color

of word i: ci ∼ Uniform(1,m). If any of the m
colors has no words, repeat this step.

4. For each color j = 1 . . .m, sample from a
multinomial distribution over patterns: wCj ∼
Mult(µ). If the words wCj are not consistent
with the color assignments, i.e., wrong number of
words or gaps, gaps not in the correct locations,
repeat this step.

Thus, the probability of generating number of words
n, words w1:n, color assignments c1:n, and number
of colors m is

p(w1:n, c1:n,m | β, µ)

=
1

Z

(
βn

n!
e−β

)(
1

n

)(
1

m

)n m∏
j=1

pµ(π(Cj))

(1)

where Z is a normalization constant required by the
potential repetition of sampling in the final two steps
of the generative story. Without Z, the model would
be deficient as we would waste probability mass on
internally inconsistent color assignments.

The core of the model is a single multinomial
distribution pµ(·) over patterns. We use a Dirich-
let process (DP) prior for this multinomial so that
we can model an unbounded set of patterns: µ ∼
DP(α, P0), where α is the concentration parameter
and P0 is the base distribution. The base distribution
includes a Poisson(ν) over the number of words in
the pattern, a uniform distribution (over word types
in the vocabulary) for each word, a uniform distri-
bution over the number of gaps given the number of
words, and a uniform distribution over the arrange-
ment of gaps given the numbers of gaps and words.2

Inference We use collapsed Gibbs sampling
for inference. Our goal is to obtain samples
from the posterior distribution p({c(i),m(i)}Si=1 |
{w(i)}Si=1, ν, α), where S is the total number of sen-
tences in the corpus and µ is marginalized out.3

2The number of ways of arranging y gaps among x words is
“(x− 1) choose y”.

3Since we assume the words are given, β is irrelevant.

513



During each iteration of Gibbs sampling, we pro-
ceed through the corpus and sample a new value for
each ci variable conditioned on the values of all oth-
ers in the corpus. Them variables are determined by
the ci variables and therefore do not need to be sam-
pled directly. When sampling ci, we first remove
ci from the corpus (and its color if the color only
contained i). Where the remaining colors in the sen-
tence are numbered from 1 to m, there are m + 1
possibilities for ci: m for each of the existing colors
and one for choosing a new color.

Since choosing a new color corresponds to creat-
ing a new instance of the pattern π({i}), the proba-
bility of choosing a new color m+ 1 is proportional
to

#π({i}) + αP0(π({i}))
# + α

(2)

where #π is the count of pattern π in the rest of the
sentence and all other sentences in the corpus, and
# is the total count of all patterns in this same set.
The probability of choosing the existing color j (for
1 ≤ j ≤ m) is proportional to

#π(Cj∪{i}) + αP0(π(Cj ∪ {i}))
#π(Cj) + αP0(π(Cj))

(3)

where the denominator encodes the fact that the
move will cause an instance of the pattern for the
color Cj to be removed from the corpus as the new
pattern for Cj ∪ {i} is added.

We note that, even though these two types of
moves will result in different numbers of colors (m)
in the sentence, we do not have to include a term for
this in the sampler because we use a uniform dis-
tribution for m and therefore all (valid) numbers of
colors have the same probability. The normalization
constant Z in Equation 1 does not affect inference
because our sampler is designed to only consider
valid (i.e., internally consistent) settings for the c(i)

and m(i) variables.
This model makes few assumptions, using uni-

form distributions whenever possible. This simpli-
fies inference and causes the resulting lexicon to be
influenced primarily by the “rich-get-richer” effect
of the DP prior. Despite its simplicity, we will show
later that this model discovers patterns that capture
a variety of linguistic phenomena.

π(  ) = .
π(  ) = baltic states

it provides either too little or too much .
it 's neither particularly complicated nor novel .

nato must either say " yes " or " no " to the baltic states .
good scientific ideas formulated in bad english either die or get repackaged .

nato must either say " yes " or " no " to the baltic states .

la otan tiene que decir " sí " o " no " a los países bálticos .

π(  ) = either      or

ππππππππππππ(  ) = either __ or

π(  ) = to the

π(  ) =
"      "

π(  ) = must
π(  ) = yes __ no

π(  ) = sayπ(  ) = nato

π(  ) = nato
otan

π(  ) =
to the

13-12 15-13 16-15

o " " a

Figure 2: A Spanish-English sentence pair with the in-
tersection of automatic word alignments in each direc-
tion. Some source words accept the colors of target words
aligned to them while others (light gray) do not. Bilingual
patterns for a few colors are shown.

4 Bilingual Pattern Models

We now present a generative model for a sentence
pair that will enable us to discover bilingual pat-
terns. In this section we present one example of ex-
tending the previous model to be bilingual, but we
note that many other extensions are possible; indeed,
flexibility is one of the key advantages of working
within the framework of probabilistic modeling.

We assume that we are given sentence pairs and
one-to-one word alignments. That is, in addition to
an n-word target sentencew1:n, we assume we have
an n′-word source sentence w′1:n′ and word align-
ments a1:n′ where ai = j iff w′i is aligned to wj and
ai = 0 if w′i is aligned to null.

To model bilingual patterns, we distinguish
source colors from target colors. A target-language
word can only be colored with a target color, but
a source word can be colored with either a source
color or with the target color of the target word it
is aligned to (if any). We have m target colors as
before and now add m′ source colors. We intro-
duce additional random variables in the form of a
binary vector g of length n′ that indicates, for each
source word, whether or not it accepts the color of
its aligned target word. We introduce an additional
parameter γ for the probability that a source word
will accept the color of its aligned word. We fix its
value to 0.5 and do not learn it during inference. Fig-
ure 2 shows an example Spanish-English sentence
pair with automatic word alignments and color as-
signments. The bilingual patterns for a few target
colors are shown.

The generative story for a sentence pair follows:

1. Sample the numbers of words in the source and
target sentences: n′, n ∼ Poisson(β)

514



2. Sample the numbers of source and target col-
ors given n′, n: m′ ∼ Uniform(1, n′),m ∼
Uniform(1, n)

3. Sample the alignment vector from any distribu-
tion that ensures links are 1-to-1:4 a1:n′ ∼ p(a)

4. For each target word index i = 1 . . . n, sample
the color of target word i from a uniform distribu-
tion over all target colors: ci ∼ Uniform(1,m).
While any of the m colors has no words, repeat
this step.

5. For each source word index i = 1 . . . n′:

1. Decide whether to use a source color or to use
the target color of the aligned target word: gi ∼
pγ(gi | ai)

2. If gi = 1, set c′i = cai ; otherwise, sample a
source color: c′i ∼ Uniform(1,m′)

6. If any source color has no words, repeat Step 5.

7. For each source color j = 1 . . .m′:

1. Sample from a multinomial over source pat-
terns: wC′j ∼ Mult(µ

′). While the words wC′j
are not consistent with the color assignments,
repeat this step.

8. For each target color j = 1 . . .m:

1. Sample from a multinomial over bilingual pat-
terns: wCj ∼ Mult(µ). While the words wCj
are not consistent with the color assignments,
repeat this step.

The distribution pγ(gi | ai) is defined below:

pγ(gi = 1 | ai 6= −1) = γ
pγ(gi = 1 | ai = −1) = 0

where γ determines how frequently source tokens
will be added to target patterns.

The probability of generating target words w1:n,
source words w′1:n′ , alignments a1:n′ , target color
assignments c1:n, source color assignments c′1:n′ ,
color propagation variables g1:n′ , number of target

4Since we assume alignments are provided during inference,
it does not matter what distribution is used, so long as only 1-
to-1 links are permitted.

colors m, and number of source colors m′ is

1

Z
p(n)p(n′)p(m | n)p(m′ | n′)p(a1:n′)

×

(
n∏
i=1

p(ci | m)

)

×

(
n′∏
i=1

pγ(gi | ai)p(c′i | m′)I[gi==0]
)

×

 m′∏
j=1

p′µ(π(C
′
j))

 m∏
j=1

pµ(π(Cj))


where Z again serves as a normalization constant to
prevent the model from leaking probability mass on
internally inconsistent configurations.

There are now two multinomial distributions over
patterns with parameter vectors µ and µ′. They both
use DP priors with identical concentration param-
eters α and differing base distributions P0 and P ′0.
The base distribution for source patterns, P ′0, takes
the same form as the base distribution for the model
described in §3.

For target patterns with aligned source words, P0
generates the target part of the pattern like the base
distribution in §3 and then generates the number
of aligned source words to each target word with
a Poisson(1) distribution; the number of aligned
source words can only be 0 or 1 when all word links
are 1-to-1. If it is 1, the base distribution generates
the aligned source word by sampling uniformly from
among all source types.

While there are connections between this model
and work on performing translation using phrase
pairs with gaps, the patterns we discover are not
guaranteed to be bilingual translation units. Rather,
they typically contain additional target-side words
that have no explicit correlate on the source side.
They can be used to assist an existing translation
model by helping to choose the best phrase trans-
lation for each source phrase. To define a genera-
tive model for phrase pairs with gaps, changes would
have to be made to the bilingual model we presented.

Inference As before, we use collapsed Gibbs sam-
pling for inference. Our goal is to obtain sam-
ples from the posterior p({〈c, c′, g,m,m′〉(i)}Si=1 |
{〈w,w′,a〉(i)}Si=1).

515



We go through each sentence pair and sample new
color assignment variables for each word. For an
aligned word pair (w′i, wj), we sample a new value
for the tuple (gi, c′i, cj). The possible values for
cj include all target colors, including a new target
color. The possible values for gi are 0, in which case
c′i can be any of the source colors, including a new
source color, and 1, for which c′i must be cj . For an
unaligned target word wj , cj can be any target color,
including a new one, and for an unaligned source
word w′i, c

′
i can be any source color, including a new

one. The full equations for sampling can be easily
derived using the equations from §3.

5 Evaluation

We conducted evaluation to determine (1) what
types of phenomena are captured by the most prob-
able patterns discovered by our models, and (2)
whether including the patterns as features can im-
prove translation quality.

5.1 Qualitative Evaluation

5.1.1 Monolingual Model
Since inference is computationally expensive,

we used the 126K-sentence English news com-
mentary corpus provided for the WMT shared
tasks (Callison-Burch et al., 2010). We ran Gibbs
sampling for 600 iterations through the data, dis-
carding the first 300 samples for burn-in and com-
puting statistics of the patterns using the remaining
300 samples. Each iteration took approximately 3
minutes on a single 2.2GHz CPU. When looking pri-
marily at the most frequent patterns, we found that
this list did not vary much when only using half of
the data instead. We set ν = 3 and α = 100; we
found these hyperparameters to have only minor ef-
fects on the results.

Since many frequent patterns include the period
(.), we found it useful to constrain the model to treat
this token differently: we modify the base distribu-
tion so that it assigns zero probability to patterns
that contain a period along with other words and we
force each occurrence of a period to be alone in its
own pattern during initialization. We do not need to
change the inference procedure at all; with the mod-
ified base distribution and with no patterns including
a period with other words, the probability of creat-

" " as as " " " "
– – the of in why ?
( ) the is , the of
the of not only but from to
, , , it is that the between and
the ( ) of " " such as ,
both and not , but either or
the of and in , in but is
more than the of , " " the
- - what ? has been
, " " between and in , ,
the " " the of ’s an of

Table 1: Top-ranked gappy patterns from samples accord-
ing to p(π); patterns without gaps are omitted. The spe-
cial string “ ” represents a gap that can be filled by any
nonempty sequence of words.

ing a new illegal pattern during inference is always
zero (Eq. 3).

We also perform inference on a transformed ver-
sion of the corpus in which every word is replaced
with its hard word class obtained from Brown clus-
tering (Brown et al., 1992). One property of Brown
clusters is that each function word effectively re-
ceives its own class, as each ends up in a cluster in
which it occupies ≥95% of the token counts of all
types in the cluster. We call clusters that satisfy this
property singleton clusters.

To obtain Brown clusters for the source and tar-
get languages, we used code from Liang (2005).5

We used the data from the news commentary cor-
pus along with the first 500K sentences of the addi-
tional monolingual newswire data also provided for
the WMT shared tasks. We used 300 clusters, ig-
noring words that appeared only once in this corpus.
We did not use the hierarchical information from the
clusters but merely converted each cluster name into
a unique integer, using one additional integer for un-
known words.

We used the same values for ν and α as above
but ran Gibbs sampling for 1,300 iterations, again
using the last 300 for collecting statistics on pat-
terns. Judging by the number of color assignments
changed on each iteration, the sampler takes longer
to converge when run on word clusters than on
words. As above, we constrain the singleton word
cluster corresponding to the period to be alone dur-
ing both initialization and inference.

5http://www.cs.berkeley.edu/˜pliang/
software

516



academy sciences regulators supervisors
beijing shanghai sine non
booms busts stalin mao
council advisers treasury secretary geithner
dominicans haitian sooner later
flemish walloons first foremost
gref program played role
heat droughts down road
humanitarian displaced freedom expression
karnofsky hassenfeld at disposal
kazakhstan kyrgyzstan take granted
portugal greece - -

Table 2: Gappy patterns with highest conditional proba-
bility p(π|w(π)).

– – whether or france germany
( ) around world he his
- - has been allow to
both and how ? for first time
not only but the ( ) china india
" " on basis what do
more than less than we our
either or on other hand over past
why ? at level prevent from
neither nor it is that in way
what ? not , but one another
rule law play role political economic

Table 3: Top-ranked gappy patterns according to
p(π)p(π|w(π)).

Pattern Ranking Statistics Several choices exist
for ranking patterns. The simplest is to take the pat-
tern count from the posterior samples, averaged over
all sampling iterations after burn-in. We refer to this
criterion as the marginal probability:

p(π) =
#π
#

where #π is the average count of the pattern across
the posterior samples and # is the count of all pat-
terns. The top-ranked gappy patterns under this cri-
terion are shown in Table 1. While many of these
patterns match our intuitions, there are also sev-
eral that are highly-ranked simply because their con-
stituent words are frequent.

Alternatively, we can rank patterns by the con-
ditional probability of the pattern given the words
that comprise it:

p(π|w(π)) = #π
#w(π)

where w(π) returns the sequence of words in the
pattern π and #w(π) is the number of occurrences

of this sequence of words in the corpus that are com-
patible with pattern π. The ranking of patterns under
this criterion is shown in Table 2. This method fa-
vors precision but also causes very rare patterns to
be highly ranked.

To address this, we also consider a product-of-
experts model by simply multiplying together the
two probabilities, resulting in the ranking shown in
Table 3. This ranking is similar to that in Table 1
but penalizes patterns that are only ranked highly be-
cause they consist of common words. Table 4 shows
a manual grouping of these highly-ranked patterns
into several categories. We show both lexical and
Brown cluster patterns.6

It is common in both types of patterns to find
long-distance dependencies involving punctuation
near the top of the ranking. Among agreement pat-
terns, the lexical model finds relationships between
pronouns and their associated possessive adjectives
while the cluster model finds more general patterns
involving classes of nouns. Cluster patterns are more
likely to capture topicality within a sentence, while
the finer granularity of the lexical model is required
to identify constructions like those shown (verbs
triggering particular prepositions).

There are also many probable patterns without
gaps, shown at the bottom of Table 4. From these
patterns we can see that our models can also be used
to find collocations, but we note that these are dis-
covered in the context of the gappy patterns. That
is, due to the use of latent variables in our models
(the color assignments), there is a natural trading-off
effect whereby the gappy patterns encourage partic-
ular non-gappy patterns to be used, and vice versa.

5.1.2 Bilingual Model
We use the news commentary corpus for each lan-

guage and take the intersection of GIZA++ (Och
and Ney, 2003) word alignments in each direction,
thereby ensuring that they are 1-to-1 alignments. We
ran Gibbs sampling for 300 iterations, averaging pat-
tern counts from the last 200. We set α = 100,
λ = 3, and γ = 0.5. We ran the model in 3 con-
ditions: source words, target words; source clusters,
target clusters; and source clusters, target words. We

6We filter Brown cluster patterns in which every cluster is
a singleton, since these patterns are typically already accounted
for in the lexical patterns.

517



Rank Gappy Lexical Patterns Rank Gappy Brown Cluster Patterns

Pu
nc

tu
at

io
n 1 -- -- 2 {what, why, whom, whatever} {?, !}

2 ( ) 6 {--, -, –} {--, -, –}
6 " " 28 {according, compared, subscribe, thanks, referring} to ,
9 why ? 178 {–, -, –} {even, especially, particularly, mostly, mainly} {–, -, –}
63 according to , 239 {obama, bush, clinton, mccain, brown} " "

A
gr

ee
m

en
t

26 he his 8 {people, things, americans, journalists, europeans} their
31 we our 12 we {our, my}
46 his his 21 {children, women, others, men, students} their
86 china its 23 {china, europe, america, russia, iran} ’s its
90 his he 43 {obama, bush, clinton, mccain, brown} his
99 you your 46 {our, my} {our, my}
136 leaders their 149 {people, things, americans, journalists, europeans} they
140 we ourselves 172 {president, bill, sen., king, senator} {obama, bush, clinton, mccain, brown} his
165 these are 180 {all, both, either} {countries, companies, banks, groups, issues}

C
on

ne
ct

iv
es

4 both and 5 {more, less} {more, less}
5 not only but 9 if , {will, would, could, should, might}
8 either or 19 {deal, plan, vote, decision, talks} {against, between, involving} and
10 neither nor 40 a {against, between, involving} and
13 whether or 45 {better, different, further, higher, lower} than
19 less than 50 {much, far, slightly, significantly, substantially} than
23 not , but 56 {yet, instead, perhaps, thus, neither} but
54 if then 68 not {only, necessarily} {also, hardly}
109 between and 98 as {much, far, slightly, significantly, substantially} as
192 relationship between and 131 is {more, less} than

To
pi

ca
lit

y

25 france germany 1 〈UNK〉 〈UNK〉
29 china india 15 {china, europe, . . .} ’s {system, crisis, program, recession, situation}
36 political economic 30 {health, security, defense, safety, intelligence} {health, . . .}
43 rich poor 47 {china, europe, . . .} {china, europe, . . .} {china, europe, . . .}
50 oil gas 62 {power, growth, interest, development} {10, 1, 20, 30, 2} {percent, %, p.m., a.m.}
62 billions dollars 72 in {iraq, washington, london, 2008, 2009} {iraq, washington, london, 2008, 2009}
96 economic social 73 the {end, cost, head, rules, average} of {prices, markets, services, problems, costs}
106 the us europe 113 {china, europe, . . .} ’s {economy, election, elections, population, investigation}
181 public private 119 {prices, markets, . . .} {oil, energy, tax, food, investment} {oil, energy, . . .}

Pr
ep

os
iti

on
s

14 around world 14 for {first, second, third, final, whole} {time, period, term, class, avenue}
18 on basis 17 in {last, next, 20th} {year, week, month, season, summer}
38 at time 51 at {end, cost, head, rules, average} of
42 in region 71 at {group, rate, leader, level, manager}
80 in manner 112 for {times, points, games, goals, reasons}
85 at expense 126 {over, around, across, behind, above} {country, company, region, nation, virus}
112 during period 190 {one, none} of {best, top, largest, main, biggest}

C
on

st
ru

ct
io

ns 33 prevent from
84 enable to
114 provide for
123 impose on
177 turn into

Non-Gappy Lexical Patterns Non-Gappy Brown Cluster Patterns
as well their own as {well, soon, quickly, seriously, slowly} as {rather, please} than
the united states prime minister the united {states, nations, airlines} {don, didn, doesn, isn, wasn} ’t
have been climate change {president, bill, sen., king, senator} {mr., mr, john, david, michael} {obama, bush, clinton, . . .}
rather than the bush administration {order, plans, needs, efforts, failed} to {make, take, give, keep, provide}
based on developing countries {will, would, could, should, might} not be {can, ’ll} be

Table 4: Gappy patterns manually divided into categories of long-distance dependencies. Patterns were ranked ac-
cording to p(π)p(π|w(π)) and manually selected from the top 300 to exemplify categories. Lower pane shows top
ranked non-gappy patterns. Clusters are shown as enough words to cover 95% of the token counts of the cluster, up to
a maximum of 5.

again ensured that the period and its word class re-
mained isolated in their own patterns for each con-
dition. We note that no source-side word order in-
formation is contained within these bilingual pat-
terns; aligned source words can be in any order in

the source sentence and the pattern will still match.
The most probable patterns included many mono-
lingual source-only and target-only patterns that are
similar to those shown in Table 4. There were also
many phrase pairs with gaps like those that are com-

518



monly extracted by heuristics (Galley and Manning,
2010). Additionally we noted examples of source
words triggering more target-side information than
merely one word. There were several examples of
patterns that encouraged inclusion of the subject in
English when translating from Spanish, as Spanish
often drops the subject when it is clear from context,
e.g., “we are(estamos)”. Also, one probable pattern
for German-English was “the of the(des)” (des is
aligned to the final the). The German determiner
des is in the genitive case, so this pattern helps to
encourage its object to also be in the genitive case
when translated.

5.2 Quantitative Evaluation

We consider the Spanish-to-English (ES→EN)
translation task from the ACL-2010 Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2010). We trained a Moses system (Koehn et al.,
2007) following the baseline training instructions for
the shared task.7 In particular, we performed word
alignment in each direction using GIZA++ (Och and
Ney, 2003), used the “grow-diag-final-and” heuristic
for symmetrization, and extracted phrase pairs up to
a maximum length of seven. After filtering sentence
pairs with one sentence longer than 50 words, we
ended up with 1.45M sentence pairs of Europarl data
and 91K sentence pairs of news commentary data.
Language models (N = 5) were estimated using the
SRI language modeling toolkit (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). Language models were trained on the
target side of the parallel corpus as well as the first 5
million additional sentences from the extra English
monolingual newswire data provided for the shared
tasks. We used news-test2008 for tuning and
news-test2009 for testing.

We also consider Chinese-English (ZH→EN) and
followed a similar training procedure as above. We
used 303K sentence pairs from the FBIS corpus
(LDC2003E14) and segmented the Chinese data
using the Stanford Chinese segmenter in “CTB”
mode (Chang et al., 2008), giving us 7.9M Chi-
nese words and 9.4M English words. A trigram lan-
guage model was estimated using modified Kneser-
Ney smoothing from the English side of the parallel

7www.statmt.org/wmt10/baseline.html.

corpus concatenated with 200M words of randomly-
selected sentences from the Gigaword v4 corpus (ex-
cluding the NY Times and LA Times). We used
NIST MT03 for tuning and NIST MT05 for test-
ing. For evaluation, we used case-insensitive IBM
BLEU (Papineni et al., 2001).

5.2.1 Training and Decoding
Unlike n-gram language models, our models have

latent structure (the color assignments), making it
difficult to compute the probability of a translation
during decoding. We leave this problem for future
work and instead simply add a feature for each of
the most probable patterns discovered by our mod-
els. Each feature counts the number of occurrences
of its pattern in the translation.

We wish to add thousands of features to our
model, but the standard training algorithm – mini-
mum error rate training (MERT; Och, 2003) – can-
not handle large numbers of features. So, we lever-
age recent work on feature-rich training for MT us-
ing online discriminative learning algorithms. Our
training procedure is shown as Algorithm 1. We
find it convenient to notationally distinguish feature
weights for the standard Moses features (λ) from
weights for our pattern features (θ). We use h(e)
to denote the feature vector for translation e. The
function Bi(t) returns the sentence BLEU score for
translation t given reference ei (i.e., treating the sen-
tence pair as a corpus).8

MERT is run to convergence on the tuning set to
obtain weights for the standard Moses features (line
1). Phrase lattices (Ueffing et al., 2002) are gen-
erated for all source sentences in the tuning set us-
ing the trained weights λM (line 2). The lattices
are used within a modified version of the margin-
infused relaxed algorithm (MIRA; Crammer et al.,
2006) for structured max-margin learning (lines 5-
15). A k-best list is extracted from the current lattice
(line 7), then the translations on the k-best list with
the highest and lowest sentence-level BLEU scores
are found (lines 8 and 9). The step size is then com-
puted using the standard MIRA formula (lines 10-
11) and the update is made (line 12). The returned
weights are averaged over all updates.

This training procedure is inspired by several
8When computing sentence BLEU, we smooth by replacing

precisions of 0.0 with 0.01.

519



Input: input sentences F = {fi}Ni=1, references
E = {ei}Ni=1, initial weights λ0, size of
k-best list k, MIRA max step size C, num.
iterations T

Output: learned weights: λM , 〈λ∗,θ∗〉
λM ← MERT (F , E, λ0);1
{`i}Ni=1 ← generateLattices (F , λM );2
λ← λM ; θ ← 0;3
〈λ̄, θ̄〉 ← 〈λ,θ〉;4
for iter ← 1 to T do5

for i← 1 to N do6
{tj}kj=1 ← Decode(`i, 〈λ,θ〉);7
e+ ← argmax1≤j≤k Bi(tj);8
e− ← argmin1≤j≤k Bi(tj);9
∆← max(0, 〈λ,θ〉> [h(e−)− h(e+)]10

+Bi(e
+)−Bi(e−));

η ← min(C, ∆
‖h(e+)−h(e−)‖2

);11
θ ← θ + η [h(e+)− h(e−)];12
〈λ̄, θ̄〉 ← 〈λ̄, θ̄〉+ 〈λ,θ〉;13

end14
end15
〈λ∗,θ∗〉 ← 〈λ̄, θ̄〉 × 1T×N+1 ;16
return λM , 〈λ∗,θ∗〉;17

Algorithm 1: Train

others that have been shown to be effective for
MT (Liang et al., 2006; Arun and Koehn, 2007;
Watanabe et al., 2007; Chiang et al., 2008). Though
not shown in the algorithm, in practice we store the
BLEU-best translation on each k-best list from all
previous iterations and use it as e+ if it has a higher
BLEU score than any on the k-best list on the cur-
rent iteration.

At decoding time, we follow a procedure similar
to training: we generate lattices for each source sen-
tence using Moses with its standard set of features
and using weights λM . We rescore the lattices us-
ing λ∗ and use cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to incorporate the gappy pattern
features with weights θ∗. Cube pruning is necessary
because the pattern features may match anywhere in
the translation; thus they are non-local in the phrase
lattice and require approximate inference.

5.3 Training Algorithm Comparison

Before adding pattern features, we evaluate our
training algorithm by comparing it to MERT us-
ing the same standard Moses features. As the ini-

ES→EN ZH→EN
MERT 25.64 32.47
Alg. 1 25.85 32.33

Table 5: Comparing MERT to our training procedure. All
numbers are %BLEU.

tial weights λ0, we used the default Moses feature
weights. We used k = 100, C = 0.0001, and
T = 15. For the n-best list size used during cube
pruning during both training and decoding, we used
n = 100. There are several Moses parameters that
affect the scope of the search during decoding and
therefore the size of the phrase lattices. We used
default values for these except for the stack size pa-
rameter, for which we used 100. The resulting lat-
tices encode up to 1050 derivations for ES→EN and
1065 derivations for ZH→EN.

Table 5 shows test set %BLEU for each language
pair and training algorithm. Our procedure per-
forms comparably to MERT. Therefore we use it as
our baseline for subsequent experiments since it can
handle a large number of feature weights; this al-
lows us to observe the contribution of the additional
gappy pattern features more clearly.

5.4 Feature Preparation

We chose monolingual and bilingual pattern features
using the posterior samples obtained via the infer-
ence procedures described above. We ranked pat-
terns using the product-of-experts formula, removed
patterns consisting of only a single token, and added
the top 10K patterns from the lexical model and the
top 15K patterns from the Brown cluster model. For
simplicity of implementation, we skipped over pat-
terns with 3 or more gaps and patterns with 2 gaps
and more than 3 total words; this procedure skipped
fewer than 1% of the top patterns. For results with
bilingual pattern features, we added 15K pattern fea-
tures (5K word-word, 5K cluster-cluster, and 5K
cluster-word).

5.5 Results

The first set of results is shown in Table 6. The
first row is the same as in Table 5, the second
row adds monolingual pattern features, the third
adds bilingual pattern features, and the final row in-
cludes both sets. While gains are modest overall,

520



ES→EN ZH→EN
Baseline 25.85 32.33
MONOPATS 25.84 32.81
BIPATS 25.92 32.68
MONOPATS + BIPATS 25.59 32.80

Table 6: Adding gappy pattern features. All numbers are
%BLEU.

Ranking %BLEU
Baseline N/A 32.33
MONOPATS p(π) 32.65
MONOPATS p(π|w(π)) 32.53
MONOPATS p(π)p(π|w(π)) 32.81
BIPATS p(π) 32.68
MONOPATS + BIPATS p(π) 32.78
MONOPATS + BIPATS p(π)p(π|w(π)) 32.80

Table 7: Comparing ways of ranking patterns from pos-
terior samples. Scores are on MT05 for ZH→EN transla-
tion.

the pattern features show an encouraging improve-
ment of 0.48 BLEU for ZH→EN. This is similar
to the improvement reported by Xiong et al. (2011)
(+0.4 BLEU when adding their trigger pair language
model). While bilingual patterns give an improve-
ment of 0.35 BLEU, using both monolingual and
bilingual features in the same model does not pro-
vide additional improvement over monolingual fea-
tures alone.

For ES→EN, the pattern features have only small
effects on BLEU; we suspect that the decreased
BLEU score for the full feature set is due to over-
fitting. It is unclear why the results differ for the two
language pairs. One possibility is the use of only
a single reference translation when tuning and test-
ing with ES→EN while four references were used
for ZH→EN. Another possibility is that our pattern
features are correcting some of the mid- to long-
range reorderings that are known to be problem-
atic for phrase-based modeling of ZH→EN transla-
tion. ES→EN exhibits less long-range reordering
and therefore may not benefit as much from our pat-
terns.

Table 7 shows additional ZH→EN results when
varying the method of ranking patterns. When us-
ing both sets of features, the “Ranking” column
contains the criterion for ranking monolingual pat-
terns; bilingual patterns are always ranked using

said that the however , the agence france presse
’s , ’s us iraq reported the
of million , likely said that and
added " - - rate percent
the {media, school, university, election, bank}

{made, established, given, taken, reached}
{said, stressed, stated, indicated, noted} that in
{meeting, report, conference, reports} {1, july, june, march, april}
{news, press, spokesman, reporter} {meeting, . . .} {1, july, . . .}
{news, press, spokesman, reporter} {1, july, june, march, april}
the {enterprises, companies, students, customers, others}

{enterprises, companies, students, customers, others}
{japan, russia, europe, 2003, 2004} {us, japanese, russian, u.s.}

Table 8: Selected features from the 15 most highly-
weighted lexical and cluster pattern features in the best
ZH→EN model.

p(π). The results show that ranking monolingual
patterns using the product-of-experts method results
in the highest BLEU scores, validating our intu-
itions from observing Tables 1-3. Table 8 shows the
most highly-weighted pattern features for the best
ZH→EN model.

6 Conclusion

We have presented generative models for monolin-
gual and bilingual gappy patterns. A qualitative
analysis shows that the models discover patterns
that match our intuitions in capturing linguistic phe-
nomena. Our experimental results show promise
for the ability of these patterns to improve trans-
lation for certain language pairs. A key advan-
tage of generative models is the ability to rapidly
develop and experiment with variations, especially
when using Gibbs sampling for inference. In order
to encourage modifications and extensions to these
models we have made our source code available at
www.ark.cs.cmu.edu/MT.

Acknowledgments

The authors thank Chris Dyer, Qin Gao, Alon Lavie,
Nathan Schneider, Stephan Vogel, and the anonymous
reviewers for helpful comments. This research was sup-
ported in part by the NSF through grant IIS-0844507, the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-
1-0533, and Sandia National Laboratories (fellowship to
K. Gimpel).

521



References

A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.

M. Bansal, C. Quirk, and R. Moore. 2011. Gappy
phrasal alignment by agreement. In Proc. of ACL.

P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
els of natural language. Computational Linguistics,
18.

C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on statistical machine translation
and metrics for machine translation. In Proc. of the
5th Workshop on Statistical Machine Translation.

P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of the Third Workshop on
Statistical Machine Translation.

S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.

D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.

D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of ACL.

D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.

K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.

J. M. Crego and F. Yvon. 2009. Gappy translation units
under left-to-right SMT decoding. In Proc. of EAMT.

M. Galley and C. D. Manning. 2010. Accurate non-
hierarchical phrase-based translation. In Proc. of
NAACL.

L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.

F. Jelinek. 1997. Statistical methods for speech recogni-
tion. MIT Press.

P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).

P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.

P. Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, Massachusetts Institute of
Technology.

F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).

F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.

K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.

R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).

M. Simard, N. Cancedda, B. Cavestro, M. Dymetman,
É. Gaussier, C. Goutte, K. Yamada, P. Langlais, and
A. Mauser. 2005. Translating with non-contiguous
phrases. In Proc. of HLT-EMNLP.

A. Stolcke. 2002. SRILM—an extensible language mod-
eling toolkit. In Proc. of ICSLP.

N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of
word graphs in statistical machine translation. In Proc.
of EMNLP.

T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.

D. Xiong, M. Zhang, and H. Li. 2011. Enhancing lan-
guage models in statistical machine translation with
backward N-grams and mutual information triggers.
In Proc. of ACL.

522


