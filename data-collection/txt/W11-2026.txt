










































Facilitating Mental Modeling in Collaborative Human-Robot Interaction through Adverbial Cues


Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 239–247,
Portland, Oregon, June 17-18, 2011. c©2011 Association for Computational Linguistics

Facilitating Mental Modeling in Collaborative Human-Robot Interaction
through Adverbial Cues

Gordon Briggs and Matthias Scheutz
Human-Robot Interaction Laboratory

Tufts University, Medford, MA 02155, USA
{gbriggs,mscheutz}@cs.tufts.edu

Abstract

Mental modeling is crucial for natural human-
robot interactions (HRI). Yet, effective mech-
anisms that enable reasoning about and com-
munication of mental states are not available.
We propose to utilize adverbial cues, routinely
employed by humans, for this goal and present
a novel algorithm that integrates adverbial
modifiers with belief revision and expression,
phrasing utterances based on Gricean conver-
sational maxims. The algorithm is demon-
strated in a simple HRI scenario.

1 Introduction

Advances in robotics and autonomous systems are
paving the way for the development of robots that
can take on increasingly complex tasks without the
need of minute human supervision. As a result
of this greater autonomy, the interaction styles be-
tween humans and robots are slowly shifting from
those of humans micromanaging robot behaviors
(e.g., via remote controls) to more higher-level in-
teractions (e.g., verbal commands) which are re-
quired for many mixed initiative tasks where hu-
mans and robots work together in teams (e.g., in
search and rescue missions). In order for these joint
human-robot interactions to be productive and effi-
cient, robots must have the ability to communicate in
natural and human-like ways (Scheutz et al., 2007).
Natural human-like communication in robots, how-
ever, requires us to tackle several challenges, includ-
ing the development of robust natural language (NL)
competencies and the ability to understand and uti-
lize a variety of affective, gestural, and other non-

linguistic cues that are indicative of the interlocu-
tor’s mental states. Hence, natural human-like in-
teraction also requires the construction and mainte-
nance of mental models of other agents, especially
in the context of collaborative team tasks where ac-
tions among multiple agents must be coordinated,
often through natural language dialogues.

Several recent efforts are aimed at endowing
robots with natural language processing capabilities
to allow for verbal instructions as a first step (e.g.,
(Brenner, 2007; Dzifcak et al., 2009; Kress-Gazit et
al., 2008; Rybski et al., 2007; Kollar et al., 2010)).
Independently, user modeling has been extensively
explored in order to generate more natural and pro-
ductive human-machine interactions (Kobsa, 2001),
including adapting the natural language output of di-
alogue systems based on mental models of human-
users (Wahlster and Kobsa, 1989). However, there
is currently no integrated robotic architecture that
includes explicit mechanisms for efficiently convey-
ing natural language information about the robot’s
“mental states” (i.e., beliefs, goals, intentions) to a
human teammate. Yet, such mechanisms are not
only desirable to make the robot’s behavior more in-
tuitive and predictable to humans, but can also be
crucial for team success (e.g., quick updates on goal
achievement or early corrections of wrong human
assumptions).

We propose a novel integrated belief revision and
expression algorithm that allows robots to track and
update the beliefs of their interlocutors in a way
that respects Gricean maxims about language usage.
The algorithm explicitly models and updates task-
relevant beliefs and intentions of all participating

239



agents. Whenever a discrepancy is detected between
a human belief (as implied in a natural language ex-
pression uttered by the human) and the robot’s men-
tal model of the human, the robot generates a natural
language response that corrects the discrepancy in
the most effective way. To achieve effectiveness, the
robot uses linguistic rules about the pragmatic im-
plications of adverbial modifiers like “yet”, “still”,
“already”, and others that are used by humans to ef-
fectively communicate their beliefs and intentions.

The rest of the paper is organized as follows. We
start with a motivation of our approach based on
Gricean maxims. Then, we introduce formalizations
of linguistic devices that humans use to generate ef-
fective task-based dialogue interactions and present
our algorithm for generating appropriate utterances
in response to human queries. Next we use a simple
remote human-robot interaction scenario to demon-
strate the operation of the algorithm, followed by a
discussion and summary of our contributions.

2 Motivation

Joint activity often requires agents to monitor and
keep track of each others’ mental states to ensure ef-
fective team performance. For example, searchers
during rescue operations in disaster zones typically
coordinate their (distributed and remote) activities
through spoken natural language interactions via
wireless audio links to keep team members informed
of discoveries and plans of other team members. Co-
ordination as part of joint activities requires two im-
portant processes in an agent: (1) building and main-
taining a mental model of the other agents’ beliefs
and intentions (based on perceived, communicated,
and inferred information), which is critical for sit-
uational awareness (Lison et al., 2010); and (2) ac-
tively supporting the maintenance of others’ mental
models of oneself (e.g., by proactively communicat-
ing new information to the other agents in ways that
will allow them to update their mental models).

Cohen et al. (1990), for example, discuss the
necessity of various communicative acts that serve
to synchronize agent belief-models. These com-
municative acts include both linguistic and non-
linguistic cues, such as utterances of confirmation
(“okay.”) or signals that indicate intention (putting
on a turn-signal). In addition to utilizing explicit

cues to synchronize belief-models, humans employ
various other mechanisms to convey information
about one’s own belief-state, in particular, various
linguistic devices. A simple, but very powerful lin-
guistic mechanism is the use of adverbial cues.

Consider a scenario where one agent wants to
know the location of another agent, e.g., whether
the agent is at home. A straightforward way to ob-
tain this information is to simply ask “Are you at
home?” The other agent can then answer “yes” or
“no” accordingly. Now, suppose the first agent knew
that the second agent was planning to be at home at
some point. In that case, the agent might ask “Are
you at home yet?” Note that semantically both ques-
tions have the same meaning, but their pragmatic
implications are different as the second implies that
that agent 1 knows that agent 2 was planning to be
at home, while no such implication can be inferred
from the first query. Conversely, suppose that agent
2 responded “not yet” in the first example (instead
of “no”). While the semantic meaning is the same as
“no”, “not yet” communicates to agent 1 that agent
2 has the goal to be home. In general, adverbs like
“yet” can be used to convey information about one’s
(or somebody else’s) beliefs concerning mutually-
recognized goals and intentions. Not surprisingly,
humans use them regularly and with ease to aid their
interlocutors with maintaining an accurate model of
their beliefs and goals.

The challenges that need to be addressed to al-
low robots to have the above kinds of linguistic ex-
changes are: (1) how to formalize the functional
roles of adverbial modifiers in different sentence
types, and how to use the formalized principles to
(2) perform belief updates and (3) generate effective
natural language responses that are natural, succinct,
and complete. To tackle these three challenges, we
turn to Gricean principles that have long be used in
pragmatics as guiding principles of human commu-
nicative exchanges.

3 NL Understanding and Generation

Grice (1975) proposed four general principles to aid
in the pragmatic analysis of utterances. Phrased as
rules, it is unsurprising that they have been used
as an inspiration for NL generation systems before.
Dale and Reiter (1995) have enlisted the maxims in

240



their design of an algorithm to generate referring ex-
pressions, while others have cited Gricean influence
in utterance selection for intelligent tutor systems
(Eugenio et al., 2008). The particular maxims we
considered are the maxims of quality (G1), quantity
(G2), and relevance (G3): (G1) requires one to not
say what one believes is false or for which one lacks
adequate evidence; (G2) requires one to make con-
tributions as informative as necessary for the current
purposes of the exchange, but not more informative;
and (G3) tersely states “be relevant.”

Our approach to belief-model synchronization
and utterance selection is based on the above max-
ims and attempts to select the most appropriate re-
sponse to another agent’s query based on relevance
of semantic content. It uses speech pragmatic mean-
ing postulates for linguistic devices such as adver-
bial modifiers to search for a succinct and natural
linguistic representation that captures the intended
updates. Rather than explicitly communicating each
and every proposition that needs to be communi-
cated to a human to allow the person to update
their mental model of the robot, the algorithm makes
heavy use of “implied meanings”, i.e., propositions
that humans will infer from the way the informa-
tion is phrased linguistically. This allows for much
shorter messages to be communicated than other-
wise possible and addresses the second maxim of
quantity.

3.1 Formalizing pragmatic implications

We start by introducing four types of sentences
as they are found in typical dialogue interac-
tions: statements (expressed through declarative
sentences), questions (expressed through interrog-
ative sentences), commands (expressed through
imperative sentences) and acknowledgments (ex-
pressed through words like “okay”, “yes”, “no”,
etc.). For simplicity, we restrict the discussion to
one predicate at(α, λ) which states that agent α is
in location λ.

3.1.1 Statements
We will use the form Stmt(α, β, φ, µ) to ex-

press that agent α communicates φ to agent β us-
ing adverbial modifiers in a set µ. For exam-
ple, Stmt(A2, A1,¬at(A2, home), yet) means that
agent A2 tells A1 that it is not at home yet. Note

that we are indifferent about the exact linguistic rep-
resentation of φ here as the goal is to capture the
pragmatic implications.

If α informs β that it is at λ without any adverbial
modifiers or additional contextual information, then
we can assume using (G1) that α is indeed at that
location:

[[Stmt(α, β, at(α, λ), {})]]c := at(α, λ) (1)

Here we use [[..]]c to denote the “pragmatic mean-
ing” of an expression in context c, which includes
task, goal, belief and discourse aspects. Next, we
inductively define the pragmatic meanings for sev-
eral adverbial modifiers “still”, “already”, “now”,
and “not yet” (the meanings of compound expres-
sions such as at(α, λ1) ∧ ¬at(α, λ2) are defined re-
cursively in the usual way).

If α states that it is “still” at λ, one can infer that
α is at λ and that α will not be at λ at some point in
the future:

[[Stmt(α, β, at(α, λ), {still})]]c := (2)
[[Stmt(α, β, at(α, λ)), {}]]c ∧ Future(¬at(α, λ))

If α states that it is “already” at λ, one can infer that
α is at λ and that α had a goal (expressed via the
“G” operator) to be at λ at some point in the past:

[[Stmt(α, at(α, λ), {already})]]c := (3)
[[Stmt(α, β, at(α, λ), {}]]c ∧ Past(G(α, at(α, λ)))

If α states that it is “now” at λ, one can infer that α
is at λ and that α had not been at λ at some point in
the past:

[[Stmt(α, β, at(α, λ), {now})]]c := (4)
[[Stmt(α, β, at(α, λ)), {}]]c ∧ Past(¬at(α, λ))

If α states that it is “not...yet” at λ, one can infer
that α is not at λ, but has an intention to be at λ.

[[Stmt(α, β,¬at(α, λ), {yet})]]c := (5)
¬at(α, λ) ∧G(α, at(α, λ))

Even in our limited domain, one must be cog-
nizant of the ambiguities that arise from how ad-
verbial cues are deployed. In addition to the simple
presence of an adverbial cue, the location of the ad-
verb in a sentence and prosodic factors may affect
the intended meaning of the utterance. For instance,
consider the statements: (a) I am now at λ; (b) I am

241



at λ now; (c) I am still at λ; and (d) I am still at λ.
Statement (a) is a simple situational update utterance
as described above, while (b) could be construed as
a statement akin to “I am already at λ. Statement
(d) could be interpreted as additionally signaling the
frustration of the agent, beyond conveying the infor-
mation from (c).

It should also be noted that our analysis of these
adverbial cues is to be understood in the limited
context of these simple task-related predicates (e.g.
at(α, λ)). Formal definition of these adverbial cues
in general cases is beyond the scope of this paper.
For instance, “yet” could be used in a context when
the predicate is not intended by the agent to which it
applies (e.g. “Has Bill been fired yet?”). In this case,
it would probably be incorrect to infer that the agent
Bill had a goal to be fired. Instead an inference could
be made regarding the probabilistic judgments of the
interlocutors regarding the topic agent’s future state.
However, in the context of this paper, it is assumed
that “yet” is used in the context of goals intended by
agents.

3.1.2 Questions
Here we will limit the discussion to two question

types, the “where” question (regarding locations)
and simple “Yes-No” questions.

If α asks β about its location in the general sense
(“where are you?”), then one can infer that α has an
intention to know (expressed via the “IK” operator,
see (Perrault and Allen, 1980)) where β is located:

[[Askloc(α, β, {})]]c := IK(α, at(β, λ)) (6)

for some λ.
If α asks β whether it is at λ, then one can infer

that α has an intention to know whether β is at λ:

[[Askyn(α, β, at(β, λ), {})]]c := IK(α, at(β, λ)) (7)

If β is asked by α whether it is “still” at λ, β can in-
fer that α believes (expressed via the “B” operator)
that β is currently at λ:

[[Askyn(α, β, at(β, λ), {still})]]c := (8)
[[Askyn(α, β, at(β, λ), {})]]c ∧B(α, at(β, λ))

If β is asked by α whether it is at λ “yet”, β can
infer that α believes that β has a goal to be at λ:

[[Askyn(α, β, at(β, λ), {yet})]]c := (9)
[[Askyn(α, β, at(β, λ), {})]]c ∧B(α,G(β, at(β, λ)))

3.1.3 Question-Answer Pairs
Next, we consider how discourse context as pro-

vided by question-answer pairs can further specify
the pragmatic implications.

If α asks β whether it is at λ with
any set of adverbial modifiers µ (i.e.,
Prior(Askyn(α, β, at(β, λ), µ)) ∈ c), and β
responds by stating that it is “still” at λ, then one
can infer that α has the belief that β was at λ in the
recent past:

[[Stmt(β, α, at(β, λ), {still})]]c := (10)
[[Stmt(β, α, at(β, λ), {})]]c
∧B(α,RecPast(at(β, λ)))

where Prior(Askyn(α, β, at(β, λ), µ)) ∈ c. Also,
RecPast(φ) denotes that φ was true in the recent
past, as distinct from φ holding at some arbitrary
point in the past (i.e. Past(φ)). This distinction is
necessary as it only makes sense to use the adverbial
cue at this point if agent α believed at(β, λ) at some
relative and recent point in the past. Formalizing this
would require keeping track of the points in time at
which certain propositions are believed. To avoid
committing to a particular temporal modeling sys-
tem, we make the simplifying assumption that the
RecPast operator is not applied in rules (10) and
(11), which is sufficient for the very simple interac-
tions examined in this paper.

If α asks β whether it is at λ with any set of ad-
verbial modifiers µ, and β responds by stating that it
is “now” at λ, then one can infer that α has the belief
that β is was not at λ in the recent past:

[[Stmt(β, α, at(β, λ), {now})]]c := (11)
[[Stmt(β, α, at(β, λ), {})]]c
∧B(α,RecPast(¬at(β, λ)))

where Prior(Askyn(α, β, at(β, λ), µ)) ∈ c.

3.1.4 Commands
We also briefly describe how command process-

ing (which we have studied elsewhere in much
greater detail (Dzifcak et al., 2009)) can be aug-
mented with the inclusion of pragmatic meanings.
If α orders β to travel to λ, then one can infer that α
has a goal for β to be at λ and that α intends to know
whether β has received its new goal:

[[Cmd(α, β, at(β, λ), {})]]c := (12)
G(α, at(β, λ))

∧IK(α,G(β, at(β, λ)))

242



It would be an oversimplification to assume that
the proposition G(β, at(β, λ)) is immediately un-
derstood by all listening agents. In order to generate
the appropriate goal belief in the target agent, ad-
ditional inference rules need to be considered. The
following rule states that β will instantiate the goal
G(β, at(β, λ)) when it believes α has the same goal
and it believes authority(α, β), which denotes that
α has command authority over β:

G(α, at(β, λ)) ∧ authority(α) ⇒
G(β, at(β, λ))

Other agents would have to wait for an acknowledg-
ment that this inference has indeed taken place (as β
could have not heard the initial command utterance).
These acknowledgment utterances are described in
the subsequent section.

3.1.5 Acknowledgments
Finally, we consider typical forms of acknowledg-

ment. If α utters an acknowledgment (e.g., “OK.”)
when the previous utterance was a positive statement
of location by β, then one can infer α no longer has
the intention to know β’s location:

[[Ack(α, β, {})]]c := ¬IK(α, at(β, λ)) (13)

for some λ where for any M
Prior(Stmt(β, α, at(β, λ), {M})) ∈ c.

If α utters an acknowledgment (e.g., “OK.”) when
the previous utterance was a command by β to be at
λ, then one can infer that

[[Ack(α, β, {})]]c := (14)
G(α, at(α, λ)) ∧G(β, at(α, λ))
∧¬IK(β,G(α, at(α, λ)

where Prior(Cmd(β, α, at(α, λ), {M})) ∈ c for
any M .

We should note here that the distinction between
explicitly not intending-to-know and the lack of an
intention-to-know has been blurred in the above
rules for the sake of simplicity. As described
in the subsequent section, agent beliefs are re-
moved when contradicted in the current system (i.e.
Remove(φ,Bα) ⇔ (¬φ) ∈ Bα). A more com-
prehensive belief update system should allow for a
mechanism to remove beliefs without the need for
explicit contradiction.

3.2 Agent Modeling and Belief Updates

Belief updates occur whenever an agent α receives
an utterance Utt from another agent β in context
c. First, [[Utt]]c is computed using the pragmatic
principles and definitions developed in Section 3.1.
For simplicity, we assume that agents adhere to
the Gricean maxim of quality and, therefore, do
not communicate information they do not believe.
Hence, all propositions φ ∈ [[Utt]]c are assumed
to be true and to the extent that they are inconsis-
tent with existing beliefs of α as determined by α′s
inference algorithm ⇒bα, the conflicting beliefs are
removed from the agent’s sets of beliefs Belself (b
here denotes some finite bound on the inference al-
gorithm, e.g., resources, computation time, etc.).1

To model other agents hearing the utterance, agent
α derives the set BαBγ = {ψ|B(γ, ψ) ∈ Belself}
for all other agents γ 6= α. The agent updates these
belief sets by applying the same rules as it does to
Belself .

It should be noted that these belief update rules
are indeed simplifications designed to avoid the is-
sue of resolving conflicting information from dif-
ferent sources. These belief update rules would be
problematic, for instance, when agents have incor-
rect beliefs (and proceed to communicate them), as
no method for belief disputation exists. For the pur-
pose of illustrating the implementation and utility of
adverbial cues, however, they should suffice. We
set up our environment and rule sets such that the
autonomous agent has perfect information about it-
self (specifically location), and no utterances exists
to communicate propositions that are not about one-
self.

3.3 Sentence Generation

Depending on the sentence type α received (and the
extent to which meanings can be resolved, an issue
we will not address in this paper), different response
sentence types are appropriate (e.g., a yes-no ques-

1Note that we are not making any assumption about a partic-
ular inference algorithm or its (as it will, in general, depend on
the expressive power of the employed logic to represent mean-
ings), only that if a contradiction can be reached using the in-
ference algorithm, the existing belief needs to be removed (oth-
erwise existing beliefs are taken to be consistent with the impli-
cations of the utterance). In our implemented system, we use a
simplified version of the resolution inference principle.

243



tion requires a statement answering the question).
The generation of an appropriate response proceeds
in two steps. First, based on the agent’s current set
of beliefs Belself , we determine the set of proposi-
tions Φcomm that the agent has an interest in con-
veying. Second, we attempt to find the smallest ut-
terance Utt given a set of pragmatic principles (as
specified in Section 3.1) that communicates one or
more of these propositions and implies the rest for
recipient β.

3.3.1 What to say
In obtaining a set Φcomm of propositions to com-

municate, α may obey the Gricean maxim of qual-
ity by adding a proposition φ to Φcomm only if
φ ∈ Belself . The maxims of relevance and quan-
tity are heeded by restricting believed propositions
to be conveyed solely to those that either correct a
false belief of β or provide β some piece of infor-
mation it wants to know. Specifically, we find the
set of all propositions used to correct false beliefs
Φrev, defined as:

ψ ∈ Φrev ⇔ ∃β, φ :
B(β, φ) ∧ φ ∈ Belself ∧ (ψ ⇒bα ¬φ)

The set of all propositions other agents want to
know, ΦIK , can be defined as:

ψ ∈ ΦIK ⇔ ∃β, φ : ψ ∈ Belself∧
IK(β, φ ∈ Belself ) ∧ (ψ ⇒bα φ ∨ ψ ⇒bα ¬φ)

The final set of propositions to convey is obtained
by merging these two sets, Φcomm = Φrev ∪ ΦIK .
Note that this set is always consistent because propo-
sitions are added to Φrev and ΦIK if and only if they
exist in Belself , which is maintained to be consis-
tent.

3.3.2 How to say it
Once Φcomm has been obtained, αmust select po-

tential utterances to produce. It starts by generating
an initial set Utt0 of utterances that in the present
context c imply some subset of Φcomm:

(u ∈ Utt0) ⇔ ∃Φ ∈ Φcomm∀φ ∈ Φ : ([[u]]c ⇒bα φ)

Currently, this is achieved by searching through
the set of all utterances defined by rules such as those

found in Section 3.1. Note that while this approach
is feasible for our quite limited domain, more effi-
cient methods for identifying candidate utterances
must be developed as the number of understood ut-
terances grows.

Applying the maxim of quality, this set can be
pruned of all utterances that are defined by addi-
tional propositions that we either have no evidence
for (“unsupported”) or explicitly believe to be false:

False(φ) ⇔ ∃ψ : ψ ∈ Belself ∧ (ψ ⇒bα ¬φ)

NoSupp(φ) ⇔ ¬∃ψ : ψ ∈ Belself ∧ (ψ ⇒bα φ)

Using these conditions, we can generate a new sub-
set of utterance candidates Utt1:

(u ∈ Utt1) ⇔ ¬∃φ : ([[u]]c ⇒bα φ)
∧(False(φ) ∨NoSupp(φ)))

Applying the maxim of quantity, utterances that
revise or add the most beliefs to other agent belief-
spaces ought to be favored:

RevBel(β, φ) ⇔
∃ψ : B(β, ψ) ∈ Bself ∧ (ψ ⇒bα ¬φ)

AddBel(β, φ) ⇔ B(β, φ) 6∈ Belself

Using these definitions, we can derive the
“correction-score” of an utterance by counting
the number of propositions φ ∈ [[u]]c that revise or
add a belief for β.

If multiple candidate utterances still exist at this
point, we can again apply the maxim of quantity to
favor utterances that convey the most (true) informa-
tion. Because all definitions with false propositions
have been eliminated, we can simply count the num-
ber of true propositions derived from the utterance,
thereby favoring semantically richer utterances. At
this point, if multiple candidate utterances are still
available, the difference is of stylistic nature only
and we may choose an arbitrary one. Note that the
correct usage of adverbial modifiers emerges natu-
rally from these rules as utterances that include in-
appropriate adverbs are removed in Utt1, while ut-
terances that include appropriate adverbial cues are
subsequently favored.

244



4 Case Study

We now demonstrate the operation of the proposed
algorithm in a simple joint activity scenario where
a robot (R) is located at nav-point 1 and correctly
knows its location, having the initial belief-space
BR = {at(R,N1)}. The remote human operator
starts by asking:
O: R, where are you?

R updates its beliefs based on this question:
u := parse(“O: R, where are you?”)
→ u := Askloc(O,R, {})
[[u]]c := {IK(O, at(R,N1)), IK(O, at(R,N2)),
IK(O, at(R,N3))}
Pcontra := contradictedTerms([[u]]c, Bself )
BR := (BR − Pcontra) + [[u]]c
BRBO := (BRBO − Pcontra) + [[u]]c

which yields a new belief-space:
BR := {at(R,N1), IK(O, at(R,N1)),
IK(O, at(R,N2)), IK(O, at(R,N3)),
B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),
B(O, IK(O, at(R,N3)))}

Next, R proceeds to respond. For compactness, we
refer below to utterance candidates according to the
index of the applicable rules from Section 3.1, so
that u13 denotes Ack(α, β, {}).

BRBO := {IK(O, at(R,N1)), IK(O, at(R,N2))
IK(O, at(R,N3))}
Φrev := {}; ΦIK := {at(R,N1)}
Φcomm := {at(R,N1)};
→ Utt0 := {u1, u2, u3, u4}

R now has an initial set of candidate utterances,
which it prunes using the rules from Section 3.3.2.

[[u1]]c := at(R,N1)
[[u2]]c := at(R,N1) ∧ Future(¬at(R,N1))
[[u3]]c := at(R,N1) ∧ Past(G(R,N1))
[[u4]]c := at(R,N1) ∧ Past(¬at(R,N1))
→ Utt1 := {u1}

Thus, R chooses the utterance of the form,
Stmt(R,O, at(R,N1), {}), and responds:
R: I am at N1.

Finally, R processes its own utterance so that it can
update its beliefs according to rule (1):

BR := {at(R,N1), IK(O, at(R,N1)),
IK(O, at(R,N2)), IK(O, at(R,N3)),
B(O, IK(O, at(R,N1))), B(O, IK(O, at(R,N2))),
B(O, IK(O, at(R,N3))), B(O, at(R,N1)}

When the operator responds:
O: Okay.

R also processes this acknowledgment to update its
beliefs according to rule (13):

BR := {at(R,N1), B(O, at(R,N1)}

R proceeds to respond, but finds that it has nothing
to convey.

BRBO := {at(R,N1)}
; Φrev := {}; ΦIK := {}
; Φcomm := {};
→ Utt0 := {}

Thus, R generates no utterance. Now let us suppose
that R moves to N2, and enough time elapses such
that the operator forfeits his/her conversational turn.
R then proceeds to generate an utterance.

BR := {at(R,N2), Past(at(R,N1))}
BRBO := {at(R,N1)}
Φrev := {at(R,N2)}; ΦIK := {}
Φcomm := {at(R,N2)}
→ Utt0 := {u1, u2, u3, u4}
[[u1]]c := at(R,N2)
[[u2]]c := at(R,N2) ∧ Future(¬at(R,N2))
[[u3]]c := at(R,N2) ∧G(R,N2)
[[u4]]c := at(R,N2) ∧ Past(¬at(R,N2))
→ Utt1 := {u1, u4}

So, R must now resolve which of these candidate
utterances to select by choosing the one that revises
the most beliefs of O, or failing that, the one that has
the most true propositions.

at(R,N2)⇒ ¬at(R,N1)
→ NumRev([[u1]]c) := 1;NumRev([[u4]]c) := 1;
NumTrue([[u1]]c) := 1;NumTrue([[u4]]c) := 2;
→ Uttfinal := u4

Thus, R chooses the utterance of the form,
Stmt(R,O, at(R,N2), {now}), and responds:
R: I am now at N2.

R again processes its own utterance to update its be-
liefs according to rule (4). If O then asks:
O: R, are you still at N2?

R updates its beliefs according to rule (10):
BR := {at(R,N2), B(O, at(R,N2)),
Past(at(R,N1)), B(O,Past(at(R,N1))),
IK(O, at(R,N2)), B(O, IK(O, at(R,N2)))}

Next, R generates a response:
Φrev := {}; ΦIK := {at(R,N2)}
Φcomm := {at(R,N2)};
→ Utt0 := {u1, u2, u10, u11}
[[u1]]c := at(R,N2)
[[u2]]c := at(R,N2) ∧ Future(¬at(R,N2))
[[u10]]c := at(R,N2) ∧B(O, at(R,N2))
[[u11]]c := at(R,N2) ∧B(O,¬at(R,N2))
→ Utt1 := {u1, u10}
→ NumRev([[u1]]c) := 0;NumRev([[u10]]c) := 0;
NumTrue([[u1]]c) := 1;NumTrue([[u10]]c) := 2;
→ Uttfinal := u10

replying with the utterance:
R: I am still at N2.

245



and processes its own utterance to updates its beliefs
according to rule (10). O’s acknowledgment:
O: Okay.

causes R to update its beliefs according to rule (13):
BR := {at(R,N2), B(O, at(R,N2)),
Past(at(R,N1)), B(O,Past(at(R,N1)))}

R does not generate a response as there are no be-
liefs to revise or intentions to know. Now suppose
R moves back to N1, without O’s knowledge, after
which O commands:
O: R, go to N1.

R, updates its belief according to rule (12):
BR := {at(R,N1), B(O, at(R,N2)),
Past(at(R,N2)), B(O,Past(at(R,N2))),
G(R, at(R,N1)), G(O, at(R,N1)),
IK(O,G(R, at(R,N1))),
B(O,G(R, at(R,N1))),
B(O, IK(O,G(R, at(R,N1)))),
B(O,G(O, at(R,N1)))}

and proceeds to generate a response:
Φrev := {at(R,N1)}
ΦIK := {G(R, at(R,N1))}
Φcomm := {at(R,N1), G(R, at(R,N1))}
→ Utt0 := {u1, u2, u3, u4}
[[u1]]c := at(R,N1)
[[u2]]c := at(R,N1) ∧ Future(¬at(R,N1))
[[u3]]c := at(R,N1) ∧G(R, at(R,N1))
[[u4]]c := at(R,N1) ∧ Past(¬at(R,N1))
→ Utt1 := {u1, u3, u4}
→ NumRev([[u1]]c) := 1;NumRev([[u3]]c) := 2;
NumRev([[u4]]c) := 1
→ Uttfinal := u3

Thus, R responds:
R: I am already at N1.

5 Discussion and Related Work

While the above case study was kept simple due to
space restrictions, it demonstrates the utility of our
utterance generation method in adapting NL output
at the sentence-level based on a mental-model of an
interlocutor. In particular, we adapted utterances by
employing adverbial modifiers, which serve to make
the speaker’s belief-space more transparent and nat-
ural, which was the main motivation for the devel-
opment of the formal framework with rules for ad-
verbial modifiers in the first place. Other examples
of adaptations that are intended to make an auto-
mated system’s reasoning and internal state repre-
sentations more open and clear to human-users in-
clude the sentence-level adaptation of restaurant rec-
ommendations (Walker et al., 2007) and the adapta-

tion of query-phrasing in a robotic context (Kruijff
and Brenner, 2009). In addition to conveying in-
formation about one’s own mental state, pragmatic
principles and rules, such as those we have pre-
sented, may be deployed to reason about the in-
tentions and beliefs of others (Perrault and Allen,
1980).

The current system, while a promising step to-
wards more natural task-based dialogue interactions,
has several limitations. Aside from lexical and se-
mantic limitations, the currently implemented ad-
verbial modifiers are restricted to very simple pred-
icates. Clearly, these restrictions will have to be
addressed and the formal definitions will have to
be widened. Moreover, the system currently does
not handle situations where a human’s mental state
changes without the robot’s knowledge, which can
cause misunderstandings that need to be detected
and corrected effectively. Additionally, agents can
be mistaken about their beliefs. Real-world com-
plexities such as these suggest the inclusion of han-
dling uncertainty in a belief modeling system (Lison
et al., 2010), potentially by assigning beliefs confi-
dence values. This is clearly an important topic for
future work.

User-model based adaptation of NL output at the
sentence level that includes multi-modal compo-
nents (Walker et al., 2004) has also not been ad-
dressed. Further study is required to determine
whether our Gricean-inspired utterance selection
method can also be applied to non-linguistic com-
munication modalities. Finally, the current sys-
tem can only handle simple perceptual updates and
has limitations when handling multi-robot dialogues
(neither of which are discussed here for space rea-
sons). The challenges of perceptual updates that will
have to be addressed are investigated in the con-
text of a plan-based situated dialogue system for
robots in (Brenner, 2007) and extensions to multi-
robot scenarios are explored in (Brenner and Kruijff-
Korbayova, 2008).

6 Conclusion

Competency in mental modeling is a crucial com-
ponent in the development of natural, human-like
interaction capabilities for robots in mixed initia-
tive settings. We showed that the ability to under-

246



stand and employ adverbial modifiers can help both
in constructing mental models of human operators
and conveying one’s own mental state to others.

To this end, we made three contributions. First,
we introduced a framework for formalizing different
sentence types and the pragmatic meanings of ad-
verbial modifiers. Second, we showed how one can
perform belief updates based on implied meanings
of adverbial modifiers. And third, we introduced
a novel algorithm for generating effective responses
that obey three Gricean maxims and aid the listener
in appropriate belief updates. The core properties of
the algorithm are that it corrects false or missing be-
liefs in other agents, that it provides an agent with
information that is wanted, that it never generates an
utterance that implies false propositions, and that it
first favors utterances that convey more (true) propo-
sitions after favoring utterances that revise or add
more beliefs to the listener’s belief-space. Finally,
we demonstrated our algorithm responding to basic
operator queries in a simple case study, correctly us-
ing adverbial cues to sound more natural and convey
more information regarding its beliefs.

There are extensive avenues to pursue future
work. For instance, we plan to extend the algo-
rithm to include multi-modal perceptual integration
as well as multi-agent multi-dialogue capabilities.
A variety of empirical evaluations would be desir-
able to evaluate the efficacy and naturalness of the
proposed adverbial cues in simulated and real HRI
tasks. Additionally, empirical evaluations could also
be performed to observe additional cues to incorpo-
rate into the system.

7 Acknowledgments

This work was supported by an ONR MURI grant
#N00014-07-1-1049 to the second author. We wish
to extend our thanks to Paul Schermerhorn and the
anonymous reviewers for providing valuable feed-
back.

References

A. Kobsa. 2001. Generic User Modeling Systems. User
Modeling and User-Adapted Interaction 11: 49–63.

B. Di Eugenio, et al. 2008. Be Brief, And They Shall
Learn: Generating Concise Language Feedback for a

Computer Tutor. International Journal of Artificial In-
telligence in Education 18(4).

C. R. Perrault and J. F. Allen. 1980. A Plan-Based Anal-
ysis of Indirect Speech Acts. American Journal of
Computational Linguistics, 6(3-4):167–182.

G. M. Kruijff and M. Brenner. 2009. Phrasing Questions.
AAAI 2009 Spring Symposium.

H. Kress-Gazit and G. E. Fainekos and G. J. Pappas 2008
Translating Structured English to Robot Controllers
Advanced Robotics 22, 12, 1343–1359

H. P. Grice. 1975. Logic and conversation. Syntax and
Semantics, 3(1):43–58.

J. Dzifcak, M. Scheutz, C. Baral, and P. Schermerhorn.
2009. What to do and how to do it: Translating Nat-
ural Language Directives into Temporal and Dynamic
Logic Representation for Goal Management and Ac-
tion Execution. ICRA.

M. A. Walker, et al. 2004. Generation and evaluation of
user tailored responses in multimodal dialogue. Cog-
nitive Science 28: 811–840.

M. Walker, et al. 2007. Individual and Domain Adap-
tation in Sentence Planning for Dialogue. Journal of
Artificial Intelligence Research. 30: 413–456.

M. Scheutz, et al. 2007. First Steps toward Natural
Human-Like HRI. Autonomous Robots 22(4):411–
423.

M. Brenner. 2007. Situation-Aware Interpretation,
Planning and Execution of User Commands by Au-
tonomous Robots. RO-MAN 2007.

M. Brenner and I. Kruijff-Korbayova. 2008. Continual
planning and acting in dynamic multiagent environ-
ments. 12th SEMDIAL Workshop.

P. R. Cohen, et al. 1990. Task-Oriented Dialogue as a
Consequence of Joint Activity. Pacific Rim Interna-
tional Conference on Artificial Intelligence.

P. Lison, C. Ehrler, and G. M. Kruijff. 2010. Belief Mod-
elling for Situation Awareness in Human-Robot Inter-
action. 19th IEEE International Symposium.

R. Dale and E. Reiter. 1995. Computational Interpre-
tations of the Gricean Maxims in the Generation of
Referring Expressions. Cognitive Science, 18(1):233–
263.

P. Rybski, K. Yoon, J. Stolarz, and M. Veloso. 2007
Interactive robot task training through dialog and
demonstration HRI, 49–56

T. Kollar and S. Tellex and D. Roy and N. Roy 2010
Toward Understanding Natural Language Directions
HRI, 259-266

W. Wahlster and A. Kobsa. 1989. User Models in Dia-
log Systems. User Models in Dialog Systems, 4–34.
Springer-Verlag, Berlin.

247


