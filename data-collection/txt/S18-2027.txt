



















































A Multimodal Translation-Based Approach for Knowledge Graph Representation Learning


Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 225‚Äì234
New Orleans, June 5-6, 2018. c¬©2018 Association for Computational Linguistics

A Multimodal Translation-Based Approach for
Knowledge Graph Representation Learning

Hatem Mousselly-Sergieh‚Ä†, Teresa Botschen¬ß‚Ä†, Iryna Gurevych¬ß‚Ä†, Stefan Roth¬ß‚Ä°
‚Ä† Ubiquitous Knowledge Processing (UKP) Lab

‚Ä° Visual Inference Lab
¬ß Research Training Group AIPHES

Department of Computer Science, Technische UniversitaÃàt Darmstadt
h.m.sergieh@gmail.com

{botschen@aiphes, gurevych@ukp.informatik, stefan.roth@visinf}.tu-darmstadt.de

Abstract

Current methods for knowledge graph (KG)
representation learning focus solely on the
structure of the KG and do not exploit any
kind of external information, such as visual
and linguistic information corresponding to
the KG entities. In this paper, we propose
a multimodal translation-based approach that
defines the energy of a KG triple as the sum of
sub-energy functions that leverage both mul-
timodal (visual and linguistic) and structural
KG representations. Next, a ranking-based
loss is minimized using a simple neural net-
work architecture. Moreover, we introduce
a new large-scale dataset for multimodal KG
representation learning. We compared the per-
formance of our approach to other baselines on
two standard tasks, namely knowledge graph
completion and triple classification, using our
as well as the WN9-IMG dataset.1 The results
demonstrate that our approach outperforms all
baselines on both tasks and datasets.

1 Introduction

Knowledge Graphs (KGs), e.g., Freebase (Bol-
lacker et al., 2008) and DBPedia (Auer et al.,
2007), are stores of relational facts, which are
crucial for various kinds of tasks, such as ques-
tion answering and information retrieval. KGs are
structured as triples of head and tail entities along
with the relation that holds between them. Factual
knowledge is virtually infinite and is frequently
subject to change. This raises the question of the
incompleteness of the KGs. To address this prob-
lem, several methods have been proposed for au-
tomatic KG completion (KGC, for a survey refer
to Wang et al., 2017). In recent years, translation-
based approaches have witnessed a great success.
Their main idea is to model the entities and their

1Code and datasets are released for research
purposes: https://github.com/UKPLab/
starsem18-multimodalKB

relation as low-dimensional vector representations
(embeddings), which in turn can be used to per-
form different kinds of inferences on the KG.
These include identifying new facts or validating
existing ones. However, translation-based meth-
ods rely on the rich structure of the KG and gener-
ally ignore any type of external information about
the included entities.

In this paper, we propose a translation-based ap-
proach for KG representation learning that lever-
ages two different types of external, multimodal
representations: linguistic representations created
by analyzing the usage patterns of KG entities in
text corpora, and visual representations obtained
from images corresponding to the KG entities. To
gain initial insights into the potential benefits of
external information for the KGC task, let us con-
sider the embeddings produced by the translation-
based TransE method (Bordes et al., 2013) on the
WN9-IMG dataset (Xie et al., 2017). This dataset
contains a subset of WordNet synsets, which are
linked according to a predefined set of linguis-
tic relations, e.g. hypernym. We observed that
TransE fails to create suitable representations for
entities that appear frequently as the head/tail of
one-to-many/many-to-one relations. For example,
the entity person appears frequently in the dataset

Embedding Space Top Similar Synsets

Linguistic
n02472987 world, n02473307 Homo erectus,
n02474777 Homo sapiens, 02472293 homo,
n00004475 organism, n10289039 man

Visual
n10788852 woman, n09765278 actor,
n10495167 pursuer n10362319 nonsmoker,
n10502046 quitter, n09636339 Black

Structure (TransE)

hypernym, n00004475 organism,
n03183080 device, n07942152 people,
n13104059 tree, n00015388 animal,
n12205694 herb, n07707451 vegetable

Table 1: Closest synsets to the person synset
(n00007846) according to different embedding spaces.

225



as a head/tail of the hyponym/hypernym relation;
the same holds for entities like animal or tree.
TransE represents such entities as points that are
very close to each other in the embedding space
(cf. Tab. 1). Furthermore, the entity embeddings
tend to be very similar to the embeddings of rela-
tions in which they frequently participate. Conse-
quently, such a representation suffers from limited
discriminativeness and can be considered a main
source of error for different KG inference tasks.

To understand how multimodal representations
may help to overcome this issue, we performed the
same analysis by considering two types of external
information: linguistic and visual. The linguistic
representations are created using word embedding
techniques (Mikolov et al., 2013), and the visual
ones, called visual embeddings, are obtained from
the feature layers of deep networks for image clas-
sification (e.g., Chatfield et al., 2014) on images
that correspond to the entities of the dataset. For
the same category of entities discussed above, we
observed that both the visual and the linguistic em-
beddings are much more robust than the structure-
based embeddings of TransE. For instance, per-
son is closer to other semantically related con-
cepts, such as Homo erectus in the linguistic em-
bedding space, and to concepts with common vi-
sual characteristics (e.g., woman, actor) in the vi-
sual embedding space (cf. Tab. 1). Furthermore,
the linguistic and the visual embeddings seem to
complement each other and hence are expected to
enhance KG representations if they can be lever-
aged during the representation learning process.

The contributions of this paper can be summa-
rized as follows: (1) We propose an approach for
KG representation learning that incorporates mul-
timodal (visual and linguistic) information in a
translation-based framework and extends the def-
inition of triple energy to consider the new mul-
timodal representations; (2) we investigate dif-
ferent methods for combining multimodal rep-
resentations and evaluate their performance; (3)
we introduce a new large-scale dataset for multi-
modal KGC based on Freebase; (4) we experimen-
tally demonstrate that our approach outperforms
baseline approaches including the state-of-the-art
method of Xie et al. (2017) on the link prediction
and triple classification tasks.

2 Related Work

2.1 Translation Models
TransE (Bordes et al., 2013) is among the earli-
est translation-based approaches for KG represen-
tation learning. TransE represents entities and re-
lations as vectors in the same space, where the re-
lation is considered a translation operation from
the representation of the head to that of the tail
entity. For a correct triple, TransE assumes that
h + r ‚âà t, where h, r, t are the vector represen-
tations of the head, relation, and tail, respectively.
Additionally, TransE use a dissimilarity measure d
to define the energy of a given triple as d(h+r, t).
Finally, the representations of KG entities and re-
lations are learned by minimizing a margin-based
ranking objective that aims to score positive triples
higher than negative triples based on their energies
and a predefined margin.

TransE is a simple and effective method, how-
ever, the simple translational assumption con-
strains the performance when dealing with com-
plex relations, such as one-to-many or many-to-
one. To address this limitation, some exten-
sions of TransE have been proposed. Wang et al.
(2014) introduced TransH, which uses translations
on relation-specific hyperplanes and applies ad-
vanced methods for sampling negative triples. Lin
et al. (2015b) proposed TransR, which uses sep-
arate spaces for modeling entities and relations.
Entities are projected from their space to the cor-
responding relation space by relation-specific ma-
trices. Moreover, they propose an extension called
CTransR, in which instances of pairs of head and
tail for a specific relation are clustered such that
the members of the clusters exhibit similar mean-
ings of this relation. Lin et al. (2015a) proposed
another extension of TransE, called PTransE, that
leverages multi-step relation path information in
the process of representation learning.

The above models rely only on the structure of
the KG, and learning better KG representations
is dependent upon the complexity of the model.
In this paper, however, we follow a different ap-
proach for improving the quality of the learned
KG representation and incorporate external mul-
timodal information in the learning process, while
keeping the model as simple as possible.

2.2 Multimodal Methods
Recent advances in natural language processing
have witnessed a greater interest in leveraging

226



multimodal information for a wide range of tasks.
For instance, Shutova et al. (2016) showed that
better metaphor identification can be achieved by
fusing linguistic and visual representations. Col-
lell et al. (2017) demonstrated the effectiveness
of combining linguistic and visual embeddings
in the context of word relatedness and similarity
tasks. Regarding KG representation learning, the
first and, to the best of our knowledge, only at-
tempt that considers multimodal data is the work
of Xie et al. (2017). Their IKRL approach extends
TransE based on visual representations extracted
from images that correspond to the KG entities. In
IKRL, the energy of a triple is defined in terms of
the structure of the KG as well as the visual repre-
sentation of the entities. Our work, while building
upon the foundations of Xie et al. (2017), sets it-
self apart based on the following properties: (1) in
addition to images, our model integrates another
kind of external representation, namely linguistic
embeddings for KG entities ‚Äì thus, adding multi-
modal information; (2) we base our approach on
a simple and easily extensible neural network ar-
chitecture; (3) we introduce an additional energy
function that considers the multimodal represen-
tation of the KG entities; (4) we introduce a new
large-scale dataset for multimodal KG representa-
tion learning.

3 Proposed Approach

We denote the knowledge graph as G = (E ,R, T ),
where E is the set of entities, R is the set of rela-
tions, and T = {(h, r, t)|h, t ‚àà E , r ‚àà R} the
set of KG triples. For each head and tail entity
h, r ‚àà E , we define three kinds of representations
(embeddings): structural hIs, t

I
s ‚àà IRN , linguistic

hIw, t
I
w ‚àà IRM , and visual hIi , tIi ‚àà IRP , where

N , M and P are the corresponding numbers of
dimensions. Furthermore, we represent each rela-
tion r ‚àà R as a vector rIs ‚àà IRN in the space of
the structural information. The superscript I de-
notes that these are the input embeddings. Since
the different embeddings do not live in the same
space, we assume from now on that they can be
transformed into a common space using a multi-
layer network (e.g., hIs into hs, cf. Fig. 1). Fol-
lowing the translational assumption, given a triple
(h, r, t), we have

hs + rs ‚âà ts. (1)

3.1 Model

In general, previous works such as (Bordes et al.,
2013) start from Eq. (1) and build models for min-
imizing a ranking loss between positive and nega-
tive triples that are sampled from the KG. Conven-
tionally, negative triples are sampled by corrupting
the head, the tail, or the relation of correct triples.
We follow this idea and make it explicit by taking
two different ‚Äúviews‚Äù on the translational assump-
tion. Apart from the first view through Eq. (1), we
can also rewrite the translational assumption as

ts ‚àí rs ‚âà hs. (2)

We will learn the two views jointly. For each view,
we sample specific kinds of negative triples ac-
cording to which part of the triple has to be pre-
dicted. For the head-centric view, we define T ‚Ä≤tail,
a set of negative triples that is sampled by cor-
rupting the tail of gold triples. Similarly, for the
tail-centric view, we define T ‚Ä≤head, a set of negative
triples sampled by corrupting the head of the gold
triples:

T ‚Ä≤tail = {(h, r, t‚Ä≤)|h, t‚Ä≤‚ààE ‚àß (h, r, t‚Ä≤) /‚ààT } (3a)
T ‚Ä≤head = {(h‚Ä≤, r, t)|h‚Ä≤, t‚ààE ‚àß (h‚Ä≤, r, t) /‚ààT }. (3b)

Next, we extend the definition of triple energy in
order to integrate both the structural and the multi-
modal representations of the KG entities. For each
kind of representation as well as their combina-
tion, we define a specific energy function. Subse-
quently, the final energy of a triple is defined as the
sum of the individual energies defined below.

Structural Energy: The structure-based energy
of a triple is defined in terms of the structure of the
KG as proposed by the TransE approach (Bordes
et al., 2013). Accordingly, we define

ES = ‚Äñhs + rs ‚àí ts‚Äñ. (4)

Multimodal Energies: The multimodal repre-
sentation of a KG entity is defined by combin-
ing the corresponding linguistic and visual repre-
sentations. Let ‚äï denote the combination opera-
tor (more details in Section 3.2). Now, we define
the multimodal representations hm and tm of the
head and the tail entities, respectively, as

hm = hw ‚äï hi (5a)
tm = tw ‚äï ti. (5b)

227



 

 

 

rs 

hs 

ts 

hm 

tm 

shared weights 

shared weights 

Dense layers 
Dense layers 

Triple energy Input 
Input 

+ 

+ 

+ 

+ 

ES 

 
||‚ÑéùëÜ + ùëüùë† ‚àí ùë°ùë†|| 

EMS 
||‚Ñéùëö  +  ùëüùë† ‚àí  ùë°ùë†|| 

ESM 
||‚Ñéùë† + ùëüùë† ‚àí ùë°ùëö|| 

EM1 

 
||‚Ñéùëö + ùëüùë† ‚àí ùë°ùëö|| 

EM2 

 
||(‚Ñéùëö + ‚Ñéùë†) + ùëüùë† ‚àí (ùë°ùëö + ùë°ùë†)|| 

hs

rs

ts

hm

tm

structural ‚Ä®
embeddings multimodal‚Ä®

embeddings

fully ‚Ä®
connected fully ‚Ä®

connected

hs

rs

ts

I

I

I

hm

tm

I

I

Figure 1: Overview of the neural network architecture for calculating the total triple energy from the different
models. The fully connected networks transform the respective input embeddings into a common space.

Next, we transfer the structure-based energy func-
tion from Eq. (4) to the multimodal case where it
incorporates the multimodal representations under
the translational assumption, i.e.

EM1 = ‚Äñhm + rs ‚àí tm‚Äñ. (6)

We then extend the previous energy from Eq. (6)
to define another energy function that considers
the structural embeddings in addition to the multi-
modal ones as follows:

EM2 = ‚Äñ(hm + hs) + rs ‚àí (tm + ts)‚Äñ. (7)

These presented multimodal energies can be un-
derstood as additional constraints for the trans-
lation model. M1 states that the relation corre-
sponds to a translation operation between the mul-
timodal representation of the head and the tail en-
tities once projected into the structural space. M2
enforces that same constraint, however, on the sum
of the multimodal and the structural embeddings
of the head and the tail entities. While Eqs. (4),
(6), and (7) cannot be fulfilled at the same time, we
found that combining these complementary ener-
gies makes the results more robust.

Structural-Multimodal Energies: Next, to en-
sure that the structural and the multimodal repre-
sentations are learned in the same space, we follow
the proposal of Xie et al. (2017) and define the fol-
lowing energy functions:

ESM = ‚Äñhs + rs ‚àí tm‚Äñ (8a)
EMS = ‚Äñhm + rs ‚àí ts‚Äñ. (8b)

Finally, the overall energy of a triple for the
head and the tail views are defined as

E(h, r, t) = ES + EM1 + EM2

+ESM + EMS . (9a)

Objective Function: For both the head and the
tail view, we aim to minimize a margin-based
ranking loss between the energies of the positive
and the negative triples. The corresponding loss
functions are finally defined as

Lhead =
‚àë

(h,r,t)‚ààT

‚àë

(h,r,t‚Ä≤)‚ààT ‚Ä≤tail

max
(
Œ≥ + E(h, r, t)

‚àíE(h, r, t‚Ä≤), 0
)

(10)

Ltail =
‚àë

(h,r,t)‚ààT

‚àë

(h‚Ä≤,r,t)‚ààT ‚Ä≤head

max
(
Œ≥ + E(t,‚àír, h)

‚àíE(t,‚àír, h‚Ä≤), 0
)
. (11)

Here, Œ≥ is a margin parameter, which controls the
amount of energy difference between the positive
and the negative triples. Finally, we aim to mini-
mize the global loss

L = Lhead + Ltail. (12)

To bring the different representations (structural,
linguistic, visual) into the same space, we em-
ploy a simple feed-forward neural network archi-
tecture. The input of the network consists of the
structural and the multimodal embeddings of the
heads, the tails, and the relations (Fig. 1); the fully-
connected layers map these inputs into a common
space. Furthermore, we share the weights between

228



 

ùíÜùíä 

Mapping function 

ùíÜùíé 

ùíÜùíò 

Figure 2: The DeViSE method (Frome et al., 2013).

those fully-connected layers that receive the same
kind of input. Additionally, the weights are also
shared across the head and the tail views.

3.2 Combining Multimodal Representations
To complete the description of our approach, we
still need to define the ‚äï operator used in Eq. (5)
to combine the linguistic and visual embeddings
into a single one. To that end, we identified three
methods for multimodal representation learning
and adapted them to KG entities.

Concatenation Method: The simplest method
to create multimodal representations for KG enti-
ties is to combine the multimodal embedding vec-
tors by concatenation. Given the linguistic ew and
the visual ei embeddings of an entity e, we de-
fine the multimodal representation em = ew_ei,
where _ is the concatenation operator.

DeViSE Method: Next, we consider the deep
visual-semantic embedding model (DeViSE) of
Frome et al. (2013), which leverages textual data
to explicitly map images into a rich semantic em-
bedding space. Given the visual representation
of some concept, the goal is to learn a mapping
into the linguistic (word) embedding space. The
mapped representation can then be used as a mul-
timodal representation for the target entity. Fig. 2
illustrates the application of DeViSE to generating
multimodal representations for KG entities.

Imagined Method: Finally, we consider the
Imagined method of Collell et al. (2017) for creat-
ing multimodal representations of concepts based
on their linguistic and visual embeddings. Imag-
ined is similar to DeViSE, however, it applies the
reverse procedure. That is, for a given concept
Imagined aims to learn a mapping from the lin-
guistic embedding space of that concept into the

Dataset #Rel #Ent #Train #Valid #Test

WN9-IMG 9 6555 11 741 1337 1319
FB-IMG 1231 11 757 285 850 29 580 34 863

Table 2: Datasets statistics

 

ùíÜùíò 

Mapping function 

ùíÜùíò ùíÜùíòùíä 

ùíÜùíé =  ùíÜùíòùíä    ùíÜùíò  

 

ùíÜùíä 

Figure 3: The Imagined method (Collell et al., 2017).

visual embedding space. The mapping can be for-
mulated as a linear or nonlinear transformation us-
ing a simple neural network, and the objective is
to minimize the distance between the mapped lin-
guistic representation and the visual representation
of the entities. Subsequently, a multimodal repre-
sentation is created by applying the learned map-
ping function on the linguistic representation of
the entity and then concatenating the resulting vec-
tor with the original linguistic embedding (Fig. 3).

4 Experiments

4.1 Datasets

WN9-IMG: This dataset provided by Xie et al.
(2017) is based on WordNet. It contains a col-
lection of triples, where the entities correspond to
word senses (synsets) and the relations define the
lexical relationships between the entities. Further-
more, for each synset a collection of up to ten im-
ages obtained from ImageNet (Deng et al., 2009)
is provided.

FB-IMG: To demonstrate the scalability of our
approach to larger datasets, we created another
dataset based on FB15K (Bordes et al., 2013),
which consists of triples extracted from Freebase.
For each entity, we crawled 100 images from the
web using text search based on the entity labels.
To ensure that the crawled images are representa-
tive of the corresponding entities, we applied an
approach for image filtering based on the Page-
Rank algorithm (Page et al., 1999). First, we
created a vector representation (embedding) for
each image by feeding it into a pre-trained VGG19
neural network for image classification (Simonyan
and Zisserman, 2014). The image embeddings
consist of the 4096-dimensional activation of the
last layer (before the softmax). Next, for each
entity we create a similarity graph for the corre-
sponding images based on the cosine similarity
between their embedding vectors. Finally, we cal-
culated the PageRank score for each image in the

229



graph and kept the top 10 results. Tab. 2 gives ba-
sic statistics of the two datasets.

4.2 Representations

We now discuss the procedure we followed to ob-
tain different kinds of representations for the enti-
ties and relations of the two evaluation datasets.

Structural Representation: This baseline rep-
resentation is created based on the structure of the
KG only, without any external information. In our
experiments we created structure representations
for the entity and the relations of the two datasets
using the TransE algorithm. For both datasets, we
trained TransE with 100 dimensions and used the
same values for the other hyperparameters as rec-
ommended by Bordes et al. (2013).

Linguistic Representation: The linguistic rep-
resentations of the entities are obtained by ap-
plying word embedding techniques. For the FB-
IMG dataset, we used a pre-trained word embed-
ding model for Freebase entities as provided by
the word2vec framework (Mikolov et al., 2013).
The provided embeddings are 1000 dimensional
and are trained using the skipgram model over the
Google 100B token news dataset. We applied L2-
normalization on the generated embeddings.

The entities of the WN9-IMG dataset corre-
spond to word senses rather than to individ-
ual words. In order to create embeddings for
the synsets, we used the AutoExtend framework
(Rothe and SchuÃàtze, 2015), which enables cre-
ating sense embeddings for a given sense based
on the embeddings of the contained lemmas. For
this purpose, we initialized AutoExtend with pre-
trained 300-dimensional GloVe embeddings (Pen-
nington et al., 2014). In case where no pre-trained
embeddings are found for the sense lemmas, Au-
toExtend generates zero initialized vectors for the
corresponding synsets. In order to provide bet-
ter representations, we define the embeddings of
such synsets by copying the embeddings of the
first hyperonym synset that has non-zero AutoEx-
tend embeddings. The linguistic embeddings of
WN9-IMG entities (synsets) are 300-dimensional
vectors, which were also L2-normalized.

Visual Representation: For each image of a
given KG entity, we created a visual embedding
vector using the same procedure as for creat-
ing the FB-IMG dataset. This was done using
a pre-trained VGG model (Simonyan and Zis-

serman, 2014). For the WN9-IMG dataset, we
used the VGG19 model and extracted the 4096-
dimensional vector of the last fully-connected
layer before the softmax. For the FB-IMG
dataset, which contains much more data than
WN9-IMG and in order to speed up the train-
ing, we used the more compact VGG-m-128
CNN model (Chatfield et al., 2014), which pro-
duces 128-dimensional embedding vector for each
image. Next, the visual embeddings are L2-
normalized. We investigated two ways of combin-
ing the embedding vectors corresponding to im-
ages of a given entity. The first method defines
the visual embedding of an entity as the average
of the embeddings of all corresponding images.
The second method uses the dimension-wise max-
imum. In our experiments we observed that av-
eraging the embedding vectors outperforms the
maximum method. Hence, we only report the re-
sults obtained with averaging.

4.3 Experimental Setup

We investigated different sets of hyperparameters
for training the model. The best results were ob-
tained using the Adam optimizer (Kingma and
Ba, 2014) with a fixed learning rate of 0.001 and
batch size of 100. We used the hyperbolic tan-
gent function (tanh) for the activation and one
fully-connected layer of 100 hidden units. We ob-
served that regularization has a minor effect. In
the case of WN9-IMG, we used dropout regular-
ization (Srivastava et al., 2014) with a dropout ra-
tio of 10%; we applied no regularization on the
FB-IMG dataset. Regarding the margin of the loss
function, we experimented with several values for
both datasets Œ≥ ‚àà {4, 6, 8, 10, 12}. The best re-
sults for both datasets were obtained with Œ≥ = 10.

We investigated different configurations of our
approach: (1) Ling considers the linguistic em-
beddings only, (2) Vis considers the visual em-
beddings only, (3) multimodal where the visual
and the linguistic embeddings are considered ac-
cording to the presented multimodal combination
methods: DeViSE, Imagined, and the Concatena-
tion methods (cf. Sec. 3.2), and (4) only head in
which we use the head view only and the concate-
nation method for combining the multimodal rep-
resentations. Here, negative samples are produced
by randomly corrupting the head, the tail, or the
relation of gold triples.

We compared our approach to other baseline

230



methods including TransE (Bordes et al., 2013)
and IKRL (Xie et al., 2017). For TransE, we set
the size of the embeddings to 100 dimensions and
followed the recommendations of Bordes et al.
(2013) regarding the other hyperparameters. We
also implemented the IKRL approach and the best
results were achieved by using margins of 8 and
4 for the WN9-IMG and the FB-IMG datasets, re-
spectively. We tested two configurations of IKRL:
(1) IKRL (Vis) uses the visual representation only
(as in the original paper) and initializes the struc-
tural representations with our learned TransE em-
beddings, and (2) IKRL (Concat), which uses the
concatenation of the linguistic and the visual em-
beddings. Please note that we do not apply the
attention mechanism for creating image represen-
tations as proposed in the IKRL paper (Xie et al.,
2017). However, we include that model, referred
to as IKRL (Paper), in the comparison.

4.4 Link Prediction

Evaluation Protocol: Given a pair of a head/tail
and a relation, the goal of link prediction is to iden-
tify the missing tail/head. For each test triple, we
replaced the head/tail by all entities in the KG and
calculated the corresponding energies in ascend-
ing order. Similar to Bordes et al. (2013), we cal-
culated two measures: (1) the mean rank (MR) of
the correctly predicted entities and (2) the propor-
tion of correct entities in the top-10 ranked ones
(Hits@10). We also distinguished between two
evaluation settings, ‚ÄúRaw‚Äù and ‚ÄúFilter‚Äù. In con-
trast to the ‚ÄúRaw‚Äù setting, in the ‚ÄúFilter‚Äù setting
correct triples included in the training, validation,
and test sets are removed before ranking.

Results: Tab. 3 shows the results on the WN9-
IMG dataset. First, we can observe that lever-
aging multimodal information leads to a signifi-
cant improvement compared to the structure-only
based approach TransE, especially in terms of the
mean rank. This conclusion is in accordance with
our intuition: although the structural representa-
tions become less discriminative after the training
for certain kinds of entities (such as the one dis-
cussed in Sec. 1), the multimodal representations
compensate for this effect, thus the prediction ac-
curacy increases. Regarding the multimodal repre-
sentations, combining the linguistic and the visual
embeddings seems to outperform models that rely
on only one kind of those representations. This
holds for our approach as well as for IKRL. Re-

Method MR Hits@10 (%)

Raw Filter Raw Filter

TransE 160 152 78.77 91.21
IKRL (Paper) 28 21 80.90 93.80
IKRL (Vis) 21 15 81.39 92.00
IKRL (Concat) 18 12 82.26 93.25
Our (Ling) 19 13 80.78 90.79
Our (Vis) 20 14 80.74 92.30
Our (DeViSE) 19 13 81.80 93.21
Our (Imagined) 19 14 81.43 91.09
Our (Concat) 14 9 83.78 94.84
Our (only head) 19 13 82.37 93.21

Table 3: Link prediction results on WN9-IMG.

garding the multimodal combination method, we
surprisingly noticed that the simple concatenation
method outperforms other advanced methods like
DeViSE (Frome et al., 2013) and Imagined (Col-
lell et al., 2017). This suggests that translation-
based approaches for KG representation learning
profit more from the raw representations than gen-
eral purpose pre-combined ones, which are not
necessarily tuned for this task.

The evaluation also shows that our approach
with the concatenation method outperforms the
best IKRL model, IKRL (Concat), which was
trained on the same representations as our ap-
proach. Additionally, our model outperforms the
best performing IKRL model reported in (Xie
et al., 2017) with less than half the MR and more
than one point in Hits@10. This shows the benefit
of our additional energy term coupling structural
and multimodal embeddings. To assess the benefit
of taking two separate views on the translational
assumption, we evaluated the performance of us-
ing the head view only. We observe a considerable
drop in performance. The MR becomes 5 points
higher and the Hits@10 drops by more than one
percentage point compared to the same model that
is trained using both the head and the tail views.

Compared to WN9-IMG, the FB-IMG dataset
has a much larger number of relations, entities,
and triples (cf. Tab. 2), thus it better resembles
the characteristics of real KG. On the FB-IMG
dataset, the superiority of our model compared
to the baselines, especially IKRL, becomes even
more evident (cf. Tab. 4). Our model performs
best and achieves a significant boost in MR and
Hits@10 compared to the baselines, while IKRL
slightly outperforms TransE in terms of MR only.

231



Method MR Hits@10 (%)

Raw Filter Raw Filter

TransE 205 121 37.83 49.39
IKRL (Concat) 179 104 37.48 47.87
Our (Concat) 134 53 47.19 64.50

Table 4: Link prediction results on FB-IMG.

Therefore, the results confirm the robustness of
our method for large-scale datasets.

Finally, we observe that, in general, the perfor-
mance boost on the FB-IMG dataset is lower than
in the case of the WN9-IMG dataset. This can
be explained by the higher scale and complexity
of the FB-IMG dataset. Furthermore, the visual
representations of the FB-IMG entities are based
on images that are automatically crawled from the
Web. Accordingly, some of the crawled images
may not be representative enough or even noisy,
while the images in WN9-IMG have better quality
since they are obtained from ImageNet, which is a
manually created dataset.

4.5 Triple Classification

Evaluation Protocol: Triple classification is a
binary classification task, in which the KG triples
are classified as correct or not according to a given
dissimilarity measure (Socher et al., 2013). For
this purpose a threshold for each relation Œ¥r is
learned. Accordingly, a triple (h, r, t) is consid-
ered correct if its energy is less than Œ¥r, and incor-
rect otherwise. Since the dataset did not contain
negative triples, we followed the procedure pro-
posed by Socher et al. (2013) to sample negative
triples for both the validation and the test sets. As
a dissimilarity measure, we used the total energy
of the triple and determined the relation threshold
using the validation set and then calculated the ac-
curacy on the test set.

Results: We measured the triple classification
accuracy of our approach using ten test runs. In
each run, we sampled new negative triples for both
the validation and the test sets. We report the max-
imum, the minimum, the average, and the standard
deviation of the triple classification accuracy.

For WN9-IMG, the results (cf. Tab. 5) show that
our approach outperforms the baselines with up to
two points in maximum accuracy and around three
points in average accuracy. Please note that a di-
rect comparison with IKRL (Paper) is not possi-

Method Accuracy(%)

max min avg ¬± std
TransE 95.38 89.67 93.35¬± 1.54
IKRL (Paper) 96.90 ‚Äì ‚Äì
IKRL (Vis) 95.16 88.75 92.57¬± 1.78
IKRL (Concat) 95.40 91.77 93.56¬± 1.03
Our (Concat) 97.16 94.93 96.10 ¬± 0.87
Our (only head) 95.58 91.78 93.14¬± 1.09

Table 5: Triple classification results on WN9-IMG.

Method Accuracy(%)

max min avg ¬± std
TransE 67.13 66.47 66.81¬± 0.21
IKRL (Concat) 66.68 66.03 66.34¬± 0.20
Our (Concat) 69.04 68.16 68.62 ¬± 0.25

Table 6: Triple classification results on FB-IMG.

ble since we do not have access to the same set of
negative samples. Still, the maximum classifica-
tion accuracy of our approach is higher than that of
by IKRL (Paper). Finally, the results confirm that
using separate head and tail views leads to better
results than using the head view only.

Regarding the FB-IMG dataset, the results in
Tab. 6 emphasize the advantage of our approach.
Compared to the multimodal approach IKRL,
which fails to outperform TransE, our model em-
ploys multimodal information more effectively
and leads to more than one point improvement in
average accuracy compared to TransE.

In conclusion, the conducted evaluation demon-
strates the robustness of our approach on both
evaluation tasks and on different evaluation
datasets.

5 Conclusion

In this paper, we presented an approach for KG
representation learning that leverages multimodal
data about the KG entities including linguistic as
well as visual representations. The proposed ap-
proach confirms the advantage of multimodal data
for learning KG representations. In future work,
we will investigate the effect of multimodal data
in the context of advanced translation methods and
conduct further research on combining visual and
linguistic features for KGs.

232



Acknowledgments

The first author was supported by the German Fed-
eral Ministry of Education and Research (BMBF)
under the promotional reference 01UG1416B
(CEDIFOR). Other than this, the work has been
supported by the DFG-funded research training
group ‚ÄúAdaptive Preparation of Information form
Heterogeneous Sources‚Äù (AIPHES, GRK 1994/1).
Finally, we gratefully acknowledge the support
of NVIDIA Corporation with the donation of the
Tesla K40 GPU used for this research.

References
SoÃàren Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A nucleus for a web of open data.
In The Semantic Web, pages 722‚Äì735. Springer.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the Interna-
tional Conference on Management of Data (ACM
SIGMOD), pages 1247‚Äì1250.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
DuraÃÅn, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of the 26th Interna-
tional Conference on Advances in Neural Informa-
tion Processing Systems (NIPS), pages 2787‚Äì2795.

Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. 2014. Return of the devil in the
details: Delving deep into convolutional nets. In
Proceedings of the British Machine Vision Confer-
ence (BMVC).

Guillem Collell, Ted Zhang, and Marie-Francine
Moens. 2017. Imagined visual representations as
multimodal embeddings. In Proceedings of the 31st
Conference on Artificial Intelligence (AAAI), pages
4378‚Äì4384.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 248‚Äì255.

Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. De-
ViSE: A deep visual-semantic embedding model. In
Proceedings of the 26th International Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 2121‚Äì2129.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Yankai Lin, Zhiyuan Liu, Huan-Bo Luan, Maosong
Sun, Siwei Rao, and Song Liu. 2015a. Modeling
relation paths for representation learning of knowl-
edge bases. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 705‚Äì714.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015b. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of the 29th Conference on Artificial Intelli-
gence (AAAI).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532‚Äì1543.

Sascha Rothe and Hinrich SchuÃàtze. 2015. AutoEx-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1793‚Äì1803.

Ekaterina Shutova, Douwe Kiela, and Jean Maillard.
2016. Black holes and white rabbits: Metaphor
identification with visual features. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT), pages
160‚Äì170.

Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Proceedings of the 26th International Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 926‚Äì934.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research (JMLR), 15(1):1929‚Äì1958.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions
on Knowledge and Data Engineering, 29(12):2724‚Äì
2743.

233



Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the 28th
Conference on Artificial Intelligence (AAAI), pages
1112‚Äì1119.

Ruobing Xie, Zhiyuan Liu, Huanbo Luan, and
Maosong Sun. 2017. Image-embodied knowledge
representation learning. In Proceedings of the 26th
International Joint Conference on Artificial Intelli-
gence (IJCAI), pages 3140‚Äì3146.

234


