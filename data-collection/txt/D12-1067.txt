










































Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Parse, Price and Cut—Delayed Column and Row Generation for Graph
Based Parsers

Sebastian Riedel David Smith Andrew McCallum
Department of Computer Science

University of Massachusetts, Amherst
{riedel,dasmith,mccallum}@cs.umass.edu

Abstract

Graph-based dependency parsers suffer from
the sheer number of higher order edges they
need to (a) score and (b) consider during opti-
mization. Here we show that when working
with LP relaxations, large fractions of these
edges can be pruned before they are fully
scored—without any loss of optimality guar-
antees and, hence, accuracy. This is achieved
by iteratively parsing with a subset of higher-
order edges, adding higher-order edges that
may improve the score of the current solu-
tion, and adding higher-order edges that are
implied by the current best first order edges.
This amounts to delayed column and row gen-
eration in the LP relaxation and is guaranteed
to provide the optimal LP solution. For second
order grandparent models, our method consid-
ers, or scores, no more than 6–13% of the sec-
ond order edges of the full model. This yields
up to an eightfold parsing speedup, while pro-
viding the same empirical accuracy and cer-
tificates of optimality as working with the full
LP relaxation. We also provide a tighter LP
formulation for grandparent models that leads
to a smaller integrality gap and higher speed.

1 Introduction

Many problems in NLP, and structured prediction in
general, can be cast as finding high-scoring struc-
tures based on a large set of candidate parts. For
example, in second order graph-based dependency
parsing (Kübler et al., 2009) we have to choose a
quadratic number of first order and a cubic number
of second order edges such that the graph is both
high-scoring and a tree. In coreference, we have
to select high-scoring clusters of mentions from an

exponential number of candidate clusters, such that
each mention is in exactly one cluster (Culotta et
al., 2007). In segmentation of citation strings, we
need to consider a quadratic number of possible seg-
ments such that every token is part of exactly one
segment (Poon and Domingos, 2007).

What makes such problems challenging is the
large number of possible parts to consider. This
number not only affects the cost of search or opti-
mization but also slows down the process of scor-
ing parts before they enter the optimization prob-
lem. For example, the cubic grandparent edges in
second-order dependency parsing slow down dy-
namic programs (McDonald and Pereira, 2006), be-
lief propagation (Smith and Eisner, 2008) and LP
solvers (Martins et al., 2009), since there are more
value functions to evaluate, more messages to pass,
or more variables to consider. But to even calculate
the score for each part we need a cubic number of
operations that usually involve expensive feature ex-
traction. This step often becomes a major bottleneck
in parsing, and structured prediction in general.

Candidate parts can often be heuristically pruned.
In the case of dependency parsing, previous work
has used coarse-to-fine strategies where simpler first
order models are used to prune unlikely first or-
der edges, and hence all corresponding higher or-
der edges (Koo and Collins, 2010; Martins et al.,
2009; Riedel and Clarke, 2006). While such meth-
ods can be effective, they are more convoluted, often
require training of addition models as well as tuning
of thresholding hyper-parameters, and usually pro-
vide no guarantees of optimality.

We present an approach that can solve problems
with large sets of candidate parts without consider-
ing all of these parts in either optimization or scor-

732



ing. And in contrast to most pruning heuristics, our
algorithm can give certificates of optimality before
having optimized over, or even scored, all parts. It
does so without the need of auxiliary models or tun-
ing of threshold parameters. This is achieved by a
delayed column and row generation algorithm that
iteratively solves an LP relaxation over a small sub-
set of current candidate parts, and then finds new
candidates that score highly and can be inserted into
the current optimal solution without removing high
scoring existing structure. The latter step subtracts
from the cost of a part the price of resources the part
requires, and is often referred as pricing. Sometimes
parts may score highly after pricing, but are neces-
sary in order to make the current solution feasible.
We add such parts in a step that roughly amounts to
violated cuts to the LP.

We illustrate our approach in terms of a second-
order grandparent model for dependency parsing.
We solve these models by iteratively parsing, pric-
ing, and cutting. To this end we use a variant of the
LP relaxation formulated by Martins et al. (2009).
Our variant of this LP is designed to be amenable to
column generation. It also turns out to be a tighter
outer bound that leads to fewer fractional solutions
and faster runtimes. To find high scoring grandpar-
ent edges without explicitly enumerating all of them,
we prune out a large fraction using factorized upper
bounds on grandparent scores.

Our parse, price and cut algorithm is evaluated
using a non-projective grandparent model on three
languages. Compared to a brute force approach of
solving the full LP, we only score about 10% of the
grandparent edges, consider only 8% in optimiza-
tion, and so observe an increase in parsing speed of
up to 750%. This is possible without loss of opti-
mality, and hence accuracy. We also find that our
extended LP formulation leads to a 15% reduction
of fractional solutions, up to 12 times higher speed,
and generally higher accuracy when compared to the
grandparent formulation of Martins et al. (2009).

2 Graph-Based Dependency Parsing

Dependency trees are representations of the syntac-
tic structure of a sentence (Nivre et al., 2007). They
determine, for each token of a sentence, the syntac-
tic head the token is modifying. As a lightweight al-

ternative to phrase-based constituency trees, depen-
dency representations have by now seen widespread
use in the community in various domains such as
question answering, machine translation, and infor-
mation extraction.

To simplify further exposition, we now formalize
the task, and mostly follow the notation of Martins et
al. (2009). Consider a sentence x = 〈t0, t1, . . . , tn〉
where t1, . . . , tn correspond to the n tokens of the
sentence, and t0 is an artificial root token. Let
V , {0, . . . , n} be a set of vertices corresponding
to the tokens in x, and C ⊆ V ×V a set of candidate
directed edges. Then a directed graph y ⊆ C is a
legal dependency parse if and only if it is a tree over
V rooted at vertex 0. Given a sentence x, we use Y
to denote the set of its legal parses. Note that all of
the above definitions depend on x, but for simplicity
we omit this dependency in our notation.

2.1 Arc-Factored Models
Graph-based models define parametrized scoring
functions that are trained to discriminate between
correct and incorrect parse trees. So called arc-
factored or first order models are the most basic
variant of such functions: they assess the quality of a
tree by scoring each edge in isolation (McDonald et
al., 2005b; McDonald et al., 2005a). Formally, arc-
factored models are scoring functions of the form

s (y;x,w) =
∑

〈h,m〉∈y

s〈h,m〉 (x,w) (1)

where w is a weight vector and s〈h,m〉 (x,w) scores
the edge 〈h, m〉 with respect to sentence x and
weights w. From here on we will omit both x and w
from our notation if they are clear from the context.

Given such a scoring function, parsing amounts to
solving:

maximize
y

∑
〈h,m〉∈y

s〈h,m〉

subject to y ∈ Y.
(2)

2.2 Higher Order Models
Arc-factored models cannot capture higher order de-
pendencies between two or more edges. Higher
order models remedy this by introducing scores
for larger configurations of edges appearing in the

733



tree (McDonald and Pereira, 2006). For example,
in grandparent models, the score of a tree also in-
cludes a score sgp〈g,p,c〉 for each grandparent-parent-
child triple 〈g, p, c〉:

s (y) =
∑

〈h,m〉∈y

s〈h,m〉 +
∑

〈g,p〉∈y,〈p,c〉∈y

s
gp
〈g,p,c〉 (3)

There are other variants of higher order models
that include, in addition to grandparent triples, pairs
of siblings (adjacent or not) or third order edges.
However, to illustrate our approach we will focus
on grandparent models and note that most of what
we present can be generalized to other higher order
models.

2.3 Feature Templates
For our later exposition the factored and
parametrized nature of the scoring functions
will be crucial. In the following we therefore
illustrate this property in more detail.

The scoring functions for arcs or higher order
edges usually decompose into a sum of feature tem-
plate scores. For example, the grandparent edge
score sgp〈g,p,c〉 is defined as

s
gp
〈g,p,c〉 ,

∑
t∈T gp

s
gp,t
〈g,p,c〉 (4)

where T gp is the set of grandparent templates, and
each template t ∈ T gp defines a scoring func-
tion sgp,t〈g,p,c〉 to assess a specific property of the
grandparent-parent-child edge 〈g, p, c〉.

The template scores again decompose. Consider-
ing grandparent scores, we get

st〈g,p,c〉 , w
>
t f

t
(
htg, h

t
p, h

t
c, d

t
g,p,c

)
(5)

where hti is an attribute of token ti, say h
101
i =

Part-of-Speech (ti). The term dtg,p,c corresponds to
a representation of the relation between tokens cor-
responding to g, p and g. For example, for template
101 it could return their relative positions to each
other:

d101g,p,c , 〈I [g > p] , I [g > c] , I [p > c]〉 . (6)

The feature function f t maps the representations
of g, p and c into a vector space. For the purposes of
our work this mapping is not important, and hence
we omit details.

2.4 Learning

The scoring functions we consider are parametrized
by a family of per-template weight vectors w =
〈wt〉t∈T . During learning we need to estimate w
such that our scoring functions learns to differenti-
ate between correct and incorrect parse trees. This
can be achieved in many ways: large margin train-
ing, maximizing conditional likelihood, or variants
in between. In this work we follow Smith and Eis-
ner (2008) and train the models with stochastic gra-
dient descent on the conditional log-likelihood of the
training data, using belief propagation in order to
calculate approximate gradients.

3 LP and ILP Formulations

Riedel and Clarke (2006) showed that dependency
parsing can be framed as Integer Linear Pro-
gram (ILP), and efficiently solved using an off-the-
shelf optimizer if a cutting plane approach is used.1

Compared to tailor made dynamic programs, such
generic solvers give the practitioner more modeling
flexibility (Martins et al., 2009), albeit at the cost
of efficiency. Likewise, compared to approximate
solvers, ILP and Linear Program (LP) formulations
can give strong guarantees of optimality. The study
of Linear LP relaxations of dependency parsing has
also lead to effective alternative methods for parsing,
such as dual decomposition (Koo et al., 2010; Rush
et al., 2010). As we see later, the capability of LP
solvers to calculate dual solutions is also crucial for
efficient and exact pruning. Note, however, that dy-
namic programs provide dual solutions as well (see
section 4.5 for more details).

3.1 Arc-Factored Models

To represent a parse y ∈ Y we first introduce an
vector of variables z , 〈za〉a where za is 1 if a ∈ y
and 0 otherwise. With this representation parsing
amounts to finding a vector z that corresponds to a
legal parse tree and that maximizes

∑
a zasa. One

way to achieve this is to search through the convex
hull of all legal incidence vectors, knowing that any
linear objectives would take on its maximum on one
of the hull’s vertices. We will use Z to denote this
convex hull of incidence vectors of legal parse trees,

1Such as the highly efficient and free-for-academic-use
Gurobi solver.

734



and callZ the arborescence polytope (Martins et al.,
2009). The Minkowski-Weyl theorem tells us thatZ
can be represented as an intersection of halfspaces,
or constraints, Z = {z|Az ≤ b}. Hence optimal
dependency parsing, in theory, can be addressed us-
ing LPs.

However, it is difficult to describe Z with a com-
pact number of constraints and variables that lend
themselves to efficient optimization. In general we
therefore work with relaxations, or outer bounds, on
Z . Such outer bounds are designed to cut off all
illegal integer solutions of the problem, but still al-
low for fractional solutions. In case the optimum is
achieved at an integer vertex of the outer bound, it
is clear that we have found the optimal solution to
the original problem. In case we find a fractional
point, we need to map it onto Z (e.g., by projection
or rounding). Alternatively, we can use the outer
bound together with 0/1 constraints on z, and then
employ an ILP solver (say, branch-and-bound) to
find the true optimum. Given the NP-hardness of
ILP, this will generally be slow.

In the following we will present the outer bound
Z̄ ⊇ Z proposed by Martins et al. (2009).
Compared to the representation Riedel and Clarke
(2006), this bound has the benefit a small polyno-
mial number of constraints. Note, however, that of-
ten exponentially many constraints can be efficiently
handled if polynomial separation algorithms exists,
and that such representations can lead to tighter
outer bounds.

The constraints we employ are:

No Head For Root In a dependency tree the root
node never has a head. While this could be captured
through linear constraints, it is easier to simply re-
strict the candidate set C to never contain edges of
the form 〈·, 0〉.

Exactly One Head for Non-Roots Any non-root
token has to have exactly one head token. We can
enforce this property through the set of constraints:

m > 0 :
∑

h

z〈h,m〉 = 1. (OneHead)

No Cycles A parse tree cannot have cycles. This is
equivalent, together with the head constraints above,
to enforcing that the tree be fully connected. Mar-
tins et al. (2009) capture this connectivity constraint

using a single commodity flow formulation. This
requires the introduction of flow variables φ ,
〈φa〉a∈C . By enforcing that token 0 has n outgoing
flow, ∑

m>0

φ〈0,m〉 = n, (Source)

that any other token consumes one unit of flow,

t > 0 :
∑

h

φ〈h,t〉 −
∑
m>0

φ〈t,m〉 = 1 (Consume)

and that flow is zero on disabled arcs

φ〈h,m〉 ≤ nz〈h,m〉, (NoFlow)

connectivity can be ensured.
Assuming we have such a representation, parsing

with an LP relaxation amounts to solving

maximize
z≥0

∑
a∈A

zasa

subject to A
[

z
φ

]
≤ b.

(7)

3.2 Higher Order Models
The 1st-Order LP can be easily extended to capture
second (or higher) order models. For for the case
of grandparent models, this amounts to introduc-
ing another class of variables, zgpg,p,c, that indicate if
the parse contains both the edge 〈g, p〉 and the edge
〈p, c〉. With the help of the indicators zgp we can rep-
resent the second order objective as a linear function.
We now need an outer bound on the convex hull of
vectors 〈z, zgp〉 where z is a legal parse tree and zgp
is a consistent set of grandparent indicators. We will
refer to this convex hull as the grandparent polytope
Zgp.

We can re-use the constraints A of section 3.1 to
ensure that z is in Z . To make sure zgp is consistent
with z, Martins et al. (2009) linearize the equiva-
lence zgpg,p,c ⇔ zg,p ∧ zp,c we know to hold for legal
incidence vectors, yielding

g, p, c : z〈g,p〉 + z〈p,c〉 − z
gp
〈g,p,c〉 ≤ 1 (ArcGP)

and

g, p, c : z〈g,p〉 ≥ z
gp
〈g,p,c〉, z〈p,c〉 ≥ z

gp
〈g,p,c〉 (GPArc)

There are additional constraints we know to hold in
Zgp. First, we know that for any active edge 〈p, c〉 ∈

735



y with p > 0 there is exactly one grandparent edge
〈g, p, c〉. Likewise, for an inactive edge 〈p, c〉 /∈ y
there must be no grandparent edge 〈g, p, c〉. This
can be captured through the constraint:

p > 0, c :
∑

g

z
gp
〈g,p,c〉 = z〈p,c〉. (OneGP)

We also know that if an edge 〈g, p〉 in inactive,
there must not be any grandparent edge 〈g, p, c〉 that
goes through 〈g, p〉:

g, p :
∑

c

z
gp
〈g,p,c〉 ≤ nz〈g,p〉. (NoGP)

It can be easily shown that for integer solu-
tions the constraints ArcGP and GPArc of Martins
et al. (2009) are sufficient conditions for consis-
tency between z and zgp. It can equally be shown
that the same holds for the constraints OneGP and
NoGP. However, when working with LP relax-
ations, the two polytopes have different fractional
vertices. Hence, by combining both constraint sets,
we can get a tighter outer bound on the grandparent
polytope Zgp. In section 6 we show empirically that
this combined polytope in fact leads to fewer frac-
tional solutions. Note that when using the union of
all four types of constraints, the NoGP constraint is
implied by the constraint GPArc (left) by summing
over c on both sides, and can hence be omitted.

4 Parse, Price and Cut

We now introduce our parsing algorithm. To this
end, we first give a general description of column
and row generation for LPs; then, we illustrate how
these techniques can be applied to dependency pars-
ing.

4.1 Column and Row Generation

LPs often have too many variables and constraints
to be efficiently solved. In such cases delayed
column and row generation can substantially re-
duce runtime by lazily adding variables only when
needed (Gilmore and Gomory, 1961; Lübbecke and
Desrosiers, 2004).

To illustrate column and row generation let us
consider the following general primal LP and its cor-

responding dual problem:

Primal

maximize
z≥0

sᵀz

subject to Az ≤ b

Dual

minimize
λ≥0

λᵀb

subject to Aᵀλ ≥ s.

Say you are given a primal feasible z′ and a dual fea-
sible λ′ for which complementary slackness holds:
for all variables i we have z′i > 0⇒ si =

∑
j λ

′
jai,j

and for all constraints j we have λ′j > 0 ⇒ bj =∑
i z

′
iai,j . In this case it is easy to show that z

′ is
an optimal primal solution, λ′ and optimal dual so-
lution, and that both objectives meet at these val-
ues (Bertsekas, 1999).

The idea behind delayed column and row gener-
ation is to only consider a small subset of variables
(or columns) I and subset of constraints (or rows) J .
Optimizing over this restricted problem, either with
an off-the-shelf solver or a more specialized method,
yields the pair

(
z′I ,λ

′
J

)
of partial primal and dual

solutions. This pair is feasible and complementary
with respect to variables I and constraints J . We
can extend it to a solution (z′,y′) over all variables
and constraints by heuristically setting the remain-
ing primal and dual variables. If it so happens that
(z′,y′) is feasible and complementary for all vari-
ables and constraints, we have found the optimal so-
lution. If not, we add the constraints and variables
for which feasibility and slackness are violated, and
resolve the new partial problem.

In practice, the uninstantiated primal and dual
variables are often set to 0. In this case complemen-
tary slackness holds trivially, and we only need to
find violated primal and dual constraints. For primal
constraints,

∑
i ziai,j ≤ bi, searching for violating

constraints j is the well-known separation step in
cutting plane algorithms. For the dual constraints,∑

j λjai,j ≥ si, the same problem is referred to
as pricing. Pricing is often framed as searching for
all, or some, variables i with positive reduced cost
ri , si−

∑
j λjai,j . Note that while these problems

are, naturally, dual to each other, they can have very
different flavors. When we assess dual constraints
we need to calculate a cost si for variable i, and
usually this cost would be different for different i.
For primal constraints the corresponding right-hand-
sides are usually much more homogenous.

736



Algorithm 1 Parse, Price and Cut.
Require: Initial candidate edges and hyperedges P .
Ensure: The optimal z.

1: repeat
2: z,λ ← parse(P )
3: N ← price(λ)
4: M ← cut(z)
5: P ← P ∪N ∪M
6: until N = ∅ ∧M = ∅
7: return z

The reduced cost ri = si −
∑

j λjai,j has sev-
eral interesting interpretations. First, intuitively it
measures the score we could gain by setting zi = 1,
and subtracts an estimate of what we would loose
because zi = 1 may compete with other variables
for shared resources (constraints). Second, it cor-
responds to the coefficient of zi in the Lagrangian
L (λ, z) , sᵀz + λ [b−Az]. For any λ, Uzi=k =
maxz≥0,zi=k L (λ, z) is an upper bound on the best
possible primal objective with zi = k. This means
that ri = Uzi=1 − Uzi=0 is the difference between
an upper bound that considers zi = 1, and one that
considers zi = 0. The tighter the bound Uzi=0 is,
the closer ri is to an upper bound on the maximal
increase we can get for setting zi to 1. At conver-
gence of column generation, complementary slack-
ness guarantees that Uzi=0 is tight for all z

′
i = 0, and

hence ri is a true an upper bound.

4.2 Application to Dependency Parsing

The grandparent formulation in section 3.2 has a cu-
bic number of variables z〈g,p,c〉 as well as a cubic
number of constraints. For longer sentences this
number can slow us down in two ways. First, the
optimizer works with a large search space, and will
naturally become slower. Second, for every grand-
parent edge we need to calculate the score s〈g,p,c〉,
and this calculation can often be a major bottleneck,
in particular when using complex feature functions.
To overcome this bottleneck, our parse, price and cut
algorithm, as shown in algorithm 1, uses column and
row generation. In particular, it lazily instantiates
the grandparent edge variables zgp〈g,p,c〉, and the cor-
responding cubic number of constraints. All unin-
stantiated variables are implicitly set to 0.

The algorithm requires some initial set of vari-

ables to start with. In our case this set P contains all
first-order edges 〈h, m〉 in the candidate set C, and
for each of these one grandparent edge 〈0, h,m〉.
The primary purpose of these grandparent edges is
to ensure feasibility of the OneGP constraints.

In step 2, the algorithm parses with the current
set of candidates P by solving the corresponding LP
relaxation. The LP contains all columns and con-
straints that involve the edges and grandparent edges
of P . The solver returns both the best primal solu-
tion z (for both edges and grandparents), and a com-
plementary dual solution λ.

In step 3 the dual variables λ are used to find unin-
stantiated grandparent edges 〈g, p, c〉 with positive
reduced cost. The price routine returns such edges
in N . In step 4 the primal solution is inspected for
violations of constraint ArcGP. The cut routine per-
forms this operation, and returns M , the set of edges
〈g, p, c〉 that violate ArcGP.

In step 5 the algorithm converges if no more con-
straint violations, or promising new columns, can
be found. If there have been violations (M 6= ∅)
or promising columns (N 6= ∅), steps 2 to 4 are
repeated, with the newly found parts added to the
problem. Note that LP solvers can be efficiently
warm-started after columns and rows have been
added, and hence the cost of calls to the solver in
step 2 is substantially reduced after the first itera-
tion.

4.3 Pricing

In the pricing step we need to efficiently find a
set of grandparent edge variables zgp〈g,p,c〉 with posi-
tive reduced cost, or the empty set if no such vari-
ables exist. Let λOneGP〈p,c〉 be the dual variables for
the OneGP constraints and λNoGP〈g,p〉 the duals for con-
straints NoGP. Then for the reduced cost of zgp〈g,p,c〉
we know that:

r〈g,p,c〉 = s〈g,p,c〉 − λOneGP〈p,c〉 − λ
NoGP
〈g,p〉 . (8)

Notice that the duals for the remaining two con-
straints ArcGP and GPArc do not appear in this
equation. This is valid because we can safely set
their duals to zero without violating dual feasibility
or complementary slackness of the solution returned
by the solver.

737



4.3.1 Upper Bounds for Efficient Pricing
A naive pricing implementation would exhaus-

tively iterate over all 〈g, p, c〉 and evaluate r〈g,p,c〉
for each. In this case we can still substantially re-
duce the number of grandparent variables that en-
ter the LP, provided many of these variables have
non-positive reduced cost. However, we still need to
calculate the score s〈g,p,c〉 for each 〈g, p, c〉, an ex-
pensive operation we hope to avoid. In the follow-
ing we present an upper bound on the reduced cost,
r̄

gp
〈g,p,c〉 ≥ r

gp
〈g,p,c〉, which decomposes in a way that

allows for more efficient search. Using this bound,
we find all new grandparent edges N̄ for which this
upper bound is positive:

N̄ ←
{
〈g, p, c〉 |r̄gp〈g,p,c〉 > 0

}
. (9)

Next we prune away all but the grandparent edges
for which the exact reduced cost is positive:

N ← N̄ \ {e : rgpe > 0} . (10)

Our bound r̄gp〈g,p,c〉 on the reduced cost of 〈g, p, c〉
is based on an upper bound s̄gp〈g,p,·〉 ≥ maxc s

gp
〈g,p,c〉

on the grandparent score involving 〈g, p〉 as grand-
parent and parent, and the bound s̄gp〈·,p,c〉 ≥
maxg s

gp
〈g,p,c〉 on the grandparent score involving

〈p, c〉 as parent and child. Concretely, we have

r̄
gp
〈g,p,c〉 , min

(
s̄

gp
〈g,p,·〉, s̄

gp
〈·,p,c〉

)
− λOneGP〈p,c〉 − λ

NoGP
〈g,p〉 .

(11)
To find edges 〈g, p, c〉 for which this bound is

positive, we can filter out all edges 〈p, c〉 such that
s

gp
〈·,p,c〉− λ

OneGP
〈p,c〉 is non-positive. This is possible be-

cause NoGP is a≤ constraint and therefore λNoGP〈g,p〉 ≥
0.2 Hence r̄gp〈g,p,c〉 is at most s̄

gp
〈·,p,c〉 − λ

OneGP
〈p,c〉 . This

filtering step cuts off a substantial number of edges,
and is the main reason why can avoid scoring all
edges.

Next we filter, for each remaining 〈p, c〉, all pos-
sible grandparents g according to the definition of
r̄

gp
〈g,p,c〉. This again allows us to avoid calling the

2Notice that in section 4.1 we discussed the LP dual in
case were all constraints are inequalities. When equality con-
straints are used, the corresponding dual variables have no sign
constraints. Hence we could not make the same argument for
λOneGP〈p,c〉 .

grandparent scoring function on 〈g, p, c〉, and yields
the candidate set N̄ . Only if r̄gp〈g,p,c〉 is positive do we
have to evaluate the exact reduced cost and score.

4.3.2 Upper Bounds on Scores
What remains to be done is the calculation of up-

per bounds s̄gp〈g,p,·〉 and s̄
gp
〈·,p,c〉. Our bounds factor

into per-template bounds according to the definitions
in section 2.3. In particular, we have

s̄
gp
〈·,p,c〉 ,

∑
t∈T gp

s̄
gp,t
〈·,p,c〉 (12)

where s̄t〈·,p,c〉 is a per-template upper bound defined
as

s̄
gp,t
〈·,p,c〉 , max

v∈range(ht)
e∈range

`
dt

´ w
>
t f

t
(
v, htp, h

t
c, e

)
. (13)

That is, we maximize over all possible attribute val-
ues v any token g could have, and any possible rela-
tion e a token g can have to p and c.

Notice that these bounds can be calculated offline,
and hence amortize after deployment of the parser.

4.3.3 Tightening Duals
To price variables, we use the duals returned by

the solver. This is a valid default strategy, but may
lead to λ with overcautious reduced costs. Note,
however, that we can arbitrary alter λ to minimize
reduced costs of uninstantiated variables, as long as
we ensure that feasibility and complementary slack-
ness are maintained for the instantiated problem.

We use this flexibility for increasing λOneGP〈p,c〉 , and
hence lowering reduced costs zgp〈g,p,c〉 for all tokens c.
Assume that z〈p,c〉 = 0 and let r〈p,c〉 = λOneGP〈p,c〉 + K
be the current reduced cost for z〈p,c〉 in the instanti-
ated problem. Here K is a value depending on s〈p,c〉
and the remaining constraints z〈p,c〉 is involved in.

We know that r〈p,c〉 ≤ 0 due to dual feasibility
and hence r〈p,c〉 may be 0, but note that r〈p,c〉 < 0 in
many cases. In such cases we can increase λOneGP〈p,c〉
to −K and get r〈p,c〉 = 0. With respect to z〈p,c〉 this
maintains dual feasibility (because r〈p,c〉 ≤ 0) and
complementary slackness (because z〈p,c〉 = 0). Fur-
thermore, with respect to the zgp〈g,p,c〉 for all tokens c
this also maintains feasibility (because the increased
λOneGP〈p,c〉 appears with negative sign in 8) and com-
plementary slackness (because zgp〈g,p,c〉 = 0 due to
z〈p,c〉 = 0).

738



4.4 Separation

What happens if both z〈g,p〉 and z〈p,c〉 are active
while zgp〈g,p,c〉 is still implicitly set to 0? In this case
we violate constraint ArcGP. We could remedy this
by adding the cut z〈g,p〉 + z〈p,c〉 ≤ 1, resolve the
LP, and then use the dual variable corresponding to
this constraint to get an updated reduced cost r〈g,p,c〉.
However, in practice we found this does not happen
as often, and when it does, it is cheaper for us to add
the corresponding column r〈g,p,c〉 right away instead
of waiting to the next iteration to price it.

To find all pairs of variables for z〈g,p〉 + z〈p,c〉 ≤ 1
is violated, we first filter out all edges 〈h, m〉 for
which z〈h,m〉 = 0 as these automatically satisfy
any ArcGP constraint they appear in. Now for each
z〈g,p〉 > 0 all z〈p,c〉 > 0 are found, and if their sum
is larger than 1, the corresponding grandparent edge
〈g, p, c〉 is returned in the result set.

4.5 Column Generation in Dynamic Programs

Column and Row Generation can substantially re-
duce the runtime of an off-the-shelf LP solver, as
we will find in section 6. Perhaps somewhat sur-
prisingly, it can also be applied in the context of dy-
namic programs. It is well known that for each dy-
namic program there is an equivalent polynomial LP
formulation (Martin et al., 1990). Roughly speak-
ing, in this formulation primal variables correspond
to state transitions, and dual variables to value func-
tions (e.g., the forward scores in the Viterbi algo-
rithm).

In pilot studies we have already used DCG to
speed up (exact) Viterbi on linear chains (Belanger
et al., 2012). We believe it could be equally applied
to dynamic programs for higher order dependency
parsing.

5 Related Work

Our work is most similar in spirit to the relaxation
method presented by Riedel and Smith (2010) that
incrementally adds second order edges to a graphi-
cal model based on a gain measure—the analog of
our reduced cost. However, they always score every
higher order edge, and also provide no certificates of
optimality.

Several works in parsing, and in MAP inference
in general, perform some variant of row genera-

tion (Riedel and Clarke, 2006; Tromble and Eis-
ner, 2006; Sontag and Jaakkola, 2007; Sontag et al.,
2008). However, none of the corresponding methods
lazily add columns, too. The cutting plane method
of Riedel (2008) can omit columns, but only if their
coefficient is negative. By using the notion of re-
duced costs we can also omit columns with positive
coefficient. Niepert (2010) applies column gener-
ation, but his method is limited to the case of k-
Bounded MAP Inference.

Several ILP and LP formulations of dependency
parsing have been proposed. Our formulation is in-
spired by Martins et al. (2009), and hence uses fewer
constraints than Riedel and Clarke (2006). For the
case of grandparent edges, our formulation also im-
proves upon the outer bound of Martins et al. (2009)
in terms of speed, tightness, and utility for column
generation. Other recent LP relaxations are based
on dual decomposition (Rush et al., 2010; Koo et
al., 2010; Martins et al., 2011). These relaxations
allow the practitioner to utilize tailor-made dynamic
programs for tractable substructure, but still every
edge needs to be scored. Given that column gener-
ation can also be applied in dynamic programs (see
section 4.5), our algorithm could in fact accelerate
dual decomposition parsing as well.

Pruning methods are a major part of many struc-
tured prediction algorithms in general, and of pars-
ing algorithms in particular (Charniak and Johnson,
2005; Martins et al., 2009; Koo and Collins, 2010;
Rush and Petrov, 2012). Generally these meth-
ods follow a coarse-to-fine scheme in which sim-
pler models filter out large fractions of edges. Such
methods are effective, but require tuning of thresh-
old parameters, training of additional models, and
generally lead to more complex pipelines that are
harder to analyze and have fewer theoretical guar-
antees.

A* search (Ahuja et al., 1993) has been used
to search for optimal parse trees, for example by
Klein and Manning (2003) or, for dependency pars-
ing, by Dienes et al. (2003). There is a direct rela-
tion between both A* and Column Generation based
on an LP formulation of the shortest path problem.
Roughly speaking, in this formulation any feasible
dual assignments correspond to a consistent (and
thus admissible) heuristic, and the corresponding re-
duced costs can be used as edge weights. Run-

739



ning Dijkstra’s algorithm with these weights then
amounts to A*. Column generation for the shortest
path problem can then be understood as a method to
lazily construct a consistent heuristic. In every step
this method finds edges for which consistency is vi-
olated, and updates the heuristic such that all these
edges are consistent.

6 Experiments

We claim that LP relaxations for higher order pars-
ing can be solved without considering, and scoring,
all candidate higher order edges. In practice, how
many grandparent edges do we need to score, and
how many do we need to add to the optimization
problem? And what kind of reduction in runtime
does this reduction in edges lead to?

We have also pointed out that our outer bound on
the grandparent polytope of legal edge and grand-
parent vectors is tighter than the one presented by
Martins et al. (2009). What effect does this bound
have on the number of fractional solutions and the
overall accuracy?

To answer these questions we will focus on a set
of non-projective grandparent models, but point out
that our method and formulation can be easily ex-
tended to projective parsing as well as other types
of higher order edges. We use the Danish test data
of Buchholz and Marsi (2006) and the Italian and
Hungarian test datasets of Nivre et al. (2007).

6.1 Impact of Price and Cut

Table 1 compares brute force optimization (BF) with
the full model, in spirit of Martins et al. (2009),
to running parse, price and cut (PPC) on the same
model. This model contains all constraints presented
in 3.2. The table shows the average number of
parsed sentences per second, the average objective,
number of grandparent edges scored and added, all
relative to the brute force approach. We also present
the average unlabeled accuracy, and the percentage
of sentences with integer solutions. This number
shows us how often we not only found the optimal
solution to the LP relaxation, but also the optimal
solution to the full ILP.

We first note that both systems achieve the same
objective, and therefore, also the same accuracy.
This is expected, given that column and row gen-

eration are known to yield optimal solutions. Next
we see that the number of grandparent edges scored
and added to the problem is reduced to 5–13% of the
full model. This leads to up to 760% improvement
in speed. This improvement comes for free, without
any sacrifice in optimality or guarantees. We also
notice that in all cases at least 97% of the sentences
have no fractional solutions, and are therefore opti-
mal even with respect to the ILP. Table 1 also shows
that our bounds on reduced costs are relatively tight.
For example, in the case of Italian we score only
one percent more grandparent edges than we actu-
ally need to add.

Our fastest PCC parser processes about one sen-
tence per second. This speed falls below the reported
numbers of Martins et al. (2009) of about 0.6 sec-
onds per sentence. Crucially, however, in contrast to
their work, our speed is achieved without any first-
order pruning. In addition, we expect further im-
provements in runtime by optimizing the implemen-
tation of our pricing algorithm.

6.2 Tighter Grandparent Polytope

To investigate how the additional grandparent con-
straints in section 3.2 help, we compare three mod-
els, this time without PPC. The first model follows
Martins et al. (2009) and uses constraints ArcGP and
GPArc only. The second model uses only constraints
OneGP and NoGP. The final model incorporates all
four constraints.

Table 2 shows speed relative to the baseline model
with constraints ArcGP and GPArc, as well as the
percentage of integer solutions and the average un-
labeled accuracy—all for the Italian and Hungarian
datasets. We notice that the full model has less frac-
tional solutions than the partial models, and either
substantially (Italian) or slightly (Hungarian) faster
runtimes than ArcGP+GPArc. Interestingly, both
sets of constraints in isolation perform worse, in par-
ticular the OneGP and NoGP model.

7 Conclusion

We have presented a novel method for parsing in
second order grandparent models, and a general
blueprint for more efficient and optimal structured
prediction. Our method lazily instantiates candidate
parts based on their reduced cost, and on constraint

740



Italian Hungarian Danish
BF PPC BF PPC BF PPC

Sent./sec. relative to BF 100% 760% 100% 380% 100% 390%
GPs Scored relative to BF 100% 6% 100% 12% 100% 13%
GPs Added relative to BF 100% 5% 100% 7% 100% 7%
Objective rel. to BF 100% 100% 100% 100% 100% 100%
% of Integer Solutions 98% 98% 97% 97% 97% 97%
Unlabeled Acc. 88% 88% 81% 81% 88% 88%

Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,
relative to the speed of BF. Objective, GPs scored and added are also relative to BF.

GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 1000% 1200%
% Integer 77% 9% 98%
Unlabeled Acc. 87% 85% 88%

(a) Italian

GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 162% 105%
% Integer 71% 3% 97%
Unlabeled Acc. 80% 77% 81%

(b) Hungarian

Table 2: Different outer bounds on the grandpar-
ent polytope, for nonprojective parsing of Italian and
Danish.

violations. This allows us to discard a large fraction
of parts during both scoring and optimization, lead-
ing to nearly 800% speed-ups without loss of accu-
racy and certificates. We also present a tighter bound
on the grandparent polytope that is useful in its own
right.

Delayed column and row generation is very useful
when solving large LPs with off-the-shelf solvers.
Given the multitude of work in NLP that uses LPs
and ILPs in this way (Roth and Yih, 2004; Clarke
and Lapata, 2007), we hope that our approach will
prove itself useful for other applications. We stress
that this approach can also be used when working
with dynamic programs, as pointed out in section
4.5, and therefore also in the context of dual de-
composition. This suggests even wider applicabil-
ity, and usefulness in various structured prediction

problems.
The underlying paradigm could also be useful for

more approximate methods. In this paradigm, al-
gorithms maintain an estimate of the cost of certain
resources (duals), and use these estimates to guide
search and the propose new structures. For exam-
ple, a local-search based dependency parser could
estimate how contested certain tokens, or edges, are,
and then use these estimates to choose better next
proposals. The notion of reduced cost can give guid-
ance on what such estimates should look like.

Acknowledgements

This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.

References

Ravindra K. Ahuja, Thomas L. Magnanti, and James B.
Orlin. 1993. Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, 1 edition, February.

David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. A column generation ap-
proach to connecting regularization and map infer-
ence. In Inferning: Interactions between Inference
and Learning, ICML 2012 Workshop.

741



Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, 2nd edition, September.

Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natu-
ral Language Learning (CoNLL’ 06), CoNLL-X ’06,
pages 149–164, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ’05),
pages 173–180.

James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
’07), pages 1–11.

A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ’07), pages 81–88.

Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical a-star dependency parsing. In Pro-
ceedings of the workshop on Prospects and Advances
of the Syntax/Semantics Interface, Nancy, 2003, pp.85-
89.

P.C. Gilmore and R.E. Gomory. 1961. A linear program-
ming approach to the cutting-stock problem. Opera-
tions research, pages 849–859.

Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL ’03), pages 119–126.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ’11).

Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ’10).

Sandra Kübler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.

Marco Lübbecke and Jacques Desrosiers. 2004. Selected
topics in column generation. Operations Research,
53:1007–1023.

R. Kipp Martin, Ronald L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of discrete
dynamic programming. Oper. Res., 38(1):127–138,
February.

André F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL
’09), pages 342–350, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.

André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and Mário A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proceed-
ings of the Conference on Empirical methods in natu-
ral language processing (EMNLP ’11), EMNLP ’11,
pages 238–249, Stroudsburg, PA, USA. Association
for Computational Linguistics.

R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ’06), pages 81–88.

R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ’05), pages
91–98.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP, 2005.

Mathias Niepert. 2010. A delayed column generation
strategy for exact k-bounded map inference in markov
logic networks. In Proceedings of the 26th Annual
Conference on Uncertainty in AI (UAI ’10), pages
384–391, Corvallis, Oregon. AUAI Press.

J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915—932.

Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence (AAAI
’07), pages 913–918.

Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’06), pages 129–137.

Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency

742



parsing. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ’10), pages 760–768, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.

Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ’08), pages 468–475.

D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL’ 04), pages 1–8.

Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Joint Hu-
man Language Technology Conference/Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL ’12).

Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’10).

David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145–156, Hon-
olulu, October.

D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ’07), pages 1393–
1400.

David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ’08).

Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ’06), pages 423–
430.

743


