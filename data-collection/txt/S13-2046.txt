










































ETS: Domain Adaptation and Stacking for Short Answer Scoring


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

ETS: Domain Adaptation and Stacking for Short Answer Scoring∗

Michael Heilman and Nitin Madnani
Educational Testing Service

660 Rosedale Road
Princeton, NJ 08541, USA

{mheilman,nmadnani}@ets.org

Abstract

Automatic scoring of short text responses to
educational assessment items is a challeng-
ing task, particularly because large amounts
of labeled data (i.e., human-scored responses)
may or may not be available due to the va-
riety of possible questions and topics. As
such, it seems desirable to integrate various
approaches, making use of model answers
from experts (e.g., to give higher scores to
responses that are similar), prescored student
responses (e.g., to learn direct associations
between particular phrases and scores), etc.
Here, we describe a system that uses stack-
ing (Wolpert, 1992) and domain adaptation
(Daume III, 2007) to achieve this aim, allow-
ing us to integrate item-specific n-gram fea-
tures and more general text similarity mea-
sures (Heilman and Madnani, 2012). We re-
port encouraging results from the Joint Stu-
dent Response Analysis and 8th Recognizing
Textual Entailment Challenge.

1 Introduction

In this paper, we address the problem of automati-
cally scoring short text responses to educational as-
sessment items for measuring content knowledge.

Many approaches can be and have been taken to
this problem—e.g., Leacock and Chodorow (2003),
Nielsen et al. (2008), inter alia. The effectiveness
of any particular approach likely depends on the the
availability of data (among other factors). For exam-
ple, if thousands of prescored responses are avail-

∗System description papers for SemEval 2013 are required
to have a team ID (e.g., “ETS”) as a prefix.

able, then a simple classifier using n-gram features
may suffice. However, if only model answers (i.e.,
reference answers) or rubrics are available, more
general semantic similarity measures (or even rule-
based approaches) would be more effective.

It seems likely that, in many cases, there will
be model answers as well as a modest number of
prescored responses available, as was the case for
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (§2). There-
fore, we desire to incorporate both task-specific fea-
tures, such as n-grams, as well as more general fea-
tures such as the semantic similarity of the response
to model answers.

We also observe that some features may them-
selves require machine learning or tuning on data
from the domain, in addition to any machine learn-
ing required for the overall system.

In this paper, we describe a machine learning ap-
proach to short answer scoring that allows us to in-
corporate both item-specific and general features by
using the domain adaptation technique of Daume III
(2007). In addition, the approach employs stacking
(Wolpert, 1992) to support the integration of com-
ponents that require tuning or machine learning.

2 Task Overview

In this section, we describe the task to which we ap-
plied our system: the Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al., 2013), which was task 7 at
SemEval 2013.

The aim of the task is to classify student responses
to assessment items from two datasets represent-

275



ing different science domains: the Beetle dataset,
which pertains to basic electricity and electronics
(Dzikovska et al., 2010), and the Science Entail-
ments corpus (SciEntsBank) (Nielsen et al., 2008),
which covers a wider range of scientific topics.

Responses were organized into five categories:
correct, partially correct, contradictory, irrelevant,
and non-domain. The SciEntsBank responses were
converted to this format as described by Dzikovska
et al. (2012).

The Beetle training data had about 4,000 student
answers to 47 questions. The SciEntsBank training
data had about 5,000 prescored student answers to
135 questions from 12 domains (different learning
modules). For each item, one or more model re-
sponses were provided by the task organizers.

There were three different evaluation scenarios:
“unseen answers”, for scoring new answers to items
represented in the training data; “unseen questions”,
for scoring answers to new items from domains rep-
resented in the training data; and “unseen domains”,
for scoring answers to items from new domains
(only for SciEntsBank since Beetle focused on a sin-
gle domain).

Performance was evaluated using accuracy,
macro-average F1 scores, and weighted average F1
scores.

For additional details, see the task description pa-
per (Dzikovska et al., 2013).

3 System Details

In this section, we describe the short answer scoring
system we developed, and the variations of it that
comprise our submissions to task 7. We begin by
describing our statistical modeling approach. There-
after, we describe the features used by the model
(§3.1), including the PERP feature that relies on
stacking (Wolpert, 1992), and then the domain adap-
tation technique we used (§3.2).

Our system is a logistic regression model with
`2 regularization. It uses the implementation of lo-
gistic regression from the scikit-learn toolkit (Pe-
dregosa et al., 2011).1 To tune the C hyperparame-
ter, it uses a 5-fold cross-validation grid search (with

1The scikit-learn toolkit uses a one-versus-all scheme, us-
ing multiple binary logistic regression classifiers, rather than a
single multiclass logistic regression classifier.

C ∈ 10{−3,−2,...,3}).
During development, we evaluated performance

using 10-fold cross-validation, with the 5-fold cross-
validation grid search still used for tuning within
each training partition (i.e., each set of 9 folds used
for training during cross-validation).

3.1 Features

Our full system includes the following features.

3.1.1 Baseline Features

It includes all of the baseline features generated
with the code provided by the task organizers.2

There are four types of lexically-driven text similar-
ity measures, and each is computed by comparing
the learner response to both the expected answer(s)
and the question, resulting in eight features in total.
They are described more fully by Dzikovska et al.
(2012).

3.1.2 Intercept Feature

The system includes an intercept feature that is al-
ways equal to one, which, in combination with the
domain adaptation technique described in §3.2, al-
lows the system to model the a priori distribution
over classes for each domain and item. Having these
explicit intercept features effectively saves the learn-
ing algorithm from having to use other features to
encode the distribution over classes.

3.1.3 Word and Character n-gram Features

The system includes binary indicator features for
the following types of n-grams:

• lowercased word n-grams in the response text
for n ∈ {1, 2, 3}.

• lowercased word n-grams in the response text
for n ∈ {4, 5, . . . , 11}, grouped into 10,000
bins by hashing and using a modulo operation
(i.e., the “hashing trick”) (Weinberger et al.,
2009).

• lowercased character n-grams in the response
text for n ∈ {5, 6, 7, 8}

2At the time of writing, the baseline code could
be downloaded at http://www.cs.york.ac.uk/
semeval-2013/task7/.

276



3.1.4 Text Similarity Features
The system includes the following text similarity

features that compare the student response either to
a) the reference answers for the appropriate item, or
b) the student answers in the training set that are la-
beled “correct”.

• the maximum of the smoothed, uncased BLEU
(Papineni et al., 2002) scores obtained by com-
paring the student response to each correct
reference answer. We also include the word
n-gram precision and recall values for n ∈
{1, 2, 3, 4} for the maximally similar reference
answer.

• the maximum of the smoothed, uncased BLEU
scores obtained by comparing the student re-
sponse to each correct training set student an-
swer. We also include the word n-gram preci-
sion and recall values for n ∈ {1, 2, 3, 4} for
the maximally similar student answer.

• the maximum PERP (Heilman and Madnani,
2012) score obtained by comparing the student
response to the correct reference answers.

• the maximum PERP score obtained by compar-
ing the student response to the correct student
answers.

PERP is an edit-based approach to text similar-
ity. It computes the similarity of sentence pairs by
finding sequences of edit operations (e.g., insertions,
deletions, substitutions, and shifts) that convert one
sentence in a pair to the other. Then, using various
features of the edits and weights for those features
learned from labeled sentence pairs, it assigns a sim-
ilarity score. Heilman and Madnani (2012) provide
a detailed description of the original PERP system.
In addition, Heilman and Madnani (To Appear) de-
scribe some minor modifications to PERP used in
this work.

To estimate weights for PERP’s edit features, we
need labeled sentence pairs. First, we describe how
these labeled sentence pairs are generated from the
task data, and then we describe the stacking ap-
proach used to avoid training PERP on the same data
it will compute features for.

For the reference answer PERP feature, we use
the Cartesian product of the set of correct reference

answers (“good” or “best” for Beetle) and the set
of student answers, using 1 as the similarity score
(i.e., the label for training PERP) for pairs where the
student answer is labeled “correct” and 0 for all oth-
ers. For the student answer PERP feature, we use
the Cartesian product of the set of correct student
answers and the set of all student answers, using 1
as the similarity score for pairs where both student
answers are labeled “correct” and 0 for all others.3

We use 10 iterations for training PERP.
In order to avoid training PERP on the same re-

sponses it will compute features for, we use 10-fold
stacking (Wolpert, 1992). In this process, the train-
ing data are split up into ten folds. To compute the
PERP features for the instances in each fold, PERP
is trained on the other nine folds. After all 10 itera-
tions, there are PERP features for every example in
the training set. This process is similar to 10-fold
cross-validation.

3.2 Domain Adaptation

The system uses the domain adaptation technique
from Daume III (2007) to support generalization
across items and domains.

Instead of having a single weight for each feature,
following Daume III (2007), the system has multiple
copies with potentially different weights: a generic
copy, a domain-specific copy, and an item-specific
copy. For an answer to an unseen item (i.e., ques-
tion) from a new domain in the test set, only the
generic feature will be active. In contrast, for an an-
swer to an item represented in the training data, the
generic, domain-specific, and item-specific copies
of the feature would be active and contribute to the
score.

For our submissions, this feature copying ap-
proach was not used for the baseline features
(§3.1.1) or the BLEU and PERP text similarity fea-
tures (§3.1.4), which are less item-specific. Those
features had only general copies. We did not test
whether doing so would affect performance.

3The Cartesian product of the sets of correct student answers
and of all student answers will contain some pairs of identi-
cal correct answers. We decided to simply include these when
training PERP, since we felt it would be desirable for PERP to
learn that identical sentences should be considered similar.

277



Beetle SciEntsBank
Submission A Q A Q D
Run 1 .5520 .5470 .5350 .4870 .4470
Run 2 .7050 .6140 .6250 .3560 .4340
Run 3 .7000 .5860 .6400 .4110 .4140
maximum .7050 .6140 .6400 .4920 .4710
mean .5143 .3978 .4568 .3769 .3736

Table 1: Weighted average F1 scores for 5-way classification for our SemEval 2013 task 7 submissions, along with
the maximum and mean performance, for comparison. “A” = unseen answers, “Q” = unseen questions, “D” = unseen
domains (see §2 for details). Results that were the maximum score among submissions for part of the task are in bold.

3.3 Submissions
We submitted three variations of the system. For
each variation, a separate model was trained for Bee-
tle and for SciEntsBank.

• Run 1: This run included the baseline (§3.1.1),
intercept (§3.1.2), and the text-similarity fea-
tures (§3.1.4) that compare student responses to
reference answers (but not those that compare
to scored student responses in the training set).

• Run 2: This run included the baseline (§3.1.1),
intercept (§3.1.2), and n-gram features (§3.1.3).
• Run 3: This run included all features.

4 Results

Table 1 presents the weighted averages of F1 scores
across the five categories for the 5-way subtask, for
each dataset and scenario. The maximum and mean
scores of all the submissions are included for com-
parison. These results were provided to us by the
task organizers.

For conciseness, we do not include accuracy or
macro-average F1 scores here. We observed that, in
general, the results from different evaluation metrics
were very similar to each other. We refer the reader
to the task description paper (Dzikovska et al., 2013)
for a full report of the task results.

Interestingly, the differences in performance be-
tween the unseen answers task and the other tasks
was somewhat larger for the SciEntsBank dataset
than for the Beetle dataset. We speculate that this re-
sult is because the SciEntsBank data covered a more
diverse set of topics.

Note that Runs 1 and 2 use subsets of the features
from the full system (Run 3). While Runs 1 and 2

are not directly comparable to each other, Runs 1
and 3 can be compared to measure the effect of the
features based on other previously scored student re-
sponses (i.e., n-grams, and the PERP and BLEU fea-
tures based on student responses). Similarly, Runs 2
and 3 can be compared to measure the combined ef-
fect of all BLEU and PERP features.

It appears that features of the other student re-
sponses improve performance for the unseen an-
swers task. For example, the full system (Run 3)
performed better than Run 1, which did not include
features of other student responses, on the unseen
answers task for both Beetle and SciEntsBank.

However, it is not clear whether the PERP and
BLEU features improve performance. The full sys-
tem (Run 3) did not always outperform Run 2, which
did not include these features.

We leave to future work various additional ques-
tions, such as whether student response features or
reference answer similarity features are more use-
ful in general, and whether there are any systematic
differences between human-machine and human-
human disagreements.

5 Conclusion

We have presented an approach for short answer
scoring that uses stacking (Wolpert, 1992) and do-
main adaptation (Daume III, 2007) to support the
integration of various types of task-specific and gen-
eral features. Evaluation results from task 7 at Se-
mEval 2013 indicate that the system achieves rela-
tively high levels of agreement with human scores,
as compared to other systems submitted to the
shared task.

278



Acknowledgments

We would like to thank the task organizers for facil-
itating this research and Dan Blanchard for helping
with scikit-learn.

References
Hal Daume III. 2007. Frustratingly easy domain adapta-

tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263, Prague, Czech Republic, June. Association
for Computational Linguistics.

Myroslava O. Dzikovska, Diana Bental, Johanna D.
Moore, Natalie Steinhauser, Gwendolyn Campbell,
Elaine Farrow, and Charles B. Callaway. 2010. In-
telligent tutoring with natural language support in the
BEETLE II system. In Proceedings of Fifth European
Conference on Technology Enhanced Learning (EC-
TEL 2010).

Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200–210, Montréal, Canada, June. Association
for Computational Linguistics.

Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.

Michael Heilman and Nitin Madnani. 2012. ETS: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529–535, Montréal, Canada, 7-8 June. Associa-
tion for Computational Linguistics.

Michael Heilman and Nitin Madnani. To Appear. Henry:
Domain adapation and stacking for text similarity. In
*SEM 2013: The Second Joint Conference on Lexical
and Computational Semantics. Association for Com-
putational Linguistics.

C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.

Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008. Classification errors in a domain-independent
assessment system. In Proceedings of the Third Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications, pages 10–18, Columbus, Ohio,
June. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–
2830.

Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature hash-
ing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ’09, pages 1113–1120, New
York, NY, USA. ACM.

David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241–259.

279


