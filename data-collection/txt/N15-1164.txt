



















































Embedding a Semantic Network in a Word Space


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1428–1433,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Embedding a Semantic Network in a Word Space

Richard Johansson and Luis Nieto Piña
Språkbanken, Department of Swedish, University of Gothenburg

Box 200, SE-40530 Gothenburg, Sweden
{richard.johansson, luis.nieto.pina}@svenska.gu.se

Abstract

We present a framework for using continuous-
space vector representations of word meaning
to derive new vectors representing the mean-
ing of senses listed in a semantic network. It
is a post-processing approach that can be ap-
plied to several types of word vector represen-
tations. It uses two ideas: first, that vectors for
polysemous words can be decomposed into
a convex combination of sense vectors; sec-
ondly, that the vector for a sense is kept sim-
ilar to those of its neighbors in the network.
This leads to a constrained optimization prob-
lem, and we present an approximation for the
case when the distance function is the squared
Euclidean.

We applied this algorithm on a Swedish se-
mantic network, and we evaluate the quality
of the resulting sense representations extrinsi-
cally by showing that they give large improve-
ments when used in a classifier that creates
lexical units for FrameNet frames.

1 Introduction

Representing word meaning computationally is cen-
tral in natural language processing. Manual,
knowledge-based approaches to meaning represen-
tation maps word strings to symbolic concepts,
which can be described using any knowledge rep-
resentation framework; using the relations between
concepts defined in the knowledge base, we can in-
fer implicit facts from the information stated in a
text: a mouse is a rodent, so it has prominent teeth.

Conversely, data-driven meaning representation
approaches rely on cooccurrence patterns to derive
a vector representation (Turney and Pantel, 2010).
There are two classes of methods that compute word
vectors: context-counting and context-predicting;

while the latter has seen much interest lately, their
respective strengths and weaknesses are still being
debated (Baroni et al., 2014; Levy and Goldberg,
2014). The most important relation defined in a vec-
tor space between the meaning of two words is sim-
ilarity: a mouse is something quite similar to a rat.
Similarity of meaning is operationalized in terms of
geometry, by defining a distance metric.

Symbolic representations seem to have an advan-
tage in describing word sense ambiguity: when a
surface form corresponds to more than one concept.
For instance, the word mouse can refer to a rodent
or an electronic device. Vector-space representations
typically represent surface forms only, which makes
it hard to search e.g. for a group of words similar
to the rodent sense of mouse or to reliably use the
vectors in classifiers that rely on the semantics of
the word. There have been several attempts to create
vectors representing senses, most of them based on
some variant of the idea first proposed by Schütze
(1998): that senses can be seen as clusters of similar
contexts. Recent examples in this tradition include
the work by Huang et al. (2012) and Neelakantan et
al. (2014). However, because sense distributions are
often highly imbalanced, it is not clear that context
clusters can be reliably created for senses that occur
rarely. These approaches also lack interpretability:
if we are interested in the rodent sense of mouse,
which of the vectors should we use?

In this work, we instead derive sense vectors by
embedding the graph structure of a semantic net-
work in the word space. By combining two com-
plementary sources of information – corpus statis-
tics and network structure – we derive useful vec-
tors also for concepts that occur rarely. The method,
which can be applied to context-counting as well
as context-predicting spaces, works by decompos-

1428



ing word vectors as linear combinations of sense
vectors, and by pushing the sense vectors towards
their neighbors in the semantic network. This in-
tuition leads to a constrained optimization problem,
for which we present an approximate algorithm.

We applied the algorithm to derive vectors for
the senses in a Swedish semantic network, and we
evaluated their quality extrinsically by using them
as features in a semantic classification task – map-
ping senses to their corresponding FrameNet frames.
When using the sense vectors in this task, we saw a
large improvement over using word vectors.

2 Embedding a Semantic Network

The goal of the algorithm is to embed the seman-
tic network in a geometric space: that is, to asso-
ciate each sense sij with a sense embedding, a vec-
tor E(sij) of real numbers, in a way that reflects the
topology of the semantic network but also that the
vectors representing the lemmas are related to those
corresponding to the senses. We now formalize this
intuition, and we start by introducing some notation.

For each lemma li, there is a set of possible senses
si1, . . . , simi for which li is a surface realization.
Furthermore, for each sense sij , there is a neighbor-
hood consisting of senses semantically related to sij .
Each neighbor nijk of sij is associated with a weight
wijk representing the degree of semantic relatedness
between sij and nijk. How we define the neighbor-
hood, i.e. our notion of semantical relatedness, will
obviously have an impact on the result. In this work,
we simply assume that it can be computed from the
network, e.g. by picking a number of hypernyms
and hyponyms in a lexicon such as WordNet. We
then assume that for each lemma li, we have a D-
dimensional vector F (li) of real numbers; this can
be computed using any method described in Section
1. Finally, we assume a distance function ∆(x, y)
that returns a non-negative real number for each pair
of vectors in RD.

The algorithm maps each sense sij to a sense em-
bedding, a real-valued vector E(sij) in the same
vector space as the lemma embeddings. The lemma
and sense embeddings are related through a mix con-
straint: F (li) is decomposed as a convex combi-
nation

∑
j pijE(sij), where the {pij} are picked

from the probability simplex. Intuitively, the mix

variables correspond to the occurrence probabilities
of the senses, but strictly speaking this is only the
case when the vectors are built using simple context
counting. Since the mix gives an estimate of which
sense is the most frequent in the corpus, we get a
strong baseline for word sense disambiguation (Mc-
Carthy et al., 2007) as a bonus; see our followup
paper (Johansson and Nieto Piña, 2015) for a dis-
cussion of this.

We can now formalize the intuition above: the
weighted sum of distances between each sense and
its neighbors is minimized, while satisfying the mix
constraint for each lemma. We get the following
constrained optimization program:

minimize
E,p

∑
i,j,k

wijk∆(E(sij), E(nijk))

subject to
∑

j

pijE(sij) = F (li) ∀i∑
j

pij = 1 ∀i

pij ≥ 0 ∀i, j

(1)

The mix constraints make sure that the solution
is nontrivial. In particular, a very large number
of words are monosemous, and the procedure will
leave the embeddings of these words unchanged.

2.1 An Approximate Algorithm
The difficulty of solving the problem stated in Equa-
tion (1) obviously depends on the distance function
∆. Henceforth, we focus on the case where ∆ is
the squared Euclidean distance. This is an impor-
tant special case that is related to a number of other
distances or similarities, e.g. cosine similarity and
Hellinger distance. In this case, (1) is a quadratically
constrained quadratic problem, which is NP-hard in
general and difficult to handle with off-the-shelf op-
timization tools. We therefore resort to an approx-
imation; we show empirically in Sections 3 and 4
that it works well in practice.

The approximate algorithm works in an online
fashion by considering one lemma at a time. It ad-
justs the embeddings of the senses as well as their
mix in order to minimize the loss function

Li =
∑
jk

wijk‖E(sij)− E(nijk)‖2. (2)

1429



The embeddings of the neighbors nijk of the sense
are kept fixed at each such step. We iterate through
the whole set of lemmas for a fixed number of
epochs or until the objective is unchanged.

Furthermore, instead of directly optimizing with
respect to the sense embeddings (which involves
mi · D scalars), the sense embeddings (and there-
fore also the loss Li) can be computed analytically if
the mix variables pi1, . . . , pimi are given, so we have
reduced the optimization problem to one involving
mi − 1 scalars, i.e. it is univariate in most cases.

Given a sense sij of a lemma li, we define the
weighted centroid of the set of neighbors of sij as

cij =
∑

k wijkE(nijk)∑
k wijk

. (3)

If the mix constraints were removed, cij would be
the solution to the optimization problem: they min-
imize the weighted sum of squared Euclidean dis-
tances to the neighbors. Then, given the mix, the
residual is defined

ri =
1∑

j

p2ij∑
k wijk

∑
j

pijcij − F (li)
 . (4)

The vector ri represents the difference between the
linear combination of the weighted centroids and the
lemma embedding. Finally, we the sense embed-
dings for the lemma li become

E(sij) = cij − pij∑
k wijk

ri. (5)

Equations (4) and (5) show the role of the mix vari-
ables: if pij = 0, then the sense embedding E(sij) is
completely determined by the neighbors of the sense
(that is, it is equal to the weighted centroid). On the
other hand, if pij = 1, then the sense embedding be-
comes equal to the embedding of the lemma, F (li).
To optimize the mix variables pi1, . . . , pimi with re-
spect to the loss Li, basically any search procedure
could be used; we found it easiest to use a variant
of a simple randomized gradient-free search method
(Matyas, 1965).

3 Application to Swedish Data

The algorithm described in Section 2 was applied to
Swedish data: we started with lemma embeddings
computed from a corpus, and then created sense em-
beddings by using the SALDO semantic network
(Borin et al., 2013). The algorithm was run for a
few epochs, which seemed to be enough for reach-
ing a plateau in performance; the total runtime of the
algorithm was a few minutes.

3.1 Creating Lemma Embeddings

We created a corpus of 1 billion words downloaded
from Språkbanken, the Swedish language bank.1

The corpora are distributed in a format where the
text has been tokenized, part-of-speech-tagged and
lemmatized. Compounds have been segmented au-
tomatically and when a lemma was not listed in
SALDO, we used the parts of the compounds in-
stead. The input to the software computing the
lemma embedding consisted of lemma forms with
concatenated part-of-speech tags, e.g. dricka..vb
for the verb ‘to drink’ and dricka..nn for the noun
‘drink’. We used the word2vec tool2 to build the
lemma embeddings. All the default settings were
used, except the vector space dimensionality which
was set to 512.

3.2 SALDO, a Swedish Semantic Network

SALDO (Borin et al., 2013) is the largest freely
available lexical resource for Swedish. We used a
version from May 2014, which contains 125,781
entries organized into a single semantic network.
Compared to WordNet (Fellbaum, 1998), there are
similarities but also significant differences. Most
significantly, SALDO is organized according to the
lexical-semantic relation of association, not is-a as
in WordNet. In SALDO, when we go up in the
hierarchy we move from specialized vocabulary to
the most central vocabulary of the language (e.g.
‘move’, ‘want’, ‘who’); in WordNet we move from
specific to abstract (e.g. ‘entity’). Another differ-
ence is that sense distinctions in SALDO tend to be
more coarse-grained than in WordNet.

Each entry except a special root is connected to
other entries, its semantic descriptors. One of the

1http://spraakbanken.gu.se
2https://code.google.com/p/word2vec

1430



semantic descriptors is called the primary descrip-
tor: this is the semantic neighbor that is conceptu-
ally most central. Primary descriptors are most of-
ten hypernyms or synonyms, but they can also be
e.g. antonyms or meronyms, or be in an argument–
predicate relationship with the entry. To exemplify,
we consider the word rock, which has two senses:
a long coat and rock music, respectively. Its first
sense has the primary descriptor kappa ‘coat’, while
for the second sense it is musik ‘music’.

When embedding the SALDO network using the
algorithm in Section 2, the neighbors nijk of a
SALDO sense sij are its primary descriptor and
inverse primaries (the senses for which sij is the
primary descriptor). We did not use any descrip-
tors beside the primary. The neighborhood weights
were set so that the primary descriptor and the set
of inverse primaries were balanced, and so that all
weights sum to 1. If there were N inverse pri-
maries, we set the weight of the primary descriptor
to 12 and of each inverse primary to

1
2N . We did not

see any measurable effect of using a larger set of
neighbors (e.g. adding connections to second-order
neighbors).

3.3 Inspection of Sense Embeddings

Table 1 shows a list of the nearest neighbors of the
two senses of rock; as we can see, both lists make
sense semantically. (A similar query among the
lemma embeddings returns a list almost identical to
that of the second sense.)

rock-1 ‘coat’ rock-2 ‘rock music’
syrtut-1 ‘overcoat’ punk-1 ‘punk music’
kåpa-2 ‘cloak’ pop-1 ‘pop music’
kappa-1 ‘coat’ soul-1 ‘soul music’
kavaj-1 ‘blazer’ hårdrock-1 ‘hard rock’
jacka-1 ‘jacket’ hot-2 ‘hot jazz’

Table 1: The senses closest to the two senses of rock.

A more difficult and interesting use case, where
corpus-based methods clearly have an advantage
over knowledge-based methods, is to search among
the lemmas that have occurred in the corpus but
which are not listed in SALDO. Table 2 shows the
result of this search, and again the result is very use-
ful. We stress that the list for the first sense would

have been hard to derive in a standard word vector
space, due to the dominance of the music sense.

rock-1 ‘coat’ rock-2 ‘rock music’
midjekort ‘doublet’ indie ‘indie’
trekvartsärm ‘3/4 sleeve’ indierock ‘indie rock’
spetsbh ‘lace bra’ doo-wop ‘doo-wop’
blåjeans ‘blue jeans’ psykedelia ‘psychedelia’
treggings ‘treggings’ R&B ‘R&B’

Table 2: The unlisted lemmas closest to the two
senses of rock.

4 Evaluation

Evaluating intrinsically using e.g. a correlation be-
tween a graph-based similarity measure and geo-
metric similarity would be problematic, since this is
in some sense what our algorithm optimizes. We
therefore evaluate extrinsically, by using the sense
vectors in a classifier that maps senses to semantic
classes defined by FrameNet (Fillmore and Baker,
2009). FrameNet is a semantic database consisting
of two parts: first, an ontology of semantic frames –
the classes – and secondly a lexicon that maps word
senses to frames. In standard FrameNet terminol-
ogy, the senses assigned to a frame are called its
lexical units (LUs). Coverage is often a problem in
frame-semantic lexicons, and this has a negative im-
pact on the quality of NLP systems using FrameNet
(Palmer and Sporleder, 2010). The task of finding
LUs for frames is thus a useful testbed for evaluat-
ing lemma and sense vectors.

To evaluate, we used 567 frames from
the Swedish FrameNet (Friberg Heppin and
Toporowska Gronostaj, 2012); in total we had
28,842 verb, noun, adjective, and adverb LUs,
which we split into training (67%) and test sets
(33%). For each frame, we trained a SVM with
LIBLINEAR (Fan et al., 2008), using the LUs in that
frame as positive training instances, and all other
LUs as negative instances. Each LU was repre-
sented as a vector: its lemma or sense embedding
normalized to unit length.

Table 3 shows the precision, recall, and F -
measure for the classifiers for the five frames with
most LUs, and finally the micro-average over all
frames. In the overall evaluation as well as in four

1431



out of the five largest frames, the classifiers using
sense vectors clearly outperform those using lemma
vectors. The frame where we do not see any im-
provement by introducing sense distinctions, PEO-
PLE BY VOCATION, contains terms for professions
such as painter and builder; since SALDO derives
such terms from their corresponding verbs rather
than from a common hypernym (e.g. worker), they
do not form a coherent subnetwork in SALDO or
subregion in the embedding space.

Frame P R F
ANIMALS 0.741 0.643 0.689
FOOD 0.684 0.679 0.682
PEOPLE BY VOCATION 0.595 0.651 0.622
ORIGIN 0.789 0.691 0.737
PEOPLE BY ORIGIN 0.693 0.481 0.568
Overall 0.569 0.292 0.386

(a) Using lemma embeddings.

Frame P R F
ANIMALS 0.826 0.663 0.736
FOOD 0.726 0.743 0.735
PEOPLE BY VOCATION 0.605 0.637 0.621
ORIGIN 0.813 0.684 0.742
PEOPLE BY ORIGIN 0.756 0.508 0.608
Overall 0.667 0.332 0.443

(b) Using sense embeddings.

Table 3: FrameNet lexical unit classification.

5 Conclusion

We have presented a new method to embed a se-
mantic network consisting of linked word senses
into a continuous-vector word space; the method
is agnostic about whether the original word space
was produced using a context-counting or context-
predicting method. Unlike previous approaches for
creating sense vectors, since we rely on the network
structure, we can create representations for senses
that occur rarely in corpora. While the experiments
described in this paper have been carried out using
a Swedish corpus and semantic network, the algo-
rithm we have described is generally applicable and
the software3 can be applied to other languages and
semantic networks.

3http://demo.spraakdata.gu.se/richard/scouse

The algorithm takes word vectors and uses them
and the network structure to induce the sense vec-
tors. It is based on two separate ideas: first, sense
embeddings should preserve the structure of the se-
mantic network as much as possible, so two senses
should be close if they are neighbors in the graph;
secondly, the word vectors are a probablistic mix of
sense vectors. These two ideas are stated as an opti-
mization problem where the first becomes the objec-
tive and the second a constraint. While this is hard
to solve in the general case, we presented an approx-
imation that can be applied when using the squared
Euclidean distance.

We implemented the algorithm and used it to em-
bed the senses of a Swedish semantic network into
a word space produced using the skip-gram model.
While a qualitative inspection of nearest-neighbor
lists of a few senses gives very appealing results, our
main evaluation was extrinsic: a FrameNet lexical
unit classifier saw a large performance boost when
using sense vectors instead of word vectors.

In a followup paper (Johansson and Nieto Piña,
2015), we have shown that sense embeddings can be
used to build an efficient word sense disambiguation
system that is much faster than graph-based systems
with a similar accuracy, and that the mix variables
can be used to predict the predominant sense of a
word. In future work, we plan to investigate whether
the sense vectors are useful for retrieving rarely oc-
curring senses in corpora. Furthermore, since we
now evaluated extrinsically, it would be useful to de-
vise intrinsic sense-based evaluation schemes, e.g. a
sense analogy task similar to the word analogy task
used by Mikolov et al. (2013).

Acknowledgments

This research was funded by the Swedish Re-
search Council under grant 2013–4944, Distribu-
tional methods to represent the meaning of frames
and constructions, and grant 2012–5738, Towards a
knowledge-based culturomics. The evaluation mate-
rial used in this work was developed in the project
Swedish FrameNet++, grant 2010–6013. We also
acknowledge the University of Gothenburg for its
support of the Centre for Language Technology and
Språkbanken.

1432



References
Marco Baroni, Georgiana Dinu, and Germán Kruszewski.

2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 238–247, Baltimore,
United States.

Lars Borin, Markus Forsberg, and Lennart Lönngren.
2013. SALDO: a touch of yin to WordNet’s yang.
Language Resources and Evaluation, 47(4):1191–
1211.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press.

Charles J. Fillmore and Collin Baker. 2009. A frames ap-
proach to semantic analysis. In B. Heine and H. Nar-
rog, editors, The Oxford Handbook of Linguistic Anal-
ysis, pages 313–340. Oxford: OUP.

Karin Friberg Heppin and Maria Toporowska Gronostaj.
2012. The rocky road towards a Swedish FrameNet –
creating SweFN. In Proceedings of the Eighth confer-
ence on International Language Resources and Evalu-
ation (LREC-2012), pages 256–261, Istanbul, Turkey.

Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving word repre-
sentations via global context and multiple word pro-
totypes. In Association for Computational Linguistics
2012 Conference (ACL 2012), pages 41–48, Boston,
United States.

Richard Johansson and Luis Nieto Piña. 2015. Combin-
ing relational and distributional knowledge for word
sense disambiguation. In Proceedings of the 20th
Nordic Conference of Computational Linguistics, Vil-
nius, Lithuania.

Omer Levy and Yoav Goldberg. 2014. Linguistic regu-
larities in sparse and explicit word representations. In
Proceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning, pages 171–180,
Ann Arbor, United States.

J. Matyas. 1965. Random optimization. Automation and
Remote Control, 26(2):246–253.

Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590.

Tomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-

tion for Computational Linguistics: Human Language
Technologies, pages 746–751, Atlanta, USA.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1059–1069, Doha, Qatar,
October.

Alexis Palmer and Caroline Sporleder. 2010. Evaluating
FrameNet-style semantic parsing: the role of coverage
gaps in FrameNet. In Coling 2010: Posters, pages
928–936, Beijing, China.

Hinrich Schütze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.

1433


