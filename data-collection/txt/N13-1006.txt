










































Named Entity Recognition with Bilingual Constraints


Proceedings of NAACL-HLT 2013, pages 52–62,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Named Entity Recognition with Bilingual Constraints

Wanxiang Che† Mengqiu Wang‡ Christopher D. Manning‡ Ting Liu†

†{car, tliu}@ir.hit.edu.cn
School of Computer Science and Technology

Harbin Institute of Technology
Harbin, China, 150001

‡{mengqiu, manning}@stanford.edu
Computer Science Department

Stanford University
Stanford, CA, 94305

Abstract

Different languages contain complementary
cues about entities, which can be used to im-
prove Named Entity Recognition (NER) sys-
tems. We propose a method that formu-
lates the problem of exploring such signals on
unannotated bilingual text as a simple Inte-
ger Linear Program, which encourages entity
tags to agree via bilingual constraints. Bilin-
gual NER experiments on the large OntoNotes
4.0 Chinese-English corpus show that the pro-
posed method can improve strong baselines
for both Chinese and English. In particular,
Chinese performance improves by over 5%
absolute F1 score. We can then annotate a
large amount of bilingual text (80k sentence
pairs) using our method, and add it as up-
training data to the original monolingual NER
training corpus. The Chinese model retrained
on this new combined dataset outperforms the
strong baseline by over 3% F1 score.

1 Introduction

Named Entity Recognition (NER) is an important
task for many applications, such as information ex-
traction and machine translation. State-of-the-art su-
pervised NER methods require large amounts of an-
notated data, which are difficult and expensive to
produce manually, especially for resource-poor lan-
guages.

A promising approach for improving NER per-
formance without annotating more data is to exploit
unannotated bilingual text (bitext), which are rela-
tively easy to obtain for many language pairs, bor-
rowing from the resources made available by statis-

tical machine translation research.1 Different lan-
guages contain complementary cues about entities.
For example, in Figure 1, the word “本 (Ben)” is
common in Chinese but rarely appears as a trans-
lated foreign name. However, its aligned word on
the English side (“Ben”) provides a strong clue that
this is a person name. Judicious use of this type of
bilingual cues can help to recognize errors a mono-
lingual tagger would make, allowing us to produce
more accurately tagged bitext. Each side of the
tagged bitext can then be used to expand the orig-
inal monolingual training dataset, which may lead
to higher accuracy in the monolingual taggers.

Previous work such as Li et al. (2012) and Kim
et al. (2012) demonstrated that bilingual corpus an-
notated with NER labels can be used to improve
monolingual tagger performance. But a major draw-
back of their approaches are the need for manual
annotation efforts to create such corpora. To avoid
this requirement, Burkett et al. (2010) suggested a
“multi-view” learning scheme based on re-ranking.
Noisy output of a “strong” tagger is used as training
data to learn parameters of a log-linear re-ranking
model with additional bilingual features, simulated
by a “weak” tagger. The learned parameters are then
reused with the “strong” tagger to re-rank its own
outputs for unseen inputs. Designing good “weak”
taggers so that they complement the “view” of bilin-
gual features in the log-linear re-ranker is crucial to
the success of this algorithm. Unfortunately there is
no principled way of designing such “weak” taggers.

In this paper, we would like to explore a conceptu-
ally much simpler idea that can also take advantage

1opus.lingfil.uu.se

52



TheO chairmanO ofO theB−ORG FederalI−ORG ReserveI−ORG isO BenB−PER BernankeI−PER

美联储B−ORG 主席O 是O 本B−PER 伯南克I−PER

Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences.

of the large amount of unannotated bitext, without
complicated machinery. More specifically, we in-
troduce a joint inference method that formulates the
bilingual NER tagging problem as an Integer Linear
Program (ILP) and solves it during decoding. We
propose a set of intuitive and effective bilingual con-
straints that encourage NER results to agree across
the two languages.

Experimental results on the OntoNotes 4.0 named
entity annotated Chinese-English parallel corpus
show that the proposed method can improve the
strong Chinese NER baseline by over 5% F1 score
and also give small improvements over the English
baseline. Moreover, by adding the automatically
tagged data to the original NER training corpus
and retraining the monolingual model using an up-
training regimen (Petrov et al., 2010), we can im-
prove the monolingual Chinese NER performance
by over 3% F1 score.

2 Constraint-based Monolingual NER

NER is a sequence labeling task where we assign
a named entity tag to each word in an input sen-
tence. One commonly used tagging scheme is the
BIO scheme. The tag B-X (Begin) represents the
first word of a named entity of type X, for example,
PER (Person) or LOC (Location). The tag I-X (In-
side) indicates that a word is part of an entity but not
first word. The tag O (Outside) is used for all non-
entity words.2 See Figure 1 for an example tagged
sentence.

Conditional Random Fields (CRF) (Lafferty et al.,
2001) is a state-of-the-art sequence labeling model
widely used in NER. A first-order linear-chain CRF

2While the performance of NER is measured at the entity
level (not the tag level).

defines the following conditional probability:

PCRF (y|x) =
1

Z(x)

∏
i

Mi(yi, yi−1|x) (1)

where x and y are the input and output sequences,
respectively, Z(x) is the partition function, and Mi
is the clique potential for edge clique i. Decoding
in CRF involves finding the most likely output se-
quence that maximizes this objective, and is com-
monly done by the Viterbi algorithm.

Roth and Yih (2005) proposed an ILP inference
algorithm, which can capture more task-specific and
global constraints than the vanilla Viterbi algorithm.
Our work is inspired by Roth and Yih (2005). But
instead of directly solving the shortest-path problem
in the ILP formulation, we re-define the conditional
probability as:

PMAR(y|x) =
∏

i

P (yi|x) (2)

where P (yi|x) is the marginal probability given by
an underlying CRF model computed using forward-
backward inference. Since the early HMM litera-
ture, it has been well known that using the marginal
distributions at each position works well, as opposed
to Viterbi MAP sequence labeling (Mérialdo, 1994).
Our experimental results also supports this claim, as
we will show in Section 6. Our objective is to find
an optimal NER tag sequence:

ŷ = arg max
y

PMAR(y|x)

= arg max
y

∑
i

logP (yi|x) (3)

Then an ILP can be used to solve the inference
problem as classification problem with constraints.

53



The objective function is:

max

|x|∑
i=1

∑
y∈Y

zyi logP
y
i (4)

where Y is the set of all possible named entity tags.
P yi = P (yi = y|x) is the CRF marginal probabil-
ity that the ith word is tagged with y, and zyi is an
indicator that equals 1 iff the ith word is tagged y;
otherwise, zyi is 0.

If no constraints are identified, then Eq. (4)
achieves maximum when all zyi are assigned to 1,
which violates the condition that each word should
only be assigned a single entity tag. We can express
this with constraints:

∀i :
∑
y∈Y

zyi = 1 (5)

After adding the constraints, the probability of the
sequence is maximized when each word is assigned
the tag with highest probability. However, some in-
valid results may still exist. For example a tag O
may be wrongly followed by a tag I-X, although a
named entity cannot start with I-X. Therefore, we
can add the following constraints:

∀i,∀X : zB-Xi−1 + zI-Xi−1 − zI-Xi ≥ 0 (6)

which specifies that when the ith word is tagged with
I-X (zI-Xi = 1), then the previous word can only be
tagged with B-X or I-X (zB-Xi−1 + z

I-X
i−1 ≥ 1).

3 NER with Bilingual Constraints

This section demonstrates how to jointly perform
NER for two languages with bilingual constraints.
We assume sentences have been aligned into pairs,
and the word alignment between each pair of sen-
tences is also given.

3.1 Hard Bilingual Constraints
We first introduce the simplest hard constraints, i.e.,
each word alignment pair should have the same
named entity tag. For example, in Figure 1, the
Chinese word “美联储” was aligned with the En-
glish words “the”, “Federal” and “Reserve”. There-
fore, they have the same named entity tags ORG.3

3The prefix B- and I- are ignored.

Similarly, “本” and “Ben” as well as “伯南克” and
“Bernanke” were all tagged with the tag PER.

The objective function for bilingual NER can be
expressed as follows:

max

|xc|∑
i=1

∑
y∈Y

zyi logP
y
i +

|xe|∑
j=1

∑
y∈Y

zyj logP
y
j (7)

where P yi and P
y
j are the probabilities of the i

th Chi-
nese word and jth English word to be tagged with y,
respectively. xc and xe are respectively the Chinese
and English sentences.

Similar to monolingual constrained NER (Sec-
tion 2), monolingual constraints are added for each
language as shown in Eqs. (8) and (9):

∀i :
∑
y∈Y

zyi = 1;∀j :
∑
y∈Y

zyj = 1 (8)

∀i,∀X : zB-Xi + zI-Xi − zB-Xi+1 ≥ 0 (9)
∀j,∀X : zB-Xj + zI-Xj − zB-Xj+1 ≥ 0

Bilingual constraints are added in Eq. (10):

∀(i, j) ∈ A,∀X : zB-Xi + zI-Xi = zB-Xj + zI-Xj (10)

where A = {(i, j)} is the word alignment pair set,
i.e., the ith Chinese word and the jth English word
were aligned together. Chinese word i is tagged with
a named entity type X (zB-Xi + z

I-X
i = 1), iff English

word j is tagged with X (zB-Xj +z
I-X
j = 1). Therefore,

these hard bilingual constraints guarantee that when
two words are aligned, they are tagged with the same
named entity tag.

However, in practice, aligned word pairs do not
always have the same tag because of the difference
in annotation standards across different languages.
For example, in Figure 2(a), the Chinese word “开发
区” is a location. However, it is aligned to the words,
“development” and “zone”, which are not named en-
tities in English. Word alignment error is another se-
rious problem that can cause violation of hard con-
straints. In Figure 2(b), the English word “Agency”
is wrongly aligned with the Chinese word “电 (re-
port)”. Thus, these two words cannot be assigned
with the same tag.

To address these two problems, we present a prob-
abilistic model for bilingual NER which can lead to

54



ThisO developmentO zoneO isO locatedO inO . . .

这个O 开发区B−LOC 位O 于O . . .

(a) Inconsistent named entity standards

XinhuaB−ORG NewsI−ORG AgencyI−ORG FebruaryO 16thO

新华社B−ORG 二月B−LOC 十六日O 电O

(b) Word alignment error

Figure 2: Errors of hard bilingual constraints method.

an optimization problem with two soft bilingual con-
straints:

1) allow word-aligned pairs to have different
named entity tags; 2) consider word alignment prob-
abilities to reduce the influence of wrong word align-
ments.

3.2 Soft Constraints with Tag Uncertainty

The new probabilistic model for bilingual NER is:

P (yc,ye|xc,xe, A) =
P (yc,ye,xc,xe, A)

P (xc,xe, A)

=
P (yc,xc,xe, A)

P (xc,xe, A)
· P (ye,xc,xe, A)

P (xc,xe, A)

· P (yc,ye,xc,xe, A)P (xc,xe, A)
P (yc,xc,xe, A)P (ye,xc,xe, A)

(11)

≈ P (yc|xc)P (ye|xe)
P (yc,ye|A)

P (yc|A)P (ye|A)
(12)

where yc and ye respectively denotes Chinese and
English named entity output sequences. A is the set
of word alignment pairs.

If we assume that named entity tag assignments in
Chinese is only dependent on the observed Chinese
sentence, then we can drop the A and xe term in the
first factor of Eq. (11), and arrive at the first factor of
Eq. (12); similarly we can use the same assumption
to derive the second factor in Eq. (12) for English;
alternatively, if we assume the named entity tag as-
signments are only dependent on the cross-lingual
word associations via word alignment, then we can
drop xc and xe terms in the third factor of Eq. (11)

and arrive at the third factor of Eq. (12). These fac-
tors represent the two major sources of information
in the model: monolingual surface observation, and
cross-lingual word associations.

The first two factors of Eq. (12) can be further
decomposed into the product of probabilities of all
words in each language sentence like Eq. (2).

Assuming that the tags are independent between
different word alignment pairs, then the last factor
of Eq. (12) can be decomposed into:

P (yc,ye|A)
P (yc|A)P (ye|A)

=
∏
a∈A

P (ycayea)

P (yca)P (yea)

=
∏
a∈A

λycyea (13)

where yca and yea respectively denotes Chinese and
English named entity tags in a word alignment pair
a. λycye = P (ycye)P (yc)P (ye) is the pointwise mutual infor-
mation (PMI) score between a Chinese named en-
tity tag yc and an English named entity tag ye. If
yc = ye, then the score will be high; otherwise the
score will be low. A number of methods for calculat-
ing the scores are provided at the end of this section.

We use ILP to maximize Eq. (12). The new ob-
jective function is expressed as follow:

max

|xc|∑
i=1

∑
y∈Y

zyi logP
y
i +

|xe|∑
j=1

∑
y∈Y

zyj logP
y
j

+
∑
a∈A

∑
yc∈Y

∑
ye∈Y

zycyea log λ
ycye
a (14)

where zycyea is an indicator that equals 1 iff the Chi-
nese and English named entity tags are yc and ye
respectively, given a word alignment pair a; other-
wise, zycyea is 0.

Monolingual constraints such as Eqs. (8) and (9)
need to be added. In addition, one and only one pos-
sible named entity tag pair exists for a word align-
ment pair. This condition can be expressed as the
following constraints:

∀a ∈ A :
∑
yc∈Y

∑
ye∈Y

zycyea = 1 (15)

When the tag pair of a word alignment pair is de-
termined, the corresponding monolingual named en-

55



tity tags can also be identified. This rule can be ex-
pressed by the following constraints:

∀a = (i, j) ∈ A : zycyea ≤ zyci , zycyea ≤ z
ye
j (16)

Thus, if zycyea = 1, then z
yc
i and z

ye
j must be both

equal to 1. Here, the ith Chinese word and the jth

English word are aligned together.
In contrast to hard bilingual constraints, inconsis-

tent named entity tags for an aligned word pair are
allowed in soft bilingual constraints, but are given
lower λycye scores.

To calculate the λycye score, an annotated bilin-
gual NER corpus is consulted. We count from all
word alignment pairs the number of times yc and ye
occur together (C(ycye)) and separately (C(yc) and
C(ye)). Afterwards, λycye is calculated with maxi-
mum likelihood estimation as follows:

λycye =
P (ycye)

P (yc)P (ye)
=
N × C(ycye)
C(yc)C(ye)

(17)

where N is the total number of word alignment
pairs.

However, in this paper, we assume that no named
entity annotated bilingual corpus is available. Thus,
the above method is only used as Oracle. A real-
istic method for calculating the λycye score requires
the use of two initial monolingual NER models, such
as baseline CRF, to predict named entity tags for
each language on an unannotated bitext. We count
from this automatically tagged corpus the statistics
mentioned above. This method is henceforth re-
ferred to as Auto.

A simpler approach is to manually set the value
of λycye : if yc = ye then we assign a larger value
to λycye ; else we assign an ad-hoc smaller value. In
fact, if we set λycye = 1 iff yc = ye; otherwise,
λycye = 0, then the soft constraints backs off to hard
constraints. We refer to this set of soft constraints as
Soft-tag.

3.3 Constraints with Alignment Uncertainty
So far, we assumed that a word alignment set A is
known. In practice, only the word alignment proba-
bility Pa for each word pair a is provided. We can
set a threshold θ for Pa to tune the set A: a ∈ A
iff Pa ≥ θ. This condition can be regarded as a
kind of hard word alignment. However, the follow-
ing problem exists: the smaller the θ, the noisier the

word alignments are; the larger the θ, the more pos-
sible word alignments are lost. To ameliorate this
problem, we introduce another set of soft bilingual
constraints.

We can re-express Eq. (13) as follows:∏
a∈A

λycyea =
∏
a∈A

(λycyea )
Ia (18)

where A is the set of all word pairs between two
languages. Ia = 1 iff Pa ≥ θ; otherwise, Ia = 0.

We can then replace the hard indicator Ia with
the word alignment probability Pa, Eq. (14) is then
transformed into the following equation:

max

|Wc|∑
i

∑
y∈Y

zyi logP
y
i +

|We|∑
j

∑
y∈Y

zyj logP
y
j

+
∑
a∈A

∑
yc∈Y

∑
ye∈Y

zycyea Pa log λ
ycye
a (19)

We name the set of constraints above
Soft-align, which has the same constraints
as Soft-tag, i.e., Eqs. (8), (9), (15) and (16).

4 Experimental Setup

We conduct experiments on the latest OntoNotes
4.0 corpus (LDC2011T03). OntoNotes is a large,
manually annotated corpus that contains various text
genres and annotations, such as part-of-speech tags,
named entity labels, syntactic parse trees, predicate-
argument structures and co-references (Hovy et al.,
2006). Aside from English, this corpus also con-
tains several Chinese and Arabic corpora. Some of
these corpora contain bilingual parallel documents.
We used the Chinese-English parallel corpus with
named entity labels as our development and test
data. This corpus includes about 400 document pairs
(chtb 0001-0325, ectb 1001-1078). We used odd-
numbered documents as development data and even-
numbered documents as test data. We used all other
portions of the named entity annotated corpus as
training data for the monolingual systems. There
were a total of∼660 Chinese documents (∼16k sen-
tences) and ∼1,400 English documents (∼39k sen-
tences). OntoNotes annotates 18 named entity types,
such as person, location, date and money. In this
paper, we selected the four most common named
entity types, i.e., PER (Person), LOC (Location),

56



Chinese NER Templates
00: 1 (class bias param)
01: wi+k,−1 ≤ k ≤ 1
02: wi+k−1 ◦ wi+k, 0 ≤ k ≤ 1
03: shape(wi+k),−4 ≤ k ≤ 4
04: prefix(wi, k), 1 ≤ k ≤ 4
05: prefix(wi−1, k), 1 ≤ k ≤ 4
06: suffix(wi, k), 1 ≤ k ≤ 4
07: suffix(wi−1, k), 1 ≤ k ≤ 4
08: radical(wi, k), 1 ≤ k ≤ len(wi)
Unigram Features
yi◦ 00 – 08
Bigram Features
yi−1 ◦ yi◦ 00 – 08

Table 1: Basic features of Chinese NER.

ORG (Organization) and GPE (Geo-Political Enti-
ties), and discarded the others.

Since the bilingual corpus is only aligned at the
document level, we performed sentence alignment
using the Champollion Tool Kit (CTK).4 After re-
moving sentences with no aligned sentence, a total
of 8,249 sentence pairs were retained.

We used the BerkeleyAligner,5 to produce
word alignments over the sentence-aligned datasets.
BerkeleyAligner also gives posterior probabilities
Pa for each aligned word pair.

We used the CRF-based Stanford NER tagger (us-
ing Viterbi decoding) as our baseline monolingual
NER tool.6 English features were taken from Finkel
et al. (2005). Table 1 lists the basic features of
Chinese NER, where ◦ means string concatenation
and yi is the named entity tag of the ith word wi.
Moreover, shape(wi) is the shape of wi, such as
date and number. prefix/suffix(wi, k) denotes the
k-characters prefix/suffix of wi. radical(wi, k) de-
notes the radical of the kth Chinese character of wi.7

len(wi) is the number of Chinese characters in wi.
To make the baseline CRF taggers stronger, we

added word clustering features to improve gener-
alization over unseen data for both Chinese and
English. Word clustering features have been suc-
cessfully used in several English tasks, including

4champollion.sourceforge.net
5code.google.com/p/berkeleyaligner
6nlp.stanford.edu/software/CRF-NER.shtml,

which has included our English and Chinese NER implementations.
7The radical of a Chinese character can be found at: www.

unicode.org/charts/unihan.html

NER (Miller et al., 2004) and dependency pars-
ing (Koo et al., 2008). To our knowledge, this work
is the first use of word clustering features for Chi-
nese NER. A C++ implementation of the Brown
word clustering algorithms (Brown et al., 1992) was
used to obtain the word clusters (Liang, 2005).8

Raw text was obtained from the fifth edition of Chi-
nese Gigaword (LDC2011T13). One million para-
graphs from Xinhua news section were randomly
selected, and the Stanford Word Segmenter with
LDC standard was applied to segment Chinese text
into words.9 About 46 million words were obtained
which were clustered into 1,000 word classes.

5 Threshold Tuning

During development, we tuned the word alignment
probability thresholds to find the best value. Figure 3
shows the performance curves.

When the word alignment probability threshold θ
is set to 0.9, the hard bilingual constraints perform
well for both Chinese and English. But as the thresh-
olds value gets smaller, and more noisy word align-
ments are introduced, we see the hard bilingual con-
straints method starts to perform badly.

In Soft-tag setting, where inconsistent tag as-
signments within aligned word pairs are allowed but
penalized, different languages have different optimal
threshold values. For example, Chinese has an opti-
mal threshold of 0.7, whereas English has 0.2. Thus,
the optimal thresholds for different languages need
to be selected with care when Soft-tag is applied
in practice.
Soft-align eliminates the need for careful

tuning of word alignment thresholds, and therefore
can be more easily used in practice. Experimen-
tal results of Soft-align confirms our hypothe-
sis – the performance of both Chinese and English
NER systems improves with decreasing threshold.
However, we can still improve efficiency by set-
ting a low threshold to prune away very unlikely
word alignments. We set the threshold to 0.1 for
Soft-align to increase speed, and we observed
very minimal performance lost when doing so.

We also found that automatically estimated bilin-
gual tag PMI scores (Auto) gave comparable results

8github.com/percyliang/brown-cluster
9nlp.stanford.edu/software/segmenter.shtml

57



0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
threshold of word alignment probability

55

60

65

70

75

p
e
rf

o
rm

a
n
ce

 (
F1

)

Hard
Soft-label (Oracle)
Soft-label (Auto)
Soft-align (Oracle)
Soft-align (Auto)

(a) Chinese

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
threshold of word alignment probability

60

65

70

75

80

p
e
rf

o
rm

a
n
ce

 (
F1

)

Hard
Soft-label (Oracle)
Soft-label (Auto)
Soft-align (Oracle)
Soft-align (Auto)

(b) English

Figure 3: Performance curves of different bilingual constraints methods on development set.

to Oracle. Therefore this technique is effective
for computing the PMI scores, avoiding the need of
manually annotating named entity bilingual corpus.

6 Bilingual NER Results

The main results on Chinese and English test sets
with the optimal word alignment threshold for each
method are shown in Table 2.

The CRF-based Chinese NER with and without
word clustering features are compared here. The
word clustering features significantly (p < 0.01) im-
proved the performance of Chinese NER, 10 giving
us a strong Chinese NER baseline.11 The effective-
ness of word clustering for English NER has been
proved in previous work.

The performance of ILP with only monolingual
constraints is quite comparable with the CRF re-
sults, especially on English. The greater ILP perfor-
mance on English is probably due to more accurate
marginal probabilities estimated by the English CRF
model.

The ILP model with hard bilingual constraints
gives a slight performance improvement on Chi-
nese, but affects performance negatively on English.
Once we introduced tagging uncertainties into the
Soft-tag bilingual constraints, we see a very sig-

10We use paired bootstrap resampling significance test (Efron
and Tibshirani, 1993).

11To the best of our knowledge, there was no performance
report of state-of-the-art NER results on the latest OntoNotes
dataset.

nificant (p < 0.01) performance boost on Chinese.
This method also improves the recall on English,
with a smaller decrease in precision. Overall, it im-
proves English F1 score by about 0.4%, which is un-
fortunately not statistically significant.

Compared with Soft-tag, the final
Soft-align method can further improve
performance on both Chinese and English. This is
likely to be because: 1) Soft-align includes
more word alignment pairs, thereby improving
recall; and 2) uses probabilities to cut wrong
word alignments, thereby improving precision. In
particular, compared with the strong CRF baseline,
the gain on Chinese side is almost 5.5% in absolute
F1 score.

Decoding/inferenc efficiency of different methods
are shown in the last column of Table 2.12 Com-
pared with Viterbi decoding in CRF, monolingual
ILP decoding is about 2.3 times slower. Bilingual
ILP decoding, with either hard or soft constraints, is
significantly slower than the monolingual methods.
The reason is that the number of monolingual ILP
constraints doubles, and there are additionally many
more bilingual constraints. The difference in speed
between the Soft-tag and Soft-align meth-
ods is attributed to the difference in number of word
alignment pairs.

Since each sentence pair can be decoded indepen-
12CPU: Intel Xeon E5-2660 2.20GHz. And the speed cal-

culation of ILP inference methods exclude the time needed to
obtain marginal probabilities from the CRF models.

58



Chinese English Speed
P R F1 P R F1 #sent/s

CRF (No Cluster) 74.74 56.17 64.13 – – – –
CRF (Word Cluster) 76.90 63.32 69.45 82.95 76.67 79.68 317.3
Monolingual ILP 76.20 63.06 69.01 82.88 76.68 79.66 138.0
Hard 74.38 65.78 69.82 82.66 75.36 78.84 21.1
Soft-tag (Auto) 77.37 71.14 74.13 81.36 78.74 80.03 5.9
Soft-align (Auto) 77.71 72.51 75.02 81.94 78.35 80.10 1.5

Table 2: Results on bilingual parallel test set.

dently, parallelization the decoding process can re-
sult in significant speedup.

7 Semi-supervised NER Results

The above results show the usefulness of our method
in a bilingual setting, where we are presented with
sentence aligned data, and are tagging both lan-
guages at the same time. To have a greater impact
on general monolingual NER systems, we employ
a semi-supervised learning setting. First, we tag a
large amount of unannotated bitext with our bilin-
gual constraint-based NER tagger. Then we mix the
automatically tagged results with the original mono-
lingual Chinese training data to train a new model.

Our bitext is derived from the Chinese-English
part of the Foreign Broadcast Information Service
corpus (FBIS, LDC2003E14). The best perform-
ing bilingual model Soft-align with threshold
θ = 0.1 was used under the same experimental set-
ting as described in Section 4

Method #sent P R F1
CRF ∼16k 76.90 63.32 69.45

Semi

10k 77.60 66.51 71.62
20k 77.28 67.26 71.92
40k 77.40 67.81 72.29
80k 77.44 68.64 72.77

Table 3: Semi-supervised results on Chinese test set.

Table 3 shows that the performance of the semi-
supervised method improves with more additional
data. We simply appended these data to the orig-
inal training data. We also have done the experi-
ments to down weight the additional training data
by duplicating the original training data. There
was some slight improvements, but not very signif-
icant. Finally, when we add 80k sentences, the F1

score is improved by 3.32%, which is significantly
(p < 0.01) better than the baseline, and most of the
contribution comes from recall improvement.

Before the end of experimental section, let us
summarize the usage of different kinds of data re-
sources used in our experiments, as shown in Ta-
ble 4, where � and × denote whether the corre-
sponding resources are required. In the bilingual
case, during training, only the monolingual named
entity annotated data (NE-mono) is necessary to
train a monolingual NER tagger. During the test,
unannotated bitext (Bitext) is required by the word
aligner and our bilingual NER tagger. Named entity
annotated bitext (NE-bitext) is used to evaluate our
bilingual model. In the semi-supervised case, be-
sides the original NE-mono data, the Bitext is used
as input to our bilingual NER tagger to product ad-
ditional training data. To evaluate the final NER
model, only NE-mono is needed.

NE-mono Bitext NE-bitext

Bilingual train � × ×test × � �
Semi train � � ×test � × ×

Table 4: Summarization of the data resource usage

8 Related Work

Previous work explored the use of bilingual corpora
to improve existing monolingual analyzers. Huang
et al. (2009) proposed methods to improve parsing
performance using bilingual parallel corpus. Li et
al. (2012) jointly labeled bilingual named entities
with a cyclic CRF model, where approximate in-
ference was done using loopy belief propagation.
These methods require manually annotated bilingual

59



corpora, which are expensive to construct, and hard
to obtain. Kim et al. (2012) proposed a method of
labeling bilingual corpora with named entity labels
automatically based on Wikipedia. However, this
method is restricted to topics covered by Wikipedia.

Similar to our work, Burkett et al. (2010) also as-
sumed that annotated bilingual corpora are scarce.
Beyond the difference discussed in Section 1, their
re-ranking strategy may lose the correct named en-
tity results if they are not included in the top-N out-
puts. Furthermore, we consider the word alignment
probabilities in our method which can reduce the in-
fluence of word alignment errors. Finally, we test
our method on a large standard publicly available
corpus (8,249 sentences), while they used a much
smaller (200 sentences) manually annotated bilin-
gual NER corpus for results validation.

In addition to bilingual corpora, bilingual dictio-
naries are also useful resources. Huang and Vo-
gel (2002) and Chen et al. (2010) proposed ap-
proaches for extracting bilingual named entity pairs
from unannotated bitext, in which verification is
based on bilingual named entity dictionaries. How-
ever, large-scale bilingual named entity dictionaries
are difficult to obtain for most language pairs.

Yarowsky and Ngai (2001) proposed a projection
method that transforms high-quality analysis results
of one language, such as English, into other lan-
guages on the basis of word alignment. Das and
Petrov (2011) applied the above idea to part-of-
speech tagging with a more complex model. Fu et al.
(2011) projected English named entities onto Chi-
nese by carefully designed heuristic rules. Although
this type of method does not require manually an-
notated bilingual corpora or dictionaries, errors in
source language results, wrong word alignments and
inconsistencies between the languages limit applica-
tion of this method.

Constraint-based monolingual methods by using
ILP have been successfully applied to many natural
language processing tasks, such as Semantic Role
Labeling (Punyakanok et al., 2004), Dependency
Parsing (Martins et al., 2009) and Textual Entail-
ment (Berant et al., 2011). Zhuang and Zong (2010)
proposed a joint inference method for bilingual se-
mantic role labeling with ILP. However, their ap-
proach requires training an alignment model with a
manually annotated corpus.

9 Conclusions

We proposed a novel ILP based inference algorithm
with bilingual constraints for NER. This method
can jointly infer bilingual named entities without
using any annotated bilingual corpus. We in-
vestigate various bilingual constraints: hard and
soft constraints. Out empirical study on large-
scale OntoNotes Chinese-English parallel NER data
showed that Soft-align method, which allows
inconsistent named entity tags between two aligned
words and considers word alignment probabilities,
can significantly improve over the performance of
a strong Chinese NER baseline. Our work is the
first to evaluate performance on a large-scale stan-
dard dataset. Finally, we can also improve mono-
lingual Chinese NER performance significantly, by
combining the original monolingual training data
with new data obtained from bitext tagged by our
method. The final ILP-based bilingual NER tag-
ger with soft constraints is publicly available at:
github.com/carfly/bi_ilp

Future work could apply the bilingual constraint-
based method to other tasks, such as part-of-speech
tagging and relation extraction.

Acknowledgments

The authors would like to thank Rob Voigt and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowledge
the support of the National Natural Science Foun-
dation of China (NSFC) via grant 61133012, the
National “863” Project via grant 2011AA01A207
and 2012AA011102, the Ministry of Education Re-
search of Social Sciences Youth funded projects
via grant 12YJCZH304, the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Lan-
guage Translation (BOLT) program through IBM.

Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.

60



References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.

2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610–619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.

Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467–479, December.

David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 46–54, Uppsala, Sweden, July.
Association for Computational Linguistics.

Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010.
On jointly recognizing and aligning bilingual named
entities. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 631–639, Uppsala, Sweden, July. Association
for Computational Linguistics.

Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600–609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 363–370, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.

Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating
chinese named entity data from a parallel corpus. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 264–272, Chiang
Mai, Thailand, November. Asian Federation of Natural
Language Processing.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ’06,
pages 57–60, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, ICMI 2002,
Washington, DC, USA. IEEE Computer Society.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222–1231, Singapore, August. Association for
Computational Linguistics.

Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 694–702, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.

Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595–603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.

Shankar Kumar. 2005. Minimum bayes-risk techniques
in automatic speech recognition and statistical ma-
chine translation. Ph.D. thesis, Baltimore, MD, USA.
AAI3155633.

John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.

Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei
Huang. 2012. Joint bilingual name tagging for paral-
lel corpora. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM 2012), Honolulu, Hawaii, Octo-
ber.

Percy Liang. 2005. Semi-supervised learning for natural
language. Master’s thesis, MIT.

Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342–350, Sun-
tec, Singapore, August. Association for Computational
Linguistics.

Bernard Mérialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20(2):155–
171.

61



Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 337–342, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.

Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705–713, Cambridge, MA,
October. Association for Computational Linguistics.

Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of Coling
2004, pages 1346–1352, Geneva, Switzerland, Aug
23–Aug 27. COLING.

Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ’05, pages 736–743, New
York, NY, USA. ACM.

David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies, NAACL ’01, pages 1–8, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 304–314, Cambridge,
MA, October. Association for Computational Linguis-
tics.

62


