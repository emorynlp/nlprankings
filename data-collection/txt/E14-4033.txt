



















































Data Driven Language Transfer Hypotheses


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 169–173,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Data Driven Language Transfer Hypotheses

Ben Swanson
Brown University

Providence, RI
chonger@cs.brown.edu

Eugene Charniak
Brown University

Providence, RI
ec@cs.brown.edu

Abstract

Language transfer, the preferential second
language behavior caused by similarities
to the speaker’s native language, requires
considerable expertise to be detected by
humans alone. Our goal in this work is to
replace expert intervention by data-driven
methods wherever possible. We define a
computational methodology that produces
a concise list of lexicalized syntactic pat-
terns that are controlled for redundancy
and ranked by relevancy to language trans-
fer. We demonstrate the ability of our
methodology to detect hundreds of such
candidate patterns from currently available
data sources, and validate the quality of
the proposed patterns through classifica-
tion experiments.

1 Introduction

The fact that students with different native lan-
guage backgrounds express themselves differ-
ently in second language writing samples has
been established experimentally many times over
(Tetreault et al., 2013), and is intuitive to most
people with experience learning a new language.
The exposure and understanding of this process
could potentially enable the creation of second
language (L2) instruction that is tailored to the na-
tive language (L1) of students.

The detectable connection between L1 and L2
text comes from a range of sources. On one end of
the spectrum are factors such as geographic or cul-
tural preference in word choice, which are a pow-
erful L1 indicator. On the other end lie linguistic
phenomena such as language transfer, in which the
preferential over-use or under-use of structures in

the L1 is reflected in the use of corresponding pat-
terns in the L2. We focus on language transfer in
this work, based on our opinion that such effects
are more deeply connected to and effectively uti-
lized in language education.

The inherent challenge is that viable language
transfer hypotheses are naturally difficult to con-
struct. By the requirement of contrasting different
L1 groups, hypothesis formulation requires deep
knowledge of multiple languages, an ability re-
served primarily for highly trained academic lin-
guists. Furthermore, the sparsity of any particular
language pattern in a large corpus makes it diffi-
cult even for a capable multilingual scholar to de-
tect the few patterns that evidence language trans-
fer. This motivates data driven methods for hy-
pothesis formulation.

We approach this as a representational problem,
requiring the careful definition of a class of lin-
guistic features whose usage frequency can be de-
termined for each L1 background in both L1 and
L2 text (e.g. both German and English written
by Germans). We claim that a feature exhibiting
a sufficiently non-uniform usage histogram in L1
that is mirrored in L2 data is a strong language
transfer candidate, and provide a quantified mea-
sure of this property.

We represent both L1 and L2 sentences in a
universal constituent-style syntactic format and
model language transfer hypotheses with con-
tiguous syntax sub-structures commonly known
as Tree Substitution Grammar (TSG) fragments
(Post and Gildea, 2009)(Cohn and Blunsom,
2010). With these features we produce a concise
ranked list of candidate language transfer hypothe-
ses and their usage statistics that can be automati-
cally augmented as increasing amounts of data be-
come available.

169



2 Related Work

This work leverages several recently released data
sets and analysis techniques, with the primary
contribution being the transformations necessary
to combine these disparate efforts. Our analy-
sis methods are closely tied to those described
in Swanson and Charniak (2013), which con-
trasts techniques for the discovery of discrimina-
tive TSG fragments in L2 text. We modify and
extend these methods to apply to the universal de-
pendency treebanks of McDonald et al. (2013),
which we will refer to below to as the UTB. Bilin-
gual lexicon construction (Haghighi et al., 2008)
is also a key component, although previous work
has focused primarily on nouns while we focus on
stopwords. We also transform the UTB into con-
stituent format, in a manner inspired by Carroll
and Charniak (1992).

There is a large amount of related research in
Native Language Identification (NLI), the task of
predicting L1 given L2 text. This work has culmi-
nated in a well attended shared task (Tetreault et
al., 2013), whose cited report contains an excellent
survey of the history of this task. In NLI, however,
L1 data is not traditionally used, and patterns are
learned directly from L2 text that has been anno-
tated with L1 labels. One notable outlier is Brooke
and Hirst (2012), which attempts NLI using only
L1 data for training using large online dictionar-
ies to tie L2 English bigrams and collocations to
possible direct translations from native languages.
Jarvis and Crossley (2012) presents another set of
studies that use NLI as a method to form language
transfer hypotheses.

3 Methodology

The first of the four basic requirements of our pro-
posed method is the definition of a class of features
F such that a single feature F ∈ F is capable
of capturing language transfer phenomenon. The
second is a universal representation of both L1 and
L2 data that allows us to count the occurrences of
any F in an arbitrary sentence. Third, as any suf-
ficiently expressive F is likely to be very large, a
method is required to propose an initial candidate
list C ⊂ F . Finally, we refine C into a ranked list
H of language transfer hypotheses, where H has
also been filtered to remove redundancy.

In this work we define F to be the set of Tree
Substitution Grammar (TSG) fragments in our
data, which allows any connected syntactic struc-

ture to be used as a feature. As such, our universal
representation of L1/L2 data must be a constituent
tree structure of the general form used in syntactic
parsing experiments on the Penn Treebank. The
UTB gets us most of the way to our goal, defining
a dependency grammar with a universal set of part
of speech (POS) tags and dependency arc labels.

Two barriers remain to the use of standard TSG
induction algorithms. The first is to define a map-
ping from the dependency tree format to con-
stituency format. We use the following depen-
dency tree to illustrate our transformation.

ROOT DT NN VBZ PRP
The poodle chews it

root

det nsubj dobj

Under our transformation, the above dependency
parse becomes

ROOT

root

VBZ-L

nsubj

NN-L

det

DT

the

NN

poodle

VBZ

chews

VBZ-R

dobj

PRP

it

We also require a multilingual lexicon in the form
of a function ML(w) for each language L that
maps words to clusters representing their meaning.
In order to avoid cultural cues and reduce noise
in our mapping, we restrict ourselves to clusters
that correspond to a list of L2 stopwords. Any L2
words that do not appear on this list are mapped
to the unknown “UNK” symbol, as are all for-
eign words that are not good translations of any
L2 stopword. Multiple words from a single lan-
guage can map to the same cluster, and it is worth
noting that this is true for L2 stopwords as well.

To determine the mapping functions ML we
train IBM translation models in both directions be-
tween the L2 and each L1. We create a graph in
which nodes are words, either the L2 stopwords or
any L1 word with some translation probability to

170



or from one of the L2 stopwords. The edges in this
graph exist only between L2 and L1 words, and
are directed with weight equal to the IBM model’s
translation probability of the edge’s target given
its source. We construct ML by removing edges
with weight below some threshold and calculating
the connected components of the resulting graph.
We then discard any cluster that does not contain
at least one word from each L1 and at least one L2
stopword.

To propose a candidate list C, we use the TSG
induction technique described in Swanson and
Charniak (2013), which simultaneously induces
multiple TSGs from data that has been partitioned
into labeled types. This method permits linguisti-
cally motivated constraints as to which grammars
produce each type of data. For an experimental
setup that considers n different L1s, we use 2n+1
data types; Figure 1 shows the exact layout used
in our experiments. Besides the necessary n data
types for each L1 in its actual native language form
and n in L2 form, we also include L2 data from
L2 native speakers. We also define 2n + 1 gram-
mars. We begin with n grammars that can each
be used exclusively by one native language data
type, representing behavior that is unique to each
native language (grammars A-C in Figure 1) . This
is done for the L2 as well (grammar G). Finally,
we create an interlanguage grammar for each of
our L1 types that can be used in derivation of both
L1 and L2 data produced by speakers of that L1
(grammars D-F).

The final step is to filter and rank the TSG frag-
ments produced in C, where filtering removes re-
dundant features and ranking provides some quan-
tification of our confidence in a feature as a lan-
guage transfer hypothesis. Swanson and Char-
niak (2013) provides a similar method for pure L2
data, which we modify for our purposes. For re-
dundancy filtering no change is necessary, and we
use their recommended Symmetric Uncertainty
method. For a ranking metric of how well a frag-
ment fits the profile of language transfer we adopt
the expected per feature loss (or risk) also de-
scribed in their work. For an arbitrary feature F ,
this is defined as

R(F ) = 1|TF |
∑
t∈TF

PF (L 6= L∗t )

where TF is the subset of the test data that contains
the feature F , and L∗t is the gold label of test da-

L2
Data

L1
Data

DE DE

FR FR

ES ES

EN

A

B

C

D

E

F

G

Figure 1: The multi-grammar induction setup used
in our experiments. Squares indicate data types,
and circles indicate grammars. Data type labels
indicate the native language of the speaker, and all
L2 data is in English.

tum t. While in their work the predictive distribu-
tion PF (L) is determined by the observed counts
of F in L2 training data, we take our estimates
directly from the L1 data of the languages under
study. This metric captures the extent to which the
knowledge of a feature F ’s L1 usage can be used
to predict its usage in L2.

The final result is a ranked and filtered list of hy-
potheses H . The elements of H can be subjected
to further investigation by experts and the accom-
panying histogram of counts contains the relevant
empirical evidence. As more data is added, the
uncertainty in the relative proportions of these his-
tograms and their corresponding R is decreased.
One additional benefit of our method is that TSG
induction is a random process, and repeated runs
of the sampling algorithm can produce different
features. Since redundancy is filtered automati-
cally, these different feature lists can be combined
and processed to potentially find additional fea-
tures given more computing time.

4 Results

Limited by the intersection of languages across
data sets, we take French, Spanish, and German
as our set of L1s with English as the L2. We use
the UTB for our native language data, which pro-
vides around 4000 sentences of human annotated
text for each L1. For our L2 data we use the ETS
Corpus of Non-Native English (Blanchard et al.,
2013), which consists of over 10K sentences per
L1 label drawn from TOEFLr exam essays. Fi-

171



nally, we use the Penn Treebank as our source of
native English data, for a total of seven data types;
four in English, and one in each L1.

When calculating metrics such as redundancy
and R(F ) we use all available data. For TSG
sampling, we balance our data sets to 4000 sen-
tences from each data type and sample using the
Enbuske sampler that was released with Swanson
and Charniak (2013). To construct word clusters,
we use Giza++ (Och and Ney, 2003) and train on
the Europarl data set (Koehn, 2005), using .25 as
a threshold for construction on connected compo-
nents.

We encourage the reader to peruse the full list
of results1, in which each item contains the infor-
mation in the following example.

advcl

VERB-L

mark

VERB

110

VERB-R

ES DE FR

L1 4.2 0.0 0.0
L2 2.3 0.3 0.3

This fragment corresponds to an adverbial
clause whose head is a verb in the cluster 110,
which contains the English word “is” and its vari-
ous translations. This verb has a single left depen-
dent, a clause marker such as “because”, and at
least one right dependent. Its prevalence in Span-
ish can explained by examining the translations of
the English sentence “I like it because it is red”.

ES Me gusta porque es rojo.
DE Ich mag es, weil es rot ist.
FR Je l’aime parce qu’il est rouge.

Only in the Spanish sentence is the last pronoun
dropped, as in “I like it because is red”. This
observation, along with the L1/L2 profile which
shows the count per thousand sentences in each
language provides a strong argument that this pat-
tern is indeed a form of language transfer.

Given our setup of three native languages, a fea-
ture with R(F ) < .66 is a candidate for language
transfer. However, several members of our filtered
list have R(F ) > .66, which is to say that their

1bllip.cs.brown.edu/download/interlanguage corpus.pdf

 0.34

 0.36

 0.38

 0.4

 0.42

 0.44

 0  10  20  30  40  50  60  70  80  90

C
la

ss
if

ic
at

io
n

 A
cc

u
ra

cy
 (

%
)

Sentences Per Test Case

Figure 2: Creating test cases that consist of sev-
eral sentences mediates feature sparsity, providing
clear evidence for the discriminative power of the
chosen feature set.

L2 usage does not mirror L1 usage. This is to be
expected in some cases due to noise, but it raises
the concern that our features withR(F ) < .66 are
also the result of noise in the data. To address this,
we apply our features to the task of cross language
NLI using only L1 data for training. If the varia-
tion ofR(F ) around chance is simply due to noise
then we would expect near chance (33%) classifi-
cation accuracy. The leftmost point in Figure 2
shows the initial result, using boolean features in
a log-linear classification model, where a test case
involves guessing an L1 label for each individual
sentence in the L2 corpus. While the accuracy
does exceed chance, the margin is not very large.

One possible explanation for this small margin
is that the language transfer signal is sparse, as it
is likely that language transfer can only be used to
correctly label a subset of L2 data. We test this by
combining randomly sampled L2 sentences with
the same L1 label, as shown along the horizontal
axis of Figure 2. As the number of sentences used
to create each test case is increased, we see an in-
crease in accuracy that supports the argument for
sparsity; if the features were simply weak predic-
tors, this curve would be flat. The resulting margin
is much larger, providing evidence that a signifi-
cant portion of our features with R(F ) < .66 are
not selected due to random noise in R and are in-
deed connected to language transfer.

The number and strength of these hypotheses is
easily augmented with more data, as is the number
of languages under consideration. Our results also
motivate future work towards automatic genera-
tion of L1 targeted language education exercises,
and the fact that TSG fragments are a component
of a well studied generative language model makes
them well suited to such generation tasks.

172



References

Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013. Toefl11:
A corpus of non-native english. Technical report,
Educational Testing Service.

Julian Brooke and Graeme Hirst. 2012. Measur-
ing Interlanguage: Native Language Identification
with L1-influence Metrics. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation (LREC-2012), pages 779–
784, Istanbul, Turkey, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L12-1016.

Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Technical Report CS-92-16,
Brown University, Providence, RI, USA.

Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225–230. Association for Computational Linguis-
tics.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771–779.

Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach, volume 64. Multilingual Matters Limited,
Bristol, UK.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT Summit.

Ryan T. McDonald, Joakim Nivre, Yvonne
Quirmbach-Brundage, Yoav Goldberg, Dipan-
jan Das, Kuzman Ganchev, Keith Hall, Slav Petrov,
Hao Zhang, Oscar Täckström, Claudia Bedini,
Núria Bertomeu Castelló, and Jungmee Lee. 2013.
Universal dependency annotation for multilingual
parsing. In ACL (2), pages 92–97.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.

Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45–48. Association for Computational Linguistics.

Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 85–94, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.

Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, Atlanta, GA, USA, June.
Association for Computational Linguistics.

173


