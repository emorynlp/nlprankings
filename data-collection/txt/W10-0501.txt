










































The "Nays" Have It: Exploring Effects of Sentiment in Collaborative Knowledge Sharing


Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 1–2,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

The “Nays” Have It: Exploring Effects of Sentiment in Collaborative
Knowledge Sharing

Ablimit Aji , Eugene Agichtein
Mathematics & Computer Science Department

Emory University
{aaji,eugene}@mathcs.emory.edu

Abstract

In this paper we study what effects sentiment
have on the temporal dynamics of user inter-
action and content generation in a knowledge
sharing setting. We try to identify how senti-
ment influences interaction dynamics in terms
of answer arrival, user ratings arrival, commu-
nity agreement and content popularity. Our
study suggests that “Negativity Bias” triggers
more community attention and consequently
more content contribution. Our findings pro-
vide insight into how users interact in online
knowledge sharing communities, and helpful
for improving existing systems.

1 Introduction

Recently, Collaborative Knowledge Sharing sites( or
CQA sites), such as Naver and Yahoo! Answers
have exploded in popularity. Already, for many in-
formation needs, these sites are becoming valuable
alternatives to search engines. Previous studies iden-
tified visibility as an important factor for content
popularity and developed models in static settings.
However, when users post social media content, they
might either explicitly or implicitly express their
personal attitudes or sentiment. The following ex-
ample illustrates a question with negative sentiment.

Q :Obama keeps saying we need to sacrifice.
What sacrifices has he and the gov made collec-
tively and individually?
A1: Our hard earned tax dollars. 17 ↑, 2 ↓
A2: None and they never will. 18 ↑, 2 ↓

Psychological studies (Smith et al., 2008) suggest
that our brain has “Negativity Bias” - that is, people
automatically devote more attention to negative in-
formation than to positive information. Thus, our
attitudes may be more heavily influenced by nega-
tive opinions. Our hypothesis is that this kind of hu-
man cognitive bias would have measurable effects
on how users respond to information need in CQA

communities. Our goal in this paper is to under-
stand how question sentiment influence the dynam-
ics of the user interactions in CQA - that is, to un-
derstand how users respond to questions of different
sentiment, how question sentiment affects commu-
nity agreement on best answer and question popu-
larity.

2 Sentiment Influence
While (Aji et al., 2010) suggests that question cat-
egory has a patent influence on interaction dynam-
ics, we mainly focus on sentiment in this exploratory
study, for the reason that sentiment is a high level
but prominent facet in every piece of content. We
focused on how may sentiment effect the following
dimensions:
• Answer Arrival: Measured as number of an-

swers arrived every minute.
• Vote Arrival: Measured as number of votes ar-

rived per answer.
• Community Agreement: Mean Reciprocal Rank

(MRR), computed by ranking the answers in or-
der of decreasing “Thumbs up” ratings, and iden-
tifying the rank of the actual “best” answer, as se-
lected by the asker.

MRR =
1

|Q|

N∑
i=1

1

ranki
(1)

where ranki is the rank of the best answer among
the answers submitted for question i.

• Answer Length, Question Length: We examine
whether questions with different sentiment exhibit
variations in question and answer length.

• Interest “Stars”: How many users marked ques-
tion as interesting.

3 Dataset Description
For our study we tracked a total of approximately
10,000 questions, sampled from 20 categories from
Yahoo! Answers. Specifically, each new question in
our tracking list crawled every five minutes until it’s
closed. As a result, we obtained approximately 22

1



million question-answer-feedback snapshots in to-
tal. Since labeling all the questions would be ex-
pensive, we randomly selected 2000 questions from
this dataset for human labeling. We then utilized the
Amazon Mechanical Turk Service1. Five workers
labeled each question as either positive, negative or
neutral; the ratings were filtered by using majority
opinion (at least 3 out of 5 labels). Overall statistics
of this dataset are reported in Table 1. The overall
inter-rater agreement was 65%.

Positive Negative Neutral Total
379 173 548 1,100

Table 1: Statistics of the Temporal dataset
.4 Results and Discussion

Figure 1 reports answer arrival dynamics for ques-
tion with varying sentiment. Answers to negative
questions arrive substantially faster than answers to
positive or neutral questions, whereas the difference
between positive and neutral questions are minor.
This strongly confirms the “Negative Bias” effect.
Given the fact that questions stay in the category
front page relatively same amount of time where
their visibility contributes potential answers, on av-
erage, negative sentiment questions managed to get
more answers than two other types of questions (4.3
vs. 3.3 and 3.5). It seems, sentiment expressed in a
question contributes to the answer arrival more than
visibility.

 0
 0.5

 1
 1.5

 2
 2.5

 3
 3.5

 4
 4.5

 1  10  100  1000

A
n
s
w
e
r
s

time (in minutes)

negative sentiment

neutral sentiment

positive sentiment

Figure 1: Cumulative answer arrival
Figure 2 reports rating arrival dynamics. Interest-

ingly, positive ratings arrive much faster to negative
questions, whereas positive and negative ratings ar-
rive roughly at the same rate for positive and neutral
questions. While this might be partially due to the
fact that negative sentiment questions are more “at-
tention grabbing” than other types of questions, we
conjecture that this effect is caused by the selection
bias of the raters participating in negative question
threads, who tend to support answers that strongly

1http://www.mturk.com

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 1  10  100  1000

R
a
t
i
n
g
s

time (in minutes)

negative +
neutral +

positive +
negative -

neutral -
positive -

Figure 2: Cumulative user ratings arrival

agree (or strongly disagree) with the question asker.
Surprisingly, community agreement(MRR) on the

Type MRR QLength ALength Stars
Negative 0.47 78 49 0.25
Positive 0.56 58 52 0.16
Neutral 0.57 52 47 0.15

Table 2: Agreement, Question length, Answer Length
and Star count averaged over question type

best answer is lower for negative sentiment ques-
tions. On average, negative sentiment questions
were marked as interesting more than positive or
neutral questions were marked as interesting. Al-
though this may sound counterintuitive, it is not sur-
prising if we recall how the “Negative Bias” influ-
ences user behavior and may increase implicit “visi-
bility”. All the above mentioned differences are sta-
tistically significant(t-test p = 0.05).

In summary, our preliminary exploration indi-
cates that sentiment may have a powerful effect on
the content contribution dynamics in collaborative
question answering, and is a promising direction for
further study of knowledge sharing communities.
Acknowledgments
We thank HP Social Computing Labs for support.

References
Ablimit Aji. Eugene Agichtein. 2010. Deconstructing

Interaction Dynamics in Knowledge Sharing Commu-
nities. International Conference on Social Computing,
Behavioral Modeling, & Prediction.

Gabor Szabo. Bernardo Huberman. 2008. Predicting the
popularity of online content. HP Labs Technical Re-
port.

Kristina Lerman. 2007. Social Information Processing
in Social News Aggregation. IEEE Internet Comput-
ing: Special Issue on Social Search.

N. Kyle Smith Jeff T. Larsen Tanya L. Chartrand John
T. Cacioppo 2006. Affective Context Moderates the
Attention Bias Toward Negative Information. Journal
of Personality and Social Psychology.

2


