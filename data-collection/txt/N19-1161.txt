















































Density Matching for Bilingual Word Embedding


Proceedings of NAACL-HLT 2019, pages 1588–1598
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1588

Density Matching for Bilingual Word Embedding

Chunting Zhou, Xuezhe Ma, Di Wang, Graham Neubig
Language Technologies Institute

Carnegie Mellon University
ctzhou,xuezhem,diwang,gneubig@cs.cmu.edu

Abstract

Recent approaches to cross-lingual word em-
bedding have generally been based on lin-
ear transformations between the sets of em-
bedding vectors in the two languages. In
this paper, we propose an approach that in-
stead expresses the two monolingual embed-
ding spaces as probability densities defined by
a Gaussian mixture model, and matches the
two densities using a method called normal-
izing flow. The method requires no explicit
supervision, and can be learned with only a
seed dictionary of words that have identical
strings. We argue that this formulation has
several intuitively attractive properties, partic-
ularly with the respect to improving robust-
ness and generalization to mappings between
difficult language pairs or word pairs. On a
benchmark data set of bilingual lexicon in-
duction and cross-lingual word similarity, our
approach can achieve competitive or superior
performance compared to state-of-the-art pub-
lished results, with particularly strong results
being found on etymologically distant and/or
morphologically rich languages.1

1 Introduction

Cross-lingual word embeddings represent words
in different languages in a single vector space,
capturing the syntactic and semantic similarity of
words across languages in a way conducive to use
in computational models (Upadhyay et al., 2016;
Ruder et al., 2017). These embeddings have been
shown to be an effective tool for cross-lingual
NLP, e.g. the transfer of models trained on high-
resource languages to low-resource ones (Klemen-
tiev et al., 2012; Guo et al., 2015; Zoph et al.,
2016; Zhang et al., 2018; Gu et al., 2018) or un-
supervised learning (Artetxe et al., 2018c).

1Code/scripts can be found at https://github.
com/violet-zct/DeMa-BWE.

dog

cat

canine

⽝犬

猫
⿃鳥

mapping function

bird

Japanese Space English Space

Figure 1: An illustration of our method. Thicker lines
represent higher mixture weights, linked to word fre-
quency. The pink point is a continuous training sample
in the Japanese embedding space while the blue point
is a mapped point in the English space.

There are two major paradigms in the learn-
ing of cross-lingual word embeddings: “online”
and “offline”. “Online” methods learn the cross-
lingual embeddings directly from parallel corpora
(Hermann and Blunsom, 2014), optionally aug-
mented with monolingual corpora (Gouws et al.,
2015). In contrast, “offline” approaches learn a
bilingual mapping function or multilingual pro-
jections from pre-trained monolingual word em-
beddings or feature vectors (Haghighi et al., 2008;
Mikolov et al., 2013; Faruqui and Dyer, 2014). In
this work, we focus on this latter offline approach.

The goal of bilingual embedding is to learn a
shared embedding space where words possessing
similar meanings are projected to nearby points.
Early work focused on supervised methods maxi-
mizes the similarity of the embeddings of words
that exist in a manually-created dictionary, ac-
cording to some similarity metric (Mikolov et al.,
2013; Faruqui and Dyer, 2014; Jawanpuria et al.,
2018; Joulin et al., 2018). In contrast, recently
proposed unsupervised methods frame this prob-
lem as minimization of some form of distance be-
tween the whole set of discrete word vectors in
the chosen vocabulary, e.g. Wasserstein distance
or Jensen–Shannon divergence (Xu et al., 2018;
Conneau et al., 2017; Zhang et al., 2017; Grave



1589

et al., 2018). While these methods have shown
impressive results for some language pairs despite
the lack of supervision, regarding the embedding
space as a set of discrete points has some limi-
tations. First, expressing embeddings as a single
point in the space doesn’t take into account the
inherent uncertainty involved in learning embed-
dings, which can cause embedding spaces to dif-
fer significantly between training runs (Wendlandt
et al., 2018). Second, even in a fixed embedding
space the points surrounding those of words that
actually exist in the pre-trained vocabulary also of-
ten are coherent points in the embedding space.

In this work, we propose a method for den-
sity matching for bilingual word embedding
(DeMa-BWE). Instead of treating the embedding
space as a collection of discrete points, we express
it as a probability density function over the entire
continuous space over word vectors. We assume
each vector in the monolingual embedding space
is generated from a Gaussian mixture model with
components centered at the pretrained word em-
beddings (Fig. 1), and our approach then learns
a bilingual mapping that most effectively matches
the two probability densities of the two monolin-
gual embedding spaces.

To learn in this paradigm, instead of using
the pre-trained word embeddings as fixed train-
ing samples, at every training step we obtain sam-
ples from the Gaussian mixture space. Thus, our
method is exploring the entire embedding space
instead of only the specific points assigned for
observed words. To calculate the density of the
transformed samples, we use volume-preserving
invertible transformations over the target word
embeddings, which make it possible to perform
density matching in a principled and efficient
way (Rezende and Mohamed, 2015; Papamakar-
ios et al., 2017; He et al., 2018). We also
have three additional ingredients in the model
that proved useful in stabilizing training: (1) a
back-translation loss to allow the model to learn
the mapping jointly in both directions, (2) an
identical-word-matching loss that provides weak
supervision by encouraging the model to have
words with identical spellings be mapped to a sim-
ilar place in the space, and (3) frequency-matching
based Gaussian mixture weights that accounts for
the approximate frequencies of aligned words.

Empirical results are strong; our method is
able to effectively learn bilingual embeddings

that achieve competitive or superior results on
the MUSE dataset (Conneau et al., 2017) over
state-of-the-art published results on bilingual word
translation and cross-lingual word similarity tasks.
The results are particularly encouraging on ety-
mologically distant or morphologically rich lan-
guages, as our model is able to explore the inte-
gration over the embedding space by treating the
space as a continuous one. Moreover, unlike pre-
vious unsupervised methods that are usually sen-
sitive to initialization or require sophisticated op-
timization procedures, our method is robust and
requires no special initialization.

2 Background: Normalizing Flows

In this section, we will briefly describe normaliz-
ing flows - the backbone of DeMa-BWE.

As mentioned in the introduction and detailed
later, our model is based on matching two prob-
ability density functions, one representing the
source embedding space and one representing the
target embedding space. To learn in this frame-
work, we will use the concept of normalizing flows
(Rezende and Mohamed, 2015). We will explain
them briefly here, but refer readers to Rezende
and Mohamed (2015) for details due to space con-
straints.

Concretely, let u denote a high dimensional
random vector (e.g. representing a point in the
source embedding space) and z be a latent variable
that corresponds to u (e.g. a point in the target
embedding space). Flow-based generative mod-
els (Kingma et al., 2016; Kingma and Dhariwal,
2018) learn invertible transformations fθ(z) from
the distribution over z to the distribution over u.
The generative story of the model is defined as:

z ∼ pθ(z), u = fθ(z) (1)

where pθ(z) is the prior distribution. This prior
can be any distribution for which we can tractably
compute the density of sample points z. A
common choice of such distribution is a spheri-
cal multivariate Gaussian distribution: pθ(z) =
N (z; 0, I) (Dinh et al., 2016). Assuming the trans-
formation function fθ(·) is invertible, using the
rule for change of variables, the probability den-
sity of u can be calculated as:

pθ(u) = pθ(z)|det(J(f−1(u)))| (2)

where det(J(f−1(u))) is determinant of the Ja-
cobian matrix of the function inverse. This term



1590

accounts for the way in which f locally expands
or contracts regions of z, and enforces the invert-
ibility of the function. A “normalizing flow” is a
cascaded sequence of such invertible transforma-
tions, which is learned by maximizing the density
in Equation (2) over observed data points u. One
computational issue with these models lies in cal-
culating the Jacobian matrix, which is expensive in
the general case. A common method is to choose
transformations whose Jacobians’ are a triangular
matrix, which renders this computation tractable
(Dinh et al., 2016; Papamakarios et al., 2017).

3 Proposed Method

In this section, we present notation used in our
method, describe the prior we define for the mono-
lingual embedding space, then detail our density
matching method.

3.1 Notation

Given two sets of independently trained monolin-
gual embeddings, the problem of bilingual em-
bedding mapping is to learn a mapping function
that aligns the two sets in a shared space. Let
x ∈ Rd,y ∈ Rd denote vectors in the source and
target language embedding space respectively. Let
xi and yj denote an actual word in the pretrained
source and target vocabularies respectively. Words
are sorted by their occurrence counts in the mono-
lingual corpus and the index of the word repre-
sents its rank. We use xi and yj to denote the pre-
trained word embeddings for word xi in the source
language and word yj in the target language re-
spectively. Given a pair of languages s and t, our
approach learns two mapping functions: fxy that
maps the source embedding to the target space and
fyx that gives the mapping in the reverse direction.

3.2 Density Estimation in Monolingual Space

To learn the mapping, we project a vector x in the
source embedding space into the target space y.
We learn this mapping by maximizing the density
of data points in the source space. The density can
be computed using the idea of normalizing flow
described above. Thus, for the the monolingual
embedding spaces, we need to define tractable
density functions p(x) and p(y).

While any number of functions could be con-
ceived to calculate these densities, in the cur-
rent method we opt to use a Gaussian Mixture
Model (GMM) with Gaussian components cen-

tered at each pretrained word embedding. This is
motivated by the assumption that embeddings are
likely to appear in the neighborhood of other em-
beddings, where we define the “neighborhood” to
be characterized as closeness in Euclidean space,
and the uncertainty of each neighborhood as be-
ing Gaussian. Concretely, let Nx and Ny denote
the number of pretrained word embeddings that
serve as Gaussian component centers during train-
ing for the source and target languages, respec-
tively. Then we can express the density of any
point in the source embedding space as:

p(x) =
∑

i∈{1,...,Nx}

π(xi)p̃(x|xi) (3)

where π(xi) is the frequency of word xi normal-
ized within the Nx component words, and p̃(x|xi)
is a Gaussian distribution centered at the embed-
ding of word xi. We simply use a fixed variance
σ2x for all Gaussian components:

p̃(x|xi) = N (x|xi, σ2xI) (4)

Similarly, the density of any point in the target em-
bedding space can be written as:

p(y) =
∑

j∈{1,...,Ny}

π(yj)p̃(y|yj) (5)

where p̃(y|yj) = N (y|yj , σ2yI).

3.3 Density Matching
With the Gaussian mixture model as the prior dis-
tribution in the monolingual space, our goal is
to learn a mapping function from one embedding
space to the other such that the log probabilistic
density is maximized in the source space.

While we are jointly learning the two mapping
functions fxy and fyx simultaneously, for con-
ciseness we will illustrate our approach using the
source to target mapping fxy. First, a continuous
vector x is sampled from the Gaussian mixture
model (Eq. (3)) by sampling xi ∼ π(xi) then x ∼
p̃(x|xi) (4). Next, we apply the mapping function
fxy to obtain the transformed vector y in the target
space. Concretely, the mapping functions we em-
ploy in this work are two linear transformations:
fxy(·) = Wxy· and fyx = Wyx·. Connecting to
the transformation function in Sec. 2, we see that
x = f(y) = W−1xy y, y = f

−1(x) = Wxyx, and
J(f−1(x)) = Wxy. We can then express the log
density of a sample x as:

log p(x;Wxy) = log p(y)+log
∣∣det(Wxy)∣∣ (6)



1591

where the Jacobian regularization term accounts
for the volume expansion or contraction resulting
from the projection matrix Wxy.

We maximize the likelihood function which is
equivalent to minimizing expectation of the KL-
divergence between the prior distribution and the
model distribution of x. This provides a natural
objective for optimization:

minimize: KL(p(x)||p(x;Wxy)) (7)

By replacing Wxyx with y, this is equivalent to
maximizing the log density of transformed source
samples in the target space (see Eq. (6)):

Lxy = Ex∼p(x)[log p(y) + log
∣∣det(Wxy)∣∣] (8)

The objective Lxy contains two parts: the log den-
sity function log p(y) and a regularization term
log det(Wxy). Likewise, for the target to source
mapping Wyx, we have the density matching ob-
jective Lyx.

Conditional Density Matching The above
marginal density matching method does not take
into account the dependency between the embed-
dings in the two monolingual spaces. To address
this issue, we extend the density matching method
to the conditional density function:

log p(x|xi;Wxy) = log p(y|xi)+log
∣∣det(Wxy)∣∣

The conditional density p(y|xi) is the prior dis-
tribution in this simple normalizing flow, and for
this we use a Gaussian mixture model in the target
monolingual space:

p(y|xi) =
∑

j∈{1,...,Ny}

p(y, yj |xi)

=
∑

j∈{1,...,Ny}

p̃(y|yj)π(yj |xi) (9)

Similarly, where p̃(y|yj) is the Gaussian density
function in the mixture model defined in Equation
(5). π(yj |xi) allows us to incorporate a-priori as-
sumptions about whether two words are likely to
match. In fact, the density matching method in
Eq. (6) can be regarded as a special case of the
conditional density matching method by adopting
a naive prior π(yj |xi) := π(yj). However previ-
ous work (Zhang et al., 2017) has noted that word
frequency information is a strong signal – words
with similar frequency are likely to be matched –

and thus we use π(yj |xi) to incorporate this prior
knowledge.

In this work, we assume that the frequencies of
aligned bilingual words in the individual mono-
lingual corpus should be correlated, and to match
words that are ranked similarly, we model the
Gaussian mixture weights as the negative abso-
lute difference between log-scale word ranks and
normalize over all the target Gaussian component
words by a softmax function with temperature τ :

π(yj |xi) =
exp(−| log(j)− log(i)|)/τ∑Ny
k=1 exp(−| log(k)− log(i)|/τ)

Thus, if a word xi has similarly frequency rank as
word yj , the sample x from the Gaussian distribu-
tion centered at xi will be assigned higher weight
for the component p̃(y|yj) = p̃(Wxyx|yj). Al-
though this assumption will not hold always (e.g.
for languages that have different levels of morpho-
logical complexity), intuitively we expect that us-
ing this signal will help more overall than it will
hurt, and empirically we find that this weighting is
not sensitive to language variation and works well
in practice.

The updated objective is

Lxy = Exi∼π(xi)[KL(p̃(x|xi)||p(x|xi;Wxy))]
= Exi∼π(xi),x∼p̃(x|xi)

[
log p(y|xi) (10)

+ log
∣∣det(Wxy)∣∣]

In the conditional density above, both the
frequency-matching weight and the Gaussian den-
sity function play an important role in matching
the density of a source-space sample with the tar-
get embedding space. The former matches bilin-
gual words with their frequency ranks while the
latter matches words with their vector distances.

3.4 Weak Orthogonality Constraint
A common choice of bilingual mapping func-
tion is a linear transformation with an orthogo-
nality constraint. Various motivations have been
proposed for the orthogonality constraint such as
length normalization of embeddings (Xing et al.,
2015), and reversible mapping (Smith et al.,
2017). In this work, we add a weak orthogonality
constraint to the bilingual mappings via a back-
translation loss as follows:

Lbt = Exi∼π(xi),x∼p̃(x|xi)
[
g(WyxWxyx,x)

]
+ Eyj∼π(yj),y∼p̃(y|xj)

[
g(WxyWyxy,y)

]



1592

where g(·, ·) = 1 − cosine(·, ·) is the cosine loss.
Jointly learning the two mapping matrices by min-
imizing this cyclic loss encourages Wxy and Wyx
to be orthogonal to each other.

3.5 Weak Supervision with Identical Words

To reduce the search space of the mapped bilingual
embeddings, we add an additional weakly super-
vised loss over words that have identical strings in
both the source and target languages denotedWid.

Lsup =
∑
v∈Wid

g(vxW
T
xy,vy) + g(vyW

T
yx,vx)

where vx and vy are the pretrained word embed-
ding of word v in the source and target side re-
spectively, and g(·, ·) is the cosine loss described
above. Although using identical strings for super-
vision is very noisy, especially for languages with
little overlap in vocabularies, we find that they pro-
vide a enough guidance to training to prevent the
model from being trapped in poor local optima.

Putting everything together, the overall objec-
tive function of DeMa-BWE includes three parts:
the density matching loss, the weak orthogonality
loss and the weak supervised loss:

L = Lxy + Lyx + λ · Lbt + α · Lsup (11)

where λ and α are coefficients that tradeoff be-
tween different losses.

4 Retrieval and Refinement

4.1 Retrieval Method

One standard use case for bilingual embeddings
is in bilingual lexicon induction, where the em-
beddings are used to select the most likely trans-
lation in the other language given these embed-
dings. In this case, it is necessary to have a re-
trieval metric that selects word or words likely to
be translations given these embeddings. When
performing this retrieval, it has been noted that
high-dimensional embedding spaces tend to suffer
from the “hubness” problem (Radovanović et al.,
2010) where some vectors (known as hubs) are
nearest neighbors of many other points, which is
detrimental to reliably retrieving translations in the
bilingual space. To mitigate the hubness prob-
lem, we adopt the Cross-Domain Similarity Lo-
cal Scaling (CSLS) metric proposed in (Conneau
et al., 2017) that penalizes the similarity score of

the hubs. Specifically, given two mapped embed-
dings Wxyx denoted x′ and y, CSLS first com-
putes the average cosine similarity of x′ and y
for their k nearest neighbors denoted rT (x′) and
rS(y) in the other language respectively, then the
corrected similarity measure CSLS(·, ·) is defined
as:

CSLS(x′,y) = 2cos(x′,y)− rT (x′)− rS(y)

where cos(·, ·) is the cosine similarity. Following
(Conneau et al., 2017), k is set to be 10.

CSLS consistently outperform cosine similar-
ity on nearest neighbor retrieval, however it does
not consider the relative frequency of bilingual
words which we hypothesize can be useful in dis-
ambiguation. As we discussed in Sec. 3.2, our
density matching objective considers both the rel-
ative frequencies and vector similarities. The con-
ditional density p(y|xi) (Eq. (9)) in our density
matching objective (Eq. (8)) is a marginalized
distribution over all target component words yj
where the density of each component p(y, yj |xi)
can be directly used as a similarity score for a pair
of words (yj , xi) to replace the cosine similarity
cos(x′,y) in CSLS. Let CSLS-D denote this mod-
ified CSLS metric, which we compare to standard
CSLS in experiments. We find that using CSLS-
D for nearest neighbor retrieval outperforms the
CSLS metric in most cases on the bilingual dictio-
nary induction task.

4.2 Iterative Procrustes Refinement
Iterative refinement, which learns the new map-
ping matrix by constructing a bilingual lexicon it-
eratively, has been shown as an effective method
for improving the performance of unsupervised
lexicon induction models (Conneau et al., 2017).
Given a learned bilingual embedding mapping W,
the refinement starts by inferring an initial bilin-
gual dictionary using the retrieval method above
on the most frequent words. Let X and Y denote
the ordered embedding matrices for the inferred
dictionary words for source and target languages
respectively. Then a new mapping matrix W∗ is
induced by solving the Procrustes problem:

W∗ = argmin
W∈Od(R)

||WX−Y||F = UVT

s.t.UΣVT = SVD(YXT )

The step above can be applied iteratively by induc-
ing a new dictionary with the new mapping W.



1593

DeMa-BWE is able to achieve very competi-
tive performance without further refinement, but
for comparison we also report results with the re-
finement procedure, which brings small improve-
ments in accuracy for most language pairs. Note
that for bilingual dictionary induction during re-
finement, we use CSLS as the retrieval metric
across all experiments for fair comparison to the
refinement step in previous work.

5 Experiments

5.1 Dataset and Task

We evaluate our approach extensively on the bilin-
gual lexicon induction (BLI) task, which measures
the word translation accuracy in comparison to a
gold standard. We report results on the widely
used MUSE dataset (Conneau et al., 2017), which
consists of FastText monolingual embeddings pre-
trained on Wikipedia (Bojanowski et al., 2017),
and dictionaries for many language pairs divided
into train and test sets. We follow the evalua-
tion setups of (Conneau et al., 2017). We evalu-
ate DeMa-BWE by inducing lexicons between En-
glish and different languages including related lan-
guages, e.g. Spanish; etymologically distant lan-
guages, e.g. Japanese; and morphologically rich
languages, e.g. Finnish.

5.2 Implementation Details

Embedding Normalization Following (Artetxe
et al., 2018b), we pre-process the monolingual em-
beddings by first applying length normalization,
then mean center each dimension, and then length
normalize again to ensure that the final embed-
dings have a unit length. We observe that this nor-
malization method helps stabilize training and ac-
celerate convergence.

Other Experimental Details We held out 1000
translation pairs randomly sampled from the train-
ing set in the MUSE dataset as our validation data.
We also tried the unsupervised validation criterion
proposed in (Conneau et al., 2017) as the model
selection method that computes the average cosine
similarity over the model induced dictionary pairs
and found that this unsupervised criterion can se-
lect models that achieve similar performance as
the supervised validation criterion. All hyperpa-
rameters are tuned on the validation set and in-
clude the following: For the number of base words
used as Gaussian components in the GMM, we

typically choose the most frequent 20,000 words
for all language pairs but en-ja for which we use
10,000 which achieves better performance. We
use a batch size of 2048 for all languages but en-
ja for which we use 1024. We use Adam (Kingma
and Ba, 2014) for optimization with default hyper-
parameters.

We empirically set the Gaussian variance to be
0.01 for both the source and target languages in
en-es, en-de, en-fr, en-ru; in the experiments for
morphologically rich languages (Sec. 5.4), we set
the variance to be 0.015 for all these languages ex-
cept for et whose variance is set to be 0.02 while
keeping the variance of English to be 0.01. In the
experiments for etymologically distant language
pairs en-ja and en-zh, we set different variances for
the source and target languages in different map-
ping directions. For details of the variance setting
please check the scripts in our code base. We em-
pirically find that for a language pair the variance
of the language with relatively more complex mor-
phological properties needed to be set larger than
the other language, indicating that the model needs
to explore more in the embedding space for the
morphologically richer language.

We initialize mapping matrices Wxy and Wyx
with a random orthogonal matrix. For the weak or-
thogonality constraint loss Lbt, we set the weight
λ to be 0.5 throughout all language pairs. For the
weak supervision loss Lsup, we set the weight α
to be 10 for all languages except for en-zh where
we find 5 performs better. We set the temperature
τ used in the softmax function for Gaussian mix-
ture weights to be 2 across all languages.

5.3 Main Results on BLI

In Tab. 1, we compare the performance of DeMa-
BME extensively with the best performing un-
supervised and supervised methods on the com-
monly benchmarked language pairs.

Our unsupervised baselines are: (1) MUSE
(U+R) (Conneau et al., 2017), a GAN-based un-
supervised method with refinement. (2) A strong
and robust unsupervised self-learning method SL-
unsup from (Artetxe et al., 2018b). We also run
their published code with identical words as the
initial dictionary for fair comparison with our ap-
proach, denoted SL-unsup-ID. (3) Sinkhorn (Xu
et al., 2018) that minimizes the Sinkhorn distance
between the source and target word vectors. (4)
An iterative matching method from (Hoshen and



1594

en-es es-en en-de de-en en-fr fr-en en-ru ru-en en-zh zh-en en-ja ja-en

Supervised

Procrustes (R) 81.4 82.9 73.5 72.4 81.1 82.4 51.7 63.7 42.7 36.7 14.2 7.44
MSF-ISF 79.9 82.1 73.0 72.0 80.4 81.4 50.0 65.3 28.0 40.7 - -
MSF 80.5 83.8 73.5 73.5 80.5 83.1 50.5 67.3 32.3 43.4 - -
CSLS-Sp 80.7 83.9 75.1 72.1 81.7 83.2 51.1 63.8 - - - -
GeoMM 81.4 85.5 74.7 76.7 82.1 84.1 51.3 67.6 49.1 45.3 - -

Unsupervised

MUSE (U+R) 81.7 83.3 74.0 72.2 82.3 81.1 44.0 59.1 32.5 31.4 0.0 4.2
SL-unsup 82.3 84.7 75.1 74.3 82.3 83.6 49.2 65.6 0.0 0.0 2.9 0.2
SL-unsup-ID 82.3 84.6 75.1 74.1 82.2 83.7 48.8 65.7 37.4 34.2 48.5 33.7
Sinkhorn∗ 79.5 77.8 69.3 67.0 77.9 75.5 - - - - - -
Non-Adv 81.1 82.1 73.7 72.7 81.5 81.3 44.4 55.6 0.0 0.0 0.0 0.0
Non-Adv (R) 82.1 84.1 74.7 73.0 82.3 82.9 47.5 61.8 0.0 0.0 0.0 0.0
WS-Procrustes (R) 82.8 84.1 75.4 73.3 82.6 82.9 43.7 59.1 - - - -

DeMa-BME

CSLS (w/o R) 82.0 85.4 75.3 74.9 82.6 82.4 46.9 62.4 39.6 40.0 46.7 32.9
CSLS-D (w/o R) 82.3 85.1 76.3 75.1 83.7 82.5 48.0 61.7 40.5 37.7 45.3 32.4
CSLS (w/ R) 82.8 84.5 75.6 74.1 82.5 83.3 47.3 63.5 41.9 37.7 50.7 35.2
CSLS-D (w/ R) 82.8 84.9 77.2 74.4 83.1 83.5 49.2 63.6 42.5 37.9 52.0 35.6

Table 1: Precision@1 for the MUSE BLI task compared with previous work. All the baseline results employ CSLS
as the retrieval metric except for Sinkhorn∗ which uses cosine similarity. R represents refinement. Bold and italic
indicate the best unsupervised and overall numbers respectively. (’en’ is English, ’es’ is Spanish, ’de’ is German,
’fr’ is French, ’ru’ is Russian, ’zh’ is traditional Chinese, ’ja’ is Japanese.)

Wolf, 2018)): Non-Adv and Non-Adv (R) with
refinement. (5) WS-Procrustes (R) using refine-
ment by (Grave et al., 2018). Our supervised
methods include: (1) The iterative Procrustes
method Procrustes (R) (Smith et al., 2017). (2)
A multi-step framework MSF-ISF (Artetxe et al.,
2018a) and its variant MSF which uses CSLS
for retrieval, whose results are from (Jawanpuria
et al., 2018). (3) CSLS-Sp by (Joulin et al., 2018)
that optimizes the CSLS score, and (4) a geometric
approach GeoMM by (Jawanpuria et al., 2018).
For fair comparisons, all supervised results are
trained with the training dictionaries in the MUSE
dataset. All baseline methods employ CSLS for
retrieval except for the Sinkhorn method.

For DeMa-BME, we present results with and
without refinement, and with CSLS and CSLS-D
as retrieval methods. From Tab. 1, we can see
the overall performance of DeMa-BME is remark-
able comparing with other unsupervised methods
and is also competitive with strong supervised
methods. The results without the iterative refine-
ment CSLS (w/o R) are strong on almost all lan-
guage pairs with particularly strong performance
being observed on es-en, en-de and en-fr on which

DeMa-BME outperforms or is on par with the
best performing methods. Applying refinement to
DeMa-BME brings slightly better performance on
most language pairs but degrades the performance
on some language pairs such as es-en, zh-en for
which the DeMa-BME already obtains very good
results.

Also, DeMa-BME demonstrates notably better
performance on distant language pairs (en-ru, en-
ja and en-zh) over other unsupervised methods,
which often achieve good performance on etymo-
logically close languages but fail to converge on
the distant language pairs. However, when the
dictionary is initialized with identical strings for
SL-unsup, we obtain decent results on these lan-
guages. The strong performance of supervised
methods on Russian and Chinese demonstrates
that on some language pairs supervised seed lexi-
cons are still necessary.

Finally, when our density-based metric CSLS-D
is employed for retrieval, it could achieve further
gains in accuracy for most language pairs com-
pared to its counterpart.

5.4 Morphologically Rich Language Results
Søgaard et al. (2018) found that the commonly



1595

en-et et-en en-fi fi-en en-el el-en en-hu hu-en en-pl pl-en en-tr tr-en

MUSE (U+R) 1.7 0. 1 0.1 59.8 39.1 59.0 50.2 0.1 53.9 0.0 45.4 0.0
5k+Procrustes (R) 31.9 45.6 47.3 59.5 44.6 58.5 53.3 64.8 58.2 66.9 46.3 59.2
id+Procrustes (R) 29.7 40.6 45.0 59.1 40.7 55.1 52.6 63.7 57.3 66.7 45.4 61.4
id+Procrustes (R)∗ 31.5 - 28.0 - 42.9 - 46.6 - 52.6 - 39.2 -

CSLS (w/o R) 32.9 45.3 47.5 58.7 43.6 57.8 55.3 64.5 59.9 69.1 50.3 60.8
CSLS-D (w/o R) 35.9 45.2 49.4 58.5 45.0 58.1 57.6 64.7 60.9 68.3 52.8 61.6
CSLS (w/ R) 34.4 47.8 48.9 60.2 44.5 61.1 55.1 63.9 59.7 69.0 50.3 60.8
CSLS-D (w/ R) 37.0 47.9 50.7 60.8 46.3 61.6 58.3 64.5 61.5 69.1 53.4 61.2

Table 2: BLI Precision (@1) for morphologically complex languages. id+Procrustes (R)∗ is the result reported
in (Søgaard et al., 2018). 5k+Procrustes (R) uses the training dictionary with 5k unique query words. (’et’ is
Estonian, ’fi’ is Finnish, ’el’ is Greek, ’hu’ is Hungarian, ’pl’ is Persian, ’tr’ is Turkish.)

benchmarked languages are morphologically poor
isolating or exclusively concatenating languages.
They select several languages with different mor-
phological traits and complexity then studied the
impacts of language similarities on the bilingual
lexicon induction.

They show that a simple trick, harvesting the
identical word strings in both languages as an ini-
tial dictionary and running the iterative Procrustes
analysis described in Sec. 4.2, enables more robust
and competitive bilingual induction results over
the GAN-based unsupervised method with refine-
ment. We denote this method ‘id+Procrustes (R)’.

Tab. 2 shows results on the morphologically
complex languages used by Søgaard et al. (2018).
For each language pair we run experiments in both
directions. The baseline methods in Søgaard et al.
(2018) include id+Procrustes (R) and the MUSE
(U+R). We run id+Procrustes (R) ourselves and
obtain different results from them: except for en-
et and en-el, we obtain significantly better results
on other language pairs. In addition, we add an-
other strong supervised baseline (5k+Procrustes
(R)) with the training dictionary in the MUSE
dataset and iterative Procrustes refinement. From
Tab. 2, we observe that even without refinement,
DeMa-BME (CSLS (w/o R)) outperforms both
the unsupervised and supervised baselines on even
these difficult morphologically rich languages.

5.5 Cross-lingual Word Similarity

We evaluate DeMa-BWE on the cross-lingual
word similarity task from SemEval 2017
(Camacho-Collados et al., 2016) and compare
with some strong baselines in Xu et al. (2018). In
Tab. 5, following the convention in benchmark
evaluation for this task, we report the Pearson
correlation scores (×100). DeMa-BME achieves

Supervised de-en es-en fa-en it-en

Xing et al. (2015) 72 71 69 72
Shigeto et al. (2015) 72 72 69 72
Artetxe et al. (2016) 73 72 70 73
Artetxe et al. (2017) 70 70 67 71

Unsupervised

Conneau et al. (2017) 71 71 68 71
Xu et al. (2018) 71 71 67 71
DeMa-BME (w/o R) 72.2 72.2 68.6 72.2

Table 3: Pearson rank correlation (×100) on cross-
lingual word similarity task. Bold indicates the best
unsupervised numbers.

the best performance among all the listed unsu-
pervised methods. Compared with the supervised
methods, DeMa-BME is also very competitive.

en-fr fr-en en-ja ja-en

w/o Lxy & Lyx 45.4 73.3 6.3 27.3
w/o Lbt 82.3 82.3 46.2 31.1
w/o Lsup 0.1 0.1 0.0 0.0
π(yj |xi) := π(yj) 82.1 82.1 46.0 32.5
Full Model 82.6 82.4 46.7 32.9

Table 4: Ablation study on different components of
DeMa-BME.

5.6 Ablation Study
Finally, we perform ablation studies in Tab. 4
to examine the effects of different components
of DeMa-BWE. In comparison to the full model,
we remove the density matching loss Lxy &
Lyx, the weakly supervised loss Lsup, the back-
translation loss Lbt respectively. First, we observe
that without the identical strings as the supervised
loss, DeMa-BWE fails to converge as the density



1596

matching is difficult given a high-dimensional em-
bedding space to search. Second, when we remove
the proposed density matching loss, the model is
able to produce reasonable accuracy for fr-en and
ja-en, but has undesirable results on en-fr and en-
ja, which verifies the necessity of the unsupervised
density matching. Third, the back-translation loss
is not a crucial component in DeMa-BME; remov-
ing it only degrades the model’s performance by
a small margin. This indicates that orthogonality
is not must-have constraint given the model has
enough capacity to learn a good transformation.

In addition, we also compare the frequency-
matching based Gaussian mixture weights in (9)
with the naive target frequency based weights. As
shown in the fourth row of Tab. 4, the performance
of DeMa-BWE with the naive weights is nomi-
nally worse than the model using the frequency-
matching based mixture weights.

6 Conclusion

In this work, we propose a density matching based
unsupervised method for learning bilingual word
embedding mappings. DeMa-BWE performs well
in the task of bilingual lexicon induction. In the fu-
ture work, we will integrate Gaussian embeddings
(Vilnis and McCallum, 2015) with our approach.

Acknowledgments

This work is sponsored by Defense Advanced Re-
search Projects Agency Information Innovation
Office (I2O), Program: Low Resource Languages
for Emergent Incidents (LORELEI), issued by
DARPA/I2O under Contract No. HR0011-15-
C-0114. The authors would also like to thank
Ruochen Xu, Barun Patra, Joel Ruben Antony
Moni for their helpful discussions during drafting
this paper.

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.

Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Generalizing and improving bilingual word
embedding mappings with a multi-step framework
of linear transformations. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intel-
ligence (AAAI-18).

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018b. A robust self-learning method for fully un-
supervised cross-lingual mappings of word embed-
dings. In The 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Mel-
bourne, Australia.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018c. Unsupervised statistical machine transla-
tion. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Brussels, Bel-
gium.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

José Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artifi-
cial Intelligence, 240:36–64.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-
gio. 2016. Density estimation using real NVP.
arXiv preprint arXiv:1605.08803.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 462–471, Gothenburg,
Sweden. Association for Computational Linguistics.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In International
Conference on Machine Learning, pages 748–756.

Edouard Grave, Armand Joulin, and Quentin Berthet.
2018. Unsupervised alignment of embeddings
with Wasserstein Procrustes. arXiv preprint
arXiv:1805.11222.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK
Li. 2018. Universal neural machine translation for
extremely low resource languages. In Meeting of
the North American Chapter of the Association for
Computational Linguistics (NAACL), New Orleans,
USA.



1597

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771–779, Columbus, Ohio. Associ-
ation for Computational Linguistics.

Junxian He, Graham Neubig, and Taylor Berg-
Kirkpatrick. 2018. Unsupervised learning of syn-
tactic structure with invertible neural projections. In
Proceedings of EMNLP.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 58–68.

Yedid Hoshen and Lior Wolf. 2018. Non-adversarial
unsupervised word translation. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 469–478.

Pratik Jawanpuria, Arjun Balgovind, Anoop
Kunchukuttan, and Bamdev Mishra. 2018. Learn-
ing multilingual word embeddings in latent metric
space: a geometric approach. arXiv preprint
arXiv:1808.08773.

Armand Joulin, Piotr Bojanowski, Tomas Mikolov,
Hervé Jégou, and Edouard Grave. 2018. Loss in
translation: Learning bilingual word mapping with a
retrieval criterion. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2979–2984.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Diederik P Kingma, Tim Salimans, Rafal Jozefowicz,
Xi Chen, Ilya Sutskever, and Max Welling. 2016.
Improved variational inference with inverse autore-
gressive flow. In Advances in Neural Information
Processing Systems, pages 4743–4751.

Durk P Kingma and Prafulla Dhariwal. 2018. Glow:
Generative flow with invertible 1x1 convolutions.
In Advances in Neural Information Processing Sys-
tems, pages 10235–10244.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. Proceedings of COLING 2012,
pages 1459–1474.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.

George Papamakarios, Iain Murray, and Theo
Pavlakou. 2017. Masked autoregressive flow
for density estimation. In Advances in Neural
Information Processing Systems, pages 2338–2347.

Miloš Radovanović, Alexandros Nanopoulos, and Mir-
jana Ivanović. 2010. Hubs in space: Popular nearest
neighbors in high-dimensional data. J. Mach. Learn.
Res., 11:2487–2531.

Danilo Jimenez Rezende and Shakir Mohamed. 2015.
Variational inference with normalizing flows. In
Proceedings of the 32nd International Conference
on International Conference on Machine Learning-
Volume 37, pages 1530–1538. JMLR. org.

Sebastian Ruder, Ivan Vulic, and Anders Søgaard.
2017. A survey of cross-lingual embedding models.
CoRR, abs/1706.04902.

Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi
Shimbo, and Yuji Matsumoto. 2015. Ridge re-
gression, hubness, and zero-shot learning. In Joint
European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 135–151.
Springer.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. arXiv preprint arXiv:1702.03859.

Anders Søgaard, Sebastian Ruder, and Ivan Vulić.
2018. On the limitations of unsupervised bilingual
dictionary induction. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 778–
788. Association for Computational Linguistics.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. arXiv preprint
arXiv:1604.00425.

Luke Vilnis and Andrew McCallum. 2015. Word rep-
resentations via gaussian embedding. International
Conference on Learning Representations.

Laura Wendlandt, Jonathan K. Kummerfeld, and Rada
Mihalcea. 2018. Factors influencing the surprising
instability of word embeddings. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 2092–2102. Association for Computa-
tional Linguistics.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1006–1011.



1598

Ruochen Xu, Yiming Yang, Naoki Otani, and Yuexin
Wu. 2018. Unsupervised cross-lingual transfer of
word embedding spaces. In Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Brussels, Belgium.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Earth mover’s distance minimization for
unsupervised bilingual lexicon induction. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1934–
1945.

Zhisong Zhang, Wasi Uddin Ahmad, Xuezhe Ma,
Eduard Hovy, Kai-Wei Chang, and Nanyun Peng.
2018. Near or far, wide range zero-shot cross-
lingual dependency parsing. arXiv preprint
arXiv:1811.00570.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).


