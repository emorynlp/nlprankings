



















































Stochastic Top-k ListNet


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 676–684,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Stochastic Top-k ListNet

Tianyi Luo1, Dong Wang1,2, Rong Liu1,3, Yiqiao Pan1,4
1CSLT, RIIT, Tsinghua University, China

2Tsinghua National Lab for Information Science and Technology
3Huilan Limited, Beijing, China

4Beijing University of Posts and Telecommunications, China
{lty, lr, pyq}@cslt.riit.tsinghua.edu.cn

wangdong99@mails.tsinghua.edu.cn

Abstract

ListNet is a well-known listwise learning
to rank model and has gained much atten-
tion in recent years. A particular problem
of ListNet, however, is the high computa-
tion complexity in model training, main-
ly due to the large number of object per-
mutations involved in computing the gra-
dients. This paper proposes a stochastic
ListNet approach which computes the gra-
dient within a bounded permutation sub-
set. It significantly reduces the computa-
tion complexity of model training and al-
lows extension to Top-k models, which is
impossible with the conventional imple-
mentation based on full-set permutation-
s. Meanwhile, the new approach utilizes
partial ranking information of human la-
bels, which helps improve model quality.
Our experiments demonstrated that the s-
tochastic ListNet method indeed leads to
better ranking performance and speeds up
the model training remarkably.

1 Introduction

Learning to rank aims to learn a model to re-
rank a list of objects, e.g., candidate documents
in document retrieval. Recent studies show that
listwise learning delivers better performance in
general than traditional pairwise learning (Liu,
2009), partly attributed to its capability of learning
human-labelled scores as a full rank list. A poten-
tial disadvantage of listwise learning, however, is
the high computation complexity in model train-
ing, which is mainly caused by the large number
of permutations of the objects to rank.

A typical listwise learning method is the List-
Net model proposed by Cao et al. (2007). This
model has been utilized to tackle many ranking
problems, e.g. modeling the hiring behavior in on-

line labor markets (Kokkodis et al., 2015), rank-
ing sentences in document summarization (Jin et
al., 2010), improving detection of musical con-
cepts (Yang et al., 2009) and ranking the results
in video search (Yang and Hsu, 2008). Basical-
ly, ListNet implements the rank function as a neu-
ral network (NN), with the objective function set
to be the cross entropy between two probability
distributions over the object permutations, one de-
rived from the human-labelled scores and the other
derived from the model prediction (network out-
put). In order to deal with the high computation
complexity associated with the large number of
permutations, Cao et al. (2007) proposed a Top-k
approach, which clusters the permutations by the
first k objects, so the number of distinct probabili-
ties that need to evaluate in model training reduces
from n! to n!(n−k)! , where n is the number of objects
in the list.

To ensure efficiency, k = 1 was selected in the
seminal paper (Cao et al., 2007) and in the open
source implementation of RankLib (Dang, 2013).
This Top-1 approach is a harsh approximation to
the full listwise learning and may constrain the
power of the ListNet method. We therefore seek
to extend the Top-1 approximation to Top-k ( k >
1) models.

The major obstacle for the Top-k extension is
the large number of permutations, or more pre-
cisely, permutation classes in the Top-k setting. A
key idea of this paper is that the rank information
involved in the permutation classes is highly re-
dundant and so a small number such permutation
classes are sufficient to convey the rank informa-
tion required to train the model. Meanwhile, the
partial rank information associated with the sub-
set of permutation classes may represent more de-
tailed knowledge for model training, leading to
better ListNet models.

Based on these two conjectures, we propose a s-
tochastic ListNet method, which samples a subset

676



of the permutation classes (object lists) in mod-
el training and based on this subset to train the
ListNet model. Three methods are proposed to
conduct the sampling. In the uniform distribution
method, the candidate objects are selected follow-
ing a uniform distribution; in the fixed distribution
method, the candidate objects are selected follow-
ing a distribution derived from the human-labeled
scores; in the adaptive distribution method, the
candidates are selected following a distribution de-
fined by the rank function, i.e., the neural network
output. Experimental results demonstrated that the
stochastic ListNet method can significantly reduce
the computation cost in model training. In fact,
if the size of the permutation subset is fixed, the
computation complexity is bounded, which allows
training Top-k models where k is large. Mean-
while, better performance was obtained with the
stochastic ListNet approach, probably due to the
learning of partial rank information.

The contributions of the paper are three-fold:
(1) proposes a stochastic ListNet method that sig-
nificantly reduces the training complexity and de-
livers better ranking performance; (2) investigates
Top-k models based on the stochastic ListNet, and
studies the impact of a large k; (3) provides an
open source implementation based on RankLib.

The rest of the paper is organized as follows.
Section 2 introduces some related works, and Sec-
tion 3 presents the stochastic ListNet method. Sec-
tion 4 presents the experiments, and the paper is
concluded by Section 6.

2 Related Work

This work is an extension of the Top-k List-
Net method proposed by Cao et al. (2007). The
novelty is that we propose a stochastic learing
method which not only speeds up the model train-
ing but also produces stronger models. The code
is based on the Top-1 ListNet implementation of
RankLib (Dang, 2013).

Another related work is the SVM-based pair-
wise learning to rank model based on stochastic
gradient descent (SGD) (Sculley, 2009). In this
approach, training instances (queries) are selected
randomly and for each query, a number of object
pairs are sampled from the object list. These pairs
are used to train the SVM model. In the stochas-
tic ListNet method proposed in this paper, the ran-
domly selected training samples are permutation
classes (object lists) rather than pairs of objects,

and a set of object lists rather than a single pair
forms a training sample.

3 Methods

3.1 Review of ListNet
The ListNet approach proposed by Cao et al.
(2007) trains a neural network which predicts the
scores z(i) of a list of candidate objects x(i) given
a query q(i), formulated by z(i) = fw(x(i)), where
fw stands for the scoring function defined by the
NN. The objective function is given by:

L =
∑

i

L(y(i), z(i))

=
∑

i

∑
∀g∈Gk

Py(i)(g)log(Pz(i)(g)) (1)

where y(i) denotes the human-labelled scores, and
Gk is the set of permutation classes defined by:

Gk = {Gk(j1, j2, ..., jk)|jt = 1, 2, ..., n,
s.t. ju 6= jv for ∀u 6= v} (2)

where n is the number of candidate objects, jt
is the object ranked at the t-th position, and
Gk(j1, j2, ..., jk) is a permutation class which in-
volves all the permutations whose first k object-
s are exactly (j1, j2, ..., jk). Following Cao et al.
(2007), the probability of Gk(j1, j2, ..., jk) can be
computed by:

Ps(G (j1, j2, ..., jk)) =
k∏

t=1

esjt∑n
l=t e

sjl
. (3)

where sjt is the score of object at position jt(t =
1, 2, , , k) at a certain permutation. By this defi-
nition of permutation probability, Eq. (1) defines
a cross entropy between the distributions over
permutations (precisely, permutation classes) de-
rived from the human-labelled scores and the NN-
predicted scores. Therefore, optimizing the objec-
tive function Eq. (1) with respect to the NN model
fw leads to a scoring function that approximates
the human-labelled ranking.

3.2 Stochastic Top-k ListNet
A particular difficulty of the Top-k ListNet method
is that it requires very demanding computation in
model training. Refer to Eq. (2), the permu-
tation set Gk involves n!(n−k)! members, and for
each member, computing its probability involves
(2n−k+1)k

2 summations plus k multiplications and

677



divisions. To let the algorithm practical, k=1 was
selected in (Cao et al., 2007), as well as the pub-
lic toolkit RankLib (Dang, 2013). Although this is
a good solution and reduces computation dramat-
ically, we argue that this approach largely buries
the power of ListNet. In fact, setting k=1 effec-
tively marginalizes all the probabilities over the
candidate objects of a permutation class except the
top one. By this approximation, Eq. (3) reduces to
a softmax over the candidate objects, which mean-
s that it actually focuses on how the probabilities
are distributed over individual objects, rather than
how the probabilities are distributed over objec-
t lists. This potentially loses much rank informa-
tion involved in the human labels.

Another disadvantage of the Top-1 model is that
it learns the rank information of the full list, but
ignores the rank information of partial sequences,
which may lead to ineffective learning. As an ex-
ample, considering an object list where the score
of the most relevant object is much higher than the
scores of others, then the learning is dominated
by the highest score, and largely throws away the
rank information conveyed by the scores of other
objects. It would be quite helpful if the rank infor-
mation involved in partial sequences of the candi-
date objects can be learned. Top-k models place
distributions over object lists (in length k), and so
can learn partial sequences of objects.

We are interested in how to learn Top-k (k > 1)
models while keeping the computation tractable.
To achieve the goal, we propose a stochastic List-
Net approach, which samples a small set of the
Top-k permutation classes (object lists), and train
the Top-k model based on this small set instead
of the full set of permutation classes. As a com-
parison, the full set of permutation classes of the
Top-k model is n!(n−k)! , which is computationally
prohibitive if k > 1. With stochastic ListNet, a
subset of the permutation classes that involves on-
ly l members are randomly selected. Training the
Top-k model based on this subset greatly reduces
the computation cost, even with a large k. In fact,
the subset approach imposes a bound of the com-
putation cost that is largely determined by the the
size of the subset (l), while independent of the to-
tal number of objects n and the model order k.

Interestingly, the stochastic approach offers not
only quick learning, but also a chance of learning
partial ranks. This is obvious because only a sub-
set of the object lists are selected in model train-

ing, and so the rank information involved in the
subset of the permutation classes can be learned.
With the Top-1 model, partial ranks reduces to par-
tial sequences since each object list involves only
one object. As we have discussed, learning partial
sequences is an advantage of Top-k models with
k > 1. This means that stochastic Top-1 List-
Net possesses some advantages of Top-k ListNet,
while the computation cost is much lower.

3.3 Sampling methods for stochastic ListNet

The training process of stochastic ListNet start-
s from sampling l permutation classes, or object
lists. For each object list, k objects are sampled
following a particular distribution. As mentioned
in Section 1, three distributions are studied in this
paper: uniform distribution, fixed distribution and
adaptive distribution. They are presented as fol-
lows.

Uniform distribution sampling: In this
method, all the k objects of a particular object list
are sampled with an equal probability. This sam-
pling method is simple but biased towards irrele-
vant candidates, since there are much more irrele-
vant objects than relevant ones in the training data.
A re-sampling approach is proposed to remedy the
bias, as will be discussed in Section 4.

Fixed distribution sampling: In this method,
the objects are sampled following a distribution
proportional to the human-labelled scores. For in-
stance, in the LETOR dataset that is used in this s-
tudy, each candidate object (document) is labelled
as 2 (very relevant), 1 (relevant) or 0 (irrelevan-
t). These scores are normalized by softmax and
are used as the probability distribution when sam-
pling objects. Because the probabilities of relevant
objects are larger than those of irrelevant object-
s, more relevant objects would be selected by this
sampling approach in model training.

Adaptive distribution sampling: The fixed
distribution sampling mentioned above relies on
human-labelled scores, which may be impacted by
label errors. Moreover, the absolute values of hu-
man labels are not good measures of object rel-
evance. To solve these problems, we choose the
outputs of the ‘current’ neural network as the rel-
evance scores, and sample the objects according
to these scores. Note that the network outputs are
natural measures of object relevance based on the
present ranking model. As the model (the neural
network) keeps updated during model training, the

678



relevance scores are accordingly changed. In each
iteration, the relevance scores are re-calculated,
and the sampling is based on the new scores in the
next iteration.

3.4 Gradients with linear networks
Cao et al. (2007) optimized the ListNet model by
gradient descent. For each query, the learn rule is
formuated by:

w = w − η∆w
where η is the learning rate, and w denotes the pa-
rameters of the model fw. ∆w denotes the gradi-
ent and it can be computed as follows:

∆w =
∑
∀g∈Gk

∂Pz(i)(fw)(g)

∂w

Py(i)(g)
Pz(i)(fw)(g)

.

For simplicity, a linear NN model was used by Cao
et al. (2007). This has been adopted in our study as
well, written by z(i) = fw(x

(i)
j ) = w

Tx
(i)
j , where

x
(i)
j denotes the feature vector of the j-th object of

the i-th query. In the case of the Top-1 model, it
shows that:

∆w =
∑

j

[σ(z(i), j)− σ(y(i), j)]x(i)j

where σ(s, j) is the j-th value of the softmax func-
tion of the score vector s, given by:

σ(s(i), j) =
es

(i)
j∑n(i)

t=1 e
s
(i)
t

.

In the case of the Top-k model, the gradien-
t(Derivative of cross entropy between Pz(i) and
Py(i) when k >= 2) is a bit complex, but still man-
ageable:

∆w =
∑
g∈Gk

[(
k∏

t=1

σ̂(y(i), t))·

(
k∑

f=1

{x(i)jf −
n(i)∑
v=f

σ̂(z(i), v)x(i)jv })]
(4)

where σ̂(·) defines a ‘partial’ softmax(The partial
softmax means that the σ(s, f) has a similar for-
m as softmax, however when computing the value
for each f, the denominator is not the summation
from 1 to n, instead a partial sequence from f to
n.), given by:

σ̂(s(i), f) =
e
s
(i)
jf∑n(i)

t=f e
s
(i)
jt

.

3.5 Stochastic Top-k ListNet algorithm
We present the stochastic Top-k ListNet algorith-
m, by employing the techniques described above.
The gradient descent (GD) approach is adopted.
All the training samples are processed sequentially
in an iteration. The training runs several iterations
until the convergence criterion is reach. Another
detail is that the learning rate is multiplied by 0.1
whenever the objective function is worse than the
previous iteration. The procedure is illustrated in
Algorithm 1, where L(t) denotes value of the ob-
jective function after the t-th iteration.

Algorithm 1 Stochastic Top-k ListNet
Require:

Input:
D = {(q(1), x(1), y(1)), ..., (q(m), x(m), y(m))}:
training data
T: number of iterations
η: learning rate

Procedure:
1: Randomly initialize w
2: for t = 1 to T do
3: for i = 1 to m do
4: select the i-th training instance

(q(i), x(i), y(i)) ∈ D
5: Sample the permutation classes Gk
6: Compute ∆w according to Eq. (4)
7: Update fw: w = w − η∆w
8: end for
9: if L(t) < L(t− 1) then

10: η = 0.1η
11: end if
12: end for

4 Experiments

4.1 Data
The proposed stochastic Top-k ListNet method is
tested on the document retrieval task based on the
MQ2008 dataset of LETOR 4.0 (Liu et al., 2007).
This database was released in early 2007 and has
been widely used in learning to rank studies. It
contains queries and corresponding candidate doc-
uments. The human-labelled scores are among
three values {0, 1, 2}, representing little, medium,
and strong relevance between queries and candi-
date documents, respectively. The training set,
validation set and test data all contain 784 queries.
The document features used in this study include
term frequency, inverse document frequency, B-

679



M25, and language model scores for IR. Some
new features proposed recently are also included,
such as HostRank, feature propagation, and topi-
cal PageRank.

4.2 Experiment Setup

In our experiments, we consider Top-k model-
s where k = 1, 2, 3, and 4. Although any k is
possible with the proposed stochastic ListNet, we
will show that simply increasing the model order
k does not improve performance. The P@1 and
P@10 performance is used as the evaluation met-
ric.

Specially, for all the three distribution sam-
pling methods, the sampling process involves t-
wo steps: pre-selection and re-sampling. The pre-
selection step samples a list of documents fol-
lowing three distributions mentioned above, and
in the re-sampling step, document lists including
more relevant documents are retained with a high-
er probability. For example, denoting the pre-
selected document list by (v1,v2,...,vk) where k is
the length of the list, and denoting the correspond-
ing human-labelled scores by (s1, s2,...,sk), the
probability that the list is retained is given by∑k

i=1 si
kS

where S is the maximum value of the human-
labelled scores, which is 2 in our case. The re-
sampling approach is designed to encourage doc-
ument lists containing more relevant documents,
which is the most important for the uniform distri-
bution sampling.

In stochastic Top-k ListNet, the learning rate is
set as 10−3 for k = 1, and 10−5 for k > 1. These
values are set to achieve the best performance on
the validation set. Another important parameter of
the stochastic Top-k ListNet approach is the num-
ber of samples of the document lists (or the size
of subset of permutation classes selected), denoted
by l. Various settings of l are experimented with in
this study. To eliminate randomness in the results,
all the experiments are repeated 20 times and the
averaged performance is reported.

4.3 Experimental results

The P@1 results on the test dataset with different
orders of Top-k ListNet are reported in Figure 1
to Figure 4. In each figure, the number of docu-
ment lists varies from 5 to 500. For comparison,

50 60 70 80 90 100 500
0.4105

0.411

0.4115

0.412

0.4125

0.413

0.4135

Number of samples

P
@

1 
pe

rf
or

m
an

ce

 

 

ADS ListNet (k = 1)
FDS ListNet (k = 1)
UDS ListNet (k = 1)
Conventional ListNet (k = 1)

Figure 1: The P@1 performance on the test data
with the Top-1 ListNet utilizing the three sampling
approaches. The size of the permutation subset
varies from 50 to 500.

5 10 15 20 25 30 500
0.402

0.404

0.406

0.408

0.41

0.412

0.414

0.416

0.418

0.42

Number of samples

P
@

1 
pe

rf
or

m
an

ce

 

 

ADS ListNet ( k = 2)
FDS ListNet ( k = 2)
UDS ListNet ( k = 2)
Conventional ListNet ( k = 2)

Figure 2: The P@1 performance on the test data
with the Top-2 ListNet utilizing the three sampling
approaches. The size of the permutation subset
varies from 5 to 500.

5 10 15 20 25 30 500
0.41

0.411

0.412

0.413

0.414

0.415

0.416

0.417

0.418

0.419

Number of samples

P
@

1 
pe

rf
or

m
an

ce

 

 

ADS ListNet (k = 3)
FDS ListNet (k = 3)
UDS ListNet (k = 3)

Figure 3: The P@1 performance on the test data
with the Top-3 ListNet utilizing the three sampling
approaches. The size of the permutation subset
varies from 5 to 500.

the results with the conventional ListNet are al-
so presented. Note that the re-sampling approach
was not applied to the Top-1 model as we found
it caused performance reduction. This is perhaps
because the sampling space is small with the Top-
1 model, and so re-sampling tends to cause over-
emphasis on relevant documents.

680



5 10 15 20 25 30 500
0.404

0.406

0.408

0.41

0.412

0.414

0.416

0.418

0.42

Number of samples

P
@

1 
pe

rf
or

m
an

ce

 

 

ADS ListNet (k = 4)
FDS ListNet (k = 4)
UDS ListNet (k = 4)

Figure 4: The P@1 performance on the test data
with the Top-4 ListNet utilizing the three sampling
approaches. The size of the permutation subset
varies from 5 to 500.

From these results, we first observe that s-
tochastic ListNet with either fixed or adaptive dis-
tribution sampling tends to outperform the conven-
tional ListNet approach, particularly with a large
k. This confirms our argument that rank informa-
tion can be learned from a subset of the permu-
tation classes that are randomly selected, and the
partial rank learning can lead to even better per-
formance than the full rank learning, the case of
conventional ListNet. This is an interesting result
and demonstrates the stochastic ListNet is both
faster and better than the conventional ListNet. It
is also seen that the adaptive distribution sampling
performs slightly better than the fixed distribution
sampling. This is not surprising as the adaptive
distribution sampling uses a more reasonable rel-
evance score (neural network output) to balance
relevant and irrelevant documents. The uniform
distribution sampling performs a little worse than
the other two sampling methods, probably caused
by the less informative uniform distribution.

Another observation is that in all the four fig-
ures, the performance of the stochastic ListNet
methods increases with more samples of the ob-
ject lists. However if there are too many samples,
the performance starts to decrease. This can be
explained by the fact that the sampling prefers rel-
evant documents which are more informative. A
larger sample set often includes more informative
documents; however if the set is too large, many ir-
relevant documents will be selected and the perfor-
mance is reduced. In the case that the number of
samples is very large (500 for example for Top-1),
the stochastic ListNet falls back to the convention-
al ListNet, and their performance becomes similar.

Comparing the results with different k, it can
be seen that a larger k leads to a better perfor-

mance with stochastic ListNet. This confirms that
high-order Top-k models can learn more ranking
information. However, this is not necessarily the
case with the conventional ListNet. For example,
the Top-2 model does not offer better performance
than the Top-1 model. This is perhaps because
high-order Top-k models consider a large num-
ber of document lists and most of them are not
informative, which leads to ineffective learning.
Remind that the conventional ListNet is a special
case of the stochastic ListNet with a very large
sample set, and we have discussed that an over
large sample set actually reduces performance.

The averaged training time and the performance
in precession are presented in Table 1. For pre-
cession, both P@1 and P@10 results are report-
ed, though we focus on P@1 since it is more con-
cerned for applications such as QA. Note that for
stochastic ListNet, the optimal number of samples
(document lists) has been selected according to the
P@1 performance on the validate set.

From these results, it can be seen that the con-
ventional Top-1 ListNet is rather fast, however the
Top-2 model is thousands of times slower. With
k > 2, the training time becomes prohibitive and
so they are not listed in the Table. This is expected
since the conventional ListNet considers the full
set of permutations which is a huge number with
a large k. With the stochastic ListNet, the training
time is dramatically reduced. Even with a large k,
the computation cost is still manageable, because
the computation is mostly determined by the num-
ber of object lists, rather than the value of k. When
comparing the three sampling methods, it can be
found the convergence speed of the uniform distri-
bution approach is the slowest, probably due to the
ineffective selection for relevant documents. The
adaptive distribution sampling is the fastest, prob-
ably attributed to the collaborative update of the
model and the distribution.

As for the P@1 performance, the stochastic
ListNet method generally outperforms its non-
stochastic counterpart, particularly with the adap-
tive distribution sampling. For example, the best
P@1 results obtained on the test data with the s-
tochastic Top-1 ListNet is 0.4127, which outper-
forms the conventional Top-1 ListNet (0.4119).
This advantage of stochastic ListNet, as we ar-
gued, is largely attributed to its capability of learn-
ing partial rank information with samples of par-
tial sequences of the rank list.

681



Comparing the results with different k values,
it can be seen that a larger k tends to offer better
P@1 performance on the training set, with either
the conventional ListNet or the stochastic ListNet.
For example, with the conventional ListNet, the
results are 0.4101 vs. 0.4119 with the Top-1 and
Top-2 models respectively. However, the perfor-
mance gap is rather marginal, and the advantage
with the large k does not propagate to the results
on the test data (as has been seen in Figure 1 and
Figure 2). This indicates that for the conventional
ListNet, the Top-1 model is not the only choice in
the sense of computation complexity, but also the
best choice in the sense of P@1 performance.

For stochastic ListNet, the performance im-
proves with k increases. In contrast to the con-
ventional ListNet, this improvement propagates to
the results on the test data. For example, with the
adaptive distribution sampling, the P@1 results on
the training set are 0.4102 vs. 0.4184 with the
Top-1 and Top-3 models respectively, and the re-
sults on the test data are 0.4121 vs. 0.4177 respec-
tively.

Nevertheless, the P@1 performance improve-
ment with a large k is rather marginal, and an
over large k simply reduces the performance. To
make it clear, we vary the value of k from 1 to
100 and plot the P@1 results in Figure 5). It can
be seen that larger k (> 4) does not offer any
merit but causes performance instability, particu-

1 2 3 4 10 50 100
0.37

0.38

0.39

0.4

0.41

0.42

k 

P
@

1 
pe

rf
or

m
an

ce

 

 

ADS ListNet
FDS ListNet
UDS ListNet

Figure 5: The P@1 performance on the test data
with the stochastic Top-k ListNet approach, where
k varies from 1 to 100.

larly with the adaptive sampling approach. As we
have discussed, with the stochastic ListNet, par-
tial rank information can be learned with simple
Top-k models, even the Top-1 model. This capa-
bility of partial rank learning with simple models
reduces the necessity of employing complex Top-k
models. This is a highly valuable conclusion, and
it suggests that a simple Top-1 or Top-2 model is
sufficient for the ListNet method, if the stochas-
tic method is applied. Considering the trade-off
between computation cost and model strength, we
recommend stochastic Top-2 ListNet which deliv-
ers better P@1 performance than the Top-1 model
consistently, with sufficiently fast computing. If
more computation is affordable, stochastic Top-3
ListNet can be used to obtain better performance.

Finally, we highlight that the conclusions ob-

P@1 P@10
Model Top-k Sampling Time (s) Train Val. Test Train Val. Test

C-ListNet k=1 - 2.509 0.4101 0.4107 0.4119 0.2684 0.2684 0.2676
S-ListNet k=1 UDS 0.753 0.4097 0.4106 0.4120 0.2680 0.2683 0.2676
S-ListNet k=1 FDS 0.391 0.4094 0.4090 0.4127 0.2679 0.2681 0.2676
S-ListNet k=1 ADS 0.375 0.4102 0.4097 0.4121 0.2680 0.2682 0.2677
C-ListNet k=2 - 2275.5 0.4119 0.4043 0.4043 0.2678 0.2674 0.2674
S-ListNet k=2 UDS 2.898 0.4140 0.4143 0.4130 0.2682 0.2686 0.2681
S-ListNet k=2 FDS 2.410 0.4145 0.4144 0.4164 0.2684 0.2688 0.2684
S-ListNet k=2 ADS 2.013 0.4162 0.4168 0.4145 0.2686 0.2689 0.2687
S-ListNet k=3 UDS 4.358 0.4167 0.4204 0.4152 0.2686 0.2681 0.2680
S-ListNet k=3 FDS 3.997 0.4137 0.4205 0.4131 0.2687 0.2695 0.2685
S-ListNet k=3 ADS 3.483 0.4184 0.4196 0.4177 0.2692 0.2697 0.2689
S-ListNet k=4 UDS 6.161 0.4145 0.4226 0.4104 0.2686 0.2694 0.2687
S-ListNet k=4 FDS 5.773 0.4145 0.4232 0.4150 0.2690 0.2695 0.2686
S-ListNet k=4 ADS 4.358 0.4149 0.4247 0.4164 0.2692 0.2700 0.2689

Table 1: Averaged training time (in seconds), P@1 and P@10 on training, validation (Val.) and test
data with different Top-k methods. ‘C-ListNet’ stands for conventional ListNet, ‘S-ListNet’ stands for
stochastic ListNet.

682



tained from the P@1 results and the P@10 re-
sults perfectly match. In fact, the P@10 results
look more consistent between training and test da-
ta, and the advantage of the stochastic approach
seems more clear, particularly with the adaptive
sampling. This is not surprising as the optimiza-
tion goal of ListNet is essentially to form a good
rank that involves multiple candidates, and so
P@10 is apt to measure the superiority of a bet-
ter rank approach.

5 Discussion

An interesting observation with the stochastic
ListNet approach is that sampling more relevan-
t documents improves performance. This can be
explained by the data imbalance between relevan-
t and irrelevant documents, i.e., there are much
more irrelevant documents than relevant docu-
ments in the training data. This imbalance lead-
s to biased models that tend to classify all docu-
ments as irrelevant. The re-sampling approach can
be regarded as a way of balancing the two classes,
and the fixed and adaptive distribution sampling
can be regarded as another way to achieve the
goal. Note that in the fixed distribution sampling,
the distribution is solely dependent on the human-
labeled scores. These scores are good measures
of the rank of relevance but not good measures of
the relevance itself. A possible way to solve this
problem is to learn a scoring function that map-
s human-labelled scores to more reasonable mea-
sures of document relevance, though we took a d-
ifferent way that employs the network outputs as
the relevance measures, which is what the adaptive
distribution sampling method does. Note that the
network output is a natural measure of documen-
t relevance, so the adaptive distribution sampling
works the best in our experiments.

Another related issue is the harsh labelling of
the AM2008 dataset. In this dataset, documents
are labelled by only three values {0, 1, 2}, which
is rather imprecise and the rank information is very
limited. This harsh labeling is another reason why
the uniform distribution sampling does not work:
by uniform distribution sampling, there is a large
probability that the sampled object lists involve
documents that are all labelled by 0. This lead-
s to an inefficient learning. Another consequence
of the harsh labeling is that the power of compli-
cated ranking models is largely constrained. For
example, with the Top-k (k > 1) ListNet model,

many of the k documents in a candidate list are la-
belled as the same score, resulting in limited rank
information for the Top-k model to learn. This is
why Top-k models did not exhibit much superi-
ority to the Top-1 model in our experiments. We
argue that top-k models would provide more con-
tributions with more thorough labels (e.g., scores
in real values). This is an ongoing research of our
group.

Finally, we highlight that the stochastic ap-
proach is not limited to the ListNet model, but any
model for listwise learning. It is well known that
listwise learning outperforms pairwise learning,
due to it is capability of learning full ranks (Li-
u, 2009). However learning full ranks requires
unaffordable computation and so is infeasible in
practice, even with the Top-k approximation. Our
work demonstrated that learning full ranks can be
approximated by learning partial ranks, and a lim-
ited number samples of such partial ranks is suf-
ficient to convey the rank information. This s-
tochastic learning is very fast, and even delivers
better performance. It can be regarded as a gener-
al framework that treats both the pair-wise learn-
ing and the full rank learning as two special cases.
In fact, if the set of partial ranks involves all the
permutation classes, it reduces to the convention-
al listwise learning, and if the set of partial ranks
involves all object pairs, it resembles the pairwise
learning. A wide range of listwise learning meth-
ods can benefit from the idea of stochastic learning
provided in this paper.

6 Conclusion

This paper proposed a stochastic ListNet method
to speed up the training of ListNet models and im-
prove the ranking performance. The basic idea is
to approximate the full rank learning by learning a
small number of partial ranks. Three sampling ap-
proaches were proposed to select the partial ranks,
and Top-k ListNet models with various complexi-
ty (k values) were investigated.

Our preliminary results on the MQ2008 dataset
confirmed that the stochastic ListNet approach can
dramatically speeds up the model training, and
more interestingly, it can produce better ranking
performance than the conventional ListNet. Espe-
cially, the adaptive distribution sampling method
delivered the best P@1 performance. An appeal-
ing observation is that the simple Top-2 model is
very effective and more complex Top-k models

683



seem not very necessary, considering the trade-off
between training complexity and model strength.
This observation, however, is purely based on the
MQ2008 dataset. As have been discussed, more
detailed human labels may require more complex
models, for which the stochastic method proposed
in this paper is essential to conduct the model
training. For the future work, we plan to study
Top-k ListNet models with other databases and ap-
ply the stochastic learning approach to other list-
wise learning to rank methods.

Acknowledgments

This research was supported by the National
Science Foundation of China (NSFC) under the
project No. 61371136, and the MESTDC PhD
Foundation Project No. 20130002120011. It was
also supported by Sinovoice and Pachira.

References
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and

Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th international conference on Machine learning,
pages 129–136. ACM.

Van B. Dang. 2013. Ranklib.
http://people.cs.umass.edu/vdang/ranklib.html/.

Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A
comparative study on ranking and selection strate-
gies for multi-document summarization. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 525–533.
Association for Computational Linguistics.

Marios Kokkodis, Panagiotis Papadimitriou, and Pana-
giotis G Ipeirotis. 2015. Hiring behavior models for
online labor markets. In Proceedings of the Eighth
ACM International Conference on Web Search and
Data Mining, pages 223–232. ACM.

Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and
Hang Li. 2007. Letor: Benchmark dataset for re-
search on learning to rank for information retrieval.
In Proceedings of SIGIR 2007 workshop on learning
to rank for information retrieval, pages 3–10.

Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225–331.

D Sculley. 2009. Large scale learning to rank. In NIPS
Workshop on Advances in Ranking, pages 1–6.

Yi-Hsuan Yang and Winston H Hsu. 2008. Video
search reranking via online ordinal reranking. In
Multimedia and Expo, 2008 IEEE International
Conference on, pages 285–288. IEEE.

Yi-Hsuan Yang, Yu-Ching Lin, Ann Lee, and Homer H
Chen. 2009. Improving musical concept detection
by ordinal regression and context fusion. In ISMIR,
pages 147–152. Citeseer.

684


