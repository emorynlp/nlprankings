



















































Improved Relation Extraction with Feature-Rich Compositional Embedding Models


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1774–1784,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Improved Relation Extraction with
Feature-Rich Compositional Embedding Models

Matthew R. Gormley1⇤ and Mo Yu2⇤ and Mark Dredze1
1Human Language Technology Center of Excellence

Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD, 21218

2Machine Intelligence and Translation Lab
Harbin Institute of Technology, Harbin, China

gflfof@gmail.com, {mrg, mdredze}@cs.jhu.edu

Abstract

Compositional embedding models build
a representation (or embedding) for a
linguistic structure based on its compo-
nent word embeddings. We propose
a Feature-rich Compositional Embedding
Model (FCM) for relation extraction that
is expressive, generalizes to new domains,
and is easy-to-implement. The key idea
is to combine both (unlexicalized) hand-
crafted features with learned word em-
beddings. The model is able to directly
tackle the difficulties met by traditional
compositional embeddings models, such
as handling arbitrary types of sentence an-
notations and utilizing global information
for composition. We test the proposed
model on two relation extraction tasks,
and demonstrate that our model outper-
forms both previous compositional models
and traditional feature rich models on the
ACE 2005 relation extraction task, and the
SemEval 2010 relation classification task.
The combination of our model and a log-
linear classifier with hand-crafted features
gives state-of-the-art results. We made our
implementation available for general use1.

1 Introduction

Two common NLP feature types are lexical
properties of words and unlexicalized linguis-
tic/structural interactions between words. Prior
work on relation extraction has extensively stud-
ied how to design such features by combining dis-
crete lexical properties (e.g. the identity of a word,

⇤⇤Gormley and Yu contributed equally.
1https://github.com/mgormley/pacaya

its lemma, its morphological features) with as-
pects of a word’s linguistic context (e.g. whether it
lies between two entities or on a dependency path
between them). While these help learning, they
make generalization to unseen words difficult. An
alternative approach to capturing lexical informa-
tion relies on continuous word embeddings2 as
representative of words but generalizable to new
words. Embedding features have improved many
tasks, including NER, chunking, dependency pars-
ing, semantic role labeling, and relation extrac-
tion (Miller et al., 2004; Turian et al., 2010; Koo
et al., 2008; Roth and Woodsend, 2014; Sun et
al., 2011; Plank and Moschitti, 2013; Nguyen and
Grishman, 2014). Embeddings can capture lexi-
cal information, but alone they are insufficient: in
state-of-the-art systems, they are used alongside
features of the broader linguistic context.

In this paper, we introduce a compositional
model that combines unlexicalized linguistic con-
text and word embeddings for relation extraction,
a task in which contextual feature construction
plays a major role in generalizing to unseen data.
Our model allows for the composition of embed-
dings with arbitrary linguistic structure, as ex-
pressed by hand crafted features. In the follow-
ing sections, we begin with a precise construction
of compositional embeddings using word embed-
dings in conjunction with unlexicalized features.
Various feature sets used in prior work (Turian et
al., 2010; Nguyen and Grishman, 2014; Hermann
et al., 2014; Roth and Woodsend, 2014) are cap-

2Such embeddings have a long history in NLP, in-
cluding term-document frequency matrices and their low-
dimensional counterparts obtained by linear algebra tools
(LSA, PCA, CCA, NNMF), Brown clusters, random projec-
tions and vector space models. Recently, neural networks /
deep learning have provided several popular methods for ob-
taining such embeddings.

1774



Class M1 M2 Sentence Snippet
(1) ART(M1,M2) a man a taxicab A man driving what appeared to be a taxicab
(2) PART-WHOLE(M1,M2) the southern suburbs Baghdad direction of the southern suburbs of Baghdad
(3) PHYSICAL(M2,M1) the united states 284 people in the united states , 284 people died

Table 1: Examples from ACE 2005. In (1) the word “driving” is a strong indicator of the relation ART3 between M1 and M2.
A feature that depends on the embedding for this context word could generalize to other lexical indicators of the same relation
(e.g. “operating”) that don’t appear with ART during training. But lexical information alone is insufficient; relation extraction
requires the identification of lexical roles: where a word appears structurally in the sentence. In (2), the word “of” between
“suburbs” and “Baghdad” suggests that the first entity is part of the second, yet the earlier occurrence after “direction” is of no
significance to the relation. Even finer information can be expressed by a word’s role on the dependency path between entities.
In (3) we can distinguish the word “died” from other irrelevant words that don’t appear between the entities.

tured as special cases of this construction. Adding
these compositional embeddings directly to a stan-
dard log-linear model yields a special case of our
full model. We then treat the word embeddings
as parameters giving rise to our powerful, efficient,
and easy-to-implement log-bilinear model. The
model capitalizes on arbitrary types of linguistic
annotations by better utilizing features associated
with substructures of those annotations, including
global information. We choose features to pro-
mote different properties and to distinguish differ-
ent functions of the input words.

The full model involves three stages. First, it
decomposes the annotated sentence into substruc-
tures (i.e. a word and associated annotations).
Second, it extracts features for each substructure
(word), and combines them with the word’s em-
bedding to form a substructure embedding. Third,
we sum over substructure embeddings to form a
composed annotated sentence embedding, which
is used by a final softmax layer to predict the out-
put label (relation).

The result is a state-of-the-art relation extractor
for unseen domains from ACE 2005 (Walker et al.,
2006) and the relation classification dataset from
SemEval-2010 Task 8 (Hendrickx et al., 2010).

Contributions This paper makes several contri-
butions, including:

1. We introduce the FCM, a new compositional
embedding model for relation extraction.

2. We obtain the best reported results on ACE-
2005 for coarse-grained relation extraction in
the cross-domain setting, by combining FCM
with a log-linear model.

3. We obtain results on on SemEval-2010 Task
8 competitive with the best reported results.

Note that other work has already been published
that builds on the FCM, such as Hashimoto et al.
(2015), Nguyen and Grishman (2015), dos Santos

3In ACE 2005, ART refers to a relation between a person
and an artifact; such as a user, owner, inventor, or manufac-
turer relationship

et al. (2015), Yu and Dredze (2015) and Yu et al.
(2015). Additionally, we have extended FCM to
incorporate a low-rank embedding of the features
(Yu et al., 2015), which focuses on fine-grained
relation extraction for ACE and ERE. This paper
obtains better results than the low-rank extension
on ACE coarse-grained relation extraction.

2 Relation Extraction

In relation extraction we are given a sentence as in-
put with the goal of identifying, for all pairs of en-
tity mentions, what relation exists between them,
if any. For each pair of entity mentions in a sen-
tence S, we construct an instance (y,x), where
x = (M1, M2, S, A). S = {w1, w2, ..., wn} is
a sentence of length n that expresses a relation
of type y between two entity mentions M1 and
M2, where M1 and M2 are sequences of words in
S. A is the associated annotations of sentence S,
such as part-of-speech tags, a dependency parse,
and named entities. We consider directed rela-
tions: for a relation type Rel, y=Rel(M1, M2)
and y0=Rel(M2, M1) are different relations. Ta-
ble 1 shows ACE 2005 relations, and has a strong
label bias towards negative examples. We also
consider the task of relation classification (Se-
mEval), where the number of negative examples
is artificially reduced.

Embedding Models Word embeddings and
compositional embedding models have been suc-
cessfully applied to a range of NLP tasks, however
the applications of these embedding models to re-
lation extraction are still limited. Prior work on
relation classification (e.g. SemEval 2010 Task 8)
has focused on short sentences with at most one
relation per sentence (Socher et al., 2012; Zeng
et al., 2014). For relation extraction, where neg-
ative examples abound, prior work has assumed
that only the named entity boundaries and not
their types were available (Plank and Moschitti,
2013; Nguyen et al., 2015). Other work has as-

1775



sumed that the order of two entities in a relation
are given while the relation type itself is unknown
(Nguyen and Grishman, 2014; Nguyen and Grish-
man, 2015). The standard relation extraction task,
as adopted by ACE 2005 (Walker et al., 2006),
uses long sentences containing multiple named en-
tities with known types4 and unknown relation di-
rections. We are the first to apply neural language
model embeddings to this task.

Motivation and Examples Whether a word is
indicative of a relation depends on multiple prop-
erties, which may relate to its context within the
sentence. For example, whether the word is in-
between the entities, on the dependency path be-
tween them, or to their left or right may provide
additional complementary information. Illustra-
tive examples are given in Table 1 and provide
the motivation for our model. In the next section,
we will show how we develop informative repre-
sentations capturing both the semantic information
in word embeddings and the contextual informa-
tion expressing a word’s role relative to the entity
mentions. We are the first to incorporate all of
this information at once. The closest work is that
of Nguyen and Grishman (2014), who use a log-
linear model for relation extraction with embed-
dings as features for only the entity heads. Such
embedding features are insensitive to the broader
contextual information and, as we show, are not
sufficient to elicit the word’s role in a relation.

3 A Feature-rich Compositional
Embedding Model for Relations

We propose a general framework to construct an
embedding of a sentence with annotations on its
component words. While we focus on the rela-
tion extraction task, the framework applies to any
task that benefits from both embeddings and typi-
cal hand-engineered lexical features.

3.1 Combining Features with Embeddings

We begin by describing a precise method for con-
structing substructure embeddings and annotated
sentence embeddings from existing (usually un-
lexicalized) features and embeddings. Note that
these embeddings can be included directly in a
log-linear model as features—doing so results in

4Since the focus of this paper is relation extraction, we
adopt the evaluation setting of prior work which uses gold
named entities to better facilitate comparison.

a special case of our full model presented in the
next subsection.

An annotated sentence is first decomposed into
substructures. The type of substructures can vary
by task; for relation extraction we consider one
substructure per word5. For each substructure in
the sentence we have a hand-crafted feature vec-
tor fwi and a dense embedding vector ewi . We
represent each substructure as the outer product
⌦ between these two vectors to produce a matrix,
herein called a substructure embedding: hwi =
fwi ⌦ ewi . The features fwi are based on the local
context in S and annotations in A, which can in-
clude global information about the annotated sen-
tence. These features allow the model to pro-
mote different properties and to distinguish differ-
ent functions of the words. Feature engineering
can be task specific, as relevant annotations can
change with regards to each task. In this work
we utilize unlexicalized binary features common
in relation extraction. Figure 1 depicts the con-
struction of a sentence’s substructure embeddings.

We further sum over the substructure embed-
dings to form an annotated sentence embedding:

ex =
nX

i=1

fwi ⌦ ewi (1)

When both the hand-crafted features and word em-
beddings are treated as inputs, as has previously
been the case in relation extraction, this anno-
tated sentence embedding can be used directly as
the features of a log-linear model. In fact, we
find that the feature sets used in prior work for
many other NLP tasks are special cases of this
simple construction (Turian et al., 2010; Nguyen
and Grishman, 2014; Hermann et al., 2014; Roth
and Woodsend, 2014). This highlights an im-
portant connection: when the word embeddings
are constant, our constructions of substructure and
annotated sentence embeddings are just specific
forms of polynomial (specifically quadratic) fea-
ture combination—hence their commonality in the
literature. Our experimental results suggest that
such a construction is more powerful than directly
including embeddings into the model.

3.2 The Log-Bilinear Model
Our full log-bilinear model first forms the sub-
structure and annotated sentence embeddings from

5We use words as substructures for relation extraction, but
use the general terminology to maintain model generality.

1776



054
055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107

Based on above ideas, we achieve a general model and can easily apply to model to an NLP task
without the need for designing model structures or selecting features from scratch. Specifically, if
we denote a instance as (y, S), where S is an arbitrary language structure and y is the label for
the structure. Then we decompose the structure to some factors following S = {f}. For each
factor f , there is a list of m associated features g = g1, g2, ..., gm, and a list of t associated words
wf,1, wf,2, ..., wf,t 2 f . Here we suppose that each factor has the same number of words, and there
is a transformation from the words in a factor to a hidden layer as follows:

hf = �
��

ewf,1 : ewf,2 : ... : ewf,t
� · W� , (1)

where ewi is the word embedding for word wi. Suppose the word embeddings have de dimensions
and the hidden layer has dh dimensions. Here W = [W1W2...Wt], each Wj is a de ⇥ dh matrix,
is a transformation from the concatenation of word embeddings to the inputs of the hidden layer.
Then the sigmoid transformation � will be used to get the values of hidden layer from its inputs.

Dev MRR Test MRR
Model Fine-tuning supervison 1,000 10,000 100,000 1,000 10,000 100,000

SUM
- - 46.95 35.29 30.69 52.63 41.19 37.32
Y PPDB 50.81 36.81 32.92 57.23 45.01 41.23

RNN (d=50) Y PPDB 45.67 30.86 27.05 54.84 39.25 35.49
RNN (d=200) Y PPDB 48.97 33.50 31.13 53.59 40.50 38.57

FCT
N PPDB 47.53 35.58 31.31 54.33 41.96 39.10
Y PPDB 51.22 36.76 33.59 61.11 46.99 44.31

FCT
- LM 49.43 37.46 32.22 53.56 42.63 39.44
Y LM + PPDB 53.82 37.48 34.43 65.47 49.44 45.65

joint LM + PPDB 56.53 41.41 36.45 68.52 51.65 46.53

Table 9: Performance on the semantic similarity task with PPDB data.

@`

@T
=

nX
i=1

@`

@R
⌦ fwi ⌦ ewi , (13)

@`

@ew
=

nX
i=1

I [wi = w] T � @`
@R
� fwi (14)

T :
nX

i=1

fi ⌦ ewi � representation (15)

T �
 

nX
i=1

fi ⌦ ewi
!

(16)

T �
 

nX
i=1

fi ⌦ ewi
!

=
nX

i=1

T � fi� �� � �ewi
Mi

nX
i=1

fi ⌦ ewi� �� � (17)

���� (18)
+ (19)

(wi, fi = f (wi, S)) (20)

wi 2 S (21)

T � (⌦fwi ⌦ ewi) , (22)

� (23)
�2,0 � 0 (24)

�2,1, �2,2, �2,3 � 0 (25)

054
055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107

Based on above ideas, we achieve a general model and can easily apply to model to an NLP task
without the need for designing model structures or selecting features from scratch. Specifically, if
we denote a instance as (y, S), where S is an arbitrary language structure and y is the label for
the structure. Then we decompose the structure to some factors following S = {f}. For each
factor f , there is a list of m associated features g = g1, g2, ..., gm, and a list of t associated words
wf,1, wf,2, ..., wf,t 2 f . Here we suppose that each factor has the same number of words, and there
is a transformation from the words in a factor to a hidden layer as follows:

hf = �
��

ewf,1 : ewf,2 : ... : ewf,t
� · W� , (1)

where ewi is the word embedding for word wi. Suppose the word embeddings have de dimensions
and the hidden layer has dh dimensions. Here W = [W1W2...Wt], each Wj is a de ⇥ dh matrix,
is a transformation from the concatenation of word embeddings to the inputs of the hidden layer.
Then the sigmoid transformation � will be used to get the values of hidden layer from its inputs.

Figure 1: Tensor representation of the FCT model. (a) Representation of an input sentence. (b)
Representation for the parameter space.

Based on above notations, we can represent each factor as the outer product between the feature
vector and the hidden layer of transformed embedding gf⌦hf . The we use a tensor T = L⌦E⌦F
as in Figrure 1(b) to transform this input matrix to the labels. Here L is the set of labels, E refers to
all dimensions of hidden layer (|E| = 200) and F is the set of features.
In order to predict the conditional probability of a label y given the structure S, we have

P (y|S; T ) = exp{s(y, S; T )}P
y�2L exp{s(y�, S; T )}

, (2)

where s(y, S; T ) is the score of label y computed with our model. Since we decompose the struc-
ture S to factors, each factor fi 2 S will contribute to the score based on the model parameters.
Specifically, each label y corresponds to a slice of the tensor Ty , which is a matrix �(y, ·, ·). Then
each factor fi will contribute a score

s(y, fi) = Ty � gf � hf , (3)
where � correspond to tensor product, while in the case of Eq.(3), it has the equivalent form:

Ty � gf � hf = Ty � (gf ⌦ hf ) = (�(y, ·, ·) · gf )T hf .
In this way, the target score of label y given an instance S and parameter tensor T can be written as:

s(y, S; T ) =
nX

i=1

s(y, fi; T ) =
nX

i=1

Ty � gfi � hfi . (4)

The FCM model only performs linear transformations on each view of the tensor, making the model
efficient and easy to implement.

Learning In order to train the parameters we optimize the following cross-entropy objective:

`(D; T, W ) =
X

(y,S)2D
log P (y|S; T, W )

where D is the set of all training data. We used AdaGrad [9] to optimize above
objective. Therefore we are performing stochastic training; and for each in-
stance (y, S) the loss function ` = `(y, S; T, W ) = log P (y|S; T, W ). Then

2

162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215

Classifier Features F1
SVM [] POS, prefixes, morphological, WordNet, dependency parse, 82.2
(Best in SemEval2010) Levin classed, ProBank, FrameNet, NomLex-Plus,

Google n-gram, paraphrases, TextRunner

RNN word embeddings, syntactic parse 74.8word embeddings, syntactic parse, POS, NER, WordNet 77.6

MVRNN word embeddings, syntactic parse 79.1word embedding, syntactic parse, POS, NER, WordNet 82.4
FCM (fixed-embedding) word embeddings, dependency parse, WordNet 82.0
FCM (fine-tuning) word embeddings, dependency parse, WordNet 82.3
FCM + linear word embeddings, dependency parse, WordNet

Table 2: Feature sets used in FCM.

References
[1] Yoshua Bengio, Holger Schwenk, Jean-Sébastien Senécal, Fréderic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer, 2006.

[2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In International conference on Machine learning, pages 160–167.
ACM, 2008.

[3] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations
of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546, 2013.

[4] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for
semi-supervised learning. In Association for Computational Linguistics, pages 384–394, 2010.

[5] Ronan Collobert. Deep learning for efficient discriminative parsing. In International Conference on
Artificial Intelligence and Statistics, 2011.

[6] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word represen-
tations via global context and multiple word prototypes. In Association for Computational Linguistics,
pages 873–882, 2012.

[7] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
Empirical Methods in Natural Language Processing, pages 1631–1642, 2013.

[8] Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. Semantic frame identification
with distributed word representations. In Proceedings of ACL. Association for Computational Linguistics,
June 2014.

[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

[10] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó,
Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-way clas-
sification of semantic relations between pairs of nominals. In Proceedings of the SemEval-2 Workshop,
Uppsala, Sweden, 2010.

[11] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211,
Jeju Island, Korea, July 2012. Association for Computational Linguistics.

[12] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition,
june. Linguistic Data Consortium, LDC2011T07, 2011.

ewf,1

ewf,2

...

ewf,t

(9)

4

Dev MRR Test MRR
Model Fine-tuning supervison 1,000 10,000 100,000 1,000 10,000 100,000

SUM
- - 46.95 35.29 30.69 52.63 41.19 37.32
Y PPDB 50.81 36.81 32.92 57.23 45.01 41.23

RNN (d=50) Y PPDB 45.67 30.86 27.05 54.84 39.25 35.49
RNN (d=200) Y PPDB 48.97 33.50 31.13 53.59 40.50 38.57

FCT
N PPDB 47.53 35.58 31.31 54.33 41.96 39.10
Y PPDB 51.22 36.76 33.59 61.11 46.99 44.31

FCT
- LM 49.43 37.46 32.22 53.56 42.63 39.44
Y LM + PPDB 53.82 37.48 34.43 65.47 49.44 45.65

joint LM + PPDB 56.53 41.41 36.45 68.52 51.65 46.53

Table 9: Performance on the semantic similarity task with PPDB data.

@`

@T
=

nX
i=1

@`

@R
⌦ fwi ⌦ ewi , (13)

@`

@ew
=

nX
i=1

I [wi = w] T � @`
@R
� fwi (14)

T :
nX

i=1

fi ⌦ ewi � representation (15)

T �
 

nX
i=1

fi ⌦ ewi
!

(16)

T �
 

nX
i=1

fi ⌦ ewi
!

=
nX

i=1

T � fi� �� � �ewi
Mi

nX
i=1

fi ⌦ ewi� �� � (17)

���� (18)
+ (19)

(wi, fi = f (wi, S)) (20)

wi 2 S (21)

T � (⌦fwi ⌦ ewi) , (22)

� (23)
�2,0 � 0 (24)

�2,1, �2,2, �2,3 � 0 (25)

054
055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107

Based on above ideas, we achieve a general model and can easily apply to model to an NLP task
without the need for designing model structures or selecting features from scratch. Specifically, if
we denote a instance as (y, S), where S is an arbitrary language structure and y is the label for
the structure. Then we decompose the structure to some factors following S = {f}. For each
factor f , there is a list of m associated features g = g1, g2, ..., gm, and a list of t associated words
wf,1, wf,2, ..., wf,t 2 f . Here we suppose that each factor has the same number of words, and there
is a transformation from the words in a factor to a hidden layer as follows:

hf = �
��

ewf,1 : ewf,2 : ... : ewf,t
� · W� , (1)

where ewi is the word embedding for word wi. Suppose the word embeddings have de dimensions
and the hidden layer has dh dimensions. Here W = [W1W2...Wt], each Wj is a de ⇥ dh matrix,
is a transformation from the concatenation of word embeddings to the inputs of the hidden layer.
Then the sigmoid transformation � will be used to get the values of hidden layer from its inputs.

Figure 1: Tensor representation of the FCT model. (a) Representation of an input sentence. (b)
Representation for the parameter space.

Based on above notations, we can represent each factor as the outer product between the feature
vector and the hidden layer of transformed embedding gf⌦hf . The we use a tensor T = L⌦E⌦F
as in Figrure 1(b) to transform this input matrix to the labels. Here L is the set of labels, E refers to
all dimensions of hidden layer (|E| = 200) and F is the set of features.
In order to predict the conditional probability of a label y given the structure S, we have

P (y|S; T ) = exp{s(y, S; T )}P
y�2L exp{s(y�, S; T )}

, (2)

where s(y, S; T ) is the score of label y computed with our model. Since we decompose the struc-
ture S to factors, each factor fi 2 S will contribute to the score based on the model parameters.
Specifically, each label y corresponds to a slice of the tensor Ty , which is a matrix �(y, ·, ·). Then
each factor fi will contribute a score

s(y, fi) = Ty � gf � hf , (3)
where � correspond to tensor product, while in the case of Eq.(3), it has the equivalent form:

Ty � gf � hf = Ty � (gf ⌦ hf ) = (�(y, ·, ·) · gf )T hf .
In this way, the target score of label y given an instance S and parameter tensor T can be written as:

s(y, S; T ) =
nX

i=1

s(y, fi; T ) =
nX

i=1

Ty � gfi � hfi . (4)

The FCM model only performs linear transformations on each view of the tensor, making the model
efficient and easy to implement.

Learning In order to train the parameters we optimize the following cross-entropy objective:

`(D; T, W ) =
X

(y,S)2D
log P (y|S; T, W )

where D is the set of all training data. We used AdaGrad [9] to optimize above
objective. Therefore we are performing stochastic training; and for each in-
stance (y, S) the loss function ` = `(y, S; T, W ) = log P (y|S; T, W ). Then

2

162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215

Classifier Features F1
SVM [] POS, prefixes, morphological, WordNet, dependency parse, 82.2
(Best in SemEval2010) Levin classed, ProBank, FrameNet, NomLex-Plus,

Google n-gram, paraphrases, TextRunner

RNN word embeddings, syntactic parse 74.8word embeddings, syntactic parse, POS, NER, WordNet 77.6

MVRNN word embeddings, syntactic parse 79.1word embedding, syntactic parse, POS, NER, WordNet 82.4
FCM (fixed-embedding) word embeddings, dependency parse, WordNet 82.0
FCM (fine-tuning) word embeddings, dependency parse, WordNet 82.3
FCM + linear word embeddings, dependency parse, WordNet

Table 2: Feature sets used in FCM.

References
[1] Yoshua Bengio, Holger Schwenk, Jean-Sébastien Senécal, Fréderic Morin, and Jean-Luc Gauvain. Neural

probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer, 2006.

[2] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In International conference on Machine learning, pages 160–167.
ACM, 2008.

[3] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations
of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546, 2013.

[4] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for
semi-supervised learning. In Association for Computational Linguistics, pages 384–394, 2010.

[5] Ronan Collobert. Deep learning for efficient discriminative parsing. In International Conference on
Artificial Intelligence and Statistics, 2011.

[6] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word represen-
tations via global context and multiple word prototypes. In Association for Computational Linguistics,
pages 873–882, 2012.

[7] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In
Empirical Methods in Natural Language Processing, pages 1631–1642, 2013.

[8] Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. Semantic frame identification
with distributed word representations. In Proceedings of ACL. Association for Computational Linguistics,
June 2014.

[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

[10] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian Padó,
Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. Semeval-2010 task 8: Multi-way clas-
sification of semantic relations between pairs of nominals. In Proceedings of the SemEval-2 Workshop,
Uppsala, Sweden, 2010.

[11] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211,
Jeju Island, Korea, July 2012. Association for Computational Linguistics.

[12] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword fifth edition,
june. Linguistic Data Consortium, LDC2011T07, 2011.

ewf,1

ewf,2

...

ewf,t

(9)

4

Figure 1: Tensor representation of the FCT model. (a) Representation of an input structure. (b)
Representation for the parameter space.

Based on above notations, we can represent each factor as the outer product between the feature
vector and the hidden layer of transformed embedding gf⌦hf . The we use a tensor T = L⌦E⌦F
as in Figrure 1(b) to transform this input matrix to the labels. Here L is the set of labels, E refers to
all dimensions of hidden layer (|E| = 200) and F is the set of features.
In order to predict the conditional probability of a label y given the structure S, we have

P (y|S; T ) = exp{s(y, S; T )}P
y�2L exp{s(y�, S; T )}

, (2)

where s(y, S; T ) is the score of label y computed with our model. Since we decompose the struc-
ture S to factors, each factor fi 2 S will contribute to the score based on the model parameters.
Specifically, each label y corresponds to a slice of the tensor Ty , which is a matrix �(y, ·, ·). Then
each factor fi will contribute a score

s(y, fi) = Ty � gf � hf , (3)
where � correspond to tensor product, while in the case of Eq.(3), it has the equivalent form:

Ty � gf � hf = Ty � (gf ⌦ hf ) = (�(y, ·, ·) · gf )T hf .
In this way, the target score of label y given an instance S and parameter tensor T can be written as:

s(y, S; T ) =
nX

i=1

s(y, fi; T ) =
nX

i=1

Ty � gfi � hfi . (4)

The FCM model only performs linear transformations on each view of the tensor, making the model
efficient and easy to implement.

Learning In order to train the parameters we optimize the following cross-entropy objective:

`(D; T, W ) =
X

(y,S)2D
log P (y|S; T, W )

where D is the set of all training data. We used AdaGrad [9] to optimize above
objective. Therefore we are performing stochastic training; and for each in-
stance (y, S) the loss function ` = `(y, S; T, W ) = log P (y|S; T, W ). Then

2

bc cts wl
Model P R F1 P R F1 P R F1
HeadEmb
CNN (wsize=1) + local features
CNN (wsize=3) + local features
FCT local only
FCT global 60.69 42.39 49.92 56.41 34.45 42.78 41.95 31.77 36.16
FCT global (Brown) 63.15 39.58 48.66 62.45 36.47 46.05 54.95 29.93 38.75
FCT global (WordNet) 59.00 44.79 50.92 60.20 39.60 47.77 50.95 34.18 40.92
PET (Plank and Moschitti, 2013) 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0
BOW (Plank and Moschitti, 2013) 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7
Best (Plank and Moschitti, 2013) 55.3 43.1 48.5 54.1 38.1 44.7 39.9 35.8 37.8

Table 7: Performance on ACE2005 test sets. The first part of the table shows the performance of different models on
different sources of entity types, where ”G” means that the gold types are used and ”P” means that we are using the
predicted types. The second part of the table shows the results under the low-resource setting, where the entity types
are unknown.

Dev MRR Test MRR
Model Fine-tuning 1,000 10,000 100,000 1,000 10,000 100,000
SUM - 46.95 35.29 30.69 52.63 41.19 37.32
SUM Y 50.81 36.81 32.92 57.23 45.01 41.23
Best Recursive NN (d=50) Y 45.67 30.86 27.05 54.84 39.25 35.49
Best Recursive NN (d=200) Y 48.97 33.50 31.13 53.59 40.50 38.57
FCT N 47.53 35.58 31.31 54.33 41.96 39.10
FCT Y 51.22 36.76 33.59 61.11 46.99 44.31
FCT + LM - 49.43 37.46 32.22 53.56 42.63 39.44
FCT + LM +supervised Y 53.82 37.48 34.43 65.47 49.44 45.65

joint 56.53 41.41 36.45 68.52 51.65 46.53

Table 8: Performance on the semantic similarity task with PPDB data.

Appendix 1: Features Used in FCT

7.1 Overall performances on ACE 2005

SUM(AB) �= SUM(BA) (7)

2n
2 |V |n (8)

A A0 of B0 B (9)

A B A0 of B0 (10)

T � f � e� Relations (11)
f ⌦ e [f : e]
FCT CNN

@`

@R

@`

@T
=

@`

@R

@R

@T

L1, L2

@L

@R
=

@L1
@R

+
@L2
@R

s(l, e1, e2, S; T ) =
nX

i=1

s(l, ewi , fwi)

=
nX

i=1

Tl � fwi � ewi (12)

@`

@T
=

nX
i=1

@`

@R
⌦ fwi ⌦ ewi , (13)

-.5 .3 .8 .7
0 0 0 0

-.5 .3 .8 .7
0 0 0 0
0 0 0 0

-.5 .3 .8 .7
-.5 .3 .8 .7

-.5 .3 .8 .7

1
0
1
0
0
1

�� � 0 �� (�2 � 0)
-.5 .3 .8 .7 1 0 1 0 0 1

-.5 .3 .8 .7
0 0 0 0

-.5 .3 .8 .7
0 0 0 0
0 0 0 0

-.5 .3 .8 .7
-.5 .3 .8 .7

-.5 .3 .8 .7

1
0
1
0
0
1

�� � 0 �� (�2 � 0)
-.5 .3 .8 .7 1 0 1 0 0 1

bc cts wl
Model P R F1 P R F1 P R F1
HeadEmb
CNN (wsize=1) + local features
CNN (wsize=3) + local features
FCT local only
FCT global 60.69 42.39 49.92 56.41 34.45 42.78 41.95 31.77 36.16
FCT global (Brown) 63.15 39.58 48.66 62.45 36.47 46.05 54.95 29.93 38.75
FCT global (WordNet) 59.00 44.79 50.92 60.20 39.60 47.77 50.95 34.18 40.92
PET (Plank and Moschitti, 2013) 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0
BOW (Plank and Moschitti, 2013) 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7
Best (Plank and Moschitti, 2013) 55.3 43.1 48.5 54.1 38.1 44.7 39.9 35.8 37.8

Table 7: Performance on ACE2005 test sets. The first part of the table shows the performance of different models on
different sources of entity types, where ”G” means that the gold types are used and ”P” means that we are using the
predicted types. The second part of the table shows the results under the low-resource setting, where the entity types
are unknown.

Dev MRR Test MRR
Model Fine-tuning 1,000 10,000 100,000 1,000 10,000 100,000
SUM - 46.95 35.29 30.69 52.63 41.19 37.32
SUM Y 50.81 36.81 32.92 57.23 45.01 41.23
Best Recursive NN (d=50) Y 45.67 30.86 27.05 54.84 39.25 35.49
Best Recursive NN (d=200) Y 48.97 33.50 31.13 53.59 40.50 38.57
FCT N 47.53 35.58 31.31 54.33 41.96 39.10
FCT Y 51.22 36.76 33.59 61.11 46.99 44.31
FCT + LM - 49.43 37.46 32.22 53.56 42.63 39.44
FCT + LM +supervised Y 53.82 37.48 34.43 65.47 49.44 45.65

joint 56.53 41.41 36.45 68.52 51.65 46.53

Table 8: Performance on the semantic similarity task with PPDB data.

Appendix 1: Features Used in FCT

7.1 Overall performances on ACE 2005

SUM(AB) �= SUM(BA) (7)

2n
2 |V |n (8)

A A0 of B0 B (9)

A B A0 of B0 (10)

T � f � e� Relations (11)
f ⌦ e [f : e]
FCT CNN

@`

@R

@`

@T
=

@`

@R

@R

@T

L1, L2

@L

@R
=

@L1
@R

+
@L2
@R

s(l, e1, e2, S; T ) =
nX

i=1

s(l, ewi , fwi)

=
nX

i=1

Tl � fwi � ewi (12)

@`

@T
=

nX
i=1

@`

@R
⌦ fwi ⌦ ewi , (13)

w 1
w 2

,…
,w n

ew

fw

fwi ewi

ewi

fwi

(wi=“driving”)

(wi is on path?)

y

M1 man M =taxicab

w1=“A” wi=“driving”

A
fwifw1

[A man]M1 driving what appeared to be [a taxicab]M2

Figure 1: Example construction of substructure embeddings. Each substructure is a word wi in S, augmented by the target
entity information and related information from annotation A (e.g. a dependency tree). We show the factorization of the
annotated sentence into substructures (left), the concatenation of the substructure embeddings for the sentence (middle), and a
single substructure embedding from that concatenation (right). The annotated sentence embedding (not shown) would be the
sum of the substructure embeddings, as opposed to their concatenation.

the previous subsection. The model uses its pa-
rameters to score the annotated sentence embed-
ding and uses a softmax to produce an output la-
bel. We call the entire model the Feature-rich
Compositional Embedding Model (FCM).

Our task is to determine the label y (relation)
given the instance x = (M1, M2, S, A). We for-
mulate this as a probability.

P (y|x; T, e) = exp (
Pn

i=1 Ty � (fwi ⌦ ewi))
Z(x)

(2)
where � is the ‘matrix dot product’ or Frobe-
nious inner product of the two matrices. The
normalizing constant which sums over all possi-
ble output labels y0 2 L is given by Z(x) =P

y02L exp
�Pn

i=1 Ty0 � (fwi ⌦ ewi)
�
. The pa-

rameters of the model are the word embeddings
e for each word type and a list of weight matrix
T = [Ty]y2L which is used to score each label
y. The model is log-bilinear 6 (i.e. log-quadratic)
since we recover a log-linear model by fixing ei-
ther e or T . We study both the full log-bilinear and
the log-linear model obtained by fixing the word
embeddings.

3.3 Discussion of the Model

Substructure Embeddings Similar words (i.e.
those with similar embeddings) with similar func-
tions in the sentence (i.e. those with similar fea-
tures) will have similar matrix representations. To
understand our selection of the outer product, con-
sider the example in Fig. 1. The word “driving”
can indicate the ART relation if it appears on the

6Other popular log-bilinear models are the log-bilinear
language models (Mnih and Hinton, 2007; Mikolov et al.,
2013).

dependency path between M1 and M2. Suppose
the third feature in fwi indicates this on-path
feature. Our model can now learn parameters
which give the third row a high weight for the
ART label. Other words with embeddings similar
to “driving” that appear on the dependency path
between the mentions will similarly receive high
weight for the ART label. On the other hand, if the
embedding is similar but is not on the dependency
path, it will have 0 weight. Thus, our model gen-
eralizes its model parameters across words with
similar embeddings only when they share similar
functions in the sentence.

Smoothed Lexical Features Another intuition
about the selection of outer product is that it is
actually a smoothed version of traditional lexical
features used in classical NLP systems. Consider
a lexical feature f = u ^ w, which is a conjunc-
tion (logic-and) between non-lexical property u
and lexical part (word) w. If we represent w as
a one-hot vector, then the outer product exactly re-
covers the original feature f . Then if we replace
the one-hot representation with its word embed-
ding, we get the current form of our FCM. There-
fore, our model can be viewed as a smoothed ver-
sion of lexical features, which keeps the expres-
sive strength, and uses embeddings to generalize
to low frequency features.

Time Complexity Inference in FCM is much
faster than both CNNs (Collobert et al., 2011) and
RNNs (Socher et al., 2013b; Bordes et al., 2012).
FCM requires O(snd) products on average with
sparse features, where s is the average number of
per-word non-zero feature values, n is the length
of the sentence, and d is the dimension of word
embedding. In contrast, CNNs and RNNs usually

1777



have complexity O(C · nd2), where C is a model
dependent constant.

4 Hybrid Model

We present a hybrid model which combines the
FCM with an existing log-linear model. We do so
by defining a new model:

pFCM+loglin(y|x) = 1
Z

pFCM(y|x)ploglin(y|x) (3)
The log-linear model has the usual form:
ploglin(y|x) / exp(✓ · f(x, y)), where ✓ are the
model parameters and f(x, y) is a vector of fea-
tures. The integration treats each model as a pro-
viding a score which we multiply together. The
constant Z ensures a normalized distribution.

5 Training

FCM training optimizes a cross-entropy objective:

`(D; T, e) =
X

(x,y)2D
log P (y|x; T, e)

where D is the set of all training data and e
is the set of word embeddings. To optimize
the objective, for each instance (y,x) we per-
form stochastic training on the loss function ` =
`(y,x; T, e) = log P (y|x; T, e). The gradi-
ents of the model parameters are obtained by
backpropagation (i.e. repeated application of
the chain rule). We define the vector s =
[
P

i Ty � (fwi ⌦ ewi)]1yL, which yields
@`

@s
=
h�

I[y0 = y]� P (y0|x; T, e)�
1yL

iT
,

where the indicator function I[x] equals 1 if x is
true and 0 otherwise. We have the following gradi-
ents: @`@T =

@`
@s ⌦

Pn
i=1 fwi ⌦ ewi , which is equiv-

alent to:

@`

@Ty0
=
�
I[y = y0]� P (y0|x; T, e)� · nX

i=1

fwi ⌦ ewi .

When we treat the word embeddings as parameters
(i.e. the log-bilinear model), we also fine-tune the
word embeddings with the FCM model:

@`

@ew
=

nX
i=1

  X
y

@`

@sy
Ty

!
· fi
!

· I[wi = w].

As is common in deep learning, we initialize
these embeddings from an neural language model
and then fine-tune them for our supervised task.
The training process for the hybrid model (§ 4)
is also easily done by backpropagation since each
sub-model has separate parameters.

Set Template
HeadEmb {I[i = h1], I[i = h2]}

(wi is head of M1/M2)⇥{�, th1 , th2 , th1 � th2}
Context I[i = h1 ± 1] (left/right token of wh1 )

I[i = h2 ± 1] (left/right token of wh2 )
In-between I[i > h1]&I[i < h2] (in between )

⇥{�, th1 , th2 , th1 � th2}
On-path I[wi 2 P ] (on path)

⇥{�, th1 , th2 , th1 � th2}

Table 2: Feature sets used in FCM.

6 Experimental Settings

Features Our FCM features (Table 2) use a fea-
ture vector fwi over the word wi, the two tar-
get entities M1, M2, and their dependency path.
Here h1, h2 are the indices of the two head words
of M1, M2, ⇥ refers to the Cartesian product be-
tween two sets, th1 and th2 are entity types (named
entity tags for ACE 2005 or WordNet supertags for
SemEval 2010) of the head words of two entities,
and � stands for the empty feature. � refers to the
conjunction of two elements. The In-between
features indicate whether a word wi is in between
two target entities, and the On-path features in-
dicate whether the word is on the dependency
path, on which there is a set of words P , between
the two entities.

We also use the target entity type as a feature.
Combining this with the basic features results in
more powerful compound features, which can help
us better distinguish the functions of word embed-
dings for predicting certain relations. For exam-
ple, if we have a person and a vehicle, we know
it will be more likely that they have an ART rela-
tion. For the ART relation, we introduce a corre-
sponding weight vector, which is closer to lexical
embeddings similar to the embedding of “drive”.

All linguistic annotations needed for fea-
tures (POS, chunks7, parses) are from Stanford
CoreNLP (Manning et al., 2014). Since SemEval
does not have gold entity types we obtained Word-
Net and named entity tags using Ciaramita and
Altun (2006). For all experiments we use 200-
d word embeddings trained on the NYT portion
of the Gigaword 5.0 corpus (Parker et al., 2011),
with word2vec (Mikolov et al., 2013). We use the
CBOW model with negative sampling (15 nega-
tive words). We set a window size c=5, and re-
move types occurring less than 5 times.

Models We consider several methods. (1) FCM
in isolation without fine-tuning. (2) FCM in isola-
tion with fine-tuning (i.e. trained as a log-bilinear

7Obtained from the constituency parse using the CONLL
2000 chunking converter (Perl script).

1778



model). (3) A log-linear model with a rich binary
feature set from Sun et al. (2011) (Baseline)—
this consists of all the baseline features of Zhou et
al. (2005) plus several additional carefully-chosen
features that have been highly tuned for ACE-style
relation extraction over years of research. We ex-
clude the Country gazetteer and WordNet features
from Zhou et al. (2005). The two remaining meth-
ods are hybrid models that integrate FCM as a sub-
model within the log-linear model (§ 4). We con-
sider two combinations. (4) The feature set of
Nguyen and Grishman (2014) obtained by using
the embeddings of heads of two entity mentions
(+HeadOnly). (5) Our full FCM model (+FCM).
All models use L2 regularization tuned on dev
data.

6.1 Datasets and Evaluation

ACE 2005 We evaluate our relation extraction
system on the English portion of the ACE 2005
corpus (Walker et al., 2006).8 There are 6 do-
mains: Newswire (nw), Broadcast Conversation
(bc), Broadcast News (bn), Telephone Speech
(cts), Usenet Newsgroups (un), and Weblogs
(wl). Following prior work we focus on the do-
main adaptation setting, where we train on one set
(the union of the news domains (bn+nw), tune
hyperparameters on a dev domain (half of bc)
and evaluate on the remainder (cts, wl, and
the remainder of bc) (Plank and Moschitti, 2013;
Nguyen and Grishman, 2014). We assume that
gold entity spans and types are available for train
and test. We use all pairs of entity mentions to
yield 43,518 total relations in the training set. We
report precision, recall, and F1 for relation extrac-
tion. While it is not our focus, for completeness
we include results with unknown entity types fol-
lowing Plank and Moschitti (2013) (Appendix 1).

SemEval 2010 Task 8 We evaluate on the Se-
mEval 2010 Task 8 dataset9 (Hendrickx et al.,
2010) to compare with other compositional mod-
els and highlight the advantages of FCM. This task
is to determine the relation type (or no relation)
between two entities in a sentence. We adopt the
setting of Socher et al. (2012). We use 10-fold

8Many relation extraction systems evaluate on the ACE
2004 corpus (Mitchell et al., 2005). Unfortunately, the most
common convention is to use 5-fold cross validation, treating
the entirety of the dataset as both train and evaluation data.
Rather than continuing to overfit this data by perpetuating the
cross-validation convention, we instead focus on ACE 2005.

9
http://docs.google.com/View?docid=dfvxd49s_36c28v9pmw

cross validation on the training data to select hy-
perparameters and do regularization by early stop-
ping. The learning rates for FCM with/without
fine-tuning are 5e-3 and 5e-2 respectively. We
report macro-F1 and compare to previously pub-
lished results.

7 Results

ACE 2005 Despite FCM’s (1) simple feature set,
it is competitive with the log-linear baseline (3)
on out-of-domain test sets (Table 3). In the typi-
cal gold entity spans and types setting, both Plank
and Moschitti (2013) and Nguyen and Grishman
(2014) found that they were unable to obtain im-
provements by adding embeddings to baseline fea-
ture sets. By contrast, we find that on all do-
mains the combination baseline + FCM (5) obtains
the highest F1 and significantly outperforms the
other baselines, yielding the best reported results
for this task. We found that fine-tuning of em-
beddings (2) did not yield improvements on our
out-of-domain development set, in contrast to our
results below for SemEval. We suspect this is be-
cause fine-tuning allows the model to overfit the
training domain, which then hurts performance on
the unseen ACE test domains. Accordingly, Ta-
ble 3 shows only the log-linear model.

Finally, we highlight an important contrast be-
tween FCM (1) and the log-linear model (3): the
latter uses over 50 feature templates based on a
POS tagger, dependency parser, chunker, and con-
stituency parser. FCM uses only a dependency
parse but still obtains better results (Avg. F1).

SemEval 2010 Task 8 Table 4 shows FCM
compared to the best reported results from the
SemEval-2010 Task 8 shared task and several
other compositional models.

For the FCM we considered two feature sets. We
found that using NE tags instead of WordNet tags
helps with fine-tuning but hurts without. This may
be because the set of WordNet tags is larger mak-
ing the model more expressive, but also introduces
more parameters. When the embeddings are fixed,
they can help to better distinguish different func-
tions of embeddings. But when fine-tuning, it be-
comes easier to over-fit. Alleviating over-fitting is
a subject for future work (§ 9).

With either WordNet or NER features, FCM
achieves better performance than the RNN and
MVRNN. With NER features and fine-tuning, it
outperforms a CNN (Zeng et al., 2014) and also

1779



bc cts wl Avg.
Model P R F1 P R F1 P R F1 F1

(1) FCM only (ST) 66.56 57.86 61.90 65.62 44.35 52.93 57.80 44.62 50.36 55.06
(3) Baseline (ST) 74.89 48.54 58.90 74.32 40.26 52.23 63.41 43.20 51.39 54.17
(4) + HeadOnly (ST) 70.87 50.76 59.16 71.16 43.21 53.77 57.71 42.92 49.23 54.05
(5) + FCM (ST) 74.39 55.35 63.48 74.53 45.01 56.12 65.63 47.59 55.17 58.26

Table 3: Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is our
reimplementation of the features of Nguyen and Grishman (2014).

Classifier Features F1
SVM (Rink and Harabagiu, 2010) POS, prefixes, morphological, WordNet, dependency parse,

82.2(Best in SemEval2010) Levin classed, ProBank, FrameNet, NomLex-Plus,
Google n-gram, paraphrases, TextRunner

RNN word embedding, syntactic parse 74.8
RNN + linear word embedding, syntactic parse, POS, NER, WordNet 77.6
MVRNN word embedding, syntactic parse 79.1
MVRNN + linear word embedding, syntactic parse, POS, NER, WordNet 82.4
CNN (Zeng et al., 2014) word embedding, WordNet 82.7
CR-CNN (log-loss) word embedding 82.7
CR-CNN (ranking-loss) word embedding 84.1
RelEmb (word2vec embedding) word embedding 81.8
RelEmb (task-spec embedding) word embedding 82.8
RelEmb (task-spec embedding) + linear word embedding, dependency paths, WordNet, NE 83.5
DepNN word embedding, dependency paths 82.8
DepNN + linear word embedding, dependency paths, WordNet, NER 83.6

(1) FCM (log-linear) word embedding, dependency parse, WordNet 82.0word embedding, dependency parse, NER 81.4

(2) FCM (log-bilinear) word embedding, dependency parse, WordNet 82.5word embedding, dependency parse, NER 83.0

(5) FCM (log-linear) + linear (Hybrid) word embedding, dependency parse, WordNet 83.1word embedding, dependency parse, NER 83.4

Table 4: Comparison of FCM with previously published results for SemEval 2010 Task 8.

the combination of an embedding model and a tra-
ditional log-linear model (RNN/MVRNN + lin-
ear) (Socher et al., 2012). As with ACE, FCM uses
less linguistic resources than many close competi-
tors (Rink and Harabagiu, 2010).

We also compared to concurrent work on en-
hancing the compositional models with task-
specific information for relation classification, in-
cluding Hashimoto et al. (2015) (RelEmb), which
trained task-specific word embeddings, and dos
Santos et al. (2015) (CR-CNN), which proposed
a task-specific ranking-based loss function. Our
Hybrid methods (FCM + linear) get comparable re-
sults to theirs. Note that their base compositional
model results without any task-specific enhance-
ments, i.e. RelEmb with word2vec embeddings
and CR-CNN with log-loss, are still lower than the
best FCM result. We believe that FCM can be also
improved with these task-specific enhancements,
e.g. replacing the word embeddings to the task-
specific ones from (Hashimoto et al., 2015) in-
creases the result to 83.7% (see §7.2 for details).
We leave the application of ranking-based loss to
future work.

Finally, a concurrent work (Liu et al., 2015)
proposes DepNN, which builds representations for
the dependency path (and its attached subtrees)
between two entities by applying recursive and
convolutional neural networks successively. Com-
pared to their model, our FCM achieves compa-
rable results. Of note, our FCM and the RelEmb
are also the most efficient models among all above
compositional models since they have linear time
complexity with respect to the dimension of em-
beddings.

7.1 Effects of the embedding sub-models

We next investigate the effects of different types of
features on FCM using ablation tests on ACE 2005
(Table 5.) We focus on FCM alone with the fea-
ture templates of Table 2. Additionally, we show
results of using only the head embedding features
from Nguyen and Grishman (2014) (HeadOnly).
Not surprisingly, the HeadOnly model performs
poorly (F1 score = 14.30%), showing the impor-
tance of our rich binary feature set. Among all the
features templates, removing HeadEmb results in
the largest degradation. The second most im-

1780



Feature Set Prec Rec F1
HeadOnly 31.67 9.24 14.30
FCM 69.17 56.73 62.33

-HeadEmb 66.06 47.00 54.92
-Context 70.89 55.27 62.11
-In-between 66.39 51.86 58.23
-On-path 69.23 53.97 60.66

FCM-EntityTypes 71.33 34.68 46.67

Table 5: Ablation test of FCM on development set.

portant feature template is In-between, while
Context features have little impact. Remov-
ing all entity type features (thi) does significantly
worse than the full model, showing the value of
our entity type features.

7.2 Effects of the word embeddings

Good word embeddings are critical for both FCM
and other compositional models. In this section,
we show the results of FCM with embeddings
used to initialize other recent state-of-the-art mod-
els. Those embeddings include the 300-d baseline
embeddings trained on English Wikipedia (w2v-
enwiki-d300) and the 100-d task-specific embed-
dings (task-specific-d100)10 from the RelEmb pa-
per (Hashimoto et al., 2015), the 400-d embed-
dings from the CR-CNN paper (dos Santos et al.,
2015). Moreover, we list the best result (DepNN)
in Liu et al. (2015), which uses the same embed-
dings as ours. Table 6 shows the effects of word
embeddings on FCM and provides relative compar-
isons between FCM and the other state-of-the-art
models. We use the same hyperparameters and
number of iterations in Table 4.

The results show that using different embed-
dings to initialize FCM can improve F1 beyond
our previous results. We also find that increas-
ing the dimension of the word embeddings does
not necessarily lead to better results due to the
problem of over-fitting (e.g.w2v-enwiki-d400 vs.
w2v-enwiki-d300). With the same initial embed-
dings, FCM usually gets better results without any
changes to the hyperparameters than the compet-
ing model, further confirming the advantage of
FCM at the model-level as discussed under Ta-
ble 4. The only exception is the DepNN model,
which gets better result than FCM on the same
embeddings. The task-specific embeddings from
(Hashimoto et al., 2015) leads to the best perfor-
mance (an improvement of 0.7%). This observa-

10In the task-specific setting, FCM will represent entity
words and context words with separate sets of embeddings.

Embeddings Model F1

w2v-enwiki-d300 RelEmb 81.8(2) FCM (log-bilinear) 83.4

task-specific-d100
RelEmb 82.8
RelEmb+linear 83.5
(2) FCM (log-bilinear) 83.7

w2v-enwiki-d400 CR-CNN 82.7(2) FCM (log-bilinear) 83.0

w2v-nyt-d200 DepNN 83.6(2) FCM (log-bilinear) 83.0

Table 6: Evaluation of FCMs with different word
embeddings on SemEval 2010 Task 8.

tion suggests that the other compositional models
may also benefit from the work of Hashimoto et
al. (2015).

8 Related Work

Compositional Models for Sentences In order
to build a representation (embedding) for a sen-
tence based on its component word embeddings
and structural information, recent work on compo-
sitional models (stemming from the deep learning
community) has designed model structures that
mimic the structure of the input. For example,
these models could take into account the order of
the words (as in Convolutional Neural Networks
(CNNs)) (Collobert et al., 2011) or build off of
an input tree (as in Recursive Neural Networks
(RNNs) or the Semantic Matching Energy Func-
tion) (Socher et al., 2013b; Bordes et al., 2012).

While these models work well on sentence-level
representations, the nature of their designs also
limits them to fixed types of substructures from the
annotated sentence, such as chains for CNNs and
trees for RNNs. Such models cannot capture arbi-
trary combinations of linguistic annotations avail-
able for a given task, such as word order, depen-
dency tree, and named entities used for relation
extraction. Moreover, these approaches ignore the
differences in functions between words appearing
in different roles. This does not suit more general
substructure labeling tasks in NLP, e.g. these mod-
els cannot be directly applied to relation extraction
since they will output the same result for any pair
of entities in a same sentence.

Compositional Models with Annotation Fea-
tures To tackle the problem of traditional com-
positional models, Socher et al. (2012) made the
RNN model specific to relation extraction tasks by
working on the minimal sub-tree which spans the
two target entities. However, these specializations

1781



to relation extraction does not generalize easily to
other tasks in NLP. There are two ways to achieve
such specialization in a more general fashion:

1. Enhancing Compositional Models with Fea-
tures. A recent trend enhances compositional
models with annotation features. Such an ap-
proach has been shown to significantly improve
over pure compositional models. For example,
Hermann et al. (2014) and Nguyen and Grishman
(2014) gave different weights to words with dif-
ferent syntactic context types or to entity head
words with different argument IDs. Zeng et al.
(2014) use concatenations of embeddings as fea-
tures in a CNN model, according to their posi-
tions relative to the target entity mentions. Be-
linkov et al. (2014) enrich embeddings with lin-
guistic features before feeding them forward to a
RNN model. Socher et al. (2013a) and Hermann
and Blunsom (2013) enhanced RNN models by
refining the transformation matrices with phrase
types and CCG super tags.

2. Engineering of Embedding Features. A dif-
ferent approach to combining traditional linguistic
features and embeddings is hand-engineering fea-
tures with word embeddings and adding them to
log-linear models. Such approaches have achieved
state-of-the-art results in many tasks including
NER, chunking, dependency parsing, semantic
role labeling, and relation extraction (Miller et al.,
2004; Turian et al., 2010; Koo et al., 2008; Roth
and Woodsend, 2014; Sun et al., 2011; Plank and
Moschitti, 2013). Roth and Woodsend (2014) con-
sidered features similar to ours for semantic role
labeling.

However, in prior work both of above ap-
proaches are only able to utilize limited informa-
tion, usually one property for each word. Yet there
may be different useful properties of a word which
can contribute to the performances of the task. By
contrast, our FCM can easily utilize these features
without changing the model structures.

In order to better utilize the dependency anno-
tations, recently work built their models according
to the dependency paths (Ma et al., 2015; Liu et
al., 2015), which share similar motivations to the
usage of On-path features in our work.

Task-Specific Enhancements for Relation Clas-
sification An orthogonal direction of improving
compositional models for relation classification is
to enhance the models with task-specific informa-
tion. For example, Hashimoto et al. (2015) trained

task-specific word embeddings, and dos Santos et
al. (2015) proposed a ranking-based loss function
for relation classification.

9 Conclusion

We have presented FCM, a new compositional
model for deriving sentence-level and substruc-
ture embeddings from word embeddings. Com-
pared to existing compositional models, FCM can
easily handle arbitrary types of input and handle
global information for composition, while remain-
ing easy to implement. We have demonstrated
that FCM alone attains near state-of-the-art perfor-
mances on several relation extraction tasks, and
in combination with traditional feature based log-
linear models it obtains state-of-the-art results.

Our next steps in improving FCM focus on en-
hancements based on task-specific embeddings or
loss functions as in Hashimoto et al. (2015; dos
Santos et al. (2015). Moreover, as the model pro-
vides a general idea for representing both sen-
tences and sub-structures in language, it has the
potential to contribute useful components to vari-
ous tasks, such as dependency parsing, SRL and
paraphrasing. Also as kindly pointed out by one
anonymous reviewer, our FCM can be applied to
the TAC-KBP (Ji et al., 2010) tasks, by replac-
ing the training objective to a multi-instance multi-
label one (e.g. Surdeanu et al. (2012)). We plan to
explore the above applications of FCM in the fu-
ture.

Acknowledgments

We thank the anonymous reviewers for their com-
ments, and Nicholas Andrews, Francis Ferraro,
and Benjamin Van Durme for their input. We
thank Kazuma Hashimoto, Cı́cero Nogueira dos
Santos, Bing Xiang and Bowen Zhou for sharing
their word embeddings and many helpful discus-
sions. Mo Yu is supported by the China Scholar-
ship Council and by NSFC 61173073.

References
Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir

Globerson. 2014. Exploring compositional archi-
tectures and word vector representations for prepo-
sitional phrase attachment. Transactions of the As-
sociation for Computational Linguistics, 2:561–572.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. A semantic matching en-

1782



ergy function for learning with multi-relational data.
Machine Learning, pages 1–27.

Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP2006, pages 594–602, July.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying relations by ranking with con-
volutional neural networks. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 626–634, Beijing,
China, July. Association for Computational Linguis-
tics.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2015. Task-oriented
learning of word embeddings for semantic relation
classification. arXiv preprint arXiv:1503.00095.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
SemEval-2 Workshop.

Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In Association for Computational
Linguistics, pages 894–904.

Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1448–1458, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text
Analysis Conference (TAC 2010).

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595–603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Qi Li and Heng Ji. 2014. Incremental joint extrac-
tion of entity mentions and relations. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 402–412, Baltimore, Maryland, June.
Association for Computational Linguistics.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng WANG. 2015. A dependency-based
neural network for relation classification. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 285–
290, Beijing, China, July. Association for Computa-
tional Linguistics.

Mingbo Ma, Liang Huang, Bowen Zhou, and Bing Xi-
ang. 2015. Dependency-based convolutional neural
networks for sentence embedding. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 174–179, Beijing,
China, July. Association for Computational Linguis-
tics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. arXiv preprint arXiv:1310.4546.

Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.

Alexis Mitchell, Stephanie Strassel, Shudong Huang,
and Ramez Zakhary. 2005. Ace 2004 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia.

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.

Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying word representations and regularization for
domain adaptation of relation extraction. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 68–74, Baltimore, Maryland, June.
Association for Computational Linguistics.

Thien Huu Nguyen and Ralph Grishman. 2015. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In Proceedings of NAACL Workshop
on Vector Space Modeling for NLP.

Thien Huu Nguyen, Barbara Plank, and Ralph Gr-
ishman. 2015. Semantic representations for do-

1783



main adaptation: A case study on the tree kernel-
based method for relation extraction. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 635–644,
Beijing, China, July. Association for Computational
Linguistics.

Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword
fifth edition, june. Linguistic Data Consortium,
LDC2011T07.

Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1498–1507, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Bryan Rink and Sanda Harabagiu. 2010. Utd: Clas-
sifying semantic relations by combining lexical and
semantic resources. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
256–259, Uppsala, Sweden, July. Association for
Computational Linguistics.

Michael Roth and Kristian Woodsend. 2014. Com-
position of word representations improves semantic
role labelling. In EMNLP.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211, Jeju Island, Korea, July. Association for
Computational Linguistics.

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Empirical Methods in Natural Language
Processing, pages 1631–1642.

Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521–529, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical

Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465. Association for Computational Linguistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Association for
Computational Linguistics, pages 384–394.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia.

Mo Yu and Mark Dredze. 2015. Learning composition
models for phrase embeddings. Transactions of the
Association for Computational Linguistics, 3:227–
242.

Mo Yu, Matthew R. Gormley, and Mark Dredze. 2015.
Combining word embeddings and feature embed-
dings for fine-grained relation extraction. In Pro-
ceedings of NAACL.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344, Dublin, Ireland, August. Dublin
City University and Association for Computational
Linguistics.

GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Association for Computational Linguis-
tics, pages 427–434.

1784


