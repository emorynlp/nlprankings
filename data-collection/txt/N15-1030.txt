



















































Empty Category Detection With Joint Context-Label Embeddings


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263–271,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Empty Category Detection With Joint Context-Label Embeddings

Xun Wang, Katsuhito Sudoh and Masaaki Nagata
NTT Communication Science Laboratories

Kyoto 619-0237, Japan
wang.xun,sudoh.katsuhito,nagata.masaaki@lab.ntt.co.jp

Abstract

This paper presents a novel technique for
empty category (EC) detection using dis-
tributed word representations. A joint
model is learned from the labeled data to
map both the distributed representations
of the contexts of ECs and EC types to
a low dimensional space. In the testing
phase, the context of possible EC positions
will be projected into the same space for
empty category detection. Experiments on
Chinese Treebank prove the effectiveness
of the proposed method. We improve the
precision by about 6 points on a subset of
Chinese Treebank, which is a new state-of-
the-art performance on CTB.

1 Introduction
The empty category (EC) is an important con-
cept in linguistic theories. It is used to de-
scribe nominal words that do not have ex-
plicit phonological forms (they are also called
“covert nouns”). This kind of grammatical phe-
nomenons is usually caused by the omission or
dislocation of nouns or pronouns. Empty cat-
egories are the “hidden” parts of text and are
essential for syntactic parsing (Gabbard et al.,
2006; Yang and Xue, 2010). As a basic prob-
lem in NLP, the resolution of ECs also has a
huge impact on lots of downstream tasks, such
as co-reference resolution (Ponzetto and Strube,
2006; Kong and Ng, 2013), long distance de-
pendency relation analysis (Marcus et al., 1993;
Xue et al., 2005). Research also uncovers the

important role of ECs in machine translation.
Some recent work (Chung and Gildea, 2010; Xi-
ang et al., 2013) demonstrates the improvements
they manage to obtain through EC detection in
Chinese-English translation.

To resolve ECs, we need to decide 1) the po-
sition and type of the EC and 2) the content
of the EC (to which element the EC is linked
to if plausible). Existing research mainly fo-
cuses on the first problem which is referred to
as EC detection (Cai et al., 2011; Yang and
Xue, 2010), and so is this paper. As ECs are
words or phrases inferable from their context,
previous work mainly designs features mining
the contexts of ECs and then trains classifica-
tion models or parsers using these features (Xue
and Yang, 2013; Johnson, 2002; Gabbard et al.,
2006; Kong and Zhou, 2010). One problem with
these human-developed features are that they
are not fully capable of representing the seman-
tics and syntax of contexts. Besides, the feature
engineering is also time consuming and labor in-
tensive.

Recently neural network models have proven
their superiority in capturing features using low
dense vector compared with traditional manu-
ally designed features in dozens of NLP tasks
(Bengio et al., 2006; Collobert and Weston,
2008; Socher et al., 2010; Collobert et al., 2011;
Li and Hovy, 2014; Li et al., 2014).

This paper demonstrates the advantages of
distributed representations and neural networks
in predicting the locations and types of ECs.
We formulate the EC detection as an annotation

263



task, to assign predefined labels (EC types) to
given contexts. Recently, Weston et al. (2011)
proposed a system taking advantages of the hid-
den representations of neural networks for image
annotation which is to annotate images with a
set of textual words. Following the work, we de-
sign a novel method for EC detection. We rep-
resent possible EC positions using the word em-
beddings of their contexts and then map them
to a low dimension space for EC detection.

Experiments on Chinese Treebank show that
the proposed model obtains significant improve-
ments over the previous state-of-the-art meth-
ods based on strict evaluation metrics. We also
identify the dependency relations between ECs
and their heads, which is not reported in pre-
vious work. The dependency relations can help
us with the resolution of ECs and benefit other
tasks, such as full parsing and machine transla-
tion in practice.

2 Proposed Method

We represent each EC as a vector by concate-
nating the word embeddings of its contexts. As
is shown in Fig. 1, we learn a map MAPA from
the annotated data, to project the ECs’ feature
vectors to a low dimension space K. Meanwhile,
we also obtain the distributed representations of
EC types in the same low dimension space K. In
the testing phase, for each possible EC position,
we use MAPA to project its context feature to
the same space and further compare it with the
representations of EC types for EC detection.

Figure 1: System Architecture

Distributed representations are good at cap-
turing the semantics and syntax of contexts. For
example, with word embeddings we are able to
tell that “吃/eat” and “喝/drink” have a closer
relationship than “吃/eat” and “走/walk” or
“喝/drink” and “走/walk”. Thus the knowledge
we learn from: “EC(你/You)-吃/have-EC(晚
饭/supper)-了/past tense marker-么/question
marker” could help us to detect ECs in sentences
such as “EC(你/You)-饮料/beverage-喝/drink-
了/past tense marker-么/question marker”,
which are similar, though different from the
original sentence.

Below is a list of EC types contained in the
Chinese Treebank, which are also the types of
EC we are to identity in this work.

• pro: small pro, refer to dropped pronouns.

• PRO: big PRO, refer to shared elements
in control structures or elements that have
generic references.

• OP: null operator, refer to empty relative
pronouns.

• T: trace left by A’-movement, e.g., topical-
ization, relativization.

• RNR: used in right nodes rising.

• *: trace left by passivization, raising.

• Others: other ECs.

According to the reason that one EC is
caused, we are able to assign it one of the above
categories.

We can formulate EC detection as a combi-
nation of a two-class classification problem (is
there an EC or not) and a seven-class classifi-
cation problem (what type the EC is if there is
one) following the two-pass method. For one-
pass method, EC detection can be formulated
as an eight-class (seven EC types listed above
plus a dummy “No” type) classification prob-
lem. Previous research shows there is no sig-
nificant differences between their performances
(Xue and Yang, 2013). Here we adopt the one-
pass method for simplicity.

264



2.1 System Overview
The proposed system consists of two maps.

MAPA is from the feature vector of an EC
position to a low dimensional space.

MAPA : Rn → Rk, k ≪ n
fA(X) → WAX

(1)

MAPA is a linear transformation, and WA is a
k ∗ n matrix.

The other one is from labels to the same low
dimensional space.

MAPB : {Label1, Label2, ...} ∈ R → Rk
fB(Labeli) → W iB

(2)

MAPB is also a linear transformation. W iB
is a k dimensional vector and it is also the dis-
tributed representation of Labeli in the low di-
mensional space.

The two maps are learned from the training
data simultaneously. In the testing phase, for
any possible EC position to be classified, we ex-
tract the corresponding feature vector X, and
then map it to the low dimensional space using
fA(X) = WAX. Then we have gi(X) for each
Labeli as follows:

gi(X) = (fA(X))T W iB (3)

For each possible label Labeli, gi(X) is the score
that the example having a Labeli and the label
predicted for the example is the i that maximizes
gi(X).

Following the method of Weston et al. (2011),
we try to minimize a weighted pairwise loss,
learned using stochastic gradient descent:∑

X

∑
i̸=c

L(rankc(X))max(0, (gi(X)− gc(X)))

(4)
Here c is the correct label for example X, and
rankc(X) is the rank of Label c among all pos-
sible labels for X. L is a function which reflects
our attitude towards errors. A constant func-
tion L = C implies we aim to optimize the full
ranking list. Here we adopt L(α) =

∑α
i=1 1/i,

which aims to optimize the top 1 in the rank-
ing list, as stated in (Usunier et al., 2009). The

learning rate and some other parameters of the
stochastic gradient descent algorithm are to be
optimized using the development set.

An alternative method is to train a neural
network model for multi-class classification di-
rectly. It is plausible when the number of classes
is not large. One of the advantages of represent-
ing ECs and labels in a hidden space is that EC
detection usually serves as an intermediate task.
Usually we want to know more about the ECs
such as their roles and explicit content. Rep-
resenting labels and ECs as dense vectors will
greatly benefit other work such as EC resolution
or full parsing. Besides, such a joint embedding
framework can scale up to the large set of la-
bels as is shown in the image annotation task
(Weston et al., 2011), which makes the identifi-
cation of dependency types of ECs (which is a
large set) possible.

2.2 Context Features Construction
2.2.1 Defining Locations

In a piece of text, possible EC positions can be
described with references to tokens, e.g., before
the nth token (Yang and Xue, 2010). One prob-
lem with such methods is that if there are more
than one ECs preceding the nth token, they will
occupy the same position and can not be distin-
guished. One solution is to decide the number
of ECs for each position, which complicates the
problem. But if we do nothing, some ECs will
be ignored.

A compromised solution is to describe posi-
tions using parse trees (Xue and Yang, 2013).
Adjacent ECs before a certain token usually
have different head words, which means they are
attached to different nodes (head words) in a
parse tree. Therefore it is possible to define po-
sitions using “head word, following word” pairs.
Thus the problem of EC detection can be formu-
lated as a classification problem: for each “head
word, following word” pair, what is the type of
the EC? An example is shown in figure 2, in
which there are 2 possible EC positions, (吃,了)
and (吃, 。)1.

1Note that there are still problems with the tree based
method. As is shown in Fig. 3, the pro and T are at-
tached to the same head word (告别) and share the same

265



ROOT

吃

。Position-2了Position-1

Figure 2: Possible EC Positions in a Dependency
Tree

Besides, we keep punctuations in the parse
tree so that we can describe all the possible po-
sitions using the “head node, following word”
pairs, as no elements will appear after a full stop
in a sentence.

2.2.2 Feature Extraction
The feature vector is constructed by concate-

nating the word embeddings of context words
that are expected to contribute to the detection
of ECs.

1. The head word (except the dummy root
node). Suppose words are represented us-
ing d dimension vectors, we need d elements
to represent this feature. The distributed
representations of the head word would be
placed at the corresponding positions.

2. The following word in the text. This feature
is extracted using the same method with
head words.

3. “Nephews”, the sons of the following word.
We choose the leftmost two.

4. Words in dependency paths. ECs usu-
ally have long distance dependencies with
words which cannot be fully captured by
the above categories. We need a new fea-
ture to describe such long distance seman-
tic relations: Dependency Paths. From the
training data, we collect all the paths from
root nodes to ECs (ECs excluded) together
with dependency types. Below we give an
example to illustrate the extraction of this
kind of features using a complex sentence

following word (德国). But such phenomenas are rare, so
here we still adopt the tree based method.

with a multi-layer hierarchical dependency
tree as in Fig. 3. If we have m kinds of
such paths with different path types or de-
pendency types, we need md elements to
represent this kind of features. The dis-
tributed representations of the words would
be placed at the corresponding positions in
the feature vector and the remaining are set
to 0.

Previous work usually involves lots of syntac-
tic and semantic features. In the work of (Xue
and Yang, 2013), 6 kinds of features are used,
including those derived from constituency parse
trees, dependency parse trees, semantic roles
and others. Here we use only the dependency
parse trees for the feature extraction. The words
in dependency paths we use have proven their
potential in representing the meanings of text
in frame identification (Hermann et al., 2014).

Take the OP in the sentence shown in Fig. 3
for example. For the OP, its head word is “的”,
its following word is “告别” and its nephews are
“NULL” and “NULL” (ECs are invisible).

The dependency path from root to OP is:
Root

ROOT−−−−→ 举行/hold COMP−−−−−→ 仪式/ceremony
RELC−−−−→ 的/DE COMP−−−−−→ OP
For such a path, we have the following

subpaths:

Root
ROOT−−−−→ . COMP−−−−−→ . RELC−−−−→ X

Root
ROOT−−−−→ . COMP−−−−−→ X

Root
ROOT−−−−→ X

For the position of the OP in the given exam-
ple, the words with corresponding dependency
paths are “的”, “仪式” and “举行”. Similarly,
we collects all the paths from other ECs in the
training examples to build the feature template.

In the testing phase, for each possible EC po-
sition, we place the distributed representations
of the right words at the corresponding positions
of its feature vector.

266



俄罗斯 军队 31 日 举行 了 OP pro T 告别 德国 的 最后 仪式 。

ROOT

SUBJ

TMP PRT

COMP
UNK

NMOD
RELC

AMOD

COMP

COMPSBJ
ADV COMP

俄罗斯/Russian 军队/troops 31 日/31rd 举行/hold 了/past-tense-marker 告别/farewell 德
国/Germany 的/DE 最后/final 仪式/ceremony 。

Figure 3: ECs in a Dependency Tree

Train Dev Test
File 81-325, 400-454 41-80 1-40

500-554, 590-596 901-931
600-885, 900

#pro 1023 166 297
#PRO 1089 210 298
#OP 2099 301 575
#T 1981 287 527

#RNR 91 15 32
#* 22 0 19

#Others 0 0 0
Total 6305 979 1748

Table 1: Data Division and EC Distribution

3 Experiments on CTB
3.1 Data
The proposed method can be applied to various
kinds of languages as long as annotated corpus
are available. In our experiments, we use a sub-
set of Chinese Treebank V7.0.

We split the data set into three parts, train-
ing, development and test data. Following the
previous research, we use File 1-40 and 901-931
as the test data, File 41-80 as the development
data. The training data includes File {81-325,
400-454, 500-554, 590-596, 6000-885, 900}. The
development data is used to tune parameters
and the final results are reported on the test
data. CTB trees are transferred to dependency
trees for feature extraction with ECs preserved
(Xue, 2007).

The distributed word representation we use

is learned using the word2vec toolkit (Mikolov
et al., 2013). We train the model on a large
Chinese news copora provided by Sogou2, which
contains about 1 billion words after necessary
preprocessing. The text is segmented into words
using ICTCLAS(Zhang et al., 2003)3.

3.2 Experiment Settings
Initialization WA is initialized according to
uniform[− 24din+dhidden , 24din+dhidden ].
And WB is initialized using
uniform[− 24dhidden+dout ,

24
dhidden+dout

].
Here din, dhidden and dout are the dimensions of
the input layer, the hidden space and the label
space.

Parameter Tuning To optimize the param-
eters, firstly, we set the dimension of word vec-
tors to be 80, the dimension of hidden space to
be 50. We search for the suitable learning rate
in {10−1, 10−2, 10−4}. Then we deal with the
dimension of word vectors {80, 100, 200}. Fi-
nally we tune the dimension of hidden space in
{50, 200, 500} against the F-1 scores. . Those
underlined figures are the value of the param-
eters after optimization. We use the stochas-
tic gradient descent algorithm to optimize the
model. The details can be checked here (Weston
et al., 2011). The maximum iteration number
we used is 10K. In the following experiments,

2http://www.sogou.com/labs/dl/cs.html
3The word segment standards used by CTB and ICT-

CLAS are roughly the same with minor differences.

267



we set the parameters to be learning rate=10−1,
word vector dimension=80 and hidden layer di-
mension=500.

From the experiments for parameter tuning,
we find that for the word embeddings in the
proposed model, low dimension vectors are bet-
ter than high dimensions one for low dimension
vectors are better in sharing meanings. For the
hidden space which represents inputs as uninter-
preted vectors, high dimensional vectors are bet-
ter than low dimensional vectors. The learning
rates also have an impact on the performance.
If the learning rate is too small, we need more
iterations to achieve convergence. If we stop it-
erations too early, we will suffer under-fitting.

3.3 Results
3.3.1 Metrics and Evaluation

Previous work reports results based on dif-
ferent evaluation metrics. Some work uses lin-
ear positions to describe ECs. ECs are judged
on a “whether there is an EC of type A before
a certain token in the text” basis (Cai et al.,
2011). Collapsing ECs before the same token to
one, Cai et al. (2011) has 1352 ECs in the test
data. Xue and Yang (2013) has stated that some
ECs that share adjacent positions have different
heads in the parse tree. They judge ECs on a
“whether there is an EC of type A with a certain
head word and a certain following token in the
text” basis. Using this kind of metric, they gets
1765 ECs.

Here we use the same evaluation metric with
Xue and Yang (2013). Note that we still cannot
describe all the 1838 ECs in the corpora, for on
some occasions ECs preceding the same token
share the same head word. We also omit some
ECs which cause cycles in dependency trees as
described in the previous sections. We have 1748
ECs, 95% of all the ECs in the test data, very
close to 1765 used by Xue and Yang (2013). The
total number of ECs has an impact on the re-
call. In Table 3, we include results based on
each method’s own EC count (1748, 1765, 1352
for Ours, Xue’s and Cai’s respectively) and the
real total EC count 1838 (figures in brackets).

Yang and Xue (2010) report an experiment
result based on a classification model in a unified

Type PRO pro T OP RNR * Others Total
297 298 575 527 32 19 0 1748

Xue 305 298 584 527 32 19 0 1765
Cai 299 290 578 134 32 19 0 1352

Table 2: EC Distribution in the Test Data

class correct p r F1
PRO 162 .479 .545 .510
pro 161 .564 .540 .552
OP 409 .707 .776 .740
T 506 .939 .88 .908
RNR 23 .767 .719 .742
* 0 0 0 0
Overall 1261 .712 .721 .717

(.686) (.699)
(Xue) 903 .653 .512 .574

(.491) (.561)
(Cai) 737 .660 .545 .586

(.401) (.499)

Table 3: Performance on the CTB Test Data

parsing frame. We do not include it for it uses
different and relativelyThe distributions of ECs
in the test data are shown in Table 2.

The results are shown in Table 3. We present
the results for each kind of EC and compare our
results with two previous state-of-the-art meth-
ods(Cai et al., 2011; Xue and Yang, 2013).

The proposed method yields the newest state-
of-the-art performances on CTB as far as we
know. We also identify the dependency types
between ECs and their heads. Some ECs, such
as pro and PRO, are latent subjects of sen-
tences. They usually serve as SBJ with very
few exceptions. While the others may play var-
ious roles. There are 31 possible (EC, Dep)
pairs. Using the same model, the overall result
is p = 0.701, r = 0.703, f = 0.702.

3.3.2 Analysis
We compare the effectiveness of different fea-

tures by eliminating each kind of features de-
scribed in the previous section. As Table 4
shows, the most important kind is the depen-
dency paths, which cause a huge drop in per-
formance if eliminated. Dependency paths en-
code words and path pattern information which
is proved essential for the detection of ECs. Be-
sides, headwords are also useful. While for the

268



-dep -head -following -nephews
F1 .501 .604 .703 .716

(-.216) (-.103) (-.014) (-.001)

Table 4: Effectiveness of Features

others, we cannot easily make the conclusion
that they are of little usage in the identification
of ECs. They are not fully explored in the pro-
posed model, but may be vital for EC detection
in reality.

Worth to mention is that of the several kinds
of ECs, the proposed method shows the best
performance on ECs of type T, which repre-
sents ECs that are the trace of “A’”-movement,
which moves a word to a position where no
fixed grammatical function is assigned. Here we
give an example:

“[ ] 看起来/seem A 喜欢/like B.”
“A 看起来/seem (EC) 喜欢/like B.”

A is moved to the head of the sentence as the
topic (topicalization) and left a trace which is
the EC. To detect this EC, we need information
about the action “喜欢/like”, the link verb “看起
来/seem” and the arguments “A” and “B”. ECs
of type T are very common in Chinese, since
Chinese is a topic-prominent language. Using
distributed representations, it is easy to encode
the context information in our feature vectors
for EC detection.

We also have satisfying results and significant
improvements for the other types except * (trace
of A-movement), which make up about 1% of all
the ECs in the test data. Partly because there
are too few * examples in the training data. We
need to further improve our models to detect
such ECs.

4 Discussion
The proposed method is capable of handling
large set of labels. Hence it is possible to detect
EC types and dependency types simultaneously.
Besides, some other NLP tasks can also be for-
mulated as annotation tasks, and therefore can
be resolved using the same scheme, such as the
frame identification for verbs (Hermann et al.,

2014).
This work together with some previous work

that uses classification methods (Cai et al., 2011;
Xue and Yang, 2013; Xue, 2007), regards ECs
in a sentence as independent to each other and
even independent to words that do not appear in
the feature vectors. Such an assumption makes
it easier to design models and features but does
not reflect the grammatic constraints of lan-
guages. For example, simple sentences in Chi-
nese contain one and only one subject, whether
it is an EC or not. If it is decided there is an EC
as a subject in a certain place, there should be no
more ECs as subjects in the same sentence. But
such an important property is not reflected in
these classification models. Methods that adopt
parsing techniques take the whole parse tree as
input and output a parse tree with EC anchored.
So we can view the sentence as a whole and deal
with ECs with regarding to all the words in the
sentence. Iida and Poesio (2011) also take the
grammar constraints into consideration by for-
mulating EC detection as an ILP problem. But
they usually yield poor performances compared
with classification methods partly because the
methods they use can not fully explore the syn-
tactic and semantic features.

5 Related Work

Empty category is a complex problem (Li and
Hovy, 2015). Existing methods for EC detec-
tion mainly explores syntactic and semantic fea-
tures using classification models or parsing tech-
niques.

Johnson (2002) proposes a simple pattern
based algorithm to recover ECs, both the posi-
tions and their antecedents in phrase structure
trees. Gabbard et al. (2006) presents a two stage
parser that uses syntactical features to recover
Penn Treebank style syntactic analyses, includ-
ing the ECs. The first stage, sentences are parse
as usual without ECs, and in the second stage,
ECs are detected using a learned model with rich
text features in the tree structures. Kong and
Zhou (2010) reports a tree kernel-based model
which takes as input parse trees for EC detec-
tion. They also deal with EC resolution, to

269



link ECs to text pieces if possible. They re-
ports their results on Chinese Treebank. Yang
and Xue (2010) try to restore ECs from parse
trees using a Maximum Entropy model. Iida
and Poesio (2011) propose an cross-lingual ILP-
based model for zero anaphora detection. Cai et
al. (2011) reports a classification model for EC
detection. Their method is based on “is there
an EC before a certain token”.

Recently Xue and Yang (2013) further de-
velop the method of Yang and Xue (2010) and
explore rich syntactical and semantical features,
including paths in parse trees and semantic
roles, to train an ME classification model for
EC detection and yield the best performance re-
ported using a strict evaluation metric on Chi-
nese Treebank as far as we know.

As we have stated, the traditional features
used by above methods are not good at cap-
turing the meanings of contexts. Currently the
distributed representations together with deep
neural networks have proven their ability not
only in representing meaning of words, inferring
words from the context, but also in represent-
ing structures of text (Socher et al., 2010; Li
et al., 2015). Deep neural networks are capable
of learning features from corpus, therefore saves
the labor of feature engineering and have proven
their ability in lots of NLP task (Collobert et al.,
2011; Bengio et al., 2006).

The most relevant work to this paper are that
of Weston et al. (2011) and that of Hermann
et al. (2014). Weston et al. (2011) propose a
deep neural network scheme exploring the hid-
den space for image annotation. They map both
the images and labels to the same hidden space
and annotate new images according to their rep-
resentations in the hidden space. Hermann et
al. (2014) extend the scheme to frame identifi-
cation, for which they obtain satisfying results.
This paper further uses it for empty category
detection with features designed for EC detec-
tion.

Compared with previous research, the pro-
posed model simplifies the feature engineering
greatly and produces distributed representations
for both ECs and EC types which will benefit
other tasks.

6 Conclusion
In this paper, we propose a new empty category
detection method using distributed word repre-
sentations. Using the word embeddings of the
contexts of ECs as features enables us to employ
rich information in the context without much
feature engineering. Experiments on CTB have
verified the advantages of the proposed method.
We successfully beat the existing state-of-the-
art methods based on a strict evaluation metric.
The proposed method can be further applied to
other languages such as Japanese. We will fur-
ther explore the feasibility of using neural net-
works to resolve empty categories: to link ECs
to their antecedents.

References
Yoshua Bengio, Holger Schwenk, Jean-Sébastien

Senécal, Fréderic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.

Shu Cai, David Chiang, and Yoav Goldberg. 2011.
Language-independent parsing with empty ele-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies: short
papers-Volume 2, pages 212–216. Association for
Computational Linguistics.

Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. In
Proceedings of EMNLP, pages 636–645. ACL.

Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing:
Deep neural networks with multitask learning. In
Proceedings of the 25th international conference
on Machine learning, pages 160–167. ACM.

Ronan Collobert, Jason Weston, Léon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (al-
most) from scratch. The Journal of Machine
Learning Research, 12:2493–2537.

Ryan Gabbard, Mitchell Marcus, and Seth Kulick.
2006. Fully parsing the penn treebank. In Pro-
ceedings of the main conference on human lan-
guage technology conference of the North Amer-
ican chapter of the association of computational
linguistics, pages 184–191. Association for Com-
putational Linguistics.

Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame

270



identification with distributed word representa-
tions. In Proceedings of ACL.

Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ilp solution to zero anaphora resolution. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies-Volume 1, pages 804–813.
Association for Computational Linguistics.

Mark Johnson. 2002. A simple pattern-matching
algorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Annual
Meeting on Association for Computational Lin-
guistics, pages 136–143. ACL.

Fang Kong and Hwee Tou Ng. 2013. Exploiting
zero pronouns to improve chinese coreference res-
olution. In EMNLP, pages 278–288.

Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 882–891. Association for Computa-
tional Linguistics.

Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2061–2069.

Jiwei Li and Eduard Hovy. 2015. The nlp engine: A
universal turing machine for nlp. arXiv preprint
arXiv:1503.00168.

Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Re-
cursive deep models for discourse parsing. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, pages
2061–2069.

Jiwei Li, Dan Jurafsky, and Eudard Hovy. 2015.
When are tree structures necessary for deep
learning of representations? arXiv preprint
arXiv:1503.00185.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational linguistics, 19(2):313–330.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Proceedings of NIPS2013, pages
3111–3119.

Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, wordnet and
wikipedia for coreference resolution. In Proceed-
ings of the main conference on Human Language
Technology Conference of the North American

Chapter of the Association of Computational Lin-
guistics, pages 192–199. Association for Computa-
tional Linguistics.

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recur-
sive neural networks. In Proceedings of the NIPS-
2010 Deep Learning and Unsupervised Feature
Learning Workshop, pages 1–9.

Nicolas Usunier, David Buffoni, and Patrick Galli-
nari. 2009. Ranking with ordered weighted pair-
wise classification. In Proceedings of ICML2009,
pages 1057–1064. ACM.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary im-
age annotation. In Proceedings of IJCAI2011, vol-
ume 11, pages 2764–2770.

Bing Xiang, Xiaoqiang Luo, and Bowen Zhou. 2013.
Enlisting the ghost: Modeling empty categories
for machine translation. In ACL (1), pages 822–
831. Citeseer.

Nianwen Xue and Yaqin Yang. 2013. Dependency-
based empty category detection via phrase struc-
ture trees. In HLT-NAACL, pages 1051–1060.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural
language engineering, 11(02):207–238.

Nianwen Xue. 2007. Tapping the implicit informa-
tion for the ps to ds conversion of the chinese tree-
bank. In Proceedings of the Sixth International
Workshop on Treebanks and Linguistics Theories.

Yaqin Yang and Nianwen Xue. 2010. Chasing the
ghost: recovering empty categories in the chinese
treebank. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics:
Posters, pages 1382–1390. ACL.

Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexi-
cal analyzer ictclas. In Proceedings of the sec-
ond SIGHAN workshop on Chinese language
processing-Volume 17, pages 184–187. Association
for Computational Linguistics.

271


