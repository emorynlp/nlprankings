










































VCA: An Experiment with a Multiparty Virtual Chat Agent


Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43–48,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

VCA: An Experiment With A Multiparty Virtual Chat Agent 

Samira Shaikh1, Tomek Strzalkowski1, 2, Sarah Taylor3, Nick Webb1 
1ILS Institute, University at Albany, State University of New York 

2Institute of Computer Science, Polish Academy of Sciences 
3Advancded Technology Office, Lockheed Martin IS&GS 

E-mail: ss578726@albany.edu, tomek@albany.edu 
 
 

Abstract 

The purpose of this research was to advance 
the understanding of the behavior of small 
groups in online chat rooms. The research was 
conducted using Internet chat data collected 
through planned exercises with recruited par-
ticipants. Analysis of the collected data led to 
construction of preliminary models of social 
behavior in online discourse. Some of these 
models, e.g., how to effectively change the 
topic of conversation, were subsequently im-
plemented into an automated Virtual Chat 
Agent (VCA) prototype. VCA has been dem-
onstrated to perform effectively and convinc-
ingly in Internet conversation in multiparty 
chat environments.  

1 Introduction 

Internet chat rooms provide a ready means of 
communication for people of most age groups 
these days. More often than not, these virtual 
chat rooms have multiple participants conversing 
on a wide variety of topics, using a highly infor-
mal and free-form text dialect. An increasing use 
of virtual chat rooms by a variety of demograph-
ics such as small children and impressionable 
youth leads to the risk of exploitation by deceit-
ful individuals or organizations. Such risks might 
be reduced by presence of virtual chat agents that 
could keep conversations from progressing into 
certain topics by changing the topic of conversa-
tion.  

Our aim was to study the behavior of small 
groups of online chat participants and derive 
models of social phenomena that occur fre-
quently in a virtual chat environment. We used 
the MPC chat corpus (Shaikh et al., 2010), which 
is 20 hours of multi-party chat data collected 
through a series of carefully designed online chat 
sessions. Chat data collected from public chat 
rooms, while easily available, presents signifi-
cant concerns regarding its adaptability for our 
research use. Publicly available chat data is com-

pletely anonymous, has a high level of noise and 
lack of focus, in addition to engendering user 
privacy issues for its use in modeling tasks. The 
MPC corpus was used in (1) understanding how 
certain social behaviors are reflected in language 
and (2) building an automated chat agent that 
could effectively achieve certain (initially lim-
ited) social objectives in the chat-room. A brief 
description of the MPC corpus and its relevant 
characteristics is given in Section 3 of this paper. 

One specific phenomenon of social behavior 
we wanted to model was an effective change of 
conversation topic, when a participant or a group 
of participants deliberately (if perhaps only tem-
porarily) shift the discussion to a different, pos-
sibly related topic. Both success and failure of 
these actions was of interest because the outcome 
depended upon the choice of utterance, the per-
sons to whom it was addressed, their reaction, 
and the time when it was produced. Our analysis 
of the corpus for such phenomena led to the use 
of an annotation scheme that allows us to anno-
tate for topic and focus change in conversation. 
We describe the annotation scheme used in Sec-
tion 4.  

We constructed an autonomous virtual chat 
agent (VCA) that could achieve initially limited 
social goals in a chat room with human partici-
pants. We used a novel approach of exploiting 
the topic of conversation underway to search the 
web and find related topics that could be inserted 
in the conversation to change its flow. We tested 
the first prototype with the capability to opportu-
nistically change to topic of conversation using a 
combination of linguistic, dialogic, and topic 
reference devices, which we observed effectively 
deployed by the most influential chat participants 
in the MPC corpus.  The VCA design, architec-
ture and mode of operation are described in de-
tail in Section 5 of this paper. 

2 Related Work 

Automated dialogue agents such as the early 
ELIZA (Weizenbaum, 1966) and PARRY 

43



(Colby, 1974) could conduct a one-on-one “con-
versation” with a human using rules and pattern-
matching algorithms. More recently, the addition 
of heuristic pattern matching in A.L.I.C.E 
(Wallace, 2008) led to development of chat bots 
using AIML1 and its variations, such as Project 
CyN2. Most of the work on conversational agents 
was limited to one-on-one situations, where a 
single agent converses with a human user, 
whether to perform a transaction (such as book-
ing a flight or banking transactions) (Hardy et al., 
2006) or for companionship (e.g., browsing of 
family photographs) (Wilks, 2010). Many of 
these systems were inspired by the challenge of 
the Turing Test or its more limited variants such 
as Loebner Prize. 
  Research in the field of developing a multi-user 
chat-room agent has been limited. This is some-
what surprising because a multi-user setting 
makes the agent’s task of maintaining conversa-
tion far less onerous than in one-on-one situa-
tions. In a chat-room, with many users engaged 
in conversations, it is much easier for an agent to 
pass as just another user. Indeed, a skillfully de-
signed agent may be able to influence an ongoing 
conversation. 

3 MPC Chat Corpus 
The MPC chat corpus is a collection of 20 hours 
of chat sessions with multiple participants (on 
average 4), conversing for about 90 minutes in a 
secure online chat room. The topics of conversa-
tion vary from free-flowing chat in the initial 
collection phase to allow participants to build 
comfortable a rapport with each other, to specific 
task-oriented dialogues in the latter phase; such 
as choosing the right candidate for a job inter-
view from a list of given resumes. This corpus is 
suitable for our research purposes since the chat 
sessions were designed around enabling the so-
cial phenomena we were interested in modeling. 

4 Annotation Scheme 

We wished to annotate the data we collected to 
derive models from language use for social phe-
nomena. These represent complex pragmatic 
concepts that are difficult to annotate directly, let 
alone detect automatically. Our approach was to 
build a multi-level annotation scheme.  

In this paper we briefly outline our annotation 
scheme that consists of three layers: communica-
                                                
1 http://www.alicebot.org/aiml.html 
2 http://www.daxtron.com/123start.htm?Cyn 

tive links, dialogue acts, and topic/focus changes. 
A more detailed description of the annotation 
scheme will be presented in a future publication.  

4.1 Communicative Links 
Annotators are asked to mark each utterance in 
one of three categories – utterance is addressed 
to a participant or a set of participants, it is in 
response to a specific prior utterance by another 
participant or it is a continuation of the partici-
pant’s own prior utterance. By an utterance, we 
mean the set of words in a single turn by a par-
ticipant. In multi-party chat, participants do not 
generally add addressing information in their 
utterances and it is often ambiguous to whom 
they are speaking. Communicative link annota-
tion allows us to accurately map who is speaking 
to whom in the conversation, which is required 
for tracking social phenomena across partici-
pants.  

4.2 Dialogue Acts 
At this annotation level, we developed a hierar-
chy of 20 dialogue acts, based loosely on 
DAMSL (Allen & Core, 1997) and SWBD-
DAMSL (Jurafsky et al., 1997), but greatly re-
duced and more tuned to dialogue pragmatics. 
For example, the utterance “It is cold here today” 
may function as a Response-Answer when given 
in response to a question about the weather, and 
would act as an Assertion-Opinion if it is evalu-
ated alone. The dialogue acts, thus augmented, 
become an important feature in modeling partici-
pant behavior for our research purpose. A de-
tailed description of the tags is beyond the scope 
of this paper. 

4.3 Topic and Focus boundaries 
The flow of discussion in chat shifts quite rapidly 
from one topic to another. Furthermore, within 
each topic (e.g., music bands) the focus of conver-
sation (e.g., dc for cutie) moves just as rapidly. We 
distinguish between topic and focus to accom-
modate both broader thematic shifts and more 
narrow aspect changes of the topic being dis-
cussed. For example, participants might discuss 
the topic of healthcare reform, by focusing on 
President Obama, and then switch the focus to some 
particulars of the reform, such as the “public op-
tion”. Similarly, topics may shift while the focus 
remains the same (e.g., moving on to Obama’s 
economic policies), although such changes are 
less common. Annotators typically marked the 
first mention of a substantive noun phrase as a 
topic or focus introduction. 

44



The effect of topic change is apparent when a 
subsequent utterance by another participant is 
about the same topic. This is a successful attempt 
at changing the topic. Shown in Figure 1 is an 
example of topic shift annotated in our data col-
lection. 

 

 
Figure 1. A topic change in dialogue, with three 

participants (AA, KA and KN) 
 

We found this model of topic change fairly con-
sistently exhibited, where the participants would 
ask an open question, in order to get other par-
ticipants to respond to them, thereby changing 
the course of conversation. We collected all ut-
terances marked topic shifts and focus shifts and 
created a set of templates from them.  These 
templates served as a model for the VCA to util-
ize when creating a response.  

Another model of behavior that we found as a 
consequence of topic change is topic sustain. 
This is an instance where the utterance is marked 
to be on the same topic as the one currently being 
discussed, for example, utterance 5 in Figure 1. 
These may be in the form of offering support or 
agreement with a previous utterance or asking a 
question about a new in-topic aspect. 

We gave our annotators a fair amount of lev-
erage on how to label the topics and how to rec-
ognize the focus. Our primary interest was in an 
accurate detection of topic/focus boundaries and 
shifts. Of the 14 sessions we selected from the 
MPC corpus, we selected 10 for annotation, with 
at least 3 annotators for each session. In Table 1 
some of the overall statistics computed from this 
set are shown. We computed inter-annotator 
agreement on all three levels of our annotation, 
i.e. Communication Links, Dialogue Acts and 

Topic/Focus Shifts. Topic and Focus shifts had 
the highest inter-annotator agreement scores on 
different measures such as Krippendorf’s Alpha 
(Krippendorff, 1980) and Fliess’ Kappa (Fliess, 
1971). In Figure 2, we show inter-annotator 
agreement measures on Topic/Focus shift anno-
tation for four of the annotated sessions. Krip-
pendorff’s Alpha and Fleiss’ Kappa measures 
show inter-annotator agreement on topic shift 
alone, and Conflated Krippendorff’s Alpha 
measures show the agreement when topic and 
focus are conflated as one category. With such 
high degree of agreement, we can reliably derive 
models of topic shift behavior from our anno-
tated data. 

 
Total Number of Sessions Annotated 10 
Number of annotators per file 3 
Total Utterances Annotated 4640 
Average number of utterances per ses-
sion ~520 

Total topics identified per session 174 
Total topic shifts identified per ses-
sion 344 

Table 1. Selected statistics from annotated 
data set 

 

 
Figure 2. Inter-annotator agreement measures for 

Topic/Focus shifts 

5 VCA Design 

A virtual chat agent is an automated program 
with the ability to respond to utterances in chat. 
Our VCA is distinctive in its ability to participate 
in multi-party chat and manage to steer the flow 
of conversation to a new topic. We exploit the 
dialogue mechanism underlying HITIQA (Small 
et al. 2009) to drive the dialogue in VCA.  
The topic as defined by the information con-
tained in the participant’s utterance is used to 
mine outside data sources (e.g., a corpus, the 
web) in order to locate and learn additional in-
formation about that topic. The objective is to 
identify some of the salient concepts that appear 

0	  
0.2	  
0.4	  
0.6	  
0.8	  
1	  

Krippendorff	  
's	  Alpha	  

Con9lated	  
Krippendorff	  
's	  Alpha	  

Fleiss	  '	  Kappa	  

AA 1: did anyone watch the morning talk 
shows today (MTP, for example)? 
KA 2: nope! 
AA 3: I missed them – I was hoping 
someone else had. 
AA 4: My kids tell me the band you’re 
going to hear (dc for cutie) is great. 
(TOPIC: music bands, FOCUS: dc for cutie) 
KA 5: oh cool! Their lyrics are nice, I 
think. 
(TOPIC: music bands, FOCUS: dc for cutie) 
KA 6. what kind of music do you guys 
listen to? 
(TOPIC: music, FOCUS: none) 
KN 7: I don’t really have a favorite 
genre….you on youtube right now? 
(TOPIC: music, FOCUS: youtube) 
 

45



associated with the topic, but are not directly 
mentioned in the utterance. Such associations 
may be postulated because additional concepts 
are repeatedly found near the concepts men-
tioned in the utterance.  
An illustrative example found in our annotated 
corpus is the utterance, “Lars Ulrich might have a 
thing or two to say about technology.” Here, the topic 
of conversation prior to this utterance was “tech-
nology” and it was changed to “music” after this 
utterance. Here, “Lars Ulrich” is the bridge that 
connects the two concepts “technology” and “mu-
sic” together. 

5.1 VCA Architecture 
The VCA is composed of the following modules 
that interact as shown in Figure 3.  

 
5.1.1 Chat Analyzer 
Every utterance in chat is first analyzed by the 
Chat Analyzer component. This process removes 
stop words, emoticons and punctuation, as well 
as any participant nicknames from the utterance. 
We postulate that the remaining content bearing 
words in the utterance represent the topic of that 
utterance. We call this analyzed utterance our 
chat “query” which is sent in parallel to the 
Document Retrieval and NL Processing compo-
nent.  

 
5.1.2 Document Retrieval 
The document retrieval process retrieves docu-
ments from either the web or a test document 

corpus. We use Google AJAX api for our web 
retrieval process and InQuery (Callan et al., 
1992) retrieval engine for our offline mode of 
operation to retrieve documents from the test 
corpus. The test document corpus was collected 
by mining the web for all utterances in our data 

collection, creating a stable document set for ex-
perimental purposes. Currently, the document 
corpus contains about 1Gb of text data.  

 
5.1.3 Clustering 
We cluster the paragraphs in documents retrieved 
using clustering method in Hardy et al. (Hardy et 
al., 2009) This process groups the paragraphs 
containing salient entities into sets of closely as-
sociated concepts. From each cluster, we choose 
the most representative paragraph, usually called 
the “seed” paragraph for further NL processing. 
Each seed paragraph and the chat query undergo 
the same further NL processing sequence.   

 
5.1.4 Natural Language Processing 
We process each chat query by performing 
stemming, part-of-speech tagging and named-
entity recognition on it. Each seed paragraph is 
also run through same three natural language 
processing tasks. We are using Stanford POS 
tagger for our part-of-speech tagging. For named 
entity recognition, we have the ability to choose 
between BBN’s IdentiFinder and AeroText™ 
(Taylor, 2004). 

 
5.1.5 Framing 
We build frames from the entities and attributes 
found in both the chat query and the paragraphs.. 
This work extends the concept of framing devel-
oped for HITIQA (Small et al, 2009) and COL-
LANE (Strzalkowski, 2009). Framing provides 
an informative handle on text, which can be ex-

ploited to compare the underlying textual repre-
sentations, as we explain in the next section. 

 
5.1.6 Scoring and Frame Matching 
Using the information in the frames built in the 
previous step; we compare the chat query frame 

Figure 3. VCA Architecture 

46



built from the chat query, to the frames created 
from the paragraphs, called paragraph frames. 
We assign a score for each paragraph frame 
based on how many attributes and their corre-
sponding values match; in the current version of 
VCA a very basic approach to counting how 
many attribute-value pairs match is taken. Of all 
the paragraph frames we select the highest scor-
ing frames and select the attribute-value pairs 
that are not part of the chat query frame. For ex-
ample, as shown in Figure 4a below, the chat 
utterance “Aruba might be nice!” created the fol-
lowing chat query frame. 

 

 
a. Example chat query frame 

 

 
b. Frame Matching, Scoring and Template  

Selection 
 

Figure 4. From frames to VCA responses 
 
Correspondingly, we select all PLACE type en-

tities from the highest-ranking paragraph frames. 
These are shown in Figure 4b as Aruba Entity 
list.  The entities “NASCAR”, “Women Seeking Men” 
and “Mateo” are not of entity type – PLACE, we 
assign them a score of 0. The score is the fre-
quency of occurrence of that entity in the para-
graph; in this example it is found to be 1. Assign-
ing scores by frequency of occurrence ensures 
that the most commonly occurring concept 
around the one that is being discussed in the chat 
query utterance will be used to respond with. 

 
5.1.7 Template Selection 
Once we have chosen the entity to respond with, 
we select a template from the set of templates for 
that entity. These are templates that are created 
based on the models created from topic change 
utterances annotated in our data set. For a select 
group of entities, which are quite frequently en-

countered in our data collection such as PLACE, 
PERSON, ORGANIZATION etc., we have a set of 
templates specific to that entity type. We also 
have several generic templates that may be used 
if the entity type does not match the ones that we 
have selected. For example, a PLACE specific 
template is “Have you ever been to __?” and a PER-
SON specific template is “You heard about __?”. Not 
all templates are formulated as questions. An-
other example of a generic template is “__rules!”.  

6 Example of VCA Interaction 

Figure 5 represents an example of the VCA in 
action in a simulated environment; the VCA is 
the participant “renee”. We can see how the con-
versation changes from “gun laws” to “hunting” 
after renee’s utterance at 11:48 AM. 

 

 
 

Figure 5. Topic change example 

7 Evaluation  
We ran two tests of this initial VCA prototype 

in a public chat-room. VCA was inserted into a 
public chat-room with multiple participants on 
two separate occasions. The general topic of dis-
cussion during both instances was “anime”. We 
have developed an evaluation protocol in order 
to test the effectiveness of the VCA prototype in 
a realistic setting. The initial metric of VCA ef-
fectiveness is the rate of involvement measured 
in the number of utterances generated by the 
VCA during the test period. These utterances are 
subsequently judged for appropriateness using 
the metric developed for the Companions Project 
(Webb, 2010). The actual appropriateness anno-
tation scheme can be quite involved, but for this 
simple test we reduced the coding to only binary 
assessment, so that the VCA utterances were an-
notated as either appropriate or inappropriate, 
given the content of the utterance and the flow of 
dialogue thus far. Using this coarse grain evalua-
tion on a live chat segment we noted that the 
VCA made 9 appropriate utterances and 7 inap-

[POS] 
NNP, Aruba 
JJ, nice 
[ENT] PLACE 
 

Aruba Entity List: 
VALUE = NASCAR and TYPE = ORGANIZATION 
and SCORE = 0 
VALUE = Dallas and TYPE = PLACE and SCORE = 
1 
VALUE = Mateo and TYPE = PERSON and SCORE 
= 0 
 
VCA: How about Dallas? 

47



propriate utterances, which gives the appropri-
ateness score of 56%. While some of VCA utter-
ances seem inappropriate (i.e., not related to the 
conversation topic), we noted also that other 
posters generally tolerated these inappropriate 
utterances that occurred early in the dialogue. 
Moreover, these early inappropriate utterances 
did generate appropriate responses from the hu-
man users. This “positive” dynamic changed 
gradually as the dialogue progressed, when the 
participants began to ignore VCA’s utterances.  

While this coarse grained evaluation is useful, 
our plan is to conduct evaluation experiments by 
recruiting subjects for chat sessions and inserting 
the VCA in the discussion. We will measure the 
impact of the VCA in the chat session by having 
participants fill out post-session questionnaires, 
which can elicit their responses regarding (a) if 
they detect presence of a VCA at any time during 
the dialogue; (b) who was the VCA; (c) who 
changed the topic of conversation most often; 
and so on. Another metric of interest is the level 
of engagement of the VCA, which can be meas-
ured by the number of direct responses to an ut-
terance by the VCA. We are developing the 
evaluation process, and report on the results in a 
separate publication. 

References  
Allen, J. M. Core. (1997). Draft of DAMSL: Dialog 

Act Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/resource
s/damsl/  

Callan, J. P., W. B. Croft, and S. M. Harding. 1992. 
The INQUERY Retrieval System, in Proceedings of 
the 3rd Inter- national Conference on Database and 
Expert Systems. 

 Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turing-
like indistinguishability tests for the validation of a 
computer simulation of paranoid processes. In: Ar-
tificial Intelligence , Vol. 3, p. 199-221. 

Fleiss, Joseph L. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 74(5):378{382. 

Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalk-
owski, Ting Liu, Bowden Wise and Xinyang 
Zhang. 2002. Cross-document summarization by 
concept classification. In Proceedings of ACM 
SIGIR '02 Conference, pages 121-128, Tampere, 
Finland. 

Hardy, H., A Biermann, R. Bryce Inouye, A. 
McKenzie, T. Strzalkowski, C. Ursu, N. Webb and 
M. Wu. 2006. The AMITIES System: Data-Driven 
Techniques for Automated Dialogue. In 

Speech Communication 48 (3-4), pages 354-
373.  Elsevier. 

Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca. 
(1997). Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual. 
http://stripe.colorado.edu/~jurafsky/manual.august
1.html 

Krippendorff, Klaus. 1980. Content Analysis, an In-
troduction to its Methodology. Sage Publications, 
Thousand Oaks, CA. 

Samira S., Tomek Strzalkowski, Sarah Taylor and 
Jonathan Smith (2009) Comparing an Integrated 
QA system performance - A Preliminary Model. 
Proceedings of PACLING Conference, Sapporo, 
Japan. 

Shaikh, S., Strzalkowski, T.,  Broadwell, A., Stromer-
Galley, J., Taylor, Sarah and Webb, N. 2010. Pro-
ceedings of LREC Conference, Malta. 

Sharon Small and Tomek Strzalkowski. 2009. 
HITIQA: High-Quality Intelligence through Inter-
active Question Answering. Journal of Natural 
Language Engineering, Vol. 15 (1), pp. 31—54. 
Cambridge.  

Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, 
Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony 
Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song 
Chen. 2009. COLLANE: An experiment in com-
puter-mediated tacit collaboration. In Aspects of 
Natural Language Processing (M. Marciniak and 
A. Mykowiecka, editors). Springer.  

Taylor, Sarah M. 2004. "Information Extraction 
Tools: Deciphering Human Language." IT Profes-
sional. Vol. 06, no. 6, pages: 28-34. Novem-
ber/December, 2004. Online. 
http://ieeexplore.ieee.org/iel5/6294/30282/0139087
0.pdf?tp=&arnumber=1390870&isnumber=30282 

Wallace, R. 2008. The Anatomy of A.L.I.C.E. In 
Parsing the Turing Test. (Robert Epstein, 
Gary Roberts and Grace Beber, editors). Springer. 

Webb, N., D. Benyon, P. Hansen and O. Mival. 2010. 
Evaluating Human-Machine Conversation for Ap-
propriateness. In Proceedings of the 7th Interna-
tional Conference on Language Resources and 
Evaluation (LREC2010), Valletta, Malta. 

Weizenbaum, Joseph. January 1966. "ELIZA — A 
Computer Program For the Study of Natural Lan-
guage Communication Between Man And Ma-
chine", Communications of the ACM 9 (1): 36–45. 

Wilks, Y. 2010. Artificial Companions. In: Y.Wilks 
(ed.) Close Engagement with Companions: scien-
tific, economic, psychological and philosophical 
perspectives. John Benjamins: Amsterdam. 

48


