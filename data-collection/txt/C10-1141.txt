Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1254–1262,

Beijing, August 2010

1254

Discriminative Training for Near-Synonym Substitution 

 
Liang-Chih Yu1, Hsiu-Min Shih2, Yu-Ling Lai2, Jui-Feng Yeh3 and Chung-Hsien Wu4

1Department of Information Management, Yuan Ze University 
2Department of Mathematics, National Chung Cheng University 

3Department of CSIE, National Chia-Yi University 

4Department of CSIE, National Cheng Kung University 

Contact: lcyu@saturn.yzu.edu.tw 

 

 

 

 

Abstract 

Near-synonyms  are  useful  knowledge  re-
sources  for  many  natural  language  applica-
tions such as query expansion for information 
retrieval  (IR)  and  paraphrasing  for  text  gen-
eration. However, near-synonyms are not nec-
essarily  interchangeable  in  contexts  due  to 
their  specific  usage  and  syntactic  constraints. 
Accordingly, it is worth to develop algorithms 
to verify whether near-synonyms do match the 
given contexts. In this paper, we consider the 
near-synonym substitution task as a classifica-
tion task, where a classifier is trained for each 
near-synonym  set  to  classify  test  examples 
into one of the near-synonyms in the set. We 
also propose the use of discriminative training 
to  improve  classifiers  by  distinguishing  posi-
tive  and  negative  features  for  each  near-
synonym.  Experimental  results  show  that  the 
proposed  method  achieves  higher  accuracy 
than both pointwise mutual information (PMI) 
and  n-gram-based  methods  that  have  been 
used in previous studies. 

Introduction 

1 
Near-synonym  sets  represent  groups  of  words 
with  similar  meaning,  which  are  useful  knowl-
edge resources for many natural language appli-
cations. For instance, they can be used for query 
expansion in information retrieval (IR) (Moldo-
van  and  Mihalcea,  2000;  Bhogal  et  al.,  2007), 
where a query term can be expanded by its near-
synonyms  to  improve  the  recall  rate.  They  can 
also be used in an intelligent thesaurus that can 
automatically suggest alternative words to avoid 
repeating  the  same  word  in  the  composing  of 
text  when  there  are  suitable  alternatives  in  its 

synonym  set  (Inkpen  and  Hirst,  2006;  Inkpen, 
2007). These near-synonym sets can be derived 
from  manually  constructed  dictionaries  such  as 
WordNet (called synsets) (Fellbaum, 1998), Eu-
roWordNet  (Rodríguez  et  al.,  1998),  or  clusters 
derived using statistical approaches (Lin, 1998). 
Although  the  words  in  a  near-synonym  set 
have  similar  meaning,  they  are  not  necessarily 
interchangeable in practical use due to their spe-
cific usage and collocational constraints. Pearce 
(2001)  presented  an  example  of  collocational 
constraints for the context “         coffee”. In the 
given near-synonym  set  {strong,  powerful},  the 
word “strong” is  more suitable than “powerful” 
to fill the gap, since “powerful coffee” is an anti-
collocation. Inkpen (2007) also presented several 
examples  of  collocations  (e.g.  ghastly  mistake) 
and  anti-collocations  (e.g.  ghastly  error).  Yu  et 
al.  (2007)  described  an  example  of  the  context 
mismatch problem for the context “        under 
the  bay”  and  the  near-synonym  set  {bridge, 
overpass,  viaduct,  tunnel}  that  represents  the 
meaning  of  a  physical  structure  that  connects 
separate  places  by  traversing  an  obstacle.  The 
original word (target word) in the given context 
is  “tunnel”,  and  cannot  be  substituted  by  the 
other words in the same set since all the substitu-
tions are semantically implausible. Accordingly, 
it  is  worth  to  develop  algorithms  to  verify 
whether near-synonyms do match the given con-
texts.  Applications  can  benefit  from  this  ability 
to provide more effective services. For instance, 
a  writing  support  system  can  assist  users  to  se-
lect  an  alternative  word  that  best  fits  a  given 
context from a list of near-synonyms. 

In measuring the substitutability of words, the 
co-occurrence information between a target word 

1255

(the  gap)  and  its  context  words  is  commonly 
used  in  statistical  approaches.  Edmonds  (1997) 
built a lexical co-occurrence network from 1989 
Wall  Street  Journal  to  determine  the  near-
synonym  that  is  most  typical  or  expected  in  a 
given context. Inkpen (2007) used the pointwise 
mutual  information  (PMI)  formula  to  select  the 
best  near-synonym  that  can  fill  the  gap  in  a 
given  context.  The  PMI  scores  for  each  candi-
date near-synonym are computed using a larger 
web corpus, the Waterloo terabyte corpus, which 
can  alleviate  the  data  sparseness  problem  en-
countered  in  Edmonds’  approach.  Following 
Inkpen’s  approach,  Gardiner  and  Dras  (2007) 
also used the PMI formula with a different cor-
pus  (the  Web  1T  5-gram  corpus)  to  explore 
whether near-synonyms differ in attitude. 

Yu et al. (2007) presented a method to com-
pute  the  substitution  scores  for  each  near-
synonym based on n-gram frequencies obtained 
by querying Google. A statistical test is then ap-
plied to determine whether or not a target word 
can  be  substituted  by  its  near-synonyms.  The 
dataset  used  in  their  experiments  are  derived 
from  the  OntoNotes  copus  (Hovy  et  al.,  2006; 
Pradhan et al., 2007), where each near-synonym 
set  corresponds  to  a  sense  pool  in  OntoNotes. 
Another  direction  to  the  task  of  near-synonym 
substitution  is  to  identify  the  senses  of  a  target 
word  and  its  near-synonyms  using  word  sense 
disambiguation (WSD), comparing whether they 
were of the same sense (McCarthy, 2002; Dagan 
et al., 2006). Dagan et al. (2006) described that 
the use of WSD is an indirect approach since it 
requires  the  intermediate  sense  identification 
step, and thus presented a sense matching tech-
nique to address the task directly. 

In  this  paper,  we  consider  the  near-synonym 
substitution task as a classification task, where a 
classifier is trained for each near-synonym set to 
classify  test  examples  into  one  of  the  near-
synonyms  in  the  set.  However,  near-synonyms 
share  more  common  context  words  (features) 
than  semantically  dissimilar  words  in  nature. 
Such  similar  contexts  may  decrease  classifiers’ 
ability  to  discriminate  among  near-synonyms. 
Therefore,  we  propose  the  use  of  a  supervised 
discriminative  training  technique  (Ohler  et  al., 
1999; Kuo and Lee, 2003; Zhou and He, 2009) 
to improve classifiers by distinguishing positive 
and negative features for each near-synonym. To 

Sentence: This will make the           message 
easier to interpret. 
Original word: error 
Near-synonym set: {error, mistake, oversight}

Figure 1. Example of a near-synonym set and a 
sentence to be evaluated. 

our  best  knowledge,  this  is  the  first  study  that 
uses discriminative training for near-synonym or 
lexical  substitution.  The  basic  idea  of  discrimi-
native training herein is to adjust feature weights 
according  to  the  minimum  classification  error 
(MCE) criterion. The features that contribute to 
discriminating  among  near-synonyms  will  re-
ceive  a  greater  positive  weight,  whereas  the 
noisy  features  will  be  penalized  and  might  re-
ceive  a  negative  weight.  This  re-weighting 
scheme helps increase the separation of the cor-
rect class against its competing classes, thus im-
proves the classification performance.  

The proposed supervised discriminative train-
ing is compared with two unsupervised methods, 
the PMI-based (Inkpen, 2007) and n-gram-based 
(Yu  et  al.,  2007)  methods.  The  goal  of  the 
evaluation is described as follows. Given a near-
synonym set and a sentence with one of the near-
synonyms  in  it,  the  near-synonym  is  deleted  to 
form  a  gap  in  this  sentence.  Figure  1  shows  an 
example. Each method is then applied to predict 
an  answer  (best  near-synonym)  that  can  fill  the 
gap.  The  possible  candidates  are  all  the  near-
synonyms  (including  the  original  word)  in  the 
given set. Ideally, the correct answers should be 
provided by human experts. However, such data 
is usually unavailable, especially for a large set 
of test examples. Therefore, we follow Inkpen’s 
experiments to consider the original word as the 
correct answer. The proposed methods can then 
be evaluated by examining whether they can re-
store the original word by filling the gap with the 
best near-synonym.  

The rest of this work is organized as follows. 
Section  2  describes  the  PMI  and  n-gram-based 
methods  for  near-synonym  substitution.  Section 
3 presents the discriminative training technique. 
Section 4 summarizes comparative results. Con-
clusions are finally drawn in Section 5. 

1256

2

P x y
( ,
)
P x P y
( )
( )
)
( ,

2  Unsupervised Methods 
2.1  PMI-based method 
The  mutual  information  can  measure  the  co-
occurrence  strength  between  a  near-synonym 
and the words in a given context. A higher mu-
tual  information  score  indicates  that  the  near-
synonym  fits  well  in  the  given  context,  thus  is 
more likely to be the correct answer. The point-
wise  mutual  information  (Church  and  Hanks, 
1991) between two words x and y is defined as  

PMI x y
( ,

)

=

log

,

  

         (1) 

)

)

=

( ,

P x y
( ,

C x y N

 denotes  the  prob-
where 
ability  that  x  and  y  co-occur; 
C x y  is  the 
number of times x and y co-occur in the corpus, 
and N is the total number of words in the corpus. 
,  where  C(x)  is  the 
Similarly, 
number  of  times  x  occurs  in  the  corpus,  and 
P y
,  where  C(y)  is  the  number  of 
( )
times y occurs in the corpus. Therefore, (1) can 
be re-written as  

C y N

C x N

P x
( )

( )

( )

=

=

PMI x y
( ,

)

=

log

2

)

C x y N
( ,
⋅
C x C y
( )
( )

⋅

.

 

         (2) 

Inkpen (2007) computed the PMI scores for each 
near-synonym  using  the  Waterloo  terabyte  cor-
pus  and  a  context  window  of  size  2k  (k=2). 
Given 
gap, 
s
...
, the PMI score for 
a near-synonym NSi to fill the gap is defined as  

sentence 
w+
...
1
2

s  with 
...

a 
w w
k
1

w
       
k

...

a 

=

k

PMI NS s
, )

(

j

=

                        

∑
∑

k
i
1
=
k
2
i k
1
= +

,

)

(

PMI NS w
i
PMI NS w
i

(

,

j

j

+

).

           (3) 

The near-synonym with the highest score is con-
sidered as the answer. In this paper, we use the 
Web 1T 5-gram corpus to compute PMI scores, 
the  same  as  in  (Gardiner  and  Dras,  2007).  The 
frequency  counts  C(‧)  are  retrieved  from  this 
corpus  in  the  same  manner  within  the  5-gram 
boundary.  
2.2  N-gram-based method 
The n-grams can capture contiguous word asso-
ciations in given contexts. Given a sentence with 
a gap, the n-gram scores for each near-synonym 

are  computed  as  follows.  First,  all  possible  n-
grams containing the gap are extracted from the 
sentence.  Each  near-synonym  then  fills  the  gap 
to compute a normalized frequency according to  

log

(

Z ngram

(

i
NS

j

)

=

C ngram
(
i
NS
C NS
)
log (

j

j

)
) 1
+

,

        (4) 

Z ngram
(

i
NS

j

),

        (5) 

j

)

(

(

)

(

i
NS

i
NS

C ngram

where 
 denotes the frequency of an 
C NS  
n-gram  containing  a  near-synonym, 
)j
denotes  the  frequency  of  a  near-synonym,  and 
Z ngram
 denotes  the  normalized  frequency 
of an n-gram, which is used to reduce the effect 
of high frequencies on measuring n-gram scores. 
All  of  the  above  frequencies  are  retrieved  from 
the Web 1T 5-gram corpus.  

The  n-gram  score  for  a  near-synonym  to  fill 

j

the gap is computed as  
1
= ∑
R
NGRAM NS s
, )

NGRAM NS s
, )

(

R

j

i

1
=
 denotes 

j

(

where 
the  n-gram 
score of a near-synonym, which is computed by 
averaging  the  normalized  frequencies  of  all  the 
n-grams  containing  the  near-synonym,  and  R  is 
the total number of n-grams containing the near-
synonym.  Again,  the  near-synonym  with  the 
highest score is the proposed answer. We herein 
use  the  4-gram  frequencies  to  compute  n-gram 
scores as Yu et al. (2007) reported that the use of 
4-grams  achieved  higher  performance  than  the 
use of bigrams and trigrams.  
3  Discriminative Training 
3.1  Classifier 
The supervised classification technique can also 
be  applied  to  for  near-synonym  substitution. 
Each  near-synonym  in  a  set  corresponds  to  a 
class. The classifiers for each near-synonym set 
are  built  from  the  labeled  training  data,  i.e.,  a 
collection  of  sentences  containing  the  near-
synonyms.  Such  training  data  is  easy  to  obtain 
since it requires no human annotation. The train-
ing  data  used  herein  is  collected  by  extracting 
the 5-grams containing the near-synonyms from 
the  Web  1T  5-gram  corpus.  The  features  used 
for training are the words occurring in the con-
text of the near-synonyms in the 5-grams.  

1257

)
)
)

g x M
( ,
1
g x M
( ,
2
g x M
( ,
3
max

j k
≠

1

 
 
x m  
)
cos( ,
=
x m  
)
cos( ,
=
x m  
cos( ,
)
=
3
ig x M  
)
( ,
=
kd x M  
( ,
) =
kl x M  
( ,
) =

2

Example 

2 
0.6* 
0.5 
0.4 
0.5 
-0.1 

3 
0.8 
0.3* 
0.1 
0.8 
0.5 

1 
0.9* 
0.3 
0.2 
0.3 
-0.6 

0.047 

0.378 

0.924

                (γ=5) 

Table  1.  Examples  of  correct  classification 
and misclassification. * denotes the scores of the 
correct class.  

For each near-synonym set, an  F K×

 feature-
class matrix, denoted as M, is created for classi-
fication. The rows represent the F distinct words 
(features) and the columns represent the K near-
synonyms  (classes)  in  the  same  set.  Each  entry 
in the matrix, mij, represents a weight of word i 
respect to near-synonym j, which is computed as 
the number of times word i appears in the con-
texts  of  near-synonym  j  divided  by  the  total 
number  of  context  words  of  near-synonym  j. 
This  frequency-based  weight  can  then  be  trans-
formed into a probabilistic form, i.e., divided by 
the  sum  of  the  weights  of  word  i  respect  to  all 
near-synonyms. Each test sentence is also trans-
formed into an F-dimensional feature vector. Let 
x
 denote  the  feature  vector  of 
an  input  sentence.  The  classification  is  per-
formed  by  computing  the  cosine  similarity  be-
tween x and the column vectors (near-synonyms) 
in the matrix, defined as 

,...,

,...,

x
1[

x
i

=

x

]

F

 

=

NS

∧
j

arg max cos( ,

x m

j

)

j

        

=

arg max

j

 

 

i
x m
   
x m

j

j

                             (6)

 

        

=

arg max

j

∑

F
i
1
=
2

∑
x
i

F
i
1
=

x m
i
ij
∑
F
i
1
=

,

2

m
ij

jm  is the j-th column vector in the matrix 
where 
M.  The  near-synonym  with  the  highest  cosine 
NS ,  is  the  predicted  class  of 
similarity  score, 
the classifier. 

∧
j

3.2  Minimum classification error criterion 
According to the decision rule of the classifier, a 
classification  error  will  occur  when  the  near-
synonym with the highest cosine score is not the 
correct  class.  Table  1  shows  some  examples, 
where Example 3 is an example of misclassifica-
tion. On the other hand, although Example 2 is a 
correct  classification,  it  might be  an  ambiguous 
case  to  classifiers  since  the  scores  are  close 
among classes. Therefore, if we can increase the 
separation  of  the  correct  class  from  its  compet-
ing ones, then the classification performance can 
be  improved  accordingly.  This  can  be  accom-
plished  by  adjusting  the  feature  weights  of  the 
matrix M that have direct influence on the com-
putation  of  cosine  scores.  The  discriminative 
training  performs  the  adjustment  in  the  training 
phase  according  to  the  minimum  classification 
error criterion. The detailed steps are as follows. 
Given  an  input  vector  x,  the  classifier  com-
putes the cosine scores between x and each class 
using  (6).  The  discriminant  function  for  a  class 
can thus be defined as the cosine measure; that is, 
          (7) 

x m  
).

cos( ,

g x M
j

( ,

=

)

 

j

where  j  denotes  a  class  in  K.  Since  the  correct 
class of each input vector is known in the train-
ing phase, we can determine whether or not the 
input  vector  is  misclassified  by  comparing  the 
discriminant  function  (cosine  score)  of  the  cor-
rect  class  against  its  competing  classes.  In  the 
case of misclassification, the cosine score of the 
correct class will be smaller than the competing 
cosine scores. Let k be the correct class of x, the 
misclassification function can be defined as  

 

k

)

)

)

)

+

),

( ,

( ,

( ,

( ,

( ,

= −

d x M
k

G x M
 

g x M
 
k

           (8) 
kg x M  is the discriminant function for 
where 
the  correct  class  k,  and 
kG x M  is  the  anti-
discriminant  function  that  represents  the  other 
1K −  competing classes, defined as 
1
⎤
η
)
η
⎥
⎦

⎡
= ⎢
⎣
When 
,  the  anti-discriminant  function 
kG x M  is  the  average  of  all  the  competing 
( ,
cosine  scores.  With 
increase  of  η , 
kG x M is gradually dominated by the biggest 

1η=
)

g x M
j

G x M

1
−

∑

the 

( ,

( ,

( ,

K

             (9) 

1

j k
≠

)

)

,

k

1258

competing  class.  In  the  extreme  case,  i.e., 
η→ ∞ , the anti-discriminant function becomes 

G x M

( ,

k

) max  
=

j k
≠

g x M
j

( ,

).

 

 

       (10) 

 

j k
≠

),

( ,

( ,

( ,

( ,

= −

) max  
+

g x M
j

g x M
k

d x M
k

The misclassification function in (8) can thus be 
rewritten as 
)

   (11) 
In this case, the classification error is determined 
by  comparing  the  discriminant  function  of  the 
correct class against that of the biggest compet-
ing  class.  Obviously, 
 implies  a 
classification  error.  For  instance,  in  Example  3, 
the discriminant function for the correct class is 
g x M =
2( ,
, and that of the biggest compet-
ing  class  is 
,  thus 
. On the 
the classification error is 
other  hand,  the  classification  error  will  be  a 
negative  value  for  correct  classifications,  as 
shown in Example 1 and 2. 

g x M g x M
),
( ,
1
3
kd x M
)
( ,

kd x M >
) 0

0.3
max(

))
0.5=

Intuitively,  a  greater  classification  error  also 
results in a greater loss.  We herein use the sig-
moid function as the class loss function; that is, 

0.8=

( ,

)

l x M l d
(
k

( ,

=

)

)

=

k

1
1 exp
+

,

 

d
γ−

k

        (12) 

where  γ is a constant that controls the slope of 
the  sigmoid  function.  The  sigmoid  function 
maps the values of classification error within the 
range  of  0  to  1.  For  correct  classifications,  a 
greater  separation  of  the  correct  class  from  the 
biggest competing class leads to a greater nega-
tive value of dk, i.e., a smaller classification error, 
resulting  in  a  smaller  loss  tends  asymptotically 
to  0  (Example  1),  whereas  a  moderate  loss  is 
yielded if the separation was close (Example 2). 
For the cases of misclassification, a greater sepa-
ration leads to a greater positive value of dk, i.e., 
a  greater  classification  error,  resulting  in  a 
greater loss tends asymptotically to 1 (Example 
3). The overall loss of the entire training set can 
then be estimated as 

L M
(

)

K

1
= ∑ ∑
Q

x C
1
= ∈
k

k

l x M
k

( ,

),

 

                     (13) 

+

m
t
( 1)
ij

where  Ck  denotes  the  set  of  training  vectors  of 
class k, and Q is the total number of vectors in 
the training set. The goal now is to minimize the 
loss. According to the above discussions on the 

three examples, to minimize the loss is to mini-
mize the classification error, and to improve the 
separation of the correct class against its compet-
ing classes. This can be accomplished by adjust-
ing the feature weights of the matrix M to distin-
guish  positive  and  negative  features  for  each 
class.  We  herein  adopt  a  gradient  descent 
method such as the generalized probabilistic de-
scent (GPD) algorithm (Katagiri et al., 1998) to 
update  the  matrix  M.  The  detailed  steps  are  as 
follows. 

Let the feature weights of the matrix M be the 
parameter  set  to  be  adjusted.  The  adjustment  is 
performed iteratively according to the following 
update formula. 
t
( )

       (14) 
where  t  denotes  the  t-th  iteration, εt  is  an  ad-
justment  step  of  a  small  positive  real  number, 
is  the  gradient  of  the  loss 
and 
function,  which  is  computed  by  the  following 
two parts 

l x M
k

kl x M
(

t
( 1)
+ =

− ∇

ε
t

M

M

∇

),

t
( )

t
( )

t
( )

t
( )

(

)

,

 

,

∇

t
( )

l x M
k

(

,

t
( )

)

=

l
∂
k
d
∂

k

d x M
∂
k

(

,
t
( )
m
∂
ij

t
( )

)

,

         (15) 

where  

l
∂
k
d
∂

k

=

l d
(
γ
k

k

)(1

−

l d
(
k

k

)),

 

 

       (16) 

and from (7), (8), and (9),  

t
( )

)

d x M
∂
k

(

,
t
( )
m
∂
ij

⎧
⎪
= ⎨
⎪
⎩

−

x
i

x
  
i

j
,                                          if  
G x M g x M
)
η

)
,
g x M
j

j
, if  

)
1
η
−

(
t
( )

t
( )

t
( )

(

(

,

,

k

j

∑

j k
≠

=

k

,

≠

k

                                                                         (17) 
where xi is an element of the input vector x. By 
 in (14) with the two parts 
replacing 
in (15), at each iteration each feature weight mij 
in M is adjusted by 
  

l x M
k

∇

(

)

,

t

t

+

ε
t

x
i

,                                         if  

j

=

k

⎧
m
t
( )
⎪
ij
⎪= ⎨
⎪
m
t
( )
ij
⎪
⎩

k

l
∂
k
d
∂
l
∂
k
d
∂
 

 

.

−

ε
t

x
i

,

(

t
( )

t
( )

G x M g x M
k
)
η

)
,
g x M
j

(
t
( )

j

)
1
−
η

k

(

,
 
                     (18) 
The weight xi represents whether or not a dimen-
sion  word  occurs  in  an  input  sentence.  A  zero 

j k
≠

 

∑
 

, if  

j

≠

k

1259

weight  indicates  that  the  dimension  word  does 
not  occur  in  the  input  sentence,  thus  the  corre-
sponding dimension of each column vector will 
not be adjusted. On the contrary, the correspond-
ing  dimension  of  the  column  vector  of  the  cor-
k= ) is adjusted by adding a value, 
rect class ( j
while those of the competing classes ( j
k≠ ) are 
adjusted by subtracting a value from them. After 
a sequence of adjustments over the training set, 
the positive and negative features can be distin-
guished by adjusting their weights that result in a 
greater  positive  or  negative  value  for  each  of 
them. The separation of the correct class against 
its competing ones can thus be increased.  

k

k

k

l
∂

d
∂

d
∂

l
∂
k

The weight adjustment in (18) is in proportion 
to  the  adjustment  step εt  and  the  slope  of  the 
.  The  adjustment  step 
sigmoid  function 
εt  can be determined empirically. As (16) shows, 
the  slope 
 converges  asymptotically  to 
zero as the classification error dk approaches to a 
very large (or small) value. This leads to a small 
weight adjustment. For instance, the weight ad-
justment  in  Example  1  is  small  due  to  a  small 
value of dk, or, say, due to a large separation be-
tween  the  correct  class  and  its  competing  ones. 
This is reasonable because classifiers often per-
form  well  in  such  cases.  Similarly,  the  weight 
adjustment  in  Example  3  (misclassification)  is 
also small due to a large  value of dk. A greater 
adjustment is not employed because such a large 
separation is difficult to be reduced significantly. 
Additionally,  over-adjusting  some  features  may 
introduce  negative  effects  on  other  useful  fea-
tures  in  the  matrix.  Therefore,  discriminative 
training  is  more  effective  on  the  cases  with  a 
moderate value of dk, like Example 2. Such cases 
usually  fall  within  the  decision  boundary  and 
tend  to  be  confusing  to  classifiers.  Hence,  im-
proving  the  separation  on  such  cases  helps  sig-
nificantly improve the classification performance. 
4  Experimental Results 
4.1  Experiment setup 
1)  Data:  The  near-synonym  sets  used  for  ex-
periments included the seven sets (Exp1) and the 
eleven  sets  (Exp2)  used  in  the  previous  studies 
(Edmonds,  1997;  Inkpen,  2007;  Gardiner  and 
Dras, 2007), as shown in Table 2. The Web 1T 
5-gram  corpus  was  used  to  build  classifiers, 

0.76

0.74

y
c
a
r
u
c
c
A

0.72

0.7

0.68

Test set
Development set

0

20

40

60

Iteration

80

100

Figure 2. The change of classification accuracy 
during discriminative training.  

where the corpus was randomly split into a train-
ing set, a development set, and a test set with an 
8:1:1 ratio. For efficiency consideration, we ran-
domly sampled up to 100 5-grams from the test 
set  for  each  near-synonym.  This  sampling  pro-
cedure was repeated five times for evaluation of 
the classifiers. 
2)  Classifiers:  The  classifiers  were 
imple-
mented  using  PMI,  n-grams,  and  discriminative 
training (DT) methods, respectively.  
PMI:  Given  a  near-synonym  set  and  a  test  5-
gram with a gap, the PMI scores for each near-
synonym  were  calculated  using  (3),  where  the 
size of the  context window k  was  set  to 2. The 
frequency  counts  between  each  near-synonym 
and  its  context  words  were  retrieved  from  the 
training set. 
NGRAM:  For  each  test  5-gram  with  a  gap,  all 
possible  4-grams  containing  the  gap  were  first 
extracted  (excluding  punctuation  marks).  The 
averaged  4-gram  scores  for  each  near-synonym 
were  then  calculated  using  (5).  Again,  the  fre-
quency  counts  of  the  4-grams  were  retrieved 
from the training set. 
DT:  For  each  near-synonym  set,  the  matrix  M 
was  built  from  the  training  set.  Each  5-gram  in 
the development set was taken as input to itera-
tively compute the cosine score, loss, classifica-
tion error, respectively, and finally to adjust the 
feature weights of M. The parameters of DT in-
cluded η for  the  anti-discriminative  function,  γ 

1260

No. 

Near-synonym set 

difficult, hard, tough 
error, mistake, oversight 
job, task, duty 
responsibility, burden, obligation, commitment 

1. 
2. 
3. 
4. 
5.  material, stuff, substance 
6. 
7. 

give, provide, offer 
settle, resolve 

Exp1

1. 
2. 
3. 
4. 

benefit, advantage, favor, gain, profit 
low, gush, pour, run, spout, spurt, squirt, stream 
deficient, inadequate, poor, unsatisfactory 
afraid, aghast, alarmed, anxious, apprehensive, 
fearful, frightened, scared, terror-stricken* 
disapproval, animadversion*, aspersion*, blame, 
criticism, reprehension* 

5. 
6.  mistake, blooper, blunder, boner, contretemps*,  
7. 
8. 

error, faux pas*, goof, slip, solecism* 
alcoholic, boozer*, drunk, drunkard, lush, sot 
leave, abandon, desert, forsake 
opponent, adversary, antagonist, competitor, 
enemy, foe, rival 
thin, lean, scrawny, skinny, slender, slim, spare, 
svelte, willowy*, wiry 
lie, falsehood, fib, prevarication*,  
rationalization, untruth 

9. 

10. 

11. 

No. of
cases
300 
300 
300 
400 
300 
300 
200 
2,100
500 
800 
400 
789 

300 

618 

433 
400 

700 

734 

425 

NGRAM 

58.60 
68.47 
68.93 
69.80 
70.20 
58.87 
69.30 
66.33 
71.44 
65.45 
65.65 
49.84 

72.80 

62.27 

64.90 
65.85 

58.51 

57.74 

57.55 

Accuracy (%) 
PMI 
61.67 
78.33 
70.40 
66.95 
67.93 
66.47 
68.10 
68.50 
69.88 
65.00 
69.40 
44.74 

COS 
60.13 
77.20 
74.00 
68.75 
71.07 
64.13 
77.10 
69.94 
69.44 
68.68 
70.35 
47.00 

DT 
63.13 
79.20 
75.67 
69.55 
75.13 
68.27 
84.10 
72.89 
71.36 
71.08 
74.35 
49.33 

79.47 

80.00 

82.53 

59.61 

80.65 
66.05 

59.51 

68.41 

77.88 
69.35 

63.31 

71.65 

84.34 
74.35 

67.14 

61.99 

55.72 

64.58 

63.58 

69.46 

74.21 

69.26 
Table 2. Accuracy of classifiers on Exp1 (7 sets) and Exp2 (11 sets). The words marked with * are 

61.69 

65.15 

63.32 

6,099

Exp2

excluded from the experiments because their 5-grams are very rare in the corpus. 

,

35γ=

25η=

tε for  the  adjust-
for  the  sigmoid  function,  and 
,  and 
ment  step.  The  settings, 
,  were  determined  by  performing  DT 
310ε
−
=t
for  several  iterations  through  the  training  set. 
These  setting  were  used  for  the  following  ex-
periments. 
3)  Evaluation  metric:  The  answers  proposed 
by  each  classifier  are  the  near-synonyms  with 
the  highest  score.  The  correct  answers  are  the 
near-synonyms originally in the gap of the test 5-
grams. The performance is measure by the accu-
racy, which is defined as the number of correct 
answers made by each classifier, divided by the 
total number of test 5-grams. 

In the following sections, we first demonstrate 
the  effect  of  DT  on  classification  performance, 
followed by the comparison of the classifiers. 
4.2  Evaluation on discriminative training 
This  experiment  is  to  investigate  the  perform-
ance change during discriminative training. Fig-
ure  2  shows  the  accuracy  at  the  first  100  itera-
tions for both development set and test set, with 
the 8th set in Exp2 as an example. The accuracy 
increased  rapidly  in  the  first  20  iterations,  and 
stabilized  after  the  40th  iteration.  The  discrimi-
native training is stopped until the accuracy has 
not been changed over 30 iterations or the 300th 
iteration has been reached. 

1261

Features 

made 
biggest 
message 
internal 

supervision 

audit 

Near-synonym set 

mistake 
0.076 
0.074 
-0.004 
0.001 
-0.001 
-0.002 

error 
-0.004 
-0.001 
0.039 
0.026 
-0.006 
-0.003 

oversight
-0.005 
-0.004 
-0.010 
-0.001 
0.031 
0.028 

Table 3. Example of feature weights after dis-
criminative training.  

4.3  Comparative results 
Table  2  shows  the  comparative  results  of  the 
classification accuracy for the 18 near-synonym 
sets  (Exp1  +  Exp2).  The  accuracies  for  each 
near-synonym set were the average accuracies of 
the five randomly sampled test sets. The cosine 
measure  without  discrimination  training  (COS) 
was also considered for comparison. The results 
show that NGRAM performed worst among the 
four classifiers. The major reason is that not all 
4-grams of the test examples can be found in the 
corpus. Instead of contiguous word associations 
used  by  NGRAM,  PMI  considers  the  words  in 
the  contexts  independently  to  select  the  best 
synonyms.  The  results  show  that  PMI  achieved 
better  performance  than  NGRAM.  The  two  su-
pervised  methods,  COS  and  DT,  outperformed 
the  two  unsupervised  methods,  NGRAM  and 
PMI. As indicated in the bold numbers, using the 
supervised  method  alone  (without  DT),  COS 
yielded higher average accuracy by 5% and 2% 
over  NGRAM  and  PMI,  respectively,  on  Exp1, 
and by 6% and 3%, respectively, on Exp2. When 
DT  was  employed,  the  average  accuracy  was 
further  improved  by  4%  and  6%  on  Exp1  and 
Exp2, respectively, compared with COS. 

The use of DT can improve the classification 
accuracy mainly because it can adjust the feature 
weights iteratively to improve the separation be-
tween  the  correct  class  and  its  competing  ones, 
which helps tackle the ambiguous test examples 
that  fall  within  the  decision  boundary.  Table  3 
presents  several  positive  and  negative  features 
for  the  near-synonym  set  {mistake,  error,  over-
sight}.  The  feature  weights  were  adjusted  ac-

Exp1 
NGRAM

PMI 
COS 
DT 
Exp2 
NGRAM

PMI 
COS 
DT 

Rank 1 
66.33% 
68.50% 
69.94% 
72.89% 
Rank 1 
61.69% 
63.32% 
65.15% 
69.26% 

Rank 2 
79.35% 
88.99% 
89.93% 
91.06% 
Rank 2 
68.48% 
79.11% 
80.52% 
82.86% 

Diff. 

+19.63% 
+29.91% 
+28.58% 
+24.93% 

Diff. 

+11.01% 
+24.94% 
+23.59% 
+19.64% 

Table  4.  Accuracy  of  Rank  1  and  Rank  2  for 
each classifier.  

cording  to  their  contributions  to  discriminating 
among  the  near-synonyms  in  the  set.  For  in-
stance,  the  features  “made”  and  “biggest”  both 
received a positive value for the class “mistake”, 
and  a  negative  value  for  the  competing  classes 
“error”  and  “oversight”.  These  positive  and 
negative weights help distinguish useful features 
from  noisy  ones  for  classifier  training.  On  the 
other  hand,  if  the  feature  weights  were  evenly 
distributed  among  the  classes,  these  features 
would not be unlikely to contribute to the classi-
fication performance.  
4.4  Accuracy of Rank 1 and Rank 2 
The  accuracy  presented  in  Table  2  was  com-
puted based on the classification results at Rank 
1,  i.e.,  a  test  sample  was  considered  correctly 
classified  only  if  the  near-synonym  with  the 
highest score was the word originally in the gap 
of  the  test  sample.  Similarly,  the  accuracy  at 
Rank 2 can be computed by considering the top 
two near-synonyms proposed by each classifier. 
That  is,  if  either  the  near-synonym  with  the 
highest or the second highest score was the cor-
rect answer, the test sample was considered cor-
rectly classified. Table 4 shows the accuracy of 
Rank  1  and  Rank  2  for  each  classifier.  The  re-
sults show that the improvement of Rank 1 accu-
racy  on  Exp1  was  about  20  to  30  percentage 
points, and was 25.76 in average. For Exp2, the 
average  improvement  was  19.80  percentage 
points 

1262

5  Conclusion  
This  work  has  presented  the  use  of  discrimina-
tive training for near-synonym substitution. The 
discriminative  training  can  improve  classifica-
tion performance by iteratively re-weighting the 
positive  and  negative  features  for  each  class. 
This helps improve the separation of the correct 
class against its competing ones, making classi-
fiers more effective on ambiguous cases close to 
the  decision  boundary.  Experimental  results 
show that the supervised discriminative training 
technique achieves higher accuracy than the two 
unsupervised  methods,  the  PMI-based  and  n-
gram-based methods. The availability of a large 
labeled  training  set  also  encourages  the  use  of 
the proposed supervised method.  

Future  work  will  investigate  on  the  use  of 
multiple features for discriminating among near-
synonyms.  For  instance,  the  predicate-argument 
structure,  which  can  capture  long-distance  in-
formation, will be combined with currently used 
local contextual features to boost the classifica-
tion performance. More experiments will also be 
conducted to evaluate classifiers’ ability to rank 
multiple answers. 

References  
J. Bhogal, A. Macfarlane, and P. Smith. 2007. A Re-
view  of  Ontology  based  Query  Expansion.  Infor-
mation Processing & Management, 43(4):866-886. 
K.  Church  and  P.  Hanks.  1991.  Word  Association 
Norms,  Mutual  Information  and  Lexicography. 
Computational Linguistics, 16(1):22-29. 

I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, 
and  C.  Strapparava.  2006.  Direct  Word  Sense 
Matching  for  Lexical  Substitution.  In  Proc.  of 
COLING/ACL-06, pages 449-456. 

P. Edmonds. 1997. Choosing the Word Most Typical 
in  Context  Using  a  Lexical  Co-occurrence  Net-
work. In Proc. of ACL-97, pages 507-509. 

C. Fellbaum. 1998. WordNet: An Electronic Lexical 

Database. MIT Press, Cambridge, MA. 

M.  Gardiner  and  M.  Dras.  2007.  Exploring  Ap-
proaches to Discriminating among Near-Synonyms, 
In Proc. of the Australasian Technology Workshop, 
pages 31-39. 

E. H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and 
R.  Weischedel.  2006.  OntoNotes:  The  90%  Solu-
tion. In Proc. of HLT/NAACL-06, pages 57–60. 

D. Inkpen. 2007. Near-Synonym Choice in an Intelli-
gent Thesaurus. In Proc. of NAACL/HLT-07, pages 
356-363. 

D. Inkpen and G. Hirst. 2006. Building and Using a 
Lexical Knowledge-base of Near-Synonym Differ-
ences. Computational Linguistics, 32(2):1-39. 

S. Katagiri, B. H. Juang, and C. H. Lee. 1998. Pattern 
Recognition Using a Family of Design Algorithms 
based  upon  the  Generalized  Probabilistic  Descent 
Method, Proc. of the IEEE, 86(11):2345-2373. 

H.  K.  J.  Kuo  and  C.  H.  Lee.  2003.  Discriminative 
Training of Natural Language Call Routers, IEEE 
Trans. Speech and Audio Processing, 11(1):24-35. 
D. Lin. 1998. Automatic Retrieval and Clustering of 
Similar  Words.  In  Proc.  of  ACL/COLING-98, 
pages 768-774. 

D.  McCarthy.  2002.  Lexical  Substitution  as  a  Task 
the 
for  WSD  Evaluation. 
SIGLEX/SENSEVAL  Workshop  on  Word  Sense 
Disambiguation at ACL-02, pages 109-115. 

In  Proc. 

of 

D. Moldovan and R. Mihalcea. 2000. Using Wordnet 
Internet 

and  Lexical  Operators 
Searches. IEEE Internet Computing, 4(1):34-43. 

Improve 

to 

U.  Ohler,  S.  Harbeck,  and  H.  Niemann.  1999.  Dis-
criminative Training of Language Model Classifi-
ers, In Proc. of Eurospeech-99, pages 1607-1610. 

D. Pearce. 2001. Synonymy in Collocation Extraction. 
In  Proc. of  the  Workshop on  WordNet  and Other 
Lexical Resources at NAACL-01. 

S.  Pradhan,  E.  H.  Hovy,  M.  Marcus,  M.  Palmer,  L. 
Ramshaw, and R. Weischedel. 2007. OntoNotes: A 
Unified  Relational  Semantic  Representation.  In 
Proc.  of  the  First  IEEE  International  Conference 
on Semantic Computing (ICSC-07), pages 517-524. 
H. Rodríguez, S. Climent, P. Vossen, L. Bloksma, W. 
Peters, A. Alonge, F. Bertagna, and A. Roventint. 
1998.  The  Top-Down  Strategy  for  Building  Eeu-
roWordNet: Vocabulary Coverage, Base Concepts 
and Top Ontology, Computers and the Humanities, 
32:117-159. 

L. C. Yu, C. H. Wu, A. Philpot, and E. H. Hovy. 2007. 
OntoNotes: Sense Pool Verification Using Google 
N-gram  and  Statistical  Tests,  In  Proc.  of  the  On-
toLex Workshop at the 6th International Semantic 
Web Conference (ISWC-07). 

D. Zhou and Y. He. 2009. Discriminative Training of 
the Hidden Vector State Model for Semantic Pars-
ing,  IEEE  Trans.  Knowledge  and Data  Engineer-
ing, 21(1):66-77. 

