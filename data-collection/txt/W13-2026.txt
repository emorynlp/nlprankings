










































Extracting Gene Regulation Networks Using Linear-Chain Conditional Random Fields and Rules


Proceedings of the BioNLP Shared Task 2013 Workshop, pages 178–187,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

Extracting Gene Regulation Networks Using
Linear-Chain Conditional Random Fields and Rules

Slavko Žitnik†‡ Marinka Žitnik† Blaž Zupan† Marko Bajec†

†Faculty of Computer and Information Science
University of Ljubljana

Tržaška cesta 25
SI-1000 Ljubljana

{name.surname}@fri.uni-lj.si

‡Optilab d.o.o.
Dunajska cesta 152
SI-1000 Ljubljana

Abstract

Published literature in molecular genetics
may collectively provide much informa-
tion on gene regulation networks. Ded-
icated computational approaches are re-
quired to sip through large volumes of text
and infer gene interactions. We propose a
novel sieve-based relation extraction sys-
tem that uses linear-chain conditional ran-
dom fields and rules. Also, we intro-
duce a new skip-mention data represen-
tation to enable distant relation extraction
using first-order models. To account for a
variety of relation types, multiple models
are inferred. The system was applied to the
BioNLP 2013 Gene Regulation Network
Shared Task. Our approach was ranked
first of five, with a slot error rate of 0.73.

1 Introduction

In recent years we have witnessed an increas-
ing number of studies that use comprehensive
PubMed literature as an additional source of in-
formation. Millions of biomedical abstracts and
thousands of phenotype and gene descriptions re-
side in online article databases. These represent
an enormous amount of knowledge that can be
mined with dedicated natural language process-
ing techniques. However, extensive biological
insight is often required to develop text mining
techniques that can be readily used by biomedi-
cal experts. Profiling biomedical research litera-
ture was among the first approaches in disease-
gene prediction and is now becoming invaluable
to researchers (Piro and Di Cunto, 2012; Moreau
and Tranchevent, 2012). Information from pub-
lication repositories was often merged with other
databases. Successful examples of such integra-
tion include an OMIM database on human genes
and genetic phenotypes (Amberger et al., 2011),

GeneRIF function annotation database (Osborne
et al., 2006), Gene Ontology (Ashburner et al.,
2000) and clinical information about drugs in the
DailyMed database (Polen et al., 2008). Biomed-
ical literature mining is a powerful way to iden-
tify promising candidate genes for which abundant
knowledge might already be available.

Relation extraction (Sarawagi, 2008) can iden-
tify semantic relationships between entities from
text and is one of the key information extrac-
tion tasks. Because of the abundance of publica-
tions in molecular biology computational methods
are required to convert text into structured data.
Early relation extraction systems typically used
hand-crafted rules to extract a small set of rela-
tion types (Brin, 1999). Later, machine learning
methods were adapted to support the task and were
trained over a set of predefined relation types. In
cases where no tagged data is available, some un-
supervised techniques offer the extraction of rela-
tion descriptors based on syntactic text properties
(Bach and Badaskar, 2007). Current state-of-the-
art systems achieve best results by combining both
machine learning and rule-based approaches (Xu
et al., 2012).

Information on gene interactions are scattered
in data resources such as PubMed. The reconstruc-
tion of gene regulatory networks is a longstanding
but fundamental challenge that can improve our
understanding of cellular processes and molecular
interactions (Sauka-Spengler and Bronner-Fraser,
2008). In this study we aimed at extracting a gene
regulatory network of the popular model organism
the Bacillus subtilis. Specifically, we focused on
the sporulation function, a type of cellular differ-
entiation and a well-studied cellular function in B.
subtilis.

We describe the method that we used for our
participation in the BioNLP 2013 Gene Regula-
tion Network (GRN) Shared Task (Bossy et al.,
2013). The goal of the task was to retrieve the

178



genic interactions. The participants were provided
with manually annotated sentences from research
literature that contain entities, events and genic
interactions. Entities are sequences of text that
identify objects, such as genes, proteins and reg-
ulons. Events and relations are described by type,
two associated entities and direction between the
two entities. The participants were asked to pre-
dict relations of interaction type in the test data
set. The submitted network of interactions was
compared to the reference network and evaluated
with Slot Error Rate (SER) (Makhoul et al., 1999)
SER = (S + I + D)/N that measures the frac-
tion of incorrect predictions as the sum of relation
substitutions (S), insertions (I) and deletions (D)
relative to the number of reference relations (N).

We begin with a description of related work and
the background of relation extraction. We then
present our extension of linear-chain conditional
random fields (CRF) with skip-mentions (Sec. 3).
Then we explain our sieve-based system archi-
tecture (Sec. 4), which is the complete pipeline
of data processing that includes data preparation,
linear-chain CRF and rule based relation detection
and data cleaning. Finally, we describe the results
at BioNLP 2013 GRN Shared Task (Sec. 6).

2 Related Work

The majority of work on relation extraction fo-
cuses on binary relations between two entities.
Most often, the proposed systems are evaluated
against social relations in ACE benchmark data
sets (Bunescu and Mooney, 2005; Wang et al.,
2006). There the task is to identify pairs of enti-
ties and assign them a relation type. A number of
machine learning techniques have been used for
relation extraction, such as sequence classifiers,
including HMM (Freitag and McCallum, 2000),
CRF (Lafferty et al., 2001) and MEMM (Kamb-
hatla, 2004), and binary classifiers. The latter most
oftem employ SVM (Van Landeghem et al., 2012).

The ACE 2004 data set (Mitchell et al., 2005)
contains two-tier hierarchical relation types. Thus,
a relation can have another relation as an attribute
and second level relation must have only atomic
attributes. Therefore, two-tier relation hierarchies
have the maximum height of two. Wang et al.
(2006) employed a one-against-one SVM classi-
fier to predict relations in ACE 2004 data set using
semantic features from WordNet (Miller, 1995).
The BioNLP 2013 GRN Shared Task aims to de-

tect three-tier hierarchical relations. These rela-
tions describe interactions that can have events or
other interactions as attributes. In contrast to pair-
wise approach of Wang et al. (2006), we extract
relations with sequence classifiers and rules.

The same relation in text can be expressed
in many forms. Machine-learning approaches
can resolve this heterogeneity by training mod-
els on large data sets using a large number of
feature functions. Text-based features can be
constructed through application of feature func-
tions. An approach to overcome low cover-
age of different relation forms was proposed by
Garcia and Gamallo (2011). They introduced
a lexico-syntactic pattern-based feature functions
that identify dependency heads and extracts rela-
tions. Their approach was evaluated over two re-
lation types in two languages and achieved good
results. In our study we use rules to account for
the heterogeneity of relation representation.

Generally, when trying to solve a rela-
tion extraction task, data sets are tagged us-
ing the IOB (inside-outside-beginning) nota-
tion (Ramshaw and Marcus, 1995), such that the
first word of the relation is tagged as B-REL, other
consecutive words within it as I-REL and all others
as O. The segment of text that best describes a pre-
defined relation between two entities is called a re-
lation descriptor. Li et al. (2011) trained a linear-
chain CRF to uncover these descriptors. They also
transformed subject and object mentions of the re-
lations into dedicated values that enabled them to
correctly predict relation direction. Additionally,
they represented the whole relation descriptor as
a single word to use long-range features with a
first-order model. We use a similar model but pro-
pose a new way of token sequence transformation
which discovers the exact relation and not only the
descriptor. Banko and Etzioni (2008) used linear
models for the extraction of open relations (i.e.
extraction of general relation descriptors without
any knowledge about specific target relation type).
They first characterized the type of relation ap-
pearance in the text according to lexical and syn-
tactic patterns and then trained a CRF using these
data along with synonym detection (Yates and Et-
zioni, 2007). Their method is useful when a few
relations in a massive corpus are unknown. How-
ever, if higher levels of recall are desired, tradi-
tional relation extraction is a better fit. In this
study we therefore propose a completely super-

179



vised relation extraction method.
Methods for biomedical relation extraction have

been tested within several large evaluation initia-
tives. The Learning language in logic (LLL) chal-
lenge on genic interaction extraction (Nédellec,
2005) is similar to the BioNLP 2013 GRN Shared
Task, which contains a subset of the LLL data
set enriched with additional annotations. Giu-
liano et al. (2006) solved the task using an SVM
classifier with a specialized local and global con-
text kernel. The local kernel uses only mention-
related features such as word, lemma and part-of-
speech tag, while the global context kernel com-
pares words that appear on the left, between and
on the right of two candidate mentions. To de-
tect relations, they select only documents contain-
ing at least two mentions and generate

(n
k

)
train-

ing examples, where n is the number of all men-
tions in a document and k is number of mentions
that form a relation (i.e. two). They then predict
three class values according to direction (subject-
object, object-subject, no relation). Our approach
also uses context features and syntactic features
of neighbouring tokens. The direction of relations
predicted in our model is arbitrary and it is further
determined using rules.

The BioNLP 2011 REL Supporting Shared Task
addressed the extraction of entity relations. The
winning TESS system (Van Landeghem et al.,
2012) used SVMs in a pipeline to detect entity
nodes, predict relations and perform some post-
processing steps. They predict relations among ev-
ery two mention pairs in a sentence. Their study
concluded that the term detection module has a
strong impact on the relation extraction module.
In our case, protein and entity mentions (i.e. men-
tions representing genes) had already been identi-
fied, and we therefore focused mainly on extrac-
tion of events, relations and event modification
mentions.

3 Conditional Random Fields with
Skip-Mentions

Conditional random fields (CRF) (Lafferty et al.,
2001) is a discriminative model that estimates
joint distribution p(y|x) over the target sequence
y, conditioned on the observed sequence x. The
following example shows an observed sequence x
where mentions are printed in bold:

“Transcription of cheV initiates from a
sigma D-dependent promoter element

both in vivo and in vitro, and expression
of a cheV-lacZ fusion is completely de-
pendent on sigD.” 1

Corresponding sequences xPOS , xPARSE ,
xLEMMA contain part-of-speech tags, parse tree
tokens and lemmas for each word, respectively.
Different feature functions fj (Fig. 2), employed
by CRF, use these sequences in order to model
the target sequence y, which also corresponds
to tokens in x. Feature function modelling is an
essential part when training CRF. Selection of
feature functions contributes the most to an in-
crease of precision and recall when training CRF
classifiers. Usually these are given as templates
and the final features are generated by scanning
the entire training data set. The feature functions
used in our model are described in Sec. 3.1.

CRF training finds a weight vector w that pre-
dicts the best possible (i.e. the most probable) se-
quence ŷ given x. Hence,

ŷ = arg max
y

p(y|x, w), (1)

where the conditional distribution equals

p(y|x, w) =
exp(

∑m
j=1 wj

∑n
i=1 fj(y, x, i))

C(x, w)
.

(2)
Here, n is the length of the observed sequence x,
m is the number of feature functions and C(x, w)
is a normalization constant computed over all pos-
sible y. We do not consider the normalization con-
stant because we are not interested in exact target
sequence probabilities. We select only the target
sequence that is ranked first.

y1

x1

yn

xn

y2

x2

y3

x3

Figure 1: The structure of a linear-chain CRF
model. It shows an observable sequence x and tar-
get sequence y containing n tokens.

The structure of a linear-chain CRF (LCRF)
model or any other more general graphical model
is defined by references to the target sequence la-
bels within the feature functions. Fig. 1 shows the

1The sentence is taken from BioNLP 2013 GRN training
data set, article PMID-8169223-S5.

180



function f(y, x, i):
if (yi−1 == O and

yi == GENE and
xi−1 == transcribes) then

return 1
else

return 0

Figure 2: An example of a feature function. It
checks if the previous label was Other, the current
is Gene and the previous word was “transcribes”,
returns 1, otherwise 0.

structure of the LCRF. Note that the i-th factor
can depend only on the current and the previous
sequence labels yi and yi−1. LCRF can be effi-
ciently trained, whereas exact inference of weights
in CRF with arbitrary structure is intractable due
to an exponential number of partial sequences.
Thus, approximate approaches must be adopted.

3.1 Data Representation
The goal of our task is to identify relations be-
tween two selected mentions. If we process the
input sequences as is, we cannot model the de-
pendencies between two consecutive mentions be-
cause there can be many other tokens in between.
From an excerpt of the example in the previous
section, “cheV initiates from a sigmaD”, we can
observe the limitation of modelling just two con-
secutive tokens. With this type of labelling it is
hard to extract the relationships using a first-order
model. Also, we are not interested in identify-
ing relation descriptors (i.e. segments of text that
best describe a pre-defined relation); therefore, we
generate new sequences containing only mentions.
Mentions are also the only tokens that can be an
attribute of a relation. In Fig. 3 we show the trans-
formation of our example into a mention sequence.
The observable sequence x contains sorted en-
tity mentions that are annotated. These annota-
tions were part of the training corpus. The target
sequence y is tagged with the none symbol (i.e.
O) or the name of the relationship (e.g. Interac-
tion.Requirement). Each relationship target token
represents a relationship between the current and
the previous observable mention.

The mention sequence as demonstrated in Fig. 3
does not model the relationships that exist be-
tween distant mentions. For example, the men-
tions cheV and promoter are related by a Promoter

O

cheV

Interaction.
Transcription

sigma D

Master of
 promoter

promoter

O

cheV

Interaction.
Requirement

sigD

Promoter of 

Figure 3: A mention sequence with zero skip-
mentions. This continues our example from
Sec. 3.

of relation, which cannot be identified using only
LCRF. Linear model can only detect dependen-
cies between two consecutive mentions. To model
such relationships on different distances we gen-
erate appropriate skip-mention sequences. The
notion of skip-mention stands for the number of
other mentions between two consecutive mentions
which are not included in a specific skip-mention
sequence. Thus, to model relationships between
every second mention, we generate two one skip-
mention sequences for each sentence. A one skip-
mention sequence identifies the Promoter of rela-
tion, shown in Fig. 4.

O

cheV

Promoter of

promoter

O

sigD

Figure 4: A mention sequence with one skip-
mention. This is one out of two generated men-
tion sequences with one skip-mention. The other
consists of tokens sigmaD and cheV.

For every s skip-mention number, we gen-
erate s + 1 mention sequences of length dns e.
After these sequences are generated, we train
one LCRF model per each skip-mention number.
Model training and inference of predictions can
be done in parallel due to the sequence indepen-
dence. Analogously, we generate model-specific
skip-mention sequences for inference and get tar-
get labellings as a result. We extract the identified
relations between the two mentions and represent
them as an undirected graph.

Fig. 5 shows the distribution of distances be-

181



0 1 2 3 4 5 6 7 8 9 11 13 15 17 19

BioNLP 2013 GRN dataset 
 relation Mention distance distribution

Mention distance between relation arguments

N
um

be
r 

of
 r

el
at

io
ns

hi
ps

0
20

40
60

80

46

28

69
73

45

31

14 14

7
4 3

6
2 2 1 2 1 2 1 1

Figure 5: Distribution of distances between two
mentions connected with a relation.

tween the relation mention attributes (i.e. agents
and targets) in the BioNLP 2013 GRN training and
development data set. The attribute mention data
consists of all entity mentions and events. We ob-
serve that most of relations connect attributes on
distances of two and three mentions.

To get our final predictions we train CRF mod-
els on zero to ten skip-mention sequences. We use
the same unigram and bigram feature function set
for all models. These include the following:

• target label distribution,

• mention type (e.g. Gene, Protein) and ob-
servable values (e.g., sigma D) of mention
distance 4 around current mention,

• context features using bag-of-words match-
ing on the left, between and on the right side
of mentions,

• hearst concurrence features (Bansal and
Klein, 2012),

• token distance between mentions,

• parse tree depth and path between mentions,

• previous and next lemmas and part-of-speech
tags.

4 Data Analysis Pipeline

We propose a pipeline system combining multi-
ple processing sieves. Each sieve is an indepen-
dent data processing component. The system con-
sists of eight sieves, where the first two sieves

prepare data for relation extraction, main sieves
consist of linear-chain CRF and rule-based rela-
tion detection, and the last sieve cleans the out-
put data. Full implementation is publicly available
(https://bitbucket.org/szitnik/iobie). We use CRF-
Suite (http://www.chokkan.org/software/crfsuite)
for faster CRF training and inference.

First, we transform the input data into a format
appropriate for our processing and enrich the data
with lemmas, parse trees and part-of-speech tags.
We then identify additional action mentions which
act as event attributes (see Sec. 4.3). Next, we em-
ploy the CRF models to detect events. We treat
events as a relation type. The main relation pro-
cessing sieves detect relations. We designed sev-
eral processing sieves, which support different re-
lation attribute types and hierarchies. We also em-
ploy rules at each step to properly set the agent
and target attributes. In the last relation processing
sieve, we perform rule-based relation extraction to
detect high precision relations and boost the recall.
In the last step we clean the extracted results and
export the data.

The proposed system sieves are executed in the
following order:

i Preprocessing Sieve

ii Mention Processing Sieve

iii Event Processing Sieve

iv Mention Relations Processing Sieve

v Event Relations Processing Sieve

vi Gene Relations Processing Sieve

vii Rule-Based Relations Processing Sieve

viii Data Cleaning Sieve

In the description of the sieves in the follow-
ing sections, we use general relation terms, nam-
ing the relation attributes as subject and object, as
shown in Fig. 6.

subject object

relation

Figure 6: General relation representation.

182



4.1 Preprocessing Sieve
The preprocessing sieve includes data import, sen-
tence detection and text tokenization. Addition-
ally, we enrich the data using part-of-speech tags,
parse trees (http://opennlp.apache.org) and lem-
mas (Juršic et al., 2010).

4.2 Mention Processing Sieve
The entity mentions consist of Protein, Gene-
Family, ProteinFamily, ProteinComplex, Poly-
meraseComplex, Gene, Operon, mRNA, Site, Reg-
ulon and Promoter types. Action mentions (e.g.
inhibits, co-transcribes) are automatically de-
tected as they are needed as event attributes for
the event extraction. We therefore select all lem-
mas of the action mentions from the training data
and detect new mentions from the test data set by
comparing lemma values.

4.3 Event Processing Sieve
The general definition of an event is described as
a change on the state of a bio-molecule or bio-
molecules (e.g. “expression of a cheV-lacZ fusion
is completely dependent on sigD”). We represent
events as a special case of relationship and name
them “EVENT”. In the training data, the event sub-
ject types are Protein, GeneFamily, PolymeraseC-
omplex, Gene, Operon, mRNA, Site, Regulon and
Promoter types, while the objects are always of
the action type (e.g. “expression”), which we dis-
cover in the previous sieve. After identifying event
relations using the linear-chain CRF approach, we
apply a rule that sets the action mention as an ob-
ject and the gene as a subject attribute for every
extracted event.

4.4 Relations Processing Sieves
According to the task relation properties (i.e. dif-
ferent subject and object types), we extract rela-
tions in three phases (iv, v, vi). This enables us to
extract hierarchical relations (i.e. relation contains
another relation as subject or object) and achieve
higher precision. All sieves use the proposed
linear-chain CRF-based extraction. The process-
ing sieves use specific relation properties and are
executed as follows:

(iv) First, we extract relations that contain only
entity mentions as attributes (e.g. “Transcrip-
tion of cheV initiates from a sigmaD” re-
solves into the relation sigmaD → Interac-
tion.Transcription→ cheV).

(v) In the second stage, we extract relations that
contain at least one event as their attribute.
Prior to execution we transform events into
their mention form. Mentions generated from
events consist of two tokens. They are taken
from the event attributes and the new event
mention is included into the list of existing
mentions. Its order within the list is deter-
mined by the index of the lowest mention to-
ken. Next, relations are identified following
the same principle as in the first step.

(vi) According to an evaluation peculiarity of the
challenge, the goal is to extract possible inter-
actions between genes. Thus, when a relation
between a gene G1 and an event E should
be extracted, the GRN network is the same
as if the method identifies a relation between
a gene G1 and gene G2, if G2 is the object
of event E. We exploit this notion by gen-
erating training data to learn relation extrac-
tion only between B. subtilis genes. During
this step we use an external resource of all
known genes of the bacteria retrieved from
the NCBI2.

The training and development data sets include
seven relation instances that have a relation as an
attribute. We omitted this type of hierarchy extrac-
tion due to the small number of data instances and
execution of relation extraction between genes.

There are also four negative relation instances.
The BioNLP task focuses on positive relations, so
there would be no increase in performance if neg-
ative relations were extracted. Therefore, we ex-
tract only positive relations. According to the data
set, we could simply add a separate sieve which
would extract negations by using manually defined
rules. Words that explicitly define these negations
are not, whereas, neither and nor.

4.5 Rule-Based Relations Processing Sieve
The last step of relation processing uses rules that
extract relations with high precision. General rules
consist of the following four methods:

• The method that checks all consequent men-
tion triplets that contain exactly one action
mention. As input we set the index of the ac-
tion mention within the triplet, its matching
regular expression and target relation.

2http://www.ncbi.nlm.nih.gov/nuccore/
AL009126

183



• The method that processes every two con-
sequent B. subtilis entity mentions. It takes
a regular expression, which must match the
text between the mentions, and a target rela-
tion.

• The third method is a modification of the pre-
vious method that supports having a list of
entity mentions on the left or the right side.
For example, this method extracts two rela-
tions in the following example: “rsfA is under
the control of both sigma(F) and sigma(G)”.

• The last method is a variation of the sec-
ond method, which removes subsentences
between the two mentions prior to relation
extraction. For example, the method is able
to extract distant relation from the following
example: “sigma(F) factor turns on about 48
genes, including the gene for RsfA, and the
gene for sigma(G)”. This is sigma(F) → In-
teraction.Activation→ sigma(G).

We extract the Interaction relations using regu-
lar expression and specific keywords for the tran-
scription types (e.g. keywords transcrib, directs
transcription, under control of), inhibition (key-
words repress, inactivate, inhibits, negatively reg-
ulated by), activation (e.g. keywords governed
by, activated by, essential to activation, turns on),
requirement (e.g. keyword require) and binding
(e.g. keywords binds to, -binding). Notice that in
biomedical literature, a multitude of expressions
are often used to describe the same type of genetic
interaction. For instance, researchers might prefer
using the expression to repress over to inactivate
or to inhibit. Thus, we exploit these synsets to im-
prove the predictive accuracy of the model.

4.6 Data Cleaning Sieve

The last sieve involves data cleaning. This consists
of removing relation loops and eliminating redun-
dancy.

A relation is considered a loop if its attribute
mentions represent the same entity (i.e. men-
tions corefer). For instance, sentence “... sigma
D element, while cheV-lacZ depends on sigD ...”
contains mentions sigma D and sigD, which can-
not form a relationship because they represent the
same gene. By removing loops we reduce the
number of insertions. Removal of redundant re-
lations does not affect the final score.

5 Data in BioNLP 2013 GRN Challenge

Table 1 shows statistics of data sets used in our
study. For the test data set we do not have tagged
data and therefore cannot show the detailed eval-
uation analysis for each sieve. Each data set
consists of sentences extracted from PubMed ab-
stracts on the topic of the gene regulation network
of the sporulation of B. subtilis. The sentences in
both the training and the development data sets are
manually annotated with entity mentions, events
and relations. Real mentions in Table 1 are the
mentions that refer to genes or other structures,
while action mentions refer to event attributes (e.g.
transcription). Our task is to extract Interaction
relations of the types regulation, inhibition, acti-
vation, requirement, binding and transcription for
which the extraction algorithm is also evaluated.

The extraction task in GRN Challenge is two-
fold: given annotated mentions, a participant
needs to identify a relation and then determine the
role of relation attributes (i.e. subject or object)
within the previously identified relation. Only pre-
dictions that match the reference relations by both
relation type and its attributes are considered as a
match.

6 Results and Discussion

We tested our system on the data from BioNLP
2013 GRN Shared Task using the leave one out
cross validation on the training data and achieved
a SER of 0.756, with 4 substitutions, 81 dele-
tions, 14 insertions and 46 matches, given 131 ref-
erence relations. The relatively high number of
deletions in these results might be due to ambigu-
ities in the data. We identified the following num-
ber of extracted relations in the relation extraction
sieves (Sec. 4): (iii) 91 events, (iv) 130 relations
between mentions only, (v) 27 relations between
an event and a mention, (vi) 39 relations between
entity mentions, and (vii) 44 relations using only
rules. Our approach consists of multiple submod-
ules, each designed for a specific relation attribute
type (e.g. either both attributes are mentions, or an
event and a mention, or both are genes). Also, the
total sum of extracted relations exceeds the num-
ber of final predicted relations, which is a conse-
quence of their extraction in multiple sieves. Du-
plicates and loops were removed in the data clean-
ing sieve.

The challenge test data set contains 290 men-
tions across 67 sentences. To detect relations

184



Data set Documents Tokens Real
mentions

Action
mentions

Events Relations Interaction
relations

dev 48 1321 205 55 72 105 71
train 86 2380 422 102 157 254 159
test 67 1874 290 86 / / /

Table 1: BioNLP 2013 GRN Shared Task development (dev), training (train) and test data set properties.

in the test data, we trained our models on the
joint development and training data. At the time
of submission we did not use the gene relations
processing sieve (see Sec. 4) because it had not
yet been implemented. The results of the par-
ticipants in the challenge are shown in Table 2.
According to the official SER measure, our sys-
tem (U. of Ljubljana) was ranked first. The
other four competing systems were K. U. Leuven
(Provoost and Moens, 2013), TEES-2.1 (Björne
and Salakoski, 2013), IRISA-TexMex (Claveau,
2013) and EVEX (Hakala et al., 2013). Partici-

Participant S D I M SER
U. of Ljubljana 8 50 6 30 0.73
K. U. Leuven 15 53 5 20 0.83
TEES-2.1 9 59 8 20 0.86
IRISA-TexMex 27 25 28 36 0.91
EVEX 10 67 4 11 0.92

Table 2: BioNLP 2013 GRN Shared Task results.
The table shows the number of substitutions (S),
deletions (D), insertions (I), matches (M) and slot
error rate (SER) metric.

pants aimed at a low number of substitutions, dele-
tions and insertions, while increasing the number
of matches. We got the least number of substi-
tutions and fairly good results in the other three
indicators, which gave the best final score. Fig. 7
shows the predicted gene regulation network with
the relations that our system extracted from test
data. This network does not exactly match our
submission due to minor algorithm modifications
after the submission deadline.

7 Conclusion

We have proposed a sieve-based system for re-
lation extraction from text. The system is based
on linear-chain conditional random fields (LCRF)
and domain-specific rules. In order to support the
extraction of relations between distant mentions,
we propose an approach called skip-mention lin-
ear chain CRF, which extends LCRF by varying

Interaction.Activation

Interaction.Binding

Interaction.Inhibition

Interaction.Regulation

Interaction.Requirement

Interaction.Transcription

Figure 7: The predicted gene regulation network
by our system at the BioNLP 2013 GRN Shared
Task.

185



the number of skipped mentions to form mention
sequences. In contrast to common relation extrac-
tion approaches, we inferred a separate model for
each relation type.

We applied the proposed system to the BioNLP
2013 Gene Regulation Network Shared Task. The
task was to reconstruct the gene regulation net-
work of sporulation in the model organism B. sub-
tilis. Our approach scored best among this year’s
submissions.

Acknowledgments

The work has been supported by the Slovene Re-
search Agency ARRS within the research program
P2-0359 and in part financed by the European
Union, European Social Fund.

References
Joanna Amberger, Carol Bocchini, and Ada Hamosh.

2011. A new face and new challenges for online
Mendelian inheritance in man (OMIM). Human
Mutation, 32(5):564–567.

Michael Ashburner, Catherine A. Ball, Judith A. Blake,
David Botstein, Heather Butler, Michael J. Cherry,
Allan P. Davis, Kara Dolinski, Selina S. Dwight,
Janan T. Eppig, Midori A. Harris, David P. Hill, Lau-
rie Issel-Tarver, Andrew Kasarskis, Suzanna Lewis,
John C. Matese, Joel E. Richardson, Martin Ring-
wald, Gerald M. Rubin, and Gavin Sherlock. 2000.
Gene Ontology: Tool for the unification of biology.
Nature Genetics, 25(1):25–29.

Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature Review for Lan-
guage and Statistics II, pages 1–15.

Michele Banko and Oren Etzioni. 2008. The trade-
offs between open and traditional relation extraction.
Proceedings of ACL-08: HLT, page 28–36.

Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, page
389–398.

Jari Björne and Tapio Salakoski. 2013. TEES 2.1: Au-
tomated annotation scheme learning in the bioNLP
2013 shared task. In Proceedings of BioNLP Shared
Task 2013 Workshop, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.

Robert Bossy, Philippe Bessir̀es, and Claire Nédellec.
2013. BioNLP shared task 2013 - an overview of
the genic regulation network task. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases, page 172–183. Springer.

Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, page 724–731.

Vincent Claveau. 2013. IRISA participation to
bioNLP-ST13: lazy-learning and information re-
trieval for information extraction tasks. In Pro-
ceedings of BioNLP Shared Task 2013 Workshop,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Dayne Freitag and Andrew McCallum. 2000. In-
formation extraction with HMM structures learned
by stochastic optimization. In Proceedings of the
National Conference on Artificial Intelligence, page
584–589.

Marcos Garcia and Pablo Gamallo. 2011.
Dependency-based text compression for semantic
relation extraction. Information Extraction and
Knowledge Acquisition, page 21.

Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical liter-
ature. In Proceedings of the Eleventh Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL-2006), page 401–408.

Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,
Yves Van de Peer, and Filip Ginter. 2013. EVEX
in ST’13: Application of a large-scale text mining
resource to event extraction and network construc-
tion. In Proceedings of BioNLP Shared Task 2013
Workshop, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Matjaž Juršic, Igor Mozetič, Tomaž Erjavec, and Nada
Lavrač. 2010. LemmaGen: multilingual lemmati-
sation with induced ripple-down rules. Journal of
Universal Computer Science, 16(9):1190–1214.

Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.

Yaliang Li, Jing Jiang, Hai L. Chieu, and Kian M.A.
Chai. 2011. Extracting relation descriptors with
conditional random fields. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing, pages 392–400, Thailand. Asian
Federation of Natural Language Processing.

186



John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proceedings of DARPA
Broadcast News Workshop, page 249–252.

George A. Miller. 1995. WordNet: a lexical database
for English. Commun. ACM, 38(11):39–41.

Alexis Mitchell, Stephanie Strassel, Shudong Huang,
and Ramez Zakhary. 2005. ACE 2004 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia.

Yves Moreau and Léon-Charles Tranchevent. 2012.
Computational tools for prioritizing candidate
genes: boosting disease gene discovery. Nature Re-
views Genetics, 13(8):523–536.

Claire Nédellec. 2005. Learning language in logic-
genic interaction extraction challenge. In Proceed-
ings of the 4th Learning Language in Logic Work-
shop (LLL05), volume 7, pages 1–7.

John D. Osborne, Simon Lin, Warren A. Kibbe, Li-
hua J. Zhu, Maria I. Danila, and Rex L. Chisholm.
2006. GeneRIF is a more comprehensive, cur-
rent and computationally tractable source of gene-
disease relationships than OMIM. Technical report,
Northwestern University.

Rosario M Piro and Ferdinando Di Cunto. 2012.
Computational approaches to disease-gene predic-
tion: rationale, classification and successes. The
FEBS Journal, 279(5):678–96.

Hyla Polen, Antonia Zapantis, Kevin Clauson, Jennifer
Jebrock, and Mark Paris. 2008. Ability of online
drug databases to assist in clinical decision-making
with infectious disease therapies. BMC Infectious
Diseases, 8(1):153.

Thomas Provoost and Marie-Francine Moens. 2013.
Detecting relations in the gene regulation network.
In Proceedings of BioNLP Shared Task 2013 Work-
shop, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.

Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the Third ACL Workshop on Very
Large Corpora, page 82–94.

Sunita Sarawagi. 2008. Information extraction. Foun-
dations and Trends in Databases, 1(3):261–377.

Tatjana Sauka-Spengler and Marianne Bronner-Fraser.
2008. A gene regulatory network orchestrates neu-
ral crest formation. Nature reviews Molecular cell
biology, 9(7):557–568.

Sofie Van Landeghem, Jari Björne, Thomas Abeel,
Bernard De Baets, Tapio Salakoski, and Yves Van de
Peer. 2012. Semantically linking molecular enti-
ties in literature through entity relationships. BMC
Bioinformatics, 13(Suppl 11):S6.

Ting Wang, Yaoyong Li, Kalina Bontcheva, Hamish
Cunningham, and Ji Wang. 2006. Automatic ex-
traction of hierarchical relations from text. The
Semantic Web: Research and Applications, page
215–229.

Yan Xu, Kai Hong, Junichi Tsujii, I Eric, and Chao
Chang. 2012. Feature engineering combined with
machine learning and rule-based methods for struc-
tured information extraction from narrative clinical
discharge summaries. Journal of the American Med-
ical Informatics Association, 19(5):824–832.

Alexander Yates and Oren Etzioni. 2007. Unsuper-
vised resolution of objects and relations on the web.
In Proceedings of NAACL HLT, page 121–130.

187


