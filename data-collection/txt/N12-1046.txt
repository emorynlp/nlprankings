










































Encouraging Consistent Translation Choices


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417–426,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Encouraging Consistent Translation Choices

Ferhan Ture,1 Douglas W. Oard,2,4 Philip Resnik3,4
1Department of Computer Science

2College of Information Studies
3Department of Linguistics

4Institute for Advanced Computer Studies
University of Maryland, College Park, MD 20740 USA
fture@cs.umd.edu, oard@umd.edu, resnik@umd.edu

Abstract
It has long been observed that monolingual text
exhibits a tendency toward “one sense per dis-
course,” and it has been argued that a related
“one translation per discourse” constraint is op-
erative in bilingual contexts as well. In this pa-
per, we introduce a novel method using forced
decoding to confirm the validity of this con-
straint, and we demonstrate that it can be ex-
ploited in order to improve machine translation
quality. Three ways of incorporating such a
preference into a hierarchical phrase-based MT
model are proposed, and the approach where all
three are combined yields the greatest improve-
ments  for  both  Arabic-English  and  Chinese-
English translation experiments.

1 Introduction
In statistical Machine Translation (MT), the state-of-
the-art approach is to translate phrases in the context
of a sentence and to re-order those phrases appro-
priately. Intuitively, it seems as if it should also be
possible to draw on information outside of a single
sentence to further improve translation quality. In
this paper, we challenge the conventional approach
of translating each sentence independently, and ar-
gue that it can indeed also be beneficial to consider
document-scale context when translating text. Mo-
tivated by the success of a “one sense per discourse”
heuristic in Word Sense Disambiguation (WSD), we
explore the potential  benefit of leveraging a “one
translation per discourse” heuristic in MT.

The paper is organized as follows. We begin with
related work in Section 2. Next, we provide new

confirmation that the hypothesized one-translation-
per-discourse  condition  does  indeed  often  hold,
based  on  a  novel  analysis  using  forced  decoding
(Section 3). We incorporate this idea into a hierarchi-
cal MT framework by adding three new document-
scale features to the translation model (Section 4).
We then present  experimental  results  demonstrat-
ing  solid  improvements  in  translation  quality  ob-
tained by leveraging these features, both for Arabic-
English (Ar-En) and Chinese-English (Zh-En) trans-
lation (Section 5). Conclusions and future work are
presented in Section 6.

2 Related work
Exploiting  discourse-level  context  has  to  date
received  only  limited  attention  in  MT re-
search (e.g., (Giménez  and  Màrquez, 2007; Liu
et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et
al., 2011)). Exploratory analysis of reference trans-
lations by Carpuat  (2009)  motivates  a  hypothesis
that MT systems might benefit from the “one sense
per discourse” heuristic, first introduced by Gale et
al. (1992), which has proven to be effective in the
context of WSD (Yarowsky, 1995). Carpuat’s ap-
proach was to do post-processing on the translation
output to impose a “one translation per discourse”
constraint where the system would otherwise have
made a different choice. A manual evaluation on
a sample of sentences suggested promise from the
technique, which  Carpuat  suggested  in  favor  of
exploring more integrated approaches.

Xiao et al. (2011) took this one step further and
implement an approach where they identified am-
biguous translations within each document, and at-

417



tempt to fix them by replacing each ambiguity with
the most frequent translation choice. Based on their
error analysis, the authors indicate two shortcomings
when trying to find the correct translation of a given
phrase. First, frequency may not provide sufficient
information to distinguish between translation can-
didates, which is why we take rareness into account
when scoring translation candidates. Another prob-
lem is, like any other heuristic, that there may be
cases where the heuristic fails and there are multi-
ple senses per discourse. Guaranteeing consistency
hurts performance in such situations, which is why
we implement the heuristic as a model feature, and
let the model score decide for each case.

We are aware of a few other analyses that have
shown promising results based on a similar motiva-
tion. For instance, Wasser and Dorr (2008)’s ap-
proach biases the MT system based on term statistics
from relevant documents in comparable corpora. Ma
et al. (2011) show that a translation memory can be
used to find similar source sentences, and consecu-
tively adapt translation choices towards consistency.
Domain adaptation for MT has has also been shown
to be useful in some cases (Bertoldi and Federico,
2009; Hildebrand et al., 2005; Sanchis-Trilles and
Casacuberta, 2010; Tiedemann, 2010; Zhao et al.,
2004), so to the extent we consider documents to be
micro-domains we might expect similar approaches
to be useful at document scale. Indeed, hints that
such ideas may work have been available for some
time. For example, there is clear evidence that the
behavior of human translators can provide evidence
that is often useful for automating WSD (Diab and
Resnik, 2002; Ng et al., 2003). When coupled with
the one-sense-per-discourse heuristic, this suggests
that the reverse may also be true.

3 Exploratory analysis
It is well known that writing styles vary by genre,
and in particular that the amount of vocabulary vari-
ation within a document depends to some extent on
the genre (e.g., higher in poetry than in engineering
writing). The degree to which authors tend to make
consistent word choices in any particular genre is,
therefore, an empirical question. In order to gain in-
sight into the extent to which human translators make
consistent vocabulary choices in the types of materi-

als that we wish to translate (in this work, news sto-
ries), we first explore the degree of support for our
one-translation-per-discourse hypothesis in the ref-
erence translations of a standard MT test collection.

We used the Ar-En MT08 data set, which con-
tains 74 newswire documents with a total  of 813
sentences, each of which has four reference trans-
lations. Throughout this paper we consistently use
the  document  (i.e., one  news  story)  as  a  conve-
nient discourse unit, although of course finer-scale or
broader-scale discourse units might also be explored
in future work. Moreover, throughout this paper we
use the hierarchical phrase-based translation system
(Hiero), which is based on a synchronous context-
free grammar (SCFG) model (Chiang, 2005). In a
SCFG, the rule [X] ||| α ||| β indicates that con-
text free expansion X → α in the source language
can occur synchronously with X → β in the target
language. In this case, we call α the left hand side
(LHS) of the rule, and β the right hand side (RHS)
of the rule.

To determine the extent and nature of translation
consistency choices made by human translators, we
randomly selected one of the four sets of reference
translations (first set, with id 0) and we used forced
decoding to find all possible sequences of rules that
could transform the source sentence into the target
sentence. In forced decoding, given a pair of source
and target sentences, and a grammar consisting of
learned translation rules with associated probabili-
ties, the decoder searches all possible derivations for
the one sequence of rules that is most likely (under
the learned translation model) to synchronously pro-
duce the source sentence on the LHS and the target
sentence on the RHS. For instance, consider the fol-
lowing Arabic sentence as input:

رابط بين الاعتداءات الثلاثة .

and its uncased reference translation:

there is a link between the three attacks .

The following four rules, which are part of the SCFG
learned from the the same translation pairs, allows
the decoder to find a sequence of derivations that
“translates” the source-side Arabic sentence into the

418



X16 

X7  بني X12 
X3 

 الثالثة 
 رابط 

. االعتداءات 

R1 

R2 
R3 

R4 

X16 

X7 between X12 

X3 

 the 
 there 

 attacks .  three 
 is  a  link 

R1 

R2 R3 

R4 

Figure 1: Illustration of forced decoding.

target-side reference translation.1

R1. [X12] ||| رابط ||| there is a link
R2. [X16] ||| [2] بين [1] ||| [X12, 1] between [X7, 2]
R3. [X7] ||| [1] الاعتداءات . ||| [X3, 1] attacks .
R4. [X3] ||| الثلاثة ||| the three
Figure 1 illustrates how the decoder uses these

rules  to  produce the source and target  sides  syn-
chronously.

As we repeated this  procedure  for  all  sentence
pairs, we kept track of all rules that were actually
used by the decoder to generate a reference English
translation from the corresponding Arabic sentences.

Our next step was to identify cases in which the
SCFG could reasonably have produced a substan-
tially  different  translation. Whenever  an  Arabic
phrase f occurs multiple times in a document, and f
appears on the LHS of two or more different gram-
mar rules in the SCFG, we count this as a single
“case”.2 These cases correspond to unique (source
phrase f , document d) pairs in which a translation
process using that SCFG could have chosen to pro-
duce two or more different translations of f in d.
Since the multiple appearances of f are distributed
among sentences of d, each counted case may cor-
respond to a number of sentences ranging from 1 to
the number of sentences in that document.

Table 1 shows a small sample of the cases (i.e.,
(source phrase f , document d) pairs) identified as a
result of forced decoding. There were 321 such cases
in our dataset and there were 672 sentences in which
at least one case occurred. This is not an uncommon
phenomenon; these 672 sentences comprise 83% of

1Since our goal was an exploratory analysis, the MT08 test
set was combined with the training set in order to ensure reach-
ability of the reference translations using the learned grammar.
Proper train/dev/test splits were, of course, used for the evalua-
tion results reported in Section 5.

2We define a phrase as any text that constitutes the entire
LHS of a grammar rule.

the test set. However, many of these cases repre-
sent either unlikely choices or inconsequential dif-
ferences, so some post-processing is called for.

Since  grammar  rules  are  typically  more  fine-
grained than is necessary for our purposes (e.g., to
capture various punctuation and determiner differ-
ences that do not affect the “sense” of the transla-
tion), we applied a few simple heuristics to edit the
source and target  sides and group all  such minor
variations into a single “mega-rule” (e.g., “how”∼“,
how”, “third”∼“a third”, “want”∼“we want”). For
this, we removed nonterminal symbols and punc-
tuation, and  considered  two target  phrases e and
e′ to  be different only  if edit distance(e, e′) >
max(length(e), length(e′))/2, where the edit dis-
tance is based on character removal and insertion.
For instance, the third example in Table 1 would
have been considered to be translated consistently
as a result of this heuristic, as opposed to the first
example. We also eliminated cases in which no rea-
sonable alternatives were available in the translation
grammar (i.e., cases where the second most probable
rule with the same LHS was assigned a probability
below 0.1 in the grammar). Cases 4 and 5 would
have been removed by this heuristic.

After this filtering and aggregation we were left
with 176 (f , d) pairs in which the translation model
could reasonably have selected between rules that
would have produced substantially different English
translations of f in d (such as cases 1–3 and 6–9).
It was these 176 cases, affecting a total of 512 sen-
tences (63% of test set) for which we then examined
what forced decoding could tell us about translation
consistency.

So now that we know what the human who pro-
duced the reference translations actually did (accord-
ing to forced decoding), and in which cases they
might reasonably have chosen to do something sub-
stantially different (according to the SCFG), we can
ask in which cases the human (effectively) made a
consistent choice of translation rules when encoun-
tering the same Arabic phrase in the same document.
In 128 of the 176 cases, that is what they did (i.e.,
when the same phrase occurred multiple times in a
single document and more than one translation was
reasonably possible, forced decoding indicated that
the human translator translated that phrase in essen-
tially the same way). These cases affected the trans-

419



Case Translation countsSource phrase Doc #
مقتل 566 that killed = 1

killing of = 1
الرهائن 782 hostages = 2
الرهائن 138 hostage = 1

hostages = 2
كوريا 466 korea = 2
كوريا 763 korea = 2
من 30 from = 2
من 7 of = 1

from = 1
من الراهن 717 of the current = 2

التي 30 the = 1
which =1

Table 1: A sample of cases (i.e., (source phrase f , docu-
ment d) pairs) identified as a result of forced decoding.

lation of 455 sentences (56% of the test set), suggest-
ing that if we can replicate this human behavior in a
system, it might affect a nontrivial number of trans-
lation choices.

These statistics also suggest, however, that there
may be some risk incurred in such a process, since
in 48 of the 176 cases, the human translator opted
for a substantially different translation. When we
closely examined these 48 instances, we found that
19 (40%) involved changing a content-bearing word
(sometimes to a word with similar meaning). The re-
maining 29 (60%) involved function words or simi-
lar constructions. See Figures 2 and 3 for examples.
1a. [X] ||| سمحت ||| had allowed
1b. [X] ||| سمحت ||| has permitted
2a. [X] ||| [X,1] تدرس ||| examining [X,1]
2b. [X] ||| [X,1] تدرس ||| is considering [X,1]
3a. [X] ||| [X,1] الجوار ||| neighbors
3b. [X] ||| [X,1] الجوار ||| neighboring countries
Figure 2: Examples of differences in lexical choice for
content-bearing words within the same document.

We can make several observations based on this
analysis. First, there does indeed seem to be ev-
idence to support the one-translation-per-discourse
heuristic, and to suggest that respecting that heuris-

4a. [X] ||| في ||| on
4b. [X] ||| في ||| in
4c. [X] ||| في ||| ’s
5a. [X] ||| قد ||| had
5b. [X] ||| قد ||| was
Figure 3: Examples of differences in lexical choice for
other types of lexical units within the same document.

tic could improve translation outcomes for a substan-
tial number of sentences. Second, even when a ref-
erence translation contains different translations of
the same phrase, this may sometimes be the result of
stylistic choices rather than an intent by the transla-
tor to affect the expressed meaning. If a system were
try to “fix” such cases by enforcing consistent trans-
lation, the resulting translation might be somewhat
more stilted, but perhaps not less accurate or less in-
telligible. Finally, sentence structure conventions or
other language-specific phenomena may sometimes
require the same phrase to be translated differently,
so some way of encouraging consistency while still
allowing the model to consider other contextual fac-
tors might be better than always imposing a hard con-
sistency constraint.

4 Approach
To incorporate document-level features into an MT
system  that  would  otherwise  operate  with  only
sentence-level  evidence, we  added  three  super-
sentential “consistency features” to the translation
model. The decoder computes scores for these fea-
tures in two passes over each document; in each pass,
each sentence in the document is decoded. In the
first pass, the decoder keeps track of the number of
occurrences of some aspects of each grammar rule
and stores that information. The consistency fea-
tures are disabled during this pass, and do not affect
decoder scoring. In the second pass, each grammar
rule is assigned as many as three consistency feature
scores, each of which is based on some frozen counts
from the first pass. These features are designed to
introduce a bias towards translation consistency, but
to leave the final decision to the decoder, which of
course also has access to other  features from the
translation and language model. At this point we are
more interested in effectiveness than efficiency, so

420



we simply note that this approach doubles the run-
ning time of the decoder and that future work on a
more elegant implementation might be productive.

We explore three ways to compute features in this
section. The essential idea behind all of them is to
define some feature function that increases monoton-
ically with an increase in some count that we believe
to be informative, and in which the rate of increase is
damped more strongly as that count increases. Sev-
eral feature functions could satisfy those broad re-
quirements; in this section, we describe three vari-
ants, C1, C2 and C3, and discuss the potential bene-
fits and drawbacks of each.
C1: Counting rules In this variant, we count in-
stances of the same entire grammar rule, where a rule
r contains both the source phrase f and the target
phrase e. During the first pass, whenever a grammar
rule is chosen by the decoder for the one-best output,
the count for that rule is incremented. Given a gram-
mar rule r and the number of times r was counted in
the first pass (given by N{r}), the consistency fea-
ture score is computed as follows:

C1(r) =
2.2N{r}

1.2 + N{r}
(1)

Equation 1 is the term frequency component of the
well known Okapi BM25 term weighting function,
when parameters are set to the conventional values
k = 1.2, b = 0. This is an increasing and con-
cave function in which the count has a diminishing
marginal effect on the feature score. It has proven
to be useful in information retrieval applications, in
which the goal is to model “aboutness” based on term
counts (Robertson et al., 1994). Because our goal is
to demonstrate the potential of consistency features,
it seemed reasonable to work with some simple func-
tion that has a shape like the one we desired. We
leave exploration of optimal damping functions for
future work.

A drawback of this C1 approach is that as we saw
in Section 3, grammar rules in phrase-based MT sys-
tems tend to be somewhat more fine-grained than
seems optimal for constructing a consistency fea-
ture. For instance, consider the following rules that
all translate the same Arabic term:

R1. [X] ||| [X,1] اجهزة ||| [X,1] the bodies
R2. [X] ||| [X,1] اجهزة ||| [X,1] the organs

R3. [X] ||| [X,1] اجهزة ||| [X,1] organs
R4. [X] ||| اجهزة [X,1] ||| the organs of [X,1]
R5. [X] ||| اجهزة [X,1] ||| [X,1] bodies

Based on these grammar rules, we as human read-
ers infer that this Arabic phrase can be translated in
two different ways: as organs or as bodies. An opti-
mal application of the one-translation-per-discourse
heuristic would thus group the rules based on the
presence of one of those words. However, in the C1
variant, each of these rules would be counted sepa-
rately because of differences that in some cases do
not directly affect the choice of content words. For
instance, on the source side, the Arabic token ap-
pears to the right of the nonterminal symbol in R1,
R2 and R3, while it is to the left of the nonterminal in
R4 and R5. On the target side, differences are due to
both nonterminal symbol position and the existence
of determiners. Motivated by many examples like
this, we came up with an alternative way of count-
ing rules.
C2: Counting target tokens To partially address
this sparseness issue, variant C2 focuses only on the
target side. We extract all target tokens whenever a
grammar rule is used by the decoder in a one-best
derivation and increment a counter for each. Since
we are mainly interested in content words (e.g. bod-
ies, organs), we use simple pattern matching to dis-
card nonterminal symbols and punctuation, and we
ignore terms that appear in more than 50% of all doc-
uments (a convenient way of discarding common to-
kens such as the, or, and). This approach separates
the rules in the example above into two groups: rules
with bodies on the target side and rules with organs
on the target side. Upon completion of the first pass,
the consistency feature score for rule r is then de-
termined by first computing a score for each unique
target-side token w using:

bm25(w) =
2.2N{w}

1.2 + N{w}
log D + 1

DF (w) + 0.5
(2)

where in this case N{w}maps tokens to their respec-
tive counts in the document, D is the total number
of documents in the collection, and DF (document
frequency) is the number of documents in which the
token occurs. This is a fuller version of the BM25
function in which (in the information retrieval ap-
plication) both high term frequencies and rare terms

421



are rewarded. We then set the feature score for each
rule r to the maximum score of any of its target-side
terminal tokens:

C2(r) = max
e∈RHS(r)

bm25(e) (3)
Our motivation for choosing the maximum is that
when there is more than one content word that sur-
vives the pruning of common terms, we want the
score to be influenced most strongly by the most im-
portant of those terms. Since BM25 term weights
can be thought of as a measure of term importance,
taking the maximum is a simple expedient.

Although counting only target-side tokens yields
coarser granularity than counting rules, ignoring the
source side of the rule risks combining target side
statistics from translations of unrelated source lan-
guage terms. Consider the following grammar rule:
R6. [X] ||| <s> [X,1] اجهزة ||| <s> [X,1] life support
Since the counter for life and support will both be
incremented whenever rule R6 fires in the one-best
decoding during the first pass, problems could arise
if a rule with a different LHS that also contains sup-
port on the RHS were to fire in the same document,
for example:

R7. [X] ||| الارها ||| support
If we don’t take the source side into account, both oc-
currences of support will be grouped together when
counting and R7 will receive extra score from the
consistency feature whenever R6 is used by the de-
coder. Of course, this problem will only arise when
the LHS of R6 and R7 are present in the same doc-
ument, and how often that happens (and thus how
large the risk from this factor is) is an empirical ques-
tion. We therefore developed a third alternative as a
middle ground between the fine-grained C1 and the
coarse-grained C2.
C3: Counting  token  translation  pairs In  this
variant, we count each terminal (source token, tar-
get token) pair that survives pruning. Specifically,
if grammar rule [X]|||f1f2...fm|||e1e2...en fires, we
increment the count of every pair ⟨fi, ej⟩, where fi
is aligned to ej . After the first pass, we compute the
feature value of each observed pair, based on this
count and the DF of the target-side of the pair. We
chose to use only the target token in the DF com-
putation (i.e., aggregating over all source tokens) to

reduce sparsity effects. Similar to C2, the feature of
a rule r is defined by the maximum of scores of all
pairs extracted from r.

C3(r) = max
f∈LHS(r)
e∈RHS(r)
⟨f,e⟩ aligned

bm25(⟨f, e⟩) (4)

Since each variant has its benefits and drawbacks, we
can include all three in the system and let the tuning
process decide on how each should be weighted.

5 Evaluation and Discussion
We have evaluated the one-translation-per-discourse
feature using the cdec MT system (Dyer et al., 2010).
We started by building a baseline system using stan-
dard features in cdec: lexical and phrase transla-
tion probabilities in both directions, word and arity
penalty features, and a 5-gram language model. We
then added each of the three consistency feature vari-
ants, along with all two-way and the one three-way
combinations of them, thus yielding a total of eight
systems for comparison, including the baseline.

For training the Ar-En system, we used the dataset
from the DARPA GALE evaluation (Olive et  al.,
2011), which consists of NIST and LDC releases.
The corpus was filtered to remove sentence pairs
with  anomalous  length  ratios  and  subsampled  to
yield a training set containing 3.4 million parallel
sentence pairs. The Arabic text was preprocessed to
produce two different segmentations (simple punctu-
ation tokenization with orthographic normalization,
and LDC’s ATBv3 representation (Maamouri et al.,
2008)), represented together using cdec’s lattice in-
put format (Dyer et al., 2008).

The Zh-En system was trained on parallel train-
ing text consisting of the non-UN portions and non-
HK Hansards portions of the NIST training corpora.
Chinese was automatically segmented by the Stan-
ford segmenter (Tseng et al., 2005), and traditional
characters were simplified. After subsampling and
filtering, we obtain a training corpus of 1.6 million
parallel sentences.

Both  training  sets  were  word-aligned  with
GIZA++ (Och and Ney, 2003), using 5 Model  1
and  5  HMM iterations. A SCFG was  then  ex-
tracted from these alignments using a suffix array
extractor (Chiang, 2007). Evaluation was done with
multi-reference BLEU (Papineni et al., 2002) on test

422



sets with four references for each language pair, and
MIRA was used for tuning (Crammer et al., 2006).
In our experiments, we run the first decoding phase
using feature weights that are guessed heuristically
based on weights from previously tuned systems.
All feature weights, including the discourse feature,
were then tuned together, based on the output  of
the  second  decoding  phase. For  Ar-En  parame-
ter  tuning, we  used  the  MT06 newswire  dataset,
which contains 104 documents and a total of 1,797
sentences. For testing, we used the MT08 dataset
described  above  (74  documents, 813  sentences).
For Zh-En experiments, the MT02 newswire dataset
(100 documents, 878 sentences) was used for tuning,
and evaluation was done on the MT06 test set (79
documents, 1,664 sentences). For  both language
pairs, DF values were computed from the tuning
set for both tuning and evaluation experiments.

When we used NIST’s official metric (BLEU-4)
to compare our results to the official NIST evalu-
ation (NIST, 2006; NIST, 2008), our baseline sys-
tem achieved 54.70 for  Ar-En and 31.69 for  Zh-
En. Based on reported NIST results, our baseline
would have ranked 4th in the Zh-En MT06 evalua-
tion, and would have outperformed all Ar-En MT08
systems. We used a slightly different IBM-BLEU
metric for the rest of our evaluation. In this case,
the baseline system achieved 53.07 BLEU points
for  Ar-En  and  30.43  points  for  Zh-En. Among
more recent papers, the best reported results were
56.87  for  Ar-En  MT08 (Zhao  et  al., 2011a)  and
35.87 for Zh-En MT06 (Zhao et al., 2011b), although
many papers report BLEU scores below 53 points
for Arabic (Carpuat et al., 2011) and 32 points for
Chinese (Monz, 2011). The systems that outper-
formed our baseline applied novel techniques, and
used larger language models, as well as many non-
standard features. We argue that these novelties are
complementary to our approach, and therefore do not
damage the credibility of our baseline.

Among the single-feature runs, C3 had the best
performance  in  Ar-En  experiments, with  53.84
BLEU points, whereas C2 yielded the best results
for Zh-En with a BLEU score of 30.96. In any case,
all three variants outperformed the baseline (see Ta-
ble 2). When multiple features were combined, we
generally observed an increase in BLEU, suggesting
that our features have usefully different error char-

Method BLEU
Ar-En Zh-En

Baseline 53.07 30.43
C1 53.82 30.59
C2 53.70 30.96
C3 53.84 30.54
C12 53.82 30.79
C13 53.82 30.76
C23 53.88 30.63
C123 53.98 31.42

Table 2: Evaluation results: BLEU scores with four ref-
erences for Ar-En and Zh-En experiments.

Method # documents
Ar-En Zh-En

Docs 74 79
C1 37 30
C2 37 35
C3 42 36

C123 43 41

Table 3: Doc-level analysis: Number of documents where
each variant outperforms baseline.

acteristics. The combination of all three variants,
C123, yielded the best results, nearly 1.0 BLEU point
higher  than  the  baseline  for  both  language  pairs.
Evaluation results are summarized in Table 2.

Given our focus on documents, it  is  natural  to
ask  what  fraction  of  the  documents  were  helped
or  harmed  by  consistency  features. Document-
level  BLEU scores  for  Arabic-to-English  transla-
tions show that C3 outperformed the baseline on a
larger number of documents than any other single
feature (42/74=57%), compared with 37/74 (50%)
for both C1 and C2. C123 did better by this measure
as well, with BLEU increasing for 43 of the docu-
ments. There were no documents where the BLEU
score  was  exactly  the  same, therefore  the  BLEU
score declined for the remaining documents. As Ta-
ble 3 indicates, document-level BLEU for the Zh-En
experiments shows similar results.

We can also look at our results in a more fine-
grained way, focusing on differences in how each
system translated the same source-language phrase.
For  this  analysis, we  defined  English  phrases e
and e′ to  be different if edit distance(e, e′) >

423



Method Ar-En Zh-En
Cases Test set Cases Test set

C1 77 24% 401 48%
C2 127 35% 686 60%
C3 101 33% 491 53%

Any 197 68% 968 94%
C123 141 41% 651 59%

Table 4: Effect of applying variants of the consistency
feature (Any=C1 or C2 or C3).

max(length(e), length(e′))/2. By  this  way  of
counting, there are 197 unique (Arabic phrase, docu-
ment) pairs for which at least one single-feature sys-
tem produced translations differently from the base-
line system. Together, these cases affect 553 sen-
tences (68%) in 67 of the 74 documents, with as
many as 12 differences observed in a single doc-
ument. The  number  of  such  differences  is  even
higher for Chinese-to-English translation, probably
due to lower confidence from the translation model
and longer documents. Table 4 shows the number of
changes by each system, and the percentage of the
test set affected by these changes.

In order to gain greater insight into the effect of
the consistency features, we randomly sampled 60
of the 197 cases and analyzed the influence of the
change to the document BLEU score. In 25 of the
sampled cases, at least one of the three systems made
a change that improved the BLEU score, whereas the
score was adversely affected for at least one system
in 13 cases. BLEU remained unchanged in the re-
maining 22 cases, mostly due to the use of multi-
ple reference translations. When we analyze the ef-
fect of each system separately, we see that C2 was
the most aggressive, making 25 changes that influ-
enced BLEU (16 positive, 9 negative). C1 was the
most conservative, with only 13 such changes (8 pos-
itive, 5 negative). Consistent with the overall BLEU
scores, C3 evidenced the best ratio between benefit
and harm, making 20 changes that affected the score
(16 positive, 4 negative).

Looking at specific cases can yield some insight
into how the consistency features achieve improve-
ments. For example, results improved when trans-
lating the phrase ,تنظيمية (Eng. organizational,
regulatory), which appears in the context of organi-

zational groups that support terrorist ideology. The
baseline system translated this as organizational in
one case, and regulatory in another. Variants C1
and C2 changed this behavior, so that the translation
was organizational in both cases. One of the refer-
ence translations used organizational in one case and
dropped the phrase in the other, and the other three
translators  provided  consistent  translations  (using
organized and organizational). As a result, applying
the one-translation-per-discourse heuristic improved
the multi-reference BLEU score.

On the other hand, here is one of the cases where
our  feature  hurt  performance. The  phrase 边防
部队 (Eng. border/frontier troops/guards) appears
in two sentences of a Chinese news story about vio-
lence along the India - Nepal border. All reference
translations consistently used the word border in the
translation, as it is a better choice in this context.
The baseline system translated the phrase as fron-
tier guards and border troops in the two sentences.
All system variants replaced border with frontier to
maintain consistency, and therefore produced worse
translations, causing a decrease in BLEU score.

Examples can, however, also point up limitations
in our ability to measure improvements. In one of
the test documents, the Arabic phrase التسلل الي
(Eng. sneak, infiltrate, enter without approval) ap-
pears in the context of Turkey trying to enter the Eu-
ropean Union. This was translated by the baseline
system as sneak into in one occurrence and infiltrate
into in another. C1 didn’t change the output, but
C2 and C3 translated the phrase as infiltrate into in
both cases. Although all of the four reference trans-
lators were consistent within their choices, each of
them chose different translations, namely worm its
way, enter, sneak and sneak into. This resulted in
a decrease in BLEU score for the two systems that
chose infiltrate into. This case illustrates a limita-
tion to fine-grained use of BLEU alone as a basis
for analysis, since we might argue that infiltrate into
is no less appropriate than sneak into in this con-
text. In other words, some of the reductions we see
in BLEU may not be actual errors but rather sim-
ply changes that take us outside of the coverage of
the test set. We did not find any cases in our sample
in which improvements in BLEU seemed to reward
changes that adversely affected meaning. From this,

424



we conclude that BLEU is a somewhat conservative
measure when used in this way, and that the actual
overall improvement in translation quality over our
baseline may be somewhat more than our roughly
1.0 measured BLEU improvement would suggest.

6 Conclusions and Future Work
In this paper, we started with a new way of look-
ing at, and largely supporting, the “one translation
per discourse” hypothesis using forced decoding of
human reference translations. We then leveraged
insights  from that  analysis  to  design  the  transla-
tion model consistency features, obtaining solid im-
provements for both Ar-En and Zh-En translation.
In future work, we plan to explore additional vari-
ants. For example, we can further address sparsity by
incorporating monolingual paraphrase detection on
the source side, the target side or both. We can and
should explore other monotonically increasing con-
cave feature functions in addition to the Okapi BM25
function that we have found to be useful in this work,
we should explore alternatives to our use of the max-
imum function in C2 and C3, and we should con-
sider optimizing to measures other than BLEU (e.g.,
METEOR) that extend the range of rewarded lexical
choices by leveraging monolingual paraphrase evi-
dence.

In designing our features we were guided by our
intuition about which kinds of consistency should be
rewarded. Data can be superior to intuition, how-
ever, and our forced decoding technique might also
be helpful in generating new insights that could help
to guide the design of even more useful features. For
example, our forced decoding clearly points to cases
in which translators have chosen different structural
variants when translating the same phrase, and closer
examination of these cases might help us to automat-
ically detect which kinds of structural variation can
most profitably be moderated using a consistency
feature. We should also note that we have only done
forced decoding to date in one language pair (Ar-
En), and there might be more to be learned about
language-specific issues from doing the same anal-
ysis for additional language pairs.

Finally, the time seems propitious to reconsider
our choice of document-scale as our discourse con-
text. Documents have much to recommend them, but

much of the content that we might wish to translate
(conversational speech, text chat, email threads, . . . )
doesn’t present the kinds of obvious and unambigu-
ous document boundaries that we find in MT test
collections that are built from news stories. More-
over, some documents (e.g., textbooks) may be too
diverse for an entire document to be the right scale
for consistency. We might also be able to produc-
tively group similar documents into clusters in which
the vocabulary choices are (or should be) mutually
reinforcing.

We therefore  end where  we began, with  many
questions to be answered. Now, however, we have
somewhat different questions – not whether to en-
courage consistency at a super-sentential scale, but
rather when and how best to do that.

Acknowledgements
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA.

References
Nicola  Bertoldi  and  Marcello  Federico. 2009. Do-

main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation (StatMT
’09), pages 182–189.

Ralf D. Brown. 2008. Exploiting document-level context
for data-driven machine translation. In Proceedings of
the the Eighth Conference of the Association for Ma-
chine Translation in the Americas (AMTA ’08).

Marine Carpuat, Yuval Marton, and Nizar Habash. 2011.
Improved Arabic-to-English statistical machine trans-
lation  by  reordering  post-verbal  subjects  for  word
alignment. Machine Translation, pages 1–16.

Marine Carpuat. 2009. One translation per discourse. In
Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ’09,
pages 19–27.

David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL ’05.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201–228.

425



Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL ’02.

Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing Word Lattice Translation. In Pro-
ceedings of ACL-HLT’08, pages 1012–1020, June.

Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-
vitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACLDe-
mos ’10, pages 7–12.

William A.  Gale, Kenneth W.  Church, and  David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, HLT ’91, pages 233–237.

Jesús Giménez and Llúıs Màrquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings  of  StatMT ’07, pages
159–166.

AS Hildebrand, M Eck, S Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of The European Association for Machine
Translation (EAMT ’05).

Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2010.
Improving statistical machine translation with mono-
lingual collocation. In ACL ’10.

Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrimina-
tive learning: a translation memory-inspired approach.
In Proceedings of ACL-HLT’11, pages 1239–1248.

Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhancing the Arabic Treebank: A Collaborative Ef-
fort toward New Annotation Guidelines. In LREC ’08.

Christof Monz. 2011. Statistical Machine Translation
with Local Language Models. In EMNLP ’11.

Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In ACL ’03.

NIST. 2006. http://www.itl.nist.gov/iad/mig/tests/mt/2006/.
NIST. 2008. http://www.itl.nist.gov/iad/mig/tests/mt/2008/.
Franz Och and Hermann Ney. 2003. A systematic com-

parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.

Joseph Olive, Caitlin  Christianson, and John McCary.
2011. Handbook  of  Natural  Language  Processing
and Machine Translation: DARPA Global Autonomous
Language  Exploitation. Springer  Publishing  Com-
pany, Inc., 1st edition.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ’02.

Stephen E. Robertson, Steve Walker, Susan Jones, Miche-
line  Hancock-Beaulieu, and  Mike  Gatford. 1994.
Okapi at TREC-3. In TREC.

Germán  Sanchis-Trilles  and  Francisco  Casacuberta.
2010. Bayesian  adaptation  for  statistical  machine
translation. In Proceedings of the workshop on Struc-
tural and Syntactic Pattern Recognition (SSPR ’10),
pages 620–629.

Jörg Tiedemann. 2010. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the workshop on
Domain Adaptation for Natural Language Processing
(DANLP ’10), pages 8–15.

Huihsin Tseng, Pi-Chuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional  random  field  word  segmenter. In Fourth
SIGHAN Workshop on Chinese Language Processing.

Michael M.  Wasser  and  Bonnie  Dorr. 2008. Ma-
chine  translation  with  cross-lingual  information  re-
trieval based document relevance scores. Unpublished.

Tong Xiao, Jingbo Zhu, Shujie  Yao, and Hao Zhang.
2011. Document-level consistency verification in ma-
chine translation. In Machine Translation Summit XIII
(MTS’11).

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In ACL ’95.

Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Lan-
guage model adaptation for statistical machine transla-
tion with structured query models. In COLING ’04.

Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011a. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL-HLT ’11, pages 846–855.

Yinggong Zhao, Yangsheng Ji, Ning Xi, Shujian Huang,
and Jiajun Chen. 2011b. Language model weight
adaptation based on cross-entropy for statistical ma-
chine translation. In Pacific Asia Conference on Lan-
guage, Information and Computation (PACLIC ’11).

426


