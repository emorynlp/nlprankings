










































Event Schema Induction with a Probabilistic Entity-Driven Model


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Event Schema Induction with a Probabilistic Entity-Driven Model

Nathanael Chambers
United States Naval Academy

Annapolis, MD 21402
nchamber@usna.edu

Abstract

Event schema induction is the task of learning
high-level representations of complex events
(e.g., a bombing) and their entity roles (e.g.,
perpetrator and victim) from unlabeled text.
Event schemas have important connections to
early NLP research on frames and scripts,
as well as modern applications like template
extraction. Recent research suggests event
schemas can be learned from raw text. In-
spired by a pipelined learner based on named
entity coreference, this paper presents the first
generative model for schema induction that in-
tegrates coreference chains into learning. Our
generative model is conceptually simpler than
the pipelined approach and requires far less
training data. It also provides an interesting
contrast with a recent HMM-based model. We
evaluate on a common dataset for template
schema extraction. Our generative model
matches the pipeline’s performance, and out-
performs the HMM by 7 F1 points (20%).

1 Introduction

Early research in language understanding focused
on high-level semantic representations to drive their
models. Many proposals, such as frames and scripts,
used rich event schemas to model the situations de-
scribed in text. While the field has since focused on
more shallow approaches, recent work on schema
induction shows that event schemas might be learn-
able from raw text. This paper continues the trend,
addressing the question, can event schemas be in-
duced from raw text without prior knowledge? We
present a new generative model for event schemas,

and it produces state-of-the-art induction results, in-
cluding a 7 F1 point gain over a different generative
proposal developed in parallel with this work.

Event schemas are unique from most work in in-
formation extraction (IE). Current relation discovery
(Banko et al., 2007a; Carlson et al., 2010b) focuses
on atomic facts and relations. Event schemas build
relations into coherent event structures, often called
templates in IE. For instance, an election template
jointly connects that obama won a presidential elec-
tion with romney was the defeated, the election oc-
curred in 2012, and the popular vote was 50-48. The
entities in these relations fill specific semantic roles,
as in this template schema:

Template Schema for Elections
(events: nominate, vote, elect, win, declare, concede)

Date: Timestamp
Winner: Person
Loser: Person
Position: Occupation
Vote: Number

Traditionally, template extractors assume fore-
knowledge of the event schemas. They know a Win-
ner exists, and research focuses on supervised learn-
ing to extract winners from text. This paper focuses
on the other side of the supervision spectrum. The
learner receives no human input, and it first induces
a schema before extracting instances of it.

Our proposed model contributes to a growing
line of research in schema induction. The majority
of previous work relies on ad-hoc clustering algo-
rithms (Filatova et al., 2006; Sekine, 2006; Cham-
bers and Jurafsky, 2011). Chambers and Jurafsky
is a pipelined approach, learning events first, and
later learning syntactic patterns as fillers. It requires

1797



several ad-hoc metrics and parameters, and it lacks
the benefits of a formal model. However, central to
their algorithm is the use of coreferring entity men-
tions to knit events and entities together into an event
schema. We adapt this entity-driven approach to a
single model that requires fewer parameters and far
less training data. Further, experiments show state-
of-the-art performance.

Other research conducted at the time of this pa-
per also proposes a generative model for schema in-
duction (Cheung et al., 2013). Theirs is not entity-
based, but instead uses a sequence model (HMM-
based) of verb clauses. These two papers thus pro-
vide a unique opportunity to compare two very dif-
ferent views of document structure. One is entity-
driven, modeling an entity’s role by its coreference
chain. The other is clause-driven, classifying indi-
vidual clauses based on text sequence. Each model
makes unique assumptions, providing an interest-
ing contrast. Our entity model outperforms by 7 F1
points on a common extraction task.

The rest of the paper describes in detail our
main contributions: (1) the first entity-based gen-
erative model for schema induction, (2) a direct
pipeline/formal model comparison, (3) results im-
proving state-of-the-art performance by 20%, and
(4) schema induction from the smallest amount of
training data to date.

2 Previous Work

Unsupervised learning for information extraction
usually learns binary relations and atomic facts.
Models can learn relations like Person is married to
Person without labeled data (Banko et al., 2007b), or
rely on seed examples for ontology induction (dog is
a mammal) and attribute extraction (dogs have tails)
(Carlson et al., 2010b; Carlson et al., 2010a; Huang
and Riloff, 2010; Durme and Pasca, 2008). These do
not typically capture the deeper connections mod-
eled by event schemas.

Algorithms that do focus on event schema extrac-
tion typically require both the schemas and labeled
corpora, such as rule-based approaches (Chinchor
et al., 1993; Rau et al., 1992) and modern super-
vised classifiers (Freitag, 1998; Chieu et al., 2003;
Bunescu and Mooney, 2004; Patwardhan and Riloff,
2009; Huang and Riloff, 2011). Classifiers rely on

the labeled examples’ surrounding context for fea-
tures (Maslennikov and Chua, 2007). Weakly su-
pervised learning removes some of the need for la-
beled data, but most still require the event schemas.
One common approach is to begin with unlabeled,
but clustered event-specific documents, and extract
common word patterns as extractors (Riloff and
Schmelzenbach, 1998; Sudo et al., 2003; Riloff et
al., 2005; Filatova et al., 2006; Patwardhan and
Riloff, 2007; Chen et al., 2011). Bootstrapping with
seed examples of known slot fillers has been shown
to be effective (Yangarber et al., 2000; Surdeanu et
al., 2006).

Shinyama and Sekine (2006) presented unre-
stricted relation discovery to discover relations in
unlabeled documents. Their algorithm used redun-
dant documents (e.g., all describe Hurricane Ivan)
to observe repeated proper nouns. The approach re-
quires many documents about the exact same event
instance, and relations are binary (not schemas) over
repeated named entities. Our model instead learns
schemas from documents with mixed topics that
don’t describe the same event, so repeated proper
nouns are less helpful.

Chen et al. (2011) perform relation extraction
with no supervision on earthquake and finance do-
mains. Theirs is a generative model that represents
relations as predicate/argument pairs. As with oth-
ers, training data is pre-clustered by event type and
there is no schema connection between relations.

This paper builds the most on Chambers and Ju-
rafsky (2011). They learned event schemas with a
three-stage clustering algorithm that included a re-
quirement to retrieve extra training data. This paper
removes many of these complexities. We present
a formal model that uniquely models coreference
chains. Advantages include a joint clustering of
events and entities, and a formal probabilistic inter-
pretation of the resulting schemas. We achieve better
performance, and do so with far less training data.

Cheung et al. (2013) is most related as a genera-
tive formulation of schema induction. They propose
an HMM-based model over latent event variables,
where each variable generates the observed clauses.
Latent schema variables generate the event vari-
ables (in the spirit of preliminary work by O’Connor
(2012)). There is no notion of an entity, so learning
uses text mentions and relies on the local HMM win-

1798



message: id dev-muc3-0112 (bellcore, mitre)
incident: date 10 mar 89
incident: location peru: huanuco, ambo (town)
incident: type bombing
incident: stage accomplished
incident: instrument explosive: ”-”
perp: individual ”shining path members”
perp: organization ”shining path”

Figure 1: A subset of the slots in a MUC-4 template.

dow for event transitions. Their model was created
in parallel with our work, and provides a nice con-
trast in both approach and results. Ours outperforms
their model by 20% on a MUC-4 evaluation.

In summary, this paper extends most previous
work on event schema induction by removing the
supervision. Of the recent ‘unsupervised’ work, we
present the first entity-driven generative model, and
we experiment on a mixed-domain corpus.

3 Dataset: The MUC-4 Corpus

The corpus from the Message Understanding Con-
ference (MUC-4) serves as the challenge text (Sund-
heim, 1991), and will ground discussion of our
model. MUC-4 is also used by the closest previ-
ous work. It contains Latin American newswire
about terrorism events, and it provides a set of
hand-constructed event schemas that are tradition-
ally called template schemas. It also maps labeled
templates to the text, providing a dataset for tem-
plate extraction evaluations. Until very recently,
only extraction has been evaluated. We too evalu-
ate our model through extraction, but we also com-
pare our learned schemas to the hand-created tem-
plate schemas. An example of a filled in MUC-4
template is given in Figure 1.

The MUC-4 corpus defines six template types:
Attack, Kidnapping, Bombing, Arson, Robbery,
and Forced Work Stoppage. Documents are often
labeled with more than one template and type. Many
include multiple events at different times in different
locations. The corpus is particularly challenging be-
cause template schemas are inter-mixed and entities
can play multiple roles across instances.

The training corpus contains 1300 documents,
733 of which are labeled with at least one schema.
567 documents are not labeled with any schemas.

These unlabeled documents are articles that report
on non-specific political events and speeches. They
make the corpus particularly challenging. The de-
velopment and test sets each contain 200 documents.

4 A Generative Model for Event Schemas

This paper’s model is an entity-based approach, sim-
ilar in motivation to Haghighi and Klein (2010) and
the pipelined induction of Chambers and Jurafsky
(2011). Coreference resolution guides the learning
by providing a set of pre-resolved entities. Each
entity receives a schema role label, so it allows all
mentions of the entity to inform that role choice.
This important constraint links coreferring mentions
to the same schema role, and distinguishes our ap-
proach from others (Cheung et al., 2013).

4.1 Illustration

The model represents a document as a set of enti-
ties. An entity is a set of entity mentions clustered
by coreference resolution. We will use the following
two sentences for illustration:

A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.

This text contains five entity mentions. A perfect
coreference resolution system will resolve these five
mentions into three entities:

Entity Mentions Entities Roles
a truck bomb (a truck bomb, it) Instrument
the embassy (the embassy) Target
three militia (three militia, they) Perpetrator
it
they

The schema roles, or template slots, are the type
of target knowledge we want to learn. Each en-
tity will be labeled with both a slot variable s and
a template variable t (e.g., the s=perpetrator of a
t=bombing). The lexical context of the entity men-
tions guides the learning model to this end.

4.2 Definitions

A document d ∈ D is represented as a set of entities
Ed. Each entity e ∈ Ed is a triple: e = (h,M,F )

1. he is the canonical word for the entity (typically
the first mention’s head word)

1799



Text
A truck bomb exploded near the embassy.
Three militia planted it, and then they fled.

Entity Representation
entity 1: h = bomb, F = {PHYS-OBJ},

M = { (p=explode, d=subject-explode)
(p=plant, d=object-plant) }

entity 2: h = militia, F = {PERSON, ORG},
M = { (p=plant, d=subject-plant),

(p=flee, subject-flee) }
entity 3: h = embassy, F = {PHYS-OBJ, ORG},

M = { (p=explode, d=prep near-explode) }

Figure 2: Example text mapped to our entities.

2. Me is a set of entity mentions m ∈ Me. Each
mention is a pairm = (p, d): the predicate, and
the typed dependency from the predicate to the
mention (e.g., push and subject-push).

3. Fe is a set of binary entity features. This paper
only uses named entity types as features, but
generalizes to other features as well.

A document is thus reduced to its entities, their
grammatical contexts, and entity features. Figure 2
continues our example using this formulation. he is
chosen to be e’s longest non-pronoun mention m ∈
Me. Mentions are labeled with NER and WordNet
synsets to create an entity’s features Fe ⊆ {Person,
Org, Loc, Event, Time, Object, Other}. We use the
Stanford NLP toolkit to parse, extract typed depen-
dencies, label with NER, and run coreference.

4.3 The Generative Models
Similar to topics in LDA, each document d in our
model has a corresponding multinomial over schema
types θd, drawn from a Dirichlet. For each entity in
the document, a hidden variable t is drawn accord-
ing to θd. These t variables represent the high level
schema types, such as bombing or kidnapping. The
predicates associated with each of the entity’s men-
tions are then drawn from the schema’s multinomial
over predicates Pt. The variable t also generates
a hidden variable s from its distribution over slots,
such as perpetrator and victim. Finally, the entity’s
canonical head word is generated from βs, all entity
mentions’ typed dependencies from δs, and named
entity types from γs.

The most important characteristic of this model
is the separation of event words from the lexical
properties of specific entity mentions. The schema
type variables t only model the distribution of event
words (bomb, plant, defuse), but the slot variables
s model the syntax (subject-bomb, subject-plant,
object-arrest) and entity words (suspect, terrorist,
man). This allows the high-level schemas to first se-
lect predicates, and then forces predicate arguments
to prefer slots that are in the parent schema type.

Formally, a document d receives a labeling Zd
where each entity e ∈ Ed is labeled Zd,e = (t, s)
with a schema type t and a slot s. The joint distribu-
tion of a document and labeling is then as follows:

P (d, Zd) =
∏

e∈Ed

P (t|θ)× P (s|t)

×
∏

e∈Ed

P (he|s)

×
∏

e∈Ed

∏
f∈Fe

P (f |s)

×
∏

e∈Ed

∏
m∈Me

P (dm|s) ∗ P (pm|t) (1)

The plate diagram for the model is given in Fig-
ure 3. The darker circles correspond to the observed
entity components in Figure 2. We assume the fol-
lowing generative process for a document d:

Generate θd from Dir(α)
for each schema type t = 1...m do

Generate Pt from Dir(η)
for each slot st = 1...k do

Generate βs from Dir(µ)
Generate γs from Dir(ν)
Generate δs from Dir(ϕ)

for each entity e ∈ Ed do
Generate schema type t from Multinomial(θd)
Generate slot s from UniformDist(k)
Generate head word h from Multinomial(βs)
for each mention m ∈Me do

Generate predicate token p from Multinomial(Pt)
Generate typed dependency d from Multinomial(δs)

for each entity type i = 1..|Fe| do
Generate entity type f from Multinomial(γs)

The number of schema types m and the number
of slots per schema k are chosen based on training
set performance.

1800



s

h d

θ

f

β γ δ
k

E

MF

t

v

V
m

s

h d

θ

f

β γ δ
kxm

D

E

MF

t

p

P
m

NAMED ENTITIES

ENTITY MENTIONSENTITY FEATURESENTITY HEAD

DOCUMENTS

Figure 3: The full plate diagram for the event schema
model. Hyper-parameters are omitted for readability.

The Flat Relation Model
We also experiment with a Flat Relation Model that
removes the hidden t variables, ignoring schema
types. Figure 4 visually compares this flat model
with the full model. We found that the predicate
distribution Pt hurts performance in a flat model.
Predicates are more informative at the higher level,
but less so for slots where syntax is more important.
We thus removed Pt from the model, and everything
else remains the same. This flat model now learns
a large set of k slots S that aren’t connected by a
high-level schema variable. Each slot s ∈ S has a
corresponding triple of multinomials (h,M,F ) sim-
ilar to above: (1) a multinomial over the head men-
tions βs, (2) a multinomial over the grammatical re-
lations of the entity mentions δs, and (3) a multino-
mial over the entity features γs. For each entity in
a document, a hidden slot s ∈ S is first drawn from
Θ, and then the observed entity (h,M,F ) is drawn
according to the multinomials (βs, γs, δs). We later
evaluate this flat model to show the benefit of added
schema structure.

4.4 Inference
We use collapsed Gibbs sampling for inference,
sampling the latent variables te,d and se,d in se-

s

d

θ

δ
k

D
E

M

h

β

s

d

θ

δ
k

D
E

M

t

p

P
|T|

h

β

Figure 4: Simplified plate diagrams comparing the flat
relation model to the full template model. The observed
f ∈ F variables are not included for clarity.

quence conditioned on a full setting of all the other
variables (Griffiths and Steyvers, 2004). Initial pa-
rameter values are set by randomly setting t and s
variables from the uniform distribution over schema
types and slots, then computing the other parameter
values based on these initial settings. The hyperpa-
rameters for the dirichlet distributions were chosen
from a small grid search (see Experiments).

Beyond standard inference, we added one con-
straint to the model that favors grammatical distri-
butions δs that do not contain conflicts. The subject
and direct object of a verb should not both receive
high probability mass under the same schema slot
δs. For instance, the victim of a kidnapping should
not favor both the subject and object of a single verb.
Semantic roles should (typically) select one syntac-
tic slot, so this constraint encourages that behavior.
During sampling of se,d, we use a penalty factor λ
to make conflicting relations less likely. Formally,
P (se,d = s|θ, he, Fe,Me) = λ iff there exists an
m ∈ Me such that P (m|σs) < P (inv(m)|σs) and
P (inv(m)|σs) > 0.1, where inv(m) = object if
m = subject and vice versa. Otherwise, the proba-
bility is computed as normal. We normalize the dis-
tributions after penalties are computed.

4.5 Entity Extraction for Template Filling

Inducing event schemas is only one benefit of the
model. The learned model can also extract spe-
cific instances of the learned schemas without ad-

1801



ditional complexity. To evaluate the effectiveness of
the model, we apply the model to perform standard
template extraction on MUC-4. Previous MUC-4
induction required an extraction algorithm separate
from induction because induction created hard clus-
ters (Chambers and Jurafsky, 2011). Cluster scores
don’t have a natural interpretation, so extraction re-
quired several parameters/thresholds to tune. Our
model instead simply relies on model inference.

We run inference as described above and each en-
tity receives a template label te,d and a template slot
label se,d. These labels are the extractions, and it re-
quires no other parameters. The model thus requires
far less machinery than a pipeline, and the exper-
iments below further show that this simpler model
outperforms the pipeline.

Beyond parameters, the question of “irrelevant”
documents is a concern in MUC-4. Approximately
half the corpus are documents that are not labeled
with a template, so past algorithms required extra
processing stages to filter out these irrelevant doc-
uments. Patwardhan and Riloff (2009) and Cham-
bers and Jurafsky (2011) make initial decisions as to
whether they should extract or not from a document.
Huang and Riloff (2011) use a genre detector for this
problem. Even the generative HMM-based model of
Cheung et al. (Cheung et al., 2013) requires an ex-
tra filtering parameter. Our formal model is unique
in not requiring additional effort. Ours is the only
approach that doesn’t require document filtering.

5 Evaluation Setup

Evaluating on MUC-4 has a diverse history that
complicates comparison. The following balances
comparison against previous work and enables fu-
ture comparison to our results.

5.1 Template Schema Slots
Most systems do not evaluate performance on all
MUC-4 template slots. They instead focus on four
main slots, ignoring the parameterized slots that in-
volve deeper reasoning (such as ‘stage of execution’
and ‘effect of incident’). The four slots and example
entity fillers are shown here:

Perpetrator: Shining Path members
Victim: Sergio Horna
Target: public facilities
Instrument: explosives

We also focus only on these four slots. We merged
MUC’s two perpetrator slots (individuals and orgs)
into one gold Perpetrator. Previous work has both
split the two and merged the two. We merge them
because the distinction between an individual and
an organization is often subtle and not practically
important to analysts. This is also consistent with
the most recent event schema induction in Chambers
and Jurafsky (2011) and Cheung et al. (2013).

One peculiarity in MUC-4 is that some templates
are labeled as optional (i.e., all its slots are optional),
and some required templates contain optional slots
(i.e., a subset of slots are optional). We ignore
both optional templates and specific optional slots
when computing recall, as in previous work (Pat-
wardhan and Riloff, 2007; Patwardhan and Riloff,
2009; Chambers and Jurafsky, 2011).

Comparison between the extracted strings and the
gold template strings uses head word scoring. We
do not use gold parses for the text, so head words
are defined simply as the rightmost word in the noun
phrase. The exception is when the extracted phrase
is of the form “A of B”, then the rightmost word in
“A” is used as the head. This is again consistent with
previous work1. The standard evaluation metrics are
precision, recall, and F1 score.

5.2 Mapping Learned Slots
Induced schemas need to map to gold schemas be-
fore evaluation. Which learned slots correspond to
MUC-4 slots? There are two methods of mapping.
The first ignores the schema type variables t, and
simply finds the best performing s variable for each
gold template slot2. We call this the slot-only map-
ping evaluation. The second approach is to map each
template variable t to the best gold template type g,
and limit the slot mapping so that only the slots un-
der t can map to slots under g. We call this the tem-
plate mapping evaluation. The slot-only mapping
can result in higher scores since it is not constrained
to preserve schema structure in the mapping.

Chambers and Jurafsky (2011) used template
mapping in their evaluation. Cheung et al. (2013)
used slot-only mapping. We run both evaluations in
this paper and separately compare both.

1Personal communications with Patwardhan and Riloff
2bombing-victim is a template slot distinct from kidnap-

victim. Both need to be mapped.

1802



6 Experiments

We use the Stanford CoreNLP toolkit for text pro-
cessing and parsing. We developed the models on
the 1300 document MUC-4 training set. We then
learned once on the entire 1700 training/dev/test set,
and report extraction numbers from the inferred la-
bels on the 200 document test set. Each experiment
was repeated 10 times. Reported numbers are aver-
aged across these runs.

There are two structure variables for the model:
the number of schema types and the number of slots
under each type. We searched for the optimal values
on the training set before evaluating on test. The
hyperparameters for all evaluations were set to α =
η = µ = ν = 1, ϕ = .1 based on a grid search.

6.1 Template Schema Induction

The first evaluation compares the learned schemas
to the gold schemas in MUC-4.

Since most previous work assumes this knowl-
edge ahead of time, we align our schemas with the
main MUC-4 template types to measure quality. We
inspected the learned event schemas that mapped to
MUC-4 schemas based on the template mapping ex-
traction evaluation.

Figure 5 shows some of the learned distribu-
tions for two mapped schemas: kidnappings and
bombings. The predicate distribution for each event
schema is shown, as well as the top 5 head words
and grammatical relations for each slot. The words
and events that were jointly learned in these exam-
ples appear quite accurate. The bombing and kidnap
schemas learned all of the equivalent MUC-4 gold
slots. Interestingly, our model also learned Loca-
tions and Times as important entities that appear in
the text. These entities are not traditionally included
in the MUC-4 extraction task.

Figure 6 lists the MUC-4 slots that we did and
did not learn for the four most prevelant types. We
report 71% recall, with almost all errors due to the
model’s failure to learn about arsons. Arson tem-
plates only occur in 40 articles, much less than the
200 bombing and over 400 attack. We show below
that overall extraction performs well despite this.
The learned distributions for Attack end up extract-
ing Arson perpetrators and Arson victims in the ac-
tual extraction evaluation.

Bomb Kidnap Attack Arson
Perpetrator X X X x
Victim X X X X
Target X - X x
Instrument X - x x
Location X X X X
Date/Time X X X x

Figure 6: The MUC-4 gold slots that were learned. The
bottom two are not in the traditional evaluation, but were
learned by our model nonetheless.

Evaluation: Template Mapping
Prec Recall F1

C & J 2011 .48 .25 .33
Formal Template Model .42 .27 .33

Table 1: MUC-4 extraction with template mapping. A
learned schema first maps to a gold MUC template.
Learned slots can then only map to slots in that template.

6.2 Extraction Experiments

We now present the full extraction experiment that
is traditionally used for evaluating MUC-4 per-
formance. Although our learned schemas closely
match gold schemas, extraction depends on how
well the model can extract from diverse lexical con-
texts. We ran inference on the full training and test
sets, and used the inferred labels as schema labels.
These labels were mapped and evaluated against the
gold MUC-4 labels as discussed in Section 5.

Performance is compared to two state-of-the-art
induction systems. Since these previous two mod-
els used different methods to map their learned
schemas, we compare separately. Table 1 shows the
template mapping evaluation with Chambers and Ju-
rafsky (C&J). Table 2 shows the slot-only mapping
evaluation with Cheung et al.

Our model achieves an F1 score comparable to
C&J, and 20% higher than Cheung et al. Part of the
greater increase over Cheung et al. is the mapping
difference. For each MUC-4 type, such as bombing,
any four learned slots can map to the four MUC-
4 bombing slots. There is no constraint that the
learned slots must come from the same schema type.
The more strict template mapping (Table 1) ensures
that entire schema types are mapped together, and it
reduces our performance from .41 to .33.

1803



Kidnapping Entities
Victim (Person 88%)

businessman object-kidnap
citizen object-release
Soares prep of-kidnapping
Kent possessive-release

hostage object-found

Perpetrator (Person 62%, Org 30%)
guerrilla subject-kidnap

ELN subject-hold
group subject-attack

extraditables subject-demand
man subject-announce

Date (TimeDate 89%)
TIME tmod-kidnap

February prep on-kidnap
hours tmod-release

morning prep on-release
night tmod-take

Bombing Entities
Victim (Person 86%, Location 8%)
person object-kill

guerrilla object-wound
soldier subject-die

man subject-blow up
civilian subject-try

Physical Target (Object 65%, Event 42%)
building object-destroy

office object-damage
explosive object-use

station and-office
vehicle prep of-number

Instrument (Event 56%, Object 39%)
bomb subject-explode

explosion subject-occur
attack object-cause
charge object-place
device subject-destroy

Figure 5: Select distributions for two learned events. Left columns are head word distributions β, right columns are
syntactic relation distributions δ, and entity types in parentheses are the learned γ. Most probable words are shown.

Evaluation: Slot-Only Mapping
Prec Recall F1

Cheung et al. 2013 .32 .37 .34
Flat Relation Model .26 .45 .33
Formal Template Model .41 .41 .41

Table 2: MUC-4 extraction with slot-only mapping. Any
learned slot is allowed to map to any gold slot.

Entity Role Performance
Prec Recall F1

Perpetrator .40 .20 .26
Victim .42 .31 .34
Target .38 .28 .31
Instrument .57 .39 .45

Table 3: Results for each MUC-4 template slot using the
template-mapping evaluation.

The macro-level F1 scores can be broken down
into individual slot performance. Table 3 shows
these results ranging from .26 to .45. The Instrument
role proves easiest to learn, consistent with C&J.

A large portion of MUC-4 includes irrelevant
documents. Cheung et al. (2013) evaluated their
model without irrelevant documents in the test set
that to see how performance is affected. We com-
pare against their numbers in Table 4. Results are
closer now with ours outperforming .46 to .43 F1.
This suggests that the HMM-based approach stum-
bles more on spurious documents, but performs bet-
ter on relevant ones.

Gold Document Evaluation
Prec Recall F1

Cheung et al. 2013 .41 .44 .43
Formal Template Model .49 .43 .46

Table 4: Full MUC-4 extraction with gold document clas-
sification. These results ignore false positives extracted
from “irrelevant” documents in the test set.

6.3 Model Ablation

Table 2 shows that the flat relation model (no latent
type variables t) is inferior to the full schema model.
F1 drops 20% without the explicit modeling of both
schema types t and their entity slots s. The entity
features Fe are less important. Experiments with-
out them show a slight drop in performance (2 F1
points), small enough that they could be removed for
efficiency. However, it is extremely useful to learn
slots with NER labels like Person or Location.

Finally, we experimented without the sub-
ject/object constraint (Section 4.4). Performance
drops 5-10% depending on the number of schemas
learned. Anecdotally, it merges too many schema
slots that should be separate. We recommend using
this constraint as it has little impact on CPU time.

6.4 Extension: Reduce Training Size

One of the main benefits of this generative model
appears to be the reduction in training data. The
pipelined approach in C&J required an information
retrieval stage to bring in hundreds of other docu-

1804



ments from an external corpus. This paper’s genera-
tive model doesn’t require such a stage.

We thus attempted to induce and extract event
schemas from just the 200 test set documents, with
no training or development data. We repeated this
experiment 30 times and averaged the results, setting
the number of templates t = 20 and slots s = 10 as
in the main experiment. The resulting F1 score for
the template-mapping evaluation fell to 0.27 from
the full data experiment of 0.33 F1. Adding more
training documents in another experiment did not
significantly increase performance over 0.27 until
all training and development documents were in-
cluded. This could be explained by the develop-
ment set being more similar to the test set than train-
ing. We did not investigate further to prevent over-
experimentation on test.

7 Discussion

Our model is one of the first generative formula-
tions of schema induction. It produces state-of-the-
art performance on a traditional extraction task, and
performs with less training data as well as a more
complex pipelined approach. Further, our unique
entity-driven approach outperforms an HMM-based
model developed in parallel to this work.

Our entity-driven proposal is strongly influenced
by the ideas in the pipeline model of Chambers and
Jurafsky (2011). Coreference chains have been used
in a variety of learning tasks, such as narrative learn-
ing and summarization. Here we are the first to show
how it can be used for schema induction in a proba-
bilistic model, connecting predicates across a docu-
ment in a way that is otherwise difficult to represent.
The models perform similarly, but ours also includes
significant benefits like a reduction in complexity,
reproducibility, and a large reduction in training data
requirements.

This paper also implies that learning and ex-
traction need not be independent algorithms. Our
model’s inference procedure to learn schemas is the
same one that labels text for extraction. C&J re-
quired 3-4 separate pipelined steps. Cheung et al.
(2013) required specific cutoffs for document classi-
fication before extraction. Not only does our model
perform well, but it does so without these steps.

Highlighted here are key differences between this

proposal and the HMM-based model of Cheung et
al. (2013). One of the HMM strengths is the in-
clusion of sequence-based knowledge. Each slot la-
bel is influenced by the previous label in the text,
encouraging syntactic arguments of a predicate to
choose the same schema. This knowledge is only
loosely present in our document distribution θ. Che-
ung et al. also include a hidden event variable be-
tween the template and slot variables. Our model
collapses this event variable and makes fewer depen-
dency assumptions. This difference requires further
investigation as it is unclear if it provides valuable
information, or too much complexity.

We also note a warning for future work on proper
evaluation methodology. This task is particularly
difficult to compare to other models due to its
combination of both induction and then extraction.
There are many ways to map induced schemas to
gold answers, and this paper illustrates how ex-
traction performance is significantly affected by the
choice. We suggest the template-mapping evalua-
tion to preserve learned structure.

Finally, these induced results are far behind su-
pervised learning (Huang and Riloff, 2011). There
is ample room for improvement and future research
in event schema induction.

Acknowledgments

This work was partially supported by a grant from
the Office of Naval Research. It was also sup-
ported, in part, by the Johns Hopkins Human Lan-
guage Technology Center of Excellence. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author.
Thanks to Eric Wang for his insights into Bayesian
modeling, Brendan O’Connor for his efforts on nor-
malizing MUC-4 evaluation details, Frank Ferraro
and Benjamin Van Durme for helpful conversations,
and to the reviewers for insightful feedback.

1805



References
Michele Banko, Michael J Cafarella, Stephen Soderland,

Matt Broadhead, and Oren Etzioni. 2007a. Learning
relations from the web. In Proceedings of the Interna-
tional Joint Conferences on Artificial Intelligence (IJ-
CAI).

Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007b. Open in-
formation extraction from the web. In Proceedings of
the International Joint Conferences on Artificial Intel-
ligence (IJCAI).

Razvan Bunescu and Raymond Mooney. 2004. Collec-
tive information extraction with relational markov net-
works. In Proceedings of the Association for Compu-
tational Linguistics (ACL), pages 438–445.

Andrew Carlson, J. Betteridge, B. Kisiel, B. Settles,
E.R. Hruschka Jr., and T.M. Mitchell. 2010a. To-
ward an architecture for never-ending language learn-
ing. In Proceedings of the Conference on Artificial
Intelligence (AAAI).

Andrew Carlson, J. Betteridge, R.C. Wang, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010b. Coupled semi-
supervised learning for information extraction. In Pro-
ceedings of the ACM International Conference on Web
Search and Data Mining (WSDM).

Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the Association for Computational Lin-
guistics.

Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL).

Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.

Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In Proceedings of the Association for Computational
Linguistics (ACL).

Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19:3:409–449.

Benjamin Van Durme and Marius Pasca. 2008. Finding
cars, goddesses and enzymes: Parametrizable acquisi-
tion of labeled instances for open-domain information
extraction. In Proceedings of the 23rd Annual Con-
ference on Artificial Intelligence (AAAI-2008), pages
1243–1248.

Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of the Association for Compu-
tational Linguistics (ACL).

Dayne Freitag. 1998. Toward general-purpose learning
for information extraction. In Proceedings of the As-
sociation for Computational Linguistics (ACL), pages
404–408.

T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. In Proceedings of the National Academy of
Sciences of the United States of America, pages 5228–
5235.

Aria Haghighi and Dan Klein. 2010. An entity-level ap-
proach to information extraction. In Proceedings of
the Association for Computational Linguistics (ACL).

Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the Association for Com-
putational Linguistics (ACL).

Ruihong Huang and Ellen Riloff. 2011. Peeling back the
layers: Detecting event role fillers in secondary con-
texts. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics (ACL).

Mstislav Maslennikov and Tat-Seng Chua. 2007. Auto-
matic acquisition of domain knowledge for informa-
tion extraction. In Proceedings of the Association for
Computational Linguistics (ACL).

Brendan O’Connor. 2012. Learning frames from text
with an unsupervised latent variable model. Technical
report, Carnegie Mellon University.

Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing (EMNLP).

Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In Proceedings of the Conference on
Empirical Methods on Natural Language Processing
(EMNLP).

Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Ge nltoolset: Muc-4 test results
and analysis. In Proceedings of the Message Under-
standing Conference (MUC-4), pages 94–99.

Ellen Riloff and Mark Schmelzenbach. 1998. An em-
pirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large
Corpora.

Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve infor-
mation extraction. In Proceedings of AAAI-05.

Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the Joint Conference of the

1806



International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics, pages 731–738.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
ie using unrestricted relation discovery. In Proceed-
ings of NAACL.

Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL), pages 224–231.

Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.

Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 2006.
A hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the EACL Work-
shop on Adaptive Text Extraction and Mining.

Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisi-
tion of domain knowledge for information extraction.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING), pages 940–
946.

1807


