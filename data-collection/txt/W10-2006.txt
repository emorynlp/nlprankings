










































Close = Relevant? The Role of Context in Efficient Language Production


Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45–53,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

Close = Relevant? The Role of Context in Efficient Language Production

Ting Qian and T. Florian Jaeger
Department of Brain and Cognitive Sciences

University of Rochester
Rochester, NY 14627 United States

{tqian,fjaeger}@bcs.rochester.edu

Abstract
We formally derive a mathematical model
for evaluating the effect of context rele-
vance in language production. The model
is based on the principle that distant con-
textual cues tend to gradually lose their
relevance for predicting upcoming linguis-
tic signals. We evaluate our model against
a hypothesis of efficient communication
(Genzel and Charniak’s Constant Entropy
Rate hypothesis). We show that the devel-
opment of entropy throughout discourses
is described significantly better by a model
with cue relevance decay than by previ-
ous models that do not consider context ef-
fects.

1 Introduction

In this paper, we present a study on the effect
of context relevance decay on the entropy of lin-
guistic signals in natural discourses. Context rele-
vance decay refers to the phenomenon that contex-
tual cues that are distant from an upcoming event
(e.g. production of a new linguistic signal) are less
likely to be relevant to the event, as discourse con-
tents that are close to one another are likely to be
semantically related. One can also view the words
and sentences in a discourse as time steps, where
distant context becomes less relevant simply due
to normal forgetting over time (e.g. activation de-
cay in memory). The present study investigates
how this decaying property of discourse context
might affect the development of entropy of lin-
guistic signals in discourses. We first introduce
the background on efficient language production
and then propose our hypothesis.

1.1 Background on Efficient Language
Production

The metaphor “communication channel”, bor-
rowed from Shannon’s information theory (Shan-

non, 1948), can be conceived of as an abstract en-
tity that defines the constraints of language com-
munication (e.g. ambient noise, distortions in ar-
ticulation). For error free communication to occur,
the ensemble of messages that a speaker may utter
must be encoded in a system of signals whose en-
tropy is under the capacity of the communication
channel. Entropy of these signals, in this context,
correlates with the average number of upcoming
messages that the speaker can choose from for a
particular signal (e.g. a word to be spoken) given
preceding discourse context. In other words, if
the average number of choices given any linguis-
tic signal exceeds the channel capacity, it cannot
be guaranteed that the receiver can correctly infer
the originally intended message. Such transmis-
sion errors will reduce the efficiency of language
communication.

Keeping the entropy of linguistic signals be-
low the channel capacity alone is not efficient, for
one can devise a code where each signal corre-
sponds to a distinct message. With a unique choice
per signal, this encoding achieves an entropy of
zero at the cost of requiring a look-up table that
is too large to be possible (cf. Zipf (1935), who
makes a similar argument for meaning and form).
In fact, the most efficient code requires language
users to encode messages into signals of the en-
tropy bounded by the capacity of the channel. One
implication of this efficient encoding is that over
time, the entropy of the signals is constant. One
of the first studies to investigate such constancy
is Genzel and Charniak (2002), in which the au-
thors proposed the Constant Entropy Rate (CER)
hypothesis: in written text, the entropy per sig-
nal symbol is constant across sentence positions in
discourses. That is, if we view sentence positions
as a measure of time steps, then the entropy per
word at each step should be the same in order to
achieve efficient communication (word is selected
as the unit of signal, although it does not have to

45



be case; cf. Qian and Jaeger (2009)).

The difficulty in testing this direct prediction is
computationally specifying the code used by hu-
man speakers to obtain a context-sensitive esti-
mate of the entropy per word. An ngram model
overestimates the entropy of upcoming messages
by relying on only the preceding n-1 words within
a sentence, while in reality the upcoming message
is also constrained by extra-sentential context that
accumulates within a discourse. The more extra-
sentential context that the ngram model ignores,
the higher estimate for entropy will be. Hence,
the CER hypothesis indirectly predicts that the
entropy of signals, as estimated by ngrams, will
increase across sentence positions. While some
studies have found the predicted positive correla-
tion between sentence position and the per-word
entropy of signals estimated by ngrams, most of
them assumed the correlation to be linear (Genzel
and Charniak, 2002; Genzel and Charniak, 2003;
Keller, 2004; Piantadosi and Gibson, 2008). How-
ever, in previous work, we found that a log-linear
regression model was a better fit for empirical data
than a simple linear regression model based on
data of 12 languages (Qian and Jaeger, under re-
view). Why this would be case remained a puzzle.

Our research question is closely related to this
indirect prediction of the Constant Entropy Rate
hypothesis. Intuitively, the number of possible
messages that a speaker can choose from for an
upcoming signal in a discourse is often restricted
by the presence of discourse context. Contex-
tual cues in the preceding discourse can make the
upcoming content more predictable and thus ef-
fectively reduces signal entropy. As previously
mentioned, however, different contextual cues, de-
pending on how long ago they were provided, have
various degrees of effectiveness in reducing sig-
nal entropy. Thus we ask the question whether
the decay of context relevance could explain the
sublinear relation between entropy and discourse
progress that has been observed in previous stud-
ies.

We formally derive two nonlinear models for
testing our Relevance Decay Hypothesis (intro-
duced next). In addition to the constant entropy as-
sumption in CER, our model assumed that the rel-
evance of early sentences in the discourse system-
atically decays as a function of discourse progress.
Our models provide the best fit to the distribution
of entropy of signals, suggesting the availability

of discourse context can affect the planning of the
rest of a discourse.

1.2 Relevance Decay Hypothesis

We hypothesize the sublinear relation between the
entropy of signals, when estimated out of dis-
course context (hereafter, out-of-context entropy
of signals) using an ngram model, and sentence
position (Piantadosi and Gibson, 2008; Qian and
Jaeger, under review) is due to the role of dis-
course context (hereafter, context). Consider the
following example. Assume that context at the kth
sentence position comes from the 1 . . . k − 1 sen-
tences in the past. If k is large enough, context
from the early sentences 1 . . . i (i � k) is essen-
tially no longer relevant. Rather, the nearby k − i
sentences are contributing most of the discourse
context. As a result, the constraint on the entropy
of signals at sentence position k is mostly due to
the nearby window of k − i sentences. Then if we
look ahead to the (k + 1)th sentence position and
follow the same steps of reasoning, context at that
point also mostly comes from the nearby window
of k− i sentences (i.e. (k+ 1)− (i+ 1) = k− i).
Hence, for later sentence positions, the difference
in available context is minimal. Consequently,
their out-of-context entropy of signals increases
at a very small rate. On the other hand, when k
is fairly small, to the extent that the k − i win-
dow covers the entire preceding discourse, all of
the 1 . . . k − 1 sentences are contributing relevant
context. As k increases, the number of preced-
ing sentences increases, which results in a more
significant change in relevant context, but the rel-
evance of each individual sentence decreases with
its distance to k, which results in a sublinear pat-
tern of relevant context with respect to sentence
position overall. As we will show, the relation of
out-of-context entropy of signals to sentence posi-
tion follows from the relation of relevant context
to sentence position, exhibiting a sublinear form
as well.

The problem of interest here is to specify how
quickly the relevance of a preceding sentence de-
cays as a function of its distance to a target sen-
tence position k. We experimented with two forms
of decay functions – power law decay and expo-
nential decay. It has been established that many
types of human behaviors can be well described by
the power function (Wixted and Ebbesen, 1991),
so we mainly focus on building a model under the

46



Language Training Data Test Data

in words in sentences in words in sentences per position

Danish 154,514 5,640 8,048 270 18
Dutch 50,309 3,255 2,105 90 6
English 597,698 23,295 31,276 1155 77
French 229,461 9,300 11,371 435 29
Italian 97,198 4,245 4,524 225 15
Mandarin Chinese 145,127 4,875 4,310 150 10
Norwegian 89,724 4,125 2,973 150 10
Portuguese 170,342 5,340 9,044 240 16
Russian 398,786 18,075 20,668 930 62
Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138
Spanish (European) 255,366 7,485 8,653 240 16
Swedish 266,348 11,535 13,369 555 37

Table 1: Number of words and sentences in the training and test data for each of the twelve languages.
The last column gives the number of sentences at each sentence position (which is identical to the number
of documents contained in the corpora).

power law, and examine if the model under the ex-
ponential law yields any difference. Under the as-
sumptions of true entropy rate is constant across
sentences, we predict that our models will bet-
ter characterize the changes in estimated entropy
of signals than general regression models that are
blind to the role of context.

2 Methods

2.1 Data

We used the Reuters Corpus Volume 1 and 2
(Lewis et al., 2004). The corpus contains about
810,000 English news articles and over 487,000
news articles in thirteen languages. Because of in-
consistent annotation, we excluded the data from
three languages, Chinese, German, and Japanese.
For Chinese, we substituted the Treebank Cor-
pus (Xue et al., 2005) for the Reuters data, leav-
ing us with twelve languages: Danish, Dutch,
English, French, Italian, Mandarin Chinese, Nor-
wegian, Portuguese, Russian, European Spanish,
Latin-American Spanish, and Swedish. In order
to estimate out-of-context entropy per word (i.e.
per signal symbol) for each sentence position, ar-
ticles were divided into a training set (95% of all
stories) for training language models and a test set
(the remaining 5%) for analysis (see Table 1 for
details). Out-of-context entropy per word was es-
timated by computing the average log probability
of sentences at that position, normalized by their
lengths in words (i.e. for an individual sentence
token s, the term to be averaged is − log p(s)length(s) bits per
word). Standard trigram language models were
used to compute these probabilities (Clarkson and

Rosenfeld, 1997). The majority of the 12 lan-
guages belong to the Indo-European family, while
Mandarin Chinese is a Sino-Tibetan language.

2.2 Modeling Relevance Decay of Context
Formally, we define the relevance of context in the
same unit as entropy of signals – bits per word.
Let r0 denote the entropy of signals that efficiently
encode the ensemble of messages a speaker can
choose from for any sentence position, a constant
under the assumption of CER. According to Infor-
mation Theory, r0 is equivalent to the uncertainty
associated with any sentence position if context is
considered. Thus, in error free communication,
linguistic signals presented at the kth sentence po-
sition are said to have resolved the uncertainty at
k and therefore are r0-bit relevant at the kth sen-
tence position. Then, at the (k+i)th sentence posi-
tion, these linguistic signals have become context
by definition and their relevance has decayed to
some r bits. Our models start from defining the
value of r as a function of the distance between
context and a target sentence position.

2.2.1 Power-law Decay Model
If the relevance of a cue q (e.g. a preceding sen-
tence), which is originally r0-bit relevant at po-
sition kq, decays at the rate following the power
function, its remaining relevance at target sentence
position k is:

relevancepow(k, q) = r0(k − kq + 1)−λ (1)

In Equation (1), k > kq and λ is the decay rate.
This means at position k, the relevance of the cue

47



from the (k−1)th sentence is r0∗2−λ-bit relevant;
the relevance of the cue from the (k−2)th sentence
is r0 ∗ 3−λ-bit relevant, and so on. As a result, the
relevance of discourse-specific context at position
k is the marginalization of all cues up to qk−1:

contextpow(k) = r0
∑

qi∈{q1...qk−1}

(k − kqi + 1)
−λ (2)

The general trend predicted by Equation (2)
is that discourse-specific context increases more
rapidly at the beginning of a discourse and much
more slowly towards the end due to the relevance
decay of distant cues. Rewriting Equation (2) in
a closed-form formula so that a model can be fit-
ted to data is not a trivial task without knowing the
rate λ, but the paradox is that λ has to be estimated
from the data. As a workaround, we approximated
the value of Equation (2) by computing a definite
integral of Equation (1), where ∆i is a shorthand
for k − kq + 1:

contextpow(k) ≈
∫ k

1
r0∆i

−λd∆i

= r0(
k1−λ − 1

1− λ
) (3)

Equation (3) uses an integral to approximate the
sum of a series defined as a function. The result
is usually acceptable as long as λ is greater than
1 so that the series defined by Equation 1 is con-
vergent (this assumption is empirically supported;
see Figure 5). Note that Equation (3) produces
the desirable effect that upon encountering the
first sentence of a discourse, no discourse-specific
contextual cues are available to the speaker (i.e.
context(1) = 0).

Now that we know the maximum relevance of
context at sentence position k, we can predict the
amount of out-of-context entropy of signals r(k)
based on the idea of uncertainty again. There are
new linguistic signals that are r0-bit relevant in
context at any sentence position. In addition, we
now know context(k) bits of relevant context are
also available. Thus, the sum of r0 and context(k)
defines the maximum amount of out-of-context
uncertainty that can be resolved at sentence posi-
tion k. Therefore, the out-of-context entropy of
signals at k is at most:

rpow(k) = context(k) + r0 (4)

= r0
k1−λ − 1

1− λ
+ r0

Whether speakers will utilize all available con-
text as predicted by Equation (4) is another de-
bate. Here we adopt the view that speakers are
maximally efficient in that they do make use of
all available context. Thus, we make the predic-
tion that out-of-context entropy of signals, as ob-
served empirically from data, can be described by
this model. Figure 1 shows the behavior of this
function with various parameter sets.

2 4 6 8 10 12 14

5
6

7
8

9
10

11
12

Sentence Position

M
od

el
−

P
re

di
ct

ed
 E

nt
ro

py
 p

er
 W

or
d

r0 = 5.5,λ = 2
r0 = 5.5,λ = 2.2
r0 = 5,λ = 2
r0 = 5,λ = 2.2

Figure 1: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is a power function.

2.2.2 Exponential Decay Model
The second model assumes the relevance of con-
text decays exponentially. Following the same no-
tations as before, the relevance of a cue q at posi-
tion k is:

relevanceexp(k, q) = r0e
−λ(k−kq) (5)

The major difference between the power func-
tion and the exponential one is that the relevance
of a contextual cue drops more slowly in the expo-
nential case (Anderson, 1995). The relevance of
all discourse-specific context for a speaker at k is:

contextexp(k) = r0

k−1∑
i=1

e−λi (6)

48



Equation (6) is the sum of a geometric progres-
sion series. We can write Equation (6) in a closed-
form:

contextexp(k) =
r0

eλ − 1
(1− e−(k−1)λ) (7)

As a result, the out-of-context entropy of signals
is:

rexp(k) =
r0

eλ − 1
(1− e−(k−1)λ) + r0 (8)

Figure 2 schematically shows the behavior of
this function. One can notice this function con-
verges against a ceiling more quickly than the
power function. Thus, this model makes a slightly
different prediction from the power law model.

2 4 6 8 10 12 14

5
6

7
8

9
10

11
12

Sentence Position

M
od

el
−

P
re

di
ct

ed
 E

nt
ro

py
 p

er
 W

or
d

r0 = 5.5,λ = 0.6
r0 = 5.5,λ = 0.8
r0 = 5,λ = 0.6
r0 = 5,λ = 0.8

Figure 2: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is an exponential func-
tion.

2.3 Nonlinear Regression Analysis

To test whether the proposed models (i.e. Equa-
tions 4 and 8) better characterize the data, we
built nonlinear regression models with document-
specific random effects, where the out-of-context
entropy of signals, rij , is regressed on sentence
position, kj . Based on the power law model, we
have

rij = (β1+b1i)
kj

1−β2 − 1
1− β2

+(β1+b1i)+�ij (9)

where β1 corresponds to r0, the theoretical con-
stant entropy of signals under an ideal encod-
ing. b1i represents the document-specific devia-
tions from the overall mean. β2 corresponds to λ,
the mean rate at which the relevance of a past cue
decays, which is unfortunately not considered for
random effects for the practical purpose of making
computation feasible in the current work. Finally,
�ij represents the errors independently distributed
as N (0, σ2), orthogonal to document specific de-
viations.

For the exponential model, the nonlinear model
is the following (symbols have the same interpre-
tations as in Equation 9):

rij =
(β1 + b1i)

eβ2 − 1
(1−e−(kj−1)β2)+(β1+b1i)+�ij

(10)
Fitting data with the above nonlinear models

requires starting estimates for fixed-effect coeffi-
cients (i.e. β1s and β2s). Unfortunately, there are
no principled methods for selecting these values.
We heuristically selected 6 for β1 and 2 for β2 as
starting values for the power law model, and 4 and
0.5 as starting values for the exponential model.

3 Results

We examined the quality of the models and the pa-
rameters in the models: r0, the within-context en-
tropy rate, and λ, the rate of context decay.

3.1 Model Quality Comparison

The CER hypothesis indirectly predicts that out-
of-context entropy of signals of sentence positions
(bits per word) should increase throughout a dis-
course. The two models go one step further to
predict specific sublinear increase patterns, based
on the speaker’s considerations of the relevance of
past contextual cues. We compared the quality of
models in terms of Bayesian Information Criterion
(BIC) within languages. A lower BIC score indi-
cates a better fit. As shown by Figure 3, we find
our models best explain the data in 9 out of the 12
languages, reporting lower BIC scores than both
the linear and log-linear models as reported in our
previous work (Qian and Jaeger, under review).
For Danish, English and Italian, although neither
of our models produced a better score than the log-
linear model, the relative difference is small: 0.54
on average (comparing to BIC scores on the order
of 102 to 103).

49



Danish Dutch English French Italian Mandarin Norwegian Portuguese Russian E.Spanish L.Spanish Swedish

Power Law Exponential Loglinear Linear
−

20
−

10
0

10
20

Figure 3: Our models yield superior BIC scores in most languages. The y-axis shows the differences
between BIC scores of individual models for a language and mean BIC of the models for that language
(E.Spanish = European Spanish; L.Spanish = Latin-American Spanish).

Specifically, in terms of BIC scores, the power-
law model is better than the linear model (t(11) =
−3.98, p < 0.01), and the log-linear model
(t(11) = −3.10, p < 0.05). The exponen-
tial model is also better than the linear model
(t(11) = −3.98, p < 0.01), and the log-linear
model (t(11) = −3.18, p < 0.01). The power-
law model and the exponential model are not sig-
nificantly different from each other (t(11) = 0.5,
p > 0.5).

3.2 Interpretation of Parameters
Constant Entropy of Signals r0. Both models
are constructed in such a way that the first param-
eter r0, in theory, corresponds to the theoretical
within-context entropy of signals of sentence po-
sitions. This parameter refers to how many bits per
word are needed to encode the ensemble of mes-
sages at a sentence position when context is taken
into account. The CER hypothesis directly pre-
dicts that this rate should be constant throughout
a discourse. Although we are unable to test this
prediction directly, it is nevertheless interesting to
compare whether these two independently devel-
oped models yield the same estimates for this pa-
rameter in each language.

Figure 4 shows encouraging results. Not only
the estimates made by the power model are well
correlated with those by the exponential model,
but also the slope of this correlation is equal to 1
(t(10) = 1.01, p < 0.0001). Since there are no
reasons a priori to suspect that these two models

●

●

4.0 4.5 5.0 5.5 6.0 6.5

4.
0

4.
5

5.
0

5.
5

6.
0

6.
5

Estimated Within−context Entropy per Word (Exponential Model)

E
st

im
at

ed
 W

ith
in

−
co

nt
ex

t E
nt

ro
py

 p
er

 W
or

d 
(P

ow
er

−
la

w
 M

od
el

)

●

●

Danish
Dutch
English
French
Italian
Mandarin
Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish

Figure 4: Estimates of r0 correlate between both
models with a slope of 1.

would give the same estimates, this is a first step to
confirming the entropy per word in sentence pro-
duction is indeed a tractable constant throughout
discourses.

Among all languages, r0 has a mean of 5.0
bits in both models, and a variance of 0.46 in
the power-law model and 0.48 in the exponential
model, both remarkably small. The similarity in
r0 between languages may lead one to speculate
whether the amount of uncertainty per word in dis-
courses is largely the same regardless of the actual
language used by the speakers. On the other hand,

50



the differences in r0 may reveal the specific prop-
erties of different languages. Meanwhile, precau-
tions need to be taken in interpreting those esti-
mates given that the corpora are of different sizes,
and the ngram model is simplistic in nature.

Decay Rate λ. The second parameter λ corre-
sponds to the rate of relevance decay in both mod-
els. Since the base relevance r0 varies between
languages, λ can be more intuitively interpreted as
to indicate the percentage of the original relevance
of a contextual cue still remains in n positions. In
the power-law model, for example, the context in-
formation from a previous sentence in Danish, on
average, is only 11.6% (2−3.10 = 0.116) as rele-
vant. Hence, the relevance of a contextual cue de-
creases rather quickly for Danish. Table 2 shows
this is in fact the general picture for all languages
we tested.

Language Relevance of Context in Discourse (%)

1 pos. before 2 pos. before 3 pos. before

Danish 11.6 3.3 1.4
Dutch 10.4 2.8 1.1
English 0.1 0.0 0.0
French 8.5 2.0 0.7
Italian 10.2 2.7 1.0
Mandarin 7.7 1.7 0.6
Norwegian 18.9 7.1 3.6
Portuguese 5.5 1.0 0.3
Russian 12.7 3.8 1.6
E. Spanish 0.8 0.0 0.0
L. Spanish 2.7 0.3 0.1
Swedish 5.8 1.1 0.3

Table 2: In the power model, relevance of a con-
textual cue decays rather quickly for each lan-
guage.

The picture of λ looks a little different in the
exponential model. The relevance percentage on
average is significantly higher, which confirms an
earlier point that the power function decreases
more quickly than the exponential function. Table
3 shows a summary for the 12 languages.

One may note that the decay rate varies greatly
between languages under the prediction of both
models. However, these number are only approxi-
mations since the entropy estimated by the ngram
language model is far from psychological real-
ity. Furthermore, it is unlikely that speakers of
one language would exhibit the same decay rate
of context relevance in their production, let alone
speakers of different languages, who may be sub-
ject to language-specific constraints during pro-

Language Relevance of Context in Discourse (%)

1 pos. before 2 pos. before 3 pos. before

Danish 30.1 9.1 2.7
Dutch 28.7 8.2 2.4
English 9.6 0.9 0.1
French 26.7 7.1 1.9
Italian 28.7 8.2 2.4
Mandarin 25.7 6.6 1.7
Norwegian 42.3 17.9 7.6
Portuguese 22.5 5.1 1.1
Russian 34.6 12.0 4.2
E. Spanish 14.2 2.0 0.3
L. Spanish 18.6 3.5 0.6
Swedish 23.7 5.6 1.3

Table 3: In the exponential model, relevance of a
contextual cue decays more slowly.

duction. Therefore, the variation in estimates of
λ seems reasonable.

Correlation between r0 and λ. Interestingly, r0
and λ are highly correlated (r2 = 0.39, p < 0.05
in the power model, Figure 5; r2 = 0.47, p < 0.01
in the exponential model, Figure 6): a high rel-
evance decay rate tends to be coupled with high
within-context entropy of signals. This unan-
ticipated observation is in fact compatible with
the account of efficient language production: a
high within-context entropy of signals indicates
the base relevance of a contextual cue (i.e. r0)
is high. It is then useful for its relevance to de-
cay more quickly to allow the speaker to inte-
grate context from other cues. Otherwise, the to-
tal amount of relevant context may presumably
overload working memory. However, our cur-
rent results come from only cross-linguistic sam-
ples. Cross-validation in within-language samples
is needed for confirming this hypothesis.

3.3 The Bigger Picture

Having obtained the estimates for r0 and λ, we are
now in a position to examine how out-of-context
entropy of signals increases as a function of sen-
tence positions, given the estimates of these two
parameters. As shown in Figure 7, the predictions
from both models are qualitative similar except
that 1) when the decay rate in the power-law model
is low, out-of-context entropy of signals converges
more slowly than in the exponential model (Figure
7, right panel); 2) when the decay rate in the power
model is high, it almost converges as quickly as
the exponential model, and only minor differences
exist in their predictions (Figure 7, left panel).

51



●

●

4.0 4.5 5.0 5.5 6.0

4
6

8
10

Within−Context Entropy per Word

R
el

ev
an

ce
 D

ec
ay

 R
at

e
●

●

Danish
Dutch
English
French
Italian
Mandarin

Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish

Figure 5: The rate of relevance decay is corre-
lated with within-context entropy of signals in the
power-law model.

●

●

4.5 5.0 5.5 6.0

1.
0

1.
5

2.
0

Within−Context Entropy per Word

R
el

ev
an

ce
 D

ec
ay

 R
at

e

●

●

Danish
Dutch
English
French
Italian
Mandarin

Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish

Figure 6: The rate of relevance decay is correlated
with within-context entropy of signals in the expo-
nential model.

Because of the nonlinearity in our models, it
is not possible to report the results in an intuitive
manner as in “an increase in sentence position cor-
responds to an increase ofX bits of out-of-context
entropy per word”. Instead, we can analytically
solve for the derivative of the predicted out-of-
context entropy of signals with respect to sentence
position (Equation 4 and 8). This gives us:

rpower(k)
′ = r0k

−λ (11)

for the power-law model, showing the rate of in-
crease in predicted out-of-context entropy of sig-
nals is a monotonically decreasing power function,
and

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

● ●

●

●

●
●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

2 4 6 8 12

4
5

6
7

8

Sentence Position

O
ut

−
of

−
co

nt
ex

t E
nt

ro
py

 p
er

 W
or

d 
in

 D
ut

ch

Power:λ = 3.27
Exp:λ = 1.25 ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
● ●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

● ● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

2 4 6 8 12

4
5

6
7

8

Sentence Position

O
ut

−
of

−
co

nt
ex

t E
nt

ro
py

 p
er

 W
or

d 
in

 N
or

w
eg

ia
n

Power:λ = 2.4
Exp:λ = 0.86

Figure 7: Predicted out-of-context entropy of sig-
nals by the power-law model (solid) and the expo-
nential model (dashed) in Dutch and Norwegian,
with the actual distributions plotted on the back-
ground.

rexp(k)
′ =

r0λ

eλ − 1
(e−(k−1)λ) (12)

for the exponential model, showing the rate of in-
crease is a monotonically decreasing exponential
function. These mathematical properties indeed
match our observations in Figure 7.

4 Discussion and Future Work

The models introduced in this paper try to answer
this question: if the relevance of a contextual cue
for predicting an upcoming linguistic signal de-
cays over the course of a discourse, how much un-
certainty (entropy) is associated with each individ-
ual sentence position? We have shown under that
models that incorporate (power law or exponen-
tial) cue relevance decay in most cases describe
the relation of out-of-context entropy of signals
to sentence position are better accounted for than
previously suggested models.

We are continuing to investigate along this line.
Specifically, we are interested in finding the role of
semantic memory in affecting the relevance decay
of context. To test that, we plan to implement a
probabilistic topic model, in which topic continu-
ity between a preceding sentence and an upcom-
ing sentence is quantitatively measured. Thus, the
decay of contextual cues can be based on the esti-

52



mated semantic relatedness between sentences, in
addition to the abstract notion of rate as used in
this paper.

Finally, our relevance decay model can be ap-
plied to the domain of language processing as
well. For instance, the distance between a con-
textual cue and the target word may affect how
quickly a comprehender can process the informa-
tion conveyed by the word. We plan to address
these question in future work.

5 Conclusion

We have presented a new approach for examin-
ing the distribution of entropy of linguistic sig-
nals in discourses, showing that not only the out-
of-context entropy of signals increases sublinearly
with sentence position, but also the sublinear trend
is better explained by our nonlinear models than
by log-linear models of previous work. Our mod-
els are built on the assumption that the relevance
of a contextual cue for predicting a linguistic sig-
nal in the future decays with its distance to the tar-
get, and predict the relation of out-of-context en-
tropy of signals to sentence position in discourses.
These results indirectly lend support to the hypoth-
esis that speakers maintain a constant entropy of
signals across sentence positions in a discourse.

Acknowledgements

We wish to thank Meredith Brown, Alex Fine and
three anonymous reviewers for their helpful com-
ments on this paper. This work was supported by
NSF grant BCS-0845059 to TFJ.

References
John R. Anderson. 1995. Learning and Memory: An

integrated approach. John Wiley & Sons.

Philip R. Clarkson and Roni Rosenfeld. 1997. Sta-
tistical language modeling using the cmu-cambridge
toolkit. In Proceedings of ESCA Eurospeech.

Dimitry Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In ACL, pages 199–206.

Dimitry Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function
of the sentence number. in. In EMNLP, pages 65–
72.

Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317–324.

D. D. Lewis, Y. Yang, T. Rose, and F Li. 2004. Rcv1:
A new benchmark collection for text categorization
research. J Mach Learn Res, 5:361–397.

Steve Piantadosi and Edwards Gibson. 2008. Uniform
information density in discourse: a cross-corpus
analysis of syntactic and lexical predictability. In
CUNY.

Ting Qian and T. Florian Jaeger. 2009. Evidence
for efficient language production in chinese. In
CogSci09, pages 851–856.

Ting Qian and T. Florian Jaeger. under review. En-
tropy profiles in language: A cross-linguistic inves-
tigation.

C. E. Shannon. 1948. A mathematical theory of com-
munications. Bell Labs Tech J, 27(4):623–656.

J. T. Wixted and E. B. Ebbesen. 1991. On the form of
forgetting. Psychological Science, 2:409–415.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat Lang
Eng, 11:207–238.

G. K. Zipf. 1935. Psycho-Biology of Languages.
Houghton-Mifflin.

53


