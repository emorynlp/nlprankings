



















































Improving event prediction by representing script participants


Proceedings of NAACL-HLT 2016, pages 546–551,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Improving event prediction by representing script participants

Simon Ahrendt and Vera Demberg
Saarland University
66123 Saarbrücken

Germany
{simona,vera}@coli.uni-saarland.de

Abstract

Automatically learning script knowledge has
proved difficult, with previous work not or
just barely beating a most-frequent baseline.
Script knowledge is a type of world knowl-
edge which can however be useful for various
task in NLP and psycholinguistic modelling.
We here propose a model that includes partici-
pant information (i.e., knowledge about which
participants are relevant for a script) and show,
on the Dinners from Hell corpus as well as
the InScript corpus, that this knowledge helps
us to significantly improve prediction perfor-
mance on the narrative cloze task.

1 Introduction

Scripts represent knowledge about typical event se-
quences (Schank and Abelson, 1977), for exam-
ple the sequence of events happening when eating
at a restaurant. Script knowledge thereby includes
events like order, bring and eat as well as par-
ticipants of those events, e.g., menu, waiter, food,
guest. Script knowledge is a form of structured
world knowledge that is useful in NLP applications
for natural language understanding tasks (e.g., ambi-
guity resolution Rahman and Ng, 2012), as well as
for psycholinguistic models of human language pro-
cessing, which need to represent event knowledge
to model human expectations (Zwaan et al., 1995;
Schütz-Bosbach and Prinz, 2007) of upcoming ref-
erents and utterances.

One recent line of research has tried to learn
scripts in an unsupervised way from large text col-
lections. The core idea in Chambers and Jurafsky

(2008, 2009); Jans et al. (2012) is to use corefer-
ence chains to identify events involving the same en-
tity, with the intuition that these events would, if ob-
served in many texts, be likely to represent a proto-
typical event sequence. Rudinger et al. (2015) show
that this method is also applicable for learning spe-
cific targeted scripts from a domain-specific corpus,
shown at the example of “Dinners From Hell” sto-
ries and the restaurant script.

Pichotta and Mooney (2014) (P&M) have demon-
strated that using richer event representations con-
taining multiple arguments improves prediciton ac-
curacy on the narrative cloze task over the simpler
models by Chambers and Jurafsky (2008). While
they represent a script event as a pair of a verb and
a dependency (an example of an event chain would
be <call,obj>; <bring,subj>; <take,subj>), which
is problematic for weak verbs and verb ambigu-
ity, P&M represent events using a multi-argument
event representation, e.g., call(guest,waiter,*);
bring(waiter,menu,*); take(waiter,order,*).

This richer event representation however still has
some shortcomings. As the representation is based
on coreference chains, the model runs into diffi-
culties for entities that are in a chain of length
one. Entities in a chain are internally mapped
onto variables, but all single entities are mapped
onto a common category Other. This means
that all information about such referents is lost,
e.g. enjoy(customer, fish, ∗) can not be distin-
guished from enjoy(customer, silence, ∗) when
neither fish or silence have appeared before in the
text.

The coreference chains provide a good approxi-

546



mation for identifying events that involve the same
participants. But would performance improve sub-
stantially if we could represent event participants?
This specifically addresses the problem of unlinked
coreference chains (e.g., “food”, “it”, “steak”) not
appearing in the same coreference chain even though
they represent the same role within the script, and
the problem of mapping referents which are not part
of a chain onto a single “other” representation.

Kampmann et al. (2015) show that referring ex-
pressions in a script can be automatically catego-
rized in terms of the role they play within the script
by using coreference chains, as well as information
from WordNet (telling us e.g., that a steak is a kind
of food).

In this paper, we extend the existing approach by
P&M and demonstrate that explicitly labelling par-
ticipants (instead of using coreference chains) leads
to improved event prediction performance. We fur-
thermore provide a systematic evaluation of the ef-
fect of automatically-annotated coreference chains
vs. gold coreference chains, and automatically-
annotated script participants vs. gold participant an-
notation. We evaluate our approach on the Dinners
from Hell corpus (Rudinger et al., 2015), as well
as the newly available InScript corpus (Modi et al.,
2016).

Following earlier work, we evaluate the quality
of script models using the so-called narrative cloze
task, where the model has to predict a missing event
given surrounding events in the text.

2 Methods

2.1 Participant-labeled Events

In order to capture script-relevant information con-
veyed by arguments we represent texts as chains of
participant-labeled events (PLEs). A PLE is a
verb accompanied with the participant labels of its
arguments.

The general form of a PLE is
verb(psubj , pdobj , piobj), where psubj , pdobj and
piobj are the participant labels of the subject,
direct object and indirect object, respectively. For
example, in the sentence The waitress brought
us some water, the corresponding PLE would be
bring(waiter, drink, customer).

To automatically create PLEs from our training

data, we first extract syntactic relations between
verbs and their arguments as well as coreference in-
formation using Stanford CoreNLP (Manning et al.
(2014)). We then use the max-hypernym heuristic
described in Kampmann et al. (2015) to label the ar-
guments with participant roles. This approach as-
signs to token w the participant label with the high-
est hyponym-similarity score between the wordnet-
synsets associated with the label and one of the
synsets of any word present in the coreference chain
connected to w.

Where an argument slot of the event is not filled
syntactically or the argument is not a participant of
the script, a dummy participant O serves as a place-
holder to indicate the absence of a labeled argument.
Every extracted event that contains at least one par-
ticipant is included into the chain.

Knowledge about the participants provides a
much richer represenation of events. With this rep-
resentation we are able to generalize from a specific
word or entity to its overall role in the script. This
way the model can also learn from cases where mul-
tiple entities fill one participant role or where a par-
ticipant occurs in the text only once.

2.2 Predictive Model

Our script model is an adapted version of the bi-
gram model in Jans et al. (2012) with an extension of
the skip-gram option to skip all possible intervening
events. This means we rank an event e to belong to a
given ordered event sequence c at insertion point m
according to its score as defined by:

Score(e) =
m∑

k=0

logP (e|ck) +
n∑

k=m+1

logP (ck|e),

(1)
where ck denotes the kth event in the chain and the
conditional probabilties are estimated by skip-all bi-
gram counts:

P (e2|e1) = freq(e1, e2)∑
e′ freq(e1, e′)

(2)

with freq(e1, e2) being the the number of times
e1 has been encountered prior to e2 with an arbi-
trary number of events between them. This counting
method might be noisy in case of long documents or
a high number of unrelated events but it significantly

547



reduces data sparsity. In our specialized and rather
small data sets of between 87 and 133 stories per
scenario, sparsity is more of an issue than unrelat-
edness, so skip-all performs well, but other counting
techniques might prove more suitable for different
corpora.

In case our model assigns the same score to sev-
eral events, we backoff to the simple unigram model
described in section 4.2.

3 Data

The Dinners from Hell corpus (Rudinger et al.,
2015) contains stories from an internet blog about
terrible restaurant experiences. The corpus contains
143 stories (out of which 10 are reserved as a devel-
opment set), which all have to do with the script of
going to a restaurant. All non-copula verbs in this
corpus are annotated as to whether they are relevant
to the restaurant script.

The InScript corpus is a novel resource (Modi
et al., 2016), which contains a total of 910 short sto-
ries containing on average 12 sentences each. The
stories were collected via Mechanical Turk, instruct-
ing workers to describe a specific instance of an ac-
tivity, as if explaining it to a child. The corpus con-
tains 10 different scenario types, for which there are
about 90 stories each. This corpus also contains an-
notation for whether a verb is script-relevant, coref-
erence annotation and participant type information.

4 Evaluation

4.1 Narrative Cloze Task

The evaluation on the narrative cloze task (origi-
nally suggested by Chambers and Jurafsky, 2008)
expresses how well a model can predict a missing
event in a sequence of events. In order to make the
methods comparable, all predictions of our model
are mapped onto the encoding used by the simple
pair event model, e.g. <order,obj>, as follows:

We map an PLE v(psubj , pdobj , piobj) to a verb-
dependency pair < v, d > relating to a participant p
if p fills the PLE slot of dependency d. We subse-
quently define the score of a pair event as the max-
imum score of all PLEs which are mapped to this
pair event. When evaluating on pair events, we rank
events according to this redefined score.

4.2 Systems

Unigram Model This baseline is a simple model
that ranks any event (whether it is a participant-
labeled event or a pair event) by its overall frequency
in the training data. It was first used in Pichotta and
Mooney (2014) and has proven to be a very compet-
itive baseline on the task.

Verb-dependecy Pair Event Model This is a bi-
gram model over verb-dependency pair events as in-
troduced by Jans et al. (2012) and following the gen-
eral idea of Chambers and Jurafsky (2008). It has
been slightly modified to model not only subjects
and objects, but also indirect objects. We use the
setting Rudinger et al. (2015) has shown performs
best: Skip-all as a counting method, a count thresh-
old of 1, a document threshold of 5 and absolute dis-
counting. Note that their results on the same data set
differ from ours as we do not include syntactic rela-
tions other than ’subj’, ’dobj’ and ’iobj’ into training
and evaluation.

Pichotta and Mooney We re-implemented the ap-
proach by Pichotta and Mooney (2014) with the ex-
ception that we use v(esubj , edobj , eiobj) instead of
v(esubj , eobj , eprep) to represent events. That is, we
do not model prepositional arguments of an event
but discriminate between direct and indirect objects
of verbs.

Participant-based model Our model, as de-
scribed in section 2.2.

4.3 Automatic labels vs. gold standard

Automatic Coreference Chains We evaluate how
much the results are effected by the quality of the au-
tomatic coreference chains produced by the Stanford
Parser vs. annotated gold-chains on our data.

Automatic Participant Labelling We further-
more investigate to what extent our approach suffers
from imperfect participant labeling, i.e. how good
our model could have been if the labeling process
was 100%-accurate. Kampmann et al. (2015) report
a 0.59 micro F-score on the DinnersFromHell data,
leaving an arguably large room for improvement (al-
though they have a ceiling of 0.84 in terms of micro
F-score because of a mismatch between their partici-
pant label set and the gold-standard labels they eval-

548



uate on). To compare against a perfect participant
labeler, we use the participant annotation described
in the same paper.

4.4 Testing

Following Rudinger et al. (2015), we perform leave-
one-out testing at the document level, i.e., we use
133 folds for Dinners from Hell, and between 87
and 97 for InScript scenarios). We use the anno-
tation provided for the corpora to construct a test set
for every document as follows: Every verb that has
been annotated as script-relevant is regarded as a test
case if it takes an argument in any coreference chain
of at least length two, and the dependency between
the verb and the argument is either ’subj’, ’dobj’ or
’iobj’. The test then consists of inferring the verb
and dependency, given the model’s representation of
the remaining events in the document after this held-
out event is removed.

5 Results

Methods that encode events in a more complex way
have a higher risk of running into sparsity issues,
i.e. cases where the model has not encountered any
of the events in the current context. Table 1 shows
Recall@10 as a measure of prediction precision. We
can see that our model beats the baseline models by
a large margin on this measure. The core advan-
tage of our model are its participant representations,
which allow it to make more correct generalizations,
and generate less noisy predictions. This is also re-
flected in its lower coverage: our model does not
predict events when the context (including partici-
pant labels) has not been observed, while other mod-
els may predict based on non-matching participant
types, and hence generalize incorrectly.

We also report the Recall@10 with respect to
predicting the entire PLE in Table 1 (shown as

Model Coverage R@10 R@10full
this 0.757 0.41 0.18
P&M 0.957 0.26
Jans 0.809 0.25
MostFreq 0.936 0.27 0.13

Table 1: Performance for our model is reported with both au-
tomatic coreference chains and participant labels; R@10full

refers to the evaluation on PLE’s instead of pair events.

1 2 5 10 20 50 100 500

0.
0

0.
2

0.
4

0.
6

0.
8

1.
0

Performance on Narrative Cloze task

Recall@x (log scale)

R
at

e 
of

 ta
sk

s 
w

ith
 c

or
re

ct
 g

ue
ss

 in
 to

p 
x

●

●

●

●

●
●

●

●
●

●
●

● ● ● ● ● ●●●●

● this model
P&M 2014
Jans 2012
most freq baseline

Figure 1: The participant-based method outperforms the other
models and most frequent baseline. Performance shown for au-

tomatic chains and automatic participants.

R@10full), as we believe that inferring more struc-
tured events makes for a qualitative improvement on
script modelling, we here provide a baseline for later
work.

Figure 1 shows that the participant model suc-
ceeds in ranking the correct event high up more fre-
quently than the other models. If the model cannot
make any prediction due to coverage problems, it
has to guess from unigram frequencies. This is re-
flected in our model’s lower performance for Recall
in sets larger than the top 500. We would however
argue that performance at small Recall@x values is
much more relevant for most applications, as it may
matter little for most tasks where exactly in the low
ranks 500-1000 a model manages to rank the cor-
rect solution. For future work, this lack of cover-
age could be compensated for by backing off to the
P&M model.

Next, we’d like to see how the automatic versions
of the models compare to a setting where the models
have access to gold coreference chains and partici-
pants given by the annotation. Figure 2 shows that
using automatic or gold corefernce chains makes no
significant difference, but that there is quite a bit of
scope for performance improvements if one can im-
prove on the automatic participant labelling task.

Finally, we evaluated all models also on the 10
scenarios of the InScript dataset, to check whether
the good performance of our model generalizes also
to other datasets. We find that our model consis-

549



bath bicycle bus cake flight grocery haircut library train tree

0.0

0.1

0.2

0.3

0.4

0.5
re

ca
ll

Model
baseline R@1

baseline R@10

Jans et al. R@1

Jans et al. R@10

P&M R@1

P&M R@10

this model R@1

this model R@10

Model Performance for 10 Scenarios from InScript corpus

Figure 3: Performance of the different models with automatic participant labelling and automatic coreference chain annotation, on
the stories for InScript dataset, showing results by scenario.

1 2 5 10 20 50

0.
0

0.
2

0.
4

0.
6

Gold vs. automatic labels

Recall@x (log scale)

R
at

e 
of

 c
lo

ze
 ta

sk
s 

w
ith

 c
or

re
ct

 in
 to

p 
x

●

●

●

●

●

● ●

●
●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

this model gold
this model auto
P&M, 2014 gold
P&M, 2014 auto
Jans, 2012 gold
Jans, 2012 auto

Figure 2: Gold models employ gold coreference chains and
participants. There is little to no difference between using gold

or automatic corefernce chains, but improving on participant

labeling would help to further improve the model.

tently outperforms prior work also on this dataset,
in particular with respect to succeeding to rank the
correct event very high up on the list in the narra-
tive cloze task. Figure 3 shows the Recall@1 and
the Recall@10 measure separately for each of the
scenarios from the InScript corpus.

6 Discussion and Conclusions

We have shown that the participant-based model can
make much more accurate predictions in the narra-
tive cloze task than previous models which do not

have access to participant information; this even
holds for automatic participant labelling, where we
use a simple WordNet based method suggested in
Kampmann et al. (2015). Our evaluation showed
that the participant-based model substantially out-
performs the state-of-the-art on the narrative cloze
task, and that this performance holds for a set of nat-
uralistic texts from blogs as well as for a corpus of
narratives collected via crowd-sourcing. The present
results hence represent an important step towards au-
tomatic inferencing for domains where knowledge
of event sequences is relevant.

The automatic participant labeller takes as input a
set of script participants, which can for example be
acquired using the method of Regneri et al. (2010).
The current approach therefore represents a way of
combining the existing Mturk-based script acquisi-
tion methods by Regneri et al. (2010) with the unsu-
pervised methods suggested in Chambers and Juraf-
sky (2008); Jans et al. (2012); Pichotta and Mooney
(2014). Future work should further develop auto-
mated methods for participant labelling.

Acknowledgments

This research was funded by the German Research
Foundation (DFG) as part of SFB 1102 ‘Informa-
tion Density and Linguistic Encoding’ and the Clus-
ter of Excellence ‘Multimodal Computing and Inter-
action’ (EXC 284).

550



References

Chambers, N. and Jurafsky, D. (2008). Unsu-
pervised learning of narrative event chains. In
Proceedings of ACL-08: HLT, pages 789–797,
Columbus, Ohio. Association for Computational
Linguistics.

Chambers, N. and Jurafsky, D. (2009). Unsuper-
vised learning of narrative schemas and their par-
ticipants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 602–
610, Suntec, Singapore. Association for Compu-
tational Linguistics.

Jans, B., Bethard, S., Vulić, I., and Moens, M.-F.
(2012). Skip n-grams and ranking functions for
predicting script events. In Proceedings of the
13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
336–344, Avignon, France. Association for Com-
putational Linguistics.

Kampmann, A., Thater, S., and Pinkal, M. (2015).
A case-study of automatic participant labeling. In
Proceedings of the International Conference of
the German Society for Computational Linguis-
tics and Language Technology, pages 97–105.

Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J.,
Bethard, S. J., and McClosky, D. (2014). The
Stanford CoreNLP natural language processing
toolkit. In Proceedings of 52nd Annual Meeting
of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.

Modi, A., Anikina, T., Ostermann, S., and Pinkal,
M. (2016). Inscript: Narrative texts annotated
with script information. In Proceedings of the
10th edition of the Language Resources and Eval-
uation Conference.

Pichotta, K. and Mooney, R. (2014). Statistical
script learning with multi-argument events. In
Proceedings of the 14th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics, pages 220–229, Gothenburg,
Sweden. Association for Computational Linguis-
tics.

Rahman, A. and Ng, V. (2012). Resolving com-
plex cases of definite pronouns: the winograd

schema challenge. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning, pages 777–789. Associ-
ation for Computational Linguistics.

Regneri, M., Koller, A., and Pinkal, M. (2010).
Learning script knowledge with web experiments.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 979–988, Uppsala, Sweden. Association
for Computational Linguistics.

Rudinger, R., Demberg, V., Modi, A., Van Durme,
B., and Pinkal, M. (2015). Learning to predict
script events from domain-specific text. Lexi-
cal and Computational Semantics (* SEM 2015),
pages 205–210.

Schank, R. and Abelson, R. (1977). Scripts, plans,
goals and understanding: An inquiry into human
knowledge structures. Lawrence Erlbaum Asso-
ciates, Hillsdale, NJ.

Schütz-Bosbach, S. and Prinz, W. (2007). Prospec-
tive coding in event representation. Cognitive pro-
cessing, 8(2):93–102.

Zwaan, R. A., Langston, M. C., and Graesser, A. C.
(1995). The construction of situation models
in narrative comprehension: An event-indexing
model. Psychological science, pages 292–297.

551


