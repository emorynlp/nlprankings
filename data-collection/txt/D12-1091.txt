










































An Empirical Investigation of Statistical Significance in NLP


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 995–1005, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

An Empirical Investigation of Statistical Significance in NLP

Taylor Berg-Kirkpatrick David Burkett Dan Klein
Computer Science Division

University of California at Berkeley
{tberg, dburkett, klein}@cs.berkeley.edu

Abstract

We investigate two aspects of the empirical
behavior of paired significance tests for NLP
systems. First, when one system appears
to outperform another, how does significance
level relate in practice to the magnitude of the
gain, to the size of the test set, to the similar-
ity of the systems, and so on? Is it true that for
each task there is a gain which roughly implies
significance? We explore these issues across
a range of NLP tasks using both large collec-
tions of past systems’ outputs and variants of
single systems. Next, once significance lev-
els are computed, how well does the standard
i.i.d. notion of significance hold up in practical
settings where future distributions are neither
independent nor identically distributed, such
as across domains? We explore this question
using a range of test set variations for con-
stituency parsing.

1 Introduction

It is, or at least should be, nearly universal that NLP
evaluations include statistical significance tests to
validate metric gains. As important as significance
testing is, relatively few papers have empirically in-
vestigated its practical properties. Those that do
focus on single tasks (Koehn, 2004; Zhang et al.,
2004) or on the comparison of alternative hypothe-
sis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani
and Ney, 2004; Riezler and Maxwell, 2005).

In this paper, we investigate two aspects of the
empirical behavior of paired significance tests for
NLP systems. For example, all else equal, larger
metric gains will tend to be more significant. How-
ever, what does this relationship look like and how
reliable is it? What should be made of the conven-
tional wisdom that often springs up that a certain
metric gain is roughly the point of significance for
a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU

in machine translation)? We show that, with heavy
caveats, there are such thresholds, though we also
discuss the hazards in their use. In particular, many
other factors contribute to the significance level, and
we investigate several of them. For example, what
is the effect of the similarity between the two sys-
tems? Here, we show that more similar systems tend
to achieve significance with smaller metric gains, re-
flecting the fact that their outputs are more corre-
lated. What about the size of the test set? For ex-
ample, in designing a shared task it is important to
know how large the test set must be in order for sig-
nificance tests to be sensitive to small gains in the
performance metric. Here, we show that test size
plays the largest role in determining discrimination
ability, but that we get diminishing returns. For ex-
ample, doubling the test size will not obviate the
need for significance testing.

In order for our results to be meaningful, we must
have access to the outputs of many of NLP sys-
tems. Public competitions, such as the well-known
CoNLL shared tasks, provide one natural way to ob-
tain a variety of system outputs on the same test
set. However, for most NLP tasks, obtaining out-
puts from a large variety of systems is not feasible.
Thus, in the course of our investigations, we propose
a very simple method for automatically generating
arbitrary numbers of comparable system outputs and
we then validate the trends revealed by our synthetic
method against data from public competitions. This
methodology itself could be of value in, for exam-
ple, the design of new shared tasks.

Finally, we consider a related and perhaps even
more important question that can only be answered
empirically: to what extent is statistical significance
on a test corpus predictive of performance on other
test corpora, in-domain or otherwise? Focusing on
constituency parsing, we investigate the relationship
between significance levels and actual performance

995



on data from outside the test set. We show that when
the test set is (artificially) drawn i.i.d. from the same
distribution that generates new data, then signifi-
cance levels are remarkably well-calibrated. How-
ever, as the domain of the new data diverges from
that of the test set, the predictive ability of signifi-
cance level drops off dramatically.

2 Statistical Significance Testing in NLP

First, we review notation and standard practice in
significance testing to set up our empirical investi-
gation.

2.1 Hypothesis Tests
When comparing a new system A to a baseline sys-
tem B, we want to know if A is better than B on
some large population of data. Imagine that we sam-
ple a small test set x = x1, . . . , xn on which A
beats B by δ(x). Hypothesis testing guards against
the case where A’s victory over B was an unlikely
event, due merely to chance. We would therefore
like to know how likely it would be that a new, in-
dependent test set x′ would show a similar victory
for A assuming that A is no better than B on the
population as a whole; this assumption is the null
hypothesis, denoted H0.

Hypothesis testing consists of attempting to esti-
mate this likelihood, written p(δ(X) > δ(x)|H0),
where X is a random variable over possible test sets
of size n that we could have drawn, and δ(x) is a
constant, the metric gain we actually observed. Tra-
ditionally, if p(δ(X) > δ(x)|H0) < 0.05, we say
that the observed value of δ(x) is sufficiently un-
likely that we should reject H0 (i.e. accept that A’s
victory was real and not just a random fluke). We
refer to p(δ(X) > δ(x)|H0) as p-value(x).

In most cases p-value(x) is not easily computable
and must be approximated. The type of approxi-
mation depends on the particular hypothesis testing
method. Various methods have been used in the NLP
community (Gillick and Cox, 1989; Yeh, 2000; Rie-
zler and Maxwell, 2005). We use the paired boot-
strap1 (Efron and Tibshirani, 1993) because it is one

1Riezler and Maxwell (2005) argue the benefits of approx-
imate randomization testing, introduced by Noreen (1989).
However, this method is ill-suited to the type of hypothesis we
are testing. Our null hypothesis does not condition on the test
data, and therefore the bootstrap is a better choice.

1. Draw b bootstrap samples x(i) of size n by
sampling with replacement from x.

2. Initialize s = 0.
3. For each x(i) increment s if δ(x(i)) > 2δ(x).
4. Estimate p-value(x) ≈ s

b

Figure 1: The bootstrap procedure. In all of our experiments
we use b = 106, which is more than sufficient for the bootstrap
estimate of p-value(x) to stabilize.

of the most widely used (Och, 2003; Bisani and Ney,
2004; Zhang et al., 2004; Koehn, 2004), and be-
cause it can be easily applied to any performance
metric, even complex metrics like F1-measure or
BLEU (Papineni et al., 2002). Note that we could
perform the experiments described in this paper us-
ing another method, such as the paired Student’s t-
test. To the extent that the assumptions of the t-test
are met, it is likely that the results would be very
similar to those we present here.

2.2 The Bootstrap

The bootstrap estimates p-value(x) though a com-
bination of simulation and approximation, drawing
many simulated test sets x(i) and counting how often
A sees an accidental advantage of δ(x) or greater.
How can we get sample test sets x(i)? We lack the
ability to actually draw new test sets from the un-
derlying population because all we have is our data
x. The bootstrap therefore draws each x(i) from x
itself, sampling n items from x with replacement;
these new test sets are called bootstrap samples.

Naively, it might seem like we would then check
how often A beats B by more than δ(x) on x(i).
However, there’s something seriously wrong with
these x(i) as far as the null hypothesis is concerned:
the x(i) were sampled from x, and so their average
δ(x(i)) won’t be zero like the null hypothesis de-
mands; the average will instead be around δ(x). If
we ask how many of these x(i) have A winning by
δ(x), about half of them will. The solution is a re-
centering of the mean – we want to know how often
A does more than δ(x) better than expected. We ex-
pect it to beat B by δ(x). Therefore, we count up
how many of the x(i) have A beating B by at least
2δ(x).2 The pseudocode is shown in Figure 1.

2Note that many authors have used a variant where the event
tallied on the x(i) is whether δ(x(i)) < 0, rather than δ(x(i)) >
2δ(x). If the mean of δ(x(i)) is δ(x), and if the distribution of
δ(x(i)) is symmetric, then these two versions will be equivalent.

996



As mentioned, a major benefit of the bootstrap is
that any evaluation metric can be used to compute
δ(x).3 We run the bootstrap using several metrics:
F1-measure for constituency parsing, unlabeled de-
pendency accuracy for dependency parsing, align-
ment error rate (AER) for word alignment, ROUGE
score (Lin, 2004) for summarization, and BLEU
score for machine translation.4 We report all met-
rics as percentages.

3 Experiments

Our first goal is to explore the relationship be-
tween metric gain, δ(x), and statistical significance,
p-value(x), for a range of NLP tasks. In order to say
anything meaningful, we will need to see both δ(x)
and p-value(x) for many pairs of systems.

3.1 Natural Comparisons

Ideally, for a given task and test set we could obtain
outputs from all systems that have been evaluated
in published work. For each pair of these systems
we could run a comparison and compute both δ(x)
and p-value(x). While obtaining such data is not
generally feasible, for several tasks there are pub-
lic competitions to which systems are submitted by
many researchers. Some of these competitions make
system outputs publicly available. We obtained sys-
tem outputs from the TAC 2008 workshop on auto-
matic summarization (Dang and Owczarzak, 2008),
the CoNLL 2007 shared task on dependency parsing
(Nivre et al., 2007), and the WMT 2010 workshop
on machine translation (Callison-Burch et al., 2010).

For cases where the metric linearly decomposes over sentences,
the mean of δ(x(i)) is δ(x). By the central limit theorem, the
distribution will be symmetric for large test sets; for small test
sets it may not.

3Note that the bootstrap procedure given only approximates
the true significance level, with multiple sources of approxima-
tion error. One is the error introduced from using a finite num-
ber of bootstrap samples. Another comes from the assumption
that the bootstrap samples reflect the underlying population dis-
tribution. A third is the assumption that the mean bootstrap gain
is the test gain (which could be further corrected for if the metric
is sufficiently ill-behaved).

4To save time, we can compute δ(x) for each bootstrap sam-
ple without having to rerun the evaluation metric. For our met-
rics, sufficient statistics can be recorded for each sentence and
then sampled along with the sentences when constructing each
x(i) (e.g. size of gold, size of guess, and number correct are suf-
ficient for F1). This makes the bootstrap very fast in practice.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2

1
  

- 
 p

-v
al

u
e

ROUGE

Different research groups
Same research group

Figure 2: TAC 2008 Summarization: Confidence vs.
ROUGE improvement on TAC 2008 test set for comparisons
between all pairs of the 58 participating systems at TAC 2008.
Comparisons between systems entered by the same research
group and comparisons between systems entered by different
research groups are shown separately.

3.1.1 TAC 2008 Summarization
In our first experiment, we use the outputs of the

58 systems that participated in the TAC 2008 work-
shop on automatic summarization. For each possi-
ble pairing, we compute δ(x) and p-value(x) on the
non-update portion of the TAC 2008 test set (we or-
der each pair so that the gain, δ(x), is always pos-
itive).5 For this task, test instances correspond to
document collections. The test set consists of 48
document collections, each with a human produced
summary. Figure 2 plots the ROUGE gain against
1 − p-value, which we refer to as confidence. Each
point on the graph corresponds to an individual pair
of systems.

As expected, larger gains in ROUGE correspond
to higher confidences. The curved shape of the plot
is interesting. It suggests that relatively quickly we
reach ROUGE gains for which, in practice, signif-
icance tests will most likely be positive. We might
expect that systems whose outputs are highly corre-
lated will achieve higher confidence at lower met-
ric gains. To test this hypothesis, in Figure 2 we

5In order to run bootstraps between all pairs of systems
quickly, we reuse a random sample counts matrix between boot-
strap runs. As a result, we no longer need to perform quadrat-
ically many corpus resamplings. The speed-up from this ap-
proach is enormous, but one undesirable effect is that the boot-
strap estimation noise between different runs is correlated. As a
remedy, we set b so large that the correlated noise is not visible
in plots.

997



 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2  2.5  3  3.5  4

1
  

- 
 p

-v
al

u
e

Unlabeled Acc.

Different research groups
Same research group

Figure 3: CoNLL 2007 Dependency parsing: Confidence vs.
unlabeled dependency accuracy improvement on the Chinese
CoNLL 2007 test set for comparisons between all pairs of the
21 participating systems in CoNLL 2007 shared task. Com-
parisons between systems entered by the same research group
and comparisons between systems entered by different research
groups are shown separately.

separately show the comparisons between systems
entered by the same research group and compar-
isons between systems entered by different research
groups, with the expectation that systems entered by
the same group are likely to have more correlated
outputs. Many of the comparisons between systems
submitted by the same group are offset from the
main curve. It appears that they do achieve higher
confidences at lower metric gains.

Given the huge number of system comparisons in
Figure 2, one obvious question to ask is whether
we can take the results of all these statistical sig-
nificance tests and estimate a ROUGE improvement
threshold that predicts when future statistical sig-
nificance tests will probably be significant at the
p-value(x) < 0.05 level. For example, let’s say we
take all the comparisons with p-value between 0.04
and 0.06 (47 comparisons in all in this case). Each
of these comparisons has an associated metric gain,
and by taking, say, the 95th percentile of these met-
ric gains, we get a potentially useful threshold. In
this case, the computed threshold is 1.10 ROUGE.

What does this threshold mean? Well, based on
the way we computed it, it suggests that if somebody
reports a ROUGE increase of around 1.10 on the ex-
act same test set, there is a pretty good chance that a
statistical significance test would show significance
at the p-value(x) < 0.05 level. After all, 95% of

the borderline significant differences that we’ve al-
ready seen showed an increase of even less than 1.10
ROUGE. If we’re evaluating past work, or are in
some other setting where system outputs just aren’t
available, the threshold could guide our interpreta-
tion of reports containing only summary scores.

That being said, it is important that we don’t over-
interpret the meaning of the 1.10 ROUGE threshold.
We have already seen that pairs of systems submit-
ted by the same research group and by different re-
search groups follow different trends, and we will
soon see more evidence demonstrating the impor-
tance of system correlation in determining the rela-
tionship between metric gain and confidence. Addi-
tionally, in Section 4, we will see that properties of
the test corpus have a large effect on the trend. There
are many factors are at work, and so, of course, met-
ric gain alone will not fully determine the outcome
of a paired significance test.

3.1.2 CoNLL 2007 Dependency Parsing
Next, we run an experiment for dependency pars-

ing. We use the outputs of the 21 systems that par-
ticipated in the CoNLL 2007 shared task on depen-
dency parsing. In Figure 3, we plot, for all pairs,
the gain in unlabeled dependency accuracy against
confidence on the CoNLL 2007 Chinese test set,
which consists of 690 sentences and parses. We
again separate comparisons between systems sub-
mitted by the same research group and those submit-
ted by different groups, although for this task there
were fewer cases of multiple submission. The re-
sults resemble the plot for summarization; we again
see a curve-shaped trend, and comparisons between
systems from the same group (few that they are)
achieve higher confidences at lower metric gains.

3.1.3 WMT 2010 Machine Translation
Our final task for which system outputs are pub-

licly available is machine translation. We run an ex-
periment using the outputs of the 31 systems par-
ticipating in WMT 2010 on the system combination
portion of the German-English WMT 2010 news test
set, which consists of 2,034 German sentences and
English translations. We again run comparisons for
pairs of participating systems. We plot gain in test
BLEU score against confidence in Figure 4. In this
experiment there is an additional class of compar-

998



 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.2  0.4  0.6  0.8  1

1
  

- 
 p

-v
al

u
e

BLEU

Different research groups
Same research group
System combination

Figure 4: WMT 2010 Machine translation: Confidence vs.
BLEU improvement on the system combination portion of the
German-English WMT 2010 news test set for comparisons be-
tween pairs of the 31 participating systems at WMT 2010.
Comparisons between systems entered by the same research
group, comparisons between systems entered by different re-
search groups, and comparisons between system combination
entries are shown separately.

isons that are likely to have specially correlated sys-
tems: 13 of the submitted systems are system com-
binations, and each take into account the same set
of proposed translations. We separate comparisons
into three sets: comparisons between non-combined
systems entered by different research groups, com-
parisons between non-combined systems entered by
the same research group, and comparisons between
system-combinations.

We see the same curve-shaped trend we saw for
summarization and dependency parsing. Differ-
ent group comparisons, same group comparisons,
and system combination comparisons form distinct
curves. This indicates, again, that comparisons be-
tween systems that are expected to be specially cor-
related achieve high confidence at lower metric gain
levels.

3.2 Synthetic Comparisons

So far, we have seen a clear empirical effect, but, be-
cause of the limited availability of system outputs,
we have only considered a few tasks. We now pro-
pose a simple method that captures the shape of the
effect, and use it to extend our analysis.

3.2.1 Training Set Resampling
Another way of obtaining many different sys-

tems’ outputs is to obtain implementations of a

handful of systems, and then vary some aspect of
the training procedure in order to produce many dif-
ferent systems from each implementation. Koehn
(2004) uses this sort of amplification; he uses a sin-
gle machine translation implementation, and then
trains it from different source languages. We take
a slightly different approach. For each task we pick
some fixed training set. Then we generate resampled
training sets by sampling sentences with replace-
ment from the original. In this way, we can gen-
erate as many new training sets as we like, each of
which is similar to the original, but with some vari-
ation. For each base implementation, we train a new
system on each resampled training set. This results
in slightly tweaked trained systems, and is intended
to very roughly approximate the variance introduced
by incremental system changes during research. We
validate this method by comparing plots obtained by
the synthetic approach with plots obtained from nat-
ural comparisons.

We expect that each new system will be differ-
ent, but that systems originating from the same base
model will be highly correlated. This provides a use-
ful division of comparisons: those between systems
built with the same model, and those between sys-
tems built with different models. The first class can
be used to approximate comparisons of systems that
are expected to be specially correlated, and the latter
for comparisons of systems that are not.

3.2.2 Dependency Parsing
We use three base models for dependency parsing:

MST parser (McDonald et al., 2005), Maltparser
(Nivre et al., 2006), and the ensemble parser of Sur-
deanu and Manning (2010). We use the CoNLL
2007 Chinese training set, which consists of 57K
sentences. We resample 5 training sets of 57K sen-
tences, 10 training sets of 28K sentences, and 10
training sets of 14K sentences. Together, this yields
a total of 75 system outputs on the CoNLL 2007
Chinese test set, 25 systems for each base model
type. The score ranges of all the base models over-
lap. This ensures that for each pair of model types
we will be able to see comparisons where the metric
gains are small. The results of the pairwise compar-
isons of all 75 system outputs are shown in Figure
5, along with the results of the CoNLL 2007 shared
task system comparisons from Figure 3.

999



 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2  2.5  3  3.5  4

1
  

- 
 p

-v
al

u
e

Unlabeled Acc.

Different model types
Same model type

CoNLL 2007 comparisons

Figure 5: Dependency parsing: Confidence vs. unlabeled de-
pendency accuracy improvement on the Chinese CoNLL 2007
test set for comparisons between all pairs of systems gener-
ated by using resampled training sets to train either MST parser,
Maltparser, or the ensemble parser. Comparisons between sys-
tems generated using the same base model type and compar-
isons between systems generated using different base model
types are shown separately. The CoNLL 2007 shared task com-
parisons from Figure 3 are also shown.

The overlay of the natural comparisons suggests
that the synthetic approach reasonably models the
relationship between metric gain and confidence.
Additionally, the different model type and same
model type comparisons exhibit the behavior we
would expect, matching the curves corresponding to
comparisons between specially correlated systems
and standard comparisons respectively.

Since our synthetic approach yields a large num-
ber of system outputs, we can use the procedure
described in Section 3.1.1 to compute the thresh-
old above which the metric gain is probably signifi-
cant. For comparisons between systems of the same
model type, the threshold is 1.20 unlabeled depen-
dency accuracy. For comparisons between systems
of different model types, the threshold is 1.51 un-
labeled dependency accuracy. These results indi-
cate that the similarity of the systems being com-
pared is an important factor. As mentioned, rules-
of-thumb derived from such thresholds cannot be
applied blindly, but, in special cases where two sys-
tems are known to be correlated, the former thresh-
old should be preferred over the latter. For example,
during development most comparisons are made be-
tween incremental variants of the same system. If
adding a feature to a supervised parser increases un-

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.2  0.4  0.6  0.8  1

1
  

- 
 p

-v
al

u
e

BLEU

Different model types
Same model type

WMT 2010 comparisons

Figure 6: Machine translation: Confidence vs. BLEU im-
provement on the system combination portion of the German-
English WMT 2010 news test set for comparisons between all
pairs of systems generated by using resampled training sets to
train either Moses or Joshua. Comparisons between systems
generated using the same base model type and comparisons be-
tween systems generated using different base model types are
shown separately. The WMT 2010 workshop comparisons from
Figure 4 are also shown.

labeled accuracy by 1.3, it is useful to be able to
quickly estimate that the improvement is probably
significant. This still isn’t the full story; we will
soon see that properties of the test set also play a
major role. But first, we carry our analysis to sev-
eral more tasks.

3.2.3 Machine Translation
Our two base models for machine translation

are Moses (Koehn et al., 2007) and Joshua (Li et
al., 2009). We use 1.4M sentence pairs from the
German-English portion of the WMT-provided Eu-
roparl (Koehn, 2005) and news commentary corpora
as the original training set. We resample 75 training
sets, 20 of 1.4M sentence pairs, 29 of 350K sentence
pairs, and 26 of 88K sentence pairs. This yields a
total of 150 system outputs on the system combi-
nation portion of the German-English WMT 2010
news test set. The results of the pairwise compar-
isons of all 150 system outputs are shown in Figure
6, along with the results of the WMT 2010 workshop
system comparisons from Figure 4.

The natural comparisons from the WMT 2010
workshop align well with the comparisons between
synthetically varied models. Again, the different
model type and same model type comparisons form
distinct curves. For comparisons between systems

1000



 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2

1
  

- 
 p

-v
al

u
e

AER

Different model types
Same model type

Figure 7: Word alignment: Confidence vs. AER improve-
ment on the Hansard test set for comparisons between all pairs
of systems generated by using resampled training sets to train
either the ITG aligner, the joint HMM aligner, or GIZA++.
Comparisons between systems generated using the same base
model type and comparisons between systems generated using
different base model types are shown separately.

of the same model type the computed p-value <
0.05 threshold is 0.28 BLEU. For comparisons be-
tween systems of different model types the threshold
is 0.37 BLEU.

3.2.4 Word Alignment
Now that we have validated our simple model of

system variation on two tasks, we go on to gen-
erate plots for tasks that do not have competitions
with publicly available system outputs. The first
task is English-French word alignment, where we
use three base models: the ITG aligner of Haghighi
et al. (2009), the joint HMM aligner of Liang et al.
(2006), and GIZA++ (Och and Ney, 2003). The last
two aligners are unsupervised, while the first is su-
pervised. We train the unsupervised word aligners
using the 1.1M sentence pair Hansard training cor-
pus, resampling 20 training sets of the same size.6

Following Haghighi et al. (2009), we train the super-
vised ITG aligner using the first 337 sentence pairs
of the hand-aligned Hansard test set; again, we re-
sample 20 training sets of the same size as the origi-
nal data. We test on the remaining 100 hand-aligned
sentence pairs from the Hansard test set.

Unlike previous plots, the points corresponding
to comparisons between systems with different base

6GIZA++ failed to produce reasonable output when trained
with some of these training sets, so there are fewer than 20
GIZA++ systems in our comparisons.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2

1
  

- 
 p

-v
al

u
e

F1

Different model types
Same model type

Figure 8: Constituency parsing: Confidence vs. F1 improve-
ment on section 23 of the WSJ corpus for comparisons between
all pairs of systems generated by using resampled training sets
to train either the Berkeley parser, the Stanford parser, or the
Collins parser. Comparisons between systems generated us-
ing the same base model type and comparisons between sys-
tems generated using different base model types are shown sep-
arately.

model types form two distinct curves. It turns out
that the upper curve consists only of comparisons
between ITG and HMM aligners. This is likely due
to the fact that the ITG aligner uses posteriors from
the HMM aligner for some of its features, so the
two models are particularly correlated. Overall, the
spread of this plot is larger than previous ones. This
may be due to the small size of the test set, or possi-
bly some additional variance introduced by unsuper-
vised training. For comparisons between systems of
the same model type the p-value < 0.05 threshold
is 0.50 AER. For comparisons between systems of
different model types the threshold is 1.12 AER.

3.2.5 Constituency Parsing
Finally, before we move on to further types of

analysis, we run an experiment for the task of con-
stituency parsing. We use three base models: the
Berkeley parser (Petrov et al., 2006), the Stanford
parser (Klein and Manning, 2003), and Dan Bikel’s
implementation (Bikel, 2004) of the Collins parser
(Collins, 1999). We use sections 2-21 of the WSJ
corpus (Marcus et al., 1993), which consists of 38K
sentences and parses, as a training set. We resample
10 training sets of size 38K, 10 of size 19K, and 10
of size 9K, and use these to train systems. We test
on section 23. The results are shown in Figure 8.

For comparisons between systems of the same

1001



model type, the p-value < 0.05 threshold is 0.47
F1. For comparisons between systems of different
model types the threshold is 0.57 F1.

4 Properties of the Test Corpus

For five tasks, we have seen a trend relating met-
ric gain and confidence, and we have seen that the
level of correlation between the systems being com-
pared affects the location of the curve. Next, we
look at how the size and domain of the test set play
a role, and, finally, how significance level predicts
performance on held out data. In this section, we
carry out experiments for both machine translation
and constituency parsing, but mainly focus on the
latter because of the availability of large test corpora
that span more than one domain: the Brown corpus
and the held out portions of the WSJ corpus.

4.1 Varying the Size

Figure 9 plots comparisons for machine translation
on variously sized initial segments of the WMT
2010 news test set. Similarly, Figure 10 plots com-
parisons for constituency parsing on initial segments
of the Brown corpus. As might be expected, the
size of the test corpus has a large effect. For both
machine translation and constituency parsing, the
larger the corpus size, the lower the threshold for
p-value < 0.05 and the smaller the spread of the
plot. At one extreme, the entire Brown corpus,
which consists of approximately 24K sentences, has
a threshold of 0.22 F1, while at the other extreme,
the first 100 sentences of the Brown corpus have a
threshold of 3.00 F1. Notice that we see diminishing
returns as we increase the size of the test set. This
phenomenon follows the general shape of the cen-
tral limit theorem, which predicts that variances of
observed metric gains will shrink according to the
square root of the test size. Even using the entire
Brown corpus as a test set there is a small range
where the result of a paired significance test was not
completely determined by metric gain.

It is interesting to note that for a fixed test size,
the domain has only a small effect on the shape of
the curve. Figure 11 plots comparisons for a fixed
test size, but with various test corpora.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.2  0.4  0.6  0.8  1

1
  

- 
 p

-v
al

u
e

BLEU

100 sent.
200 sent.
400 sent.
800 sent.

1600 sent.

Figure 9: Machine translation; varying test size: Confidence
vs. BLEU improvement on portions of the German-English
WMT 2010 news test set.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2

1
  

- 
 p

-v
al

u
e

F1

100 sent.
200 sent.
400 sent.
800 sent.

1600 sent.
3200 sent.

Entire Brown corpus

Figure 10: Constituency parsing; varying test size: Con-
fidence vs. F1 improvement on portions of the Brown corpus.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

 0  0.5  1  1.5  2

1
  

- 
 p

-v
al

u
e

F1

WSJ sec 22
WSJ sec 23
WSJ sec 24

Brown corpus

Figure 11: Constituency parsing; varying domain: Confi-
dence vs. F1 improvement on the first 1,600 sentences of sec-
tions 22, 23, and 24 of the WSJ corpus, and the Brown corpus.

1002



4.2 Empirical Calibration across Domains

Now that we have a way of generating outputs for
thousands of pairs of systems, we can check empir-
ically the practical reliability of significance testing.
Recall that the bootstrap p-value(x) is an approxi-
mation to p(δ(X) > δ(x)|H0). However, we often
really want to determine the probability that the new
system is better than the baseline on the underlying
test distribution or even the distribution from another
domain. There is no reason a priori to expect these
numbers to coincide.

In our next experiment, we treat the entire Brown
corpus, which consists of 24K sentences, as the true
population of English sentences. For each system
generated in the way described in Section 3.2.5 we
compute F1 on all of Brown. Since we are treat-
ing the Brown corpus as the actual population of En-
glish sentences, for each pair of parsers we can say
that the sign of the F1 difference indicates which is
the truly better system. Now, we repeatedly resam-
ple small test sets from Brown, each consisting of
1,600 sentences, drawn by sampling sentences with
replacement. For each pair of systems, and for each
resampled test set, we compute p-value(x) using the
bootstrap. Out of the 4K bootstraps computed in this
way, 942 had p-value between 0.04 and 0.06, 869
of which agreed with the sign of the F1 difference
we saw on the entire Brown corpus. Thus, 92% of
the significance tests with p-value in a tight range
around 0.05 correctly identified the better system.

This result is encouraging. It suggests that sta-
tistical significance computed using the bootstrap is
reasonably well calibrated. However, test sets are
almost never drawn i.i.d. from the distribution of in-
stances the system will encounter in practical use.
Thus, we also wish to compute how calibration de-
grades as the domain of the test set changes. In an-
other experiment, we look at how significance near
p-value = 0.05 on section 23 of the WSJ corpus
predicts performance on sections 22 and 24 and the
Brown corpus. This time, for each pair of generated
systems we run a bootstrap on section 23. Out of
all these bootstraps, 58 system pairs had p-value be-
tween 0.04 and 0.06. Of these, only 83% had the
same sign of F1 difference on section 23 as they did
on section 22, 71% the had the same sign on sec-
tion 23 as on section 24, and 48% the same sign on

Sec. 23 p-value % Sys. A > Sys. BSec. 22 Sec. 24 Brown
0.00125 - 0.0025 97% 95% 73%
0.0025 - 0.005 92% 92% 60%
0.005 - 0.01 92% 85% 56%
0.01 - 0.02 88% 92% 54%
0.02 - 0.04 87% 78% 51%
0.04 - 0.08 83% 74% 48%

Table 1: Empirical calibration: p-value on section 23 of the
WSJ corpus vs. fraction of comparisons where system A beats
system B on section 22, section 24, and the Brown corpus. Note
that system pairs are ordered so that A always outperforms B on
section 23.

section 23 as on the Brown corpus. This indicates
that reliability degrades as we switch the domain. In
the extreme, achieving a p-value near 0.05 on sec-
tion 23 provides no information about performance
on the Brown corpus.

If we intend to use our system on out-of-domain
data, these results are somewhat discouraging. How
low does p-value(x) have to get before we start get-
ting good information about out-of-domain perfor-
mance? We try to answer this question for this par-
ticular parsing task by running the same domain cal-
ibration experiment for several different ranges of
p-value. The results are shown in Table 1. From
these results, it appears that for constituency pars-
ing, when testing on section 23, a p-value level be-
low 0.00125 is required to reasonably predict perfor-
mance on the Brown corpus.

It should be considered a good practice to include
statistical significance testing results with empiri-
cal evaluations. The bootstrap in particular is easy
to run and makes relatively few assumptions about
the task or evaluation metric. However, we have
demonstrated some limitations of statistical signifi-
cance testing for NLP. In particular, while statistical
significance is usually a minimum necessary condi-
tion to demonstrate that a performance difference is
real, it’s also important to consider the relationship
between test set performance and the actual goals
of the systems being tested, especially if the system
will eventually be used on data from a different do-
main than the test set used for evaluation.

5 Conclusion
We have demonstrated trends relating several im-
portant factors to significance level, which include

1003



both properties of the systems being compared and
properties of the test corpus, and have presented a
simple approach to approximating the response of
these factors for tasks where large numbers of sys-
tem outputs are not available. Our results reveal
that the relationship between metric gain and sta-
tistical significance is complex, and therefore sim-
ple thresholds are not a replacement for significance
tests. Indeed, we strongly advocate the use of statis-
tical significance testing to validate metric gains in
NLP, but also note that informal rules-of-thumb do
arise in popular discussion and that, for some set-
tings when previous systems are unavailable, these
empirical results can supplement less sensitive un-
paired tests (e.g. bar-overlaps-point test) in evalua-
tion of progress. Finally, even formal testing has its
limits. We provide cautionary evidence to this ef-
fect, showing that the information provided by a test
quickly degrades as the target corpus shifts domain.

Acknowledgements

This work was partially supported by NSF fellow-
ships to the first and second authors and by the NSF
under grant 0643742.

References

D.M. Bikel. 2004. Intricacies of collins’ parsing model.
Computational Linguistics.

M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in asr performance evaluation. In
Proc. of ICASSP.

C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proc. of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR.

M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.

H.T. Dang and K. Owczarzak. 2008. Overview of the
tac 2008 update summarization task. In Proc. of Text
Analysis Conference.

B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall/CRC.

L. Gillick and S.J. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proc. of ICASSP.

A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL.

D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.

P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.

P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit.

Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W.N.G. Thornton,
J. Weese, and O.F. Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proc. of the Fourth Workshop on Statistical Machine
Translation.

P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.

C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proc. of the Workshop on Text
Summarization.

M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
The penn treebank. Computational linguistics.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajič. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of EMNLP.

J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.

J. Nivre, J. Hall, S. Kübler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.

E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley, New
York.

F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics.

F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.

K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.

S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.

1004



S. Riezler and J.T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt.
In Proc. of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.

M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Proc.
of NAACL.

A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proc. of ACL.

Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system. In Proc. of LREC.

1005


