



















































HMM Word-to-Phrase Alignment with Dependency Constraints


Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109,
COLING 2010, Beijing, August 2010.

HMM Word-to-Phrase Alignment with Dependency Constraints

Yanjun Ma Andy Way
Centre for Next Generation Localisation

School of Computing
Dublin City University

{yma,away}@computing.dcu.ie

Abstract

In this paper, we extend the HMM word-
to-phrase alignment model with syntac-
tic dependency constraints. The syn-
tactic dependencies between multiple
words in one language are introduced
into the model in a bid to produce co-
herent alignments. Our experimental re-
sults on a variety of Chinese–English
data show that our syntactically con-
strained model can lead to as much as
a 3.24% relative improvement in BLEU
score over current HMM word-to-phrase
alignment models on a Phrase-Based
Statistical Machine Translation system
when the training data is small, and
a comparable performance compared to
IBM model 4 on a Hiero-style system
with larger training data. An intrin-
sic alignment quality evaluation shows
that our alignment model with depen-
dency constraints leads to improvements
in both precision (by 1.74% relative) and
recall (by 1.75% relative) over the model
without dependency information.

1 Introduction

Generative word alignment models including
IBM models (Brown et al., 1993) and HMM
word alignment models (Vogel et al., 1996) have
been widely used in various types of Statisti-
cal Machine Translation (SMT) systems. This
widespread use can be attributed to their robust-
ness and high performance particularly on large-
scale translation tasks. However, the quality

of the alignment yielded from these models is
still far from satisfactory even with significant
amounts of training data; this is particularly true
for radically different languages such as Chinese
and English.

The weakness of most generative models of-
ten lies in the incapability of addressing one to
many (1-to-n), many to one (n-to-1) and many
to many (m-to-n) alignments. Some research di-
rectly addresses m-to-n alignment with phrase
alignment models (Marcu and Wong, 2002).
However, these models are unsuccessful largely
due to intractable estimation (DeNero and Klein,
2008). Recent progress in better parameteri-
sation and approximate inference (Blunsom et
al., 2009) can only augment the performance of
these models to a similar level as the baseline
where bidirectional word alignments are com-
bined with heuristics and subsequently used to
induce translation equivalence (e.g. (Koehn et
al., 2003)). The most widely used word align-
ment models, such as IBM models 3 and 4, can
only model 1-to-n alignment; these models are
often called “asymmetric” models. IBM models
3 and 4 model 1-to-n alignments using the notion
of “fertility”, which is associated with a “defi-
ciency” problem despite its high performance in
practice.

On the other hand, the HMM word-to-phrase
alignment model tackles 1-to-n alignment prob-
lems with simultaneous segmentation and align-
ment while maintaining the efficiency of the
models. Therefore, this model sets a good ex-
ample of addressing the tradeoffs between mod-
elling power and modelling complexity. This
model can also be seen as a more generalised

101



case of the HMM word-to-word model (Vogel et
al., 1996; Och and Ney, 2003), since this model
can be reduced to an HMM word-to-word model
by restricting the generated target phrase length
to one. One can further refine existing word
alignment models with syntactic constraints (e.g.
(Cherry and Lin, 2006)). However, most re-
search focuses on the incorporation of syntactic
constraints into discriminative alignment mod-
els. Introducing syntactic information into gen-
erative alignment models is shown to be more
challenging mainly due to the absence of appro-
priate modelling of syntactic constraints and the
“inflexibility” of these generative models.

In this paper, we extend the HMM word-to-
phrase alignment model with syntactic depen-
dencies by presenting a model that can incor-
porate syntactic information while maintaining
the efficiency of the model. This model is based
on the observation that in 1-to-n alignments,
the n words bear some syntactic dependencies.
Leveraging such information in the model can
potentially further aid the model in producing
more fine-grained word alignments. The syn-
tactic constraints are specifically imposed on the
n words involved in 1-to-n alignments, which
is different from the cohesion constraints (Fox,
2002) as explored by Cherry and Lin (2006),
where knowledge of cross-lingual syntactic pro-
jection is used. As a syntactic extension of the
open-source MTTK implementation (Deng and
Byrne, 2006) of the HMM word-to-phrase align-
ment model, its source code will also be released
as open source in the near future.

The remainder of the paper is organised as fol-
lows. Section 2 describes the HMM word-to-
phrase alignment model. In section 3, we present
the details of the incorporation of syntactic de-
pendencies. Section 4 presents the experimental
setup, and section 5 reports the experimental re-
sults. In section 6, we draw our conclusions and
point out some avenues for future work.

2 HMM Word-to-Phrase Alignment
Model

In HMM word-to-phrase alignment, a sentence
e is segmented into a sequence of consecutive

phrases: e = vK1 , where vk represents the k
th

phrase in the target sentence. The assumption
that each phrase vk generated as a translation of
one single source word is consecutive is made to
allow efficient parameter estimation. Similarly
to word-to-word alignment models, a variable
aK1 is introduced to indicate the correspondence
between the target phrase index and a source
word index: k → i = ak indicating a mapping
from a target phrase vk to a source word fak . A
random process φk is used to specify the num-
ber of words in each target phrase, subject to the
constraints J =

∑K
k=1 φk, implying that the to-

tal number of words in the phrases agrees with
the target sentence length J .

The insertion of target phrases that do not cor-
respond to any source words is also modelled
by allowing a target phrase to be aligned to a
non-existent source word f0 (NULL). Formally,
to indicate whether each target phrase is aligned
to NULL or not, a set of indicator functions
εK1 = {ε1, · · · , εK} is introduced (Deng and
Byrne, 2008): if εk = 0, then NULL → vk; if
εk = 1, then fak → vk.

To summarise, an alignment a in an HMM
word-to-phrase alignment model consists of the
following elements:

a = (K,φK1 , a
K
1 , ε

K
1 )

The modelling objective is to define a condi-
tional distribution P (e,a|f) over these align-
ments. Following (Deng and Byrne, 2008),
P (e,a|f) can be decomposed into a phrase count
distribution (1) modelling the segmentation of a
target sentence into phrases (P (K|J, f) ∝ ηK
with scalar η to control the length of the hy-
pothesised phrases), a transition distribution (2)
modelling the dependencies between the current
link and the previous links, and a word-to-phrase
translation distribution (3) to model the degree
to which a word and a phrase are translational to
each other.

P (e,a|f) = P (vK1 ,K, aK1 , εK1 , φK1 |f)
= P (K|J, f) (1)

P (aK1 , ε
K
1 , φ

K
1 |K,J, f) (2)

P (vK1 |aK1 , εK1 , φK1 ,K, J, f)(3)

102



The word-to-phrase translation distribution
(3) is formalised as in (4):

P (vK1 |aK1 , εK1 , φK1 ,K, J, f)

=

K∏

k=1

pv(vk|εk · fak , φk) (4)

Note here that we assume that the translation
of each target phrase is conditionally indepen-
dent of other target phrases given the individual
source words.

If we assume that each word in a target phrase
is translated with a dependence on the previ-
ously translated word in the same phrase given
the source word, we derive the bigram transla-
tion model as follows:

pv(vk|fak , εk, φk) = pt1(vk[1]|εk, fak)
φk∏

j=2

pt2(vk[j]|vk[j − 1], εk, fak)

where vk[1] is the first word in phrase vk, vk[j]
is the jth word in vk, pt1 is an unigram transla-
tion probability and pt2 is a bigram translation
probability. The intuition is that the first word
in vk is firstly translated by fak and the transla-
tion of the remaining words vk[j] in vk from fak
is dependent on the translation of the previous
word vk[j − 1] from fak . The use of a bigram
translation model can address the coherence of
the words within the phrase vk so that the qual-
ity of phrase segmentation can be improved.

3 Syntactically Constrained HMM
Word-to-Phrase Alignment Models

3.1 Syntactic Dependencies for
Word-to-Phrase Alignment

As a proof-of-concept, we performed depen-
dency parsing on the GALE gold-standard word
alignment corpus using Maltparser (Nivre et al.,
2007).1 We find that 82.54% of the consec-
utive English words have syntactic dependen-
cies and 77.46% non-consecutive English words
have syntactic dependencies in 1-to-2 Chinese–
English (ZH–EN) word alignment (one Chi-
nese word aligned to two English words). For

1http://maltparser.org/

English–Chinese (EN–ZH) word alignment, we
observe that 75.62% of the consecutive Chinese
words and 71.15% of the non-consecutive Chi-
nese words have syntactic dependencies. Our
model represents an attempt to encode these lin-
guistic intuitions.

3.2 Component Variables and Distributions

We constrain the word-to-phrase alignment
model with a syntactic coherence model. Given
a target phrase vk consisting of φk words, we
use the dependency label rk between words vk[1]
and vk[φk] to indicate the level of coherence.
The dependency labels are a closed set obtained
from dependency parsers, e.g. using Maltparser,
we have 20 dependency labels for English and
12 for Chinese in our data. Therefore, we have
an additional variable rK1 associated with the se-
quence of phrases vK1 to indicate the syntactic
coherence of each phrase, defining P (e,a|f) as
below:

P (rK1 , v
K
1 ,K, a

K
1 , ε

K
1 , φ

K
1 |f) = P (K|J, f)

P (aK1 , φ
K
1 , ε

K
1 |K,J, f)P (vK1 |aK1 , εK1 , φK1 ,K, J, f)

P (rK1 |aK1 , εK1 , φK1 , vK1 ,K, J, f) (5)

The syntactic coherence distribution (5) is
simplified as in (6):

P (rK1 |aK1 , εK1 , φK1 , vK1 ,K, J, f)

=
K∏

k=1

pr(rk; ε, fak , φk) (6)

Note that the coherence of each target phrase
is conditionally independent of the coherence of
other target phrases given the source words fak
and the number of words in the current phrase
φk. We name the model in (5) the SSH model.
SSH is an abbreviation of Syntactically con-
strained Segmental HMM, given the fact that
the HMM word-to-phrase alignment model is a
Segmental HMM model (SH) (Ostendorf et al.,
1996; Murphy, 2002).

As our syntactic coherence model utilises syn-
tactic dependencies which require the presence
of at least two words in target phrase vk, we
therefore model the cases of φk = 1 and φk ≥ 2

103



separately. We rewrite (6) as follows:

pr(rk; ε, fak , φk) ={
pφk=1(rk; ε, fak ) if φk = 1

pφk≥2(rk; ε, fak ) if φk ≥ 2

where pφk=1 defines the syntactic coherence
when the target phrase only contains one word
(φk = 1) and pφk≥2 defines the syntactic co-
herence of a target phrase composed of multiple
words (φk ≥ 2). We define pφk=1 as follows:

pφk=1(rk; ε, fak ) ∝ pn(φk = 1; ε, fak )

where the coherence of the target phrase (word)
vk is defined to be proportional to the probability
of target phrase length φk = 1 given the source
word fak . The intuition behind this model is that
the syntactic coherence is strong iff the probabil-
ity of the source fak fertility φk = 1 is high.

For pφk≥2, which measures the syntactic co-
herence of a target phrase consisting of more
than two words, we use the dependency label rk
between words vk[1] and vk[φk] to indicate the
level of coherence. A distribution over the values
rk ∈ R = {SBJ,ADJ, · · · } (R is the set of de-
pendency types for a specific language) is main-
tained as a table for each source word associated
with all the possible lengths φ ∈ {2, · · · ,N})
of the target phrase it can generate, e.g. we set
N = 4 for ZH–EN alignment and N = 2 for
EN–ZH alignment in our experiments.

Given a target phrase vk containing φk(φk ≥
2) words, it is possible that there are no depen-
dencies between the first word vk[1] and the last
word vk[φk]. To account for this fact, we intro-
duce a indicator function ϕ as in below:

ϕ(vk[1], φk) =





1 if vk[1] and vk[φk]have

syntactic dependencies

0 otherwise

We can thereafter introduce a distribution pϕ(ϕ),
where pϕ(ϕ = 0) = ζ (0 ≤ ζ ≤ 1) and
pϕ(ϕ = 0) = 1− ζ , with ζ indicating how likely
it is that the first and final words in a target phrase
do not have any syntactic dependencies. We can
set ζ to a small number to favour target phrases

satisfying the syntactic constraints and to a larger
number otherwise. The introduction of this vari-
able enables us to tune the model towards our
different end goals. We can now define pφk≥2
as:

pφk≥2(rk; ε, fak) = p(rk|ϕ; ε, fak )pϕ(ϕ)
where we insist that p(rk|ϕ; ε, fak ) = 1 if
ϕ = 0 (the first and last words in the target
phrase do not have syntactic dependencies) to
reflect the fact that in most arbitrary consecu-
tive word sequences the first and last words do
not have syntactic dependencies, and otherwise
p(rk|ϕ; ε, fak ) if ϕ = 1 (the first and last words
in the target phrase have syntactic dependen-
cies).

3.3 Parameter Estimation

The Forward-Backward Algorithm (Baum,
1972), a version of the EM algorithm (Dempster
et al., 1977), is specifically designed for unsu-
pervised parameter estimation of HMM models.
The Forward statistic αj(i, φ, ε) in our model
can be calculated recursively over the trellis as
follows:

αj(i, φ, ε) = {
∑

i′,φ′,ε′
αj−φ(i

′, φ′, ε′)pa(i|i′, ε; I)}

pn(φ; ε, fi)ηpt1(ej−φ+1|ε, fi)
j∏

j′=j−φ+2
pt2(ej′ |ej′−1, ε, fi)pr(rk; ε, fi, φ)

which sums up the probabilities of every path
that could lead to the cell 〈j, i, φ〉. Note that the
syntactic coherence term pr(rk; ε, fi, φ) can ef-
ficiently be added into the Forward procedure.
Similarly, the Backward statistic βj(i, φ, ε) is
calculated over the trellis as below:

βj(i, φ, ε) =
∑

i′,φ′,ε′
βj+φ′(i

′, φ′, ε′)pa(i
′|i, h′; I)

pn(φ
′; ε′, fi′)ηpt1(ej+1|ε′, fi′)

j+φ′∏

j′=j+2

pt2(ej′ |ej′−1, ε′, fi′)pr(rk; ε′, fi′ , φ′)

Note also that the syntactic coherence term
pr(rk; ε

′, fi′ , φ′) can be integrated into the Back-
ward procedure efficiently.

104



Posterior probability can be calculated based
on the Forward and Backward probabilities.

3.4 EM Parameter Updates

The Expectation step accumulates fractional
counts using the posterior probabilities for each
parameter during the Forward-Backward passes,
and the Maximisation step normalises the counts
in order to generate updated parameters.

The E-step for the syntactic coherence model
proceeds as follows:

c(r′; f, φ′) =
∑

(f ,e)∈T

∑

i,j,φ,fi=f

γj(i, φ, ε = 1)

δ(φ, φ′)δ(ϕj(e, φ), r′)

where γj(i, φ, ε) is the posterior probability that
a target phrase tjj−φ+1 is aligned to source word
fi, and ϕj(e, φ) is the syntactic dependency label
between ej−φ+1 and ej . The M-step performs
normalisation, as below:

pr(r
′; f, φ′) =

c(r′; f, φ′)∑
r c(r; f, φ

′)

Other component parameters can be estimated
in a similar manner.

4 Experimental Setup

4.1 Data

We built the baseline word alignment and
Phrase-Based SMT (PB-SMT) systems using ex-
isting open-source toolkits for the purposes of
fair comparison. A collection of GALE data
(LDC2006E26) consisting of 103K (2.9 million
English running words) sentence pairs was firstly
used as a proof of concept (“small”), and FBIS
data containing 238K sentence pairs (8 million
English running words) was added to construct a
“medium” scale experiment. To investigate the
intrinsic quality of the alignment, a collection
of parallel sentences (12K sentence pairs) for
which we have manually annotated word align-
ment was added to both “small” and “medium”
scale experiments. Multiple-Translation Chinese
Part 1 (MTC1) from LDC was used for Mini-
mum Error-Rate Training (MERT) (Och, 2003),
and MTC2, 3 and 4 were used as development

test sets. Finally the test set from NIST 2006
evaluation campaign was used as the final test
set.

The Chinese data was segmented using the
LDC word segmenter. The maximum-entropy-
based POS tagger MXPOST (Ratnaparkhi, 1996)
was used to tag both English and Chinese texts.
The syntactic dependencies for both English and
Chinese were obtained using the state-of-the-art
Maltparser dependency parser, which achieved
84% and 88% labelled attachment scores for
Chinese and English respectively.

4.2 Word Alignment

The GIZA++ (Och and Ney, 2003) implementa-
tion of IBM Model 4 (Brown et al., 1993) is used
as the baseline for word alignment. Model 4 is
incrementally trained by performing 5 iterations
of Model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4. We
compared our model against the MTTK (Deng
and Byrne, 2006) implementation of the HMM
word-to-phrase alignment model. The model
training includes 10 iterations of Model 1, 5 it-
erations of Model 2, 5 iterations of HMM word-
to-word alignment, 20 iterations (5 iterations re-
spectively for phrase lengths 2, 3 and 4 with un-
igram translation probability, and phrase length
4 with bigram translation probability) of HMM
word-to-phrase alignment for ZH–EN alignment
and 5 iterations (5 iterations for phrase length
2 with uniform translation probability) of HMM
word-to-phrase alignment for EN–ZH. This con-
figuration is empirically established as the best
for Chinese–English word alignment. To allow
for a fair comparison between IBM Model 4
and HMM word-to-phrase alignment models, we
also restrict the maximum fertility in IBM model
4 to 4 for ZH–EN and 2 for EN–ZH (the default
is 9 in GIZA++ for both ZH–EN and EN–ZH).
“grow-diag-final” heuristic described in (Koehn
et al., 2003) is used to derive the refined align-
ment from bidirectional alignments.

4.3 MT system

The baseline in our experiments is a standard
log-linear PB-SMT system. With the word align-
ment obtained using the method described in

105



section 4.2, we perform phrase-extraction using
heuristics described in (Koehn et al., 2003), Min-
imum Error-Rate Training (MERT) (Och, 2003)
optimising the BLEU metric, a 5-gram language
model with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
MOSES (Koehn et al., 2007) for decoding. A
Hiero-style decoder Joshua (Li et al., 2009) is
also used in our experiments. All significance
tests are performed using approximate randomi-
sation (Noreen, 1989) at p = 0.05.

5 Experimental Results

5.1 Alignment Model Tuning

In order to find the value of ζ in the SSH model
that yields the best MT performance, we used
three development test sets using a PB-SMT sys-
tem trained on the small data condition. Figure 1
shows the results on each development test set
using different configurations of the alignment
models. For each system, we obtain the mean
of the BLEU scores (Papineni et al., 2002) on
the three development test sets, and derive the
optimal value for ζ of 0.4, which we use here-
after for final testing. It is worth mentioning
that while IBM model 4 (M4) outperforms other
models including the HMM word-to-word (H)
and word-to-phrase (SH) alignment model in our
current setup, using the default IBM model 4 set-
ting (maximum fertility 9) yields an inferior per-
formance (as much as 8.5% relative) compared
to other models.

 0.11

 0.115

 0.12

 0.125

 0.13

 0.135

 0.14

M
4

H SH SSH-0.05

SSH-0.1

SSH-0.2

SSH-0.3

SSH-0.4

SSH-0.5

SSH-0.6

B
LE

U
 s

co
re

alignment systems

MTC2
MTC3
MTC4

Figure 1: BLEU score on development test set
using PB-SMT system

PB-SMT Hiero
small medium small medium

H 0.1440 0.2591 0.1373 0.2595
SH 0.1418 0.2517 0.1372 0.2609
SSH 0.1464 0.2518 0.1356 0.2624
M4 0.1566 0.2627 0.1486 0.2660

Table 1: Performance of PB-SMT using different
alignment models on NIST06 test set

5.2 Translation Results

Table 1 shows the performance of PB-SMT and
Hiero systems using a small amount of data for
alignment model training on the NIST06 test set.
For the PB-SMT system trained on the small data
set, using SSH word alignment leads to a 3.24%
relative improvement over SH, which is statis-
tically significant. SSH also leads to a slight
gain over the HMM word-to-word alignment
model (H). However, when the PB-SMT system
is trained on larger data sets, there are no sig-
nificant differences between SH and SSH. Addi-
tionally, both SH and SSH models underperform
H on the medium data condition, indicating that
the performance of the alignment model tuned
on the PB-SMT system with small training data
does not carry over to PB-SMT systems with
larger training data (cf. Figure 1). IBM model
4 demonstrates stronger performance over other
models for both small and medium data condi-
tions.

For the Hiero system trained on a small data
set, no significant differences are observed be-
tween SSH, SH and H. On a larger training set,
we observe that SSH alignment leads to better
performance compared to SH. Both SH and SSH
alignments achieved higher translation quality
than H. Note that while IBM model 4 outper-
forms other models on a small data condition, the
difference between IBM model 4 and SSH is not
statistically significant on a medium data condi-
tion. It is also worth pointing out that the SSH
model yields significant improvement over IBM
model 4 with the default fertility setting, indicat-
ing that varying the fertility limit in IBM model
4 has a significant impact on translation quality.

In summary, the SSH model which incorpo-
rates syntactic dependencies into the SH model
achieves consistently better performance than

106



ZH–EN EN–ZH
P R P R

H 0.5306 0.3752 0.5282 0.3014
SH 0.5378 0.3802 0.5523 0.3151
SSH 0.5384 0.3807 0.5619 0.3206
M4 0.5638 0.3986 0.5988 0.3416

Table 2: Intrinsic evaluation of the alignment us-
ing different alignment models

SH in both PB-SMT and Hiero systems under
both small and large data conditions. For a
PB-SMT system trained on the small data set,
the SSH model leads to significant gains over
the baseline SH model. The results also en-
tail an observation concerning the suitability of
different alignment models for different types
of SMT systems; trained on a large data set,
our SSH alignment model is more suitable to
a Hiero-style system than a PB-SMT system,
as evidenced by a lower performance compared
to IBM model 4 using a PB-SMT system, and
a comparable performance compared to IBM
model 4 using a Hiero system.

5.3 Intrinsic Evaluation

In order to further investigate the intrinsic qual-
ity of the word alignment, we compute the Preci-
sion (P), Recall (R) and F-score (F) of the align-
ments obtained using different alignment mod-
els. As the models investigated here are asym-
metric models, we conducted intrinsic evalua-
tion for both alignment directions, i.e. ZH–EN
word alignment where one Chinese word can be
aligned to multiple English words, and EN–ZH
word alignment where one English word can be
aligned to multiple Chinese words.

Table 2 shows the results of the intrinsic eval-
uation of ZH–EN and EN–ZH word alignment
on a small data set (results on the medium data
set follow the same trend but are left out due
to space limitations). Note that the P and R
are all quite low, demonstrating the difficulty of
Chinese–English word alignment in the news do-
main. For the ZH–EN direction, using the SSH
model does not lead to significant gains over SH
in P or R. For the EN–ZH direction, the SSH
model leads to a 1.74% relative improvement in
P, and a 1.75% relative improvement in R over

the SH model. Both SH and SSH lead to gains
over H for both ZH–EN and EN–ZH directions,
while gains in the EN–ZH direction appear to be
more pronounced. IBM model 4 achieves signif-
icantly higher P over other models while the gap
in R is narrow.

Relating Table 2 to Table 1, we observe that
the HMM word-to-word alignment model (H)
can still achieve good MT performance despite
the lower P and R compared to other mod-
els. This provides additional support to previ-
ous findings (Fraser and Marcu, 2007b) that the
intrinsic quality of word alignment does not nec-
essarily correlate with the performance of the re-
sulted MT system.

5.4 Alignment Characteristics

In order to further understand the characteristics
of the alignment that each model produces, we
investigated several statistics of the alignment re-
sults which can hopefully reveal the capabilities
and limitations of each model.

5.4.1 Pairwise Comparison

Given the asymmetric property of these align-
ment models, we can evaluate the quality of the
links for each word and compare the alignment
links across different models. For example, in
ZH–EN word alignment, we can compute the
links for each Chinese word and compare those
links across different models. Additionally, we
can compute the pairwise agreement in align-
ing each Chinese word for any two alignment
models. Similarly, we can compute the pairwise
agreement in aligning each English word in the
EN–ZH alignment direction.

For ZH–EN word alignment, we observe that
the SH and SSH models reach a 85.94% agree-
ment, which is not surprising given the fact that
SSH is a syntactic extension over SH, while IBM
model 4 and SSH reach the smallest agreement
(only 65.09%). We also observe that there is a
higher agreement between SSH and H (76.64%)
than IBM model 4 and H (69.58%). This can be
attributed to the fact that SSH is still a form of
HMM model while IBM model 4 is not. A simi-
lar trend is observed for EN–ZH word alignment.

107



ZH–EN EN–ZH
1-to-0 1-to-1 1-to-n 1-to-0 1-to-1 1-to-n

con. non-con. con. non-con.
HMM 0.3774 0.4693 0.0709 0.0824 0.4438 0.4243 0.0648 0.0671
SH 0.3533 0.4898 0.0843 0.0726 0.4095 0.4597 0.0491 0.0817
SSH 0.3613 0.5092 0.0624 0.0671 0.3990 0.4835 0.0302 0.0872
M4 0.2666 0.5561 0.0985 0.0788 0.3967 0.4850 0.0592 0.0591

Table 3: Alignment types using different alignment models

5.4.2 Alignment Types

Again, by taking advantage of the asymmet-
ric property of these alignment models, we can
compute different types of alignment. For both
ZH–EN (EN–ZH) alignment, we divide the links
for each Chinese (English) word into 1-to-0
where each Chinese (English) word is aligned
to the empty word “NULL” in English (Chi-
nese), 1-to-1 where each Chinese (English) word
is aligned to only one word in English (Chinese),
and 1-to-n where each Chinese (English) word
is aligned to n (n ≥ 2) words in English (Chi-
nese). For 1-to-n links, depending on whether
the n words are consecutive, we have consecu-
tive (con.) and non-consecutive (non-con.) 1-to-
n links.

Table 3 shows the alignment types in the
medium data track. We can observe that for
ZH–EN word alignment, both SH and SSH pro-
duce far more 1-to-0 links than Model 4. It can
also be seen that Model 4 tends to produce more
consecutive 1-to-n links than non-consecutive 1-
to-n links. On the other hand, the SSH model
tends to produce more non-consecutive 1-to-n
links than consecutive ones. Compared to SH,
SSH tends to produce more 1-to-1 links than 1-
to-n links, indicating that adding syntactic de-
pendency constraints biases the model towards
only producing 1-to-n links when the n words
follow coherence constraint, i.e. the first and last
word in the chunk have syntactic dependencies.
For example, among the 6.24% consecutive ZH–
EN 1-to-n links produced by SSH, 43.22% of
them follow the coherence constraint compared
to just 39.89% in SH. These properties can have
significant implications for the performance of
our MT systems given that we use the grow-
diag-final heuristics to derive the symmetrised
word alignment based on bidirectional asymmet-

ric word alignments.

6 Conclusions and Future Work

In this paper, we extended the HMM word-to-
phrase word alignment model to handle syntac-
tic dependencies. We found that our model was
consistently better than that without syntactic de-
pendencies according to both intrinsic and ex-
trinsic evaluation. Our model is shown to be ben-
eficial to PB-SMT under a small data condition
and to a Hiero-style system under a larger data
condition.

As to future work, we firstly plan to investi-
gate the impact of parsing quality on our model,
and the use of different heuristics to combine
word alignments. Secondly, the syntactic co-
herence model itself is very simple, in that it
only covers the syntactic dependency between
the first and last word in a phrase. Accordingly,
we intend to extend this model to cover more so-
phisticated syntactic relations within the phrase.
Furthermore, given that we can construct dif-
ferent MT systems using different word align-
ments, multiple system combination can be con-
ducted to avail of the advantages of different sys-
tems. We also plan to compare our model with
other alignment models, e.g. (Fraser and Marcu,
2007a), and test this approach on more data and
on different language pairs and translation direc-
tions.

Acknowledgements

This research is supported by the Science Foundation Ire-
land (Grant 07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin City Uni-
versity. Part of the work was carried out at Cambridge Uni-
versity Engineering Department with Dr. William Byrne.
The authors would also like to thank the anonymous re-
viewers for their insightful comments.

108



References

Baum, Leonard E. 1972. An inequality and associ-
ated maximization technique in statistical estimation for
probabilistic functions of Markov processes. Inequali-
ties, 3:1–8.

Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of ACL-IJCNLP
2009, pages 782–790, Singapore.

Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-
Pietra, and Robert L. Mercer. 1993. The mathematics of
Statistical Machine Translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.

Cherry, Colin and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING-ACL 2006, pages
105–112, Sydney, Australia.

Dempster, Arthur, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.

DeNero, John and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-08:
HLT, Short Papers, pages 25–28, Columbus, OH.

Deng, Yonggang and William Byrne. 2006. MTTK: An
alignment toolkit for Statistical Machine Translation. In
Proceedings of HLT-NAACL 2006, pages 265–268, New
York City, NY.

Deng, Yonggang and William Byrne. 2008. HMM word
and phrase alignment for Statistical Machine Transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494–507.

Fox, Heidi. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the EMNLP 2002, pages
304–3111, Philadelphia, PA, July.

Fraser, Alexander and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Pro-
ceedings of EMNLP-CoNLL 2007, pages 51–60, Prague,
Czech Republic.

Fraser, Alexander and Daniel Marcu. 2007b. Measuring
word alignment quality for Statistical Machine Transla-
tion. Computational Linguistics, 33(3):293–303.

Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE ICASSP, volume 1, pages 181–
184, Detroit, MI.

Koehn, Philipp, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL 2003, pages 48–54, Edmonton, AB,
Canada.

Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
Statistical Machine Translation. In Proceedings of ACL
2007, pages 177–180, Prague, Czech Republic.

Li, Zhifei, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation. In
Proceedings of the WMT 2009, pages 135–139, Athens,
Greece.

Marcu, Daniel and William Wong. 2002. A Phrase-Based,
joint probability model for Statistical Machine Transla-
tion. In Proceedings of EMNLP 2002, pages 133–139,
Philadelphia, PA.

Murphy, Kevin. 2002. Hidden semi-markov models (seg-
ment models). Technical report, UC Berkeley.

Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(2):95–135.

Noreen, Eric W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.

Och, Franz and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Compu-
tational Linguistics, 29(1):19–51.

Och, Franz. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings of ACL 2003,
pages 160–167, Sapporo, Japan.

Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kim-
ball. 1996. From HMMs to segment models: A uni-
fied view of stochastic modeling for speech recognition.
IEEE Transactions on Speech and Audio Processing,
4(5):360–378.

Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of ACL
2002, pages 311–318, Philadelphia, PA.

Ratnaparkhi, Adwait. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133–142, Somerset, NJ.

Stolcke, Andreas. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO.

Vogel, Stefan, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of ACL 1996, pages 836–841,
Copenhagen, Denmark.

109


