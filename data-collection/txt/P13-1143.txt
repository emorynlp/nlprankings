



















































Grammatical Error Correction Using Integer Linear Programming


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1456–1465,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Grammatical Error Correction Using Integer Linear Programming

Yuanbin Wu

Department of Computer Science

National University of Singapore

13 Computing Drive

Singapore 117417

wuyb@comp.nus.edu.sg

Hwee Tou Ng

Department of Computer Science

National University of Singapore

13 Computing Drive

Singapore 117417

nght@comp.nus.edu.sg

Abstract

We propose a joint inference algorithm for

grammatical error correction. Different

from most previous work where different

error types are corrected independently,

our proposed inference process considers

all possible errors in a uni�ed framework.

We use integer linear programming (ILP)

to model the inference process, which can

easily incorporate both the power of exist-

ing error classi�ers and prior knowledge

on grammatical error correction. Exper-

imental results on the Helping Our Own

shared task show that our method is com-

petitive with state-of-the-art systems.

1 Introduction

Grammatical error correction is an important task

of natural language processing (NLP). It has many

potential applications and may help millions of

people who learn English as a second language

(ESL). As a research �eld, it faces the challenge of

processing ungrammatical language, which is dif-

ferent from other NLP tasks. The task has received

much attention in recent years, and was the focus

of two shared tasks on grammatical error correc-

tion in 2011 and 2012 (Dale and Kilgarriff, 2011;

Dale et al., 2012).

To detect and correct grammatical errors, two

different approaches are typically used  knowl-

edge engineering or machine learning. The �rst

relies on handcrafting a set of rules. For exam-

ple, the superlative adjective best is preceded by

the article the. In contrast, the machine learn-

ing approach formulates the task as a classi�cation

problem based on learning from training data. For

example, an article classi�er takes a noun phrase

(NP) as input and predicts its article using class

labels a/an, the, or ɛ (no article).
Both approaches have their advantages and dis-

advantages. One can readily handcraft a set of

rules to incorporate various prior knowledge from

grammar books and dictionaries, but rules often

have exceptions and it is dif�cult to build rules

for all grammatical errors. On the other hand, the

machine learning approach can learn from texts

written by ESL learners where grammatical errors

have been annotated. However, training data may

be noisy and classi�ers may need prior knowledge

to guide their predictions.

Another consideration in grammatical error cor-

rection is how to deal with multiple errors in an

input sentence. Most previous work deals with

errors individually: different classi�ers (or rules)

are developed for different types of errors (article

classi�er, preposition classi�er, etc). Classi�ers

are then deployed independently. An example is

a pipeline system, where each classi�er takes the

output of the previous classi�er as its input and

proposes corrections of one error type.

One problem of this pipeline approach is that

the relations between errors are ignored. For ex-

ample, assume that an input sentence contains a

cats. An article classi�er may propose to delete

a, while a noun number classi�er may propose

to change cats to cat. A pipeline approach will

choose one of the two corrections based purely

on which error classi�er is applied �rst. Another

problem is that when applying a classi�er, the sur-

rounding words in the context are assumed to be

correct, which is not true if grammatical errors ap-

pear close to each other in a sentence.

In this paper, we formulate grammatical er-

ror correction as a task suited for joint inference.

Given an input sentence, different types of errors

are jointly corrected as follows. For every possi-

ble error correction, we assign a score which mea-

sures how grammatical the resulting sentence is if

the correction is accepted. We then choose a set

of corrections which will result in a corrected sen-

tence that is judged to be the most grammatical.

The inference problem is solved by integer lin-

1456



ear programming (ILP). Variables of ILP are indi-

cators of possible grammatical error corrections,

the objective function aims to select the best set of

corrections, and the constraints help to enforce a

valid and grammatical output. Furthermore, ILP

not only provides a method to solve the inference

problem, but also allows for a natural integration

of grammatical constraints into a machine learn-

ing approach. We will show that ILP fully utilizes

individual error classi�ers, while prior knowledge

on grammatical error correction can be easily ex-

pressed using linear constraints. We evaluate our

proposed ILP approach on the test data from the

Helping Our Own (HOO) 2011 shared task (Dale

and Kilgarriff, 2011). Experimental results show

that the ILP formulation is competitive with state-

of-the-art grammatical error correction systems.

The remainder of this paper is organized as fol-

lows. Section 2 gives the related work. Section

3 introduces a basic ILP formulation. Sections

4 and 5 improve the basic ILP formulation with

more constraints and second order variables, re-

spectively. Section 6 presents the experimental re-

sults. Section 7 concludes the paper.

2 Related Work

The knowledge engineering approach has been

used in early grammatical error correction systems

(Murata and Nagao, 1993; Bond et al., 1995; Bond

and Ikehara, 1996; Heine, 1998). However, as

noted by (Han et al., 2006), rules usually have ex-

ceptions, and it is hard to utilize corpus statistics

in handcrafted rules. As such, the machine learn-

ing approach has become the dominant approach

in grammatical error correction.

Previous work in the machine learning approach

typically formulates the task as a classi�cation

problem. Article and preposition errors are the two

main research topics (Knight and Chander, 1994;

Han et al., 2006; Tetreault and Chodorow, 2008;

Dahlmeier and Ng, 2011). Features used in classi-

�cation include surrounding words, part-of-speech

tags, language model scores (Gamon, 2010), and

parse tree structures (Tetreault et al., 2010). Learn-

ing algorithms used include maximum entropy

(Han et al., 2006; Tetreault and Chodorow, 2008),

averaged perceptron, na�̈ve Bayes (Rozovskaya

and Roth, 2011), etc. Besides article and prepo-

sition errors, verb form errors also attract some

attention recently (Liu et al., 2010; Tajiri et al.,

2012).

Several research efforts have started to deal with

correcting different errors in an integrated manner

(Gamon, 2011; Park and Levy, 2011; Dahlmeier

and Ng, 2012a). Gamon (2011) uses a high-order

sequential labeling model to detect various errors.

Park and Levy (2011) models grammatical error

correction using a noisy channel model, where a

prede�ned generative model produces correct sen-

tences and errors are added through a noise model.

The work of (Dahlmeier and Ng, 2012a) is proba-

bly the closest to our current work. It uses a beam-

search decoder, which iteratively corrects an in-

put sentence to arrive at the best corrected output.

The difference between their work and our ILP

approach is that the beam-search decoder returns

an approximate solution to the original inference

problem, while ILP returns an exact solution to an

approximate inference problem.

Integer linear programming has been success-

fully applied to many NLP tasks, such as depen-

dency parsing (Riedel and Clarke, 2006; Martins

et al., 2009), semantic role labeling (Punyakanok

et al., 2005), and event extraction (Riedel and Mc-

Callum, 2011).

3 Inference with First Order Variables

The inference problem for grammatical error cor-

rection can be stated as follows: Given an input

sentence, choose a set of corrections which results

in the best output sentence. In this paper, this

problem will be expressed and solved by integer

linear programming (ILP).

To express an NLP task in the framework of ILP

requires the following steps:

1. Encode the output space of the NLP task us-

ing integer variables;

2. Express the inference objective as a linear

objective function; and

3. Introduce problem-speci�c constraints to re-

�ne the feasible output space.

In the following sections, we follow the above

formulation. For the grammatical error correc-

tion task, the variables in ILP are indicators of the

corrections that a word needs, the objective func-

tion measures how grammatical the whole sen-

tence is if some corrections are accepted, and the

constraints guarantee that the corrections do not

con�ict with each other.

1457



3.1 First Order Variables

Given an input sentence, the main question that a

grammatical error correction system needs to an-

swer is: What corrections at which positions? For

example, is it reasonable to change the word cats

to cat in the sentence A cats sat on the mat? Given

the corrections at various positions in a sentence,

the system can readily come up with the corrected

sentence. Thus, a natural way to encode the output

space of grammatical error correction requires in-

formation about sentence position, error type (e.g.,

noun number error), and correction (e.g., cat).

Suppose s is an input sentence, and |s| is its
length (i.e., the number of words in s). De�ne �rst
order variables:

Zkl,p ∈ {0, 1}, (1)

where

p∈ {1, 2, . . . , |s|} is a position in a sentence,
l∈ L is an error type,
k∈ {1, 2, . . . , C(l)} is a correction of type l.
L: the set of error types,

C(l): the number of corrections for error type l.

If Zkl,p = 1, the word at position p should be cor-
rected to k that is of error type l. Otherwise, the
word at position p is not applicable for this correc-
tion. Deletion of a word is represented as k = ɛ.
For example, ZaArt,1 = 1 means that the article
(Art) at position 1 of the sentence should be a. If
ZaArt,1 = 0, then the article should not be a. Ta-
ble 1 contains the error types handled in this work,

their possible corrections and applicable positions

in a sentence.

3.2 The Objective Function

The objective of the inference problem is to �nd

the best output sentence. However, there are expo-

nentially many different combinations of correc-

tions, and it is not possible to consider all com-

binations. Therefore, instead of solving the orig-

inal inference problem, we will solve an approx-

imate inference problem by introducing the fol-

lowing decomposable assumption: Measuring the

output quality of multiple corrections can be de-

composed into measuring the quality of the indi-

vidual corrections.

Let s′ be the resulting sentence if the correction
Zkl,p is accepted for s, or for simplicity denoting

it as s
Zkl,p−−→ s′. Let wl,p,k ∈ R, measure how

grammatical s′ is. De�ne the objective function as

max
∑

l,p,k

wl,p,kZ
k
l,p.

This linear objective function aims to select a set

of Zkl,p, such that the sum of their weights is the
largest among all possible candidate corrections,

which in turn gives the most grammatical sentence

under the decomposable assumption.

Although the decomposable assumption is a

strong assumption, it performs well in practice,

and one can relax the assumption by using higher

order variables (see Section 5).

For an individual correction Zkl,p, we measure
the quality of s′ based on three factors:
1. The language model score h(s′,LM) of s′

based on a large web corpus;

2. The con�dence scores f(s′, t) of classi�ers,
where t ∈ E and E is the set of classi�ers. For ex-
ample, an article classi�er trained on well-written

documents will score every article in s′, and mea-
sure the quality of s′ from the perspective of an
article expert.

3. The disagreement scores g(s′, t) of classi-
�ers, where t ∈ E. A disagreement score mea-
sures how ungrammatical s′ is from the perspec-
tive of a classi�er. Take the article classi�er as an

example. For each article instance in s′, the clas-
si�er computes the difference between the maxi-

mum con�dence score among all possible choices

of articles, and the con�dence score of the ob-

served article. This difference represents the dis-

agreement on the observed article by the article

classi�er or expert. De�ne the maximum differ-

ence over all article instances in s′ to be the article
classi�er disagreement score of s′. In general, this
score is large if the sentence s′ is more ungram-
matical.

The weight wl,p,k is a combination of these
scores:

wl,p,k = νLMh(s
′,LM) +

∑

t∈E
λtf(s

′, t)

+
∑

t∈E
µtg(s

′, t), (2)

where νLM, λt, and µt are the coef�cients.

3.3 Constraints

An observation on the objective function is that

it is possible, for example, to set ZaArt,p = 1 and

1458



Type l Correction k C(l) Applicable Variables

article a, the, ɛ 3 article or NP ZaArt,p, Z
the
Art,p,Z

ɛ
Art,p

preposition on, at, in, . . . |confusion set| preposition ZonPrep,p, ZatPrep,p, Z inPrep,p, . . .
noun number singular, plural 2 noun Z

singular
Noun,p , Z

plural
Noun,p

punctuation punctuation symbols |candidates| determined by rules ZoriginalPunct,p , Zcand1Punct,p, Zcand2Punct,p,. . .
spelling correctly spelled |candidates| determined by a ZoriginalSpell,p , Zcand1Spell,p, Zcand2Spell,p,. . .

words spell checker

Table 1: Error types and corrections. The Applicable column indicates which parts of a sentence are

applicable to an error type. In the �rst row, ɛ means deleting an article.

Z theArt,p = 1, which means there are two corrections
a and the for the same sentence position p, but ob-
viously only one article is allowed.

A simple constraint to avoid these con�icts is

∑

k

Zkl,p = 1, ∀ applicable l, p

It reads as follows: for each error type l, only one
output k is allowed at any applicable position p
(note that Zkl,p is a Boolean variable).
Putting the variables, objective function, and

constraints together, the ILP problem with respect

to �rst order variables is as follows:

max
∑

l,p,k

wl,p,kZ
k
l,p (3)

s.t.
∑

k

Zkl,p = 1, ∀ applicable l, p (4)

Zkl,p ∈ {0, 1} (5)

The ILP problem is solved using lp solve1, an

integer linear programming solver based on the re-

vised simplex method and the branch-and-bound

method for integers.

3.4 An Illustrating Example

To illustrate the ILP formulation, consider an ex-

ample input sentence s:

A cats sat on the mat . (6)

First, the constraint (4) at position 1 is:

ZaArt,1 + Z
the
Art,1 + Z

ɛ
Art,1 = 1,

which means only one article in {a, the, ɛ} is se-
lected.

1http://lpsolve.sourceforge.net/

Next, to compute wl,p,k, we collect language
model score and con�dence scores from the arti-

cle (ART), preposition (PREP), and noun number

(NOUN) classi�er, i.e., E = {ART,PREP,NOUN}.
The weight for Z

singular
Noun,2 is:

wNoun,2,singular = νLMh(s
′,LM)+

λARTf(s
′,ART) + λPREPf(s

′,PREP) + λNOUNf(s
′,NOUN)+

µARTg(s
′,ART) + µPREPg(s

′,PREP) + µNOUNg(s
′,NOUN).

where s
Z

singular
Noun,2−−−−→ s′ = A cat sat on the mat .

The con�dence score f(s′, t) of classi�er t is
the average of the con�dence scores of t on the
applicable instances in s′.
For example, there are two article instances in

s′, located at position 1 and 5 respectively, hence,

f(s′,ART)=
1

2

�
f(s′[1], 1,ART) + f(s′[5], 5,ART)

�

=
1

2

�
f(a, 1,ART) + f(the, 5,ART)

�
.

Here, the symbol ft(s
′[p], p,ART) refers to the

con�dence score of the article classi�er at position

p, and s′[p] is the word at position p of s′.
Similarly, the disagreement score g(s′,ART) of

the article classi�er is

g(s′,ART) = max(g1, g2)

g1= arg max
k

f(k, 1,ART) − f(a, 1,ART)
g2= arg max

k
f(k, 5,ART) − f(the, 5,ART)

Putting them together, the weight forZ
singular
Noun,2 is:

wNoun,2,singular = νLMh(s
′,LM)

+
λART
2

�
f(a, 1,ART) + f(the, 5,ART)

�

+ λPREPf(on, 4,PREP)

+
λNOUN

2

�
f(cat, 2,NOUN) + f(mat, 6,NOUN)

�

+ µARTg(s
′,ART)

+ µPREPg(s
′,PREP)

+ µNOUNg(s
′,NOUN)

1459



Input A cats sat on the mat

Corrections The, ɛ cat at, in a, ɛ mats

ZaArt,1 Z
singular
Noun,2 Z

on
Prep,4 Z

a
Art,5 Z

singular
Noun,6

Variables Z theArt,1 Z
plural
Noun,2 Z

at
Prep,4 Z

the
Art,5 Z

plural
Noun,6

ZɛArt,1 Z
in
Prep,4 Z

ɛ
Art,5

Table 2: The possible corrections on example (6).

3.5 Complexity

The time complexity of ILP is determined by

the number of variables and constraints. Assume

that for each sentence position, at most K classi-
�ers are applicable2. The number of variables is

O(K|s|C(l∗)), where l∗ = arg maxl∈LC(l). The
number of constraints is O(K|s|).

4 Constraints for Prior Knowledge

4.1 Modi�cation Count Constraints

In practice, we usually have some rough gauge

of the quality of an input sentence. If an input

sentence is mostly grammatical, the system is ex-

pected to make few corrections. This require-

ment can be easily satis�ed by adding modi�ca-

tion count constraints.

In this work, we constrain the number of modi�-

cations according to error types. For the error type

l, a parameter Nl controls the number of modi�-
cations allowed for type l. For example, the mod-
i�cation count constraint for article corrections is

∑

p,k

ZkArt,p ≤ NArt, where k 6= s[p]. (7)

The condition ensures that the correction k is dif-
ferent from the original word in the input sentence.

Hence, the summation only counts real modi�ca-

tions. There are similar constraints for preposi-

tion, noun number, and spelling corrections:

∑

p,k

ZkPrep,p≤ NPrep, where k 6= s[p], (8)
∑

p,k

ZkNoun,p≤ NNoun, where k 6= s[p], (9)
∑

p,k

ZkSpell,p≤ NSpell, where k 6= s[p]. (10)

2In most cases, K = 1. An example of K > 1 is a noun
that requires changing the word form (between singular and
plural) and inserting an article, for which K = 2.

4.2 Article-Noun Agreement Constraints

An advantage of the ILP formulation is that it

is relatively easy to incorporate prior linguistic

knowledge. We now take article-noun agreement

as an example to illustrate how to encode such

prior knowledge using linear constraints.

A noun in plural form cannot have a (or an)

as its article. That two Boolean variables Z1 and
Z2 are mutually exclusive can be handled using a
simple inequality Z1 + Z2 ≤ 1. Thus, the fol-
lowing inequality correctly enforces article-noun

agreement:

ZaArt,p1 + Z
plural
Noun,p2

≤ 1, (11)

where the article at p1 modi�es the noun at p2.

4.3 Dependency Relation Constraints

Another set of constraints involves dependency

relations, including subject-verb relation and

determiner-noun relation. Speci�cally, for a noun

n at position p, we check the word w related to n
via a child-parent or parent-child relation. Ifw be-
longs to a set of verbs or determiners (are, were,

these, all) that takes a plural noun, then the noun

n is required to be in plural form by adding the
following constraint:

Z
plural
Noun,p = 1. (12)

Similarly, if a noun n at position p is required to
be in singular form due to subject-verb relation

or determiner-noun relation, we add the following

constraint:

Z
singular
Noun,p = 1. (13)

5 Inference with Second Order Variables

5.1 Motivation and De�nition

To relax the decomposable assumption in Section

3.2, instead of treating each correction separately,

one can combine multiple corrections into a single

correction by introducing higher order variables.

1460



Consider the sentence A cat sat on the mat.

When measuring the gain due to Z
plural
Noun,2 = 1

(change cat to cats), the weight wNoun,2,plural is
likely to be small since A cats will get a low lan-

guage model score, a low article classi�er con-

�dence score, and a low noun number classi�er

con�dence score. Similarly, the weight wArt,1,ɛ of
ZɛArt,1 (delete article A) is also likely to be small
because of the missing article. Thus, if one con-

siders the two corrections separately, they are both

unlikely to appear in the �nal corrected output.

However, the correction from A cat sat on the

mat. toCats sat on the mat. should be a reasonable

candidate, especially if the context indicates that

there are many cats (more than one) on the mat.

Due to treating corrections separately, it is dif�cult

to deal with multiple interacting corrections with

only �rst order variables.

In order to include the correction ɛ Cats, one
can use a new set of variables, second order vari-

ables. To keep symbols clear, let Z = {Zu|Zu =
Zkl,p, ∀l, p, k} be the set of �rst order variables, and
wu = wl,p,k be the weight of Zu = Z

k
l,p. De�ne a

second order variable Xu,v:

Xu,v = Zu ∧ Zv, (14)

where Zu and Zv are �rst order variables:

Zu , Zk1l1,p1 , Zv , Z
k2
l2,p2

. (15)

The de�nition of Xu,v states that a second order
variable is set to 1 if and only if its two compo-
nent �rst order variables are both set to 1. Thus, it
combines two corrections into a single correction.

In the above example, a second order variable is

introduced:

Xu,v = Z
ɛ
Art,1 ∧ ZpluralNoun,2,

s
Xu,v−−−→ s′ = Cats sat on the mat .

Similar to �rst order variables, let wu,v be the
weight of Xu,v. Note that de�nition (2) only de-
pends on the output sentence s′, and the weight of
the second order variable wu,v can be de�ned in
the same way:

wu,v = νLMh(s
′,LM) +

∑

t∈E
λtf(s

′, t)

+
∑

t∈E
µtg(s

′, t). (16)

5.2 ILP with Second Order Variables

A set of new constraints is needed to enforce con-

sistency between the �rst and second order vari-

ables. These constraints are the linearization of

de�nition (14) of Xu,v:

Xu,v = Zu ∧ Zv ⇔
Xu,v ≤ Zu
Xu,v ≤ Zv
Xu,v ≥ Zu + Zv − 1

(17)

A new objective function combines the weights

from both �rst and second order variables:

max
∑

l,p,k

wl,p,kZ
k
l,p +

∑

u,v

wu,vXu,v. (18)

In our experiments, due to noisy data, some

weights of second order variables are small, even

if both of its �rst order variables have large

weights and satisfy all prior knowledge con-

straints. They will affect ILP proposing good cor-

rections. We �nd that the performance will be bet-

ter if we change the weights of second order vari-

ables to w′u,v, where

w′u,v , max{wu,v, wu, wv}. (19)

Putting them together, (20)-(25) is an ILP for-

mulation using second order variables, whereX is
the set of all second order variables which will be

explained in the next subsection.

max
∑

l,p,k

wl,p,kZ
k
l,p +

∑

u,v

w′u,vXu,v (20)

s.t.
∑

k

Zkl,p = 1, ∀ applicable l, p (21)

Xu,v ≤ Zu, (22)
Xu,v ≤ Zv, (23)
Xu,v ≥ Zu + Zv − 1, ∀Xu,v ∈ X (24)
Xu,v, Z

k
l,p ∈ {0, 1} (25)

5.3 Complexity and Variable Selection

Using the notation in section 3.5, the num-

ber of second order variables is O(|Z|2) =
O(K2|s|2C(l∗)2) and the number of constraints is
O(K2|s|2C(l∗)2). More generally, for variables
with higher order h ≥ 2, the number of variables
(and constraints) is O(Kh|s|hC(l∗)h).
Note that both the number of variables and the

number of constraints increase exponentially with

increasing variable order. In practice, a small

subset of second order variables is suf�cient to

1461



Data set Sentences Words Edits

Dev set 939 22,808 1,264

Test set 722 18,790 1,057

Table 3: Overview of the HOO 2011 data sets.

Corrections are called edits in the HOO 2011

shared task.

achieve good performance. For example, noun

number corrections are only coupled with nearby

article corrections, and have no connection with

distant or other types of corrections.

In this work, we only introduce second or-

der variables that combine article corrections and

noun number corrections. Furthermore, we re-

quire that the article and the noun be in the same

noun phrase. The set X of second order variables
in Equation (24) is de�ned as follows:

X ={Xu,v = Zu ∧ Zv|l1 = Art, l2 = Noun,
s[p1], s[p2] are in the same noun phrase},

where l1, l2, p1, p2 are taken from Equation (15).

6 Experiments

Our experiments mainly focus on two aspects:

how our ILP approach performs compared to other

grammatical error correction systems; and how

the different constraints and the second order vari-

ables affect the ILP performance.

6.1 Evaluation Corpus and Metric

We follow the evaluation setup in the HOO 2011

shared task on grammatical error correction (Dale

and Kilgarriff, 2011). The development set and

test set in the shared task consist of conference and

workshop papers taken from the Association for

Computational Linguistics (ACL). Table 3 gives

an overview of the data sets.
System performance is measured by precision,

recall, and F measure:

P =
# true edits

# system edits
, R =

# true edits

# gold edits
, F =

2PR

P + R
.

(26)

The dif�culty lies in how to generate the system

edits from the system output. In the HOO 2011

shared task, participants can submit system edits

directly or the corrected plain-text system output.

In the latter case, the of�cial HOO scorer will ex-

tract system edits based on the original (ungram-

matical) input text and the corrected system output

text, using GNU Wdiff3.

Consider an input sentence The data is simi-

lar with test set. taken from (Dahlmeier and Ng,

2012a). The gold-standard edits are with → to and
ɛ → the. That is, the grammatically correct sen-
tence should be The data is similar to the test set.

Suppose the corrected output of a system to be

evaluated is exactly this perfectly corrected sen-

tence The data is similar to the test set. However,

the of�cial HOO scorer using GNUWdiff will au-

tomatically extract only one system edit with → to
the for this system output. Since this single system

edit does not match any of the two gold-standard

edits, the HOO scorer returns an F measure of 0,
even though the system output is perfectly correct.

In order to overcome this problem, the Max-

Match (M2) scorer was proposed in (Dahlmeier
and Ng, 2012b). Given a set of gold-standard ed-

its, the original (ungrammatical) input text, and

the corrected system output text, the M2 scorer
searches for the system edits that have the largest

overlap with the gold-standard edits. For the above

example, the system edits automatically deter-

mined by the M2 scorer are identical to the gold-
standard edits, resulting in an F measure of 1 as we
would expect. We will use the M2 scorer in this
paper to determine the best system edits. Once the

system edits are found, P , R, and F are computed
using the standard de�nition (26).

6.2 ILP Con�guration

6.2.1 Variables

The �rst order variables are given in Table 1. If

the inde�nite article correction a is chosen, then

the �nal choice between a and an is decided by a

rule-based post-processing step. For each prepo-

sition error variable ZkPrep,p, the correction k is re-
stricted to a pre-de�ned confusion set of prepo-

sitions which depends on the observed preposi-

tion at position p. For example, the confusion
set of on is { at, for, in, of }. The list of prepo-
sitions corrected by our system is about, among,

at, by, for, in, into, of, on, over, to, under, with,

and within. Only selected positions in a sentence

(determined by rules) undergo punctuation correc-

tion. The spelling correction candidates are given

by a spell checker. We used GNU Aspell4 in our

work.

3http://www.gnu.org/software/wdiff/
4http://aspell.net

1462



6.2.2 Weights

As described in Section 3.2, the weight of each

variable is a linear combination of the language

model score, three classi�er con�dence scores,

and three classi�er disagreement scores. We use

the Web 1T 5-gram corpus (Brants and Franz,

2006) to compute the language model score for

a sentence. Each of the three classi�ers (article,

preposition, and noun number) is trained with the

multi-class con�dence weighted algorithm (Cram-

mer et al., 2009). The training data consists of all

non-OCR papers in the ACL Anthology5, minus

the documents that overlap with the HOO 2011

data set. The features used for the classi�ers fol-

low those in (Dahlmeier and Ng, 2012a), which

include lexical and part-of-speech n-grams, lexi-

cal head words, web-scale n-gram counts, depen-

dency heads and children, etc. Over 5 million

training examples are extracted from the ACL An-

thology for use as training data for the article and

noun number classi�ers, and over 1 million train-

ing examples for the preposition classi�er.

Finally, the language model score, classi�er

con�dence scores, and classi�er disagreement

scores are normalized to take values in [0, 1],
based on the HOO 2011 development data. We use

the following values for the coef�cients: νLM = 1
(language model); λt = 1 (classi�er con�dence);
and µt = −1 (classi�er disagreement).

6.2.3 Constraints

In Section 4, three sets of constraints are in-

troduced: modi�cation count (MC), article-noun

agreement (ANA), and dependency relation (DR)

constraints. The values for the modi�cation count

parameters are set as follows: NArt = 3, NPrep =
2, NNoun = 2, and NSpell = 1.

6.3 Experimental Results

We compare our ILP approach with two other sys-

tems: the beam search decoder of (Dahlmeier and

Ng, 2012a) which achieves the best published per-

formance to date on the HOO 2011 data set, and

UI Run1 (Rozovskaya et al., 2011) which achieves

the best performance among all participating sys-

tems at the HOO 2011 shared task. The results are

given in Table 4.

The HOO 2011 shared task provides two sets of

gold-standard edits: the original gold-standard ed-

its produced by the annotator, and the of�cial gold-

5http://aclweb.org/anthology-new/

System Original Of�cial
P R F P R F

UI Run1 40.86 11.21 17.59 54.61 14.57 23.00
Beam search 30.28 19.17 23.48 33.59 20.53 25.48
ILP 20.54 27.93 23.67 21.99 29.04 25.03

Table 4: Comparison of three grammatical error

correction systems.

standard edits which incorporated corrections pro-

posed by the HOO 2011 shared task participants.

All three systems listed in Table 4 use the M2

scorer to extract system edits. The results of the

beam search decoder and UI Run1 are taken from

Table 2 of (Dahlmeier and Ng, 2012a).

Overall, ILP inference outperforms UI Run1 on

both the original and of�cial gold-standard edits,

and the improvements are statistically signi�cant

at the level of signi�cance 0.01. The performance

of ILP inference is also competitive with the beam

search decoder. The results indicate that a gram-

matical error correction system bene�ts from cor-

rections made at a whole sentence level, and that

joint correction of multiple error types achieves

state-of-the-art performance.

Table 5 provides the comparison of the beam

search decoder and ILP inference in detail. The

main difference between the two is that, except for

spelling errors, ILP inference gives higher recall

than the beam search decoder, while its precision

is lower. This indicates that ILP inference is more

aggressive in proposing corrections.

Next, we evaluate ILP inference in different

con�gurations. We only focus on article and noun

number error types. Table 6 shows the perfor-

mance of ILP in different con�gurations. From

the results, MC and DR constraints improve pre-

cision, indicating that the two constraints can help

to restrict the number of erroneous corrections. In-

cluding second order variables gives the best F
measure, which supports our motivation for intro-

ducing higher order variables.

Adding article-noun agreement constraints

(ANA) slightly decreases performance. By exam-

ining the output, we �nd that although the overall

performance worsens slightly, the agreement re-

quirement is satis�ed. For example, for the input

We utilize search engine to . . . , the output without

ANA isWe utilize a search engines to . . . but with

ANA is We utilize the search engines to . . . , while

the only gold edit inserts a.

1463



Original Of�cial

Error type Beam search ILP Beam search ILP

P R F P R F P R F P R F
Spelling 36.84 0.69 1.35 60.00 0.59 1.17 36.84 0.66 1.30 60.00 0.57 1.12
+ Article 19.84 12.59 15.40 18.54 14.75 16.43 22.45 13.72 17.03 20.37 15.61 17.68
+ Preposition 22.62 14.26 17.49 17.61 18.58 18.09 24.84 15.14 18.81 19.24 19.68 19.46
+ Punctuation 24.27 18.09 20.73 20.52 23.50 21.91 27.13 19.58 22.75 22.49 24.98 23.67
+ Noun number 30.28 19.17 23.48 20.54 27.93 23.67 33.59 20.53 25.48 21.99 29.04 25.03

Table 5: Comparison of the beam search decoder and ILP inference. ILP is equipped with all constraints

(MC, ANA, DR) and default parameters. Second order variables related to article and noun number error

types are also used in the last row.

Setting Original Of�cial

P R F P R F

Art+Nn, 1st ord. 17.19 19.37 18.22 18.59 20.44 19.47
+ MC 17.87 18.49 18.17 19.23 19.39 19.31

+ ANA 17.78 18.39 18.08 19.04 19.11 19.07

+ DR 17.95 18.58 18.26 19.23 19.30 19.26

+ 2nd ord. 18.75 18.88 18.81 20.04 19.58 19.81

Table 6: The effects of different constraints and second order variables.

7 Conclusion

In this paper, we model grammatical error correc-

tion as a joint inference problem. The inference

problem is solved using integer linear program-

ming. We provide three sets of constraints to in-

corporate additional linguistic knowledge, and in-

troduce a further extension with second order vari-

ables. Experiments on the HOO 2011 shared task

show that ILP inference achieves state-of-the-art

performance on grammatical error correction.

Acknowledgments

This research is supported by the Singapore Na-

tional Research Foundation under its International

Research Centre @ Singapore Funding Initiative

and administered by the IDM Programme Of�ce.

References

Francis Bond and Satoru Ikehara. 1996. When and
how to disambiguate? countability in machine trans-
lation. In Proceedings of the International Seminar
on Multimodal Interactive Disambiguation.

Francis Bond, Kentaro Ogura, and Tsukasa Kawaoka.
1995. Noun phrase reference in Japanese-to-English
machine translation. In Proceedings of the 6th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation.

Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1.1. Technical report, Google
Research.

Koby Crammer, Mark Dredze, and Alex Kulesza.
2009. Multi-class con�dence weighted algorithms.
In Proceedings of EMNLP.

Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of ACL.

Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
Proceedings of EMNLP.

Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In Pro-
ceedings of NAACL.

Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th EuropeanWorkshop on Natural Lan-
guage Generation.

Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Seventh Workshop on Innovative Use of
NLP for Building Educational Applications, pages
5462.

Michael Gamon. 2010. Using mostly native data to
correct errors in learners' writing. In Proceedings of
NAACL.

1464



Michael Gamon. 2011. High-order sequence model-
ing for language learner error detection. In Proceed-
ings of the Sixth Workshop on Innovative Use of NLP
for Building Educational Applications.

Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2).

Julia Heine. 1998. De�niteness predictions for
Japanese noun phrases. In Proceedings of ACL-
COLING.

Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In Proceedings of AAAI.

Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of EMNLP.

Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of ACL-
IJCNLP.

Masaki Murata and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in
Japanese sentences for machine translation into En-
glish. In Proceedings of the 5th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation.

Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. In Proceedings of ACL.

Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of IJCAI.

Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proceedings of EMNLP.

Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.

Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of ACL.

Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois system in
HOO text correction shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.

Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of ACL.

Joel R. Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In Proceedings of COLING.

Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of ACL.

1465


