



















































Learning Compressed Sentence Representations for On-Device Text Processing


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 107‚Äì116
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

107

Learning Compressed Sentence Representations
for On-Device Text Processing

Dinghan Shen1‚àó, Pengyu Cheng1‚àó , Dhanasekar Sundararaman1

Xinyuan Zhang1, Qian Yang1, Meng Tang3, Asli Celikyilmaz2, Lawrence Carin1
1 Duke University 2 Microsoft Research 3 Stanford University

dinghan.shen@duke.edu

Abstract

Vector representations of sentences, trained
on massive text corpora, are widely used as
generic sentence embeddings across a variety
of NLP problems. The learned representa-
tions are generally assumed to be continuous
and real-valued, giving rise to a large memory
footprint and slow retrieval speed, which hin-
ders their applicability to low-resource (mem-
ory and computation) platforms, such as mo-
bile devices. In this paper, we propose four
different strategies to transform continuous
and generic sentence embeddings into a bi-
narized form, while preserving their rich se-
mantic information. The introduced methods
are evaluated across a wide range of down-
stream tasks, where the binarized sentence em-
beddings are demonstrated to degrade perfor-
mance by only about 2% relative to their con-
tinuous counterparts, while reducing the stor-
age requirement by over 98%. Moreover, with
the learned binary representations, the seman-
tic relatedness of two sentences can be evalu-
ated by simply calculating their Hamming dis-
tance, which is more computational efficient
compared with the inner product operation be-
tween continuous embeddings. Detailed anal-
ysis and case study further validate the effec-
tiveness of proposed methods.

1 Introduction

Learning general-purpose sentence representa-
tions from large training corpora has received
widespread attention in recent years. The learned
sentence embeddings can encapsulate rich prior
knowledge of natural language, which has been
demonstrated to facilitate a variety of downstream
tasks (without fine-tuning the encoder weights).
The generic sentence embeddings can be trained
either in an unsupervised manner (Kiros et al.,
2015; Hill et al., 2016; Jernite et al., 2017; Gan

‚àó Equal contribution.

et al., 2017; Logeswaran and Lee, 2018; Pagliar-
dini et al., 2018), or with supervised tasks such
as paraphrase identification (Wieting et al., 2016),
natural language inference (Conneau et al., 2017),
discourse relation classification (Nie et al., 2017),
machine translation (Wieting and Gimpel, 2018),
etc.

Significant effort has been devoted to design-
ing better training objectives for learning sentence
embeddings. However, prior methods typically as-
sume that the general-purpose sentence represen-
tations are continuous and real-valued. However,
this assumption is sub-optimal from the following
perspectives: i) the sentence embeddings require
large storage or memory footprint; ii) it is com-
putationally expensive to retrieve semantically-
similar sentences, since every sentence represen-
tation in the database needs to be compared, and
the inner product operation is computationally in-
volved. These two disadvantages hinder the appli-
cability of generic sentence representations to mo-
bile devices, where a relatively tiny memory foot-
print and low computational capacity are typically
available (Ravi and Kozareva, 2018).

In this paper, we aim to mitigate the above is-
sues by binarizing the continuous sentence em-
beddings. Consequently, the embeddings require
much smaller footprint, and similar sentences can
be obtained by simply selecting those with clos-
est binary codes in the Hamming space (Kiros and
Chan, 2018). One simple idea is to naively bi-
narize the continuous vectors by setting a hard
threshold. However, we find that this strategy
leads to significant performance drop in the em-
pirical results. Besides, the dimension of the bi-
nary sentence embeddings cannot be flexibly cho-
sen with this strategy, further limiting the practice
use of the direct binarization method.

In this regard, we propose three alternative
strategies to parametrize the transformation from



108

pre-trained generic continuous embeddings to
their binary forms. Our exploration spans from
simple operations, such as a random projection,
to deep neural network models, such as a regu-
larized autoencoder. Particularly, we introduce
a semantic-preserving objective, which is aug-
mented with the standard autoenoder architec-
ture to encourage abstracting informative binary
codes. InferSent (Conneau et al., 2017) is em-
ployed as the testbed sentence embeddings in our
experiments, but the binarization schemes pro-
posed here can easily be extended to other pre-
trained general-purpose sentence embeddings. We
evaluate the quality of the learned general-purpose
binary representations using the SentEval toolkit
(Conneau et al., 2017). It is observed that the in-
ferred binary codes successfully maintain the se-
mantic features contained in the continuous em-
beddings, and only lead to around 2% perfor-
mance drop on a set of downstream NLP tasks,
while requiring merely 1.5% memory footprint of
their continuous counterparts.

Moreover, on several sentence matching bench-
marks, we demonstrate that the relatedness be-
tween a sentence pair can be evaluated by sim-
ply calculating the Hamming distance between
their binary codes, which perform on par with
or even superior than measuring the cosine sim-
ilarity between continuous embeddings (see Ta-
ble 1). Note that computing the Hamming distance
is much more computationally efficient than the
inner product operation in a continuous space. We
further perform a K-nearest neighbor sentence re-
trieval experiment on the SNLI dataset (Bowman
et al., 2015), and show that those semantically-
similar sentences can indeed be efficiently re-
trieved with off-the-shelf binary sentence repre-
sentations. Summarizing, our contributions in this
paper are as follows:

i) to the best of our knowledge, we con-
duct the first systematic exploration on learn-
ing general-purpose binarized (memory-efficient)
sentence representations, and four different strate-
gies are proposed;

ii) an autoencoder architecture with a carefully-
designed semantic-preserving loss exhibits strong
empirical results on a set of downstream NLP
tasks;

iii) more importantly, we demonstrate, on sev-
eral sentence-matching datasets, that simply eval-
uating the Hamming distance over binary repre-

sentations performs on par or even better than cal-
culating the cosine similarity between their contin-
uous counterparts (which is less computationally-
efficient).

2 Related Work

Sentence representations pre-trained from a large
amount of data have been shown to be effective
when transferred to a wide range of downstream
tasks. Prior work along this line can be roughly
divided into two categories: i) pre-trained models
that require fine-tuning on the specific transferring
task (Dai and Le, 2015; Ruder and Howard, 2018;
Radford et al., 2018; Devlin et al., 2018; Cer et al.,
2018); ii) methods that extract general-purpose
sentence embeddings, which can be effectively
applied to downstream NLP tasks without fine-
tuning the encoder parameters (Kiros et al., 2015;
Hill et al., 2016; Jernite et al., 2017; Gan et al.,
2017; Adi et al., 2017; Logeswaran and Lee, 2018;
Pagliardini et al., 2018; Tang and de Sa, 2018).
Our proposed methods belong to the second cat-
egory and provide a generic and easy-to-use en-
coder to extract highly informative sentence rep-
resentations. However, our work is unique since
the embeddings inferred from our models are bi-
narized and compact, and thus possess the advan-
tages of small memory footprint and much faster
sentence retrieval.

Learning memory-efficient embeddings with
deep neural networks has attracted substantial at-
tention recently. One general strategy towards this
goal is to extract discrete or binary data repre-
sentations (Jang et al., 2016; Shu and Nakayama,
2017; Dai et al., 2017; Chen et al., 2018; Shen
et al., 2018; Tissier et al., 2019). Binarized em-
beddings are especially attractive because they are
more memory-efficient (relative to discrete em-
beddings), and they also enjoy the advantages of
fast retrieval based upon a Hamming distance cal-
culation. Previous work along this line in NLP has
mainly focused on learning compact representa-
tions at the word-level (Shu and Nakayama, 2017;
Chen et al., 2018; Tissier et al., 2019), while much
less effort has been devoted to extracting binarized
embeddings at the sentence-level. Our work aims
to bridge this gap, and serves as an initial attempt
to facilitate the deployment of state-of-the-art sen-
tence embeddings on on-device mobile applica-
tions.

Our work is also related to prior research on
semantic hashing, which aims to learn binary



109

text embeddings specifically for the information
retrieval task (Salakhutdinov and Hinton, 2009;
Zhang et al., 2010; Wang et al., 2014; Xu et al.,
2015; Shen et al., 2018). However, these methods
are typically trained and evaluated on documents
that belong to a specific domain, and thus cannot
serve as generic binary sentence representation ap-
plicable to a wide variety of NLP taks. In contrast,
our model is trained on large corpora and seeks
to provide general-purpose binary representations
that can be leveraged for various application sce-
narios.

3 Proposed Approach

We aim to produce compact and binarized repre-
sentations from continuous sentence embeddings,
and preserve the associated semantic information.
Let x and f denote, respectively, an input sentence
and the function defined by a pre-trained general-
purpose sentence encoder. Thus, f(x) represents
the continuous embeddings extracted by the en-
coder. The goal of our model is to learn a universal
transformation g that can convert f(x) to highly
informative binary sentence representations, i.e.,
g(f(x)), which can be used as generic features for
a collection of downstream tasks. We explore four
strategies to parametrize the transformation g.

3.1 Hard Threshold

We use h and b to denote the continuous and bi-
nary sentence embeddings, respectively, and L de-
notes the dimension of h. The first method to bi-
narize the continuous representations is to simply
convert each dimension to either 0 or 1 based on
a hard threshold. This strategy requires no train-
ing and directly operates on pre-trained continu-
ous embeddings. Suppose s is the hard threshold,
we have, for i = 1, 2, ......, L:

b(i) = 1h(i)>s =
sign(h(i) ‚àí s) + 1

2
, (1)

One potential issue of this direct binarization
method is that the information contained in the
continuous representations may be largely lost,
since there is no training objective encouraging the
preservation of semantic information in the pro-
duced binary codes (Shen et al., 2018). Another
disadvantage is that the length of the resulting bi-
nary code must be the same as the original contin-
uous representation, and can not be flexibly cho-
sen. In practice, however, we may want to learn

shorter binary embeddings to save more memory
footprint or computation.

3.2 Random Projection
To tackle the limitation of the above direct bina-
rization method, we consider an alternative strat-
egy that requires no training either: simply apply-
ing a random projection over the pre-trained con-
tinuous representations. Wieting and Kiela (2018)
has shown that random sentence encoders can ef-
fectively construct universal sentence embeddings
from word vectors, while possessing the flexibility
of adaptively altering the embedding dimensions.
Here, we are interested in exploring whether a ran-
dom projection would also work well while trans-
forming continuous sentence representations into
their binary counterparts.

We randomly initialize a matrix W ‚àà RD√óL,
whereD denotes the dimension of the resulting bi-
nary representations. Inspired by the standard ini-
tialization heuristic employed in (Glorot and Ben-
gio, 2010; Wieting and Kiela, 2018), the values
of the matrix are initialized as sampled uniformly.
For i = 1, 2, . . . , D and j = 1, 2, . . . , L, we have:

Wi,j ‚àº Uniform(‚àí
1‚àö
D
,

1‚àö
D

), (2)

After converting the continuous sentence embed-
dings to the desired dimension D with the ma-
trix randomly initialized above, we further apply
the operation in (1) to binarize it into the dis-
crete/compact form. The dimension D can be set
arbitrarily with this approach, which is easily ap-
plicable to any pre-trained sentence embeddings
(since no training is needed). This strategy is re-
lated to the Locality-Sensitive Hashing (LSH) for
inferring binary embeddings (Van Durme and Lall,
2010).

3.3 Principal Component Analysis
We also consider an alternative strategy to adap-
tively choose the dimension of the resulting bi-
nary representations. Specifically, Principal Com-
ponent Analysis (PCA) is utilized to reduce the
dimensionality of pre-trained continuous embed-
dings.

Given a set of sentences {xi}Ni=1 and their corre-
sponding continuous embeddings {hi}Ni=1 ‚äÇ RL,
we learn a projection matrix to reduce the embed-
ding dimensions while keeping the embeddings
distinct as much as possible. After centralizing the
embeddings as hi = hi ‚àí 1N

‚àëN
i=1 hi, the data, as



110

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

0
<latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit><latexit sha1_base64="JxFURWUH6R+vJirwMs9sg7WrHjU=">AAAB+HicbVBNSwMxEJ2tX7V+VT16CRbBU8mKoMeiF49V7Ae0S8mm2TY0yS5JVqhL/4FXPXsTr/4bj/4T03YPtvXBwOO9GWbmhYngxmL87RXW1jc2t4rbpZ3dvf2D8uFR08SppqxBYxHrdkgME1yxhuVWsHaiGZGhYK1wdDv1W09MGx6rRztOWCDJQPGIU2Kd9IBLvXIFV/EMaJX4OalAjnqv/NPtxzSVTFkqiDEdHyc2yIi2nAo2KXVTwxJCR2TAOo4qIpkJstmlE3TmlD6KYu1KWTRT/05kRBozlqHrlMQOzbI3Ff/zOqmNroOMqyS1TNH5oigVyMZo+jbqc82oFWNHCNXc3YrokGhCrQtnYUsoJy4TfzmBVdK8qPq46t9fVmo3eTpFOIFTOAcfrqAGd1CHBlCI4AVe4c179t69D+9z3lrw8pljWID39QtO0pNe</latexit>

1<latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit><latexit sha1_base64="TLhiafyz+7kVW7hdyQ94ij177M0=">AAAB+XicbVBNSwMxEJ2tX3X9qnr0EiyCp7IRQY9FLx4r2lpol5JNs21okl2SrFCW/gSvevYmXv01Hv0npu0ebOuDgcd7M8zMi1LBjQ2Cb6+0tr6xuVXe9nd29/YPKodHLZNkmrImTUSi2xExTHDFmpZbwdqpZkRGgj1Fo9up//TMtOGJerTjlIWSDBSPOSXWSQ/Y93uValALZkCrBBekCgUavcpPt5/QTDJlqSDGdHCQ2jAn2nIq2MTvZoalhI7IgHUcVUQyE+azUyfozCl9FCfalbJopv6dyIk0Ziwj1ymJHZplbyr+53UyG1+HOVdpZpmi80VxJpBN0PRv1OeaUSvGjhCqubsV0SHRhFqXzsKWSE5cJng5gVXSuqjhoIbvL6v1myKdMpzAKZwDhiuowx00oAkUBvACr/Dm5d679+F9zltLXjFzDAvwvn4BhVSTcw==</latexit>

ùíâ ùíÉ

Threshold

ùíâ
ùíÉ

Threshold

ùíÉ#

Random Projection
or
PCA

Encoder Decoder

ùíâ
ùíÉùíÉ#

ùíâ$

Threshold

(a) (b) (c)

Semantic-preserving Loss

Figure 1: Proposed model architectures: (a) direct binarization with a hard threshold s; (b) reducing the dimension-
ality with either a random projection or PCA, followed by a binarization step; (c) an encoding-decoding framework
with an additional semantic-preserving loss.

a matrix H = (h1, h2, . . . , hN ), has the singular
value decomposition (SVD):

H = UŒõV T ,

where Œõ is an L√óN matrix with descending sin-
gular values of X on its diagonal, with U and V
orthogonal matrices. Then the correlation matrix
can be written as: HHT = UŒõ2UT . Assume that
the diagonal matrix Œõ2 = diag(Œª1, Œª2, . . . , ŒªL)
has descending elements Œª1 ‚â• Œª2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• ŒªL ‚â•
0. We select first D rows of U as our projection
matrix W = U1:D, then the correlation matrix
of WH is WHHTW T = diag(Œª1, Œª2, . . . , ŒªD),
which indicates that the embeddings are projected
to D independent and most distinctive axes.

After projecting continuous embeddings to a
representative lower dimensional space, we apply
the hard threshold function at the position 0 to ob-
tain the binary representations (since the embed-
dings are zero-centered).

3.4 Autoencoder Architecture
The methods proposed above suffer from the com-
mon issue that the model objective does not ex-
plicitly encourage the learned binary codes to re-
tain the semantic information of the original con-
tinuous embeddings, and a separate binarization
step is employed after training. To address this
shortcoming, we further consider an autoencoder
architecture, that leverages the reconstruction loss
to hopefully endow the learned binary representa-
tions with more information. Specifically, an en-
coder network is utilized to transform the contin-
uous into a binary latent vector, which is then re-
constructed back with a decoder network.

For the encoder network, we use a matrix op-
eration, followed by a binarization step, to extract
useful features (similar to the random projection

setup). Thus, for i = 1, 2, . . . , D, we have:

b(i) = 1œÉ(Wi¬∑h+k(i))>s(i)

=
sign(œÉ(Wi ¬∑ h+ k(i))‚àí s(i)) + 1

2
, (3)

where k is the bias term and k(i) corresponds to the
i-th element of k. s(i) denotes the threshold deter-
mining whether the i-th bit is 0 or 1. During train-
ing, we may use either deterministic or stochastic
binarization upon the latent variable. For the de-
terministic case, s(i) = 0.5 for all dimensions; in
the stochastic case, s(i) is uniformly sampled as:
s(i) ‚àº Uniform(0, 1). We conduct an empirical
comparison between these two binarization strate-
gies in Section 4.

Prior work have shown that linear decoders
are favorable for learning binary codes under the
encoder-decoder framework (Carreira-PerpinaÃÅn
and Raziperchikolaei, 2015; Dai et al., 2017; Shen
et al., 2018). Inspired by these results, we employ
a linear transformation to reconstruct the original
continuous embeddings from the binary codes:

hÃÇ(i) = W ‚Ä≤i ¬∑ b+ k‚Ä≤
(i)
, (4)

where W ‚Ä≤ and k‚Ä≤ are weight and bias term respec-
tively, which are learned. The mean square error
between h and hÃÇ is employed as the reconstruction
loss:

Lrec =
1

D

D‚àë
i=1

(h(i) ‚àí hÃÇ(i))2, (5)

This objective imposes the binary vector b to en-
code more information from h (leading to smaller
reconstruction error). Straight-through (ST) esti-
mator (Hinton, 2012) is utilized to estimate the
gradients for the binary variable. The autoencoder



111

model is optimized by minimizing the reconstruc-
tion loss for all sentences. After training, the en-
coder network is leveraged as the transformation
to convert the pre-trained continuous embeddings
into the binary form.

3.4.1 Semantic-preserving Regularizer

Although the reconstruction objective can help the
binary variable to endow with richer semantics,
there is no loss that explicitly encourages the bi-
nary vectors to preserve the similarity information
contained in the original continuous embeddings.
Consequently, the model may lead to small recon-
struction error but yield sub-optimal binary repre-
sentations (Tissier et al., 2019). To improve the
semantic-preserving property of the inferred bi-
nary embeddings, we introduce an additional ob-
jective term.

Consider a triple group of sentences
(xŒ±, xŒ≤, xŒ≥), whose continuous embeddings
are (hŒ±, hŒ≤, hŒ≥), respectively. Suppose that the
cosine similarity between hŒ± and hŒ≤ is larger
than that between hŒ≤ and hŒ≥ , then it is desirable
that the Hamming distance between bŒ± and bŒ≤
should be smaller than that between bŒ≤ and bŒ≥
(notably, both large cosine similarity and small
Hamming distance indicate that two sentences are
semantically-similar).

Let dc(¬∑, ¬∑) and dh(¬∑, ¬∑) denote the cosine sim-
ilarity and Hamming distance (in the continuous
and binary embedding space), respectively. De-
fine lŒ±,Œ≤,Œ≥ as an indicator such that, lŒ±,Œ≤,Œ≥ = 1 if
dc(hŒ±, hŒ≤) ‚â• dc(hŒ≤, hŒ≥), and lŒ±,Œ≤,Œ≥ = ‚àí1 other-
wise. The semantic-preserving regularizer is then
defined as:

Lsp =
‚àë
Œ±,Œ≤,Œ≥

max{0, lŒ±,Œ≤,Œ≥ [dh(bŒ±, bŒ≤)‚àí dh(bŒ≤, bŒ≥)]},

(6)

By penalizing Lsp, the learned transformation
function g is explicitly encouraged to retain the se-
mantic similarity information of the original con-
tinuous embeddings. Thus, the entire objective
function to be optimized is:

L = Lrec + ŒªspLsp, (7)

where Œªsp controls the relative weight between the
reconstruction loss (Lrec) and semantic-preserving
loss (Lsp).

3.5 Discussion

Another possible strategy is to directly train the
general-purpose binary embeddings from scratch,
i.e., jointly optimizing the continuous embed-
dings training objective and continuous-to-binary
parameterization. However, our initial attempts
demonstrate that this strategy leads to inferior em-
pirical results. This observation is consistent with
the results reported in (Kiros and Chan, 2018).
Specifically, a binarization layer is directly ap-
pended over the InferSent architecture (Conneau
et al., 2017) during training, which gives rise to
much larger drop in terms of the embeddings‚Äô
quality (we have conducted empirical compar-
isons with (Kiros and Chan, 2018) in Table 1).
Therefore, here we focus on learning universal bi-
nary embeddings based on pretained continuous
sentence representations.

4 Experimental setup

4.1 Pre-trained Continuous Embeddings

Our proposed model aims to produce highly in-
formative binary sentence embeddings based upon
pre-trained continuous representations. In this pa-
per, we utilize InferSent (Conneau et al., 2017)
as the continuous embeddings (given its effective-
ness and widespread use). Note that all four pro-
posed strategies can be easily extended to other
pre-trained general-purpose sentence embeddings
as well.

Specifically, a bidirectional LSTM architecture
along with a max-pooling operation over the hid-
den units is employed as the sentence encoder, and
the model parameters are optimized on the natu-
ral language inference tasks, i.e., Standford Nat-
ural Language Inference (SNLI) (Bowman et al.,
2015) and Multi-Genre Natural Language Infer-
ence (MultiNLI) datasets (Williams et al., 2017).

4.2 Training Details

Our model is trained using Adam (Kingma and Ba,
2014), with a value 1 √ó 10‚àí5 as the learning rate
for all the parameters. The number of bits (i.e.,
dimension) of the binary representation is set as
512, 1024, 2048 or 4096, and the best choice for
each model is chosen on the validation set, and
the corresponding test results are presented in Ta-
ble 1. The batch size is chosen as 64 for all model
variants. The hyperparameter over Œªsp is selected
from {0.2, 0.5, 0.8, 1} on the validation set, and
0.8 is found to deliver the best empirical results.



112

Model Dim MR CR SUBJ MPQA SST STS14 STSB SICK-R MRPC
Continuous (dense) sentence embeddings

fastText-BoV 300 78.2 80.2 91.8 88.0 82.3 .65/.63 58.1/59.0 0.698 67.9/74.3
SkipThought 4800 76.5 80.1 93.6 87.1 82.0 .29/.35 41.0/41.7 0.595 57.9/66.6

SkipThought-LN 4800 79.4 83.1 93.7 89.3 82.9 .44/.45 - - -
InferSent-FF 4096 79.7 84.2 92.7 89.4 84.3 .68/.66 55.6/56.2 0.612 67.9/73.8
InferSent-G 4096 81.1 86.3 92.4 90.2 84.6 .68/.65 70.0/68.0 0.719 67.4/73.2

Binary (compact) sentence embeddings
InferLite-short 256 73.7 81.2 83.2 86.2 78.4 0.61/- 63.4/63.3 0.597 61.7/70.1

InferLite-medium 1024 76.3 83.2 87.8 88.4 81.3 0.67/- 64.9/64.9 0.642 64.1/72.0
InferLite-long 4096 77.7 83.7 89.6 89.1 82.3 0.68/- 67.9/67.6 0.663 65.4/72.9

HT-binary 4096 76.6 79.9 91.0 88.4 80.6 .62/.60 55.8/53.6 0.652 65.6/70.4
Rand-binary 2048 78.7 82.7 90.4 88.9 81.3 .66/.63 65.1/62.3 0.704 65.7/70.8
PCA-binary 2048 78.4 84.5 90.7 89.4 81.0 .66/.65 63.7/62.8 0.518 65.0/ 69.7
AE-binary 2048 78.7 84.9 90.6 89.6 82.1 .68/.66 71.7/69.7 0.673 65.8/70.8

AE-binary-SP 2048 79.1 84.6 90.8 90.0 82.7 .69/.67 73.2/70.6 0.705 67.2/72.0

Table 1: Performance on the test set for 10 downstream tasks. The STS14, STSB and MRPC are evaluated with
Pearson and Spearman correlations, and SICK-R is measured with Pearson correlation. All other datasets are
evaluated with test accuracy. InferSent-G uses Glove (G) as the word embeddings, while InferSent-FF employs
FastText (F) embeddings with Fixed (F) padding. The empirical results of InferLite with different lengths of binary
embeddings, i.e., 256, 1024 and 4096, are considered.

The training with the autoencoder setup takes only
about 1 hour to converge, and thus can be readily
applicable to even larger datasets.

4.3 Evaluation

To facilitate comparisons with other baseline
methods, we use SentEval toolkit1 (Conneau
and Kiela, 2018) to evaluate the learned binary
(compact) sentence embeddings. Concretely, the
learned representations are tested on a series of
downstream tasks to assess their transferability
(with the encoder weights fixed), which can be cat-
egorized as follows:

‚Ä¢ Sentence classification, including sentiment
analysis (MR, SST), product reviews (CR),
subjectivity classification (SUBJ), opinion
polarity detection (MPQA) and question type
classification (TREC). A linear classifier is
trained with the generic sentence embeddings
as the input features. The default SentEval
settings is used for all the datasets.

‚Ä¢ Sentence matching, which comprises se-
mantic relatedness (SICK-R, STS14, STSB)
and paraphrase detection (MRPC). Particu-
larly, each pair of sentences in STS14 dataset
is associated with a similarity score from 0
to 5 (as the corresponding label). Hamming
distance between the binary representations
is directly leveraged as the prediction score
(without any classifier parameters).

1https://github.com/facebookresearch/SentEval

For the sentence matching benchmarks, to allow
fair comparison with the continuous embeddings,
we do not use the same classifier architecture in
SentEval. Instead, we obtain the predicted relat-
edness by directly computing the cosine similar-
ity between the continuous embeddings. Conse-
quently, there are no classifier parameters for both
the binary and continuous representations. The
same valuation metrics in SentEval(Conneau and
Kiela, 2018) are utilized for all the tasks. For
MRPC, the predictions are made by simply judg-
ing whether a sentence pair‚Äôs score is larger or
smaller than the averaged Hamming distance (or
cosine similarity).

4.4 Baselines
We consider several strong baselines to compare
with the proposed methods, which include both
continuous (dense) and binary (compact) repre-
sentations. For the continuous generic sentence
embeddings, we make comparisons with fastText-
BoV (Joulin et al., 2016), Skip-Thought Vectors
(Kiros et al., 2015) and InferSent (Conneau et al.,
2017). As to the binary embeddings, we consider
the binarized version of InferLite (Kiros and Chan,
2018), which, as far as we are concerned, is the
only general-purpose binary representations base-
line reported.

5 Experimental Results

We experimented with five model variants to
learn general-purpose binary embeddings: HT-
binary (hard threshold, which is selected from



113

{0, 0.01, 0.1} on the validation set), Rand-binary
(random projection), PCA-binary (reduce the di-
mensionality with principal component analysis),
AE-binary (autoencoder with the reconstruction
objective) and AE-binary-SP (autoencoder with
both the reconstruction objective and Semantic-
Preserving loss). Our code will be released to en-
courage future research.

5.1 Task transfer evaluation

We evalaute the binary sentence representations
produced by different methods with a set of trans-
ferring tasks. The results are shown in Table 1.
The proposed autoencoder architecture generally
demonstrates the best results. Especially while
combined with the semantic-preserving loss de-
fined in (7), AE-binary-SP exhibits higher perfor-
mance compared with a standard autoencoder. It
is worth noting that the Rand-binary and PCA-
binary model variants also show competitive per-
formance despite their simplicity. These strategies
are also quite promising given that no training is
required given the pre-trained continuous sentence
representations.

Another important result is that, the AE-binary-
SP achieves competitive results relative to the In-
ferSent, leading to only about 2% loss on most
datasets and even performing at par with InferSent
on several datasets, such as the MPQA and STS14
datasets. On the sentence matching tasks, the
yielded binary codes are evaluated by merely uti-
lizing the hamming distance features (as men-
tioned above). To allow fair comparison, we com-
pare the predicted scores with the cosine similarity
scores based upon the continuous representations
(there are no additional parameters for the classi-
fier). The binary codes brings out promising em-
pirical results relative to their continuous counter-
parts, and even slightly outperform InferSent on
the STS14 dataset.

We also found that our AE-binary-SP model
variant consistently demonstrate superior results
than the InferLite baselines, which optimize the
NLI objective directly over the binary representa-
tions. This may be attributed to the difficulty of
backpropagating gradients through discrete/binary
variables, and would be an interesting direction for
future research.

5.2 Nearest Neighbor Retrieval

Case Study One major advantage of binary sen-
tence representations is that the similarity of two

sentences can be evaluated by merely calculating
the hamming distance between their binary codes.
To gain more intuition regarding the semantic in-
formation encoded in the binary embeddings, we
convert all the sentences in the SNLI dataset into
continuous and binary vectors (with InferSent-G
and AE-binary-SP, respectively). The top-3 closet
sentences are retrieved based upon the correspond-
ing metrics, and the resulting samples are shown
in Table 2. It can be observed that the sentences
selected based upon the Hamming distance indeed
convey very similar semantic meanings. In some
cases, the results with binary codes are even more
reasonable compared with the continuous embed-
dings. For example, for the first query, all three
sentences in the left column relate to ‚Äúwatching
a movie‚Äù, while one of the sentences in the right
column is about ‚Äúsleeping‚Äù.

Retrieval Speed The bitwise comparison is
much faster than the element-wise multiplication
operation (between real-valued vectors) (Tissier
et al., 2019). To verify the speed improvement,
we sample 10000 sentence pairs from SNLI and
extract their continuous and binary embeddings
(with the same dimension of 4096), respectively.
We record the time to compute the cosine sim-
ilarity and hamming distance between the corre-
sponding representations. With our Python imple-
mentation, it takes 3.67¬µs and 288ns respectively,
indicating that calculating the Hamming distance
is over 12 times faster. Our implementation is
not optimized, and the running time of computing
Hamming distance can be further improved (to be
proportional to the number of different bits, rather
than the input length2).

5.3 Ablation Study

5.3.1 The effect of semantic-preserving loss

To investigate the importance of incorporating the
locality-sensitive regularizer, we select different
values of Œªsp (ranging from 0.0 to 1.0) and ex-
plore how the transfer results would change ac-
cordingly. The Œªsp controls the relative weight of
the semantic-preserving loss term. As shown in
Table 3, augmenting the semantic-preserving loss
consistently improves the quality of learned binary
embeddings, while the best test accuracy on the
MR dataset is obtained with Œªsp = 0.8.

2https://en.wikipedia.org/wiki/
Hamming_distance

https://en.wikipedia.org/wiki/Hamming_distance
https://en.wikipedia.org/wiki/Hamming_distance


114

Hamming Distance (binary embeddings) Cosine Similarity (continuous embeddings)
Query: Several people are sitting in a movie theater .

A group of people watching a movie at a theater . A group of people watching a movie at a theater .
A crowd of people are watching a movie indoors . A man is watching a movie in a theater .
A man is watching a movie in a theater . Some people are sleeping on a sofa in front of the television .

Query: A woman crossing a busy downtown street .
A lady is walking down a busy street . A woman walking on the street downtown .
A woman is on a crowded street . A lady is walking down a busy street .
A woman walking on the street downtown . A man and woman walking down a busy street .

Query: A well dressed man standing in front of piece of artwork .
A well dressed man standing in front of an abstract fence painting . A man wearing headphones is standing in front of a poster .
A man wearing headphones is standing in front of a poster . A man standing in front of a chalkboard points at a drawing .
A man in a blue shirt standing in front of a garage-like structure
painted with geometric designs .

A man in a blue shirt standing in front of a garage-like structure
painted with geometric designs .

Query: A woman is sitting at a bar eating a hamburger .
A woman sitting eating a sandwich . A woman is sitting in a cafe eating lunch .
A woman is sitting in a cafe eating lunch . A woman is eating at a diner .
The woman is eating a hotdog in the middle of her bedroom . A woman is eating her meal at a resturant .

Query: Group of men trying to catch fish with a fishing net .
Two men are on a boat trying to fish for food during a sunset . There are three men on a fishing boat trying to catch bass .
There are three men on a fishing boat trying to catch bass . Two men are trying to fish .
Two men pull a fishing net up into their red boat . Two men are on a boat trying to fish for food during a sunset .

Table 2: Nearest neighbor retrieval results on the SNLI dataset. Given a a query sentence, the left column shows
the top-3 retrieved samples based upon the hamming distance with all sentences‚Äô binary representations, while the
right column exhibits the samples according to the cosine similarity of their continuous embeddings.

Œªsp 0.0 0.2 0.5 0.8 1.0

Accuracy 78.2 78.5 78.5 79.1 78.4

Table 3: Ablation study for the AE-binary-SP model
with different choices of Œªsp (evaluated with test accu-
racy on the MR dataset).

5.3.2 Sampling strategy
As discussed in Section 3.4, the binary latent vec-
tor b can be obtained with either a deterministic
or stochastically sampled threshold. We compare
these two sampling strategies on several down-
stream tasks. As illustrated in Figure 2, setting a
fixed threshold demonstrates better empirical per-
formance on all the datasets. Therefore, determin-
istic threshold is employed for all the autoencoder
model variants in our experiments.

MR CR MPQA SUBJ SST SICKE MRPC

Dataset

0.65

0.70

0.75

0.80

0.85

0.90

0.95

P
e
rf

o
rm

a
n
ce

Deterministic

Stochastic

Figure 2: The comparison between deterministic and
stochastic sampling for the autoencoder strategy.

5.3.3 The effect of embedding dimension
Except for the hard threshold method, other three
proposed strategies all possess the flexibility of
adaptively choosing the dimension of learned bi-
nary representations. To explore the sensitivity of

512 1024 2048 4096
Number of Bits

71
72
73
74
75
76
77
78
79
80

A
cc

u
ra

cy
 (

%
)

Random

PCA

AE

AE-SP

Figure 3: The test accuracy of different model on the
MR dataset across 512, 1024, 2048, 4096 bits for the
learned binary representations.

extracted binary embeddings to their dimensions,
we run four model variants (Rand-binary, PCA-
binary, AE-binary, AE-binary-SP) with different
number of bits (i.e., 512, 1024, 2048, 4096), and
their corresponding results on the MR dataset are
shown in Figure 3.

For the AE-binary and AE-binary-SP models,
longer binary codes consistently deliver better re-
sults. While for the Rand-binary and PCA-binary
variants, the quality of inferred representations is
much less sensitive to the embedding dimension.
Notably, these two strategies exhibit competitive
performance even with only 512 bits. Therefore,
in the case where less memory footprint or little
training is preferred, Rand-binary and PCA-binary
could be more judicious choices.

6 Conclusion
This paper presents a first step towards learning
binary and general-purpose sentence representa-
tions that allow for efficient storage and fast re-
trieval over massive corpora. To this end, we ex-



115

plore four distinct strategies to convert pre-trained
continuous sentence embeddings into a binarized
form. Notably, a regularized autoencoder aug-
mented with semantic-preserving loss exhibits the
best empirical results, degrading performance by
only around 2% while saving over 98% memory
footprint. Besides, two other model variants with
a random projection or PCA transformation re-
quire no training and demonstrate competitive em-
bedding quality even with relatively small dimen-
sions. Experiments on nearest-neighbor sentence
retrieval further validate the effectiveness of pro-
posed framework.

References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer

Lavi, and Yoav Goldberg. 2017. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. CoRR, abs/1608.04207.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP.

Miguel A Carreira-PerpinaÃÅn and Ramin Raziperchiko-
laei. 2015. Hashing with binary autoencoders. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 557‚Äì566.

Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris
Tar, Yun-Hsuan Sung, Brian Strope, and Ray
Kurzweil. 2018. Universal sentence encoder. CoRR,
abs/1803.11175.

Ting Chen, Martin Renqiang Min, and Yizhou Sun.
2018. Learning k-way d-dimensional discrete codes
for compact embedding representations. arXiv
preprint arXiv:1806.09464.

Alexis Conneau and Douwe Kiela. 2018. Senteval: An
evaluation toolkit for universal sentence representa-
tions. arXiv preprint arXiv:1803.05449.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loƒ±Ãàc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079‚Äì3087.

Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and
Le Song. 2017. Stochastic generative hashing. In
Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 913‚Äì922.
JMLR. org.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li,
Xiaodong He, and Lawrence Carin. 2017. Learning
generic sentence representations using convolutional
neural networks. In EMNLP.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neu-
ral networks. In Proceedings of the thirteenth in-
ternational conference on artificial intelligence and
statistics, pages 249‚Äì256.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In HLT-NAACL.

G Hinton. 2012. Neural networks for machine learn-
ing. coursera,[video lectures].

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144.

Yacine Jernite, Samuel R. Bowman, and David A Son-
tag. 2017. Discourse-based objectives for fast un-
supervised sentence representation learning. CoRR,
abs/1705.00557.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jamie Kiros and William Chan. 2018. Inferlite: Sim-
ple universal sentence representations from natural
language inference data. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 4868‚Äì4874.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
In NIPS.

Lajanugen Logeswaran and Honglak Lee. 2018. An
efficient framework for learning sentence represen-
tations. ICLR.

Allen Nie, Erin D. Bennett, and Noah D. Good-
man. 2017. Dissent: Sentence representation
learning from explicit discourse relations. CoRR,
abs/1710.04334.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2018. Unsupervised learning of sentence embed-
dings using compositional n-gram features. In
NAACL-HLT.



116

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Sujith Ravi and Zornitsa Kozareva. 2018. Self-
governing neural networks for on-device short text
classification. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 804‚Äì810.

Sebastian Ruder and Jeremy Howard. 2018. Universal
language model fine-tuning for text classification. In
ACL.

Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. International Journal of Approxi-
mate Reasoning, 50(7):969‚Äì978.

Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa,
Wenlin Wang, Guoyin Wang, Lawrence Carin, and
Ricardo Henao. 2018. Nash: Toward end-to-end
neural architecture for generative semantic hashing.
In ACL.

Raphael Shu and Hideki Nakayama. 2017. Compress-
ing word embeddings via deep compositional code
learning. arXiv preprint arXiv:1711.01068.

Shuai Tang and Virginia R de Sa. 2018. Improving sen-
tence representations with multi-view frameworks.
arXiv preprint arXiv:1810.01064.

Julien Tissier, Amaury Habrard, and Christophe
Gravier. 2019. Near-lossless binarization of word
embeddings. AAAI.

Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 231‚Äì235. Association for Computational
Linguistics.

Jingdong Wang, Heng Tao Shen, Jingkuan Song, and
Jianqiu Ji. 2014. Hashing for similarity search: A
survey. arXiv preprint arXiv:1408.2927.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
tence embeddings. CoRR, abs/1511.08198.

John Wieting and Kevin Gimpel. 2018. Paranmt-50m:
Pushing the limits of paraphrastic sentence embed-
dings with millions of machine translations. In ACL.

John Wieting and Douwe Kiela. 2018. No training
required: Exploring random encoders for sentence
classification. CoRR, abs/1901.10444.

Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426.

Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun
Zhao, Fangyuan Wang, and Hongwei Hao. 2015.
Convolutional neural networks for text hashing. In
Twenty-Fourth International Joint Conference on
Artificial Intelligence.

Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu.
2010. Self-taught hashing for fast similarity search.
In Proceedings of the 33rd international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 18‚Äì25. ACM.


