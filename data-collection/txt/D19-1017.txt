



















































Interpretable Relevant Emotion Ranking with Event-Driven Attention


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 177â€“187,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

177

Interpretable Relevant Emotion Ranking with Event-Driven Attention

Yang Yangâ€  Deyu Zhouâˆ—â€  Yulan HeÂ§ Meng Zhangâ€ 
â€ School of Computer Science and Engineering, Key Laboratory of Computer Network

and Information Integration, Ministry of Education, Southeast University, China
Â§Department of Computer Science, University of Warwick, UK
{yyang, d.zhou}@seu.edu.cn, y.he@cantab, m.zhang@seu.edu.cn.

Abstract

Multiple emotions with different intensities
are often evoked by events described in doc-
uments. Oftentimes, such event information is
hidden and needs to be discovered from texts.
Unveiling the hidden event information can
help to understand how the emotions are e-
voked and provide explainable results. How-
ever, existing studies often ignore the latent
event information. In this paper, we proposed
a novel interpretable relevant emotion rank-
ing model with the event information incor-
porated into a deep learning architecture using
the event-driven attentions. Moreover, corpus-
level event embeddings and document-level
event distributions are introduced respectively
to consider the global events in corpus and the
document-specific events simultaneously. Ex-
perimental results on three real-world corpora
show that the proposed approach performs re-
markably better than the state-of-the-art emo-
tion detection approaches and multi-label ap-
proaches. Moreover, interpretable results can
be obtained to shed light on the events which
trigger certain emotions.

1 Introduction

The advent and prosperity of social media enable
users to share their opinions, feelings and attitudes
online. Apart from directly expressing their opin-
ions on social media posts, users can also vote for
their emotional states after reading an article on-
line. An example of a news article crawled from
Sina News Society Channel together with its as-
sociated emotion votes received from readers is il-
lustrated in Figure 1. Treating these emotion votes
as labels for the news article, we can define the e-
motion detection problem as an emotion ranking
problem that ranks emotions based on their in-
tensities. Moreover, some of the emotion labels

âˆ—Corresponding author

å¥³å­è™æ‰“å°å­©è¢«æ‹

ä¸€æ®µè¿‘5åˆ†é’Ÿçš„å¥³å­è™æ‰“å„¿ç«¥çš„è§†é¢‘å¼•å‘ç½‘å‹å…³æ³¨ã€‚ä¸€åå„¿ç«¥è¢«å¥³å­ç”¨

å°æ£è¿ç»­ç”¨åŠ›æŠ½æ‰“ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œå„¿ç«¥å“­æ³£å£°ä¸æ–­ã€‚æ‰“äººå®¶é•¿æ˜¯å¦è¿æ³•

ç­‰åœ¨è¿›ä¸€æ­¥è°ƒæŸ¥ä¸­ã€‚

A women beating her child abusively were photographed

A nearly five-minute video of women beating a child aroused the attention

of the public. A child was beaten severely and continuously by a woman

with a stick. The child kept crying throughout the whole process. Further

investigations will be conducted to determine whether the parent beating the

child is illegal or notâ€¦â€¦.

Moved              Shocked              Funny               Sad                 Strange              Angry

2                      30                       1                       86                      1                      159

Figure 1: An example of an online news article with the
readersâ€™ votes on various emotion categories. Words
highlighted in red are event-indicative words.

could be considered as irrelevant emotions. For
example, the emotion categories â€˜Movedâ€™, â€˜Fun-
nyâ€™ and â€˜Strangeâ€™ in Figure 1 only received one
or two votes. These emotion votes could be nois-
es (e.g., readers accidentally clicked on a wrong
emotion button) and hence can be considered as
irrelevant emotions. We need to separate the rel-
evant emotions from irrelevant ones and only pre-
dict the ranking results for the relevant emotion la-
bels. Therefore, the task we need to perform is the
relevant emotion ranking. Understanding and au-
tomatically ranking usersâ€™ emotional states would
be potentially useful for downstream application-
s such as dialogue systems (Picard and Picard,
1997). Multiple emotion detection from texts has
been previously addressed in (Zhou et al., 2016)
which predicted multiple emotions with different
intensities based on emotion distribution learning.
A relevant emotion ranking framework was pro-
posed in (Yang et al., 2018) to predict multiple
relevant emotions as well as the rankings based on
their intensities. However, existing emotion detec-
tion approaches do not model the events in texts



178

which are crucial for emotion detection. More-
over, most of the existing approaches only produce
emotion classification or ranking results, and they
do not provide interpretations such as identifying
which event triggers a certain emotion.

We argue that emotions may be evoked by laten-
t events in texts. Let us refer back to the example
shown in Figure 1 and read the text more careful-
ly. We notice that words such as â€˜beatâ€™, â€˜childâ€™ and
â€˜stickâ€™ marked in red are event-related words indi-
cating the event of â€œchild abuseâ€ which may evoke
the emotions of â€œAngerâ€, â€œSadnessâ€ and â€œShockâ€.

The above example shows that it is important to
simultaneously consider the latent events in texts
for relevant emotion ranking. In this paper we
proposed an interpretable relevant emotion rank-
ing model with event-driven attention (IRER-EA).
We focus on relevant emotion ranking (RER) by
discriminating relevant emotions from irrelevan-
t ones and only learn the rankings of the relevant
emotions based on their intensities.

Our main contributions are summarized below:

â€¢ A novel interpretable relevant emotion rank-
ing model with event-driven attention (IRER-
EA) is proposed. The latent event infor-
mation is incorporated into a deep learning
architecture through event-driven attentions
which can provide clues of how the emotion-
s are evoked with interpretable results. To
the best of our knowledge, it is the first deep
event-driven neural approach for RER.

â€¢ To consider event information comprehen-
sively, corpus-level event embeddings are in-
corporated to consider global events in cor-
pus and document-level event distributions
are incorporated to learn document-specific
event-related attention respectively.

â€¢ Experimental results on three different real-
world corpora show that the proposed method
performs better than the state-of-the-art emo-
tion detection methods and multi-label learn-
ing methods. Moreover, the event-driven at-
tention enables dynamically highlighting im-
portant event-related parts evoking the emo-
tions in texts.

2 Related Work

In general, emotion detection methods can main-
ly be categorized into two classes: lexicon-based

methods and learning-based methods. Lexicon-
based approaches utilize emotion lexicons includ-
ing emotion words and their emotion labels for de-
tecting emotions from texts. For example, emo-
tion lexicons are used in (Aman and Szpakowicz,
2007) to distinguish emotional and non-emotional
sentences. Emotion dictionaries could also be
used to predict the readersâ€™ emotion of new arti-
cles (Rao et al., 2012; Lei et al., 2014). Wang
et al. (2015) proposed a model with several con-
straints using non-negative matrix factorization
based on emotion lexicon for multiple emotion de-
tection. However, these approaches often suffer
from low recall.

Learning-based approaches can be further cate-
gorized into unsupervised and supervised learning
methods. Unsupervised learning approaches do
not require labeled training data (Blei et al., 2003).
Supervised learning methods typically frame emo-
tion detection as a classification problem by train-
ing supervised classifiers from texts with emotion
categories (Rao et al., 2014; Wang and Pal, 2015;
Rao, 2016). Lin et al. (2008) studied the reader-
sâ€™ emotion detection with various kinds of feature
sets on news articles. Quan et al. (2015) detect-
ed emotions from texts with a logistic regression
model introducing the intermediate hidden vari-
ables to model the latent structure of input tex-
t corpora. Zhou et al. (2016) predicted multiple
emotions with intensities based on emotion distri-
bution learning. A relevant label ranking frame-
work for emotion detection was proposed to pre-
dict multiple relevant emotions as well as the rank-
ings of emotions based on their intensities (Yang
et al., 2018; Zhou et al., 2018). However, these
approaches do not model the latent events in texts.

In recent years, deep neural network models
have been widely used for text classification. In
particular, the attention-based recurrent neural net-
works (RNNs) (Schuster and Paliwal, 2002; Yang
et al., 2016) prevail in text classification. However,
these approaches ignore the latent events in texts
thus fail to attend on event-related parts. More-
over, they are lack of interpretation.

Our work is partly inspired by (Yang et al.,
2018) for relevant emotion ranking, but with the
following significant differences: (1) our model
incorporates corpus-level event embeddings and
document-level event distributions by an event-
driven attention mechanism attending to event-
related words, which are ignored in the mod-



179

Emotion

RER 

Lossğ‘ 1

â‹¯

ğ‘ 2 ğ‘ ğ¾â‹¯

Word Attention

â‹¯

ğ‘1
ğ‘¤ â‹¯ğ‘2

ğ‘¤ ğ‘ğ‘
ğ‘¤

ğ‘’1

â‹¯

ğ‘’2 ğ‘’ğ¾â‹¯

Event-driven Attention

â‹¯

ğ‘1
ğ‘’

â‹¯ğ‘2
ğ‘’ ğ‘ğ‘

ğ‘’ â‹¯
ğ‘1 ğ‘2 ğ‘ğ¾â‹¯

Event Distribution

â„1

â‹¯

â„2 â„ğ‘â‹¯â„ğ‘

Softmax

Input Embedding Layer

Encoder Layer

Attention Layer

Output Layer

ğ›½

ğœ“ ğ‘¤

ğ‘§

ğœƒ

ğ›¼

N

D

LDA

Event Embedding

ğ‘¥1

â‹¯

ğ‘¥2 ğ‘¥ğ‘â‹¯
Word Embedding

Corpus-level 

Event-driven

Attention

Document-level 

Event-driven

Attention

ğ‘š1

â‹¯

ğ‘š2 ğ‘šğ¾â‹¯

Figure 2: The IRER-EA model.

el (Yang et al., 2018) simply using a Kullback-
Leibler (KL) divergence to approximatively learn-
ing the documentsâ€™ topic distributions; (2) our
model incorporates the event information into a
deep learning architecture thus can consider the
sequential information of texts which is ignored
in the model (Yang et al., 2018) based on shallow
bag-of-words representations.

3 The IRER-EA Model

3.1 Problem Setting

Assuming a set of T emotions, L = {l1, l2, ...lT },
and a set of Q document instances, D =
{d1, d2, d3, ..., dQ}, each instance di is associat-
ed with a list of its relevant emotions Ri âŠ† L
ranked by their intensities and also a list of irrele-
vant emotions Ri = L âˆ’ Ri. Relevant emotion
ranking aims to learn a score function g(di) =
[g1(di), ..., gT (di)] which assigns a score gj(di) to
each emotion lj , (j âˆˆ {1, ..., T}). Relevant emo-
tions and their rankings can be obtained simulta-
neously according to the scores assigned by the
learned ranking function g.

The learning objective of relevant emotion rank-
ing (RER) is to both discriminate relevant emo-
tions from irrelevant ones and to rank relevant e-
motions according to their intensities. Therefore,
to fulfil the requirements of RER, the global ob-

jective function is defined as follows:

E =

Qâˆ‘
i=1

âˆ‘
ltâˆˆRi

âˆ‘
lsâˆˆâ‰º(lt)

1

normt,s

[ exp(âˆ’(gt(di)âˆ’ gs(di)))+
Ï‰ts(gt(di)âˆ’ gs(di))2 ]

(1)

Here, ls âˆˆâ‰º (lt) represents that emotion ls is less
relevant than emotion lt. The normalization term
normt,s is used to avoid terms dominated by the
sizes of emotion pairs. The term gt(di) âˆ’ gs(di)
measures the difference between two emotions.
Ï‰ts represents the relationship between two emo-
tions lt and ls which is calculated by Pearson cor-
relation coefficient (Nicewander, 1988).

We present the overall architecture of the pro-
posed interpretable relevant emotion ranking with
event-driven attention (IRER-EA) model in Fig-
ure 2. It consists of four layers: (1) the Input Em-
bedding Layer including both word embeddings
and event embeddings; (2) the Encoder Layer in-
cluding both the word encoder and the event en-
coder; (3) the Attention Layer which computes the
word-level attention scores and the event-driven
attention scores taking into account of the corpus-
level and document-level event information, re-
spectively; (4) the Output Layer which generates
the emotion ranking results.

3.2 Input Embedding Layer
The Input Embedding Layer contains word em-
beddings and event embeddings. Assuming a doc-
ument di consisting of N words represented as



180

di = {w1, w2, ..., wN}, the pre-trained word vec-
tor, GloVe (Pennington et al., 2014), is used to ob-
tain the fixed word embedding of each word and
di can be represented as di = {x1, x2, ..., xN} as
shown in Figure 2.

Since nouns and verbs are more important than
other word types in referring to specific events,
they are utilized as inputs of topic model such
as Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) to generate events automatically. Therefore,
the granularity of extracted events is controlled by
the predefined K, the number of events. For the
corpus D consisting of K events {e1, e2, ..., eK},
the event embedding of the kth event ek can be
obtained from the output event-word distribution
matrix E of the topic model. For the single docu-
ment di, the event distribution p = (p1, p2, ..., pK)
obtained from the topic model represents the prob-
ability of the text expressing each event.

3.3 Encoder Layer

â„1
ğ‘¡ â‹¯â„2ğ‘¡ â„3ğ‘¡ â„4ğ‘¡ â„ğ‘

ğ‘¡â‹¯

â„ğ‘
ğ‘¡

ğ‘Ÿğ‘¤

ğ‘¥1 ğ‘¥2 ğ‘¥ğ‘â€¦

ğ‘1
ğ‘¤

â‹¯ ğ‘ğ‘ğ‘¤

Figure 3: Word Encoder.

The Encoder Layer contains both the word en-
coder and event encoder. As for the word encoder,
an alternative RNN structure (Zhang et al., 2018)
is used to encode texts into semantic representa-
tions since it has been shown to be more effective
in encoding longer texts. For document di, for-
mally, a state at time step t can be denoted by:

Ht =< ht1, ..., h
t
N , h

t
q > (2)

which consists of sub-states hti for the ith word wi
in document di and a document-level sub-state htq
as shown in Figure 3. The hidden states are in-
dependent of each other at the present recurren-
t step and are connected across recurrent steps,
which can capture long-range dependencies. The
recurrent state transition process is used to model

information exchange between those sub-states to
enrich state representations incrementally. The s-
tate transition is similar to LSTM (Hochreiter and
Schmidhuber, 1997) and a recurrent cell cti for
each wordwi and a cell ctq for document-level sub-
state hq are used. The value of each hti is comput-
ed based on the values of xi, htâˆ’1iâˆ’1, h

tâˆ’1
i , h

tâˆ’1
i+1,

htâˆ’1q at two adjacent recurrent time steps. Note
that the number of window size between two adja-
cent steps can be set manually. Hence, the hidden
sub-states hi for individual words wi and a global
document hidden state hq for di are obtained.

As for event encoder, event representations are
produced by the ReLU-actived neural perceptrons
taking the event-word weight matrix E âˆˆ V Ã—
K as inputs. Hence, each event representation sk
representing event k is obtained according to the
event embedding ek, k âˆˆ {1, 2, 3, ...,K}.

3.4 Attention Layer
Given a word wn in document di, hn is the hid-
den representation of wn after encoder. Given an
event embedding ek in the corpus, sk is the even-
t representation of ek generated by the event en-
coder. Then we utilize attention weights to en-
hance the word representations and event repre-
sentations from different aspects.

Our model contains two kinds of attention
mechanisms including the word-level attentions
and the event-driven attentions.

3.4.1 Word-Level Attention
As for word-level attentions, since not all words
contribute equally to the meaning of a documen-
t, we introduce an attention mechanism to extract
words with greater importance and aggregate the
representations of those informative words to for-
m the document representation, which is shown in
the left part of Figure 2. More concretely,

Ï•wi = tanh(Ww(hi + hq) + bw)

awi =
exp(Ï•>wiuw)âˆ‘
i exp(Ï•

>
wiuw)

rw =
âˆ‘
i

awi hi

(3)

where the weight awi is the attention of the word
wi and Ww, bw and uw are parameters similar to
(Pappas and Popescu-Belis, 2017). Note that we
further incorporate the global information of the
document representation hq obtained from the en-
coder to strengthen the word attention.



181

3.4.2 Event-Driven Attention

In our model, we use the event-driven atten-
tion mechanism to attend to event-related words,
which can discover words more important for text-
related events. The event-driven attention lever-
ages the corpus-level event information based on
each event representation sk, k âˆˆ {1, 2, 3, ...,K}
obtained from the corpus and the document-level
event information based on the documentâ€™s event
distribution p = (p1, p2, ..., pK).
Corpus-level Event-Driven Attention

The model utilizes the corpus-level event infor-
mation by a joint attention mechanism to consider
global events in corpus, which aggregates the se-
mantic representations h = (h1, h2, ..., hN ) of an
input text obtained and measures the interaction
the words in the text with the event representations
s = (s1, s2, ..., sK) by the event-driven attention.
The corpus-level event-driven attention is calculat-
ed as follows:

Ï•c = tanh(Wch+ bc)

mck = (Ï•
c)>sk

ac = softmax(
Kâˆ‘
k=1

mck)

rc = (ac)>h

(4)

where h = (h1, h2, ..., hN )>stands for the com-
bination of all the hidden states of words in the
document and Wc and bc are parameters needed
to be learnt for corpus-level event-driven atten-
tion. Ï•c = (Ï•c1, Ï•

c
2, ..., Ï•

c
N ) refers to the hidden

representation of state h through a fully connect-
ed layer. Given the event representation sk, we
measure the interaction of the words in the docu-
ment and the event by an attention weight vector
mck which can be computed as the inner produc-
t of event sk and Ï•c followed by a softmax layer.
ac = (ac1, a

c
2, ..., a

c
N ) stands for the average atten-

tion weights of all the events for words which con-
tribute to discover event keywords of a document
according to different events in corpus. Then we
construct the text representation rc with the sum
of hidden states weighted by ac.
Document-level Event-driven Attention

We further incorporate the document-level
event-driven attention mechanism. Our model
can attend to the event distributions of the cur-
rent document in order to strengthen the effec-
t of the current document expressing each event

and learn document-specific event related atten-
tion. For each document, p = (p1, p2, ..., pK)
denotes the event distributions of the documen-
t, with each dimension representing the level of
prominence of the corresponding event occurred
in the document. The corpus-level event-driven
attention weights can be further strengthened by
including document-level event distributions. The
document-level event-driven attention is calculat-
ed as follows:

Ï•d = tanh(Wdh+ bd)

mdk = (Ï•
d)>sk

ae = softmax(
Kâˆ‘
k=1

mdkpk)

re = (ae)>h

(5)

where h = (h1, h2, ..., hN )> stands for the aggre-
gation of all the hidden states of words in the doc-
ument and Wd and bd are parameters needed to
be learnt for the document-level event-driven at-
tention. Ï•d = (Ï•d1, Ï•

d
2, ..., Ï•

d
N ) refers to the hid-

den representation of state h through a fully con-
nected layer. mdk represents the interaction of the
words in the document and the event which can
be computed as the inner product of event sk and
Ï•d. Then mdk is weighted by the document-level
event distribution, p = (p1, p2, ..., pK), followed
by a softmax layer, and ae = (ae1, a

e
2, ..., a

e
N ) s-

tands for the attention weight after incorporating
the document-level event distributions. Then we
construct the text representation re with the sum
of hidden states weighted by ae. Finally, re is
used as the final text representation obtained by the
event-driven attention which simultaneously takes
into account both the corpus-level event informa-
tion and the document-level event information.

3.5 Output Layer
At last, we concatenate both the representation-
s calculated by the word-level attention and the
event-driven attention to obtain the final represen-
tation r = [rw, re], which is fed to a multi-layer
perceptron and a softmax layer for identifying rel-
evant emotions and their rankings.

4 Experiments

To evaluate our proposed approach, we conducted
experiments on the following three corpora:
Sina Social News (News) (Zhou et al., 2018) con-
sists of 5,586 news articles collected from the Sina



182

news Society channel. Each document was kept
together with the readersâ€™ emotion votes of the six
emotions including Funny, Moved, Angry, Sad, S-
trange, and Shocked.
Ren-CECps corpus (Blogs) (Quan and Ren,
2010) is a Chinese data set containing 1,487 blogs
annotated with eight basic emotions from writerâ€™s
perspective, including Anger, Anxiety, Expect,
Hate, Joy, Love, Sorrow and Surprise. The
emotions are represented by their emotion scores
in the range of [0, 1]. Higher scores represent
higher emotion intensities.
SemEval (Strapparava and Mihalcea, 2007) con-
tains 1,250 English news headlines extracted from
Google news, CNN, and many other portals,
which are manually annotated with a fine-grained
valence scale of 0 to 100 across 6 emotions, in-
cluding Anger, Disgust, Fear, Joy, Sad and Sur-
prise.

News Blogs SemEval

Category #Votes Category #Scores Category #Scores

Touching 694,006 Joy 349.2 anger 12042

Shock 572,651 Hate 174.2 disgust 7634

Amusement 869,464 Love 610.6 fear 20306

Sadness 837,431 Sorrow 408.4 joy 23613

Curiosity 212,559 Anxiety 422.6 sad 24039

Anger 1,109,315 Surprise 59.2 surprise 21495

Anger 116.4

Expect 385.5

All 4,295,426 All 2526.1 All 109,129

Table 1: Statistics for the three corpora used in our ex-
periments.

The statistics for the three corpora used in our
experiments are shown in Table 1.

In our experiments, the News and Blog corpo-
ra were preprocessed using the python jieba seg-
menter1 for word segmentation and filtering. The
third corpus SemEval is in English and was tok-
enized by white space. Stanford CoreNLP2 was
applied for parts of speech tagging to obtain the
nouns and verbs of the documents. Stop words
and words appeared less than twice were removed
from documents. We used the pre-trained Chinese
GloVe and English GloVe3 vectors as the word
embeddings in the experiments and the dimension
of the word embeddings was 300.

1https://github.com/fxsjy/jieba
2https://stanfordnlp.github.io/

CoreNLP/index.html
3https://nlp.stanford.edu/projects/

glove/

Name Definition

PRO Loss 1n
âˆ‘n

i=1

âˆ‘
etâˆˆRiâˆª{Î˜}

âˆ‘
esâˆˆâ‰º(et)

1
normt,s

lt,s

lt,s is a modified 0-1 error;

normt,sis the set size of label pair(et, es)

Hamming Loss 1nT
âˆ‘n

i=1 |RÌ‚i4Ri|

The predicted relevant emotions: RÌ‚i.

Ranking Loss 1n
âˆ‘n

i=1

(
âˆ‘

(et,es)âˆˆRiÃ—Ri
Î´[gt(xi)<gs(xi)])

(|Ri|Ã—|Ri|)

where Î´ is the indicator function.

One Error 1n
âˆ‘n

i=1 Î´[argmax
et

gt(xi) /âˆˆ Ri]

Average Precision 1n
âˆ‘n

i=1
1
|Ri| Ã—

âˆ‘
t:etâˆˆRi

|{esâˆˆRi|gs(xi)>gt(xi)}|
|{es|gs(xi)>gt(xi)}|

Coverage 1n
âˆ‘n

i=1 maxt:etâˆˆRi |{es|gs(xi) > gt(xi)}|

F1exam
1
n

âˆ‘n
i=1

2|Riâˆ©RÌ‚i|
(|Ri|+|RÌ‚i|)

MicroF1 F1(
Tâˆ‘
t=1

TPt,
Tâˆ‘
t=1

FPt,
Tâˆ‘
t=1

TNt,
Tâˆ‘
t=1

FNt)

MacroF1 1T
Tâˆ‘
t=1

F1(TPt, FPt, TNt, FNt)

Table 2: Evaluation criteria for the Multi-Label Learn-
ing (MLL) methods. TPt, FPt, TNt, FNt represent
the number of true positive, false positive, true nega-
tive, and false negative test examples with respect to
emotion t respectively. F1(TPt, FPt, TNt, FNt) rep-
resent specific binary classification metric F1 (Man-
ning et al., 2008).

The event embeddings and event distributions
used in the proposed method are derived in dif-
ferent ways. For long documents including News
and Blogs, LDA was employed to generate even-
t embeddings and event distributions using verbs
and nouns as the input. For short texts in SemEval
with the sparsity problem, Bi-term Topic Model
(BTM) (Cheng et al., 2014) was chosen. The num-
ber of topics was 60. The parameters were chosen
from the validation set which is 10% of the train-
ing set. The encoder was trained using a learn-
ing rate of 0.001, a dropout rate of 0.5, a window
size of 1 and a layer number of 3. The number of
epochs was 10 and the mini batch (Cotter et al.,
2011) size was 16. For each method, 10-fold cross
validation was conducted to get the final results.

The baselines can be categorized into two class-
es, emotion detection methods and multi-label
methods. Most these baselines are either re-
implemented or cited from published papers. For
instance, the results of multi-label methods are re-
implemented, since they are not proposed for rele-
vant emotion ranking. The performances of some
emotion detection methods, such as EDL, EmoD-
etect, RER and INNRER, are cited from the pub-

https://github.com/fxsjy/jieba
https://stanfordnlp.github.io/CoreNLP/index.html
https://stanfordnlp.github.io/CoreNLP/index.html
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


183

Corpus Category Method
Criteria

PL(â†“) HL(â†“) RL(â†“) OE(â†“) AP(â†‘) Cov(â†“) F1(â†‘) MiF1(â†‘) MaF1(â†‘)

News

Emotion Detection

EDL 0.2348 0.2510 0.1616 0.2243 0.8372 2.1940 0.6260 0.6454 0.5703

EmoDetect 0.2157 0.2575 0.1538 0.1627 0.8605 2.1761 0.6697 0.6739 0.5359

RER 0.2142 0.2498 0.1491 0.1513 0.8633 2.1989 0.6820 0.6919 0.6198

INN-RER 0.1973 0.2312 0.1353 0.1331 0.8764 2.1339 0.7108 0.7161 0.6282

Multi-label

LIFT 0.2224 0.3363 0.1382 0.1411 0.8234 2.1394 0.6646 0.6801 0.6151

Rank-SVM 0.2842 0.2872 0.2114 0.2034 0.7967 2.5358 0.5066 0.5656 0.5298

MLLOC 0.4458 0.4206 0.4500 0.4193 0.6531 3.9032 0.3000 0.4060 0.3327

BP-MLL 0.2118 0.2399 0.1443 0.1544 0.8677 2.1738 0.6881 0.6915 0.6013

Our model IRER-EA 0.1674 0.1958 0.0989 0.0846 0.9137 1.9108 0.7475 0.7379 0.6358

Blogs

Emotion Detection

EDL 0.3385 0.3916 0.2550 0.4206 0.6962 4.2491 0.5060 0.5396 0.4131

EmoDetect 0.3115 0.3848 0.2123 0.2880 0.7617 4.1650 0.5340 0.5492 0.4387

RER 0.3007 0.3657 0.2043 0.2728 0.7746 4.1638 0.5957 0.6084 0.5342

INN-RER 0.2829 0.3209 0.1924 0.2626 0.7784 3.6418 0.6187 0.6225 0.5133

Multi-label

LIFT 0.3452 0.3817 0.3089 0.3306 0.7557 3.1290 0.6053 0.6113 0.5155

Rank-SVM 0.3888 0.3786 0.3356 0.3219 0.7030 4.0801 0.3489 0.3686 0.3210

MLLOC 0.4999 0.5135 0.5115 0.6554 0.5569 5.7432 0.4104 0.4411 0.4391

BP-MLL 0.2987 0.3281 0.2141 0.2727 0.7267 3.9802 0.5844 0.6065 0.4833

Our model IRER-EA 0.2609 0.2874 0.1638 0.1845 0.8149 3.6510 0.6304 0.6320 0.4804

SemEval

Emotion Detection

EDL 0.4130 0.4291 0.3401 0.3875 0.7345 3.3433 0.4002 0.413 0.3813

EmoDetect 0.3176 0.3167 0.2411 0.2308 0.8241 3.0439 0.6275 0.6245 0.5385

RER 0.2907 0.3128 0.2389 0.2220 0.8302 2.9963 0.6839 0.6898 0.6283

INN-RER 0.3194 0.3005 0.2302 0.2261 0.8379 2.8632 0.7081 0.7156 0.6093

Multi-label

LIFT 0.4279 0.4651 0.3627 0.4113 0.7344 3.2823 0.6299 0.6469 0.6112

Rank-SVM 0.3452 0.3617 0.3083 0.3006 0.7557 3.1290 0.6253 0.6472 0.5955

MLLOC 0.4458 0.4206 0.4500 0.4193 0.6531 3.9032 0.3000 0.4060 0.3327

BP-MLL 0.3790 0.3656 0.3605 0.3790 0.7945 3.2097 0.5868 0.6101 0.5402

Our model IRER-EA 0.2754 0.3065 0.1999 0.1286 0.8976 3.5799 0.7394 0.7538 0.6841

Table 3: Comparison with Emotion Detection Methods and Multi-label Methods. â€™PLâ€™ represent Pro Loss, â€™HLâ€™
represents Hamming Loss, â€™RLâ€™ represents ranking loss, â€™OEâ€™ represents one error, â€™APâ€™ represent average preci-
sion, â€™Covâ€™ represent coverage, â€™F1â€™ represents F1exam, â€™MiF1â€™ represents MicroF1, â€™MaF1â€™ represents MacroF1.
â€œâ†“â€ indicates â€œthe smaller the betterâ€, while â€œâ†‘â€ indicates â€œthe larger the betterâ€. The best performance on each
evaluation measure is highlighted by boldface.

lished paper (Yang et al., 2018) as they use the
same experimental data as ours.

Evaluation metrics typically used in multi-label
learning and label ranking are employed in our
experiments which are different from those of
classical single-label learning systems (Sebastian-
i, 2001). The detailed explanations of evaluation
metrics are presented in Table 2.

4.1 Compared Methods

There are several emotion detection approaches
addressing multiple emotions detection from texts.

â€¢ Emotion Distribution Learn-
ing(EDL) (Zhou et al., 2016) learns a

mapping function from sentences to their
emotion distributions.

â€¢ EmoDetect (Wang and Pal, 2015) employs a
constraint optimization framework with sev-
eral constraints to obtain multiple emotions.

â€¢ RER (Zhou et al., 2018) uses support vec-
tor machines to predict relevant emotions and
rankings in one text based on their intensities.

â€¢ INN-RER (Yang et al., 2018) designs a
three-layer network combined with topic
model to solve relevant emotion ranking.

Relevant emotion ranking can be treated as an
extension of multi-label learning, so we also com-



184

Corpus Category Method
Criteria

PL(â†“) HL(â†“) RL(â†“) OE(â†“) AP(â†‘) Cov(â†“) F1(â†‘) MiF1(â†‘) MaF1(â†‘)

News
Baselines

IRER-EA(-EA) 0.1786 0.1981 0.1023 0.0854 0.9122 1.9294 0.7423 0.7299 0.6116

IRER-EA(-DEA) 0.1700 0.1947 0.1092 0.0835 0.9086 2.0014 0.7437 0.7358 0.6038

Our model IRER-EA 0.1674 0.1958 0.0989 0.0846 0.9137 1.9108 0.7475 0.7379 0.6358

Blogs
Baselines

IRER-EA(-EA) 0.2860 0.3010 0.1819 0.2004 0.7816 3.8320 0.5534 0.5422 0.3805

IRER-EA(-DEA) 0.2852 0.3077 0.1844 0.1986 0.7936 3.8290 0.6027 0.6053 0.4220

Our model IRER-EA 0.2609 0.2874 0.1638 0.1845 0.8149 3.6510 0.6304 0.6320 0.4804

SemEval
Baselines

IRER-EA(-EA) 0.2760 0.2968 0.2076 0.1916 0.8565 2.8235 0.6494 0.6491 0.5480

IRER-EA(-DEA) 0.2879 0.3220 0.2205 0.1534 0.8795 3.2893 0.6852 0.6891 0.5927

Our model IRER-EA 0.2754 0.3065 0.1999 0.1286 0.8976 3.5799 0.7394 0.7538 0.6841

Table 4: Comparison of different IRER-EA components.â€œâ†“â€ indicates â€œthe smaller the betterâ€, while â€œâ†‘â€ indicates
â€œthe larger the betterâ€. The best performance on each evaluation measure is highlighted by boldface.

pare the proposed IRER-EA with several widely-
used multi-label learning methods.
â€¢ LIFT (Zhang, 2011) constructs features spe-

cific to each label.
â€¢ Rank-SVM (Zhang and Zhou, 2014) distin-

guishes relevant labels from irrelevant ones
with large margin strategy.
â€¢ MLLOC (Huang and Zhou, 2012) exploits

local emotion correlations in expression data.

â€¢ BP-MLL (Zhang and Zhou, 2006) employs
a novel error function into back propaga-
tion algorithm to capture the characteristics
of multi-label learning.

For the MLL methods, linear kernel is used in
LIFT. Rank-SVM uses the RBF kernel with the
width Ïƒ equals to 1.

Experimental results on three corpora are shown
in Table 3. It can be summarized from the ta-
ble that: (1) IRER-EA performs better than state-
of-art emotion detection baselines on almost al-
l evaluation metrics across three corpora, which
obviously shows the effectiveness of incorporat-
ing event information to obtain event-driven at-
tentions for relevant emotion ranking; (2) IRER-
EA achieves remarkably better results than MLL
methods. It further confirms the effectiveness of
IRER-EA, which uses a deep learning architecture
incorporating event-driven attention for better per-
formance.

4.2 Model Analysis
To further validate the effectiveness of event-
driven attention components, we compare IRER-
EA with two sub-networks based on our architec-
tures.

â€¢ IRER-EA(-EA): removes event-driven at-
tention from IRER-EA.

â€¢ IRER-EA(-DEA): removes document-level
event-driven attention from IRER-EA.

Experimental results on three corpora are shown
in Table 4. It can be summarized from the ta-
ble that: (1) All the sub-networks cannot compete
with IRER-EA on three corpora, which indicates
the corpus-level and document-level event infor-
mation are effective for relevant emotion ranking
task; (2) IRER-EA(-DEA) performs better than
IRER-EA(-EA) on most of the evaluation metric-
s, which verifies the effectiveness of incorporat-
ing corpus-level event-driven attention; (3) IRER-
EA achieves better results than IRER-EA(-DEA)
on almost all the evaluation metrics which further
proves the effectiveness of document-level event-
driven attention.

4.3 Case Study of Interpretability

To further investigate whether the event-driven
attention is able to capture the event-associated
words in a given document and provide inter-
pretable results, we compare the results of the at-
tention mechanisms of the word-level attention-
s and event-driven attentions by visualizing the
weights of words in the same documents as shown
in Figure 4. As the document of the News cor-
pus and Blogs corpus are too long, we manu-
ally simplified the texts for better visualization
results and provided English translations of the
texts. The words marked in red represent highly-
weighted ones according to the event-driven atten-
tions, while the words with blue underlines are the



185

 

Corpora Texts Emotions  

News 

å¤ç´å°é™„è¿‘ä¸€åå¥³å­è½»ç”Ÿæººäº¡ã€‚ä»å¥¹èµ°è¿›æ°´é‡Œåˆ°æœ€ç»ˆæººäº¡ï¼ŒæŒç»­æ—¶é—´å°†
è¿‘ä¸¤åˆ†é’Ÿã€‚å½“æ—¶æœ‰ 10 ä½™äººå›´è§‚ï¼Œç«Ÿæ— äººä¸‹æ°´æ–½æ•‘ã€‚è¿™ç§ç°è±¡è®©æˆ‘ä»¬å¯’
å¿ƒã€‚ 
A woman drowned near Guqin Terrace. It took nearly two minutes from she 

stepping into the water to her final death. More than 10 people were watching 

her, but no one came to rescue. Such phenomenon chills us. 

Angry 

Sad 

Shocked 

Blog 

åœ°éœ‡è¿‡åä¸¤ç™¾ä¸ªå°æ—¶é‡Œï¼Œæ¯ä¸€å¼ å‰æ²¿çš„ç…§ç‰‡ï¼Œæ¯ä¸€æ®µè·æ•‘çš„è§†é¢‘ï¼Œæ¯ä¸ª
çˆ¸çˆ¸å¦ˆå¦ˆå‘è‡ªå¿ƒåº•çš„ç—›å“­ï¼Œéƒ½ä»¤æˆ‘å¿ƒç—›ã€‚ 
During the two hundred hours after the earthquake, each photo and video 

about rescue process and the crying from the parents make my heart broken. 

Sorrow 

Love 

SemEval teacher in hide after attack on islam stir threat Fear Sad 

 

Figure 4: Case Study of Interpretability on Three Corpora.

ones with higher attention weights according to
the word-level attentions.

From the visualization results on an example
News article, it can be observed that different from
word-level attention that pays more attentions to
emotion-associated words, such as â€˜chillâ€™ which
may only evoke the emotion â€œSadâ€, the event-
driven attentions can find words indicating latent
events in the document, such as â€˜drownâ€™, â€˜deathâ€™,
â€˜no one rescueâ€™ which are all closely related to the
event â€œSuicide without rescueâ€™, which may evoke
emotions such as â€œAngryâ€ and â€œShockedâ€. In an
example Blog article, word-level attentions high-
light emotion-associated words such as â€˜cryingâ€™
and â€˜brokenâ€™ which may evoke the emotion â€œSor-
rowâ€, while event-driven attentions focus on the
event-related words such as â€˜earthquakeâ€™ and
â€˜rescuedâ€™ representing the event â€œEarthquake Re-
lief â€™. Finally, in an example from the SemEval
corpus, we can see that the word-level attention
mechanism only gives a higher attention weight to
the word â€™threatâ€™ and ignores the word â€™attackâ€™,
which is also an important indicator of the emo-
tions â€œFearâ€ and â€œSadâ€. On the contrary, the
event-driven attention mechanism highlights both
â€™attackâ€™ and â€˜threatâ€™, representing the event â€œTer-
rorist attackâ€™.

In summary, we can observe from Figure 4
that: (1) Event-driven attention can capture word-
s representing latent events in texts; (2) Com-
pared with the word-level attention which is prone
to attend on emotion-associated keywords, event-
driven attention can find words representing one or

more hidden events in a document, which can pro-
vide more explainable clues of which event trig-
gers certain emotions; (3) Event-driven attention
can achieve better performance especially in doc-
uments without any emotion-associated words.

5 Conclusion

In this paper, we have proposed an interpretable
relevant emotion ranking model with event-driven
attention. The event information is incorporated
into a neural model through event-driven atten-
tions which can provide clues of how the emotion-
s are evoked with explainable results. Moreover,
corpus-level event embeddings and document-
level event distributions are incorporated respec-
tively to consider event information comprehen-
sively. Experimental results show that the pro-
posed method performs better than the state-of-
the-art emotion detection methods and multi-label
learning methods.

Acknowledgments

We would like to thank anonymous reviewers
for their valuable comments and helpful sugges-
tions. This work was funded by the National
Key Research and Development Program of Chi-
na (2017YFB1002801), the National Natural Sci-
ence Foundation of China (61772132), the Nat-
ural Science Foundation of Jiangsu Province of
China (BK20161430) and Innovate UK (grant no.
103652).



186

References
Saima Aman and Stan Szpakowicz. 2007. Identify-

ing expressions of emotion in text. Lecture Notes
in Computer Science, 4629:196â€“205.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993â€“1022.

Xueqi Cheng, Xiaohui Yan, Yanyan Lan, and Jiafeng
Guo. 2014. Btm: Topic modeling over short texts.
IEEE Transactions on Knowledge and Data Engi-
neering, 26(12):2928â€“2941.

Andrew Cotter, Ohad Shamir, Nathan Srebro, and
Karthik Sridharan. 2011. Better mini-batch algo-
rithms via accelerated gradient methods. Advances
in Neural Information Processing Systems, pages
1647â€“1655.

S Hochreiter and J Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735â€“
1780.

Sheng Jun Huang and Zhi Hua Zhou. 2012. Multi-label
learning by exploiting label correlations locally. In
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence, pages 949â€“955.

Jingsheng Lei, Yanghui Rao, Qing Li, Xiaojun Quan,
and Liu Wenyin. 2014. Towards building a social
emotion detection system for online news. Future
Generation Computer Systems, 37:438â€“448.

Kevin Hsin-Yih Lin, Changhua Yang, and Hsin-Hsi
Chen. 2008. Emotion classification of online news
articles from the readerâ€™s perspective. In Proceed-
ings of the 2008 IEEE/WIC/ACM International Con-
ference on Web Intelligence and Intelligent Agent
Technology-Volume 01, pages 220â€“226. IEEE Com-
puter Society.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. An introduction to infor-
mation retrieval. Journal of the American Society
for Information Science and Technology, 43(3):824â€“
825.

W. Alan Nicewander. 1988. Thirteen ways to look at
the correlation coefficient. American Statistician,
42(1):59â€“66.

Nikolaos Pappas and Andrei Popescu-Belis. 2017.
Multilingual hierarchical attention networks for doc-
ument classification. CoRR, abs/1707.00896.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Natu-
ral Language Processing, pages 1532â€“1543.

Rosalind W Picard and Roalind Picard. 1997. Affective
computing, volume 252. MIT press Cambridge.

Changqin Quan and Fuji Ren. 2010. Sentence emotion
analysis and recognition based on emotion words us-
ing ren-cecps. International Journal of Advanced
Intelligence Paradigms, 2(1):105â€“117.

Xiaojun Quan, Qifan Wang, Ying Zhang, Luo Si, and
Liu Wenyin. 2015. Latent discriminative models for
social emotion detection with emotional dependen-
cy. ACM Trans. Inf. Syst., 34(1):2:1â€“2:19.

Yanghui Rao. 2016. Contextual sentiment topic mod-
el for adaptive social emotion classification. IEEE
Intelligent Systems, 31(1):41â€“47.

Yanghui Rao, Qing Li, Xudong Mao, and Wenyin Liu.
2014. Sentiment topic models for social emotion
mining. Information Sciences, 266(5):90â€“100.

Yanghui Rao, Xiaojun Quan, Liu Wenyin, Qing Li,
and Mingliang Chen. 2012. Building word-emotion
mapping dictionary for online news. In SDAD 2012
The 1st International Workshop on Sentiment Dis-
covery from Affective Data, page 28.

M. Schuster and K.K. Paliwal. 2002. Bidirectional re-
current neural networks. IEEE Transactions on Sig-
nal Processing, 45(11):2673â€“2681.

Fabrizio Sebastiani. 2001. Machine learning in auto-
mated text categorization. Acm Computing Surveys,
34(1):1â€“47.

Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70â€“74. Association for Computational
Linguistics.

Yichen Wang and Aditya Pal. 2015. Detecting e-
motions in social media: A constrained optimiza-
tion approach. In Proceedings of the Twenty-Fourth
International Joint Conference on Artificial Intelli-
gence (IJCAI 2015), pages 996â€“1002.

Yang Yang, Deyu Zhou, and Yulan He. 2018. An in-
terpretable neural network with topical information
for relevant emotion ranking. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October
31 - November 4, 2018, pages 3423â€“3432.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
pages 1480â€“1489.

Min Ling Zhang. 2011. Lift: multi-label learning with
label-specific features. In International Joint Con-
ference on Artificial Intelligence, pages 1609â€“1614.

Min Ling Zhang and Zhi Hua Zhou. 2006. Multilabel
neural networks with applications to functional ge-
nomics and text categorization. IEEE Transaction-
s on Knowledge Data Engineering, 18(10):1338â€“
1351.

http://arxiv.org/abs/1707.00896
http://arxiv.org/abs/1707.00896
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://aclanthology.info/papers/D18-1379/d18-1379
https://aclanthology.info/papers/D18-1379/d18-1379
https://aclanthology.info/papers/D18-1379/d18-1379
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174


187

Min-Ling Zhang and Zhi-Hua Zhou. 2014. A re-
view on multi-label learning algorithms. IEEE
transactions on knowledge and data engineering,
26(8):1819â€“1837.

Yue Zhang, Qi Liu, and Linfeng Song. 2018. Sentence-
state lstm for text representation. In Proceedings of
the 56th Annual Meeting of the Association for Com-
putational Linguistics, pages 317â€“327. Association
for Computational Linguistics.

Deyu Zhou, Yang Yang, and Yulan He. 2018. Relevan-
t emotion ranking from text constrained with emo-
tion relationships. In Meeting of the North American
Chapter of the Association for Computation Linguis-
tics, pages 561â€“571.

Deyu Zhou, Xuan Zhang, Yin Zhou, Quan Zhao, and
Xin Geng. 2016. Emotion distribution learning from
texts. In Conference on Empirical Methods in Natu-
ral Language Processing, pages 638â€“647.

http://aclweb.org/anthology/P18-1030
http://aclweb.org/anthology/P18-1030

