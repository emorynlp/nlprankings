



















































Encoding World Knowledge in the Evaluation of Local Coherence


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1087–1096,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Encoding World Knowledge in the Evaluation of Local Coherence

Muyu Zhang1∗, Vanessa Wei Feng2, Bing Qin1, Graeme Hirst2, Ting Liu1 and Jingwen Huang1
1Research Center for Social Computing and Information Retrieval

Harbin Institute of Technology, Harbin, China
2Department of Computer Science, University of Toronto, Toronto, ON, Canada

{myzhang,qinb,tliu,jwhuang}@ir.hit.edu.cn
{weifeng,gh}@cs.toronto.edu

Abstract

Previous work on text coherence was primar-
ily based on matching multiple mentions of
the same entity in different parts of the text;
therefore, it misses the contribution from se-
mantically related but not necessarily coref-
erential entities (e.g., Gates and Microsoft).
In this paper, we capture such semantic relat-
edness by leveraging world knowledge (e.g.,
Gates is the person who created Microsoft),
and use two existing evaluation frameworks.
First, in the unsupervised framework, we in-
troduce semantic relatedness as an enrichment
to the original graph-based model of Guin-
audeau and Strube (2013). In addition, we
incorporate semantic relatedness as additional
features into the popular entity-based model
of Barzilay and Lapata (2008). Across both
frameworks, our enriched model with seman-
tic relatedness outperforms the original meth-
ods, especially on short documents.

1 Introduction

In a well-written document, sentences are organized
and presented in a logical and coherent form, which
makes the text fluent and easily understood. There-
fore, coherence is a fundamental aspect of high text
quality, and the evaluation of coherence is a crucial
component of many NLP applications, such as essay
scoring (Miltsakaki and Kukich, 2004), story gener-
ation (McIntyre and Lapata, 2010), and document
summarization (Barzilay et al., 2002).

∗ This work was partly done while the first author was vis-
iting University of Toronto.

A particularly popular model for evaluating text
coherence is the entity-based local coherence model
of Barzilay and Lapata (2008) (B&L), which ex-
tracts mentions of entities in adjacent sentences, and
captures local coherence in terms of the transitions
in the grammatical role of each mention. Follow-
ing this direction, a number of extensions have been
proposed (Elsner and Charniak, 2008; Elsner and
Charniak, 2011; Lin et al., 2011; Feng et al., 2014),
the majority of which focus on enriching the origi-
nal entity features. An exception is the unsupervised
model of Guinaudeau and Strube (2013) (G&S),
which converts the document into a graph of sen-
tences, and evaluates the text coherence by comput-
ing the average out-degree over the entire graph.

However, despite the apparent success of these
methods, they rely merely on matching mentions of
the same entity, but neglect the contribution from
semantically related but not necessarily coreferen-
tial entities. For example, the text in Figure 1a1 has
no common entity in s2 and s3. However, the tran-
sition between them is perfectly coherent, because
there exists close semantic relatedness between two
distinct entities, Gates in s2 and Microsoft in s3,
which can be captured by the world knowledge that
Gates is the person who created Microsoft (repre-
sented by Gates-create-Microsoft). In fact, the is-
sue of absence of common entities between adjacent
sentences is quite prevalent. Analyzing the CoNLL
2012 dataset (Pradhan et al., 2012), we found that
42.34% of the time, adjacent sentences do not share
common entities. As a result, methods which rely
on strict entity matching would fail on these cases.

1Based on a news item: http://www.cnbc.com/id/101576926

1087



s1: In 1980, [ Gates ]S licensed [ 86-DOS ]O from [ Tim Paterson ]X for 
       $50,000, which marketed it as [ PC-DOS ]X.
s2: [ Gates' smartest move ]S was retaining [ ownership of the source 
         code ]O of what he and [ Allen ]X would develop as [ MS-DOS ]X.
s3: [ Microsoft ]S got a [ licensing fee ]O every time [ IBM ]S sold a [ PC ]O.

s1: In 1980, [ Gates ]S licensed [ 86-DOS ]O from [ Tim Paterson ]X for 
       $50,000, which marketed it as [ PC-DOS ]X.
s2: [ Gates' smartest move ]S was retaining [ ownership of the source 
         code ]O of what he and [ Allen ]X would develop as [ MS-DOS ]X.
s3: [ Microsoft ]S got a [ licensing fee ]O every time [ IBM ]S sold a [ PC ]O.

(a) A fragment of news text

s1

S     -     -     -    S    O   O   O    O    -     -     -     -     - s2

s3

M
o

ve

O
w

n
er

sh
ip

So
u

rc
e

C
o

d
e

M
S-

D
O

S

Fe
e

Ti
m

e

IB
M

P
C

  -     -     -     -     -     -    -     -     -     S    O    X    S    O

M
ic

ro
so

ft

G
at

es

8
6

-D
O

S

P
at

er
so

n

P
C

-D
O

S

S    O    X    X    -     -    -     -      -    -     -     -     -     - s1

S     -     -     -    S    O   O   O    O    -     -     -     -     - s2

s3

M
o

ve

O
w

n
er

sh
ip

So
u

rc
e

C
o

d
e

M
S-

D
O

S

Fe
e

Ti
m

e

IB
M

P
C

  -     -     -     -     -     -    -     -     -     S    O    X    S    O

M
ic

ro
so

ft

G
at

es

8
6

-D
O

S

P
at

er
so

n

P
C

-D
O

S

S    O    X    X    -     -    -     -      -    -     -     -     -     - 

(b) The corresponding entity grid

Figure 1: A news text fragment with its corresponding entity grid constructed following B&L (2008). Al-
though s2 and s3 share no entity, their transition is still coherent, because Gates and Microsoft are semanti-
cally related by the knowledge Gates-create-Microsoft.

We wish to incorporate semantic relatedness be-
tween different entities into existing models to tackle
the problem described above. In particular, we pro-
pose to capture such semantic relatedness between
different entities with world knowledge represented
as triples, e.g., Gates-create-Microsoft. Given a
text to be evaluated, we first retrieve relevant world
knowledge from multiple sources. For the unsuper-
vised framework of G&S, we integrate knowledge
into the original graph-based document representa-
tion, in which sentences are the nodes and edges are
formed by shared entities and our world knowledge.
Then, we adopt a dynamic programming algorithm
to produce a coherence score for the text. For the
supervised framework of B&L, we incorporate the
world knowledge as a novel set of features into the
original entity-based model, and train a model to dis-
criminate different degrees of text coherence.

To evaluate the impact of incorporating seman-
tic relatedness, we conduct experiments on two
datasets, each of which resembles a real sub-task
in the text coherence modeling: sentence ordering
and summary coherence rating. On both tasks,
across two frameworks, supervised and unsuper-
vised, we perform a direct comparison between our
enhanced model and the original one. On both tasks,
our models are shown to be more powerful than the
models relying on entity matching only. Moreover,
for sentence ordering, world knowledge is shown to
be especially useful on short documents.

2 Background

2.1 Entity-based local coherence modeling

The initial entity-based model was developed by
B&L. It is based on the intuition that there exists

a canonical order of how entities occur in the text.
Therefore, we can model text coherence by measur-
ing how mentions of various entities are distributed
within the text. Specifically, for a given document d,
an entity grid is constructed in which the rows rep-
resent the sentences and the columns represent enti-
ties. Each grid cell ri j corresponds to the syntactic
role of entity e j in sentence si: subject (S ), object
(O), other (X), or nothing (−). For example, Figure
1b shows the entity grid of the text shown in Figure
1a. If an entity serves multiple syntactic roles in a
sentence, its grammatical role is resolved according
to the priority order: S � O � X � −.

Based on the entity grid representation, a lo-
cal coherence transition is defined as a sequence
{S ,O, X,−}n, representing the grammatical roles or
absence of a particular entity across n adjacent sen-
tences. Then, the document is encoded as a feature
vector Φ(d) = (p1(d), p2(d), . . . , pm(d)), where pt(d)
is the normalized frequency of the transition t in the
entity grid, and m is the number of predefined tran-
sitions. pt(d) is computed as the number of occur-
rences of transition t among all entities in the entity
grid, divided by the total number of transitions of the
same length. Using this feature encoding, the model
is then trained as a preference ranking problem be-
tween documents of different degrees of coherence.

2.2 Graph-based local coherence modeling

As mentioned previously, most extensions to the
entity-based local coherence model focus on enrich-
ing the feature set (Filippova and Strube, 2007; El-
sner and Charniak, 2011; Lin et al., 2011; Feng et
al., 2014), all of which follow a supervised learning
framework. To the best of our knowledge, the only
exception is the unsupervised method proposed by

1088



G&S, which transforms the entity grid into a sen-
tence graph and measures text coherence by comput-
ing the average out-degree of the graph. For a docu-
ment d, its entity grid is constructed first, following
the method described in Section 2.1. Then, a bipar-
tite graph G = (Vs,Ve, L,W) is constructed, where
Vs is the set of nodes representing sentences in the
text; Ve is the set of nodes representing entities; L
is the set of edges associated with a weight w ∈ W.
An edge exists between a sentence sx and an entity
e, if and only if e occurs in sx. Each edge is further
associated with a weight w(e, sx), determined by the
grammatical role of the entity e in sentence sx: 3 for
subject (S ), 2 for object (O), 1 for other (X), and 0
for nothing (−). Note that graph G consists of both
sentence nodes and entity nodes.

Then, G is converted to another graph P, which
consists of sentence nodes only, where an edge con-
nects two sentence nodes if and only if at least one
entity is shared between these two sentences. In P,
the weight of each edge is computed by aggregating
the edge weights in the original bipartite graph G:

w(P)(sx, sy) =
∑

e∈Exy w
(G)(e, sx) ∗ w(G)(e, sy), (1)

where Exy is the set of entities shared by two sen-
tences sx and sy, and w(G)(e, sx) is the weight of edge
between entity e and sentence sx as illustrated be-
fore. The coherence of the document is thus mea-
sured by the average out-degree of graph P.

Although this method is purely unsupervised, it
achieves a performance comparable with its super-
vised counterparts, e.g., B&L. However, since this
method still relies on matching multiple mentions of
the same entity, it misses the important contribution
from those semantically related yet distinct entities,
e.g. Gates and Microsoft in Figure 1a.

3 Finding relevant world knowledge

To supplement existing models with information de-
rived from semantic relatedness, given a document
d to be evaluated, we first retrieve all world knowl-
edge related to d. There are two major issues for this
process: (1) knowledge sources: where can we ob-
tain this knowledge?, and (2) knowledge selection:
how do we pinpoint the most relevant ones?

Knowledge sources There are two main kinds of
knowledge sources: (1) manually edited knowledge

bases, such as YAGO (Hoffart et al., 2013), which
consists of about 4 million human-edited instances
from on-line encyclopedias such as WikiPedia (De-
noyer and Gallinari, 2007) and FreeBase (Bol-
lacker et al., 2008), and (2) automatically con-
structed knowledge bases, such as Reverb (Fader
et al., 2011), which covers about 20 million in-
stances extracted from raw texts. Generally speak-
ing, manually edited knowledge bases have better
accuracy but lower coverage, while automatically
extracted knowledge bases are the opposite. To
seek a good balance, we use both YAGO and Re-
verb as our knowledge sources. In addition, the
automatically constructed knowledge bases can be
extracted from raw texts of any domain, which
makes our method adaptable. Both sources are pre-
sented in triples, argument1-predicate-argument2,
(e.g., Gates-create-Microsoft), where the two argu-
ments are usually entities and the predicate is the
relation between them (Zhang et al., 2014).

Knowledge selection For each document d, we
then select the subset of relevant knowledge in-
stances, in the sense that they represent relations be-
tween the entities in d. In particular, we extract all
entities in d, and query the knowledge bases to ob-
tain all the knowledge instances in which both of the
two arguments, argument1 and argument2, match
some of the entities in d.

One issue in knowledge selection is whether to
retrieve knowledge instances using exact or partial
matching. For a given pair of entities in the text, the
chance is rather low to find instances in the knowl-
edge bases where the two arguments perfectly match
the pair of entities, because entities in the source
document might appear in aliases or abbreviations.
In contrast, partial matching between arguments and
entities usually increases coverage but at risk of in-
troducing more noise. In our work, to balance ac-
curacy and coverage, when retrieving world knowl-
edge, we use partial matching and form queries only
for those entities realized as noun phrases in the text.

4 World knowledge encoding

4.1 Unsupervised graph-based framework
As described in Section 2.2, G&S represents the text
as a graph and measures the coherence by the aver-
age out-degree of the graph. In this part, we describe

1089



S1 S2 S3

e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4

Gates MS-DOS Microsoft PC Product

S1 S2 S2

S1 S2 S2

S1 S2 S2

(a) Entity graph with background knowledge

b-1. Without BK

b-2. With BK between adjacent sentence

(b) Sentence graph

b-3. With BK between non-adjacent sentence

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

e1 e1 e3 e4 e5e2

(a)-1

(a)-2

S1 S2 S3

S1 S2 S3

S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-3

S1 S2 S3

S1 S2 S3

S1 S2 S3

(b)-1

(b)-2

(b)-3

(a) Entity graph (b) Sentence graph

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3S1

S2

S3

S1

S2

S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

(a) Traditional entity graph relying on entity matching

S1 S2 S3

e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4

Gates MS-DOS Microsoft PC Product

S1 S2 S2

S1 S2 S2

S1 S2 S2

(a) Entity graph with background knowledge

b-1. Without BK

b-2. With BK between adjacent sentence

(b) Sentence graph

b-3. With BK between non-adjacent sentence

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

e1 e1 e3 e4 e5e2

(a)-1

(a)-2

S1 S2 S3

S1 S2 S3

S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-3

S1 S2 S3

S1 S2 S3

S1 S2 S3

(b)-1

(b)-2

(b)-3

(a) Entity graph (b) Sentence graph

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3S1

S2

S3

S1

S2

S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3

S1 S2 S3

S1 S2 S3

(b) Traditional sentence graph converted from (a)

S1 S2 S3

e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4

Gates MS-DOS Microsoft PC Product

S1 S2 S2

S1 S2 S2

S1 S2 S2

(a) Entity graph with background knowledge

b-1. Without BK

b-2. With BK between adjacent sentence

(b) Sentence graph

b-3. With BK between non-adjacent sentence

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

e1 e1 e3 e4 e5e2

(a)-1

(a)-2

S1 S2 S3

S1 S2 S3

S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-3

S1 S2 S3

S1 S2 S3

S1 S2 S3

(b)-1

(b)-2

(b)-3

(a) Entity graph (b) Sentence graph

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3S1

S2

S3

S1

S2

S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

(c) Our entity graph encoding relatedness between entities

S1 S2 S3

e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4

Gates MS-DOS Microsoft PC Product

S1 S2 S2

S1 S2 S2

S1 S2 S2

(a) Entity graph with background knowledge

b-1. Without BK

b-2. With BK between adjacent sentence

(b) Sentence graph

b-3. With BK between non-adjacent sentence

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

e1 e1 e3 e4 e5e2

(a)-1

(a)-2

S1 S2 S3

S1 S2 S3

S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-3

S1 S2 S3

S1 S2 S3

S1 S2 S3

(b)-1

(b)-2

(b)-3

(a) Entity graph (b) Sentence graph

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

S1

S2

S3

Source target

e1 e1 e3 e4 e5e2

(a)-1

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

(a)-2

S1 S2 S3 S1 S2 S3

(b)-1

(b)-2

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3S1

S2

S3

S1

S2

S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3 S1 S2 S3

e1 e1 e3 e4 e5e2

S1 S2 S3

S1 S2 S3

S1 S2 S3

(d) Our sentence graph converted from (c)

Figure 2: (a) and (b) show the traditional entity and sentence graph based on matching multiple mentions
of the same entity; while (c) and (d) represent our entity and sentence graph encoding semantic relatedness
between those semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft) by
adding world-knowledge edges (dashed lines) accroding to world knowledge (e.g., Gates-create-Microsoft).

how we capture semantic relatedness by encoding
world knowledge to the graph-based model. The
outline of our method is as follows. Given a docu-
ment d, we first retrieve relevant world knowledge
from multiple sources (see Section 3). Then, we
construct an entity graph with world knowledge to
capture both the distribution information and seman-
tic relatedness between entities (see Section 4.1.1).
After that, we convert the entity graph into a sen-
tence graph (see Section 4.1.2), in which two sen-
tences are connected not only through common en-
tities but also through world knowledge. Finally, we
apply our novel reachability score computation over
the sentence graph to produce a coherence score
for the text to be evaluated (see Section 4.1.3).

4.1.1 Entity graph

As shown in Figure 2c, there are two kinds of
edges in our entity graph: (1) common-entity edges
(solid lines), which connect different mentions of the
same entity, such as Gates in s1 and Gates in s2; (2)
world-knowledge edges (dashed lines), which con-
nect different entities through certain world knowl-
edge, such as Gates in s2 and Microsoft in s3 re-
lated by Gates-create-Microsoft. This representa-
tion captures not only the distribution information
of individual entities but also the semantic related-
ness between different entities. In contrast, the orig-
inal graph-based model by G&S (Figures 2a and
2b) includes common-entity edges only and misses
the semantic relatedness information. Formally, for
a document d, we define its entity graph as G =
(V, Lm, Lk,Wm,Wk), where V denotes the nodes of

entities; Lm denotes the set of common-entity edges
and Lk denotes the set of world-knowledge edges;
and Wm and Wk are the two sets of weights associ-
ated with Lm and Lk respectively.

Following G&S, for each common-entity edge
lm ∈ Lm, which connects two mentions of the same
entity e appearing in different sentences sx and sy,
we compute its weight as w(e, sx) × w(e, sy), where
the value of w(e, sx) is based on the grammatical
role of the entity e in the sentence sx as follows:
3 for subject (S ), 2 for object (O), 1 for other (X),
and 0 for nothing(−). When multiple mentions of
the same entity appear with different grammatical
roles in the same sentence, the role with the high-
est weight is chosen to represent the entity. For each
world-knowledge edge lk ∈ Lk, which connects two
different entities eix and e jy appearing in sentence sx
and sy respectively, we consider three factors when
assigning the weight to the edge: (1) semantic relat-
edness between eix and e jy: higher relatedness leads
to a higher weight; (2) the grammatical roles of eix
and e jy in sx and sy: different roles correspond to dif-
ferent weights; and (3) textual distance between eix
and e jy: longer distance results in a lower weight.
Therefore we compute the weight of lk between eix
and e jy as below:

wk(eix, e jy) =
r(ei, e j) × w(eix, sx) × w(e jy, sy)

d(eix, e jy) × 2 , (2)

where w(eix, sx) is associated to the grammatical role
of eix in sx as illustrated before; d(eix, e jy) is the dis-
tance between eix and e jy; and r(ei, e j) is the seman-
tic relatedness between eix and e jy as shown in For-

1090



mula 3 below. Note that the value of r(ei, e j) is in-
dependent of the sentence in which ei and e j appear,
so we denote it as r(ei, e j) instead of r(eix, e jy).

r(ei, e j) =


log n(ei, e j)

max
lmn∈Lk

log n(em, en)
if n(ei, e j) > 2,

0 otherwise.
(3)

where n(ei, e j) corresponds to the number of world
knowledge instances relating ei and e j. For instance,
Figure 1a contains an edge between Gates (e1 in
s1) and Microsoft (e2 in s3), in which w(e11, s1)
and w(e23, s3) are 3, and d(e11, e23) is 2. Note that
we consider the grammatical roles in both com-
mon edges and background knowledge edges be-
cause we treat them independently from each other.
The grammatical information is important to both of
these two kinds of edges.

4.1.2 Sentence graph

Figure 2d shows the sentence graph converted
from the entity graph in Figure 2c. In our
work, in order to incorporate world knowledge, we
adopt an enriched representation of sentence graph,
G′ = (V ′, L′m, L′k,W

′
m,W

′
k), where V

′ is the sentence
nodes; L′m denotes the set of common-entity edges
(solid lines); L′k denotes the set of world-knowledge
edges (dashed lines), and W′m and W′k correspond to
the weights associated with the edges in L′m and L′k.

Intuitively, the semantic relatedness between two
sentences can be measured as the total relatedness
of each entity pair in the two sentences. There-
fore, in our enhanced sentence graph representa-
tion, for a pair of sentences sx and sy, the weight
of their common-entity edge, w′m(sx, sy), is com-
puted as w′m(sx, sy) =

∑
ei∈Vxy wm(eix, eiy), where Vxy

is the set of entities shared by two sentences sx
and sy, and wm(eix, eiy) is the weight of the corre-
sponding common-entity edge in the entity graph
(see Section 4.1.1). Similarly, the weight of their
world-knowledge edge, w′k(sx, sy), is computed as
w′k(sx, sy) =

∑
ei∈Vx,e j∈Vy wk(eix, e jy), where Vx and

Vy denote the set of entities in sx and sy respectively,
and wk(eix, e jy) is the weight of the corresponding
world-knowledge edge in the entity graph.

Algorithm 1: Reachability score computation.
Input: G′ = (V ′, L′m, L′k,W

′
m,W

′
k).

Output: The final reachability score S .
1 n← |V ′|
2 for j = 1→ n do
3 score(v′j)← 0
4 for j = 1→ n do
5 for i = 0→ j − 1 do
6 if l′m(i, j) ∈ L′m then
7 score(v′j)← score(v′i) + w′m(i, j)
8 if l′k(i, j) ∈ L′k then
9 score(v′j)← score(v′i) + w′k(i, j)

10 return S =

∑
v′j∈V′∧out(v′j)=0 score(v

′
j)

|{v′j : v′j ∈ V ′ ∧ out(v′j) = 0}|

4.1.3 Reachability score computation

Based on our sentence graph representation, we
compute a reachability score for each sentence node.
To produce a final coherence score for the text,
we compute the average reachability score among
those nodes whose out-degree is equal to 0 in the
graph, rather than among all nodes, because of the
intuition that, if a sentence node has no subsequent
nodes, their reachability score therefore reflects the
tightness between this sentence and the preceding
part of the text. For a certain sentence node v′j,
its reachability score is defined as the sum of edge
weights on all paths from the starting node (i.e., the
first sentence) to v′j, and the contribution of each path
to the final reachability score depends on the total
weight of that path as shown in Equation 4.

score(v′j) =∑
v′i∈V′,v′i,v′j

(score(v′i) + w
′
m(i, j) + w

′
k(i, j)) (4)

where score(v′i) denotes the reachability score of
v′i , and w

′
m(i, j) and w

′
k(i, j) are the weights of the

common-entity edge and the world-knowledge edge
between v′i and v

′
j; if there is no such edge in the

graph, the corresponding weight is set to 0.
Algorithm 1 summarizes our reachability score

computation, in which the reachability score of each
node is initially 0, and iteratively updated.

1091



S1 S2 S2

(a) Entity graph (b) Sentence graph

S1 S2 S2

S1 S2 S2

S1 S2 S2

S1 S2 S2

S1 S2 S2

S1 S2 S2

S1 S2 S2

S1 S2 S2 S1 S2 S2

S1 S2 S2 S1 S2 S2

S1 S2 S2 S1 S2 S2

S1 S2 S2 S1 S2 S2

Figure 3: Eight patterns of how world knowledge is
distributed among three adjacent sentences.

4.2 Supervised entity-based framework

As mentioned previously, numerous extensions have
been proposed to the original entity-based model of
B&L. However, those extensions mostly rely on en-
tity matching and thus fail to incorporate the infor-
mation from semantically related yet distinct enti-
ties. We propose a novel extension by introducing
world knowledge to capture entity-wise relatedness.

Inspired by the original entity-based model, in
which local coherence is reflected by the patterns of
how entities act grammatically from one sentence to
the next, we believe that local coherence can also be
characterized by the patterns of how world knowl-
edge relates a pair of sentences. Specifically, given
a set of sentences, there are different patterns of how
knowledge instances are distributed among them.
We consider modeling those patterns within a win-
dow of 3 sentences, in which there are 23 = 8 differ-
ent distribution patterns, as shown in Figure 3. We
then use the frequencies of these distribution pat-
terns over the entire document as additional features
into the entity-based model. In particular, for each
particular distribution pattern bk, its corresponding

frequency p(bk) =
|bk|
|V ′| − 2, where |bk| is the number

of occurrences of bk in the sentence graph.

In addition to p(bk), we also compute another fea-
ture, p(E), which is the frequency that two nodes
are connected by certain world knowledge over the
sentence graph, reflecting the overall semantic re-
latedness within the graph. p(E) is computed as

p(E) =
|L′k|
|V ′| . With these world knowledge features

incorporated into the original entity-based model,
we obtain an enhanced model with an emphasis on
semantic relatedness between different entities.

5 Experiments

To evaluate the impact of incorporating semantic re-
latedness, we conduct experiments on two datasets,
each of which resembles a real sub-task in modeling
text coherence: sentence ordering and summary
coherence rating. Since text coherence is a rela-
tive concept rather than a binary distinction, in both
tasks, we formulate the problem as pairwise prefer-
ence ranking. Specifically, given a set of texts with
different degrees of coherence, we train a ranker to
prefer the more coherent text over the less coherent
one. Performance is therefore measured as the frac-
tion of correct pairwise rankings as recognized by
the ranker. We use SVMlight (Joachims, 2002) with
the ranking configuration to train and evaluate our
models, with all parameters set to default values.

On both tasks, across two frameworks, supervised
and unsupervised, we directly compare our modified
model against the original one, i.e., B&L in the su-
pervised framework and G&S in the unsupervised
framework. In our experiments, we use the Stan-
ford parser (Marneffe et al., 2006) to automatically
extract the grammatical role for each entity mention.

5.1 Sentence ordering

The task of sentence ordering attempts to simu-
late the situation where, given a predefined set of
information-bearing items, we need to determine the
best order to arrange those items.

In this paper, we follow G&S and introduce
CoNLL 20122 (Pradhan et al., 2012) as our dataset,
which is composed of documents from multiple
news sources. For each text, we randomly shuffle its
sentences to generate 20 permutations with incorrect
sentence order. For a fair comparison, we also evalu-
ate our model on a filtered subset of documents with
an average length of 31.8 sentences. Therefore, our
dataset contains 72 documents and 72 × 20 = 1440
permutations, among which the shortest one con-
tains 25 sentences. For our enhanced graph-based
model (introduced in Section 4.1), which is purely
unsupervised, we evaluate our model over the entire
dataset. For our enhanced entity-based model (in-
troduced in Section 4.2), which is purely supervised,
we use half of the complete CoNLL dataset for train-
ing (237 documents plus permutations) and use half

2http://conll.cemantix.org/2012/data.html

1092



of the filtered subset (36 documents plus permuta-
tions). The training and test sets do not overlap.

In this task, each training and test instance is com-
posed of a pair of a source document and one of
its permutations, and the source document is always
considered more coherent than its permutation.

5.2 Summary coherence rating
The second task is summary coherence rating, in
which, given a pair of summaries about the same
set of source documents, we determine the rank-
ing of these two summaries based on their de-
grees of coherence. The performance of the model
is assessed by comparing model-induced rankings
against the rankings given by human judges. We use
the same dataset (DUC 2003) as B&L and G&S did,
which consists of summaries generated either by
human writers or by automatic summarization sys-
tems. Each summary was given a coherence score
by averaging among seven judges. Often, machine-
generated summaries receive low coherence scores
because they contain sentences taken out of context
and thus display problems with respect to coherence.

This dataset consists of 16 input document clus-
ters, each of which is associated with five machine-
generated summaries along with a human-written
summary. In total, we have 96 summaries (for more
details, see B&L). We form pairwise rankings by
taking any two summaries originating from the same
document cluster, given that the two summaries re-
ceive different coherence scores: 144 of the resulting
rankings are used for training and 80 are for testing.

5.3 Experiment results
In this section, we demonstrate the performance of
our models with world knowledge encoded in one of
the two ways: paths in a sentence graph or features
in an entity grid. We compare our models against
the original graph-based model (G&S) and entity-
based model (B&L). The evaluation is conducted on
the two tasks, sentence ordering and summary co-
herence rating, and the accuracy is the fraction of
correct pairwise rankings.

Table 1 shows the performance of various models
on both tasks. The first section shows the results of
G&S’s graph-based local coherence model, includ-
ing the performance reported in their original paper
and that achieved by our re-implementation, repre-

Model SO SCR
Graph model (G&S) 88.9 80.0
Graph model (Implemented) 89.6 48.8
Graph model + K 91.3** 50.0
Graph model + K + Avg R 93.4** 55.0*
Entity model (B&L) 88.9 83.8
Entity model (Implemented) 93.7 90.0
Entity model + K 95.1** 91.3

Table 1: Accuracies (%) of various models on
the two tasks, sentence ordering (SO) and sum-
mary coherence rating (SCR). Models that per-
form significantly better than their corresponding re-
implemented basic models are denoted by ** (p <
.01) or * (p < .05), verified using paired t-test.

senting the effect with no world knowledge encoded.
The second section shows the performance of our
two graph-based models with world knowledge en-
coded. Graph model + K is the basic model with
world knowledge encoded, but coherence is simply
measured as the average out-degree as in G&S’s ap-
proach. Graph model + K + Avg R replaces the
out-degree measurement by our average reachabil-
ity score (described in Section 4.1.3), which mea-
sures coherence in a more sophisticated way. The
third section shows the results of B&L’s entity-based
local coherence model, including the originally re-
ported performance and that obtained by our re-
implementation, in which no world knowledge fea-
tures are included. The last section, Entity model +
K, shows the result of entity-based model with our
world knowledge features encoded. Note that the
random baseline of both tasks is 50%.

Firstly, for graph-based models, our Graph model
+ K outperforms the original models, suggesting
that world knowledge is truly helpful for capturing
more coherence information3. Moreover, by intro-

3The large discrepancy between the performance reported
by G&S and that of our re-implementation in Task 2 is due to
the fact that G&S experimented with a set of specially formed
summary pairs (see their paper for detail), which we have no
access to. They also did not give sufficient details about how
they constructed those summary pairs, which has a great im-
pact on the final result. This made it difficult for us to fully re-
implement their experiment. So we use B&L’s set of summary
pairs, which are generated randomly and are more difficult to
distinguish, which explains our differing results from theirs.

1093



Figure 4: Graph-based models, with and without
world knowledge (labeled as With K and Without K),
tested on sets with different numbers of sentences.

ducing the scoring scheme of average reachability
score, our Graph model + K + Avg R achieves the
best performance among all graph-based models.

Secondly, for entity-based models, our en-
hanced model with knowledge features encoded
also achieves superior performance than our re-
implemented model, again confirming the useful-
ness of world knowledge. Interestingly, we observe
that our re-implementation obtains higher accuracy
compared to the performance reported by B&L. This
is partly due to the fact that the documents in our
dataset have an average length of 31.5 sentences,
which are longer than those used in B&L’s experi-
ments. We will further discuss this problem in Sec-
tion 5.4 and show that document length is an impor-
tant factor to the overall performance.

However, on the task of summary coherence rat-
ing, the difference between our extended models and
the original ones is generally not significant, primar-
ily due to the fact that the sample size for this task is
too small, i.e., 80 pairwise rankings.

5.4 Effect of document length

5.4.1 Effect of document length on the overall
performance

We further analyze the impact of document length
on the task of sentence ordering. We partition our
original dataset, which consists of 214 documents
and their permutations, into 8 non-overlapping sub-
sets, according to the length of documents: 1-5, 6-
10,. . . , and 36-40 sentences. To illustrate the cor-
relation between the performance and the length

Model Accuracy (%)
Graph Model 27.4
Graph Model + K 44.2
Entity Model 65.8
Entity Model + K 71.1

Table 2: Performance of various models with and
without world knowledge in the sentence ordering
task, tested on short documents with 1–5 sentences.

of document, we test our models with and with-
out world knowledge encoded on each subset sepa-
rately. Since the size of the available training data in
each subset is relatively small, the supervised entity-
based model suffers from sparsity. Therefore, we fo-
cus on the unsupervised graph-based models only.

Figure 4 shows the performance on different sub-
sets. We can see that the performance of both
models generally improves as the number of sen-
tences increases. This observation is quite intuitive,
because the longer a document is, the higher the
chance is that, after being shuffled, adjacent sen-
tences in the resulting permutation would be com-
pletely irrelevant to each other. Therefore, for longer
documents, it is much easier for the model to distin-
guish a permutation from its source document.

5.4.2 Effect of document length on the model
with world knowledge

Moreover, we also observe that the document
length has a non-universal effect, in terms of how
the model could benefit from incorporating world
knowledge. Specifically, we find that world knowl-
edge has a greater effect on short documents, as
demonstrated in Table 2. Evaluated on a set of doc-
uments composed of 30 extremely short documents
only (1–5 sentences), we see that our enhanced
graph-based model is able to improve the perfor-
mance by 16.8% over the basic model, and our en-
hanced entity-based model achieves 5.3% improve-
ment (both differences are significant at p < .01).
We postulate that it is primarily because a document
with fewer sentences tends to shift to another sub-
topic immediately without elaborating on the previ-
ous one, and strict entity matching would find it dif-
ficult to establish coherent transitions between them.
Therefore, the contribution from semantic related-
ness tends to dominate the overall performance.

1094



6 Conclusions and future work

In this paper, for the evaluation of text coherence, we
go beyond strict entity matching and model the se-
mantic relatedness between distinct entities through
the use of world knowledge. Specifically, we in-
corporate world knowledge into two existing frame-
works: (1) the unsupervised graph-based model
(G&S), and (2) the supervised entity-grid model
(B&L). Across the two frameworks, on both of our
evaluation tasks, sentence ordering and summary
coherence rating, our enhanced models with world
knowledge encoded are shown to be stronger than
the corresponding basic models, confirming that se-
mantic relatedness is truly important for coherence
modeling and such relatedness can be effectively
captured by world knowledge. Moreover, we ob-
serve that world knowledge is particularly useful for
short documents in sentence ordering, as it provides
additional clues to relate sub-topics in the text.

In our future work, we wish to explore the ef-
fect of our world knowledge in conjunction with dis-
course relations. Specifically, we plan to incorporate
world knowledge into the framework of discourse
role matrix (Lin et al., 2011; Feng et al., 2014). In
addition, we also plan to develop a more sophis-
ticated feature encoding by distinguishing different
types of predicates in world knowledge triples.

Acknowledgments

We would like to thank Mao Zheng and Yanyan
Zhao for their great help. This work was partly
supported by National Natural Science Founda-
tion of China via grant 61133012, the National
863 Leading Technology Research Project via grant
2012AA011102 and the National Natural Science
Foundation of China Surface Project via grant
61273321.

References

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1–34.

Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2002. Inferring strategies for sentence or-
dering in multidocument news summarization. Jour-
nal of Artificial Intelligence Research, 17(1):35–55.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, pages 1247–1250. ACM.

Ludovic Denoyer and Patrick Gallinari. 2007. The
Wikipedia XML corpus. In Comparative Evaluation
of XML Information Retrieval Systems, pages 12–19.
Springer.

Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies:
Short Papers, pages 41–44. Association for Computa-
tional Linguistics.

Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 125–129.
Association for Computational Linguistics.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1535–1545. Association for Computational Linguis-
tics.

Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst. 2014.
The impact of deep hierarchical discourse structures
in the evaluation of text coherence. In Proceedings of
the 25th International Conference on Computational
Linguistics, pages 940–949.

Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139–142. Association for Computational Lin-
guistics.

Camille Guinaudeau and Michael Strube. 2013. Graph-
based local coherence modeling. In Proceedings of the
51st Annual Meeting of the Association for Computa-
tional Linguistics, pages 93–103.

Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. Yago2: a spatially and
temporally enhanced knowledge base from Wikipedia.
Artificial Intelligence, 194:28–61.

Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133–142.
ACM.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse

1095



relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 997–
1006. Association for Computational Linguistics.

M. Marneffe, B. Maccartney, and C. Manning. 2006.
Generating typed dependency parses from phrase
structure parses. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation, volume 6, pages 449–454. European Language
Resources Association (ELRA).

Neil McIntyre and Mirella Lapata. 2010. Plot induction
and evolutionary search for story generation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1562–1572.
Association for Computational Linguistics.

Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(1):25–55.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in ontonotes. In Joint Conference on
EMNLP and CoNLL-Shared Task, pages 1–40. Asso-
ciation for Computational Linguistics.

Muyu Zhang, Bing Qin, Ting Liu, and Mao Zheng. 2014.
Triple based background knowledge ranking for doc-
ument enrichment. In Proceedings of the 25th In-
ternational Conference on Computational Linguistics,
pages 917–927.

1096


