614

Coling 2010: Poster Volume, pages 614–622,

Beijing, August 2010

A Linguistically Grounded Graph Model for Bilingual Lexicon

Extraction

Florian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,

Ulrich Heid, Hinrich Sch¨utze

Institute for Natural Language Processing

Universit¨at Stuttgart

{lawsfn,michells,dorowbe}@ims.uni-stuttgart.de

Abstract

We present a new method, based on
graph theory, for bilingual
lexicon ex-
traction without relying on resources with
limited availability like parallel corpora.
The graphs we use represent
linguis-
tic relations between words such as ad-
jectival modiﬁcation. We experiment
with a number of ways of combining
different linguistic relations and present
a novel method, multi-edge extraction
(MEE), that is both modular and scalable.
We evaluate MEE on adjectives, verbs
and nouns and show that it is superior
to cooccurrence-based extraction (which
does not use linguistic analysis). Finally,
we publish a reproducible baseline to es-
tablish an evaluation benchmark for bilin-
gual lexicon extraction.

1 Introduction

Machine-readable translation dictionaries are an
important resource for bilingual tasks like ma-
chine translation and cross-language information
retrieval. A common approach to obtaining bilin-
gual translation dictionaries is bilingual lexicon
extraction from corpora. Most work has used
parallel text for this task. However, parallel cor-
pora are only available for few language pairs and
for a small selection of domains (e.g., politics).
For other language pairs and domains, monolin-
gual comparable corpora and monolingual lan-
guage processing tools may be more easily avail-
able. This has prompted researchers to investigate
bilingual lexicon extraction based on monolingual
corpora (see Section 2) .

In this paper, we present a new graph-theoretic
method for bilingual
lexicon extraction. Two
monolingual graphs are constructed based on syn-
tactic analysis, with words as nodes and relations

(such as adjectival modiﬁcation) as edges. Each
relation acts as a similarity source for the node
types involved. All available similarity sources
interact to produce one ﬁnal similarity value for
each pair of nodes. Using a seed lexicon, nodes
from the two graphs can be compared to ﬁnd a
translation.

Our main contributions in this paper are: (i) we
present a new method, based on graph theory,
for bilingual lexicon extraction without relying
on resources with limited availability like paral-
lel corpora; (ii) we show that with this graph-
theoretic framework, information obtained by lin-
guistic analysis is superior to cooccurrence data
obtained without linguistic analysis; (iii) we ex-
periment with a number of ways of combining dif-
ferent linguistic relations in extraction and present
a novel method, multi-edge extraction, which is
both modular and scalable; (iv) progress in bilin-
gual lexicon extraction has been hampered by the
lack of a common benchmark; we therefore pub-
lish a benchmark and the performance of MEE as
a baseline for future research.

The paper discusses related work in Section 2.
We then describe our translation model (Sec-
tion 3) and multi-edge extraction (Section 4). The
benchmark we publish as part of this paper is de-
scribed in Section 5. Section 6 presents our ex-
perimental results and Section 7 analyzes and dis-
cusses them. Section 8 summarizes.

2 Related Work

Rapp (1999) uses word cooccurrence in a vector
space model for bilingual lexicon extraction. De-
tails are given in Section 5.

Fung and Yee (1998) also use a vector space
approach, but use TF/IDF values in the vector
components and experiment with different vec-
tor similarity measures for ranking the translation
candidates. Koehn and Knight (2002) combine

615

a vector-space approach with other clues such as
orthographic similarity and frequency. They re-
port an accuracy of .39 on the 1000 most frequent
English-German noun translation pairs.

Garera et al. (2009) use a vector space model
with dependency links as dimensions instead of
cooccurring words. They report outperforming
a cooccurrence vector model by 16 percentage
points accuracy on English-Spanish.

Haghighi et al. (2008) use a probabilistic model
over word feature vectors containing cooccur-
rence and orthographic features. They then use
canonical correlation analysis to ﬁnd matchings
between words in a common latent space. They
evaluate on multiple languages and report high
precision even without a seed lexicon.

Most previous work has used vector spaces and
(except for Garera et al. (2009)) cooccurrence
data. Our approach uses linguistic relations like
subcategorization, modiﬁcation and coordination
in a graph-based model. Further, we evaluate our
approach on different parts of speech, whereas
some previous work only evaluates on nouns.

3 Translation Model

Our model has two components: (i) a graph repre-
senting words and the relationships between them
and (ii) a measure of similarity between words
based on these relationships. Translation is re-
garded as cross-lingual word similarity. We rank
words according to their similarity and choose the
top word as the translation.

We employ undirected graphs with typed nodes
and edges. Node types represent parts of speech
(POS); edge types represent different kinds of re-
lations. We use a modiﬁed version of SimRank
(Jeh and Widom, 2002) as a similarity measure
for our experiments (see Section 4 for details).

SimRank is based on the idea that two nodes
are similar if their neighbors are similar. We ap-
ply this notion of similarity across two graphs. We
think of two words as translations if they appear
in the same relations with other words that are
translations of each other. Figure 1 illustrates this
idea with verbs and nouns in the direct object rela-
tion. Double lines indicate seed translations, i.e.,
known translations from a dictionary (see Sec-
tion 5). The nodes buy and kaufen have the same

house

magazine

book

thought

buy

read

kaufen

lesen

Haus

Zeitschrift

Buch

Gedanke

Figure 1: Similarity through seed translations

objects in the two languages; one of these (maga-
zine – Zeitschrift) is a seed translation. This re-
lationship contributes to the similarity of buy –
kaufen. Furthermore, book and Buch are similar
(because of read – lesen) and this similarity will
be added to buy – kaufen in a later iteration. By
repeatedly applying the algorithm, the initial sim-
ilarity introduced by seeds spreads to all nodes.

To incorporate more detailed linguistic infor-
mation, we introduce typed edges in addition to
typed nodes. Each edge type represents a linguis-
tic relation such as verb subcategorization or ad-
jectival modiﬁcation. By designing a model that
combines multiple edge types, we can compute
the similarity between two words based on mul-
tiple sources of similarity. We superimpose dif-
ferent sets of edges on a ﬁxed set of nodes; a node
is not necessarily part of every relation.

The graph model can accommodate any kind of
nodes and relations. In this paper we use nodes
to represent content words (i.e., non-function
words): adjectives (a), nouns (n) and verbs (v).
We extracted three types of syntactic relations
from a corpus: see Table 1.

Nouns participate in two bipartite relations
(amod, dobj) and one unipartite relation (ncrd).
This means that the computation of noun similar-
ities will beneﬁt from three different sources.

Figure 2 depicts a sample graph with all node
and edge types. For the sake of simplicity, a
monolingual example is shown. There are four
nouns in the sample graph all of which are (i)
modiﬁed by the adjectives interesting and polit-
ical and (ii) direct objects of the verbs like and

616

example

a, n
v, n
n, n

relation entities description
used in this paper
amod
dobj
ncrd
other possible relations
vsub
poss
acrd

v, n
n, n
a, a

adjectival modiﬁcation
object subcategorization drive a car
noun coordination

a fast car

cars and busses

subject subcategorization a man sleeps
the child’s toy
possessive
adjective coordination
red or blue car

Table 1: Relations used in this paper (top) and
possible extensions (bottom).

verb

adjective

noun

interesting

dobj
amod
ncrd

political

idea

article

book

magazine

pair frequency (≥ 3). We process noun pairs by
applying a frequency threshold on words (≥ 100)
and pairs (≥ 3). Verb-object pairs (the smallest
data set) were not frequency-ﬁltered. Based on
the resulting frequency counts, we calculate asso-
ciation scores for all relationships using the log-
likelihood measure (Dunning, 1993). For noun
pairs, we discard all pairs with an association
score < 3.84 (signiﬁcance at α = .05). For all
three relations, we discard pairs whose observed
frequency was smaller than their expected fre-
quency (Evert, 2004, p. 76).
As a last step,
we further reduce noise by removing nodes of de-
gree 1. Key statistics for the resulting graphs are
given in Table 2.

We have found that accuracy of extraction is
poor if unweighted edges are used. Using the
log-likelihood score directly as edge weight gives
too much weight to “semantically weak” high-
frequency words like put and take. We there-
fore use the logarithms of the log-likelihood score
as edge weights in all SimRank computations re-
ported in this paper.

like

promote

Figure 2: Graph snippet with typed edges

nodes

edges

de
en

a
10,067
12,878
amod

n
34,545
22,257
ncrd

v
2,828
4,866
dobj
de
65,299 417,151 143,906
en 288,889 686,073 510,351

promote. Based on amod and dobj, the four nouns
are equally similar to each other. However, the
greater similarity of article, book, and magazine
to each other can be deduced from the fact that
these three nouns also occur in the relation ncrd.
We exploit this information in the MEE method.
Data and Preprocessing. Our corpus in this
paper is the Wikipedia. We parse all German
and English articles with BitPar (Schmid, 2004)
to extract verb-argument relations. We extract
adjective-noun modiﬁcation and noun coordina-
tions with part-of-speech patterns based on a
version of the corpus tagged with TreeTagger
(Schmid, 1994). We use lemmas instead of sur-
face forms. Because we perform the SimRank
matrix multiplications in memory, we need to ﬁl-
ter out rare words and relations; otherwise, run-
ning SimRank to convergence would not be feasi-
ble. For adjective-noun pairs, we apply a ﬁlter on

Table 2: Node and edge statistics

4 SimRank

Our work is based on the SimRank graph similar-
ity algorithm (Jeh and Widom, 2002). In (Dorow
et al., 2009), we proposed a formulation of Sim-
Rank in terms of matrix operations, which can be
applied to (i) weighted graphs and (ii) bilingual
problems. We now brieﬂy review SimRank and
its bilingual extension. For more details we refer
to (Dorow et al., 2009).

The basic idea of SimRank is to consider two
nodes as similar if they have similar neighbor-
hoods. Node similarity scores are recursively
computed from the scores of neighboring nodes:
the similarity Sij of two nodes i and j is computed

617

as the normalized sum of the pairwise similarities
of their neighbors:

Sij =

c

|N (i)| |N (j)| Xk∈N (i),l∈N (j)

Skl.

where N (i) and N (j) are the sets of i’s and j’s
neighbors. As the basis of the recursion, Sij is set
to 1 if i and j are identical (self-similarity). The
constant c (0 < c < 1) dampens the contribution
of nodes further away. Following Jeh and Widom
(2002), we use c = 0.8. This calculation is re-
peated until, after a few iterations, the similarity
values converge.

For bilingual problems, we adapt SimRank for
comparison of nodes across two graphs A and B.
In this case, i is a node in A and j is a node in B,
and the recursion basis is changed to S(i, j) = 1 if
i and j are a pair in a predeﬁned set of node-node
equivalences (seed translation pairs).

Sij =

c

|NA(i)| |NB(j)| Xk∈NA(i),l∈NB (j)

Skl.

Multi-edge Extraction (MEE) Algorithm To
combine different
information sources, corre-
sponding to edges of different types, in one Sim-
Rank computation, we use multi-edge extrac-
tion (MEE), a variant of SimRank (Dorow et al.,
2009). It computes an aggregate similarity matrix
after each iteration by taking the average similar-
ity value over all edge types T :

Sij =

c

|T | Xt∈T

1

f (|NA,t(i)|)f (|NB,t(j)|) Xk∈NA,t(i),

Skl.

l∈NB,t (j)

f is a normalization function (either f = g,
g(n) = n as before or the normalization discussed
in the next section).

While we have only reviewed the case of un-
weighted graphs, the extended SimRank can also
be applied to weighted graphs.
(See (Dorow et
al., 2009) for details.) In what follows, all graph
computations are weighted.

Square Root Normalization Preliminary ex-
periments showed that SimRank gave too much
inﬂuence to words with few neighbors. We there-
fore modiﬁed the normalization function g(n) =

n. To favor words with more neighbors, we want
f to grow sublinearly with the number of neigh-
bors. On the other hand, it is important that,
even for nodes with a large number of neigh-
bors, the normalization term is not much smaller
than |N (i)|, otherwise the similarity computation
does not converge. We use the function h(n) =

√n∗pmaxk(|N (k)|). h grows quickly for small

node degrees, while returning values close to the
linear term for large node degrees. This guaran-
tees that nodes with small degrees have less inﬂu-
ence on ﬁnal similarity scores. In all experiments
reported in this paper, the matrices ˜A, ˜B are nor-
malized with f = h (rather than using the stan-
dard normalization f = g). In one experiment,
accuracy of the top-ranked candidate (acc@1) was
.52 for h and .03 for g, demonstrating that the
standard normalization does not work in our ap-
plication.

larger

Threshold Sieving For
experiments,
there is a limit to scalability, as the similarity ma-
trix ﬁlls up with many small entries, which take up
a large amount of memory. Since these small en-
tries contribute little to the ﬁnal result, Lizorkin et
al. (2008) proposed threshold sieving: an approxi-
mation of SimRank using less space by deleting
all similarity values that are below a threshold.
The quality of the approximation is set by a pa-
rameter δ that speciﬁes maximum acceptable dif-
ference of threshold-sieved similarity and the ex-
act solution. We adapted this to the matrix formu-
lation by integrating the thresholding step into a
standard sparse matrix multiplication algorithm.

We veriﬁed that this approximation yields use-
ful results by comparing the ranks of exact and ap-
proximate solutions. We found that for the high-
ranked words that are of interest in our task, siev-
ing with a suitable threshold does not negatively
affect results.

5 Benchmark Data Set

Rapp’s (1999) original experiment was carried out
on newswire corpora and a proprietary Collins
dictionary. We use the free German (280M to-
kens) and English (850M tokens) Wikipedias as
source and target corpora. Reinhard Rapp has
generously provided us with his 100 word test set

618

training set
TS100
TS1000

n
.61
.65
.66

a
.31
.28
.14

v
.08
.07
.20

Table 3: Percentages of POS in test and training

(TS100) and given us permission to redistribute
it. Additionally, we constructed a larger test set
(TS1000) consisting of the 1000 most frequent
words from the English Wikipedia. Unlike the
noun-only test sets used in other studies, (e.g.,
Koehn and Knight (2002), Haghighi et al. (2008)),
TS1000 also contains adjectives and verbs. As
seed translations, we use a subset of the dict.cc
online dictionary. For the creation of the sub-
set we took raw word frequencies from Wikipedia
as a basis. We extracted all verb, noun and ad-
jective translation pairs from the original dictio-
nary and kept the pairs whose components were
among the 5,000 most frequent nouns, the 3,500
most frequent adjectives and the 500 most fre-
quent verbs for each language. These numbers are
based on percentages of the different node types
in the graphs. The resulting dictionary contains
12,630 pairs: 7,767 noun, 3,913 adjective and 950
verb pairs. Table 3 shows the POS composition of
the training set and the two test sets. For experi-
ments evaluated on TS100 (resp. TS1000), the set
of 100 (resp. 1000) English words it contains and
all their German translations are removed from the
seed dictionary.

Baseline. Our baseline is a reimplementation
of the vector-space method of Rapp (1999). Each
word in the source corpus is represented as a word
vector, the dimensions of which are words of seed
translation pairs. The same is done for corpus
words in the target language, using the translated
seed words as dimensions. The value of each di-
mension is determined by association statistics of
word cooccurrence. For a test word, a vector is
constructed in the same way. The labels on the
dimensions are then translated, yielding an input
vector in the target language vector space. We
then ﬁnd the closest corpus word vector in the tar-
get language vector space using the city block dis-
tance measure. This word is taken as the transla-
tion of the test word.

We went to great lengths to implement Rapp’s
method, but omit the details for reasons of space.
Using the Wikipedia/dict.cc-based data set, we
achieve 50% acc@1 when translating words from
English to German. While this is somewhat lower
than the performance reported by Rapp, we be-
lieve this is due to Wikipedia being more hetero-
geneous and less comparable than news corpora
from identical time periods used by Rapp.

Publication. In conjunction with this paper we
publish the benchmark for bilingual lexicon ex-
traction described. It consists of (i) two Wikipedia
dumps from October 2008 and the linguistic re-
lations extracted from them, (ii) scripts to recre-
ate the training and test sets from the dict.cc
data base, (iii) the TS100 and TS1000 test sets,
and (iv) performance numbers of Rapp’s system
and MEE. These can serve as baselines for fu-
ture work. Note that (ii)–(iv) can be used in-
dependently of (i) – but in that case the effect
of the corpus on performance would not be con-
trolled. The data and scripts are available at
http://ifnlp.org/wiki/extern/WordGraph

6 Results

In addition to the vector space baseline experi-
ment described above, we conducted experiments
with the SimRank model. Because TS100 only
contains one translation per word, but words can
have more than one valid translation, we manu-
ally extended the test set with other translations,
which we veriﬁed using dict.cc and leo.org. We
report the results separately for the original test set
(“strict”) and the extended test set in Table 4. We
also experimented with single-edge models con-
sisting of three separate runs on each relation.

The accuracy columns report the percentage of
test cases where the correct translation was found
among the top 1 (acc@1) or top 10 (acc@10)
candidate words found by the translation mod-
els. Some test words are not present in the data at
all; we count these as 0s when computing acc@1
and acc@10. The acc@10 measure is more use-
ful for indicating topical similarity while acc@1
measures translation accuracy.

MRR is Mean Reciprocal Rank of correct trans-
lations: 1
(Voorhees and Tice, 1999).
MRR is a more ﬁne-grained measure than acc@n,

ranki

1

nPn

i

619

TS100, strict

acc@1

.50
.44
.52

acc@10 MRR
.56
.52
.62

.67
.67
.79†

TS100, extended

acc@1

.54
.49
.58

acc@10 MRR
.60
.56
.68

.70
.68
.82†

TS1000
acc@10 MRR
.41
.50
.58

.56
.70‡
.76‡

acc@1

.33
.40‡
.48‡

baseline
single
MEE

Table 4: Results compared to baseline∗

e.g., it will distinguish ranks 2 and 10. All MRR
numbers reported in this paper are consistent with
acc@1/acc@10 and support our conclusions.

The results for acc@1, the measure that most
directly corresponds to utility in lexicon extrac-
tion, show that the SimRank-based models out-
perform the vector space baseline – only slightly
on TS100, but signiﬁcantly on TS1000. Using the
various relations separately (single) already yields
a signiﬁcant improvement compared to the base-
line. Using all relations in the integrated MEE
model further improves accuracy. With an acc@1
score of 0.48, MEE outperforms the baseline by
.15 compared to TS1000. This shows that a com-
bination of several sources of information is very
valuable for ﬁnding the correct translation.

MEE outperforms the baseline on TS1000 for
all parts of speech, but performs especially well
compared to the baseline for adjectives and verbs
(see Table 5). It has been suggested that vector
space models perform best for nouns and poorly
for other parts of speech. Our experiments seem to
conﬁrm this. In contrast, MEE exhibits good per-
formance for nouns and adjectives and a marked
improvement for verbs.

On acc@10, MEE is consistently better than the
baseline, on both TS100 and TS1000. All three
differences are statistically signiﬁcant.

6.1 Relation Comparison

Table 5 compares baseline, single-edge and MEE
accuracy for the three parts of speech covered.
Each single-edge experiment can compute noun
similarity; for adjectives and verbs, only amod,
dobj and MEE can be used.

Performance for nouns varies greatly depend-
ing on the relation used in the model. ncrd per-

∗We indicate statistical signiﬁcance at the α = 0.05 (†)
and 0.01 level (‡) when compared to the baseline. We did
not calculate signiﬁcance for MRR.

forms best, while dobj shows the worst perfor-
mance. We hypothesize that dobj performs badly
because (i) many verbs are semantically non-
restrictive with respect to their arguments, (e.g.,
use, contain or include) and as a result seman-
tically unrelated nouns become similar because
they share the same verb as a neighbor; (ii) light
verb constructions (e.g., take a walk or give an ac-
count) dilute the extracted relations; and (iii) dobj
is the only relation we extracted with a syntac-
tic parser. The parser was trained on newswire
text, a genre that is very different from Wikipedia.
Hence, parsing is less robust than the relatively
straightforward POS patterns used for the other
relations.

Similarly, many semantically non-restrictive
adjectives such as ﬁrst and new can modify vir-
tually any noun, diluting the quality of the amod
source. We conjecture that ncrd exhibits the best
performance because there are fewer semantically
non-restrictive nouns than non-restrictive adjec-
tives and verbs.

MEE performance for nouns (.45) is signiﬁ-
cantly better than that of the single-edge models.
The information about nouns that is contained in
the verb-object and adjective-noun data is inte-
grated in the model and helps select better trans-
lations. This, however, is only true for the noun

TS100 baseline

amod
ncrd
dobj
MEE

TS1000 baseline

MEE

noun adj verb all
.43 .29 .50
.55
.15
.71
.30
-
-
-
.34
.22
-
.02
.43 .04
.71 .43 .52
.45
.42
.26 .18 .33
.55 .27 .48
.53

Table 5: Relation comparison, acc@1

620

source
dobj
amod
amod+dobj
ncrd+dobj
ncrd
ncrd+amod
MEE

acc@1

acc@10

.02
.15
.22
.32
.34
.49
.45

.10
.37
.43
.65
.60
.74
.77

Table 6: Accuracy of sources for nouns

node type, the “pivot” node type that takes part in
edges of all three types. For adjectives and verbs,
the performance of MEE is the same as that of the
corresponding single-edge model.

We ran three additional experiments each of
which combines only two of the three possible
sources for noun similarity, namely ncrd+amod,
ncrd+dobj and amod+dobj and performed strict
evaluation (see Table 6). We found that in gen-
eral combination increases performance except
for ncrd+dobj vs. ncrd. We attribute this to the
lack of robustness of dobj mentioned above.

6.2 Comparison MEE vs. All-in-one

An alternative to MEE is to use untyped edges in
one large graph. In this all-in-one model (AIO),
we connect two nodes with an edge if they are
linked by any of the different linguistic relations.
While MEE consists of small adjacency matrices
for each type, the two adjacency matrices for AIO
are much larger. This leads to a much denser sim-
ilarity matrix taking up considerably more mem-
ory. One reason for this is that AIO contains simi-
larity entries between words of different parts of
speech that are 0 (and require no memory in a
sparse matrix representation) in MEE.

Since AIO requires more memory, we had to
ﬁlter the data much more strictly than before to be
able to run an experiment. We applied the follow-
ing stricter thresholds on relationships to obtain
a small graph: 5 instead of 3 for adjective-noun

MEEsmall AIOsmall

acc@1
acc@10
MRR

.51
.72
.62

.52
.75
.59

Table 7: MEE vs. AIO

pairs, and 3 instead of 0 for verb-object pairs,
thereby reducing the total number of edges from
2.1M to 1.4M. We also applied threshold sieving
(see Section 4) with δ = 10−10 for AIO. The re-
sults on TS100 (strict evaluation) are reported in
Table 7. For comparison, MEE was also run on
the smaller graph. Performance of the two models
is very similar, with AIO being slightly better (not
signiﬁcant). The slight improvement does not jus-
tify the increased memory requirements. MEE is
able to scale to more nodes and edge types, which
allows for better coverage and performance.

7 Analysis and Discussion

Error analysis. We examined the cases where a
reference translation was not at the top of the sug-
gested list of translation candidates. There are a
number of elements in the translation process that
can cause or contribute to this behavior.

Our method sometimes picks a cohyponym of
the correct translation. In many of these cases, the
correct translation is in the top 10 (together with
other words from the same semantic ﬁeld). For
example, the correct translation of moon, Mond, is
second in a list of words belonging to the semantic
ﬁeld of celestial phenomena: Komet (comet), Mond
(moon), Planet (planet), Asteroid (asteroid), Stern (star),
Galaxis (galaxy), Sonne (sun), . . . While this behavior
is undesirable for strict lexicon extraction, it can
be exploited for other tasks, e.g. cross-lingual se-
mantic relatedness (Michelbacher et al., 2010).

Similarly,

the method sometimes puts the
antonym of the correct translation in ﬁrst place.
For example, the translation for swift (schnell) is
in second place behind langsam (slow). Based
on the syntactic relations we use, it is difﬁcult to
discriminate between antonyms and semantically
similar words if their syntactic distributions are
similar.

Ambiguous source words also pose a problem
for the system. The correct translation of square
(the geometric shape) is Quadrat. However, 8 out
of its top 10 translation candidates are related to
the location sense of square. The other two are ge-
ometric shapes, Quadrat being listed second. This
is only a concern for strict evaluation, since cor-
rect translations of a different sense were included
in the extended test set.

621

bed is also ambiguous (piece of furniture vs.
river bed). This introduces translation candidates
from the geographical domain. As an additional
source of errors, a number of bed’s neighbors
from the furniture sense have the German transla-
tion Bank which is ambiguous between the furni-
ture sense and the ﬁnancial sense. This ambiguity
in the target language German introduces spurious
translation candidates from the ﬁnancial domain.
Discussion. The error analysis demonstrates
that most of the erroneous translations are words
that are incorrect, but that are related, in some ob-
vious way, to the correct translation, e.g. by co-
hyponymy or antonymy. This suggests another
application for bilingual lexicon extraction. One
of the main challenges facing statistical machine
translation (SMT) today is that it is difﬁcult to
distinguish between minor errors (e.g., incorrect
word order) and major errors that are completely
implausible and undermine the users’ conﬁdence
in the machine translation system. For example,
at some point Google translated “sarkozy sarkozy
sarkozy” into “Blair defends Bush”. Since bilin-
gual lexicon extraction, when it makes mistakes,
extracts closely related words that a human user
can understand, automatically extracted lexicons
could be used to discriminate smaller errors from
grave errors in SMT.

As we discussed earlier, parallel text is not
available in sufﬁcient quantity or for all impor-
tant genres for many language pairs. The method
we have described here can be used in such cases,
provided that large monolingual corpora and ba-
sic linguistic processing tools (e.g. POS tagging)
are available. The availability of parsers is a more
stringent constraint, but our results suggest that
more basic NLP methods may be sufﬁcient for
bilingual lexicon extraction.

In this work, we have used a set of seed trans-
lations (unlike e.g., Haghighi et al. (2008)). We
believe that in most real-world scenarios, when
accuracy and reliability are important, seed lexica
will be available. In fact, seed translations can be
easily found for many language pairs on the web.
Although a purely unsupervised approach is per-
haps more interesting from an algorithmic point
of view, the semisupervised approach taken in this
paper may be more realistic for applications.

In this paper, we have attempted to reimplement
Rapp’s system as a baseline, but have otherwise
refrained from detailed comparison with previous
work as far as the accuracy of results is concerned.
The reason is that none of the results published so
far are easily reproducible. While previous publi-
cations have tried to infer from differences in per-
formance numbers that one system is better than
another, these comparisons have to be viewed with
caution since neither the corpora nor the gold stan-
dard translations are the same. For example, the
paper by Haghighi et al. (2008) (which demon-
strates how orthography and contextual informa-
tion can be successfully used) reports 61.7% ac-
curacy on the 186 most conﬁdent predictions of
nouns. But since the evaluation data sets are not
publicly available it is difﬁcult to compare other
work (including our own) with this baseline. We
simply do not know how methods published so far
stack up against each other.

For this reason, we believe that a benchmark
is necessary to make progress in the area of bilin-
gual lexicon extraction; and that our publication of
such a benchmark as part of the research reported
here is an important contribution, in addition to
the linguistically grounded extraction and the new
graph-theoretical method we present.

8 Summary

We have presented a new method, based on graph
theory, for bilingual lexicon extraction without re-
lying on resources with limited availability like
parallel corpora. We have shown that with this
graph-theoretic framework, information obtained
by linguistic analysis is superior to cooccurrence
data obtained without linguistic analysis. We have
presented multi-edge extraction (MEE), a scalable
graph algorithm that combines different linguis-
tic relations in a modular way. Finally, progress
in bilingual lexicon extraction has been hampered
by the lack of a common benchmark. We publish
such a benchmark with this paper and the perfor-
mance of MEE as a baseline for future research.

9 Acknowledgement

This research was funded by the German Re-
search Foundation (DFG) within the project A
graph-theoretic approach to lexicon acquisition.

622

Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49.

Schmid, Helmut. 2004. Efﬁcient parsing of highly
ambiguous context-free grammars with bit vectors.
In COLING ’04, page 162.

Voorhees, Ellen M. and Dawn M. Tice. 1999. The
In

TREC-8 question answering track evaluation.
Proceedings of the 8th Text Retrieval Conference.

References
Dorow, Beate, Florian Laws, Lukas Michelbacher,
Christian Scheible, and Jason Utt. 2009. A graph-
theoretic algorithm for automatic extension of trans-
lation lexicons. In EACL 2009 Workshop on Geo-
metrical Models of Natural Language Semantics.

Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61–74.

Evert, Stefan. 2004. The Statistics of Word Cooccur-
rences - Word Pairs and Collocations. Ph.D. thesis,
Institut f¨ur maschinelle Sprachverarbeitung (IMS),
Universit¨at Stuttgart.

Fung, Pascale and Lo Yuen Yee. 1998. An IR ap-
proach for translating new words from nonparallel,
comparable texts.
In COLING-ACL, pages 414–
420.

Garera, Nikesh, Chris Callison-Burch, and David
Yarowsky. 2009.
Improving translation lexicon
induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL ’09: Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 129–137, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.

Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771–779, Columbus, Ohio, June.
Association for Computational Linguistics.

Jeh, Glen and Jennifer Widom. 2002. Simrank: A
In KDD

measure of structural-context similarity.
’02, pages 538–543.

Koehn, Philipp and Kevin Knight. 2002. Learning a
In
translation lexicon from monolingual corpora.
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition, pages 9–16.

Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,
and Denis Turdakov. 2008. Accuracy estimate and
optimization techniques for simrank computation.
PVLDB, 1(1):422–433.

Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-
rich Heid, and Hinrich Sch¨utze. 2010. Building
a cross-lingual relatedness thesaurus using a graph
similarity measure. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC’10), Valletta, Malta, may.

Rapp, Reinhard. 1999. Automatic identiﬁcation of
word translations from unrelated English and Ger-
man corpora. In COLING 1999.

