775

Coling 2010: Poster Volume, pages 775–781,

Beijing, August 2010

Connective-Based Measuring of the Inter-Annotator Agreement 

in the Annotation of Discourse in PDT

Jiří Mírovský, Lucie Mladová, Šárka Zikánová

Charles University in Prague

Institute of Formal and applied Linguistics

{mirovsky,mladova,zikanova}@ufal.mff.cuni.cz

Abstract

We present several ways of measuring 
the inter-annotator agreement in the on-
going annotation of semantic inter-sen-
tential discourse relations in the Prague 
Dependency   Treebank   (PDT).   Two 
ways have been employed to overcome 
limitations of measuring the agreement 
on   the   exact   location   of   the   start/end 
points of the relations. Both methods – 
skipping one tree level in the start/end 
nodes,   and   the   connective-based   mea-
sure – are focused on a recognition of 
the existence and of the type of the rela-
tions, rather than on fixing the exact po-
sitions of the start/end points of the con-
necting arrows.

1

Introduction

Prague Dependency Treebank 2.0

1.1
The   Prague   Dependency   Treebank   2.0   (PDT 
2.0; Hajič et al., 2006) is a manually annotated 
corpus of Czech. It belongs to the most complex 
and elaborate linguistically annotated treebanks 
in the world. The texts are annotated on three 
layers of language description: morphological, 
analytical (which expresses the surface syntactic 
structure),   and   tectogrammatical   (which   ex-
presses the deep syntactic structure). On the tec-
togrammatical layer, the data consist of almost 
50 thousand sentences.

For the upcoming release of PDT, many addi-
tional features are planned, coming as results of 
several   projects.   Annotation   of   semantic   in-
ter-sentential discourse relations is one of the 
planned additions.

To ensure the highest possible quality of the 
annotated data, it would be best if several anno-

tators annotated the whole data in parallel. After 
solving discrepancies in the annotations of the 
individual   annotators,  we would  get  a high-
-quality annotation. This approach is sometimes 
employed, but most of the times, the available 
resources prohibit it (which is also the case of 
the discourse annotation project). Manual anno-
tation of data is a very expensive and time con-
suming task. To overcome the restriction of lim-
ited resources, each part of the data is annotated 
by one annotator only, with the exception of a 
small overlap for studying and measuring the 
inter-annotator (dis-)agreement.

1.2

Inter-Annotator Agreement in Compu-
tational Linguistics

Measuring   the   inter-annotator   agreement   has 
long been studied (not only) in computational 
linguistics. It is a complex field of research and 
different domains require different approaches.
  Classical   measures  recall,  precision  and  F-
measure offer the most straightforward and in-
tuitively interpretable results. Since they do take 
into account neither the contribution of chance 
in agreement, nor different importance of differ-
ent types of disagreement, etc., other more or 
less elaborate coefficients for measuring the in-
ter-annotator agreement have been  developed. 
Cohen's κ (Cohen, 1960) is suitable for classifi-
cation tasks and tries to measure the agreement 
“above   chance”.   Krippendorff's  α  (Krippen-
dorff, 1980) can be used if we need to distin-
guish various levels of disagreement. Rebecca 
Passonneau (2004) offered a solution for mea-
suring agreement between sets of elements (like 
words in coreferential chains). Variants of these 
coefficients can be used for measuring agree-
ment among more than two annotators. A com-
prehensive overview of methods for measuring 
the inter-annotator agreement in various areas of 

776

computational linguistics was given in Artstein 
and Poesio (2008).

For measuring the inter-annotator agreement 
in   the   annotation   of   semantic   inter-sentential 
discourse relations in PDT, we have chosen two 
measures.   The   relations   do   not   form   natural 
chains   (unlike   e.g.   textual   and   grammatical 
coreference) and a simple  F1-measure  is well 
suited for the agreement on existence of the re-
lations. For the agreement on types of the rela-
tions, which is a typical classification task, we 
use Cohen's κ.

Our research has then been focused not on 
“how to measure” the agreement (which coeffi-
cient to use), but rather on “what to measure” 
(which phenomena), which is the topic of this 
paper.

2 Annotated Phenomena
Since the Prague Dependency Treebank 2.0 al-
ready contains three layers of linguistic annota-
tion, two of which (the analytical layer – surface 
syntax, and the tectogrammatical layer – under-
lying syntax and semantics) are tree representa-
tions,   we   took   advantage   of   these   existing 
analyses and carry  out the annotation  of dis-
course phenomena directly on the trees (the tec-
togrammatical layer). It means that we capture 
the   discourse   relation   between   any   two 
(sub)trees in the document by drawing a link (an 
arrow)   between   the   highest   nodes   in   the 
(sub)trees, see Figure 1.

Figure 1. A discourse arrow between two nodes 

represents a discourse relation between two 

trees – subtrees of the nodes.

Discourse relations we annotate are in princi-
ple semantic relations that apply between two 
abstract   objects   (Asher,   1993)   (i.e.  discourse 
units or text spans) and help make the text a co-
herent whole. These relations are often signaled 
by the presence of a discourse connective, i.e. 
 “tedy”, 
expressions   as
“ovšem” 
(in   English  “but”,  “although”, 
“then”, “however” etc. In the first phase of the 
project,   we   only   annotate   relations   (link   the 
(sub)trees) where such a connective is present.

 ačkoliv”,

 “ale”,

Every  relation  gets  assigned two important 
attributes:   first,   the   discourse   connective   that 
anchors the relation, and, second, the semantic 
type of the relation. For assigning semantic rela-
tions in the discourse, we developed a set of 22 
discourse-semantic tags (Mladová et al., 2009). 
It is inspired partly by the set of semantic labels 
used for the annotation of the tectogrammatical 
layer in  PDT 2.0, relations within the sentence 
(the   tectogrammatical   syntactico-semantic   la-
bels called functors,  Mikulová et al., 2005) – 
since some of the semantic relations apply also 
intra-sententially, like causal or contrastive rela-
tions; and partly by the set of semantic tags in 
the Penn Discourse Treebank 2.0 (Prasad et al., 
2008), a discourse annotation project for Eng-
lish with similar aims.

Hence, there are three important issues for 
the   inter-annotator   measurement   on   the   dis-
course level of annotation in PDT: the agree-
ment on the start and target nodes of the dis-
course relation (and so the extent of the dis-
course arguments), the agreement  on the dis-
course connective assigned to the relation, and, 
last but not least, the agreement on the semantic 
type of the relation.

3 Measuring   the   Inter-Annotator 
Agreement in the Annotation of Dis-
course in PDT 2.0

Simple (Strict) Approach

3.1
The basic method we use for measuring the in-
ter-annotator   agreement   requires   a   perfect 
match in the start and end points of the rela-
tions.   We   calculate  recall  and  precision  be-
tween the two annotators. Since these measures 
are not symmetric in respect to the annotators, 
we use their combination – F1-measure – which 
is symmetric. At each node, we compare target 

777

nodes of the discourse relations created by the 
two annotators. We consider two relations to be 
in agreement strictly only if they share both the 
start node and the target node.

A second number we measure is an agree-
ment on the relation and the type. For consider-
ing two relations to be in agreement, we require 
that they share their start and target nodes, and 
also have attached the same type. 

Similarly, we measure an agreement on the 
relation and the connective, and an agreement 
on the relation, the type and the connective.

Attaching a type to a relation can be under-
stood as a classification task. We calculate two 
numbers – simple ratio agreement and Cohen's 
κ  –   on   the   types   attached   to   those   relations 
where the annotators agreed on the start and the 
target   nodes.   Cohen's  κ  shows   the   level   of 
agreement on the types above chance.

For completeness, we also calculate simple 
ratio agreement on the connectives attached to 
those relations the annotators agreed on. 

Table 1 shows results of these measurements 
on two hundred sentences annotated in parallel 
by two annotators.1

the inter-annotator agreement. It often happens 
that the annotators recognize the same discourse 
relation in the data but they disagree either in 
the start node or the target node of the relation.

In  Zikánová et al. (2010), we elaborate on 
typical cases of this type of disagreement and 
show that in many times, the difference in the 
start node or the target node is only one level in 
the tree. We have also shown that these dis-
agreements usually depend on a subtle and not 
crucial   difference   in   the   interpretation   of   the 
text.

Figure 2 shows an example of a disagreement 
caused by a one-level difference in the target 
node of a relation. The two trees (a cut of them) 
represent these two sentences:

“Vím, že se nás Rusů bojíte, že nás nemáte 
rádi, že námi trochu pohrdáte. Ale Rusko není  
jenom Žirinovskij, Rusko není jenom vraždění v 
Čečensku.”

(In English: “I know that you are afraid of us 
Russians, that you dislike us, that you despise  
us a little. But Russia is not only Zhirinovsky,  
Russia is not only murdering in Chechnya.”)

measure
F1-measure on relations
F1-measure on relations + types
F1-measure on relations + connectives
F1-measure on rel. + types + connect.
agreement on types
agreement on connectives
Cohen's κ on types

value
0.43
0.34
0.41
0.32
0.8
0.95
0.74

Table 1. The inter-annotator agreement for a 

strict match.

Skipping a Tree Level

3.2
Requiring a perfect agreement on the start node 
and the target node of the discourse relations 
turns out to be too strict for a fair evaluation of 

1 The annotators did not know which part of the data will 
be used for the measurement. The agreement was mea-
sured on 200 sentences (6 documents). PDT 2.0 contains 
data from three sources. The proportion of the sentences 
selected for the measurement reflected the total proportion 
of these data sources in the whole treebank.

Figure 2. Disagreement in the target node.

Both annotators recognized the discourse re-
lation between the two sentences, both selected 
the same type (opposition), and both marked the 
same connective (“Ale”, in English “But”). The 
disagreement in the target node is caused by the 
fact that one annotator has connected the second 
sentence with “knowing that something is going 
on”, while the other has connected it directly 
with the expression“something is going on”.

We have shown in Zikánová et al. (2010) that 
allowing for skipping one tree level either at the 
start node or the target node of the relations 
leads to an improvement in the inter-annotator 

778

agreement   (F1-measure  on   the   relations)   of 
about 10%. To be exact, by allowing to skip one 
tree level we mean: if node A is a parent of 
node   B,   then   we   consider   arrows   A→C   and 
B→C  to  be in agreement,  as well  as arrows 
D→A and D→B. Table 2 shows present results 
of this type of measurement, performed on the 
same data as Table 1.

(In   English:  A   rational   calculation   of   the 
owners of the apartments will lead them to the  
only conclusion: any investment in repairs and 
renovation of the rental housing resources is  
and will be loss-making. Therefore, further di-
lapidation   of   the   apartment   buildings   is   in-
evitable.”)

measure
F1-measure on relations
F1-measure on relations + types
F1-measure on relations + connectives
F1-measure on rel. + types + connect.
agreement on types
agreement on connectives
Cohen's κ on types

value
0.54
0.43
0.49
0.39
0.8
0.92
0.73

Table 2. The inter-annotator agreement with 

one-level skipping.

The results seem to be consistent, since the 
improvement here is similar to the previously 
published test.  The F1-measure on the relations 
improved from 0.43 to 0.54. On the other hand 
(and also consistently with the previous test), 
simple   ratio   agreement  on   types   (or   connec-
tives) and Cohen's κ on types, all measured on 
those arrows the annotators agreed on, do not 
change (more or less) after skipping one level is 
allowed. For these three measures, skipping one 
level only adds more data to evaluate and does 
not change conditions of the evaluation.

3.3 Connective-Based Approach
Further studies of discrepancies in parallel an-
notations show that skipping one level does not 
cover all “less severe” cases of disagreement. 

Figure 3 presents an example of a disagree-
ment in the start node of a relation with a two-
level distance between the nodes. The two trees 
(a cut of them) represent these two sentences:

“Racionální   kalkulace   vlastníků   nájemních 
bytů je proto povede k jedinému závěru: jakéko-
liv investice do oprav a modernizace nájemního 
bytového fondu jsou a budou ztrátové. Proto je 
další chátrání nájemních domů neodvratné.”

Figure 3. Two-level disagreement in the start 

nodes

The difference between the annotators is that 
one of them started the relation at the phrase 
“will lead to the only conclusion: any invest-
ment ... is and will be ...”, while the other start-
ed the relation directly at the phrase  “any in-
vestment … is and will be ...”.

However, both the annotators admittedly rec-
ognized the existence of the discourse relation, 
they also selected the same type (reason), and 
marked the same connective (“Proto”, in Eng-
lish “Therefore”).

Figure 4 shows an example of a disagreement 
caused by a different selection of nodes and by 
the opposite direction of the arrows.  The trees 
represent these sentences: “To je jasné, že bych 
byl radši, kdyby tady dosud stál zámek a ne  
tohle   monstrum.   Ale   proč   o   tom   stále  
uvažovat?”

(In English: It is clear that I would prefer if 
there still was a castle here and not this mon-
ster. But why keep thinking about it forever?”) 

779

cate administration tried to exert pressure on  
the government and force it to pay the debts.”)

Figure 5. Disagreement in the range of the dis-

course relation.

The difference between the annotators is in 
the range of the start part of the arrows. One of 
the annotators marked the two first sentences as 
a   start   point   of   the   relation,   while   the   other 
marked the second sentence as the start point 
only. They agreed on the target point of the rela-
tion being the third sentence.

Inspired by these examples, we designed an-
other – a connective-based – measure for evalu-
ating the inter-annotator agreement of the dis-
course relations. It seems that although the an-
notators   sometimes   fail   to   mark   the   same 
start/target nodes, or to select the same type or 
the  same range  of the  relations,  they  usually 
agree on the connective. This idea is also sup-
ported by high levels of the simple ratio agree-
ment on connectives measured on relations the 
annotators agreed on from Tables 1 and 2 (0.95 
and 0.91). These numbers show that once the 
annotators agree on a relation, they almost al-
ways agree also on the connective.2

The connective-based measure considers the 
annotators to be in agreement on recognizing a 
discourse relation if they agree on recognizing 
the same connective (please note that we only 
annotate discourse relations with explicitly ex-
pressed connectives).

Table 3 shows results of the evaluation of the 
inter-annotator agreement, performed using the 
connective-based measure, on the same data as 
Tables 1 and 2.

2 This is only an interpretation of the numbers, not a 
description of the annotation process; in fact, the an-
notators usually first find a connective and then 
search for the arguments of the discourse relation.

Figure 4. Disagreement in the nodes and in the 

direction of the arrows.

This time, both annotators recognized a pres-
ence   of   a   discourse   relation   and   marked   the 
same   connective   (“Ale”,   in   English  “But”). 
They did not agree on the start/end nodes and 
on the type of the relation (opposition vs. con-
cession).

Figure 5 shows another type of “slight” dis-
agreement. This time, the annotators agreed on 
everything but the range of the relation. They 
agreed both on the type (reason) and the con-
nective (“tak”, in English  “Thus”). The three 
trees (again a cut of them) represent these three 
sentences:

“Podle šéfa kanceláře představenstva a. s.  
Škoda Zdeňka Lavičky jsou však v říjnu schop-
ny   fungovat   prakticky   všechny   závody   bez 
vážnějšího omezení. To je v rozporu s tvrzením  
vedení koncernu z minulého týdne, ve kterém  
škodovácký management tvrdil, že se odstávka  
dotkne většiny provozů a závodů Škody Plzeň, 
která   má   v   současnosti   28000   zaměstnanců. 
Vzniká   tak   podezření,   že   se   vedení   koncernu  
snažilo vyvinout tlak na vládu a donutit ji k za-
placení dluhů.”

(In English:  “According to Zdeněk Lavička, 
the  chief   of   the   board   of   directors   of   Škoda 
corp., virtually all factories are able to operate 
in October without serious limitations. It con-
tradicts the statement of the syndicate adminis-
tration from the last week, in which the man-
agement of Škoda claimed that the downtime 
would affect most of the plants and factories of  
Škoda Plzeň, which presently has 28,000 em-
ployees. Thus a suspicion arises that the syndi-

780

measure
F1-measure on relations
F1-measure on relations + types
F1-measure on rel. + start/end nodes
F1-measure on rel. + types + nodes
agreement on types
agreement on start/end nodes
Cohen's κ on types

value
0.86
0.56
0.43
0.34
0.65
0.50
0.56

Table 3. The inter-annotator agreement evaluat-

ed with the connective-based measure.

This time (compared with Tables 1 and 2, i.e. 
the simple strict measure and the one-level skip-
ping measure), the agreement (F1-measure) on 
relations is much higher – 0.86 (vs. 0.43 and 
0.54). On the other hand, simple ratio agreement 
(and Cohen's  κ) measured on relations recog-
nized by both annotators are lower than in Ta-
bles  1  and  2.  Although   the  annotators  might 
have recognized the same discourse relation, a 
(possibly small) difference in the interpretation 
of the text caused sometimes not only a dis-
agreement   in   the   positions   of   the   start/end 
nodes, but also in the type of the relation.

The  simple  ratio  agreement   on  types  from 
Table 3 (0.65) is probably the closest measure 
to   the   way   of   measuring   the   inter-annotator 
agreement on subtypes in the annotation of dis-
course relations in the Penn Discourse Treebank 
2.0,   reported   in  Prasad   et   al.   (2008).   Their 
agreement was 0.8.

4 Conclusion
We have presented several ways of measuring 
the inter-annotator agreement in the project of 
annotating   the   semantic   inter-sentential   dis-
course relations with explicitly expressed con-
nectives in the Prague Dependency Treebank. 
We have shown examples from parallel annota-
tions that substantiate the importance of the al-
ternative   approaches   to   the   evaluation   of   the 
agreement.

Skipping a tree level in the start node or the 
end node of the relations helps to recognize fac-
tual agreement in some cases where the strict 
approach detects disagreement. We have shown 
that it is still too strict and that there are cases 

which we would like to classify as agreement 
but the measure does not recognize them.

The connective-based measure seems to be 
the closest one to what we would like to consid-
er a criterion of agreement. It disregards the ac-
tual nodes that are connected with a discourse 
relation, and even disregards the direction of the 
relation. In this sense, it is the most benevolent 
of the three measures.

It does not mean that the simple strict mea-
sure or skipping a tree level are inferior or obso-
lete ways of measuring the agreement. All the 
measures   focus   on   different   aspects   of   the 
agreement   and   they   are   all   important   in   the 
process of annotating the corpus, studying the 
parallel annotations and improving the annota-
tion instructions. We may agree on the fact that 
on this level of language description, it is very 
hard to achieve perfect agreement (Lee at al., 
2006), yet we should never cease the effort to 
further specify and clarify the ways of annota-
tion, in order to catch the same  linguistic phe-
nomena in the same way, and thus provide sys-
tematic and coherent linguistic data.

Acknowledgments
We   gratefully   acknowledge   support   from   the 
Czech   Ministry   of   Education   (grant   MSM-
0021620838), the Grant Agency of the Czech 
Republic
and 
P406/2010/0875),   the  Czech  Science   Founda-
tion (grant 201/09/H057), and the Grant Agency 
of   Charles   University   in   Prague   (GAUK 
103609).

  405/09/0729  

(grants

 

References
Artstein R. and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational 
Linguistics 34/4, pp. 555–596.

Asher, N.  1993.  Reference  to Abstract Objects in 

Discourse. Kluwer Academic Publishers 

Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales.  Educational and Psychological Mea-
surement, 20(1), pp. 37–46.

Hajič, J., Panevová, J., Hajičová, E., Sgall, P., Pajas, 
P.,   Štěpánek,   J.,   Havelka,   J.,   Mikulová,   M., 
Žabokrtský,   Z.,   and   M.   Ševčíková-Razímová. 
2006.  Prague   Dependency   Treebank   2.0.  CD-
ROM,   LDC2006T01,   Linguistic   Data   Consor-
tium, Philadelphia, USA.

781

Krippendorff, K. 1980. Content Analysis: An Intro-
duction   to   Its   Methodology.  Chapter   12.   Sage, 
Beverly Hills, CA, USA.

Lee, A., Prasad, R., Joshi, A., Dinesh, N., and B. 
Weber. 2006. Complexity of dependencies in dis-
course: Are dependencies in discourse more com-
plex than in syntax? J. Hajič and J. Nivre, (eds.). 
Proceedings of the 5th Workshop on Treebanks 
and   Linguistic   Theories   (TLT   2006).   Prague, 
Czech Republic, pp. 79–90.

Mikulová, M. et al. 2005:  Annotation on the tec-
togrammatical layer in the Prague Dependency  
Treebank.   Annotation   manual.  Avaiable   from 
http://ufal.mff.cuni.cz/pdt2.0/doc/manuals/en/t-
layer/pdf/t-man-en.pdf

Mladová L., Zikánová, Š., Bedřichová, Z., and E. 

Hajičová. 2009. Towards a Discourse Corpus of 
Czech. Proceedings of the Corpus Linguistics 
Conference, Liverpool, Great Britain, in press 
(online proceedings: http://ucrel.lancs.ac.uk/publi-
cations/cl2009/).

Passonneau,   R.   2004.  Computing   Reliability   for 
Coreference.  Proceedings of LREC, vol. 4, Lis-
bon,  Portugal, pp. 1503–1506.

Prasad, R., Dinesh N., Lee A., Miltsakaki, E., Robal-
do, L., Joshi, A., Webber, B. 2008. The Penn Dis-
course Treebank 2.0. Proceedings of the 6th Inter-
national Conference on Language Resources and 
Evaluation (LREC 2008), Morocco.

Zikánová   Š.,   Mladová   L.,   Mírovský   J.,   and   P. 
Jínová. 2010.  Typical Cases of Annotators’ Dis-
agreement  in Discourse Annotations in Prague 
Dependency   Treebank.  LREC   2010,   Malta,   in 
press.

