















































Generating Question Relevant Captions to Aid Visual Question Answering


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3585‚Äì3594
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

3585

Generating Question Relevant Captions to Aid Visual Question Answering

Jialin Wu, Zeyuan Hu and Raymond J. Mooney
Department of Computer Science

University of Texas at Austin
{jialinwu, iamzeyuanhu, mooney}@cs.utexas.edu

Abstract

Visual question answering (VQA) and im-
age captioning require a shared body of gen-
eral knowledge connecting language and vi-
sion. We present a novel approach to improve
VQA performance that exploits this connec-
tion by jointly generating captions that are tar-
geted to help answer a specific visual ques-
tion. The model is trained using an exist-
ing caption dataset by automatically determin-
ing question-relevant captions using an on-
line gradient-based method. Experimental re-
sults on the VQA v2 challenge demonstrates
that our approach obtains state-of-the-art VQA
performance (e.g. 68.4% on the Test-standard
set using a single model) by simultaneously
generating question-relevant captions.

1 Introduction

In recent years, visual question answering (VQA)
(Antol et al., 2015) and image captioning (Don-
ahue et al., 2015; Rennie et al., 2017) have been
widely studied in both the computer vision and
NLP communities. Most recent VQA research (Lu
et al., 2017; Pedersoli et al., 2017; Anderson et al.,
2018; Lu et al., 2018) concentrates on directly uti-
lizing visual input features including detected ob-
jects, attributes, and relations between pairs of ob-
jects.

However,little VQA research works on exploit-
ing textual features from the image which are able
to tersely encode the necessary information to an-
swer the questions. This information could be
richer than the visual features in that the sentences
have fewer structural constraints and can easily in-
clude the attributes of and relation among multi-
ple objects. In fact, we observe that appropriate
captions can be very useful for many VQA ques-
tions. In particular, we trained a model to answer
visual questions for the VQA v2 challenge (An-
tol et al., 2015) only using the human annotated

Human Captions :
1) A man on a blue surfboard on top of some rough water.
2) A young surfer in a wetsuit surfs a small wave.
3) A young man rides a surf board on a small wave while 
a man swims in the background.
4) A young man is on his surf board with someone in the background.
5) A boy riding waves on his surf board in the ocean.

Question 1: Does this boy have a full wetsuit on?
Caption: A young man wearing wetsuit surfing on a wave.
Question 2: What color is the board?
Caption: A young man riding a wave on a blue surfboard.

Figure 1: Examples of our generated question-relevant
captions. During the training phase, our model selects
the most relevant human captions for each question
(marked by the same color).

captions without images and achieved a score of
59.6%, outperforming a large number of VQA
models that use image features. Existing work
using captions for VQA has generated question-
agnostic captions using a pretrained captioner (Li
et al., 2018a). This approach can provide addi-
tional general information; however, this informa-
tion is not guaranteed to be relevant to the given
VQA question.

We explore a novel approach that generates
question-relevant image descriptions, which con-
tain information that is directly relevant to a partic-
ular VQA question. Fig. 1 shows examples of our
generated captions given different questions. In
order to encourage the generation of relevant cap-
tions, we propose a novel greedy algorithm that
aims to minimize the cross entropy loss only for



3586

the most relevant and helpful gold-standard cap-
tions. Specifically, helpfulness is measured us-
ing the inner-product of the gradients from the
caption generation loss and the VQA answer pre-
diction loss. A positive inner-product means the
two objective functions share some descent direc-
tions in the optimization process, and therefore in-
dicates that the corresponding captions help the
VQA training process.

In order to incorporate the caption information,
we propose a novel caption embedding module
that, given the question and image features for a
visual question, recognizes important words in the
caption, and produces a caption embedding tai-
lored for answer prediction. In addition, the cap-
tion embeddings are also utilized to adjust the vi-
sual top-down attention weights for each object.

Furthermore, generating question-relevant cap-
tions ensures that both image and question infor-
mation is encoded in their joint representations,
which reduces the risk of learning from question
bias (Li et al., 2018a) and ignoring the image con-
tent when high accuracy can be achieved from the
questions alone.

Experimental evaluation of our approach shows
significant improvements on VQA accuracy over
our baseline Up-Down (Anderson et al., 2018)
model on the VQA v2 validation set (Antol et al.,
2015), from 63.2% to 67.1% with gold-standard
human captions from the COCO dataset (Chen
et al., 2015) and 65.8% with automatically gener-
ated question-relevant captions. Our single model
is able to score 68.4% on the test-standard split,
and an ensemble of 10 models scores 69.7%.

2 Background Related Work

2.1 Visual Question Answering
Recently, a large amount of attention-based deep-
learning methods have been proposed for VQA,
including top-down (Ren et al., 2015a; Fukui et al.,
2016; Wu et al., 2016; Goyal et al., 2017; Li et al.,
2018a) and bottom-up attention methods (Ander-
son et al., 2018; Li et al., 2018b; Wu and Mooney,
2019). Specifically, a typical model first extracts
image features using a pre-trained CNN, and then
trains an RNN to encode the question, using an
attention mechanism to focus on specific features
of the image. Finally, both question and attended
image features are used to predict the final answer.

However, answering visual questions requires
not only information about the visual content but

also common knowledge, which is usually too
hard to directly learn from only a limited number
of images with human annotated answers as su-
pervision. However, comparatively little previous
VQA research has worked on enriching the knowl-
edge base. We are aware of two related papers. Li
et al. (2018a) use a pre-trained captioner to gen-
erate general captions and attributes with a fixed
annotator and then use them to predict answers.
Therefore, the captions they generate are not nec-
essarily relevant to the question, and they may ig-
nore image features needed for answer prediction.
Narasimhan et al. (2018) employed an out-of-the-
box knowledge base and trained their model to
filter out irrelevant facts. After that, graph con-
volutional networks use this knowledge to build
connections to the relevant facts and predict the fi-
nal answer. Unlike them, we generate captions to
provide information that is directly relevant to the
VQA process.

2.2 Image Captioning

Most recent image captioning models are also
attention-based deep-learning models (Donahue
et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals
et al., 2015; Luo et al., 2018; Liu et al., 2018).
With the help of large image description datasets
(Chen et al., 2015), these models have demon-
strated remarkable results. Most of them en-
code the image using a CNN, and build an atten-
tional RNN (i.e. GRU (Cho et al., 2014), LSTM
(Hochreiter and Schmidhuber, 1997)) on top of
the image features as a language model to generate
image captions.

However, deep neural models still tend to gener-
ate general captions based on the most significant
objects (Vijayakumar et al., 2016). Although pre-
vious works (Luo et al., 2018; Liu et al., 2018)
build captioning models that are encouraged to
generate different captions with discriminability
objectives, the captions are usually less informa-
tive and fail to describe most of the objects and
their relationships diversely. In this work, we de-
velop an approach to generating captions that di-
rectly focus on the critical objects in the VQA
process and provide information that can help the
VQA module predict the answer.

3 Approach

We first describe the overall structure of our joint
model in Sec. 3.1 and explain the foundational



3587

Word 
Embedding GRU

!"#

Image CNN

Caption
Generation

Word 
Embedding

Caption 
Embedding

!$#
Answer 

Prediction

Question

Phase 1: Gold 
Standard Captions

%√ó2048

%√ó2048

%

%

+

, ,"

,"$

$Phase 2: Model  
Generated Captions

Figure 2: Overall structure of our model that generates question-relevant captions to aid VQA. Our model is first
trained to generate question-relevant captions as determined in an online fashion in phase 1. Then, the VQA model
is fine-tuned with generated captions from the first phase to predict answers. ‚å¶ denotes element-wise multiplication
and ÔøΩ denotes element-wise addition. Blue arrows denote fully-connected layers (fc) and yellow arrows denote
attention embedding.

feature representations (i.e. image, question and
caption embeddings) in Sec. 3.2. Then, the VQA
module is presented in Sec. 3.3, which takes ad-
vantage of the generated image captions to im-
prove the VQA performance. In Sec. 3.4, we ex-
plain the image captioning module which gener-
ates question-relevant captions. Finally, the train-
ing and implementation details are provided in
Sec. 3.5.

3.1 Overview
As illustrated in Fig. 2, the proposed model first
extracts image features V = {v1, v2, ..., vK} using
bottom-up attention and question features q to pro-
duce their joint representation and then generates
question-related captions. Next, our caption em-
bedding module encodes the generated captions
as caption features c as detailed in Sec. 3.2. Af-
ter that, both question features q and caption fea-
tures c are utilized to generate the visual attention
Acv to weight the images‚Äô feature set V, produc-
ing attended image features vqc. Finally, we add
vqc to the caption features c and further perform
element-wise multiplication with the question fea-
tures q (Anderson et al., 2018) to produce the joint
representation of the question, image and caption,
which is then used to predict the answer.

3.2 Feature Representation
In this section, we explain the details of this joint
representation. We use f(x) to denote fully-
connected layers, where f(x) = LReLU(Wx+ b)
with input features x and ignore the notation of
weights and biases for simplicity, where these fc

layers do not share weights. LReLU denotes a
Leaky ReLU (He et al., 2015).

Image and Question Embedding
We use object detection as bottom-up attention
(Anderson et al., 2018), which provides salient im-
age regions with clear boundaries. In particular,
we use a Faster R-CNN head (Ren et al., 2015b)
in conjunction with a ResNet-101 base network
(He et al., 2016) as our detection module. The
detection head is first pre-trained on the Visual
Genome dataset (Krishna et al., 2017) and is ca-
pable of detecting 1, 600 objects categories and
400 attributes. To generate an output set of im-
age features V, we take the final detection outputs
and perform non-maximum suppression (NMS)
for each object category using an IoU threshold
of 0.7. Finally, a fixed number of 36 detected ob-
jects for each image are extracted as the image fea-
tures (a 2, 048 dimensional vector for each object)
as suggested by Teney et al. (2017).

For the question embedding, we use a standard
GRU (Cho et al., 2014) with 1, 280 hidden units
and extract the output of the hidden units at the
final time step as the question features q. Follow-
ing Anderson et al. (2018), the question features
q and image feature set V are further embedded
together to produce a question-attended image
feature set Vq via question visual-attention Aqv as
illustrated in Fig. 2.

Caption Embedding
Our novel caption embedding module takes as in-



3588

put the question-attended image feature set Vq,
question features q, and C captions Wci =
{wci,1, wci,2, ..., wci,T }, where T denotes the length
of the captions and i = 1, ..., C are the caption
indices, and then produces the caption features c.

Word GRU

ùêÄùêú

Word Embedding ùêñùêûŒ†i, tc

ùêïqv

Caption GRU

‚Ñéùëñ, ùë°2

‚Ñéùëñ, ùë°1

Figure 3: Overview of the caption embedding module.
The Word GRU is used to generate attention to iden-
tify the relevant words in each caption, and the Caption
GRU generates the final caption embedding. We use
question-attended image features Vqv to compute the
attention. Blue arrows denote fc layers and yellow ar-
rows denote attention embedding.

The goals of the caption module are to serve
as a knowledge supplement to aid VQA, and to
provide additional clues to identify the relevant
objects better and adjust the top-down attention
weights. To achieve this, as illustrated in Fig. 3,
we use a two-layer GRU architecture. The first-
layer GRU (called the Word GRU) sequentially
encodes the words in a caption Wci at each time
step as h1i,t.

h1i,t = GRU(We‚áßci,t, h1i,tÔøΩ1) (1)

where We is the word embedding matrix, and ‚áßci,t
is the one-hot embedding for the word wci,t.

Then, we design a caption attention module Ac
which utilizes the question-attended feature set
Vq, question features q, and h1i,t to generate the
attention weight on the current word in order to in-
dicate its importance. Specifically, the Word GRU
first encodes the words embedding ‚áßci,t in Eq. 1,
and then we feed the outputs h1i,t and Vq to the
attention module Ac as shown in Eq. 4.

vq =
KX

k=1

vqk (2)

aci,t = h
1
i,t ÔøΩ f(vq) + h1i,t ÔøΩ f(q) (3)

‚Üµci,t = ÔøΩ(a
c
i,t) (4)

where ÔøΩ denotes the sigmoid function, and K is
the number of objects in the bottom-up attention.

Next, the attended words in the caption are used
to produce the final caption representation in Eq.
5 via the Caption GRU. Since the goal is to gather
more information, we perform element-wise max
pooling across the representations of all of the in-
put captions ci in Eq. 7.

h2i,t = GRU(‚Üµ
c
i,tWe‚áßci,t, h2i,tÔøΩ1) (5)

ci = f(h2i,T ) (6)
c = max(ci) (7)

where max denotes the element-wise max pool-
ing across all of caption representations ci of the
image.

3.3 VQA Module
This section describes the details of the VQA
module. The generated captions are usually ca-
pable of capturing relations among the question-
relevant objects; however these relations are ab-
sent in the bottom-up attention. Therefore, our
VQA module utilizes the caption embeddings c
to adjust the top-down attention weights in VQA
in order to produce the final caption-attended fea-
tures vqc in Eq. 10:

acvk = f(f(c) ÔøΩ f(v
q
k)) (8)

‚Üµcvk = softmax(a
cv
c,k) (9)

vqc =
KX

k

vqk‚Üµ
cv
k (10)

where k traverses the K objects features.
To better incorporate the information from the
captions into the VQA process, we add the caption
features c to the attended image features vqc, and
then element-wise multiply by the question fea-
tures as shown in Eq. 11:

h = q ÔøΩ (f(vqc) + f(c)) (11)
sÃÇ = ÔøΩ(f(h)) (12)

We frame the answer prediction task as a multi-
label regression problem (Anderson et al., 2018).
In particular, we use the soft scores in the gold-
standard VQA-v2 data (which are used in the eval-
uation metric), as labels to supervise the sigmoid-
normalized predictions as shown in Eq. 13:

Lvqa = ÔøΩ
NX

j=1

sj log sÃÇj+(1ÔøΩsj) log(1ÔøΩsÃÇj) (13)



3589

where the index j runs over N candidate answers
and s are the soft answer scores.

In case of multiple feasible answers, the soft
scores capture the occasional uncertainty in the
ground-truth annotations. As suggested by Teney
et al. (2017), we collect the candidate answers that
appear more than 8 times in the training set, which
results in 3, 129 answer candidates.

3.4 Image Captioning Module

We adopt an image captioning module similar
to that of Anderson et al. (2018), which takes
the object detection features as inputs and learns
attention weights over those objects‚Äô features in
order to predict the next word at each step. The
key difference between our module and theirs lies
in the input features and the caption supervision.
Specifically, we use the question-attended image
features Vq as inputs, and only use the most
relevant caption, which is automatically deter-
mined in an online fashion (detailed below), for
each question-image pair to train the captioning
module. This ensures that only question-relevant
captions are generated.

Selecting Relevant Captions for Training
Previously, Li et al. (2018b) selected relevant cap-
tions for VQA based on word similarities between
captions and questions, however, their approach
does not take into account the details of the VQA
process. In contrast, during training, our approach
dynamically determines for each problem, the cap-
tion that will most improve VQA. We do this
by updating with a shared descent direction (Wu
et al., 2018) which decreases the loss for both cap-
tioning and VQA. This ensures a consistent tar-
get for both the image captioning module and the
VQA module in the optimization process.

During training, we compute the cross-entropy
loss for the i-th caption using Eq. 14, and back-
propagate the gradients only from the most rele-
vant caption determined by solving Eq. 15.

Lci =ÔøΩ
TX

t=1

log(p(wci,t|wci,tÔøΩ1)) (14)

In particular, we require the inner product of the
current gradient vectors from the predicted answer
and the human captions to be greater than a posi-
tive constant ‚á†, and further select the caption that

maximizes that inner product.

argmax
i

KX

k=0

‚úì
@sÃÇpred
@vqk

‚óÜT @ log(p(Wci ))
@vqk

s.t.
KX

k=0

‚úì
@sÃÇpred
@vqk

‚óÜT @ log(p(Wci ))
@vqk

> ‚á†

(15)

where the sÃÇpred is the logit1 for the predicted an-
swer, Wci denotes the i-th human caption for the
image and k traverses the K object features.

Therefore, given the solution to Eq. 15, i?, the
final loss of our joint model is the sum of the VQA
loss and the captioning loss for the selected cap-
tions as shown in Eq. 16. If Eq. 15 has no feasible
solution, we ignore the caption loss.

L = Lvqa + Lci? (16)

3.5 Training and Implementation Details
We train our joint model using the AdaMax op-
timizer (Kingma and Ba, 2015) with a batch size
of 384 and a learning rate of 0.002 as suggested
by Teney et al. (2017). We use the validation set
for VQA v2 to tune the initial learning rate and
the number of epochs, yielding the highest over-
all VQA score. We use 1, 280 hidden units in
the question embedding and attention model in the
VQA module with 36 object detection features for
each image. For captioning models, the dimension
of the LSTM hidden state, image feature embed-
ding, and word embedding are all set to 512. We
also use Glove vectors (Pennington et al., 2014) to
initialize the word embedding matrix in the cap-
tion embedding module.

We initialize the training process with human
annotated captions from the COCO dataset (Chen
et al., 2015) and pre-train the VQA and caption-
generation modules for 20 epochs with the fi-
nal joint loss in Eq. 16. After that, we generate
question-relevant captions for all question-image
pairs in the COCO train, validation, and test sets.
In particular, we sample 5 captions per question-
image pair. We fine-tune our model using the gen-
erated captions with 0.25 ‚á• learning-rate for an-
other 10 epochs.

4 Experiments

We perform extensive experiments and ablation
studies to evaluate our joint model on VQA.

1The input to the softmax function.



3590

Test-standard
Yes/No Num Other All

Prior (Goyal et al., 2017) 61.20 0.36 1.17 25.98
Language-only (Goyal et al., 2017) 67.01 31.55 27.37 44.26
MCB (Fukui et al., 2016) 78.82 38.28 53.36 62.27
Up-Down (Anderson et al., 2018) 82.20 43.90 56.26 65.32
VQA-E (Li et al., 2018b) 83.22 43.58 56.79 66.31
Ours(single) 84.69 46.75 59.30 68.37
Ours(Ensemble-10) 86.15 47.41 60.41 69.66

Table 1: Comparison of our results on VQA with the state-of-the-art methods on the test-standard data. Accuracies
in percentage (%) are reported.

4.1 Datasets and Evaluation Metrics

VQA Dataset
We use the VQA v2.0 dataset (Antol et al., 2015)
for the evaluation of our proposed joint model,
where the answers are balanced in order to min-
imize the effectiveness of learning dataset priors.
This dataset is used in the VQA 2018 challenge
and contains over 1.1M questions from the over
200K images in the MSCOCO 2015 dataset (Chen
et al., 2015).

Following Anderson et al. (2018), we perform
standard text pre-processing and tokenization. In
particular, questions are first converted to lower
case and then trimmed to a maximum of 14 words,
and the words that appear less than 5 times are
replaced with an ‚Äú<unk>‚Äù token. To evaluate
answer quality, we report accuracies using the
official VQA metric using soft scores, which
accounts for the occasional disagreement between
annotators for the ground truth answers.

Image Captioning Dataset
We use the MSCOCO 2014 dataset (Chen et al.,
2015) for the image caption module. To main-
tain consistency with the VQA tasks, we use
the dataset‚Äôs official configuration that includes
82, 372 images for training and 40, 504 for valida-
tion. Similar to the VQA question pre-processing,
we first convert all sentences to lower case, tok-
enizing on white spaces, and filtering words that
do not occur at least 5 times.

4.2 Results on VQA

We first report the experimental results on the
VQA task and compare our results with the state-
of-the-art methods in this section. After that, we
perform ablation studies to verify the contribution

of additional knowledge from the generated cap-
tions, and the effectiveness of using caption repre-
sentations to adjust the top-down visual attention
weights.

As demonstrated in Table 1, our single model
outperforms other state-of-the-art single models
by a clear margin, i.e. 2.06%, which indicates
the effectiveness of including caption features as
additional inputs. In particular, we observe that
our single model outperforms other methods, es-
pecially in the ‚ÄôNum‚Äô and ‚ÄôOther‚Äô categories. This
is because the generated captions are capable of
providing more numerical clues for answering the
‚ÄôNum‚Äô questions, since the captions can describe
the number of relevant objects and provide gen-
eral knowledge for answering the ‚ÄôOther‚Äô ques-
tions. Furthermore, an ensemble of 10 models
with different initialization seeds results in a score
of 69.7% for the test-standard set.

Fig. 4 shows several examples of our generated
question-relevant captions. These examples
illustrate how different captions are generated for
the same image when the question is changed.
They also show how the objects in the image
that are important to answering the question are
described in the question-relevant captions.

Comparison Between Using Generated and
Human Captions
Next, we analyze the difference between using
automatically generated captions and using those
provided by human annotators. In particular, we
train our model with generated question-agnostic
captions using the Up-Down (Anderson et al.,
2018) captioner, question-relevant captions from
our caption generation module, and human anno-
tated captions from the COCO dataset.

As demonstrated in Table 2, our model gains



3591

Caption:

Caption:

Q: What is he doing?

Q: Is he wearing a hat?

Caption:

Caption:

Q: Is the cat watching TV?

Q: Is the tv on?

Q: What colors are on the couch?
Caption:

Q: Is there a picture on the wall?
Caption:

Caption:

Caption:

Q: What color is the vase?

Q: What color are the flowers?

A: Yes.A: Taking picture. A: Purple and white. A: White.

A: Yes.A: Yes. A: Yes. A: Red.

Figure 4: Examples of our generated question-relevant captions. The influential objects with attention weights
greater than 0.1 are indicated by bounding boxes (annotated with their visual attention weights in the blue box),
and the gray-scale levels in the caption words indicate the word attentions from the caption embedding module.

Validation
Up-Down (Anderson et al., 2018) 63.2
Ours with Up-Down captions 64.6
Ours with our generated captions 65.8
Ours with human captions 67.1

Table 2: Comparison of the performance using gener-
ated and human captions. Both of them provide signif-
icant improvements to the baseline model. However,
there is still a reasonable gap between generated and
human captions.

about 4% improvement from using human cap-
tions and 2.5% improvement from our generated
question-relevant captions on the validation set.
This indicates the insufficiency of directly answer-
ing visual questions using a limited number of
detection features, and the utility of incorporating
additional information about the images. We also
observe that our generated question-relevant cap-
tions trained with our caption selection strategy
provide more helpful clues for the VQA process
than the question-agnostic Up-Down captions,
outperforming their captions by 1.2%.

Effectiveness of Adjusting Top-Down Attention
In this section, we quantitatively analyze the ef-

Question: What colors is the surfboard?

Answer: Yellow and blue Answer: Yellow and red

Answer: yellow and red

Visual attention Caption adjusted visual attention
Caption: A group of people standing next to yellow board.

Figure 5: An example of caption attention adjustment.
The question-relevant caption helps the VQA module
adjust the visual attention from both the yellow board
and the blue sail to the yellow board only.

fectiveness of utilizing captions to adjust the top-
down attention weights, in addition to the advan-
tage of providing additional information. In par-
ticular, we compare our model with a baseline ver-
sion where the top-down attention-weight adjust-
ment factor Acv is manually set to 1.0 (resulting in
no adjustment).

As demonstrated in Tables 3 and 4, we observe
an improvement when using caption features to
adjust the attention weights. This indicates that the
caption features help the model to more robustly
locate the objects that are helpful to the VQA pro-



3592

cess. We use w CAA to indicate with caption at-
tention adjustment and w/o CAA to indicate with-
out it. Fig. 5 illustrates an example of caption at-
tention adjustment. Without CAA, the top-down
visual attention focuses on both the yellow surf-
board and the blue sail, generating the incorrect
answer ‚Äúyellow and blue.‚Äù. However, with ‚Äúyellow
board‚Äù in the caption, the caption attention adjust-
ment (CAA) helps the VQA module focus atten-
tion just on the yellow surfboard, thereby gener-
ating the correct answer ‚Äúyellow and red‚Äù (since
there is some red coloring in the surfboard).

Test-standard
All Yes/No Num Other

Up-Down 65.3 82.2 43.9 56.3
Ours w/o CAA 67.4 84.0 44.5 57.9
Ours w CAA 68.4 84.7 46.8 59.3

Table 3: Evaluation of the effectiveness of caption-
based attention adjustment (CAA) on the test-standard
data. Accuracies in percentage (%) are reported.

Validation
All Yes/No Num Other

Up-Down 63.2 80.3 42.8 55.8
Ours w/o CAA 65.2 82.1 43.6 55.8
Ours w CAA 65.8 82.6 43.9 56.4

Table 4: Evaluation of the effectiveness of CAA on
the validation data. Accuracies in percentage (%) are
reported.

Next, in order to directly demonstrate that
our generated question-relevant captions help the
model to focus on more relevant objects via atten-
tion adjustment, we compare the differences be-
tween the generated visual attention and human-
annotated important objects from the VQA-X
dataset (Park et al., 2018), which has been used to
train and evaluate multimodal (visual and textual)
VQA explanation (Wu and Mooney, 2018). The
VQA-X dataset contains 2, 000 question-image
pairs from the VQA v2 validation set with hu-
man annotations indicating the objects which most
influence the answer to the question. In particu-
lar, we used Earth Mover Distance (EMD) (Rub-
ner et al., 2000) to compare the highly-attended
objects in the VQA process to the objects high-
lighted by human judges. This style of evalua-
tion using EMD has previously been employed to

compare automatic visual explanations to human-
attention annotations (Selvaraju et al., 2017; Park
et al., 2018).

We resize all of the 2, 000 human annotations
in VQA-X dataset to 14‚á•14 and adjust the object
bounding boxes in the images accordingly. Next,
we assign the top-down attention weights to the
corresponding bounding boxes, both before and
after caption attention adjustment, and add up the
weights of all 36 detections. Then, we normalize
attention weights over the 14 ‚á• 14 resized images
to sum to one, and finally compute the EMD be-
tween the normalized visual attentions and the hu-
man annotations.

Table 5 reports the EMD results for the atten-
tions weights both before and after the caption at-
tention adjustments.

w/o CAA w CAA Human
EMD 2.38 2.30 2.25

Table 5: EMD results comparing the top-down atten-
tion weights (with or without caption attention adjust-
ments) to human attention-annotation from the VQA-
X dataset. Results are shown for both automatically
generated captions and human captions. Lower EMD
indicates a closer match to human attention.

The results indicate that caption attention ad-
justment improves the match between automated
attention and human-annotated attention, even
though the approach is not trained on supervised
data for human attention. Not surprisingly, human
captions provide a bit more improvement than au-
tomatically generated ones.

5 Conclusion

In this work, we have explored how generat-
ing question-relevant image captions can improve
VQA performance. In particular, we present a
model which jointly generates question-related
captions and uses them to provide additional in-
formation to aid VQA. This approach only utilizes
existing image-caption datasets, automatically de-
termining which captions are relevant to a given
question. In particular, we design the training al-
gorithm to only update the network parameters in
the optimization process when the caption gen-
eration and VQA tasks agree on the direction of
change. Our single model joint system outper-
forms the current state-of-the-art single model for
VQA.



3593

Acknowledgement

This research was supported by the DARPA XAI
program under a grant from AFRL.

References
Peter Anderson, Xiaodong He, Chris Buehler, Damien

Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-Up and Top-Down Attention
for Image Captioning and VQA. In CVPR, vol-
ume 3, page 6.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual Question An-
swering. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2425‚Äì2433.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr DollaÃÅr, and
C Lawrence Zitnick. 2015. Microsoft COCO Cap-
tions: Data Collection and Evaluation Server. arXiv
preprint arXiv:1504.00325.

Kyunghyun Cho, Bart Van MerrieÃànboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations Using RNN Encoder-
Decoder for Statistical Machine Translation. arXiv
preprint arXiv:1406.1078.

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-
term Recurrent Convolutional Networks for Visual
Recognition and Description. In CVPR, pages
2625‚Äì2634.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal Compact Bilinear Pooling for
Visual Question Answering and Visual Grounding.
EMNLP.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017. Making the V
in VQA Matter: Elevating the Role of Image Under-
standing in Visual Question Answering. In CVPR,
volume 1, page 9.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving Deep into Rectifiers: Surpass-
ing Human-Level Performance on Imagenet Classi-
fication. In ICCV, pages 1026‚Äì1034.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep Residual Learning for Image
Recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
770‚Äì778.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long Short-Term Memory. Neural computation,
9(8):1735‚Äì1780.

Andrej Karpathy and Li Fei-Fei. 2015. Deep Visual-
semantic Alignments for Generating Image Descrip-
tions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
3128‚Äì3137.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In ICLR.

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017. Visual Genome: Connecting Language and
Vision Using Crowdsourced Dense Image Annota-
tions. International Journal of Computer Vision,
123(1):32‚Äì73.

Qing Li, Jianlong Fu, Dongfei Yu, Tao Mei, and Jiebo
Luo. 2018a. Tell-and-Answer: Towards Explain-
able Visual Question Answering using Attributes
and Captions. arXiv preprint arXiv:1801.09041.

Qing Li, Qingyi Tao, Shafiq Joty, Jianfei Cai, and
Jiebo Luo. 2018b. VQA-E: Explaining, Elaborating,
and Enhancing Your Answers for Visual Questions.
ECCV.

Xihui Liu, Hongsheng Li, Jing Shao, Dapeng Chen,
and Xiaogang Wang. 2018. Show, Tell and Dis-
criminate: Image Captioning by Self-retrieval with
Partially Labeled Data. ECCV.

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing When to Look: Adaptive
Attention via a Visual Sentinel for Image Caption-
ing. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR),
volume 6.

Pan Lu, Lei Ji, Wei Zhang, Nan Duan, Ming Zhou, and
Jianyong Wang. 2018. R-vqa: learning visual re-
lation facts with semantic attention for visual ques-
tion answering. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 1880‚Äì1889. ACM.

Ruotian Luo, Brian Price, Scott Cohen, and Gregory
Shakhnarovich. 2018. Discriminability Objective
for Training Descriptive Captions. In The IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR).

Medhini Narasimhan, Svetlana Lazebnik, and Alexan-
der Schwing. 2018. Out-of-The-Box: Reasoning
with Graph Convolution Nets for Factual Visual
Question Answering. In NIPS, pages 2655‚Äì2666.

Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata,
Anna Rohrbach, Bernt Schiele, Trevor Darrell, and
Marcus Rohrbach. 2018. Multimodal Explanations:
Justifying Decisions and Pointing to the Evidence.
In CVPR.

Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and
Jakob Verbeek. 2017. Areas of Attention for Image
Captioning. In ICCV-International Conference on
Computer Vision.



3594

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532‚Äì1543.

Mengye Ren, Ryan Kiros, and Richard Zemel. 2015a.
Exploring Models and Data for Image Question An-
swering. In NIPS, pages 2953‚Äì2961.

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian
Sun. 2015b. Faster R-CNN: Towards Real-time Ob-
ject Detection with Region Proposal Networks. In
NIPS, pages 91‚Äì99.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
Sequence Training for Image Captioning. In CVPR,
volume 1, page 3.

Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas.
2000. The Earth Mover‚Äôs Distance as a Metric for
Image Retrieval. ICCV, 40(2):99‚Äì121.

Ramprasaath R Selvaraju, Michael Cogswell, Ab-
hishek Das, Ramakrishna Vedantam, Devi Parikh,
Dhruv Batra, et al. 2017. Grad-CAM: Visual Ex-
planations from Deep Networks via Gradient-Based
Localization. In ICCV, pages 618‚Äì626.

Damien Teney, Peter Anderson, Xiaodong He, and An-
ton van den Hengel. 2017. Tips and Tricks for Vi-
sual Question Answering: Learnings from the 2017
Challenge. arXiv preprint arXiv:1708.02711.

Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasath R Selvaraju, Qing Sun, Stefan Lee,
David Crandall, and Dhruv Batra. 2016. Di-
verse Beam Search: Decoding Diverse Solutions
from Neural Sequence Models. arXiv preprint
arXiv:1610.02424.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and Tell: A Neural Im-
age Caption Generator. In Computer Vision and Pat-
tern Recognition (CVPR), 2015 IEEE Conference
on, pages 3156‚Äì3164. IEEE.

Jialin Wu, Dai Li, Yu Yang, Chandrajit Bajaj, and Xi-
angyang Ji. 2018. Dynamic Filtering with Large
Sampling Field for Convnets. ECCV.

Jialin Wu and Raymond J Mooney. 2018. Faithful Mul-
timodal Explanation for Visual Question Answering.
arXiv preprint arXiv:1809.02805.

Jialin Wu and Raymond J Mooney. 2019. Self-
critical reasoning for robust visual question answer-
ing. arXiv preprint arXiv:1905.09998.

Jialin Wu, Gu Wang, Wukui Yang, and Xiangyang
Ji. 2016. Action Recognition with Joint Atten-
tion on Multi-level Deep Features. arXiv preprint
arXiv:1607.02556.


