Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 976–983,

Beijing, August 2010

976

Weakly Supervised Morphology Learning for Agglutinating Languages

Using Small Training Sets

Ksenia Shalonova
Computer Science,
University of Bristol

Bruno Gol´enia
Computer Science,
University of Bristol

ksenia@cs.bris.ac.uk

csbsgg@bristol.ac.uk

Abstract

The paper describes a weakly supervised
approach for decomposing words into all
morphemes: stems, preﬁxes and sufﬁxes,
using wordforms with marked stems as
training data. As we concentrate on
under-resourced languages,
the amount
of training data is limited and we need
some amount of supervision in the form
of a small number of wordforms with
marked stems.
In the ﬁrst stage we in-
troduce a new Supervised Stem Extrac-
tion algorithm (SSE). Once stems have
been extracted, an improved unsupervised
segmentation algorithm GBUMS (Graph-
Based Unsupervised Morpheme Segmen-
tation) is used to segment sufﬁx or preﬁx
sequences into individual sufﬁxes and pre-
ﬁxes. The approach, experimentally val-
idated on Turkish and isiZulu languages,
gives high performance on test data and is
comparable to a fully supervised method.

Introduction

1
The major function of morphological analysis is
decomposition of words into their constituents -
stems and preﬁxes/sufﬁxes. In recent years Ma-
chine Learning approaches were used for word de-
composition. There is a number of both unsuper-
vised morphology learning systems that use ”raw”
wordforms as training data (Creutz and Lagus,
2002; Goldsmith, 2001; Kazakov and Manand-
har, 2001) and supervised morphology learning
systems using segmented wordforms into stems
and afﬁxes as training data (Oﬂazer et al., 2001).
The supervised morphology learning systems are
usually based on two-level morphology (Kosken-
niemmi, 1983). There is also a weakly supervised
approach that uses, for example, wordpairs as in-

put, and this was applied mainly to fusional lan-
guages for stem extraction (Erjavec and Dzeroski,
2004). Our project concerns developing speech
technology for under-resourced languages. For
this type of applications we need a relatively fast,
cheap (i.e. does not require large training sets),
almost knowledge-free approach that gives high
performance. We have chosen to use wordforms
with marked stems as training data in order to ful-
ﬁll the criteria mentioned above.
Morphological analysis is used in many prac-
tical Natural Language Processing applications
such as Machine Translation, Text Mining, spell-
checkers etc. Our near-term goal is the integration
of the morphology learning algorithms into the
language-independent Text-to-Speech (TTS) sys-
tem for improvement of grapheme-to-phoneme
rules, stress prediction and tone assignment.
In
particular,
the morphology learning algorithms
described in this paper will be incorporated into
the available isiZulu TTS system for automatic
prediction of lexical tones. In the isiZulu language
lexical tone assignment depends on the morpheme
boundary. The current isiZulu TTS system is
tone-deaf due to the lack of morphological de-
composition. A number of perception tests will
be carried out in order to evaluate which perfor-
mance of morphology decomposition is accept-
able for TTS and will improve the quality of the
synthesised speech.
It seems that the unsuper-
vised morphology learning systems can be rela-
tively easy to implement from scratch, but their
performance probably cannot be regarded as high
enough to improve the performance of the synthe-
sized speech. In order to overcome this problem
we present a novel synthesis of supervised and un-
supervised induction techniques for morphology
learning.
Our approach consists of two parts: the new su-
pervised stem extraction algorithm for agglutinat-

977

ing languages and the improved version of the
unsupervised algorithm for segmenting afﬁx se-
quences. In (Shalonova et al., 2009) the authors
presented the function learning approach called
TASR (Tree of Aligned Sufﬁx Rules) for extract-
ing stems in fusional languages given wordpairs
(word in grammatical form - word in basic form).
While this algorithm gives good performance for
Russian and English, it gives quite poor perfor-
mance for agglutinating languages as shown in
Section 4. A new approach for stem extraction in
agglutinating languages is required for two main
reasons. Firstly, sufﬁx (or preﬁx) sequences in
agglutinating languages can be much longer than
in fusional languages and TASR does not seem
to be efﬁcient on long afﬁx sequences as it does
not generalise data in the efﬁcient way and gen-
erates too many speciﬁc rules. This leads to poor
performance on unseen data. Secondly, in some
agglutinating languages it could be easier for na-
tive speakers to provide a stem (i.e. to provide a
list of wordforms with annotated stems), whereas
in highly inﬂective fusional languages the stem is
often strongly bound with sufﬁx sequences, and
providing a proper stem requires high linguistic
expertise. TASR approach is more appropriate for
word-and-paradigm or realizational morphology
that focuses on the whole word form rather than
on word segmentation. For example, in Russian
the inﬁnitive verb govorit’ (’to speak’) generates
a set of grammatical forms or a paradigm - gov-
orivshij, govor’aschij, govorim etc.
The second part of our approach is the im-
proved version of GBUAS algorithm (Shalonova
et al., 2009) that provides afﬁx segmentation given
unannotated afﬁx sequences. Given stem bound-
aries in the training set, our method splits the
input word into all morphemes: stems and pre-
ﬁxes/sufﬁxes. Our two-stage approach is tested
on the under-resourced language isiZulu contain-
ing both preﬁxes and sufﬁxes, as well as on Turk-
ish containing only sufﬁxes. Turkish is the most
commonly spoken of the Turkic languages (over
77 million people).
isiZulu is the Bantu lan-
guage with about 10 million speakers and it is
the most widely spoken home language in South
Africa. Both Turkish and isiZulu use agglutina-
tion to form new words from noun and verb stems.

In comparison to Turkish, isiZulu is a tonal lan-
guage.
In contrast to East Asian languages, in
isiZulu there are three steps for tone assignment:
lexical, morphemic and terraced. For TTS the lex-
ical and morphemic tones will need to be recov-
ered from the lexicon and the grammar as the or-
thography has no tone marking. The terraced tone
relation can in general be recovered and marked
automatically from the tone sequence with a ﬁnite
state model.

2 Stem Extraction Algorithm
The Stem Extraction Algorithm (SSE) is the su-
pervised algorithm for stem extraction. The train-
ing data for the SSE represent wordforms with
the marked stem boundaries. During the train-
ing stage we collect a set of all possible stem ex-
traction rules from training data and assign pre-
cision measures to each rule. Each rule is of the
form L R where ” ” is the stem boundary, L and
R are the left and right graphemic contexts of a
stem boundary of different lengths. We differen-
tiate preﬁx Lpre f Rstem and sufﬁx Lstem Rsu f f
stem extraction rules that correspond to the rules
containing the left-hand stem boundary and the
right-hand stem boundary respectively. For ex-
ample, the Turkish word yer (’earth’) with the
marked word boundary #ye r# generates the fol-
lowing Lstem Rsu f f rules: #ye r#, #ye r, ye r#,
#ye , ye r, e r#,
r, and e , where
the symbol ’#’ signiﬁes the word initial and ﬁ-
nal positions. We are implementing similar fea-
ture vectors used for automatic pronunciation pre-
diction based on the focal grapheme (in our case
it is a stem boundary) and left/right graphemic
contexts of different length (Davel and Barnard,
2008). The idea of implementing expanding con-
text in NLP tasks is usually applied for two-level
data like grapheme-to-phoneme mapping rules
(Torkkola, 1993), whereas in our case we use it
for one-level data.

r#, ye , e r,

The precision measure for each rule is calcu-
lated by the formula p/(p+n+ε) where p and n are
the number of positive and negative examples, and
ε is used to cover the cases where there are no neg-
ative examples. A high precision is desirable and
this occurs when there are high values of p and
low values of n (i.e. many positive examples and

978

few negative examples). Using negative examples
in contrast to using only rule frequencies (or pos-
itive examples) improves the performance of the
algorithm.

Algorithm 1 Choosing rule pair for stem extrac-
tion.
input W = raw wordform; P and S are sets of

unique Lpre f Rstem and Lstem Rsu f f rules

output result rule pair

Deﬁnition 1. The number of positive examples for
the rule Lstem Rsuff (or rule Lpref Rstem) is the
number of training instances of Stem Su f f ixes
(or Pre f ixes Stem) containing the substring L R.

Deﬁnition 2. The number of negative exam-
ples for rule Lstem Rsuff (or Lpref Rstem) is the
number of training instances Stem Su f f ixes (or
Pre f ixes Stem) such that Stem + Su f f ixes (or
Pre f ixes + Stem) contains substring L+R and
Stem Su f f ixes (or Pre f ixes Stem) does not con-
tain substring L R where ’+’ denotes string con-
catenation.

In the above deﬁnitions ’ ’ is a stem boundary.

Example 1. Suppose we have only three isiZulu
verbs: zi bek e, zi nak eke and a hlul eke. For
the Lstem Rsu f f rule ’ek e’, the word zi bek e
generates one positive example and the two other
words zi nak eke and a hlul eke generate one
negative example each.

The approach given in Algorithm 1 aims to ﬁnd
the unique longest rule-pair ’Lpre f Rstem and
Lstem Rsu f f ’ with the highest precision that is
applied to the input wordform for stem extraction.
In case the language does not have preﬁxes like
Turkish, the longest rule Lstem Rsu f f with the
highest precision is applied. The decision of us-
ing either a rule-pair or just a single sufﬁx rule
is inﬂuenced by prior knowledge that a particu-
lar language has got either both preﬁxes and suf-
ﬁxes like isiZulu or only sufﬁxes like Turkish.
From now on we will use the term ’rulepair’ in
application both to the rulepair ’Lpre f Rstem and
Lstem Rsu f f ’ in case of isiZulu and to the rule-
pair ’ and Lstem Rsu f f ’ with an empty ﬁrst ele-
ment in case of Turkish.

result rule pair ← /0
iMaxlength ← ∞
repeat

(p1,s1) ← getrulepair
(P × S, W,
iMaxlength)
(p2,s2) ← getrulepair (P × S \ (p1,s1), W,
iMaxlength)
iMaxlength ← length(p1,s1)

length (p1,s1)

(p1,s1) = /0 or precision (p1,s1)
(cid:3)=

until
(cid:3)= precision(p2,s2) or
length(p2,s2)
result rule pair ← (p1,s1)
function getrulepair(PS, W, iMaxlength)
ilength ← 0
r ← /0
for all (p,s) ∈ PS do

if (p,s) matches W then

length(p,s) < iMaxlength

if
length(p,s) > ilength then
ilength ← length(p,s)
r ← (p,s)
else
if
precision(p,s) > precision(r) then

length(p,s) = ilength

and

and

r ← (p,s)

end if

end if

end if
end for
return r
end function

The search is carried out on the set of rule
pairs matching an input raw wordform. The set
is sorted by length ﬁrst, and then by precision
measure within each length category.

For example, if rulepairs have the following

length-precision values:
’4-0.5,’4-0.5’,’4-0.2’

979

’3-0.4’,’3-0.3’
’2-0.3’
rulepair with the value 3-0.4 is selected.

The rulepair matches

the input word if
Lpre f Rstem and Lstem Rsu f f rules can be ap-
plied without contradicting each other. For exam-
ple, the rule pair ’#a hl’ and ’l eke’ matches the
word a hlul eke, whereas the rule pair ’#a hlulek’
and ’le ke’ does not match this word. For each
input wordform the set of its own rulepair candi-
dates is generated. The search in the algorithm
among these rulepairs starts from the longest rule-
pairs, and this allows more speciﬁc rules and ex-
ceptions to be applied ﬁrst, whereas the more gen-
eral rules are applied if no speciﬁc rules cover the
input wordform.

3 Graph-Based Unsupervised

Morpheme Segmentation

In this section we extend GBUMS (Graph-
Based Unsupervised Morpheme Segmentation)
that segments sequences of preﬁxes and sufﬁxes
(Gol´enia et al., 2009). We propose an exten-
sion of GBUMS which uses the graph structure of
GBUMS through a brute-force method. Our ex-
periments showed the improved results on train-
ing set and allowed GBUMS to be run on the test
sets for two languages: Turkish and isiZulu.

The algorithm GBUMS was originally devel-
oped in (Shalonova et al., 2009) under the name
GBUSS (Graph-Based Unsupervised Sufﬁx Seg-
mentation) to extract sufﬁx sequences efﬁciently
and it was applied to Russian and Turkish lan-
guages on training sets. We refer to preﬁxes and
sufﬁxes generally as morphemes. GBUMS uses
a morpheme graph in a bottom-up way. Similar
to Harris (Harris, 1955), the algorithm is based
on letter frequencies. However, when Harris uses
successor and predecessor frequencies, they use
position-independent n-gram statistics to merge
single letters into morphemes until a stopping cri-
terion is fulﬁlled.

In the morpheme graph, each node represents a
morpheme and each directed edge the concatena-
tion of two morphemes labelled with the frequen-
cies in a M-corpus (see Figure 1). M-corpus is a
list of morpheme sequences

Deﬁnition 3. Let M = {mi|1 ≤ i ≤ |M|} be a set
of morphemes, let fi be the frequency with which
morpheme mi occurs in a M-corpus of morpheme
sequences, let vi = (mi, fi) for 1 ≤ i ≤ n, and let
fi, j denote the number of morpheme sequences in
the corpus in which morpheme mi is followed by
morpheme m j. The morpheme graph G = (V,E)
is a directed graph with vertices or nodes V =
{vi|1 ≤ i ≤ |V|} and edges E = {(vi,v j)| fi, j > 0}.
We treat fi, j as the label of the edge from vi to v j.
In G, each node is initialised with a letter ac-
cording to a M-corpus, then one by one, nodes
are merged to create the real morphemes. To
merge nodes, an evaluation function is required.
In (Gol´enia et al., 2009), Golenia et al. employed
the Morph Lift evaluation function based on its re-
lation to the lift of a rule for association rules in
data mining (Brin et al., 1997).
Deﬁnition 4. Morph Li f t is deﬁned for a pair of
morphemes m1 and m2 as follows:

Morph Li f t(m1,m2) =

f1,2

f1 + f2

(1)

From now on, we know how to merge nodes.
Now, we need to ﬁgure out the most important
part of GBUMS, which is the stopping crite-
rion. The stopping criterion is to prevent over-
generalisation.
the algorithm
needs to be stopped before getting the initial M-
corpus (since no merging is possible). This cri-
terion is based on the Bayesian Information Cri-
terion (BIC) and Jensen-Shannon divergence (Li,
2001).

In other words,

BIC is used for selecting a model (set of mor-
phemes) which ﬁts a data set (M-Corpus) without
being too complex. We want to point out that BIC
is related to MDL. BIC is a trade-off between the
maximum likelihood, the parameters of the model
(probability and length of each morpheme) and
the number of elements in the data set (frequency
of each morpheme). A smaller value of BIC cor-
responds to a better model ﬁt. The maximum of
the Jensen-Shannon divergence is used in order to
analyse the increase of log-likelihood among all
possible models. The Jensen-Shannon divergence
is deﬁned as follows (Dagan et al., 1997):
Deﬁnition 5. The Jensen-Shannon divergence is
deﬁned for two morphemes m1 and m2 as the de-

980

crease in entropy between the concatenated and
the individual morphemes:

Lm1H(m1) + Lm2H(m2)

DJS(m1,m2) = H(m1·m2)−
(2)
where H(m) = −P(m)log2 P(m) N = ∑m Freq(m)
and Lm is the string length of m.

N

Stopping criterion requires
which translates to:

that ΔBIC < 0

max
m1,m2

DJS(m1,m2) ≤ 2log2 N

(3)

Algorithm 2 The GBUMS morpheme segmenta-
tion algorithm
input M-Corpus = List of Strings
output M-CorpusSeg = List of Strings

M-CorpusSeg ← SegmentInLetters(M-
Corpus);
Graph ← InitialiseGraph(M-CorpusSeg);
repeat

Max ← 0;
for all (p,q) ∈ Graph do

ML Max ← Morph Lift(p, q);
if ML Max > Max then

Max ← ML Max;
pMax ← p;
qMax ← q;

end if
end for
Graph ← MergeNodes(Graph,
qMax);
M-CorpusSeg ← DeleteBoundaries(M-
CorpusSeg, Label(pMax), Label(qMax));
Graph ← AdjustGraph(M-corpusSeg,
Graph);

pMax,

until StoppingCriterion(pMax, qMax, Max)

After several merging iterations, the output of
the algorithm is the graph shown in Figure 1. The
GBUMS is presented in Algorithm 2.
Note that the M-Corpus is completely segmented
at the beginning of the algorithm. Then,
the
boundaries in the segmented M-Corpus are re-
moved step by step according to a pair found in the
graph with the maximum value for Morph Li f t.

When the stopping criterion is fulﬁlled, the seg-
mented M-Corpus represents the morpheme se-
quences.

At

this point we present our extension of
GBUMS based on a brute-force heuristic which
scores every possible segmentation of an input
morpheme sequence using graph values. We
consider the morpheme graph as a model where
each morpheme sequence can be extracted by the
MGraph function (eq. 4).
Deﬁnition 6. We deﬁne MGraph of a morpheme
sequence without boundaries x as follows:

MGraph(x) = argmax

t⊆x

where

1

Nt −Ct ∑

m∈t

Lmlog( fm +1)
(4)

• t is a morpheme sequence with boundaries of

x,

• m is a morpheme of t,
• fm is the frequency of the morpheme m,
• Nt is the number of morphemes existing in

the graph,

• Ct is the number of morphemes existing and

contiguous in the graph.

Firstly, as a post-processing procedure the
MGraph function improves the performance on
training data. Secondly, it permits the identiﬁca-
tion of unseen morphemes. That is why the model
generated by GBUMS can be run on test data sets.
Example 2. Let our ﬁnal morpheme graph be as
shown in Figure 1 where nodes represent sufﬁxes
and their frequencies.
Let x=”ekwe” be our input sufﬁx sequence that we
want to segment into individual sufﬁxes. We split
this input sequence into all possible substrings
from individual characters up to size of the input
string length: e-k-w-e, e-k-we, e-kw-e, ek-w-e, . . . ,
ekwe.
Using equation 4, we evaluate each substring and
select the one with the highest score as the correct
segmentation. Here, we have 7 potential segmen-
tations with a score higher than 0 (MGraph > 0),
e.g: e-k-w-e = (log(3) + log(3))/2 = 1.0986, ek-
w-e =(2log(4) +log(3))/2 = 1.9356 and ek-we =

981

2log(4) = 2.7726.
Consequently, ek-we is chosen as the correct seg-
mentation for the substring ”ekwe”.

We would like to highlight that our new method
can identify unseen cases with M-Graph, for in-
stance, in the previous example sufﬁx ”we” was
not present in the training graph, but was correctly
extracted.















Figure 1: Example of a sufﬁx subgraph in the
training phase for isiZulu.

4 Results
Our experiments were based on Turkish data con-
taining 1457 verbs and 2267 nouns, and isiZulu
data containing 846 nouns and 931 verbs, with
one single unambiguous segmentation per word.1
Both isiZulu and Turkish data were uniquely sam-
pled from the most frequent word lists.

Our ﬁrst experiments compared TASR and the
new SSE algorithm for stem extraction (10-fold
cross validation assumes the following training
and test set sizes:
training sets containing 1311
wordforms for verbs and 2040 wordforms for
nouns;
test sets containing 146 wordforms for
verbs and 227 wordforms for nouns). As can be
seen from the Table 1, the performance of the SSE
algorithm on Turkish data is much higher than that
of TASR on the same dataset. As we mentioned in
Section 1, TASR is not suitable for agglutinating
languages with long sufﬁx sequences. Although
TASR algorithm gives an excellent performance
on Russian, for most Turkish words it fails to ex-
tract proper stems.

1In agglutinating languages some wordforms even within
one POS category can have several possible segmentations.

Test FMea

TASR Nouns
Verbs
Nouns
Verbs

SSE

20.7±6.8
12.6±5.9
84.3±3.2
82.1±3.7

Table 1: Comparison of TASR and SSE for Turk-
ish using 10-fold cross validation.

Our next experiments evaluated the perfor-
mance of GBUMS on its own given unsegmented
sufﬁx sequences from Turkish nouns and verbs as
training data. The performance on these training
data increased by approximately 3-4 % in com-
parison to the results presented in (Shalonova et
al., 2009). We would like to point out that the
results in (Shalonova et al., 2009) are based on
training data rather than on test data, whereas in
the current paper we run our algorithms on test
(or unseen) data. Our ﬁnal experiments examined
performance on the test sets and were run both
on Turkish and isiZulu data. We compared our
approach with Morfessor run both in supervised
and in unsupervised mode. Although Morfessor is
known as one of the best unsupervised morphol-
ogy learning systems, it is possible to run it in the
supervised mode as well (Spiegler et al., 2008).
The training data for SSE+ GBUMS contained
wordforms with marked stems. During training
stage the SSE algorithm was collecting informa-
tion about stem boundaries and the GBUMS al-
gorithm was run on unlabelled sufﬁx and pre-
ﬁx sequences from the same training set. The
test stage for the SSE+GBUMS approach was run
on ”raw” wordforms by applying the SSE algo-
rithm ﬁrst for stem extraction and then running
GBUMS algorithm for segmenting preﬁx or suf-
ﬁx sequences after the SSE has extracted stems.
Training data for supervised Morfessor used the
same wordforms as for the SSE+GBUMS train-
ing set and contained wordforms segmented into
stems and afﬁxes (i.e. words segmentated into
all morphemes were given as training data). The
test data for supervised Morfesor were the same
as those used for SSE+GBUMS. Morfessor in un-
supervised mode was run on ”raw” wordforms as
training data. To evaluate our current work we ap-

982

Test FMea

Test FMea

SSE+ GBUMS

Supervised Morfessor

Nouns
Verbs
Nouns
Verbs
Unsupervised Morfessor Nouns
Verbs

74.6±2.3
84.5±2.2
78.8±2.4
76.9±0.7
26.6±2.6
28.4±2.8
Table 2:
and
SSE+GBUMS for Turkish using 10-fold cross
validation.

Comparison of Morfessor

SSE+ GBUMS

Supervised Morfessor

Nouns
Verbs
Nouns
Verbs
Unsupervised Morfessor Nouns
Verbs

76.7±1.6
88.5±2.4
87.9±1.9
84.5±2.5
27.4±5.1
26.9±5.0
Table 3:
and
SSE+GBUMS for isiZulu using 10-fold cross
validation.

Comparison of Morfessor

plied the SSE+GBUMS approach for the under-
resourced agglutinating language isiZulu contain-
ing both preﬁxes and sufﬁxes and for Turkish con-
taining only sufﬁxes. The results show (Table 2
and Table 3) that our weakly supervised approach
is comparable with the supervised Morfessor and
decisively outperforms the unsupervised Morfes-
sor. We think that it is useful to point out that un-
supervised morphology learning systems in gen-
eral require much larger training sets for better
performance. F-measure is the harmonic mean
of precision and recall, whereas precision is the
proportion of true morpheme boundaries among
the boundaries found, recall is the proportion
of boundaries found among the true boundaries.
In our experiments the GBUMS algorithm had
no restrictions on afﬁx length (Shalonova et al.,
2009), but if there were restrictions, performance
could be better. For isiZulu nouns our approach
signiﬁcantly outperformed supervised Morfessor,
whereas for Turkish verbs SSE+GBUMS per-
formed much worse. The best overall results
obtained by GBUMS were based on the isiZulu
nouns where about 53% of all afﬁxes were sin-
gle letter afﬁxes, whereas the worst results our ap-
proach gave for Turkish verbs where only about
12% of afﬁxes are composed of one letter. It is
important to notice that the GBUMS algorithm,
which is completely unsupervised, gives better re-
sults for extracting one letter afﬁxes compared to
Morfessor.

5 Conclusions

In the paper we described a weakly supervised
approach for learning morphology in agglutinat-

ing languages. We were successful in our ulti-
mate goal of synthesis of supervised and unsuper-
vised induction techniques by achieving high per-
formance on small amount of training data. Our
weakly supervised approach is comparable with
the supervised morphology learning system. As
we are working with the languages for which lin-
guistic resources are very limited (in particular
words with morpheme boundaries), the developed
method fulﬁlls our goals of providing key compo-
nents for speech and language products for such
under-resourced languages. We speculate that the
current performance might be improved by adding
a small amount of completely ”raw” data to the
training set.
The integration of our algorithms into working
TTS systems is of key importance. As our near-
term goal is the integration of morphology learn-
ing component into the currently working isiZulu
TTS system, we will have to analyse the neces-
sity of a Part of Speech Tagger (POS) and mor-
phological disambiguation. In agglutinating lan-
guages some wordforms can be segmented in dif-
ferent ways (i.e. have different surface forms)
and Machine Learning approaches normally se-
lect the most probable segmentation, and there-
fore our morphology disambiguation can be im-
portant. Morphological disambiguation for TTS
can be considered a less complex problem than
full morphological disambiguation as it can be
linked, for example, to lexical tone disambigua-
tion that may not require the full POS tag set.
We intend to carry out user perception tests in or-
der to evaluate the possible improvement in the
isiZulu TTS quality after morphology information
is added.

983

Recognition and Production. Ph.D. thesis, Univer-
sity of Helsinki.

Li, W.

2001. New stopping criteria for segment-
Physical Review Letters,

ing DNA sequences.
86(25):5815–5818.

Oﬂazer, K., M. McShane, and S. Nirenburg. 2001.
Bootstrapping morphological analyzers by combin-
ing human elicitation and machine learning. Com-
putational Linguistics, 27(1):59–85.

Shalonova, K., B. Golenia, and P. Flach.

2009.
Towards learning morphology for under-resourced
languages.
IEEE Transactions on Audio, Speech
and Language Procesing, 17(5):956–965.

Spiegler, S., B. Golenia, K. Shalonova, P. Flach, and
R. Tucker. 2008. Learning the morphology of Zulu
with different degrees of supervision. IEEE Spoken
Language Technology Workshop, pages 9–12.

Torkkola, K. 1993. An efﬁcient way to learn English
grapheme-to-phoneme rules automatically. Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, pages 199–202.

6 Acknowledgment
We would like to thank Kemal Oﬂazer from Sa-
banci University in Istanbul for his help and the
Turkish data. We also thank Viktor Zimu and Eti-
enne Barnard from CSIR in Pretoria for provid-
ing us isiZulu data. We also thank Roger Tucker
for his support in the project. The work was
sponsored by the EPSRC Grant EP/E010857/1
’Learning the morphology of complex synthetic
languages’.

References
Brin, S., R. Motwani, J. Ullman, and S. Tsur. 1997.
Dynamic itemset counting and implication rules for
market basket data. In ACM SIGMOD international
conference on Management of data, pages 255–264.
ACM.

Creutz, M. and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. Proceedings of the Workshop on
Morphological and Phonological Learning of ACL-
02, pages 21–30.

Dagan, I., L. Lee, and F. Pereira. 1997. Similarity-
Based Methods for Word Sense Disambiguation.
Thirty-Fifth Annual Meeting of the ACL and Eighth
Conference of the EACL, pages 56–63.

Davel, M. and E. Barnard. 2008. Pronunciation pre-
diction with default reﬁne. Computer Speech and
Language, 22:374–393.

Erjavec, T. and S. Dzeroski. 2004. Machine learn-
ing of morphosyntactic structure: Lemmatising un-
known Slovene words. Applied Artiﬁcial Intelli-
gence, 18(1):17–40.

Goldsmith, J. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27:153–198.

Gol´enia, B., S. Spiegler, and P. Flach. 2009. UN-
GRADE: UNsupervised GRAph DEcomposition.
In Working Notes for the CLEF 2009 Workshop,
CLEF 2009, Corfu, Greece.

Harris, Z. 1955. From Phoneme to Morpheme. Lan-

guage, 31(2):190–222.

Kazakov, D. and S. Manandhar. 2001. Unsupervised
learning of word segmentation rules with genetic
algorithms and inductive logic programming. Ma-
chine Learning, 43:121–162.

Koskenniemmi, K.

1983. Two-level Morphology:
A General Computational Model for Word Form

