










































Learning Hierarchical Linguistic Descriptions of Visual Datasets


Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ’13), pages 20–28,
Atlanta, Georgia, 14 June 2013. c©2013 Association for Computational Linguistics

Learning Hierarchical Linguistic Descriptions of Visual Datasets

Roni Mittelman†, Min Sun‡, Benjamin Kuipers†, Silvio Savarese†
† Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor

‡ Department of Computer Science and Engineering, University of Washington, Seatle
rmittelm,kuipers,silvio@umich.edu, sunmin@cs.washington.edu

Abstract

We propose a method to learn succinct hi-
erarchical linguistic descriptions of visual
datasets, which allow for improved navigation
efficiency in image collections. Classic ex-
ploratory data analysis methods, such as ag-
glomerative hierarchical clustering, only pro-
vide a means of obtaining a tree-structured
partitioning of the data. This requires the user
to go through the images first, in order to re-
veal the semantic relationship between the dif-
ferent nodes. On the other hand, in this work
we propose to learn a hierarchy of linguistic
descriptions, referred to as attributes, which
allows for a textual description of the seman-
tic content that is captured by the hierarchy.
Our approach is based on a generative model,
which relates the attribute descriptions asso-
ciated with each node, and the node assign-
ments of the data instances, in a probabilistic
fashion. We furthermore use a nonparametric
Bayesian prior, known as the tree-structured
stick breaking process, which allows for the
structure of the tree to be learned in an unsu-
pervised fashion. We also propose appropriate
performance measures, and demonstrate supe-
rior performance compared to other hierarchi-
cal clustering algorithms.

1 Introduction

With the abundance of images available both for per-
sonal use and in large internet based datasets, such
as Flickr and Google Image Search, hierarchies of
images are an important tool that allows for conve-
nient browsing and efficient search and retrieval. In-
tuitively, desirable hierarchies should capture simi-

larity in a semantic space, i.e. nearby nodes should
include categories which are semantically more sim-
ilar, as compared to nodes which are more distant.
Recent works that are concerned with learning im-
age hierarchies (Bart et al., 2011; Sivic et al., 2008),
have relied on a bag of visual-words feature space,
and therefore have been shown to provide unsatis-
factory results with respect to the latter requirement
(Li et al., 2010).

A recent trend in visual recognition systems, has
been to shift from using a low-level feature based
representation to an attribute based feature space,
which can capture higher level semantics (Farhadi
et al., 2009; Lampert et al., 2009; Parikh & Grau-
man, 2011; Berg et al., 2011; Ferrari & Zisserman,
2007). Attributes are detectors that are trained using
annotation data, to identify particular properties in
an instance image. By evaluating these detectors on
a query image, one can obtain a linguistic descrip-
tion of the image. Therefore, learning a visual hier-
archy based on an attribute representation can allow
for an improved semantic grouping, as compared to
the previous use of low-level image features.

In this work we wish to utilize an attribute based
representation to learn a hierarchical linguistic de-
scription of a visual dataset, in which few attributes
are associated with each node of the tree. As is il-
lustrated in Figure 1, such an attribute hierarchy is
tightly related to a category hierarchy, in which the
instances associated with every node are described
using all the attributes associated with all the nodes
along the path leading up to the root node (the in-
stances in Figure 1 are described by the correspond-
ing photographs). This “duality” between the at-

20



!"#$%&'()*+#$!'##$,'-.#//$01-2+34$

%5(6#$ 7#"(.5#/$

8(29-:;$$
<#'-9=2<>(.$
/+'*.+*'#$

,#9<5;$:"##5$

?#+<5$8--9$

8"##5$

•  1"<55#2@#/A$B..5*/(-2;$C-(/=$<&'()*+#$/.-'#/3$
Figure 1: Attribute and category hierarchies.

tribute and category hierarchies, offers an impor-
tant advantage when characterizing the dataset to the
end-user, since it eliminates the need to visually in-
spect the images assigned to each node, in order to
reveal the semantic relationship between the cate-
gories that are associated with the different nodes.
Exploratory data analysis methods for learning hier-
archies, such as agglomerative hierarchical cluster-
ing (AHC) (Jain & Dubes, 1988 p. 59), only assign
instances to different nodes in the tree, whereas our
approach learns an attribute hierarchy which is used
to assign the instances to the nodes of the tree. The
attribute hierarchy provides a linguistic description
of a category hierarchy.

We develop a generative model, which we refer
to as the attribute tree process (ATP), and which ties
together the attribute and category hierarchies in a
probabilistic fashion. The tree structure is learned
by incorporating a nonparametric Bayesian prior,
known as the tree-structured stick breaking process
(TSSBP) (Adams et al., 2010), in the probabilis-
tic formulation. An important observation which
we make about the attribute hierarchies which are
learned using the ATP, is that attributes which are
related to more image instances tend to be associ-
ated with nodes which are closer to the root, and vice
versa, attributes which are associated with fewer in-
stances tend to be associated with leaf nodes. A hi-
erarchical clustering algorithm that is based on the
TSSBP for binary feature vectors was developed in
(Adams et al., 2010), and is known as the factored
Bernoulli likelihood model (FBLM). However, sim-
ilarly to AHC, it does not produce the attribute hier-
archy in which we are interested.

In order to evaluate the ATP quantitatively, we
compare its performance to other hierarchical clus-
tering algorithms. If the ground truth of the category
hierarchy is available, we propose to use the seman-

tic distance between the categories, that is given by
the ground truth hierarchy, to evaluate the degree to
which the semantic distance between the categories
is preserved by the hierarchical clustering algorithm.
If the ground truth is not available, we use two cri-
teria, which as we argue, capture the properties that
should be demonstrated by desirable semantic hier-
archies. The first is the “purity criterion” (Manning
et al., 2009 p. 357) which measures the degree to
which each node is occupied by instances from a
single class, and the second is the “locality criterion”
which we propose, and which measures the degree
to which instances from the same class are assigned
to nearby nodes in the hierarchy. Our experimen-
tal results show that when compared to AHC and
FBLM, our approach captures the ground truth se-
mantic distance between the categories more accu-
rately, and without significant dependence on hyper-
parameters.

The remaining of this paper is organized as fol-
lows. In Sec. 2 we provide background on agglom-
erative hierarchical clustering, and on the TSSBP,
and in Sec. 3 we develop the generative model for
the ATP. In Sec. 4 we propose evaluation metrics for
the attribute hierarchy, and in Sec. 5 we present the
experimental results. Sec. 6 concludes this paper.

2 Background

2.1 Agglomerative hierarchical clustering

AHC uses a bottom up approach to clustering. In
the first iteration, each cluster includes a single in-
stance of the dataset, and at each following iteration,
the two clusters which are closest to each other are
joined into a single cluster. This requires a distance
metric, which measures the distance between clus-
ters, to be defined. The algorithm concludes when
the distance between the farthest clusters is smaller
than some threshold.

2.2 Tree structured stick breaking process

The TSSBP is an infinite mixture model, where each
mixture component has one-to-one correspondence
with one of the nodes of an infinitely branching and
infinitely deep tree. The weights of the infinite mix-
ture model are generated by interleaving two stick-
breaking processes (Teh et al., 2006), which allows
the number of mixture components to be inferred

21



zi

xi
N

!!0

!1 !2 !3

!1!1 !1!2 !2!1 !3!1 !3!2

(a) TSSBP

zi

yi
N

!!0

!1 !2 !3

!1!1 !1!2 !2!1 !3!1 !3!2 !

xi

(b) ATP

Figure 2: The graphical model representations of the probability distribution functions for the (a) TSSBP, and (b) ATP
(ours). The parameter θ� is associated with node � in the tree, and T denotes the parameters {π�}�∈T from the TSSBP
construction, where T is the set of all the nodes in the tree.

from the data in a Bayesian fashion. Since each mix-
ture component is associated with a unique node in
the infinite tree, this is equivalent to inferring the
structure of the tree from the data. Let T denote
the infinite set of node indices, and let π� denote the
corresponding weight of the mixture component as-
sociated with node � ∈ T , then one can sample a
node z in the tree using

z ∼
∑
�∈T

π�δ�(z), (1)

where δ�() denotes a Dirac delta function at �.
Since the cardinality of the set T is unbounded,

sampling from (1) is not trivial, however, an effi-
cient sampling scheme was presented in (Adams et
al., 2010). Similarly, an efficient scheme for sam-
pling from the posterior of {π�}�∈T given the node
assignments of all the data instances, was also de-
veloped in (Adams et al., 2010).

2.2.1 Factored Bernoulli likelihood model
The FBLM was used in (Adams et al., 2010)

to perform hierarchical clustering of color images
using binary feature vectors. Let xi ∈ {0, 1}D,
i = 1, . . . , N , denote a set of binary training vec-
tors that are available for learning the hierarchy.
The graphical model representation of the proba-
bility distribution function is shown in Figure 2a,
where for the sake of clarity of the exposition, the
tree that is shown here is finite. The parameters
θ� = [θ

(1)
� , . . . , θ

(D)
� ]T satisfy

θ(d)� = θ
(d)
Pa(�) + n

(d)
� , d = 1, . . . , D, (2)

where n(d)� ∼ N (0, σ2), and Pa(�) denotes the par-
ent of node �. The indicator variable z is sampled
using (1), and the likelihood of a binary observation

vector x = [x(1), . . . , x(D)]T follows a Bernoulli
distribution whose parameter is a logistic function

f(x|θz) =
D∏
d=1

(1+ exp {−θ(d)z })−x
(d)

× (1 + exp {θ(d)z })−(1−x
(d)).

(3)

3 The attribute tree process

In this section, we develop a new generative model
that is based on the TSSBP, however unlike the
FBLM it also reveals a hierarchy of attributes, which
allows for a linguistic description of the image
dataset. This is achieved by relating the attribute hi-
erarchy, and the assignment of image instances to
nodes, in a probabilistic manner.

3.1 The attribute hierarchy
In order to allow for a probabilistic description of the
attribute hierarchy, we associate a parameter vector
θ = [θ

(1)
� , . . . , θ

(D)
� ]T with each node � ∈ T , where

D denotes the number of attributes. The attributes
y

(d)
i , d = 1, . . . , D that are associated with a data

instance i that is assigned to node �, are generated
using the following scheme:

1. For each �′ ∈ A(�), draw ξ(d)�′,i ∼
Bernoulli(θ(d)�′ ), d = 1, . . . , D,

2. Set y(d)i =
⊕

�′∈A(�) ξ
(d)
�′,i, d = 1, . . . , D,

where ξ(d)�′,i is an auxiliary random variable,
⊕

denotes the logical or operation, and A(�) de-
notes the set composed of all the ancestors of
node �. By marginalizing with respect to ξ(d)�′,i,
we obtain a simplified representation: first set

22



h
(d)
� = 1−

∏
�′∈A(�)(1− θ

(d)
�′ ), and then sample

y
(d)
i ∼ Bernoulli(h

(d)
� ) for every d = 1, . . . , D.

We use the parameters h(d)� to define the attribute
hierarchy, since they represent the probability of an
attribute being associated with an instance assigned
to node �. Furthermore, they satisfy the property
that the likelihood of any attribute can only increase
when moving deeper into the tree. We can obtain
an attribute hierarchy, similar to that in Figure 1, by
thresholding h(d)� , and only displaying attributes that
have not been detected at any ancestor node.

In order to complete our probabilistic formulation
for the attribute hierarchy, we need to specify the
prior for the node parameters θ�. We use a finite ap-
proximation to a hierarchical Beta process (Thibaux
& Jordan, 2007; Paisley & Carin, 2009). This choice
promotes sparsity, and therefore only few attributes
will be associated with each node. Specifically, the
parameters at the root node follow

θ
(d)
0 ∼ Beta(a/D, b(D − 1)/D), d = 1, . . . , D,

(4)
and the parameters in the other nodes follow

θ(d)� ∼ Beta(c(d)θ
(d)
Pa(�), c

(d)(1−θ(d)Pa(�))), d = 1, . . . , D,
(5)

where Pa(�) denotes the parent of node �, and where
a, b, and c(d), d = 1, . . . , D are positive scalar pa-
rameters.

In this work we used a uniform prior for the pre-
cision hyper-parameter c(d) ∼ U [l, u] with ` = 20,
and u = 100. We also used the hyper-parameter val-
ues a = 10, and b = 5 (unless otherwise stated). In
Section 5.1.1 we demonstrate that the performance
of our algorithm depends only weakly on the choice
of these parameters.

3.2 Assigning images to nodes
In order to assign every image instance to one of the
nodes, we combine the attribute hierarchy with the
TSSBP. The resulting graphical model representa-
tion of the probability distribution function is shown
in Figure 2b. For every data instance i, a node zi
in the tree is sampled from the TSSBP. The ob-
served attribute vector xi is obtained by sampling
y

(d)
i ∼ Bernoulli(h

(d)
zi ), and flipping y

(d)
i with prob-

ability ω, which models the effect of the noisy at-
tribute detectors. By marginalizing over y(d)i , we

have that

p(x
(d)
i = 1|−) = 1− ((1− h

(d)
zi )(1− ω

(d)).

+ h(d)zi ω
(d)). (6)

The prior for ω is ω ∼ Beta(ρ0, ρ1), where in this
work we used ρ0 = 5, and ρ1 = 20, which pro-
motes smaller values for ω. Our algorithm is highly
insensitive to the choice of ρ0, ρ1, as long as they are
chosen to promote small values of ω.

3.3 Inference
Inference in the ATP is based on Gibbs sampling
scheme. In order to sample from the posterior of
the node parameter θ(d)� , we note that

p(θ(d)� |−) ∝∏
�′∈�∪D(�)

(1− ((1− h(d)�′ )(1− ω
(d)) + h

(d)
�′ ω

(d)))n
(1,d)

�′

× ((1− h(d)�′ )(1− ω
(d)) + h

(d)
�′ ω

(d))n
(0,d)

�′

×
∏

�′′∈Ch(�)

Beta(θ
(d)

�′′
; c(d)θ(d)� , c

(d)(1− θ(d)� ))

× Beta(θ(d)� ; a(d)� , b(d)� ), (7)

where n(j,d)� =
∑

i|zi=� δj(x
(d)
i ) for j = 0, 1, D(�)

denotes the set composed of all the descendants of
node �, Ch(�) denotes the child nodes of node �, and
a

(d)
� = a/D, b

(d)
� = b(D − 1)/D, for � = 0 (the

root node), and for any other node: a(d)� = c(d)θ
(d)
� ,

b
(d)
� = c(d)(1 − θ(d)� ). The expression in (7) is a

highly complicated function of θ(d)� , and therefore
we use slice-sampling (Neal, 2000) in order to sam-
ple from the posterior. The slice-sampler is very
much a “black-box” algorithm, which only requires
the log likelihood of (7) and very few parameters,
and returns a sample from the posterior. We sam-
ple the node parameters using a two-pass approach,
starting from the leaf nodes and moving up to the
root, and subsequently moving down the tree from
the root to the leaves.

In order to sample from ω, we first sample the
binary random variables y(d)i using

p(y
(d)
i = j|−) ∝ p(y

(d)
i = j|−)(δj(x

(d)
i )(1− ω

(d))

+ δ1−j(x
(d)
i )ω

(d)), j = 0, 1, (8)

23



and then sample ω using

ω(d)|− ∼Beta
(
ρ0 +

N∑
i=1

δ1(y
(d)
i xor x

(d)
i ),

ρ1 +
N∑
i=1

δ0(y
(d)
i xor x

(d)
i )
)
. (9)

Sampling from the posterior of the hyper-parameter
c(d) was also performed using slice sampling. We
note that slice sampling each of the parameters θ(d)�
for d = 1, . . . , D, and each of c(d) for d = 1, . . . , D,
can be implemented in a parallel fashion. There-
fore, the computational bottleneck in the ATP is the
number of nodes in the tree, rather than the num-
ber of attributes. Sampling from the posterior of
the TSSBP parameters is performed using the algo-
rithms developed in (Adams et al., 2010). The pa-
rameters of the stick-breaking processes involved in
the TSSBP construction are also learned from the
data using slice-sampling, by assuming a uniform
prior on some interval (as was also performed in
(Adams et al., 2010)).

4 Evaluating the attribute hierarchy

In order to quantify the performance of the attribute
hierarchies, we evaluate the performance of the ATP
as a hierarchical clustering algorithm. We consider
two cases, in the first, the ground truth category hi-
erarchy is available and can be used to compare dif-
ferent hierarchies quantitatively. In the second case,
the ground truth is unavailable.

4.1 Using the ground truth category hierarchy
The category hierarchy should capture the distance
between the categories in a semantic space. For
instance, since car and bus are both vehicles, they
should be assigned to nodes which are closer, com-
pared to the categories car and sheep, which are se-
mantically less similar. Given the ground truth cate-
gory hierarchy, we can “measure” the semantic dis-
tance between different categories by counting the
number of edges that separate any two categories in
the graph.

In order to compare the hierarchies learned using
different hierarchical clustering algorithms, we pro-
pose a criterion which measures the degree to which
the semantic distance which a hierarchy assigns to

different image instances, diverges from the seman-
tic distance that is given by the ground truth cate-
gory hierarchy. Let dGT (c1, c2) denote the num-
ber of edges separating categories c1 and c2 in the
ground truth category hierarchy, and let dH(i, j) de-
note the number of edges separating instances i and
j in a hierarchy that is learned using a hierarchical
clustering algorithm. Our proposed criterion, which
we refer to as the average edge error (AEE), takes
the form

2

N(N − 1)

N−1∑
i=1

N∑
j=i+1

|dH(i, j)− dGT (c(i), c(j))|,

(10)
where c(i) denotes the category of instance i, andN
denotes the number of image instances.

4.2 Without ground truth hierarchy

When the ground truth of the category is unavail-
able, we propose to use the following two criteria in
order to evaluate the hierarchies. The first is known
as the purity criterion (Manning et al., 2009 p. 357),
and the second is the locality criterion which we pro-
pose. The purity criterion measures the degree to
which each node is occupied by instances from a
single class, and takes the form

Purity =
1

N

∑
�∈T

N�∑
i=1

δc∗� (c(i)), (11)

whereN� denotes the number of instances in node �,
and c∗� is the class which is most frequent in node �.

The locality criterion measures the degree to
which each class is concentrated in few adjacent
nodes. Quantitatively we define the category local-
ity for class c as

CLc = −
2

(|C| − 1)|C|
∑

i, j ∈ C,
i 6= j

dist(�i, �j),

(12)
where | · | denotes the cardinality of a set, C =
{i|ci = c} where ci is the class associated with in-
stance i, and dist(�i, �j) denotes the number of edges
along the path separating nodes �i and �j . The cate-
gory locality is negative or equal to zero. Values that
are closer to 0 indicate that the instances of category

24



c are concentrated in a few adjacent nodes, and neg-
ative values indicate that the category instances are
more dispersed in the tree. We define the locality as
the weighted average of the category locality, where
the weights are the category instance frequencies.

We note that each of these objectives can gener-
ally be improved on the account of the other: locality
can usually be improved by joining nodes (which in
general makes purity worse), and purity can usually
be improved by splitting nodes (which in general
makes locality worse). Therefore, we argue that a
desirable hierarchy should offer an acceptable com-
promise between these two performance measures.

5 Experimental results

In this section we learn the attribute hierarchy using
our proposed ATP algorithm. In order to evaluate
the performance we evaluate the ATP as a hierar-
chical clustering algorithm, and compare it to the
FBLM and AHC. We use subsets of the PASCAL
VOC2008, and SUN09 datasets, for which attribute
annotations are available. We learn hierarchies us-
ing the ground truth attribute annotation of the train-
ing set, and using the attribute scores obtained for
the image instances in the testing set, where the at-
tribute detectors are trained using the training set.
We used the FBLM implementation which is avail-
able online. Our implementation of the ATP is based
the TSSBP implementation which is available on-
line, where we extended it to implement our ATP
generative model. We used the AHC implementa-
tion available at (Mullner, ), where we used the aver-
age distance metric, which is also known as the Un-
weighted Pair Group Method with Arithmetic Mean
(UMPGA) (Murtagh, 1984).

5.1 Object category hierarchy

Here we consider the PASCAL VOC 2008 dataset.
We use the bounding boxes and attribute annotation
data that were collected in (Farhadi et al., 2009), and
are available online, along with the low-level image
features. Each of the training and testing sets con-
tains over 6000 instances of the object classes: per-
son, bird, cat, cow, dog, horse, sheep, airplane, bi-
cycle, boat, bus, car, motorcycle, train, bottle, chair,
dining-table, potted-plant, sofa, and tv/monitor. We
used the annotation and features available for the

training set, to train the attribute detectors using a
linear SVM classifier (Fan et al., 2008). We used
88 attributes, which included 24 attributes in addi-
tion to those used in (Farhadi et al., 2009): “pet”,
“vehicle”, “alive”, “animal”, and the remaining 20
attributes were identical to the object classes. The
annotation for the first 4 additional attributes was in-
ferred from the object classes. In all the experiments
presented here, we ran the Markov chain for 10,000
iterations and used the tree and model parameters
from the final iteration.

The attribute hierarchies for the PASCAL dataset
are shown in Figure 3, when using the annotation for
the training set, and when using the attribute scores
obtained for the testing set. The hierarchies were
obtained by thresholding h(d)� with the threshold pa-
rameter 0.7 (this parameter is only used to create the
visualization, and it is not used when learning the
hierarchies), and only displaying the attributes that
are not already associated with an ancestor node. It
can be seen that the attribute hierarchies can accu-
rately capture the semantic space that is represented
by the 20 categories in the PASCAL dataset. An im-
portant observation is that attributes which are asso-
ciated with more categories, such as alive or vehicle,
are assigned to nodes that are closer to the root node,
as compared to more specialized attributes such as
eye or window.

In order to evaluate the performance of the at-
tribute hierarchies quantitatively, we use the ground
truth category hierarchy for the 20 categories in the
PASCAL dataset, which is available at (Binder et al.,
2012), and is shown in Figure 4. In Figure 5 we
show the AEE performance measure, which we dis-
cussed in the previous section, for the different hi-
erarchical clustering algorithms which we consider
here. It can be seen that for the AHC, the AEE is
very sensitive to the threshold parameter, which ef-
fectively determines the number of clusters. A poor
choice of the parameter can adversely affect the per-
formance significantly. On the other hand, the per-
formance of the ATP and FBLM is significantly less
sensitive to the choice of the hyper-parameters, since
all the parameters are learned in a Bayesian fash-
ion with weak dependence on the hyper-parameters.
This is demonstrated for the ATP in Section 5.1.1.
The ATP significantly improves the AEE as com-

25



!"#$%&'%()*+&,%-./&+*)%/&&
0-1%/&)2#+/&1"*34&

5-(/&6*734/&
%8%/&3*()*/&&
,-+./&-(6/&"%9&

:11"7.%./&%-(/&
6*734/&4-#(/&%8%&

,%-./&3*()*/&
)2#+/&1"*34&

!(6/&"%9&

!+#6-"&

,%-./&
;%3&

5-(/&)+*73/&%8%/&
3*()*/&"%9/&07((8&

<*()*/&
)4%%;&

,%-./&%-(/&)+*73/&
"%9/&=**"&

,%-./&
>#(.&

<-#"/&?%-2/&%8%/&
3*()*/&0%-34%(/&"%9&&

,%-./&3*()*/&
07((8&

,%-./&)+*73/&&
07((8/&4*()%/&
3*()*/&"%9&

@**3A)4*%/&
%-(/&%8%&

B%4#1"%&

C%3-"&

D#+9/&
-#(;"-+%& EF&>*G8/&.**(/&&

=4%%"/&&
=#+.*=/&)4#+8&H%3&%+9#+%/&

=#+.*=/&(*=I
=#+./&=4%%"/&
3%G3/&4*(#J*+3-"&
18"#+.%(&

?*-3&

C%3-"& D#+.*=/&
(*=I=#+.&

EF&>*G8/&3(-#+&
=#+.*=&

EF&>*G8/&
9"-))/&>7)&

C%3-"/&1-(/&
$%4#1"%&

D#+9/&
-#(;"-+%&

C%3-"/&
$%4#1"%&

D4%%"/&>#181"%/&
4-+."%>-()&&

,-+."%>-()/&
=4%%"/&
6*3*(>#2%&

K4#+8&

L4-#(/&
07(+#37(%&
>-12&

?*M"%/&$%(N1-"&
18"#+.%(&

!"#$%&

O%-0/&;*3/&
$%9%3-N*+/&
$%(N1-"&
18"#+.%(/&
;*M%.I;"-+3&

:11"7.%./&
.#+#+9I3->"%&

K1(%%+/&
;"-)N1&

(a) Using the attribute annotation of the training set.

!""#$%&%'

!""#$%&%' ()*+,-./,01'
2"0&&-1'3#422'

5#,/61'4#.)&1'
7&02,-'

8&4%1'/,02,1'
29.-'

:0+1'
'#&3'

8&4%1'-,2&1'
64.01';4"&1'29.-'<401'+,$/61'

&=&1'/,02,'

<401'+,$/61'&=&1'
/,02,1'64-%1'40+'

8&4%1'4#.)&'
<401'2-,$/1'
&=&1'/,02,1'
4-.+4#'

>&31';,,/?
26,&1';$00='

(,02,1'#&3'(,02,1'
;&4/6&0'

@&/4#1'
)&6."#&'

A6&&#'A.-%,B'
CD'E,F=1'
B6&&#1'26.-='

A.-31'4.07#4-&1'
6,0.G,-/4#'"=#.-%&0'

>&4;1'7,/1'4#.)&1'
)&3&/4H,-1'
7,I&%*7#4-/'

J,I#&1''
)&0H"4#''
"=#.-%&0'

(b) Using the attribute scores of the testing set.

Figure 3: Attribute hierarchy learned for the PASCAL dataset, using the (a) attribute annotation available for the
training set, and (b) attribute scores obtained by applying the attribute detectors to the image instances of the testing
set. The largest circle denotes the root node.

pared to the FBLM, both for the training and testing
sets. We also note, that for the ATP, the AEE ob-
tained for the training set is better than that obtained
using the testing set’s attribute scores (training: 1.76,
testing: 2.82), which is consistent with our expecta-
tion. This is not the case for the FBLM (training:
6.55, testing: 5.63).

5.1.1 Sensitivity to hyper-parameters

In order to validate our claim that the ATP is
highly insensitive to the choice of hyper-parameters,
we performed experiments with different hyper-
parameter values. In Table 1 we compare the per-
formance when using different values for the hyper-
parameters a, and b in (4). It can be seen that
when comparing to AHC in Figure 5, the ATP is
significantly less sensitive to the choice of hyper-
parameters. When comparing to the FBLM, even

Figure 4: The ground truth category hierarchy, for the 20
categories in the PASCAL dataset.

for the the worst choice of a, b the AEE is still sig-
nificantly better.

5.2 Scene category hierarchy
Here we used the SUN09 dataset which is comprised
of indoor and outdoor scenes. We use the training

26



2 2.5 3 3.5 4 4.5 5
0

2

4

6

8

10

12

AHC threshold

AE
E

 

 
AHC
ATP
FBLM

(a) Training

2 2.5 3 3.5 4 4.5 5
0

5

10

15

20

25

30

AHC threshold

AE
E

 

 
AHC
ATP
FBLM

(b) Testing

Figure 5: The average edge error (AEE) (10) vs. the
AHC threshold parameter, for the hierarchies learned us-
ing the (a) training set’s attribute annotations, and (b)
attribute detectors applied to the testing set image in-
stances. Smaller values indicate better performance. It
can be seen that our ATP algorithm outperforms the
FBLM, and unlike the AHC, it is not as sensitive to the
choice of the hyper-parameters.

Table 1: Average edge error using the attribute annotation
of the training set, for different hyper-parameters.

a b AEE
1 10 1.97
5 5 1.93

10 5 1.76
10 10 1.585
10 20 1.569

and testing sets which were used in (Myung et al.,
2012), each containing over 4000 images. The an-
notation of 111 objects in the training set, and object
detector scores for the testing set, are available on-
line. Objects in the scene have the role of attributes
in describing the scene. The object classifiers were
trained using logistic regression classifiers based on
Gist features that were extracted from the training
set.

Since the ground truth category hierarchy is un-
available for this dataset, we use the locality and
purity criteria, which we described in the previous
section. We computed both of these measures with
respect to the indoor and outdoor categories. Fig-
ure 6 shows the locality and purity measures for the
training and testing sets. It can be seen that the AHC
is very sensitive to the threshold parameter, and can
produce unsatisfactory performance for a wide range
of parameter values. The FBLM slightly outper-
forms the ATP with respect to the purity measure,
however, its locality is very poor. Therefore, we con-
clude that the ATP provides an improved compro-

2.5 3 3.5 4
−25

−20

−15

−10

−5

0

AHC threshold

Lo
ca

lit
y

 

 

AHC
ATP
FBLM

(a) Training- Locality

2.5 3 3.5 4
0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

AHC threshold

Pu
rit

y

 

 

AHC
ATP
FBLM

(b) Training- Purity

2.5 3 3.5 4
−7

−6

−5

−4

−3

−2

−1

0

Lo
ca

lit
y

AHC threshold

 

 
AHC
ATP
FBLM

(c) Testing- Locality

2.5 3 3.5 4
0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

AHC threshold

Pu
rit

y

 

 
AHC
ATP
FBLM

(d) Testing- Purity

Figure 6: The locality and purity measures vs. the AHC
threshold parameter, using the training set’s attribute an-
notation, and for the testing set’s attribute scores. Larger
values indicate better performance. It can be seen that
our ATP has significantly better locality, and only slightly
worse purity, compared to the FBLM. Furthermore, the
performance of the AHC depends significantly on the
choice of threshold parameter.

mise with respect to the two criteria, which shows
that the ATP captures the properties of a desirable
hierarchy better than the FBLM.

6 Conclusions
We developed an algorithm, which we refer to as
the attribute tree process (ATP), that uses an attribute
based representation to learn a hierarchy of linguis-
tic descriptions, and can be used to describe a visual
dataset verbally. In order to quantitatively evaluate
the performance of our algorithm, we proposed ap-
propriate performance metrics for the cases where
the ground truth category hierarchy is known, and
when it is unknown. We compared the ATP’s per-
formance as a hierarchical clustering algorithm to
other competing methods, and demonstrated that our
method can more accurately capture the ground truth
semantic distance between the different categories.
Furthermore, we demonstrated that our method has
weak sensitivity to the choice of hyper-parameters.

Acknowledgments

We acknowledge the support of the NSF Grant CPS-
0931474.

27



References
[Adams et al.2010] R. P. Adams, Z. Ghahramani, and

M. I. Jordan. 2010. Tree-Structured Stick Breaking
for Hierarchical Data. NIPS.

[Bart et al.2011] E. Bart, and M. Welling, and P. Perona.
2011. Unsupervised organization of Image Collec-
tions: Taxonomies and Beyond. IEEE Tran. on PAMI,
33(11):2302–2315.

[Berg et al.2011] T. L. Berg, A. C. Berg and J. Shih. 2010.
Automatic Attribute Discovery and Characterization
from Noisy Web Data. CVPR.

[Binder et al.2012] A. Binder, K. R. Muller, and
M. Kawanabe. 2012. On Taxonomies for Multi-class
Image Categorization. International Journal of
Computer Vision, 99:281–301.

[Fan et al.2008] R. Fan, K. Chang, C. Hsieh, X. Wang,
and C. Lin. 2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine Learning
Research, 9:1871–1874.

[Farhadi et al.2009] A. Farhadi, I. Endres, D. Hoiem, and
David Forsyth. 2009. Describing objects by their at-
tributes. CVPR.

[Ferrari & Zisserman2007] V. Ferrari and A. Zisserman.
2010. Learning Visual Attributes. CVPR.

[Jain & Dubes1988 p. 59] A. K. Jain and R. C. Dubes.
1988. Algorithms for Clustering Data. Prentice-Hall,
Englewood Cliffs, NJ.

[Lampert et al.2009] C. H. Lampert, H. Nickisch, and
S. Harmeling. 2009. Learning to detect unseen ob-
ject classes by between class attribute transfer. CVPR.

[Li et al.2010] L. J. Li, and C. Wang, and Y. Lim, and
D. M. Blei, and L. Fei-Fei. 2010. Building and us-
ing a semantivisual image hierarchy. CVPR.

[Manning et al.2009 p. 357] C. D. Manning, P. Ragha-
van, and H. Schtze. 2009. An Introduc-
tion to information retrieval. Available online at
http://nlp.stanford.edu/IR-book/.

[Mullner] D. Mulner. fastcluster: Fast hierar-
chical clustering routines for R and Python.
http://math.stanford.edu/ muellner/index.html.

[Murtagh1984] F. Murtagh. 1984. Complexities of Hi-
erarchic Clustering Algorithms: the state of the art.
Computational Statistics Quarterly, 1: 101–113.

[Myung et al.2012] M. J. Choi, A. Torralba and
A. S. Willsky. 2012. A Tree-Based Context
Model for Object Recognition. IEEE Tran. on PAMI,
34(2):240–252.

[Neal2000] R. Neal. 2000. Nonparametric factor analysis
with beta process priors. Annals of Statistics, 31:705–
767.

[Paisley & Carin2009] J. W. Paisley, and L. Carin. 2009.
Nonparametric factor analysis with beta process pri-
ors. ICML.

[Parikh & Grauman2011] P. Devi and G. Kristen. 2011.
Interactively building a discriminative vocabulary of
nameable attributes. CVPR.

[Sivic et al.2008] J. Sivic, and B. C. Russel, and A. Zis-
serman, and W. T. Freeman, and A. A. Efros. 2008.
Unsupervised Discovery of visual object class hierar-
chies. CVPR.

[Teh et al.2006] Y. W. Teh and M. I. Jordan and M. J. Beal
and D. M. Blei. 2006. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

[Thibaux & Jordan2007] R. Thibaux and M. I. Jordan.
2007. Hierarchical Beta Processes and the Indian Buf-
fet Process. AISTATS.

28


