










































Distributions on Minimalist Grammar Derivations


Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 1–11,
Sofia, Bulgaria, August 9, 2013. c©2013 Association for Computational Linguistics

Distributions on Minimalist Grammar Derivations

Tim Hunter
Department of Linguistics

Cornell University
159 Central Ave., Ithaca, NY, 14853
tim.hunter@cornell.edu

Chris Dyer
School of Computer Science
Carnegie Mellon University

5000 Forbes Ave., Pittsburgh, PA, 15213
cdyer@cs.cmu.edu

Abstract

We present three ways of inducing proba-
bility distributions on derivation trees pro-
duced by Minimalist Grammars, and give
their maximum likelihood estimators. We
argue that a parameterization based on lo-
cally normalized log-linear models bal-
ances competing requirements for mod-
eling expressiveness and computational
tractability.

1 Introduction

Grammars that define not just sets of trees or
strings but probability distributions over these ob-
jects have many uses both in natural language pro-
cessing and in psycholinguistic models of such
tasks as sentence processing and grammar ac-
quisition. Minimalist Grammars (MGs) (Stabler,
1997) provide a computationally explicit formal-
ism that incorporates the basic elements of one
of the most common modern frameworks adopted
by theoretical syntacticians, but these grammars
have not often been put to use in probabilistic set-
tings. In the few cases where they have (e.g. Hale
(2006)), distributions over MG derivations have
been over-parametrized in a manner that follows
straightforwardly from a conceptualization of the
derivation trees as those generated by a particu-
lar context-free grammar, but which does not re-
spect the characteristic perspective of the under-
lying MG derivation. We propose an alternative
approach with a smaller number of parameters that
are straightforwardly interpretable in terms that re-
late to the theoretical primitives of the MG formal-
ism. This improved parametrization opens up new
possibilities for probabilistically-based empirical
evaluation of MGs as a cognitive hypothesis about
the discrete primitives of natural language gram-
mars, and for the use of MGs in applied natural
language processing.

In Section 2 we present MGs and their equiv-
alence to MCFGs, which provides a context-
free characterization of MG derivation trees. We
demonstrate the problems with the straightforward
method of supplementing a MG with probabili-
ties that this equivalence permits in Section 3, and
then introduce our proposed reparametrization that
solves these problems in Section 4. Section 5 con-
cludes and outlines some suggestions for future re-
lated work.

2 Minimalist Grammars and Multiple
Context-Free Grammars

2.1 Minimalist Grammars

A Minimalist Grammar (MG) (Sta-
bler and Keenan, 2003)1 is a five-tuple
G = 〈Σ,Sel ,Lic,Lex ,c〉 where:

• Σ is a finite alphabet
• Sel (“selecting types”) and Lic (“licensing

types”) are disjoint finite sets which together
determine the set Syn (“syntactic features”),
which is the union of the following four sets:

selectors = {=f | f ∈ Sel}
selectees = { f | f ∈ Sel}
licensors = {+f | f ∈ Lic}
licensees = {-f | f ∈ Lic}

• Lex (“the lexicon”) is a finite subset of
Σ∗ × (selectors ∪ licensors)∗ × selectees ×
licensees∗

• c ∈ Sel is a designated type of completed ex-
pressions

(A sample lexicon is shown in Fig. 3 below.)

1We restrict attention here to MGs without head move-
ment as presented by Stabler and Keenan (2003). Weak gen-
erative capacity is unaffected by this choice (Stabler, 2001).

1



Given an MG G, an expression is an ordered
binary tree with non-leaf nodes labeled by an ele-
ment of {<,>}, and with leaf nodes labeled by an
element of Σ∗ × Syn∗. We take elements of Lex
to be one-node trees, hence expressions. We often
write elements of Σ∗ × Syn∗ with the two com-
ponents separated by a colon (e.g. arrive : +d v).
Each application of one of the derivational opera-
tions MERGE and MOVE, defined below, “checks”
or deletes syntactic features on the expression(s)
to which it applies.

The head of a one-node expression is the ex-
pression’s single node; the head of an expression
[< e1 e2] is the head of e1; the head of an expres-
sion [> e1 e2] is the head of e2. An expression is
complete iff the only syntactic feature on its head
is a selectee feature c and there are no syntactic
features on any of its other nodes. Given an ex-
pression e, yield(e) ∈ Σ∗ is result of concatenat-
ing the leaves of e in order, discarding all syntactic
features.

CL(G) is the set of expressions generated
by taking the closure of Lex under the func-
tions MERGE and MOVE, defined in Fig. 1;
intuitive graphical illustrations are given in
Fig 2. The language generated by G is {s | ∃e ∈
CL(G) such that e is complete and yield(e) = s}.

An example derivation, using the grammar in
Fig. 3, is shown in Fig. 4. This shows both the
“history” of derivational operations — although
operations are not shown explicitly, all binary-
branching nodes correspond to applications of
MERGE and all unary-branching nodes to MOVE
— and the expression that results from each op-
eration. Writing instead only MERGE or MOVE
at each internal node would suffice to determine
the eventual derived expression, since these op-
erations are functions. A derivation tree is a
tree that uses this less redundant labeling: more
precisely, a derivation tree is either (i) a lexi-
cal item, or (ii) a tree [MERGE τ1 τ2] such that
MERGE(eval(τ1), eval(τ2)) is defined, or (iii) a
tree [MOVE τ ] such that MOVE(eval(τ)) is defined;
where eval is the “interpretation” function that
maps a derivation tree to an expression in the ob-
vious way. We define Ω(G) to be the set of all
derivation trees using the MG G.

An important property of the definition of
MOVE is that it is only defined on τ [+fα] if there
is a unique subtree of this tree whose (head’s) first
feature is -f . From this it follows that in any

pierre : d who : d -wh
marie : d will : =v =d t
praise : =d v � : =t c
often : =v v � : =t +wh c

Figure 3: A Minimalist Grammar lexicon. The
type of completed expressions is c.

>

who : <

� : c >

marie : <

will : <

praise : � :

<

� : +wh c >

marie : <

will : <

praise : who : -wh

� : =t +wh c

>

marie : <

will : t <

praise : who : -wh

<

will : =d t <

praise : who : -wh

will : =v =d t
<

praise : v who : -wh

praise : =d v who : d -wh

marie : d

Figure 4: An MG derivation of an embedded ques-
tion

2



MERGE
(
e1[=f α], e2[f β]

)
=

{
[< e1[α] e2[β]] if e1[=f α] ∈ Lex
[> e2[β] e1[α]] otherwise

MOVE
(
e1[+f α]

)
= [> e2[β] e

′
1[α]]

where e2[-f β] is a unique subtree of e1[+f α]

and e′1 is like e1 but with e2[-f β] replaced by an empty leaf node � : �

Figure 1: Definitions of MG operations MERGE and MOVE. The first case of MERGE creates comple-
ments, the second specifiers. f ranges over Sel ∪ Lic; α and β range over Syn∗; and e[α] is an MG
expression whose head bears the feature-sequence α.

=f α

f β

MERGE

β

α

<

=f α f β

MERGE

β α

>

+f α

-f β

MOVE αβ

>

Figure 2: Graphical illustrations of definitions of MERGE and MOVE. Rectangles represent single-node
trees. Triangles represent either single-node trees or complex trees, but the second case of MERGE applies
only when the first case does not (i.e. when the =f α tree is complex).

3



derivation of a complete expression, every inter-
mediate derived expression will have at most |Lic|
subtrees whose (head’s) first feature is of the form
-g for any g ∈ Lic.

2.2 Multiple Context-Free Grammars
Multiple Context-Free Grammars (MCFGs) (Seki
et al., 1991; Kallmeyer, 2010) are a mildly
context-sensitive grammar formalism in the sense
of Joshi (1985).2 They bring additional expressive
capacity over context-free grammars (CFGs) by
generalizing to allow nonterminals to categorize
not just single strings, but tuples of strings. For
example, while a CFG might categorize eats cake
as a VP and the boy as an NP, an MCFG could
categorize the tuple 〈says is tall,which girl〉 as a
VPWH (intuitively, a VP containing a WH which
will move out of it). Correspondingly, MCFG
production rules (construed as recipes for build-
ing expressions bottom-up) can specify not only,
for example, how to combine a string which is an
NP and a string which is a VP, but also how to
combine a string which is an NP with a tuple of
strings which is a VPWH. The CFG rule which
would usually be written ‘S → NP VP’ is shown
in (1) in a format that makes explicit the string-
concatenation operation; (2) uses this notation to
express an MCFG rule that combines an NP with
a VPWH to form a string of category Q, an em-
bedded question. (We often omit angle brackets
around one-tuples.) An example application of
this rule is shown in (3).

st :: S ⇒ s :: NP t :: VP (1)
t2st1 :: Q ⇒ s :: NP 〈t1, t2〉 :: VPWH (2)

which girl the boy says is tall :: Q ⇒
the boy :: NP 〈says is tall,which girl〉 :: VPWH

(3)

Every nonterminal in an MCFG derives (only) n-
tuples of strings, for some n known as the non-
terminal’s rank. In the examples above NP, VP,
S and Q are of rank 1, and VPWH is of rank 2.
A CFG is an MCFG where every nonterminal has
rank 1.

Michaelis (2001) showed that it is possible to
reformulate MGs in a way that uses categorized

2MCFGs are almost identical to Linear Context-Free
Rewrite Systems (Vijay-Shanker et al., 1987). Seki et al.
(1991) show that the two formalisms are weakly equivalent.

string-tuples, of the sort that MCFGs manipulate,
as derived structures (or expressions) instead of
trees. The “purpose” of the internal tree structure
that we assign to derived objects is, in effect, to
allow a future application of MOVE to break them
apart and rearrange their pieces, as illustrated in
Fig. 2. But since the placement of the syntactic
features on a tree determines the parts that will be
rearranged by a future application of MOVE (in any
derivation of a complete expression), we lose no
relevant information by splitting up a tree’s yield
into the components that will be rearranged and
then ignoring all other internal structure. Thus the
following tree:

+f α

-f β -g γ (4)

becomes a tuple of categorized strings (we will ex-
plain the 0 subscript shortly):〈

s : +f α , t : -f β , u : -g γ
〉

0

or, equivalently, a tuple of strings, categorized by
a tuple-of-categories:

〈s, t, u〉 :: 〈+f α,-f β,-g γ〉0 (5)

The order of the components is irrelevant except
for the first component, which contains the entire
structure’s head node; intuitively, this is the com-
ponent out of which the others move.

Based on this idea, Michaelis (2001) shows
how to construct, for any MG, a correspond-
ing MCFG whose nonterminals are tuples like
〈+f α,-f β,-g γ〉0 from above. The uniqueness
requirement in the definition of MOVE ensures that
we need only a finite number of such nontermi-
nals. The feature sequences that comprise the
MCFG nonterminals, in combination with the MG
operations, determine the MCFG production rules
in which each MCFG nonterminal appears. For
example, the arrangement of features on the tree
in (4) dictates that MOVE is the only MG opera-
tion that can apply to it; thus the internals of the
complex category in (5) correspondingly dictate
that the only MCFG production that takes (5) as
“input” (again, thinking right-to-left or bottom-up
as in (1) and (2)) is one that transforms it in ac-
cord with the effects of MOVE. If β = �, then this

4



effect will be to transform the three-tuple into a
two-tuple as shown in (6), since the t-component
now has no remaining features and has therefore
reached its final position:

〈ts, u〉 :: 〈α,-g γ〉0 ⇒
〈s, t, u〉 :: 〈+f α,-f,-g γ〉0 (6)

This is analogous — modulo the presence of the
additional u : -g γ component — to the rule that
is used in the final step of the derivation in Fig. 5,
which is the MCFG equivalent of Fig. 4.

If, on the other hand, β 6= �, then the t-
component will need to move again later in the
derivation, and so we keep it as a separated com-
ponent:

〈s, t, u〉 :: 〈α, β,-g γ〉0
⇒ 〈s, t, u〉 :: 〈+f α,-f β,-g γ〉0 (7)

The subscript 0 on the tuples above indi-
cates that the corresponding expressions are non-
lexical; for lexical expressions, the subscript is 1.
This information is not relevant to MOVE oper-
ations, but is crucial for distinguishing between
the complement and specifier cases of MERGE.
For example, in the simplest cases where no to-
be-moved subconstituents are present, the con-
structed MCFG must contain two rules corre-
sponding to MERGE as follows. (n matches either
1 or 0.)

st :: 〈α〉0 ⇒ s :: 〈=f α〉1 t :: 〈f〉n (8)
ts :: 〈α〉0 ⇒ s :: 〈=f α〉0 t :: 〈f〉n (9)

By similar logic, it is possible to construct
all the necessary MCFG rules corresponding to
MERGE and MOVE; see, for example, Stabler and
Keenan (2003, p.347) for (a presentation of the
MG operations that can also be straightforwardly
be read as) the general schemas that generate these
rules. One straightforward lexical/preterminal rule
is added for each lexical item in the MG, and
the MCFG’s start symbol is 〈c〉0.3 The resulting
MCFG is weakly equivalent to the original MG,
and strongly equivalent in the sense that one can
straightforwardly convert back and forth between
the two grammars’ derivation trees. The MCFG
equivalent of the MG in Fig. 3 is shown in Fig. 6
(ignoring the weights for now, which we come to
below).4

3We exclude 〈c〉1 on the simplifying assumption that the

who marie will praise :: 〈c〉0

〈marie will praise,who〉 :: 〈+wh c,-wh〉0

� :: 〈=t +wh c〉1 〈marie will praise,who〉 :: 〈t,-wh〉0

〈will praise,who〉 :: 〈=d t,-wh〉0

will :: 〈=v =d t〉1 〈praise,who〉 :: 〈v,-wh〉0

praise :: 〈=d v〉1 who :: 〈d -wh〉1

marie :: 〈d〉1

Figure 5: The MG derivation from Fig. 4 illus-
trated with tuples of strings instead of trees as the
derived structures.

Notation. We define the above conversion pro-
cess to be an (invertible) function π from MGs to
MCFGs. That is, for an valid MG, G it holds that
π(G) is an equivalent MCFG and π−1(π(G)) =
G. By abuse of notation, we will use π as the func-
tion for converting from MG derivation trees to
equivalent MCFG derivation trees. By an MCFG
derivation tree we mean a tree like Fig. 5 but with
non-leaf nodes labelled only by nonterminals (not
tuples of strings). The derivation tree language of
an MCFG is thus a local tree language, just as for
a CFG; that of an MG is non-local but regular (Ko-
bele et al., 2007).

3 Distributions on Derivations

Assume a Minimalist Grammar, G. In this sec-
tion and the next, we will consider various ways of
defining probability distributions on the derivation
trees in Ω(G).5 The first approach, introduced in
Section 3.2, is conceptually straightforward but is
problematic in certain respects that we discuss in
Section 3.3. We present a different approach that
resolves these problems in Section 4.

We also consider the problem of estimating the
parameters of these distributions from a finite sam-
ple of training data, specified by a function f̃ :
Ω(G) → N, where f̃(τ) is the number of times
derivation τ occurs in the sample. To this end, it

MG has no lexical item whose only feature is the selectee c.
4This MCFG includes only the rules that are “reachable”

from the lexical items. For example, we leave aside rules
involving the nonterminal 〈=c v -wh〉0, even though the
schemas in Stabler and Keenan (2003) generate them.

5We use the terms derivation tree and derivation inter-
changeably.

5



θERF

2/2 � :: 〈=t +wh c〉1
95/95 � :: 〈=t c〉1
97/97 will :: 〈=v =d t〉1
6/6 often :: 〈=v v〉1

97/97 praise :: 〈=d v〉1
95/192 marie :: 〈d〉1
97/192 pierre :: 〈d〉1
2/2 who :: 〈d -wh〉1

θERF

2/2 〈st, u〉 :: 〈+wh c,-wh〉0 ⇒ s :: 〈=t +wh c〉1 〈t, u〉 :: 〈t,-wh〉0
95/95 st :: 〈=d t〉0 ⇒ s :: 〈=v =d t〉1 t :: 〈v〉0
2/2 〈st, u〉 :: 〈=d t,-wh〉0 ⇒ s :: 〈=v =d t〉1 〈t, u〉 :: 〈v,-wh〉0
2/97 ts :: 〈c〉0 ⇒ 〈s, t〉 :: 〈+wh c,-wh〉0

95/97 st :: 〈c〉0 ⇒ s :: 〈=t c〉1 t :: 〈t〉0
95/95 ts :: 〈t〉0 ⇒ s :: 〈=d t〉0 t :: 〈d〉1
2/2 〈ts, u〉 :: 〈t,-wh〉0 ⇒ 〈s, u〉 :: 〈=d t,-wh〉0 t :: 〈d〉1

95/100 st :: 〈v〉0 ⇒ s :: 〈=d v〉1 t :: 〈d〉1
5/100 st :: 〈v〉0 ⇒ s :: 〈=v v〉1 t :: 〈v〉0
2/3 〈s, t〉 :: 〈v,-wh〉0 ⇒ s :: 〈=d v〉1 t :: 〈d -wh〉1
1/3 〈st, u〉 :: 〈v,-wh〉0 ⇒ s :: 〈=v v〉1 〈t, u〉 :: 〈v,-wh〉0

Figure 6: The MCFG produced from the MG in Fig. 3, as described in Section 2.2; with weights com-
puted by relative frequency estimation based on the naive parametrization, as described in Section 3.

will be useful to define the empirical distribution
on derivations to be p̃(τ) = f̃(τ)/

∑
τ ′ f̃(τ

′).

3.1 Stochastic MCFGs
As with CFGs, it is straightforward to imbue
an MCFG, H , with production probabilities and
thereby create a stochastic MCFG.6 In stochas-
tic MCFGs (as in CFGs) the probability of a non-
terminal rewrite in a derivation is conditionally in-
dependent of all other rewrite decisions, given the
non-terminal type. This formulation defines a dis-
tribution over MCFG derivations in terms of a ran-
dom branching process that begins with probabil-
ity 1 at the start symbol and recursively expands
frontier nodes N , drawing branching decisions
from the the conditional distribution p(· | N); the
process terminates when lexical items have been
produced on all frontiers.

If p(δ | N) is the probability that N rewrites as
δ and fτ (N ⇒ δ) is the number of times N ⇒ δ
occurs in derivation tree τ , then

p(τ) =
∏

(N⇒δ)∈H

p(δ | N)fτ (N⇒δ). (10)

With mild assumptions to ensure consistency (Chi,
1999), the p(τ)’s form a proper probability distri-
bution over all derivations in H .7

Because the derivation trees of the MG G stand
in a bijection with the derivation trees of the
MCFG π(G), stochastic MCFGs can be used to
define a distribution on MG derivations.

6Although MCFGs have a greater generative capacity
than CFGs, the statistical properties do not change at all, un-
less otherwise noted.

7The estimators that are based on empirical frequencies
in a derivation bank which we use in this paper will always
yield consistent estimates. Refer Chi (1999) for more detail.

3.2 The naive parametrization
The most straightforward way to parameterize a
stochastic MCFG uses individual parameters θδ|N
to represent each production probability, i.e., p(δ |
N)

.
= θδ|N . When applied to an MCFG that is

derived from an MG, we will refer to this as the
naive parametrization.

This is the parametrization used by Hale (2006)
to define a probability distribution over the deriva-
tions of MGs in order to explore the predictions
of an information-theoretic hypothesis concerning
sentence comprehension difficulty.

MLE. The arguably most standard technique for
setting the parameters of a probability distribution
is so that they maximize the likelihood of a sam-
ple of training data. In the naive parameterization,
the maximum likelihood estimate (MLE) for each
parameter θ̂ERFδ|N is the empirical relative frequency
of the rewrite N ⇒ δ in the training data (Abney,
1997):

θ̂ERFδ|N =

∑
τ f̃(τ)fπ(τ)(N ⇒ δ)∑

τ f̃(τ)
∑

(N⇒δ′)∈π(G) fπ(τ)(N ⇒ δ′)
.

3.3 Unfaithfulness to MGs
While the naive parameterization with MLE esti-
mation is simple, it is arguably a poor choice for
parameterizing distributions on MGs. The prob-
lem is that, relative to the independence assump-
tions encoded in the MG formalism, each step of
the MCFG derivation both conditions on and pre-
dicts “too much” structure. As a result, common-
alities across different applications of the same
MG operation are modeled independently and do
not share statistical strength. This arises because

6



90 pierre will praise marie
5 pierre will often praise marie
1 who pierre will praise
1 who pierre will often praise

Figure 7: An artificial corpus of sentences deriv-
able from the grammars in Figures 3 and 6.

of the way the MCFG’s nonterminals multiply out
all relevant arrangements of features.8 We illus-
trate the problem with an example.

Consider the corpus in Fig. 7, where each sen-
tence is preceded by its frequency. Since each sen-
tence is assigned a unique derivation by our exam-
ple MG, this is equivalent to a treebank.

One reasonable statistical interpretation of the
first two lines is that a verb phrase comprises a
verb and an object 95% of the time, and comprises
the adverb often and another verb phrase 5% of
the time (since pierre will often praise marie has
two nested verb phrase constituents). The last two
lines provide an analogous pair of sentences in-
volving wh-movement of the object. A priori, one
would expect that the 95:5 relative frequency that
describes the presence of the adverb also applies
here; however, the ERF estimator will use 2:1 in-
stead. Why is this? The VP category in the MCFG
is “split” into two to indicate whether it has a wh-
feature inside it, and each has its own parameters.
We criticize this on the grounds that it is not in line
with our main goal of defining a distribution over
the derivations of the MG: from the perspective of
the MG, there is a sense in which it is “the same
instance” of MERGE that combines often with a
verb phrase, whether or not the verb phrase’s ob-
ject bears a -wh feature. In other words, the differ-
ences between the following two trees seem unre-
lated to the way in which they are both candidates
to be merged with often : =v v.

<

praise : v who : -wh

<

praise : v marie :

From the perspective of the MCFG, however, the
introduction of the adverb is mediated by expan-
sions of the nonterminal 〈v〉0 in cases without
object wh-movement, but by expansions of the
distinct nonterminal 〈v,-wh〉0 in cases with it.
Therefore the information about adverb inclusion
that is conveyed by the movement-free entries in

8Stabler (forthcoming) also discusses the sense in which
MCFG rules “miss generalizations” found in MGs.

the corpus is interpreted as only relevant to simi-
larly movement-free derivations. This can be seen
in the weights of the last four rules in Fig. 6, which
were computed by relative frequency estimation
on the basis of the corpus.

Relative to the underlying MG, the naive
parametrization has too many degrees of freedom:
the model is overparameterized and is capable of
capturing statistical distinctions that we have the-
oretical reasons to dislike. Of course, it is possi-
ble that VPs have meaningfully different distribu-
tions depending on whether or not they contain a
wh-feature; however, we would like a parameter-
ization that provides the flexibility to treat these
two different contexts as identical, as different, or
to share statistical strength between them in some
other way. In the next section we propose two
alternative parametrizations that provide this con-
trol.

4 Log-linear MCFGs

4.1 Globally normalized log-linear models
An alternative mechanism for inducing a distribu-
tion on Ω(G) that provides more control over in-
dependence assumptions is the globally normal-
ized log-linear model (also called a Markov ran-
dom field, undirected model, or Gibbs distribu-
tion). Unlike the model in the previous section,
log-linear models are not stochastic in nature—
they assign probabilities to structured objects, but
they do not rely on a random branching process
to do so. Rather, they use a d-dimensional vector
of feature functions Φ = 〈Φ1,Φ2, . . . ,Φd〉, where
Φi : Ω(G) → R, to extract features of the deriva-
tion, and a real-valued weight vector λ ∈ Rd.9
Together, Φ and λ define the score of a derivation
τ as a monotonic function of the weighted sum of
the feature values Φ1(τ), . . . ,Φd(τ):

sλ(τ) = exp(λ ·Φ(τ)).

Using this function, a Gibbs distribution on the
derivations in Ω(G) is

pλ(τ) =
sλ(τ)∑

τ ′∈Ω(G) sλ(τ
′)
, (11)

9The term feature here refers to functions of a derivation;
it should not be confused with the syntactic features dis-
cussed immediately above. However, in as much as syntactic
features characterize the steps in a derivation, it is natural that
they would play a central role in defining distributions over
derivations, and indeed, our proposed feature functions ex-
amine syntactic features almost exclusively.

7



provided that the sum in the denominator is fi-
nite.10

Notice that (11) is similar to the formula for
a relative frequency, the difference being that we
use a derivation’s score sλ(τ) rather than its em-
pirical count. This use of scores provides a way
to express the kind of “missed similarities” we
discussed in Section 3.3 via the choice of feature
functions. Returning to the example from above,
in order to express the similarity between the two
adverb-introducing rules — one involving the non-
terminal 〈v〉0, the other involving 〈v,-wh〉0 —
we could define a particular feature function Φi
that maps a derivation to 1 if it contains either one
of these rules and 0 otherwise. Then, all else be-
ing equal, setting the corresponding parameter λi
to a higher value will increase the score sλ(τ),
and hence the probability pλ(τ), of any derivation
τ that introduces an adverb, with or without wh-
movement of the object.

MLE. As with the naive parameterization, the
the parameters λmay be set to maximize the (log)
likelihood of the training data, i.e.,

λ̂ = arg max
λ

n∏
i=1

pλ(τi)
f̃(τi)

= arg max
λ

n∑
i=1

f̃(τi) log pλ(τi)︸ ︷︷ ︸
=L [log likelihood]

. (12)

We remark that maximizing the log likelihood
of data in this parameterization is equivalent to
finding the distribution pλ(τ) in which the ex-
pected value of Φ(τ) is equal to the expected
value of the same under the empirical distribution
(i.e., under p̃(τ)) and whose entropy is maximized
(Della Pietra et al., 1997). This equivalence is par-
ticularly clear when the gradient of L (see (12))
with respect to λ is examined:

∇λL = Ep̃(τ)[Φ(τ)]− Epλ(τ)[Φ(τ)]. (13)

This form makes clear thatL achieves an optimum
when the expectations of Φ match under the two
distributions.11

10There are several conditions under which this is true. It
is trivially true if |Ω(G)| < ∞. When Ω is infinite, the de-
nominator may still be finite if features functions grow (su-
per) linearly with the derivation size in the limiting case as
the size tends to infinity. Then, if feature weights are nega-
tive, the denominator will either be equal to or bounded from
above by an infinite geometric series with a finite sum. Refer
to Goodman (1999) and references therein.

11While the maximizing point cannot generally be solved

4.2 Feature locality

Notice that the approach just outlined is extremely
general: the feature functions Φ can examine the
derivation trees as a whole. It is possible to define
features that pay attention to arbitrary or global
properties of a derivation. While such features
might in fact generalize well to new data — for ex-
ample, one could mimic a bigram language model
by including features counting bigrams in the
string that is generated by the derivation — these
are intuitively “bad” since they ignore the deriva-
tion’s structure. Furthermore, there is a substantial
practical downside to allowing unrestricted feature
definitions: features that do not “agree” with the
derivation structure make inference computation-
ally intractable. Specifically, finding the best most
probable derivation of a sentence with “global”
features is NP-hard (Koller and Friedman, 2009).

For these reasons, it is advantageous to require
that Φ decompose additively in terms of local fea-
ture functions, ϕ over the steps that make up a
derivation. For defining distributions under an MG
G, we will assume that feature functions decom-
pose over the productions in a derivation under the
MCFG projection π(G), i.e.,

Φ(τ) =
∑

(N⇒δ)∈π(τ)

ϕ (N ⇒ δ) .

Under the locality assumption, we may rewrite the
score sλ(τ) as∏

(N⇒δ)∈π(G)

(exp(λ ·ϕ(N ⇒ δ)))fπ(τ)(N⇒δ) .

This (partially) addresses the issue of computa-
tional tractability, enforces our intuition that the
score of a derivation tree should be a function of
scores of its component steps, and still gives us the
ability to avoid the overconditioning that we iden-
tified in Section 3.3.12

4.3 Locally normalized log-linear models

Even with our assumption of feature locality, find-
ing λ̂ remains challenging since the second term

for analytically, gradient based optimization techniques may
be effectively used to find it (and it is both guaranteed to exist
and guaranteed to be unique).

12We say that the issue of computational tractability is only
partially resolved because only certain operations — identi-
fying the most probable derivation of a string — are truly ef-
ficient. Computing the model’s normalization function, while
no longer NP-hard, still not practical.

8



in (13) is difficult to compute.13 In this section we
suggest a parameterization that admits both effi-
cient ML estimation and retains the ability to use
feature functions to control the distribution.

To do so, we revisit the approach of defining
distributions on derivations in terms of a stochas-
tic process from Section 3.1, but rather than defin-
ing the branching distributions with independent
parameters for each MCFG nonterminal rewrite
type, we parameterize it in terms of locally nor-
malized log-linear models, also called a condi-
tional logit model (Murphy, 2012). Given an MG
G, a weight vector w ∈ Rd, and rule-local feature
functions ϕ as defined above,14 let the branching
probability

pw(δ | N)
.
=

exp(w ·ϕ(N ⇒ δ))∑
(N⇒δ′)∈π(G) exp(w ·ϕ(N ⇒ δ′))

.

Like the parametrization in Section 4.1, this
new parametrization is based on log-linear mod-
els and therefore allows us to express similarities
among derivational operations via choices of fea-
ture functions. However, rather than defining fea-
ture functions Φi on entire derivations, these fea-
tures can only “see” individual MCFG rules. Put
differently, the same technique we used in Sec-
tion 4.1 to define a probability distribution over the
entire set of derivations, is used here to define each
of the local conditional probability distributions
over the expansions of a single MCFG nontermi-
nal. Via the perspective familiar from stochastic
MCFGs, these individual conditional probability
distributions together define a distribution on the
entire set of derivations.

MLE. As with the previous two models, we can
set parametersw to maximize the likelihood of the
training data. Here, the global likelihood is ex-
pressed in terms of the probabilities of condition-
ally independent rewrite events, each defined in a
log-linear model:

Lc =
∑
τ

f̃(τ)
∑

(N⇒δ)∈π(τ)

fπ(τ)(N ⇒ δ) log pw(δ | N).

13Specifically, it requires computing expectations under all
possible derivations in Ω(π(G)) during each step of gradient
ascent, which requires polynomial space/time in the size of
the lexicon to compute exactly.

14The notational shift fromλ tow to emphasizes that these
two parameter vectors have very different semantics. The
former parameterizes potential functions in a globally nor-
malized random field while the later is used to determine a
family of conditional probability distributions used to define
a stochastic process.

Its gradient with respect to w is therefore

∇wLc =
∑
τ

f̃(τ)
∑

(N⇒δ)∈π(τ)

fπ(τ)(N ⇒ δ)
[

ϕ(N ⇒ δ)− Epw(δ′|N)ϕ(N ⇒ δ
′)
]
.

As with the globally normalized model, ∇wLc =
0 has no closed form solution; however, gradient-
based optimization is likewise effective. How-
ever, unlike (13), this gradient is straightforward to
compute since it requires summing only over the
different rewrites of each non-terminal category
during each iteration of gradient ascent, rather
than over all possible derivations in Ω(G)!

4.4 Example parameter estimation

In this section we compare the probability esti-
mates for productions in a stochastic MCFGs ob-
tained using the naive parameterization discussed
in Section 3.2 that conditions on “too much” infor-
mation and those obtained using locally normal-
ized log-linear models with grammar-appropriate
feature functions. Our very simple feature set con-
sists just of binary-valued feature functions that in-
dicate:

• whether a MERGE step, MOVE step, or a termi-
nating lexical-insertion step is being generated;

• what selector feature (in the case of MERGE
steps) or licensor feature (in the case of MOVE
steps) is being checked (e.g., +wh or =d or =v);
and

• what lexical item is used (e.g., marie : d or
� : =t c), in the case of terminating lexical-
insertion steps.

Table 1 shows the values of some of these features
for a sample of the MCFG rules in Fig. 6.

Table 2 compares the production probabilities
estimated for last four rules in Fig. 6 using the
naive empirical frequency method and our recom-
mended log-linear approach with the features de-
fined as above.15 The presence or absence of a
-wh feature does not affect the log-linear model’s
probability of adding an adverb to a verb phrase,
in keeping with the perspective suggested by the
derivational operations of MGs.

15The log-linear parameters were optimized using a stan-
dard quasi-Newtonian method (Liu and Nocedal, 1989).

9



Table 1: Selected feature values for a sample of MCFG rules. The first four rules are the ones that
illustrated the problems with the naive parametrization in Section 3.3.

MCFG Rule ϕMERGE ϕ=d ϕ=v ϕ=t ϕMOVE ϕ+wh
st :: 〈v〉0 ⇒ s :: 〈=d v〉1 t :: 〈d〉1 1 1 0 0 0 0
st :: 〈v〉0 ⇒ s :: 〈=v v〉1 t :: 〈v〉0 1 0 1 0 0 0

〈s, t〉 :: 〈v,-wh〉0 ⇒ s :: 〈=d v〉1 t :: 〈d -wh〉1 1 1 0 0 0 0
〈st, u〉 :: 〈v,-wh〉0 ⇒ s :: 〈=v v〉1 〈t, u〉 :: 〈v,-wh〉0 1 0 1 0 0 0

st :: 〈c〉0 ⇒ s :: 〈=t c〉1 t :: 〈t〉0 1 0 0 1 0 0
ts :: 〈c〉0 ⇒ 〈s, t〉 :: 〈+wh c,-wh〉0 0 0 0 0 1 1

Table 2: Comparison of probability estimators.

MCFG Rule Naive p̂ Log-linear p̂
st :: 〈v〉0 ⇒ s :: 〈=d v〉1 t :: 〈d〉1 0.95 0.94
st :: 〈v〉0 ⇒ s :: 〈=v v〉1 t :: 〈v〉0 0.05 0.06

〈s, t〉 :: 〈v,-wh〉0 ⇒ s :: 〈=d v〉1 t :: 〈d -wh〉1 0.67 0.94
〈st, u〉 :: 〈v,-wh〉0 ⇒ s :: 〈=v v〉1 〈t, u〉 :: 〈v,-wh〉0 0.33 0.06

5 Conclusion and Future Work

We have presented a method for inducing a prob-
ability distribution on the derivations of a Min-
imalist Grammar in a way that remains faithful
to the way the derivations are conceived of in
this formalism, and for obtaining the maximum
likelihood estimate of its parameters. Our pro-
posal takes advantage of the MG-MCFG equiva-
lence in the sense that it uses the underlying prob-
abilistic branching process of a stochastic MCFG,
but avoids the problems of overparametrization
that come with the naive approach that reifies the
MCFG itself.

Our parameterization has several applications
worth noting. It provides a new way to compare
variants of the MG formalism that propose slightly
different sets of primitives (operations, types of
features, etc.) but are equivalent once transformed
into MCFGs. Examples of such variants include
the addition of an ADJOIN operation (Frey and
Gärtner, 2002), or replacing MERGE and MOVE
with a single feature-checking operation (Stabler,
2006; Hunter, 2011). Derivations using these dif-
ferent versions of the formalism often boil down
to the same string-concatenation operations and
will therefore be expressible using equivalent sets
of MCFG rules. The naive parametrization will
therefore not distinguish them, but in the same
way that our proposal above “respects” standard
MGs’ classification of MCFG rules according to

one set of derivational primitives, one could de-
fine feature vectors that respect different classifi-
cations.

Outside of MGs, the strategy is applicable to
any other formalisms whose derivations can be re-
cast as those of MCFGs, such as TAGs and CCGs.
More generally still, it could be applied to any
formalism whose derivation tree languages can be
characterized by a local tree grammar; in our case,
the relevant local tree language is obtained via a
projection from the regular tree language of MG
derivation trees.

Acknowledgments

Thanks to John Hale for helpful discussion and to
the anonymous reviewers for their insightful com-
ments. This work was sponsored by NSF award
number 0741666, and by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533.

References
Steven P. Abney. 1997. Stochastic attribute-value

grammars. Computational Linguistics.

Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics.

Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19(4).

10



Werner Frey and Hans-Martin Gärtner. 2002. On the
treatment of scrambling and adjunction in minimal-
ist grammars. In Gerhard Jäger, Paola Monachesi,
Gerald Penn, and Shuly Wintner, editors, Proceed-
ings of Formal Grammar 2002, pages 41–52.

Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4).

John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30:643–672.

Tim Hunter. 2011. Insertion Minimalist Gram-
mars: Eliminating redundancies between merge and
move. In Makoto Kanazawa, András Kornai, Mar-
cus Kracht, and Hiroyuki Seki, editors, The Mathe-
matics of Language (MOL 12 Proceedings), volume
6878 of LNCS, pages 90–107. Springer, Berlin Hei-
delberg.

Aravind Joshi. 1985. How much context-sensitivity
is necessary for characterizing structural descrip-
tions? In David Dowty, Lauri Karttunen, and
Arnold Zwicky, editors, Natural Language Process-
ing: Theoretical, Computational and Psychological
Perspectives, pages 206–250. Cambridge University
Press, New York.

Laura Kallmeyer. 2010. Parsing Beyond Context-Free
Grammars. Springer-Verlag, Berlin Heidelberg.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-
vati. 2007. An automata theoretic approach to
minimalism. In James Rogers and Stephan Kepser,
editors, Proceedings of the Workshop on Model-
Theoretic Syntax at 10; ESSLLI ’07.

Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503–528.

Jens Michaelis. 2001. Derivational minimalism is
mildly context-sensitive. In Michael Moortgat, ed-
itor, Logical Aspects of Computational Linguistics,
LACL 1998, volume 2014 of LNCS, pages 179–198.
Springer, Berlin Heidelberg.

Kevin P. Murphy. 2012. Machine Learning: A Proba-
bilistic Perspective. MIT Press.

Hiroyuki Seki, Takashi Matsumara, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191–
229.

Edward P. Stabler and Edward L. Keenan. 2003. Struc-
tural similarity within and among languages. Theo-
retical Computer Science, 293:345–363.

Edward P. Stabler. 1997. Derivational minimalism. In
Christian Retoré, editor, Logical Aspects of Compu-
tational Linguistics, volume 1328 of LNCS, pages
68–95. Springer, Berlin Heidelberg.

Edward P. Stabler. 2001. Recognizing head move-
ment. In Philippe de Groote, Glyn Morrill, and
Christian Retoré, editors, Logical Aspects of Com-
putational Linguistics, volume 2099 of LNCS, pages
254–260. Springer, Berlin Heidelberg.

Edward P. Stabler. 2006. Sidewards without copying.
In Shuly Wintner, editor, Proceedings of The 11th
Conference on Formal Grammar, pages 157–170.
CSLI Publications, Stanford, CA.

Edward Stabler. forthcoming. Two models of min-
imalist, incremental syntactic analysis. Topics in
Cognitive Science.

K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proc. 25th Meeting of Assoc. Computational Lin-
guistics, pages 104–111.

11


