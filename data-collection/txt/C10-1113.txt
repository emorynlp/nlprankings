Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1002–1010,

Beijing, August 2010

1002

Metaphor Identiﬁcation Using Verb and Noun Clustering

Ekaterina Shutova, Lin Sun and Anna Korhonen

Computer Laboratory, University of Cambridge

es407,ls418,alk23@cam.ac.uk

Abstract

We present a novel approach to auto-
matic metaphor identiﬁcation in unre-
stricted text. Starting from a small seed set
of manually annotated metaphorical ex-
pressions, the system is capable of har-
vesting a large number of metaphors of
similar syntactic structure from a corpus.
Our method is distinguished from previ-
ous work in that it does not employ any
hand-crafted knowledge, other than the
initial seed set, but, in contrast, captures
metaphoricity by means of verb and noun
clustering. Being the ﬁrst to employ un-
supervised methods for metaphor identiﬁ-
cation, our system operates with the pre-
cision of 0.79.
Introduction

1
Besides enriching our thought and communica-
tion with novel
the phenomenon of
metaphor also plays a crucial structural role in our
use of language. Metaphors arise when one con-
cept is viewed in terms of the properties of the
other. Below are some examples of metaphor.

imagery,

(1) How can I kill a process? (Martin, 1988)

(2) Inﬂation has eaten up all my savings. (Lakoff

and Johnson, 1980)

(3) He shot down all of my arguments. (Lakoff

and Johnson, 1980)

(4) And then my heart with pleasure ﬁlls,

And dances with the daffodils.1

In metaphorical expressions seemingly unrelated
features of one concept are associated with an-
other concept. In the computer science metaphor
1“I wandered lonely as a cloud”, William Wordsworth,

1804.

in (1) the computational process is viewed as
something alive and, therefore, its forced termi-
nation is associated with the act of killing. Lakoff
and Johnson (1980) explain metaphor as a system-
atic association, or a mapping, between two con-
cepts or conceptual domains: the source and the
target. The metaphor in (3) exempliﬁes a map-
ping of a concept of argument to that of war. The
argument, which is the target concept, is viewed
in terms of a battle (or a war), the source concept.
The existence of such a link allows us to talk about
arguments using the war terminology, thus giving
rise to a number of metaphors.

Characteristic to all areas of human activity
(from poetic to ordinary to scientiﬁc) and, thus,
to all types of discourse, metaphor becomes an
important problem for natural language process-
ing (NLP). In order to estimate the frequency of
the phenomenon, Shutova and Teufel (2010) con-
ducted a corpus study on a subset of the British
National Corpus (BNC) (Burnard, 2007) repre-
senting various genres. They manually anno-
tated metaphorical expressions in this data and
found that 241 out of 761 sentences contained a
metaphor, whereby in 164 phrases metaphoricity
was introduced by a verb. Due to such a high fre-
quency of their use, a system capable of recog-
nizing and interpreting metaphorical expressions
in unrestricted text would become an invaluable
component of any semantics-oriented NLP appli-
cation.

Automatic processing of metaphor can be
clearly divided into two subtasks: metaphor
identiﬁcation (distinguishing between literal and
metaphorical
language in text) and metaphor
interpretation (identifying the intended literal
meaning of a metaphorical expression). Both of
them have been repeatedly attempted in NLP.

To date the most

inﬂuential account of
metaphor identiﬁcation is that of Wilks (1978).

1003

According to Wilks, metaphors represent a viola-
tion of selectional restrictions in a given context.
Consider the following example.

(5) My car drinks gasoline. (Wilks, 1978)

(7) Diana and Charles did not succeed in mend-

ing their marriage.

(8) The wheels of Stalin’s regime were well oiled

and already turning.

The verb drink normally takes an animate subject
and a liquid object. Therefore, drink taking a car
as a subject is an anomaly, which may as well in-
dicate metaphorical use of drink.

This approach was automated by Fass (1991)
in his met* system. However, Fass himself in-
dicated a problem with the method:
it detects
any kind of non-literalness or anomaly in lan-
guage (metaphors, metonymies and others), i.e.,
it overgenerates with respect to metaphor. The
techniques met* uses to differentiate between
those are mainly based on hand-coded knowledge,
which implies a number of limitations. In a sim-
ilar manner manually created knowledge in the
form of WordNet (Fellbaum, 1998) is employed
by the system of Krishnakumaran and Zhu (2007),
which essentially differentiates between highly
lexicalized metaphors included in WordNet, and
novel metaphorical senses.

Alternative approaches (Gedigan et al., 2006)
search for metaphors of a speciﬁc domain deﬁned
a priori (e.g. MOTION metaphors) in a speciﬁc
type of discourse (e.g. Wall Street Journal).
In
contrast, the scope of our experiments is the whole
of the British National Corpus (BNC) (Burnard,
2007) and the domain of the expressions we iden-
tify is unrestricted. However, our technique is also
distinguished from the systems of Fass (1991) and
Krishnakumaran and Zhu (2007) in that it does
not rely on any hand-crafted knowledge, but rather
captures metaphoricity in an unsupervised way by
means of verb and noun clustering.

The motivation behind the use of clustering
methods for metaphor identiﬁcation task lies in
the nature of metaphorical reasoning based on as-
sociation. Compare, for example, the target con-
cepts of marriage and political regime. Having
quite distinct meanings, both of them are cogni-
tively mapped to the source domain of mecha-
nism, which shows itself in the following exam-
ples:

(6) Our relationship is not really working.

We expect that such relatedness of distinct tar-
get concepts should manifest itself in the exam-
ples of language use, i.e. target concepts that are
associated with the same source concept should
appear in similar lexico-syntactic environments.
Thus, clustering concepts using grammatical rela-
tions (GRs) and lexical features would allow us to
capture their relatedness by association and har-
vest a large number of metaphorical expressions
beyond our seed set. For example, the sentence
in (6) being part of the seed set should enable the
system to identify metaphors in both (7) and (8).
In summary, our system (1) starts from a seed
set of metaphorical expressions exemplifying a
range of source–target domain mappings; (2) per-
forms unsupervised noun clustering in order to
harvest various target concepts associated with the
same source domain; (3) by means of unsuper-
vised verb clustering creates a source domain verb
lexicon; (4) searches the BNC for metaphorical
expressions describing the target domain concepts
using the verbs from the source domain lexicon.

We tested our system starting with a collection
of metaphorical expressions representing verb-
subject and verb-object constructions, where the
verb is used metaphorically. We evaluated the pre-
cision of metaphor identiﬁcation with the help of
human judges. In addition to this we compared
our system to a baseline built upon WordNet,
whereby we demonstrated that our method goes
far beyond synonymy and captures metaphors not
directly related to any of those seen in the seed set.

2 Experimental Data
2.1 Seed Phrases
We used the dataset of Shutova (2010) as a seed
set. Shutova (2010) annotated metaphorical ex-
pressions in a subset of the BNC sampling vari-
ous genres: literature, newspaper/journal articles,
essays on politics, international relations and his-
tory, radio broadcast (transcribed speech). The
dataset consists of 62 phrases that are single-word

1004

metaphors representing verb-subject and verb-
object relations, where a verb is used metaphor-
ically. The seed phrases include e.g.
stir ex-
citement, reﬂect enthusiasm, accelerate change,
grasp theory, cast doubt, suppress memory, throw
remark (verb - direct object constructions) and
campaign surged,
tension
mounted, ideology embraces, changes operated,
approach focuses, example illustrates (subject -
verb constructions).

factor shaped [..],

2.2 Corpus
The search space for metaphor identiﬁcation was
the British National Corpus (BNC) that was
parsed using the RASP parser of Briscoe et al.
(2006). We used the grammatical relations out-
put of RASP for BNC created by Andersen et al.
(2008). The system searched the corpus for the
source and target domain vocabulary within a par-
ticular grammatical relation (verb-object or verb-
subject).

3 Method
Starting from a small seed set of metaphorical ex-
pressions, the system implicitly captures the as-
sociations that underly their production and com-
prehension. It generalizes over these associations
by means of unsupervised verb and noun clus-
tering. The obtained clusters then represent po-
tential source and target concepts between which
metaphorical associations hold. The knowledge
of such associations is then used to annotate
metaphoricity in a large corpus.

3.1 Clustering Motivation
Abstract concepts that are associated with the
same source domain are often related to each
other on an intuitive and rather structural level,
but their meanings, however, are not necessarily
synonymous or even semantically close. The re-
sults of previous research on corpus-based lexi-
cal semantics suggest that the linguistic environ-
ment in which a lexical item occurs can shed light
on its meaning. A number of works have shown
that it is possible to automatically induce seman-
tic word classes from corpus data via clustering of
contextual cues (Pereira et al., 1993; Lin, 1998;
Schulte im Walde, 2006). The consensus is that

the lexical items exposing similar behavior in a
large body of text most likely have the same mean-
ing. However, the concepts of marriage and po-
litical regime, that are also observed in similar
lexico-syntactic environments, albeit having quite
distinct meanings are likewise assigned by such
methods to the same cluster. In contrast to con-
crete concepts, such as tea, water, coffee, beer,
drink, liquid, that are clustered together due to
meaning similarity, abstract concepts tend to be
clustered together by association with the same
source domain. It is the presence of this associ-
ation that explains the fact that they share com-
mon contexts. We exploit this idea for identiﬁ-
cation of new target domains associated with the
same source domain. We then use unsupervised
verb clustering to collect source domain vocab-
ulary, which in turn allows us to harvest a large
number of new metaphorical expressions.

3.2 Verb and Noun Clustering
Since Levin (1993) published her classiﬁcation,
there have been a number of attempts to automati-
cally classify verbs into semantic classes using su-
pervised and unsupervised approaches (Lin, 1998;
Brew and Schulte im Walde, 2002; Korhonen et
al., 2003; Schulte im Walde, 2006; Joanis et al.,
2008; Sun and Korhonen, 2009). Similar methods
were also applied to acquisition of noun classes
from corpus data (Rooth et al., 1999; Pantel and
Lin, 2002; Bergsma et al., 2008).

We adopt a recent verb clustering approach of
Sun and Korhonen (2009), who used rich syntac-
tic and semantic features extracted using a shallow
parser and a clustering method suitable for the re-
sulting high dimensional feature space. When Sun
and Korhonen evaluated their approach on 204
verbs from 17 Levin classes, they obtained 80.4
F-measure (which is high in particular for an un-
supervised approach). We apply this approach to a
much larger set of 1610 verbs: all the verb forms
appearing in VerbNet (Kipper et al., 2006) with
the exception of highly infrequent ones. In addi-
tion, we adapt the approach to noun clustering.

3.2.1 Feature Extraction

Our verb dataset is a subset of VerbNet com-
piled as follows. For all the verbs in VerbNet we

1005

extracted their occurrences (up to 10,000) from
the raw corpus data collected originally by Korho-
nen et al. (2006) for construction of VALEX lexi-
con. Only the verbs found in this data more than
150 times were included in the experiment.

For verb clustering, we adopted the best per-
forming features of Sun and Korhonen (2009):
automatically acquired verb subcategorization
frames (SCFs) parameterized by their selectional
preferences (SPs). We obtained these features us-
ing the SCF acquisition system of Preiss et al.
(2007). The system tags and parses corpus data
using the RASP parser and extracts SCFs from the
resulting GRs using a rule-based classiﬁer which
identiﬁes 168 SCF types for English verbs. It pro-
duces a lexical entry for each verb and SCF com-
bination occurring in corpus data. We obtained
SPs by clustering argument heads appearing in the
subject and object slots of verbs in the resulting
lexicon.

Our noun dataset consists of 2000 most fre-
quent nouns in the BNC. Following previous
works on semantic noun classiﬁcation (Pantel and
Lin, 2002; Bergsma et al., 2008), we used GRs as
features for noun clustering. We employed all the
argument heads and verb lemmas appearing in the
subject, direct object and indirect object relations
in the RASP-parsed BNC.

The feature vectors were ﬁrst constructed from
the corpus counts, and subsequently normalized
by the sum of the feature values before applying
clustering.

3.2.2 Clustering Algorithm

We use spectral clustering (SPEC) for both
verbs and nouns. This technique has proved to be
effective in previous verb clustering works (Brew
and Schulte im Walde, 2002; Sun and Korhonen,
2009) and in related NLP tasks involving high di-
mensional data (Chen et al., 2006). We use the
MNCut algorithm for SPEC which has a wide ap-
plicability and a clear probabilistic interpretation
(Meila and Shi, 2001).

The task is to group a given set of words W =
n=1 into a disjoint partition of K classes.
{wn}N
SPEC takes a similarity matrix as input. We
construct it using the Jensen-Shannon divergence
(JSD) as a measure. The JSD between two feature

vectors w and w(cid:2) is djsd(w, w(cid:2)) = 1
2 D(w||m) +
1
2 D(w(cid:2)||m) where D is the Kullback-Leibler di-
vergence, and m is the average of the w and w(cid:2).
The similarity matrix S is constructed where
Sij = exp(−djsd(w, w(cid:2))).
In SPEC, the simi-
larities Sij are viewed as weights on the edges
ij of a graph G over W . The similarity matrix
S is thus the adjacency matrix for G. The de-
j=1 Sij. A cut be-
tween two partitions A and A(cid:2) is deﬁned to be

gree of a vertex i is di = (cid:2)N
Cut(A, A(cid:2)) =(cid:2)m∈A,n∈A(cid:2) Smn.

The similarity matrix S is then transformed into

a stochastic matrix P .

P = D−1S

(1)
The degree matrix D is a diagonal matrix where
Dii = di.

It was shown by Meila and Shi (2001) that if P
has the K leading eigenvectors that are piecewise
constants2 with respect to a partition I∗ and their
eigenvalues are not zero, then I∗ minimizes the
multiway normalized cut (MNCut):

Cut(Ik,Ik)
Cut(Ik,I)

MNCut(I) = K −(cid:2)K

k=1

Pmn can be interpreted as the transition probabil-
ity between the vertexes m, n. The criterion can
thus be expressed as MNCut(I) = (cid:2)K
k=1(1 −
P (Ik → Ik|Ik)) (Meila, 2001), which is the sum
of transition probabilities across different clusters.
This criterion ﬁnds the partition where random
walks are most likely to happen within the same
cluster.
In practice, the leading eigenvectors of
P are not piecewise constants. However, we can
extract the partition by ﬁnding the approximately
equal elements in the eigenvectors using a cluster-
ing algorithm, such as K-Means.

Since SPEC has elements of randomness, we ran
the algorithm multiple times and the partition that
minimizes the distortion (the distances to cluster
centroid) is reported. Some of the clusters ob-
tained as a result of applying the algorithm to our
noun and verb datasets are demonstrated in Fig-
ures 1 and 2 respectively. The noun clusters rep-
resent target concepts that we expect to be asso-
ciated with the same source concept (some sug-
gested source concepts are given in Figure 1, al-
though the system only captures those implicitly).
2An eigenvector v is piecewise constant with respect to I

if v(i) = v(j)∀i, j ∈ Ik and k ∈ 1, 2...K

1006

Source: MECHANISM
Target Cluster: consensus relation tradition partnership
resistance foundation alliance friendship contact reserve
unity link peace bond myth identity hierarchy relation-
ship connection balance marriage democracy defense
faith empire distinction coalition regime division
Source: STORY; JOURNEY
Target Cluster: politics practice trading reading occupa-
tion profession sport pursuit affair career thinking life
Source: LOCATION; CONTAINER
Target Cluster: lifetime quarter period century succes-
sion stage generation decade phase interval future
Source: LIVING BEING; END
Target Cluster: defeat fall death tragedy loss collapse de-
cline disaster destruction fate

Figure 1: Clustered target concepts

Source Cluster: sparkle glow widen ﬂash ﬂare gleam
darken narrow ﬂicker shine blaze bulge
Source Cluster: gulp drain stir empty pour sip spill swal-
low drink pollute seep ﬂow drip purify ooze pump bub-
ble splash ripple simmer boil tread
Source Cluster: polish clean scrape scrub soak
Source Cluster: kick hurl push ﬂing throw pull drag haul
Source Cluster: rise fall shrink drop double ﬂuctuate
dwindle decline plunge decrease soar tumble surge spiral
boom

Figure 2: Clustered verbs (source domains)

The verb clusters contain coherent lists of source
domain vocabulary.

3.3 Selectional Preference Strength Filter
Following Wilks (1978), we take metaphor to rep-
resent a violation of selectional restrictions. How-
ever, not all verbs have an equally strong capacity
to constrain their arguments, e.g. remember, ac-
cept, choose etc. are weak in that respect. We
suggest that for this reason not all the verbs would
be equally prone to metaphoricity, but only the
ones exhibiting strong selectional preferences. We
test this hypothesis experimentally and expect that
placing this criterion would enable us to ﬁlter out
a number of candidate expressions, that are less
likely to be used metaphorically.

We automatically acquired selectional pref-
erence distributions for Verb-Subject and
Verb-Object relations from the BNC parsed
by RASP. We ﬁrst clustered 2000 most frequent
nouns in the BNC into 200 clusters using SPEC
as described in the previous section. The ob-
tained clusters formed our selectional preference
classes. We adopted the selectional preference

measure proposed by Resnik (1993) and success-
fully applied to a number of tasks in NLP includ-
ing word sense disambiguation (Resnik, 1997).
Resnik models selectional preference of a verb in
probabilistic terms as the difference between the
posterior distribution of noun classes in a partic-
ular relation with the verb and their prior distri-
bution in that syntactic position regardless of the
identity of the predicate. He quantiﬁes this dif-
ference using the relative entropy (or Kullback-
Leibler distance), deﬁning the selectional prefer-
ence strength (SPS) as follows.

SR(v) = D(P (c|v)||P (c)) =

P (c|v) log

P (c|v)
P (c)

,

(cid:3)c

(2)

where P (c) is the prior probability of the noun
class, P (c|v) is the posterior probability of the
noun class given the verb and R is the gram-
matical relation in question. SPS measures how
strongly the predicate constrains its arguments.

We use this measure to ﬁlter out the verbs with
weak selectional preferences. The optimal SPS
threshold was set experimentally on a small held-
out dataset and approximates to 1.32. We ex-
cluded expressions containing the verbs with pref-
erence strength below this threshold from the set
of candidate metaphors.

4 Evaluation and Discussion
In order to prove that our metaphor identiﬁcation
method generalizes well over the seed set and goes
far beyond synonymy, we compared its output to
that of a baseline taking WordNet synsets to repre-
sent source and target domains. We evaluated the
quality of metaphor tagging in terms of precision
with the help of human judges.

4.1 Comparison against WordNet Baseline
The baseline system was implemented using syn-
onymy information from WordNet to expand on
the seed set. Assuming all the synonyms of the
verbs and nouns in seed expressions to represent
the source and target vocabularies respectively,
the system searches for phrases composed of lex-
ical items belonging to those vocabularies. For
example, given a seed expression stir excitement,
the baseline ﬁnds phrases such as arouse fervour,

1007

stimulate agitation, stir turmoil etc. However, it is
not able to generalize over the concepts to broad
semantic classes, e.g. it does not ﬁnd other feel-
ings such as rage, fear, anger, pleasure etc., which
is necessary to fully characterize the target do-
main. The same deﬁciency of the baseline system
manifests itself in the source domain vocabulary:
the system has only the knowledge of direct syn-
onyms of stir, as opposed to other verbs charac-
teristic to the domain of liquids, e.g. pour, ﬂow,
boil etc., successfully identiﬁed by means of clus-
tering.

To compare the coverage achieved by unsuper-
vised clustering to that of the baseline in quanti-
tative terms, we estimated the number of Word-
Net synsets, i.d. different word senses, in the
metaphorical expressions captured by the two sys-
tems. We found that the baseline system covers
only 13% of the data identiﬁed using clustering
and does not go beyond the concepts present in
the seed set. In contrast, most metaphors tagged
by our method are novel and represent a con-
siderably wider range of meanings, e.g. given
the seed metaphors stir excitement, throw remark,
cast doubt the system identiﬁes previously unseen
expressions swallow anger, hurl comment, spark
enthusiasm etc. as metaphorical.

4.2 Comparison with Human Judgements
In order to access the quality of metaphor identiﬁ-
cation by both systems we used the help of human
annotators. The annotators were presented with
a set of randomly sampled sentences containing
metaphorical expressions as annotated by the sys-
tem and by the baseline. They were asked to mark
the tagged expressions that were metaphorical in
their judgement as correct.

The annotators were encouraged to rely on their
own intuition of metaphor. However, we also pro-
vided some guidance in the form of the following
deﬁnition of metaphor3:

1. For each verb establish its meaning in con-
text and try to imagine a more basic meaning
of this verb on other contexts. Basic mean-
ings normally are: (1) more concrete; (2) re-
3taken from the annotation procedure of Shutova and
Teufel (2010) that is in turn partly based on the work of Prag-
glejaz Group (2007).

CKM 391 Time and time again he would stare at the
ground, hand on hip, if he thought he had received a bad
call, and then swallow his anger and play tennis.
AD9 3205 He tried to disguise the anxiety he felt when
he found the comms system down, but Tammuz was
nearly hysterical by this stage.
AMA 349 We will halt the reduction in NHS services
for long-term care and community health services which
support elderly and disabled patients at home.
ADK 634 Catch their interest and spark their enthu-
siasm so that they begin to see the product’s potential.
K2W 1771 The committee heard today that gangs regu-
larly hurled abusive comments at local people, making
an unacceptable level of noise and leaving litter behind
them.

Figure 3:
(metaphors in bold)

Sentences tagged by the system

lated to bodily action; (3) more precise (as
opposed to vague); (4) historically older.

2. If you can establish the basic meaning that
is distinct from the meaning of the verb in
this context, the verb is likely to be used
metaphorically.

We had 5 volunteer annotators who were all na-
tive speakers of English and had no or sparse lin-
guistic knowledge. Their agreement on the task
was 0.63 in terms of κ (Siegel and Castellan,
1988), whereby the main source of disagreement
was the presence of highly lexicalized metaphors,
e.g. verbs such as adopt, convey, decline etc.
We then evaluated the system performance against
their judgements in terms of precision. Precision
measures the proportion of metaphorical expres-
sions that were tagged correctly among the ones
that were tagged. We considered the expressions
tagged as metaphorical by at least three annota-
tors to be correct. As a result our system identi-
ﬁes metaphor with the precision of 0.79, whereas
the baseline only attains 0.44. Some examples of
sentences annotated by the system are shown in
Figure 3.

Such a striking discrepancy between the per-
formance levels of the clustering approach and
the baseline can be explained by the fact that a
large number of metaphorical senses are included
in WordNet. This means that in WordNet synsets
source domain verbs are mixed with more abstract
terms. For example, the metaphorical sense of
shape in shape opinion is part of the synset (de-

1008

termine, shape, mold, inﬂuence, regulate). This
results in the baseline system tagging literal ex-
pressions as metaphorical, erroneously assuming
that the verbs from the synset belong to the source
domain.

The main source of confusion in the output of
our clustering method was the conventionality of
some metaphorical expressions, e.g. hold views,
adopt traditions, tackle a problem. The system
is capable of tracing metaphorical etymology of
conventional phrases, but their senses are highly
lexicalized. This lexicalization is reﬂected in the
data and affects clustering in that conventional
metaphors are sometimes clustered together with
literally used terms, e.g. tackle a problem and re-
solve a problem, which may suggest that the lat-
ter are metaphorical. It should be noted, however,
that such errors are rare.

Since there is no large metaphor-annotated cor-
pus available, it was impossible for us to reli-
ably evaluate the recall of the system. How-
ever, the system identiﬁed a total number of 4456
metaphorical expressions in the BNC starting with
a seed set of only 62, which is a promising result.

5 Related Work
One of the ﬁrst attempts to identify and inter-
pret metaphorical expressions in text automati-
cally is the approach of Fass (1991). Fass devel-
oped a system called met*, capable of discrimi-
nating between literalness, metonymy, metaphor
and anomaly. It does this in three stages. First,
literalness is distinguished from non-literalness
using selectional preference violation as an in-
In the case that non-literalness is de-
dicator.
tected,
the respective phrase is tested for be-
ing a metonymic relation using hand-coded pat-
terns (such as CONTAINER-for-CONTENT). If
the system fails to recognize metonymy, it pro-
ceeds to search the knowledge base for a rele-
vant analogy in order to discriminate metaphor-
ical relations from anomalous ones. E.g., the
sentence in (5) would be represented in this
framework as (car,drink,gasoline), which does
not satisfy the preference (animal,drink,liquid),
as car is not a hyponym of animal. met*
then searches its knowledge base for a triple
containing a hypernym of both the actual ar-

gument and the desired argument and ﬁnds
(thing,use,energy source), which represents the
metaphorical interpretation.

Birke and Sarkar

(2006) present a sen-
tence clustering approach for non-literal
lan-
guage recognition implemented in the TroFi sys-
tem (Trope Finder). This idea originates from
a similarity-based word sense disambiguation
method developed by Karov and Edelman (1998).
The method employs a set of seed sentences,
where the senses are annotated, computes simi-
larity between the sentence containing the word
to be disambiguated and all of the seed sentences
and selects the sense corresponding to the anno-
tation in the most similar seed sentences. Birke
and Sarkar (2006) adapt this algorithm to perform
a two-way classiﬁcation:
literal vs. non-literal,
and they do not clearly deﬁne the kinds of tropes
they aim to discover. They attain a performance
of 53.8% in terms of f-score.

The method of Gedigan et al. (2006) discrimi-
nates between literal and metaphorical use. They
trained a maximum entropy classiﬁer for this pur-
pose. They obtained their data by extracting the
lexical items whose frames are related to MO-
TION and CURE from FrameNet (Fillmore et al.,
2003). Then they searched the PropBank Wall
Street Journal corpus (Kingsbury and Palmer,
2002) for sentences containing such lexical items
and annotated them with respect to metaphoric-
ity. They used PropBank annotation (arguments
and their semantic types) as features to train the
classiﬁer and report an accuracy of 95.12%. This
result is, however, only a little higher than the per-
formance of the naive baseline assigning majority
class to all instances (92.90%). These numbers
can be explained by the fact that 92.00% of the
verbs of MOTION and CURE in the Wall Street
Journal corpus are used metaphorically, thus mak-
ing the dataset unbalanced with respect to the tar-
get categories and the task notably easier.

Both Birke and Sarkar (2006) and Gedigan et
al. (2006) focus only on metaphors expressed by
a verb. As opposed to that the approach of Kr-
ishnakumaran and Zhu (2007) deals with verbs,
nouns and adjectives as parts of speech. They
use hyponymy relation in WordNet and word bi-
gram counts to predict metaphors at the sentence

1009

level. Given an IS-A metaphor (e.g. The world is
a stage4) they verify if the two nouns involved are
in hyponymy relation in WordNet, and if this is
not the case then this sentence is tagged as con-
taining a metaphor. Along with this they con-
sider expressions containing a verb or an adjec-
tive used metaphorically (e.g. He planted good
ideas in their minds or He has a fertile imagi-
nation). Hereby they calculate bigram probabil-
ities of verb-noun and adjective-noun pairs (in-
cluding the hyponyms/hypernyms of the noun in
question). If the combination is not observed in
the data with sufﬁcient frequency, the system tags
the sentence containing it as metaphorical. This
idea is a modiﬁcation of the selectional prefer-
ence view of Wilks. However, by using bigram
counts over verb-noun pairs as opposed to verb-
object relations extracted from parsed text Kr-
ishnakumaran and Zhu (2007) loose a great deal
of information. The authors evaluated their sys-
tem on a set of example sentences compiled from
the Master Metaphor List (Lakoff et al., 1991),
whereby highly conventionalized metaphors (they
call them dead metaphors) are taken to be neg-
ative examples. Thus, they do not deal with lit-
eral examples as such: essentially, the distinc-
tion they are making is between the senses in-
cluded in WordNet, even if they are conventional
metaphors, and those not included in WordNet.

6 Conclusions and Future Directions

We presented a novel approach to metaphor iden-
tiﬁcation in unrestricted text using unsupervised
methods. Starting from a limited set of metaphor-
ical seeds, the system is capable of capturing the
regularities behind their production and annotat-
ing a much greater number and wider range of
previously unseen metaphors in the BNC.

Our system is the ﬁrst of its kind and it is capa-
ble of identifying metaphorical expressions with a
high precision (0.79). By comparing its coverage
to that of a WordNet baseline, we proved that our
method goes far beyond synonymy and general-
izes well over the source and target domains. Al-
though at this stage we tested our system on verb-
subject and verb-object metaphors only, we are

4William Shakespeare

convinced that the described identiﬁcation tech-
niques can be similarly applied to a wider range
of syntactic constructions. Extending the system
to deal with more parts of speech and types of
phrases is part of our future work.

One possible limitation of our approach is that
it is seed-dependent, which makes the recall of the
system questionable. Thus, another important fu-
ture research avenue is the creation of a more di-
verse seed set. We expect that a set of expres-
sions representative of the whole variety of com-
mon metaphorical mappings, already described in
linguistics literature, would enable the system to
attain a very broad coverage of the corpus. Mas-
ter Metaphor List (Lakoff et al., 1991) and other
existing metaphor resources could be a sensible
starting point on a route to such a dataset.

Acknowledgments
We are very grateful to our anonymous reviewers
for their useful feedback on this work and the vol-
unteer annotators for their interest, time and help.
This research is funded by generosity of Cam-
bridge Overseas Trust (Katia Shutova), Dorothy
Hodgkin Postgraduate Award (Lin Sun) and the
Royal Society, UK (Anna Korhonen).

References
Andersen, O. E., J. Nioche, E. Briscoe, and J. Carroll.
2008. The BNC parsed with RASP4UIMA. In Pro-
ceedings of LREC 2008, Marrakech, Morocco.

Bergsma, S., D. Lin, and R. Goebel. 2008. Discrimi-
native learning of selectional preference from unla-
beled text. In Proceedings of the EMNLP.

Birke, J. and A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonlit-
eral language. In In Proceedings of EACL-06, pages
329–336.

Brew, C. and S. Schulte im Walde. 2002. Spectral
In Proceedings of

clustering for German verbs.
EMNLP.

Briscoe, E., J. Carroll, and R. Watson. 2006. The sec-
ond release of the rasp system. In Proceedings of
the COLING/ACL on Interactive presentation ses-
sions, pages 77–80.

Burnard, L. 2007. Reference Guide for the British

National Corpus (XML Edition).

1010

Chen, J., D. Ji, C. Lim Tan, and Z. Niu. 2006. Un-
supervised relation disambiguation using spectral
clustering. In Proceedings of COLING/ACL.

Fass, D. 1991. met*: A method for discriminating

Lin, D. 1998. Automatic retrieval and clustering of
similar words.
In Proceedings of the 17th inter-
national conference on Computational linguistics,
pages 768–774.

metonymy and metaphor by computer. Computa- Martin, J. H. 1988. Representing regularities in the
metaphoric lexicon. In Proceedings of the 12th con-
tional Linguistics, 17(1):49–90.
ference on Computational linguistics, pages 396–
401.

Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database (ISBN: 0-262-06197-X). MIT
Press, ﬁrst edition.

Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.

2003. Background to FrameNet.
Journal of Lexicography, 16(3):235–250.

Meila, M. and J. Shi. 2001. A random walks view of

spectral segmentation. In AISTATS.

International Meila, M. 2001. The multicut lemma. Technical re-

port, University of Washington.

Gedigan, M., J. Bryant, S. Narayanan, and B. Ciric.
2006. Catching metaphors. In In Proceedings of the
3rd Workshop on Scalable Natural Language Un-
derstanding, pages 41–48, New York.

Pantel, P. and D. Lin.

2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 613–619. ACM.

Joanis, E., S. Stevenson, and D. James. 2008. A gen-
eral feature space for automatic verb classiﬁcation.
Natural Language Engineering, 14(3):337–367.

Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of English words. In Proceedings
of ACL-93, pages 183–190, Morristown, NJ, USA.

Karov, Y. and S. Edelman. 1998. Similarity-based
word sense disambiguation. Computational Lin-
guistics, 24(1):41–59.

Pragglejaz Group. 2007. MIP: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and Symbol, 22:1–39.

Kingsbury, P. and M. Palmer. 2002. From TreeBank
to PropBank. In Proceedings of LREC-2002, Gran
Canaria, Canary Islands, Spain.

Kipper, K., A. Korhonen, N. Ryant, and M. Palmer.
2006. Extensive classiﬁcations of English verbs.
In Proceedings of the 12th EURALEX International
Congress.

Korhonen, A., Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame dis-
tributions semantically.
In Proceedings of ACL
2003, Sapporo,Japan.

Preiss, J., T. Briscoe, and A. Korhonen. 2007. A sys-
tem for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from cor-
pora. In Proceedings of ACL-2007, volume 45, page
912.

Resnik, P. 1993. Selection and Information: A Class-
based Approach to Lexical Relationships. Ph.D.
thesis, Philadelphia, PA, USA.

Resnik, P. 1997. Selectional preference and sense dis-
ambiguation.
In ACL SIGLEX Workshop on Tag-
ging Text with Lexical Semantics, Washington, D.C.

Korhonen, A., Y. Krymolowski, and T. Briscoe. 2006.
A large subcategorization lexicon for natural lan-
guage processing applications.
In Proceedings of
LREC 2006.

Rooth, M., S. Riezler, D. Prescher, G. Carroll, and
F. Beil. 1999.
Inducing a semantically annotated
lexicon via EM-based clustering. In Proceedings of
ACL 99, pages 104–111.

Krishnakumaran, S. and X. Zhu. 2007. Hunting elu-
sive metaphors using lexical resources. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 13–20, Rochester,
NY.

Lakoff, G. and M. Johnson. 1980. Metaphors We Live

By. University of Chicago Press, Chicago.

Lakoff, G., J. Espenson, and A. Schwartz. 1991. The
master metaphor list. Technical report, University
of California at Berkeley.

Levin, B. 1993. English Verb Classes and Alterna-

tions. University of Chicago Press, Chicago.

Schulte im Walde, S. 2006. Experiments on the au-
tomatic induction of German semantic verb classes.
Computational Linguistics, 32(2):159–194.

Shutova, E. and S. Teufel. 2010. Metaphor corpus
annotated for source - target domain mappings. In
Proceedings of LREC 2010, Malta.

Shutova, E. 2010. Automatic metaphor interpretation
In Proceedings of NAACL

as a paraphrasing task.
2010, Los Angeles, USA.

Siegel, S. and N. J. Castellan. 1988. Nonparametric
statistics for the behavioral sciences. McGraw-Hill
Book Company, New York, USA.

