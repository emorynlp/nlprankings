



















































UTDMet: Combining WordNet and Corpus Data for Argument Coercion Detection


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 252–255,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

UTDMet: Combining WordNet and Corpus Data for
Argument Coercion Detection

Kirk Roberts and Sanda Harabagiu
Human Language Technology Research Institute

University of Texas at Dallas
Richardson, Texas, USA

{kirk,sanda}@hlt.utdallas.edu

Abstract

This paper describes our system for the
classification of argument coercion for
SemEval-2010 Task 7. We present two ap-
proaches to classifying an argument’s se-
mantic class, which is then compared to
the predicate’s expected semantic class to
detect coercions. The first approach is
based on learning the members of an arbi-
trary semantic class using WordNet’s hy-
pernymy structure. The second approach
leverages automatically extracted seman-
tic parse information from a large corpus
to identify similar arguments by the pred-
icates that select them. We show the re-
sults these approaches obtain on the task
as well as how they can improve a tradi-
tional feature-based approach.

1 Introduction

Argument coercion (a type of metonymy) occurs
when the expected semantic class (relative to the
a predicate) is substituted for an object of a dif-
ferent semantic class. Metonymy is a pervasive
phenomenon in language and the interpretation of
metonymic expressions can impact tasks from se-
mantic parsing (Scheffczyk et al., 2006) to ques-
tion answering (Harabagiu et al., 2005). A seminal
example in metonymy from (Lakoff and Johnson,
1980) is:

(1) The ham sandwich is waiting for his check.

The ARG1 for the predicate wait is typically an
animate, but the “ham sandwich” is clearly not an
animate. Rather, the argument is coerced to ful-
fill the predicate’s typing requirement. This coer-
cion is allowed because an object that would nor-
mally fulfill the typing requirement (the customer)
can be uniquely identified by an attribute (the ham
sandwich he ordered).

SemEval-2010 Task 7 (“Argument Selection
and Coercion”) (Pustejovsky and Rumshisky,
2009) was designed to evaluate systems that de-
tect such coercions and provide a “compositional
history” of argument selection relative to the pred-
icate. In order to accomplish this, an argument is
annotated with both the semantic class to which it
belongs (the “source” type) as well as the class ex-
pected by the predicate (the “target” type). How-
ever, in the data provided, the target type was un-
ambiguous given the lemmatized predicate, so the
remainder of this paper discusses source type clas-
sification. The detection of coercion is then sim-
ply performed by checking if the classified source
type and target type are different.

In our system, we explore two approaches with
separate underlying assumptions about how arbi-
trary semantic classes can be learned. In our first
approach, we assume a semantic class can be de-
fined a priori from a set of seed terms and that
WordNet is capable of defining the membership
of that semantic class. We apply the PageRank al-
gorithm in order to weight WordNet synsets given
a set of seed concepts. In our second approach,
we assume that arguments in the same semantic
class will be selected by similar verbs. We apply a
statistical test to determine the most representative
predicates for an argument. This approach benefits
from a large corpus from which we automatically
extracted 200 million predicate-argument pairs.

The remainder of this paper is organized as fol-
lows. Section 2 discusses our WordNet-based ap-
proach. Section 3 describes our corpus approach.
Section 4 discusses our experiments and results.
Section 5 provides a conclusion and direction for
future work. Due to space limitations, previous
work is discussed when relevant.

2 PageRanking WordNet Hypernyms

Our first approach assumes that semantic class
members can be defined and acquired a priori.

252



Given a set of seed concepts, we mine WordNet
for other concepts that may be in the same seman-
tic class. Clearly, this approach has both practical
limitations (WordNet does not contain every pos-
sible concept) and linguistic limitations (concepts
may belong to different semantic classes based on
their context). However, given the often vague na-
ture of semantic classes (is a building an ARTI-
FACT or a LOCATION?), access to a weighted list
of semantic class members can prove useful for ar-
guments not seen in the train set.

Using (Esuli and Sebastiani, 2007) as inspira-
tion, we have implemented our own naive ver-
sion of WordNet PageRank. They use sense-
disambiguated glosses provided by eXtended
WordNet (Harabagiu et al., 1999) to link synsets
by starting with positive (or negative) sentiment
concepts in order to find other concepts with pos-
itive (or negative) sentiment values. For our
task, however, hypernymy relations are more ap-
propriate for determining a given synset’s mem-
bership in a semantic class. Hypernymy de-
fines an IS-A relationship between the parent
class (the hypernym) and one of its child classes
(the hyponym). Furthermore, while PageRank as-
sumes directed edges (e.g., hyperlinks in a web
page), we use undirected edges. In this way, if
HYPERNYMOF(A, B), then A’s membership in a
semantic class strengthens B’s and vice versa.

Briefly, the formula for PageRank is:

a(k) = αa(k−1)W + (1 − α)e (1)
where a(k) is the weight vector containing weights
for every synset in WordNet at time k; Wi,j is the
inverse of the total number of hypernyms and hy-
ponyms for synset i if synset j is a hypernym or
hyponym of synset i; e is the initial score vector;
and α is a tuning parameter. In our implementa-
tion, a(0) is initialized to all zeros; α is fixed at
0.5; and ei = 1 if synset i is in the seed set S,
and zero otherwise. The process is then run until
convergence, defined by |a(k)i − a(k−1)i | < 0.0001
for all i.

The result of this PageRank is a weighted list
containing every synset reachable by a hyper-
nym/hyponym relation from a seed concept. We
ran the PageRank algorithm six times, once for
each semantic class, using the arguments in the
train set as seeds. For arguments that are polyse-
mous, we make a first WordNet sense assumption.
Representative examples of the concepts gener-
ated from this approach are shown in Table 1.

ARTIFACT DOCUMENT
funny wagon .377 white paper .342

liquor .353 progress report .342
iced tea .338 screenplay .324
tartan .325 papyrus .313
alpaca .325 pie chart .308

EVENT LOCATION
rock concert .382 heliport .381

rodeo .369 mukataa .380
radium therapy .357 subway station .342

seminar .347 dairy farm .326
pub crawl .346 gateway .320

PROPOSITION SOUND
dibs .363 whoosh .353

white paper .322 squish .353
tall tale .319 yodel .339

commendation .310 theme song .320
field theory .309 oldie .312

Table 1: Some of the concepts (and scores) learned
from applying PageRank to WordNet hypernyms.

3 Leveraging a Large Corpus of
Semantic Parse Annotations

Our second approach assumes that semantic class
members are arguments of similar predicates. As
(Pustejovsky and Rumshisky, 2009) elaborate,
predicates select an argument from a specific se-
mantic class, therefore terms that belong in the
same semantic class should be selected by simi-
lar predicates. However, this assumption is often
violated: type coercion allows predicates to have
arguments outside their intended semantic class.
Our solution to this problem, partially inspired by
(Lapata and Lascarides, 2003), is to collect statis-
tics from an enormous amount of data in order to
statistically filter out these coercions.

The English Gigaword Forth Edition corpus1

contains over 8.5 million documents of newswire
text collected over a 15 year period. We processed
these documents with the SENNA2 (Collobert and
Weston, 2009) suite of natural language tools,
which includes a part-of-speech tagger, phrase
chunker, named entity recognizer, and PropBank
semantic role labeler. We chose SENNA due to its
speed, yet it still performs comparably with many
state-of-the-art systems. Of the 8.5 million doc-
uments in English Gigaword, 8 million were suc-
cessfully processed. For each predicate-argument
pair in these documents, we gathered counts by
argument type and argument head. The head was
determined with simple heuristics from the chunk
parse and parts-of-speech for each argument (ar-
guments consisting of more than three phrase
chunks were discarded). When available, named
entity types (e.g., PERSON, ORGANIZATION, LO-

1LDC2009T13
2http://ml.nec-labs.com/senna/

253



coffee book meeting station report voice
drink write hold own release hear
sip read attend build publish raise

brew publish schedule open confirm give
serve title chair attack issue add
spill sell convene close comment have
smell buy arrange operate submit silence
sell balance call fill deny sound
pour illustrate host shut file lend
buy research plan storm prepare crack
rise review make set voice find

Table 2: Top ten predicates for the most common
word in the train set for the six semantic classes.

CATION) were substituted for heads. This resulted
in over 511 million predicate-argument pairs for
argument types ARG0, ARG1, and ARG2. For this
task, however, we chose only to use ARG1 argu-
ments (direct objects), which resulted in 210 mil-
lion pairs, 7.65 million of which were unique. The
ARG1 argument was chosen because most of the
arguments in the data are direct objects 3.

The “best” predicates for a given argument are
defined by a ranking based on Fisher’s exact test
(Fisher, 1922):

p =
(a + b)!(c + d)!(a + c)!(b + d)!

n!a!b!c!d!
(2)

where a is the number of times the given argument
was used with the given predicate, b is the number
of times the argument was used with a different
predicate, c is the number of times the predicate
was used with a different argument, d is the num-
ber of times neither the given argument or predi-
cate was used, and n = a+b+c+d. The top ranked
(lowest p) predicates for the most common argu-
ments in the training data are shown in Table 2.

4 Experiments

We have conducted several experiments to test
the performance of the approaches outlined in
Sections 2 and 3 along with additional features
commonly found in information extraction liter-
ature. All experiments were conducted using the
SVMmulticlass support vector machine library4.

4.1 WordNet PageRank

We experimented with the output of our WordNet
PageRank implementation along three separate di-
mensions: (1) which sense to use (since we did
not incorporate a word sense disambiguation sys-
tem), (2) whether to use the highest scoring se-

3The notable exception to this, however, is arrive, where
the data uses the destination argument. In the PropBank
scheme (Palmer et al., 2005), this would correspond to the
ARG4, which usually signifies an end state.

4http://svmlight.joachims.org/svm multiclass.html

mantic class or every class an argument belonged
to, and (3) how to use the weight output by the al-
gorithm. The results of these experiments yielded
a single feature for each class that returns true if
the argument is in that class, regardless of weight.
This resulted in a micro-precision score of 75.6%.

4.2 Gigaword Predicates

We experimented with both (i) the number of pred-
icates to use for an argument and (ii) the score
threshold to use. Ultimately, the Fisher score did
not prove nearly as useful as a classifier as it did
as a ranker. Since the distribution of predicates
for each argument varied significantly, choosing a
high number of predicates would yield good re-
sults for some arguments but not others. However,
because of size of the training data, we were able
to choose the top 5 predicates for each argument
as features and still achieve a reasonable micro-
precision score of 89.6%.

4.3 Other Features

Many other features common in information ex-
traction are well-suited for this task. Given that
SVMs can support millions of features, we chose
to add many features simpler than those previously
described in order to improve the final perfor-
mance of the classifier. These include the lemma
of the argument (both the last word’s lemma and
every word’s lemma), the lemma of the predicate,
the number of words in the argument, the casing of
the argument, the part-of-speech of the argument’s
last word, the WordNet synset and all (recursive)
hypernyms of the argument. Additionally, since
the EVENT class is both the most common and
the most often confused, we introduced two fea-
tures based on annotated resources. The first fea-
ture indicates the most common part-of-speech for
the un-lemmatized argument in the Treebank cor-
pus. This helped classify examples such as think-
ing which was confused with a PROPOSITION for
the predicate deny. Second, we introduced a fea-
ture that indicated if the un-lemmatized argument
was considered an event in the TimeBank cor-
pus (Pustejovsky et al., 2003) at least five times.
This helped to distinguish events such as meet-
ing, which was confused with a LOCATION for the
predicate arrive.

4.4 Ablation Test

We conducted an ablation test using combina-
tions of five feature sets: (1) our WordNet PageR-

254



+WNSH +WNPR +GWPA +EVNT

WORD 89.2 94.2 95.0 95.6 96.1

EVNT 31.1 89.7 89.9 90.8

GWPA 89.6 90.8 91.0

WNPR 75.6 89.4

WNSH 89.0

Table 3: Ablation test of feature sets showing
micro-precision scores.

Precision Recall

Selection vs. Coercion
Macro 95.4 95.7
Micro 96.3 96.3

Source Type
Macro 96.5 95.7
Micro 96.1 96.1

Target Type
Macro 100.0 100.0
Micro 100.0 100.0

Joint Type
Macro 85.5 95.2
Micro 96.1 96.1

Table 4: Results for UTDMET on SemEval-2010
Task 7.

ank feature (WNPR), (2) our Gigaword Predicates
feature (GWPA), (3) word, lemma, and part-of-
speech features (WORD), (4) WordNet synset and
hypernym features (WNSH), and (5) Treebank and
TimeBank features (EVNT). Of these 25 − 1 =
31 tests, 15 are shown in Table 3. The Giga-
word Predicates (GWPA) was the best overall fea-
ture, but each feature set ended up helping the fi-
nal score. WordNet PageRank (WNPR) even im-
proved the score when combined WordNet hyper-
nym features (WNSH) despite the fact that they
are heavily related. Ultimately, WordNet PageR-
ank had a greater precision, while the other Word-
Net features had greater recall.

4.5 Task 7 Results

Table 4 shows the official results for UTDMET on
the Task 7 data. The target type was unambigu-
ous given the lemmatized predicate. For classify-
ing selection vs. coercion, we simply checked to
see if the classified source type was the same as
the target type. If this was the case, we returned
selection, otherwise a coercion existed.

5 Conclusion

We have presented two approaches for determin-
ing the semantic class of a predicate’s argument.
The two approaches capture different information
and combine well to classify the “source” type in
SemEval-2010 Task 7. We showed how this can be
incorporated into a system to detect coercions as
well as the argument’s compositional history rel-
ative to its predicate. In future work we plan to
extend this system to more complex tasks such as

when the predicate may be polysemous or unseen
predicates may be encountered.

Acknowledgments

The authors would like to thank Bryan Rink for
several insights during the course of this work.

References
Ronan Collobert and Jason Weston. 2009. Deep

Learning in Natural Language Processing. Tutorial
at NIPS.

Andrea Esuli and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An Application to Opin-
ion Mining. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 424–431.

Ronald A. Fisher. 1922. On the interpretation of χ2

from contingency tables, and the calculation of p.
85(1):87–94.

Sanda Harabagiu, George Miller, and Dan Moldovan.
1999. WordNet 2 - A Morphologically and Se-
mantically Enhanced Resource. In Proceedings of
the SIGLEX Workshop on Standardizing Lexical Re-
sources, pages 1–7.

Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with Inter-
active Question-Answering. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 205–214.

George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. University of Chicago Press.

Maria Lapata and Alex Lascarides. 2003. A Proba-
bilistic Account of Logical Metonymy. Computa-
tional Linguistics, 21(2):261–315.

Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.

James Pustejovsky and Anna Rumshisky. 2009.
SemEval-2010 Task 7: Argument Selection and Co-
ercion. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 88–93.

James Pustejovsky, Patrick Hanks, Roser Saurı́, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics,
pages 647–656.

Jan Scheffczyk, Adam Pease, and Michael Ellsworth.
2006. Linking FrameNet to the Suggested Upper
Merged Ontology. In Proceedings of Formal Ontol-
ogy in Information Systems, pages 289–300.

255


