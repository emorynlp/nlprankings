










































Multilingual Multi-Document Summarization with POLY2


Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 45–49,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

Multilingual Multi-Document Summarization with POLY2

Marina Litvak
Department of Software Engineering

Shamoon College of Engineering
Beer Sheva, Israel

marinal@sce.ac.il

Natalia Vanetik
Department of Software Engineering

Shamoon College of Engineering
Beer Sheva, Israel

natalyav@sce.ac.il

Abstract

In this paper we present a linear model for
the problem of text summarization, where
a summary preserves the information cov-
erage as much as possible in comparison
to the original document set. We reduce
the problem of finding the best summary
to the problem of finding the point on a
convex polytope closest to the given hy-
perplane, and solve it efficiently with the
help of fractional linear programming. We
supply here an overview of our system, ti-
tled POLY2, that participated in the Multi-
Ling contest at ACL 2013.

1 Introduction

Automated text summarization is an active field
of research in various communities like Informa-
tion Retrieval (IR), Natural Language Processing
(NLP), and Text Mining (TM).

Some authors reduce summarization to the
maximum coverage problem (Takamura and Oku-
mura, 2009; Gillick and Favre, 2009) that, de-
spite a great performance, is known as NP-
hard (Khuller et al., 1999). Linear Program-
ming helps to find an accurate approximated so-
lution to this problem and became very popular
in summarization field in the last years (Gillick
and Favre, 2009; Woodsend and Lapata, 2010; Hi-
toshi Nishikawa and Kikui, 2010; Makino et al.,
2011). However, most mentioned works use expo-
nential number of constraints.

Trying to solve a trade-off between summary
quality and time complexity, we propose a novel
summarization model solving the approximated
maximum coverage problem by linear program-
ming in polynomial time. We measure informa-
tion coverage by terms1 and strive to obtain a sum-
mary that preserves the optimal value of the cho-

1normalized meaningful words

sen objective function as much as possible in com-
parison to the original document. Various objec-
tive functions combining different parameters like
term’s position and its frequency are introduced
and evaluated.

Our method ranks and extracts significant sen-
tences into a summary and it can be generalized
for both single-document and multi-document
summarization. Also, it can be easily adapted to
cross-lingual/multilingual summarization.

Formally speaking, in this paper we introduce
(1) a novel text representation model expanding a
classic Vector Space Model (Salton et al., 1975)
to Hyperplane and Half-spaces, (2) re-formulated
extractive summarization problem as an optimiza-
tion task and (3) its solution using linear program-
ming. The main challenge of this paper is a new
text representation model making possible to rep-
resent an exponential number of extracts without
computing them explicitly, and finding the opti-
mal one by simple minimizing a distance function
in polynomial time.

2 Our Method

2.1 Definitions

We are given a set of sentences S1, ..., Sn derived
from a document or a cluster of related documents.
Meaningful words in these sentences are entirely
described by terms T1, ..., Tm. Our goal is to find
a subset Si1 , ..., Sik consisting of sentences such
that (1) there are at most N terms in these sen-
tences, (2) term frequency is preserved as much
as possible w.r.t. the original sentence set, (3) re-
dundant information among k selected sentences
is minimized.

We use the standard sentence-term matrix, A =
(aij) of size m× n, for initial data representation,
where aij = k if term Ti appears in the sentence
Sj precisely k times.

Our goal is to find subset i1, ..., ik of A’s

45



columns so that the chosen submatrix represents
the best possible summary under some constraints.
Since it is hard to determine what is the best
summary mathematically (this task is usually left
to human experts), we wish to express summary
quality as a linear function of the underlying ma-
trix. We strive to find a summary that gives an op-
timal value once the function in question has been
determined.

Basic text preprocessing includes sentence
splitting and tokenization. Also, additional steps
like stopwords removal, stemming, synonym res-
olution, etc. may be performed for resource-rich
languages.

2.2 Polytope as a document representation

We represent every sentence by a hyperplane and
the lower half-space of that hyperplane. In a way,
the hyperplane bounding each half-space is the
sentence itself, and a half-space below it is an
approximation of that sentence. An intersection
of lower half-spaces in Euclidean space forms a
convex polyhedron, and in our case the faces of
this polyhedron are intersections of hyperplanes
bounding lower half-spaces that stand for docu-
ment sentences. We add trivial constraints so that
the polyhedron representing the entire document
is bounded, i.e. is a polytope. All possible extracts
from the document can be represented by hyper-
plane intersections. Thus the boundary of the re-
sulting polytope is a good approximation for ex-
tracts that can be generated from the given docu-
ment.

We view every column of the sentence-term ma-
trix as a linear constraint representing a lower
half-space in Rmn. An occurrence of term ti in
sentence Sj is represented by variable xij . The
maximality constraint on the number of terms in
the summary can be easily expressed as a con-
straint on the sum of these variables. Note that
each sentence constraint uses its n unique vari-
ables, thus making sure that the intersection of ev-
ery subset of sentence hyperplane is not empty and
is well-defined.

Every sentence Sj in our document is a lower
half-space of a hyperlane in Rmn, defined with
columns of A and variables x1j , . . . , xmj repre-
senting the terms in this sentence:

A[][j] = [a1j , . . . , amj ]
xj = [x1j , . . . , xmj ] for all 1 ≤ j ≤ n

We define a system of linear inequalities

A[][j] · xTj =
∑m

i=1 aijxij ≤
≤ A[][j] · 1T =

∑m
i=1 aij

(1)

Every inequality of this form defines a lower half-
space of a hyperplane

Hi := (A[][j] · xTj = A[][j] · 1T )

To say that every term is either present or absent
from the chosen extract, we add constraints 0 ≤
xij ≤ 1. Intuitively, the entire hyperplane Hi and
therefore every point p ∈ Hi represents sentence
Si. Then a subset of r sentences is represented by
intersection of r hyperplanes.

2.3 Summary constraints
We express summarization constraints in the form
of linear inequalities. Maximality constraint on
the number of terms in the summary can be eas-
ily expressed as a constraint on the sum of all term
variables xij , and the same goes for minimality
constraint.

2.4 The polytope model
Having defined linear inequalities that describe
each sentence in a document separately and the
total number of terms in sentence subset, we can
now look at them together as a system:

∑m
i=1 ai1xi1 ≤

∑m
i=1 ai1

. . .∑m
i=1 ainxin ≤

∑m
i=1 ain

Tmin ≤
∑m

i=1

∑n
j=1 xij ≤ Tmax

Wmin ≤
∑m

i=1

∑n
j=1 aijxij ≤Wmax

0 ≤ xij ≤ 1

(2)

First n inequalities describe sentences S1, . . . , Sn,
the next two inequalities describes constraints on
the total number of terms and words in a summary,
and the final constraint determines upper and
lower boundaries for all sentence-term variables.
Intersections of S1, . . . , Sn are well-defined, since
every pair of sentences is described by a linear
constraints on different n-tuple of variables xij out
of total mn variables. Since every inequality in the
system (2) is linear, the entire system describes a
convex polyhedron, which we denote by P.

2.5 Objectives and summary extraction
We assume here that the surface of the polyhedron
P is a suitable representation of all the possible

46



Function Formula Description
Maximal Weighted max

∑m
i=1 witi, Maximizes the information coverage as a weighted term sum.

Term Sum (OBJ1) ti =
∑n

j=1 xji We used the following types of term weights wi.
(1) POS EQ, where wi = 1 for all i;
(2) POS F, where wi = 1app(i) and app(i) is the index of a sentence
in the document where the term Ti first appeared;
(3) POS B, where wi = max{ 1app(i) ,

1
n−app(i)+1};

(4) TF, where wi = tf(i) and tf(i) is the term frequency of term Ti;
(5) TFISF, where wi = tf(i) ∗ isf(i)
and isf(i) is the inverse sentence frequency of Ti

Distance Function min
∑m

i=1(t̂i − pi)
2, Minimizes the Eucledian distance between terms t = (t1, . . . , tm)

(OBJ2) (1) t̂i = ti =
∑n

j=1 xji (a point on the polytope P representing a generated summary)
and ∀i pi = 1, or and the vector p = (p1, . . . , pm) (expressing document properties
(2) t̂i = ti∑m

j=1 tj
we wish to preserve and representing the ”ideal” summary).

and pi = tf(i) We used the following options for t and p representation.
(1) MTC, where t is a summary term count vector
and p contains all the terms precisely once, thus
minimizing repetition but increasing terms coverage.
(2) MTF, where t contains term frequency of terms in a summary
and p contains term frequency for terms in documents.

Sentence Overlap min
∑n

j=1

∑n
k=j+1 ovljk, Minimizes the Jaccard similarity between sentences in a summary

(OBJ3) ovljk =
|Sj∩Sk|
|Sj∪Sk|

= (denoted by ovljk for Sj and Sk).

=
∑m

i=1 w(aij ,aik)(xij+xik)∑m
i=1(aij+aik)

w(aij , aik) is 1 if the term Ti is present in both sentences Sj and Sk
and is 0 otherwise.

Maximal Bigram max
∑

i,j biij , Maximizes the information coverage as a bigram sum.
Sum (OBJ4) where ∀i, j 0 ≤ biij ≤ 1 Variable biij is defined for every bigram (Ti, Tj) in the text.

Table 1: Objective functions for summarization using polytope model.

sentence subsets. Fortunately, we do not need to
scan the whole set of P’s surfaces but rather to find
the point on P that optimizes the chosen objective
function. Table 1 contains four different objective
functions that we used for summarization, along
with descriptions of the changes in the model that
were required for each function.

Since fractional LP method not only finds
the optimal value of objective function but also
presents an evidence to that optimality in the form
of a point x = (xij), we use the point’s data to find
what sentences belong to the chosen summary.
The point x may satisfy several of the sentence
inequalities as equalities, while other inequalities
may not turn into equalities. If sentence inequality
is turned into equality by x, the sentence necessar-
ily belongs to the chosen summary. Otherwise, the
point x that optimizes the chosen objective func-
tion represents a summary that does not contain
sufficient number of words or terms. In this case
we add additional sentences to the summary and
we choose these sentences on the basis of their
distance from the point x. Sentence hyperplanes
that are the nearest to the point x are chosen in a
greedy manner and added to the summary. This
test is straightforward and takes O(mn) time.

3 POLY2: system description

We title our system POLY2 as a double POLY
occured in ”POLYnomial Summarization using
POLYtope model”. POLY2 is implemented in
Java and it uses lp-solve software (Berkelaar,
1999) in order to perform Linear Programming.
The input for our system is initial collection of
texts to be summarized. Four main parts of POLY2

are: preprocessing, linear program generation, lin-
ear program application and testing. Data flow of
the system is depicted in Figure 1.

The preprocessing step consists of following
parts. During the very first step, initial documents
undergo stop word removal, stemming and syn-
onym resolution (if available for the chosen lan-
guage). Then, a sentence-term matrix is gener-
ated in the form of a text file, where every line
describes a term and every column – a sentence.
Also, the index list for multi-document summa-
rization is generated. In this list serial number of
each sentence is paired up with its serial number
in its document. This information is used later
in order to decide how close each sentence is to
its document’s boundaries. Finally, we generate
list of bigrams (a consecutive appearance of two
terms) for every sentence. All of the files gener-

47



Figure 1: Data flow of the POLY2 system.

Figure 2: Linear program generated by POLY2

system.

ated during preprocessing are text files.
The main part of POLY2 is linear program

generation. The system allows to select document
matrix files and auxiliary files and objective func-
tion and generates a system of linear inequalities
together with an objective function in format ac-
ceptable by lp-solve software. Figure 2 shows a
sample LP file contents. Note that file generation
for every one of our objective function takes only
seconds for a single documents.

The next step is to run linear program and ex-
tract its results. We use lp-solve Java API to per-
form this task and extract coordinates of point x
that optimizes the chosen objective function. In
order to construct the summary, we measure the
normalized distance from point x to every one of
the sentence hyperplanes. Since this information
is readily available through lp-solve API, the task
requires sorting of n real numbers, where n is the
number of sentences in all of the documents to-

gether. Hyperplanes whose distance to x is mini-
mal represent the sentences participating in the fi-
nal summary. Running time of this step for a sin-
gle file does not exceed three seconds.

The final (optional) step is to verify gener-
ated summaries, that can be performed with the
help of any evaluation system. In our case, the
ROUGE (Lin, 2004) system has been used.

4 Conclusions and Future Work

In this paper we present an extractive summariza-
tion system POLY2 based on a linear program-
ming model. We represent the document as a set
of intersecting hyperplanes. Every possible sum-
mary of a document is represented as the inter-
section of two or more hyperlanes. We consider
the summary to be the best if the optimal value of
objective function is preserved during summariza-
tion, and translate the summarization problem into
a problem of finding a point on a convex polytope
which is the closest to the hyperplane describing
the ”ideal” summary. We introduce multiple ob-
jective functions describing the distance between
a summary (a point on a convex polytope) and
the best summary (the hyperplane). Since the in-
troduced objectives behaves differently on differ-
ent languages, only two of them were indicated
as primary systems and evaluated by MultiLing
2013 organizers–OBJPOS F1 and OBJ3–denoted
by ID5 and ID51.

Below we summarize the results of automated
evaluations in MultiLing 2013 (ROUGE-1,2,3,4
and three N-gram graph methods) for POLY2 in
three languages.

English: 7th place in all ROUGE metrics
(stemmed) and AutoSummENG, and 8th place in
MeMoG and NPowER (out of 10 systems);

Hebrew: 3rd place in ROUGE-1, 5th place in
ROUGE-2 and MeMoG, 4th rank in ROUGE-3
and ROUGE-4, and only 6th rank in terms of Au-
toSummENG and NPowER (out of 9 participants);

Arabic: 7th rank in ROUGE-1,2 and all NGG
metrics, 6th rank in terms of ROUGE-3, and 5th

place in ROUGE-4 (out of 10 summarizers).
As it can be seen, the best performance for

POLY2 has been achieved on the dataset of He-
brew documents.

Since fractional linear programming problem
can be solved in polynomial time (Karmarkar,
1984), the time complexity of our approach is
polynomial.

48



Acknowledgments

Authors thank Igor Vinokur for the software main-
tenance and running of experiments.

References
Michel Berkelaar. 1999. lp-solve free soft-

ware. http://lpsolve.sourceforge.
net/5.5/.

Dan Gillick and Benoit Favre. 2009. A Scalable
Global Model for Summarization. In Proceedings
of the NAACL HLT Workshop on Integer Linear Pro-
gramming for Natural Language Processing, pages
10–18.

Yoshihiro Matsuo Hitoshi Nishikawa,
Takaaki Hasegawa and Genichiro Kikui. 2010.
Opinion Summarization with Integer Linear Pro-
gramming Formulation for Sentence Extraction and
Ordering. In Coling 2010: Poster Volume, pages
910–918.

N. Karmarkar. 1984. New polynomial-time algorithm
for linear programming. Combinatorica, 4:373–
395.

L. G. Khachiyan and M. J. Todd. 1993. On the com-
plexity of approximating the maximal inscribed el-
lipsoid for a polytope. Mathematical Programming,
61:137–159.

L. G. Khachiyan. 1996. Rounding of polytopes in the
real number model of computation. Mathematics of
Operations Research, 21:307–320.

Samir Khuller, Anna Moss, and Joseph (Seffi) Naor.
1999. The budgeted maximum coverage problem.
Information Precessing Letters, 70(1):39–45.

Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Workshop on
Text Summarization Branches Out, Post-Conference
Workshop of ACL 2004, pages 25–26.

Takuya Makino, Hiroya Takamura, and Manabu Oku-
mura. 2011. Balanced coverage of aspects for text
summarization. In TAC ’11: Proceedings of Text
Analysis Conference.

G. Salton, C. Yang, and A. Wong. 1975. A vector-
space model for information retrieval. Communica-
tions of the ACM, 18.

Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In EACL ’09: Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 781–789.

Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic Generation of Story Highlights. In ACL ’10:
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 565–
574.

49


