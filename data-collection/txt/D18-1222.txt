















































Differentiating Concepts and Instances for Knowledge Graph Embedding


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1971â€“1979
Brussels, Belgium, October 31 - November 4, 2018. cÂ©2018 Association for Computational Linguistics

1971

Differentiating Concepts and Instances for Knowledge Graph Embedding

Xin Lv, Lei Houâˆ—, Juanzi Li, Zhiyuan Liu
Department of Computer Science and Technology,

Tsinghua University, China 100084
{lv-x18@mails.,houlei@,lijuanzi@,liuzy@}tsinghua.edu.cn

Abstract

Concepts, which represent a group of differ-
ent instances sharing common properties, are
essential information in knowledge represen-
tation. Most conventional knowledge embed-
ding methods encode both entities (concepts
and instances) and relations as vectors in a
low dimensional semantic space equally, ig-
noring the difference between concepts and in-
stances. In this paper, we propose a novel
knowledge graph embedding model named
TransC by differentiating concepts and in-
stances. Specifically, TransC encodes each
concept in knowledge graph as a sphere and
each instance as a vector in the same seman-
tic space. We use the relative positions to
model the relations between concepts and in-
stances (i.e., instanceOf), and the rela-
tions between concepts and sub-concepts (i.e.,
subClassOf). We evaluate our model on
both link prediction and triple classification
tasks on the dataset based on YAGO. Ex-
perimental results show that TransC outper-
forms state-of-the-art methods, and captures
the semantic transitivity for instanceOf
and subClassOf relation. Our codes and
datasets can be obtained from https://
github.com/davidlvxin/TransC.

1 Introduction

Knowledge graphs (KGs) aim at semantically rep-
resenting the worldâ€™s truth in the form of machine-
readable graphs composed of triple facts. Knowl-
edge graph embedding encodes each element (en-
tities and relations) in knowledge graph into a
continuous low-dimensional vector space. The
learned representations make the knowledge graph
essentially computable and have been proved to be
helpful for knowledge graph completion and infor-
mation extraction (Bordes et al., 2013; Wang et al.,
2014; Lin et al., 2015b; Ji et al., 2015, 2016).

âˆ—corresponding author

Alice

Bob

Chris

David

Professor

Associate
Professor

Assistant
Professor

Academic
Staff
Member

Staff
Member

InstanceOf

SubClassOf

IsA Transitivity

Instances Concepts

Figure 1: An example of concepts, instances, and isA
transitivity.

In recent years, various knowledge graph em-
bedding methods have been proposed, among
which the translation-based models are simple and
effective with good performances. Inspired by
word2vec (Mikolov et al., 2013), given a triple
(h, r, t), TransE learns vector embeddings h, r
and t which satisfy r â‰ˆ t âˆ’ h. Afterwards,
TransH (Wang et al., 2014), TransR/CTransR (Lin
et al., 2015b) and TransD (Ji et al., 2015), etc
are proposed to address the problem of TransE
when modeling 1-to-N, N-to-1, and N-to-N rela-
tions. As extensions of RESCAL(Nickel et al.,
2011), which is a bilinear model, HolE(Nickel
et al., 2016), DistMult(Yang et al., 2014) and
ComplEx(Trouillon et al., 2016) achieve the state-
of-the-art performances. Meanwhile, there are
also some different methods using a variety of ex-
ternal information such as entity types (Xie et al.,
2016), textual descriptions (Wang and Li, 2016),
as well as logical rules to strengthen representa-
tions of knowledge graphs (Wang et al., 2015; Guo
et al., 2016; RocktaÌˆschel et al., 2015).

However, all these methods ignore to distin-
guish between concepts and instances, and regard
both as entities to make a simplification. Actually,
concepts and instances are organized differently in
many real world datasets like YAGO (Suchanek



1972

et al., 2007), Freebase (Bollacker et al., 2008), and
WordNet (Miller, 1995). Hierarchical concepts in
these knowledge bases provide a natural way to
categorize and locate instances. Therefore, the
common simplification in previous work will lead
to the following two drawbacks:

Insufficient concept representation: Con-
cepts are essential information in knowledge
graph. A concept is a fundamental category of ex-
istence (Rosch, 1973) and can be reified by all of
its actual or potential instances. Figure 1 presents
an example of concepts and instances about uni-
versity staffs. Most knowledge embedding meth-
ods encode both concepts and instances as vectors,
cannot explicitly represent the difference between
concepts and instances.

Lack transitivity of both isA relations:
instanceOf and subClassOf (generally
known as isA) are two special relations in knowl-
edge graph. Different from most other relations,
isA relations exhibit transitivity, e.g., the dotted
lines in Figure 1 represent the facts inferred by
isA transitivity. The indiscriminate vector repre-
sentation for all relations in previous work cannot
reserve this property well (see Section 5.3 for de-
tails).

To address these issues, we propose a novel
translation embedding model named TransC in
this paper. Inspired by (Tenenbaum et al., 2011),
concepts in peopleâ€™s mind are organized hierarchi-
cally and instances should be close to concepts
that they belong to. Hence in TransC, each con-
cept is encoded as a sphere and each instance as
a vector in the same semantic space, and relative
positions are employed to model the relations be-
tween concepts and instances. More specifically,
instanceOf relation is naturally represented by
checking whether an instance vector is inside a
concept sphere. For the subClassOf relation,
we enumerate and quantify four possible relative
positions between two concept spheres. We also
define loss functions to measure the relative posi-
tions and optimize knowledge graph embeddings.
Finally, we incorporate them into translation-
based models to jointly learn the knowledge rep-
resentations of concepts, instances and relations.

Experiments on real world datasets extracted
from YAGO show that TransC outperforms pre-
vious work like TransE, TransD, HolE, DistMult
and ComplEx in most cases. The contributions of
this paper can be summarized as follows:

1. To the best of our knowledge, we are the
first to propose and formalize the problem of
knowledge graph embedding which differen-
tiates between concepts and instances.

2. We propose a novel knowledge embedding
method named TransC, which distinguishes
between concepts and instances and deals
with the transitivity of isA relations.

3. We construct a new dataset based on YAGO
for evaluation. Experiments on link pre-
diction and triple classification demonstrate
that TransC successfully addresses the above
problems and outperforms state-of-the-art
methods.

2 Related Work

There are a variety of models for knowledge graph
embedding. We divide them into three kinds and
introduce them respectively.

2.1 Translation-based Models

TransE (Bordes et al., 2013) regards a relation r
as a translation from h to t for a triple (h, r, t) in
training set. The vector embeddings of this triple
should satisfy h + r â‰ˆ t. Hence, t should be the
nearest neighbor of h+ r, and the loss function is

fr(h, t) = ||h+ râˆ’ t||22. (1)

TransE is suitable for 1-to-1 relations, but it has
problems when handling 1-to-N, N-to-1, and N-
to-N relations.
TransH (Wang et al., 2014) attempts to alleviate
the problems of TransE above. It regards a relation
vector r as a translation on a hyperplane with wr
as the normal vector. The vector embeddings will
be first projected to the hyperplane of relation r
and get hâŠ¥ = hâˆ’w>r hwr and tâŠ¥ = tâˆ’w>r twr.
The loss function of TransH is

fr(h, t) = ||hâŠ¥ + râˆ’ tâŠ¥||22. (2)

TransR/CTransR (Lin et al., 2015b) addresses
the issue in TransE and TransH that some entities
are similar in the entity space but comparably dif-
ferent in other specific aspects. It sets a transfer
matrix Mr for each relation r to map entity em-
bedding to relation vector space. Its loss function
is

fr(h, t) = ||Mrh+ râˆ’Mrt||22. (3)



1973

TransD (Ji et al., 2015) considers the different
types of entities and relations at the same time.
Each relation-entity pair (r, e) will have a map-
ping matrix Mre to map entity embedding into
relation vector space. And the projected vectors
could be defined as hâŠ¥ = Mrhh and tâŠ¥ = Mrtt.
The loss function of TransD is

fr(h, t) = ||hâŠ¥ + râˆ’ tâŠ¥||22. (4)

There are many other translation-based models
in recent years. For example, TranSparse (Ji et al.,
2016) simplifies TransR by enforcing the sparse-
ness on the projection matrix, PTransE (Lin et al.,
2015a) considers relation paths as translations be-
tween entities for representation learning, (Xiao
et al., 2016a) proposes a manifold-based embed-
ding principle (ManifoldE) for precise link predic-
tion, TransF (Feng et al., 2016) regards relation as
translation between head entity vector and tail en-
tity vector with flexible magnitude, (Xiao et al.,
2016b) proposes a new generative model TransG,
and KG2E (He et al., 2015) uses Gaussian embed-
ding to model the data uncertainty. All these mod-
els can be seen in (Wang et al.).

2.2 Bilinear Models

RESCAL(Nickel et al., 2011) is the first bilinear
model. It associates each entity with a vector to
capture its latent semantics. Each relation is rep-
resented as a matrix which models pairwise inter-
actions between latent factors.

Many extensions of RESCAL have been pro-
posed by restricting bilinear functions in recent
years. For example, DistMult (Yang et al.,
2014) simplifies RESCAL by restricting the ma-
trices representing relations to diagonal matrices.
HolE(Nickel et al., 2016) combines the expressive
power of RESCAL with the efficiency and sim-
plicity of DistMult. It represents both entities and
relations as vectors in Rd. ComplEx(Trouillon
et al., 2016) extends DistMult by introducing
complex-valued embeddings so as to better model
asymmetric relations.

2.3 External Information Learning Models

External information like textual information is
significant for knowledge representation. TEKE
(Wang and Li, 2016) uses external context infor-
mation in a text corpus to represent both entities
and words into a joint vector space with alignment
models. DKRL (Xie et al., 2016) directly learns

entity representations from entity descriptions.
(Wang et al., 2015; Guo et al., 2016; RocktaÌˆschel
et al., 2015) use logical rules to strengthen repre-
sentations of knowledge graphs.

All models above do not differentiate between
concepts and instances. To the best of our knowl-
edge, our proposed TransC is the first attempt
which represents concepts, instances, and rela-
tions differently in the same space.

3 Problem Formulation

In this section, we formulate the problem of
knowledge graph embedding with concepts and
instances. Before that, we first introduce the in-
put knowledge graph.

Knowledge Graph KG describes concepts, in-
stances, and the relations between them. It can
be formalized as KG = {C, I,R,S}. C and
I denote the sets of concepts and instances re-
spectively. Relation set R can be formalized as
R = {re, rc} âˆª Rl, where re is an instanceOf
relation, rc is a subClassOf relation, and Rl is
the instance relation set. Therefore, the triple set
S can be divided into three disjoint subsets:

1. InstanceOf triple set Se =
{(i, re, c)k}nek=1, where i âˆˆ I is an in-
stance, c âˆˆ C is a concept, and ne is the size
of Se.

2. SubClassOf triple set Sc =
{(ci, rc, cj)k}nck=1, where ci, cj âˆˆ C are
concepts, ci is a sub-concept of cj , and nc is
the size of Sc.

3. Relational triple set Sl = {(h, r, t)k}nlk=1,
where h, r âˆˆ I are head instance and tail in-
stance, r âˆˆ Rl is an instance relation, and nl
is the size of Sl.

Given knowledge graph KG, knowledge graph
embedding with concepts and instances aims at
learning embeddings for instances, concepts, and
relations in the same space Rk. For each concept
c âˆˆ C, we learn a sphere s(p,m) with p âˆˆ Rk and
m denoting the sphere center and radius. For each
instance i âˆˆ I and instance relation r âˆˆ Rl, we
learn a low-dimensional vector i âˆˆ Rk and r âˆˆ Rk
respectively. Specifically, the instanceOf and
subClassOf representations are well-designed
so that the transitivity of isA relations can be
reserved, namely, instanceOf-subClassOf



1974

ğ‘ "
ğ‘ "

ğ‘ "ğ‘ # ğ‘ #

ğ‘ #

ğ‘‘ ğ‘‘ğ‘‘

ğ‘ 		ğ‘‘	 â‰¥ 	 |ğ‘š" +	ğ‘š#| ğ‘ 		|ğ‘š" 	âˆ’	ğ‘š# | â‰¤ 	ğ‘‘	 < |ğ‘š" +	ğ‘š#|	 ğ‘‘ 		ğ‘‘	 < |ğ‘š" âˆ’	ğ‘š#|	 âˆ§ ğ‘š" 	â‰¥	ğ‘š#

ğ‘ "

ğ‘ #

ğ‘‘

ğ‘ 		ğ‘‘	 < |ğ‘š" âˆ’	ğ‘š#|	 âˆ§ ğ‘š" 	<	ğ‘š#	(ğºğ‘‚ğ´ğ¿)

ğ‘š"

ğ‘š7

ğ‘š" ğ‘š7

ğ‘š#
ğ‘š"

ğ‘š7

ğ‘š"

Figure 2: Four relative positions between sphere si and sj .

transitivity shown in Equation 5:

(i, re, c1) âˆˆ Se âˆ§ (c1, rc, c2) âˆˆ Sc â†’ (i, re, c2) âˆˆ Se, (5)

and subClassOf-subClassOf transitivity
shown in Equation 6:

(c1, rc, c2) âˆˆ Sc âˆ§ (c2, rc, c3) âˆˆ Sc â†’ (c1, rc, c3) âˆˆ Sc. (6)

Based on the definition, how to model concepts
and isA relations is critical to solve this problem.

4 Our Approach

To differentiate between concepts and instances
for knowledge graph embedding, we propose a
novel method named TransC. We define different
loss functions to measure the relative positions in
embedding space, and then jointly learn the rep-
resentations of concepts, instances, and relations
based on the translation-based models.

4.1 TransC
We have three kinds of triples in our triple set S
and define different loss function for them respec-
tively.

InstanceOf Triple Representation. For a
given instanceOf triple (i, re, c), if it is a true
triple, i should be inside the sphere s to represent
the instanceOf relation between them. Actu-
ally, there is another relative position that i is out-
side the sphere s. In this condition, the embed-
dings still need to be optimized. The loss function
is defined as

fe(i, c) = ||iâˆ’ p||2 âˆ’m. (7)

SubClassOf Triple Representation. For a
subClassOf triple (ci, rc, cj), just like before,
concepts ci, cj are encoded as spheres si(pi,mi)
and sj(pj ,mj). We first denote the distance be-
tween the centers of the two spheres as

d = ||pi âˆ’ pj ||2. (8)

If (ci, rc, cj) is a true triple, sphere si should
be inside sphere sj (Figure 2a) to represent the
subClassOf relation between them. Actually,
there are three other relative positions between
sphere si and sj (as shown in Figure 2). We also
have three loss functions under these three condi-
tions:

1. si is separate from sj (Figure 2b). The em-
beddings still need to be optimized. In this
condition, the two spheres need to get closer
in optimalization. Therefore, the loss func-
tion is defined as

fc(ci, cj) = ||pi âˆ’ pj ||2 +mi âˆ’mj . (9)

2. si intersects with sj (Figure 2c). This condi-
tion is similar to condition 1. The loss func-
tion is defined as

fc(ci, cj) = ||pi âˆ’ pj ||2 +mi âˆ’mj . (10)

3. sj is inside si (Figure 2d). It is different from
our target and we should reduce mj and in-
crease mi. Hence, the loss function is

fc(ci, cj) = mi âˆ’mj . (11)

Relational Triple Representation. For a re-
lational triple (h, r, t), TransC will learn low-
dimensional vectors h, t, r âˆˆ Rk for instances and
relations. Just like TransE (Bordes et al., 2013),
the loss function of this kind of triples is defined
as

fr(h, t) = ||h+ râˆ’ t||22. (12)

After having embeddings above, TransC can
easily deal with the transitivity of isA relations.
If we have true triples (i, re, ci) and (ci, rc, cj),
which means i is inside the sphere si and si is in-
side sj , we can get a result that i is also inside the
sphere sj . It can be concluded that (i, re, cj) is a



1975

true triple and TransC can handle instanceOf-
subClassOf transitivity. Similarly, if we have
true triples (ci, rc, cj) and (cj , rc, ck), we can get
a result that sphere si is inside sphere sk. It means
(ci, re, ck) is a true triple and TransC can deal with
subClassOf-subClassOf transitivity.

In experiments, we enforce constrains as
||h||2 â‰¤ 1, ||r||2 â‰¤ 1, ||t||2 â‰¤ 1 and ||p||2 â‰¤ 1.

4.2 Training Method
For instanceOf triples, we use Î¾ and Î¾â€² to de-
note a positive triple and a negative triple. Se and
S â€²e are used to describe the positive triple set and
negative triple set. Then we can define a margin-
based ranking loss for instanceOf triples:

Le =
âˆ‘
Î¾âˆˆSe

âˆ‘
Î¾â€²âˆˆSâ€²e

[Î³e + fe(Î¾)âˆ’ fe(Î¾â€²)]+, (13)

where [x]+ , max (0, x) and Î³e is the margin sep-
arating positive triplets and negative triplets. Sim-
ilarly, for subClassOf triples, we will have a
ranking loss:

Lc =
âˆ‘
Î¾âˆˆSc

âˆ‘
Î¾â€²âˆˆSâ€²c

[Î³c + fc(Î¾)âˆ’ fc(Î¾â€²)]+, (14)

and for relational triples, we will have a ranking
loss:

Ll =
âˆ‘
Î¾âˆˆSl

âˆ‘
Î¾â€²âˆˆSâ€²l

[Î³l + fr(Î¾)âˆ’ fr(Î¾â€²)]+. (15)

Finally, we define the overall loss function as lin-
ear combinations of these three functions:

L = Le + Lc + Ll. (16)

The goal of training TransC is to minimize the
above function, and iteratively update embeddings
of concepts, instances, and concepts.

Every triple in our training set has a label to
indicate whether the triple is positive or negative.
But existing knowledge graph only contains posi-
tive triples. We need to generate negative triples
by corrupting positive triples. For a relational
triple (h, r, t), we replace h or t to generate a
negative triple (hâ€², r, t) or (h, r, tâ€²). For exam-
ple, we get hâ€² by randomly picking from a set
Mt =M1âˆªM2âˆªÂ· Â· Â·âˆªMn, where n is the num-
ber of concepts that t belongs to andMi = {a|a âˆˆ
I âˆ§ (a, re, ci) âˆˆ Se âˆ§ (t, re, ci) âˆˆ Se âˆ§ t 6= a}.
For the other two kinds of triples, we follow the
same policy to generate negative triples. We also
use two strategies â€œunifâ€ and â€œbernâ€ described in
(Wang et al., 2014) to replace instances or con-
cepts.

DataSets YAGO39K M-YAGO39K
#Instance 39,374 39,374
#Concept 46,110 46,110
#Relation 39 39

#Relational Triple 354,997 354,997
#InstanceOf Triple 442,836 442,836
#SubClassOf Triple 30,181 30,181

#Valid (Relational Triple) 9,341 9,341
#Test (Relational Triple) 9,364 9,364

#Valid (InstanceOf Triple) 5,000 8,650
#Test (InstanceOf Triple) 5,000 8,650
#Valid (SubClassOf Triple) 1,000 1,187
#Test (SubClassOf Triple) 1,000 1,187

Table 1: Statistics of YAGO39K and M-YAGO39K.

5 Experiments and Analysis

We evaluate our method on two typical tasks com-
monly used in knowledge graph embedding: link
prediction (Bordes et al., 2013) and triple classifi-
cation (Socher et al., 2013).

5.1 Datasets
Most previous work used FB15K and WN18 (Bor-
des et al., 2013) for evaluation. But these two
datasets are not suitable for our model because
FB15K mainly consists of instances and WN18
mainly contains concepts. Therefore, we use an-
other popular knowledge graph YAGO (Suchanek
et al., 2007) for evaluation, which contains a lot
of concepts from WordNet and instances from
Wikipedia. We construct a subset of YAGO named
YAGO39K for evaluation through the following
steps:

(1) We randomly select some relational triples
like (h, r, t) from the whole YAGO dataset as our
relational triple set Sl.

(2) For every instance and instance relation ex-
isted in our relational triples, we save it to con-
struct instance set I and instance relation set Rl
respectively.

(3) For every instanceOf triple (i, re, c) in
YAGO, if i âˆˆ I, we save this triple to construct
instanceOf triple set Se.

(4) For every concept existed in instanceOf
triple set Se, we save it to construct concept set C.

(5) For every subClassOf triple (ci, rc, cj) in
YAGO, if ci âˆˆ C âˆ§ cj âˆˆ C, we save this triple to
construct subClassOf triple set Sc.

(6) Finally, we achieve our triple set S = Se âˆª
Sc âˆª Sl and our relation setR = {re, rc} âˆª Rl.

To evaluate every modelâ€™s performance in han-
dling the transitivity of isA relations, we generate
some new triples based on YAGO39K using the
transitivity of isA relations. These new triples will



1976

Experiments Link Prediction Triple Classification(%)

Metric MRR Hits@N(%) Accuracy Precision Recall F1-Score
Raw Filter 1 3 10

TransE 0.114 0.248 12.3 28.7 51.1 92.1 92.8 91.2 92.0
TransH 0.102 0.215 10.4 24.0 45.1 90.8 91.2 90.3 90.8
TransR 0.112 0.289 15.8 33.8 56.7 91.7 91.6 91.9 91.7
TransD 0.113 0.176 8.9 19.0 35.4 89.3 88.1 91.0 89.5
HolE 0.063 0.198 11.0 23.0 38.4 92.3 92.6 91.9 92.3

DistMult 0.156 0.362 22.1 43.6 66.0 93.5 93.9 93.0 93.5
ComplEx 0.058 0.362 29.2 40.7 48.1 92.8 92.6 93.1 92.9

TransC (unif) 0.087 0.421 28.3 50.0 69.2 93.5 94.3 92.6 93.4
TransC (bern) 0.112 0.420 29.8 50.2 69.8 93.8 94.8 92.7 93.7

Table 2: Experimental results on link prediction and triple classification for relational triples. Hits@N uses results
of â€œFilterâ€ evaluation setting.

be added to valid and test datasets of YAGO39K
to create a new dataset named M-YAGO39K. Spe-
cific steps are described as follows:

(1) For every instanceOf triple (i, re, c) in
valid and test dataset, if (c, rc, cj) exists in train-
ing dataset, we save a new instanceOf triple
(i, re, cj).

(2) For every subClassOf triple (ci, rc, cj) in
valid and test dataset, if (cj , rc, ck) exists in train-
ing dataset, we save a new subClassOf triple
(ci, rc, ck).

(3) We add these new triples to valid and test
dataset of YAGO39K to get M-YAGO39K.

The statistics of YAGO39K and M-YAGO39K
are shown in Table 1.

5.2 Link Prediction

Link Prediction aims to predict the missing h or t
for a relational triple (h, r, t). In this task, we need
to give a ranking list of candidate instances from
the knowledge graph, instead of only giving one
best result.

For every test relational triple (h, r, t), we re-
move the head or tail instance and replace it with
all instances existed in knowledge graph, and rank
these instances in ascending order of distances cal-
culated by loss function fr. Just like (Bordes
et al., 2013), we use two evaluation metrics in this
task: (1) the mean reciprocal rank of all correct in-
stances (MRR) and (2) the proportion of correct
instances that rank no larger than N (Hits@N).
A good embedding model should achieve a high
MRR and a high Hits@N. We note that a corrupted
triple may also exist in knowledge graph, which
should also be regarded as a correct prediction.
However, the above evaluations do not handle this
issue and may underestimate the results. Hence,
we filter out every triple appeared in our knowl-
edge graph before getting the ranking list. The first

evaluation setting is called â€œRawâ€ and the second
one is called â€œFilter.â€ We report the experiment re-
sults on both settings.

In this task, we use dataset YAGO39K for eval-
uation. We select learning rate Î» for SGD among
{0.1, 0.01, 0.001}, the three margins Î³l, Î³e and Î³c
among {0.1, 0.3, 0.5, 1, 2}, the dimension of in-
stance vectors and relation vectors n among {20,
50, 100}. The best configurations are determined
according to the Hits@10 in valid set. The opti-
mal configurations are: Î³l = 1, Î³e = 0.1, Î³c = 1,
Î» = 0.001, n = 100 and taking L2 as dissimilar-
ity. We train every model for 1000 rounds in this
task.

Evaluation results on YAGO39K are shown in
Table 2. From the table, we can conclude that: (1)
TransC significantly outperforms other models in
terms of Hits@N. This indicates that TransC can
use isA triplesâ€™ information better than other mod-
els, which is helpful for instance representation
learning. (2) TransC performs a little bit worse
than DistMult in some settings. The reason may
be that we determine the best configurations only
according to the Hits@10, which may lead to a
low MRR. (3) The â€œbernâ€ sampling trick works
well for TransC.

5.3 Triple Classification

Triple Classification aims to judge whether a given
triple is correct or not, which is a binary classifica-
tion task. This triple can be a relational triple, an
instanceOf triple or a subClassOf triple.

Negative triples are needed for evaluation of
binary classification. Hence, we construct some
negative triples following the same setting in
(Socher et al., 2013). There are as many true
triples as negative triples in both valid and test set.

For triple classification, we set a threshold Î´r
for every relation r. For a given test triple, if its



1977

Datasets YAGO39K M-YAGO39K
Metric Accuracy Precision Recall F1-Score Accuracy Precision Recall F1-Score
TransE 82.6 83.6 81.0 82.3 71.0â†“ 81.4â†“ 54.4â†“ 65.2â†“
TransH 82.9 83.7 81.7 82.7 70.1â†“ 80.4â†“ 53.2â†“ 64.0â†“
TransR 80.6 79.4 82.5 80.9 70.9â†“ 73.0â†“ 66.3â†“ 69.5â†“
TransD 83.2 84.4 81.5 82.9 72.5â†“ 73.1â†“ 71.4â†“ 72.2â†“
HolE 82.3 86.3 76.7 81.2 74.2â†“ 81.4â†“ 62.7â†“ 70.9â†“

DistMult 83.9 86.8 80.1 83.3 70.5â†“ 86.1â†“ 49.0â†“ 62.4â†“
ComplEx 83.3 84.8 81.1 82.9 70.2â†“ 84.4â†“ 49.5â†“ 62.4â†“

TransC (unif) 80.2 81.6 80.0 79.7 85.5â†‘ 88.3â†‘ 81.8â†‘ 85.0â†‘
TransC (bern) 79.7 83.2 74.4 78.6 85.3â†‘ 86.1â†‘ 84.2â†‘ 85.2â†‘

Table 3: Experimental results on instanceOf triple classification(%).

Datasets YAGO39K M-YAGO39K
Metric Accuracy Precision Recall F1-Score Accuracy Precision Recall F1-Score
TransE 77.6 72.2 89.8 80.0 76.9â†“ 72.3â†‘ 87.2â†“ 79.0â†“
TransH 80.2 76.4 87.5 81.5 79.1â†“ 72.8â†“ 92.9â†‘ 81.6â†‘
TransR 80.4 74.7 91.9 82.4 80.0â†“ 73.9â†“ 92.9â†‘ 82.3â†“
TransD 75.9 70.6 88.8 78.7 76.1â†‘ 70.7â†‘ 89.0â†‘ 78.8â†‘
HolE 70.5 73.9 63.3 68.2 66.6â†“ 72.3â†“ 53.7â†“ 61.7â†“

DistMult 61.9 68.7 43.7 53.4 60.7â†“ 71.7â†‘ 35.5â†“ 47.7â†“
ComplEx 61.6 71.5 38.6 50.1 59.8â†“ 65.6â†“ 41.4â†‘ 50.7â†‘

TransC (unif) 82.9 77.1 93.7 84.6 83.0â†‘ 77.5â†‘ 93.1â†“ 84.7â†‘
TransC (bern) 83.7 78.1 93.9 85.2 84.4â†‘ 80.7â†‘ 90.4â†“ 85.3â†‘

Table 4: Experimental results on subClassOf triple classification(%).

loss function is smaller than Î´r, it will be classi-
fied as positive, otherwise negative. Î´r is obtained
by maximizing the classification accuracy on valid
set.

In this task, we use dataset YAGO39K and M-
YAGO39K for evaluation. Parameters are selected
in the same way as in link prediction. The best
configurations are determined by accuracy in valid
set. The optimal configurations for YAGO39K
are: Î³l = 1, Î³e = 0.1, Î³c = 0.1, Î» = 0.001,
n = 100 and taking L2 as dissimilarity. The opti-
mal configurations for M-YAGO39K are: Î³l = 1,
Î³e = 0.1, Î³c = 0.3, Î» = 0.001, n = 100 and
taking L2 as dissimilarity. For both datasets, we
traverse all the training triples for 1000 rounds.

Our datasets have three kinds of triples. Hence,
we do experiments on them respectively. Experi-
mental results for relational triples, instanceOf
triples, and subClassOf triples are shown in Ta-
ble 2, Table 3, and Table 4 respectively. In Table
3 and Table 4, a rising arrow means performance
of this model have a promotion from YAGO39K
to M-YAGO39K and a down arrow means a drop.

From Table 2, we can learn that: (1) TransC out-
performs all previous work in relational triple clas-
sification. (2) The â€œbernâ€ sampling trick works
better than â€œunifâ€ in TransC.

From Table 3 and Table 4, we can conclude that:
(1) On YAGO39K, some compared models per-
form better than TransC in instanceOf triple

classification. This is because that instanceOf
has most triples (53.5%) among all relations in
YAGO39K. This relation is trained superabundant
times and nearly achieves the best performance,
which has an adverse effect on other triples.
TransC can find a balance between them and
all triples achieve a good performance. (2) On
YAGO39K, TransC outperforms other models in
subClassOf triple classification. As shown
in Table 1, subClassOf triples are much less
than instanceOf triples. Hence, other models
can not achieve the best performance under the
bad influence of instanceOf triples. (3) On
M-YAGO39K, TransC outperforms previous
work in both instanceOf triple classification
and subClassOf triple classification, which
indicates that TransC can handle the transitivity
of isA relations much better than other models.
(4) After comparing experimental results in
YAGO39K and M-YAGO39K, we can find that
most previous workâ€™s performance suffers a big
drop in instanceOf triple classification and a
small drop in subClassOf triple classification.
This shows that previous work can not deal with
instanceOf-subClassOf transitivity well.
(5) In TransC, nearly all performances have a
significant promotion from YAGO39K to M-
YAGO39K. Both instanceOf-subClassOf
transitivity and subClassOf-subClassOf
transitivity are solved well in TransC.



1978

5.4 Case Study

We have shown that TransC have a good per-
formance for knowledge graph embedding and
dealing with transitivity of isA relations. In this
section, we present an example of finding new
instanceOf triples and subClassOf triples
using results of TransC.

As shown in Figure 3, New York City is an in-
stance and others are concepts. The solid lines
represent the triples from our datasets and the dot-
ted lines represent the facts inferred by our model.
TransC can find two new instanceOf triples
(New York City, instanceOf, City) and (New
York City, instanceOf, Municipality). It can
also find a new subClassOf triple (Port Cities,
subClassOf, City). Following the transitivity
of isA relations, we can know all these three new
triples are right. Unfortunately, most previous
work regards these three triples as wrong, which
means they can not handle transitivity of isA rela-
tions well.

New York
City

Port cities

City

Municipality

Northeastern
United States

InstanceOf

SubClassOf

IsA Transitivity

Figure 3: An inference example of TransC.

6 Conclusion and Future Work

In this paper, we propose a new knowledge em-
bedding model named TransC. TransC embeds in-
stances, concepts, and relations in the same space
to deal with the transitivity of isA relations. We
create a new dataset YAGO39K for evaluation.
Experiment results show that TransC outperforms
previous translation-based models in most cases.
Besides, It can also handle the transitivity of isA
relations much better than other models. In our fu-
ture work, we will explore the following research
directions: (1) Sphere is a simple model to repre-
sent a concept in semantic space, but it still have
some limits since it is too naive. we will try to
find a more expressive model instead of spheres to
represent concepts. (2) A concept may have dif-
ferent meanings in different triples. We will try to

use several typical vectors of instances as a con-
ceptâ€™s centers to represent different meanings of a
concept. Then a concept can have different em-
beddings in different triples.

Acknowledgments

The work is supported by NSFC key project (No.
61533018, U1736204, 61661146007), Ministry of
Education and China Mobile Research Fund (No.
20181770250), and THUNUS NExT Co-Lab.

References
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim

Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In NIPS.

Jun Feng, Minlie Huang, Mingdong Wang, Mantong
Zhou, Yu Hao, and Xiaoyan Zhu. 2016. Knowledge
graph embedding by flexible translation. In KR.

Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and
Li Guo. 2016. Jointly embedding knowledge graphs
and logical rules. In EMNLP.

Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015.
Learning to represent knowledge graphs with gaus-
sian embedding. In CIKM.

Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun
Zhao. 2015. Knowledge graph embedding via dy-
namic mapping matrix. In ACL.

Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. 2016.
Knowledge graph completion with adaptive sparse
transfer matrix. In AAAI.

Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun,
Siwei Rao, and Song Liu. 2015a. Modeling rela-
tion paths for representation learning of knowledge
bases. In EMNLP.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015b. Learning entity and relation em-
beddings for knowledge graph completion. In AAAI.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM.

Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog-
gio, et al. 2016. Holographic embeddings of knowl-
edge graphs. In AAAI.



1979

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML.

Tim RocktaÌˆschel, Sameer Singh, and Sebastian Riedel.
2015. Injecting logical background knowledge into
embeddings for relation extraction. In NAACL.

Eleanor H. Rosch. 1973. Natural categories. Cognitive
Psychology, 4(3):328â€“350.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
NIPS.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.

Joshua B Tenenbaum, Charles Kemp, Thomas L Grif-
fiths, and Noah D Goodman. 2011. How to grow a
mind: Statistics, structure, and abstraction. science,
331(6022):1279â€“1285.

TheÌo Trouillon, Johannes Welbl, Sebastian Riedel, EÌric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In ICML.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
Knowledge graph embedding: A survey of ap-
proaches and applications. IEEE Transactions on
Knowledge and Data Engineering.

Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge
base completion using embeddings and rules. In IJ-
CAI.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI.

Zhigang Wang and Juan-Zi Li. 2016. Text-enhanced
representation learning for knowledge graph. In IJ-
CAI.

Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016a.
From one point to a manifold: knowledge graph em-
bedding for precise link prediction. In AAAI.

Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016b.
Transg: A generative model for knowledge graph
embedding. In ACL.

Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and
Maosong Sun. 2016. Representation learning of
knowledge graphs with entity descriptions. In AAAI.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575.


