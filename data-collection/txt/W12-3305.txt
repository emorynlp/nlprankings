










































Evaluating Unsupervised Ensembles when applied to Word Sense Induction


Proceedings of the 2012 Student Research Workshop, pages 25–30,

Jeju, Republic of Korea, 8-14 July 2012. c©2012 Association for Computational Linguistics

Evaluating Unsupervised Ensembles when applied to Word Sense Induction

Keith Stevens1,2
1University of California Los Angeles; Los Angeles , California, USA

2Lawrence Livermore National Lab; Livermore, California, USA∗

kstevens@cs.ucla.edu

Abstract

Ensembles combine knowledge from distinct
machine learning approaches into a general
flexible system. While supervised ensembles
frequently show great benefit, unsupervised
ensembles prove to be more challenging. We
propose evaluating various unsupervised en-
sembles when applied to the unsupervised task
of Word Sense Induction with a framework for
combining diverse feature spaces and cluster-
ing algorithms. We evaluate our system us-
ing standard shared tasks and also introduce
new automated semantic evaluations and su-
pervised baselines, both of which highlight the
current limitations of existing Word Sense In-
duction evaluations.

1 Introduction

Machine learning problems often benefit from many
differing solutions using ensembles (Dietterich,
2000) and supervised Natural Language Processing
tasks have been no exception. However, use of un-
supervised ensembles in NLP tasks has not yet been
rigorously evaluated. Brody et al. (2006) first con-
sidered unsupervised ensembles by combining four
state of the art Word Sense Disambiguation systems
using a simple voting scheme with much success.
Later, Brody and Lapata (2009) combined different
feature sets using a probabilistic Word Sense Induc-
tion model and found that only some combinations
produced an improved system. These early and lim-
ited evaluations show both the promise and draw-
back of combining different unsupervised models:

∗This work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National Lab-
oratory under Contract DE-AC52-07NA27344 (LLNL-CONF-
530791).

particular combinations provide a benefit but select-
ing these combinations is non-trivial.

We propose applying a new and more gen-
eral framework for combining unsupervised systems
known as Ensemble Clustering to unsupervised NLP
systems and focus on the fully unsupervised task
of Word Sense Induction. Ensemble Clustering can
combine together multiple and diverse clustering al-
gorithms or feature spaces and has been shown to
noticeably improve clustering accuracy for both text
based datasets and other datasets (Monti et al., 2003;
Strehl et al., 2002). Since Word Sense Induction is
fundamentally a clustering problem, with many vari-
ations, it serves well as a NLP case study for Ensem-
ble Clustering.

The task of Word Sense Induction extends the
problem of Word Sense Disambiguation by simply
assuming that a model must first learn and define a
sense inventory before disambiguating multi-sense
words. This induction step frees the disambiguation
process from any fixed sense inventory and can in-
stead flexibly define senses based on observed pat-
terns within a dataset (Pedersen, 2006). However,
this induction step has proven to be greatly chal-
lenging, in the most recent shared tasks, induction
systems either appear to perform poorly or fail to
outperform the simple Most Frequent Sense baseline
(Agirre and Soroa, 2007a; Manandhar et al., 2010).

In this work, we propose applying Ensemble
Clustering as a general framework for combining
not only different feature spaces but also a variety of
different clustering algorithms. Within this frame-
work we will explore which types of models should
be combined and how to best combine them. In ad-
dition, we propose two new evaluations: (1) new se-
mantic coherence measures that evaluate the seman-

25



Figure 1: The Ensemble Clustering model: individ-
ual clustering algorithms partition perturbations of
the dataset and all partitions are combined via a con-
sensus function to create a final solution, π∗.

tic quality and uniqueness of induced word senses
without referring to an external sense inventory (2)
and a new set of baseline systems based on super-
vised learning algorithms. With the new evaluations
and a framework for combining general induction
models, we intend to find not only improved models
but a better understanding of how to improve later
induction models.

2 Consensus Clustering

Ensemble Clustering presents a new method for
combining together arbitrary clustering algorithms
without any supervision (Monti et al., 2003; Strehl
et al., 2002). The method adapts simple boosting
and voting approaches from supervised ensembles
to merge together diverse clustering partitions into
a single consensus solution. Ensemble Clustering
forms a single consensus partition by processing a
data set in two steps: (1) create a diverse set of en-
sembles that each partition some perturbation of the
full dataset and (2) find the median partition that best
agrees with each ensemble’s partition. Figure 1 vi-
sually displays these two steps.

Variation in these two steps accounts for the wide
variety of Ensemble Clustering approaches. Each
ensemble can be created from either a large col-
lection of distinct clustering algorithms or through
a boosting approach where the same algorithm is
trained on variations of the dataset. Finding the me-
dian partition turns out to be an NP-Complete prob-
lem under most settings (Goder and Filkov, 2008)
and thus must be approximated with one of sev-
eral heuristics. We consider several well tested ap-

proaches to both steps.
Formally, we define Ensemble Clustering to

operate over a dataset of N elements: D =
{d1, . . . , dN}. Ensemble Clustering then creates H
ensembles that each partition a perturbation Dh of D
to create H partitions, Π = {π1, · · · , πH}. The con-
sensus algorithm then approximates the best consen-
sus partition π∗ that satisfies:

argmin
π∗

∑

πh∈Π

d(πh, π
∗) (1)

according to some distance metric d(πi, πj) between
two partitions. We use the symmetric difference dis-
tance as d(πi, πj). Let Pi be the set of co-cluster
data points in πi. The distance metric is then defined
to be

d(π1, π2) = |P1 \ P2|+ |P2 \ P1|

2.1 Forming Ensembles

Ensemble clustering can combine together overlap-
ping decisions from many different clustering algo-
rithms or it can similarly boost the performance of a
single algorithm by using different parameters. We
consider two simple formulations of ensemble cre-
ation: Homogeneous Ensembles and Heterogeneous
Ensembles. We secondly consider approaches for
combining the two creation methods.

Homogeneous Ensembles partition randomly
sampled subsets of the data points from D without
replacement. By sampling without replacement,
each ensemble will likely see different representa-
tions of each cluster and can specialize its partition
the around observed subset. Furthermore, each
ensemble will observe less noise and can better
define each true cluster (Monti et al., 2003). We
note that since each ensemble only observes an
incomplete subset of D, some clusters may not be
represented at all in some partitions.

Heterogeneous Ensembles create diverse parti-
tions by simply using complete partitions over D
from different clustering algorithms, either due to
different parameters or due to completely different
clustering models (Strehl et al., 2002).

26



Combined Heterogeneous and Homogeneous En-
sembles can be created by creating many homo-
geneous variations of each distinct clustering algo-
rithm within a heterogeneous ensemble. In this
framework, each single method can be boosted by
subsampling the data in order to observe the true
clusters and then combined with other algorithms
using differing cluttering criteria.

2.2 Combining data partitions

Given the set of partitions, Π = {πi, · · · , πh}, the
consensus algorithm must find a final partition, π∗

that best minimizes Equation 1. We find an approx-
imation to π∗ using the following algorithms.

Agglomerative Clustering first creates a consen-
sus matrix, M that records the aggregate decisions
made by each partition. Formally, M records the
fraction of partitions that observed two data point
and assigned them to the same cluster:

M(i, j) =

∑h
k=1 1{di, dj ∈ π

c
k}∑h

k=1 1{di, dj ∈ πk}

Where di refers to element i, πck refers to cluster c in
partition πk, and 1{∗} is the indicator function. The
consensus partition, π∗ is then the result of creat-
ing C partitions with Agglomerative Clustering us-
ing the Average Link criterion and M as the simi-
larity between each data point (Monti et al., 2003).

Best of K simply sets π∗ as the partition πh ∈ Π
that minimizes Equation 1 (Goder and Filkov, 2008).

Best One Element Move begins with an initial
consensus partition π̂∗ and repeatedly changes the
assignment of a single data point such that Equa-
tion 1 is minimized and repeats until no move can
be found. We initialize this with Best of K.

Filtered Stochastic Best One Element Move
also begins with an initial consensus partition π̂∗ and
repeatedly finds the best one element move, but does
not compare against every partition in Π for each it-
eration. It instead maintains a history of move costs
and updates that history with a stochastically se-
lected partition from Π for each move iteration and
ends after some fixed number of iterations (Zheng et
al., 2011).

Figure 2: The general Word Sense Induction Model:
models extract distributional data from contexts and
induce senses by clustering the extracted informa-
tion. Models then use representations of each sense
to disambiguate new contexts.

3 Word Sense Induction Models

Word Sense Induction models define word senses in
terms of the distributional hypothesis, whereby the
meaning of a word can be defined by the surround-
ing context (Haris, 1985). Rather than form a single
representation for any word, induction models repre-
sent the distinct contexts surrounding a multi-sense
word and find commonalities between the observed
contexts by clustering. These similar contexts then
define a particular word sense and can be used to
later recognize later instances of the sense, Figure 2.

Models can be roughly categorized based on their
context model and their clustering algorithm into
two categories: feature vector methods and graph
methods. Feature vector methods simply transform
each context into a feature vector that records con-
textual information and then cluster with any algo-
rithm that can partition individual data points. Graph
methods build a large distributional graph that mod-
els lexical features from all contexts and then parti-
tions the graph using a graph-based clustering algo-
rithm. In both cases, models disambiguate new uses
of a word by finding the sense with the most features
in common with the new context.

3.1 Context Models

Context models follow the distributional hypothesis
by encoding various lexical and syntactic features
that frequently occur with a multi-sense word. Each
context model records different levels of informa-
tion, and in different formats, but are limited to fea-

27



tures available from syntactic parsing. Below we
summarize our context models which are based on
previous induction systems:

Word Co-occurence (WoC) acts as the core fea-
ture vector method and has been at the core of nearly
all systems that model distributional semantics (Ped-
ersen, 2006). The WoC model represents each con-
text simply as the words within ±W words from
the multi-sense word. Each co-occurring word is
weighted by the number of times it occurs within
the window.

Parts of Speech (PoS) extends the WoC model
by appending each lexical feature with its part of
speech. This provides a simple disambiguation
of each feature so that words with multiple parts
of speech are not conflated into the same feature.
(Brody et al., 2006).

Dependency Relations (DR) restrains word co-
occurrence to words that are reachable from the
multi-sense word via a syntactic parse composed
of dependency relationships limited by some length
(Padó and Lapata, 2007). We treat each reachable
word and the last relation in the path as a feature
(Van de Cruys and Apidianaki, 2011).

Second Order Co-occurrence (SndOrd) provides
a rough compositional approach to representing sen-
tences that utilizes word co-occurrence and partially
solves the data sparsity problem observed with the
WoC model. The SndOrd model first builds a large
distributional vector for each word in a corpus and
then forms context vectors by adding the distribu-
tional vector for each co-occurring context word
(Pedersen, 2006).

Graph models encode rich amounts of linguistic
information for all contexts as a large distributional
graph. Each co-occurring context word is assigned
a node in the graph and edges are formed between
any words that co-occur in the same context. The
graph is refined by comparing nodes and edges to a
large representative corpus and dropping some oc-
currences (Klapaftis and Manandhar, 2010).

Latent Factor Models projects co-occurrence in-
formation into a latent feature space that ties to-
gether relationships between otherwise distinct fea-
tures. We consider three latent models: the Singular

Value Decomposition (SVD) (Schütze, 1998), Non-
negative Matrix Factorization (NMF) (Van de Cruys
and Apidianaki, 2011), and Latent Dirichlet Alloca-
tion (Brody and Lapata, 2009). We note that SVD
and NMF operate as a second step over any feature
vector model whereas LDA is a standalone model.

3.2 Clustering Algorithms

Distributional clustering serves as the main tool
for detecting distinct word senses. Each algorithm
makes unique assumptions about the distribution of
the dataset and should thus serve well as diverse
models, as needed by supervised ensembles (Diet-
terich, 2000). While many WSI models automat-
ically estimate the number of clusters for a word,
we initially simplify our evaluation by assuming the
number of clusters is known a priori and instead fo-
cus on the distinct underlying clustering algorithms.
Below we briefly summarize each base algorithm:

K-Means operates over feature vectors and iter-
atively refines clusters by associating each context
vector with its most representative centroid and then
reformulating the centroid (Pedersen and Kulkarni,
2006).

Hierarchical Agglomerative Clustering can be
applied to both feature vectors and collocation
graphs. In both cases, each sentences or collocation
vertex is placed in their own clusters and then the
two most similar clusters are merged together into a
new cluster (Schütze, 1998).

Spectral Clustering separates an associativity
matrix by finding the cut with the lowest conduc-
tance. We consider two forms of spectral clustering:
EigenCluster (Cheng et al., 2006), a method origi-
nally designed to cluster snippets for search results
into semantically related categories, and GSpec (Ng
et al., 2001), a method that directly clusters a collo-
cation graph.

Random Graph Walks performs a series of ran-
dom walks through a collocation graph in order to
discover nodes that serve as central discriminative
points in the graph and tightly connected compo-
nents in the graph. We consider Chinese Whispers
(Klapaftis and Manandhar, 2010) and a hub selec-
tion algorithm (Agirre and Soroa, 2007b).

28



4 Proposed Evaluation

We first propose evaluating ensemble configurations
of Word Sense Induction models using the standard
shared tasks from SemEval-1 (Agirre and Soroa,
2007a) and SemEval-2 (Manandhar et al., 2010).
We then propose comparing these results, and past
SemEval results, to supervised baselines as a gauge
of how well the algorithms do compared to more in-
formed models. We then finally propose an intrin-
sic evaluation that rates the semantic interpretability
and uniqueness of each induced sense.

Evaluating Ensemble Configurations must be
done to determine which variation of Ensemble
Clustering best applies to the Word Sense Induction
tasks. Preliminary research has shown that Homoge-
neous ensemble combined with the HAC consensus
function typically improve base models while com-
bining heterogeneous induction models greatly re-
duces performance. We thus propose various sets of
ensembles to evaluate whether or not certain context
models or clustering algorithms can be effectively
combined:

1. mixing different feature vector models with the same
clustering algorithm,

2. mixing different clustering algorithms using the same
context model,

3. mixing feature vector context models and graph context
models using matching clustering algorithms,

4. mixing all possible models,
5. and improving each heterogeneous algorithm by first

boosting them with homogeneous ensembles.

SemEval Shared Tasks provide a shared corpus
and evaluations for comparing different WSI Mod-
els. Both shared tasks from SemEval provide a cor-
pus of training data for 100 multi-sense words and
then compare the induced sense labels generated for
a set of test contexts with human annotated sense
using a fixed sense inventory. The task provides two
evaluations: an unsupervised evaluation that treats
each set of induced senses as a clustering solution
and measures accuracy with simple metrics such as
the Paired F-Score, V-Measure, and Adjusted Mu-
tual Information; and a supervised evaluation that
builds a simple supervised word sense disambigua-
tion system using the sense labels (Agirre and Soroa,
2007a; Manandhar et al., 2010).

Supervised Baselines should set an upper limit
on the performance we can expect from most unsu-
pervised algorithms, as has been observed in other
NLP tasks. We train these baselines by using feature
vector models in combination with the SemEval-1
dataset1. We propose several standard supervised
machine learning algorithms as different baselines:
Naive Bayes, Logistic Regression, Decision Trees,
Support Vector Machines, and various ensembles of
each such as simple Bagged Ensembles.

Semantic Coherence evaluations balance the
shared task evaluations by functioning without a
sense inventory. Any evaluation against an existing
inventory cannot accurately measure newly detected
senses, overlapping senses, or different sense gran-
ularities. Therefore, our proposed sense coherence
measures focus on the semantic quality of a sense,
adapted from topic coherence measures (Newman
et al., 2010; Mimno et al., 2011). These evaluate
the degree to which features in an induced sense de-
scribe the meaning of the word sense, where highly
related features constitute a more coherent sense and
unrelated features indicate an incoherent sense. Fur-
thermore, we adapt the coherence metric to evaluate
the amount of semantic overlap between any two in-
duced senses.

5 Concluding Remarks

This research will better establish the benefit of
Ensemble Clustering when applied to unsuper-
vised Natural Language Processing tasks that cen-
ter around clustering by examining which feature
spaces and algorithms can be effectively combined
along with different different ensemble configura-
tions. Furthermore, this work will create new base-
lines that evaluate the inherent challenge of Word
Sense Induction and new automated and knowledge
lean measurements that better evaluate new or over-
lapping senses learned by induction systems. All of
the work will be provided as part of a flexible open
source framework that can later be applied to new
context models and clustering algorithms.

1We cannot use graph context models as they do not model
contexts individually, nor can we use the SemEval-2 dataset be-
cause the training set lacks sense labels needed for training su-
pervised systems

29



References

Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
’07, pages 7–12, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: a
graph based unsupervised system for induction and
classification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
’07, pages 346–349, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 103–
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 97–104, Sydney, Australia, July. Association for
Computational Linguistics.

David Cheng, Ravi Kannan, Santosh Vempala, and Grant
Wang. 2006. A divide-and-merge methodology for
clustering. ACM Trans. Database Syst., 31:1499–
1525, December.

Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Proceedings of the First Interna-
tional Workshop on Multiple Classifier Systems, MCS
’00, pages 1–15, London, UK. Springer-Verlag.

Andrey Goder and Valdimir Filkov, 2008. Consensus
Clustering Algorithms: Comparison and Refinement.,
pages 109–117.

Zellig Haris, 1985. Distributional Structure, pages 26–
47. Oxford University Press.

Ioannis P. Klapaftis and Suresh Manandhar. 2010. Word
sense induction & disambiguation using hierarchical
random graphs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’10, pages 745–755, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. Semeval-2010 task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63–68, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.

David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing

semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262–272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.

Stefano Monti, Pablo Tamayo, Jill Mesirov, and Todd
Golub. 2003. Consensus clustering – a resampling-
based method for class discovery and visualization of
gene expression microarray data. Machine Learning,
52:91–118, July.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
’10, pages 215–224, New York, NY, USA. ACM.

A. Ng, M. Jordan, and Y. Weiss. 2001. On Spectral Clus-
tering: Analysis and an algorithm. In T. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems, pages 849–
856. MIT Press.

Sebastian Padó and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161–199.

Ted Pedersen and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume: demonstrations, NAACL-
Demonstrations ’06, pages 276–279, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Ted Pedersen. 2006. Unsupervised corpus-based meth-
ods for WSD. In Word Sense Disambiguation: Algo-
rithms and Applications, pages 133–166. Springer.

Hinrich Schütze. 1998. Automatic word sense discrimi-
nation. Comput. Linguist., 24:97–123, March.

Alexander Strehl, Joydeep Ghosh, and Claire Cardie.
2002. Cluster ensembles - a knowledge reuse frame-
work for combining multiple partitions. Journal of
Machine Learning Research, 3:583–617.

Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 1476–1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Haipeng Zheng, S.R. Kulkarni, and V.H. Poor. 2011.
Consensus clustering: The filtered stochastic best-
one-element-move algorithm. In Information Sciences
and Systems (CISS), 2011 45th Annual Conference on,
pages 1 –6, march.

30


