



















































Learning to Map Dependency Parses to Abstract Meaning Representations


Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 41–46,
Beijing, China, July 28, 2015. c©2015 Association for Computational Linguistics

Learning to Map Dependency Parses to Abstract Meaning
Representations

Wei-Te Chen
Department of Computer Science
University of Colorado at Boulder
Weite.Chen@colorado.edu

Abstract

Abstract Meaning Representation (AMR)
is a semantic representation language used
to capture the meaning of English sen-
tences. In this work, we propose an AMR
parser based on dependency parse rewrite
rules. This approach transfers dependency
parses into AMRs by integrating the syn-
tactic dependencies, semantic arguments,
named entity and co-reference informa-
tion. A dependency parse to AMR graph
aligner is also introduced as a preliminary
step for designing the parser.

1 Introduction

Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic formalism that
expresses the logical meanings of English sen-
tences in the form of a directed, acyclic graph.
AMR focuses on the semantic concepts (nodes
on the graph), and relations (labeled edges on the
graph) between those concepts. AMR relies heav-
ily on predicate-argument structures defined in the
PropBank (PB) (Palmer et al., 2005). The repre-
sentation encodes rich information, including se-
mantic roles, named entities, and co-reference in-
formation. Fig. 1 shows an example AMR.

In this proposal, we focus on the design of an
automatic AMR parser in a supervised fashion
from dependency parses. In contrast with recent
semantic parsing algorithms, we start the parsing
process from the dependency parses rather than
the sentences. A dependency parse provides both
the semantic dependency information for the sen-
tence, and the structure of the relations between
the head word and their dependencies. These can
provide strong features for semantic parsing. By
using a binary-branching bottom-up shift-reduced
algorithm, the statistical model for the rewrite
rules can be learned discriminatively. Although

Figure 1: The AMR annotation of sentence “Pierre
Vinken, 61 years old, will join the board as a
nonexecutive director Nov. 29.”

the AMR parser is my thesis topic, in this proposal
we will pay more attention to preliminary work -
the AMR -Dependency Parse aligner.

To extract the rewrite rules and the statistical
model, we need the links between AMR con-
cepts and the word nodes within the dependency
parse. An example alignment is shown in Fig.
2. Alignment between an AMR concept and de-
pendency node is needed because 1) it represents
the meaning of the sub-graph of the concept and
its child concepts corresponding to the phrase of
the head word node, and 2) the dependency node
contains sufficient information for the extraction
of rewrite rules. For example, the word node
“Vinken” on the dependency parse side in Fig. 2
links to the lexical concept “Vinken” and, further-
more, links to the “p2/name” and the “p/person”
concepts since “Vinken” is the head of the named
entity (NE) “Pierre Vinken” and the head of the
noun phrase “Pierre Vinken, 61 years old.” The
secondary aim of this proposal is to design an
alignment model between AMR concepts and de-
pendency parses. We use EM to search the hidden
derivations by combining the features of lexical
form, relation label, NE, semantic role, etc. Af-
ter EM processing, both the alignments and all the
feature probabilities can be estimated.

The design of a rewrite-based AMR parser is
described in Sec. 2, and the aligner is in Sec. 3.
Our preliminary experiments and results are pre-

41



Figure 2: The alignment between an AMR sub-
graph and a dependency parse. A red line links
the corresponding concept and dependency node.

sented in Sec. 4, followed by future work.

2 Rewrite Based AMR Parser

AMR is a rooted, directed, acyclic graph. For ex-
ample, the concept join-01 in Fig. 1 is the root
meaning of the sentence, which links to the child
concepts Arg0, Arg1, and time. AMR adheres to
the following principles (Banarescu et al., 2013):

• AMRs are rooted acyclic graphs with labels (re-
lations) on edges. These labels indicate the di-
rected relation between two concepts.
• AMRs abstract away from syntactic idiosyncra-

cies of a language, and instead attempt to capture
only the core meaning of a sentence.
• AMRs use the PB framesets as relation la-

bels (Palmer et al., 2005). For example, the rela-
tion labels (i.e., ARG0, ARG1) of “join-01” con-
cept in Fig. 1 correspond to the roles of the PB
frame “join-v.”
• AMRs combine multiple layers of linguistic an-

notation, like coreference, NE, semantic role,
etc., in a single structure.

The above basic characteristics make the parsing
of AMRs a difficult task. First, because AMR
abstracts away from syntactic idiosyncrasies, we
need a model to link the AMR concepts to words
in the original sentence, in order to obtain exter-
nal lexical, syntactic and semantic features. Sec-
ondly, the parser should learn the different feature
transformation probabilities jointly since AMRs
combine several linguistic annotations. Moreover,

(x)/NP→ :op(x) -r1
(x) nn (y)→ :name (name(x, y)) -r2
(x)/CD→ :quant(x) -r3
(x)/NNS→ :unit(x) -r4
npadvmod (x)(y)→ temporal-quanity(x, y) -r5
old/JJ (x)→ :age(x) -r6
NE PERSON(x)(y)→ person(x, y) -r7

Table 1: Sample Rewrite Rules

AMR uses graph variables and reentrancy to ex-
press coreference (e.g., “p” variable in Fig 1 ap-
pears twice – in :ARG0 of join-01 and :ARG0 of
have-org-role-91). The reentrancy prevents the
AMR graph from begin a tree structure. Dur-
ing parsing decoding, a polynomial time algorithm
should be replaced by alternative algorithms, like
beam search, to avoid an exponential running time.

JAMR (Flanigan et al., 2014) is the first sys-
tem for AMR parsing, which identifies the con-
cepts, and then searches for a maximum span-
ning connected subgraph (MSCG) on a fully con-
nected graph to identify the relations between con-
cepts. The search algorithm is similar to the maxi-
mum spanning tree algorithms. To assure the fi-
nal connected graph conforms to linguistic con-
straints, JAMR uses Lagrangian relaxation (Geof-
frion, 2010) to supplement the MSCG algorithm.
JAMR reaches a 58% Smatch score (Cai and
Knight, 2013) on automatic concept and relation
identification data, and 80% on gold concept and
automatic relation identification data.

2.1 Our Shift-Reduce Rewrite Rule Parser

Rewrite rule based parser is a bottom-up converter
from dependency parses to AMRs. The process
starts from the leaf word node on the dependency
parse. By applying rewrite-rules to each word
node, we obtain and assemble the sub-graphs of
our target AMR. Sample rewrite rules are listed in
Table 1. In these rules, the left hand side contains
the dependency information (e.g. word lemma,
POS, relation label, NE tag, etc). The right hand
side is the AMR concept and its template for filling
variables from previous parsing steps. The sample
derivation steps are listed in Table 2. For every
step, it shows the derivation rule applied (in Table
1), and the concept name, c1-c8.

This approach to parsing could be implemented
with a shift-reduce algorithm (Wang et al., 2015).
We define a stack and a list of tokens, which stores
the dependency words in the order of tree traver-
sal. Several actions are defined to operate on the
list(L) and the stack(S):

42



Derivation Apply
Rule

Concept
Name

Pierre/NNP→ :op Pierre r1 c1
Vinken/NNP→ :op Vinken r1 c2
(c1) nn (c2)
→ :name (name :op1 Pierre :op2 Vinekn) r2 c3
61/CD→ :quant 61 r3 c4
years/NNS→ :unit year r4 c5
npadvmod (c4)(c5) r5 c6
→ temporal-quanity :quant 61 :unit year
old/JJ (c6)
→ :age (temporal-quanity :quant 61 :unit year) r6 c7
NE PERSON (c3)(c7)
→ person :name (name :op1 Pierre :op2 Vinekn)
:age (temporal-quanity :quant 61 :unit year)

r7 c8

Table 2: The derivation for parsing “Pierre
Vinken, 61 years old” from dep. parse to AMR

• Shift Remove the dependency word from L, ap-
ply the rules, and push the new concept to S.
• Reduce Move the two top sub-concepts from S,

apply the rules, and push it back to S.
• Unary Move the top sub-concept from S, apply

the rules, and push it back to S.
• Finish If no more dependency words are in the

list, and one concept is in S, then return.

The final AMR concept would be stored at the top
of the stack. It is guaranteed that all the AMR
expressions can be derived from the dependency
parses by using the shift-reduce algorithm.

3 Dependency Parses to AMR Aligner

A preliminary step for our rewrite-based parser is
the alignment between the AMR and the depen-
dency parse. JAMR (Flanigan et al., 2014) pro-
vides a heuristic aligner between an AMR concept
and the word or phrase of a sentence. They use
a set of aligner rules, like NE, fuzzy NE, data en-
tity, etc., with a greedy strategy to match the align-
ments. This aligner achieves a 90% F1 score on
hand aligned AMR-sentence pairs. On the other
hand, Pourdamghani et al. (2014) present a gen-
erative model to align from AMR graphs to sen-
tence strings. They raises concerns about the lack
of sufficient data for learning derivation rules. In-
stead, they propose a string-to-string alignment
model, which transfers the AMR expression to a
linearized string representation. Then they use
several IBM word alignment models (Brown et
al., 1993) on this task. IBM Model-4 with a sym-
metric method reaches the highest F1 score of
83.1%. Separately analyzing the alignments of
roles and non-roles (lexical leaf on AMR), the F1
scores are 49.3% and 89.8%, respectively.

In comparison to previous work, our aligner es-
timates the alignments by learning the transforma-

tion probability of lexical form, relations, named
entities and semantic roles features jointly. Both
the alignment and transformation probabilities are
initialized for the training of parser.

3.1 Our Aligner Model with EM Algorithm
Our approach, based on the existing IBM Model
(Brown et al., 1993), is an AMR-to-Dependency
parse aligner, which represents one AMR as a list
of Concepts C = 〈c1, c2, . . . , c|C|〉, and the cor-
responding dependency parse as a list of depen-
dency word nodes D = 〈d1, d2, . . . , d|D|〉. The
alignmentA is a set of mapping functions a, which
link Concept cj to dependency word node di, a :
cj → di. Our model adopts an asymmetric EM
approach, instead of the standard symmetric one.
We can always find the dependency label path be-
tween any pair of dependency word nodes. How-
ever, the number of concept relation label paths is
not deterministic. Thus, we select the alignment
direction of AMR to dependency parse only, and
one-to-one mapping, in our model.

The objective function is to learn the parameter
θ in the AMR-to-Dependency Parse of EM:

θ = argmaxLθ(AMR|DEP )

Lθ(AMR|DEP ) =
|S|∑
k=1

∑
A

P (C(k), A|D(k); t, q)

where Lθ is the likelihood that we would like to
maximize, S is the training data set. We will
explain the transformation probability t and the
alignment probability q below.

Expectation-Step
The E-Step estimates the likelihood of the input
AMR and dependency parse by giving the trans-
formation probability t and alignment probability
q. The likelihood can be calculated using:

P (A|C,D) =
|C|∏
j=1

P (cj |a(cj))

P (cj |di, |C|, |D|) = t(cj |di) ∗ q(di|cj , |C|, |D|)
We would like to calculate all the probabilities
of possible alignments A between cj and di.
The transformation probability t is a combination
(multiple) probability of several different features:

• Plemma(cj |di): the lemma probability is the
probability of the concept name of cj , condi-
tioned on the dependency word of di.

43



• Prel(Label(cj , cpj )|RelPathdep(a(cj), a(cpj ))):
the relation probability is the probability of the
relation label between ci and its parent concept
cpi , given the relation path between the depen-
dency word nodes a(ci) and a(c

p
i ). e.g., the

relation probability of cj = 61 and a(cj) = 61
in Fig. 2 is P (quant|npadvmod ↓ num ↓).
• PNE(Name(cj)|TypeNE(a(cj))): the NE

probability is the probability of the name of cj ,
given the NE type (e.g., PERSON, DATE, ORG,
etc.) contained by a(cj)).
• PSR(Label(cj , cpj )|Pred(a(cpj )), Arg(a(cj))):

the semantic role probability is the probability
of relation label between cj and its parent c

p
j ,

conditioned on the predicate word of a(cpj ) and
argument type of a(cj) if a(cj) is semantic
argument of predicate a(cpj ).

On the other hand, the alignment probability
q(Dist(a(cj), a(c

p
j ))|cj , |C|, |D|) can be inter-

preted as the probability of the distance between
a(cj) and a(c

p
j ) on dependency parse D, condi-

tioned on cj , the lengths of D and C.

Maximization-Step
In the M-Step, the parameter θr is updated from
the previous round of θr−1, in order to maximize
the likelihood Lθ(AMR|DEP ):
t(C|D;AMR,DEP ) =∑

(AMR,DEP ) cnt(C|D;AMR,DEP )∑
C

∑
(AMR,DEP ) cnt(C|D;AMR,DEP )

q(D|C;AMR,DEP ) =∑
(AMR,DEP ) cnt(D|C;AMR,DEP )∑

D

∑
(AMR,DEP ) cnt(D|C;AMR,DEP )

where cnt is the normalized count that is collected
from the accumulating probability of all possible
alignment from the E-step. EM iterates the E-step
and M-step until convergence.

Initialization
Before iterating, the transformation probability t
and alignment probability q must be initialized.
We use these steps to initialize the parameters:

1. Assign a fixed value, say 0.9, to Plemma(cj |di)
if the concept name of cj is identical or a partial
match to the dependency word node di. Other-
wise, initialize it uniformly;

2. Run the EM algorithm with the initialized
Plemma only (Similar to IBM Model 1, which
is only concerned with translation probability);

3. Initialize all the other parameters, i.e., Prel,
PNE , PSR, and q with the development data;

4. Run the completed EM algorithm with the
Plemma we obtained from Step 2 and other prob-
abilities from Step 3.

The extra EM for the initialization of Plemma is to
estimate a more reasonable Plemma, and to speed
up the convergence of the second round of EM.

Decoding
To find the alignment of 〈C,D〉, we define the
search for alignments as follows:

argmax
A

P (A|C,D)

= argmax
A

|C|∏
j=1

t(cj |a(cj)) ∗ q(a(cj)|cj , |C|, |D|)

This decoding problem finds the alignmentA with
the maximum likelihood. A dynamic program-
ming (DP) algorithm is designed to extract the tar-
get alignment without exhaustively searching all
candidate alignments, which will take O(|D||C|).

This DP algorithm starts from the leaf concepts
and then walks through parent concepts. In cj , we
need to produce the following likelihoods:

1. Accumulated likelihood for aligning to any di
from all the child concepts of cj

2. Likelihood of Plemma and PNE
3. Likelihood of Prel and PSR for parent concept
cpj aligned to any dependency word node dl.

In step (3), we need to find the dl, aligned by c
p
j ,

that maximizes the likelihood. The accumulated
likelihood is then stored in a list with size=|D|. We
can trace back and find the most likely alignments
in the end. The running time of this algorithm
is O(|C||D|2). This algorithm does not include
reentrancy cases. One solution to be explored in
future work is to use a beam-search algorithm in-
stead.

4 Preliminary Experiments and Results

Here, we describe a preliminary experiment for
the AMR-Dependency Parse aligner, including the
data description, experimental setup, and results.

4.1 Data
The LDC AMR release 1.0 consists of 13,051
AMR-English sentence pairs1. To match an AMR

1LDC AMR release 1.0, Release date: June 16, 2014
https://catalog.ldc.upenn.edu/LDC2014T12

44



Split Sent. Tokens # of
NE

# of
Pred.

# of
Args

Train 1,000 19,923 1,510 4,231 7,739
Dev. 100 2,328 239 235 526
Test 100 1,672 80 199 445

Table 3: The data split of train/dev./test set. “# of
NE”, “# of Pred.” and “# of Args” stand for the
number of named entities, predicate and argument
annotations in the data set, respectively.

P R F1
Plemma 56.7 50.5 53.4

Combination 61.1 53.4 57.0

Table 4: Experiment Results

with its corresponding dependency parse, we se-
lect the sentences which appear in the OntoNotes
5.0 release2 as well, then randomly select 1,000 of
them as our training set. The OntoNotes data con-
tains TreeBank, PB, and NE annotations. Statis-
tics about the AMR and OntoNotes corpus and the
train/dev./test splits are given in Table 3. We man-
ually align the AMR concepts and dependency
word nodes in the dev. and test sets. We initial-
ize Prel, PNE , and PSR with the dev. set.

4.2 Results
We run our first round of EM (Step 2 in Initializa-
tion of Sec. 3.1) for 100 iterations, then use the
second round (Step 4 in Initialization of Sec. 3.1)
for another 100 iterations. We run our decoding al-
gorithm and evaluation on the test set after the first
and second round of EM. Due to time constraints,
we did not train the q here.

The experimental results are listed in Table 4.
We evaluate the performance on the precision, re-
call, and F1 score. Using just the Plemma (a simi-
lar approach to (Pourdamghani et al., 2014)), we
achieve 53.4% F1 score on the test set. On the
other hand, our aligner reaches 57.0% F1 score
with the full aligner.

5 Conclusion and Future Work

In this research, we briefly introduce AMR. We
describe the design principles and characteristics
of AMR, and show how the AMR parser task
is important, yet difficult. We present the basic
idea for a proposed AMR parser, based on the
shift-reduce algorithm. We also present an AMR-
Dependency Parse aligner, because such an aligner

2LDC OntoNotes Release 5.0, Release date: October 16,
2013 https://catalog.ldc.upenn.edu/LDC2013T19

will be a necessary first step before parsing. The
alignment and the estimated feature probabilities
are obtained by running the EM algorithm, which
could be use directly for the AMR parser.

In the future, we will be following these steps to
develop the proposed rewrite-based parser:

Implemention of our rewrite-based AMR
parser: We would like to implement the proposed
rewrite-based AMR parser. In comparison to the
parser of Flanigan (2014) , we believe our parser
could perform better on the runtime. We also plan
to experiment with the data generated by an auto-
matic dependency parser.

Expand the experimental data of aligner:
One problem discovered in our preliminary ex-
periments was that of data sparsity, especially for
Plemma. The LDC AMR Release contains 18,779
AMR/English sentences, and 8,996 of them are
contained in the OntoNotes release as well. There-
fore, increasing the training data size from the re-
lease is one solution to improve the performance
of our aligner from the unsatisfactory results. Us-
ing external lexical resources, like WordNet, is an-
other promissing solution to extend to snyonyms.

Evaluation of the aligner with existing
parser: Since our aligner provides the align-
ment between the dependency word node and both
the AMR leaf concept and role concept, we as-
sume that our aligner could improve not only our
rewrite-based parser but other parsers as well. To
verify this, we hope to submit our improved align-
ment results to a state-of-the-art AMR parser, and
evaluate the parsing results.

Acknowledgments

We gratefully acknowledge the support of the Na-
tional Science Foundation Grants IIS-1116782,
A Bayesian Approach to Dynamic Lexical
Resources for Flexible Language Processing,
0910992 IIS:RI: Richer Representations for Ma-
chine Translation, and NSF IIA-0530118 PIRE
(a subcontract from Johns Hopkins) for the 2014
Frederick Jelinek Memorial Workshop for Mean-
ing Representations in Language and Speech
Processing, and funding under the BOLT and
Machine Reading programs, HR0011-11-C-0145
(BOLT) FA8750-09-C-0179 (M.R.). Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.

45



References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking.

Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311, June.

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752. Association for Computa-
tional Linguistics.

J. Flanigan, S. Thomson, J. Carbonell, C. Dyer, and
N. A. Smith. 2014. A discriminative graph-based
parser for the abstract meaning representation. In
Proc. of ACL, Baltimore, Maryland, June. Associa-
tion for Computational Linguistics.

ArthurM. Geoffrion. 2010. Lagrangian relaxation for
integer programming. In Michael Jnger, Thomas M.
Liebling, Denis Naddef, George L. Nemhauser,
William R. Pulleyblank, Gerhard Reinelt, Giovanni
Rinaldi, and Laurence A. Wolsey, editors, 50 Years
of Integer Programming 1958-2008, pages 243–281.
Springer Berlin Heidelberg.

Martha Palmer, Dan Guildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–105, March.

Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
425–429. Association for Computational Linguis-
tics.

Chuan Wang, Xue Nianwen, and Pradhan Sameer.
2015. A transition-based algorithm for amr parsing.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics.

46


