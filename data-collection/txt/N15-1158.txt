



















































Sampling Techniques for Streaming Cross Document Coreference Resolution


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1391–1396,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Sampling Techniques for Streaming Cross Document Coreference
Resolution

Luke Shrimpton∗ Victor Lavrenko† Miles Osborne§
∗ School of Informatics, University of Edinburgh: luke.shrimpton@ed.ac.uk
† School of Informatics, University of Edinburgh: vlavrenk@inf.ed.ac.uk

§ Bloomberg, London: mosborne29@bloomberg.net

Abstract

We present the first truly streaming cross doc-
ument coreference resolution (CDC) system.
Processing infinite streams of mentions forces
us to use a constant amount of memory and so
we maintain a representative, fixed sized sam-
ple at all times. For the sample to be repre-
sentative it should represent a large number of
entities whilst taking into account both tempo-
ral recency and distant references. We intro-
duce new sampling techniques that take into
account a notion of streaming discourse (cur-
rent mentions depend on previous mentions).
Using the proposed sampling techniques we
are able to get a CEAFe score within 5% of
a non-streaming system while using only 30%
of the memory.

1 Introduction

Cross document coreference resolution (CDC) -
identifying mentions that refer to the same entity
across documents - is a prerequisite when combin-
ing entity specific information from multiple docu-
ments. Typically large scale CDC involves apply-
ing a scalable clustering algorithm to all the men-
tions. We consider streaming CDC, hence our sys-
tem must conform to the streaming computational
resource model (Muthukrishnan, 2005). Each men-
tion is processed in bounded time and only a con-
stant amount of memory is used. Honoring these
constraints ensures our system can be applied to in-
finite streams such as newswire or social media.

Storing all the mentions in memory is clearly in-
feasible, hence we need to either compress mentions

or store a sample. Compression is more computa-
tionally expensive as it involves merging/forgetting
mention components (for example: components of
a vector) whereas sampling decides to store or for-
get whole mentions. We investigate sampling tech-
niques due to their computational efficiency. We ex-
plore which mentions should be stored while per-
forming streaming CDC. A sample should repre-
sent a diverse set of entities while taking into ac-
count both temporal recency and distant mentions.
We show that using a notion of streaming discourse,
where what is currently being mentioned depends
on what was previously mentioned significantly im-
proves performance on a new CDC annotated Twit-
ter corpus.

2 Related Work

There are many existing approaches to CDC (Bagga
and Baldwin, 1998; Lee et al., 2012; Andrews et
al., 2014). Few of them scale to large datasets.
Singh et al. (2011) proposed a distributed hierar-
chical factor graph approach. While it can process
large datasets, the scalability comes from distribut-
ing the problem. Wick et al. (2012) proposed a sim-
ilar approach based on compressing mentions, while
scalable it does not conform to the streaming re-
source model. The only prior work that addressed
online/streaming CDC (Rao et al., 2010) was also
not constrained to the streaming model. None of
these approaches operate over an unbounded stream
processing mentions in constant time/memory.

1391



3 Entities in Streams

Streams like Twitter are well known as being real-
time and highly bursty. Some entities are continu-
ally mentioned throughout the stream (eg: President
Obama) whereas others burst, suddenly peak in pop-
ularity then decay (eg: Phillip Hughes, a cricketer
who died following a bowling injury).

Capturing the information required to perform
streaming CDC in constant space requires us to sam-
ple from the stream. For example we may consider
only the most recent information (eg: the previous
24 hours worth of mentions). This may not be ideal
as it would result in a sample biased towards burst-
ing entities, neglecting the continually mentioned
entities. We propose three properties that are im-
portant when sampling mentions from a stream of
tweets:

• Recency: There should be some bias towards
recent mentions to take into account the real-
time nature of the stream. The set of entities
mentioned one day is likely to be similar to the
set of entities mentioned on the following day.

• Distant Reference: The temporal gap between
mentions of the same entity can vary drasti-
cally, recency captures the small gaps though to
capture the larger gaps older mentions should
be stored. A mention should be correctly re-
solved if the last mention of the entity was ei-
ther a day or a week ago.

• Entity Diversity: The sample should contain
mentions of many entities instead of storing
lots of mentions about a few entities. If the
sample only contains mentions of the most
tweeted about entity (the one that is bursting)
it is impossible to resolve references to other
entities.

These properties suggest we should take into ac-
count a notion of streaming discourse when sam-
pling: mentions sampled should depend on the pre-
vious mentions (informed sampling).

4 Approach

We implemented a representative pairwise stream-
ing CDC system using single link clustering. Men-
tion similarity is a linear combination of mention

text and contextual similarity (weighted 0.8 and 0.2
respectively) similar to Rao et al. (2010). Mention
text similarity is measured using cosine similarity of
character skip bigram indicator vectors and contex-
tual similarity is measured using tf-idf weighted co-
sine similarity of tweet terms. The stream is pro-
cessed sequentially: we resolve each mention by
finding its nearest neighbor in the sample, linking
the two mentions if the similarity is above the link-
ing threshold.

5 Sampling Techniques

The sampling techniques we investigate are summa-
rized below. Each technique has an insertion and
removal policy that are followed each time a men-
tion is processed. New sampling techniques are indi-
cated by a star (*). The new sampling techniques re-
quire the nearest neighbor to be identified. As this is
already computed during resolution hence the over-
head of these new techniques is very low. Parame-
ters particular to each sampling technique are noted
in square brackets and are set using a standard grid
search on a training dataset.

• Exact: To provide an upper bound on perfor-
mance we forgo the constraints of the stream-
ing resource model and store all previously
seen mentions:

Insertion: Add current mention to sample. Re-
moval: Do nothing.

• Window: We sample a moving window of the
most recent mentions (first in, first out). For
example this technique assumes that if we are
processing mentions on Monday with a win-
dow of approximately 24 hours all relevant en-
tities were mentioned since Sunday.

Insertion: Add current mention to sample. Re-
moval: Remove oldest mention.

• Uniform Reservoir Sampling (Uniform-R):
A uniform sample of previously seen mentions
will capture a diverse set of entities from the
entire stream - taking into account diversity and
distant references. This can be achieved using
a reservoir sample (Vitter, 1985). We assume
each previously seen mention is equally likely
to help resolve the current mention.

1392



Insertion: Add current mention with probabil-
ity pi. Removal: If a mention was inserted
choose a mention uniformly at random to re-
move.

Setting pi = k/N where k is the sample size
and N is the number of items seen ensures the
sample is uniform (Vitter, 1985).

• Biased Reservoir Sampling (Biased-R): To
resolve both distant and recent references we
should store recent mentions and some older
mentions. An uninformed approach from ran-
domized algorithms is to use an exponentially
biased reservoir sample (Aggarwal, 2006; Os-
borne et al., 2014). It will store mostly re-
cent mentions but will probabilistically allow
older mentions to stay in the sample. For ex-
ample this technique will sample lots of men-
tions from yesterday and less from the day be-
fore yesterday.

Insertion: Add current mention with proba-
bility pi Removal: If a mention was inserted
choose a mention uniformly at random to re-
move.

Unlike uniform reservoir sampling pi is con-
stant. A higher value puts more emphasis on
the recent past. [pi]

• Cache*: We should keep past mentions criti-
cal to resolving current references (an informed
implementation of recency) and allow men-
tions to stay in the sample for an arbitrary pe-
riod of time to help resolve distance references.
For example if the same mention is used to re-
solve a reference on Saturday and Sunday it
should be in the sample on Monday.

Insertion: Add current mention to sample. Re-
moval: Choose a mention that was not recently
used to resolve a reference uniformly at random
and remove it.

When a mention is resolved we find its most
similar mention in the sample, recording its use
in a first in, first out cache of size n. The men-
tion to be removed is chosen from the set of
mentions not in the cache. We set n equal to
a proportion of the sample size. [Proportion of
mentions to keep]

• Diversity*: If we store fewer mentions about
each distinct entity we can represent more enti-
ties in the sample. For example if news breaks
that a famous person died yesterday the sample
should not be full of mentions about that en-
tity at the expense of other entities mentioned
today.

Insertion: Add current mention to sample. Re-
moval: If there is a sufficiently similar mention
in the sample remove it else choose uniformly
at random to be removed.

We remove the past mention most similar to the
current mention, but only if the similarity ex-
ceeds a threshold. [Replacement Threshold]

• Diversity-Cache (D-C)*: We combine Diver-
sity and Cache sampling.

Insertion: Add current mention to sample. Re-
moval: If there is a sufficiently similar mention
in the sample remove it else remove a mention
that has not recently been used to resolve a ref-
erence chosen uniformly at random.

For this technique we first choose the replace-
ment threshold then the proportion of mentions
to keep. [Replacement threshold and propor-
tion of mentions to keep]

6 Dataset

We collected 52 million English tweets from the 1%
sample of all tweets sent over a 77 day period. We
performed named entity recognition using Ritter et
al. (2011). It is clearly infeasible for us to annotate
all the mentions in the dataset. Hence we annotated
a sample of the entities. As with most prior work we
focused on person named entity mentions (of which
there is approximately 6 million in the dataset).

To select the entities we first sampled two names
based on how frequently they occur in the dataset:
‘Roger’ was chosen randomly from the low fre-
quency names (between 1,000 and 10,000 occur-
rences) and ‘Jessica’ was chosen similarly from
medium frequency names (10,000 to 100,0000 oc-
currences). We first annotated all mentions of
the names ‘Roger’ and ‘Jessica’ discarding entities
mentioned once. For the remaining entities we an-
notated all their mentions (not restricting to men-
tions that contained the words ‘Roger’ or ‘Jessica’).

1393



Figure 1: Mention Frequency Distribution.

This covers a diverse selection of people including:
‘Roger Federer’ (tennis player) and ‘Jessie J’ (the
singer whose real name is ‘Jessica Cornish’) as well
as less popular entities such as porn stars and jour-
nalists1.

Some statistics of the dataset are summarized in
table 1. We also plot the mention frequency (how of-
ten each entity was mentioned) distribution in figure
1 which shows a clear power law distribution similar
to what Rao et al. (2010) reported on the New York
Times annotated corpus. We show that recency and
distant reference are important aspects by plotting
the time since previous mention of the same entity
(gap) for each mention in figure 2. The gap is of-
ten less than 24 hours demonstrating the importance
of recency. There are also plenty of mentions with
much larger gaps, demonstrating the need to be able
to resolve distant references.

Source Mentions Entities Wiki Page Exists
Roger 5,794 137 69%
Jessica 10,543 129 46%

All 16,337 266 58%

Table 1: Mention/entity counts and percentage of entities
that have a Wikipedia page.

7 Experiments

As we are processing a stream we use a rolling eval-
uation protocol. Our corpus is split up into 11 con-
stant sized temporally adjacent blocks each lasting
1The annotations, including links to Wikipedia pages when
available, can be downloaded from https://sites.
google.com/site/lukeshr/.

Figure 2: Distribution of time since previous mention of
the same entity (gap). Each bar represents 24 hours.

approximately one week. Parameters are set using a
standard grid search on one block then we progress
to the next block to evaluate. The first block is re-
served for setting the linking threshold prior to our
rolling evaluation. We report the average over the
remaining blocks. For all sampling techniques that
have a randomized component we report an average
over 10 runs.

As the sample size will have a large effect on per-
formance we evaluate using various sample sizes.
We base our sample size on the average amount of
mentions per day (78,450) and evaluate our system
with sample sizes of 0.25,0.5,1,2 times the average
amount of mentions per day.

We evaluate using CEAFe (Luo, 2005), it is the
only coreference evaluation metric that can be triv-
ially adapted to datasets with a sample of annotated
entities. With no adaption it measures how well a
small amount of entities align with a system out-
put over a large amount. To make the evaluation
more representative we only use response clusters
that contain an annotated mention. This scales pre-
cision and maintains interpretability. We determine
if observed differences are significant by using a
Wilcoxon signed-rank test with a p value of 5% over
the 9 testing points. Results are shown in table 2.

• Window: This shows the performance that can
be achieved by only considering recency.

• Uniform Reservoir Sampling (Uniform-R):
This shows the performance achieved by using
an uninformed technique to store a diverse set
of older mentions.

1394



Sample Sampling CEAFe
Size Technique P R F
0.25 Days Window 24.2 67.2 35.6

Uniform-R 23.0 67.2 34.3
Biased-R 24.8 67.8 36.3

19,613 Cache ∗ 25.5 67.2 37.0
Mentions Diversity ∗ 27.6 69.2 39.4

D-C ∗ † 30.1 69.7 42.0
0.5 Days Window 31.3 69.4 43.1

Uniform-R 29.9 69.1 41.7
Biased-R 31.6 69.9 43.5

39,225 Cache ∗ 32.9 69.7 44.7
Mentions Diversity ∗ 37.5 71.6 49.2

D-C ∗ † 40.1 72.3 51.6
1.0 Days Window 40.7 72.0 52.0

Uniform-R 39.3 71.4 50.6
Biased-R 40.9 72.3 52.3

78,450 Cache ∗ 42.0 72.0 53.1
Mentions Diversity ∗ 48.5 74.3 58.7

D-C ∗ † 49.8 74.5 59.7
2.0 Days Window 50.2 74.1 59.8

Uniform-R 49.2 73.7 58.9
Biased-R 50.3 74.1 59.9

156,900 Cache ∗ 50.9 74.1 60.4
Mentions Diversity ∗ 55.2 75.2 63.7

D-C ∗ 55.5 75.3 63.9
≈600,000 Exact 59.7 75.4 66.6
Mentions

Table 2: CEAFe performance for various sample sizes
and sampling techniques. ∗ indicates significant improve-
ment over Window sampling. † indicates significant im-
provement over Diversity sampling

• Biased Reservoir Sampling (Biased-R): Un-
informed sampling of older mentions is not suf-
ficient to significantly improve performance.

• Cache: By using an informed model of recency
we keep mentions critical to resolving refer-
ences currently being tweeted resulting in a sig-
nificant performance improvement.

• Diversity: By using an informed technique to
increase the amount of distinct entities repre-
sented in the sample we significantly improve
performance.

• Diversity-Cache (D-C): By combining the
new sampling techniques we significantly im-
prove performance. Once we have increased
the amount of entities represented in the sam-
ple we are still able to benefit from an informed
model of recency.

Using uninformed sampling techniques (reservoir
sampling) does not result in a significant perfor-
mance improvement over Window sampling, only
informed sampling techniques show a significant
improvement. As the sample size increases the per-
formance difference decreases. With larger samples
there is space to represent more entities and it is less
likely to remove a useful mention at random.

8 Conclusion

We presented the first truly streaming CDC sys-
tem, showing that significantly better performance is
achieved by using an informed sampling technique
that takes into account a notion of streaming dis-
course. We are able to get to within 5% of an ex-
act system’s performance while using only 30% of
the memory required. Instead of improving perfor-
mance by using an uninformed sampling technique
and doubling the memory available, similar perfor-
mance can be achieved by using the same amount of
memory and a informed sampling technique. Fur-
ther work could look at improving the similarity
metric used, applying these sampling techniques to
other streaming problems or adding a mention com-
pression component.

References
Charu C Aggarwal. 2006. On biased reservoir sampling

in the presence of stream evolution. In Proceedings of
the 32nd international conference on Very large data
bases, pages 607–618. VLDB Endowment.

Nicholas Andrews, Jason Eisner, and Mark Dredze.
2014. Robust entity clustering via phylogenetic in-
ference. In Association for Computational Linguistics
(ACL).

Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 17th international con-
ference on Computational linguistics-Volume 1, pages
79–85. Association for Computational Linguistics.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and

1395



event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500. Association for Computational Linguistics.

Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing.

S Muthukrishnan. 2005. Data streams: Algorithms and
applications. Now Publishers Inc.

Miles Osborne, Ashwin Lall, and Benjamin Van Durme.
2014. Exponential reservoir sampling for streaming
language models. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 687–692. Asso-
ciation for Computational Linguistics.

Saša Petrović, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 181–189. Association for Computational Lin-
guistics.

Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference resolu-
tion. In Coling 2010: Posters, pages 1050–1058. Col-
ing 2010 Organizing Committee.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP.

Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
793–803. Association for Computational Linguistics.

Jeffrey S Vitter. 1985. Random sampling with a reser-
voir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37–57.

Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A discriminative hierarchical model for fast
coreference at large scale. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 379–388.
Association for Computational Linguistics.

1396


