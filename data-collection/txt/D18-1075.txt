



















































An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 702–707
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

702

An Auto-Encoder Matching Model for Learning Utterance-Level
Semantic Dependency in Dialogue Generation

Liangchen Luo∗, Jingjing Xu∗, Junyang Lin, Qi Zeng, Xu Sun
MOE Key Lab of Computational Linguistics, School of EECS, Peking University
{luolc,jingjingxu,linjunyang,pkuzengqi,xusun}@pku.edu.cn

Abstract

Generating semantically coherent responses is
still a major challenge in dialogue genera-
tion. Different from conventional text gener-
ation tasks, the mapping between inputs and
responses in conversations is more compli-
cated, which highly demands the understand-
ing of utterance-level semantic dependency, a
relation between the whole meanings of in-
puts and outputs. To address this problem, we
propose an Auto-Encoder Matching (AEM)
model to learn such dependency. The model
contains two auto-encoders and one mapping
module. The auto-encoders learn the seman-
tic representations of inputs and responses,
and the mapping module learns to connect the
utterance-level representations. Experimental
results from automatic and human evaluations
demonstrate that our model is capable of gen-
erating responses of high coherence and flu-
ency compared to baseline models.1

1 Introduction

Automatic dialogue generation task is of great
importance to many applications, ranging from
open-domain chatbots (Higashinaka et al., 2014;
Vinyals and Le, 2015; Li et al., 2016, 2017a; Su
et al., 2018) to goal-oriented technical support
agents (Bordes and Weston, 2016; Zhou et al.,
2017; Asri et al., 2017). Recently there is an
increasing amount of studies about purely data-
driven dialogue models, which learn from large
corpora of human conversations without hand-
crafted rules or templates. Most of them are based
on the sequence-to-sequence (Seq2Seq) frame-
work (Sutskever et al., 2014) that maximizes the
probability of gold responses given the previous
dialogue turn. Although such methods offer great

∗Equal Contribution
1The code is available at https://github.com/

lancopku/AMM

promise for generating fluent responses, they still
suffer from the poor semantic relevance between
inputs and responses (Xu et al., 2018). For exam-
ple, given “What’s your name” as the input, the
models generate “I like it” as the output.

Recently, the neural attention mechanism (Lu-
ong et al., 2015; Vaswani et al., 2017) has been
proved successful in many tasks including neural
machine translation (Ma et al., 2018b) and abstrac-
tive summarization (Lin et al., 2018), for its ability
of capturing word-level dependency by associat-
ing a generated word with relevant words in the
source-side context. Recent studies (Mei et al.,
2017; Serban et al., 2017) have applied the at-
tention mechanism to dialogue generation to im-
prove the dialogue coherence. However, conversa-
tion generation is a much more complex and flex-
ible task as there are less “word-to-words” rela-
tions between inputs and responses. For exam-
ple, given “Try not to take on more than you can
handle” as the input and “You are right” as the
response, each response word can not find any
aligned words from the input. In fact, this task re-
quires the model to understand the utterance-level
dependency, a relation between the whole mean-
ings of inputs and outputs. Due to the lack of
utterance-level semantic dependency, the conven-
tional attention-based methods that simply capture
the word-level dependency achieve less satisfying
performance in generating high-quality responses.

To address this problem, we propose a
novel Auto-Encoder Matching model to learn
utterance-level dependency. First, motivated by
Ma et al. (2018a), we use two auto-encoders to
learn the semantic representations of inputs and re-
sponses in an unsupervised style. Second, given
the utterance-level representations, the mapping
module is taught to learn the utterance-level de-
pendency. The advantage is that by explicitly sep-
arating representation learning and dependency

https://github.com/lancopku/AMM
https://github.com/lancopku/AMM


703

Mapping Module

How are you

How are you

I am fine

I am fine

Figure 1: An illustration of the Auto-Encoder Match-
ing model. The encoder and decoder are two auto-
encoders that are responsible for learning the semantic
representations. The mapping module is responsible
for learning the utterance-level dependency.

learning, the model has a stronger modeling ability
compared to traditional Seq2Seq models. Exper-
imental results show that our model substantially
outperforms baseline methods in generating high-
quality responses.

Our contributions are listed as follows:

• To promote coherence in dialogue gener-
ation, we propose a novel Auto-Encoder
Matching model to learn the utterance-level
dependency.

• In our proposed model, we explicitly separate
utterance representation learning and depen-
dency learning for a better expressive ability.

• Experimental results on automatic evaluation
and human evaluation show that our model
can generate much more coherent text com-
pared to baseline models.

2 Approach

In this section, we introduce our proposed model.
An overview is presented in Section 2.1. The de-
tails of the modules are shown in Sections 2.2, 2.3
and 2.4. The training method is introduced in Sec-
tion 2.5.

2.1 Overview
The proposed model contains three modules: an
encoder, a decoder, and a mapping module, as
shown in Figure 1.

In general, our model is different from the con-
ventional sequence-to-sequence models. The en-
coder and decoder are both implemented as auto-
encoders (Baldi, 2012). They learn the internal
representations of inputs and target responses, re-
spectively. In addition, a mapping module is built
to map the internal representations of the input and
the response.

2.2 Encoder

The encoder Eθ is an unsupervised auto-encoder
based on Long Short Term Memory Networks
(LSTM) (Hochreiter and Schmidhuber, 1996). As
it is essentially a LSTM-based Seq2Seq model,
we name the encoder and decoder of the auto-
encoder “source-encoder” and “source-decoder”.
To be specific, the encoder Eθ receives the source
text x = {x1, x2, ..., xn}, and encodes it to an in-
ternal representation h, and then decodes h to a
new sequence x̃ = {x̃1, x̃2, ..., x̃n} for the recon-
struction of the input. We extract the hidden state
h as the semantic representation. The encoder Eθ
is trained to reduce the reconstruction loss, whose
loss function is defined as follows:

J1(θ) = − logP (x̃|x; θ) (1)

where θ refers to the parameters of the encoderEθ.

2.3 Decoder

Similar to the encoder, our decoder Dφ is also a
LSTM-based auto-encoder. However, as there is
no target text provided in the testing stage, we pro-
pose the customized implementation, which is il-
lustrated in Section 2.5. Here in the introduction
of the decoder, we do not provide the testing de-
tails. Similarly, we name the encoder and decoder
of the auto-encoder “target-encoder” and “target-
decoder”. The target-encoder receives the target
y = {y1, y2, ..., yn} and encodes it to a utterance-
level semantic representation s, and then decodes
s to a new sequence to approximate the target text.
The loss function is identical to that of the en-
coder:

J2(φ) = − logP (ỹ|y;φ) (2)

2.4 Mapping Module

As our model is constructed for dialogue genera-
tion, we design the mapping module to ensure that
the generated response is semantically consistent



704

with the source. There are many matching mod-
els that can be used to learn such dependency rela-
tions (Hu et al., 2014; Guo et al., 2016; Pang et al.,
2016; Chen et al., 2018). For simplicity, we only
use a simple feedforward network for implemen-
tation. The mapping module Mγ transforms the
source semantic representation h to a new repre-
sentation t. To be specific, we implement a multi-
layer perceptron (MLP) g(·) forMγ and train it by
minimizing the L2-norm loss J3(γ) of the trans-
formed representation t and the semantic represen-
tation of target response s:

t = g(h)

J3(γ) =
1

2
‖t− s‖22

(3)

2.5 Training and Testing
In the testing stage, given an input utterance, the
encoder Eθ, the decoder Dφ, and the matching
module Mγ work together to produce a dialogue
response. The source-encoder first receives the in-
put x and encodes it to a semantic representation h
of the source utterance. Then, the mapping mod-
ule transforms h to t, a target response represen-
tation. Finally, t is sent to the target-decoder for
response generation.

In the training stage, besides the auto-encoder
loss and the mapping loss, we also use an end-to-
end loss J4(θ, φ, γ):

J4(θ, φ, γ) = − logP (y|x; θ, φ, γ) (4)

= −
T∑
t=1

logP (yt|x, y1..t−1; θ, φ, γ)

(5)

where x is the source input, y is the target re-
sponse, and T is the length of response sequence.
The model learns to generate ỹ to approximate y
by minimizing the reconstruction losses J1(θ) and
J2(φ), the mapping loss J3(γ), and the end-to-end
loss J4(θ, φ, γ). The details are illustrated below:

J = λ1 [J1(θ) + J2(φ)] + λ2J3(γ)

+ λ3J4(θ, φ, γ)
(6)

where J refers to the total loss, and λ1, λ2, and λ3
are hyperparameters.

3 Experiment

We conduct experiments on a high-quality dia-
logue dataset called DailyDialog built by Li et al.

(2017b). The dialogues in the dataset reflect
our daily communication and cover various topics
about our daily life. We split the dataset into three
parts with 36.3K pairs for training, 11.1K pairs for
validation, and 11.1K pairs for testing.

3.1 Experimental Details
For dialogue generation, we set the maximum
length to 15 words for each generated sentence.
Based on the performance on the validation set,
we set the hidden size to 512, embedding size to
64 and vocabulary size to 40K for baseline mod-
els and the proposed model. The parameters are
updated by the Adam algorithm (Kingma and Ba,
2014) and initialized by sampling from the uni-
form distribution ([−0.1, 0.1]). The initial learn-
ing rate is 0.002 and the model is trained in mini-
batches with a batch size of 256. λ1 and λ3 are
set to 1 and λ2 is set to 0.01 in Equation (6). It is
important to note that for a fair comparison, we re-
implement the baseline models with the best set-
tings on the validation set. After fixing the hyper-
parameters, we combine the training and valida-
tion sets together as a larger training set to produce
the final model.

3.2 Results
We use BLEU (Papineni et al., 2002), to com-
pare the performance of different models, and
use the widely-used BLEU-4 as our main BLEU
score. The results are shown in Table 1. The
proposed AEM model significantly outperforms
the Seq2Seq model. It demonstrates the effec-
tiveness of utterance-level dependency on improv-
ing the quality of generated text. Furthermore,
we find that the utterance-level dependency also
benefits the learning of word-level dependency.
The improvement from the AEM model to the
AEM+Attention model2 is 0.68 BLEU-4 point. It
is much more obvious than the improvement from
the Seq2Seq model to the Seq2Seq+Attention,
which is 0.29 BLEU-4 point.

We also report the diversity of the generated re-
sponses by calculating the number of distinct un-
igrams, bigrams, and trigrams. The results are
shown in Table 2. We find that the AEM model
achieves significant improvement on the diversity
of generated text. The number of unique tri-
gram of the AEM model is almost six times more

2With the additional attention mechanism, the outputs of
attention-based decoder and our decoder are combined to-
gether to predict the probability of response words.



705

Models BLEU-1 BLEU-2 BLEU-3 BLEU-4
Seq2Seq 12.43 4.57 2.69 1.84
AEM 13.55 4.89 3.04 2.16
Seq2Seq+Attention 13.63 4.99 3.05 2.13
AEM+Attention 14.17 5.69 3.78 2.84

Table 1: BLEU scores for the AEM model and the Seq2Seq model.

Models Dist-1 Dist-2 Dist-3
Seq2Seq 0.8K 2.7K 5.5K
AEM 3.1K 14.8K 31.2K
Seq2Seq+Attention 2.5K 13.6K 34.6K
AEM+Attention 3.3K 23.2K 53.9K

Table 2: Diversity scores for the AEM model and the Seq2Seq model. Dist-1, Dist-2, and Dist-3 are respectively
the number of distinct unigrams, bigrams, and trigrams in the generated text.

Models Fluency Coherence G-Score
Seq2Seq 6.97 3.51 4.95
AEM 8.11 4.18 5.82
Seq2Seq+Attention 5.11 3.30 4.10
AEM+Attention 7.92 4.97 6.27

Table 3: Human evaluation results of the AEM model
and the Seq2Seq model.

than that of the Seq2Seq model. Also, it should
be noticed that the attention mechanism performs
almost the same compared to the AEM model
(31.2K vs. 34.6K in terms of Dist-3), which indi-
cates that the utterance-level dependency and the
word-level dependency are both indispensable for
dialogue generation. Therefore, by combining the
two dependencies together, the AEM+Attention
model achieves the best results. Such improve-
ments are expected. With the increase of the rel-
evance of the generated text, it gets harder for
the model to generate repeated responses. In our
experimental results, the number of repetitive “I
don’t know” in the AEM+Attention model is re-
duced by 50% compared to the Seq2Seq model.

For dialogue generation, human evaluation is
more convincing, so we also report human eval-
uation results on the test set. We randomly choose
100 utterances in daily communication style for
the human evaluation, each of which is sent to
different models to generate responses. The re-
sults are distributed to the annotators who have
no knowledge about which model the sentence is
from. All annotators have linguistic background.
They are asked to score the generated responses
in terms of fluency and coherence. Fluency rep-

Input 1: My name is David. What is my
name?
Seq2Seq: Ours, maria.
AEM: Pardon me? It is unreasonable!
Input 2: I’m going to kill you!
Seq2Seq: I know. I’m a good man.
AEM: What?!?!!!! No!!
Input 3: I hate you.
Seq2Seq: I’m not.
AEM: You do? you’re a sweet girl.
Input 4: How are you?
Seq2Seq: Fine.
AEM: Fine. I’m good.
Input 5: Can you say ‘Hello’?
Seq2Seq: Yeah, I guess.
AEM: Sure, I guess.

Table 4: Examples generated by the AEM model and
the Seq2Seq model.

resents whether each sentence is in correct gram-
mar. Coherence evaluates whether the generated
response is relevant to the input. The score ranges
from 1 to 10 (1 is very bad and 10 is very good).
To evaluate the overall performance, we use the
geometric mean of fluency and coherence as the
final evaluation metric.

Table 3 shows the results of human evaluation.
The inter-annotator agreement is satisfactory con-
sidering the difficulty of human evaluation. The
Pearson’s correlation coefficient is 0.69 on co-
herence and 0.57 on fluency, with p < 0.0001.
First, it is clear that the AEM model outperforms
the Seq2Seq model with a large margin, which
proves the effectiveness of the AEM model on



706

generating high quality responses. Second, it is
interesting to note that with the attention mech-
anism, the coherence is decreased slightly in the
Seq2Seq model but increased significantly in the
AEM model. It suggests that the utterance-level
dependency greatly benefits the learning of word-
level dependency. Therefore, it is expected that the
AEM+Attention model achieves the best G-score.

Table 4 shows the examples generated by the
AEM model and the Seq2Seq model. For easy
questions (ex. 4 and ex. 5), they both perform
well. For hard questions (ex. 1 and ex. 2), the pro-
posed model obviously outperforms the Seq2Seq
model. It shows that the utterance-level depen-
dency learned by the proposed model is useful for
handling complex inputs.

3.3 Error Analysis

Although our model achieves the best perfor-
mance, there are still several failure cases. We find
that the model performs badly for the inputs with
unseen words. For instance, given “Bonjour” as
the input, it generates “Stay out of here” as the out-
put. It shows that the proposed model is sensitive
to the unseen utterance representations. Therefore,
we would like to explore more approaches to ad-
dress this problem in the future work. For exam-
ple, the auto-encoders can be replaced by varia-
tional auto-encoders to ensure that the distribution
of utterance representations is normal, which has
a better generalization ability.

4 Conclusion

In this work, we propose an Auto-Encoder Match-
ing model to learn the utterance-level semantic de-
pendency, a critical dependency relation for gener-
ating coherent and fluent responses. The model
contains two auto-encoders that learn the utter-
ance representations in an unsupervised way, and a
mapping module that builds the mapping between
the input representation and response representa-
tion. Experimental results show that the proposed
model significantly improves the quality of gener-
ated responses according to automatic evaluation
and human evaluation, especially in coherence.

Acknowledgements

This work was supported in part by National Natu-
ral Science Foundation of China (No. 61673028).
We thank all reviewers for providing the construc-

tive suggestions. Xu Sun is the corresponding au-
thor of this paper.

References
Layla El Asri, Hannes Schulz, Shikhar Sharma,

Jeremie Zumer, Justin Harris, Emery Fine, Rahul
Mehrotra, and Kaheer Suleman. 2017. Frames: a
corpus for adding memory to goal-oriented dialogue
systems. In Proceedings of the 18th Annual SIGdial
Meeting on Discourse and Dialogue, Saarbrücken,
Germany, August 15-17, 2017, pages 207–219.

Pierre Baldi. 2012. Autoencoders, unsupervised learn-
ing, and deep architectures. In Unsupervised and
Transfer Learning - Workshop held at ICML 2011,
Bellevue, Washington, USA, July 2, 2011, pages 37–
50.

Antoine Bordes and Jason Weston. 2016. Learn-
ing end-to-end goal-oriented dialog. CoRR,
abs/1605.07683.

Deli Chen, Shuming Ma, Pengcheng Yang, and
Xu Sun. 2018. Identifying high-quality chinese
news comments based on multi-target text matching
model. CoRR, abs/1808.07191.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce
Croft. 2016. A deep relevance matching model for
ad-hoc retrieval. In Proceedings of the 25th ACM In-
ternational Conference on Information and Knowl-
edge Management, CIKM 2016, Indianapolis, IN,
USA, October 24-28, 2016, pages 55–64.

Ryuichiro Higashinaka, Kenji Imamura, Toyomi Me-
guro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki
Sugiyama, Toru Hirano, Toshiro Makino, and Yoshi-
hiro Matsuo. 2014. Towards an open-domain con-
versational system fully based on natural language
processing. In COLING 2014, 25th International
Conference on Computational Linguistics, Proceed-
ings of the Conference: Technical Papers, August
23-29, 2014, Dublin, Ireland, pages 928–939.

Sepp Hochreiter and Jürgen Schmidhuber. 1996.
LSTM can solve hard long time lag problems. In
NIPS, 1996, pages 473–479.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Annual Conference on Neural Information Process-
ing Systems 2014, December 8-13 2014, Montreal,
Quebec, Canada, pages 2042–2050.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,



707

Austin, Texas, USA, November 1-4, 2016, pages
1192–1202.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,
Alan Ritter, and Dan Jurafsky. 2017a. Adversar-
ial learning for neural dialogue generation. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2017,
Copenhagen, Denmark, September 9-11, 2017,
pages 2157–2169.

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017b. Dailydialog: A man-
ually labelled multi-turn dialogue dataset. In Pro-
ceedings of the Eighth International Joint Con-
ference on Natural Language Processing, IJCNLP
2017, Taipei, Taiwan, November 27 - December 1,
2017 - Volume 1: Long Papers, pages 986–995.

Junyang Lin, Xu Sun, Shuming Ma, and Qi Su.
2018. Global encoding for abstractive summariza-
tion. CoRR, abs/1805.03989.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1412–1421.

Shuming Ma, Xu Sun, Junyang Lin, and Houfeng
Wang. 2018a. Autoencoder as assistant supervisor:
Improving text representation for chinese social me-
dia text summarization. CoRR, abs/1805.04869.

Shuming Ma, Xu Sun, Yizhong Wang, and Junyang
Lin. 2018b. Bag-of-words as target for neural ma-
chine translation. CoRR, abs/1805.04871.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2017. Coherent dialogue with attention-based
language models. In Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence, Febru-
ary 4-9, 2017, San Francisco, California, USA.,
pages 3252–3258.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,
Shengxian Wan, and Xueqi Cheng. 2016. Text
matching as image recognition. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intel-
ligence, February 12-17, 2016, Phoenix, Arizona,
USA., pages 2793–2799.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 311–318.

Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kar-
tik Talamadupula, Bowen Zhou, Yoshua Bengio,
and Aaron C. Courville. 2017. Multiresolution re-
current neural networks: An application to dia-
logue response generation. In Proceedings of the

Thirty-First AAAI Conference on Artificial Intelli-
gence, February 4-9, 2017, San Francisco, Califor-
nia, USA., pages 3288–3294.

Hui Su, Xiaoyu Shen, Pengwei Hu, Wenjie Li, and Yun
Chen. 2018. Dialogue generation with GAN. In
Proceedings of the Thirty-Second AAAI Conference
on Artificial Intelligence, New Orleans, Louisiana,
USA, February 2-7, 2018.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. CoRR, abs/1506.05869.

Jingjing Xu, Xuancheng Ren, Junyang Lin, and
Xu Sun. 2018. Diversity-promoting gan: A cross-
entropy based generative adversarial network for di-
versified text generation. In EMNLP, 2018.

Li Zhou, Kevin Small, Oleg Rokhlenko, and Charles
Elkan. 2017. End-to-end offline goal-oriented di-
alog policy learning via policy gradient. CoRR,
abs/1712.02838.


