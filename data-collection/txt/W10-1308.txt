










































A Multimodal Vocabulary for Augmentative and Alternative Communication from Sound/Image Label Datasets


Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62–70,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

A Multimodal Vocabulary for Augmentative and Alternative Communi-

cation from Sound/Image Label Datasets 

    
Xiaojuan Ma Christiane Fellbaum Perry R. Cook 
 Princeton University  
 

 

35 Olden St. Princeton, NJ 08544, USA 
{xm,fellbaum,prc}@princeton.edu 

 

 

 

Abstract 

Existing Augmentative and Alternative Com-

munication vocabularies assign multimodal 

stimuli to words with multiple meanings. The 

ambiguity hampers the vocabulary effective-

ness when used by people with language dis-

abilities. For example, the noun “a missing 

letter” may refer to a character or a written 

message, and each corresponds to a different 

picture. A vocabulary with images and sounds 

unambiguously linked to words can better 

eliminate misunderstanding and assist com-

munication for people with language disorders. 

We explore a new approach of creating such a 

vocabulary via automatically assigning se-

mantically unambiguous groups of synonyms 

to sound and image labels. We propose an un-

supervised word sense disambiguation (WSD) 

voting algorithm, which combines different 

semantic relatedness measures. Our voting al-

gorithm achieved over 80% accuracy with a 

sound label dataset, which significantly out-

performs WSD with individual measures. We 

also explore the use of human judgments of 

evocation between members of concept pairs, 

in the label disambiguation task. Results show 

that evocation achieves similar performance to 

most of the existing relatedness measures.  

1 Introduction 

In natural languages, a word form may refer to dif-

ferent meanings. For instance, the word “fly” 

means “travel through the air” in context like “fly 

to New York,” while it refers to an insect in the 

phrase “a fly on the trashcan.” Speakers determine 

the appropriate sense of a polysemous word based 

on the context. However, people with language 

disorders and access/retrieval problems, may have 

great difficulty in understanding words individual-

ly or in a context. To overcome such language bar-

riers, visual and auditory representations are intro-

duced to help illustrate concepts (Ma et al., 

2009a)(Ma et al., 2010). For example, a person 

with a language disability can tell the word “fly” 

refers to “travel through the air” when he sees a 

plane in the image (rather than an insect); likewise 

he can distinguish the meaning of “fly” given the 

plane engine sound vs. the insect buzzing sound.  

This approach has been employed in Augmentative 

and Alternative Communication (AAC), in the 

form of multimodal vocabularies in assistive de-

vices (Steele et al. 1989)(Lingraphica, 2010). 

However, current AAC vocabularies assign vis-

ual stimuli to words instead of specific meanings, 

and thus bring in ambiguity when a user with lan-

guage disability tries to comprehend and commu-

nicate a concept. For example, for the word “fly,” 

Lingraphica only has an icon showing a plane and 

a flock of birds flying. Confusion arises when a 

sentence like “I want to kill the fly (the insect)” is 

explained using the airplane/bird icon. Similarly, it 

will lead to miscommunication if the sound of keys 

jingling is used to express “a key is missing” when 

the person intends to refer to a key on the keyboard. 

People with language impairment are relying on 

the AAC vocabularies for language access, and any 

ambiguity may result in communication failure.  

To address this problem, we propose building a 

semantic multimodal AAC vocabulary with visual 

and auditory representations expressing concepts 

rather than words (Figure 1), as the backbone of 

the language assistant system for people with 

aphasia (Ma et al. 2009b). Our work is exploratory 

with the following innovations: 1) we target the 

insufficiency of current assistive vocabularies by 

resolving ambiguity; 2) we enrich concept invento-

ry and connect concepts through language, envi-

ronmental sounds, and images (little research has 

looked into conveying concepts through natural 

nonspeech sounds); and 3) our vocabulary has a 

dynamic scalable semantic network structure rather 

62



than simply grouping words into categories as 

conventional assistive devices do.  

One intuitive way to build a disambiguated mul-

timodal vocabulary is to manually assign meanings 

to each word in the existing vocabulary. However, 

the task is time consuming with poor scalability – 

no new multimedia representations are generated 

for concepts that are missing in the vocabulary. 

ImageNet (Jia et al., 2009) was constructed by 

people verifying the assignment of web images to 

given synonym sets (synsets). ImageNet has over 

nine million images linked to about 15 thousands 

noun synsets in WordNet (Fellbaum, 1998). De-

spite the huge human effort, ImageNet, with the 

goal of creating a computer vision database, does 

not yet include all the most commonly used words 

across different parts of speech. It is not yet suita-

ble for a language support application. 

We explore a new approach for generating a vo-

cabulary with concept to sound/image associations, 

that is, conducting word sense disambiguation 

(WSD) techniques used in Natural Language 

Processing on sound/image label datasets. For ex-

ample, the labels “car, drive, fast” for the sound 

“car – passing.wav” are assigned to synsets “car: a 

motor vehicle,” “drive: operate or control a ve-

hicle,” and “fast: quickly or rapidly” via WSD. It 

means the sound “car – passing.wav” can be used 

to depict those concepts. This approach is viable 

because the words in the sound/image labels were 

shown to evoke one another based on the audito-

ry/visual content, and their meanings can be identi-

fied by considering all the tags generated for a 

given sound or image as a context.  With the avail-

ability of large sound/image label datasets, the vo-

cabulary created from WSD can be easily 

expanded. 

A variety of WSD methods (e.g. knowledge-

based methods (Lesk, 1986), unsupervised me-

thods (Lin, 1997), semi-supervised methods 

(Hearst, 1991) (Yarowsky, 1995), and supervised 

methods (Novischi et al., 2007)) were developed 

and evaluated with corpus data and other text doc-

uments like webpages. Compared to the text data 

that WSD methods work with, labels for sounds 

and images have unique characteristics. The labels 

are a bag of words related to the visual/auditory 

content; there is no syntactic or part of speech in-

formation, nor are the words necessarily contextual 

neighbors. For example, contexts suggest land-

scape senses for the word pair “bank” and “water”, 

whereas in an image, a person may drink water 

inside a bank building. Furthermore, few annotated 

image or sound label datasets are available, making 

it hard to apply supervised or semi-supervised 

WSD methods.  

To efficiently and effectively create a disambi-

guated multimodal vocabulary, we need to achieve 

two goals. First, optimize the accuracy of the WSD 

algorithm to minimize the work required for ma-

nual checking and correction afterwards. Second, 

construct a semantic network across different parts 

of speech, and thus explore linking semantic rela-

tedness measures that can capture aspects different 

from existing ones. In this paper, we target the first 

goal by proposing an unsupervised sense disam-

 
Figure 1. Disambiguated AAC multimedia vocabulary; dash arrows are semantic relations between concepts. 

63



biguation algorithm combining a variety of seman-

tic relatedness measures. We chose an unsuper-

vised method because of the lack of a large 

manually annotated gold standard. The measure-

combined voting algorithm presented here draws 

advantages from different semantic relatedness 

measures and has them vote for the best-fitting 

sense to assign to a label. Evaluation shows that 

the voting algorithm significantly exceeds WSD 

with each individual measure. 

To approach the second goal, we proposed and 

tested a semantic relatedness measure called evo-

cation (Boyd-Graber et al., 2006) in disambigua-

tion of sound/image labels. Evocation measures 

human judgements of relatedness between a di-

rected concepts pair. It provides cross parts of 

speech evocativeness information which supple-

ments most of the knowledge-based semantic rela-

tedness measures. Evaluation results showed that 

the performance of WSD with evocation is no 

worse than most of the relatedness measures that 

we applied, despite the relatively small size of the 

current evocation dataset. 

2 Dataset: Semantic Labels for Environ-

mental Sounds and Images 

Our ultimate goal is to create an AAC vocabulary 

of associations between environmental sounds and 

images and groups of synonymous words that are 

relevant to the content. We are working with two 

datasets of human labels for multimedia data, 

SoundNet and the Peekaboom dataset. 

2.1 SoundNet Sound Label Dataset 

The SoundNet Dataset (Ma, Fellbaum, and Cook, 

2009) consists of 327 environmental “soundnails” 

(5-second audio clips) each with semantic labels 

collected from participants via a large scale Ama-

zon Mechanical Turk (AMT) study. The sound-

nails cover a wide range of auditory scenes, from 

vehicle (e.g. car starting), mechanical tools (e.g. 

handsaw) and electrical devices (e.g. TV), to natu-

ral phenomena (e.g. rain), animals (e.g. a dog bark-

ing), and human sounds (e.g. a baby crying). In the 

AMT study, participants were asked to generate 

tags for each soundnail labeling its source, possible 

location, and actions involved in making the sound.  

Each soundnail was labeled by over 100 people. 

The tags were clustered into meaning units that 

SoundNet refers to as “sense sets.” A sense set in-

cludes a set of words with similar meanings. For 

instance, for the soundnail pre-labeled “bag, zipO-

pen” which is the sound of opening the zipper of a 

bag, the following sense sets were generated:  

(a) “zipper” {zipper, zip up, zip, unzip};  

(b) “bag” {bag, duffle bag, nylon bag, suitcase, 

luggage, backpack, purse, pack, briefcase};  

(c) “house” {house, home, building}, and 

(d) “clothes” {clothes, jacket, coat, pants, jeans, 

dress, garment}.  

 The word in bold is was judged by SoundNet to 

be the best representative of the sense set, and oth-

er words, possibly belonging to different parts of 

speech are included in the curly brackets enclosing 

the sense sets. SoundNet uses sense sets rather than 

single words because 1) people may use different 

words to describe the same underlying concept, 

(e.g. “baby” and “infant;” “rain” as a noun and as a 

verb); 2) people cannot draw fine distinctions be-

tween objects and events that generate similar 

sounds, and thus may come up with different but 

related categories (e.g. “plate,” “cup,” and “bowl” 

for the dish clinking sound); and 3) people may 

perceive objects and events that are not explicitly 

presented in the sound very differently (e.g. “bag” 

vs. “clothes” for the sound made by a zipper). In 

this experiment, only sense sets (labels) that were 

generated by at least 25% of the labelers were 

used.  

In our disambiguation experiment, two kinds of 

contexts were explored. In the Context 1 scheme, 

each label is treated separately: all its members 

plus the representatives of the other sense sets are 

considered. Take the soundnail “bag, zipOpen” as 

an example. The context for disambiguating label 

(a) “zipper” {zipper, zip up, zip, unzip} is: 

zipper, zip up, zip, unzip, bag, house, clothes. 

The context for label (d) “clothes” {clothes, jacket, 

coat, pants, jeans, dress, garment} is:  

clothes, jacket, coat, pants, jeans, dress, garment, 

zipper, bag, house.   

In the Context 1 scheme, all representative 

words will be disambiguated multiple times. The 

final result will be the synset that gets the most 

votes. In the Context 2 scheme, as for the image 

dataset described below, all members from each 

sense set are put together to create the context, and 

each word is disambiguated only once. 

2.2 Peekaboom Image Label Dataset 

64



The ESP Game Dataset (Von Ahn and Dabbish, 

2004) contains a large number of web images and 

human labels produced via an online game. For 

example, an image of a glass of hard liquor is la-

beled “full, shot, alcohol, clear, drink, glass, beve-

rage.” The Peekaboom Game (Von Ahn et al., 

2006) is the successor of the ESP Game. In our 

experiment, part of the Peekaboom Dataset (3,086 

images) was used. For each image, all the labels 

together form the context for sense disambigua-

tion.  

The Peekaboom labels are noisier than the 

SoundNet labels for several reasons. First, random 

objects may appear in a picture and thus be in-

cluded in the labels. For example, an image is la-

beled “computer, shark” because there is a shark 

picture on the computer screen. Second, texts in 

the images are often included in the labels. For 

example, the word “green” is one of the labels for 

an image with a street sign “Green St.” Third, the 

Peekaboom labels are not stemmed, which adds 

another layer of ambiguity. For example, the labels 

“bridge, building” could refer to a building event 

or to a built entity. In the experiment, all labels for 

an image are used in their unstemmed form to con-

struct the context for WSD.  

3 Evocation and Other Semantic Related-

ness Measures 

A set of measures were selected to assess the rela-

tedness between possible senses of words in the 

sound/image labels. Apart from existing methods, 

an additional measure, evocation, is introduced. 

3.1 Evocation 

Evocation (Boyd-Graber et al., 2006) measures 

concept similarity based on human judgment. It is 

a directed measure, with evocation(synset A, syn-

set B) defined as how much synset A brings to 

mind synset B. The evocation dataset has been ex-

tended to scores for 100,000 directed synset pairs 

(Nikolova et al., 2009).  

The evocation data were collected independently 

of WordNet or corpus data. We propose the use of 

evocation in WSD for image and sound labels for 

the following reasons. First, the sound and image 

labels are generated based on human perception of 

the content and common knowledge. In SoundNet 

in particular, many of the evoked labels reflected 

the most obvious objects or events in a sound 

scene. For example, “bag” and “coat” were evoked 

from the zipper soundnail. In this case, the evoca-

tion score may be a good evaluation of the related-

ness between the labels. Second, evocation 

assesses relatedness of concepts across different 

parts of speech, which is suitable for identifying 

image and sound labels containing nouns, verbs, 

adjectives, adverbs, etc. 

This paper is a first attempt to compare the ef-

fectiveness of the use of evocation measure in 

sense disambiguation to the conventional, relative-

ly better tested similarity measures, in the context 

of assigning synsets to sound/image labels. Consi-

dering that the evocation dataset is small in size 

and susceptible to noise given the method by 

which it was collected, we have not yet incorpo-

rated evocation into the measure-combined voting 

algorithm described in the Section 4. 

3.2 Semantic Relatedness Measures 

Nine measures of semantic relatedness
1
 between 

synsets are used in the experiment, both as contri-

butors to the voting algorithm and as baselines for 

comparison, including: 

1) WordNet path based measures. 

• “path” – shortest path length between syn-

sets,  inversely proportional to the number 

of nodes on the path. 

• “wup” (Wu and Palmer, 1994) – ratio of the 

depth of the Least Common Subsumer 

(LCS) to the depths of two synsets in the 

Wordnet taxonomy. 

• “lch” (Leacock and Chodorow, 1998) – 

considering the length of the shortest path 

between two synsets to the depth of the 

WordNet taxonomy. 

2) Information and content based measures. 

• “res” (Resnik, 1995) – the informational 

content (IC) of a given corpus of the LCS 

between two synsets. 

• “lin” (Lin, 1997) – the ratio of the IC of the 

LCS to the IC of the two synsets. 

• “jcn” (Jiang and Conrath, 1997) – inversely 

proportional to the difference between the 

IC of the two synsets and the IC of the LCS. 

                                                           
1 “hso” (Hirst and St-Onge, 1998) extensively slows down the 

WSD process with over five context words, and thus, is not 

included in the experiment. 

65



3) WordNet definition based measures. 

• “lesk” (Banerjee and Pedersen, 2002) – 

overlaps in the definitions of two synsets.  

• “vector” (Patwardhan and Pedersen, 2006) 

– cosine of the angle between the co-

occurrence vector computed from the defi-

nitions around the two synsets. 

• “vector_pairs” – co-occurrence vectors are 

computed from definition pairs separately. 

The computation of the relatedness scores using 

measures listed above were carried out by codes 

from the WordNet::Similarity (Pedersen et al., 

2004) and WordNet::SenseRelate projects (Peder-

sen and Kolhatkar, 2009). In contrast to Word-

Net::SenseRelated, which employs only one 

similarity measure in the WSD process, this paper 

proposes a strategy of having several semantic re-

latedness measures vote for the best synset for each 

word. The voting algorithm intends to improve 

WSD performance by combining conclusions from 

various measures to eliminate a false result. Since 

there is no syntax among the words generated for a 

sound/image, they should all be considered for 

WSD. Thus, the width of the context window is the 

total number of words in the context. 

4 Label Sense Disambiguation Algorithm 

 
Figure 2 shows the overall process of the measure-

combined voting algorithm for disambiguating 

sound/image labels. After the context for WSD is 

generated, the process is divided into two steps. In 

Step I, the relatedness scores of each sense of a 

word based on the context is computed by each 

measure separately. Step II combines results from 

all measures and generates the disambiguated syn-

sets for all words in the sound/image labels. Evo-

cation did not participate in Step II. 

4.1 Step I: Generate Candidate Synsets Based 

on Individual Measures 

Given the context of M words (w1, …, wM), and K 

relatedness measures (k = 1, …, K), the task is to 

assign each word wj (j = 1, …, M) to the synset 

sx,wj that is the most appropriate within the context. 

Here, the word wj has Nj synsets, denoted as sn,wj (n 

= 1, …, Nj). Step I is to calculate the relatedness 

score for each synset of each word in the context. 

, , ,
1,...,

1,...,

( ) max ( ( , ))
j j m

m

m j

k i w k i w n w
n N

m M

score s measure s s
≠

=
=

= ∑  

The evocation score between two sysnets sa, sb is 

the maximum of the directed evocation ratings.  

( , ) max( ( , ), ( , ))a b a b b a
evocation
score s s evocation s s evocation s s=  

, , ,
1,...,

1,...,

( ) max ( ( , ))
j j m

m

m j

i w i w n w
evocation n N evocation

m M

score s score s s
≠

=
=

= ∑  

The synset that evocation assigns to word j is the 

one with the highest score. 

, ,j jw x ws s if=

=

  
, ,

1,...,
( ) max ( ( ))

j j
j

x w i w
evocation i N evocation
score s score s

=
=

 

4.2 Step II: Vote for the Best Candidate 

Three voting schemes were tested, including un-

weighted simple votes, weighted votes among top 

candidates, and weighted votes among all synsets. 

1) Unweighted Simple Votes 

Synset sn,wj of word wj gets a vote from related-

ness measure k if its scorek is the maximum among 

all the synsets for wj, and it becomes the candidate 

synset for wj elected by measure k (Ck,wj): 

, ,
1,...,

,

1, ( ) max ( ( ))
( )

0,

j j
j

j

k x w k i w
i N

k x w

if score s score s
vote s

else

=
=

= 


 
, ,( ) , ( ) 1j j jk w x w k x wcandidate s s if vote s= =  

The candidate list for word wj (candidates(Swj)) 

is the union of all candidate synsets elected by in-

dividual relatedness measures. 

1,...,
( ) ( ( ))

j jw k wk K
candidates s union candidate s

=
=  

For each candidate in the list, the votes from all 

measures are calculated. The one receiving the 

most votes becomes the proposed synset for wj. 

, ,

1

( ) ( )
j j

K

i w k i w

k

voteCount s vote s
=

=∑  

Figure 2. Measure-Combined Voting Algorithm. 

66



,

,

, ,
( )

,

( ) max ( ( ))

j j

j j
i w wj j

w x w

x w i w
s candidates s

s s if

voteCount s voteCount s
∈

=

=
 

2) Weighted Votes among Top Candidates 

The weighted voting scheme avoids a situation 

where the false results win by a very small margin. 

The weight under relatedness measure k for si,wj is 

calculated as the relative score to the maximum 

scorek among all synsets for word wj. It suggests 

how big of a difference in relatedness score of any 

given synset is to the highest score among all the 

possible synsets for the target word. 

, , ,
1,...,

( ) ( ) / max ( ( ))
j j j

j

k x w k x w k i w
i N

weight s score s score s
=

=  

The weighted votes synset si,wj receives over all 

measures is the sum of its weight under individual 

measure. In voting scheme 2, the synset from the 

candidate list which gets the highest weighted 

votes becomes the winner. 

, ,

1

( ) ( )
j j

K

i w k i w

k

weightedVote s weight s
=

=∑  

,

,

, ,
( )

,

( ) max ( ( ))

j j

j j
i w wj j

w x w

x w i w
s candidates s

s s if

weightedVote s weightedVote s
∈

=

=
 

3) Weighted Votes among All Synsets 

Voting scheme 3 differs from 2 in that the synset 

from all synsets for word wj which gets the highest 

weighted votes is the proposed synset for wj. 

,

, ,
1,...,

,

( ) max ( ( ))

j j

j j
j

w x w

x w i w
i N

s s if

weightedVote s weightedVote s
=

=

=
 

5 Evaluation 

The evaluation of WSD with evocation and the 

measure-combined voting algorithm was carried 

out primarily on the SoundNet label dataset be-

cause of the availability of ground truth data. 

SoundNet provides manual annotation for 1,553 

different words for 327 soundnails (e.g. the word 

“road” appears in 41 sounds). 

The accuracy rate (precision) was computed for 

each WSD method. The sound level accuracy of a 

WSDk is the average percentage of correct sense 

assignments over the 327 sounds. The word level 

accuracy is the mean over 1553 distinctive words. 

Accuracy rates of different measures at both level 

accepted the null hypothesis in homogeneity test. 
327

1

1553

1

( ) ( (% ) ) / 327

( ) ( (% ) ) /1553

k i
sound level i

k w
word level w

accuracy WSD correctness

accuracy WSD correctness

− =

− =

=

=

∑

∑

 

Due to the lack of ground truth in the Peekaboom 

dataset, we only computed the overlap between the 

WSD result of 3,086 images from the voting algo-

rithm, evocation and each relatedness measures. 

5.1 Overall Comparison across WSD me-

thods with Various Relatedness Measures 

Figures 3 show the overall comparison among dif-

ferent methods at both sound level and word level. 

It suggests that the performance of the evocation 

measure in sense disambiguation is as good as the 

path-based and context-based measures. The defi-

nition-based measures (“lesk” and “vector”) are 

significantly better than other measures if used in-

dividually (similar to (Patwardhan et al.2003)). 

 
Figure 3. Accuracy rate at word and sound level in comparison among evocation, voting, and nine individual 

sense similarity measures. 

67



However, the voting algorithms proposed in this 

work significantly outperformed each individual 

measure based on ANOVA results. At sound level, 

Context 1: (F(12, 20176) = 102.92, p < 0.001); 

Context 2: (F(12, 4238) = 89.42, p < 0.001). At 

word level, Context 1: (F(12, 20176) = 68.78, p < 

0.001); Context 2: (F(12, 4238) = 60.72, p < 0.001). 

The scheme of composing context (Section 2.1) 

has significant impact on the accuracy, with Con-

text 1 (taking all members in the related sense set 

and representatives from the others) outperforming 

Context 2 (taking all words in all sense sets) at the 

word level (F(1, 40352) = 20.19, p < 0.001). The 

influence of context scheme is not significant at the 

sound level (F(1, 8476) = 0.35, p = 0.5546). The 

interaction between measures and context schemes 

is not significant, indicating that accuracy differ-

ences are similar regardless of context construction. 

5.2 Performance of the Voting Algorithm  

Figure 4 shows the histogram (distribution) for the 

accuracy rate at sound and word levels. We see 

that for the voting algorithm, the accuracy rates are 

greater than 0.7 for most of the sounds, and greater 

than 0.9 for majority of the words to disambiguate. 

Figure 5 show the percentage of sense disam-

biguation results overlapping between voting algo-

rithm and individual relatedness measures. Note 

that any two methods may come up with different 

correct results (e.g. “lesk” assigned “chirp” as “a 

sharp sound” while the voting algorithm assigned 

“chirp” as “making a sharp sound”). This indicates 

the change of the contribution of each relatedness 

measures in different voting schemes. In the simple 

voting scheme, more disambiguation results came 

from the “path,” “wup,” and “lch” (the WordNet 

path based measures), while the weighted voting 

 
Figure 4. Histogram of accuracy rate at sound (327, left) and word level (1553, right) among different measures, 

contexts, and voting schemes. EVC1 = Evocation (Context 1); SR11 = Voting (Context 1, voting scheme 1). 

 
Figure 5. Percentage of sense disambiguation results overlap between voting algorithm, evocation, and individ-

ual sense relatedness measures at image (3,086 images) and sound (327 sounds) level. 

68



scheme took more of the recommendations from 

“lesk,” “lin,” and “jcn” (context and definition 

based measures) into consideration. At the sound 

level, there is no significant accuracy difference 

among the three voting schemes, and the influence 

of the context composition is similar. However, at 

the word level (Figure 3), the weighted voting 

schemes significantly outperformed the simple vot-

ing scheme (F(2, 9312) = 5.20, p = 0.0055), and all 

of them have significantly better accuracy when 

the context contains mainly members from the 

same sense set (F(1, 9312) = 4.79, p = 0.0287). 

5.3 Performance of WSD with Evocation 

As shown in Figures 3, the performance of the 

evocation measure is not significantly different 

from path-based and some context-based measures 

at sound level, including “path,” “wup,” “lch,” 

“res,” “lin,” and “jcn” (for Context 1, F(6, 2282) = 

2.0582, p = 0.0551; for Context 2, F(6, 2282) = 

1.6679, p = 0.1249); and is significantly better than 

the vector_pairs measure (for Context 1, F(1, 652) 

= 61.37, p < 0.001; for Context 2, F(1, 652) = 

36.47, p < 0.001). At the word level, the perfor-

mance of the evocation measure is not significantly 

different from that of measures including “path,” 

“wup,” “lch,” “res” (F(4, 7760) = 0.39, p = 0.8135), 

and “lin,”  “jcn,” and “vector_pairs” (F(3, 6208) = 

1.52, p = 0.2077). Figure 8 (SoundNet) and Figure 

9 (Peekaboom) show the percentage of synset as-

signment overlap between evocation and the other 

nine relatedness measures. The overlap with “lesk” 

and “vector” are significantly higher than that with 

the other measures (F(8, 5877) = 34.67, p < 0.001). 

It suggests that evocation as a semantic relatedness 

measure may be closer to the definition-based 

measures than path and content based measures. 

For the SoundNet dataset, 34% to 44% of evoca-

tion WSD results overlap with that of other meas-

ures; for the Peekaboom dataset, the overlap is 

25% to 35% (Figure 6). Given that evocation per-

formed similarly in accuracy to most of other 

measures with relatively low overlap in WSD re-

sults, evocation may capture different aspects of 

semantic relatedness from existing measures. 

6 Conclusion and Future Work 

We explored the construction of a sense disambi-

guated semantic AAC multimodal vocabulary from 

sound/image label datasets. Two WSD approaches 

are introduced to assign specific meanings to envi-

ronmental sound and image labels, and further 

create concept-sound/image associations. The 

measure-combined voting algorithm targets the 

accuracy of WSD and achieves significantly better 

performance than each relatedness measure indivi-

dually. Our second approach applies a new rela-

tedness measure, evocation. Evocation achieves 

similar performance to most of the existing rela-

tedness measures with sound labels. Results sug-

gest that evocation provides different semantic 

information from current measures. 

Future work includes: 1) expanding the evoca-

tion dataset and investigating the potential im-

provement in its WSD accuracy; 2) incorporating 

the extended evocation dataset into the voting al-

gorithm; 3) exploring additional information such 

as image and sound similarity to help with WSD. 

Acknowledgments  

 
Figure 6. Percentage of WSD results overlap between evocation and various relatedness measures. 

69



We thank the Kimberley and Frank H. Moss ’71 

Princeton SEAS Research Fund for supporting our 

project. 

References  

Satanjeev Banerjee and Ted Pedersen. 2002. An 

Adapted Lesk Algorithm for Word Sense Disambig-

uation Using WordNet. Proceedings of the 3
rd
 Inter-

national Conference on Intelligent Text Processing 

and Computational Linguistics. 

Jordan Boyd-Graber, Christaine Fellbaum, Daniel 

Osherson, and Robert Schapire. 2006. Adding Dense, 

Weighted Connections to WordNet. Proceedings of 

the Thirds International WordNet Conference. 

Jia Deng, Wei Dong, Richard Socher, Li -J. Li, Kai Li 

and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hie-

rarchical Image Database. Proceedings of the IEEE 

Computer Vision and Pattern Recognition (CVPR). 

Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Lexical Database. MIT Press. 

Marti Hearst. 1991. Noun Homograph Disambiguation 

Using Local Context in Large Text Corpora. Proc. of 

the 7
th
 Annual Conference of the University of Water-

loo Center for the New OED and Text Research. 

Graeme Hirst and David St. Onge. 1998. Lexical Chains 

as Representations of Context for the Detection and 

Correction of Malapropisms. In Christiane Fellbaum, 

editor, WordNet: An Electronic Lexical Database. 

Jay Jiang and David Conrath. 1997. Semantic Similarity 

Based on Corpus Statistics and Lexical Taxonomy. 

Proceedings on International Conference on Re-

search in Computational Linguistics.  

Claudia Leacock and Martin Chodorow. 1998. Combin-

ing Local Context and WordNet Similarity for Word 

Sense Identification. In Christiane Fellbaum, editor, 

WordNet: An Electronic Lexical Database. 

Michael Lesk. 1986. Automatic Sense Disambiguation 

Using Machine Readable Dictionaries: How to Tell a 

Pine Cone from an Ice Cream Cone. Proceedings of 

SIGDOC’86. 

Dekang Lin. 1997. Using Syntactic Dependency as a 

Local Context to Resolve Word Sense Ambiguity. 

Proceedings of the 35
th
 Annual Meeting of the Asso-

ciation for Computational Linguistics, pp. 64-71. 

Lingraphica. http://www.aphasia.com/. 2010. 

Xiaojuan Ma, Christiane Fellbaum. and Perry Cook. 

2010. SoundNet: Investigating a Language Com-

posed of Environmental Sounds. In Proc. CHI 2010. 

Xiaojuan Ma, Jordan Boy-Graber, Sonya Nikolova, and 

Perry Cook. 2009a. Speaking Through Pictures: Im-

ages vs. Icons. Proceedings of ASSETS09. 

Xiaojuan Ma, Sonya Nikolova and Perry Cook. 2009b. 

W2ANE: When Words Are Not Enough - Online 

Multimedia Language Assistant for People with 

Aphasia. Proceedings of ACM Multimedia 2009.  

Sonya Nikolova, Jordan Boyd-Graber, and Christiane 

Fellbaum. 2009. Collecting Semantic Similarity Rat-

ings to Connect Concepts in Assistive Communica-

tion Tools (in press). Modelling, Learning and 

Processing of Text-Technological Data Structures, 

Springer Studies in Computational Intelligence. 

Adrian Novischi, Muirathnam Srikanth, and Andrew 

Bennett. 2007. Lcc-wsd: System Description for 

English Coarse Grained All Words Task at SemEval 

2007. Proceedings of the 4
th
 International Workshop 

on Semantic Evaluations(SemEval-2007), pp 223-226. 

Siddharth Patwardhan, Satanjeev Benerjee and Ted Pe-

dersen. Using Measures of Semantic Relatedness for 

Word Sense Disambiguation. 2003. Proceeding of 

CICLing2003, pp. 241-257. 

Siddharth Patwardhan and Ted Pedersen Using Word-

Net Based Context Vectors to Estimate the Semantic 

Relatedness of Concepts. 2006. Proceedings of the 

EACL 2006 Workshop Making Sense of Sense - 

Bringing Computational Linguistics and Psycholin-

guistics Together, pp. 1-8 

Ted Pedersen, Siddharth Patwardhan, and Jason Miche-

lizzi. 2004. WorNet::Similarity – Measuring the Re-

latedness of Concepts.  Proceedings of Human 

Language Technology Conference of the North 

American Chapter of the Association for Computa-

tional Linguistics Demonstrations, pp. 38-41. 

Ted Pedersen and Varada Kolhatkar. 2009. Word-

Net::SenseRelate::AllWords - A Broad Coverage 

Word Sense Tagger that Maximimizes Semantic Re-

latedness. Proceedings of Human Language Tech-

nology Conference of the North American Chapter of 

the Association for Computational Linguistics Dem-

onstrations, pp. 17-20. 

Philip Resnik. 1995. Using Information Content to Eva-

luate Semantic Similarity in a Taxonomy. Proceed-

ings of the 14
th
 International Joint Conference on 

Artificial Intelligence. 

Richard Steele, Michael Weinrich, Robert Wertz, Gloria 

Carlson, and Maria Kleczewska. Computer-based 

visual communication in aphasia. Neuropsychologia. 

27(4): pp 409-26. 1989. 

Luis von Ahn, Laura Dabbish. 2004. Labeling images 

with a computer game. Proceedings of the SIGCHI 

conference on Human factors in computing systems, 

p.319-326. 

Luis von Ahn, Ruoran Liu, Manuel Blum. 2006 Peeka-

boom: a game for locating objects in images. Pro-

ceedings of the SIGCHI conference on Human 

Factors in computing systems. 

Zhibiao Wu and Martha Palmer. 1994. Verb Semantics 

and Lexical Selection. Proc. of ACL, pp 133-138. 

David Yarowsky. 1995. Unsupervised Word Sense Dis-

ambiguation Rivaling Supervised Methods. Proceed-

ings of the 33
rd
 Annual Meeting on Association For 

Computational Linguistics. 

70


