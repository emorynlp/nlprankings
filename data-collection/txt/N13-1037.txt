










































What to do about bad language on the internet


Proceedings of NAACL-HLT 2013, pages 359–369,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

What to do about bad language on the internet

Jacob Eisenstein
jacobe@gatech.edu

School of Interactive Computing
Georgia Institute of Technology

Abstract

The rise of social media has brought compu-
tational linguistics in ever-closer contact with
bad language: text that defies our expecta-
tions about vocabulary, spelling, and syntax.
This paper surveys the landscape of bad lan-
guage, and offers a critical review of the NLP
community’s response, which has largely fol-
lowed two paths: normalization and domain
adaptation. Each approach is evaluated in the
context of theoretical and empirical work on
computer-mediated communication. In addi-
tion, the paper presents a quantitative analy-
sis of the lexical diversity of social media text,
and its relationship to other corpora.

1 Introduction

As social media becomes an increasingly important
application domain for natural language processing,
we encounter language that is substantially different
from many benchmark corpora. The following ex-
amples are all from the social media service Twitter:

• Work on farm Fri. Burning piles of brush
WindyFire got out of control. Thank God for
good naber He help get undr control Pants-
BurnLegWound. (Senator Charles Grassley)

• Boom! Ya ur website suxx bro
(Sarah Silverman)

• ...dats why pluto is pluto it can neva b a star
(Shaquille O’Neil)

• michelle obama great. job. and. whit all my.
respect she. look. great. congrats. to. her.
(Ozzie Guillen)

These examples are selected from celebrities (for
privacy reasons), but they contain linguistic chal-
lenges that are endemic to the medium, including
non-standard punctuation, capitalization, spelling,
vocabulary, and syntax. The consequences for lan-
guage technology are dire: a series of papers has
detailed how state-of-the-art natural language pro-
cessing (NLP) systems perform significantly worse
on social media text. In part-of-speech tagging, the
accuracy of the Stanford tagger (Toutanova et al.,
2003) falls from 97% on Wall Street Journal text to
85% accuracy on Twitter (Gimpel et al., 2011). In
named entity recognition, the CoNLL-trained Stan-
ford recognizer achieves 44% F-measure (Ritter et
al., 2011), down from 86% on the CoNLL test
set (Finkel et al., 2005). In parsing, Foster et al.
(2011) report double-digit decreases in accuracy for
four different state-of-the-art parsers when applied
to social media text.

The application of language technology to so-
cial media is potentially transformative, leveraging
the knowledge and perspectives of millions of peo-
ple. But to deliver on this potential, the problems
at the core of the NLP pipeline must be addressed.
A growing thread of research takes up this chal-
lenge, including a shared task and workshop on
“parsing the web,” with new corpora which appear
to sit somewhere between the Wall Street Journal
and Twitter on the spectrum of bad language (Petrov
and McDonald, 2012). But perhaps surprisingly,
very little of this research has considered why social
media language is so different. This review paper
attempts to shed some light on this question, sur-
veying a strong tradition of empirical and theoreti-

359



cal research on computer-mediated communication
(CMC). I argue that the two main computational ap-
proaches to dealing with bad language — normaliza-
tion and domain adaptation — are based on theories
of social media language that are not descriptively
accurate. I have worked and continue to work in
both of these areas, so I make this argument not as
a criticism of others, but in a spirit of self-reflection.
It is hoped that a greater engagement with sociolin-
guistic and CMC research will lead to new, nuanced
approaches to the challenge of bad language.

Why so much Twitter? Most of the examples
in this paper will focus on Twitter, a microblog-
ging service. Munro and Manning (2012) argue
that Twitter has unfairly dominated recent research,
at the expense of email and SMS text messages,
which they found to be both linguistically distinct
from Twitter and significantly more prevalent (in
2010). This matches earlier research arguing that
email contained relatively little “neography,” com-
pared with text messages and chat (Anis, 2007).

A crucial advantage for Twitter is that it is public
by default, while SMS and email are private. This
makes Twitter data less problematic from a privacy
standpoint,1 far easier to obtain, and more amenable
to target applications such as large-scale mining of
events (Sakaki et al., 2010; Benson et al., 2011) and
opinions (Sauper et al., 2011). Similar argument
could be made on behalf of other public social me-
dia, such as blog comments (Ali-Hasan and Adamic,
2007), forums, and chatrooms (Paolillo, 2001). The
main advantage of Twitter over these media is con-
venience in gathering large datasets through a sin-
gle streaming interface. More comparative evalu-
ation is needed to determine linguistic similarities
and differences between Twitter and these other me-
dia; Section 4 presents an evaluation of the lexical
similarity between Twitter and political blogs.

2 A tour of bad language

While many NLP researchers and engineers have
wrestled with the difficulties imposed by bad lan-
guage, there has been relatively little considera-
tion of why language in social media is so differ-
ent from our other corpora. A survey of laypeo-

1boyd and Crawford (2012) note that “public by default”
data still raises important ethical considerations.

ple found that more than half of the respondents
agreed with the following partial explanations for
non-standard spelling on the internet: “people are
unsure of the correct spellings,” “it’s faster,” “it’s be-
come the norm,” and “people want to represent their
own dialects and/or accents” (Jones, 2010). Let us
now consider the evidence for these and other poten-
tial explanations.

2.1 Illiteracy

Some commentators have fixated on the proposal
that the authors of non-standard language in social
media are simply unaware or incapable of using
more standard language (Thurlow, 2006). But em-
pirical research suggests that many users of bad lan-
guage are capable of using more traditional forms.
Drouin and Davis (2009) find no significant differ-
ences in the literacy scores of individuals who do
or do not use non-standard vocabulary in text mes-
sages. Tagliamonte and Denis (2008) review traces
of instant messaging conversations among students,
arguing that they “pick and choose ... from the en-
tire stylistic repertoire of the language” in a way that
would be impossible without skilled command of
both formal and informal registers. While news text
is usually more carefully composed and edited than
much of the language in social media, there is little
evidence that bad language results from an inability
to speak anything else.

2.2 Length limits

In the case of Twitter, the limit of 140 characters for
each message is frequently cited as an explanation
for bad language (Finin et al., 2010). Does Twitter’s
character limit cause users to prefer shorter words,
such as u instead of you? If so, one might expect
shortening to be used most frequently in messages
that are near the 140-character limit. Using a dataset
of one million English-language tweets (Bamman et
al., 2012), I have computed the average length of
messages containing both standard words and their
non-standard alternatives, focusing on the top five
non-standard shortenings identified by the automatic
method of Gouws et al. (2011a). The shortening ur
can substitute for both your and you’re. While wit
and bout are also spellings for standard words, man-
ual examination of one hundred randomly selected
examples for each surface form revealed only one

360



standard length alternative length
your 85.1± 0.4

ur 81.9± 0.6
you’re 90.0± 0.1
with 87.9± 0.3 wit 78.8± 0.7

going 82.7± 0.5 goin 72.2± 1.0
know 86.1± 0.4 kno 78.4± 1.0
about 88.9± 0.4 bout 74.5± 0.7

Table 1: Average length of messages containing standard
forms and their shortenings

case in which the standard meaning was intended for
wit, and none for bout.

The average message lengths are shown in Ta-
ble 1. In all five cases, the non-standard form tends
to be used in shorter messages — not in long mes-
sages near the 140 character limit. Moreover, this
difference is substantially greater than the saving of
one or two characters offered by shortened form.
This is not consistent with the explanation that Twit-
ter’s character limit is the primary factor driving the
use of shortened forms. It is still possible that Twit-
ter’s length limitations might indirectly cause word
shortenings: for example, by legitimizing shortened
forms or causing authors to develop a habit of pre-
ferring them. But factors other than the length limit
must be recruited to explain why such conventions
or habits apply only to some messages and not oth-
ers.

2.3 Text input affordances

Text input affordances — whether standard key-
boards or predictive entry on mobile devices — play
a role in computer-mediated communication that is
perhaps under-appreciated. Gouws et al. (2011b) in-
vestigate orthographic variation on Twitter, and find
differences across devices: for example, that mes-
sages from iPhones include more contractions than
messages from Blackberries, and that tweets sent
from the web browser are more likely to drop vow-
els. While each affordance facilitates some writ-
ing styles and inhibits others, the affordances them-
selves are unevenly distributed across users. For ex-
ample, older people may prefer standard keyboards,
and wealthier people may be more likely to own
iPhones. Affordances are a moving target: new de-
vices and software are constantly becoming avail-
able, the software itself may adapt to the user’s in-

put, and the user may adapt to the software and de-
vice.

2.4 Pragmatics

Emoticons are frequently thought of as introduc-
ing an expressive, non-verbal component into writ-
ten language, mirroring the role played by facial ex-
pressions in speech (Walther and D’Addario, 2001),
but they can also be seen as playing a pragmatic
function: marking an utterance as facetious, or
demonstrating a non-confrontational, less invested
stance (Dresner and Herring, 2010). In many cases,
phrasal abbreviations like lol (laugh out loud),
lmao (laughing my ass off ), smh (shake my head),
and ikr (i know, right?) play a similar role: yea she
dnt like me lol; lmao I’m playin son. A key differ-
ence from emoticons is that abbreviations can act
as constituents, as in smh at your ignorance. An-
other form of non-standard language is expressive
lengthening (e.g., coooolllllll), found by Brody and
Diakopoulos (2011) to indicate subjectivity and sen-
timent. In running dialogues — such as in online
multiplayer games — the symbols * and ˆ can play
an explicit pragmatic function (Collister, 2011; Col-
lister, 2012).

2.5 Social variables

A series of papers has documented the interac-
tions between social media text and social vari-
ables such as age (Burger and Henderson, 2006;
Argamon et al., 2007; Rosenthal and McKeown,
2011), gender (Burger et al., 2011; Rao et al., 2010),
race (Eisenstein et al., 2011), and location (Eisen-
stein et al., 2010; Wing and Baldridge, 2011). From
this literature, it is clear that many of the features
that characterize bad language have strong associa-
tions with specific social variables. In some cases,
these associations mirror linguistic variables known
from speech — such as geographically-associated
lexical items like hella, or transcriptions of phono-
logical variables like “g-dropping” (Eisenstein et al.,
2010). But in other cases, apparently new lexical
items, such as the abbreviations ctfu, lls, and af,
acquire surprisingly strong associations with geo-
graphical areas and demographic groups (Eisenstein
et al., 2011).

A robust finding from the sociolinguistics litera-
ture is that non-standard forms that mark social vari-

361



ables, such as regional dialects, are often inhibited in
formal registers (Labov, 1972). For example, while
the Pittsburgh spoken dialect sometimes features the
address term yinz (Johnstone et al., 2006), one would
not expect to find many examples in financial re-
ports. Other investigators have found that much of
the content in Twitter concerns social events and self
presentation (Ramage et al., 2010), which may en-
courage the use of less formal registers in which
socially-marketed language is uninhibited.

The use of non-standard language is often seen
as a form of identity work, signaling authentic-
ity, solidarity, or resistance to norms imposed from
above (Bucholtz and Hall, 2005). In spoken lan-
guage, many of the linguistic variables that perform
identity work are phonological — for example, Eck-
ert (2000) showed how the northern cities vowel
shift was used by a subset of suburban teenagers to
index affiliation with Detroit. The emergence of new
linguistic variables in social media suggests that this
identity work is as necessary in social media as it
is in spoken language. Some of these new variables
are transcriptions of existing spoken language vari-
ables: like finna, which transcribes fixing to. Oth-
ers — abbreviations like ctfu and emoticons — seem
to be linguistic inventions created to meet the needs
of social communication in a new medium. In an
early study of variation in social media, Paolillo
(1999) notes that code-switching between English
and Hindi also performs this type of identity work.

Finally, it is an uncomfortable fact that the text
in many of our most frequently-used corpora was
written and edited predominantly by working-age
white men. The Penn Treebank is composed of
professionally-written news text from 1989, when
minorities comprised 7.5% of the print journalism
workforce; the proportion of women in the journal-
ism workforce was first recorded in 1999, when it
was 37% (American Society of Newspaper Editors,
1999). In contrast, Twitter users in the USA con-
tain an equal proportion of men and women, and
a higher proportion of young adults and minorities
than in the population as a whole (Smith and Brewer,
2012). Such demographic differences are very likely
to lead to differences in language (Green, 2002;
Labov, 2001; Eckert and McConnell-Ginet, 2003).

Overall, the reasons for language diversity in so-
cial media are manifold, though some of the most

frequently cited explanations (illiteracy and length
restrictions) do not hold up to scrutiny. The in-
creasing prevalence of emoticons, phrasal abbrevi-
ations (lol, ctfu), and expressive lengthening may
reflect the increasing use of written language for
ephemeral social interaction, with the concomitant
need for multiple channels through which to express
multiple types of meaning. The fact many such neol-
ogisms are closely circumscribed in geography and
demographics may reflect diffusion through social
networks that are assortative on exactly these dimen-
sions (Backstrom et al., 2010; Thelwall, 2009). But
an additional consideration is that non-standard lan-
guage is deliberately deployed in the performance of
identity work and stancetaking. This seems a partic-
ularly salient explanation for the use of lexical vari-
ables that originate in spoken language (jawn, hella),
and for the orthographic transcription of phonolog-
ical variation (Eisenstein, 2013). Determining the
role and relative importance of social network diffu-
sion and identity work as factors in the diversifica-
tion of social media language is an exciting direction
for future research.

3 What can we do about it?

Having surveyed the landscape of bad language and
its possible causes, let us now turn to the responses
offered by the language technology research com-
munity.

3.1 Normalization

One approach to dealing with bad language is to
turn it good: “normalizing” social media or SMS
messages to better conform to the sort of language
that our technology expects. Approaches to normal-
ization include the noisy-channel model (Cook and
Stevenson, 2009), string and distributional similar-
ity (Han and Baldwin, 2011; Han et al., 2012), se-
quence labeling (Choudhury et al., 2007; Liu et al.,
2011a), and machine translation (Aw et al., 2006).
As this task has been the focus of substantial atten-
tion in recent years, labeled datasets have become
available and accuracies have climbed.

That said, it is surprisingly difficult to find a
precise definition of the normalization task. Writ-
ing before social media was a significant focus for
NLP, Sproat et al. (2001) proposed to replace non-

362



standard words with “the contextually appropriate
word or sequence of words.” In some cases, this
seems clear enough: we can rewrite dats why pluto
is pluto with that’s why... But it is not difficult to find
cases that are less clear, putting would-be normaliz-
ers in a difficult position. The labeled dataset of Han
and Baldwin (2011) addresses a more tractable sub-
set of the normalization problem, annotating only
token-to-token normalizations. Thus, imma — a
transcription of I’m gonna, which in turn transcribes
I’m going to — is not normalized in this dataset. Ab-
breviations like LOL and WTF are also not normal-
ized, even when they are used to abbreviate syntac-
tic constituents, as in wtf is the matter with you? Nor
are words like hella and jawn normalized, since they
have no obvious one-word transcription in standard
English. These decisions no doubt help to solidify
the reliability of the annotations, but they provide an
overly optimistic impression of the ability of string
edit distance and related similarity-based techniques
to normalize bad language. The resulting gold stan-
dard annotations seem little more amenable to au-
tomated parsing and information extraction than the
original text.

But if we critique normalization for not going
far enough, we must also ask whether it goes too
far. The logic of normalization presupposes that the
“norm” can be identified unambiguously, and that
there is a direct mapping from non-standard words
to the elements in this normal set. On closer exami-
nation, the norm reveals itself to be slippery. Whose
norm are we targeting? Should we normalize flvr to
flavor or flavour? Where does the normal end and
the abnormal begin? For example, Han and Baldwin
normalize ain to ain’t, but not all the way to isn’t.
While ain’t is certainly well-known to speakers of
Standard American English, it does not appear in the
Penn Treebank and probably could not be used in the
Wall Street Journal, except in quotation.

Normalization is often impossible without chang-
ing the meaning of the text. Should we normalize the
final word of ya ur website suxx bro to brother? At
the very least, this adds semantic ambiguity where
there was none before (is she talking to her biolog-
ical brother? or possibly to a monk?). Language
variation does not arise from passing standard text
through a noisy channel; it often serves a pragmatic
and/or stancetaking (Du Bois, 2007) function. Elim-

inating variation would strip those additional lay-
ers of meaning from whatever propositional content
might survive the normalization process. Sarah Sil-
verman’s ya ur website suxx bro can only be under-
stood as a critique from a caricatured persona — the
type of person who ends sentences with bro. Sim-
ilarly, we can assume that Shaquille O’Neil is ca-
pable of writing that’s why Pluto is Pluto, but that
to do so would convey an undesirably didactic and
authoritative stance towards the audience and topic.

This is not to deny that there is great poten-
tial value in research aimed at understanding or-
thographic variation through a combination of lo-
cal context, string similarity, and related finite-state
machinery. Given the productivity of orthographic
substitutions in social media text, it is clear that lan-
guage technology must be made more robust. Nor-
malization may point the way towards such robust-
ness, even if we do not build an explicit normaliza-
tion component directly into the language process-
ing pipeline. Another potential benefit of this re-
search is to better understand the underlying ortho-
graphic processes that lead to the diversity of lan-
guage in social media, how these processes diffuse
over social networks, and how they impact compre-
hensibility for both the target and non-target audi-
ences.

3.2 Domain adaptation
Rather than adapting text to fit our tools, we may
instead adapt our tools to fit the text. A series of
papers has followed the mold of “NLP for Twit-
ter,” including part-of-speech tagging (Gimpel et al.,
2011; Owoputi et al., 2013), named entity recogni-
tion (Finin et al., 2010; Ritter et al., 2011; Liu et al.,
2011b), parsing (Foster et al., 2011), dialogue mod-
eling (Ritter et al., 2010) and summarization (Sharifi
et al., 2010). These papers adapt various parts of the
natural language processing pipeline for social me-
dia text, and make use of a range of techniques:

• preprocessing to normalize expressive length-
ening, and eliminate or group all hashtags,
usernames, and URLs (Gimpel et al., 2011;
Foster et al., 2011)

• new labeled data, enabling the application of
semi-supervised learning (Finin et al., 2010;
Gimpel et al., 2011; Ritter et al., 2011)

363



• new annotation schemes specifically cus-
tomized for social media text (Gimpel et al.,
2011)

• self-training on unlabeled social media
text (Foster et al., 2011)

• distributional features to address the sparsity
of bag-of-words features (Gimpel et al., 2011;
Owoputi et al., 2013; Ritter et al., 2011)

• joint normalization, incorporated directly into
downstream application (Liu et al., 2012)

• distant supervision, using named entity on-
tologies and topic models (Ritter et al., 2011)

Only a few of these techniques (normalization and
new annotation systems) are specific to social me-
dia; the rest can found in other domain adaptation
settings. Is domain adaptation appropriate for social
media? Darling et al. (2012) argue that social me-
dia is not a coherent domain at all, and that a POS
tagger for Twitter will not necessarily generalize to
other social media. One can go further: Twitter it-
self is not a unified genre, it is composed of many
different styles and registers, with widely varying
expectations for the degree of standardness and di-
mensions of variation (Androutsopoulos, 2011). I
am the co-author on a paper entitled “Part-of-speech
tagging for Twitter,” but if we take this title literally,
it is impossible on a trivial level: Twitter contains
text in dozens or hundreds of languages, including
many for which no POS tagger exists. Even within
a single language — setting aside issues of code-
switching (Paolillo, 1996) — Twitter and other so-
cial media can contain registers ranging from hash-
tag wordplay (Naaman et al., 2011) to the official
pronouncements of the British Monarchy. And even
if all good language is alike, bad language can be
bad in many different ways — as Androutsopoulos
(2011) notes when contrasting the types of variation
encountered when “visiting a gamer forum” versus
“joining the Twitter profile of a rap star.”

4 The lexical coherence of social media

The internal coherence of social media — and its
relationship to other types of text — can be quan-
tified in terms of the similarity of distributions over

bigrams. While there are many techniques for com-
paring word distributions, I apply the relatively sim-
ple method of counting out-of-vocabulary (OOV) bi-
grams. The relationship between OOV rate and do-
main adaptation has been explored by McClosky et
al. (2010), who use it as a feature to predict how well
a parser will perform when applied across domains.2

Specifically, the datasets A and B are compared
by counting the number of bigram tokens in A that
are unseen in B. The following corpora are com-
pared:

• Twitter-month: randomly selected tweets
from each month between January 2010 to Oc-
tober 2012 (Eisenstein et al., 2012).

• Twitter-hour: randomly selected tweets from
each hour of the day, randomly sampled during
the period from January 2010 to October 2012.

• Twitter-#: tweets in which the first token is a
hashtag. The hashtag itself is not included in
the bigram counts; see below for more details
on which bigrams are included.

• Twitter-@: tweets in which the first token is a
username. The username itself is not included
in the bigram counts.

• Penn Treebank: sections 2-21

• Infinite Jest: the text of the 1996 novel by
David Foster Wallace (Wallace, 2012). Con-
sists of only 482,558 tokens.

• Blog articles: A randomly-sampled subset
of the American political blog posts gathered
by Yano et al. (2009).

• Blog comments: A randomly-selected subset
of comments associated with the blog posts de-
scribed above.

In all corpora, only fully alphabetic tokens are
counted; thus, all hashtags and usernames are dis-
carded. The Twitter text is tokenized using Tweet-

2A very recent study compares Twitter with other corpora,
using a number of alternative metrics, such as the use of high
and low frequency words, pronouns, and intensifiers (Hu et al.,
2013). This is complementary to the present study, which fo-
cuses on the degree of difference in the lexical distributions of
corpora gathered from various media.

364



5 10 15 20
month gap

1.000

1.005

1.010

1.015

1.020

1.025

1.030

1.035
re

la
ti

v
e
 p

ro
p
o
rt

io
n
 o

f 
O

O
V

 b
ig

ra
m

s

with NEs

without NEs

Figure 1: Lexical mismatch increases over time, as social
media language evolves.

motif;3 the Penn Treebank data uses the gold stan-
dard tokenization; Infinite Jest and the blog data are
tokenized using NLTK (Bird et al., 2009). All to-
kens are downcased, and sequences of three or more
consecutive identical characters are reduced to three
characters (e.g., coooool→ coool). All Twitter cor-
pora are subject to the following filters: messages
must be from the United States and should be written
in English,4 they may not include hyperlinks (elim-
inating most marketing messages), they may not be
retweets, and the author must not have more than
1,000 followers or follow more than 1,000 people.
These criteria serve to eliminate text from celebri-
ties, businesses, or automated bots.

Twitter over time Figure 1 shows how the pro-
portion of out-of-vocabulary bigrams increases over
time. It is possible that the core features of language
are constant but the set of named entities that are
mentioned changes over time. To control for this,
the CMU Twitter Part-of-Speech tagger (Owoputi et
al., 2013) was used to identify named entity men-
tions, and they were replaced with a special token.

3https://github.com/brendano/tweetmotif
4Approximate language detection was performed as follows.

We first identify the 1000 most common words, then sort all au-
thors by the proportion of these types that they used, and elim-
inate the bottom 10%. This filtering mechanism eliminates in-
dividuals who never write in English, but a small amount of
foreign language still enters the dataset via code-switching au-
thors. The effect of more advanced language detection meth-
ods (Bergsma et al., 2012) on these results may be considered
in future work.

2 4 6 8 10 12
hour gap

1.000

1.005

1.010

1.015

1.020

1.025

1.030

1.035

1.040

re
la

ti
v
e
 p

ro
p
o
rt

io
n
 o

f 
O

O
V

 b
ig

ra
m

s

with NEs

without NEs

Figure 2: Different times of day have unique lexical sig-
natures, reflecting differing topics and authors.

The OOV rate is standardized with respect to a one-
month time gap, where it is 24.4% when named en-
tities are included, and 21.3% when they are not.
These rates reach maxima at 25.2% and 22.0% re-
spectively, with dips at 12 and 24 months indicat-
ing cyclic yearly effects. While the proportion of
OOV tokens is smaller when named entities are not
included, the rate of growth is similar in each case.
The steadily increasingly rate of OOV bigrams sug-
gests that we cannot annotate our way out of the bad
language problem. An NLP system trained from
data gathered in January 2010 will be increasingly
outdated as time passes and social media language
continues to evolve.

One need not wait months to see language change
on Twitter: marked changes can be observed over
the course of a single day (Golder and Macy, 2011).
A quantitative comparison is shown in Figure 2.
Here the OOV rate is standardized with respect to
a one-hour gap, where it is 24.2% when named en-
tities are included, and 21.1% when they are not.
These rates rise monotonically as the time gap in-
creases, peaking at 25.1% and 21.9% respectively.
Such diurnal changes may reflect the diverse lan-
guage of the different types of authors who post
throughout the day.

Types of usage The Twitter-# and Twitter-@ cor-
pora are designed to capture the diversity of ways
in which social media is used to communicate.
Twitter-# contains tweets that begin with hashtags,
and are thus more likely to be part of running jokes

365



or trending topics (Naaman et al., 2011). Twitter-
@ contains tweets that begin with usernames — an
addressing mechanism that is used to maintain dia-
logue threads on the site. These datasets are com-
pared with a set of randomly selected tweets from
June 2011, and with several other corpora: Penn
Treebank, the novel Infinite Jest, and text and com-
ments from political blogs. There was no attempt to
remove named entities from any of these corpora, as
such a comparison would merely reflect the different
accuracy levels of NER in each corpus.

The results are shown in Table 2. A few observa-
tions stand out. First, the Penn Treebank is the clear
outlier: a PTB dictionary has by far the most OOV
tokens for all three Twitter domains and Infinite Jest,
although it is a better match for the blog corpora
than Infinite Jest is. Second, the social media are
fairly internally coherent: the Twitter datasets bet-
ter match each other than any other corpus, with a
maximum OOV rate of 33.4 for Twitter-# against
Twitter-@, though this is significantly higher than
the OOV rate of 27.8 between two separate generic
Twitter samples drawn from the same month. Fi-
nally, the OOV rate increase between Twitter and
blogs — also social media — is substantial. Con-
trary to expectations, the Blog-body corpus was no
closer to the PTB standard than Blog-comment.

These results suggest that the Penn Treebank cor-
pus is so distant from social media that there are in-
deed substantial gains to be reaped by adapting from
news text towards generic Twitter or Blog target do-
mains. The internal differences within these social
media — at least as measured by the distinctions
drawn in Table 2 — are much smaller than the dif-
ferences between these corpora and the PTB stan-
dard. However, in the long run, the effectiveness
of this approach will be limited, as it is clear from
Figure 1 that social media is a moving target. Any
static system that we build today, whether by man-
ual annotation or automated adaptation, will see its
performance decay over time.

5 What to do next

Language is shaped by a constant negotiation be-
tween processes that encourage change and linguis-
tic diversity, and countervailing processes that en-
force existing norms. The decision of the NLP com-

munity to focus so much effort on news text is em-
inently justified on practical grounds, but has unin-
tended consequences not just for technology but for
language itself. By developing software that works
best for standard linguistic forms, we throw the
weight of language technology behind those forms,
and against variants that are preferred by disempow-
ered groups. By adopting a model of “normaliza-
tion,” we declare one version of language to be the
norm, and all others to be outside that norm. By
adopting a model of “domain adaptation,” we con-
fuse a medium with a coherent domain. Adapting
language technology towards the median Tweet can
improve accuracy on average, but it is certain to
leave many forms of language out.

Much of the current research on the relationship
between social media language and metadata has the
goal of using language to predict the metadata —
revealing who is a woman or a man, who is from
Oklahoma or New Jersey, and so on. This perspec-
tive on social variables and personal identity ignores
the local categories that are often more linguisti-
cally salient (Eckert, 2008); worse, it strips individ-
uals of any agency in using language as a resource
to create and shape their identity (Coupland, 2007),
and conceals the role that language plays in creating
and perpetuating categories like gender (Bucholtz
and Hall, 2005). An alternative possibility is to re-
verse the relationship between language and meta-
data, using metadata to achieve a more flexible and
heterogeneous domain adaptation that is sensitive to
the social factors that shape variation. Such a re-
versal would help language technology to move be-
yond false dichotomies between normal and abnor-
mal text, source and target domains, and good and
bad language.

Acknowledgments

This paper benefitted from discussions with David
Bamman, Natalia Cecire, Micha Elsner, Sharon
Goldwater, Scott Kiesling, Brendan O’Connor,
Tyler Schnoebelen, and Yi Yang. Many thanks to
Brendan O’Connor and David Bamman for provid-
ing Twitter datasets, Tae Yano for the blog com-
ment dataset, and Byron Wallace for the Infinite Jest
dataset. Thanks also to the anonymous reviewers for
their helpful feedback.

366



Tw-June Tw-@ Tw-# Blog-body Blog-comment Infinite-Jest PTB
Tw-June 28.7 29.3 47.1 48.6 54.0 63.9
Tw-@ 25.9 29.7 47.8 49.9 56.3 66.4
Tw-# 29.8 33.4 49.6 51.0 54.7 66.2
Blog-body 41.9 44.1 43.8 27.2 49.1 48.0
Blog-comment 47.4 49.6 49.2 30.2 53.0 48.4
Infinite-Jest 49.4 51.1 49.9 48.3 47.4 55.5
PTB 72.2 73.1 72.7 64.5 61.9 71.9

Table 2: Percent OOV bigram tokens across corpora. Rows are the dataset providing the tokens, columns are the
dataset providing the dictionary.

References

Noor Ali-Hasan and Lada Adamic. 2007. Expressing so-
cial relationships on the blog through links and com-
ments. In Proceedings of ICWSM.

American Society of Newspaper Editors. 1999. 1999
Newsroom Census: Minority Employment Inches up in
Daily Newspapers. American Society of Newspaper
Editors, Reston, VA.

Jannis Androutsopoulos. 2011. Language change and
digital media: a review of conceptions and evidence.
In Nikolas Coupland and Tore Kristiansen, editors,
Standard Languages and Language Standards in a
Changing Europe. Novus, Oslo.

Jacques Anis. 2007. Neography: Unconventional
spelling in French SMS text messages. In Brenda
Danet and Susan C. Herring, editors, The multilingual
internet: Language, culture, and communication on-
line, pages 87 – 115. Oxford University Press.

Shlomo Argamon, Moshe Koppel, James W. Pennebaker,
and Jonathan Schler. 2007. Mining the blogosphere:
age, gender, and the varieties of self-expression. First
Monday, 12(9).

AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of ACL, pages 33–40.

Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
WWW, pages 61–70.

David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in twitter: Styles, stances, and
social networks. Technical Report 1210.4567, arXiv,
October.

Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of ACL.

Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter

collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65–74, June.

Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python. O’Reilly Me-
dia, Incorporated.

danah boyd and Kate Crawford. 2012. Critical questions
for big data. Information, Communication & Society,
15(5):662–679, May.

Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP.

Mary Bucholtz and Kira Hall. 2005. Identity and inter-
action: A sociocultural linguistic approach. Discourse
studies, 7(4-5):585–614.

John D. Burger and John C. Henderson. 2006. An explo-
ration of observable features related to blogger age. In
AAAI Spring Symposium: Computational Approaches
to Analyzing Weblogs.

John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on twit-
ter. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10(3):157–174.

Lauren B. Collister. 2011. *-repair in online discourse.
Journal of Pragmatics, 43(3):918–921, February.

Lauren B. Collister. 2012. The discourse deictics ˆ and
<-- in a world of warcraft community. Discourse,
Context & Media, 1(1):9–19, March.

Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Pro-
ceedings of the NAACL-HLT Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71–
78.

Nikolas Coupland. 2007. Style (Key Topics in Sociolin-
guistics). Cambridge University Press, July.

367



William M. Darling, Michael J. Paul, and Fei Song.
2012. Unsupervised part-of-speech tagging in
noisy and esoteric domains with a syntactic-semantic
bayesian hmm. In Proceedings of EACL Workshop on
Semantic Analysis in Social Media.

Eli Dresner and Susan C. Herring. 2010. Functions of
the non-verbal in cmc: Emoticons and illocutionary
force. Communication Theory, 20(3):249–268.

Michelle Drouin and Claire Davis. 2009. R u txting? is
the use of text speak hurting your literacy? Journal of
Literacy Research, 41(1):46–67.

John W. Du Bois. 2007. The stance triangle. In Robert
Engelbretson, editor, Stancetaking in discourse, pages
139–182. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.

Penelope Eckert and Sally McConnell-Ginet. 2003. Lan-
guage and Gender. Cambridge University Press, New
York.

Penelope Eckert. 2000. Linguistic variation as social
practice. Blackwell.

Penelope Eckert. 2008. Variation and the indexical field.
Journal of Sociolinguistics, 12(4):453–476.

Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of EMNLP.

Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of ACL.

Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. Technical Report 1210.5268,
arXiv.

Jacob Eisenstein. 2013. Phonological factors in social
media writing. In Proceedings of the NAACL Work-
shop on Language Analysis in Social Media.

Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In Proceedings of the NAACL HLT Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk.

Jenny R. Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proceedings of ACL, pages 363–370.

Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From news to comment:
Resources and benchmarks for parsing the language
of web 2.0. In Proceedings of IJCNLP.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and

Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In Pro-
ceedings of ACL.

Scott A. Golder and Michael W. Macy. 2011. Di-
urnal and seasonal mood vary with work, sleep,
and daylength across diverse cultures. Science,
333(6051):1878–1881, September.

Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011a.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82–90, July.

Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011b. Contextual bearing on linguistic
variation in social media. In Proceedings of the ACL
Workshop on Language in Social Media.

Lisa Green. 2002. African American English. Cam-
bridge University Press.

Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a# twitter. In
Proceedings of ACL, volume 1.

Bo Han, Paul Cook, and Timothy Baldwin. 2012. Auto-
matically constructing a normalisation dictionary for
microblogs. In Proceedings of EMNLP.

Yuheng Hu, Kartik Talamadupula, and Subbarao Kamb-
hampati. 2013. Dude, srsly?: The surprisingly for-
mal nature of twitter’s language. In Proceedings of
ICWSM.

Barbara Johnstone, Jennifer Andrus, and Andrew E
Danielson. 2006. Mobility, indexicality, and the en-
registerment of pittsburghese. Journal of English Lin-
guistics, 34(2):77–104.

Lucy Jones. 2010. The changing face of spelling on the
internet. Technical report, The English Spelling Soci-
ety.

William Labov. 1972. Sociolinguistic patterns.
Philadelphia: University of Pennsylvania Press.

William Labov. 2001. Principles of linguistic change.
Vol.2 : Social factors. Blackwell Publishers, Oxford.

Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of ACL, pages 71–76.

Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of ACL.

Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang
Fu, and Furu Wei. 2012. Joint inference of named en-
tity recognition and normalization for tweets. In Pro-
ceedings of ACL.

David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adaptation for parsing. In
Proceedings of NAACL, pages 28–36, June.

368



Robert Munro and Christopher D. Manning. 2012.
Short message communications: users, topics, and in-
language processing. In Proceedings of the 2nd ACM
Symposium on Computing for Development.

Mor Naaman, Hila Becker, and Luis Gravano. 2011. Hip
and trendy: Characterizing emerging trends on twit-
ter. Journal of the American Society for Information
Science and Technology, 62(5):902–918.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.

John C. Paolillo. 1996. Language choice on
soc.culture.punjab. Electronic Journal of Communica-
tion/La Revue Electronique de Communication, 6(3).

John C. Paolillo. 1999. The virtual speech community:
Social network and language variation on irc. Journal
of Computer-Mediated Communication, 4(4):0.

John C. Paolillo. 2001. Language variation on internet
relay chat: A social network approach. Journal of So-
ciolinguistics, 5(2):180–213.

Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).

Daniel Ramage, Sue Dumais, and D. Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of ICWSM.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of Workshop on
Search and mining user-generated contents.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations. In Pro-
ceedings of NAACL.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an experi-
mental study. In Proceedings of EMNLP.

Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and Post-Social media generations. In
Proceedings of ACL.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of WWW,
pages 851–860.

Christina Sauper, Aria Haghighi, and Regina Barzilay.
2011. Content models with attitude. In Proceedings
of ACL.

Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proceedings of NAACL.

Aaron Smith and Joanna Brewer. 2012. Twitter use
2012. Technical report, Pew Research Center, May.

Richard Sproat, Alan W Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, 15(3):287–333.

Sali A. Tagliamonte and Derek Denis. 2008. Linguistic
ruin? lol! instant messaging and teen language. Amer-
ican Speech, 83(1):3–34, March.

Mike Thelwall. 2009. Homophily in MySpace. J. Am.
Soc. Inf. Sci., 60(2):219–231.

Crispin Thurlow. 2006. From statistical panic to moral
panic: The metadiscursive construction and popular
exaggeration of new media language in the print me-
dia. J. Computer-Mediated Communication, pages
667–701.

Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.

Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling infinite jest. In Proceedings of
NAACL.

Joseph B. Walther and Kyle P. D’Addario. 2001. The
impacts of emoticons on message interpretation in
computer-mediated communication. Social Science
Computer Review, 19(3):324–347.

Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of ACL.

Tae Yano, William W. Cohen, and Noah A. Smith. 2009.
Predicting response to political blog posts with topic
models. In Proceedings of NAACL.

369


