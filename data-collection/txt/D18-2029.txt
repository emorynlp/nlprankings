



















































Universal Sentence Encoder for English


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations), pages 169–174
Brussels, Belgium, October 31–November 4, 2018. c©2018 Association for Computational Linguistics

169

Universal Sentence Encoder for English

Daniel Cera†, Yinfei Yanga†, Sheng-yi Konga, Nan Huaa, Nicole Limtiacob,
Rhomni St. Johna, Noah Constanta, Mario Guajardo-Céspedesa, Steve Yuanc,

Chris Tara, Yun-Hsuan Sunga, Brian Stropea, Ray Kurzweila

aGoogle AI
Mountain View, CA

bGoogle AI
New York, NY

cGoogle
Cambridge, MA

Abstract

We present easy-to-use TensorFlow Hub
sentence embedding models having good
task transfer performance. Model vari-
ants allow for trade-offs between accuracy
and compute resources. We report the re-
lationship between model complexity, re-
sources, and transfer performance. Com-
parisons are made with baselines with-
out transfer learning and to baselines that
incorporate word-level transfer. Transfer
learning using sentence-level embeddings
is shown to outperform models without
transfer learning and often those that use
only word-level transfer. We show good
transfer task performance with minimal
training data and obtain encouraging re-
sults on word embedding association tests
(WEAT) of model bias.

1 Introduction

We present easy-to-use sentence-level embed-
ding models with good transfer task performance
even when using remarkably little training data.1

Model engineering characteristics allow for trade-
offs between accuracy versus memory and com-
pute resource consumption.

2 Model Toolkit

Models are implemented in TensorFlow (Abadi
et al., 2016) and are made publicly available on
TensorFlow Hub.2 Listing 1 provides an example

† Corresponding authors:
{cer, yinfeiy}@google.com

1We describe our publicly released models. See Yang
et al. (2018) and Henderson et al. (2017) for additional ar-
chitectural details of models similar to those presented here.

2 https://www.tensorflow.org/hub/, Apache
2.0 license, with models available as saved TF graphs.

import tensorflow_hub as hub

embed = hub.Module("https://tfhub.dev/google/"
"universal-sentence-encoder/2")

embedding = embed(["Hello World!"])

Listing 1: Python sentence embedding code.

code snippet to compute a sentence-level embed-
ding from a raw untokenized input string.3 The re-
sulting embedding can be used directly or incorpo-
rated into a downstream model for a specific task.4

3 Encoders

Two sentence encoding models are provided: (i)
transformer (Vaswani et al., 2017), which achieves
high accuracy at the cost of greater resource con-
sumption; (ii) deep averaging network (DAN)
(Iyyer et al., 2015), which performs efficient in-
ference but with reduced accuracy.

3.1 Transformer

The transformer sentence encoding model con-
structs sentence embeddings using the encod-
ing sub-graph of the transformer architecture
(Vaswani et al., 2017). The encoder uses atten-
tion to compute context aware representations of
words in a sentence that take into account both the
ordering and identity of other words. The context
aware word representations are averaged together
to obtain a sentence-level embedding.

We train for broad coverage using multi-task
learning, with the same encoding model support-
ing multiple downstream tasks. The task types
include: a Skip-Thought like task (Kiros et al.,

3Basic text preprocessing and white-space tokenization is
performed internally. Preprocessing lowercases the text and
removes punctuation. OOV items are handled using string
hashing to index into 400,000 OOV embeddings.

4Visit https://colab.research.google.com/ to try the code
snippet in Listing 1. Example code and documentation is
available on the TF Hub website.

https://www.tensorflow.org/hub/
https://colab.research.google.com/


170

2015);5 conversational response prediction (Hen-
derson et al., 2017); and a select supervised classi-
fication task that improves sentence embeddings.6

The transformer encoder achieves the best transfer
performance. However, this comes at the cost of
compute time and memory usage scaling dramati-
cally with sentence length.

3.2 Deep Averaging Network (DAN)

The DAN sentence encoding model begins by
averaging together word and bi-gram level em-
beddings. Sentence embeddings are then obtain
by passing the averaged representation through
a feedforward deep neural network (DNN). The
DAN encoder is trained similar to the transformer
encoder. Multitask learning trains a single DAN
encoder to support multiple downstream tasks. An
advantage of the DAN encoder is that compute
time is linear in the length of the input sequence.
Similar to Iyyer et al. (2015), our results demon-
strate that DANs achieve strong baseline perfor-
mance on text classification tasks.

3.3 Encoder Training Data

Unsupervised training data are drawn from a va-
riety of web sources. The sources are Wikipedia,
web news, web question-answer pages and discus-
sion forums. We augment unsupervised learning
with training on supervised data from the Stanford
Natural Language Inference (SNLI) corpus (Bow-
man et al., 2015) in order to further improve our
representations (Conneau et al., 2017). Since the
only supervised training data is SNLI, the models
can be used for a wide range of downstream super-
vised tasks that do not overlap with this dataset.7

4 Transfer Tasks

This section presents the data used for the transfer
learning experiments and word embedding asso-
ciation tests (WEAT): (MR) Movie review senti-
ment on a five star scale (Pang and Lee, 2005);
(CR) Sentiment of customer reviews (Hu and
Liu, 2004); (SUBJ) Subjectivity of movie re-
views and plot summaries (Pang and Lee, 2004);

5The Skip-Thought like task replaces the LSTM (Hochre-
iter and Schmidhuber, 1997) in the original formulation with
a transformer model.

6SNLI (Bowman et al., 2015; Conneau et al., 2017)
7For questions on downstream evaluations possibly over-

lapping with the encoder training data, visit the TF Hub
discussion board, https://groups.google.com/a/
tensorflow.org/d/forum/hub, or e-mail the corre-
sponding authors.

DATASET TRAIN DEV TEST
SST 67,349 872 1,821

STS Bench 5,749 1,500 1,379
TREC 5,452 - 500
MR - - 10,662
CR - - 3,775

SUBJ - - 10,000
MPQA - - 10,606

Table 1: Transfer task evaluation sets.

(MPQA) Phrase opinion polarity from news data
(Wiebe et al., 2005); (TREC) Fine grained ques-
tion classification sourced from TREC (Li and
Roth, 2002); (SST) Binary phrase sentiment clas-
sification (Socher et al., 2013); (STS Benchmark)
Semantic textual similarity (STS) between sen-
tence pairs scored by Pearson r with human judg-
ments (Cer et al., 2017); (WEAT) Word pairs from
the psychology literature on implicit association
tests (IAT) that are used to characterize model bias
(Caliskan et al., 2017).8 Table 1 gives the number
of samples for each transfer task.

5 Transfer Learning Models

For sentence classification transfer tasks, the out-
put of the sentence encoders are provided to a task
specific DNN. For the pairwise semantic similar-
ity task, the similarity of sentence embeddings u
and v is assessed using − arccos

(
uv

||u|| ||v||

)
.9

5.1 Baselines
For each transfer task, we include baselines that
only make use of word-level transfer and baselines
that make use of no transfer learning at all. For
word-level transfer, we incorporate word embed-
dings from a word2vec skip-gram model trained
on a corpus of news data (Mikolov et al., 2013).
The pretrained word embeddings are included as
input to two model types: a convolutional neural
network model (CNN) (Kim, 2014); a DAN. The
baselines that use pretrained word embeddings al-
low us to contrast word- vs. sentence-level trans-
fer. Additional baseline CNN and DAN models
are trained without using any pretrained word or
sentence embeddings. For reference, we com-
pare with InferSent (Conneau et al., 2017) and

8For MR, CR, SUBJ, SST, and TREC we use the prepa-
ration of the data provided by Conneau et al. (2017).

9arccos converts cosine similarity into an angular distance
that obeys the triangle inequality. We find that angular dis-
tance performs better on STS than cosine similarity.

https://groups.google.com/a/tensorflow.org/d/forum/hub
https://groups.google.com/a/tensorflow.org/d/forum/hub


171

Skip-Thought with layer normalization (Ba et al.,
2016) on sentence-classification tasks. On the STS
Benchmark, we compare with InferSent and the
state-of-the-art neural STS systems CNN (HCTI)
(Shao, 2017) and gConv (Yang et al., 2018).

5.2 Combined Transfer Models
We explore combining the sentence and word-
level transfer models by concatenating their rep-
resentations prior to the classification layers. For
completeness, we report results providing the clas-
sification layers with the concatenating of the
sentence-level embeddings and the representations
produced by baseline models that do not make use
of word-level transfer learning.

6 Experiments

Experiments use our most recent transformer and
DAN encoding models.10 Transfer task model hy-
perparamaters are tuned using a combination of
Vizier (Golovin et al., 2017) and light manual tun-
ing. When available, model hyperparameters are
tuned using task dev sets. Otherwise, hyperparam-
eters are tuned by cross-validation on task train-
ing data or the evaluation test data when neither
training nor dev data are provided. Training re-
peats ten times for each task with randomly ini-
tialized weights and we report results by averaging
across runs. Transfer learning is important when
training data is limited. We explore using vary-
ing amounts of training data for SST. Contrasting
the transformer and DAN encoders demonstrates
trade-offs in model complexity and the training
data required to reach a desired level of task ac-
curacy. Finally, to assess bias in our encoders, we
evaluate the strength of biased model associations
on WEAT. We compare to Caliskan et al. (2017)
who discovered that word embeddings reproduce
human-like biases on implicit association tasks.

7 Results

Table 2 presents results on classification tasks. Us-
ing transformer sentence-level embeddings alone
outperforms InferSent on MR, SUBJ, and TREC.
The transformer sentence encoder also strictly out-
performs the DAN encoder. Models that make use
of just the transformer sentence-level embeddings
tend to outperform all models that only use word-
level transfer, with the exception of TREC and

10universal-sentence-encoder/2 (DAN); universal-
sentence-encoder-large/3 (Transformer).

MODEL MR CR SUBJ MPQA TREC SST
Sentence Embedding Transfer Learning

UT 82.2 84.2 95.5 88.1 93.2 83.7
UD 72.2 78.5 92.1 86.9 88.1 77.5

Word Embedding Transfer Learning
CNNw2v 75.1 80.5 91.1 80.3 96.6 84.1
DANw2v 74.7 75.3 90.2 82.1 83.5 80.6

Sentence Embedding Transfer Learning
+ DNN/CNN with word-level transfer

UT +CNNw2v 80.1 85.2 95.8 88.4 98.7 85.3
UT +DANw2v 81.4 86.4 93.7 87.5 97.0 86.0
UD+CNNw2v 76.7 82.0 91.2 85.2 97.1 85.1
UD+DANw2v 76.4 81.0 94.0 88.0 92.6 82.2

Sentence Embedding Transfer Learning
+ DNN/CNN without word-level transfer

UT +CNNrnd 82.7 88.6 93.6 87.8 98.5 88.9
UT +DANrnd 80.6 84.8 94.3 86.0 98.6 86.2
UD+CNNrnd 78.0 82.9 90.2 87.8 96.2 83.2
UD+DANrnd 76.4 84.9 94.0 85.3 98.1 86.2

Baselines with No Transfer Learning
CNNrnd 76.5 81.0 89.6 82.2 97.9 85.0
DANrnd 74.6 81.2 91.8 79.9 93.9 82.0

Prior Work
InferSent 81.1 86.3 92.4 90.2 88.2 84.6

Skip Thght 79.4 83.1 93.7 89.3 - -

Table 2: Classification tasks. UT uses the trans-
former encoder for transfer learning, while UD
uses the DAN encoder. DAN/CNNw2v use pre-
trained w2v emb. DAN/CNNrnd train rand. init.
word emb. on the final classification task.

SST, on which CNNw2v performs better. Trans-
fer learning with DAN sentence embeddings tends
to outperform a DAN with word-level transfer, ex-
cept on MR and SST. Models with sentence- and
word-level transfer often outperform similar mod-
els with sentence-level transfer alone.

MODEL DEV TEST
Transformer Encoder 0.802 0.766

DAN Encoder 0.760 0.717
Prior Work

gConv (Yang et al., 2018) 0.835 0.808
CNN (HCTI) (Shao, 2017) 0.834 0.784

InferSent (Conneau et al., 2017) 0.801 0.758

Table 3: STS Benchmark Pearson’s r. Our prior
gConv model (Yang et al., 2018) is a variant of
our TF Hub transformer model tuned to STS.

Table 3 compares our models to strong base-
lines on the STS Benchmark. Our transformer em-
beddings outperform the sentence representations
produced by InferSent. Moreover, computing sim-
ilarity scores by directly comparing the repre-
sentations produced by our encoders approaches



172

the performance of state-of-the-art neural models
whose representations are fit to the STS task.

Table 4 illustrates transfer task performance for
varying amounts of training data. With small
quantities of training data, sentence-level trans-
fer achieves surprisingly good performance. Us-
ing only 1k labeled examples and the transformer
embeddings for sentence-level transfer surpasses
the performance of transfer learning using In-
ferSent on the full training set of 67.3k exam-
ples. Training with 1k labeled examples and the
transformer sentence embeddings surpasses word-
level transfer using the full training set, CNNw2v,
and approaches the performance of the best model
without transfer learning trained on the complete
dataset, CNNrnd@67.3k. Transfer learning is not
always helpful when there is enough task training
data. However, we observe that our best perform-
ing model still makes use of transformer sentence-
level transfer but combined with a CNN with no
word-level transfer, UT+CNNrnd.

Table 5 contrasts Caliskan et al. (2017)’s find-
ings on bias within GloVe embeddings with results
from the transformer and DAN encoders. Similar
to GloVe, our models reproduce human associa-
tions between flowers vs. insects and pleasantness
vs. unpleasantness. However, our models demon-
strate weaker associations than GloVe for probes
targeted at revealing ageism, racism and sexism.11

Differences in word association patterns can be at-
tributed to training data composition and the mix-
ture of tasks used to train the representations.

8 Resource Usage

This section describes memory and compute re-
source usage for the transformer and DAN sen-
tence encoding models over different batch sizes
and sentence lengths. Figure 1 plots model re-
source consumption against sentence length.12

Compute Usage The transformer model time
complexity is O(n2) in sentence length, while the

11The development of our models did not target reducing
bias. Researchers and developers are strongly encouraged to
independently verify whether biases in their overall model
or model components impacts their use case. For resources
on ML fairness visit https://developers.google.com/machine-
learning/fairness-overview/.

12 All benchmark values are averaged over 25 runs that
follow 5 priming runs. CPU and mem. benchmarks are per-
formed on a machine with an Intel(R) Xeon(R) Platinum
P-8136 CPU @ 2.00GHz CPU. GPU benchmarks use an
Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz CPU and
NVIDIA Tesla P100 GPU.

MODEL SST 1K SST 4K SST 16K SST 67.3K
Sentence Embedding Transfer Learning

UT 84.8 84.8 84.8 83.7
UD 78.7 78.6 76.9 77.5

Word Embedding Transfer Learning

CNNw2v 70.7 73.8 81.5 84.1
DANw2v 67.5 75.1 78.4 80.6

Sentence Embedding Transfer Learning

+ DNN/CNN with word-level transfer
UT +CNNw2v 84.9 84.9 85.4 85.3
UT +DANw2v 85.1 85.4 85.0 86.0
UD+CNNw2v 78.6 79.7 80.9 85.1
UD+DANw2v 78.7 79.1 81.6 82.2

Sentence Embedding Transfer Learning

+ DNN/CNN without word-level transfer
UT +CNNrnd 83.1 83.3 84.9 88.9
UT +DANrnd 84.9 84.2 86.0 86.2
UD+CNNrnd 77.5 77.9 81.3 83.2
UD+DANrnd 78.5 78.8 82.5 86.2

Baselines with No Transfer Learning

CNNrnd 68.9 74.6 81.5 85.0
DANrnd 68.4 73.1 78.0 82.0

Prior Work

InferSent - - - 84.6

Table 4: SST performance varying the amount of
training data. Model types are the same as Table
2. Using 1k examples, UT transfer learning rivals
models trained on the full training set, 67.3k.

DAN model is O(n). As seen in Figure 1 (a-
b), for short sentences, the transformer encoding
model is only moderately slower than the much
simpler DAN model. However, compute time
for transformer increases noticeably with sentence
length. In contrast, the compute time for the
DAN model stays nearly constant across different
lengths. When running on GPU, even for large
batches and longer sentence lengths, the trans-
former model still achieves performance that can
be used within an interactive systems.

Memory Usage The transformer model space
complexity also scales quadratically, O(n2), in
sentence length, while the DAN is linear, O(n).
Similar to compute usage, memory for the trans-
former model increases quickly with sentence
length, while the memory for the DAN model re-
mains nearly constant. For the DAN model, mem-
ory is dominated by the parameters used to store
the model unigram and bigram embeddings. Since
the transformer model only stores unigrams, for

https://developers.google.com/machine-learning/fairness-overview/
https://developers.google.com/machine-learning/fairness-overview/


173

(a) CPU Time vs. Sentence Length (b) GPU Time vs. Sentence Length (c) Memory vs. Sentence Length

Figure 1: Resource usage for the Universal Sentence Encoder DAN (USE-D) and Transformer (USE-T)
models for different batch sizes and sentence lengths.

Target words Attrib. words Ref GloVe U. Enc. DAN U. Enc. Trans.d p d p d p
Eur.- vs. Afr.-American names Pleasant vs. Unpleasant a 1.41 10−8 0.36 0.04 0.22 0.14
Eur.- vs. Afr.-American names Pleasant vs.Unpleasant from (a) b 1.50 10−4 -0.37 0.87 0.21 0.27
Eur.- vs. Afr.-American names Pleasant vs. Unpleasant from (c) b 1.28 10−3 0.72 0.02 0.93 10−2

Male vs. female names Career vs. family c 1.81 10−3 0.02 0.48 0.95 0.03
Math vs. arts Male vs. female terms c 1.06 0.02 0.59 0.12 0.12 0.41

Science vs. arts Male vs. female terms d 1.24 10−2 0.24 0.32 -0.21 0.67
Mental vs. physical disease Temporary vs. permanent e 1.38 10−2 1.60 10−2 0.42 0.23
Young vs old peoples names Pleasant vs unpleasant c 1.21 10−2 1.01 0.02 0.06 0.46

Flowers vs. Insects Pleasant vs. Unpleasant a 1.50 10−7 1.38 10−6 1.47 10−7

Instruments vs. Weapons Pleasant vs Unpleasant a 1.53 10−7 1.44 10−7 1.65 10−7

Table 5: WEAT for GloVe vs. our DAN and transformer encoding models. Effect size is reported as
Cohen’s d over the mean cosine similarity scores across grouped attribute words. Statistical significance
uses one-tailed p-scores. The Ref column indicates the source of the IAT word lists: (a) Greenwald
et al. (1998) (b) Bertrand and Mullainathan (2004) (c) Nosek et al. (2002a) (d) Nosek et al. (2002b) (e)
Monteith and Pettit (2011).

very short sequences transformer requires almost
half as much memory as the DAN model.

9 Conclusion

Our encoding models provide sentence-level em-
beddings that demonstrate strong transfer perfor-
mance on a number of NLP tasks. The encoding
models make different trade-offs regarding accu-
racy and model complexity that should be consid-
ered when choosing the best one for a particular
application. Overall, our sentence-level embed-
dings tend to surpass the performance of trans-
fer using word-level embeddings alone. Models
that make use of sentence- and word-level trans-
fer often achieve the best performance. Sentence-
level transfer using our models can be exception-
ally helpful when limited training data is avail-
able. The pre-trained encoding models are pub-
licly available for research and use in industry

applications that can benefit from a better under-
standing of natural language.

Acknowledgments

We thank our teammates from Descartes, Ai.h and
other Google groups for their feedback and sug-
gestions. Special thanks goes to Ben Packer and
Yoni Halpern for implementing the WEAT assess-
ments and discussions on model bias.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorflow: A system for large-scale machine learning.
In Proceedings of USENIX OSDI’16.



174

Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. CoRR, abs/1607.06450.

Marianne Bertrand and Sendhil Mullainathan. 2004.
Are emily and greg more employable than lakisha
and jamal? a field experiment on labor market
discrimination. The American Economic Review,
94(4).

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of EMNLP.

Aylin Caliskan, Joanna J. Bryson, and Arvind
Narayanan. 2017. Semantics derived automatically
from language corpora contain human-like biases.
Science, 356(6334):183–186.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation. In Proceedings of
SemEval-2017.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. arXiv preprint
arXiv:1705.02364.

Daniel Golovin, Benjamin Solnik, Subhodeep Moitra,
Greg Kochanski, John Karro, and D. Sculley. 2017.
Google vizier: A service for black-box optimization.
In Proceedings of KDD ’17.

Anthony G. Greenwald, Debbie E. McGhee, and Jor-
dan L. K. Schwartz. 1998. Measuring individual
differences in implicit cognition: the implicit asso-
ciation test. Journal of personality and social psy-
chology, 74(6).

Matthew Henderson, Rami Al-Rfou, Brian Strope,
Yun-Hsuan Sung, László Lukács, Ruiqi Guo, San-
jiv Kumar, Balint Miklos, and Ray Kurzweil. 2017.
Efficient natural language response suggestion for
smart reply. CoRR, abs/1705.00652.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of KDD
’04.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of ACL/IJCNLP.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of EMNLP.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In In
Proceedings of NIPS.

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of COLING ’02.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS’13.

Lindsey L. Monteith and Jeremy W. Pettit. 2011. Im-
plicit and explicit stigmatizing attitudes and stereo-
types about depression. Journal of Social and Clin-
ical Psychology, 30(5).

Brian A. Nosek, Mahzarin R. Banaji, and Anthony G.
Greenwald. 2002a. Harvesting implicit group at-
titudes and beliefs from a demonstration web site.
Group Dynamics, 6(1).

Brian A. Nosek, Mahzarin R. Banaji, and Anthony G
Greenwald. 2002b. Math = male, me = female,
therefore math me. Journal of Personality and So-
cial Psychology,, 83(1).

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), Main Volume.

Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
ACL’05.

Yang Shao. 2017. Hcti at semeval-2017 task 1:
Use convolutional neural network to evaluate se-
mantic textual similarity. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017), pages 130–133.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of EMNLP.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NIPS.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165–210.

Yinfei Yang, Steve Yuan, Daniel Cer, Sheng yi Kong,
Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan
Sung, Brian Strope, and Ray Kurzweil. 2018.
Learning semantic textual similarity from conversa-
tions. Proceedings of RepL4NLP workshop at ACL.

http://arxiv.org/abs/1607.06450
https://doi.org/10.1126/science.aal4230
https://doi.org/10.1126/science.aal4230
http://www.aclweb.org/anthology/S17-2001
http://www.aclweb.org/anthology/S17-2001
http://www.aclweb.org/anthology/S17-2001
https://doi.org/10.1145/3097983.3098043
http://arxiv.org/abs/1705.00652
http://arxiv.org/abs/1705.00652
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.3115/1072228.1072378
https://doi.org/10.3115/1072228.1072378
https://doi.org/10.1007/s10579-005-7880-9
https://doi.org/10.1007/s10579-005-7880-9

