



















































A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics


Proceedings of the First Workshop on Argumentation Mining, pages 64–68,
Baltimore, Maryland USA, June 26, 2014. c©2014 Association for Computational Linguistics

A Benchmark Dataset for Automatic Detection of Claims and    

Evidence in the Context of Controversial Topics 
 

Ehud Aharoni* Anatoly Polnarov* Tamar Lavee† Daniel Hershcovich 

IBM Haifa Research 

Lab, Haifa, Israel 

Hebrew University, 

Israel 

IBM Haifa Research 

Lab, Haifa, Israel 

IBM Haifa Research 

Lab, Haifa, Israel 

Ran Levy Ruty Rinott Dan Gutfreund Noam Slonim‡ 

IBM Haifa Research 
Lab, Haifa, Israel 

IBM Haifa Research 

Lab, Haifa, Israel 

IBM Haifa Research 

Lab, Haifa, Israel 

IBM Haifa Research 

Lab, Haifa, Israel 

 
 

Abstract 

We describe a novel and unique argumenta-

tive structure dataset. This corpus consists of 

data extracted from hundreds of Wikipedia ar-

ticles using a meticulously monitored manual 

annotation process. The result is 2,683 argu-

ment elements, collected in the context of 33 

controversial topics, organized  under a simple 

claim-evidence structure. The obtained data 

are publicly available for academic research.  

1 Introduction 

One major obstacle in developing automatic ar-
gumentation mining techniques is the scarcity of 
relevant high quality annotated data. Here, we 
describe a novel and unique benchmark data that 
relies on a simple argument model and elabo-
rates on the associated annotation process. Most 
importantly, the argumentative elements were 
gathered in the context of pre-defined controver-
sial topics, which distinguishes our work from 
other previous related corpora.

1
 Two recent 

                                                 
*  These authors contributed equally to this manuscript. 

†  Present affiliation: Yahoo! 

‡ Corresponding author, at noams@il.ibm.com 

works that are currently under review [Rinott et 
al, Levy et al] have reported first results over 
different subsets of this data, which is now pub-
lically available for academic research upon re-
quest. We believe that this novel corpus should 
be of practical importance to many researches, 
and in particular to the emerging community of 
argumentation mining. 

Unlike the classical Toulmin model (Freeley 
and Steinberg 2008), we considered a simple and 
robust argument structure comprising only two 
components – claim and associated supporting 
evidence. The argumentative structures were 
carefully annotated under a pre-defined topic, 
introduced as a debate motion. As the collected 
data covers a diverse set of 33 motions, we ex-
pect it could be used to develop generic tools for 
automatic detection and construction of argu-

mentative structures in the context of new topics.  

2 Data Model 

We defined and implemented the following con-
cepts:  
Topic – a short, usually controversial statement 
that defines the subject of interest. Context De-

                                                                         
1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and 

Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 

2013).  

64



pendent Claim (CDC) – a general concise 
statement that directly supports or contests the 
Topic. Context Dependent Evidence (CDE) – a 
text segment that directly supports a CDC in the 
context of a given Topic. Examples are given in 
Section 6. 

Furthermore, since one can support a claim us-
ing different types of evidence (Rieke et al 2012, 
Seech 2008), we defined and considered three 
CDE types: Study: Results of a quantitative 
analysis of data given as numbers or as conclu-
sions. Expert: Testimony by a person / group / 
committee / organization with some known ex-
pertise in or authority on the topic. Anecdotal: a 
description of specific event(s)/instance(s) or 

concrete example(s). 

3 Labeling Challenges and Approach 

The main challenge we faced in collecting the 
annotated data was the inherently elusive nature 
of concepts such as "claim" and "evidence." To 
address that we formulated two sets of criteria 
for CDC and CDE, respectively, and relied on a 
team of about 20 carefully trained in-house la-
belers whose work was closely monitored. To 
further enhance the quality of the collected data 
we adopted a two-stage labeling approach. First, 
a team of five labelers worked independently on 
the same text and prepared the initial set of can-
didate CDCs or candidate CDEs. Next, a team of 
five labelers—not necessarily the same five—
independently crosschecked the joint list of the 
detected candidates, each of which was either 
confirmed or rejected. Candidates confirmed by 
at least three labelers were included in the corpus.  

4 Labeling Guidelines 

The labeling guidelines defined the concepts of 
Topic, CDC, CDE, and CDE types, along with 
relevant examples. According to these guidelines, 
given a Topic, a text fragment should be labeled 
as a CDC if and only if it complies with all of 

the following five CDC criteria: Strength: 
Strong content that directly supports or contests 
the provided Topic. Generality: General content 
that deals with a relatively broad idea. Phrasing: 
Is well phrased, or requires at most a single and 
minor "allowed" change.

2
 Keeping text spirit: 

Keeps the spirit of the original text from which it 
was extracted. Topic unity: Deals with one, or at 
most two related topics. Four CDE criteria were 
defined in a similar way, given a Topic and a 

CDC, except for the generality criterion. 

5 Labeling Details 

The labeling process was carried out in the 
GATE environment (https://gate.ac.uk/). The 33 
Topics were selected at random from the debate 
motions at http://idebate.org/ database. The la-
beling process was divided into five stages:  

Search: Given a Topic, five labelers were 
asked to independently search English Wikipe-
dia

3
 for articles with promising content.  

Claim Detection: At this stage, five labelers 
independently detected candidate CDCs support-
ing or contesting the Topic within each article 
suggested by the Search team.  

Claim Confirmation: At this stage, five la-
belers independently cross-examined the candi-
date CDCs suggested at the Claim Detection 
stage, aiming to confirm a candidate and its sen-
timent as to the given Topic, or reject it by refer-
ring to one of the five CDC Criteria it fails to 
meet. The candidate CDCs confirmed by at least 
three labelers were forwarded to the next stage.  

Evidence Detection: At this stage, five la-
belers independently detected candidate CDEs 
supporting a confirmed CDC in the context of 
the given Topic. The search for CDEs was done 

                                                 
2 For example, anaphora resolution. The enclosed data set 

contains the corrected version as well, as proposed by the 

labelers.  

3 We considered the Wikipedia dump as of April 3, 2012. 

65



only in the same article where the corresponding 
CDC was found.  

Evidence Confirmation: This stage was car-
ried out in a way similar to Claim Confirmation. 
The only difference was that the labelers were 
required to classify accepted CDE under one or 
more CDE types.  

Labelers training and feedback: Before join-
ing actual labeling tasks, novice labelers were 
assigned with several completed tasks and were 
expected to show a reasonable degree of agree-
ment with a consensus solution prepared in ad-
vance by the project administrators. In addition, 
the results of each Claim Confirmation task were 
examined by one or two of the authors (AP and 
NS) to ensure the conformity to the guidelines. 
In case crude mistakes were spotted, the corre-
sponding labeler was requested to revise and 
resubmit. Due to the large numbers of CDE can-
didates, it was impractical to rely on such a rig-
orous monitoring process in Evidence Confirma-
tion. Instead, Evidence Consensus Solutions  
were created for selected articles by several ex-
perienced labelers, who first solved the tasks 
independently and then reached consensus in a 
joint meeting. Afterwards, the tasks were as-
signed to the rest of the labelers. Their results on 
these tasks were juxtaposed with the Consensus 
Solutions, and on the basis of this comparison 
individual feedback reports were drafted and 
sent to the team members. Each labeler received 

such a report on an approximately weekly basis.   

6 Data Summary 

For 33 debate motions, a total of 586 Wikipedia 
articles were labeled. The labeling process re-
sulted in 1,392 CDCs distributed across 321 ar-
ticles. In 12 debate motions, for which 350 dis-
tinct CDCs were confirmed across 104 articles, 
we further completed the CDE labeling, ending 
up with a total of 1,291 confirmed CDEs – 431 
of type Study, 516 of type Expert, and 529 of 
type Anecdotal. Note that some CDEs were as-

sociated with more than one type (for example, 
118 CDEs were classified both under the type 
Study and Expert). 

Presented in Tables 1 and 2 are several exam-
ples of CDCs and CDEs gathered under the 
Topics we worked with, as well as some inac-
ceptable candidates illustrating some of the sub-

tleties of the performed work. 

Topic 
The sale of violent video games to mi-

nors should be banned 

(Pro) 

CDC 

Violent video games can increase chil-

dren’s aggression 

(Pro) 

CDC 

Video game publishers unethically train 

children in the use of weapons 
Note that a valid CDC is not necessarily fa c-

tual .  
(Con) 

CDC 
Violent games affect children positively 

Invalid  

CDC 1 

Video game addiction is excessive or 

compulsive use of computer and video 

games that interferes with daily life. 
This  statement defines  a concept relevant to 
the Topic, not a relevant claim.  

Invalid  

CDC 2 

Violent TV shows just mirror the vio-

lence that goes on in the real world.  
This  claim is not relevant enough to Topic. 

Invalid  

CDC 3 

Violent video games should not be sold 

to children. 
This candidate simply repeats the Topic and 

thus  is not considered a va lid CDC.  

Invalid  

CDC 4 

“Doom” has been blamed for nationally 

covered school shooting. 
This candidate fails the generali ty cri terion, 

as it focuses on a speci fic single video game. 
Note that i t could serve as CDE to a more 

general CDC.   

Table 1: Examples of CDCs and invalid CDCs.  

 

Topic 1 
The sale of vio lent video games to 

minors should be banned 

(Pro) CDC 
Violent video games increase youth 

violence 

CDE 
(Study) 

The most  recent large scale meta-

analysis—examining 130 studies with 

over 130,000 subjects worldwide—

concluded that exposure to violent 

66



video games causes both short term 

and long term aggression in players 

CDE 
(Anecdotal) 

In April 2000, a 16-year-old teenager 

murdered his father, mother and sister 

proclaiming that he was on an "aveng-

ing mission" for the main character of 

the video game Final Fantasy VIII 

Invalid 
CDE 

While most experts reject any link 

between video games content and re-

al-life violence, some media scholars 

argue that the connection exists. 

Invalid, because i t includes information 
that contests the CDC. 

Topic 2 
The use of performance enhancing 

drugs in sports should be permitted 

(Con) CDC 
Drug abuse can be harmful to one’s 

health and even deadly. 

CDE 
(Expert) 

According to some nurse practition-

ers, stopping substance abuse can 

reduce the risk of dying early and also 

reduce some health risks like heart 

disease, lung disease, and strokes  

Invalid 

CDE 

Suicide is very common in adolescent 

alcohol abusers, with 1 in 4 suicides 

in adolescents being related to alcohol 

abuse. 
Although the candidate CDE does support 

the CDC, the notion of adolescent alcohol 
abusers  is irrelevant to the Topic. There-

fore, the candidate is invalid. 

Table 2: Examples of CDE and invalid CDE.  

7 Agreement and Recall Results 

To evaluate the labelers’ agreement we used Co-
hen’s kappa coefficient (Landis and Koch 1977). 
The average measure was calculated over all 
labelers' pairs, for each pair taking those articles 
on which the corresponding labelers worked to-
gether and omitting labeler pairs which labeled 
together less than 100 CDCs/CDEs. This strate-
gy was chosen since no two labelers worked on 
the exact same tasks, so standard multi-rater 
agreement measures could not be applied. The 
obtained average kappa was 0.39 and 0.4 in the 
Claim confirmation and Evidence confirmation 

stages, respectively, which we consider satisfac-
tory given the subtlety of the concepts involved 
and the fact that the tasks naturally required a 
certain extent of subjective decision making.  

We further employed a simple method to ob-
tain a rough estimate of the recall at the detection 
stages. For CDCs (and similarly for CDEs), let n 
be the number of CDCs detected and confirmed 
in a given article, and x be the unknown total 
number of CDCs in this article. Assuming the i-

th labeler detects a ratio  of x, and taking a 

strong assumption of independence between the 
labelers, we get:  

. 

We estimated  from the observed data, and 

computed x for each article. We were then able 
to compute the estimated recall per motion, end-
ing up with the estimated average recall of 
90.6% and 90.0% for CDCs and CDEs, respec-

tively.  

8 Future Work and Conclusion 

There are several natural ways to proceed further. 
First, a considerable increase in the quantity of 
gathered CDE data can be achieved by expand-
ing the search scope beyond the article in which 
the CDC is found. Second, the argument model 
can be enhanced – for example, to include coun-
ter-CDE (i.e., evidence that contest the CDC). 
Third, one may look into ways to add more la-
beling layers on the top of the existing model 
(for example, distinguishing between factual 
CDCs, value CDCs, and so forth). Fourth, new 
topics and new sources besides Wikipedia can be 
considered.  

The data is released and available upon request 
for academic research.

 
We hope that it will prove 

useful for different data mining communities, 
and particularly for various purposes in the field 

of Argumentation Mining.  

67



References 

Austin J. Freeley and David L. Steinberg. 2008. 

Argumentation and Debate. Wadsworth, 

Belmont, California. 

Chris Reed. 2005. ―Preliminary Results from an 

Argument Corpus‖ in Proceedings of the IX 

Symposium on Social Communication, San-

tiago de Cuba, pp. 576-580. 

J. Richard Landis and Gary G. Koch. 1977. ―The 

measurement of observer agreement for 

categorical data.‖ Biometrics 33:159-174. 

Kevid D. Ashley and Vern R. Walker. 2013. 

―Toward Constructing Evidence-Based Le-

gal Arguments Using Legal Decision Doc-

uments and Machine Learning‖ in Proceed-

ings of the Fourteenth International Con-

ference on Artificial Intelligence and Law 

(ICAIL ’13), Rome, Italy, pp. 176-180. 

Marie-Francine Moens, Erik Boiy, Raquel Mo-

chales Palau, and Chris Reed. 2007. ―Au-

tomatic Detection of Arguments in Legal 

Texts‖ in Proceedings of the International 

Conference on AI & Law (ICAIL-2007), 

Stanford, CA, pp. 225-230. 

Richard D. Rieke, Malcolm O. Sillars and Tarla 

Rai Peterson. 2012. Argumentation and 

Critical Decision Making (8e). Prentice 

Hall, USA. 

Ran Levy, Yonatan Bilu, Daniel Hershcovich, 

Ehud Aharoni and Noam Slonim. ―Context 

Dependent Claim Detection.‖ Submitted 

Ruty Rinott, Lena Dankin, Carlos Alzate, Ehud 

Aharoni and Noam Slonim. "Show Me 

Your Evidence – an Automatic Method for 

Context Dependent Evidence Detection.‖ 

Submitted. 

Zachary Seech. 2008. Writing Philosophy Pa-

pers (5
th

 edition). Wadsworth, Cengage 

Learning, Belmont, California.  

68


