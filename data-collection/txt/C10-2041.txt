356

Coling 2010: Poster Volume, pages 356–364,

Beijing, August 2010

A Semantic Network Approach to Measuring Relatedness

Brian Harrington

Oxford University Computing Laboratory

brian.harrington@comlab.ox.ac.uk

Abstract

Humans are very good at judging the
strength of relationships between two
terms, a task which, if it can be au-
tomated, would be useful in a range
of applications. Systems attempting to
solve this problem automatically have
traditionally either used relative po-
sitioning in lexical resources such as
WordNet, or distributional relation-
ships in large corpora. This paper pro-
poses a new approach, whereby rela-
tionships are derived from natural lan-
guage text by using existing nlp tools,
then integrated into a large scale se-
mantic network. Spreading activation
is then used on this network in order
to judge the strengths of all relation-
ships connecting the terms.
In com-
parisons with human measurements,
this approach was able to obtain re-
sults on par with the best purpose built
systems, using only a relatively small
corpus extracted from the web. This
is particularly impressive, as the net-
work creation system is a general tool
for information collection and integra-
tion, and is not speciﬁcally designed for
tasks of this type.

1

Introduction

The ability to determine semantic relatedness
between terms is useful for a variety of nlp ap-
plications, including word sense disambigua-
tion, information extraction and retrieval, and
text summarisation (Budanitsky and Hirst,
2006). However, there is an important dis-
tinction to be made between semantic relat-
edness and semantic similarity. As (Resnik,

1999) notes, “Semantic similarity represents a
special case of semantic relatedness:
for ex-
ample, cars and gasoline would seem to be
more closely related than, say, cars and bi-
cycles, but the latter pair are certainly more
similar”.
(Budanitsky and Hirst, 2006) fur-
ther note that “Computational applications
typically require relatedness rather than just
similarity; for example, money and river are
cues to the in-context meaning of bank that
are just as good as trust company”.

Systems for automatically determining the
degree of semantic relatedness between two
terms have traditionally either used a mea-
surement based on the distance between the
terms within WordNet (Banerjee and Ped-
ersen, 2003; Hughes and Ramage, 2007), or
used co-occurrence statistics from a large cor-
pus (Mohammad and Hirst, 2006; Pad´o and
Lapata, 2007). Recent systems have, how-
ever, shown improved results using extremely
large corpora (Agirre et al., 2009), and ex-
isting large-scale resources such as Wikipedia
(Strube and Ponzetto, 2006).

In this paper, we propose a new approach
to determining semantic relatedness, in which
a semantic network is automatically created
from a relatively small corpus using exist-
ing NLP tools and a network creation system
called ASKNet (Harrington and Clark, 2007),
and then spreading activation is used to de-
termine the strength of the connections within
that network. This process is more analogous
to the way the task is performed by humans.
Information is collected from fragments and
assimilated into a large semantic knowledge
structure which is not purposely built for a
single task, but is constructed as a general
resource containing a wide variety of infor-
mation. Relationships represented within this

357

structure can then be used to determine the
total strength of the relations between any two
terms.

and Brew, 2004; Pad´o and Lapata, 2007) or
from the web using search engine results (Tur-
ney, 2001).

2 Existing Approaches

2.1 Resource Based Methods

A popular method for automatically judging
semantic distance between terms is through
WordNet (Fellbaum, 1998), using the lengths
of paths between words in the taxonomy as a
measure of distance. While WordNet-based
approaches have obtained promising results
for measuring semantic similarity (Jiang and
Conrath, 1997; Banerjee and Pedersen, 2003),
the results for the more general notion of se-
mantic relatedness have been less promising
(Hughes and Ramage, 2007).

One disadvantage of using WordNet for
evaluating semantic relatedness is its hierar-
chical taxonomic structure. This results in
terms such as car and bicycle being close in
the network, but terms such as car and gaso-
line being far apart. Another diﬃculty arises
from the non-scalability of WordNet. While
the quality of the network is high, the man-
ual nature of its construction means that arbi-
trary word pairs may not occur in the network.
Hence in this paper we pursue an approach in
which the resource for measuring semantic re-
latedness is created automatically, based on
naturally occurring text.

A similar project, not using WordNet is
WikiRelate (Strube and Ponzetto, 2006),
which uses the existing link structure of
Wikipedia as its base network, and uses sim-
ilar path based measurements to those found
in WordNet approaches to compute semantic
relatedness. This project has seen improved
results over most WordNet base approaches,
largely due to the nature of Wikipedia, where
articles tend to link to other articles which are
related, rather than just ones which are simi-
lar.

2.2 Distributional Methods

An alternative method for judging semantic
distance is using word co-occurrence statistics
derived from a very large corpus (McDonald

In a recent paper, Agirre et al. (2009) parsed
4 billion documents (1.6 Terawords) crawled
from the web, and then used a search func-
tion to extract syntactic relations and con-
text windows surrounding key words. These
were then used as features for vector space,
in a similar manner to work done in (Pad´o
and Lapata, 2007), using the British National
Corpus (bnc). This system has produced ex-
cellent results, indicating that the quality of
the results for these types of approaches is re-
lated to the size and coverage of their corpus.
This does however present problems moving
forward, as 1.6 Terawords is obviously an ex-
tremely large corpus, and it is likely that there
would be a diminishing return on investment
for increasingly large corpora. In the same pa-
per, another method was shown which used
the pagerank algorihm, run over a network
formed from WordNet and the WordNet gloss
tags in order to produce equally impressive re-
sults.

3 A Semantic Network Approach

The resource we use is a semantic network,
automatically created by the large scale net-
work creation program, ASKNet. The rela-
tions between nodes in the network are based
on the relations returned by a parser and se-
mantic analyser, which are typically the argu-
ments of predicates found in the text. Hence
terms in the network are related by the chain
of syntactic/semantic relations which connect
the terms in documents, making the network
ideal for measuring the general notion of se-
mantic relatedness.

Distinct occurrences of terms and entities
are combined into a single node using a novel
form of spreading activation (Collins and Lof-
tus, 1975). This combining of distinct men-
tions produces a cohesive connected network,
allowing terms and entities to be related
across sentences and even larger units such as
documents. Once the network is built, spread-
ing activation is used to determine semantic

358

relatedness between terms. For example, to
determine how related car and gasoline are,
activation is given to one of the nodes, say
car, and the network is “ﬁred” to allow the
activation to spread to the rest of the net-
work. The amount of activation received by
gasoline is then a measure of the strength of
the semantic relation between the two terms.
We use three datasets derived from human
judgements of semantic relatedness to test our
technique. Since the datasets contain general
terms which may not appear in an existing
corpus, we create our own corpus by harvest-
ing text from the web via Google. This ap-
proach has the advantage of requiring little
human intervention and being extensible to
new datasets. Our results using the semantic
network derived from the web-based corpus
are comparable to the best performing exist-
ing methods tested on the same datasets.

4 Creating the Semantic Networks

ASKNet creates the semantic networks using
existing nlp tools to extract syntactic and se-
mantic information from text. This informa-
tion is then combined using a modiﬁed version
of the update algorithm used by Harrington
and Clark (2007) to create an integrated large-
scale network. By mapping together concepts
and objects that relate to the same real-world
entities, the system is able to transform the
output of various nlp tools into a single net-
work, producing semantic resources which are
more than the sum of their parts. Combin-
ing information from multiple sources results
in a representation which would not have been
possible to obtain from analysing the original
sources separately.

The nlp tools used by ASKNet are the
C&C parser (Clark and Curran, 2007) and
the semantic analysis program Boxer (Bos
et al., 2004), which operates on the ccg
derivations output by the parser to produce
a ﬁrst-order representation. The named en-
tity recognizer of Curran and Clark (2003)
is also used to recognize the standard set of
muc entities, including person, location and
organisation.

As an example of the usefulness of infor-
mation integration, consider the monk -asylum
example, taken from the rg dataset (de-
scribed in Section 5.1). It is possible that even
a large corpus could contain sentences linking
monk with church, and linking church with
asylum, but no direct links between monk and
asylum. However, with an integrated seman-
tic network, activation can travel across mul-
tiple links, and through multiple paths, and
will show a relationship, albeit probably not
a very strong one, between monk and asylum,
which corresponds nicely with our intuition.

Figure 1, which gives an example net-
work built from duc documents describing the
Elian Gonzalez custody battle, gives an indi-
cation of the kind of network that ASKNet
builds. This ﬁgure does not give the full net-
work, which is too large to show in a sin-
gle ﬁgure, but shows the “core” of the net-
work, where the core is determined using the
technique described in (Harrington and Clark,
2009). The black boxes represent named en-
tities mentioned in the text, which may have
been mentioned a number of times across doc-
uments, and possibly using diﬀerent names
(e.g. Fidel Castro vs. President Castro). The
diamonds are named directed edges, which
represent relationships between entities.

A manual evaluation using human judges
has been performed to measure the accuracy
of ASKNet networks. On a collection of duc
documents, the “cores” of the resulting net-
works were judged to be 80% accurate on av-
erage, where accuracy was measured for the
merged entity nodes in the networks and the
relations between those entities (Harrington
and Clark, 2009). The motivation for fully au-
tomatic creation is that very large networks,
containing millions of edges, can be created in
a matter of hours.

Automatically creating networks does result
in lower precision than manual creation, but
this is oﬀset by the scalability and speed of
creation. The experiments described in this
paper are a good test of the automatic cre-
ation methodology.

359

Figure 1: Example network core derived from duc documents describing a news story from 2000.

The update algorithm

mation contained within the network.

In order to merge information from multi-
ple sources into a single cohesive resource,
ASKNet uses a spreading activation based up-
date algorithm (Harrington and Clark, 2007).
As the system encounters new information, it
attempts to map named entities in the new
sentences it encounters to those already in its
network. Each new sentence is turned into an
update fragment; a fragment of the network
representing the information contained in the
sentence.
Initial mapping scores, based on
lexical similarity and named entity type, are
made between the update fragment’s named
entities and those in the main network. The
job of the update algorithm is to improve upon
those initial scorings using the semantic infor-

The update algorithm iterates over each
named entity node in the update fragment.
This base node is provided with activation,
which is allowed to spread throughout the
fragment. All named entities which receive
activation in this process, then transfer their
activation to their target named entity nodes
(nodes in the main network with which they
have a current mapping score greater than
zero). The amount transferred is based on the
strength of the mapping score. The activation
is then allowed to circulate through the main
network until it reaches a stable state. At this
point, the base node’s mappings are updated
based on which of its target nodes received
activation. The more activation a target node
receives, the more its mapping score with the

360

base node will increase.

The intuition behind the update algorithm
is that we can use relatedness of nodes in
the update fragment to determine appropriate
mappings in the main network. So if our base
node has the label “Crosby”, and is related
to named entity nodes referring to “Canada”,
“Vancouver” and “2010”, those nodes will
pass their activation onto their main network
targets, and hopefully onto the node repre-
senting the ice hockey player Sidney Crosby.
We would then increase the mapping score be-
tween our base node and this target, while at
the same time decreasing the mapping score
between out base node and the singer Bing
Crosby, who (hopefully) would have received
little or no activation. The update algorithm
is also self-reinforcing, as in the successive
stages, the improved scores will focus the ac-
tivation further. In our example, in successive
iterations, more of the activation coming to
the “Crosby” node will be sent to the appro-
priate target node, and therefore there will be
less spurious activation in the network to cre-
ate noise.

For the purposes of these experiments, we
extended the update algorithm to map to-
gether general object nodes, rather than fo-
cusing solely on named entities. This was nec-
essary due to the nature of the task. Sim-
ply merging named entities would not be suf-
ﬁcient, as many of the words in datasets would
not likely be associated strongly with any par-
ticular named entities. Extending the algo-
rithm in this way resulted in a much higher
frequency of mapping, and a much more con-
nected ﬁnal network. Because of this, we
found that several of the parameters had to
be changed from those used in Harrington
and Clark (2009). Our initial activation in-
put was set to double that used in Harring-
ton and Clark’s experiments (100 instead of
50), to compensate for the activation lost over
the higher number of links. We also found
that the number of iterations required to reach
a stable state had increased to more than 4
times the previous number. This was to be
expected due to the increased number of links

passing activation. We also had to remove the
named entity type calculation from the initial
mapping score, thus leaving the initial scor-
ing to be simply the ratio of labels in the two
nodes which overlapped. These changes were
all done after manual observation of test net-
works built from searches not relating to any
dataset, and were not changed once the exper-
iments had begun.

4.1 Measuring semantic relatedness

Once a large-scale network has been con-
structed from a corpus of documents, spread-
ing activation can be used to eﬃciently obtain
a distance score between any two nodes in the
network, which will represent the semantic re-
latedness of the pair. Each node in the net-
work has a current amount of activation and
a threshold (similar to classical ideas from the
neural network literature). If a node’s activa-
tion exceeds its threshold, it will ﬁre, sending
activation to all of its neighbours, which may
cause them to ﬁre, and so on. The amount of
activation sent between nodes decreases with
distance, so that the eﬀect of the original ﬁr-
ing is localized. The localized nature of the
algorithm is important because it means that
semantic relatedness scores can be calculated
eﬃciently even for pairs of nodes in very large
networks.

To obtain a score between nodes x and y,
ﬁrst a set amount of activation is placed in
node x; then the network is ﬁred until it sta-
bilises, and the total amount of activation re-
ceived by node y is stored as act(x,y). This
process is repeated starting with node y to ob-
tain act(y,x). The sum of these two values,
which we call dist(x,y), is used as the mea-
sure of semantic relatedness between x and y.1
the total
strength of connection between nodes x and
y, relative to the other nodes in their region.
This takes into account not just direct paths,
but also indirect paths, if the links along those
paths are of suﬃcient strength.
Since the

dist(x,y) is a measure of

1The average could be used also but this has no
eﬀect on the ranking statistics used in the later exper-
iments.

361

networks potentially contain a wide variety
of relations between terms, the calculation of
dist(x,y) has access to a wide variety of in-
formation linking the two terms.
If we con-
sider the (car, gasoline) example mentioned
earlier, the intuition behind our approach is
that these two terms are likely to be closely
related in a semantic network built from text,
either fairly directly because they appear in
the same sentence or document, or indirectly
because they are related to the same entities.

5 Experiments

The purpose of the experiments was to de-
velop an entirely automated approach for
replicating human judgements of semantic re-
latedness of words. We used three existing
datasets of human judgements:
the Hodg-
son, Rubenstein & Goodenough (rg) and
Wordsimilarity-353 (ws-353) datasets. For
each dataset we created a corpus using re-
sults returned by Google when queried for
each word independently (Described in Sec-
tion 5.2). We then built a semantic network
from that corpus and used the spreading acti-
vation technique described in the previous sec-
tion to measure semantic relatedness between
the word pairs in the dataset.

The parser and semantic analysis tool used
to create the networks were developed on
newspaper data (a ccg version of the Penn
Treebank (Steedman and Hockenmaier, 2007;
Clark and Curran, 2007)), but our impres-
sion from informally inspecting the parser
output was that the accuracy on the web
data was reasonable. The experimental re-
sults show that the resulting networks were
of high enough quality to closely replicate hu-
man judgements.

5.1 The datasets

Many studies have shown a marked prim-
ing eﬀect for semantically related words.
In
his single-word lexical priming study, (Hodg-
son, 1991) showed that the presentation of
a prime word such as election directly fa-
cilitates processing of a target word such as
vote. Hodgson showed an increase in both re-

sponse speed and accuracy when the prime
and target are semantically related. 143 word
pairs were tested across 6 diﬀerent lexical re-
lations: antonymy (e.g., enemy, friend ); con-
ceptual association (e.g., bed, sleep); category
coordination (e.g., train, truck ); phrasal as-
sociation (e.g., private, property); superordi-
nation/subordination (e.g., travel, drive); and
synonymy (e.g., value, worth). It was shown
that equivalent priming eﬀects (i.e., reduced
processing time) were present across all rela-
tion types, thus indicating that priming was a
result of the terms’ semantic relatedness, not
merely their similarity or other simpler rela-
tion type.

The Hodgson dataset consists of the 143
word pairs divided by lexical category. There
were no scores given as all pairs were shown
to have relatively similar priming eﬀects. No
examples of unrelated pairs are given in the
dataset. We therefore used the unrelated pairs
created by McDonald and Brew (2004).

The task in this experiment was to obtain
scores for all pairs, and to do an ANOVA test
to determine if there is a signiﬁcant diﬀerence
between the scores for related and unrelated
pairs.

The ws-353 dataset (Finkelstein et al.,
2002) contains human rankings of the seman-
tic distance between pairs of terms. Although
the name may imply that the scores are based
on similarity, human judges were asked to
score 353 pairs of words for their relatedness
on a scale of 1 to 10, and so the dataset is
ideal for our purposes. For example, the pair
(money, bank) is in the dataset and receives
a high relatedness score of 8.50, even though
the terms are not lexically similar.

The dataset contains regular nouns and
named entities, as well as at
least one
term which does not appear in WordNet
(Maradona).
In this experiment, we calcu-
lated scores for all word pairs, and then used
rank correlation to compare the similarity of
our generated scores to those obtained from
human judgements.

The rg dataset (Rubenstein and Goode-
nough, 1965) is very similar to the ws-353,

362

though with only 65 word pairs, except that
the human judges were asked to judge the
pairs based on synonymy, rather than over-
all relatedness. Thus, for example, the pair
(monk,asylum), receives a signiﬁcantly lower
score than the pair (crane,implement).

5.2 Data collection, preparation and

processing

In order to create a corpus from which to build
the semantic networks, we ﬁrst extracted each
individual word from the pairings, resulting
in a list of 440 words for the ws-353 collec-
tion, 48 words for the rg (some words were
used in multiple pairings), and 282 words for
the Hodgson collection. For each of the words
in this list, we then performed a query using
Google, and downloaded the ﬁrst 5 page re-
sults for that query. The choice of 5 as the
number of documents to download for each
word was based on a combination of infor-
mal intuition about the precision and recall of
search engines, as well as the practical issue of
obtaining a corpus that could be processed in
reasonable space and time.

Each of the downloaded web pages was then
cleaned by a set of Perl scripts which removed
all HTML markup. Statistics for the resulting
corpora are given in Table 1.

Three rules were added to the retrieval pro-
cess to deal with problems encountered in for-
matting of web-pages:

1. Pages from which no text could be re-
trieved were ignored and replaced with
the next result.

2. HTML lists preceded by a colon were re-

combined into sentences.

3. For Wikipedia disambiguation pages
(pages which consist of a list of links to
articles relating to the various possible
senses of a word), all of the listed links
were followed and the resulting pages
added to the corpus.

Each of these heuristics was performed au-
tomatically and without human intervention.
The largest of the networks, created for the
ws-353 dataset, took slightly over 24 hours

corpus
Hodgson
rg
ws-353

sentences
814,779
150,165
1,042,128

words

3,745,870
573,148
5,027,947

Table 1: Summary statistics for the corpora gener-
ated for the experiments.

to complete, including time for parsing and
semantic analysis.

6 Results

6.1 Hodgson priming dataset

After processing the Hodgson corpus
to
build a semantic network with approximately
500,000 nodes and 1,300,000 edges, the appro-
priate node pairs were ﬁred to obtain the dis-
tance measure as previously described. Those
measurements were then recorded as measure-
ments of semantic relatedness between two
terms. If a term was used as a label in two
or more nodes, all nodes were tried, and the
highest scoring pairs were used.

As the Hodgson dataset did not provide ex-
amples of unrelated pairs against which we
could compare, unrelated pairs were gener-
ated as described in (McDonald and Brew,
2004). This is not an ideal method, as sev-
eral pairs that were identiﬁed as unrelated did
have some relatively obvious relationship (e.g.
tree – house, poker – heart). However we chose
to retain the methodology for consistency with
previous literature as it was also used in (Pad´o
and Lapata, 2007).

Scores were obtained from the network for
the word pairs, and for each target an aver-
age score was calculated for all primes in its
category. Example scores are given in Table
2.

Two-way analysis of variance (ANOVA)
was carried out on the network scores with
the the relatedness status of the pair being
the independent variable. A reliable eﬀect
was observed for the network scores with the
primes for related words being signiﬁcantly
larger than those for unrelated words. The
results are given in Table 3.

The use of ANOVA shows that there is a

363

Word pair
empty - full
coﬀee - mug
horse - goat
dog - leash
friend - antonym
vote - conceptual
property - phrasal
drive - super/sub

Related Network Score
10.13
5.86
0.96
4.70
0.53
1.37
2.47
1.86

Yes
Yes
Yes
Yes
No
No
No
No

Table 2: Example scores obtained from the network
for related and unrelated word pairs from the Hodgson
dataset

diﬀerence in the scores of the related and un-
related word pairs that cannot be accounted
for by random variance. However, in order
to compare the strength of the experimental
eﬀects between two experiments, additional
statistics must be used. Eta-squared (η2) is
a measure of the strength of an experimental
eﬀect. A high η2 indicates that the indepen-
dent variable accounts for more of the variabil-
ity, and thus indicates a stronger experimen-
tal eﬀect.
In our experiments, we found an
η2 of 0.411, which means that approximately
41% of the overall variance can be explained
by the relatedness scores.

For comparison, we provide the ANOVA
results for experiments by (McDonald and
Brew, 2004) and (Pad´o and Lapata, 2007) on
the same dataset. Both of these experiments
obtained scores using vector based models
populated with data from the bnc.

We also include the results obtained from
performing the same ANOVA tests on Point-
wise Mutual Information scores collected over
our corpus. These results were intended to
provide a baseline when using the web-based
corpus. To calculate the pmi scores for this ex-
periment, we computed scores for the number
of times the two words appeared in the same
paragraph or document, and the total number
of occurrences of words in the corpus. The pmi
scores were calculated by simply dividing the
number of times the words co-occurred within
a paragraph, by the product of the number of
occurrences of each word within a document.

F

McDonald & Brew 71.73
182.46
Pad´o & Lapata
42.53
pmi
Network
50.71

MSE p
0.004 < 0.001

η2

0.332
0.93 < 0.01
3.79 < 0.001
0.263
3.28 < 0.0001 0.411

Table 3: ANOVA results of scores generated from
the Hodgson dataset compared to those reported for
existing systems. (F = F-test statistic, MSE = Mean
squared error, p = P-value, η2 = Eﬀect size)

6.2 ws-353 and rg datasets

The methodology used to obtain scores for the
ws-353 and rg collections was identical to
that used for the Hodgson data, except that
scores were only obtained for those pairs listed
in the data set. Because both collections pro-
vided direct scores, there was no need to re-
trieve network scores for unrelated pairings.

WikiRelate!
Hughes-Ramage
Agirre Et Al
pmi
Network

ws-353

0.48
0.55
0.66
0.41
0.62

rg
0.86
0.84
0.89
0.80
0.86

Table 4: Rank correlation scores for the semantic
network and pmi-based approaches, calculated on the
ws-353 and rg collections, shown against scores for
existing systems.

For consistency with previous literature, the
scores obtained by the semantic network were
compared with those from the collections us-
ing Spearman’s rank correlation. The correla-
tion results are given in Table 4. For compar-
ison, we have included the results of the same
correlation on scores from three top scoring
systems using the approaches described above.
We also include the scores obtained by using
a simple pmi calculation as in the previous ex-
periment.

The scores obtained by our system were not
an improvement on those obtained by existing
systems. However, our scores were on par with
the best performing systems, which were pur-
pose built for this application, and at least in
the case of the system by Agirre et al. used a
corpus several orders of magnitude larger.

364

7 Conclusion
In this paper we have shown that a semantic
network approach to determining semantic re-
latedness of terms can achieve performance on
par with the best purpose built systems. This
is interesting for two reasons. Firstly, the ap-
proach we have taken in this paper is much
more analogous to the way humans perform
similar tasks. Secondly, the system used was
not purpose built for this application.
It is
instead a general tool for information collec-
tion and integration, and this result indicates
that it will likely be useful for a wide variety
of language processing applications.

References
Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL-HLT.

Banerjee, Satanjeev and T. Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Con-
ference on Artiﬁcial Intelligence, Acapulco, Mexico.

Bos, Johan, Stephen Clark, Mark Steedman, James R.
2004. Wide-
Curran, and Julia Hockenmaier.
coverage semantic representations from a CCG
parser.
the 20th Interna-
tional Conference on Computational Linguistics
(COLING-04), pages 1240–1246, Geneva, Switzer-
land.

In Proceedings of

Budanitsky, Alexander and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of semantic dis-
tance.
Computational Linguistics, 32:13 – 47,
March.

Clark, Stephen and James R. Curran. 2007. Wide-
coverage eﬃcient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.

Collins, Allan M. and Elizabeth F. Loftus. 1975. A
spreading-activation theory of semantic processing.
Psychological Review, 82(6):407–428.

Curran, James R. and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the Seventh Conference on Natu-
ral Language Learning, pages 164–167, Edmonton,
Canada.

Fellbaum, Christiane, editor. 1998. WordNet : An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass, USA.

Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. In ACM Transactions on Infor-
mation Systems, volume 20(1), pages 116–131.

Harrington, Brian and Stephen Clark. 2007. Asknet:
Automated semantic knowledge network.
In Pro-
ceedings of the 22nd National Conference on Arti-
ﬁcial Intelligence (AAAI’07), pages 889–894, Van-
couver, Canada.

Harrington, Brian and Stephen Clark. 2009. Asknet:
Creating and evaluating large scale integrated se-
mantic networks. International Journal of Seman-
tic Computing, 2(3):343–364.

Hodgson, James. 1991.

Information constraints on
pre-lexical priming. Language and Cognitive Pro-
cesses, 6:169 – 205.

Hughes, Thad and Daniel Ramage. 2007. Lexical se-
In
mantic relatedness with random graph walks.
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
581–589, Prague, Czech Republic.

Jiang, J. J. and D. W. Conrath.

Seman-
tic similarity based on corpus statistics and lexi-
cal taxonomy. In International Conference on Re-
search on Computational Linguistics, Taipei, Tai-
wan, September.

1997.

McDonald, Scott and Chris Brew. 2004. A distri-
butional model of semantic context eﬀects in lexi-
cal processing. In Proceedings of the 42th Annual
Meeting of the Association for Computational Lin-
guistics, pages 17 – 24, Barcelona, Spain.

Mohammad, Saif and Graeme Hirst.

2006. Dis-
tributional measures of concept-distance: A task-
oriented evaluation.
In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2006), Sydney, Australia.

Pad´o,

Sebastian and Mirella Lapata.

2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.

Resnik, Philip. 1999. Semantic similarity in a tax-
onomy: An information-based measure and its ap-
plication to problems of ambiguity in natural lan-
guage. Journal of Artiﬁcial Intelligence Research,
11:95–130.

Rubenstein, H. and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Computational Lin-
guistics, 8:627 – 633.

Steedman, Mark and Julia Hockenmaier. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33:355–396.

Strube, Michael and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st national con-
ference on Artiﬁcial intelligence, pages 1419–1424.
AAAI Press.

Turney, Peter D. 2001. Lecture notes in computer
science 1: Mining the web for synonyms: PMI-IR
versus LSA on TOEFL.

