



















































Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614–1623,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Clustering Aspect-related Phrases by Leveraging Sentiment Distribution
Consistency

Li Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
National Laboratory for Information Science and Technology

Dept. of Computer Science and Technology, Tsinghua University, Beijing, PR China
*China Information Technology Security Evaluation Center

zhaoli19881113@126.com aihuang@tsinghua.edu.cn

Abstract

Clustering aspect-related phrases in terms
of product’s property is a precursor pro-
cess to aspect-level sentiment analysis
which is a central task in sentiment analy-
sis. Most of existing methods for address-
ing this problem are context-based models
which assume that domain synonymous
phrases share similar co-occurrence con-
texts. In this paper, we explore a novel
idea, sentiment distribution consistency,
which states that different phrases (e.g.
“price”, “money”, “worth”, and “cost”) of
the same aspect tend to have consistent
sentiment distribution. Through formal-
izing sentiment distribution consistency as
soft constraint, we propose a novel unsu-
pervised model in the framework of Poste-
rior Regularization (PR) to cluster aspect-
related phrases. Experiments demonstrate
that our approach outperforms baselines
remarkably.

1 Introduction

Aspect-level sentiment analysis has become a cen-
tral task in sentiment analysis because it can ag-
gregate various opinions according to a product’s
properties, and provide much detailed, complete,
and in-depth summaries of a large number of re-
views. Aspect finding and clustering, a precursor
process of aspect-level sentiment analysis, has at-
tracted more and more attentions (Mukherjee and
Liu, 2012; Chen et al., 2013; Zhai et al., 2011a;
Zhai et al., 2010).

Aspect finding and clustering has never been a
trivial task. People often use different words or
phrases to refer to the same product property (also
called product aspect or feature in the literature).
Some terms are lexically dissimilar while seman-
tically close, which makes the task more challeng-
ing. For example, “price”, “money” , “worth” and

“cost” all refer to the aspect “price” in reviews.
In order to present aspect-specific summaries of
opinions, we first of all, have to cluster different
aspect-related phrases. It is expensive and time-
consuming to manually group hundreds of aspect-
related phrases. In this paper, we assume that the
aspect phrases have been extracted in advance and
we keep focused on clustering domain synony-
mous aspect-related phrases.

Existing studies addressing this problem are
mainly based on the assumption that different
phrases of the same aspect should have similar co-
occurrence contexts. In addition to the traditional
assumption, we develop a new angle to address the
problem, which is based on sentiment distribution
consistency assumption that different phrases of
the same aspect should have consistent sentiment
distribution, which will be detailed soon later.

Figure 1: A semi-structured Review.

This new angle is inspired by this simple obser-
vation (as illustrated in Fig. 1): two phrases within
the same cluster are not likely to be simultaneously
placed in Pros and Cons of the same review. A
straightforward way to use this information is to
formulate cannot-link knowledge in clustering al-
gorithms (Chen et al., 2013; Zhai et al., 2011b).
However, we have a particularly different manner
to leverage the knowledge.

Due to the availability of large-scale semi-
structured customer reviews (as exemplified in
Fig. 1) that are supported by many web sites,
we can easily get the estimation of sentiment dis-
tribution for each aspect phrase by simply count-
ing how many times a phrase appears in Pros and

1614



Cons respectively. As illustrated in Fig. 2, we
can see that the estimated sentiment distribution
of a phrase is close to that of its aspect. The
above observation suggests the sentiment distri-
bution consistency assumption: different phrases
of the same aspect tend to have the same senti-
ment distribution, or to have statistically close
distributions. This assumption is also verified by
our data: for most (above 91.3%) phrase with rela-
tively reliable estimation (whose occurrence≥50),
the KL-divergence between the sentiment distri-
bution of a phrase and that of its corresponding
aspect is less than 0.05.

Figure 2: The sentiment distribution of aspect
“battery” and its related-phrases on nokia 5130
with a large amount of reviews.

It is worth noting that, the sentiment distribution
of a phrase can be estimated accurately only when
we obtain a sufficient number of reviews. When
the number of reviews is limited, however, the es-
timated sentiment distribution for each phrase is
unreliable (as shown in Fig. 3). A key issue,
arisen here, is how to formulate this assumption in
a statistically robust manner. The proposed model
should be robust when only a limited number of
reviews are available.

Figure 3: The sentiment distribution of aspect
“battery” and its related-phrases on nokia 3110c
with a small mumber of reviews.

To deal with this issue, we model sentiment dis-
tribution consistency as soft constraint, integrated
into a probabilistic model that maximizes the data
likelihood. We design the constraint to work in
the following way: when we have sufficient ob-
servations, the constraint becomes tighter, which

plays a more important role in the learning pro-
cess; when we have limited observations, the con-
straint becomes very loose so that it will have less
effect on the model.

In this paper, we propose a novel unsupervised
model, Sentiment Distribution Consistency Reg-
ularized Multinomial Naive Bayes (SDC-MNB).
The context part is modeled by Multinomial Naive
Bayes in which aspect is treated as latent variable,
and Sentiment distribution consistency is encoded
as soft constraint within the framework of Poste-
rior Regularization (PR) (Graca et al., 2008). The
main contributions of this paper are summarized
as follows:

• We study the problem of clustering phrases
by integrating both context information
and sentiment distribution of aspect-related
phrases.

• We explore a novel concept, sentiment distri-
bution consistency(SDC), and model it as soft
constraint to guide the clustering process.

• Experiments show that our model outper-
forms the state-of-art approaches for aspect
clustering.

The rest of this paper is organized as follows.
We introduce the SDC-MNB model in Section 2.
We present experiment results in Section 3. In
Section 4, we survey related work. We summarize
the work in Section 5.

2 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes

In this section, we firstly introduce our assumption
sentiment distribution consistency formally and
show how to model the above assumption as soft
constraint , which we term SDC-constraint. Sec-
ondly, we show how to combine SDC-constraint
with the probabilistic context model. Finally, we
present the details for context and sentiment ex-
traction.

2.1 Sentiment Distribution Consistency

We define aspect as a set of phrases that refer to
the same property of a product and each phrase is
termed aspect-related phrase (or aspect phrase in
short). For example, the aspect “battery” contains
aspect phrases such as “battery”, “battery life”,
“power”, and so on.

1615



F the aspect phrase set
fj the jth aspect phrase
yj the aspect for aspect phrase fj
A the aspect set
ai the ith aspect
D the set of context documents
dj the context document of fj
V the word vocabulary
wt the tth word in vocabulary V

wdj ,k the k
th word in dj

Ntj the number of times word wt occurs in dj
P the product set
pk the kth product

uik
the sentiment distribution parameter
of aspect ai on pk

ŝjk
the estimated sentiment distribution parameter
of phrase fj on pk

njk the occurrence times of aspect phrase fj on pk
σ̂jk the sample standard deviation
θ the model parameters

pθ(ai|dj) the posterior distribution of ai given dj
q(yj = ai)

the projected posterior distribution
of ai given dj

Table 1: Notations

Let us consider the sentiment distribution on a
certain aspect ai. In a large review dataset, as-
pect ai could receive many comments from differ-
ent reviewers. For each comment, we assume that
people either praise or complain about the aspect.
So each comment on the aspect can be seen as a
Bernoulli trial, where the aspect receives positive
comments with probability pai

1. We introduce a
random variable Xai to denote the sentiment on
aspect ai, where Xai = 1 means that aspect ai
receives positive comments, Xai = 0 means that
aspect ai receives negative comments. Obviously,
the sentiment on aspect ai follows the Bernoulli
distribution,

Pr(Xai) = p
Xai
ai ∗ (1− pai)1−Xai , Xai ∈ {0, 1}. (1)

Or in short,

Xai ∼ Bernoulli(pai)

Let us see the case for aspect phrase fj , where
fj ∈ aspect ai. Similarly, each comment on an as-
pect phrase fj can also be seen as a Bernoulli trial.
We introduce a random variable Xfj to denote the
sentiment on aspect phrase fj , where Xfj = 1
means that aspect fj receives positive comments,
Xfj = 0 means that aspect fj receives negative
comments. As just discussed, we assume that each
aspect phrase follows the same distribution with

1positive comment means that an aspect term is observed
in Pros of a review.

the corresponding aspect. This leads to the fol-
lowing formal description:

• Sentiment Distribution Consistency : The
sentiment distribution of aspect phrase is the
same as that of the corresponding aspect.
Formally, for all aspect phrase fj ∈ aspect
ai, Xfj ∼ Bernoulli(pai).

2.2 Sentiment Distribution Consistency
Constraint

Assuming the sentiment distribution of aspect ai is
given in advance, we need to judge whether an as-
pect phrase fj belongs to the aspect ai with limited
observations for fj . Let’s consider the example in
Fig. 4. For aspect phrase 3, we have no definite
answer due to the limited number of observations.
For aspect phrase 1, it seems that the sentiment
distribution is consistent with that of the left as-
pect. However, we can not say that the phrase be-
longs to the aspect because the distribution may
be the same for two different aspects. For aspect
phrase 2, we are confident that its sentiment dis-
tribution is different from that of the left aspect,
given sufficient observations.

Figure 4: Sentiment distribution of an aspect, and
observations on aspect phrases.

To be concise, we judge an aspect phrase
doesn’t belong to certain aspect only when we are
confident that they follow different sentiment dis-
tributions.

Inspired by the intuition, we conduct interval
parameter estimation for parameter pfj (sentiment
distribution for phrase fj) with limited observa-
tions, and thus get a confidence interval for pfj .
If pai(sentiment distribution for aspect ai) is not
in the confidence interval of pfj , we then are con-
fident that they follow different distributions. In
other words, if aspect phrase fj ∈ aspect ai, we
are confident that pai is in the confidence interval
of pfj .

More formally, we use uik to denote the senti-
ment distribution parameter of aspect ai on prod-
uct pk, and assume that uik is given in advance.

1616



We want to know whether the sentiment distribu-
tion on aspect phrase fj is the same as that of as-
pect ai on product pk given a limited number of
observations (samples). It’s straightforward to cal-
culate the confidence interval for parameter sjk in
the Bernoulli distribution function. Let the sam-
ple mean of njk samples be ŝjk, and the sample
standard deviation be σ̂jk. Since the sample size
is small here, we use the Student-t distribution to
calculate the confidence interval. According to our
assumption, we are confident that uik is in the con-
fidence interval if fj ∈ ai.

ŝjk −C σ̂jk√
njk

≤ uik ≤ ŝjk + C σ̂jk√
njk

, ∀fj ∈ ai,∀k. (2)

where we look for t-table to find C corresponding
to a certain confidence level(such as 95%) with the
freedom of njk − 1. For simplicity, we represent
the above confidence interval by [ŝjk − djk, ŝjk +
djk], where djk = C

σ̂jk√
njk

.
We introduce an indicator variable zij to repre-

sent whether the aspect phrase fj belongs to aspect
ai, as follows:

zji =

{
1 ; if fj ∈ ai
0 ; otherwise

(3)

This leads to our SDC-constraint function.

ϕ = zji|uik − ŝjk| ≤ djk,∀i, j, k (4)

SDC-constraint are flexible for modeling Senti-
ment Distribution Consistency. The more obser-
vations we have, the smaller djk is. For frequent
aspect phrase, the constraint can be very informa-
tive because it can filter unrelated aspects for as-
pect phrase fj . The less observations we have,
the larger djk is. For rare aspect phrases, the con-
straint can be very loose, and will not have much
effect on the clustering process for aspect phrase
fj . In this way, the model can work very robustly.

SDC-constraints are data-driven constraints.
Usually we have many reviews about hundreds of
products in our dataset. For each aspect phrase,
there are |A| ∗ |P | constraints (the number of as-
pects times the number of product). With thou-
sands of constraints about which aspect it is not
likely to belong to, the model learns to which as-
pect a phrase fj should be assigned. Although
most constraints may be loose because of the lim-
ited observations, SDC-constraint can still play an
important role in the learning process.

2.3 Sentiment Distribution Consistency
Regularized Multinomial Naive Bayes
(SDC-MNB)

In this section, we present our probabilistic model
which employs both context information and sen-
timent distribution.

First of all, we extract a context document d
for each aspect phrase, which will be described in
Section 2.5. In other word, a phrase is represented
by its context document. Assuming that the doc-
uments in D are independent and identically dis-
tributed, the probability of generating D is then
given by:

pθ(D) =

|D|∏
j=1

pθ(dj) =

|D|∏
j=1

∑
yj∈A

pθ(dj , yj) (5)

where yj is a latent variable indicating the aspect
label for aspect phrase fj , and θ is the model pa-
rameter.

In our problem, we are actually more inter-
ested in the posterior distribution over aspect,
i.e., pθ(yj |dj). Once the learned parameter θ is
obtained, we can get our clustering result from
pθ(yj |dj), by assigning aspect ai with the largest
posterior to phrase fj . We can also enforce SDC-
constraint in expectation(on posterior pθ). We use
q(Y ) to denote the valid posterior distribution that
satisfy our SDC-constraint, and Q to denote the
valid posterior distribution space, as follows:

Q = {q(Y ) : Eq[zji|uik − ŝjk|] ≤ djk, ∀i, j, k}. (6)

Since posterior plays such an important role in
joining the context model and SDC-constraint, we
formulate our problem in the framework of Poste-
rior Regularization (PR). PR is an efficient frame-
work to inject constraints on the posteriors of la-
tent variables. Instead of restricting pθ directly,
which might not be feasible, PR penalizes the dis-
tance of pθ to the constraint set Q. The posterior-
regularized objective is termed as follows:

max
θ
{log pθ(D)−min

q∈Q
KL(q(Y )||pθ(Y |D))} (7)

By trading off the data likelihood of the ob-
served context documents (as defined in the first
term), and the KL divergence of the posteriors
to the valid posterior subspace defined by SDC-
constraint (as defined in the second term), the ob-
jective encourages models with both desired pos-
terior distribution and data likelihood. In essence,
the model attempts to maximize data likelihood of
context subject (softly) to SDC-constraint.

1617



2.3.1 Multinomial Naive Bayes
In spirit to (Zhai et al., 2011a), we use Multino-
mial Naive Bayes (MNB) to model the context
document. Let wdj ,k denotes the k

th word in doc-
ument dj , where each word is from the vocabulary
V = {w1, w2, ..., w|V |}. For each aspect phrase
fj , the probability of its latent aspect being ai and
generating context document di is

pθ(dj , yj = ai) = p(ai)
|dj |∏
k=1

p(wdj ,k|ai) (8)

where p(ai) and p(wdj ,k|ai) are parameters of this
model. Each word wdj ,k is conditionally indepen-
dent of all other words given the aspect ai.

Although MNB has been used in existing work
for aspect clustering, all of the studies used it in
a semi-supervised manner, with labeled data or
pseudo-labeled data. In contrast, MNB proposed
here is used in an unsupervised manner for aspect-
related phrases clustering.

2.3.2 SDC-constraint
As mentioned above, the constraint posterior set Q
is defined by

Q = {q(Y ) : q(yj = ai)|uik − ŝjk| ≤ djk,∀i, j, k}. (9)

We can see that Q denotes a set of linear con-
straints on the projected posterior distribution q.
Note that we do not directly observe uik, the sen-
timent distribution of aspect ai on product pk. For
aspect phrase fj that belongs to aspect ai, we es-
timate uik by counting all sentiment samples. We
use the posterior pθ(ai|dj) to approximately rep-
resent how likely phrase fj belongs to aspect ai.

uik =
1∑|D|

j=1 njkpθ(ai|dj)

|D|∑
j=1

njkpθ(ai|dj)ŝjk (10)

where pθ(ai|dj) is short for pθ(yj = ai|dj), the
probability that aspect phrase fj belongs to ai
given the context document dj . We estimate uik in
this way because observations for aspect are rela-
tively sufficient for a reliable estimation since ob-
servations for an aspect are aggregated from those
for all phrases belonging to that aspect.

2.4 The Optimization Algorithm
The optimization algorithm for the objective (see
Eq. 7) is an EM-like two-stage iterative algorithm.

In E-step, we first calculate the posterior distri-
bution pθ(ai|dj), then project it onto the valid pos-
terior distribution space Q. Given the parameters

θ, the posterior distribution can be calculated by
Eq. 11.

pθ(ai|dj) =
p(ai)

∏|dj |
k=1 p(wdj ,k|ai)∑|A|

r=1 p(ar)
∏|dj |

k=1 p(wdj ,k|ar)
(11)

We use the above posterior distribution to update
the sentiment parameter for each aspect by Eq. 10.
The projected posterior distribution q is calculated
by

q = argmin
q∈Q

KL(q(Y )||pθ(Y |D)) (12)

For each instance, there are |A| ∗ |P | constraints.
However, we can prune a large number of useless
constraints derived from limited observations. All
constraints with djk > 1 can be pruned, due to
the fact that the parameter uik, ŝjk is within [0,1],
and the difference can not be larger than 1. This
optimization problem in Eq. 12 is easily solved via
the dual form by the projected gradient algorithm
(Boyd and Vandenberghe, 2004):

max
λ≥0

(
−

|A|∑
i=1

|P |∑
k=1

λikdjk−

log

|A|∑
i=1

pθ(ai|dj)exp{−
|P |∑
k=1

λik|uik − ŝjk|} − ϵ∥λ∥
)
(13)

where ϵ controls the slack size for constraint. After
solving the above optimization problem and ob-
taining the optimal λ, we can calculate the pro-
jected posterior distribution q by

q(yj = ai) =
1

Z
pθ(ai|dj)exp{−

|P |∑
k=1

λik|uik−ŝjk|} (14)

where Z is the normalization factor. Note that sen-
timent distribution consistency is actually modeled
as instance-level constraint here, which makes it
very efficient to solve.

In M-step, the projected posteriors q(Y ) are
then used to compute sufficient statistics and up-
date the models parameters θ. Given the projected
posteriors q(Y ), the parameters can be updated by
Eq. 15,16.

p(ai) =
1 +

∑|D|
j=1 q(yj = ai)

|A|+ |D| (15)

p(wt|ai) =
1 +

∑|D|
j=1 Ntiq(yj = ai)

|V |+ ∑|V |m=1 ∑|D|j=1 Nmjq(yj = ai) (16)
where Ntj is the number of times that the word wt
occurs in document dj .

The parameters are initialized randomly, and we
repeat E-step and M-step until convergence.

1618



2.5 Data Extraction
2.5.1 Context Extraction
In order to extract the context document d for each
aspect phrase, we follow the approach in Zhai et
al. (2011a). For each aspect phrase, we generate
its context document by aggregating the surround-
ing texts of the phrase in all reviews. The preced-
ing and following t words of a phrase are taken as
the context where we set t = 3 in this paper. Stop-
words and other aspect phrases are removed. For
example, the following review contains two aspect
phrases, ”screen” and ”picture”,

The LCD screen gives clear picture.

For ”screen”, the surrounding texts are {the,
LCD, gives, clear, picture}. We remove stop-
words ”the”, and the aspect term ”picture”, and
the resultant context of ”screen” in this review is

context(screen) ={LCD, screen, gives, clear}.
Similarly, the context of ”picture” in this review is

context(picture) ={gives, clear}.
By aggregating the contexts of all the reviews
that contain aspect phrase fj , we obtain the cor-
responding context document dj .

2.5.2 Sentiment Extraction
Since we use semi-structured reviews, we ob-
tain the estimated sentiment distribution by sim-
ply counting how many times each aspect phrase
appears in Pros and Cons reviews for each prod-
uct respectively. So for each aspect phrase fj , let
n+jk denotes the times that fj appears in Pros of
all reviews for product pk, and let n−jk denotes the
times that fj appears in Cons of all reviews for
product pk. So the total number of occurrence of a
phrase is njk = n+jk + n

−
jk. We have samples like

(1,1,1,0,0) where 1 means a phrase occurs in Pros
of a review, and 0 in Cons. Given a sequence of
such observations, the sample mean is easily com-

puted as ŝjk =
n+jk

n+jk+n
−
jk

. And the sample standard

deviation is σ̂jk =

√
(1−ŝjk)2∗n+jk+(ŝjk)2∗n−jk

njk−1 .

3 Experiments

3.1 Data Preparation
The details of our review corpus are given
in Table 2. This corpus contains semi-
structured customer reviews from four do-
mains: Camera, Cellphone, Laptop, and MP3.

These reviews were crawled from the following
web sites: www.amazon.cn, www.360buy.com,
www.newegg.com.cn, and www.zol.com. The as-
pect label of each aspect phrases is annotated by
human curators.

Camera Cellphone Laptop MP3
#Products 449 694 702 329
#Reviews 101,235 579,402 102,439 129,471

#Aspect Phrases 236 230 238 166
#Aspect 12 10 14 8

Table 2: Statistics of the review corpus. # denotes
the size.

3.2 Evaluation Measures
We adapt three measures Purity, Entropy, and
Rand Index for performance evaluation. These
measures have been commonly used to evaluate
clustering algorithms.

Given a data set DS, suppose its gold-standard
partition is G = {g1, ..., gj , ..., gk}, where k
is the number of clusters. A clustering algo-
rithm partitions DS into k disjoint subsets, say
DS1, DS2, ..., DSk.
Entropy: For each resulting cluster, we can mea-
sure its entropy using Eq. 17, where Pi(gj) is the
proportion of data points of class gj in DSi. The
entropy of the entire clustering result is calculated
by Eq. 18.

entropy(DSi) = −
k∑

j=1

Pi(gj)log2Pi(gj) (17)

entropy(DS) =

k∑
i=1

|DSi|
|DS| entropy(DSi) (18)

Purity: Purity measures the extent that a cluster
contains only data from one gold-standard parti-
tion. The cluster purity is computed with Eq. 19.
The total purity of the whole clustering result (all
clusters) is computed with Eq. 20.

purity(DSi) = max
j

Pi(gj) (19)

purity(DS) =

k∑
i=1

|DSi|
|DS| purity(DSi) (20)

RI: The Rand Index(RI) penalizes both false posi-
tive and false negative decisions during clustering.
Let TP (True Positive) denotes the number of pairs
of elements that are in the same set in DS and in
the same set in G. TN (True Negative) denotes
number of pairs of elements that are in different
sets in DS and in different sets in G. FP (False

1619



Camera Cellphone Laptop MP3
P RI E P RI E P RI E P RI E

Kmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047
L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990
LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296
Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924
SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578

Table 3: Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random
index.)

Positive) denotes number of pairs of elements in
S that are in the same set in DS and in different
sets in G. FN (False Negative) denotes number of
pairs of elements that are in different sets in DS
and in the same set in G. The Rand Index(RI) is
computed with Eq. 21.

RI(DS) =
TP + TN

TP + TN + FP + FN
(21)

3.3 Evaluation Results

3.3.1 Comparison to unsupervised baselines
We compared our approach with several existing
unsupervised methods. Some of the methods aug-
mented unsupervised models by incorporating lex-
ical similarity and other domain knowledge. All
of them are context-based models.2 We list these
models as follows.

• Kmeans: Kmeans is the most popular cluster-
ing algorithm. Here we use the context distri-
butional similarity (cosine similarity) as the
similarity measure.

• L-EM: This is a state-of-the-art unsupervised
method for clustering aspect phrases (Zhai et
al., 2011a). L-EM employed lexical knowl-
edge to provide a better initialization for EM.

• LDA: LDA is a popular topic model(Blei et
al., 2003). Given a set of documents, it out-
puts groups of terms of different topics. In
our case, each aspect phrase is processed as a
term. 3 Each sentence in a review is consid-
ered as a document. Each aspect is consid-
ered as a topic. In LDA, a term may belong
to more than one topic/group, but we take the
topic/group with the maximum probability.

2In our method, we collect context document for each
aspect phrase. This process is conducted for L-EM and K-
means. But for LDA and Constraint-LDA, we take each sen-
tence of reviews as a document. This setting for the LDA
baselines is adapted from previous work.

3Each aspect phrase is pre-processed as a single word
(e.g., “battery life” is treated as battery-life). Other words
are normally used in LDA.

• Constraint-LDA: Constraint-LDA (Zhai et
al., 2011b) is a state-of-the-art LDA-based
method that incorporates must-link and
cannot-link constraints for this task. We set
the damping factor λ = 0.3 and relaxation
factor η = 0.9, as suggested in the original
reference.

For all methods that depend on the random ini-
tiation, we use the average results of 10 runs as the
final result. For all LDA-based models, we choose
α = 50/T , β = 0.1, and run 1000 iterations.

Experiment results are shown in Table 3. We
can see that our approach almost outperforms all
unsupervised baseline methods by a large margin
on all domains. In addition, we have the following
observations:

• LDA and Kmeans perform poorly due to the
fact that the two methods do not use any prior
knowledge. It is also shown that only using
the context distributional information is not
sufficient for clustering aspect phrases.

• Constraint-LDA and L-EM that utilize prior
knowledge perform better. We can see that
Constraint-LDA outperforms LDA in terms
of RI (Rand Index) on all domains. L-EM
achieves the best results against the baselines.
This demonstrates the effectiveness to incor-
porate prior knowledge.

• SDC-MNB produces the optimal results
among all models for clustering. Methods
that use must-links and cannot-links may suf-
fer from noisy links. For L-EM, we find
that it is sensitive to noisy must-links. As
L-EM assumes that must-link is transitive,
several noisy must-links may totally misla-
bel the softly annotated data. For Constraint-
LDA, it is more robust than L-EM, because
it doesn’t assume the transitivity of must-
link. However, it only promotes the RI (Rand
Index) consistently by leveraging pair-wise
prior knowledge, but sometimes it hurts the

1620



performance with respect to purity or en-
tropy. Our method is consistently better on
almost all domains, which shows the advan-
tages of the proposed model.

• SDC-MNB is remarkably better than base-
lines, particularly for the cellphone domain.
We argue that this is because we have the
largest number of reviews for each product
in the cellphone domain. The larger dataset
gives us more observations on each phrase,
so that we obtain more reliable estimation of
model parameters.

3.3.2 Comparison to supervised baselines

We further compare our methods with two super-
vised models. For each supervised model, we
provide a proportion of manually labeled data for
training, which is randomly selected from gold-
standard annotations. However, we didn’t use any
labeled data for our approach.

• MNB: The labeled seeds are used to train a
MNB classifier to classify all unlabeled as-
pect phrases into different classes.

• L-Kmeans: In L-Kmeans, the clusters of the
labeled seeds are fixed at the initiation and
remain unchanged during iteration.

Purity RI Entropy
MNB-5% 53.21% 85.77% 1.854
MNB-10% 59.55% 86.70% 1.656
MNB-15% 66.06% 88.39% 1.449

L-Kmeans-10% 53.54% 86.15% 1.745
L-Kmeans-15% 57.00% 86.89% 1.643
L-Kmeans-20% 60.97% 87.63% 1.528

SDC-MNB 59.49% 88.26% 1.580

Table 4: Comparison to supervised baselines.
MNB-5% means MNB with 5% labeled data.

We experiment with several settings: taking
5%, 10% and 15% of the manually labeled aspect
phrases for training, and the remainder as unla-
beled data. Experiment results is shown in Table
4 (the results are averaged over 4 domains). We
can see that our unsupervised approach is roughly
as good as the supervised MNB with 10% labeled
data. Our unsupervised approach is also slightly
better than L-Kmeans with 15% labeled data. This
result further demonstrates the effectiveness of our
model.

3.3.3 Influence of parameters
We vary the confidence level from 90% to 99.9%
to see how it impacts on the performance of SDC-
MNB. The results are presented in Fig. 5 (the re-
sults are averaged over 4 domains). We can see
that the performance of clustering is fairly stable
when changing the confidence level, which im-
plies the robustness of our model.

Figure 5: Influence of the confidence level on
SDC-MNB.

3.3.4 Analysis of SDC-constraint
As mentioned in Section 2.2, SDC-constraint is
dependent on the number of observations. More
observations we get, more informative the con-
straint is, which means the constraint is tighter and
djk (see Eq.4) is smaller. For all k, we count how
many djk is less than 0.2 (and 1) on average for
each aspect phrase fj . djk is calculated with a
confidence level of 99%. The statistics of con-
straints is given in Table 5. We can see that the
cellphone domain has the most informative and
largest constraint set, that may explain why SDC-
MNB achieves the largest purity gain(over L-EM)
in cellphone domain.

#(djk < 0.2) #(0.2 < djk < 1) purity gain
Camera 3.02 8.78 1.53%

Cellphone 17.29 30.5 15.99%
Laptop 4.6 13.22 6.58%

MP3MP4 6.1 10.7 13.82%

Table 5: Constraint statistics on different domains.

4 Related Work

Our work is related to two important research
topics: aspect-level sentiment analysis, and
constraint-driven learning. For aspect-level senti-
ment analysis, aspect extraction and clustering are
key tasks. For constraint-driven learning, a variety
of frameworks and models for sentiment analysis
have been studied extensively.

There have been many studies on clustering
aspect-related phrases. Most existing studies are

1621



based on context information. Some works also
encoded lexical similarity and synonyms as prior
knowledge. Carenini et al. (2005) proposed a
method that was based on several similarity met-
rics involving string similarity, synonyms, and lex-
ical distances defined with WordNet. Guo et al.
(2009) proposed a multi-level latent semantic as-
sociation model to capture expression-level and
context-level topic structure. Zhai et al. (2010)
proposed an EM-based semi-supervised learning
method to group aspect expressions into user-
specified aspects. They employed lexical knowl-
edge to provide a better initialization for EM. In
Zhai et al. (2011a), an EM-based unsupervised
version was proposed. The so-called L-EM model
first generated softly labeled data by grouping fea-
ture expressions that share words in common, and
then merged the groups by lexical similarity. Zhai
et al. (2011b) proposed a LDA-based method
that incorporates must-link and cannot-link con-
straints.

Another line of work aimed to extract and clus-
ter aspect words simultaneously using topic mod-
eling. Titov and McDonald (2008) proposed the
multi-grain topic models to discover global and
local aspects. Branavan et al. (2008) proposed
a method which first clustered the key-phrases
in Pros and Cons into some aspect categories
based on distributional similarity, then built a topic
model modeling the topics or aspects. Zhao et al.
(2010) proposed the MaxEnt-LDA (a Maximum
Entropy and LDA combination) hybrid model to
jointly discover both aspect words and aspect-
specific opinion words, which can leverage syn-
tactic features to separate aspects and sentiment
words. Mukherjee and Liu (2012) proposed a
semi-supervised topic model which used user-
provided seeds to discover aspects. Chen et al.
(2013) proposed a knowledge-based topic model
to incorporate must-link and cannot-link informa-
tion. Their model can adjust topic numbers auto-
matically by leveraging cannot-link.

Our work is also related to general constraint-
driven(or knowledge-driven) learning models.
Several general frameworks have been proposed to
fully utilize various prior knowledge in learning.
Constraint-driven learning (Chang et al., 2008)
(CODL) is an EM-like algorithm that incorpo-
rates per-instance constraints into semi-supervised
learning. Posterior regularization (Graca et al.,
2007) (PR) is a modified EM algorithm in which

the E-step is replaced by the projection of the
model posterior distribution onto the set of dis-
tributions that satisfy auxiliary expectation con-
straints. Generalized expectation criteria (Druck
et al., 2008) (GE) is a framework for incorporating
preferences about model expectations into param-
eter estimation objective functions. Liang et al.
(2009) developed a Bayesian decision-theoretic
framework to learn an exponential family model
using general measurements on the unlabeled data.
In this paper, we model our problem in the frame-
work of posterior regularization.

Many works promoted the performance of sen-
timent analysis by incorporating prior knowledge
as weak supervision. Li and Zhang (2009) in-
jected lexical prior knowledge to non-negative ma-
trix tri-factorization. Shen and Li (2011) further
extended the matrix factorization framework to
model dual supervision from both document and
word labels. Vikas Sindhwani (2008) proposed a
general framework for incorporating lexical infor-
mation as well as unlabeled data within standard
regularized least squares for sentiment prediction
tasks. Fang (2013)proposed a structural learning
model with a handful set of aspect signature terms
that are encoded as weak supervision to extract la-
tent sentiment explanations.

5 Conclusions

Aspect finding and clustering is an important task
for aspect-level sentiment analysis. In order to
cluster aspect-related phrases, this paper has ex-
plored a novel concept, sentiment distribution con-
sistency. We formalize the concept as soft con-
straint, integrate the constraint with a context-
based probabilistic model, and solve the problem
in the posterior regularization framework. The
proposed model is also designed to be robust with
both sufficient and insufficient observations. Ex-
periments show that our approach outperforms
state-of-the-art baselines consistently.

Acknowledgments

This work was partly supported by the following
grants from: the National Basic Research Program
(973 Program) under grant No.2012CB316301
and 2013CB329403, the National Science Foun-
dation of China project under grant No.61332007
and No. 61272227, and the Beijing Higher Educa-
tion Young Elite Teacher Project.

1622



References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.

Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.

S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Association for Computational
Linguistics (ACL).

Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In Proceedings of the 3rd International Conference
on Knowledge Capture, K-CAP ’05, pages 11–18,
New York, NY, USA. ACM.

Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with
constraints. In Proceedings of the 23rd National
Conference on Artificial Intelligence - Volume 3,
AAAI’08, pages 1513–1518. AAAI Press.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction.
In EMNLP, pages 1655–1667. ACL.

Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2008. Learning from labeled features using
generalized expectation criteria. In Proceedings of
the 31st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’08, pages 595–602, New York,
NY, USA. ACM.

Lei Fang, Minlie Huang, and Xiaoyan Zhu. 2013. Ex-
ploring weakly supervised latent sentiment expla-
nations for aspect-level review analysis. In Qi He,
Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev
Rastogi, editors, CIKM, pages 1057–1066. ACM.

Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, Ben
Taskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,
and Ben Taskar. 2007. Expectation maximization
and posterior constraints. In In Advances in NIPS,
pages 569–576.

Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ’09, pages
1087–1096, New York, NY, USA. ACM.

Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to senti-
ment classification with lexical prior knowledge. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing

of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages
244–252, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning from measurements in exponential fami-
lies. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML ’09,
pages 641–648, New York, NY, USA. ACM.

Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ’12, pages 339–348, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Chao Shen and Tao Li. 2011. A non-negative matrix
factorization based approach for active dual super-
vision from document and word labels. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’11, pages 949–
958, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Vikas Sindhwani and Prem Melville. 2008.
Document-word co-regularization for semi-
supervised sentiment analysis. In ICDM, pages
1025–1030. IEEE Computer Society.

Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International Conference on
World Wide Web, WWW ’08, pages 111–120, New
York, NY, USA. ACM.

Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics, COLING ’10, pages 1272–1280,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011a. Clustering product features for opinion min-
ing. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ’11, pages 347–354, New York, NY, USA.
ACM.

Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2011b. Constrained lda for grouping product fea-
tures in opinion mining. In Proceedings of the 15th
Pacific-Asia Conference on Advances in Knowl-
edge Discovery and Data Mining - Volume Part
I, PAKDD’11, pages 448–459, Berlin, Heidelberg.
Springer-Verlag.

Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, pages 56–
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

1623


