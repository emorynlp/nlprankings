



















































Bilingually-Guided Monolingual Dependency Grammar Induction


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063–1072,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Bilingually-Guided Monolingual Dependency Grammar Induction

Kai Liu†§, Yajuan Lü†, Wenbin Jiang†, Qun Liu‡†

†Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences

P.O. Box 2704, Beijing 100190, China
{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn

‡Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University

qliu@computing.dcu.ie
§University of Chinese Academy of Sciences

Abstract

This paper describes a novel strategy for
automatic induction of a monolingual de-
pendency grammar under the guidance
of bilingually-projected dependency. By
moderately leveraging the dependency in-
formation projected from the parsed coun-
terpart language, and simultaneously min-
ing the underlying syntactic structure of
the language considered, it effectively in-
tegrates the advantages of bilingual pro-
jection and unsupervised induction, so as
to induce a monolingual grammar much
better than previous models only using
bilingual projection or unsupervised in-
duction. We induced dependency gram-
mar for five different languages under the
guidance of dependency information pro-
jected from the parsed English translation,
experiments show that the bilingually-
guided method achieves a significant
improvement of 28.5% over the unsuper-
vised baseline and 3.0% over the best pro-
jection baseline on average.

1 Introduction

In past decades supervised methods achieved the
state-of-the-art in constituency parsing (Collins,
2003; Charniak and Johnson, 2005; Petrov et al.,
2006) and dependency parsing (McDonald et al.,
2005a; McDonald et al., 2006; Nivre et al., 2006;
Nivre et al., 2007; Koo and Collins, 2010). For
supervised models, the human-annotated corpora
on which models are trained, however, are expen-
sive and difficult to build. As alternative strate-
gies, methods which utilize raw texts have been in-
vestigated recently, including unsupervised meth-

ods which use only raw texts (Klein and Man-
ning, 2004; Smith and Eisner, 2005; William et
al., 2009), and semi-supervised methods (Koo et
al., 2008) which use both raw texts and annotat-
ed corpus. And there are a lot of efforts have also
been devoted to bilingual projection (Chen et al.,
2010), which resorts to bilingual text with one lan-
guage parsed, and projects the syntactic informa-
tion from the parsed language to the unparsed one
(Hwa et al., 2005; Ganchev et al., 2009).

In dependency grammar induction, unsuper-
vised methods achieve continuous improvements
in recent years (Klein and Manning, 2004; Smith
and Eisner, 2005; Bod, 2006; William et al., 2009;
Spitkovsky et al., 2010). Relying on a predefined
distributional assumption and iteratively maximiz-
ing an approximate indicator (entropy, likelihood,
etc.), an unsupervised model usually suffers from
two drawbacks, i.e., lower performance and high-
er computational cost. On the contrary, bilin-
gual projection (Hwa et al., 2005; Smith and Eis-
ner, 2009; Jiang and Liu, 2010) seems a promis-
ing substitute for languages with a
large amount of bilingual sentences and an exist-
ing parser of the counterpart language. By project-
ing syntactic structures directly (Hwa et al., 2005;
Smith and Eisner, 2009; Jiang and Liu, 2010)
across bilingual texts or indirectly across multi-
lingual texts (Snyder et al., 2009; McDonald et
al., 2011; Naseem et al., 2012), a better depen-
dency grammar can be easily induced, if syntactic
isomorphism is largely maintained between target
and source languages.

Unsupervised induction and bilingual projec-
tion run according to totally different principles,
the former mines the underlying structure of the
monolingual language, while the latter leverages
the syntactic knowledge of the parsed counter-

1063



Bilingual corpus Joint Optimization

Bilingually-guided

Parsing model

Unsupervised

objective

Projection

objective

Random

Treebank

Evolved

treebank

Target

sentences

Source

sentences projection

Figure 1: Training the bilingually-guided parsing model by iteration.

part language. Considering this, we propose a
novel strategy for automatically inducing a mono-
lingual dependency grammar under the guidance
of bilingually-projected dependency information,
which integrates the advantage of bilingual pro-
jection into the unsupervised framework. A
randomly-initialized monolingual treebank
evolves in a self-training iterative procedure, and
the grammar parameters are tuned to simultane-
ously maximize both the monolingual likelihood
and bilingually-projected likelihood of the evolv-
ing treebank. The monolingual likelihood is sim-
ilar to the optimization objectives of convention-
al unsupervised models, while the bilingually-
projected likelihood is the product of the projected
probabilities of dependency trees. By moderately
leveraging the dependency information projected
from the parsed counterpart language, and simul-
taneously mining the underlying syntactic struc-
ture of the language considered, we can automat-
ically induce a monolingual dependency grammar
which is much better than previous models only
using bilingual projection or unsupervised induc-
tion. In addition, since both likelihoods are fun-
damentally factorized into dependency edges (of
the hypothesis tree), the computational complexi-
ty approaches to unsupervised models, while with
much faster convergence. We evaluate the final
automatically-induced dependency parsing mod-
el on 5 languages. Experimental results show
that our method significantly outperforms previ-
ous work based on unsupervised method or indi-
rect/direct dependency projection, where we see
an average improvement of 28.5% over unsuper-
vised baseline on all languages, and the improve-
ments are 3.9%/3.0% over indirect/direct base-
lines. And our model achieves the most signif-
icant gains on Chinese, where the improvements
are 12.0%, 4.5% over indirect and direct projec-
tion baselines respectively.

In the rest of the paper, we first describe the un-
supervised dependency grammar induction frame-
work in section 2 (where the unsupervised op-
timization objective is given), and introduce the
bilingual projection method for dependency pars-
ing in section 3 (where the projected optimiza-
tion objective is given); Then in section 4 we
present the bilingually-guided induction strategy
for dependency grammar (where the two objec-
tives above are jointly optimized, as shown in Fig-
ure 1). After giving a brief introduction of previ-
ous work in section 5, we finally give the experi-
mental results in section 6 and conclude our work
in section 7.

2 Unsupervised Dependency Grammar
Induction

In this section, we introduce the unsupervised ob-
jective and the unsupervised training algorithm
which is used as the framework of our bilingually-
guided method. Unlike previous unsupervised
work (Klein and Manning, 2004; Smith and Eis-
ner, 2005; Bod, 2006), we select a self-training
approach (similar to hard EM method) to train
the unsupervised model. And the framework of
our unsupervised model builds a random treebank
on the monolingual corpus firstly for initialization
and trains a discriminative parsing model on it.
Then we use the parser to build an evolved tree-
bank with the 1-best result for the next iteration
run. In this way, the parser and treebank evolve in
an iterative way until convergence. Let’s introduce
the parsing objective firstly:

Define ei as the ith word in monolingual sen-
tence E; deij denotes the word pair dependency re-
lationship (ei → ej). Based on the features around
deij , we can calculate the probability Pr(y|deij )
that the word pair deij can form a dependency arc

1064



as:

Pr(y|deij ) =
1

Z(deij )
exp(

∑

n

λn · fn(deij , y)) (1)

where y is the category of the relationship of deij :
y = + means it is the probability that the word
pair deij can form a dependency arc and y = −
means the contrary. λn denotes the weight for fea-
ture function fn(deij , y), and the features we used
are presented in Table 1 (Section 6). Z(deij) is a
normalizing constant:

Z(deij ) =
∑

y

exp(
∑

n

λn · fn(deij , y)) (2)

Given a sentence E, parsing a dependency tree
is to find a dependency tree DE with maximum
probability PE :

PE = argmax
DE

∏

deij∈DE
Pr(+|deij ) (3)

2.1 Unsupervised Objective

We select a simple classifier objective function as
the unsupervised objective function which is in-
stinctively in accordance with the parsing objec-
tive:

θ(λ) =
∏

de∈DE

Pr(+|de)
∏

de∈D̃E

Pr(−|de) (4)

where E is the monolingual corpus and E ∈ E,
DE is the treebank that contains all DE in the cor-
pus, and D̃E denotes all other possible dependen-
cy arcs which do not exist in the treebank.

Maximizing the Formula (4) is equivalent to
maximizing the following formula:

θ1(λ) =
∑

de∈DE

logPr(+|de)

+
∑

de∈D̃E

logPr(−|de)
(5)

Since the size of edges between DE and D̃E is
disproportionate, we use an empirical value to re-
duce the impact of the huge number of negative
instances:

θ2(λ) =
∑

de∈DE

logPr(+|de)

+
|DE |
|D̃E |

∑

de∈D̃E

logPr(−|de)
(6)

where |x| is the size of x.

Algorithm 1 Training unsupervised model
1: build random DE
2: λ← train(DE , D̃E)
3: repeat
4: for each E ∈ E do ⊲ E step
5: DE ← parse(E,λ)
6: λ← train(DE , D̃E) ⊲ M step
7: until convergence

Bush held talk with Sharona

bushi yu juxingshalong huitanle

Figure 2: Projecting a Chinese dependency tree
to English side according to DPA. Solid arrows
are projected dependency arcs; dashed arrows are
missing dependency arcs.

2.2 Unsupervised Training Algorithm

Algorithm 1 outlines the unsupervised training in
its entirety, where the treebank DE and unsuper-
vised parsing model with λ are updated iteratively.

In line 1 we build a random treebank DE on
the monolingual corpus, and then train the parsing
model with it (line 2) through a training procedure
train(·, ·) which needs DE and D̃E as classifica-
tion instances. From line 3-7, we train the unsu-
pervised model in self training iterative procedure,
where line 4-5 are similar to the E-step in EM al-
gorithm where calculates objective instead of ex-
pectation of 1-best tree (line 5) which is parsed
according to the parsing objective (Formula 3) by
parsing process parse(·, ·), and update the tree
bank with the tree. Similar to M-step in EM, the
algorithm maximizes the whole treebank’s unsu-
pervised objective (Formula 6) through the train-
ing procedure (line 6).

3 Bilingual Projection of Dependency
Grammar

In this section, we introduce our projection objec-
tive and training algorithm which trains the model
with arc instances.

Because of the heterogeneity between dif-
ferent languages and word alignment errors, pro-
jection methods may contain a lot of noises. Take
Figure 2 as an example, following the Direct
Projection Algorithm (DPA) (Hwa et al., 2005)
(Section 5), the dependency relationships between
words can be directly projected from the source

1065



Algorithm 2 Training projection model
1: DP , DN ← proj(F ,DF , A,E)
2: repeat ⊲ train(DP , DN )
3: ∇φ← grad(DP , DN , φ(λ))
4: λ← climb(φ,∇φ, λ)
5: until maximization

language to the target language. Therefore, we
can hardly obtain a treebank with complete trees
through direct projection. So we extract projected
discrete dependency arc instances instead of tree-
bank as training set for the projected grammar in-
duction model.

3.1 Projection Objective

Correspondingly, we select an objective which has
the same form with the unsupervised one:

φ(λ) =
∑

de∈DP
log Pr(+|de)

+
∑

de∈DN
logPr(−|de)

(7)

where DP is the positive dependency arc instance
set, which is obtained by direct projection methods
(Hwa et al., 2005; Jiang and Liu, 2010) and DN is
the negative one.

3.2 Projection Algorithm

Basically, the training procedure in line 2,7 of Al-
gorithm 1 can be divided into smaller iterative
steps, and Algorithm 2 outlines the training step
of projection model with instances. F in Algo-
rithm 2 is source sentences in bilingual corpus,
and A is the alignments. Function grad(·, ·, ·)
gives the gradient (∇φ) and the objective is op-
timized with a generic optimization step (such as
an LBFGS iteration (Zhu et al., 1997)) in the sub-
routine climb(·, ·, ·).

4 Bilingually-Guided Dependency
Grammar Induction

This section presents our bilingually-guided gram-
mar induction model, which incorporates unsuper-
vised framework and bilingual projection model
through a joint approach.

According to following observation: unsuper-
vised induction model mines underlying syntactic
structure of the monolingual language, however, it
is hard to find good grammar induction in the ex-
ponential parsing space; bilingual projection ob-
tains relatively reliable syntactic knowledge of the

parsed counterpart, but it possibly contains a lot
of noises (e.g. Figure 2). We believe that unsu-
pervised model and projection model can comple-
ment each other and a joint model which takes bet-
ter use of both unsupervised parse trees and pro-
jected dependency arcs can give us a better parser.

Based on the idea, we propose a nov-
el strategy for training monolingual grammar in-
duction model with the guidance of unsuper-
vised and bilingually-projected dependency infor-
mation. Figure 1 outlines our bilingual-guided
grammar induction process in its entirety. In our
method, we select compatible objectives for unsu-
pervised and projection models, in order to they
can share the same grammar parameters. Then
we incorporate projection model into our iterative
unsupervised framework, and jointly optimize un-
supervised and projection objectives with evolv-
ing treebank and constant projection information
respectively. In this way, our bilingually-guided
model’s parameters are tuned to simultaneous-
ly maximizing both monolingual likelihood and
bilingually-projected likelihood by 4 steps:

1. Randomly build treebank on target sentences
for initialization, and get the projected arc in-
stances through projection from bitext.

2. Train the bilingually-guided grammar induc-
tion model by multi-objective optimization
method with unsupervised objective and pro-
jection objective on treebank and projected
arc instances respectively.

3. Use the parsing model to build new treebank
on target language for next iteration.

4. Repeat steps 1, 2 and 3 until convergence.

The unsupervised objective is optimized by the
loop—”tree bank→optimized model→new tree
bank”. The treebank is evolved for runs. The
unsupervised model gets projection constraint im-
plicitly from those parse trees which contain in-
formation from projection part. The projection ob-
jective is optimized by the circulation—”projected
instances→optimized model”, these projected in-
stances will not change once we get them.

The iterative procedure proposed here is not a
co-training algorithm (Sarkar, 2001; Hwa et al.,
2003), because the input of the projection objec-
tive is static.

1066



4.1 Joint Objective

For multi-objective optimization method, we em-
ploy the classical weighted-sum approach which
just calculates the weighted linear sum of the ob-
jectives:

OBJ =
∑

m

weightmobjm (8)

We combine the unsupervised objective (For-
mula (6)) and projection objective (Formula (7))
together through the weighted-sum approach in
Formula (8):

ℓ(λ) = αθ2(λ) + (1− α)φ(λ) (9)

where ℓ(λ) is our weight-sum objective. And α
is a mixing coefficient which reflects the relative
confidence between the unsupervised and projec-
tion objectives. Equally, α and (1−α) can be seen
as the weights in Formula (8). In that case, we can
use a single parameter α to control both weights
for different objective functions. When α = 1 it
is the unsupervised objective function in Formula
(6). Contrary, if α = 0, it is the projection objec-
tive function (Formula (7)) for projected instances.

With this approach, we can optimize the mixed
parsing model by maximizing the objective in For-
mula (9). Though the function (Formula (9)) is
an interpolation function, we use it for training
instead of parsing. In the parsing procedure, our
method calculates the probability of a dependency
arc according to the Formula (2), while the inter-
polating method calculates it by:

Pr(y|deij) =αPr1(y|deij )
+ (1− α)Pr2(y|deij )

(10)

where Pr1(y|deij ) and Pr2(y|deij ) are the proba-
bilities provided by different models.

4.2 Training Algorithm

We optimize the objective (Formula (9)) via a
gradient-based search algorithm. And the gradi-
ent with respect to λk takes the form:

∇ℓ(λk) = α
∂θ2(λ)

∂λk
+ (1− α)∂φ(λ)

∂λk
(11)

Algorithm 3 outlines our joint training proce-
dure, which tunes the grammar parameter λ simul-
taneously maximize both unsupervised objective

Algorithm 3 Training joint model
1: DP , DN ← proj(F,DF , A,E)
2: build random DE
3: λ← train(DP , DN )
4: repeat
5: for each E ∈ E do ⊲ E step
6: DE ← parse(E,λ)
7: ∇ℓ(λ)← grad(DE, D̃E , DP , DN , ℓ(λ))
8: λ←climb(ℓ(λ),∇ℓ(λ), λ) ⊲ M step
9: until convergence

and projection objective. And it incorporates un-
supervised framework and projection model algo-
rithm together. It is grounded on the work which
uses features in the unsupervised model (Berg-
Kirkpatrick et al., 2010).

In line 1, 2 we get projected dependency in-
stances from source side according to projec-
tion methods and build a random treebank (step
1). Then we train an initial model with projection
instances in line 3. From line 4-9, the objective is
optimized with a generic optimization step in the
subroutine climb(·, ·, ·, ·, ·). For each sentence we
parse its dependency tree, and update the tree into
the treebank (step 3). Then we calculate the gra-
dient and optimize the joint objective according to
the evolved treebank and projected instances (step
2). Lines 5-6 are equivalent to the E-step of the
EM algorithm, and lines 7-8 are equivalent to the
M-step.

5 Related work

The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) which
is based on POS tags. And DMV learns the gram-
mar via inside-outside re-estimation (Baker, 1979)
without any smoothing, while Spitkovsky et al.
(2010) utilizes smoothing and learning strategy
during grammar learning and William et al. (2009)
improves DMV with richer context.

The dependency projection method DPA (H-
wa et al., 2005) based on Direct Correspondence
Assumption (Hwa et al., 2002) can be described
as: if there is a pair of source words with a de-
pendency relationship, the corresponding aligned
words in target sentence can be considered as hav-
ing the same dependency relationship equivalent-
ly (e.g. Figure 2). The Word Pair Classification
(WPC) method (Jiang and Liu, 2010) modifies the
DPA method and makes it more robust. Smith
and Eisner (2009) propose an adaptation method
founded on quasi-synchronous grammar features

1067



Type Feature Template

Unigram wordi posi wordi ◦ posi
wordj posj wordj ◦ posj

Bigram wordi ◦ posj wordj ◦ posi posi ◦ posj
wordi ◦ wordj wordi ◦ posi ◦ wordj wordi ◦ wordj ◦ posj
wordi ◦ posi ◦ posj posi ◦ wordj ◦ posj
wordi ◦ posi ◦ wordj ◦ posj

Surrounding posi−1 ◦ posi ◦ posj posi ◦ posi+1 ◦ posj posi ◦ posj−1 ◦ posj
posi ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj−1 posi ◦ posi+1 ◦ posj+1
posi−1 ◦ posj−1 ◦ posj posi+1 ◦ posj ◦ posj+1 posi−1 ◦ posi ◦ posj+1
posi ◦ posi+1 ◦ posj−1 posi−1 ◦ posj ◦ posj+1 posi+1 ◦ posj−1 ◦ posj
posi−1 ◦ posi ◦ posj−1 ◦ posj posi ◦ posi+1 ◦ posj ◦ posj+1
posi ◦ posi+1 ◦ posj−1 ◦ posj posi−1 ◦ posi ◦ posj ◦ posj+1

Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj
is the child word, similar to ”pos”. ”+1” denotes the preceding token of the sentence, similar to ”-1”.

for dependency projection and annotation, which
requires a small set of dependency annotated cor-
pus of target language.

Similarly, using indirect information from mul-
tilingual (Cohen et al., 2011; Täckström et al.,
2012) is an effective way to improve unsupervised
parsing. (Zeman and Resnik, 2008; McDonald et
al., 2011; Søgaard, 2011) employ non-lexicalized
parser trained on other languages to process a
target language. McDonald et al. (2011) adapts
their multi-source parser according to DCA, while
Naseem et al. (2012) selects a selective sharing
model to make better use of grammar information
in multi-sources.

Due to similar reasons, many works are devoted
to POS projection (Yarowsky et al., 2001; Shen et
al., 2007; Naseem et al., 2009), and they also suf-
fer from similar problems. Some seek for unsu-
pervised methods, e.g. Naseem et al. (2009), and
some further improve the projection by a graph-
based projection (Das and Petrov, 2011).

Our model differs from the approaches above
in its emphasis on utilizing information from both
sides of bilingual corpus in an unsupervised train-
ing framework, while most of the work above only
utilize the information from a single side.

6 Experiments

In this section, we evaluate the performance of the
MST dependency parser (McDonald et al., 2005b)
which is trained by our bilingually-guided model
on 5 languages. And the features used in our ex-
periments are summarized in Table 1.

6.1 Experiment Setup

Datasets and Evaluation Our experiments are
run on five different languages: Chinese(ch),
Danish(da), Dutch(nl), Portuguese(pt) and

Swedish(sv) (da, nl, pt and sv are free data sets
distributed for the 2006 CoNLL Shared Tasks
(Buchholz and Marsi, 2006)). For all languages,
we only use English-target parallel data: we take
the FBIS English-Chinese bitext as bilingual cor-
pus for English-Chinese dependency projection
which contains 239K sentence pairs with about
8.9M/6.9M words in English/Chinese, and for
other languages we use the readily available data
in the Europarl corpus. Then we run tests on the
Penn Chinese Treebank (CTB) and CoNLL-X test
sets.

English sentences are tagged by the implemen-
tations of the POS tagger of Collins (2002), which
is trained on WSJ. The source sentences are then
parsed by an implementation of 2nd-ordered MST
model of McDonald and Pereira (2006), which is
trained on dependency trees extracted from Penn
Treebank.

As the evaluation metric, we use parsing accu-
racy which is the percentage of the words which
have found their correct parents. We evaluate on
sentences with all length for our method.

Training Regime In experiments, we use the
projection method proposed by Jiang and Liu
(2010) to provide the projection instances. And
we train the projection part α = 0 first for initial-
ization, on which the whole model will be trained.
Availing of the initialization method, the model
can converge very fast (about 3 iterations is suffi-
cient) and the results are more stable than the ones
trained on random initialization.

Baselines We compare our method against
three kinds of different approaches: unsupervised
method (Klein and Manning, 2004); single-
source direct projection methods (Hwa et al.,
2005; Jiang and Liu, 2010); multi-source in-
direct projection methods with multi-sources (M-

1068



60.0

61.5

          

 

 

ch

50.3

51.2

          

 

 

da

59.5

60.5

          

ac
cu

ra
cy

%

 

nl

70.5

74.5

          

 

 

pt

61.5

65.0

 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

 

alpha

sv

Figure 3: The performance of our model with re-
spect to a series of ratio α

cDonald et al., 2011; Naseem et al., 2012).

6.2 Results

We test our method on CTB and CoNLL-X free
test data sets respectively, and the performance is
summarized in Table 2. Figure 3 presents the per-
formance with different α on different languages.

Compare against Unsupervised Baseline Ex-
perimental results show that our unsupervised
framework’s performance approaches to the DMV
method. And the bilingually-guided model can
promote the unsupervised method consisten-
cy over all languages. On the best results’ aver-
age of four comparable languages (da, nl, pt, sv),
the promotion gained by our model is 28.5% over
the baseline method (DMV) (Klein and Manning,
2004).

Compare against Projection Baselines For
all languages, the model consistent-
ly outperforms on direct projection baseline.
On the average of each language’s best result, our
model outperforms all kinds of baselines, yielding
3.0% gain over the single-source direct-projection
method (Jiang and Liu, 2010) and 3.9% gain over
the multi-source indirect-projection method (Mc-
Donald et al., 2011). On the average of all results
with different parameters, our method also gain-
s more than 2.0% improvements on all baselines.
Particularly, our model achieves the most signif-
icant gains on Chinese, where the improvements
are 4.5%/12.0% on direct/indirect projection base-

Accuracy%
Model ch da nl pt sv avg

DMV 42.5∗ 33.4 38.5 20.1 44.0 —.–
DPA 53.9 —.– —.– —.– —.– —.–
WPC 56.8 50.1 58.4 70.5 60.8 59.3
Transfer 49.3 49.5 53.9 75.8 63.6 58.4
Selective 51.2 —.– 55.9 73.5 61.5 —.–
unsuper 22.6 41.6 15.2 45.7 42.4 33.5
avg 61.0 50.7 59.9 72.0 63.1 61.3
max 61.3 51.1 60.1 74.2 64.6 62.3

Table 2: The directed dependency accuracy with
different parameter of our model and the base-
lines. The first section of the table (row 3-7)
shows the results of the baselines: a unsupervised
method baseline (Klein and Manning, 2004)(D-
MV); a single-source projection method baseline
(Hwa et al., 2005) (DPA) and its improve-
ment (Jiang and Liu, 2010)(WPC); two multi-
source baselines (McDonald et al., 2011)(Trans-
fer) and (Naseem et al., 2012)(Selective). The
second section of the table (row 8) presents the
result of our unsupervised framework (unsuper).
The third section gives the mean value (avg) and
maximum value (max) of our model with different
α in Figure 3.
*: The result is based on sentences with 10
words or less after the removal of punctuation, it
is an incomparable result.

lines.
The results in Figure 3 prove that our unsuper-

vised framework α = 1 can promote the grammar
induction if it has a good start (well initialization),
and it will be better once we incorporate the infor-
mation from the projection side (α = 0.9). And
the maximum points are not in α = 1, which im-
plies that projection information is still available
for the unsupervised framework even if we employ
the projection model as the initialization. So we
suggest that a greater parameter is a better choice
for our model. And there are some random factors
in our model which make performance curves with
more fluctuation. And there is just a little improve-
ment shown in da, in which the same situation is
observed by (McDonald et al., 2011).

6.3 Effects of the Size of Training Corpus

To investigate how the size of the training corpus
influences the result, we train the model on ex-
tracted bilingual corpus with varying sizes: 10K,
50K, 100K, 150K and 200K sentences pairs.

As shown in Figure 4, our approach continu-

1069



 53

 54

 55

 56

 57

 58

 59

 60

 61

 62

 63

10K 50K 100K 150K 200K

ac
cu

ra
cy

%

size of training set

our model

baseline

Figure 4: Performance on varying sizes (average
of 5 languages, α = 0.9)

 51

 52

 53

 54

 55

 56

 57

 58

 59

 60

 61

 62

 63

 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35

ac
cu

ra
cy

%

noise rate

our model

baseline

Figure 5: Performance on different projection
quality (average of 5 languages, α = 0.9). The
noise rate is the percentage of the projected in-
stances being messed up.

ously outperforms the baseline with the increasing
size of training corpus. It is especially noteworthy
that the more training data is utilized the more su-
periority our model enjoys. That is, because our
method not only utilizes the projection informa-
tion but also avails itself of the monolingual cor-
pus.

6.4 Effect of Projection Quality

The projection quality can be influenced by the
quality of the source parsing, alignments, projec-
tion methods, corpus quality and many other fac-
tors. In order to detect the effects of varying pro-
jection qualities on our approach, we simulate the
complex projection procedure by messing up the
projected instances randomly with different noise
rates. The curves in Figure 5 show the perfor-
mance of WPC baseline and our bilingual-guided
method. For different noise rates, our model’s re-
sults consistently outperform the baselines. When
the noise rate is greater than 0.2, our improvement

49.5
...

54.6

...

58.2

58.6

59.0

59.4

59.8

60.2

0 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3

ac
cu

ra
cy

%

alpha

our model

baseline(58.5)

Figure 6: The performance curve of our model
(random initialization) on Chinese, with respect to
a series of ratio α. The baseline is the result of
WPC model.

increases with the growth of the noise rate. The re-
sult suggests that our method can solve some prob-
lems which are caused by projection noise.

6.5 Performance on Random Initialization

We test our model with random initialization on
different α. The curve in Figure 6 shows the per-
formance of our model on Chinese.

The results seem supporting our unsupervised
optimization method when α is in the range of
(0, 0.1). It implies that the unsupervised structure
information is useful, but it seems creating a nega-
tive effect on the model when α is greater than 0.1.
Because the unsupervised part can gain constraints
from the projection part. But with the increase of
α, the strength of constraint dwindles, and the
unsupervised part will gradually lose control. And
bad unsupervised part pulls the full model down.

7 Conclusion and Future Work

This paper presents a bilingually-guided strate-
gy for automatic dependency grammar induction,
which adopts an unsupervised skeleton and lever-
ages the bilingually-projected dependency infor-
mation during optimization. By simultaneous-
ly maximizing the monolingual likelihood and
bilingually-projected likelihood in the EM proce-
dure, it effectively integrates the advantages of
bilingual projection and unsupervised induction.
Experiments on 5 languages show that the novel
strategy significantly outperforms previous unsu-
pervised or bilingually-projected models.
Since its computational complexity approaches to
the skeleton unsupervised model (with much few-
er iterations), and the bilingual text aligned to

1070



resource-rich languages is easy to obtain, such a
hybrid method seems to be a better choice for au-
tomatic grammar induction. It also indicates that
the combination of bilingual constraint and unsu-
pervised methodology has a promising prospect
for grammar induction. In the future work we will
investigate such kind of strategies, such as bilin-
gually unsupervised induction.

Acknowledgments

The authors were supported by National
Natural Science Foundation of China, Con-
tracts 61202216, 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03), Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences (No. KGZD-EW-501).
Qun Liu’s work is partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. We
would like to thank the anonymous reviewers for
their insightful comments and those who helped
to modify the paper.

References

H. Alshawi. 1996. Head automata for speech transla-
tion. In Proc. of ICSLP.

James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65:S132.

T. Berg-Kirkpatrick, A. Bouchard-Côté, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In HLT: NAACL, pages 582–590.

Rens Bod. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proc. of the 21st ICCL and the
44th ACL, pages 865–872.

S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proc. of the
2002 Conference on EMNLP. Proc. CoNLL.

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proc. of the 43rd ACL, pages 173–180,
Ann Arbor, Michigan, June.

W. Chen, J. Kazama, and K. Torisawa. 2010. Bi-
text dependency parsing with bilingual subtree con-
straints. In Proc. of ACL, pages 21–29.

S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of the Conference on
EMNLP, pages 50–61.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conference on EMNLP, pages 1–8, July.

Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics.

D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.

K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of IJCNLP of the AFNLP: Vol-
ume 1-Volume 1, pages 369–377.

R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proc. of ACL, pages 392–399.

R. Hwa, M. Osborne, A. Sarkar, and M. Steedman.
2003. Corrected co-training for statistical parsers.
In ICML-03 Workshop on the Continuum from La-
beled to Unlabeled Data in Machine Learning and
Data Mining, Washington DC.

R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311–325.

W. Jiang and Q. Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proc. of ACL, pages 12–20.

D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependen-
cy and constituency. In Proc. of ACL, page 478.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of the 48th ACL,
pages 1–11, July.

T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. pages 595–
603.

R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of the 11th Conf. of EACL.

R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91–98.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.
2005b. Non-projective dependency parsing using s-
panning tree algorithms. In Proc. of EMNLP, pages
523–530.

R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of CoNLL, pages 216–
220.

1071



R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proc. of EMNLP, pages 62–72. ACL.

T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. Journal of Artificial Intelli-
gence Research, 36(1):341–385.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proc. of the 50th ACL, pages 629–637,
July.

J. Nivre, J. Hall, J. Nilsson, G. Eryig̃it, and S. Mari-
nov. 2006. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
CoNLL, pages 221–225.

J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kübler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95–135.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st ICCL
& 44th ACL, pages 433–440, July.

A. Sarkar. 2001. Applying co-training methods to sta-
tistical parsing. In Proc. of NAACL, pages 1–8.

L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In Annual
Meeting-, volume 45, page 760.

N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354–362.

D.A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Proc. of EMNLP: Volume 2-Volume
2, pages 822–831.

B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of IJCNLP of the AFNLP: Volume 1-Volume 1, pages
73–81.

Anders Søgaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
of the 49th ACL: HLT, pages 682–686.

Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
“less is more” in unsupervised dependency parsing.
In HLT: NAACL, pages 751–759, June.

O. Täckström, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.

William, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with rich-
er contexts and smoothing. In Proc. of NAACL,
pages 101–109.

D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. of HLT,
pages 1–8.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proc. of the IJCNLP-08. Proc. CoNLL.

Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained opti-
mization. ACM Transactions on Mathematical Soft-
ware (TOMS), 23(4):550–560.

1072


