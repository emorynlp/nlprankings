427

Coling 2010: Poster Volume, pages 427–435,

Beijing, August 2010

Towards Automated Related Work Summarization

Cong Duy Vu Hoang and Min-Yen Kan

Department of Computer Science

School of Computing

National University of Singapore

{hcdvu,kanmy}@comp.nus.edu.sg

Abstract

We introduce the novel problem of auto-
matic related work summarization. Given
multiple articles (e.g., conference/journal
papers) as input, a related work sum-
marization system creates a topic-biased
summary of related work speciﬁc to the
target paper. Our prototype Related Work
Summarization system, ReWoS, takes in
set of keywords arranged in a hierarchical
fashion that describes a target paper’s top-
ics, to drive the creation of an extractive
summary using two different strategies for
locating appropriate sentences for general
topics as well as detailed ones. Our initial
results show an improvement over generic
multi-document summarization baselines
in a human evaluation.

1

Introduction

In many ﬁelds, a scholar needs to show an under-
standing of the context of his problem and relate
his work to prior community knowledge. A re-
lated work section is often the vehicle for this pur-
pose; it contextualizes the scholar’s contributions
and helps readers understand the critical aspects
of the previous works that current work addresses.
Creating such a summary requires the author to
position his own work within the contextual re-
search to showcase the advantages of his method.
We envision an NLP application that assists in
creating a related work summary. We propose this
related work summarization task as a challenge
to the automatic summarization community.
In
its full form, it is a topic-biased, multi-document

summarization problem that takes as input a tar-
get scientiﬁc document for which a related work
section needs to be drafted. The output goal is to
create a related work section that ﬁnds the relevant
related works and contextually describes them in
relationship to the scientiﬁc document at hand.

We dissect the full challenge as bringing to-
gether work of disparate interests; 1) in ﬁnding
relevant documents; 2) in identifying the salient
aspects of these documents in relation to the cur-
rent work worth summarizing; and 3) in generat-
ing the ﬁnal topic-biased summary. While it is
clear that current NLP technology does not let us
build a complete solution for this task, we believe
that tackling the individual components will help
bring us towards an eventual solution.

In fact, existing works in the NLP and rec-
ommendation systems communities have already
begun work that ﬁts towards the completion of
the ﬁrst two tasks. Citation prediction (Nallapati
et al., 2008) is a growing research area that has
aimed both at predicting citation growth over time
within a community and at individual paper cita-
tion patterns. This year, an automatic keyphrase
extraction task from scientiﬁc articles was ﬁrst
ﬁelded in SemEval-2, partially addressing Task
11. Also, automatic survey generation (Moham-
mad et al., 2009) is becoming a growing ﬁeld
within the summarization community. However,
to date, we have not yet seen any work that exam-
ines topic-biased summarization of multiple sci-
entiﬁc articles. For these reasons, we focus on
Task 3 here – the creation of a related work sec-
tion, given a structured input of the topics for sum-
mary.. The remaining contributions of our paper

1http://semeval2.fbk.eu/semeval2.php

428

consists of work towards this goal:
• We conduct a study of the argumentative pat-
terns used in related work sections, to describe the
plausible summarization tactics for their creation
in Section 3.
• We describe our approach to generate an extrac-
tive related work summary given an input topic hi-
erarchy tree, using two separate strategies to dif-
ferentiate between summarizing shallow internal
nodes from deep detailed leaf nodes of the topic
tree in Section 4.

2 Related Work

Fully automated related work summarization is
signiﬁcantly different from traditional summa-
rization. While there are no existing studies on
this speciﬁc problem, there are closely related en-
deavors. The iOPENER2 project works towards
automated creation of technical surveys, given a
research topic (Mohammad et al., 2009). Stan-
dard generic multi-document summarization al-
gorithms were applied to generate technical sur-
veys. They showed that citation information was
effective in the generation process. This was also
validated earlier in (Nakov et al., 2004), which
showed that the citing sentences in other papers
can give a useful description of a target work.

Other studies focus mainly on single-document
scientiﬁc article summarization. The pioneers of
automated summarization (Luhn, 1958; Baxen-
dale, 1958; Edmundson, 1969) had envisioned
their approaches being used for the automatic cre-
ation of scientiﬁc summaries. They examined
various features speciﬁc to scientiﬁc texts (e.g.,
frequency-based, sentence position, or rhetorical
clues features) which were proven effective for
domain-speciﬁc summarization tasks.

Further, Mei and Zhai (2008) and Qazvinian
and Radev (2008) utilized citation information in
creating summaries for a single scientiﬁc article in
computational linguistics domain. Also, Schwartz
and Hearst (2006) also utilized the citation sen-
tences to summarize the key concepts and entities
in bioscience texts, and argued that citation sen-
tences may contain informative contributions of a
paper that complement its original abstract.

2http://clair.si.umich.edu/clair/iopener/

These works all center on the role of citations
and their contexts in creating a summary, using ci-
tation information to rank content for extraction.
However, they did not study the rhetorical struc-
ture of the intended summaries, targeting more on
deriving useful content. For working along this
vein, we turn to studies on the rhetorical structure
of scientiﬁc articles. Perhaps the most relevant is
work by (Teufel, 1999; Teufel and Moens, 2002)
who deﬁned and studied argumentative zoning of
texts, especially ones in computational linguistics.
While they studied the structure of an entire arti-
cle, it is clear from their studies that a related work
section would contain general background knowl-
edge (BACKGROUND zone) as well as speciﬁc in-
formation credited to others (OTHER and BASIS
zones). This vein of work has been followed by
many, including Teufel et al. (2009; Angrosh et
al. (2010).

3 Structure of Related Work Section

We ﬁrst extend the work on rhetorical analysis,
concentrating on related work sections. By study-
ing examples in detail, we gain insight on how to
approach related work summarization. We focus
on a concrete related work example for illustra-
tion, an excerpt of which is shown in Figure 1a.
Focusing on the argumentative progression of the
text, we note the ﬂow through different topics is
hierarchical and can be represented as a topic tree
as in Figure 1b.

This summary provides background knowledge
for a paper on text classiﬁcation, which is the root
of the topic tree (node 1; lines 1–5). Two top-
ics (“feature selection” and “machine learning”)
are then presented in parallel (nodes 2 & 3; lines
5–8 & 9–15), where speciﬁc details on relevant
works are selected to describe two topics. These
two topics are implicitly understood as subtopics
of a more general topic, namely “mono-lingual
text classiﬁcation” (node 4; lines 16–17). The au-
thors use the monolingual topic to contrast it with
the subsequent subtopic “multi-lingual text classi-
ﬁcation” (node 5; lines 18–21). This topic is de-
scribed by elaborating its details through two sub-
topics: “bilingual text classiﬁcation” and “cross-
lingual text classiﬁcation” (nodes 6 & 7; lines 22–
25 & 25–39) where again, various example works

429

line
1
2

9

16

21

line
22
23

32

35

40

42

2

1

2

3

4

5

6

7

1

4

contrast

5

parallel

3

parallel

6

7

Text classification (lines 1-5)

Feature selection (lines 5-8)

Machine learning (lines 9-15)

Mono-lingual text classification (lines 16-17)

Multi-lingual text classification (lines 18-21)

1

text;classification

4

5

monolingual;language

multi-language;multi-lingual;language

Bilingual text classification (lines 22-25)

2

3

6

7

Cross-lingual text classification (lines 25-39)

features;selection

learning;probabilistic

bilingual

cross-lingual

(a)

(b)

(c)

Figure 1: a) A related work section extracted from (Wu and Oard, 2008); b) An associated topic hierar-
chy tree of a); c) An associated topic tree, annotated with key words/phrases.

This

three

summary illustrates

are described and cited. The authors then con-
clude by contrasting their proposed approach with
the introduced relevant approaches (lines 40–42).
important
points. First, the topic tree is an essential input
to the summarization process. The topic tree can
be thought of as a high-level rhetorical structure
for which a process then attaches content. While
it is certainly non-trivial to build such a tree, mod-
iﬁcations to hierarchical topic modeling (M. et al.,
2004) or keyphrase extraction algorithms (Witten
et al., 1999) we believe can be used to induce a
suitable form. A resulting topic hierarchy from
such a process would provide an associated set
of key words or phrases that would describe the
node, as shown in Figure 1c.

Second, while summaries can be structured in
many ways, they can be viewed as moves along
the topic hierarchy tree.
In the example, nodes
2 and 3 are discussed before their parent, as the
parent node (node 4) serves as a useful contrast
to introduce its sibling (node 5). We ﬁnd variants
of depth-ﬁrst traversal common, but breadth-ﬁrst
traversals of nodes with multiple descendants are
more rare. They may be structured this way to
ease the reader’s burden on memory and atten-
tion. This is in line with other summary genres
where information is ordered by high-level logical
considerations that place macro level constraints
(Barzilay et al., 2002).

Third, there is a clear distinction between sen-
tences that describe a general topic and those that

describe work in detail. Generic topics are often
represented by background information, which is
not tied to a particular prior work. These include
deﬁnitions or descriptions of a topic’s purpose.
In contrast, detailed information forms the bulk
of the summary and often describes key related
work that is attributable to speciﬁc authors. Re-
cently, Jaidka et al. (2010) also present the begin-
nings of a corpus study of related work sections,
where they differentiate integrative and descrip-
tive strategies in presenting discourse work. We
see our differentiation between general and de-
tailed topics as a natural parallel to their notion
of integrative and descriptive strategies.

To introspect on these ﬁndings further, we cre-
ated a related work data set (called RWSData3),
which includes 20 articles from well-respected
venues in NLP and IR, namely SIGIR, ACL,
NAACL, EMNLP and COLING. We extracted
the related work sections directly from those re-
search articles as well as references the sections
cited. References to books and Ph.D. theses were
removed, as their verbosity would change the
problem drastically (Mihalcea and Ceylan, 2007).
Since we view each related work summary as a
topic-biased summary originating from a topic hi-
erarchy tree, annotation of such topical informa-
tion for our data set is necessary. Each article’s
data consists of the reference related work sum-
mary, the collection of the input research articles

3To be made available at http://wing.comp.nus.

edu.sg/downloads/rwsdata.

430

average
stdev
min
max

SbL−RW WbL−RW No−RAs SbL−RA WbL−RA TS TD
1.8
0.6
1
3

522.4
216.5
179
922

10.9
5.6
2
26

17.9
7.9
6
40

51739.6
26682.3

8580
112267

2386.0
1306.7

3.3
1.7
1
7

348
5549

Table 1: The demographics of RWSData. No, RW, RA, SbL, WbL, TS, and TD are labeled as
(N)umber (o)f, (R)elated (W)orks, (R)eferenced (A)rticles, (S)entence-(b)ased (L)ength of, (W)ord-
(b)ased (L)ength of, (T)ree (S)ize, and (T)ree (D)epth, respectively.

that were referenced and a manually-constructed
topic descriptions in a hierarchical fashion (topic
tree). More details on the demographics of RWS-
Data are shown in Table 1. RWSData summaries
average 17.9 sentences, 522 words in length, cit-
ing an average of 10.9 articles. While hierarchi-
cal, the topic trees are simple, averaging 3.3 topic
nodes in size and average depth of 1.8. Their sim-
plicity furthers our claim that automated methods
would be able to create such trees.

4 ReWoS: Paired General and Speciﬁc

Summarization

sentences
sentences

Pre-processor
Pre-processor

Specific Content Summarization
Specific Content Summarization

Agent-based rule
Agent-based rule

General Content Summarization
General Content Summarization

T
T

Topic relevance computation
Topic relevance computation

SCSum
SCSum

R
R

Context modeling
Context modeling

F
F

Subject-based rule
Subject-based rule

T
T
OR
OR

Weighting
Weighting

Ranking
Ranking

Re-ranking
Re-ranking

Verb-based rule
Verb-based rule

Citation-based rule
Citation-based rule

T
T

T
T

Topic relevance computation
Topic relevance computation

GCSum
GCSum

R
R

Ranking
Ranking

Post-processor
Post-processor

General content sentences
General content sentences

Specific content sentences
Specific content sentences

Generator
Generator

Related work 
Related work 

summary
summary

Figure 2: The ReWoS architecture. Decision
edges labeled as True, False and Relevant.

Inspired by the above observations, we propose
a novel strategy for related work summarization
with respect to a given topic tree. Note that while
the construction of the topic tree is central to the
process, we consider this outside the scope of the
current work (see §1); our investigation focuses

on how such input could be utilized to construct a
reasonable topic-biased related work summary.

We posit that sentences within a related work
section come about by means of two separate pro-
cesses – a process that gives general background
information and another that describes speciﬁc au-
thor contributions. A key realization in our work
is that these processes are easily mapped to the
topic tree topologically: general content is de-
scribed in tree-internal nodes, whereas leaf nodes
contribute detailed speciﬁcs.
In our approach,
these two processes are independent, and com-
bined to construct the ﬁnal summary.

We have implemented our idea in ReWoS
(Related Work Summarizer), whose general ar-
chitecture is shown in Figure 2. ReWoS is a
largely heuristic system, featuring both a General
Content Summarization (GCSum) and a Speciﬁc
Content Summarization (SCSum) modules, pre-
ﬁxed by preprocessing. A natural language tem-
plate generation system ﬁlls out the end of the
summary.

ReWoS ﬁrst applies a set of preprocessing steps
(shown in the top of Figure 2). Input sentences
(i.e., the set of sentences from each related/cited
article) ﬁrst removes sentences that are too short
(< 7 tokens) or too long (> 80 tokens), ones that
use future tense (possibly future work), and exam-
ple and navigation sentences. This last category
is ﬁltered out by checking for the presence of a
cue phrase among a lexical pattern database: e.g.,
“in the section”, “ﬁgure x shows”, “for instance”.
Lowercasing and stemming are also performed.

We then direct sentences to either GCSum or
SCSum based on whether it describes the author’s
own work or not, similar in spirit and execution to
(Teufel et al., 2009). If sentence contains indica-
tive pronouns or cue phrases (e.g., “we”, “this ap-

431

proach”), the sentence is deemed to describe own
work and is directed to SCSum; otherwise the sen-
tence is directed to the GCSum workﬂow.

4.1 (G)eneral (C)ontent (Sum)marization

GCSum extracts sentences containing useful
background information on the topics of the inter-
nal node in focus. Since general content sentences
do not speciﬁcally describe work done by the au-
thors, we only take sentences that do not have the
author-as-agent as input for GCSum.

indicative and informative.

We divide such general content sentences into
two groups:
Infor-
mative sentences give detail on a speciﬁc aspect
of the problem. They often give deﬁnitions, pur-
pose or application of the topic (“Text classiﬁca-
tion is a task that assigns a certain number of pre-
deﬁned labels for a given text.”). In contrast, in-
dicative sentences are simpler, inserted to make
the topic transition explicit and rhetorically sound
(“Many previous studies have studied monolin-
gual text classiﬁcation.”).

Indicative sentences can be easily generated by
templates, as the primary information that is trans-
mitted is the identity of the topic itself. Informa-
tive sentences, on the other hand, are better ex-
tracted from the source articles themselves, re-
quiring a speciﬁc strategy. As informative sen-
tences contain more content, our strategy with
GCSum is to attempt to locate informative sen-
tences to describe the internal nodes, failing which
GCSum falls back to using predeﬁned templates
to generate an indicative placeholder.

To implement GCSum’s informative extractor,
we use a set of heuristics in a decision cascade
to ﬁrst ﬁlter inappropriate sentences (as shown on
the RHS of Figure 2). Remaining candidates (if
any) are then ranked by relevance and the top n
are selected for the summary.

The heuristic cascade’s purpose is to ensure
sentences ﬁt the syntactic structure of commonly-
observed informative sentences. A useful sen-
tence should discuss the topic directly, so GCSum
ﬁrst checks the subject of each candidate sentence,
ﬁltering sentences whose subject do not contain at
least one topic keyword. We observed that back-
ground sentences often feature speciﬁc verbs or
citations. GCSum thus also checks whether stock

verb phrases (i.e., “based on”, “make use of” and
23 other patterns) are used as the main verb. Oth-
erwise, GCSum checks for the presence of at least
one citation – general sentences may list a set of
citations as examples.
If both the cue verb and
citation checks fail, the sentence is dropped.

S + scoreQ

S − scoreQR

GCSum’s topic relevance computation ranks
remaining sentences based on keyword content.
We state that the topic of an internal node is af-
fected by its surrounding nodes – ancestor, de-
scendants and siblings. Based on this idea, the
score of a sentence is computed in a discrimina-
tive way using the following linear combination:
scoreS → scoreQA
where scoreS is the ﬁnal relevance score, and
scoreQA
are the compo-
nent scores of the sentence S with respect to the
ancestor, current or other remaining nodes. We
give positive credit to a sentence that contains key-
words from an ancestor node, but penalize sen-
tences with keywords from other topics (as such
sentences would be better descriptors for those
other topics). Component relevance scores are
calculated using Term Frequency × Inverse Sen-
tence Frequency (TF×ISF) (Otterbacher et al.,
2005):

S , and scoreQR

S , scoreQ

(1)

S

S

scoreQ
S

rel(S, Q)

=

P Q′ rel(S, Q′)
= P w∈Q log(tf S

w + 1) × log(tf Q

w + 1) × isfw

N orm

(2)

where rel(S, Q) is the relevance of S with respect
to topic Q, N orm is a normalization factor of
w and tf Q
rel(S, Q) over all input sentences, tf S
w
are the term frequencies of token w within S or
sentences that discuss topic Q, respectively. isfw
is the inverse sentence frequency of w.

(S)peciﬁc (C)ontent (Sum)marization

4.2
SCSum aims to extract sentences that contain de-
tailed information about a speciﬁc author’s work
that is relevant to the input leaf node’s topic from
the set of sentences that exhibit the author-as-
agent. SCSum starts by computing the topic rel-
evance of each candidate sentence as shown in
Equation (3). This process is identical to the step
in GCSum, except that the term scoreQR
in Equa-
tion (1) is replaced by scoreQS
S , which is the rel-
evance of S with respect to its sibling nodes. We

S

432

S

(3)

S + scoreQ

S − scoreQS

hypothesize that given a leaf node, sibling node
topics may have an even more pronounced nega-
tive effect than other remaining nodes in the topic
tree.
scoreS → scoreQA
Context Modeling. We note that single sen-
tences occasionally do not contain enough con-
texts to clearly express the idea mentioned in orig-
inal articles. In fact, an agent-based sentence often
introduces a concept but pertinent details are of-
ten described later. Extracting just the agent-based
sentence may incompletely describe a concept and
lead to false inferences. Consider the example
in Figure 3. Here Sentences 0-5 are an contigu-
ous extract of a source article being summarized,
where Sentence 0 is an identiﬁed agent-based sen-
tence. Sentence 6 shows a related work section
sentence from a citing article that describes the
original article. It is clear that the citing descrip-
tion is composed of information taken not only
from the agent-based sentence but its context in
the following sentences as well. This observation

Weighting. The score of a candidate content
sentence is computed from topic relevance com-
putation (SCSum) that includes contributions for
keywords present in the current, ancestor and sib-
ling nodes. We observe that the presence of one or
more of current, ancestor and sibling nodes may
affect the ﬁnal score from the computation. Thus,
to partially address this, we add a new weighting
coefﬁcient for the score computed from the topic
relevance computation (SCSum) (Equation (3)) as
follows:

score∗S = wQA,Q,QS

S

× scoreS

(4)

S

where: wQA,Q,QS
is a weighting coefﬁcient that
takes on differing values based on the presence
of keywords in the sentence. Q, QA, and QS de-
note keywords from current, ancestor and sibling
nodes.
If the sentence contains keywords from
other sibling nodes, we assign a penalty of 0.1.
Otherwise, we assign a weight of 1.0, 0.5, or 0.25,
based on whether keywords are present from both
the ancestor node and current node, just the cur-
rent node or just the ancestor node.

To build the ﬁnal summary, ReWoS selects the
top scoring sentence and iteratively adds the next
most highly ranked sentence, until the n sentence
budget is reached. We use SimRank (Li et al.,
2008) to remove the next sentence to be added,
if it is too similar to the sentences already in the
summary.

4.3 Generation

Figure 3: A context modeling example.

motivates us to choose nearby sentences within a
contextual window after the agent-based sentence
to represent the topic. We set the contextual win-
dow to 5 and extract a maximum of 2 additional
sentences. These additions are chosen based on
their relevance scores to the topic, using Equa-
tion (3). Sentences with non-zero scores are then
added as contexts of the anchor agent-based sen-
tence, otherwise they are excluded. As a result,
some topics may contain only a single sentence,
but others may be described by additional contex-
tual sentences.

ReWoS generates its summaries by using depth-
ﬁrst traversal to order the topic nodes, as in RWS-
Data we observed this to be the most prevalent
discourse pattern. It calls GCSum and SCSum to
summarize individual nodes, distributing the total
sentence budget equally among nodes.

ReWoS post-processes sentences to improve
ﬂuency where possible. We ﬁrst replace agentive
forms with a citation to the articles (e.g., “we” →
“(Wu and Oard, 2008)”). ReWoS also replaces
found abbreviations with their corresponding long
forms, by connecting abbreviation with their ex-
pansions by utilizing dependency relation output
from the Stanford dependency parser.

433

System

LEAD
MEAD
ReWoS−WCM
ReWoS−CM

ROUGE-1

ROUGE Recall Scores
ROUGE-S4

ROUGE-2

ROUGE-SU4

Correctness

0.501
0.663
0.584
0.698

0.096
0.178
0.127
0.183

0.116
0.211
0.154
0.218

0.181
0.287
0.227
0.298

3.027
3.009
3.618
3.691

Human Evaluation Scores

Novelty
2.764
3.109
3.391
3.618

Fluency
3.082
2.591
3.391
2.955

Usefulness

2.745
2.700
3.609
3.573

Table 2: Evaluation results for ReWoS variants and baselines.

5 Evaluation

We wish to assess the quality of ReWoS, compar-
ing to state-of-the-art generic summarization sys-
tems. We ﬁrst detail our baseline systems used for
performance comparison, and deﬁned evaluation
measures speciﬁc to related work summary eval-
uation. In our evaluation, we use our manually-
compiled RWSData data set.

We benchmark ReWoS against two baseline
systems: LEAD and MEAD. The LEAD baseline
represents each of the cited article with an equal
number of sentences. The ﬁrst n sentences are
drawn from the article, meaning that the title and
abstract are usually extracted. The order of the ar-
ticle leads used in the resulting summary was de-
termined by the order of articles to be processed.
MEAD is a well-documented baseline extractive
multi-document summarizer, developed in (Radev
et al., 2004). MEAD offers a set of different fea-
tures that can be parameterized to create result-
ing summaries. We conducted an internal tun-
ing of MEAD to maximize its performance on
the RWSData. The optimal conﬁguation uses just
two tuned features of centroid and cosine similar-
ity. Note that the MEAD baseline does use the
topic tree keywords in computing cosine similar-
ity score. Our ReWoS system is the only sys-
tem that leverages the topic tree structure which
is central to our approach. In our experiments, we
used MEAD toolkit4 to produce the summaries for
LEAD and MEAD baseline systems.

Automatic evaluation was performed with
ROUGE (Lin, 2004), a widely used and rec-
ognized automated summarization evaluation
method. We employed a number of ROUGE vari-
ants, which have been proven to correlate with hu-
man judgments in multi-document summarization
(Lin, 2004). However, given the small size of our
summarization dataset, we can only draw notional

4http://www.summarization.com/mead/

evidence from such an evaluation; it is not possi-
ble to ﬁnd statistically signiﬁcant conclusion from
our evaluation.

To partially address this, we also conducted
a human evaluation to assess more ﬁne-grained
qualities of our system. We asked 11 human
judges to follow an evaluation guideline that we
prepared, to evaluate the summary quality, con-
sisting of the following evaluation measures:
• Correctness: Is the summary content actually
relevant to the hierarchical topics given?
• Novelty: Does the summary introduce novel in-
formation that is signiﬁcant in comparison with
the human created summary?
• Fluency: Does the summary’s exposition ﬂow
well, in terms of syntax as well as discourse?
• Usefulness: Is the summary useful in supporting
the researchers to quickly grasp the related works
given hierarchical topics?

Each judge was asked to grade the four sum-
maries according to the measures on a 5-point
scale of 1 (very poor) to 5 (very good). Sum-
maries 1 and 2 come from LEAD-based and
MEAD systems, respectively. Summaries 3 and
4 come from our proposed ReWoS systems, with-
out (ReWoS−WCM) and with (ReWoS−CM) the
context modeling in SCSum. All summarizers
were set to yield a summary with the same length
(1% of the original relevant articles, measured in
sentences). Due to limited time, only 10 out of 20
evaluation sets were assessed by the evaluators.
Each set was graded at least 3 times by 3 different
evaluators; evaluators did not know the identities
of the systems, which were randomized for each
set examined.

6 Results

ROUGE results are summarized in Table 2. Sur-
prisingly, the MEAD baseline system outperforms
both LEAD baseline and ReWoS–WCM (with-
out context modeling). Only ReWoS–CM (with

434

context modeling) is signiﬁcantly better than oth-
ers, in terms of all ROUGE variants. Here are
some possible reasons to explain this.
First,
ROUGE evaluation seems to work unreasonably
when dealing with verbose summaries, often pro-
duced by MEAD. Second, related work sum-
maries are multi-topic summaries of multi-article
references. This may cause miscalculation from
overlapping n-grams that occur across multiple
topics or references.

Since automatic evaluation with ROUGE does
not allow much introspection, we turn to our hu-
man evaluation. Results are also summarized in
Table 2. They show that both ReWoS–WCM
and ReWoS–CM perform signiﬁcantly better than
baselines in terms of correctness, novelty, and use-
fulness. This is because our system utilized fea-
tures developed speciﬁcally for related work sum-
marization. Also, our proposed systems compare
favorably with LEAD, showing that necessary in-
formation is not only located in titles or abstracts,
but also in relevant portions of the research article
body.

ReWoS–CM (with context modeling) per-
formed equivalent to ReWoS–WCM (without it)
in terms of correctness and usefulness. For nov-
elty, ReWoS–CM is better than ReWoS–WCM.
It proved that the proposed component of con-
text moding is useful in providing new informa-
tion that is necessary for the related work sum-
maries. For ﬂuency, only ReWoS–CM is bet-
ter than baseline systems. This is a negative re-
sult, but is not surprising because the summaries
from the ReWoS–CM which uses context model-
ing seems to be longer than others. It makes the
summaries quite hard to digest; some evaluators
told us that they preferred the shorter summaries.
A future extension is that using information fu-
sion techniques to fuse the contextual sentences
with its anchor agentive sentence.

A detailed error analysis of the results revealed
that there are three main types of errors produced
by our systems. The ﬁrst issue is in calculat-
ing topic relevance.
In the context of related
work summarization, our heuristics-based strate-
gies for sentence extraction cannot capture fully
this issue. Some sentences that have high relevant
scores to topics are not actually semantically rele-

vant to the topics. The second issue of anaphoric
expression is more addressable. Some extracted
sentences still contain anaphoric expression (e.g.,
“they”, “these”, “such”, . . . ), making ﬁnal gen-
erated summaries incoherent. The third issue is
paraphrasing, where substituted paraphrases re-
place the original words and phrases in the source
articles.

7 Conclusion and Future Work

According to the best of our knowledge, auto-
mated related work summarization has not been
studied before. In this paper, we have taken the
initial steps towards solving this problem, by di-
viding the task into general and speciﬁc summa-
rization processes. Our initial results show an im-
provement over generic multi-document summa-
rization baselines in human evaluation. However,
our work shows that there is much room for addi-
tional improvement, for which we have outlined a
few challenges.

A shortcoming of our current work is that we
assume that a topic hierarchy tree is given as in-
put. We feel that this is an acceptable limitation
because we feel existing techniques will be able to
create such input, and that the topic trees used in
this study were quite simple. We plan to validate
this by generating these topic trees automatically
in our future work.

Exploring related work summarization comes
at a timely moment, as scholars now have access
to a preponderous amount of scholarly literature.
Automated assistance in interpreting and organiz-
ing scholarly work will help build future applica-
tions for integration with digital libraries and ref-
erence management tools.

References

Angrosh, M. A., Stephen Craneﬁeld, and Nigel
Stanger. 2010. Context identiﬁcation of sentences
in related work sections using a conditional random
ﬁeld: towards intelligent digital libraries. In JCDL
’10: Proceedings of the 10th annual joint confer-
ence on Digital libraries, pages 293–302. ACM.

Barzilay, Regina, Noemie Elhadad, and Kathleen R.
McKeown.
Inferring strategies for sen-
tence ordering in multidocument news summariza-

2002.

435

tion. In Journal of Artiﬁcial Intelligence Research,
volume 17, pages 35–55.

Baxendale, P. B. 1958. Machine-made index for tech-
nical literature - an experiment. IBM Journal of Re-
search Development, 2(4):354–361.

Edmundson, H. P. 1969. New methods in automatic

extracting. Journal of the ACM, 16(2):264–285.

Jaidka, Kokil, Christopher S. G. Khoo, and Jin-Cheon
Na. 2010. Imitating human literature review writ-
ing: An approach to multi-document summariza-
tion. In ICADL, pages 116–119.

Li, Wenjie, Furu Wei, Qin Lu, and Yanxiang He. 2008.
PNR2: Ranking sentences with positive and nega-
tive reinforcement for query-oriented update sum-
marization.
In Proceedings of 22nd International
Conference on Computational Linguistics, pages
489–496, Manchester, UK, August.

Lin, Chin-Yew. 2004. Rouge: A package for au-
tomatic evaluation of summaries.
In Proceed-
ings of the ACL-04 Workshop Text Summarization
Branches Out, pages 74–81, Spain, July.

Luhn, H. P. 1958. The automatic creation of literature
IBM Journal of Research Development,

abstracts.
2(2):159–165.

M., Blei D., Grifﬁths T. L., Jordan M. I., and Tenen-
baum J. B. 2004. Hierarchical topic models and the
nested chinese restaurant process.
In Advances in
Neural Information Processing Systems (NIPS).

Mei, Qiaozhu and ChengXiang Zhai. 2008. Gen-
erating impact-based summaries for scientiﬁc lit-
erature.
In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 816–824, Columbus, Ohio, June.

Mihalcea, Rada and Hakan Ceylan. 2007. Explo-
rations in automatic book summarization.
In Pro-
ceedings of Empirical Methods in Natural Lan-
guage Processing - Conference on Natural Lan-
guage Learning (EMNLP-CoNLL), pages 380–389,
Prague, Czech Republic, June.

Mohammad, S., B. Dorr, M. Egan, A. Hassan,
P. Muthukrishan, V. Qazvinian, D. Radev, and
D. Zajic. 2009. Using citations to generate surveys
of scientiﬁc paradigms. In Proceedings of Human
Language Technologies - North American Associa-
tion for Computational Linguistics (HLT-NAACL),
pages 584–592, Boulder, Colorado, June.

Nakov, Preslav I., Ariel S. Schwartz, and Marti A.
Hearst. 2004. Citances: Citation sentences for se-
mantic analysis of bioscience text. In Workshop on
Search and Discovery in Bioinformatics.

Nallapati, R. M., A. Ahmed, E. P. Xing, and W. W.
Cohen. 2008. Joint latent topic models for text and
citations. In Proceeding of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
in Data and Data Mining, pages 542–550.

Otterbacher, Jahna, G¨unes¸ Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval.
In Proceedings of Hu-
man Language Technologies - Empirical Methods in
Natural Language Processing (HLT-EMNLP ’05),
pages 915–922. ACL.

Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entiﬁc paper summarization using citation summary
networks.
In Proceedings of International Con-
ference on Computational Linguistics (COLING),
pages 689–696, Manchester, UK, August.

Radev, Dragomir R., Hongyan Jing, Malgorzata Sty,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
& Management (IPM), 40(6):919–938.

Schwartz, Ariel S. and Marti Hearst. 2006. Summa-
rizing key concepts using citation sentences. In Pro-
ceedings of Natural language processing of biology
text (BioNLP ’06), pages 134–135. ACL.

Teufel, Simone and Marc Moens. 2002. Summariz-
ing scientiﬁc articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.

Teufel, Simone, Advaith Siddharthan, and Colin
Batchelor. 2009. Towards domain-independent ar-
gumentative zoning: Evidence from chemistry and
computational linguistics.
In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1493–1502, Singa-
pore, August. Association for Computational Lin-
guistics.

Teufel, Simone. 1999. Argumentative Zoning: Infor-
mation Extraction from Scientiﬁc Text. Ph.D. thesis,
University of Edinburgh.

Witten, Ian H., Gordon Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Kea:
Practical automatic keyphrase extraction.
In Pro-
ceedings of Digital Libraries 99 (DL’99), pages
254–255. ACM Press.

Wu, Yejun and Douglas W. Oard. 2008. Bilingual
topic aspect classiﬁcation with a few training exam-
ples.
In SIGIR ’08: Proceedings of the 31st an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 203–210, New York, NY, USA. ACM.

