










































Improving Statistical Machine Translation with Monolingual Collocation


Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825–833,
Uppsala, Sweden, 11-16 July 2010. c©2010 Association for Computational Linguistics

Improving Statistical Machine Translation with 

Monolingual Collocation 

 

Zhanyi Liu
1
, Haifeng Wang

2
, Hua Wu

2
, Sheng Li

1
 

1
Harbin Institute of Technology, Harbin, China 

2
Baidu.com Inc., Beijing, China 

zhanyiliu@gmail.com 

{wanghaifeng, wu_hua}@baidu.com 

lisheng@hit.edu.cn 

 

Abstract 

This paper proposes to use monolingual 

collocations to improve Statistical Ma-

chine Translation (SMT). We make use 

of the collocation probabilities, which are 

estimated from monolingual corpora, in 

two aspects, namely improving word 

alignment for various kinds of SMT sys-

tems and improving phrase table for 

phrase-based SMT. The experimental re-

sults show that our method improves the 

performance of both word alignment and 

translation quality significantly. As com-

pared to baseline systems, we achieve ab-

solute improvements of 2.40 BLEU score 

on a phrase-based SMT system and 1.76 

BLEU score on a parsing-based SMT 

system. 

1 Introduction 

Statistical bilingual word alignment (Brown et al. 

1993) is the base of most SMT systems. As com-

pared to single-word alignment, multi-word 

alignment is more difficult to be identified. Al-

though many methods were proposed to improve 

the quality of word alignments (Wu, 1997; Och 

and Ney, 2000; Marcu and Wong, 2002; Cherry 

and Lin, 2003; Liu et al., 2005; Huang, 2009), 

the correlation of the words in multi-word 

alignments is not fully considered. 

In phrase-based SMT (Koehn et al., 2003), the 

phrase boundary is usually determined based on 

the bi-directional word alignments. But as far as 

we know, few previous studies exploit the collo-

cation relations of the words in a phrase. Some 

                                                 
This work was partially done at Toshiba (China) Research 

and Development Center. 

researches used soft syntactic constraints to pre-

dict whether source phrase can be translated to-

gether (Marton and Resnik, 2008; Xiong et al., 

2009). However, the constraints were learned 

from the parsed corpus, which is not available 

for many languages.  

In this paper, we propose to use monolingual 

collocations to improve SMT. We first identify 

potentially collocated words and estimate collo-

cation probabilities from monolingual corpora 

using a Monolingual Word Alignment (MWA) 

method (Liu et al., 2009), which does not need 

any additional resource or linguistic preprocess-

ing, and which outperforms previous methods on 

the same experimental data. Then the collocation 

information is employed to improve Bilingual 

Word Alignment (BWA) for various kinds of 

SMT systems and to improve phrase table for 

phrase-based SMT. 

To improve BWA, we re-estimate the align-

ment probabilities by using the collocation prob-

abilities of words in the same cept. A cept is the 

set of source words that are connected to the 

same target word (Brown et al., 1993). An 

alignment between a source multi-word cept and 

a target word is a many-to-one multi-word 

alignment. 

To improve phrase table, we calculate phrase 

collocation probabilities based on word colloca-

tion probabilities. Then the phrase collocation 

probabilities are used as additional features in 

phrase-based SMT systems. 

The evaluation results show that the proposed 

method in this paper significantly improves mul-

ti-word alignment, achieving an absolute error 

rate reduction of 29%. The alignment improve-

ment results in an improvement of 2.16 BLEU 

score on phrase-based SMT system and an im-

provement of 1.76 BLEU score on parsing-based 

SMT system. If we use phrase collocation proba-

bilities as additional features, the phrase-based 

825



SMT performance is further improved by 0.24 

BLEU score. 

The paper is organized as follows: In section 2, 

we introduce the collocation model based on the 

MWA method. In section 3 and 4, we show how 

to improve the BWA method and the phrase ta-

ble using collocation models respectively. We 

describe the experimental results in section 5, 6 

and 7. Lastly, we conclude in section 8. 

2 Collocation Model 

Collocation is generally defined as a group of 

words that occur together more often than by 

chance (McKeown and Radev, 2000). A colloca-

tion is composed of two words occurring as ei-

ther a consecutive word sequence or an inter-

rupted word sequence in sentences, such as "by 

accident" or "take ... advice". In this paper, we 

use the MWA method (Liu et al., 2009) for col-

location extraction. This method adapts the bi-

lingual word alignment algorithm to monolingual 

scenario to extract collocations only from mono-

lingual corpora. And the experimental results in 

(Liu et al., 2009) showed that this method 

achieved higher precision and recall than pre-

vious methods on the same experimental data. 

2.1 Monolingual word alignment 

The monolingual corpus is first replicated to 

generate a parallel corpus, where each sentence 

pair consists of two identical sentences in the 

same language. Then the monolingual word 

alignment algorithm is employed to align the 

potentially collocated words in the monolingual 

sentences. 

According to Liu et al. (2009), we employ the 

MWA Model 3 (corresponding to IBM Model 3) 

to calculate the probability of the monolingual 

word alignment sequence, as shown in Eq. (1). 

 







l

j
jaj

l

i
ii

lajdwwt

wnSASp

j
1

1
3 ModelMWA 

),|()|(

)|()|,( 

    (1) 

Where lwS 1  is a monolingual sentence, i  

denotes the number of words that are aligned 

with iw . Since a word never collocates with itself, 

the alignment set is denoted as 

}&],1[|),{( ialiaiA ii  . Three kinds of prob-

abilities are involved in this model: word collo-

cation probability )|(
jaj

wwt , position colloca-

tion probability ),|( lajd j  and fertility probabili-

ty )|( ii wn  . 

In the MWA method, the similar algorithm to 

bilingual word alignment is used to estimate the 

parameters of the models, except that a word 

cannot be aligned to itself.  

Figure 1 shows an example of the potentially 

collocated word pairs aligned by the MWA me-

thod. 

 

Figure 1. MWA Example 

2.2 Collocation probability 

Given the monolingual word aligned corpus, we 

calculate the frequency of two words aligned in 

the corpus, denoted as ),( ji wwfreq . We filtered 

the aligned words occurring only once. Then the 

probability for each aligned word pair is esti-

mated as follows: 

 


w
j

ji

ji
wwfreq

wwfreq
wwp

),(

),(
)|(                  (2) 

 


w
i

ji

ij
wwfreq

wwfreq
wwp

),(

),(
)|(                   (3) 

In this paper, the words of collocation are 

symmetric and we do not determine which word 

is the head and which word is the modifier. Thus, 

the collocation probability of two words is de-

fined as the average of both probabilities, as in 

Eq. (4). 

2

)|()|(
),(

ijji

ji

wwpwwp
wwr


       (4) 

If we have multiple monolingual corpora to 

estimate the collocation probabilities, we interpo-

late the probabilities as shown in Eq. (5). 

),(),( ji
k

kkji wwrwwr             (5) 

k  denotes the interpolation coefficient for 

the probabilities estimated on the k
th
 corpus. 

3 Improving Statistical Bilingual Word 
Alignment 

We use the collocation information to improve 

both one-directional and bi-directional bilingual 

word alignments. The alignment probabilities are 

re-estimated by using the collocation probabili-

ties of words in the same cept. 

The team leader plays a key role in the project undertaking. 

The team leader plays a key role in the project undertaking. 

 

826



3.1 Improving one-directional bilingual 
word alignment 

According to the BWA method, given a bilingual 

sentence pair leE 1  and 
mfF 1 , the optimal 

alignment sequence A  between E and F can be 

obtained as in Eq. (6). 

)|,(maxarg* EAFpA
A

                    (6) 

The method is implemented in a series of five 

models (IBM Models). IBM Model 1 only em-

ploys the word translation model to calculate the 

probabilities of alignments. In IBM Model 2, 

both the word translation model and position dis-

tribution model are used. IBM Model 3, 4 and 5 

consider the fertility model in addition to the 

word translation model and position distribution 

model. And these three models are similar, ex-

cept for the word distortion models. 

One-to-one and many-to-one alignments could 

be produced by using IBM models. Although the 

fertility model is used to restrict the number of 

source words in a cept and the position distortion 

model is used to describe the correlation of the 

positions of the source words, the quality of 

many-to-one alignments is lower than that of 

one-to-one alignments. 

Intuitively, the probability of the source words 

aligned to a target word is not only related to the 

fertility ability and their relative positions, but 

also related to lexical tokens of words, such as 

common phrase or idiom. In this paper, we use 

the collocation probability of the source words in 

a cept to measure their correlation strength. Giv-

en source words }|{ iaf jj   aligned to ie , their 

collocation probability is calculated as in Eq. (7). 

)1(*

),(2

})|({

1

1 1
][][



 





 

ii

k kg
giki

jj

i i

ffr

iafr


 

     (7) 

Here, kif ][ and gif ][ denote the 
thk  word and 

thg  word in }|{ iaf jj  ; ),( ][][ giki ffr  denotes 

the collocation probability of kif ][ and gif ][ , as 

shown in Eq. (4).  

Thus, the collocation probability of the align-

ment sequence of a sentence pair can be calcu-

lated according to Eq. (8). 

 


l

i
jj iafrEAFr

1

})|({)|,(            (8) 

Based on maximum entropy framework, we 

combine the collocation model and the BWA 

model to calculate the word alignment probabili-

ty of a sentence pair, as shown in Eq. (9). 

  




'

)),,(exp(

)),,(exp(

)|,(

A i
ii

i
ii

r
AEFh

AEFh

EAFp



     (9) 

Here, ),,( AEFhi and i  denote features and 

feature weights, respectively. We use two fea-

tures in this paper, namely alignment probabili-

ties and collocation probabilities. 

Thus, we obtain the decision rule: 

}),,({maxarg* 
i

ii
A

AEFhA            (10) 

Based on the GIZA++ package
1
, we imple-

mented a tool for the improved BWA method. 

We first train IBM Model 4 and collocation 

model on bilingual corpus and monolingual cor-

pus respectively. Then we employ the hill-

climbing algorithm (Al-Onaizan et al., 1999) to 

search for the optimal alignment sequence of a 

given sentence pair, where the score of an align-

ment sequence is calculated as in Eq. (10). 

We note that Eq. (8) only deals with many-to-

one alignments, but the alignment sequence of a 

sentence pair also includes one-to-one align-

ments. To calculate the collocation probability of 

the alignment sequence, we should also consider 

the collocation probabilities of such one-to-one 

alignments. To solve this problem, we use the 

collocation probability of the whole source sen-

tence, )(Fr , as the collocation probability of 

one-word cept. 

3.2 Improving bi-directional bilingual word 
alignments 

In word alignment models implemented in GI-

ZA++, only one-to-one and many-to-one word 

alignment links can be found. Thus, some multi-

word units cannot be correctly aligned. The 

symmetrization method is used to effectively 

overcome this deficiency (Och and Ney, 2003). 

Bi-directional alignments are generally obtained 

from source-to-target alignments tsA 2  and target-

to-source alignments stA 2 , using some heuristic 

rules (Koehn et al., 2005). This method ignores 

the correlation of the words in the same align-

ment unit, so an alignment may include many 

unrelated words
2
, which influences the perfor-

mances of SMT systems. 

                                                 
1 http://www.fjoch.com/GIZA++.html 
2 In our experiments, a multi-word unit may include up to 

40 words. 

827



In order to solve the above problem, we incor-

porate the collocation probabilities into the bi-

directional word alignment process. 

Given alignment sets tsA 2  and stA 2 . We can 

obtain the union sttsts AAA 22  . The source 

sentence mf1  can be segmented into m  cepts 

mf


1 . The target sentence 
le1  can also be seg-

mented into l   cepts le


1 . The words in the same 

cept can be a consecutive word sequence or an 

interrupted word sequence. 

Finally, the optimal alignments A  between 
mf


1  and 
le


1  can be obtained from tsA   using the 

following decision rule. 

})()(),({maxarg

),,(

3
21

),(

*'
1

'
1







 Afe

jiji
AA

ml

jits

frerfep

Afe

    (11) 

Here, )( jfr  and )( ier  denote the collocation 

probabilities of the words in the source language 

and target language respectively, which are cal-

culated by using Eq. (7). ),( ji fep  denotes the 

word translation probability that is calculated 

according to Eq. (12). i  denotes the weights of 

these probabilities. 

||*||

2/))|()|((

),(
ji

ee ff

ji
fe

efpfep

fep
i j

  


 

    (12) 

)|( fep  and )|( efp  are the source-to-target 

and target-to-source translation probabilities 

trained from the word aligned bilingual corpus. 

4 Improving Phrase Table 

Phrase-based SMT system automatically extracts 

bilingual phrase pairs from the word aligned bi-

lingual corpus. In such a system, an idiomatic 

expression may be split into several fragments, 

and the phrases may include irrelevant words. In 

this paper, we use the collocation probability to 

measure the possibility of words composing a 

phrase. 

For each bilingual phrase pair automatically 

extracted from word aligned corpus, we calculate 

the collocation probabilities of source phrase and 

target phrase respectively, according to Eq. (13). 

)1(*

),(2

)(

1

1 1

1


 





 

nn

wwr

wr

n

i

n

ij
ji

n                  (13) 

Here, nw1  denotes a phrase with n words; 

),( ji wwr  denotes the collocation probability of a 

Corpora 
Chinese 

words 

English 

words 

Bilingual corpus 6.3M 8.5M 

Additional monolingual 

corpora 
312M 203M 

Table 1. Statistics of training data 

word pair calculated according to Eq. (4). For the 

phrase only including one word, we set a fixed 

collocation probability that is the average of the 

collocation probabilities of the sentences on a 

development set. These collocation probabilities 

are incorporated into the phrase-based SMT sys-

tem as features.  

5 Experiments on Word Alignment 

5.1 Experimental settings 

We use a bilingual corpus, FBIS (LDC2003E14), 

to train the IBM models. To train the collocation 

models, besides the monolingual parts of FBIS, 

we also employ some other larger Chinese and 

English monolingual corpora, namely, Chinese 

Gigaword (LDC2007T38), English Gigaword 

(LDC2007T07), UN corpus (LDC2004E12), Si-

norama corpus (LDC2005T10), as shown in Ta-

ble 1. 

Using these corpora, we got three kinds of col-

location models: 

CM-1: the training data is the additional mo-

nolingual corpora; 

CM-2: the training data is either side of the bi-

lingual corpus; 

CM-3: the interpolation of CM-1 and CM-2. 

To investigate the quality of the generated 

word alignments, we randomly selected a subset 

from the bilingual corpus as test set, including 

500 sentence pairs. Then word alignments in the 

subset were manually labeled, referring to the 

guideline of the Chinese-to-English alignment 

(LDC2006E93), but we made some modifica-

tions for the guideline. For example, if a preposi-

tion appears after a verb as a phrase aligned to 

one single word in the corresponding sentence, 

then they are glued together. 

There are several different evaluation metrics 

for word alignment (Ahrenberg et al., 2000). We 

use precision (P), recall (R) and alignment error 

ratio (AER), which are similar to those in Och 

and Ney (2000), except that we consider each 

alignment as a sure link. 

828



Experiments 
Single word alignments Multi-word alignments 

P R AER P R AER 

Baseline 0.77 0.45 0.43 0.23 0.71 0.65 

Improved BWA methods 

CM-1 0.70 0.50 0.42 0.35 0.86 0.50 

CM-2 0.73 0.48 0.42 0.36 0.89 0.49 

CM-3 0.73 0.48 0.41 0.39 0.78 0.47 

Table 2. English-to-Chinese word alignment results 

 

Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and 

the improved BWA method using CM-3. " " denotes the alignments of our method; " " denotes 

the alignments of the baseline method. 

||

||

g

rg

S

SS
P


                       (14) 

||

||

r

rg

S

SS
R


                      (15) 

||||

||*2
1

rg

rg

SS

SS
AER





              (16) 

Where, gS  and rS  denote the automatically 

generated alignments and the reference align-

ments. 

In order to tune the interpolation coefficients 

in Eq. (5) and the weights of the probabilities in 

Eq. (11), we also manually labeled a develop-

ment set including 100 sentence pairs, in the 

same manner as the test set. By minimizing the 

AER on the development set, the interpolation 

coefficients of the collocation probabilities on 

CM-1 and CM-2 were set to 0.1 and 0.9. And the 

weights of probabilities were set as 6.01  , 

2.02  and 2.03  . 

5.2 Evaluation results 

One-directional alignment results 

To train a Chinese-to-English SMT system, 

we need to perform both Chinese-to-English and 

English-to-Chinese word alignment. We only 

evaluate the English-to-Chinese word alignment 

here. GIZA++ with the default settings is used as 

the baseline method. The evaluation results in 

Table 2 indicate that the performances of our 

methods on single word alignments are close to 

that of the baseline method. For multi-word 

alignments, our methods significantly outper-

form the baseline method in terms of both preci-

sion and recall, achieving up to 18% absolute 

error rate reduction. 

Although the size of the bilingual corpus is 

much smaller than that of additional monolingual 

corpora, our methods using CM-1 and CM-2 

achieve comparable performances. It is because 

CM-2 and the BWA model are derived from the 

same resource. By interpolating CM1 and CM2, 

i.e. CM-3, the error rate of multi-word alignment 

results is further reduced. 

Figure 2 shows an example of word alignment 

results generated by the baseline method and the 

improved method using CM-3. In this example, 

our method successfully identifies many-to-one 

alignments such as "the people of the world  

世人". In our collocation model, the collocation 
probability of "the people of the world" is much 

higher than that of "people world". And our me-

thod is also effective to prevent the unrelated 

中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 

China's science and technology research has made achievements which have gained the attention of the people of the world . 

中国  的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 
zhong-guo  de     ke-xue-ji-shu      yan-jiu      qu-de       le      xu-duo   ling   shi-ren     zhu-mu     de     cheng-jiu . 

china        DE    science and         research   obtain      LE      many     let    common    attract     DE  achievement . 

                             technology                                                                            people    attention   

829



Experiments 
Single word alignments Multi-word alignments All alignments 

P R AER P R AER P R AER 

Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51 

Our methods 

WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45 

WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44 

WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40 

Table 3. Bi-directional word alignment results 

words from being aligned. For example, in the 

baseline alignment "has made ... have 取得", 
"have" and "has" are unrelated to the target word, 

while our method only generated "made  取

得", this is because that the collocation probabili-
ties of "has/have" and "made" are much lower 

than that of the whole source sentence. 

Bi-directional alignment results 

We build a bi-directional alignment baseline 

in two steps: (1) GIZA++ is used to obtain the 

source-to-target and target-to-source alignments; 

(2) the bi-directional alignments are generated by 

using "grow-diag-final". We use the methods 

proposed in section 3 to replace the correspond-

ing steps in the baseline method. We evaluate 

three methods:  

WA-1: one-directional alignment method pro-

posed in section 3.1 and grow-diag-final; 

WA-2: GIZA++ and the bi-directional bilin-

gual word alignments method proposed in 

section 3.2; 

WA-3: both methods proposed in section 3. 

Here, CM-3 is used in our methods. The re-

sults are shown in Table 3. 

We can see that WA-1 achieves lower align-

ment error rate as compared to the baseline me-

thod, since the performance of the improved one-

directional alignment method is better than that 

of GIZA++. This result indicates that improving 

one-directional word alignment results in bi-

directional word alignment improvement. 

The results also show that the AER of WA-2 

is lower than that of the baseline. This is because 

the proposed bi-directional alignment method 

can effectively recognize the correct alignments 

from the alignment union, by leveraging colloca-

tion probabilities of the words in the same cept. 

Our method using both methods proposed in 

section 3 produces the best alignment perfor-

mance, achieving 11% absolute error rate reduc-

tion. 

Experiments BLEU (%) 

Baseline 29.62 

Our methods 

WA-1 

CM-1 30.85 

CM-2 31.28 

CM-3 31.48 

WA-2 

CM-1 31.00 

CM-2 31.33 

CM-3 31.51 

WA-3 

CM-1 31.43 

CM-2 31.62 

CM-3 31.78 

Table 4. Performances of Moses using the dif-

ferent bi-directional word alignments (Signifi-

cantly better than baseline with p < 0.01) 

6 Experiments on Phrase-Based SMT 

6.1 Experimental settings 

We use FBIS corpus to train the Chinese-to-

English SMT systems. Moses (Koehn et al., 2007) 

is used as the baseline phrase-based SMT system. 

We use SRI language modeling toolkit (Stolcke, 

2002) to train a 5-gram language model on the 

English sentences of FBIS corpus. We used the 

NIST MT-2002 set as the development set and 

the NIST MT-2004 test set as the test set. And 

Koehn's implementation of minimum error rate 

training (Och, 2003) is used to tune the feature 

weights on the development set. 

We use BLEU (Papineni et al., 2002) as eval-

uation metrics. We also calculate the statistical 

significance differences between our methods 

and the baseline method by using paired boot-

strap re-sample method (Koehn, 2004). 

6.2 Effect of improved word alignment on 
phrase-based SMT 

We investigate the effectiveness of the improved 

word alignments on the phrase-based SMT sys-

tem. The bi-directional alignments are obtained 

830



 

Figure 3. Example of the translations generated by the baseline system and the system where the 

phrase collocation probabilities are added 

Experiments BLEU (%) 

Moses 29.62 

+ Phrase collocation probability 30.47 

+ Improved word alignments 

+ Phrase collocation probability 
32.02 

Table 5. Performances of Moses employing 

our proposed methods (Significantly better than 

baseline with p < 0.01) 

using the same methods as those shown in Table 

3. Here, we investigate three different collocation 

models for translation quality improvement. The 

results are shown in Table 4. 

From the results of Table 4, it can be seen that 

the systems using the improved bi-directional 

alignments achieve higher quality of translation 

than the baseline system. If the same alignment 

method is used, the systems using CM-3 got the 

highest BLEU scores. And if the same colloca-

tion model is used, the systems using WA-3 

achieved the higher scores. These results are 

consistent with the evaluations of word align-

ments as shown in Tables 2 and 3. 

6.3 Effect of phrase collocation probabili-
ties 

To investigate the effectiveness of the method 

proposed in section 4, we only use the colloca-

tion model CM-3 as described in section 5.1. The 

results are shown in Table 5. When the phrase 

collocation probabilities are incorporated into the 

SMT system, the translation quality is improved, 

achieving an absolute improvement of 0.85 

BLEU score. This result indicates that the collo-

cation probabilities of phrases are useful in de-

termining the boundary of phrase and predicting 

whether phrases should be translated together, 

which helps to improve the phrase-based SMT 

performance. 

Figure 3 shows an example: T1 is generated 

by the system where the phrase collocation prob-

abilities are used and T2 is generated by the 

baseline system. In this example, since the collo-

cation probability of "出 问题" is much higher 

than that of "问题 。", our method tends to split 

"出 问题 。" into "(出 问题) (。)", rather than 

"(出) (问题 。)". For the phrase "才能 避免" in 
the source sentence, the collocation probability 

of the translation "in order to avoid" is higher 

than that of the translation "can we avoid". Thus, 

our method selects the former as the translation. 

Although the phrase "我们 必须 采取 有效 措

施" in the source sentence has the same transla-
tion "We must adopt effective measures", our 

method splits this phrase into two parts "我们 必

须" and "采取 有效 措施", because two parts 
have higher collocation probabilities than the 

whole phrase. 

We also investigate the performance of the 

system employing both the word alignment im-

provement and phrase table improvement me-

thods. From the results in Table 5, it can be seen 

that the quality of translation is future improved. 

As compared with the baseline system, an abso-

lute improvement of 2.40 BLEU score is 

achieved. And this result is also better than  the 

results shown in Table 4. 

7 Experiments on Parsing-Based SMT 

We also investigate the effectiveness of the im-

proved word alignments on the parsing-based 

SMT system, Joshua (Li et al., 2009). In this sys-

tem, the Hiero-style SCFG model is used 

(Chiang, 2007), without syntactic information. 

The rules are extracted only based on the FBIS 

corpus, where words are aligned by "MW-3 & 

CM-3". And the language model is the same as 

that in Moses. The feature weights are tuned on 

the development set using the minimum error 

我们  必须  采取  有效  措施  才能  避免  出  问题  。 
wo-men bi-xu      cai-qu   you-xiao  cuo-shi   cai-neng  bi-mian  chu      wen-ti      . 

we          must        use      effective   measure    can        avoid    out      problem  . 

We must  adopt effective measures  in order to avoid  problems  . 

 

 

We must adopt effective measures  can we avoid  out of the  question . 

T1: 

T2: 

831



Experiments BLEU (%) 

Joshua 30.05 

+ Improved word alignments 31.81 

Table 6. Performances of Joshua using the dif-

ferent word alignments (Significantly better than 

baseline with p < 0.01) 

rate training method. We use the same evaluation 

measure as described in section 6.1. 

The translation results on Joshua are shown in 

Table 6. The system using the improved word 

alignments achieves an absolute improvement of 

1.76 BLEU score, which indicates that the im-

provements of word alignments are also effective 

to improve the performance of the parsing-based 

SMT systems. 

8 Conclusion 

We presented a novel method to use monolingual 

collocations to improve SMT. We first used the 

MWA method to identify potentially collocated 

words and estimate collocation probabilities only 

from monolingual corpora, no additional re-

source or linguistic preprocessing is needed. 

Then the collocation information was employed 

to improve BWA for various kinds of SMT sys-

tems and to improve phrase table for phrase-

based SMT. 

To improve BWA, we re-estimate the align-

ment probabilities by using the collocation prob-

abilities of words in the same cept. To improve 

phrase table, we calculate phrase collocation 

probabilities based on word collocation probabil-

ities. Then the phrase collocation probabilities 

are used as additional features in phrase-based 

SMT systems. 

The evaluation results showed that the pro-

posed method significantly improved word 

alignment, achieving an absolute error rate re-

duction of 29% on multi-word alignment. The 

improved word alignment results in an improve-

ment of 2.16 BLEU score on a phrase-based 

SMT system and an improvement of 1.76 BLEU 

score on a parsing-based SMT system. When we 

also used phrase collocation probabilities as ad-

ditional features, the phrase-based SMT perfor-

mance is finally improved by 2.40 BLEU score 

as compared with the baseline system. 

Reference 

Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, 

and Jorg Tiedemann. 2000. Evaluation of Word 

Alignment Systems. In Proceedings of the Second 

International Conference on Language Resources 

and Evaluation, pp. 1255-1261. 

Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin 

Knight, John Lafferty, Dan Melamed, Franz-Josef 

Och, David Purdy, Noah A. Smith, and David Ya-

rowsky. 1999. Statistical Machine Translation. Fi-

nal Report. In Johns Hopkins University Workshop. 

Peter F. Brown, Stephen A. Della Pietra, Vincent J. 

Della Pietra, and Robert. L. Mercer. 1993. The Ma-

thematics of Statistical Machine Translation: Pa-

rameter estimation. Computational Linguistics, 

19(2): 263-311. 

Colin Cherry and Dekang Lin. 2003. A Probability 

Model to Improve Word Alignment. In Proceed-

ings of the 41st Annual Meeting of the Association 

for Computational Linguistics, pp. 88-95. 

David Chiang. 2007. Hierarchical Phrase-Based 

Translation. Computational Linguistics, 33(2): 

201-228. 

Fei Huang. 2009. Confidence Measure for Word 

Alignment. In Proceedings of the 47th Annual 

Meeting of the ACL and the 4th IJCNLP, pp. 932-

940. 

Philipp Koehn. 2004. Statistical Significance Tests for 

Machine Translation Evaluation. In Proceedings of 

the 2004 Conference on Empirical Methods in 

Natural Language Processing, pp. 388-395. 

Philipp Koehn, Amittai Axelrod, Alexandra Birch 

Mayne, Chris Callison-Burch, Miles Osborne, and 

David Talbot. 2005. Edinburgh System Description 

for the 2005 IWSLT Speech Translation Evalua-

tion. In Processings of the International Workshop 

on Spoken Language Translation 2005. 

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 

Statistical Phrase-based Translation. In Proceed-

ings of the Human Language Technology Confe-

rence and the North American Association for 

Computational Linguistics, pp. 127-133. 

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 

Callison-Burch, Marcello Federico, Nicola Bertoldi, 

Brooke Cowan, Wade Shen, Christine Moran Ri-

chard Zens, Chris Dyer, Ondrej Bojar, Alexandra 

Constantin, and Evan Herbst. 2007. Moses: Open 

Source Toolkit for Statistical Machine Translation. 

In Proceedings of the 45th Annual Meeting of the 

ACL, Poster and Demonstration Sessions, pp. 177-

180. 

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-

nitkevitch, Sanjeev Khudanpur, Lane Schwartz, 

Wren Thornton, Jonathan Weese, and Omar Zaidan. 

2009. Demonstration of Joshua: An Open Source 

Toolkit for Parsing-based Machine Translation. In 

Proceedings of the 47th Annual Meeting of the As-

832



sociation for Computational Linguistics, Software 

Demonstrations, pp. 25-28. 

Yang Liu, Qun Liu, and Shouxun Lin. Log-linear 

Models for Word Alignment. 2005. In Proceedings 

of the 43rd Annual Meeting of the Association for 

Computational Linguistics, pp. 459-466. 

Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 

2009. Collocation Extraction Using Monolingual 

Word Alignment Method. In Proceedings of the 

2009 Conference on Empirical Methods in Natural 

Language Processing, pp. 487-495. 

Daniel Marcu and William Wong. 2002. A Phrase-

Based, Joint Probability Model for Statistical Ma-

chine Translation. In Proceedings of the 2002 Con-

ference on Empirical Methods in Natural Lan-

guage Processing,  pp. 133-139. 

Yuval Marton and Philip Resnik. 2008. Soft Syntactic 

Constraints for Hierarchical Phrase-Based Transla-

tion. In Proceedings of the 46st Annual Meeting of 

the Association for Computational Linguistics, pp. 

1003-1011. 

Kathleen R. McKeown and Dragomir R. Radev. 2000. 

Collocations. In Robert Dale, Hermann Moisl, and 

Harold Somers (Ed.), A Handbook of Natural Lan-

guage Processing, pp. 507-523. 

Franz Josef Och and Hermann Ney. 2000. Improved 

Statistical Alignment Models. In Proceedings of 

the 38th Annual Meeting of the Association for 

Computational Linguistics, pp. 440-447. 

Franz Josef Och. 2003. Minimum Error Rate Training 

in Statistical Machine Translation. In Proceedings 

of the 41st Annual Meeting of the Association for 

Computational Linguistics, pp. 160-167. 

Franz Josef Och and Hermann Ney. 2003. A Syste-

matic Comparison of Various Statistical Alignment 

Models. Computational Linguistics, 29(1): 19-52. 

Kishore Papineni, Salim Roukos, Todd Ward, and 

Weijing Zhu. 2002. BLEU: A Method for Auto-

matic Evaluation of Machine Translation. In Pro-

ceedings of 40th annual meeting of the Association 

for Computational Linguistics, pp. 311-318. 

Andreas Stolcke. 2002. SRILM - An Extensible Lan-

guage Modeling Toolkit. In Proceedings for the In-

ternational Conference on Spoken Language 

Processing, pp. 901-904. 

Dekai Wu. 1997. Stochastic Inversion Transduction 

Grammars and Bilingual Parsing of Parallel Cor-

pora. Computational Linguistics, 23(3): 377-403. 

Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 

2009. A Syntax-Driven Bracketing Model for 

Phrase-Based Translation. In Proceedings of the 

47th Annual Meeting of the ACL and the 4th 

IJCNLP, pp. 315-323. 

 

833


