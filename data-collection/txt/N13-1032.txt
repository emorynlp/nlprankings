










































Improving reordering performance using higher order and structural features


Proceedings of NAACL-HLT 2013, pages 315–324,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Improving reordering performance using higher order and structural
features

Mitesh M. Khapra
IBM Research India

mikhapra@in.ibm.com

Ananthakrishnan Ramanathan
IBM Research India

anandr42@gmail.com

Karthik Visweswariah
IBM Research India

v-karthik@in.ibm.com

Abstract

Recent work has shown that word aligned data
can be used to learn a model for reordering
source sentences to match the target order.
This model learns the cost of putting a word
immediately before another word and finds the
best reordering by solving an instance of the
Traveling Salesman Problem (TSP). However,
for efficiently solving the TSP, the model is
restricted to pairwise features which examine
only a pair of words and their neighborhood.
In this work, we go beyond these pairwise fea-
tures and learn a model to rerank the n-best
reorderings produced by the TSP model us-
ing higher order and structural features which
help in capturing longer range dependencies.
In addition to using a more informative set
of source side features, we also capture target
side features indirectly by using the transla-
tion score assigned to a reordering. Our exper-
iments, involving Urdu-English, show that the
proposed approach outperforms a state-of-the-
art PBSMT system which uses the TSP model
for reordering by 1.3 BLEU points, and a pub-
licly available state-of-the-art MT system, Hi-
ero, by 3 BLEU points.

1 Introduction

Handling the differences in word orders between
pairs of languages is crucial in producing good ma-
chine translation. This is especially true for lan-
guage pairs such as Urdu-English which have sig-
nificantly different sentence structures. For exam-
ple, the typical word order in Urdu is Subject Object
Verb whereas the typical word order in English is
Subject Verb Object. Phrase based systems (Koehn
et al., 2003) rely on a lexicalized distortion model

(Al-Onaizan and Papineni, 2006; Tillman, 2004)
and the target language model to produce output
words in the correct order. This is known to be in-
adequate when the languages are very different in
terms of word order (refer to Table 3 in Section 3).

Pre-ordering source sentences while training and
testing has become a popular approach in overcom-
ing the word ordering challenge. Most techniques
for pre-ordering (Collins et al., 2005; Wang et al.,
2007; Ramanathan et al., 2009) depend on a high
quality source language parser, which means these
methods work only if the source language has a
parser (this rules out many languages). Recent work
(Visweswariah et al., 2011) has shown that it is pos-
sible to learn a reordering model from a relatively
small number of hand aligned sentences . This elim-
inates the need of a source or target parser.

In this work, we build upon the work of
Visweswariah et al. (2011) which solves the reorder-
ing problem by treating it as an instance of the
Traveling Salesman Problem (TSP). They learn a
model which assigns costs to all pairs of words in
a sentence, where the cost represents the penalty of
putting a word immediately preceding another word.
The best permutation is found via the chained Lin-
Kernighan heuristic for solving a TSP. Since this
model relies on solving a TSP efficiently, it cannot
capture features other than pairwise features that ex-
amine the words and neighborhood for each pair of
words in the source sentence. In the remainder of
this paper we refer to this model as the TSP model.

Our aim is to go beyond this limitation of the TSP
model and use a richer set of features instead of us-
ing pairwise features only. In particular, we are in-
terested in features that allow us to examine triples
of words/POS tags in the candidate reordering per-

315



mutation (this is akin to going from bigram to tri-
gram language models), and also structural features
that allow us to examine the properties of the seg-
mentation induced by the candidate permutation. To
go beyond the set of features incorporated by the
TSP model, we do not solve the search problem
which would be NP-hard. Instead, we restrict our-
selves to an n-best list produced by the base TSP
model and then search in that list. Using a richer
set of features, we learn a model to rerank these n-
best reorderings. The parameters of the model are
learned using the averaged perceptron algorithm. In
addition to using a richer set of source side features
we also indirectly capture target side features by in-
terpolating the score assigned by our model with the
score assigned by the decoder of a MT system.

To justify the use of these informative features,
we point to the example in Table 1. Here, the head
(driver) of the underlined English Noun Phrase (The
driver of the car) appears to the left of the Noun
Phrase whereas the head (chaalak {driver}) of the
corresponding Urdu Noun Phrase (gaadi {car} ka
{of} chaalak {driver}) appears to the right of the
Noun Phrase. To produce the correct reordering of
the source Urdu sentence the model has to make an
unusual choice of putting gaadi {car} before bola
{said}. We say this is an unusual choice because the
model examines only pairwise features and it is un-
likely that it would have seen sentences having the
bigram “car said”. If the exact segmentation of the
source sentence was known, then the model could
have used the information that the word gaadi {car}
appears in a segment whose head is the noun chaalak
{driver} and hence its not unusual to put gaadi {car}
before bola {said} (because the construct “NP said”
is not unusual). However, since the segmentation
of the source sentence is not known in advance, we
use a heuristic (explained later) to find the segmen-
tation induced by a reordering. We then extract
features (such as first word current segment,
end word current segment) to approximate these
long range dependencies.

Using this richer set of features with Urdu-
English as the source language pair, our approach
outperforms the following state of the art systems:
(i) a PBSMT system which uses TSP model for re-
ordering (by 1.3 BLEU points), (ii) a hierarchical
PBSMT system (by 3 BLEU points). The overall

Input Urdu: fir gaadi ka chaalak kuch bola

Gloss: then car of driver said something
English: Then the driver of the car said something.
Ref. reordering: fir chaalak ka gaadi bola kuch

Table 1: Example motivating the use of structural features

gain is 6.3 BLEU points when compared to a stan-
dard PBSMT system which uses a lexicalized distor-
tion model (Al-Onaizan and Papineni, 2006).

The rest of this paper is organized as follows. In
Section 2 we discuss our approach of re-ranking the
n-best reorderings produced by the TSP model. This
includes a discussion of the model used, the features
used and the algorithm used for learning the parame-
ters of the model. It also includes a discussion on the
modification to the Chained Lin-Kernighan heuris-
tic to produce n-best reorderings. Next, in Section
3 we describe our experimental setup and report the
results of our experiments. In Section 4 we present
some discussions based on our study. In section 5 we
briefly describe some prior related work. Finally, in
Section 6, we present some concluding remarks and
highlight possible directions for future work.

2 Re-ranking using higher order and
structural features

As mentioned earlier, the TSP model (Visweswariah
et al., 2011) looks only at local features for a word
pair (wi, wj). We believe that for better reorder-
ing it is essential to look at higher order and struc-
tural features (i.e., features which look at the overall
structure of a sentence). The primary reason why
Visweswariah et al. (2011) consider only pairwise
bigram features is that with higher order features the
reordering problem can no longer be cast as a TSP
and hence cannot be solved using existing efficient
heuristic solvers. However, we do not have to deal
with an NP-Hard search problem because instead of
considering all possible reorderings we restrict our
search space to only the n-best reorderings produced
by the base TSP model. Formally, given a set of
reorderings, Π = [π1, π2, π3, ...., πn], for a source
sentence s, we are interesting in assigning a score,
score(π), to each of these reorderings and pick the
reordering which has the highest score. In this paper,
we parametrize this score as:

score(π) = θTφ(π) (1)

316



where, θ is the weight vector and φ(π) is a vector
of features extracted from the reordering π. The aim
then is to find,

π∗ = arg max
π∈Π

score(π) (2)

In the following sub-sections, we first briefly
describe our overall approach towards finding π∗.
Next, we describe our modification to the Lin-
Kernighan heuristic for producing n-best outputs
for TSP instead of the 1-best output used by
(Visweswariah et al., 2011). We then discuss the fea-
tures used for re-ranking these n-best outputs, fol-
lowed by a discussion on the learning algorithm used
for estimating the parameters of the model. Finally,
we describe how we interpolate the score assigned
by our model with the score assigned by the decoder
of a SMT engine to indirectly capture target side fea-
tures.

2.1 Overall approach
The training stage of our approach involves two
phases : (i) Training a TSP model which will be
used to generate n-best reorderings and (ii) Training
a re-ranking model using these n-best reorderings.
For training both the models we need a collection
of sentences where the desired reordering π∗(x) for
each input sentence x is known. These reference or-
derings are derived from word aligned source-target
sentence pairs (see first 4 rows of Figure 1). We first
divide this word aligned data into N parts and use
A−i to denote the alignments leaving out the i-th
part. We then train a TSP model M−i using refer-
ence reorderings derived from A−i as described in
(Visweswariah et al., 2011). Next, we produce n-
best reorderings for the source sentences using the
algorithm getNBestReorderings(sentence) de-
scribed later. Dividing the data into N parts is nec-
essary to ensure that the re-ranking model is trained
using a realistic n-best list rather than a very opti-
mistic n-best list (which would be the case if part i
is reordered using a model which has already seen
part i during training).

Each of the n-best reorderings is then repre-
sented as a feature vector comprising of higher
order and structural features. The weights
of these features are then estimated using the
averaged perceptron method. At test time,

getNBestReorderings(sentence) is used to gen-
erate the n-best reorderings for the test sentence us-
ing the trained TSP model. These reorderings are
then represented using higher order and structural
features and re-ranked using the weights learned ear-
lier. We now describe the different stages of our al-
gorithm.

2.2 Generating n-best reorderings for the TSP
model

The first stage of our approach is to train a TSP
model and generate n-best reorderings using it. The
decoder used by Visweswariah et al. (2011) relies
on the Chained Lin-Kernighan heuristic (Lin and
Kernighan, 1973) to produce the 1-best permutation
for the TSP problem. Since our algorithm aims at
re-ranking an n-best list of permutations (reorder-
ings), we made a modification to the Chained Lin-
Kernighan heuristic to produce this n-best list as
shown in Algorithm 1 .

Algorithm 1 getNBestReorderings(sentence)
NbestSet = φ
π∗ = Identity permutation
π∗ = linkernighan(π∗)
insert(NbestSet, π∗)
for i = 1→ nIter do
π

′
= perturb(π∗)

π
′

= linkernighan(π
′
)

if C(π
′
) < maxπ∈NbestSetC(π) then

InsertOrReplace(NbestSet, π
′
)

end if
if C(π

′
) < C(π∗) then

π∗ = π
′

end if
end for

In Algorithm 1 perturb() is a four-edge pertur-
bation described in (Applegate et al., 2003), and
linkernighan() is the Lin-Kernighan heuristic that
applies a sequence of flips that potentially returns
a lower cost permutation as described in (Lin and
Kernighan, 1973). The cost C(π) is calculated us-
ing a trained TSP model.

2.3 Features

We represent each of the n-best reorderings obtained
above as a vector of features which can be divided
into two sets : (i) higher order features and (ii) struc-

317



Segmentation Based Features
(extracted for every segment in
the induced segmentation)

Features fired for the seg-
ment [mere(PRP) ghar(NN)]
in Figure1

end lex current segment ghar
end lex prev segment Shyam
end pos current segment NN
end pos prev segment NN
length of current segment 2
first lex current segment mere
first lex next segment aaye
first pos current segment PRP
first pos next segment V RB
Higher order features Features fired for the triplet

Shyam(NN) the(Vaux)
aaye(VRB) in Figure1

lex triplet jumps lex triplet = “Shyam the
aaye” && jumps = [4,−1]

pos triplet jumps pos triplet = “NN Vaux
VRB” && jumps = [4,−1]

Table 2: Features used in our model.

tural features. The higher order features are es-
sentially trigram lexical and pos features whereas
the structural features are derived from the sentence
structure induced by a reordering (explained later).

2.3.1 Higher Order Features

Since deriving a good reordering would essen-
tially require analyzing the syntactic structure of the
source sentence, the tasks of reordering and parsing
are often considered to be related. The main motiva-
tion for using higher order features thus comes from
a related work on parsing (Koo and Collins, 2010)
where the performance of a state of the art parser
was improved by considering higher order depen-
dencies. In our model we use trigram features (see
Table 2) of the following form:

φ(rui, rui+1, rui+2, J(rui, rui+1), J(rui+1, rui+2))
where rui =word at position i in the reordered
source sentence and J(x, y) = difference between
the positions of x and y in the original source
sentence.

Figure 1 shows an example of jumps between dif-
ferent word pairs in an Urdu sentence. Since such
higher order features will typically be sparse, we
also use some back-off features. For example, in-
stead of using the absolute values of jumps we di-
vide the jumps into 3 buckets, viz., high, low and
medium and use these buckets in conjunction with
the triplets as back-off features.

Figure 1: Segmentation induced on the Urdu sentence
when it is reordered according to its English translation.
Note that the words Shyam and mere are adjacent to each
other in the original Urdu sentence but not in the re-
ordered Urdu sentence. Hence, the word mere marks the
beginning of a new segment.

2.3.2 Structural Features

The second set of features is based on the hy-
pothesis that any reordering of the source sentence
induces a segmentation on the sentence. This seg-
mentation is based on the following heuristic: if wi
and wi+1 appear next to each other in the original
sentence but do not appear next to each other in the
reordered sentence then wi marks the end of a seg-
ment and wi+1 marks the beginning of the next seg-
ment. To understand this better please refer to Fig-
ure 1 which shows the correct reordering of an Urdu
sentence based on its English translation and the cor-
responding segmentation induced on the Urdu sen-
tence. If the correct segmentation of a sentence is
known in advance then one could use a hierarchical
model where the goal would be to reorder segments
instead of reordering words individually (basically,
instead of words, treat segments as units of reorder-
ing. In principle, this is similar to what is done by
parser based reordering methods). Since the TSP
model does not explicitly use segmentation based
features it often produces wrong reorderings (refer
to the motivating example in Section 1).

Reordering such sentences correctly requires
some knowledge about the hierarchical structure of
the sentence. To capture such hierarchical informa-
tion, we use features which look at the elements

318



(words, pos tags) of a segment and its neighboring
segments. These features along with examples are
listed in Table 2. These features should help us in
selecting a reordering which induces a segmentation
which is closest to the correct segmentation induced
by the reference reordering. Note that every feature
listed in Table 2 is a binary feature which takes on
the value 1 if it fires for the given reordering and
value 0 if it does not fire for the given reordering. In
addition to the features listed in Table 2 we also use
the score assigned by the TSP model as a feature.

2.4 Estimating model parameters

We use perceptron as the learning algorithm for es-
timating the parameters of our model described in
Equation 1. To begin with, all parameters are ini-
tialized to 0 and the learning algorithm is run for N
iterations. During each iteration the parameters are
updated after every training instance is seen. For ex-
ample, during the i-th iteration, after seeing the j-th
training sentence, we update the k-th parameter θk
using the following update rule:

θ
(i,j)
k = θ

(i,j−1)
k + φk(π

gold
j )− φk(π

∗
j ) (3)

where, θ(i,j)k = value of the k-th parameter after

seeing sentence j in iteration i

φk = k-th feature

πgoldj = gold reordering for the j-th sentence

π∗j = arg max
π∈Πj

θ(i,j−1)
T

φ(π)

where Πj is the set of n-best reorderings for the j-
th sentence. π∗j is thus the highest-scoring reorder-
ing for the j-th sentence under the current parame-
ter vector. Since the averaged perceptron method is
known to perform better than the perceptron method,
we used the averaged values of the parameters at the
end of N iterations, calculated as:

θavgk =
1

N · t

N∑
i=1

t∑
j=1

θ
(i,j)
k (4)

where, N = Number of iterations

t = Number of training instances

We observed that in most cases the reference re-
ordering in not a part of the n-best list produced
by the TSP model. In such cases instead of using

φk(π
gold
j ) for updating the weights in Equation 3 we

use φk(π
closest to gold
j ) as this is known to be a better

strategy for learning a re-ranking model (Arun and
Koehn, 2007). πclosest to goldj is given by:

arg max
πij∈Πj

# of common bigram pairs in πij and π
gold
j

len(πgoldj )

where, Πj = set of n-best reorderings for jth sentence

πclosest to goldj is thus the reordering which has the

maximum overlap with πgoldj in terms of the number
of word pairs (wm, wn) where wn is put next to wm.

2.5 Interpolating with MT score
The approach described above aims at producing a
better reordering by extracting richer features from
the source sentence. Since the final aim is to im-
prove the performance of an MT system, it would
potentially be beneficial to interpolate the scores as-
signed by Equation 1 to a given reordering with the
score assigned by the decoder of an MT system to
the translation of the source sentence under this re-
ordering. Intuitively, the MT score would allow us
to capture features from the target sentence which
are obviously not available to our model. With this
motivation, we use the following interpolated score
(scoreI ) to select the best translation.

scoreI(ti) = λ·scoreθ(πi) + (1− λ) · scoreMT (ti)
where, ti =translation produced under the i-th

reordering of the source sentence

scoreθ(πi) =score assigned by our model to the

i-th reordering

scoreMT (ti) =score assigned by the MT system to ti

The weight λ is used to ensure that scoreθ(πi) and
scoreMT (πi) are in the same range (it just serves as
a normalization constant). We acknowledge that the
above process is expensive because it requires the
MT system to decode n reorderings for every source
sentence. However, the aim of this work is to show
that interpolating with the MT score which implic-
itly captures features from the target sentence helps
in improving the performance. Ideally, this interpo-
lation should (and can) be done at decode time with-
out having to decode n reorderings for every source

319



sentence (for example by expressing the n reorder-
ings as a lattice), but, we leave this as future work.

3 Empirical evaluation

We evaluated our reordering approach on Urdu-
English. We use two types of evaluation, one in-
trinsic and one extrinsic. For intrinsic evaluation,
we compare the reordered source sentence in Urdu
with a reference reordering obtained from the hand
alignments using BLEU (referred to as monolingual
BLEU or mBLEU by (Visweswariah et al., 2011) ).
Additionally, we evaluate the effect of reordering on
MT performance using BLEU (extrinsic evaluation).

As mentioned earlier, our training process in-
volves two phases : (i) Generating n-best reorder-
ings for the training data and (ii) using these n-best
reorderings to train a perceptron model. We use the
same data for training the reordering model as well
as our perceptron model. This data contains 180K
words of manual alignments (part of the NIST MT-
08 training data) and 3.9M words of automatically
generated machine alignments (1.7M words from
the NIST MT-08 training data1 and 2.2M words ex-
tracted from sources on the web2). The machine
alignments were generated using a supervised maxi-
mum entropy model (Ittycheriah and Roukos, 2005)
and then corrected using an improved correction
model (McCarley et al., 2011). We first divide the
training data into 10 folds. The n-best reorder-
ings for each fold are then generated using a model
trained on the remaining 9 folds. This division into
10 folds is done for reasons explained earlier in Sec-
tion 2.1. These n-best reorderings are then used to
train the perceptron model as described in Section
2.4. Note that Visweswariah et al. (2011) used only
manually aligned data for training the TSP model.
However, we use machine aligned data in addition
to manually aligned data for training the TSP model
as it leads to better performance. We used this im-
provised TSP model as the state of the art baseline
(rows 2 and 3 in Tables 3 and 4 respectively) for
comparing with our approach.

We observed that the perceptron algorithm con-
verges after 5 iterations beyond which there is very
little (<1%) improvement in the bigram precision on

1http://www.ldc.upenn.edu
2http://centralasiaonline.com

the training data itself (bigram precision is the frac-
tion of word pairs which are correctly put next to
each other). Hence, for all the numbers reported in
this paper, we used 5 iterations of perceptron train-
ing. Similarly, while generating the n-best reorder-
ings, we experimented with following values of n :
10, 25, 50, 100 and 200. We observed that, by re-
stricting the search space to the top-50 reorderings
we get the best reordering performance (mBLEU)
on a development set. Hence, we used n=50 for our
MT experiments.

For intrinsic evaluation we use a development set
of 8017 Urdu tokens reordered manually. Table 3
compares the performance of the top-1 reordering
output by our algorithm with the top-1 reordering
generated by the improved TSP model in terms of
mBLEU. We see a gain of 1.8 mBLEU points with
our approach.

Next, we see the impact of the better reorderings
produced by our system on the performance of
a state-of-the-art MT system. For this, we used
a standard phrase based system (Al-Onaizan and
Papineni, 2006) with a lexicalized distortion model
with a window size of +/-4 words (Tillmann and
Ney, 2003). As mentioned earlier, our training data
consisted of 3.9M words including the NIST MT-08
training data. We use HMM alignments along with
higher quality alignments from a supervised aligner
(McCarley et al., 2011). The Gigaword English
corpus was used for building the English language
model. We report results on the NIST MT-08
evaluation set, averaging BLEU scores from the
News and Web conditions to provide a single BLEU
score. Table 4 compares the MT performance
obtained by reordering the training and test data
using the following approaches:

1. No pre-ordering: A baseline system which
does not use any source side reordering as a pre-
processing step

2. HIERO : A state of the art hierarchical phrase
based translation system (Chiang, 2007)

3. TSP: A system which uses the 1-best reordering
produced by the TSP model

4. Higher order & structural features: A system

320



Approach mBLEU
Unreordered 31.2

TSP 56.6
Higher order & structural features 58.4

Table 3: mBLEU scores for Urdu to English reordering
using different models.

Approach BLEU
No pre-ordering 21.9

HIERO 25.2
TSP 26.9

Higher order & structural features 27.5
Interpolating with MT score 28.2

Table 4: MT performance for Urdu to English without re-
ordering and with reordering using different approaches.

which reranks n-best reorderings produced by TSP
using higher order and structural features

5. Interpolating with MT score : A system which
interpolates the score assigned to a reordering by
our model with the score assigned by a MT system

We used Joshua 4.0 (Ganitkevitch et al., 2012)
which provides an open source implementation of
HIERO. For training, tuning and testing HIERO
we used the same experimental setup as described
above. As seen in Table 4, we get an overall gain of
6.2 BLEU points with our approach as compared to
a baseline system which does not use any reordering.
More importantly, we outperform (i) a PBSMT sys-
tem which uses the TSP model by 1.3 BLEU points
and (ii) a state of the art hierarchical phrase based
translation system by 3 points.

4 Discussions

We now discuss some error corrections and ablation
tests.

4.1 Example of error correction

We first give an example where the proposed ap-
proach performed better than the TSP model. In the
example below, I = input sentence, E= gold English
translation, T = incorrect reordering produced by
TSP and O = correct reordering produced by our
approach. Note that the words roman catholic aur
protestant in the input sentence get translated as

Sentence length mBLEU
Unreordered TSP Our

approach
1-14 words (small) 29.7 58.7 57.8
15-22 words (med.) 28.2 56.8 59.2
23+ words (long) 33.4 55.8 58.2
All 31.2 56.6 58.4

Table 5: mBLEU improvements on sentences of different
lengths

a continuous phrase in English (Roman Catholic
and Protestant) and hence should be treated as a
single unit by the reordering model. The TSP model
fails to keep this segment intact whereas our model
(which uses segmentation based features) does so
and matches the reference reordering.

I: ab roman catholic aur protestant ke darmiyaan
ikhtilafat khatam ho chuke hai

E: The differences between Roman Catholics and
Protestants have now ended

T: ab roman ikhtilafat ke darmiyaan catholic aur
protestant hai khatam ho chuke

O: ab ikhtilafat ke darmiyaan roman catholic aur
protestant hai khatam ho chuke

4.2 Performance based on sentence length

We split the test data into roughly three equal parts
based on length, and calculated the mBLEU im-
provements on each of these parts as reported in
Table 5. These results show that the model works
much better for medium-to-long sentences. In fact,
we see a drop in performance for small sentences. A
possible reason for this could be that the structural
features that we use are derived through a heuristic
that is error-prone, and in shorter sentences, where
there would be fewer reordering problems, these er-
rors hurt more than they help. While this needs to be
analyzed further, we could meanwhile combine the
two models fruitfully by using the base TSP model
for small sentences and the new model for longer
sentences.

321



Disabled feature mBLEU
end lex current segment 57.6
end lex prev segment 57.6
end pos current segment 57.8
end pos prev segment 57.4
length 57.6
lex triplet jumps 58.0
pos triplet jumps 56.1
first lex current segment 58.2
first lex next segment 58.2
first pos current segment 57.6
first pos next segment 57.6
NONE 58.4

Table 6: Ablation test indicating the contribution of each
feature to the reordering performance.

4.3 Ablation test
To study the contribution of each feature to the
reordering performance, we did an ablation test
wherein we disabled one feature at a time and mea-
sured the change in the mBLEU scores. Table 6
summarizes the results of our ablation test. The
maximum drop in performance is obtained when the
pos triplet jumps feature is disabled. This obser-
vation supports our claim that higher order features
(more than bigrams) are essential for better reorder-
ing. The lex triplet jumps feature has the least
impact on the performance mainly because it is a
lexicalized feature and hence very sparse. Also note
that there is a high correlation between the perfor-
mances obtained by dropping one feature from each
of the following pairs :
i) first lex current segment, first lex next segment
ii) first pos current segment, first pos next segment
iii) end lex current segment, end lex next segment.
This is because these pairs of features are
highly dependent features. Note that similar to
the pos triplet jumps feature we also tried a
pos quadruplet jumps feature but it did not help
(mainly due to overfitting and sparsity).

5 Related Work

There are several studies which have shown that re-
ordering the source side sentence to match the target
side order leads to improvements in Machine Trans-
lation. These approaches can be broadly classified
into three types. First, approaches which reorder
source sentences by applying rules to the source side

parse; the rules are either hand-written (Collins et
al., 2005; Wang et al., 2007; Ramanathan et al.,
2009) or learned from data (Xia and McCord, 2004;
Genzel, 2010; Visweswariah et al., 2010). These
approaches require a source side parser which is
not available for many languages. The second type
of approaches treat machine translation decoding
as a parsing problem by using source and/or tar-
get side syntax in a Context Free Grammar frame-
work. These include Hierarchical models (Chi-
ang, 2007) and syntax based models (Yamada and
Knight, 2002; Galley et al., 2006; Liu et al., 2006;
Zollmann and Venugopal, 2006). The third type of
approaches, avoid the use of a parser (as required
by syntax based models) and instead train a reorder-
ing model using reference reorderings derived from
aligned data. These approaches (Tromble and Eis-
ner, 2009; Visweswariah et al., 2011; DeNero and
Uszkoreit, 2011; Neubig et al., 2012) have a low de-
code time complexity as reordering is done as a pre-
processing step and not integrated with the decoder.

Our work falls under the third category, as it im-
proves upon the work of (Visweswariah et al., 2011)
which is closely related to the work of (Tromble
and Eisner, 2009) but performs better. The focus
of our work is to use higher order and structural
features (based on segmentation of the source sen-
tence) which are not captured by their model. Some
other works have used collocation based segmenta-
tion (Henrı́quez Q. et al., 2010) and Multiword Ex-
pressions as segments (Bouamor et al., 2012) to im-
prove the performance of SMT but without much
success. The idea of improving performance by re-
ranking a n-best list of outputs has been used re-
cently for the related task of parsing (Katz-Brown et
al., 2011) using targeted self-training for improving
the performance of reordering. However, in contrast,
in our work we directly aim at improving the perfor-
mance of a reordering model.

6 Conclusion

In this work, we proposed a model for re-ranking
the n-best reorderings produced by a state of the
art reordering model (TSP model) which is limited
to pair wise features. Our model uses a more in-
formative set of features consisting of higher order
features, structural features and target side features

322



(captured indirectly using translation scores). The
problem of intractability is solved by restricting the
search space to the n-best reorderings produced by
the TSP model. A detailed ablation test shows that
of all the features used, the pos triplet features are
most informative for reordering. A gain of 1.3 and 3
BLEU points over a state of the art phrase based and
hierarchical machine translation system respectively
provides good extrinsic validation of our claim that
such long range features are useful.

As future work, we would like to evaluate our al-
gorithm on other language pairs. We also plan to
integrate the score assigned by our model into the
decoder to avoid having to do n decodings for ev-
ery source sentence. Also, it would be interesting
to model the segmentation explicitly, where the aim
would be to first segment the sentence and then use
a two level hierarchical reordering model which first
reorders these segments and then reorders the words
within the segment.

References

Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL, ACL-44, pages 529–536, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.

David Applegate, William Cook, and Andre Rohe. 2003.
Chained lin-kernighan for large traveling salesman
problems. In INFORMS Journal On Computing.

Abhishek Arun and Philipp Koehn. 2007. Online
learning methods for discriminative training of phrase
based statistical machine translation. In In Proceed-
ings of MT Summit.

Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2012. Identifying bilingual multi-
word expressions for statistical machine translation.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet Uur Doan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201–228, June.

Michael Collins, Philipp Koehn, and Ivona Kučerová.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,

Morristown, NJ, USA. Association for Computational
Linguistics.

John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 193–203, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 961–968, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Pack-
ing, pro, and paraphrases. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 283–291, Montréal, Canada, June. Association
for Computational Linguistics.

Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ’10,
pages 376–384, Stroudsburg, PA, USA. Association
for Computational Linguistics.

A. Carlos Henrı́quez Q., R. Marta Costa-jussà, Vidas
Daudaravicius, E. Rafael Banchs, and B. José Mariño.
2010. Using collocation segmentation to augment the
phrase table. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, WMT ’10, pages 98–102, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ’05, pages 89–96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for
machine translation reordering. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 183–192,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th

323



Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1–11, Stroudsburg, PA,
USA. Association for Computational Linguistics.

S. Lin and B. W. Kernighan. 1973. An effective heuristic
algorithm for the travelling-salesman problem. Oper-
ations Research, pages 498–516.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 609–616, Stroudsburg,
PA, USA. Association for Computational Linguistics.

J. Scott McCarley, Abraham Ittycheriah, Salim Roukos,
Bing Xiang, and Jian-ming Xu. 2011. A correc-
tion model for word alignments. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’11, pages 889–898,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 843–853, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.

Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In
Proceedings of ACL-IJCNLP.

Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.

Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97–133.

Roy Tromble and Jason Eisner. 2009. Learning linear or-
dering problems for better translation. In Proceedings
of EMNLP.

Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.

Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
the Conference on Empirical Methods in Natural

Language Processing, EMNLP ’11, pages 486–496,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP-CoNLL.

Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned rewrite
patterns. In COLING.

Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proceedings of ACL.

Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.

324


