










































An improved corpus of disease mentions in PubMed citations


Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 91–99,
Montréal, Canada, June 8, 2012. c©2012 Association for Computational Linguistics

An improved corpus of disease mentions in PubMed citations 

 

Rezarta Islamaj Doğan Zhiyong Lu 
National Center for Biotechnology Information National Center for Biotechnology Information 

8600 Rockville Pike 8600 Rockville Pike 

Bethesda, MD 20894, USA Bethesda, MD 20894, USA 
Rezarta.Islamaj@nih.gov Zhiyong.Lu@nih.gov 

 

 

 

 
 

Abstract 

The latest discoveries on diseases and their di-

agnosis/treatment are mostly disseminated in 

the form of scientific publications. However, 

with the rapid growth of the biomedical litera-

ture and a high level of variation and ambigui-

ty in disease names, the task of retrieving 

disease-related articles becomes increasingly 

challenging using the traditional keyword-

based approach. An important first step for 

any disease-related information extraction 

task in the biomedical literature is the disease 

mention recognition task. However, despite 

the strong interest, there has not been enough 

work done on disease name identification, 

perhaps because of the difficulty in obtaining 

adequate corpora. Towards this aim, we creat-

ed a large-scale disease corpus consisting of 

6900 disease mentions in 793 PubMed cita-

tions, derived from an earlier corpus. Our cor-

pus contains rich annotations, was developed 

by a team of 12 annotators (two people per 

annotation) and covers all sentences in a 

PubMed abstract. Disease mentions are cate-

gorized into Specific Disease, Disease Class, 

Composite Mention and Modifier categories. 

When used as the gold standard data for a 

state-of-the-art machine-learning approach, 

significantly higher performance can be found 

on our corpus than the previous one. Such 

characteristics make this disease name corpus 

a valuable resource for mining disease-related 

information from biomedical text. The NCBI 

corpus is available for download at 

http://www.ncbi.nlm.nih.gov/CBBresearch/Fe

llows/Dogan/disease.html. 

1 Introduction 

Identification of biomedical entities has been an 

active area of research in recent years (Rinaldi et 

al., 2011, Smith et al., 2008, Yeh et al., 2005). Au-

tomatic systems, both lexically-based and machine 

learning-based, have been built to identify medi-

cally relevant concepts and/or their relationships. 

Biomedical entity recognition research covers not 

only gene/protein mention recognition (Tanabe et 

al., 2005, Campos et al., 2012), but also other med-

ically relevant concepts such as disease names, 

chemical/drug names, treatments, procedures etc. 

Systems capable of achieving high performance on 

these tasks are highly desirable as entity recogni-

tion precedes all other information extraction and 

text mining tasks.   

Disease information is sought very frequently in 

biomedical search engines. Previous PubMed log 

usage analysis (Islamaj Dogan et al., 2009) has 

shown that disease is the most frequent non-

bibliographic information requested from PubMed 

users. Furthermore, disease information was often 

found to be queried together with Chemical/Drug 

or Gene/Protein information. Automatic recogni-

tion of disease mentions therefore, is essential not 

only for improving retrieval of relevant documents, 

but also for extraction of associations between dis-

eases and genes or between diseases and drugs. 

However, prior research shows that automatic dis-

ease recognition is a challenging task due to varia-

tions and ambiguities in disease names (Leaman et 

al., 2009, Chowdhury and Lavelli 2010).  

Lexically-based systems of disease name recog-

nition, generally refer to the Unified Medical Lan-

guage System (UMLS) (Burgun and Bodenreider 

91



2008). UMLS is a comprehensive resource of med-

ically relevant concepts and relationships and 

METAMAP(Aronson and Lang 2010) is an exam-

ple of a natural language processing (NLP) system 

that provides reliable mapping of the text of a bio-

medical document to UMLS concepts and their 

semantic types.  

Machine learning systems, on the other hand, 

have been employed in order to benefit from the 

flexibility they allow over the rule-based and other 

statistical systems. However, machine learning 

systems are strongly dependent on the data availa-

ble for their training; therefore a comprehensive 

corpus of examples representing as many varia-

tions as possible of the entity of interest is highly 

favorable. 

To our best knowledge, there is one corpus of 

disease mentions in MEDLINE citations developed 

by Leaman et al., 2009. This corpus, AZDC cor-

pus, was inspired by the work of Jimeno et al., 

2008 and its overall characteristics are given in 

Table 1. This corpus has been the study of at least 

two different groups in building automatic systems 

for disease name recognition in biomedical litera-

ture (Leaman et al., 2009, Chowdhury and Lavelli, 

2010). They both reported F-scores around 80% in 

10-fold cross-validation experiments.  

One common encountered difficulty in this do-

main is the fact that “disease” as a category has a 

very loose definition, and covers a wide range of 

concepts. “Disease” is a broadly-used term that 

refers to any condition that causes pain, suffering, 

distress, dysfunction, social problems, and/or 

death. In UMLS, the “disease” concept is covered 

by twelve different semantic types as shown in 

Table 2. The disease definition issue has been dis-

cussed extensively in other studies (Neveol et al., 

2009, Neveol and Lu 2012).   

Disease mentions are also heavily abbreviated in 

biomedical literature (Yeganova et al., 2010). The-

se abbreviations are not always standard; the same 

abbreviated form may represent different defining 

strings in different documents. It is therefore, un-

clear whether these ambiguities could be resolved 

by an abbreviation look-up list from UMLS Me-

tathesaurus and other available databases.  

In this study, we present our efforts in improv-

ing the AZDC corpus by building a richer, broader 

and more complete disease name corpus. The 

NCBI corpus reflects a more representative view 

of what constitutes a disease name as it combines 

the decisions of twelve annotators. It also provides 

four different categories of disease mentions. Our 

work was motivated by the following observations:  

 The need of a pool of experts:  
The AZDC corpus is the work of one annota-

tor. While in terms of consistency this is gen-

erally a good thing, a pool of annotators 

guarantees a more representative view of the 

entity to be annotated and an agreement be-

tween annotators is preferred for categories 

with loose definitions such as “disease”. 

Moreover, this would ensure that there would 

be fewer missed annotations within the corpus.  

 The need of annotating all sentences in a 
document:  

The AZDC corpus has disease mention annota-

tions of selected sentences in a collection of 

PubMed abstracts. In order to be able to per-

form higher level text mining tasks that ex-

plore relationships between diseases and other 

types of information such as genes or drugs, 

the disease name annotation has to include all 

sentences, as opposed to selected ones. 

Our work is also related to other corpus annota-

tion projects in the biomedical domain (Grouin et 

al., 2011, Tanabe at al., 2005, Thompson et al., 

2009, Neveol at al., 2009, Chapman et al., 2012). 

These studies generally agree on the need of multi-

ple experienced annotators for the project, the need 

of detailed annotation guidelines, and the need of 

large scale high-quality annotation corpora. The 

production of such annotated corpora facilitates the 

development and evaluation of entity recognition 

and information extraction systems. 

2 Methods 

Here we describe the NCBI corpus, and its annota-

tion process. We discuss the annotation guidelines 

and how they evolved through the process. 

2.1 The NCBI disease corpus 

The AZDC corpus contains 2,783 sentences cho-

sen from 793 PubMed abstracts. These selected 

Table 1 AZDC corpus characteristics 

Characteristics of the corpus  

Selected abstracts 793 

Sentences 2,783 

Sentences with disease mentions 1,757 

Total disease mentions 3,224 

 

92



sentences were annotated for disease mentions, 

resulting in 1,202 unique mentions and 3,224 total 

mentions. The NCBI corpus starts with this origi-

nal corpus; however, it is expanded to cover all the 

sentences in all the 793 PubMed abstracts. 

2.2 Annotation guidelines 

One fundamental problem in corpus annotation is 

the definition of what constitutes an entity to be 

tagged. Following the lead of the AZDC annota-

tions, the group of annotators working on the 

NCBI corpus decided that a textual string would be 

annotated as a disease mention if it could be 

mapped to a unique concept in the UMLS Me-

tathesaurus, if it corresponded to at least one of the 

semantic types listed in Table 2, and if it contained 

information that would be helpful to physicians 

and health care professionals. 

Annotators were invited to use their common 

knowledge, use public resources of the National 

Library of Medicine such as UMLS or PubMed 

Health, Disease Ontology (Warren et al., 2006) and 

Wikipedia and consider the viewpoint of an aver-

age user trying to find information on diseases. 

Initially, a set of 20 randomly chosen PubMed 

abstracts was used as a practice set for the devel-

opment of annotation guidelines. After each anno-

tator worked individually on the set, the results 

were shared and discussed among all annotators. 

The final annotation guidelines are summarized 

below and also made available at the corpus down-

load website. 

What to annotate? 

1. Annotate all specific disease mentions. 

A textual string referring to a disease name may 

refer to a Specific Disease, or a Disease Class. 

Disease mentions that could be described as a 

family of many specific diseases were annotated 

with an annotation category called Disease 

Class. The annotation category Specific Disease 

was used for those mentions which could be 

linked to one specific definition that does not in-

clude further categorization.  

e.g. <Specific Disease> Diastrophic dysplasia 

</> is an <Disease Class> autosomal recessive 

disease</> characterized by short stature, very 

short limbs and joint problems that restrict mo-

bility. 

2. Annotate contiguous text strings. 
A textual string may refer to two or more sepa-

rate disease mentions. Such mentions are anno-

tated with the Composite Mention category. 

e.g. The text phrase “Duchenne and Becker 

muscular dystrophy” refers to two separate dis-

eases. If this phrase is separated into two strings: 

“Duchenne” and “Becker muscular dystrophy”, 

it results in information loss, because the word 

“Duchenne” on its own is not a disease mention.  

3. Annotate disease mentions that are used as 
modifiers for other concepts 

A textual string may refer to a disease name, but 

it may not be a noun phrase and this is better ex-

pressed with the Modifier annotation category.  

e.g.: Although this mutation was initially de-

tected in four of 33 <Modifier> colorectal can-

cer </> families analysed from eastern England, 

more extensive analysis has reduced the fre-

quency to four of 52 English <Modifier> 

HNPCC </> kindreds analysed. 

4. Annotate duplicate mentions. 

Table 2 The set of UMLS semantic types that collectively cover concepts of the “disease” category 

UMLS sematic types Disease name example 

Acquired Abnormality Hernia, Varicose Veins 

Anatomical Abnormality Bernheim aneurysm,  Fistula of thoracic duct 

Congenital Abnormality Oppenheim's Disease, Ataxia Telangiectasia 

Cell or Molecular Dysfunction Uniparental disomy, Intestinal metaplasia 

Disease or Syndrome   Acute pancreatitis, Rheumatoid Arthritis 

Experimental Model of Disease Collagen-Induced Arthritis, Jensen Sarcoma 

Injury or Poisoning Contusion and laceration of cerebrum 

Mental or Behavioral Dysfunction Schizophrenia, anxiety disorder, dementia 

Neoplastic Process Colorectal Carcinoma, Burkitt Lymphoma 

Pathologic Function Myocardial degeneration, Adipose Tissue Atrophy 

Sign or Symptom Back Pain, Seizures, Skeletal muscle paralysis 

Finding Abnormal or prolonged bleeding time 

 

93



For each sentence in the PubMed abstract and ti-

tle, the locations of all disease mentions are 

marked, including duplicates within the same 

sentence.  

5. Annotate minimum necessary span of text. 
The minimum span of text necessary to include 

all the tokens expressing the most specific form 

of the disease is preferred. For example, in case 

of the phrase “insulin-dependent diabetes melli-

tus”, the disease mention including the whole 

phrase was preferred over its substrings such as 

“diabetes mellitus” or “diabetes”.  

6. Annotate all synonymous mentions.  
Abbreviation definitions such as “Huntington 

disease” (“HD”) are separated into two annotat-

ed mentions. 

What not to annotate?  

1. Do not annotate organism names. 
Organism names such as “human” were exclud-

ed from the preferred mention. Viruses, bacteria, 

and other organism names were not annotated 

unless it was clear from the context that the dis-

ease caused by these organisms is discussed.  

e.g. Studies of biopsied tissue for the presence 

of <Specific Disease> Epstein-Barr virus</> and 

<Specific Disease> cytomegalovirus </> were 

negative.  

2. Do not annotate gender.  
Tokens such as “male” and “female” were only 

included if they specifically identified a new 

form of the disease, for example “male breast 

cancer”.  

3. Do not annotate overlapping mentions. 
For example, the phrase “von Hippel-Lindau 

(VHL) disease” was annotated as one single dis-

ease mention. 

4. Do not annotate general terms.  

Very general terms such as: disease, syndrome, 

deficiency, complications, abnormalities, etc. 

were excluded. However, the terms cancer and 

tumor were retained. 

5. Do not annotate references to biological 
processes.  

For example, terms corresponding to biological 

processes such as “tumorigenesis” or “cancero-

genesis”.  

6. Do not annotate disease mentions inter-
rupted by nested mentions.  

Basically, do not break the contiguous text 

rule. E.g. WT1 dysfunction is implicated in both 

neoplastic (Wilms tumor, mesothelioma, leuke-

mia, and breast cancer) and nonneoplastic (glo-

merulosclerosis) disease. 

In this example, the list of all disease mentions 

includes: “neoplastic disease” and “nonneo-

plastic disease” in addition to the underlined 

mentions. However, they were not annotated in 

our corpus, because other tokens break up the 

phrase. 

2.3 Annotators and the annotation process 

The annotator group consisted of 12 people with 

background in biomedical informatics research and 

experience in biomedical text corpus annotation. 

The 793 PubMed citations were divided into sets 

of 25 PubMed citations each. Every annotator 

worked on 5 or 6 sets of 25 PubMed abstracts. The 

sets were divided randomly among annotators. 

Each set was shared by two people to annotate. To 

avoid annotator bias, pairs of annotators were cho-

sen randomly for each set of 25 PubMed abstracts.  

As illustrated in Figure 1, first, each abstract 

was pre-annotated using our in-house-developed 

CRF disease mention recognizer trained on the 

AZDC corpus. This process involved a 10-fold 

 
Figure 1. The annotation process 
 

94



cross-validation scheme, where all sentences from 

the same PubMed abstract were assigned to the 

same split. The learning was performed on 9-folds 

and then, the PubMed abstracts assigned to the 

10th fold were annotated for disease mentions on a 

sentence-by-sentence basis.  

Annotation Phase I consisted of each pre-

annotated abstract in the corpus being read and 

reviewed by two annotators working independent-

ly. Annotators could agree with the pre-annotation, 

remove it, or adjust its text span. Annotators could 

also add new annotations. After this initial round 

of annotations, a summary document was created 

highlighting the agreement and differences be-

tween two annotators in the annotations they pro-

duced for each abstract. This constituted the end of 

phase I. The pair of annotators working on the 

same set at this stage was given the summary doc-

ument and their own annotations of Phase I. 

In annotation Phase II, each annotator examined 

and edited his or her own annotations by reviewing 

the different annotations reported in the Phase I 

summary document. This resulted in a new set of 

annotations. After this round, a second summary 

document highlighting the agreement and differ-

ences between two annotators was created for each 

pair of annotators to review.  

After phase II, each pair of annotators organized 

meetings where they reviewed, discussed and re-

solved their differences. After these meetings, a 

reconciled set of annotations was produced for 

each PubMed abstract. The final stage of the anno-

tation process consisted of the first author going 

over all annotated segments and ensuring that an-

notations were consistent both in category and in 

text span across different abstracts and different 

annotation sets. For example if the phrase “classi-

cal galactosemia” was annotated in one abstract as 

a Specific Disease mention, all occurrences of that 

phrase throughout the corpus should receive con-

sistent annotation. Identified hard cases were dis-

cussed at a meeting where all annotators were 

present and a final decision was made to reconcile 

differences. The final corpus is available at: 

http://www.ncbi.nlm.nih.gov/CBBresearch/Fellow

s/Dogan/disease.html 

 
Figure 2. NCBI corpus annotation software. Each annotator selects a PubMed ID from the current 

working set, and is directed to this screen. Annotation categories are: Specific Disease (highlighted in 

yellow), Disease Class (green), Composite Mention (blue), or Modifier (purple). To annotate a disease 

mention in text, annotators highlight the phrase and click on the appropriate label on top of the editor 

screen. To delete a disease mention, annotators highlight the phrase and click on the Clear label on top 

of the editor. Annotators can retrieve the last saved version of their annotations for each particular 

document by clicking on “Last Saved” button. Annotators save their work by clicking on Submit but-

ton at the bottom of editor screen.  
 

95



2.4 Annotation software 
Annotation was done using a web interface (the 

prototype of PubTator (Wei et al., 2012)), as 

shown in Figure 2. Each annotator was able to log 

into the system and work independently. The sys-

tem allowed flexibility to make annotations in the 

defined categories, modify annotations, correct the 

text span, delete as well as go back and review the 

process as often as needed. At the end of each an-

notation phase, annotators saved their work, and 

the annotation results were compared to find 

agreement and consistency among annotations.  

2.5 Annotation evaluation metrics  

We measured the annotators’ agreement at phase I 

and II of the annotation process. One way to meas-

ure the agreement between two annotators is to 

measure their observed agreement on the sample of 

annotated items, as specified in Equation (1).  

Agreement statistics are measured for each an-

notator pair, for each shared annotation set. Then, 

for each annotator pair the average agreement sta-

tistic is computed over all annotation sets shared 

between the pair of annotators. The final agree-

ment statistic reflects the average and standard de-

viation computed over all annotator pairs. This is 

repeated for both phases.  

Agreement between two annotators is measured 

on two levels: one, both annotators tag the same 

exact phrase based on character indices as a dis-

ease mention, and two, both annotators tag the 

same exact phrase based on character indices as a 

disease mention of the same category. 

2.6  Application of the NCBI corpus 
To compare the two disease corpora with regard to 

their intended primary use in training and testing 

machine learning algorithms, we performed a 10-

fold cross validation experiment with BANNER 

(Leaman et al, 2009). We evaluated BANNER per-

formance and compared Precision, Recall and F-

score values for BANNER when trained and tested 

on AZDC corpus and the NCBI disease name cor-

pus, respectively. In these experiments, disease 

mentions of all categories were included and are 

discussed in the Results section.  

To compare the effect of improvement in dis-

ease name recognition, the different disease cate-

gory annotations present in the NCBI corpus were 

        
Figure 3 Inter-annotator annotation consistency measured at the span and span-category level 
 

Table 3 The annotation results and corpus characteristics 

Characteristics of the corpus NCBI corpus AZDC 

Annotators 12 1 

Annotated sentences in citation ALL Selected 

PubMed Citations 793 793 

Sentences 6,651 2,784 

Sentences with disease annotations 3,752 1,757 

Total disease mentions 6,900 3,228 

Specific Disease 3,924 - 

Disease Class 1029 - 

Modifier 1,774 - 

Composite Mention 173 - 

 

96



Table 4 NCBI corpus as training, development and testing sets for disease name recognition 

Corpus Characteristics  Training set Development set Test set 

PubMed Citations 593 100 100 

Total disease mentions 5148 791 961 

Specific Disease 2959 409 556 

Disease Class 781 127 121 

Modifier 1292 218 264 

Composite Mention 116 37 20 

 

flattened into only one single category. This made 

the NCBI corpus compatible with the AZDC cor-

pus. 

3 Results and Discussion 
3.1 Results of Inter-Annotator Agreement  

Figure 3 shows the inter-annotator agreement re-

sults after Phase I and Phase II of the annotations. 

These statistics show a good agreement between 

annotators, especially after phase II of annotations. 

In particular, both span-consistency measure and 

span-category consistency measure is above 80% 

after phase II. These values show that our corpus 

reflects a high quality of annotations and that our 

two-stage annotation steps are effective in improv-

ing corpus consistency.  

3.2 Agreement between automatic pre-
annotation and final annotation results 

In our previous work (Neveol et al, 2009) we have 

shown that automatic pre-annotation is found help-

ful by most annotators in assisting large-scale an-

notation projects with regard to speeding up the 

annotation time and improving annotation con-

sistency while maintaining the high quality of the 

final annotations. Thus, we again used pre-

annotation in this work. To demonstrate that hu-

man annotators were not biased towards the com-

puter-generated pre-annotation, we compared the 

final annotation with the pre-annotation results. 

There are a total of 3295 pre-annotated disease 

mentions: 1750 were found also in the final corpus 

while the remaining 1545 were either modified or 

deleted. Furthermore, the final corpus consists of 

additional 3605 new annotations. Overall, the 

agreement between pre-annotation and final anno-

tation results is only 35%. 

3.3 Statistics of the NCBI disease corpus 

After two rounds of annotation, several annotator 

meetings and resolving of inconsistencies, the 

NCBI corpus contains 793 fully annotated PubMed 

citations for disease mentions which are divided 

into these categories: Specific Disease, Disease 

Class, Composite Mention and Modifier. As shown 

in Table 3, the NCBI corpus contains more than 

6K sentences, of which more than half contain dis-

ease mentions. There are 2,161 unique disease 

mentions total, which can be divided into these 

categories: 1,349 unique Specific Disease men-

tions, 608 unique Disease Class mentions, 121 

unique Composite Disease mentions, and 356 

unique Modifier disease mentions. The NCBI dis-

ease name corpus is available for download and 

can be used for development of disease name 

recognition tools, identification of Composite Dis-

ease Mentions, Disease Class or Modifier disease 

mention in biomedical text. 

3.4 Characteristics of the NCBI corpus 
This annotation task was initially undertaken for 

purposes of creating a larger, broader and more 

complete corpus for disease name recognition in 

biomedical literature.  

The NCBI corpus addresses the inconsistencies 

of missed annotations by using a pool of experts 

for annotation and creating the annotation envi-

ronment of multiple discussions and multiple 

rounds of annotation. The NCBI corpus addresses 

the problem of recognition of abbreviated disease 

mentions by delivering annotations for all sentenc-

es in the PubMed abstract. Processing all sentences 

in a document allows for recognition of an abbre-

viated form of a disease name. An abbreviated 

term could be tagged for later occurrences within 

the same document, if an abbreviation definition is 

recognized in one of the preceding sentences.  

NCBI corpus provides a richer level of annota-

tions characterized by four different categories of 

disease mentions: Specific Disease, Disease Class, 

(1) 

s2Annotation1
2100  




sAnnotation

Agreement
yConsistenc  

97



Composite Mention and Modifier. Specific Disease 

mentions could be linked to one specific definition 

without further categorization, allowing for future 

normalization tasks. Composite Disease Mentions 

identify intricate lexical strings that express two or 

more disease mentions, allowing for future natural 

language processing tasks to look at them more 

closely. Modifier disease mentions identify non-

noun phrase mentions, again useful for other text 

mining tasks. 

Finally, the corpus can be downloaded and used 

for development and testing for disease name 

recognition and other tasks. To facilitate future 

work, we have divided the corpus into training, 

development and testing sets as shown in Table 4. 

 

3.5 The NCBI corpus as training data for 
disease mention recognition 

We replicated the BANNER experiments by com-

paring their cross-validation results on the original 

corpus (AZDC) and on the NCBI corpus. Our re-

sults reveal that BANNER achieves significantly 

better performance on the NCBI corpus: a 10% 

increase in F-score from 0.764 to 0.840.  Table 5 

shows detailed results for BANNER processing in 

precision, recall and F-score, for both corpora. 

In addition, we performed BANNER experi-

ments on the newly divided NCBI corpus with the 

following results: BANNER achieves an F-score of 

0.845 on a 10 fold cross-validation experiment on 

the NCBI training set, an F-score of 0.819 when 

tested on the NCBI development set, after trained 

on the NCBI training set, and an F-score of 0.818 

when tested on NCBI test set, after trained on 

NCBI training set.   

 

3.6 Limitations of this work 

The NCBI corpus was annotated manually, thus 

the tags assigned were judgment calls by human 

annotators. Annotation guidelines were established 

prior to the annotation process and they were re-

fined during the annotation process, however grey 

areas still remained for which no explicit rules 

were formulated. In particular, inclusion of qualita-

tive terms as part of the disease mention is a matter 

of further investigation as illustrated by the follow-

ing example:  

 Acute meningococcal pericarditis – Consti-
tutes a disease mention and, exists as a 

separate concept in UMLS, however 

 Acute Neisseria infection – May or may 
not include the descriptive adjective.  

Similarly: 

 Classical galactosemia – Includes the de-
scriptive adjective, because it corresponds 

to a particular form of the disease. 

 Inherited spinocerebellar ataxia – May or 
may not include the descriptive adjective. 

Names containing conjunctions are difficult to 

tag. Although it might seem excessive to require a 

named entity recognizer to identify the whole ex-

pression for cases such as:  

 Adenomatous polyps of the colon and rec-
tum, 

 Fibroepithelial or epithelial hyperplasias, 

 Stage II or stage III colorectal cancer, 
The NCBI disease name corpus rectifies this sit-

uation by annotating them as Composite Mention 

disease name category, thus, allowing for future 

NLP application to develop more precise methods 

in identifying these expressions.  

Moreover, sentences which contained nested 

disease names require further attention, as the cur-

rent annotation rule of annotating only contiguous 

phrases cannot select the outer mentions. 

Finally, our current annotation guideline re-

quires that only one of the four categories be as-

signed to each disease mention. This is not ideal 

because a disease mention may actually fit more 

than one category. For instance, a mention can be 

tagged as both “Modifier” and “Disease Class”. In 

practice, for obtaining consistent annotations, the 

priority was given in the order of “Modifier”, 

“Composite Mention”, “Disease Class”, and “Spe-

cific Disease” when more than one category deems 

appropriate. This aspect should be addressed at 

future work.   

4 Conclusions 
We have described the NCBI disease name corpus 

of tagged disease mentions in 793 PubMed titles 

and abstracts. The corpus was designed to capture 

Table 5 BANNER evaluation results on AZDC 

(original) corpus and on the NCBI corpus. 

CRF-

order 
Corpus Precision Recall F-score 

1 AZDC 0.788 0.743 0.764 

1 NCBI 0.859 0.824 0.840 

2 AZDC 0.804 0.752 0.776 

2 NCBI 0.857 0.820 0.838 

 

98



disease mentions in the most common sense of the 

word, and is particularly relevant for biomedical 

information retrieval tasks that involve diseases. 

Annotations were performed for all sentences in a 

document, facilitating the future applications of 

complex information retrieval tasks connecting 

diseases to treatments, causes or other types of in-

formation. Annotation guidelines were designed 

with the goal of allowing flexible matching to 

UMLS concepts, while retaining true meaning of 

the tagged concept. A more detailed definition on 

what constitutes a disease name, accompanied with 

additional annotation rules, could help resolve 

some existing inconsistencies.  The current corpus 

is reviewed several times by several annotators and 

describes a refined scale of annotation categories. 

It allows the separate definition and annotation of 

Composite mentions, Modifiers and distinguishes 

between Disease Class mentions versus Specific 

Diseases. The corpus is available for download
1
. 

Acknowledgments 

Funding: This research was supported by the Intramural 

Research Program of the NIH, National Library of Med-

icine.  

We sincerely thank Robert Leaman and Graciela Gon-

zalez for their help with BANNER, and the whole team 

of 12 annotators for their time and expertise on annota-

tion of this corpus.   

References  

Aronson, A., Lang, F. 2010. An overview of MetaMap: 

historical perspective and recent advances. J Am Med 

Inform Assoc, 17(3): 229-236. 

Burgun, A., Bodenreider, O. 2008. Accessing and inte-

grating data and knowledge for biomedical research. 

Yearb Med Inform, 91-101. 

Campos, D., Matos, S., Lewin, I., Oliveira, J., Rebholz-

Schuhmann, D. 2012. Harmonisation of gene/protein 

annotations: towards a gold standard MEDLINE. Bi-

oinformatics, 1;28(9):1253-61 

Chapman, W.W., Savova, G.K., Zheng, J., Tharp, M., 

Crowley, R. 2012. Anaphoric reference in clinical re-

ports: Characteristics of an annotated corpus. J Bio-

med Inform  

Chowdhury, F.M., Lavelli, A. 2010. Disease mention 

recognition with specific features. BioNLP, 91-98. 

Grouin, C., Rosset. S., Zweigenbaum, P., Fort, K., Gali-

bert, O., Quintard, L. 2011. Proposal for an extension 

                                                           
1 

http://www.ncbi.nlm.nih.gov/CBBresearch/Fellows/Dogan/dis

ease.html 

of traditional named entities: From guidelines to 

evalua-tion, an overview. 5th law workshop, 92-100.  

Islamaj Dogan, R., Murray, G. C., Neveol, A., Lu, Z. 

2009. Understanding PubMed user search behavior 

through log analysis. Database (Oxford): bap018. 

Jimeno,A., Jimnez-Ruiz, E., Lee, V., Gaudan, S., Ber-

langa,R., Reholz-Schuhmann, D.2008. Assessment of 

disease named entity recognition on a corpus of an-

no-tated sentences. BMC Bioinformatics, 9(S-3). 

Leaman, R., Miller, C., Gonzalez, G. 2009. Enabling 

Recognition of Diseases in Biomedical Text with 

Ma-chine Learning: Corpus and Benchmark. Sympo-

sium on Languages in Biology and Medicine, 82-89.  

Neveol, A., Li, J., Lu, Z. 2012. Linking Multiple Dis-

ease-related resources through UMLS. ACM Interna-

tional Health Informatics. 

Neveol, A., Islamaj Dogan, R., Lu, Z. 2011. Semi-

automatic semantic annotation of PubMed Queries: a 

study on quality, efficiency, satisfaction. J Biomed 

Inform, 44(2):310-8. 

Rinaldi, F., Kaljurand, K., Sætre, R. 2011. Terminologi-

cal resources for text mining over biomedical scien-

tific literature. Artificial intelligence in medicine 

52(2) 

Smith L., Tanabe L.K., Ando R.J., Kuo C.J., Chung I.F., 

Hsu C.N., Lin Y.S., Klinger R., Friedrich C.M., 

Ganchev K., Torii M., Liu H., Haddow B., Struble 

C.A., Povinelli R.J., Vlachos A., Baumgartner W.A. 

Jr., Hunter L., Carpenter B., Tsai R.T., Dai H.J., Liu 

F., Chen Y., Sun C., Katrenko S., Adriaans P., 

Blaschke C., Torres R., Neves M., Nakov P., Divoli 

A., Maña-López M., Mata J., Wilbur W.J. 

2008.Overview of BioCreative II gene mention 

recognition. Genome Biology, 9 Suppl 2:S2. 

Tanabe, L., Xie, N., Thom, L., Matten, W., Wilbur, W.J. 

2005. GENETAG: a tagged corpus for gene /protein 

named entity recognition. BMC Bioinformatics, 6:S3. 

Thompson, P., Iqbal, S.A., McNaught, J., Ananiadou, S. 

2009. Construction of an annotated corpus to support 

biomedical information extraction. BMC Bioinfor-

matics, 10:349. 
Warren A., Kibbe J.D.O., Wolf W.A., Smith M.E., Zhu L., 

Lin S., Chisholm R., Disease Ontology. 2006 

Wei C., Kao, H., Lu, Z., 2012. PubTator: A PubMed-

like interactive curation system for document triage 

and literature Curation. In proceedings of BioCrea-

tive workshop, 145-150.  

Yeganova, L., Comeau, D.C., Wilbur, W.J. 2011. Ma-

chine learning with naturally labeled data for identi-

fying abbreviation definitions. BMC Bioinformatics.  

S3:S6 

Yeh, A., Morgan, A., Colosime, M., Hirschman, L. 

2005. BioCreAtIvE Task 1A: gene mention finding 

evaluation. BMC Bioinformatics, 6(Suppl 1):S2 

99


