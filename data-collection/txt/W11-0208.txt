65

Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 65–73,

Portland, Oregon, USA, June 23-24, 2011. c(cid:13)2011 Association for Computational Linguistics

Automatic Acquisition of Huge Training Data
for Bio-Medical Named Entity Recognition

Yu Usami∗ †

Han-Cheol Cho†

Naoaki Okazaki‡

and Jun’ichi Tsujii§

∗ Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
† Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
‡ Inui Laboratory, Department of System Information Sciences, Tohoku University, Sendai, Japan

§ Microsoft Research Asia, Beijing, China

{yusmi, hccho}@is.s.u-tokyo.ac.jp

okazaki@ecei.tohoku.ac.jp

jtsujii@microsoft.com

Abstract

Named Entity Recognition (NER) is an im-
portant ﬁrst step for BioNLP tasks, e.g., gene
normalization and event extraction. Employ-
ing supervised machine learning techniques
for achieving high performance recent NER
systems require a manually annotated corpus
in which every mention of the desired seman-
tic types in a text is annotated. However, great
amounts of human effort is necessary to build
and maintain an annotated corpus. This study
explores a method to build a high-performance
NER without a manually annotated corpus,
but using a comprehensible lexical database
that stores numerous expressions of seman-
tic types and with huge amount of unanno-
tated texts. We underscore the effectiveness of
our approach by comparing the performance
of NERs trained on an automatically acquired
training data and on a manually annotated cor-
pus.

1 Introduction
Named Entity Recognition (NER) is the task widely
used to detect various semantic classes such as
genes (Yeh et al., 2005), proteins (Tanabe and
Wilbur, 2002), and diseases in the biomedical ﬁeld.
A na´ıve approach to NER handles the task as a
dictionary-matching problem: Prepare a dictionary
(gazetteer) containing textual expressions of named
entities of speciﬁc semantic types. Scan an input
text, and recognize a text span as a named entity if
the dictionary includes the expression of the span.

Although this approach seemingly works well, it
presents some critical issues. First, the dictionary

must be comprehensive so that every NE mention
can be found in the dictionary. This requirement
for dictionaries is stringent because new terminol-
ogy is being produced continuously, especially in
the biomedical ﬁeld. Second, this approach might
suffer from an ambiguity problem in which a dic-
tionary includes an expression as entries for multi-
ple semantic types. For this reason, we must use
the context information of an expression to make
sure that the expression stands for the target seman-
tic type.

Nadeau and Sekine (2007) reported that a strong
trend exists recently in applying machine learning
(ML) techniques such as Support Vector Machine
(SVM) (Kazama et al., 2002; Isozaki and Kazawa,
2002) and Conditional Random Field (CRF) (Set-
tles, 2004) to NER, which can address these issues.
In this approach, NER is formalized as a classiﬁ-
cation problem in which a given expression is clas-
siﬁed into a semantic class or other (non-NE) ex-
pressions. Because the classiﬁcation problem is usu-
ally modeled using supervised learning methods, we
need a manually annotated corpus for training NER
classiﬁer. However, preparing manually annotated
corpus for a target domain of text and semantic types
is cost-intensive and time-consuming because hu-
man experts are needed to reliably annotate NEs in
text. For this reason, manually annotated corpora
for NER are often limited to a speciﬁc domain and
covers a small amount of text.

In this paper we propose a novel method for au-
tomatically acquiring training data for NER from a
comprehensible lexical database and huge amounts
of unlabeled text. This paper presents four contribu-

66

record. We created a dictionary by collecting ofﬁ-
cial gene (protein) names and their synonyms from
the Entrez Gene records. For unlabeled text, we use
the all 2009 release MEDLINE (National Library
of Medicine, 2009) data. MEDLINE consists of
about ten million abstracts covering various ﬁelds of
biomedicine and health. In our study, we focused on
recognizing gene and protein names within biomed-
ical text.

Our process to construct a NER classiﬁer is as fol-
lows: We apply the GENIA tagger (Tsuruoka et al.,
2005) to split the training data into tokens and to at-
tach part of speech (POS) tags and chunk tags. In
this work, tokenization is performed by an external
program that separates tokens by a space, hyphen,
comma, period, semicolon, or colon character. Part
of speech tags present grammatical roles of tokens,
e.g. verbs, nouns, and prepositions. Chunk tags
compose tokens into syntactically correlated seg-
ments, e.g. verb phrases, noun phrases, and preposi-
tional phrases. We use the IOBES notation (Ratinov
and Roth, 2009) to represent NE mentions with label
sequences, thereby NER is formalized as a multi-
class classiﬁcation problem in which a given token
is classiﬁed into IOBES labels. To classify labels of
tokens, we use a linear kernel SVM which applies
the one-vs.-the-rest method (Weston and Watkins,
1999) to extend binary classiﬁcation to multi-class
classiﬁcation. Given the t-th token xt in a sentence,
we predict the label yt,

yt = argmax

y

s(y|xt, yt−1).

In this equation, s(y|xt, yt−1) presents the score
(sum of feature weights) when the token xt is la-
beled y. We use yt−1 (the label of the previous to-
ken) to predict yt, expecting that this feature behaves
as a label bigram feature (also called translation fea-
ture) in CRF. If the sentence consists of x1 to xT , we
repeat prediction of labels sequentially from the be-
ginning (y1) to the end (yT ) of a sentence. We used
LIBLINEAR (Fan et al., 2008) as an SVM imple-
mentation.

Table 1 lists the features used in the classiﬁer
modeled by SVM. For each token (“Human” in the
example of Table 1), we created several features in-
cluding: token itself (w), lowercase token (wl), part
of speech (pos), chunk tag (chk), character pattern of

Figure 1: Example of an Entrez Gene record.

tions:

1. We show the ineffectiveness of a na´ıve
dictionary-matching for acquiring a training
data automatically and the signiﬁcance of the
quality of training data for supervised NERs

2. We explore the use of reference information
that bridges the lexical database and unlabeled
text for acquiring high-precision and low-recall
training data

3. We develop two strategies for expanding NE
annotations, which improves the recall of the
training data

4. The proposed method acquires a large amount
of high-quality training data rapidly, decreasing
the necessity of human efforts

2 Proposed method

The proposed method requires two resources to ac-
quire training data automatically: a comprehen-
sive lexical database and unlabeled texts for a tar-
get domain. We chose Entrez Gene (National Li-
brary of Medicine, 2005) as the lexical database be-
cause it provides rich information for lexical entries
and because genes and proteins constitute an im-
portant semantic classes for Bio NLP. Entrez Gene
consists of more than six million gene or protein
records, each of which has various information such
as the ofﬁcial gene (protein) name, synonyms, or-
ganism, description, and human created references.
Figure 1 presents an example of an Entrez Gene

Gene or Protein name

Ofﬁcial name

Aliases

References

67

Name

w
wl
pos
chk
shape
shaped
type
pn(n = 1...4)
sn(n = 1...4)

Description

token
token in small letters
part of speech
chunk tag
entity pattern
entity pattern 2
token type
preﬁx n characters
sufﬁx n characters

Example Value

Human
human
NNP
B-NP
ULLLL
UL
InitCap
(H,Hu,Hum,Huma)
(n,an,man,uman)

Table 1: Example of features used in machine learning
process.

Method

dictionary matching
trained on acquired data

A

92.09
85.76

P

39.03
10.18

R

42.69
23.83

F1
40.78
14.27

Table 2: Results of the preliminary experiment.

(a) It is clear that in culture media of AM,
cystatin C and cathepsin B are present as
proteinase–antiproteinase complexes.

(b) Temperature in the puerperium is higher

in AM, and lower in PM.

token (shape), character pattern designated (shaped),
token type (type), preﬁxes of length n (pn), and suf-
ﬁxes of length n (sn). More precisely, the character
pattern of token (shape) replaces each character in
the token with either an uppercase letter (U), a low-
ercase letter (L), or a digit (D). The character pat-
tern designated (shaped) is similar to a shape feature,
but the consecutive character types are reduced to
one symbol, for example, “ULLLL” (shape) is rep-
resented with “UL” (shaped) in the example of Ta-
ble 1). The token type (type) represents whether the
token satisﬁes some conditions such as “begins with
a capital letter”, “written in all capitals”, “written
only with digits”, or “contains symbols”. We created
unigram features and bigram features (excluding wl,
pn, sn) from the prior 2 to the subsequent 2 tokens
of the current position.

2.1 Preliminary Experiment
As a preliminary experiment, we acquired training
data using a na´ıve dictionary-matching approach.
We obtained the training data from all 2009 MED-
LINE abstracts with an all gene and protein dictio-
nary in Entrez Gene. The training data consisted of
nine hundred million tokens. We constructed a NER
classiﬁer using only four million tokens of the train-
ing data because of memory limitations. For evalua-
tion, we used the Epigenetics and Post-translational
Modiﬁcation (EPI) corpus BioNLP 2011 Shared
Task (SIGBioMed, 2011). Only development data
and training data are released as the EPI corpus at
present, we used both of the data sets for evalua-
tion in this experiment. Named entities in the corpus
are annotated exhaustively and belong to a single se-
mantic class, Gene or Gene Product (GGP) (Ohta
et al., 2009). We evaluated the performance of the

Figure 2: Dictionary-based gene name annotating exam-
ple (annotated words are shown in italic typeface).

NER on four measures: Accuracy (a), Precision (P),
Recall (R), and F1-measure (F1). We used the strict
matching criterion that a predicted named entity is
correct if and only if the left and the right bound-
aries are both correct.

Table 2 presents the evaluation results of this ex-
periment. The ﬁrst model “dictionary matching”
performs exact dictionary-matching on the test cor-
pus. It achieves a 40.78 F1-score. The second model
“trained on acquired data” uses the training data
acquired automatically for constructing NER clas-
siﬁer.
It scores very low-performance (14.27 F1-
score), even compared with the simple dictionary-
matching NER. Exploring the annotated training
data, we investigate why this machine learning ap-
proach shows extremely low performance.

Figure 2 presents an example of the acquired
training data. The word “AM” in the example (a)
is correct because it is gene name, although “AM”
in the example (b) is incorrect because “AM” in (b)
is the abbreviation of ante meridiem, which means
before noon. This is a very common problem, espe-
cially with abbreviations and acronyms. If we use
this noisy training data for learning, then the result
of NER might be low because of such ambiguity. It
is very difﬁcult to resolve errors in the training data
even with the help of machine learning methods.

2.2 Using Reference Information
To obtain high-precision data, we used reference in-
formation included with each record in Entrez Gene.
Figure 3 portrays a simple example of reference in-
formation. It shows the reference information of the

68

• ... in the following order: tna, gltC, gltS,
pyrE; gltR is located near ...
• The three genes concerned (designated
entA, entB and entC) ...
• Within the hypoglossal nucleus large
amounts of acetylcholinesterase (AChE)
activity are ...

Figure 4: False negative examples.

2.3 Training Data Expansion

In the previous section, we were able to obtain train-
ing data with high-precision by exploiting reference
information in the Entrez Gene. However, the result-
ing data include many false negatives (low-recall),
meaning that correct gene names in the data are
unannotated. Figure 4 presents an example of miss-
ing annotation.
In this ﬁgure, all gene mentions
are shown in italic typeface. The underlined en-
tities were annotated by using the method in Sec-
tion 2.2, because they were in the Entrez Gene dic-
tionary and this MEDLINE abstract was referred by
these entities. However, the entities in italic type-
face with no underline were not annotated, because
these gene names in Entrez Gene have no link to
this MEDLINE abstract. Those expressions became
false negatives and became noise for learning. This
low-recall problem occurred because no guarantee
exists of exhaustiveness in Entrez Gene reference in-
formation.

To improve the low-recall while maintaining
high-precision, we focused on coordination struc-
tures. We assumed that coordinated noun phrases
belong to the same semantic class. Figure 5 portrays
the algorithm for the annotation expansion based
on coordination analysis. We expanded training
data annotation using this coordination analysis al-
gorithm to improve annotation recall. This algo-
rithm analyzes whether the words are reachable or
not through coordinate tokens such as “,”, “.”, or
“and” from initially annotated entities. If the words
are reachable and their entities are in the Entrez
Gene records (ignoring reference information), then
they are annotated.

Figure 3: Reference to MEDLINE abstract example.

Entrez Gene record which describes that the gene
“AM”. The reference information indicates PMIDs
in which the gene or protein is described.

We applied the rule whereby we annotated a
dictionary-matching in each MEDLINE abstract
only if they were referred by the Entrez Gene
records. Figure 3 shows that the gene “AM” has
reference to the MEDLINE abstract #1984484 only.
Using this reference information between the En-
trez Gene record “AM” and the MEDLINE abstract
#1984484, we can annotate the expansion “AM” in
MEDLINE abstract #1984484 only. In this way, we
can avoid incorrect annotation such as example b in
Figure 2.

We acquired training data automatically using ref-

erence information, as follows:

1. Construct a gene and protein dictionary includ-
ing ofﬁcial names, synonyms and reference in-
formation in Entrez Gene

2. Apply a dictionary-matching on the all MED-

LINE abstracts with the dictionary

3. Annotate the MEDLINE abstract only if it was
referred by the Entrez Gene records which de-
scribe the matched expressions

We obtained about 48,000,000 tokens of training
data automatically by using this process using all the
2009 MEDLINE data. This training data includes
about 3,000,000 gene mentions.

Entrez Gene Records

MEDLINE Abstracts

Reference

 PMID 1984484: 
 It is clear that in culture media of AM, 
cystatin C and cathepsin B are present as 
proteinase-antiproteinase complexes.

Gene: AM

 PMID 23456:
 Temperature in puerperium is higher in AM, 
lower in PM.

69

Input: Sequence of sentence tokens S, Set of
symbols and conjunctions C, Dictionary with-
out reference D, Set of annotated tokens A
Output: Set of Annotated tokens A

Input: Labeled training data D, Machine
learning algorithm A,
Iteration times n,
Threshold θ
Output: Training data Tn

begin
for i = 1 to |S| do
if S[i] ∈ A then

j ← i − 2
while 1 ≤ j ≤ |S| ∧ S[j] ∈ D ∧ S[j] /∈
A ∧ S[j + 1] ∈ C do
A ← A ∩ {S[j]}
j ← j − 2

end while
j ← i + 2
while 1 ≤ j ≤ |S| ∧ S[j] ∈ D ∧ S[j] /∈
A ∧ S[j − 1] ∈ C do
A ← A ∩ {S[j]}
j ← j + 2

end while

end if
end for
Output A
end

begin
T0 ← A seed data from D
i ← 0
D ← D\T0
while i 6= n do
Mi ← Construct model with Ti
U ← Sample some amount of data from D
L ← Annotate U with model Mi
Unew ← Merge U with L if their conﬁdence
values are larger than θ
Ti+1 ← Ti ∪ Unew
D ← D\U
i ← i + 1
end while
Output Tn
end

Figure 6: Self-training algorithm.

Figure 5: Coordination analysis algorithm.

2.4 Self-training
The method described in Section 2.3 reduces false
negatives based on coordination structures. How-
ever, the training data contain numerous false neg-
atives that cannot be solved through coordination
analysis. Therefore, we used a self-training algo-
rithm to automatically correct the training data. In
general, a self-training algorithm obtains training
data with a small amount of annotated data (seed)
and a vast amount of unlabeled text, iterating this
process (Zadeh Kaljahi, 2010):

1. Construct a classiﬁcation model from a seed,

then apply the model on the unlabeled text.

2. Annotate recognized expressions as NEs.

3. Add the sentences which contain newly anno-

tated expressions to the seed.

In this way, a self-training algorithm obtains a huge
amount of training data.

In contrast, our case is that we have a large
amount of training data with numerous false neg-
atives. Therefore, we adapt a self-training algo-
rithm to revise the training data obtained using the
method described in Section 2.3. Figure 6 shows
the algorithm. We split the data set (D) obtained in
Section 2.3 into a seed set (T0) and remaining set
(D\T0). Then, we iterate the cycle (0 ≤ i ≤ n):
1. Construct a classiﬁcation model (Mi) trained

on the training data (Ti).

2. Sample some amount of data (U) from the re-

maining set (D).

3. Apply the model (Mi) on the sampled data (U).

4. Annotate entities (L) recognized by this model.

5. Merge newly annotated expressions (L) with
expressions annotated in Section 2.3 (U) if
their conﬁdence values are larger than a thresh-
old (θ).

6. Add the merged data (Unew) to the training data

(Ti).

70

In this study, we prepared seed data of 683,000 to-
kens (T0 in Figure 6). In each step, 227,000 tokens
were sampled from the remaining set (U).

Because the remaining set U has high precision
and low recall, we need not revise NEs that were
annotated in Section 2.3. It might lower the qual-
ity of the training data to merge annotated entities,
thus we used conﬁdence values (Huang and Riloff,
2010) to revise annotations. Therefore, we retain the
NE annotations of the remaining set U and overwrite
a span of a non-NE annotation only if the current
model predicts the span as an NE with high conﬁ-
dence. We compute the conﬁdence of the prediction
(f(x)) which a token x is predicted as label y as,

f(x) = s(x, y) − max(∀z6=ys(x, z)).

Here, s(x, y) denotes the score (the sum of feature
weights) computed using the SVM model described
in the beginning of Section 2. A conﬁdence score
presents the difference of scores between the pre-
dicted (the best) label and the second-best label. The
conﬁdence value is computed for each token label
prediction.
If the conﬁdence value is greater than
a threshold (θ) and predicted as an NE of length 1
token (label S in IOBES notation), then we revise
the NE annotation. When a new NE with multiple
tokens (label B, I, or E in IOBES notation) is pre-
dicted, we revise the NE annotation if the average
of conﬁdence values is larger than a threshold (θ).
If a prediction suggests a new entity with multiple
tokens xi, ..., xj, then we calculate the average of
conﬁdence values as

f(xi, ..., xj) =

1

j − i + 1

f(xk).

j∑

k=i

The feature set presented in the beginning of Sec-
tion 2 uses information of the tokens themselves.
These features might overﬁt the noisy seed set, even
if we use regularization in training. Therefore, when
we use the algorithm of Figure 6, we do not gen-
erate token (w) features from tokens themselves but
only from tokens surrounding the current token. In
other words, we hide information from the tokens of
an entity, and learn models using information from
surrounding words.

Method

dictionary matching
svm
+ reference
+ coordination
+ self-training

A

92.09
85.76
93.74
93.97
93.98

P

39.03
10.18
69.25
66.79
63.72

R

42.69
23.83
39.12
47.44
51.18

F1
40.78
14.27
50.00
55.47
56.77

Table 3: Evaluation results.

3 Experiment

The training data automatically generated using the
proposed method have about 48,000,000 tokens and
3,000,000 gene mentions. However, we used only
about 10% of this data because of the computational
cost. For evaluation, we chose to use the BioNLP
2011 Shared Task EPI corpus and evaluation mea-
sures described in Section 2.1.

3.1 Evaluation of Proposed Methods
In the previous section, we proposed three methods
for automatic training data acquisition. We ﬁrst in-
vestigate the effect of these methods on the perfor-
mance of NER. Table 3 presents evaluation results.
The ﬁrst method “dictionary matching” simply
performs exact string matching with the Entrez Gene
dictionary on the evaluation corpus.
It achieves a
40.78 F1-measure; this F1-measure will be used as
the baseline performance. The second method, as
described in Section 2.1, “svm” uses training data
generated automatically from the Entrez Gene and
unlabeled texts without reference information of the
Entrez Gene. The third method, “+ reference” ex-
ploits the reference information of the Entrez Gene.
This method drastically improves the performance.
As shown in Table 3, this model achieves the highest
precision (69.25%) with comparable recall (39.12%)
to the baseline model with a 50.00 F1-measure. The
fourth method, “+ coordination”, uses coordination
analysis results to expand the initial automatic an-
notation. Compared to the “+ reference” model, the
annotation expansion based on coordination analy-
sis greatly improves the recall (+8.32%) with only
a slight decrease of the precision (-2.46%). The
last method “+ self-training” applies a self-training
technique to improve the performance further. This
model achieves the highest recall (51.18%) among
all models with a reasonable cost in the precision.

71

Figure 7: Results of self-training.

Figure 8: Manual annotation vs. our method.

To analyze the effect of self-training, we evalu-
ated the performance of this model for each itera-
tion. Figure 7 shows the F1-measure of the model
as iterations increase. The performance improved
gradually. It did not converge even for the last iter-
ation. The size of the training data at the 17th itera-
tion was used in Table 3 experiment. It is the same
to the size of the training data for other methods.

3.2 Comparison with a Manually Annotated

Corpus

NER systems achieving state-of-the-art performance
are based mostly on supervised machine learn-
ing trained on manually annotated corpus.
In
this section, we present a comparison of our best-
performing NER model with a NER model trained
on manually annotated corpus.
In addition to the
performance comparison, we investigate how much
manually annotated data is necessary to outperform
our best-performing system. In this experiment, we
used only the development data for evaluation be-
cause the training data are used for training the NER
model.

We split the training data of EPI corpus randomly
into 20 pieces and evaluated the performance of
the conventional NER system as the size of manu-
ally annotated corpus increases. Figure 8 presents
the evaluation results. The performance of our our
best-performing NER is a 62.66 F1-measure; this
is shown as horizontal line in Figure 8. The NER
model trained on the all training data of EPI cor-

pus achieves a 67.89 F1-measure. The result shows
that our best-performing models achieve compara-
ble performance to that of the NER model when us-
ing about 40% (60,000 tokens, 2,000 sentences) of
the manually annotated corpus.

3.3 Discussion
Although the proposed methods help us to obtain
training data automatically with reasonably high
quality, we found some shortcomings in these meth-
ods. For example, the annotation expansion method
based on coordination analysis might ﬁnd new enti-
ties in the training data precisely. However, it was
insufﬁcient in the following case.

tna loci, in the following order: tna, gltC,
gltS, pyrE; gltR is located near ...

In this example, all gene mentions are shown in
italic typeface. The words with underline were ini-
tial annotation with reference information. The sur-
rounding words represented in italic typeface are an-
notated by annotation expansion with coordination
analysis. Here, the ﬁrst word “tna” shown in italic
typeface in this example is not annotated, although
its second mention is annotated at the annotation ex-
pansion step. We might apply the one sense per dis-
course (Gale et al., 1992) heuristic to label this case.
Second, the improvement of self-training tech-
niques elicited less than a 1.0 F1-measure. To as-
certain the reason for this small improvement, we
analyzed the distribution of entity length both origi-

72

Muramoto et al. (2010) attempted to create train-
ing data from Wikipedia as a lexical database and
blogs as unlabeled text. It collected about one mil-
lion entities from these sources, but they did not re-
port the performance of the NER in their paper.

5 Conclusions

This paper described an approach to the acquisi-
tion of huge amounts of training data for high-
performance Bio NER automatically from a lexical
database and unlabeled text. The results demon-
strated that
the proposed method outperformed
dictionary-based NER. Utilization of reference in-
formation greatly improved its precision. Using co-
ordination analysis to expand annotation increased
recall with slightly decreased precision. Moreover,
self-training techniques raised recall. All strategies
presented in the paper contributed greatly to the
NER performance.

We showed that

the self-training algorithm
skewed the length distribution of NEs. We plan
to improve the criteria for adding NEs during self-
training. Although we obtained a huge amount of
training data by using the proposed method, we
could not utilize all of acquired training data be-
cause they did not ﬁt into the main memory. A fu-
ture direction for avoiding this limitation is to em-
ploy an online learning algorithm (Tong and Koller,
2002; Langford et al., 2009), where updates of fea-
ture weights are done for each training instance. The
necessity of coordination handling and self-training
originates from the insufﬁciency of reference infor-
mation in the lexical database, which was not de-
signed to be comprehensive. Therefore, establish-
ing missing reference information from a lexical
database to unlabeled texts may provide another so-
lution for improving the recall of the training data.

References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classiﬁcation. Journal of Ma-
chine Learning Research, 9:1871–1874.

William A. Gale, Kenneth W. Church, and David
In Pro-
Yarowsky. 1992. One sense per discourse.
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233–237.

Figure 9: Distribution of entity length.

nally included entities and newly added entities dur-
ing self-training, as shown in Figure 9. They repre-
sent the ratio of entity length to the number of total
entities. Figure 9 shows the added distribution of
entity length (Added) differs from the original one
(Original). Results of this analysis show that self-
training mainly annotates entities of the length one
and barely recognizes entities of the length two or
more. It might be necessary to devise a means to fol-
low the corpus statistics of the ratio among the num-
ber of entities of different length as the self-training
iteration proceeds.

4 Related Work
Our study focuses mainly on achieving high per-
formance NER without manual annotation. Several
previous studies aimed at reducing the cost of man-
ual annotations.

Vlachos and Gasperin (2006) obtained noisy
training data from FlyBase1 with few manually an-
notated abstracts from FlyBase. This study sug-
gested the possibility of acquiring high-quality train-
ing data from noisy training data.
It used a boot-
strapping method and a highly context-based classi-
ﬁers to increase the number of NE mentions in the
training data. Even though the method achieved a
high-performance NER in the biomedical domain, it
requires curated seed data.

Whitelaw et al. (2008) attempted to create ex-
tremely huge training data from the Web using a
seed set of entities and relations. In generating train-
ing data automatically, this study used context-based
tagging. They reported that quite a few good re-
sources (e.g., Wikipedia2) listed entities for obtain-
ing training data automatically.

1http://flybase.org/
2http://www.wikipedia.org/

Length 1

Length 2

Length 3

More than 4

Original

Added

0%

25%

50%

75%

100%

73

Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun ’ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, volume 3746, pages 382–392.

Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
In Proceedings of the HLT-
the biomedical domain.
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138–145.

Jason Weston and Chris Watkins. 1999. Support vec-
In

tor machines for multi-class pattern recognition.
ESANN’99, pages 219–224.

Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,
and Lyle Ungar. 2008. Web-scale named entity recog-
nition. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 123–
132.

Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. Biocreative task 1a: gene
BMC Bioinformatics,
mention ﬁnding evaluation.
6(1):S2.

Rasoul Samad Zadeh Kaljahi.

2010. Adapting self-
training for semantic role labeling. In Proceedings of
the ACL 2010 Student Research Workshop, pages 91–
96.

Ruihong Huang and Ellen Riloff.

Inducing
domain-speciﬁc semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275–285.

2010.

Hideki Isozaki and Hideto Kazawa. 2002. Efﬁcient sup-
port vector classiﬁers for named entity recognition. In
Proceedings of the 19th international conference on
Computational linguistics - Volume 1, pages 1–7.

Jun’ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun’ichi Tsujii. 2002. Tuning support vector ma-
chines for biomedical named entity recognition.
In
Proceedings of the ACL-02 workshop on Natural lan-
guage processing in the biomedical domain - Volume
3, pages 1–8.

John Langford, Lihong Li, and Tong Zhang.

2009.
Sparse online learning via truncated gradient. J. Mach.
Learn. Res., 10:777–801.

Hideki Muramoto, Nobuhiro Kaji, Naoki Suenaga, and
Masaru Kitsuregawa. 2010. Learning semantic cat-
In The Fifth NLP
egory tagger from unlabeled data.
Symposium for Yung Researchers. (in Japanese).

David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classiﬁcation. Lingvisti-
cae Investigationes, 30(1):3–26.

National Library of Medicine. 2005. Entrez Gene. avail-

able at http://www.ncbi.nlm.nih.gov/gene.

National Library of Medicine. 2009. MEDLINE. avail-

able at http://www.ncbi.nlm.nih.gov/.

Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun’ichi Tsujii.
Incorporating
genetag-style annotation to genia corpus. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 106–107.

2009.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition.
In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 147–155.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random ﬁelds and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications, pages 104–107.

SIGBioMed.

2011.

BioNLP 2011 Shared Task.

http://sites.google.com/site/bionlpst/.

Lorraine K. Tanabe and W. John Wilbur. 2002. Tagging
gene and protein names in biomedical text. Bioin-
formatics/computer Applications in The Biosciences,
18:1124–1132.

Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
siﬁcation. J. Mach. Learn. Res., 2:45–66.

