










































A Constrained Latent Variable Model for Coreference Resolution


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

A Constrained Latent Variable Model for Coreference Resolution

Kai-Wei Chang Rajhans Samdani Dan Roth
University of Illinois at Urbana-Champaign

{kchang10|rsamdan2|danr}@illinois.edu

Abstract

Coreference resolution is a well known clus-
tering task in Natural Language Processing. In
this paper, we describe the Latent Left Linking
model (L3M), a novel, principled, and linguis-
tically motivated latent structured prediction
approach to coreference resolution. We show
that L3M admits efficient inference and can be
augmented with knowledge-based constraints;
we also present a fast stochastic gradient based
learning. Experiments on ACE and Ontonotes
data show that L3M and its constrained ver-
sion, CL3M, are more accurate than several
state-of-the-art approaches as well as some
structured prediction models proposed in the
literature.

1 Introduction

Coreference resolution is a challenging task, that in-
volves identification and clustering of noun phrases
mentions that refer to the same real-world entity.
Most machine learning approaches to coreference
resolution learn a scoring function to estimate the
compatibility between two mentions or two sets of
previously clustered mentions. Then, a decoding al-
gorithm is designed to aggregate these scores and
find an optimal clustering assignment.

The most popular of these frameworks is the pair-
wise mention model (Soon et al., 2001; Ng and
Cardie, 2002; Bengtson and Roth, 2008), which
learns a compatibility score of mention-pairs and
uses these pairwise scores to obtain a global cluster-
ing. Recently, efforts have been made (Haghighi and
Klein, 2010; Rahman and Ng, 2011b; Rahman and
Ng, 2011c) to consider models that capture higher
order interactions, in particular, between mentions

and previously identified entities (that is, between
mentions and clusters). While such models are po-
tentially more expressive, they are largely based on
heuristics to achieve computational tractability.

This paper focuses on a novel and principled ma-
chine learning framework that pushes the state-of-
the-art while operating at a mention-pair granularity.
We present two models — the Latent Left-Linking
Model (L3M), and a version of that is augmented
with domain knowledge-based constraints, the Con-
strained Latent Left-Linking Model (CL3M). L3M
admits efficient inference, linking each mention to a
previously occurring mention to its left, much like
the existing best-left-link inference models (Ng and
Cardie, 2002; Bengtson and Roth, 2008). How-
ever, unlike previous best-link techniques, learning
in our case is performed jointly with decoding — we
present a novel latent structural SVM approach, op-
timized using a fast stochastic gradient-based tech-
nique. Furthermore, we present a probabilistic gen-
eralization of L3M that is more expressive in that
it is capable of considering mention-entity interac-
tions using scores at the mention-pair granularity.
We augment this model with a temperature-like pa-
rameter (Samdani et al., 2012) to provide additional
flexibility.

CL3M augments L3M with knowledge-based
constraints following (Roth and Yih, 2004; Denis
and Baldridge, 2007). This capability is very de-
sirable as shown by the success of the rule-based de-
terministic approach of Raghunathan et al. (2010)
in the CoNLL shared task 2011 (Pradhan et al.,
2011). In L3M, domain-specific constraints are in-
corporated into learning and inference in a straight-
forward way. CL3M scores a mention’s contribution
to its cluster by combining the corresponding score

601



of the underlying L3M model with that from a set of
constraints.

Most importantly, in our experiments on bench-
mark coreference datasets, we show that CL3M,
with just five constraints, compares favorably with
other, more complicated, state-of-the-art algorithms
on a variety of evaluation metrics. Over-
all, the main contribution of this paper is a
principled machine learning model operating at
mention-pair granularity, using easy to implement
constraint-augmented inference and learning, that
yields competitive results on coreference resolution
on Ontonotes-5.0 (Pradhan et al., 2012) and ACE
2004 (NIST, 2004).

2 Related Work

The idea of Latent Left-linking Model (L3M) is in-
spired by a popular inference approach to corefer-
ence which we call the Best-Left-Link approach (Ng
and Cardie, 2002; Bengtson and Roth, 2008). In the
best-left-link strategy, each mention i is connected
to the best antecedent mention j with j < i (i.e. a
mention occurring to the left of i, assuming a left-
to-right reading order), thereby creating a left-link.
The “best” antecedent mention is the one with the
highest pairwise score, wij ; furthermore, if wij is
below some threshold, say 0, then i is not connected
to any antecedent mention. The final clustering is
a transitive closure of these “best” links. The intu-
ition behind best-left-link strategy is based on how
humans read and decipher coreference links – they
mostly rely on information to the left of the men-
tion when deciding whether to add it to a previously
constructed cluster or not. This strategy has been
successful and commonly used in coreference res-
olution (Ng and Cardie, 2002; Bengtson and Roth,
2008; Stoyanov et al., 2009). However, most works
have developed ad-hoc approaches to implement this
idea. For instance, Bengtson and Roth (2008) train
a model w on binary training data generated by tak-
ing for each mention, the closest antecedent corefer-
ent mention as a positive example, and all the other
mentions as negative examples. Similar approaches
to training and, additionally, decoupling the training
stage from the clustering stage were used by other
systems. In this paper, we formalize the learning
problem of the best-left-link model as a structured

prediction problem and analyze our system with de-
tailed experiments. Furthermore, we generalize this
approach by considering multiple pairwise left-links
instead of just the best link, efficiently capturing the
notion of a mention-to-cluster link.

Many techniques in the coreference literature
break away from the mention pair-based, best-left-
link paradigm. Denis and Baldridge (2008) and Ng
(2005) learn a local ranker to rank the mention
pairs based on their compatibility. While these ap-
proaches achieve decent empirical performance, it
is unclear why these are the right ways to train the
model. Some techniques consider a more expres-
sive model by using features defined over mention-
cluster or cluster-cluster (Rahman and Ng, 2011c;
Stoyanov and Eisner, 2012; Haghighi and Klein,
2010). For these models, the inference and learn-
ing algorithms are usually complicated. Very re-
cently, Durrett et al. (2013) propose a probabilis-
tic model which enforces structural agreement con-
straints between specified properties of mention
cluster when using a mention-pair model. This ap-
proach is very related to the probabilistic extension
of our method as both models attempt to leverage
entity-level information from mention-pair features.
However, our approach is simpler because it directly
considers the probabilities of multiple links. Fur-
thermore, while their model performs only slightly
better than the Stanford rule-based system (Lee et
al., 2011), we significantly outperform this system.
Most importantly, our model obtains state-of-the-art
performance on OntoNotes-5.0 while still operating
at the mention-pair granularity. We believe that this
is due to our novel and principled structured predic-
tion framework which results in accurate (and effi-
cient) training.

Several structured prediction techniques have
been applied to coreference resolution in the ma-
chine learning literature. For example, McCallum
and Wellner (2003) and Finley and Joachims (2005)
model coreference as a correlational clustering prob-
lem (Bansal et al., 2002) on a complete graph over
the mentions with edge weights given by the pair-
wise classifier. However, correlational clustering is
known to be NP Hard (Bansal et al., 2002); nonethe-
less, an ILP solver or an approximate inference algo-
rithm can be used to solve this problem. Another ap-
proach proposed by Yu and Joachims (2009) formu-

602



lates coreference with latent spanning trees. How-
ever, their approach has no directionality between
mentions, whereas our latent structure captures the
natural left-to-right ordering of mentions. In our
experiments (Sec. 5), we show that our technique
vastly outperforms both the spanning tree and the
correlational clustering techniques. We also com-
pare with (Fernandes et al., 2012) and the pub-
licly available Stanford coreference system (Raghu-
nathan et al., 2010; Lee et al., 2011), a state-of-the-
art rule-based system.

Finally, some research (Ratinov and Roth, 2012;
Bansal and Klein, 2012; Rahman and Ng, 2011a)
has tried to integrate world knowledge from web-
based statistics or knowledge bases into a corefer-
ence system. World knowledge is potentially use-
ful for resolving coreference and can be injected
into our system in a straightforward way via the
constraints framework. We will show an example
of incorporating our system with name-entity and
WordNet-based similarity metric (Q. Do, 2009) in
Sec. 5. Including massive amount of information
from knowledge resources is not the focus of this
paper and may distort the comparison with other
relevant models but our results indicate that this is
doable in our model, and may provide significant
improvements.

3 Latent Left-Linking Model with
Constraints

In this section, we describe our Constrained Latent
Left-Linking Model (CL3M). CL3M is inspired by
a few ideas from the literature: (a) the popular Best-
Left-Link inference approach to coreference (Ng and
Cardie, 2002; Bengtson and Roth, 2008), and (b) the
injection of domain knowledge-based constraints for
structured prediction (Roth and Yih, 2004; Clarke
and Lapata, 2006; Chang et al., 2012b; Ganchev et
al., 2010; Koo et al., 2010; Pascal and Baldridge,
2009).

We first introduce the notion of a pairwise
mention-scorer, then introduce our Left-Linking
Model (L3M), and finally describe how to inject con-
straints into our model.

Let d be a document with md mentions. Mentions
are denoted solely using their indices, ranging from
1 to md. A coreference clustering C for document

d is a collection of disjoint sets partitioning the set
{1, . . . , md}. We represent C as a binary function
with C(i, j) = 1 if mentions i and j are coreferent,
otherwise C(i, j) = 0. Let s(C;w, d) be the score
of a given clustering C for a given document and a
given pairwise weight vector w. Then, during infer-
ence, a clustering C is predicted by maximizing the
scoring function s(C;w, d), over all valid (i.e. sat-
isfying symmetry and transitivity) clustering binary
functions C : {1, . . . , md}×{1, . . . , md} → {0, 1}.

3.1 Mention Pair Scorer

We model the task of coreference resolution using a
pairwise scorer which indicates the compatibility of
a pair of mentions. The inference routine then pre-
dicts the final clustering — a structured prediction
problem — using these pairwise scores.

Specifically, for any two mentions i and j (w.l.o.g.
j < i), we produce a pairwise compatibility score
wji using extracted features φ(j, i) as

wji = w · φ(j, i) , (1)

where w is a weight parameter that is learned.

3.2 Latent Left-Linking Model

Our inference algorithm is inspired by the best-left-
link approach. In particular, the score s(C; d,w) is
defined so that each mention links to the antecedent
mention (to its left) with the highest score (as long
as the score is above some threshold, say, 0). Specif-
ically:

s(C; d,w) =

md
∑

i=1

max
0≤j<i,C(i,j)=1

w · φ(j, i) . (2)

In order to simplify the notation, we introduce a
dummy mention with index 0, which is to the left
(i.e. appears before) of all other mentions and has
w0i = 0 for all actual mentions i > 0. For a given
clustering C, if a mention i is not co-clustered with
any previous actual mention j, 0 < j < i, then we
assume that i links to 0 and C(i, 0) = 1. In other
words, C(i, 0) = 1 iff i is the first actual item of a
cluster in C. However, such an item i is not consid-
ered to be co-clustered with 0 and for any valid clus-
tering, item 0 is always in a singleton dummy clus-
ter, which is eventually discarded. The important
property of the score s is that it is exactly maximized

603



by the best-left-link inference, as it maximizes indi-
vidual left link scores and the creation of one left-
link does not affect the creation of other left-links.

3.3 Learning

We use a max-margin approach to learn w. We are
given a training set D of documents where for each
document d ∈ D, Cd refers to the annotated ground
truth clustering. Then we learn w by minimizing

L(w) =
λ

2
‖w‖2 +

1

|D|

∑

d∈D

1

md

(

max
C

(

s(C; d,w)

+ ∆(C, Cd)
)

− s(Cd; d,w)
)

,

where ∆(C, Cd) is a loss function used in corefer-
ence. In order to achieve tractable loss-augmented
minimization — something not possible with stan-
dard loss functions used in coreference (e.g.
B3 (Bagga and Baldwin, 1998)) — we use a de-
composable loss function that just counts the num-
ber of mention pairs on which C and Cd disagree:
∆(C, Cd) =

∑md
i,j=0,j<i IC(i,j)=Cd(i,j), where I is

a binary indicator function. This loss function
is equivalent to the numerator of the Rand index
loss (Rand, 1971). With this form of loss function
and using the scoring function in Eq. (2), we can
write L(w) as

λ

2
‖w‖2 +

1

|D|

∑

d∈D

1

md

md
∑

i=1

(

max
0≤j<i

(

w · φ(j, i)

+ δ(Cd, i, j)
)

− max
0≤j<i,C(i,j)=1

(w · φ(j, i))
)

,

(3)

where δ(Cd, i, j) = 1 − Cd(i, j) is the loss-based
margin that is 1 if i and j are not coreferent in Cd,
and is 0 otherwise. In the above objective function,
the left-links remain latent while we get to observe
the clustering. This objective function is related to
latent structural SVMs (Yu and Joachims, 2009).
However Yu and Joachims (2009) use a spanning
tree based latent structure which does not have the
left-to-right directionality we exploit. We can mini-
mize the above function using Concave Convex Pro-
cedure (Yuille and Rangarajan, 2003), which is guar-
anteed to reach the local minima. However, such a
procedure is costly as it requires doing inference on
all the documents to compute a single gradient up-
date. Consequently, we choose a faster stochastic

sub-gradient descent (SGD) approach. Since L(w)
in Eq. (3) decomposes not only over training doc-
uments, but also over individual mentions in each
document, we can perform SGD on a per-mention
basis. The stochastic sub-gradient w.r.t. mention i
in document d is given by

∇L(w)id ∝ φ(j
′, i)− φ(j′′, i) + λw, where (4)

j′ = arg max
0≤j<i

(w · φ(j, i) + 1− Cd(i, j))

j′′ = arg max
0≤j<i,C(i,j)=1

w · φ(j, i)

While SGD has no theoretically convergence guar-
antee, it works excellently in our experiments.
Specifically, we observe that SGD achieves similar
training performance to CCCP with a speed-up of
around 10,000.

3.4 Incorporating Constraints

Next, we show how to incorporate domain
knowledge-based constraints into L3M and gener-
alize it to CL3M. In CL3M, we obtain a cluster-
ing by maximizing a constraint-augmented scoring
function f given by

s(C; d,w) +

nc
∑

p=1

ρpψp(d, C),

where the second term on the R.H.S. is the
score contributed by domain specific constraints
ψ1, . . . , ψnc with their respective scores ρ1, . . . , ρnc .
In particular, ψp(d, C) measures the extent to which
a given clustering C satisfies the pth constraint. Note
that this framework is general and can be applied to
inject mention-to-cluster or cluster-to-cluster level
constraints too. However, for simplicity, we con-
sider here only constraints between mention pairs.
This allows us derive fast greedy algorithm to solve
the inference problem. The details of our constraints
are presented in Sec. 5.

All of our constraints can be categorized into two
groups: “must-link” and “cannot-link”.“Must-link”
constraints encourage a pair of mentions to connect,
while “cannot-link” constraints discourage mention
pairs from being linked. Consequently, the coeffi-
cients ρp associated with “must-link” constraints are
positive while ρp for “cannot-link” constraints are
negative. In the following, we briefly discuss how to

604



solve the inference problem with these two types of
constraints.

We slightly abuse notations and use ψp(j, i) to in-
dicate the pth constraint on a pair of mentions (i, j).
ψp(j, i) is a binary function that is 1 iff two mentions
i and j satisfy the conditions specified in constraint
p. Chang et al. (2011) shows that best-left-link in-
ference can be formulated as an ILP problem. When
we add constraints, the ILP becomes:

arg max
B,C∈{0,1}

∑

i,j:j<i
wjiBji +

∑

i,j
ρpψp(j, i)Cij

s.t Ckj ≥ Cij + Cki − 1, ∀i, j, k,
∑i−1

j=0
Bji = 1, ∀i

Bji < Cji, Cji = Cji,∀i, j,

(5)

where Cij ≡ C(i, j) is a binary variable indicating
whether i and j are in the same cluster or not and
Bji is an auxiliary variable indicating the best-left-
link for mention i. The first set of inequality con-
straints in (5) enforces the transitive closure of the
clustering. The constraints Bji < Cji,∀i, j enforce
the consistency between these two sets of variables.

One can use an off-the-shelf solver to solve Eq.
(5). However, when the absolute values of the con-
straint scores (|ρp|) are high (the hard constraint
case), then the following greedy algorithm approxi-
mately solves the inference efficiently. We scan the
document from left-to-right (or in any other arbitrary
order). When processing mention i, we find

j∗ = arg max
j<i

wji +
∑

k:Ĉ(k,j)=1

∑

p
ρpψp(k, i),

(6)
where Ĉ is the current clustering obtained from the
previous inference steps. Then, we add a link be-
tween mention i and j∗. The rest of the infer-
ence process is the same as in the original best-left-
link inference. Specifically, this inference procedure
combines the classifier score for mention pair i, j,
with the constraints score of all mentions currently
co-clustered with j. We discuss this further in Sec-
tion 5.

4 Probabilistic Latent Left-Linking Model

In this section, we extend and generalize our left-
linking model approach to a probabilistic model,

Probabilistic Latent Left-Linking Model (PL3M),
that allows us to naturally consider mention-to-
entity (or mention-to-cluster) links. While in L3M,
we assumed that each mention links determinis-
tically to the max-scoring mention on its left, in
PL3M, we assume that mention i links to mention
j, j ≤ i, with probability given by

Pr[j ← i; d,w] =
e

1

γ
(w·φ(i,j))

Zi(w, γ)
. (7)

Here Zi(w, γ) =
∑

0≤k<i e
1

γ
(w·φ(i,k)) is a normal-

izing constant and γ ∈ (0, 1] is a constant tem-
perature parameter that is tuned on a development
set (Samdani et al., 2012). We assume that the event
that mention i links to a mention j is independent of
the event that mention i′ links to j′ for i 6= i′.

Inference with PL3M: Given the probability of a
link as in Eq. (7), the probability that mention i joins
an existing cluster c, Pr[c ⊙ i; d,w], is simply the
sum of the probabilities of i linking to the mentions
inside c:

Pr[c⊙ i; d,w] =
∑

j∈c,0≤j<i

Pr[j ← i; d,w]

=
∑

j∈c,0≤j<i

e
1

γ
(w·φ(i,j))

Zi(d,w, γ)
. (8)

Based on Eq. (8) and making use of the indepen-
dence assumption of left-links, we follow a simple
greedy clustering (or inference) algorithm: sequen-
tially add each mention i to a previously formed
cluster c∗, where c∗ = arg maxc Pr[c ⊙ i; d,w].
If the arg max cluster is the singleton cluster with
the dummy mention 0 (i.e. the score of all other
clusters is below the threshold of 0), then i starts a
new cluster and is not included in the dummy clus-
ter. Note that we link a mention to a cluster tak-
ing into account all the mentions inside that cluster,
mimicking the notion of a mention-to-cluster link.
This provides more expressiveness than the Best-
Left-Link inference, where a mention connects to
a cluster solely based on a single pairwise link to
some antecedent mention (the best-link mention) in
that cluster.

The case of γ = 0: As γ approaches zero, it is
easy to show that the probability P [j ← i; d, w]

605



in Eq. (7) approaches a Kronecker delta function
that puts probability 1 on the max-scoring mention
j = arg max0≤k<i w·φ(i, j) (assuming no ties), and
0 everywhere else (Pletscher et al., 2010; Samdani et
al., 2012). Consequently, as γ → 0, Pr[c ⊙ i; d,w]
in Eq. 8 approaches a Kronecker delta function cen-
tered on the cluster containing the max-scoring men-
tion, thus reducing to the best-link case of L3M.
Thus, PL3M, when tuning the value of γ, is a strictly
more general model than L3M.

Learning with PL3M We use a likelihood-based
approach to learning with PL3M, and first compute
the probability Pr[C; d,w] of generating a cluster-
ing C, given w. We then learn w by minimizing
the regularized negative log-likelihood of the data,
augmenting the partition function with a loss-based
margin (Gimpel and Smith, 2010). We omit the de-
tails of likelihood computation due to lack of space.

With PL3M, we again follow a stochastic gradi-
ent descent technique instead of CCCP for the same
reasons mentioned in Sec. 3.3. The stochastic gra-
dient (subgradient when γ = 0) w.r.t. mention i in
document d is given by

∇LL(w)id ∝
∑

0≤j<i

pjφ(i, j)−
∑

0≤j<i

p′jφ(i, j) + λw,

where pj and p′j , j = 0, . . . , i− 1, are non-negative
weights that sum to one and are given by

pj =
e

1

γ
(w·φ(i,j)+δ(Cd,i,j))

∑

0≤k<i e
1

γ
(w·φ(i,k)+δ(Cd,i,k))

and

p′j =
Cd(i, j)Zi(d,w, γ)

Zi(Cd; d,w, γ)
Pr[j ← i; d,w] .

Interestingly, the above update rule generalizes the
one for L3M, as we are incorporating a weighted
sum of all previous mentions in the update rule.
With γ → 0, the SGD in Eq. (4) converges to the
SGD update in L3M (Eq. (4)). Finally, in the pres-
ence of constraints, we can fold them inside the pair-
wise link probabilities as in Eq. (6).

5 Experiments and Results

In this section, we present our experiments on the
two commonly used benchmarks for coreference
— Ontonotes-5.0 (Pradhan et al., 2012) and ACE

2004 (NIST, 2004). Table 1 exhibits our bottom line
results: CL3M achieves the best result reported on
Ontonotes-5.0 development set and essentially ties
with (Fernandes et al., 2012) on the test set. As
shown in Table 3, CL3M is also the best algorithm
on ACE and when evaluated on the gold mentions
of Ontonotes. We show that CL3M performs partic-
ularly well on clusters containing named entity men-
tions, which are more important for many informa-
tion extraction applications. In the rest of this sec-
tion, after describing our experimental setting, we
provide careful analysis of our algorithms and com-
pare them to competitive coreference approaches in
the literature.

5.1 Experimental Setup

Datasets: ACE 2004 contains 443 documents —
we used a standard split of these documents into
268 training, 68 development, and 106 testing doc-
uments used by Culotta et al. (2007) and Bengt-
son and Roth (2008). OntoNotes-5.0 dataset, re-
leased for the CoNLL 2012 Shared Task (Pradhan et
al., 2012), is by far the largest annotated corpus on
coreference. It contains 3,145 annotated documents
drawn from a wide variety of sources — newswire,
bible, broadcast transcripts, magazine articles, and
web blogs. We report results on both development
set and test set. To test on the development set, we
further split the training data into training and devel-
opment sets.

Classifier details: For each of the pairwise ap-
proaches, we assume the pairwise score is given by
w·φ(·, ·)+t where φ are the features, w is the weight
vector learned by the approach, and t is a threshold
which we set to 0 during learning (as in Eq. (1)), but
use a tuned value (tuned on a development set) dur-
ing testing. For learning with L3M, we do stochastic
gradient descent with 5 passes over the data. Empir-
ically, we observe that this is enough to generate a
stable model. For PL3M (Sec. 4), we tune the value
of γ using the development set picking the best γ
from {0.0, 0.2, . . . , 1.0}. Recall that when γ = 0,
PL3M is the same as L3M. We refer to L3M and
PL3M with incorporating constraints during infer-
ence as CL3M and CPL3M (Sec. 3.4), respectively.

Metrics: We compare the systems using three
popular metrics for coreference — MUC (Vilain et
al., 1995), BCUB (Bagga and Baldwin, 1998), and

606



Entity-based CEAF (CEAFe) (Luo, 2005). Follow-
ing, the CoNLL shared tasks (Pradhan et al., 2012),
we use the average F1 scores of these three metrics
as the main metric of comparison.

Features: We build our system on the publicly
available Illinois-Coref system1 primarily because it
contains a rich set of features presented in Bengtson
and Roth (2008) and Chang et al. (2012a) (the latter
adds features for pronominal anaphora resolution).
We also compare with the Best-Left-Link approach
described by Bengtson and Roth (2008).

Constraints: We consider the following con-
straints in CL3M and CPL3M.
• SameSpan: two mentions must be linked to

each other if they share the same surface text
span and the number of words in the text span
is larger than a threshold (set as 5 in our imple-
mentation).

• SameDetNom: two mentions must be linked
to each other if both mentions start with a de-
terminer and the [0,1] wordnet-based similarity
score between the mention head words is above
a threshold (set to 0.8).

• SameProperName: two mentions must be
linked if they are both proper names and the
similarity score measured by a named entity-
based similarity metric, Illinois NESim2, are
higher than a threshold (set to 0.8). For a per-
son entity we add additional rules to extract the
first name, last name and professional title as
properties.

• ModifierMismatch: the constraint prevents two
mentions to be linked if the head modifiers
conflict. For example, the constraint prevents
“northern Taiwan” from linking to “southern
Taiwan”. We gather a list of mutual exclusive
modifiers from the training data.

• PropertyMismatch: the constraint prevents two
mentions to be linked if their properties con-
flict. For example, it prevents male pronouns
to link to female pronouns and “Mr. Clinton”
to link to “Mrs. Clinton” by checking the gen-
der property. The properties we consider are
gender, number, professional title and the na-

1The system is available at http://cogcomp.cs.
illinois.edu/page/software_view/Coref/

2http://cogcomp.cs.illinois.edu/page/
software_view/NESim

MUC BCUB CEAFe AVG
Dev Set

Stanford 64.30 70.46 46.35 60.37
(Chang et al., 2012a) 65.75 70.25 45.30 60.43
(Martschat et al., 2012) 66.76 71.91 47.52 62.06
(Björkelund and Farkas, ) 67.12 71.18 46.84 61.71
(Chen and Ng, 2012) 66.4 71.8 48.8 62.3
(Fernandes et al., 2012) 69.46 71.93 48.66 63.35
L3M 67.88 71.88 47.16 62.30
CL3M 69.20 72.89 48.67 63.59

Test Set
Stanford 63.83 68.52 45.36 59.23
(Chang et al., 2012a) 66.38 69.34 44.81 60.18
(Martschat et al., 2012) 66.97 70.36 46.60 61.31
(Björkelund and Farkas, ) 67.58 70.26 45.87 61.24
(Chen and Ng, 2012) 63.7 69.0 46.4 59.7
(Fernandes et al., 2012) 70.51 71.24 48.37 63.37
L3M 68.31 70.81 46.73 61.95
CL3M 69.64 71.93 48.32 63.30

Table 1: Performance on OntoNotes-5.0 with predicted
mentions. We report the F1 scores (%) on various coref-
erence metrics (MUC, BCUB, CEAF). The column AVG
shows the average scores of the three. We observe that
PL3M and CPL3M (see Sec. 4) yields the same perfor-
mance as L3M and CL3M, respectively as the tuned γ for
all the datasets turned out to be 0.

tionality.
While the “must-link” constraints described in the
paper can be treated as features, due to their high
precision, treating them as hard constraints (set ρ to
a high value) is a safe and direct way to inject hu-
man knowledge into the learning model. Moreover,
our framework allows a constraint to use informa-
tion from previous decisions (such as “cannot-link”
constraints). Treating such constraints as features
will complicate the learning model.

5.2 Performance of the End-to-End System

We compare our system with the top systems re-
ported in the CoNLL shared task 2012 as well as
with the Stanford’s publicly released rule-based sys-
tem (Lee et al., 2013; Lee et al., 2011), which won
the CoNLL 2011 Shared Task (Pradhan et al., 2011).
Note that all the systems use the same annotations
(e.g., gender prediction, part-of-speech tags, name
entity tags) provided by the shared task organizers.

607



However, each system implements its own mention
detector and pipelines the identified mentions into
the coreference clustering component. Moreover,
different systems use a different set of features. In
order to partially control for errors on mention de-
tection and better evaluate the clustering component
in our coreference system, we will also present re-
sults on correct (gold) mentions in the next section.

Table 1 shows the end-to-end results. On the
development set, only the best performing system
of Fernandes et al. (2012) is better than L3M, but this
difference disappears when we use our system with
constraints, CL3M. Although our system is much
simple, it achieves the best B3 score on the test set
and is competitive with the best system participated
in the CoNLL shared task 2012.

Performance on named entities: The corefer-
ence annotation in Ontonotes 5.0 includes various
types of mentions. However, not all mention types
are equally interesting. In particular, clusters which
contain at least one proper name or a named entity
mention are more important for information extrac-
tion tasks like Wikification (Mihalcea and Csomai,
2007; Ratinov et al., 2011), cross-document coref-
erence resolution (Bagga and Baldwin, 1998), and
entity linking and knowledge based population (Ji
and Grishman, 2011).

Inspired by this, we compare our system to the
best systems in the CoNLL shared task of 2011
(Stanford (Lee et al., 2011)) and 2012 (Fernan-
des (Fernandes et al., 2012)) on the following spe-
cific tasks on Ontonotes-5.0.
• ENT-C: Evaluate the system on clusters that

contain at least one proper name mention. We
generate the gold annotation and system out-
puts by using the gold and predicted name en-
tity tag annotations provided by the CoNLL
shard task 2012. That is, if a cluster does not
include any name entity mention, then it will
be removed from the final clustering.

• PER-C: As in the construction of ENT-C, but
here we only consider clusters which contain at
least one “Person (PER)” entity.

• ORG-C: As in the construction of Entity-C, but
here we only consider clusters which contain at
least one “Organization (ORG)” entity.

Typically, the clusters that get ignored in the above
definitions contain only first and second person

Task Stanford Fernandes L3M CL3M
ENT-C 44.06 47.05 46.63 48.02
PER-C 34.04 36.43 37.01 37.57
ORG-C 25.02 26.23 26.22 27.01

Table 2: Performance on named entities for OntoNotes-
5.0 data. We compare our system to Fernandes (Fernan-
des et al., 2012) and Stanford (Lee et al., 2013) systems.

pronouns (which often happens in transcribed dis-
course.) Also note that all the systems are trained
with the same name entity tags, provided by the
shared task organizers, and we use the same name
entity tags to construct the specific clustering. Also,
in order to further ensure fairness, we do not tune
our system to favor the evaluation of these specific
types of clusters. We chose to do so because we only
have access to the system output of Fernandes et al.
(2012).

Table 2 shows the results. The performance of
all systems degrades when considering only clusters
that contain name entities, indicating that ENT-C is
actually a harder task than the original coreference
resolution problem. In particular, resolving ORG
coreferent clusters is hard, because names of organi-
zations are sometimes confused with person names,
and they can be referred to using a range of pronouns
(including “we” and “it”). Overall, CL3M outper-
forms all the competing systems on the clusters that
contain at least one specific type of entity by a mar-
gin larger than that for the overall coreference.

5.3 Analysis on Gold Mentions

To better understand the contribution of our joint
learning and clustering model, we present experi-
ments assuming that gold mentions are given. The
definitions of gold mentions in ACE and Ontonotes
are different because Ontonotes-5.0 excludes single-
ton clusters in the annotation. In addition, Ontonotes
includes longer mentions; for example, it includes
NP and appositives in the same mention. We com-
pare with the publicly available Stanford (Lee et al.,
2011) and IllinoisCoref (Chang et al., 2012a) sys-
tems; the system of Fernandes et al. (2012) is not
publicly available. In addition, we also compare
with the following two structured prediction base-
lines that use the same set of features as L3M and
PL3M.

608



MUC BCUB CEAFe AVG
ACE 2004 Gold Ment.

All-Link-Red. 77.45 81.10 77.57 78.71
Spanning 73.31 79.25 74.66 75.74
IllinoisCoref 76.02 81.04 77.6 78.22
Stanford 75.04 80.45 76.75 77.41
(Stoyanov and Eisner, 2012) 80.1 81.8 - -
L3M 77.57 81.77 78.15 79.16
PL3M 78.18 82.09 79.21 79.83
CL3M 78.17 81.64 78.45 79.42
CPL3M 78.29 82.20 79.26 79.91

Ontonotes 5.0 Gold Ment.
All-Link-Red. 83.72 75.59 64.00 74.44
Spanning 83.64 74.83 61.07 73.18
IllinoisCoref 80.84 74.29 65.96 73.70
Stanford 82.26 76.82 61.69 73.59
L3M 83.44 78.12 64.56 75.37
PL3M 83.97 78.25 65.69 75.97
CL3M 84.10 78.30 68.74 77.05
CPL3M 84.80 78.74 68.75 77.43

Table 3: Performance on ACE 2004 and OntoNotes-5.0.
All-Link-Red. is based on correlational clustering; Span-
ning is based on latent spanning forest based clustering
(see Sec. 2). Our proposed approach is L3M (Sec. 3) and
PL3M (sec. 4). CL3M and CPL3M are the version with
incorporating constraints.

1. All-Link-Red: a reduced and faster alterna-
tive to the correlational clustering based ap-
proach (Finley and Joachims, 2005). We im-
plemented this algorithm as an ILP and droped
one of the three transitivity constraints for each
triplet of mention variables. Following Pascal
and Baldridge (2009) and Chang et al. (2011)
we observe that this slightly improves the ac-
curacy over a pure correlation clustering ap-
proach, in addition to speeding up inference.

2. Spanning: the latent spanning forest based ap-
proach presented by Yu and Joachims (2009).
We use the publicly available implementation
provided by the authors3 for the ACE data;
since their CCCP implementation is slow, we
implemented our own stochastic gradient de-
scent version to scale it to the much larger
Ontonotes data.

3Available at http://www.cs.cornell.edu/ cnyu/latentssvm/

Table 3 lists the results. Although L3M is simple
and use only the features defined on pairwise men-
tions, it compares favorably with all recently pub-
lished results. Moreover, the probabilistic general-
ization of L3M, PL3M, achieves even better perfor-
mance. For example, L3M with γ = 0.2 improves
L3M with γ = 0 by 0.7 points in ACE 2004. In par-
ticular, This shows that considering more than a one
left-links is helpful. This is in contrast with the pre-
dicted mentions where γ = 0 performed best. We
suspect that this is because noisy mentions can hurt
the performance of PL3M that takes into account
not just the best scoring links, but also weaker links
which are likely to be less reliable (more false pos-
itives). Also, as opposed to what is reported by Yu
and Joachims (2009), the correlation clustering ap-
proach performs better than the spanning forest ap-
proach. We think that this is because we compare
the systems on different metrics than they did and
also because we use exact ILP inference for corre-
lational clustering whereas Yu and Joachims (2009)
used approximate greedy inference.

Both L3M and PL3M can be benefit from using
constraints. However, The constraints improve only
marginally on the ACE 2004 data because ACE uses
shorter phrases as mentions. Consequently, con-
straints designed for leveraging information from
long mention spans are less effective. Overall, the
experiments show that L3M and PL3M perform well
on modeling coreference clustering.

5.4 Ablation Study of Constrains

Finally, we study the value of individual constraints
by adding one constraint at a time to the corefer-
ence system starting with the simple L3M model.
The system with all the constraints added is the
CL3M model introduced in Table 1. We then re-
move individual constraints from CL3M to assess
its contribution. Table 4 shows the results on the
Ontonotes dataset with predicted mentions. Overall,
it is shown that each one of the constraints has a con-
tribution, and that using all the constraints improves
the performance of the system by 1.29% in the AVG
F1 score. In particular, most of this improvement
(1.19%) is due to the must-link constraints (the first
four constraints in the table). The must-link con-
straints are more useful for L3M as L3M achieves
higher precision than recall (e.g., the precision and

609



MUC BCUB CEAFe AVG
L3M 67.88 71.88 47.16 62.30
+SameSpan 68.27 72.27 47.73 62.75
+SameDetNom 68.79 72.57 48.30 63.22
+SameProperName 69.11 72.81 48.56 63.49
+ModifierMismatch 69.11 72.81 48.58 63.50
+PropertyMismatch

69.20 72.89 48.67 63.59
(i.e. CL3M)

-SameSpan 68.91 72.66 48.36 63.31
-SameDetNom 68.62 72.51 48.06 63.06
-SameProperName 68.97 72.69 48.50 63.39
-ModifierMismatch 69.12 72.80 48.63 63.52
-PropertyMismatch 69.11 72.81 48.58 63.50

Table 4: Ablation study on constraints. We first show
cumulative performance on OntoNotes-5.0 data with pre-
dicted mentions as constraints are added one at a time into
the coreference system. Then we demonstrate the value
of individual constraints by leaving out one constraint at
each time.

recall of L3M are 78.38% and 67.96%, respectively
in B3). As a result, the must-link constraints, which
aim at improving the recall, do better when optimiz-
ing F1.

6 Conclusions

We presented a principled yet simple framework for
coreference resolution. Furthermore, we showed
that our model can be augmented in a straightfor-
ward way with knowledge based constraints, to im-
prove performance. We also presented a probabilis-
tic generalization of this model that can take into
account entity-mention links by considering mul-
tiple possible coreference links. We proposed a
fast stochastic gradient-based learning technique for
our model. Our model, while operating at men-
tion pair granularity, obtains state-of-the-art results
on OntoNotes-5.0, and performs especially well on
mention clusters containing named entities. We pro-
vided a detailed analysis of our experimental results.

Acknowledgments Supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior National

Business Center contract number D11PC20155. The U.S. Government

is authorized to reproduce and distribute reprints for Governmental pur-

poses notwithstanding any copyright annotation thereon. Disclaimer:

The views and conclusions contained herein are those of the authors and

should not be interpreted as necessarily representing the official policies

or endorsements, either expressed or implied, of IARPA, DoI/NBC, or

the U.S. Government.

References

A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In In The First International Con-
ference on Language Resources and Evaluation Work-
shop on Linguistics Coreference.

M. Bansal and D. Klein. 2012. Coreference semantics
from web features. In Proceedings of ACL, Jeju Island,
South Korea, July.

N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.

E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.

A. Björkelund and R. Farkas.

K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference protocols
for coreference resolution. In CoNLL Shared Task.

K.-W. Chang, R. Samdani, A. Rozovskaya, M. Sammons,
and D. Roth. 2012a. Illinois-coref: The UI system
in the CoNLL-2012 Shared Task. In CoNLL Shared
Task.

M. Chang, L. Ratinov, and D. Roth. 2012b. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399–431, 6.

C. Chen and V. Ng. 2012. Combining the best of two
worlds: A hybrid approach to multilingual corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task.

J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 144–151, Sydney, Australia, July. ACL.

A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL.

P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).

P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In EMNLP, pages
660–669.

G. Durrett, D. Hall, and D. Klein. 2013. Decentral-
ized entity-level modeling for coreference resolution.
In Proceedings of ACL, August.

610



E. R. Fernandes, C. N. dos Santos, and R. L. Milidiú.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task.

T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).

K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.

K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In NAACL.

A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In NAACL.

H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: successful approaches and challenges. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1.

T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.

H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford’s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the CoNLL-2011
Shared Task.

H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2013. Deterministic coref-
erence resolution based on entity-centric, precision-
ranked rules. Computational Linguistics, 39(4).

X. Luo. 2005. On coreference resolution performance
metrics. In EMNLP.

S. Martschat, J. Cai, S. Broscheit, É. Mújdricza-Maydt,
and M. Strube. 2012. A multigraph model for corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, July.

A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).

R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.

V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.

Vincent Ng. 2005. Supervised ranking for pronoun res-
olution: Some recent improvements. In AAAI, pages
1081–1086.

NIST. 2004. The ACE evaluation plan.

D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.

P. Pletscher, C. S. Ong, and J. M. Buhmann. 2010. En-
tropy and margin maximization for structured output
learning. In ECML PKDD.

S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In CoNLL.

S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL 2012.

M. Sammons Y. Tu V. Vydiswaran Q. Do, D. Roth. 2009.
Robust, light-weight approaches to compute lexical
similarity. Technical report.

K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.

A. Rahman and V. Ng. 2011a. Coreference resolution
with world knowledge. In ACL, pages 814–824.

A. Rahman and V. Ng. 2011b. Ensemble-based corefer-
ence resolution. In IJCAI.

A. Rahman and V. Ng. 2011c. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. JAIR.

W.M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336):846–850.

L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.

L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).

Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-04, pages 1–8.

R. Samdani, M. Chang, and D. Roth. 2012. Unified ex-
pectation maximization. In NAACL.

W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comput. Linguist.

V. Stoyanov and J. Eisner. 2012. Easy-first coreference
resolution. In COLING, pages 2519–2534.

V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
making sense of the state-of-the-art. In ACL.

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference

611



scoring scheme. In Proceedings of the 6th conference
on Message understanding.

C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).

A. L. Yuille and A. Rangarajan. 2003. The concave-
convex procedure. Neural Computation, 15(4).

612


