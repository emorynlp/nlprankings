



















































Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085–1097
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

1085

Generating Natural Language Adversarial Examples
through Probability Weighted Word Saliency

Shuhuai Ren Yihe Deng
Huazhong University of Science and Technology University of California, Los Angeles

shuhuai ren@hust.edu.cn yihedeng@g.ucla.edu
Kun He∗ Wanxiang Che

School of Computer Science and Technology, School of Computer Science and Technology,
Huazhong University of Science and Technology Harbin Institute of Technology

brooklet60@hust.edu.cn car@ir.hit.edu.cn

Abstract

We address the problem of adversarial attacks
on text classification, which is rarely studied
comparing to attacks on image classification.
The challenge of this task is to generate ad-
versarial examples that maintain lexical cor-
rectness, grammatical correctness and seman-
tic similarity. Based on the synonyms substi-
tution strategy, we introduce a new word re-
placement order determined by both the word
saliency and the classification probability, and
propose a greedy algorithm called probability
weighted word saliency (PWWS) for text ad-
versarial attack. Experiments on three popular
datasets using convolutional as well as LSTM
models show that PWWS reduces the classifi-
cation accuracy to the most extent, and keeps
a very low word substitution rate. A human
evaluation study shows that our generated ad-
versarial examples maintain the semantic simi-
larity well and are hard for humans to perceive.
Performing adversarial training using our per-
turbed datasets improves the robustness of the
models. At last, our method also exhibits a
good transferability on the generated adversar-
ial examples.

1 Introduction

Deep neural networks (DNNs) have exhibited vul-
nerability to adversarial examples primarily for
image classification (Szegedy et al., 2013; Good-
fellow et al., 2015; Nguyen et al., 2015). Adver-
sarial examples are input data that are artificially
modified to cause mistakes in models. For image
classifications, the researchers have proposed var-
ious methods to add small perturbations on im-
ages that are imperceptible to humans but can
cause misclassification in DNN classifiers. Due to
the variety of key applications of DNNs in com-
puter vision, the security issue raised by adversar-
ial examples has attracted much attention in liter-

∗Corresponding author.

atures since 2014, and numerous approaches have
been proposed for either attack (Goodfellow et al.,
2015; Kurakin et al., 2016; Tramèr et al., 2018;
Dong et al., 2018), as well as defense (Goodfel-
low et al., 2015; Tramèr et al., 2018; Wong and
Kolter, 2018; Song et al., 2019).

In the area of Natural Language Processing
(NLP), there is only a few lines of works done
recently that address adversarial attacks for NLP
tasks (Liang et al., 2018; Samanta and Mehta,
2017; Alzantot et al., 2018). This may be due to
the difficulty that words in sentences are discrete
tokens, while the image space is continuous to per-
form gradient descent related attacks or defnses. It
is also hard in human’s perception to make sense
of the texts with perturbations while for images
minor changes on pixels still yield a meaningful
image for human eyes. Meanwhile, the existence
of adversarial examples for NLP tasks, such as
span filtering, fake news detection, sentiment anal-
ysis, etc., raises concerns on significant security
issues in their applications.

In this work, we focus on the problem of gen-
erating valid adversarial examples for text classifi-
cation, which could inspire more works for NLP
attack and defense. In the area of NLP, as the
input feature space is usually the word embed-
ding space, it is hard to map a perturbed vector
in the feature space to a valid word in the vo-
cabulary. Thus, methods of generating adversar-
ial examples in the image field can not be directly
transferred to NLP attacks. The general approach,
then, is to modify the original samples in the word
level or in the character level to achieve adversar-
ial attacks (Liang et al., 2018; Gao et al., 2018;
Ebrahimi et al., 2018).

We focus on the text adversarial example gen-
eration that could guarantee the lexical correct-
ness with little grammatical error and semantic
shifting. In this way, it achieves “small per-



1086

turbation” as the changes will be hard for hu-
mans to perceive. We introduce a new synonym
replacement method called Probability Weighted
Word Saliency (PWWS) that considers the word
saliency as well as the classification probability.
The change value of the classification probability
is used to measure the attack effect of the pro-
posed substitute word, while word saliency shows
how well the original word affects the classifica-
tion. The change value of the classification prob-
ability weighted by word saliency determines the
final substitute word and replacement order.

Extensive experiments on three popular datasets
using convolutional as well as LSTM models
demonstrate a good attack effect of PWWS. It re-
duces the accuracy of the DNN classifiers by up to
84.03%, outperforms existing text attacking meth-
ods. Meanwhile, PWWS has a much lower word
substitution rate and exhibits a good transferabil-
ity. We also do a human evaluation to show that
our perturbations are hard for humans to perceive.
In the end, we demonstrate that adversarial train-
ing using our generated examples can help im-
prove robustness of the text classification models.

2 Related Work

We first provide a brief review on related works
for attacking text classification models.

Liang et al. (2018) propose to find appropri-
ate words for insertion, deletion and replacement
by calculating the word frequency and the highest
gradient magnitude of the cost function. But their
method involves considerable human participation
in crafting the adversarial examples. To maintain
semantic similarity and avoid human detection, it
requires human efforts such as searching related
facts online for insertion.

Therefore, subsequent research are mainly
based on the word substitution strategy so as to
avoid artificial fabrications and achieve automatic
generations. The key difference of these subse-
quent methods is on how they generate substi-
tute words. Samanta and Mehta (2017) propose
to build a candidate pool that includes synonyms,
typos and genre specific keywords. They adopt
Fast Gradient Sign Method (FGSM) (Goodfellow
et al., 2015) to pick a candidate word for replace-
ment. Papernot et al. (2016b) perturb a word
vector by calculating forward derivative (Papernot
et al., 2016a) and map the perturbed word vector to
a closest word in the word embedding space. Yang

et al. (2018) derive two methods, Greedy Attack
based on perturbation, and Gumbel Attack based
on scalable learning. Aiming to restore the in-
terpretability of adversarial attacks based on word
substitution strategy, Sato et al. (2018) restrict the
direction of perturbations towards existing words
in the input embedding space.

As the above methods all need to calculate the
gradient with access to the model structure, model
parameters, and the feature set of the inputs, they
are classified as white-box attacks. To achieve at-
tack under a black-box setting, which assumes no
access to the details of the model or the feature
representation of the inputs, Alzantot et al. (2018)
propose to use a population-based optimization al-
gorithm. Gao et al. (2018) present a DeepWord-
Bug algorithm to generate small perturbations in
the character-level for black-box attack. They sort
the tokens based on the importance evaluated by
four functions, and make random token transfor-
mations such as substitution and deletion with the
constraint of edit distance. Ebrahimi et al. (2018)
also propose a token transformation method, and
it is based on the gradients of the one-hot input
vectors. The downside of the character-level per-
turbations is that they usually lead to lexical errors,
which hurts the readability and can easily be per-
ceived by humans.

The related works have achieved good results
for text adversarial attacks, but there is still much
room for improvement regarding the percentage of
modifications, attacking success rate, maintenance
on lexical as well as grammatical correctness and
semantic similarity, etc. Based on the synonyms
substitution strategy, we propose a novel black-
box attack method called PWWS for the NLP clas-
sification tasks and contribute to the field of adver-
sarial machine learning.

3 Text Classification Attack

Given an input feature space X containing all pos-
sible input texts (in vector form x) and an output
space Y = {y1, y2, . . . , yK} containing K possi-
ble labels of x, the classifier F needs to learn a
mapping f : X → Y from an input sample x ∈ X
to a correct label ytrue ∈ Y . In the following, we
first give a definition of adversarial example for
natural language classification, and then introduce
our word substitution strategy.



1087

3.1 Text Adversarial Examples
Given a trained natural language classifier F ,
which can correctly classify the original input text
x to the label ytrue based on the maximum poste-
rior probability.

arg max
yi∈Y

P (yi|x) = ytrue. (1)

We attack the classifier by adding an imperceptible
perturbation ∆x to x to craft an adversarial exam-
ple x∗, for which F is expected to give a wrong
label:

arg max
yi∈Y

P (yi|x∗) 6= ytrue.

Eq. (2) gives the definition of the adversarial ex-
ample x∗:

x∗ = x + ∆x, ‖∆x‖p < �,
arg max

yi∈Y
P (yi|x∗) 6= arg max

yi∈Y
P (yi|x). (2)

The original input text can be expressed as x =
w1w2 . . . wi . . . wn, wherewi ∈ D is a word and D
is a dictionary of words. ‖∆x‖p defined in Eq. (3)
uses p-norm to represent the constraint on per-
turbation ∆x, and L∞, L2 and L0 are commonly
used.

‖∆x‖p =

(
n∑

i=1

|w∗i − wi|
p

) 1
p

. (3)

To make the perturbation small enough so that it
is imperceptible to humans, the adversarial exam-
ples need to satisfy lexical, grammatical, and se-
mantic constraints. Lexical constraint requires that
the correct word in the input sample cannot be
changed to a common misspelled word, as a spell
check before the input of the classifier can easily
remove such perturbation. The perturbed samples,
moreover, must be grammatically correct. Third,
the modification on the original samples should
not lead to significant changes in semantics as the
semantic constraint requires.

To meet the above constraints, we replace words
in the input texts with synonyms and replace
named entities (NEs) with similar NEs to generate
adversarial samples. Synonyms for each word can
be found in WordNet1, a large lexical database for
the English language. NE refers to an entity that
has a specific meaning in the sample text, such as
a person’s name, a location, an organization, or a
proper noun. Replacement of an NE with a sim-
ilar NE imposes a slight change in semantics but
invokes no lexical or grammatical changes.

The candidate NE for replacement is picked in
1https://wordnet.princeton.edu/

the following. Assuming that the current input
sample belongs to the class ytrue and dictionary
Dytrue ⊆ D contains all NEs that appear in the
texts with class ytrue, we can use the most fre-
quently occurring named entity NEadv in the com-
plement dictionary D−Dytrue as a substitute word.
In addition, the substitute NEadv must have the
consistent type with the original NE, e.g., they
must be both locations.

3.2 Word Substitution by PWWS

In this work, we propose a new text attack-
ing method called Probability Weighted Word
Saliency (PWWS). Our approach is based on syn-
onym replacement, and there are two key issues
that we resolve in the greedy PWWS algorithm:
the selection of synonyms or NEs and the decision
of the replacement order.

3.2.1 Word Substitution Strategy

For each word wi in x, we use WordNet to build
a synonym set Li ⊆ D that contains all synonyms
of wi. If wi is an NE, we find NEadv which has
a consistent type of wi to join Li. Then, every
w′i ∈ Li is a candidate word for substitution of
the original wi. We select a w′i from Li as the
proposed substitute word w∗i if it causes the most
significant change in the classification probability
after replacement. The substitute word selection
method R(wi,Li) is defined as follows:

w∗i = R(wi,Li)
= arg max

w′i∈Li

{
P (ytrue|x)− P (ytrue|x′i)

}
, (4)

where

x = w1w2 . . . wi . . . wn,

x′i = w1w2 . . . w
′
i . . . wn,

and x′i is the text obtained by replacing wi with
each candidate word w′i ∈ Li. Then we replace wi
with w∗i and get a new text x

∗
i :

x∗i = w1w2 . . . w
∗
i . . . wn.

The change in classification probability be-
tween x and x∗i represents the best attack effect
that can be achieved after replacing wi.

∆P ∗i = P (ytrue|x)− P (ytrue|x∗i ). (5)
For each word wi ∈ x, we find the corresponding
substitute word w∗i by Eq. (4), which solves the
first key issue in PWWS.

https://wordnet.princeton.edu/


1088

3.2.2 Replacement Order Strategy
Furthermore, in the text classification tasks, each
word in the input sample may have different level
of impact on the final classification. Thus, we in-
corporate word saliency (Li et al., 2016b,a) into
our algorithm to determine the replacement order.
Word saliency refers to the degree of change in the
output probability of the classifier if a word is set
to unknown (out of vocabulary). The saliency of a
word is computed as S(x, wi).

S(x, wi) = P (ytrue|x)− P (ytrue|x̂i) (6)

where

x = w1w2 . . . wi . . . wd,

x̂i = w1w2 . . . unknown . . . wd.

We calculate the word saliency S(x, wi) for all
wi ∈ x to obtain a saliency vector S(x) for text x.

To determine the priority of words for replace-
ment, we need to consider the degree of change in
the classification probability after substitution as
well as the word saliency for each word. Thus, we
score each proposed substitute word w∗i by evalu-
ating the ∆P ∗i in Eq. (5) and i

th value of S(x).
The score function H(x,x∗i , wi) is defined as:

H(x,x∗i , wi) = φ(S(x))i ·∆P ∗i (7)
where φ(z)i is the softmax function

φ(z)i =
ezi∑K
k=1 e

zk
. (8)

z in Eq. (8) is a vector. zi and φ(z)i indicate the
ith component of vector z and φ(z), respectively.
φ(S(x)) in Eq. (7) indicates a softmax operation
on word saliency vector S(x) and K = |S(x)|.

Eq. (7) defined by probability weighted word
saliency determines the replacement order. We
sort all the words wi in x in descending order
based on H(x,x∗i , wi), then consider each word
wi under this order and select the proposed substi-
tute word w∗i for wi to be replaced. We greedily it-
erate through the process until enough words have
been replaced to make the final classification label
change.

The final PWWS Algorithm is as shown in Al-
gorithm 1.

4 Empirical Evaluation

For empirical evaluation, we compare PWWS
with other attacking methods on three popular
datasets involving four neural network classifica-
tion models.

Algorithm 1 PWWS Algorithm

Input: Sample text x(0) before iteration;
Input: Length of sample text x(0): n = |x(0)|;
Input: Classifier F ;
Output: Adversarial example x(i)

1: for all i = 1 to n do
2: Compute word saliency S(x(0), wi)
3: Get a synonym set Li for wi
4: if wi is an NE then Li = Li ∪ {NEadv}
5: end if
6: if Li = ∅ then continue
7: end if
8: w∗i = R(wi,Li);
9: end for

10: Reorder wi such that
11: H(x,x∗1, w1) > · · · > H(x,x∗n, wn)
12: for all i = 1 to n do
13: Replace wi in x(i−1) with w∗i to craft x

(i)

14: if F (x(i)) 6= F (x(0)) then break
15: end if
16: end for

4.1 Datasets

Table 1 lists the details of the datasets, IMDB,
AG’s News, and Yahoo! Answers.

IMDB. IMDB is a large movie review dataset
consisting of 25,000 training samples and 25,000
test samples, labeled as positive or negative. We
use this dataset to train a word-based CNN model
and a Bi-directional LSTM network for sentiment
classification (Maas et al., 2011).

AG’s News. This is a collection of more than
one million news articles, which can be catego-
rized into four classes: World, Sports, Business
and Sci/Tech. Each class contains 30,000 training
samples and 1,900 testing samples.

Yahoo! Answers. This dataset consists of
ten topic categories: Society & Culture, Science
& Mathematics, Health, Education & Reference,
Computers & Internet, etc. Each category contains
140,000 training samples and 5,000 test samples.

4.2 Deep Neural Models

For deep neural models, we consider several clas-
sic as well as state-of-the-art models used for text
classification. These models include both convo-
lutional neural networks (CNN) and recurrent neu-
ral networks (RNN), for word-level or character-
level data processing.



1089

Dataset #Classes #Train samples #Test samples #Average words Task
IMDB Review 2 25,000 25,000 325.6 Sentiment analysis

AG’s News 4 120,000 7600 278.6 News categorization
Yahoo! Answers 10 1,400,000 50,000 108.4 Topic classification

Table 1: Statistics on the datasets. “#Average words” indicates the average number of words per sample text.

Word-based CNN (Kim, 2014) consists of an
embedding layer that performs 50-dimensional
word embeddings on 400-dimensional input vec-
tors, an 1D-convolutional layer consisting of 250
filters of kernel size 3, an 1D-max-pooling layer,
and two fully connected layers. This word-based
classification model is used on all three datasets.

Bi-directional LSTM consists of a 128-
dimensional embedding layer, a Bi-directional
LSTM layer whose forward and reverse are re-
spectively composed of 64 LSTM units, and a
fully connected layer. This word-based classifi-
cation model is used on IMDB dataset.

Char-based CNN is identical to the structure
in (Zhang et al., 2015) which includes two Con-
vNets. The two networks are both 9 layers deep
with 6 convolutional layers and 3 fully-connected
layers. This char-based classification model is
used for AG’s News dataset.

LSTM consists of a 100-dimensional embed-
ding layer, an LSTM layer composed of 128 units,
and a fully connected layer. This word-based
classification model is used for Yahoo! Answers
dataset.

Column 3 in Table 2 demonstrates the clas-
sification accuracies of these models on original
(clean) examples, which almost achieves the best
results of the classification task on these datasets.

4.3 Attacking Methods

We compare our PWWS 2 attacking method with
the following baselines. All the baselines use
WordNet to build the candidate synonym sets L.

Random. We randomly select a synonym for
each word in the original input text to replace, and
keep performing such replacement until the clas-
sification output changes.

Gradient. This method draws from
FGSM (Goodfellow et al., 2015), which is
previously proposed for image adversarial attack:

x∗ = x + ∆x
= x + � · sign (∇xJ (F, ytrue)) ,

(9)

2https://github.com/JHL-HUST/PWWS/

where J (F, ytrue) is the cost function used for
training the neural network.

For the sake of calculation, we will use the syn-
onym that maximizes the change of prediction out-
put ∆F (x) as the substitute word, where ∆F (x)
is approximated by forward derivative:

∆F (x) = F
(
x′
)
− F (x)

≈
(
x′i − xi

) ∂F (x)
∂xi

.
(10)

This method using Eq. (10) is the main concept
introduced in (Papernot et al., 2016b).

Traversing in word order (TiWO). This
method of traversing input sample text in word or-
der finds substitute for each word according to Eq.
(4).

Word Saliency (WS). WS (Samanta and
Mehta, 2017) sorts words in the input text based
on word saliency in Eq. (6) in descending order,
and finds substitute for each word according to Eq.
(4).

4.4 Attacking Results

We evaluate the merits of all above methods by
using them to generate 2,000 adversarial exam-
ples respectively. The more effective the attack-
ing method is, the more the classification accuracy
of the model drops. Table 2 shows the classifica-
tion accuracy of different models on the original
samples and the adversarial samples generated by
these attack methods.

Results show that our method reduces the clas-
sification accuracies to the most extent. The clas-
sification accuracies on the three datasets IMDB,
AG’s News, and Yahoo! Answers are reduced by
an average of 81.05%, 33.62%, and 38.65% re-
spectively. The effectiveness of the attack against
multi-classification tasks is not as good as that for
binary classification tasks.

Our method achieves such effects by very few
word replacements. Table 3 lists the word replace-
ment rates of the adversarial examples generated
by different methods. The rate refers to the num-
ber of substitute words divided by the total number
of words in the original clean sample texts. It indi-
cates that PWWS replaces the fewest words while

https://github.com/JHL-HUST/PWWS/


1090

Dataset Model Original Random Gradient TiWO WS PWWS

IMDB
word-CNN 86.55% 45.36% 37.43% 10.00% 9.64% 5.50%

Bi-dir LSTM 84.86% 37.79% 14.57% 3.57% 3.93% 2.00%

AG’s News
char-CNN 89.70% 67.80% 72.14% 58.50% 62.45% 56.30%
word-CNN 90.56% 74.13% 73.63% 60.70% 59.70% 56.72%

Yahoo! Answers
LSTM 92.00% 74.50% 73.80% 62.50% 62.50% 53.00%

word-CNN 96.01% 82.09% 80.10% 69.15% 66.67% 57.71%

Table 2: Classification accuracy of each selected model on the original three datasets and the perturbed datasets
using different attacking methods. Column 3 (Original) represents the classification accuracy of the model for the
original samples. A lower classification accuracy corresponds to a more effective attacking method.

Dataset Model Random Gradient TiWO WS PWWS

IMDB
word-CNN 22.01% 20.53% 15.06% 14.38% 3.81%

Bi-dir LSTM 17.77% 12.61% 4.34% 4.68% 3.38%

AG’s News
char-CNN 27.43% 27.73% 26.46% 21.94% 18.93%
word-CNN 22.22% 22.09% 20.28% 20.21% 16.76%

Yahoo! Answers
LSTM 40.86% 41.09% 37.14% 39.75% 35.10%

word-CNN 31.68% 31.29% 30.06% 30.42% 25.43%

Table 3: Word replacement rate of each attacking method on the selected models for the three datasets. The lower
the word replacement rate, the better the attacking method could be in terms of retaining the semantics of the text.

Original Prediction Adversarial Prediction Perturbed Texts
Positive Negative Ah man this movie was funny (laughable) as hell, yet strange. I like

how they kept the shakespearian language in this movie, it just felt
ironic because of how idiotic the movie really was. this movie has got
to be one of troma’s best movies. highly recommended for some
senseless fun!

Confidence = 96.72% Confidence = 74.78%

Negative Positive The One and the Only! The only really good description of the punk
movement in the LA in the early 80’s. Also, the definitive documentary
about legendary bands like the Black Flag and the X. Mainstream
Americans’ repugnant views about this film are absolutely hilarious
(uproarious)! How can music be SO diversive in a country of
supposed liberty...even 20 years after... find out!

Confidence = 72.40% Confidence = 69.03%

Table 4: Adversarial example instances in the IMDB dataset with Bi-directional LSTM model. Columns 1 and
2 represent the category prediction and confidence of the classification model for the original sample and the
adversarial examples, respectively. In column 3, the green word is the word in the original text, while the red is the
substitution in the adversarial example.

Original Prediction Adversarial Prediction Perturbed Texts
Business Sci/Tech site security gets a recount at rock the vote. grassroots movement to

register younger voters leaves publishing (publication) tools accessible
to outsiders.

Confidence = 91.26% Confidence = 33.81%

Sci/Tech World seoul allies calm on nuclear (atomic) shock. south korea’s key allies
play down a shock admission its scientists experimented to enrich
uranium.

Confidence = 74.25% Confidence = 86.66%

Table 5: Adversarial example instances in the AG’s News dataset with char-based CNN model. Columns of this
table is similar to those in Table 4.

ensuring the semantic and syntactic features of the
original sample remain unchanged to the utmost
extent.

Table 4 lists some adversarial examples gen-
erated for IMDB dataset with the Bi-directional
LSTM classifier. The original positive/negative
film reviews can be misclassified by only one syn-
onym replacement and the model even holds a
high degree of confidence. Table 5 lists some ad-

versarial examples in AG’s News dataset with the
char-based CNN. It also requires only one syn-
onym to be replaced for the model to be misled to
classify one type (Business) of news into another
(Sci/Tech). The adversarial examples still convey
the semantics of the original text such that humans
do not recognize any change but the neural net-
work classifiers are deceived.

For more example comparisons between the ad-



1091

Dataset Model Examples Accuracy of model Accuracy of human Score[1-5]

IMDB
word-CNN Original 99.0% 98.0% 1.80Adversarial 22.0% 93.0% 2.50

Bi-dir LSTM Original 86.0% 93.0% 1.70Adversarial 12.0% 88.0% 2.08

AG’s News char-CNN Original 81.0% 63.9% 2.62Adversarial 69.0% 58.0% 2.89

Table 6: Comparison with human evaluation. The fourth and fifth columns represent the classification accuracy of
the model and human, respectively. The last column represents how much the workers think the text is likely to be
modified by a machine. The larger the score, the higher the probability.

versarial examples generated by different meth-
ods, see details in Appendix.

Text classifier based on DNNs is widely used in
NLP tasks. However, the existence of such adver-
sarial samples exposes the vulnerability of these
models, limiting their applications in security-
critical systems like spam filtering and fake news
detection.

4.5 Discussions on Previous Works

Yang et al. (2018) introduce a perturbation-
based method called Greedy Attack and a scal-
able learning-based method called Gumbel At-
tack. They perform experiments on IMDB dataset
with the same word-based CNN model, and on
AG’s News dataset with a LSTM model. Their
method greatly reduces the classification accuracy
to less than 5% after replacing 5 words (Yang
et al., 2018). However, the semantics of the re-
placement words are not constrained, as antonyms
sometimes appear in their adversarial examples.
Moreover, for instance, Table 3 in (Yang et al.,
2018) shows that they change “... The plot could
give a rise a must (better) movie if the right pieces
was in the right places” to switch from negative to
positive; and they change “The premise is good,
the plot line script (interesting) and the screenplay
was OK” to switch from positive to negative. The
first sample changes the meaning of the sentence,
while the second has grammatical errors. Under
such condition, the perturbations could be recog-
nized by humans.

Gao et al. (2018) present a novel algorithm,
DeepWordBug, that generates small text perturba-
tions in the character-level for black-box attack.
This method can cause a decrease of 68% on av-
erage for word-LSTM and 48% on average for
char-CNN model when 30 edit operations were al-
lowed. However, since their perturbation exists in
the character-level, the generated adversarial ex-
amples often do not conform to the lexical con-
straint: misspelled words may exist in the text. For

instance, they change a positive review of “This
film has a special place in my heart” to get a neg-
ative review of “This film has a special plcae in
my herat”. For such adversarial examples, a spell
check on the input can easily remove the pertur-
bation, and the effectiveness of such adversarial
attack will be removed also. DeepWordBug is
still useful, as we could improve the robustness in
the training of classifiers by replacing misspelled
word with out-of-vocabulary word, or simply re-
move misspelled words. However, as DeepWord-
Bug can be easily defended by spell checking, we
did not consider it as a baseline in our comparison.

5 Further Analysis

This section provides a human evaluation to show
that our perturbation is hard for humans to per-
ceive, and studies the transferability of the gen-
erated examples by our methods. In the end, we
show that using the generated examples for adver-
sarial training helps improving the robustness of
the text classification model.

5.1 Human Evaluation

To further verify that the perturbations in the ad-
versarial examples are hard for humans to recog-
nize, we find six workers on Amazon Mechani-
cal Turk to evaluate the examples generated by
PWWS. Specifically, we select 100 clean texts in
IMDB and the corresponding adversarial exam-
ples generated on word-based CNN. Then we se-
lect another 100 clean texts in IMDB and the cor-
responding adversarial examples generated on Bi-
directional LSTM. For the third group, we select
100 clean texts from AG’s News and the corre-
sponding adversarial examples generated on char-
based CNN. For each group of date, we mix the
clean data and generated examples for the work-
ers to classify. To evaluate the similarity, we ask
the workers to give scores from 1-5 to indicate the
likelihood that the text is modified by machine.



1092

(a) Varying word replacement rates of the algorithms (b) Fixed word replacement rate of 10%

Figure 1: Transferability of adversarial examples generated by different attacking methods on IMDB. The three
color bars represent the average classification accuracies (in percentage) of the three new models on the adversarial
examples generated by word-based CNN-1. The lower the classification accuracy, the better the transferability.

Table 6 shows the comparison with human eval-
uation. The generated examples can cause mis-
classification on three different models, while the
classification accuracy of humans is still very high
comparing to their judgement on clean data. Since
there are four categories for AG’s News, the classi-
fication accuracy of workers on this dataset is sig-
nificantly lower than that on IMDB (binary clas-
sification tasks). Thus, we did not try human
evaluation on Yahoo! Answers as there are 10
categories to classify. The likelihood scores of
machine perturbation on adversarial examples are
slightly higher than that on the original texts, in-
dicating that the semantics of some synonyms are
not as accurate as the original words. Neverthe-
less, as the accuracy of humans on the two sets of
data are close, and the traces of machine modifica-
tions are still hard for humans to perceive.

5.2 Transferability

The transferability of adversarial attack refers to
its ability to reduce the accuracy of other models
to a certain extent when the examples are gener-
ated on a specific classification model (Goodfel-
low et al., 2015; Szegedy et al., 2013).

To illustrate this, we record the original word-
based CNN (described in Section 4.2) as word-
based CNN-1, and train three new proximity clas-
sification models on the IMDB dataset, labeled
respectively as word-based CNN-2, word-based
CNN-3 and Bi-directional LSTM network. Com-
pared to word-based CNN-1, word-based CNN-
2 has an additional fully connected layer. Word-
based CNN-3 has the same network structure as
CNN-1 except using GloVe (Pennington et al.,
2014) as a pretrained word embedding. The net-
work structure of Bi-directional LSTM is the one

introduced in Section 4.2.

When the adversarial examples generated by
our method are transferred to word-based CNN-
2 or Bi-dir LSTM, the attacking effect is slightly
inferior, as illustrated in Figure 1 (a). But note
that the word replacement rate of our method on
IMDB is only 3.81%, which is much lower than
other methods (Table 3). When we use the same
replacement ratio (say 10%) in the input text for
all methods, the transferability of PWWS is sig-
nificantly better than other methods. Figure 1 (b)
illustrates that the word substitution order deter-
mined by PWWS corresponds well to the impor-
tance of the words for classification, and the trans-
formation is effective across various models.

5.3 Adversarial Training

Adversarial training (Shrivastava et al., 2017) is
a popular technique mainly used in image classi-
fication to improve model robustness. To verify
whether incorporating adversarial training would
help improve the robustness of the test classifiers,
we randomly select clean samples from the train-
ing set of IMDB and use PWWS to generate 4000
adversarial examples as a set A, and train the
word-based CNN model. We then evaluate the
classification accuracy of the model on the original
test data and of the adversarial examples generated
using various methods. Figure 2 (a) shows that the
classification accuracy of the model on the original
test set is improved after adversarial training. Fig-
ure 2 (a) illustrates that the robustness of the clas-
sification model continues to improve when more
adversarial examples are added to the training set.



1093

(a) Accuracy on the original test set (b) Accuracy on the adversarial examples generated by various methods

Figure 2: The result of adversarial training on IMDB dataset. The x-axis represents the number of adversarial
examples selected from set A to join the original training set. The classification accuracies are on the original test
set and the adversarial examples generated using various methods, respectively.

6 Conclusion

We propose an effective method called Probability
Weighted Word Saliency (PWWS) for generating
adversarial examples on text classification tasks.
PWWS introduces a new word substitution order
determined by the word saliency and weighted by
the classification probability. Experiments show
that PWWS can greatly reduce the text classifica-
tion accuracy with a low word substitution rate,
and such perturbation is hard for human to per-
ceive.

Our work demonstrates the existence of adver-
sarial examples in discrete input spaces and shows
the vulnerability of NLP models using neural net-
works. Comparison with existing baselines shows
the advantage of our method. PWWS also exhibits
a good transferability, and by performing adver-
sarial training we can improve the robustness of
the models at test time. In the future, we would
like to evaluate the attacking effectiveness and ef-
ficiency of our methods on more datasets and mod-
els, and do elaborate human evaluation on the sim-
ilarity between clean texts and the corresponding
adversarial examples.

References

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei
Chang. 2018. Generating natural language adver-
sarial examples. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, Brussels, Belgium, October 31 - Novem-
ber 4, 2018, pages 2890–2896.

Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su,
Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boost-
ing adversarial attacks with momentum. In The

IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 9185–9193.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. 2018. Hotflip: White-box adversarial exam-
ples for text classification. In Proceedings of the
56th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2018, Melbourne, Aus-
tralia, July 15-20, 2018, Volume 2: Short Papers,
pages 31–36.

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-
jun Qi. 2018. Black-box generation of adversar-
ial text sequences to evade deep learning classifiers.
In 2018 IEEE Security and Privacy Workshops, SP
Workshops 2018, San Francisco, CA, USA, May 24,
2018, pages 50–56.

Ian Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing adversar-
ial examples. In International Conference on Learn-
ing Representations.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
2016. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533.

Jiwei Li, Xinlei Chen, Eduard H. Hovy, and Dan Ju-
rafsky. 2016a. Visualizing and understanding neu-
ral models in NLP. In NAACL HLT 2016, The
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, San Diego California,
USA, June 12-17, 2016, pages 681–691.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-
derstanding neural networks through representation
erasure. CoRR, abs/1612.08220.

Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,
Xirong Li, and Wenchang Shi. 2018. Deep text

http://arxiv.org/abs/1412.6572
http://arxiv.org/abs/1412.6572
http://arxiv.org/abs/1612.08220
http://arxiv.org/abs/1612.08220
http://arxiv.org/abs/1612.08220


1094

classification can be fooled. In Proceedings of the
Twenty-Seventh International Joint Conference on
Artificial Intelligence, IJCAI 2018, July 13-19, 2018,
Stockholm, Sweden., pages 4208–4215.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In The 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA, pages 142–150.

Anh Mai Nguyen, Jason Yosinski, and Jeff Clune.
2015. Deep neural networks are easily fooled: High
confidence predictions for unrecognizable images.
In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2015, Boston, MA, USA, June 7-
12, 2015, pages 427–436.

Nicolas Papernot, Patrick D. McDaniel, Somesh Jha,
Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. 2016a. The limitations of deep learning
in adversarial settings. In IEEE European Sym-
posium on Security and Privacy, EuroS&P 2016,
Saarbrücken, Germany, March 21-24, 2016, pages
372–387.

Nicolas Papernot, Patrick D. McDaniel, Ananthram
Swami, and Richard E. Harang. 2016b. Crafting
adversarial input sequences for recurrent neural net-
works. In 2016 IEEE Military Communications
Conference, MILCOM 2016, Baltimore, MD, USA,
November 1-3, 2016, pages 49–54.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Suranjana Samanta and Sameep Mehta. 2017. To-
wards crafting text adversarial samples. CoRR,
abs/1707.02812.

Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji
Matsumoto. 2018. Interpretable adversarial pertur-
bation in input embedding space for text. In Pro-
ceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence, IJCAI 2018,
July 13-19, 2018, Stockholm, Sweden., pages 4323–
4330.

Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb. 2017.
Learning from simulated and unsupervised images
through adversarial training. In CVPR, volume 2,
page 5.

Chuanbiao Song, Kun He, Liwei Wang, and John E
Hopcroft. 2019. Improving the generalization of ad-
versarial training with domain adaptation. In The
Seventh International Conference on Learning Rep-
resentations, New Orleans, Louisiana.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. 2013. Intriguing properties of neural
networks. CoRR, abs/1312.6199.

Florian Tramèr, Alexey Kurakin, Nicolas Papernot,
Ian Goodfellow, Dan Boneh, and Patrick McDaniel.
2018. Ensemble adversarial training: Attacks and
defenses. In International Conference on Learning
Representations.

Eric Wong and J. Zico Kolter. 2018. Provable defenses
against adversarial examples via the convex outer
adversarial polytope. In Proceedings of the 35th In-
ternational Conference on Machine Learning, ICML
2018, Stockholmsmässan, Stockholm, Sweden, July
10-15, 2018, pages 5283–5292.

Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling
Wang, and Michael I. Jordan. 2018. Greedy attack
and gumbel attack: Generating adversarial examples
for discrete data. CoRR, abs/1805.12316.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun.
2015. Character-level convolutional networks for
text classification. In Annual Conference on Neu-
ral Information Processing Systems 2015, Decem-
ber 7-12, 2015, Montreal, Quebec, Canada, pages
649–657.

Appendix

In the Appendix, we add more comparisons be-
tween the adversarial examples generated by dif-
ferent methods, and comparisons between the
original examples and the adversarial examples.

https://doi.org/10.1109/EuroSP.2016.36
https://doi.org/10.1109/EuroSP.2016.36
http://arxiv.org/abs/1707.02812
http://arxiv.org/abs/1707.02812
http://arxiv.org/abs/1312.6199
http://arxiv.org/abs/1312.6199
http://arxiv.org/abs/1805.12316
http://arxiv.org/abs/1805.12316
http://arxiv.org/abs/1805.12316


1095

Attack
Perturbed Texts

Methods
Random The One and the Only (Solitary) ! Agreed this movie (pic) is well (comfortably) shot (hit), but it just

(scarcely) makes no sense (mother) and no use (enjoyment) as to how they made 2 hours seem like 3 (7) just
(scarcely) over a small (belittled) love (honey) story (taradiddle), this could have been an episode (sequence)
of the bold (sheer) and the beautiful or the o.c, in short please don’t watch (learn) this movie (pic) because
there is a song every 5 minutes just to wake (stir) you up from you’re sleep (quietus), i gave this movie (pic)
1/10! cause (induce) that was the lowest, and no this is not based completely on a true story, more than half of
it is made up. I repeat the direction of photography is 7 or 8 out of 10, but the movie is just a little too much,
the actor’s nasal voice just makes me want to go blow my nose. Unless you are a real him mesh fan this movie
is a huge no-no.

Confidence
= 88.14%

Gradient The One and the Only (Solitary) ! Agreed this movie (pic) is well (easily) shot (hit), but it just (scarcely)
makes no sense (gumption) and no use (enjoyment) as to how they made 2 hours seem like 3 (7) just (simply)
over a small (belittled) love (honey) story (taradiddle), this could have been an episode (sequence) of the bold
(bluff ) and the beautiful or the o.c, in short please don’t watch (learn) this movie (pic) because there is a song
every 5 minutes just to wake (stir) you up from you’re sleep (quietus), i gave this movie (pic) 1/10! cause
(induce) that was the lowest, and no this is not based completely on a true story, more than half of it is made
up. I repeat the direction of photography is 7 or 8 out of 10, but the movie is just a little too much, the actor’s
nasal voice just makes me want to go blow my nose. Unless you are a real him mesh fan this movie is a huge
no-no.

Confidence
= 89.49%

TiWO The One and the Only (Solitary) ! Agreed this movie (film) is well (easily) shot (hit), but it just (simply)
makes no sense and no use (manipulation) as to how they made 2 hours seem like 3 (7) just (simply) over a
small (humble) love (passion) story (level), this could have been an episode (sequence) of the bold (sheer)
and the beautiful or the o.c, in short please don’t watch (keep) this movie (film) because there is a song every 5
minutes just to wake you up from you’re sleep (quietus), i gave this movie (motion) 1/10 (7)! cause (induce)
that was the lowest, and no this is not based completely on a true story, more than half of it is made up. I repeat
the direction of photography is 7 or 8 out of 10, but the movie is just a little too much, the actor’s nasal voice
just makes me want to go blow my nose. Unless you are a real him mesh fan this movie is a huge no-no.

Confidence
= 57.76%

WS The One and the Only (Solitary) ! Agreed this movie is well shot (hit), but it just (simply) makes no sense and
no use as to how they made 2 hours seem like 3 just over a small (belittled) love (passion) story (taradiddle),
this could have been an episode of the bold and the beautiful or the o.c, in short please don’t watch this movie
because there is a song every 5 minutes just to wake you up from you’re sleep (quietus), i gave this movie
(motion) 1/10! cause (induce) that was the lowest, and no this is not based (found) completely (wholly) on a
true story (level), more than half of it is made up. I repeat the direction of photography (picture) is 7 or 8 (7)
out of 10 (7), but the movie is just a little too much, the actor’s nasal voice just makes me want to go blow my
nose (nozzle). Unless you are a real him mesh fan this movie is a huge no-no.

Confidence
= 50.04%

PWWS The One and the Only! Agreed this movie is well shot, but it just makes no sense and no use as to how they
made 2 hours seem like 3 just over a small love story, this could have been an episode of the bold and the
beautiful or the o.c, in short please don’t watch this movie because there is a song every 5 minutes just to wake
you up from you’re sleep, i gave this movie 1/10 (7)! cause that was the lowest, and no this is not based
completely on a true story, more than half of it is made up. I repeat the direction of photography is 7 or 8 out
of 10, but the movie is just a little too much, the actor’s nasal voice just makes me want to go blow my nose.
Unless you are a real him mesh fan this movie is a huge no-no.

Confidence
= 89.77%

Table 7: Adversarial examples generated for the same clean input text using different attack methods on word-
based CNN. We select a clean input text from the IMDB. The correct category of the original text is negative, and
the classification confidence of word-based CNN is 82.77%. The adversarial examples generated by all methods
succeeded in making the model misclassify from negative class into positive class. There is only one word sub-
stitution needed in our approach(PWWS) to make the attack successful, and it also maintains a high degree of
confidence in the classification of wrong class.



1096

Original Adversarial
Perturbed Texts

Prediction Prediction
Positive Negative This is a great (big) show despite many negative user reviews. The aim of this show is to

entertain you by making you laugh. Two guys compete against each other to get a girl’s phone
number. Simple. The fun in this show is watching the two males try to accomplish their goal.
Some appear to hate the show for various reasons, but I think, they misunderstood this as an
”educational” show on how to pick up chicks. Well it is not, it is a comedy show, and the whole
point of it is to make you laugh, not teach you anything. If you didn’t like the show, because it
doesn’t teach you anything, don’t watch it. If you don’t like the whole clubbing thing, don’t
watch it. If you don’t like socializing don’t watch it. This show is a comical show. If you down
by watching others pick up girls, well its not making you laugh, so don’t watch it. If you are so
disappointed in yourself after watching this show and realizing that you don’t have the ability to
”pick-up” girls, there is no reason to hate the show, simply don’t watch it!”

Confidence Confidence
= 59.56% = 87.76%

Positive Negative I have just watched the season 2 finale of Doctor Who, and apart from a couple of dull episodes
this show is fantastic (tremendous). Its a sad loss that we say goodbye to a main character once
again in the season final but the show moves on. The BBC does need to increase the budget on
the show, there are only so many things that can happen in London and the surrounding areas.
Also some of the special effects all though on the main very good, on the odd occasion do need
to be a little more polished. It was a huge gamble for the BBC to bring back a show that lost its
way a long time ago and they must be congratulated for doing so. Roll on to the Christmas 2006
special, the 2005 Christmas special was by far the best thing on television.”

Confidence Confidence
= 65.10% = 60.03%

Negative Positive The One and the Only! Agreed this movie is well shot, but it just makes no sense and no use as
to how they made 2 hours seem like 3 just over a small love story, this could have been an
episode of the bold and the beautiful or the o.c, in short please don’t watch this movie because
there is a song every 5 minutes just to wake you up from you’re sleep, i gave this movie 1/10 (7)!
cause that was the lowest,and no this is not based completely on a true story, more than half of it
is made up. I repeat the direction of photography is 7 or 8 out of 10, but the movie is just a little
too much, the actor’s nasal voice just makes me want to go blow my nose. Unless you are a real
him mesh fan this movie is a huge no-no.

Confidence Confidence
= 81.73% = 89.77%

Negative Positive In all, it took me three (7) attempts to get through this movie. Although not total trash, I’ve
found a number of things to be more useful to dedicate my time to, such as taking off my
fingernails with sandpaper. The actors involved have to feel about the same as people who star in
herpes medication commercials do; people won’t really pay to see either, the notoriety you earn
won’t be the best for you personally, but at least the commercials get air time.The first one was
bad, but this gave the word bad a whole new definition, but it does have one good feature: if your
kids bug you about letting them watch R-rated movies before you want them to, tie them down
and pop this little gem in. Watch the whining stop and the tears begin. ;)

Confidence Confidence
= 69.54% = 79.15%

Negative Positive This is a very strange (unusual) film, with a no-name cast and virtually nothing known about it
on the web. It uses an approach familiar to those who have watched the likes of Creepshow in
that it introduces a trilogy of so-called ”horror” shorts and blends them together into a
connecting narrative of the people who are involved in the segments getting off a bus. There is a
narrator who prattles on about relationships, but his talking adds absolutely nothing to the mix at
all and just adds to the confusion. As for the stories themselves, well.. I swear I have not got a
clue why this movie got an 18 (7) certificate in the UK, which would bring it into line with the
likes of Nightmare On Elm Street and The Exorcist. Nothing here is even remotely scary.. there
is no gore, sex, nudity or even a swear word to liven things up, this is the kind of thing you could
put out on Children’s TV and no-one would bat an eyelid. I can only think if it had got the rating
it truly deserved (a PG) no serious horror fan would be seen dead with it, so the distributor
probably buffeted the BBFC until they relented. Anyway, here are the 3 (7) tales in summary: 1.
A man becomes dangerously obsessed with his telekinetic car to the point of alienating his
fiancee. 2. A man who lives in a filthy apartment is understandably freaked out when a living
organism evolved from his six-month old tuna casserole. 3. A woman thinks she has found the
perfect man through a computer dating service.. that is until he starts to act weird.. And there
you have it. Some of them are pretty amusing due to their outlandish premises (my favourite
being number 2) but you get the feeling they were meant to be a) frightening and b) morality
plays, unfortunately they fail miserably on both counts. To sum up then, this flick is an obscure
curiosity.. for very good reasons.”

Confidence Confidence
= 83.24% = 52.19%

Table 8: More adversarial examples instances in IMDB with word-based CNN model. The last three instances in
this table show the role of named entities(NEs) in PWWS. The true label of the last three examples are all negative,
and we use most frequently occurring cardinal number 7 in the dictionary of positive class as an NEadv . The
adversarial examples can be generated by replacing few cardinal number in the original input text with 7.



1097

Original Adversarial
Perturbed Texts

Prediction Prediction
Sci/Tec Business

surviving biotech (biotechnology)’s downturns. charly travers offers advice on withstanding the
volatility (excitability) of the biotech sector.Confidence Confidence

= 45.46% = 43.19%
Sci/Tech World

e-mail scam targets police chief (headman). wiltshire police warns about ”phishing” after its
fraud squad chief was targeted.

Confidence Confidence
= 36.85% = 43.21%

World Sports post-olympic greece tightens purse, sells family silver to fill budget holes (afp). afp - squeezed
by a swelling public deficit (shortage) and debt following last month’s costly athens olympics,
the greek government said it would cut defence spending and boost revenue by 1.5 billion euros
(1.84 billion dollars) in privatisation receipts.

Confidence Confidence
= 45.73% = 38.48%

Sci/Tech Sports prediction unit helps forecast (calculate) wildfires (ap). ap - it’s barely dawn when mike
fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he
knows what the day will bring. lightning will strike in places he expects. winds will pick up,
moist places will dry and flames will roar.

Confidence Confidence
= 36.08% = 29.73%

Table 9: Adversarial example instances in the AG’s News dataset with char-based CNN model.

Original Adversarial
Perturbed Texts

Prediction Prediction
Business Games hess truck values at a garage sale im selling some extra hess trucks at a garage sale i have all

years in boxes between except for if anyone can give me price recomendations or even a good
(unspoilt) offer before saturday it would really be apprechiated look on e bay to see what they
are fetching there my guess would be that the issue could go for about us and the most recent
could be about (well) more than what you paid Filling station Ford Motor Company Truck
Supply and demand Pickup truck Illegal drug trade Best Buy Supermarket Value added tax
(taxation) Microeconomics DVD Labor theory of value Postage stamps and postal history of the
United States Price discrimination Auction Investment bank Costco Law of value $ale of the
Century MMORPG Tax CPU (mainframe) cache Mutual fund Islamic banking Ford
Thunderbird Ford F-Series Sales promotion Napoleon Dynamite Internet fraud The Market for
Lemons Argos (retailer) Berkshire Hathaway Gasoline (Petrol) Bond Car and Driver Ten Best
First-sale doctrine Short selling UK Singles Chart Exchange value Altair 8800 Contract Card
Sharks Life insurance Endgame Deal or No Deal Topps Ashton-Tate Hybrid vehicle Externality
Google Boeing 747 Wheel of Fortune US and Canadian license plates Home Box Office Day
trading Chevrolet El Camino Branch predictor Temasek Holdings Toyota Camry The Standard
(Monetary) Privatization Protectionism Car (Railroad) boot (rush) sale Land Rover
(Series/Defender (Shielder)) Long Beach, California Labor-power Capital accumulation BC
Rail ITunes Music Store Moonshine Dead Kennedys Prices of production Massachusetts Bay
Transportation Authority National Lottery E85 MG Rover Group Ford Falcon Fair market value
Wayne Corporation Garage rock Donald Trump Paris Hilton DAF Trucks Economics Firefighter
Commodity Mortgage My Little Pony (Jigger) Electronic Arts (Graphics) Sport utility vehicle
Computer and video (television) games Mitsubishi Motors Corporation American Broadcasting
Company Videocassette recorder Electronic commerce Dodge Charger Alcohol fuel Hudson’s
Bay Company Biodiesel.

and and
Finance Recreation

Confidence Confidence
= 10.04% = 10.01%

Table 10: Adversarial example instances in the Yahoo! Answers dataset with LSTM model.


