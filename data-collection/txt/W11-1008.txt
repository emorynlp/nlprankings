










































Reestimation of Reified Rules in Semiring Parsing and Biparsing


Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70–78,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c©2011 Association for Computational Linguistics

Reestimation of Reified Rules in Semiring Parsing and Biparsing

Markus Saers and Dekai Wu
Human Language Technology Center

Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology

{masaers|dekai}@cs.ust.hk

Abstract

We show that reifying the rules from hyper-
edge weights to first-class graph nodes au-
tomatically gives us rule expectations in any
kind of grammar expressible as a deductive
system, without any explicit algorithm for cal-
culating rule expectations (such as the inside-
outside algorithm). This gives us expecta-
tion maximization training for any grammar
class with a parsing algorithm that can be
stated as a deductive system, for free. Having
such a framework in place accelerates turn-
over time for experimenting with new gram-
mar classes and parsing algorithms—to imple-
ment a grammar learner, only the parse forest
construction has to be implemented.

1 Introduction

We propose contextual probability as a quantity that
measures how often something has been used in
a corpus, and when calculated for rules, it gives
us everything needed to calculate rule expectations
for expectation maximization. For labeled spans in
context-free parses, this quantity is called outside
probability, and in semiring (bi-) parsing, it is called
reverse value. The inside-outside algorithm for rees-
timating context-free grammar rules uses this quan-
tity for the symbols occurring in the parse forest.
Generally, the contextual probability is:

The contextual probability of something
is the sum of the probabilities

of all contexts where it was used.

For symbols participating in a parse, we could state
it like this:

The contextual probability of an item
is the sum of the probabilities

of all contexts where it was used.

. . . which is exactly what we mean with outside
probability. In semiring (bi-) parsing, this quantity
is called reverse value, but in this framework it is
also defined for rules, which means that we could
restate our boxed statement as:

The contextual probability of a rule
is the sum of the probabilities

of all contexts where it was used.

This opens up an interesting line of inquiry into what
this quantity might represent. In this paper we show
that the contextual probabilities of the rules contain
precisely the new information needed in order to cal-
culate the expectations needed to reestimate the rule
probabilities. This line of inquiry was discovered
while working on a preterminalized version of lin-
ear inversion transduction grammars (LITGs), so we
will use these preterminalized LITGs (Saers and Wu,
2011) as an example throughout this paper.

We will start by examining semiring parsing
(parsing as deductive systems over semirings, Sec-
tion 3), followed by a section on how this relates to
weighted hypergraphs, a common representation of
parse forests (Section 4). This reveals a disparity be-
tween weighted hypergraphs and semiring parsing.
It seems like we are forced to choose between the
inside-outside algorithm for context-free grammars

70



on the one side, and the flexibility of grammar for-
malism and parsing algorithm development afforded
by semiring (bi-) parsing. It is, however, possible to
have both, which we will show in Section 5. An
integral part of this unification is the concept of con-
textual probability. Finally, we will offer some con-
clusions in Section 6.

2 Background

A common view on probabilistic parsing—be it
bilingual or monolingual—is that it involves the
construction of a weighted hypergraph (Billot and
Lang, 1989; Manning and Klein, 2001; Huang,
2008). This is an appealing conceptualization, as it
separates the construction of the parse forest (the ac-
tual hypergraph) from the probabilistic calculations
that need to be carried out. The calculations are,
in fact, given by the hypergraph itself. To get the
probability of the sentence (pair) being parsed, one
simply have to query the hypergraph for the value
of the goal node. It is furthermore possible to ab-
stract away the calculations themselves, by defining
the hypergraph over an arbitrary semiring. When the
Boolean semiring is used, the value of the goal node
will be true if the sentence (pair) is a member of the
language (or transduction) defined by the grammar,
and false otherwise. When the probabilistic semir-
ing is used, the probability of the sentence (pair) is
attained, and with the tropical semiring, the proba-
bility of the most likely tree is attained. To further
generalize the building of the hypergraph—the pars-
ing algorithm—a deductive system can be used. By
defining a hand-full of deductive rules that describe
how items can be constructed, the full complexi-
ties of a parsing algorithm can be very succinctly
summarized. Deductive systems to represent parsers
and semirings to calculate the desired values for the
parses were introduced in Goodman (1999).

In this paper we will reify the grammar rules
by moving them from the meta level to the object
level—effectively making them first-class citizens of
the parse trees, which are no longer weighted hyper-
graphs, but mul/add-graphs. This move allows us
to calculate rule expectations for expectation maxi-
mization (Dempster et al., 1977) as part of the pars-
ing process, which significantly shortens turn-over
time for experimenting with different grammar for-

malisms.
Another approach which achieve a similar goal is

to use a expectation semiring (Eisner, 2001; Eisner,
2002; Li and Eisner, 2009). In this semiring, all val-
ues are pairs of probabilities and expectations. The
inside-outside algorithm with the expectation semir-
ing requires the usual inside and outside calcula-
tions over the probability part of the semiring val-
ues, followed by a third traversal over the parse for-
est to populate the expectation part of the semiring
values. The approach taken in this paper also re-
quires the usual inside and outside calculations, but
o third traversal of the parse forest. Instead, the pro-
posed approach requires two passes over the rules
of the grammar per EM iteration. The asymptotic
time complexities are thus equivalent for the two ap-
proaches.

2.1 Notation

We will use w to mean a monolingual sentence,
and index the individual tokens from 0 to |w| − 1.
This means that w = w0, . . . , w|w|−1. We will fre-
quently use spans from this sentence, and denote
them wi..j , which is to be interpreted as array slices,
that is: including the token at position i, but ex-
cluding the token at position j (the interval [i, j)
over w, or wi, . . . , wj−1). A sentence w thus cor-
responds to the span w0..|w|. We will also assume
that there exists a grammar G = 〈N,Σ, S,R〉 or a
transduction grammar (over languages L0 and L1)
G = 〈N,Σ,∆, S,R〉 (depending on the context),
where N is the set of nonterminal symbols, Σ is a
set of (L0) terminal symbols, ∆ is a set of (L1) ter-
minal symbols, S ∈ N is the dedicated start symbol
and R is a set of rules appropriate to the grammar.
A stochastic grammar is further assumed to have a
parameterization function θ, that assigns probabili-
ties to all the rules in R. For general L0 tokens we
will use lower case letters from the beginning of the
alphabet, and for L1 from the end of the alphabet.
For specific sentences we will use e = e0..|e| to rep-
resent an L0 sentence and f = f0..|f | to represent an
L1 sentence.

3 Semiring parsing

Semiring parsing was introduced in Goodman
(1999), as a unifying approach to parsing. The gen-

71



eral idea is that any parsing algorithm can be ex-
pressed as a deductive system. The same algorithm
can then be used for both traditional grammars and
stochastic grammars by changing the semiring used
in the deductive system. This approach thus sepa-
rates the algorithm from the specific calculations it
is used for.

Definition 1. A semiring is a tuple 〈A,⊕,⊗,0,1〉,
where A is the set the semiring is defined over, ⊕ is
an associative, commutative operator over A, with
identity element 0 and ⊗ is an associative operator
over A distributed over ⊕, with identity element 1.
Semirings can be intuitively understood by consid-
ering the probabilistic semiring: 〈R+,+,×, 0, 1〉,
that is: the common meaning of addition and
multiplication over the positive real numbers (in-
cluding zero). Although this paper will have a
heavy focus on the probabilistic semiring, sev-
eral other exists. Among the more popular are
the Boolean semiring 〈{>,⊥},∨,∧,⊥,>〉 and the
tropical semiring 〈R+ ∪ {∞},min,+,∞, 0〉 (or
〈R− ∪ {−∞},max,+,−∞, 0〉 which can be used
for probabilities in the logarithmic domain).

The deductive systems used in semiring parsing
have three components: an item representation, a
goal item and a set of deductive rules. Taking
CKY parsing (Cocke, 1969; Kasami and Torii, 1969;
Younger, 1967) as an example, the items would have
the form Ai,j , which is to be interpreted as the span
wi..j of the sentence being parsed, labeled with the
nonterminal symbol A. The goal item would be
S0,|w|: the whole sentence labeled with the start
symbol of the grammar. Since the CKY algorithm
is a very simple parsing algorithm, it only has two
deductive rules:

A→ a, Ia(wi..j)
Ai,j

0≤i≤j≤|w| (1)

Bi,k, Ck,j , A→ BC
Ai,j

(2)

Where Ia(·) is the terminal indicator function for the
semiring. The general form of a deductive rule is
that the conditions (entities over the line) yield the
consequence (the entity under the line) given that
the side conditions (to the right of the line) are satis-
fied. We will make a distinction between conditions
that are themselves items, and conditions that are

not. The non-item conditions will be called axioms,
and are exemplified above by the indicator function
(Ia(wi..j) which has a value that depends only on the
sentence) and the rules (A→ a andA→ BC which
have values that depends only on the grammar).

The indicator function might seem unnecessary,
but allows us to reason under uncertainty regarding
the input. In this paper, we will assume that we have
perfect knowledge of the input (but for generality,
we will not place it as a side condition). The func-
tion is defined such that:

∀a ∈ Σ∗ : Ia(w) =

{
1 if a = w
0 otherwise

An important concept of semiring parsing is that
the deductive rules also specify how to arrive at the
value of the consequence. Since it is the first value
computed for a node, we will call it α, and the gen-
eral way to calculate it given a deductive rule and the
α-values of the conditions is:

α(b) =
n⊗
i=1

α(ai) iff
a1, . . . , an

b
c1,...,cm

If the same consequence can be produced in several
ways, the values are summed using the ⊕ operator:

α(b) =
⊕

n,a1,...,an
such that
a1,...,an

b

n⊗
i=1

α(ai)

The α-values of axioms depend on what kind of ax-
iom it is. For the indicator function, the α-value is
the value of the function, and for grammar rules, the
α-value is the value assigned to the rule by the pa-
rameterization function θ of the grammar.

The α-value of a consequence corresponds to the
value of everything leading up to that consequence.
If we are parsing with a context-free grammar and
the probabilistic semiring, this corresponds to the in-
side probability.

3.1 Reverse values
When we want to reestimate rule probabilities, it is
not enough to know the probabilities of arriving at
different consequences, we also need to know how
likely we are to need the consequences as a condi-
tion for other deductions. These values are called

72



S → A
A0,|e|,0,|f |

,
As,s,u,u, A→ �/�

G
,

Bs′,t,u′,v, B → [XA], X → a/x , Ia/x ( es..s′/fu..u′ )
As,t,u,v

0≤s≤s′,
0≤u≤u′,

Bs,t′,u,v′ , B → [AX], X → a/x , Ia/x ( et′..t/fv′..v )
As,t,u,v

t′≤t≤|e|,
v′≤v≤|f |,

Bs′,t,u,v′ , B → 〈XA〉, X → a/x , Ia/x ( es..s′/fv′..v )
As,t,u,v

0≤s≤s′,
v′≤v≤|f |,

Bs,t′,u′,v, B → 〈AX〉, X → a/x , Ia/x ( et′..t/fu..u′ )
As,t,u,v

t′≤t≤|e|,
0≤u≤u′

Figure 2: Deductive system describing a PLITG parser. The symbols A, B and S are nonterminal symbols, while X
represents a preterminal symbol.

S → A
A0,|e|,0,|f |

,
As,s,u,u, A→ �/�

G
,

Bs′,t,u′,v, B → [a/x A], Ia/x ( es..s′/fu..u′ )
As,t,u,v

0≤s≤s′,
0≤u≤u′,

Bs,t′,u,v′ , B → [A a/x ], Ia/x ( et′..t/fv′..v )
As,t,u,v

t′≤t≤|e|,
v′≤v≤|f |,

Bs′,t,u,v′ , B → 〈a/x A〉, Ia/x ( es..s′/fv′..v )
As,t,u,v

0≤s≤s′,
v′≤v≤|f |,

Bs,t′,u′,v, B → 〈A a/x 〉, Ia/x ( et′..t/fu..u′ )
As,t,u,v

t′≤t≤|e|,
0≤u≤u′

Figure 1: Deductive system describing an LITG parser.

reverse values in Goodman (1999), and outside
probabilities in the inside-outside algorithm (Baker,
1979). In this paper we will call them contextual
values, or β-values (since they are the second value
we calculate).

The way to calculate the reverse values is to start
with the goal node and work your way back to the
axioms. The reverse value is calculated to be:

β(x) =
⊕

n,i,b,a1,...,an
such that

a1,...,an
b

∧x=ai

β(b)⊗
⊗

{j|1≤j≤n,j 6=i}

α(aj)

That is: the reverse value of the consequence com-
bined with the values of all sibling conditions is cal-
culated and summed for all deductive rules where

the item is a condition.

3.2 SPLITG
After we introduced stochastic preterminalized
LITGs (Saers, 2011, SPLITG), the idea of express-
ing them in term of semiring parsing occurred. This
is relatively straight forward, producing a compact
set of deductive rules similar to that of LITGs. For
LITGs, the items take the form of bispans labeled
with a symbol. We will represent these bispans as
As,t,u,v, where A is the label, and the two spans be-
ing labeled are es..t and fu..v. Since we usually do
top-down parsing, the goal item is a virtual item (G)
than can only be reached by rewriting a nontermi-
nal to the empty bistring ( �/� ). Figure 1 shows the
deductive rules for LITG parsing.

A preterminalized LITG promote preterminal
symbols to a distinct class of symbols in the gram-
mar, which is only allowed to rewrite into bitermi-
nals. Factoring out the terminal productions in this
fashion allows the grammar to define one probability
distribution over all the biterminals, which is useful
for bilexica induction. It also means that the LITG
rules that produce biterminals have to be replaced
by two rules in a PLITG, resulting in the deductive
rules in Figure 2.

4 Weighted hypergraphs

A hypergraph is a graph where the nodes are con-
nected with hyperedges. A hyperedge is an edge
that can connect several nodes with one node—it has

73



Figure 3: A weighted hyperedge between three nodes,
based on the rule A → BC. The tip of the arrow points
to the head of the edge, and the two ends are the tails. The
dashed line idicates where the weight of the edge comes
from.

one head, but may have any number of tails. Intu-
itively, this is a good match to context-free gram-
mars, since each rule connects one symbol on the
left hand side (the head of the hyperedge) with any
number of symbols on the right hand side (the tails
of the hyperedge). During parsing, one node is con-
structed for each labeled (bi-) span, and the nodes
are connected with hyperedges based on the valid
applications of rules. A hyperedge will be repre-
sented as [h : t1, . . . , tn] where h is the head and ti
are the tails.

When this is applied to weighted grammar, each
hyperedge can be associated with a weight, making
the hypergraph weighted. Every time an edge is tra-
versed, its weight is combined with the value travel-
ling through the edge. Weights are assigned to hy-
peredges via a weighting function w(·).

Figure 3 contains an illustration of a weighted
hyperedge. The arrow indicates the edge itself,
whereas the dotted line indicates where the weight
comes from. Since each hyperedge corresponds to
exactly one rule from a stochastic context-free gram-
mar, we can use the inside-outside algorithm (Baker,
1979) to calculate inside and outside probabilities as
well as to reestimate the probabilities of the rules.
What we cannot easily do, however, is to change the
parsing algorithm or grammar formalism.

If the weighted hyperedge approach was a one-to-
one mapping to the semiring parsing approach, we
could, but it is not. The main difference is that rules
are part of the object level in semiring parsing, but

Figure 4: The same hyperedge as in Figure 3, where the
rule has been promoted to first-class citizen. The hyper-
edge is no longer weighted.

part of the meta level in weighted hypergraphs. To
address this disparity, we will reify the rules in the
weighted hypergraph to make them nodes. Figure 4
shows the same hyperedge as Figure 3, but with the
rule as a proper node rather than a weight associ-
ated with the hyperedge. These hyperedges are ag-
nostic to what the tail nodes represent, so we can no
longer use the inside-outside algorithm to reestimate
the rule probabilities. We can, however, still calcu-
late inside probabilities. In the weighted hyperedge
approach, the inside probability of a node is:

α(p) =
⊕

n,q1,...,qn
such that

[p:q1,...qn]

w([p : q1, . . . , qn])⊗
n⊗
i=1

α(qi)

Whereas with the rules reified, the weight simply
moved into the tail product:

α(p)
⊕

n,q1,...,qn
such that

[p:q1,...qn]

n⊗
i=1

α(qi)

By virtue of the deductive system used to build the
hypergraph, we also have the reverse values, which
correspond to outside probability:

β(x) =
⊕

i,p,n,q1,...,qn
such that

[p:q1,...qn]∧x=qi

β(p)⊗
⊗

{j|1≤j≤n,j 6=i}

α(qj)

This means that we have the inside and outside prob-
abilities of the nodes, and we could shoe-horn it into
the reestimation part of the inside-outside algorithm.

74



It also means that we have β-values for the rules,
which we are calculating as a side-effect of moving
them into the object level. In Section 5, we will take
a closer look at the semantics of the contextual prob-
abilities that we are in fact calculating for the reified
rules, and see how they can be used in reestimation
of the rules.

4.1 SPLITG

Using the hypergraph parsing framework for
SPLITGs turns out to be non-trivial. Where the stan-
dard LITG uses one rule to rewrite a nonterminal into
another nonterminal and a biterminal, the SPLITG
rewrites a nonterminal to a preterminal and a non-
terminal, and rewrites the preterminal into a biter-
minal. This causes problems within the hypergraph
framework, where each rule application should cor-
respond to one hyperedge. As it stands we have two
options:

1. Let each rule correspond to one hyperedge,
which means that we need to introduce preter-
minal nodes into the hypergraph. This has
a clear drawback for bracketing grammars,1

since it is now necessary to keep different sym-
bols apart. It also produces larger hypergraphs,
since the number of nodes is inflated.

2. Let hypergraphs be associated with one or two
rules, which means that we need to redefine hy-
peredges so that there are two different weight-
ing functions: one for the nonterminal weight
and one for the preterminal weight. Although
all hyperedges are associated with one nonter-
minal rule, some hyperedges are not associated
with any preterminal rule, making the pretermi-
nal weighting function partly defined.

Both of these approaches work in practice, but nei-
ther is completely satisfactory since they both rep-
resent work-arounds to shoe-horn the parsing algo-
rithm (as stated in the deductive system) into a for-
malism that is not completely compatible. By reify-
ing the rules into the object level, we rid ourselves
of this inconvenience, as we no longer differentiate
between different types of conditions.

1A bracketing grammar is a grammar where |N | = 1.

5 Reestimation of reified rules

As has been amply hinted at, the contextual prob-
abilities (outside probabilities, reverse values or β-
values) contain all new information we need about
the rules to reestimate their probability in an expec-
tation maximization (Dempster et al., 1977) frame-
work. To show that this is indeed the case, we
will rewrite the reestimation formulas of the inside-
outside algorithm (Baker, 1979) so that they are
stated in terms of contextual probability for the
rules.

In general, a stochastic context-free grammar can
be estimated from examples of trees generated by
the grammar by means of relative frequency. This
is also true for expectation maximization with the
caveat that we have multiple hypotheses over each
sentence (pair), and therefore calculate expectations
rather than discrete frequency counts. We thus com-
pute the updated parameterization function θ̂ based
on expectations from the current parameterization
function:

θ̂ (ϕ|p) = Eθ [p→ ϕ]
Eθ [p]

Where p ∈ N and ϕ ∈ {Σ ∪ N}+ (or ϕ ∈
{(Σ∗×∆∗)∪N}+ for transduction grammars). The
expectations are calculated from the sentences in a
corpus C:

Eθ [x] =
∑
w∈C

Eθ [x|w]

The exact way of calculating the expectation on x
given a sentence depends on what x is. For nonter-
minal symbols, the expectations are given by:

Eθ [p|w] =
Eθ [p,w]

Eθ [w]

=

∑
0≤i≤j≤|w| Pr (pi,j ,w|G)

Pr (w|G)

=

∑
0≤i≤j≤|w| α(pi,j)β(pi,j)

α(S0,|w|)β(S0,|w|)

For nonterminal rules, the expectations are shown in
Figure 5. The most noteworthy step is the last one,
where we use the fact that the summation is over
the equivalence of the rule’s reverse value. Each

75



Eθ [p→ qr|w] =
Eθ [p→ qr,w]

Eθ [w]

=

∑
0≤i≤k≤j≤|w| Pr

(
w0..i, pi,j , wj..|w|

∣∣G)Pr (wi..k|qi,k, G) Pr (wk..j |rk,j , G) θ (qr|p)
Pr (w|G)

=

∑
0≤i≤k≤j≤|w| β(pi,j)α(qi,k)α(rk,j)θ (qr|p)

α(S0,|w|)β(S0,|w|)

=
θ (qr|p)

∑
0≤i≤k≤j≤|w| β(pi,j)α(qi,k)α(rk,j)

α(S0,|w|)β(S0,|w|)
=

α(p→ qr)β(p→ qr)
α(S0,|w|)β(S0,|w|)

Figure 5: Expected values for nonterminal rules in a specific sentence.

Eθ [p→ a|w] =
Eθ [p→ a,w]

Eθ [w]

=

∑
0≤i≤j≤|w| Pr

(
w0..i, pi,j , wj..|w|

∣∣G) Ia(wi..j)θ (a|p)
Pr (w|G)

=

∑
0≤i≤j≤|w| β(pi,j)Ia(wi..j)θ (a|p)

α(S0,|w|)β(S0,|w|)

=
θ (a|p)

∑
0≤i≤j≤|w| β(pi,j)Ia(wi..j)
α(S0,|w|)β(S0,|w|)

=
α(p→ a)β(p→ a)
α(S0,|w|)β(S0,|w|)

Figure 6: Expected values of terminal rules in a specific sentence.

β(pi,j)α(qi,k)α(rk,j) term of the summation corre-
sponds to one instance where the rule was used in
the parse. Furthermore, the β value is the outside
probability of the consequence of the deductive rule
applied, and the two α values are the inside prob-
abilities of the sibling conditions of that deductive
rule. The entire summation thus corresponds to our
definition of the reverse value of a rule, or its outside
probability.

In Figure 6, the same process is carried out for ter-
minal rules. Again, the summation is over all possi-
ble ways that we can combine the inside probability
of the sibling conditions of the rule with the outside
probability of the consequence.

Since the expected values of both terminal and
nonterminal rules have the same form, we can gen-
eralize the formula for any production ϕ:

Eθ [p→ ϕ|w] =
α(p→ ϕ)β(p→ ϕ)
α(S0,|w|)β(S0,|w|)

Finally, plugging it all into the original rule estima-
tion formula, we have:

θ̂ (ϕ|p) = Eθ [p→ ϕ]
Eθ [p]

=

∑
w∈C

α(p→ϕ)β(p→ϕ)
α(S0,|w|)β(S0,|w|)

∑
w∈C

∑
0≤i≤j≤|w|

α(pi,j)β(pi,j)

α(S0,|w|)β(S0,|w|)

= α(p→ ϕ)

∑
w∈C

β(p→ϕ)
α(S0,|w|)β(S0,|w|)

∑
w∈C

∑
0≤i≤j≤|w|

α(pi,j)β(pi,j)

α(S0,|w|)β(S0,|w|)

Rather than keeping track of the expectations of non-
terminals, they can be calculated from the rule ex-
pectations by marginalizing the productions:

Eθ [p] =
∑
ϕ

Eθ [p→ ϕ]

76



Figure 7: The same hyperedge as in Figures 3 and 4, rep-
resented as a mul/add-subgraph.

5.1 SPLITG

Since this view of EM and parsing generalizes to de-
ductive systems with multiple rules as conditions,
we can apply it to the deductive system of SPLITGs.
It is, however, also interesting to note how the hy-
pergraph view of parsing is changed by this. We
effectively removed the weights from the edges, but
kept the feature that values of nodes depend entirely
on the values connected by incoming hyperedges. If
we assume the values to be from the Boolean semir-
ing, the hypergraphs we ended up with are in fact
and/or-graphs. That is: each node in the hypergraph
corresponds to an or-node, and each hyperedge cor-
responds to an and-node. We note that this can be
generalized to any semiring, since or is equivalent to
⊕ and and is equivalent to ⊗ for the Boolean semir-
ing, we can express a hypergraph over an arbitrary
semiring as a mul/add-graph.2 Figure 7 shows how
a hyperedge looks in this new graph form. The α-
value of a node is calculated by combining the val-
ues of all incoming edges using the operator of the
node. The β-values are also calculated using the op-
erator of the node, but with the edges reversed. For
this to work properly, the mul-nodes need to behave
somewhat different from add-nodes: each incoming
edge has to be reversed one at a time, as illustrated
in Figure 8.

6 Conclusions

We have shown that the reification of rules into the
parse forest graphs allows for a unified framework
where all calculations are performed the same way,

2Because it is much easier to pronounce than ⊗/⊕-graph.

Figure 8: Reverse values (β) are calculated by track-
ing backwards through all possible paths. This produces
three different paths for the mul/add-subgraph from Fig-
ure 7. Arrows pointing downward propagate α-values
while arrows pointing upward propagate β-values.

and where the calculations for the rules encompass
all information needed to reestimate them using ex-
pectation maximization. The contextual probability
of a rule—its outside probability—holds all infor-
mation needed to calculate expectations, which can
be exploited by promoting the rules to first-class cit-
izens of the parse forest. We have also seen how this
reification of the rules helped solve a real transla-
tion problem—induction of stochastic preterminal-
ized linear inversion transduction grammars using
expectation maximization.

Acknowledgments

This work was funded by the Defense Advanced
Research Projects Agency (DARPA) under GALE
Contract Nos. HR0011-06-C-0023 and HR0011-
06-C-0023, and the Hong Kong Research Grants
Council (RGC) under research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency. We would also like to thank the
three anonymous reviewers, whose feedback made
this a better paper.

References

James K. Baker. 1979. Trainable grammars for speech
recognition. In Speech Communication Papers for the
97th Meeting of the Acoustical Society of America,
pages 547–550, Cambridge, Massachusetts.

Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings

77



of the 27th annual meeting on Association for Compu-
tational Linguistics, ACL’89, pages 143–151, Strouds-
burg, Pennsylvania, USA.

John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.

Arthur Pentland Dempster, Nan M. Laird, and Don-
ald Bruce Rubin. 1977. Maximum likelihood from
incomplete data via the em algorithm. Journal of the
Royal Statistical Society. Series B (Methodological),
39(1):1–38.

Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, Proceedings of the ESSLLI Workshop on
Finite-State Methods in Natural Language Processing
(FSMNLP). Extended abstract (5 pages).

Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 1–8, Philadelphia, July.

Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.

Liang Huang. 2008. Forest-based Algorithms in Natu-
ral Language Processing. Ph.D. thesis, University of
Pennsylvania.

Tadao Kasami and Koji Torii. 1969. A syntax-analysis
procedure for unambiguous context-free grammars.
Journal of the Association for Computing Machinery,
16(3):423–431.

Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40–51, Singapore,
August.

Christopher D. Manning and Dan Klein. 2001. Parsing
and hypergraphs. In Proceedings of the 2001 Interna-
tional Workshop on Parsing Technologies.

Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.

Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.

78


