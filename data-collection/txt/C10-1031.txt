Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 268–276,

Beijing, August 2010

268

Resolving Object and Attribute Coreference in Opinion Mining

Xiaowen Ding 

Department of Computer Science 
University of Illinois at Chicago 
dingxwsimon@gmail.com

Abstract

Coreference  resolution  is  a  classic  NLP 
problem and has been studied extensively by 
many  researchers.  Most  existing  studies, 
however,  are  generic  in  the  sense  that  they 
are not focused on any specific text. In the 
past  few  years,  opinion  mining  became  a 
popular topic of research because of a wide 
range  of  applications.  However,  limited 
work  has  been  done  on  coreference  resolu-
tion  in  opinionated  text.  In  this  paper,  we 
deal  with  object  and  attribute  coreference 
resolution. Such coreference resolutions are 
important because without solving it a great 
deal of opinion information will be lost, and 
opinions may be assigned to wrong entities. 
We  show  that  some  important  features  re-
lated to opinions can be exploited to perform 
the  task  more  accurately.  Experimental  re-
sults using blog posts demonstrate the effec-
tiveness of the technique.

Introduction 

1
Opinion mining has been actively researched in 
recent years. Researchers have studied the prob-
lem  at  the  document  level  (e.g.,  Pang  et  al., 
2002;  Tuney,  2002;  Gamon  et  al.,  2005)  sen-
tence and clause level (Wilson et al., 2004; Kim 
and  Hovy,  2004),  word  level  (e.g.,  Andreevs-
kaia  and  Bergler,  2006;  Hatzivassiloglou  and 
McKeown,  1997;  Esuli  and  Sebastiani,  2006; 
Kanayama  and  Nasukawa,  2006;  Qiu  et  al., 
2009),  and  attribute  level  (Hu  and  Liu  2004; 
Popescu and Etzioni, 2005; Ku et al., 2006; Mei 
et  al.,  2007;  Titov  and  McDonald  2008).  Here 
attributes  mean  different  aspects  of  an  object 
that has been commented on. Let us use the fol-
lowing  example  blog  to  illustrate  the  problem: 
“I  bought  a  Canon  S500  camera  yesterday.  It 
looked beautiful. I took a few photos last night. 

Bing Liu 

Department of Computer Science 
University of Illinois at Chicago 

liub@cs.uic.edu

They  were  amazing”.  “It”  in  the  second  sen-
tence refers to “Canon S500 camera”, which is 
called an object. “They” in the fourth sentence 
refers  to  “photos”,  which  is  called  an  attribute
of  the  object  “Canon  S500  camera”.  The  use-
fulness of coreference resolution in this case is 
clear. Without resolving them, we lose opinions. 
That is, although we know that the second and 
fourth  sentences  express  opinions,  we  do  not 
know  on  what.  Without  knowing  the  opinion 
target, the opinion is of limited use. In (Nicolov 
et  al.,  2008),  it  was  shown  based  on  manually 
annotated  data  that  opinion  mining  results  can 
be improved by 10% if coreference resolution is 
used (the paper did not provide an algorithm).  
In this paper, we propose the problem of ob-
ject  and  attribute  coreference  resolution  –  the 
task  of  determining  which  mentions  of  objects 
and  attributes  refer  to  the  same  entities.  Note 
that  here  entities  refer  to  both  objects  and 
attributes, not the traditional named entities. To 
our knowledge, limited work has been done on 
this problem in the opinion mining context apart 
from a prior study on resolving opinion sources 
(or  holders)  (Stoyanov  and  Cardie  2006).  Opi-
nion  sources  or  holders  are  the  persons  or  or-
ganizations that hold some opinions on objects 
and attributes. In this paper, we do not deal with 
source resolution as we are mainly interested in 
opinion texts on the web, e.g., reviews, discus-
sions  and  blogs.  In  such  environments  opinion 
sources  are  usually  the  authors  of  the  posts, 
which are displayed in Web pages.   

This  work  follows  the  attribute-based  opi-
nion mining model in (Hu and Liu 2004; Popes-
cu and Etzioni, 2005). In their work, attributes 
are called features. We do not use the term “fea-
ture”  in  this  paper  to  avoid  confusion with  the 
term “feature” used in machine learning.  

Our  primary  interests  in  this  paper  are  opi-

269

nions expressed on products and services, which 
are  called  objects.  Each  object  is  described  by 
its  parts/components  and  attributes,  which  are 
all called attributes for simplicity.  

This  paper  takes  the  supervised  learning  ap-
proach to solving the problem. The key contri-
bution of this paper is the design and testing of 
two novel opinion related features for learning. 
The first feature is based on sentiment analysis 
of  normal  sentences  (non-comparative  sen-
tences), comparative sentences, and the idea of 
sentiment  consistency.  For  example,  we  have 
the sentences, “The Sony camera is better than 
the  Canon  camera. It  is cheap  too.”  It  is  clear 
that “It” means “Sony” because in the first sen-
tence, the opinion on “Sony” is positive (com-
parative  positive),  but  negative  (comparative 
negative) on “Canon”, and the second sentence 
is positive. Thus, we can conclude that “It” re-
fers  to  “Sony”  because  people  usually  express 
sentiments  in  a  consistent  way.  It  is  unlikely 
that  “It”  refers  to  “Canon”.  This  is  the  idea  of 
sentiment consistency. As we can see, this fea-
ture  requires  the  system  to  have  the  ability  to 
determine  positive  and  negative  opinions  ex-
pressed in normal and comparative sentences.  

The  second  feature  considers  what  objects 
and  attributes  are  modified  by  what  opinion 
words. Opinion words are words that are com-
monly used to express positive or negative opi-
nions, e.g., good, best, bad, and poor. Consider 
the sentences, “The picture quality of the Canon 
camera is very good. It is not expensive either.”
The  question  is  what  “It”  refers  to,  “Canon 
camera” or “picture quality”. Clearly, we know 
that “It” refers to “Canon camera” because “pic-
ture quality” cannot be expensive. To make this 
feature work, we need to identify what opinion 
words are usually associated with what objects 
or attributes, which means that the system needs 
to discover such relationships from the corpus.  
These  two  features  give  significant  boost  to 
the coreference resolution accuracy. Experimen-
tal  results  based  on  three  corpora  demonstrate 
the effectiveness of the proposed features. 
2 Related Work 
Coreference resolution is an extensively studied 
NLP problem (e.g., Morton, 2000; Ng and Car-
die,  2002;  Gasperin  and  Briscoe,  2008).  Early 
knowledge-based  approaches  were  domain  and 

linguistic  dependent  (Carbonell  and  Brown 
1988),  where  researchers  focused  on  diverse 
lexical  and  grammatical  properties  of  referring 
expressions (Soon et al., 2001; Ng and Cardie, 
2002; Zhou et al., 2004). Recent research relied 
more  on  exploiting  semantic  information.  For 
example,  Yang  et  al.  (2005)  used  the semantic 
compatibility  information,  and  Yang  and  Su 
(2007)  used  automatically  discovered  patterns 
integrated  with  semantic  relatedness  informa-
tion, while Ng (2007) employed semantic class 
knowledge  acquired  from  the  Penn  Treebank. 
Versley  et  al.  (2008)  used  several  kernel  func-
tions in learning. 

Perhaps, the most popular approach is based 
on  supervised  learning.  In  this  approach,  the 
system  learns  a  pairwise  function  to  predict 
whether  a  pair  of  noun  phrases  is  coreferent. 
Subsequently, when making coreference resolu-
tion  decisions  on  unseen  documents,  the  learnt 
pairwise  noun  phrase  coreference  classifier  is 
run, followed by a clustering step to produce the 
final clusters (coreference chains) of coreferent 
noun phrases. For both training and testing, co-
reference  resolution  algorithms  rely  on  feature 
vectors  for  pairs  of  noun  phrases  that  encode 
lexical,  grammatical,  and  semantic  information 
about the noun phrases and their local context.  
Soon et al. (2001), for example, built a noun 
phrase  coreference  system  based  on  decision 
trees and it was tested on two standard corefe-
rence resolution data sets (MUC-6, 1995; MUC-
7, 1998), achieving performance comparable to 
the  best-performing  knowledge  based  corefe-
rence  engines  at  that  time.  The  learning  algo-
rithm  used  12  surface-level  features.  Our  pro-
posed method builds on this system with addi-
tional  sentiment  related  features.  The  features 
inherit from this paper includes: 

Distance  Feature:  Its  possible  values  are  0, 
1,  2,  3  and  so  on  which  captures  the  sentence 
distance between two entities. 

Antecedent-pronoun 

feature,  anaphor-
pronoun feature: If the candidate antecedent or 
anaphor is a pronoun, it is true; false otherwise. 
Definite  noun  phrase  feature: The value is 
true  if  the  noun  phrase  starts  with  “the”;  false 
otherwise.

Demonstrative  noun  phrase  feature:  The 
value  is  true  if  the  noun  phrase  starts  with  the 
word  “this”,  “that”,  “these”,  or  “those”;  false 
otherwise.

270

Number agreement feature: If the candidate 
antecedent  and  anaphor  are  both  singular  or 
both plural, the value is true; otherwise false. 

Both-proper-name feature: If both the can-
didates are proper nouns, which are determined 
by capitalization, return true; otherwise false. 

Alias feature: It is true if one candidate is an 
alias of the other or vice versa; false otherwise. 
Ng  and  Cardie  (2002)  expanded  the  feature 
set of Soon et al. (2001) from 12 to 53 features. 
The system was further improved by Stoyanov 
and  Cardie  (2006)  who  gave  a  partially  super-
vised clustering algorithm and tackled the prob-
lem of opinion source coreference resolution.  

Centering theory is a linguistic approach tried 
to model the variation or shift of the main sub-
ject  of  the  discourse  in  focus.  In  (Grosz  et  al., 
1995;  Tetreault,  2001),  centering  theory  was 
applied to sort the antecedent candidates based 
on  the  ranking  of  the  forward-looking  centers, 
which  consist  of  those  discourse  entities  that 
can  be  interpreted  by  linguistic  expressions  in 
the sentences. Fang et al. (2009) employed the 
centering theory to replace the grammatical role 
features  with  semantic  role  information  and 
showed superior accuracy performances. 

Ding  et  al.  (2009)  studied  the  entity  assign-
ment problem. They tried to discover the prod-
uct names discussed in forum posts and assign 
the product entities to each sentence. The work 
did not deal with product attributes.  

Unsupervised  approaches  were  also  applied 
due to the cost of annotating large corpora. Ng 
(2008) used an Expectation-Maximization (EM) 
algorithm, and Poon and Domingos (2008) ap-
plied Markov Logic Network (MLN).  

Another related work is the indirect anapho-
ra, known as bridging reference. It arises when 
an entity is part of an earlier mention. Resolving 
indirect  anaphora  requires  background  know-
ledge (e.g. Fan et al., 2005), and it is thus not in 
the scope of this paper.

Our work differs from these existing studies 
as  we  work  in  the  context  of  opinion  mining, 
which  gives  us  extra  features  to  enable  us  to 
perform the task more effectively.  
3

Problem of Object and Attribute Co-
reference Resolution 

zation,  an  event,  a  topic,  etc.  Following  (Liu, 
2006), we also use the term object to denote an 
named entity that has been commented on. The 
object  has  a  set  of  components  (or  parts)  and 
also a set of attributes. For simplicity, attribute
is used to denote both component and attribute 
in this paper. Thus, we have the two concepts, 
object and attribute.
3.1 Objective 
Task objective: To carry out coreference reso-
lution on objects and attributes in opinion text.   
As  we  discussed  in  the  introduction  section, 
coreference resolution on objects and attributes 
is  important  because  they  are  the  core  entities 
on  which  people  express  opinions.  Due  to  our 
objective, we do not evaluate other types of co-
references. We assume that objects and entities 
have  been  discovered  by  an  existing  system 
(e.g.,  Hu  and  Liu  2004,  Popescu  and  Etzioni 
2005).  Recall  that  a  coreference  relation  holds 
between  two  noun  phrases  if  they  refer  to  the 
same entity. For example, we have the follow-
ing three consecutive sentences: 
s1: I love the nokia n95 but not sure how good 

the flash would be? 

s2: and also it is quite expensive so anyone got 

any ideas? 

s3: I will be going on contract so as long as i can 

get a good deal of it.
“it” in s2 refers to the entity “the nokia n95” 
in s1.  In  this case,  we  call  “the  nokia n95”  the 
antecedent and pronoun “it” in s2 the anaphor.
The referent of “it” in s3 is also “the nokia n95”, 
so the “it” in s3 is coreferent with the “it” in s2.

Our task is thus to decide which mentions of 
objects and attributes refer to the same entities. 
3.2 Overview of Our Approach 
Like  traditional  conference  resolution,  we  em-
ploy  the  supervised  learning  approach  by  in-
cluding additional new features. The main steps 
of our approach are as follows:  

Preprocessing:  We  first  preprocess  the  cor-
pus  by  running  a  POS  tagger 1 ,  and  a  Noun 
Phrase  finder2.  We  then  produce  the  set  O-NP 
which includes both possible objects, attributes 
and  other  noun  phrases.  The  noun  phrases  are 

In  general,  opinions  can  be  expressed  on  any-
thing, e.g., a product, an individual, an organi-

1 http://nlp.stanford.edu/software/tagger.shtml
2 http://crfchunker.sourceforge.net/ 

271

found using the Noun Phrase finder and the ob-
ject  names  are  consecutive  NNPs.  O-NP  thus 
contains everything that needs to be resolved.  

Feature  vector  construction:  To  perform 
machine  learning,  we  need  a  set  of  features. 
Similar  to  previous  supervised  learning  ap-
proaches (Soon et al., 2001), a feature vector is 
formed  for  every  pair  of  phrases  in  O-NP  ex-
tracted in the preprocessing step. We use some 
of the features introduced by Soon et al. (2001) 
together with some novel new features that we 
propose  in  this  work.  Since  our  focus  is  on 
products  and  attributes  in  opinionated  docu-
ments,  we  do  not  use  personal  pronouns,  the 
gender  agreement  feature,  and  the  appositive 
feature,  as  they  are  not  essential  in  blogs  and 
forum posts discussing products.  

Classifier  construction:  Using  the  feature 
vectors  obtained  from  the  previous  step,  we 
construct  the  training  data,  which  includes  all 
pairs of manually tagged phrases that are either 
object names or attributes. More precisely, each 
pair contains at least one object or one attribute. 
Using  the  training  data,  a  decision  tree  is  con-
structed using WEKA3.

Testing: The testing phase employs the same 
preprocessing  and  feature  vector  construction 
steps as described above, followed by the appli-
cation  of  the  learnt  classifier  on  all  candidate 
coreference pairs (which are represented as fea-
ture  vectors).  Since  we  are  only  interested  in 
coreference information for objects and attribute 
noun  phrases,  we  discard  non-object  and  non-
attribute noun phrases. 
4 The Proposed New Features  
On surface, object and attribute coreference res-
olution seems to be the same  as the traditional 
noun phrase coreference resolution. We can ap-
ply an existing coreference resolution technique. 
However,  as  we  mentioned  earlier,  in  the  opi-
nion mining context, we can have a better solu-
tion by integrating opinion information into the 
traditional  lexical  and  grammatical  features. 
Below  are  several  novel  features  that  we  have 
proposed.  We  use  (cid:302)i  to  denote  an  antecedent 
candidate and (cid:302)j an anaphor candidate. Note that 
we will not repeat the features used in previous 
systems, but only focus on the new features.  

3 http://www.cs.waikato.ac.nz/ml/weka/

Sentiment Consistency 

4.1
Intuitively, in a post, if the author starts express-
ing opinions on an object, he/she will continue 
to  have  the  same  opinion  on  that  object  or  its 
attributes  unless  there  are  contrary  words  such 
as “but” and “however”. For example, we have 
the  following  blog  (an  id is  added  before  each 
sentence to facilitate later discussion):  

“(1)  I  bought  Camera-A  yesterday.  (2)  I 
took a few pictures in the evening in my living 
room.  (3)  The  images  were  very  clear.  (4) 
They  were  definitely  better  than  those  from 
my  old  Camera-B.  (5a)  It  is  cheap  too.  (5b) 
The pictures of that camera were blurring for 
night shots, but for day shots it was ok”  
The comparative sentence (4) says that Cam-
era-A is superior to Camera-B. If the next sen-
tence is (5a) ((5a) and (5b) are alternative sen-
tences),  “it”  should  refer  to  the  superior  prod-
uct/object  (Camera-A)  because  sentence  (5a) 
expresses  a  positive  opinion.  Similarly,  if  the 
next sentence is sentence (5b) which expresses a 
negative  opinion  in  its  first  clause,  “that  cam-
era” should refer to the inferior product (Cam-
era-B). We call this phenomenon sentiment con-
sistency (SC), which says that consecutive sen-
timent  expressions  should  be  consistent  with 
each other unless there are contrary words such 
as “but” and “however”. It would be ambiguous 
if such consistency is not observed. 

Following the above observation, we further 
observe  that  if  the  author  wants  to  introduce  a 
new object o, he/she has to state the name of the 
object explicitly in a sentence si-1. The question 
is  what  happens  to  the  next  sentence  si  if  we 
need to resolve the pronouns in si.   

We consider several cases: 

1.  si-1 is  a  normal  sentence  (not  a  comparative 
sentence).  If  si  expresses  a  consistent  senti-
ment with si-1, it should refer to the same ob-
ject as si-1.  For example, we have  

si-1: The N73 is my favorite.
si:

It can produce great pictures. 

Here “It” in si clearly refers to “The N73” in 
the first sentence si-1.

2. si-1 is  a  normal  sentence  and  si  does  not  ex-
press  a  consistent  sentiment,  then  (cid:302)i  and  (cid:302)j
introduced in these two sentences may not be 
coreferenced. For example, we have

si-1:  The K800 is awesome.
si: That phone has short battery life. 

 
 

 
 

272

 
 

Here “The K800” and “That phone” may not 
be a coreference pair according to sentiment 
consistency. “That phone” should refer to an 
object appeared in an earlier sentence.  

3.  si-1 is a comparative sentence. If si expresses 
a  positive  (respectively  negative)  sentiment, 
the pronoun in si should refer to the superior 
(or inferior) entity in si-1 to satisfy sentiment 
consistency. This situation is depicted in the 
earlier  example  blog.  For  completeness,  we 
give another example.   

si-1: The XBR4 is brighter than the 5080.
si:  Overall, it is a great choice.  

Here “it” in si should refer to “The XBR4” in 
si-1 since they both have positive sentiments 
expressed on them. 
Opinion Mining of Comparative Sentences:
To deal with case (3), we need to identify supe-
rior entities from comparative sentences. In fact, 
we  first  need  to  find  such  comparative  sen-
tences.  There  is  a  prior  work  on  identifying 
comparative  sentences  (Jindal  and  Liu.  2006). 
Since  our  focus  is  not  to  identify  such  sen-
tences, we used several heuristic rules based on 
some  comparative  keywords,  e.g.  than, win,
superior,  etc.  They  achieve  the  F-score  of  0.9. 
We  then  followed  the  opinion  mining  method 
introduced in (Ding et al. 2009) to find superior 
entities. Since a comparative sentence typically 
has  entities  on  the  two  sides  of  a  comparative 
keyword,  i.e.,  “Camera-X  is  better  than  Cam-
era-Y”, based on opinion mining, if the sentence 
is positive, then the entities before the compara-
tive keyword is superior and otherwise they are 
inferior (with the negation considered).  

SC Feature: The possible value for this fea-
ture is 0, 1, or 2. If (cid:302)i and (cid:302)j have the same opi-
nion, return 1; different opinions, return 0; and 
if  the  opinions  cannot  be  identified  for  one  or 
both of them, return 2. Here is an example ex-
plaining how the feature is used in our system:

“My  wife  has  currently  got  a  Nokia  7390, 
which is terrible. My 6233 would always get 
great reception, hers would get no signal.”

Using our algorithm for opinion mining, “hers” 
gets a negative opinion in the second sentence. 
So the value for this feature for the pair, “hers” 
and “a Nokia 7390”, is 1. The feature value for 
the pair “hers” and “My 6233” is 0. The idea is 
that because the first sentence expresses a nega-
tive sentiment on “a Nokia 7390”, and there is 

these 

Entity and Opinion Word Association

no  discourse  connective  (such  as  “but”  and 
“however”)  between 
two  sentences. 
“Hers” should be talking about “a Nokia 7390” 
so as to satisfy sentiment consistency. 
4.2
One  of  the  most  important  factors  determining 
the orientation of opinions is the opinion words 
that  opinion  holders  use  to  express  their  opi-
nions.  Different  entities  may  be  modified  by 
different opinion words. We can use their asso-
ciation  information  with  entities  (both  objects 
and attributes) to identify their coreferences. 

Opinion  Words:  In  most  cases,  opinions  in 
sentences  are  expressed  using  opinion  words.
For example, the sentence, “The picture quality 
is amazing”, expresses a positive opinion on the 
“picture  quality”  attribute  because  of  the  posi-
tive opinion word “amazing”.  

Researchers  have  compiled  sets  of  such 
words for adjectives, adverbs, verbs, and nouns 
respectively.  Such  lists  are  collectively  called 
the opinion  lexicon.  We  obtained  an  opinion 
lexicon from the authors of (Ding et al. 2009).  

It is useful to note that opinion words used to 
express opinions on different entities are usually 
different apart from some general opinion words 
such as good, great, bad, etc, which can express 
opinions  on  almost  anything.  For  example,  we 
have the following passage:  

“i  love  the  nokia  n95  but  not  sure  how 
strong the flash would be? And also it is quite 
expensive, so anyone got any ideas?”

Here “strong” is an opinion word that expresses 
a positive opinion on “the flash”, but is seldom 
used  to  describe  “the  nokia  n95”.  “expensive”, 
on the other hand, should not be associated with 
“the flash”, but is an opinion word that indicates 
a negative opinion on “the nokia n95”. So “the 
nokia  n95”  is  more  likely  to  be  the  antecedent 
of “it” in the second sentence.  

The question is how to find such associations 
of entities and opinion words. We use their co-
occurrence  information  to  measure,  i.e.,  the 
pointwise mutual information of the two terms. 
First,  we  estimate  the  probability  of  P(NP),
P(OW) and P(NP&OW). Here NP means a noun 
phrase, e.g., an object (attribute) after removing 
determiners,  and  OW  means  an  opinion  word. 
To  compute  the  probability,  we  first  count  the 
occurrences  of the words. Then the probability 
is computed as follow: 

273

(cid:1842)(cid:4666)(cid:1840)(cid:1842)(cid:481) (cid:1841)(cid:1849)(cid:4667) (cid:3404) (cid:3)

(cid:1840)(cid:1873)(cid:1865)(cid:1867)(cid:1858)(cid:1845)(cid:4666)(cid:1840)(cid:1842)(cid:3)(cid:428)(cid:3)(cid:1841)(cid:1849)(cid:4667)
(cid:1846)(cid:1867)(cid:1872)(cid:1853)(cid:1864)(cid:1840)(cid:1873)(cid:1865)(cid:1867)(cid:1858)(cid:1845)(cid:1857)(cid:1866)(cid:1872)(cid:1857)(cid:1866)(cid:1855)(cid:1857)

where NumofS is a function that gives the num-
ber of sentences that contain the particular word 
string. P(NP,  OW)  is  computed  in  the  same 
way. Let us use the previous example again. We 
compute  P(“nokia  n95”,”expensive”)  as  the 
number  of  sentences  containing  both  “nokia 
n95” and “expensive” divided by the total num-
ber of sentences in the whole corpus. 

Then  we  use  the  pointwise  mutual  informa-
tion between a noun phrase and an opinion word 
to measure the association. 

(cid:1842)(cid:1839)(cid:1835)(cid:4666)(cid:1840)(cid:1842)(cid:481) (cid:1841)(cid:1849)(cid:4667) (cid:3404) (cid:142)(cid:145)(cid:137)

(cid:1842)(cid:4666)(cid:1840)(cid:1842)(cid:481) (cid:1841)(cid:1849)(cid:4667)

(cid:1842)(cid:4666)(cid:1840)(cid:1842)(cid:4667)(cid:1842)(cid:4666)(cid:1841)(cid:1849)(cid:4667)

(cid:3)

However, this PMI value cannot be encoded 
directly as a feature as it only captures the local 
information between antecedent candidates and 
opinion  words.  That  is,  it  cannot  be  used  as  a 
global feature in the classifier. We thus rank all 
possible  antecedents  of  anaphor  (cid:302)j  based  on 
their PMI values and use the ranking as the fea-
ture value. The highest ranked antecedent (cid:302)i has 
value 1; the second one has value 2 and so on. 
The  candidates  ranked  below  the  fourth  place 
all  have  the  value  5.  In  the  example  above,  if 
PMI(“nokia  n95”,  “expensive”)  is  greater  than 
PMI(“flash”, “expensive”), the feature for “no-
kia n95” and “it” pair will have a smaller value 
than the feature for the “flash” and “it” pair.

String Similarity Feature

One may ask if we can use all adjectives and 
adverbs to associate with objects and attributes 
rather  than  just  opinion  words  since  most  opi-
nion  words  are  adjectives  and  adverbs.  We 
tested  that,  but  the  results  were  poor.  We  be-
lieve the reason is that there are many adjectives 
and adverbs which are used for all kinds of pur-
poses and may not be meaningful for our task.  
4.3
Soon  et  al.  (2001)  has  a  string  match  feature 
(SOON STR), which tests whether the two noun 
phrases  are  the  same  string  after  removing  de-
terminers from each. Ng and Cardie (2002) split 
this  feature  into  several  primitive  features,  de-
pending  on  the  type of  noun phrases.  They  re-
place the SOON STR feature with three features 
—  PRO  STR,  PN  STR,  and  WORDS  STR  — 
which restrict the application of string matching 
to pronouns, proper names, and non-pronominal 

noun phrases, respectively.  

In the user generated opinion data, these may 
not  be  sufficient.  For  a  certain  product,  people 
can have a large number of ways to express it. 
For example, we have 

“Panasonic TH50PZ700U VS TH50PZ77U, 
Which Plasma tv should I go for. The TH77U 
is about $500.00 more than the 700U.”

Here “TH77U” is the same entity as “Panasonic 
TH50PZ77U”, and “TH50PZ700U” is the same 
as “700U”. But they cannot be easily identified 
by “same string” features mentioned above. Al-
though  “700U”  can  be  solved  using  substring 
features, “TH77U” is difficult to deal with. 

We employ a modified edit distance to com-
puting a similarity score between different men-
tions  and  use  that  as  a  feature  in  our  system. 
When  one  candidate  is  a  substring  of  another, 
return 1; otherwise, 1 plus the edit distance. 
4.4 Other Useful Features 
In the machine learning approach introduced by 
Soon et al. (2001), they had several general fea-
tures that can deal with various kinds of entities, 
e.g.,  semantic  class  agreement  features  dealing 
with  different  semantic  classes  like  date,  loca-
tion, etc., and the gender agreement feature re-
lated  to  personal  entities.  However,  these  fea-
tures are not so useful for our task because the 
semantic  class  of  a  product  in  one  domain  is 
usually  consistent,  and  dates  and  locations  are 
unlikely to be of any products that people will 
express  their  opinions.  Moreover,  we  do  not 
study opinion holders (as they are known in the 
Web environment), so personal entities are not 
the aspect that we concentrate on. Thus we did 
not  use  the  following  features:  semantic  class 
agreement  features,  the  gender  agreement  fea-
ture, and appositive feature.

However,  we  added  some  specific  features, 
which are based on two extracted entities, (cid:302)i and 
(cid:302)j, where (cid:302)i is the potential antecedent and (cid:302)j is 
the potential anaphor:  

Is-between  feature:  Its  possible  values  are 
true  and  false.  If  the  words  between  (cid:302)i  and  (cid:302)j
have an is-like verb (i.e., is, are, was, were, and 
be)  between  them  and  there  is  no  comparative 
indicators,  this  feature  has  the  value  of  true, 
e.g., “The nokia e65 is a good handset.”

In sentences similar to this example, the enti-
ties  before  and  after  “is”  usually  refer  to  the 
same object or attribute by a definition relation. 

274

And the value of this feature will be true. 

If  “is”  appears  together  with  a  comparative 
word,  it  is  probably  an  indication  that  the  two 
entities are different, and the value for this fea-
ture will be false, e.g., “Overall the K800 is far 
superior to the W810.”

Has-between  feature:  Its  possible  values  are 
also true and false. If the words between (cid:302)i and 
(cid:302)j have a has-like verb (i.e., has, have, and had), 
the value is true, and otherwise false, e.g., “The
k800 has a 3.2 megapixel camera.”

This feature usually indicates a “part-of” rela-
tion if “has” appears between two entities. They 
do  not  refer  to  the  same  entity.  Table  1  gives  a 
summary of all the features used in our system. 
5 Experiments and Discussions 
5.1 Datasets 
For evaluation, we used forum discussions from 
three domains, mobile phones, plasma and LCD 
TVs, and cars. Table 2 shows the characteristics 
of  the  three  data  sets.  Altogether,  we  down-
loaded 64 discussion threads, which contain 453 
individual posts with a total of 3939 sentences. 
All the sentences and product names were anno-
tated strictly following the MUC-7 coreference 
task annotation standard4. Here is an example: 

“Phil had <COREF ID = "6" TYPE = 

"OBJ">a z610</COREF> which has <COREF 
ID = "7" TYPE = "ATTR">a 2MP cema-
ra</COREF>, and he never had a problem 
with <COREF ID = "8" TYPE = "OBJ" REF = 
"6">it</COREF>.”

ID and REF features are used to indicate that 
there is a coreference link between two strings. 
ID  is  arbitrary  but  uniquely  assigned  to  each 
noun phrase. REF uses the ID to indicate a core-
ference link. “TYPE” can be “OBJ” (an object 
or  a  product),  or  “ATTR”  (an  attribute  of  an 
object).  The  annotation  was  done  by  the  first 
author and another student before the algorithm 
construction, and the annotated data sets will be 
made public for other researchers to use. 

For  our  experiments,  we  used  the  J48-
decision  tree  builder  in  WEKA,  a  popular 
of machine learning suite developed at the  Uni-
versity of Waikato. We conducted 10-fold cross 
validation on each dataset.  

4 http://www-nlpir.nist.gov/related_projects/muc/procee- 
dings/co_task.html

Baseline

The  performances  are  measured  using  the 
standard  evaluation  measures  of  precision  (p),
recall (r) and F-score (F), F = 2pr/(p+r). As we 
stated  in  Section  3,  we  are  only  interested  in 
object  and  attributes  noun  phrases.  So  in  the 
testing phrases, we only compute the precision 
and  recall  based  on  those  pairs  of  candidates 
that contain at least one object or attribute noun 
phrase in each pair. If both of the candidates are 
not an object or an attribute, we ignore them. 
5.2
As the baseline systems, we duplicated two rep-
resentative  systems.  Baseline1  is  the  decision 
tree system in Soon et al. (2001). We do not use 
the  semantic  class  agreement  feature,  gender 
agreement feature and appositive feature in the 
original 12 features for the reason discussed in 
Section 4.4. Thus, the total number of features 
in  Baseline1  is  9.  The  second  baseline  (base-
line2) is based on the centering theory from the 
semantic  perspective  introduced  by  Fang  et  al. 
(2009).  Centering  theory  is  a  theory  about  the 
local discourse structure that models the interac-
tion of referential continuity and the salience of 
discourse entities in the internal organization of 
a text. Fang et al. (2009) extended the centering 
theory from the grammar level to the semantic 
level in tracking the local discourse focus. 
5.3 Results Analysis 
Table  3  gives  the  experimental  results  of  the 
two  baseline  systems  and  our  system  with  dif-
ferent features included. From Table 3, we can 
make several observations.  
(1) Comparing the results of Baseline1 and our 
system with all features (Our System (All)), 
the  new  features  introduced  in  this  paper 
improves  Baseline1  on  average  by  more 
than 9% in F-score.

(2) Comparing the results of Baseline2 and our 
system with all features (Our System (All)), 
our  system  performs  better  than  Baseline2 
by about 3 - 5%. We also observe that cen-
tering  theory  (Baseline2)  is  indeed  better 
than the traditional decision tree. 

(3) Our system with sentiment consistency (SC) 
makes a major difference. It improves Base-
line1 (our method is based on Baseline1) by 
5-6% in F-score.  

(4) With  the  additional  feature  of  entity  and 
opinion  association  (EOA),  the  results  are 

275

Feature category Feature 
Opinion mining 
based features 

Opinion consistency 

 grammatical 

Entity and opinion words 
association
i-Pronoun feature 
j-Pronoun feature 
Number agreement feature 
Definite feature 
Demonstrative feature 

lexical 

Others 

Both proper-name feature 
String similarity 
Alias feature 
Distance feature 

Remark 
1, if the opinion orientation of (cid:302)i is the same as (cid:302)j, 0 if 
the opinions are different, else 2  
1, 2, 3, 4, 5 which indicate the rank positive based on the 
PMI value introduced in Section 4.2 
1, if (cid:302)i is a pronoun, else 0 
1, if (cid:302)j is a pronoun, else 0 
1, if both of the noun phrases agree in numbers, else 0 
1, if (cid:302)j starts with the word “the”, else 0 
1, if (cid:302)j starts with the word “this”, “that”, “those”, or 
“these”, else 0 
1, if (cid:302)i and (cid:302)j are both proper names, else 0 
The string similarity score between (cid:302)i and (cid:302)j
 1, If (cid:302)i is an alias of (cid:302)j or vice versa, else 0 
The sentence distance between the pair of noun phrases, 
0 if they are in the same sentence 

Keywords between features  1, if some keywords exist between (cid:302)i and (cid:302)j, else 0. De-

tails are discussed in Section 4.5 

Table 1: Feature list: (cid:302)i denotes the antecedent candidate and (cid:302)j the anaphor candidate 

 
Phone 
TVs 
Cars 
Total 

Posts 
168 
173 
112 
453 

Sentences 
1498 
1376 
1065 
3939 

Table 2: Characteristics of the datasets
TVs 

Cellphone 

 
 

 
 
1  Baseline1 
2  Baseline2 
3  Our System (SC) 
4  Our System (SC+EOA) 
5  Our System (All) 

Cars 

r 

F

p 
0.66
0.70
0.71
0.74
0.75

r 

F

p 

r 

F

p 

0.57  0.61 0.67 0.61 0.64 0.70  0.63  0.66 
0.64  0.67 0.72 0.65 0.68 0.76  0.70  0.73 
0.64  0.67 0.73 0.66 0.69 0.74  0.69  0.72 
0.68  0.71 0.74 0.68 0.71 0.77  0.71  0.74 
0.70  0.72 0.76 0.70 0.73 0.78  0.73  0.75 

Table 3: Results of object and attribute coreference resolution 

improved further by another 2-4%. 

(5)  Our  system  with  all  features  (row  5)  per-

forms the best. 
Paired t-tests  were  performed  on  the  three 
systems, i.e., baseline1, baseline2, and our sys-
tem  (row  5).  The  tests  show  that  the  improve-
ments  of  our  method  over  both  Baseline1  and 
Baseline2 are significant at the confidence level 
of 95% for the first two datasets. For the third 
dataset, the improvement over Baseline1 is also 
significant at the confidence level of 95%, while 
the improvement over Baseline2 is significant at 
the confidence level of 90%.  

In  summary,  we  can  conclude  that  the  new 
technique  is  effective  and  is  markedly  better 
than  the  existing  methods.  It  is  clear  that  the 
new features made a major difference.  

6 Conclusion
This  paper  investigated  the  coreference  resolu-
tion problem in the opinion mining context. In 
particular, it studied object and attribute resolu-
tions  which  are  crucial  for  improving  opinion 
mining  results.  Although  we  still  took  the  su-
pervised  learning  approach,  we  proposed  sev-
eral  novel  features  in  the  opinion  mining  con-
text,  e.g.,  sentiment  consistency,  and  ob-
ject/attribute  and  opinion  word  associations. 
Experimental results using forum posts demon-
strated  the  effectiveness  of  the  proposed  tech-
nique.  In  our  future  work,  we  plan  to  further 
improve  the  method  and  discover  some  other 
opinion related features that can be exploited to 
produce more accurate results. 

276

References  
A.  Andreevskaia  and  S.  Bergler.  2006.  Mining 
WordNet for Fuzzy Sentiment: Sentiment Tag Ex-
traction from WordNet Glosses. EACL’06. 

J. Carbonell and R. Brown. 1988. Anaphora resolu-

tion: a multi-strategy approach. COLING’1988.

G. Carenini, R. Ng, and A. Pauls. 2006. Interactive 

multimedia summaries of evaluative text. IUI’06.
X. Ding, B. Liu and L. Zhang. 2009. Entity Discov-
ery and Assignment for Opinion Mining Applica-
tion. KDD’09.  

A. Esuli and F. Sebastiani. 2006. Determining Term 
Subjectivity  and  Term  Orientation  for  Opinion 
Mining, EACL’06. 

J. Fan, K. Barker and B. Porter. 2005. Indirect ana-
phora  resolution  as  semantic  path  search.  K-
CAP’05. 

C.  Gasperin  and  T.  Briscoe.  2008.  Statistical  ana-
phora resolution in biomedical texts. COLING'08 
B.  J.  Grosz,  A.  K.  Joshi  and  S.  Weinstein.  1995. 
Centering:  a  framework  for  modeling  the  local 
coherence  of  discourse.  Computational  Linguis-
tics, 21(2). 

V.  Hatzivassiloglou  and  K.  McKeown.  1997.  Pre-
dicting  the  Semantic  Orientation  of  Adjectives.
ACL-EACL’97.  

M.  Hu  and  B.  Liu.  2004.  Mining  and  summarizing 

customer reviews. KDD’04. 

N.  Jindal,  and  B.  Liu.  2006.  Mining  Comparative 

Sentences and Relations. AAAI’06. 

H.  Kanayama  and  T.  Nasukawa.  2006.  Fully  Auto-
matic  Lexicon  Expansion  for  Domain-Oriented 
Sentiment Analysis. EMNLP’06.  

S.  Kim  and  E.  Hovy.  2004.  Determining  the  Senti-

ment of Opinions. COLING’04. 

N. Kobayashi, K. Inui and Y. Matsumoto. 2007. Ex-
tracting  Aspect-Evaluation  and  Aspect-of  Rela-
tions in Opinion Mining. EMNLP-CoNLL’07. 

F.  Kong,  G.  Zhou, Q. Zhu  and P. Qian.  2009. Em-
ploying the Centering Theory in Pronoun Resolu-
tion from the Semantic Perspective. EMNLP’09. 
L.-W. Ku, Y.-T. Liang and H.-H. Chen. 2006. Opi-
nion  Extraction,  Summarization  and  Tracking  in 
News and Blog Corpora. CAAW'06. 

B. Liu. 2006. Web Data Mining, Springer.  
R. McDonald, K. Hannan, T Neylon, M. Wells, and 
J.Reynar.  2007.  Structured  Models  for  Fine-to-
Coarse Sentiment Analysis. ACL-07. 

Q.  Mei,  X.  Ling,  M.  Wondra,  H.  Su,  and  C.  Zhai. 
2007. Topic Sentiment Mixture: Modeling Facets 
and Opinions in Weblogs. WWW’07. 

T.  S.  Morton.  2000.  Coreference  for  NLP  applica-

tions. ACL’00. 

V.  Ng  and  C.  Cardie.  2002.  Improving  machine 
learning  approaches  to  coreference  resolution.
ACL’02. 

V. Ng. 2007. Semantic Class Induction and Corefe-

rence Resolution. ACL’07. 

V. Ng. 2008. Unsupervised Models for Coreference 

Resolution. EMNLP’08. 

N.  Nicolov,  F.  Salvetti  and  S.  Ivanova,  Sentiment 

analysis: Does coreference matter? AISB'2008. 

B.  Pang,  L.  Lee,  and  S.  Vaithyanathan.  2002. 
Thumbs  up?  Sentiment  Classification  Using  Ma-
chine Learning Techniques. EMNLP’02.  

H. Poon and P. Domingos. 2008. Joint Unsupervised 
Coreference  Resolution  with  Markov  Logic.
EMNLP’08, 650–659. 

A-M.  Popescu  and  O.  Etzioni.  2005.  Extracting 
product  features  and  opinions  from  reviews.
EMNLP’05. 

Qiu,  Guang,  B.  Liu,  J.  Bu  and  C.  Chen.  2009  Ex-
through 

panding  Domain  Sentiment  Lexicon 
Double Propagation. IJCAI 2009. 

W. M. Soon, H. T. Ng and D. Lim. 2001. A machine 
learning  approach  to  coreference  resolution  of 
noun phrase. Computational Linguistics, 27(4). 

V.  Stoyanov,  C.  Cardie.  2006.  Partially  supervised 
coreference resolution for opinion summarization 
through structured rule learning. EMNLP’06.

I.  Titov  and  R.  McDonald.  2008,  A  joint  model  of 
text and aspect ratings for sentiment summariza-
tion, ACL’08. 

J. Tetreault. 2001. A corpus-based evaluation of cen-
tering  and  pronoun  resolution.  Computational 
Linguistics. 27(4):507-520. 

P. Turney. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL’02.  

Y.  Versley,  A.  Moschitti,  M.  Poesio  and  X.  Yang. 
2008.  Coreference  systems based  on  kernels  me-
thods. COLING’08. 

T.  Wilson,  J.  Wiebe,  and  R.  Hwa.  2004.  Just  how 
mad  are  you?  Finding  strong  and  weak  opinion 
clauses. AAAI’04. 

X. F. Yang(cid:15914) J. Su and C. L. Tan. 2005. Improving 
Pronoun  Resolution  Using  Statistics  -  Based  Se-
mantic Compatibility Information. ACL’05. 

X. F. Yang and J. Su. 2007. Coreference Resolution 
Using Semantic Relatedness Information from Au-
tomatically Discovered Patterns. ACL’07. 

G.  D.  Zhou  and  J.  Su.  2004.  A  high-performance 
coreference resolution system using a multi-agent 
strategy. COLING’04. 

