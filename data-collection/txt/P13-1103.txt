



















































Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042–1051,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources

Sumire Uematsu†
uematsu@cks.u-tokyo.ac.jp

Takuya Matsuzaki‡
takuya-matsuzaki@nii.ac.jp

Hiroki Hanaoka†
hanaoka@nii.ac.jp

Yusuke Miyao‡
yusuke@nii.ac.jp

Hideki Mima†
mima@t-adm.t.u-tokyo.ac.jp

†The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan

‡National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan

Abstract

This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.

1 Introduction

Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al.,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al.,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.

In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on

combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al.,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.

The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al., 2002) and the NAIST text corpus
(Iida et al., 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.

In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.

There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-

1042



I

NP: I′

NPy: I′

NP: I′

NP: I′

give

S\NP/NP/NP :
λxλyλz.give′yxz

them
NP :them′

NP :ythem′
>

S\NP/NP :λyλz.give′y them′z

money

NP :money′

NP :money′

NP :money′
>

S\NP :λz.give′money′them′z
<

S :give′money′them′I ′

大使
ambassador

NPnc

が
NOM

NPga\NPnc
<

NPga

NPga

NPga

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni
NPni

参加
participation

Sstem\NPga\NPni

し
do-CONT
Scont\Sstem

<B
Scont\NPga\NPni

た
PAST-BASE
Sbase\Scont
Sbase\Scont

<B
Sbase\NPga\NPni

<
Sbase\NPga

<
Sbase

政府
government

NPnc

が
NOM

NPga\NPnc
<

NPga

NPga

NPga

大使
ambassador

NPnc

を
ACC

NPwo\NPnc
<

NPwo
NPwo

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni

参加さ
participation

Svo s\NPga\NPni

せ
CAUSE

Scont\NPga\NPwo\(Svo s\NPga)
<

Scont\NPga\NPwo\NPni
<

Scont\NPga\NPwo
<

Scont\NPga
<

Scont

政府
government

NP

は
NOM

NPga\NP
<

NPga

NPga

NPga

大使を
ambassador-ACC

NPwo
NPga

NPga

交渉に
negotiation-DAT

NPni
NPga

参加さ
join

Svo s\NPga\NPni

せ
cause

Scont\NPga\NPwo\(Svo s\NPga)
<

Scont\NPga\NPwo\NPni
<

Scont\NPga\NPwo
<

Scont\NPga
<

Scont

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni
NPga

NPga

参加
participation

Sstem\NPni

さ
do

Svo s\Sstem
<B

Svo s\NPni

せ
CAUSE

Scont\Svo s
Scont\Svo s

<B
Scont\NPni

た
PAST

Sbase\Scont
Scont\Svo s

Scont
<B

Sbase\NPni
<

Sbase

1

Figure 1: A CCG derivation.

X/Y : f Y : a → X : fa (>)
Y : a X\Y : a → X : fa (<)
X/Y : f Y/Z : g → X/Z : λx.f(gx) (> B)
Y\Z : g X\Y : f → X\Z : λx.f(gx) (< B)

Figure 2: Combinatory rules (used in the current
implementation).

sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.

2 Background

2.1 Combinatory Categorial Grammar

CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.

An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-

Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S

Table 1: Typical categories for Japanese syntax.

Cat. Feature Value Interpretation
NP case ga nominal

o accusative
ni dative
to comitative, complementizer, etc.
nc none

S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative

Table 2: Features for Japanese syntax (those used
in the examples in this paper).

ety of semantic theories with CCG-based syntactic
parsing (Bos et al., 2004).

2.2 CCG-based syntactic theory for Japanese

Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).

The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki’s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.

Our implementation of the grammar basically
follows Bekki (2010)’s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),

1043



I

NP: I′

NPy: I′

NP: I′

NP: I′

give

S\NP/NP/NP :
λxλyλz.give′yxz

them
NP :them′

NP :ythem′
>

S\NP/NP :λyλz.give′y them′z

money

NP :money′

NP :money′

NP :money′
>

S\NP :λz.give′money′them′z
<

S :give′money′them′I ′

大使
ambassador

NPnc

が
NOM

NPga\NPnc
<

NPga

NPga

NPga

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni
NPni

参加
participation

Sstem\NPga\NPni

し
do-CONT
Scont\Sstem

<B
Scont\NPga\NPni

た
PAST-BASE
Sbase\Scont
Sbase\Scont

<B
Sbase\NPga\NPni

<
Sbase\NPga

<
Sbase

政府
government

NPnc

が
NOM

NPga\NPnc
<

NPga

NPga

NPga

大使
ambassador

NPnc

を
ACC

NPwo\NPnc
<

NPwo
NPwo

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni

参加さ
participation

Svo s\NPga\NPni

せ
CAUSE

Scont\NPga\NPwo\(Svo s\NPga)
<

Scont\NPga\NPwo\NPni
<

Scont\NPga\NPwo
<

Scont\NPga
<

Scont

政府
government

NP

は
NOM

NPga\NP
<

NPga

NPga

NPga

大使を
ambassador-ACC

NPwo
NPga

NPga

交渉に
negotiation-DAT

NPni
NPga

参加さ
join

Svo s\NPga\NPni

せ
cause

Scont\NPga\NPwo\(Svo s\NPga)
<

Scont\NPga\NPwo\NPni
<

Scont\NPga\NPwo
<

Scont\NPga
<

Scont

交渉
negotiation

NPnc

に
DAT

NPni\NPnc
<

NPni
NPga

NPga

参加
participation

Sstem\NPni

さ
do

Svo s\Sstem
<B

Svo s\NPni

せ
CAUSE

Scont\Svo s
Scont\Svo s

<B
Scont\NPni

た
PAST

Sbase\Scont
Scont\Svo s

Scont
<B

Sbase\NPni
<

Sbase

1

Figure 3: A simplified CCG analysis of the sentence “The ambassador participated in the negotiation.”.

S → NP/NP (RelExt)
S\NP1 → NP1/NP1 (RelIn)
S → S1/S1 (Con)
S\$1\NP1 → (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)

Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.

coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., “彼
はピアノを弾いて歌った/he played the piano and
sang”), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory’s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.

For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory’s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (大使
が), the dative phrase (交渉に), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.

2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.

Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-

Kyoto Corpus 

Chunk 

政府 が 
government NOM 

大使 を 
ambassador ACC 

交渉 に 
negotiation DAT 

参加 さ せ た 
participation do cause PAST 

NAIST Corpus 

Dep. 

Causer 
ARG-ga 

ARG-ni 

Figure 5: The Kyoto and NAIST annotations for
“The government had the ambassador participate
in the negotiation.”. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).

aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.

NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: “ga” (subject), “o”
(direct object), and “ni” (indirect object) (Fig. 5).

Japanese particle corpus (JP) (Hanaoka et al.,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) “to”. In Japanese, “to” has many functions,
including a complementizer (similar to “that”), a
subordinate conjunction (similar to “then”), a co-
ordination conjunction (similar to “and”), and a
case marker (similar to “with”).

2.4 Related work

Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al.,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel

1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.

1044



and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.

Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al., 2009), and Turkish (Çakıcı,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.

CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al., 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.

3 Corpus integration and conversion

For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)–c).

As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.

Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent

ProperNoun 

エリツィン 

Yeltsin 

NP 

ProperNoun 

ロシア 

Russia 

Noun 

大統領

president 

PostP 

に 
DAT 

PP 

NP 

Aux 

なかっ 
not 

VP 

Verb 

許さ 

forgive 

VerbSuffix 

れ 
PASSIVE 

VP Aux 

た 
PAST 

VP 

“to Russian president Yeltsin” “(one) was not forgiven” 

Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).

of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.

3.1 Dependencies to phrase structure trees

We first integrate and convert available Japanese
corpora―namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ―into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.

The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don’t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al., 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al. (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):

Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.

Verbal chunks Verbal chunks are analyzed as
left-branching structures.

The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb

1045



PP 

Noun 

誕生 
birth 

PostPcm 

から
from 

PP 

Noun 

死 
death 

PostPcm 

まで 
to 

PostPadnom 

の 
adnominal 

PP 

PP 

Noun 

過程 
process 

PostPcm 

を 
ACC 

PP 

NP 

PP 

Noun 

誕生 
birth 

PostPcm 

から
from 

PP 

Noun 

死 
death 

PostPcm 

まで 
to 

PostPadnom 

の 
adnominal 

PP PP 

Noun 

過程 
process 

PostPcm 

を 
ACC 

Para Dep 

“A process from birth to death” 

Figure 7: From inter-chunk dependencies to a tree.

(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.

In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are

rule A: Number → PrefixOfNumber Number
rule B: ClassifierPhrase → Number Classifier

in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun

約 千 人 死亡
approximately thousand people death

PrefixOfNumber Number Classifier CommonNoun
“death of approximately one thousand people”

as in
(((約 千) 人) 死亡)

(((approximately thousand) people) death)

We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.

The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent

dep modifier-type precedence
Para から/PostPcm まで/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...

Table 3: Rules to determine adjoin position.

PP 

Noun 

犬 
dog 

PostP 

に 
DAT 

VP 

NP 

Adj 

白い 
white 

NP 

VP 

Noun 

猫 
cat 

Verb 

言っ 
say 

Aux 

た 
PAST 

VP PP 

Verb 

行け 
go! 

PostP 

と 
CMP 

ARG-to 

ARG-ni 

ARG-ga 

ARG-ga 

ARG-ga 

ARG-ga ARG-ni 

ARG-CLS 

NAIST 

JP 

Figure 8: Overlay of pred-arg structure annotation
(“The white cat who said “Go!” to the dog.”).

tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.

Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.

To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, “Para”, and
the lexical head of the modifier subtree, “ から
/PostPcm”, as the key, and find the precedence “ま
で/PostPcm, */(Verb|Aux), ...”. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), “ まで/PostPcm”, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.

The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).

1046



PP 

Noun 

交渉 
negotiation 

PostPcm 

に 
DAT 

VP 

Noun 

参加 
participation 

Verb 

さ 
do 

VerbSuffix 

せ 
CAUSE 

Aux 

た 
PAST 

VP 

VP 

S 

NPni 

NP 

交渉 
negotiation 

T1 

に 
DAT T4 

T5 

参加 
participation 

S＼S 

さ 
do 

S＼S 

せ 
CAUSE 

S＼S 

た 
PAST 

T3 

T2 

S 
＜ 

＜ 

＜  or   ＜B   

＜  or   ＜B   

＜  or   ＜B   

NPni 

NPnc 

交渉 
negotiation 

NPni＼NPnc 

に 
DAT 

Svo_s＼NPni 

Svo_s＼NPni 

参加 
participation 

Svo_s＼Svo_s 

さ 
do 

Scont＼Svo_s 

せ 
CAUSE 

Sbase＼Scont 

た 
PAST 

Scont＼NPni 

Sbase＼NPni 

Sbase 

Step 2-1 

Step 2-2, 2-3 

Figure 9: A phrase structure into a CCG deriva-
tion.

In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al.,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:

1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.

2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.

In the figure, “犬/dogに/DAT” is marked as the ni-
argument of the predicate “言っ/say” (Case 1), and
“白い/white 猫/cat” is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case

VP 

友達    に 
friend-DAT 

PP VP 

会う 
meet-BASE 

NPni ＜ 

VP 

10時    に 
10 o’clock-TIME 

PP VP 

会う 
meet-BASE 

T/T ＞ 

X 

S 

友達    に 
friend-DAT 

NPni S＼NPni 

会う 
meet-BASE 

S 

10時    に 
10 o’clock-TIME 

S＼S S 

会う 
meet-BASE 

“(to) meet at ten” 

“(to) meet a friend” 

Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).

2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.

The JP corpus provides only the function label
to each particle “to” in the text. We determined
the argument phrases marked by the “to” particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.

3.2 Phrase structures to CCG derivations

This step consists of three procedures (Fig. 9):

1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.

2. Apply combinatory rules to all branching and
obtain CCG derivations.

3. Add feature constraints to terminal nodes.

3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.

Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),

1047



VP 

Verb 

話さ 
Speak-NEG 

Aux 

なかっ 
not-CONT 

Aux 

た 
PAST-BASE 

VP 

Scont＼S 

Sbase＼S 

“did not speak” 

＜ or ＜B 

＜ or ＜B Scont＼NPga 

Sneg＼NPga 

話さ 
Speak-NEG 

Scont＼Sneg 

なかっ 
not-CONT 

Sbase＼Scont 

た 
PAST-BASE 

Sbase＼NPga 

Figure 11: An auxiliary verb and its conversion.

VP 

Verb 

調べ 
inquire-NEG 

VerbSuffix 

させる 
cause-BASE 

彼女    に 
her-DAT 

PP 

VP 

ARG-ga 

“(to) have her inquire” 

＜ 

S＼NPni[1] 

S＼S 

させる 
cause-BASE 

彼女    に 
her-DAT 

NPni[1] 

S 

S＼NPni[1] 

調べ 
inquire-NEG 

ga         [1] 

NPni[1] 

ga: [1] 

Figure 12: A causative construction.

ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).

Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for “た/PAST-
BASE” and Scont\S for “なかっ/not-CONT”. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).

Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument’s category
NP, as usual, we assign a restriction to the PAS

S＼NPo[1] 

S＼NPo 

買っ 
buy-CONT 

S＼S 

た 
PAST-ATTR 

NP 

本 
book 

NP 

NP[1]／NP[1] 

VP 

Verb 

買っ 
buy-CONT 

Aux 

た 
PAST-ATTR 

Noun 

本 
book 

NP 

S＼NP[1] 

NP[1]／NP[1] 

Noun 

店 
store 

NP 

本   を 
book-ACC 

PP VP 

Verb 

買っ 
buy-CONT 

Aux 

た 
PAST-ATTR 

VP 
X 

S 

NP／NP 

NP 

店 
store 

NP 

NP／NP 

本    を 
book-ACC 

NPo 

S 

S＼NPo 

S＼NPo 

買っ 
buy-CONT 

S＼S 

た 
PAST-ATTR 

“a store where (I) bought the book” 

“a book which (I) bought” 

Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).

of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.

Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
“book” is annotated as an accusative argument of
the predicate “buy”. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP “store” is not annotated as an argument.

Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.

1048



Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624

Table 4: Statistics of input linguistic resources.

3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then “in-
versely” applied so that the constraint assigned to
the parent transfers to the children.

3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for “調べ/inquire” (Fig. 12),
the PAS for “inquire” is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as調べ ` S\NPni[1]: inquire([1]).

3.3 Lexical entries

Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.

4 Evaluation

We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text

2
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?

Kyoto\%20University\%20Text\%20Corpus

Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2

Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9

Table 5: Statistics of corpus conversion.

Sentential Coverage
Covered Uncovered Cov. (%)

Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage

Word Known Unknown
combi. cat. word

Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0

Table 6: Sentential and lexical coverage.

corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.

4.1 Corpus conversion and lexicon extraction

Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.

For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.

4.2 Evaluation of coverage

Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage

3
http://cl.naist.jp/nldata/corpus/

4
https://alaginrc.nict.go.jp/resources/tocorpus/

tocorpusabstract.html

1049



of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.

Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a “weak” sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.

4.3 Evaluation of parsing accuracy

Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.

Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser’s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09

5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.

Supertagging accuracy
Lex. Cov. Cat. Acc.

Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance

LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00

Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.

in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.

5 Conclusion

In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.

Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.

1050



References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-

tax. Kuroshio Shuppan. (In Japanese).

Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240–1246.

Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27–38.

Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.

Ruken Çakıcı. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73–78.

Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).

Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.

Hiroki Hanaoka, Hideki Mima, and Jun’ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.

Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201–209.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.

Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.

Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804–813.

Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132–139.

Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.

Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495–498. (In Japanese).

Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.

Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.

M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.

Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35–80.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.

Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.

Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.

Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.

Mark Steedman. 2001. The Syntactic Process. MIT
Press.

David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240–247.

Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010–
1014.

Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.

1051


