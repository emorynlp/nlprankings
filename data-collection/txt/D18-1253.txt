



















































Subgoal Discovery for Hierarchical Dialogue Policy Learning


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2298‚Äì2309
Brussels, Belgium, October 31 - November 4, 2018. c¬©2018 Association for Computational Linguistics

2298

Subgoal Discovery for Hierarchical Dialogue Policy Learning

Da Tang? Xiujun Li‚Ä† Jianfeng Gao‚Ä† Chong Wang‚Ä° Lihong Li‚Ä° Tony Jebara?
‚Ä†Microsoft Research, Redmond, WA, USA

?Columbia University, NY, USA
‚Ä°Google Inc., Kirkland, WA, USA
{xiul,jfgao}@microsoft.com

{datang,jebara}@cs.columbia.edu
{chongw,lihong}@google.com

Abstract

Developing agents to engage in complex goal-
oriented dialogues is challenging partly be-
cause the main learning signals are very sparse
in long conversations. In this paper, we pro-
pose a divide-and-conquer approach that dis-
covers and exploits the hidden structure of the
task to enable efficient policy learning. First,
given successful example dialogues, we pro-
pose the Subgoal Discovery Network (SDN)
to divide a complex goal-oriented task into
a set of simpler subgoals in an unsupervised
fashion. We then use these subgoals to learn
a multi-level policy by hierarchical reinforce-
ment learning. We demonstrate our method
by building a dialogue agent for the composite
task of travel planning. Experiments with sim-
ulated and real users show that our approach
performs competitively against a state-of-the-
art method that requires human-defined sub-
goals. Moreover, we show that the learned
subgoals are often human comprehensible.

1 Introduction

Consider we want to plan a trip to a distant city
using a dialogue agent. The agent must make
choices at each leg, e.g., whether to fly or to drive,
whether to book a hotel. Each of these steps in
turn involves making a sequence of decisions all
the way down to lower-level actions. For exam-
ple, to book a hotel involves identifying the loca-
tion, specifying the check-in date and time, and
negotiating the price etc.

The above process of the agent has a natural
hierarchy: a top-level process selects which sub-
goal to complete, and a low-level process chooses
primitive actions to accomplish the selected sub-
goal. Within the reinforcement learning (RL)
paradigm, such a hierarchical decision making
process can be formulated in the options frame-
work (Sutton et al., 1999), where subgoals with

their own reward functions are used to learn poli-
cies for achieving these subgoals. These learned
policies are then used as temporally extended ac-
tions, or options, for solving the entire task.

Based on the options framework, researchers
have developed dialogue agents for complex tasks,
such as travel planning, using hierarchical re-
inforcement learning (HRL) (CuayaÃÅhuitl et al.,
2010). Recently, Peng et al. (2017b) showed that
the use of subgoals mitigates the reward sparsity
and leads to more effective exploration for dia-
logue policy learning. However, these subgoals
need to be human-defined which limits the appli-
cability of the approach in practice because the do-
main knowledge required to properly define sub-
goals is often not available in many cases.

In this paper, we propose a simple yet effective
Subgoal Discovery Network (SDN) that discovers
useful subgoals automatically for an RL-based di-
alogue agent. The SDN takes as input a collection
of successful conversations, and identifies ‚Äúhub‚Äù
states as subgoals. Intuitively, a hub state is a re-
gion in the agent‚Äôs state space that the agent tends
to visit frequently on successful paths to a goal but
not on unsuccessful paths. Given the discovered
subgoals, HRL can be applied to learn a hierar-
chical dialogue policy which consists of (1) a top-
level policy that selects among subgoals, and (2) a
low-level policy that chooses primitive actions to
achieve selected subgoals.

We present the first study of learning dialogue
agents with automatically discovered subgoals.
We demonstrate the effectiveness of our approach
by building a composite task-completion dialogue
agent for travel planning. Experiments with
both simulated and real users show that an agent
learned with discovered subgoals performs com-
petitively against an agent learned using expert-
defined subgoals, and significantly outperforms an
agent learned without subgoals. We also find that



2299

the subgoals discovered by SDN are often human
comprehensible.

2 Background

A goal-oriented dialogue can be formulated as a
Markov decision process, or MDP (Levin et al.,
2000), in which the agent interacts with its en-
vironment over a sequence of discrete steps. At
each step t ‚àà {0, 1, . . .}, the agent observes the
current state st of the conversation (Henderson,
2015; MrksÃåicÃÅ et al., 2017; Li et al., 2017), and
chooses action at according to a policy œÄ. Here,
the action may be a natural-language sentence or
a speech act, among others. Then, the agent re-
ceives a numerical reward rt and switches to next
state st+1. The process repeats until the dialogue
terminates. The agent is to learn to choose op-
timal actions {at}t=1,2,... so as to maximize the
total discounted reward r0 + Œ≥r1 + Œ≥2r2 + ¬∑ ¬∑ ¬∑ ,
where Œ≥ ‚àà [0, 1] is a discount factor. This learning
paradigm is known as reinforcement learning, or
RL (Sutton and Barto, 1998).

When facing a complex task, it is often more
efficient to divide it into multiple simpler sub-
tasks, solve them, and combine the partial solu-
tions into a full solution for the original task. Such
an approach may be formalized as hierarchical RL
(HRL) in the options framework (Sutton et al.,
1999). An option can be understood as a subgoal,
which consists of an initiation condition (when
the subgoal can be triggered), an option policy
to solve the subgoal, and a termination condition
(when the subgoal is considered finished).

When subgoals are given, there exist effective
RL algorithms to learn a hierarchical policy. A
major open challenge is the automatic discovery
of subgoals from data, the main innovation of this
work is covered in the next section.

3 Subgoal Discovery for HRL

Figure 1 shows the overall workflow of our pro-
posed method of using automatic subgoal discov-
ery for HRL. First a dialogue session is divided
into several segments. Then at the end of those
segments (subgoals), we equip an intrinsic or ex-
trinsic reward for the HRL algorithm to learn a hi-
erarchical dialogue policy. Note that only the last
segment has an extrinsic reward. The details of
the segmentation algorithm and how to use sub-
goals for HRL are presented in Section 3.1 and
Section 3.3.

SDN

Intrinsic
Reward

Extrinsic 
Reward

HRLIntrinsicReward ùúã
Dialogue
Session

Figure 1: The workflow for HRL with subgoal discov-
ery. In addition to the extrinsic reward at the end of
the dialogue session, HRL also uses intrinsic rewards
induced by the subgoals (or the ends of dialogue seg-
ments). Section 3.2 details the reward design for HRL.

3.1 Subgoal Discovery Network
Assume that we have collected a set of successful
state trajectories of a task, as shown in Figure 2.
We want to find subgoal states, such as the three
red states s4, s9 and s13, which form the ‚Äúhubs‚Äù
of these trajectories. These hub states indicate the
subgoals, and thus divide a state trajectory into
several segments, each for an option1.

s0

s1

s2

s3

s4 s9

s10

s11

s12

s13

Start Goal

s6

s8

s7

Figure 2: Illustration of ‚Äúsubgoals‚Äù. As-
suming that there are three state trajectories
(s0, s1, s4, s6, s9, s10, s13), (s0, s2, s4, s7, s9, s11, s13)
and (s0, s3, s4, s8, s9, s12, s13). Then red states s4, s9,
s13 could be good candidates for ‚Äúsubgoals‚Äù.

Thus, discovering subgoals by identifying hubs
in state trajectories is equivalent to segmenting
state trajectories into options. In this work, we for-
mulate subgoal discovery as a state trajectory seg-
mentation problem, and address it using the Sub-
goal Discovery Network (SDN), inspired by the
sequence segmentation model (Wang et al., 2017).

The SDN architecture. SDN repeats a two-
stage process of generating a state trajectory seg-
ment, until a trajectory termination symbol is gen-
erated: first it uses an initial segment hidden state

1There are many ways of creating a new option „ÄàI, œÄ, Œ≤„Äâ
for a discovered subgoal state. For example, when a subgoal
state is identified at time step t, we add to I the set of states
visited by the agent from time t‚àí n to t, where n is a pre-set
parameter. I is therefore the union of all such states over all
the state trajectories. The termination condition Œ≤ is set to 1
when the subgoal is reached or when the agent is no longer in
I , and to 0 otherwise. In the deep RL setting where states are
represented by continuous vectors, Œ≤ is a probability whose
value is proportional to the vector distance e.g., between cur-
rent state and subgoal state.



2300

s0

s1

s1

s2

s2

#

s2

s3

s3

s4

s4

#

s4

s5

s5

#

s0 s1 s2 s3 s4 s5

#

M

RNN1

RNN2

Figure 3: Illustration of SDN for state trajectory
(s0, . . . , s5) with s2, s4 and s5 as subgoals. Symbol #
is the termination. The top-level RNN (RNN1) models
segments and the low-level RNN (RNN2) provides in-
formation about previous states from RNN1. The em-
bedding matrix M maps the outputs of RNN2 to low
dimensional representations so as to be consistent with
the input dimensionality of RNN1. Note that state s5 is
associated with two termination symbols #; one is for
the termination of the last segment and the other is for
the termination of the entire trajectory.

to start a new segment, or a trajectory termina-
tion symbol to terminate the trajectory, given all
previous states; if the trajectory is not terminated,
then keep generating the next state in this trajec-
tory segment given previous states until a segment
termination symbol is generated. We illustrated
this process in Figure 3.

We model the likelihood of each segment using
an RNN, denoted as RNN1. During the training, at
each time step, RNN1 predicts the next state with
the current state as input, until it reaches the op-
tion termination symbol #. Since different options
are under different conditions, it is not plausible to
apply a fixed initial input to each segment. There-
fore, we use another RNN (RNN2) to encode
all previous states to provide relevant information
and we transform these information to low dimen-
sional representations as the initial inputs for the
RNN1 instances. This is based on the causality as-
sumption of the options framework (Sutton et al.,
1999) ‚Äî the agent should be able to determine
the next option given all previous information, and
this should not depend on information related to
any later state. The low dimensional representa-
tions are obtained via a global subgoal embedding
matrix M ‚àà Rd√óD, where d and D are the di-
mensionality of RNN1‚Äôs input layer and RNN2‚Äôs
output layer, respectively. Mathematically, if the

output of RNN2 at time step t is ot, then from
time t the RNN1 instance has M ¬∑ softmax(ot)
as its initial input2. D is the number of subgoals
we aim to learn. Ideally, the vector softmax(ot) in
a well-trained SDN is close to an one-hot vector.
Therefore,M ¬∑softmax(ot) should be close to one
column in M and we can view that M provides at
most D different ‚Äúembedding vectors‚Äù for RNN1
as inputs, indicating at most D different subgoals.
Even in the case where softmax(ot) is not close
to any one-hot vector, choosing a small D helps
avoid overfitting.

Segmentation likelihood. Given the state tra-
jectory (s0, . . . , s5), assuming that s2, s4 and
s5 are the discovered subgoal states, we model
the conditional likelihood of a proposed segmen-
tation œÉ = ((s0, s1, s2), (s2, s3, s4), (s4, s5)) as
p(œÉ|s0) = p((s0, s1, s2)|s0) ¬∑ p((s2, s3, s4)|s0:2) ¬∑
p((s4, s5)|s0:4), where each probability term
p(¬∑|s0:i) is based on an RNN1 instance. And for
the whole trajectory (s0, . . . , s5), its likelihood is
the sum over all possible segmentations.

Generally, for state trajectory s = (s0, . . . , sT ),
we model its likelihood as follows3:

LS(s) =
‚àë

œÉ‚äÜS(s),length(œÉ)‚â§S

length(œÉ)‚àè
i=1

p(œÉi|œÑ(œÉ1:i)),

(1)
where S(s) is the set of all possible segmentations
for the trajectory s, œÉi denotes the ith segment in
the segmentation œÉ, and œÑ is the concatenation op-
erator. S is an upper limit on the maximal num-
ber of segments. This parameter is important for
learning subgoals in our setting since we usually
prefer a small number of subgoals. This is differ-
ent from Wang et al. (2017), where a maximum
segment length is enforced.

We use maximum likelihood estimation with
Eq. (1) for training. However, the number of pos-
sible segmentations is exponential in S(s) and
the naive enumeration is intractable. Here, dy-
namic programming is employed to compute the
likelihood in Eq. (1) efficiently: for a trajectory
s = (s0, . . . , sT ), if we denote the sub-trajectory
(si, . . . , st) of s as si:t, then its likelihood follows

2softmax(ot)i = exp(ot,i)/
D‚àë
i‚Ä≤=1

exp(ot,i‚Ä≤) ‚àà RD for

ot = (ot,1, . . . , ot,D).
3For notation convenience, we include s0 into the obser-

vational sequence, though s0 is always conditioned upon.



2301

the below recursion:

Lm(s0:t) =

Ô£±Ô£¥Ô£≤Ô£¥Ô£≥
t‚àí1‚àë
i=0

Lm‚àí1(s0:i)p(si:t|s0:i), m > 0,

I[t = 0], m = 0.

Here, Lm(s0:t) denotes the likelihood of sub-
trajectory s0:t with no more than m segments and
I[¬∑] is an indicator function. p(si:t|s0:i) is the
likelihood segment si:t given the previous history,
where RNN1 models the segment and RNN2 mod-
els the history as shown in Figure 3. With this re-
cursion, we can compute the likelihood LS(s) for
the trajectory s = (s0, . . . , sT ) in O(ST 2) time.

Learning algorithm. We denote Œ∏s as the model
parameter including the parameters of the em-
bedding matrix M , RNN1 and RNN2. We then
parameterize the segment likelihood function as
p(si:t|s0:i) = p(si:t|s0:i; Œ∏s), and the trajectory
likelihood function as Lm(s0:t) = Lm(s0:t; Œ∏s).

Given a set of N state trajectories
(s(1), . . . , s(N)), we optimize Œ∏s by mini-
mizing the negative mean log-likelihood with L2
regularization term 12Œª||Œ∏

s||2 where Œª > 0, using
stochastic gradient descent:

LS(Œ∏s, Œª) = ‚àí
1

N

N‚àë
i=1

logLS(s
(i), Œ∏s) +

1

2
Œª||Œ∏s||2. (2)

Algorithm 1 outlines the training procedure for
SDN using stochastic gradient descent.

Algorithm 1 Learning SDN
Input: A set of state trajectories (s1, . . . sN ), the number of

segments limit S, initial learning rate Œ∑ > 0.
1: Initialize the SDN parameter Œ∏s.
2: while not converged do
3: Compute the gradient ‚àáŒ∏sLS(Œ∏s, Œª) of the loss

LS(Œ∏s, Œª) as in Eq. (2).
4: Update Œ∏s ‚Üê Œ∏s ‚àí Œ∑‚àáŒ∏sLS(Œ∏s, Œª).
5: Update the learning rate Œ∑.
6: end while

3.2 Hierarchical Dialogue Policy Learning

Before describing how we use a trained SDN
model for HRL, we first present a short review of
HRL for a task-oriented dialogue system. Follow-
ing the options framework (Sutton et al., 1999),
assume that we have a state set S , an option set G
and a finite primitive action set A.

The HRL approach we take learns two Q-
functions (Peng et al., 2017b), parameterized by
Œ∏e and Œ∏i, respectively:

‚Ä¢ The top-level Q‚àó(s, g; Œ∏e) measures the maxi-
mum total discounted extrinsic reward received
by choosing subgoal g in state s and then fol-
lowing an optimal policy. These extrinsic re-
wards are the objective to be maximized by the
entire dialogue policy.
‚Ä¢ The low-level Q‚àó(s, a, g; Œ∏i) measures the max-

imum total discounted intrinsic reward received
to achieve a given subgoal g, by choosing action
a in state s and then following an optimal option
policy. These intrinsic rewards are used to learn
an option policy to achieve a given subgoal.

Suppose we have a dialogue session of T turns:
œÑ = (s0, a0, r0, . . . , sT ), which is segmented into
a sequence of subgoals g0, g1, . . . ‚àà G. Consider
one of these subgoals g which starts and ends in
steps t0 and t1, respectively.

The top-level Q-function is learned using Q-
learning, by treating subgoals as temporally ex-
tended actions:

Œ∏e ‚Üê Œ∏e+Œ± ¬∑ (q ‚àíQ(st, g; Œ∏e)) ¬∑‚àáŒ∏eQ(st, g; Œ∏e) ,

where

q =

t1‚àí1‚àë
t=t0

Œ≥t‚àít0ret + Œ≥
t1‚àít0 max

g‚Ä≤‚ààG
Q(st1 , g

‚Ä≤; Œ∏e) ,

and Œ± is the step-size parameter, Œ≥ ‚àà [0, 1] is a
discount factor. In the above expression of q, the
first term refers to the total discounted reward dur-
ing fulfillment of subgoal g, and the second to the
maximum total discounted after g is fulfilled.

The low-level Q-function is learned in a sim-
ilar way, and follows the standard Q-learning up-
date, except that intrinsic rewards for subgoal g are
used. Specifically, for t = t0, t0 + 1, . . . , t1 ‚àí 1:

Œ∏i ‚Üê Œ∏i + Œ± ¬∑ (qt ‚àíQ(st, at, g; Œ∏e)) ¬∑ ‚àáŒ∏iQ(st, at, g; Œ∏i) ,

where

qt = r
i
t + Œ≥max

a‚Ä≤‚ààA
Q(st+1, a

‚Ä≤, g; Œ∏i) .

Here, the intrinsic reward rit is provided by the in-
ternal critic of dialogue manager. More details are
in Appendix A.

In hierarchical policy learning, the combination
of the extrinsic and intrinsic rewards is expected to
help the agent to successfully accomplish a com-
posite task as fast as possible while trying to avoid
unnecessary subtask switches. Hence, we define
the extrinsic and intrinsic rewards as follows:



2302

Extrinsic Reward. Let L be the maximum
number of turns of a dialogue, and K the number
of subgoals. At the end of a dialogue, the agent re-
ceives a positive extrinsic reward of 2L for a suc-
cess dialogue, or ‚àíL for a failure dialogue; for
each turn, the agent receives an extrinsic reward
of ‚àí1 to encourage shorter dialogues.

Intrinsic Reward. When a subgoal terminates,
the agent receives a positive intrinsic reward of
2L/K if a subgoal is completed successfully, or
a negative intrinsic reward of ‚àí1 otherwise; for
each turn, the agent receives an intrinsic reward
‚àí1 to encourage shorter dialogues.

3.3 Hierarchical Policy Learning with SDN

We use a trained SDN in HRL as follows. The
agent starts from the initial state s0, keeps sam-
pling the output from the distribution related to the
top-level RNN (RNN1) until a termination symbol
# is generated, which indicates the agent reaches a
subgoal. In this process, intrinsic rewards are gen-
erated as specified in the previous subsection. Af-
ter # is generated, the agent selects a new option,
and repeats this process.

This type of naive sampling may allow the op-
tion to terminate at some places with a low proba-
bility. To stabilize the HRL training, we introduce
a threshold p ‚àà (0, 1), which directs the agent to
terminate an option if and only if the probability
of outputting # is at least p. We found this modi-
fication leads to better behavior of the HRL agent
than the naive sampling method, since it normally
has a smaller variance.

In the HRL training, the agent only uses the
probability of outputting # to decide subgoal ter-
mination. Algorithm 2 outlines the full proce-
dure of one episode for hierarchical dialogue poli-
cies with a trained SDN in the composite task-
completion dialogue system.

4 Experiments and Results

We evaluate the proposed model on a travel plan-
ning scenario for composite task-oriented dia-
logues (Peng et al., 2017b). Over the exchange
of a conversation, the agent gathers information
about the user‚Äôs intent before booking a trip. The
environment then assesses a binary outcome (suc-
cess or failure) at the end of the conversation,
based on (1) whether a trip is booked, and (2)
whether the trip satisfies the user‚Äôs constraints.

Algorithm 2 HRL episode with a trained SDN
Input: A trained SDN M, initial state s0 of an episode,

threshold p, the HRL agent A.
1: Initialize an RNN2 instanceR2 with parameters fromM

and s0 as the initial input.
2: Initialize an RNN1 instanceR1 with parameters fromM

and M ¬∑ softmax(oRNN20 ) as the initial input, where M is
the embedding matrix (fromM) and oRNN20 is the initial
output of R2.

3: Current state s‚Üê s0.
4: Select an option o using the agent A.
5: while Not reached the final goal do
6: Select an action a according to s and o using the agent

A. Get the reward r and the next state s‚Ä≤ from the
environment.

7: Place s‚Ä≤ to R2, denote oRNN2t as R2‚Äôs latest output and
take M ¬∑ softmax(oRNN2t ) as the R1‚Äôs new input. Let
ps‚Ä≤ be the probability of outputting the termination
symbol #.

8: if ps‚Ä≤ ‚â• p then
9: Select a new option o using the agent A.

10: Re-initialize R1 using the latest output from R2
and the embedding matrix M .

11: end if
12: end while

Dataset. The raw dataset in our experiments is
from a publicly available multi-domain dialogue
corpus (El Asri et al., 2017). Following Peng
et al. (2017b), a few changes were made to in-
troduce dependencies among subtasks. For exam-
ple, the hotel check-in date should be the same
with the departure flight arrival date. The data was
mainly used to create simulated users, and to build
the knowledge bases for the subtasks of booking
flights and reserving hotels.

User Simulator. In order to learn good policies,
RL algorithms typically need an environment to
interact with. In the dialogue research community,
it is common to use simulated users for this pur-
pose (Schatzmann et al., 2007; Li et al., 2017; Liu
and Lane, 2017). In this work, we adapted a pub-
licly available user simulator (Li et al., 2016) to the
composite task-completion dialogue setting with
the dataset described above. During training, the
simulator provides the agent with an (extrinsic) re-
ward signal at the end of the dialogue. A dialogue
is considered to be successful only when a travel
plan is booked successfully, and the information
provided by the agent satisfies user‚Äôs constraints.

Baseline Agents. We benchmarked the pro-
posed agent (referred to as the m-HRL Agent)
against three baseline agents:
‚Ä¢ A Rule Agent uses a sophisticated, hand-crafted

dialogue policy, which requests and informs a
hand-picked subset of necessary slots, and then



2303

confirms with the user about the reserved trip
before booking the flight and hotel.
‚Ä¢ A flat RL Agent is trained with a standard deep

reinforcement learning method, DQN (Mnih
et al., 2015), which learns a flat dialogue pol-
icy using extrinsic rewards only.
‚Ä¢ A h-HRL Agent is trained with hierarchical deep

reinforcement learning (HDQN), which learns
a hierarchical dialogue policy based on human-
defined subgoals (Peng et al., 2017b).

Collecting State Trajectories. Recall that our
subgoal discovery approach takes as input a set
of state trajectories which lead to successful out-
comes. In practice, one can collect a large set of
successful state trajectories, either by asking hu-
man experts to demonstrate (e.g., in a call center),
or by rolling out a reasonably good policy (e.g., a
policy designed by human experts). In this paper,
we obtain dialogue state trajectories from a rule-
based agent which is handcrafted by a domain ex-
pert, the performance of this rule-based agent can
achieve success rate of 32.2% as shown in Figure 4
and Table 1. We only collect the successful dia-
logue sessions from the roll-outs of the rule-based
agent, and try to learn the subgoals from these di-
alogue state trajectories.

Experiment Settings. To train SDN, we use
RMSProp (Tieleman and Hinton, 2012) to opti-
mize the model parameters. For both RNN1 and
RNN2, we use LSTM (Hochreiter and Schmid-
huber, 1997) as hidden units and set the hidden
size to 50. We set embedding matrix M with
D = 4 columns. As we discussed in Section 3.1,
D captures the maximum number of subgoals that
the model is expected to learn. Again, to avoid
SDN from learning many unnecessary subgoals,
we only allow segmentation with at most S = 4
segments during subgoal training. The values for
D and S are usually set to be a little bit larger than
the expected number of subgoals (e.g., 2 or 3 for
this task) since we expect a great proportion of the
subgoals that SDN learns are useful, but not nec-
essary for all of them. As long as SDN discovers
useful subgoals that guide the agent to learn poli-
cies faster, it is beneficial for HRL training, even
if some non-perfect subgoals are found. During
the HRL training, we use the learned SDN to pro-
pose subgoal-completion queries. In our experi-
ment, we set the maximum turn L = 60.

We collected N = 1634 successful, but imper-

0 100 200 300 400 500 600
Simulation Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Su
cc

es
s 

ra
te

Rule Agent
RL Agent
h-HRL Agent
m-HRL Agent

Figure 4: Learning curves of agents under simulation.

Agent Success Rate Turns Reward
Rule .3220 46.23 -24.02
RL .4440 45.50 -1.834

h-HRL .6485 44.23 35.32
m-HRL .6455 44.85 34.77

Table 1: Performance of agents with simulated user.

fect, dialogue episodes from the rule-based agent
in Table 1 and randomly choose 80% of these di-
alogue state trajectories for training SDN. The re-
maining 20% were used as a validation set.

As illustrated in Section 3.3, SDN starts a new
RNN1 instance and issues a subgoal-completion
query when the probability of outputting the ter-
mination symbol # is above a certain threshold p
(as in Algorithm 2). In our experiment, p is set to
be 0.2, which was manually picked according to
the termination probability during SDN training.

In dialogue policy learning, for the baseline RL
agent, we set the size of the hidden layer to 80.
For the HRL agents, both top-level and low-level
dialogue policies have a hidden layer size of 80.
RMSprop was applied to optimize the parameters.
We set the batch size to be 16. During train-
ing, we used ÔøΩ-greedy strategy for exploration with
annealing and set Œ≥ = 0.95. For each simula-
tion epoch, we simulated 100 dialogues and stored
these state transition tuples in the experience re-
play buffers. At the end of each simulation epoch,
the model was updated with all the transition tu-
ples in the buffers in a batch manner.

4.1 Simulated User Evaluation

In the composite task-completion dialogue sce-
nario, we compared the proposed m-HRL agent



2304

RL h-HRL m-HRL
0.0

0.2

0.4

0.6

0.8

1.0

1.2
Su

cc
es

s 
R

at
e

0.522
0.683 0.667

p=0.032 p=0.421
p=0.044

67
60 69

Figure 5: Performance of three agents tested with real
users: success rate, number of dialogues and p-value
are indicated on each bar (difference in mean is signif-
icant with p < 0.05).

with three baseline agents in terms of three met-
rics: success rate4, average rewards and average
turns per dialogue session.

Figure 4 shows the learning curves of all four
agents trained against the simulated user. Each
learning curve was averaged over 5 runs. Table 1
shows the test performance where each number
was averaged over 5 runs and each run gener-
ated 2000 simulated dialogues. We find that the
HRL agents generated higher success rates and
needed fewer conversation turns to achieve the
users‚Äô goals than the rule-based agent and the flat
RL agent. The performance of the m-HRL agent
is tied with that of the h-HRL agent, even though
the latter requires high-quality subgoals designed
by human experts.

4.2 Human Evaluation
We further evaluated the agents that were trained
on simulated users against real users, who were
recruited from the authors‚Äô organization. We con-
ducted a study using the one RL agent and two
HRL agents {RL, h-HRL, m-HRL}, and com-
pared two pairs: {RL, m-HRL} and {h-HRL, m-
HRL}. In each dialogue session, one agent was
randomly selected from the pool to interact with
a user. The user was not aware of which agent
was selected to avoid systematic bias. The user
was presented with a goal sampled from a user-
goal corpus, then was instructed to converse with
the agent to complete the given task. At the end of
each dialogue session, the user was asked to give a
rating on a scale from 1 to 5 based on the natural-

4Success rate is the fraction of dialogues which accom-
plished the task successfully within the maximum turns.

RL h-HRL m-HRL
0

1

2

3

4

5

6

U
se

r R
at

in
g p=4.62E-4 p=0.024

p=0.046

Figure 6: Distribution of user ratings for three agents
in human evaluation

ness and coherence of the dialogue; here, 1 is the
worst rating and 5 the best. In total, we collected
196 dialogue sessions from 10 human users.

Figure 5 summarizes the performances of these
agents against real users in terms of success rate.
Figure 6 shows the distribution of user ratings for
each agent. For these two metrics, both HRL
agents were significantly better than the flat RL
agent. Another interesting observation is that the
m-HRL agent performs similarly to the h-HRL
agent in terms of success rate in the real user
study as shown on Figure 5. Meanwhile in Fig-
ure 6, the h-HRL agent is significantly better than
m-HRL agent in terms of real user ratings. This
may be caused by the probabilistic termination of
subgoals: we used a threshold strategy to decide
whether to terminate a subgoal. This could intro-
duce variance so the agent might not behave rea-
sonably compared with human-defined subgoals
which terminate deterministically.

4.3 Subgoal Visualization
Table 2 shows the subgoals discovered by SDN in
a sample dialogue by a rule-based agent interact-
ing with the simulated user. The rule-based agent
is equipped with a human-defined subtask struc-
ture, which always solves subtask flight (turns 1‚Äì
15) before hotel (turns 16‚Äì23), as shown in the first
column. At turn 10, the user starts to talk about
hotel while the rule-based agent is still working
on the pre-defined, unfinished flight subtask until
subtask flight is finished at turn 15. At turn 16, the
user switches to hotel, and so does the rule-based
agent until the end of the dialogue. For this rule-
based agent, the human-defined subgoal (flight)
terminates at turn 15. Meanwhile, our SDN model
detected two subgoals (except for the final goal):



2305

h-Task ID Text

Fl
ig

ht
1 agt: Hi, may I know your departure city?
2 usr: I depart from Fort Lauderdale.
3 agt: And what is your destination city?
4 usr: I need economy seat to Pittsburgh.
5 agt: What is your departure date?
6 usr: 2016-09-12, what time is available for the flight?
7 agt: There is one flight at 21:00:00.
8 usr: What is the price for that flight?
9 agt: $423.

Subgoal 1: A detected subgoal terminates!
10 usr: I need to reserve a hotel for five people.
11 agt: What is your return date for the trip?
12 usr: 2016-09-18, I need to book 5 tickets for the flight.
13 agt: What time do you prefer for return flight?
14 usr: What time is available that day?
15 agt: There is one flight departing at 11:00:00AM.

Subgoal 2: A detected subgoal terminates!

H
ot

el

16 usr: What hotel is available?
17 agt: There is one hotel called Winter Hill Inn available.
18 usr: What is the price for hotel?
19 agt: $410.
20 usr: Great, could you book the trip for me?
21 agt: Okay. Trip tickets are issued.
22 usr: Thanks very much!
23 agt: You are welcome!

Table 2: Discovered subgoals (except for the final goal)
in a sample dialogue by a rule-based agent interacting
with user simulator. The left column (h-Task) shows
the human-defined subtasks for the rule-based agent.
SDN detects two subgoals that terminate at turn 9 and
15 respectively. (h-Task: human-defined subtask, ID:
turn ID, agt: Agent, usr: User)

one terminating at turn 9 (Subgoal 1), and another
terminating at turn 15 (Subgoal 2). Subgoal 2 is
consistent with the human-defined subgoal. Sub-
goal 1 is also reasonable since the user tries to
switch to hotel at turn 10. In Appendix B, Table 3
shows a sample dialogue session by m-HRL agent
interacting with a real user.

5 Related Work

Task-completion dialogue systems have attracted
numerous research efforts, and there is grow-
ing interest in leveraging reinforcement learn-
ing for policy learning. One line of research is
on single-domain task-completion dialogues with
flat deep reinforcement learning algorithms such
as DQN (Zhao and Eskenazi, 2016; Li et al.,
2017; Peng et al., 2018), actor-critic (Peng et al.,
2017a; Liu and Lane, 2017) and policy gradi-
ents (Williams et al., 2017; Liu et al., 2017). An-
other line of research addresses multi-domain di-
alogues where each domain is handled by a sepa-
rate agent (GasÃåicÃÅ et al., 2015; GasÃåicÃÅ et al., 2015;
CuayaÃÅhuitl et al., 2016). Recently, Peng et al.
(2017b) presented a composite task-completion
dialogue system. Unlike multi-domain dialogue
systems, composite tasks introduce inter-subtask
constraints. As a result, the completion of a set

of individual subtasks does not guarantee the so-
lution of the entire task.

CuayaÃÅhuitl et al. (2010) applied HRL to di-
alogue policy learning, although they focus on
problems with a small state space. Later,
Budzianowski et al. (2017) used HRL in multi-
domain dialogue systems. Peng et al. (2017b) first
presented an HRL agent with a global state tracker
to learn the dialogue policy in the composite task-
completion dialogue systems. All these works are
built based on subgoals that were pre-defined with
human domain knowledge for the specific tasks.
The only job of the policy learner is to learn a hier-
archical dialogue policy, which leaves the subgoal
discovery problem unsolved. In addition to the
applications in dialogue systems, subgoal is also
widely studied in the linguistics research commu-
nity (Allwood, 2000; Linell, 2009).

In the literature, researchers have proposed al-
gorithms to automatically discovery subgoals for
hierarchical RL. One large body of work is based
on analyzing the spatial structure of the state tran-
sition graphs, by identifying bottleneck states or
clusters, among others (Stolle and Precup, 2002;
McGovern and Barto, 2001; Mannor et al., 2004;
SÃßimsÃßek et al., 2005; Entezari et al., 2011; Bacon,
2013). Another family of algorithms identifies
commonalities of policies and extracts these par-
tial policies as useful skills (Thrun and Schwartz,
1994; Pickett and Barto, 2002; Brunskill and Li,
2014). While similar in spirit to ours, these meth-
ods do not easily scale to continuous problems as
in dialogue systems. More recently, researchers
have proposed deep learning models to discover
subgoals in continuous-state MDPs (Bacon et al.,
2017; Machado et al., 2017; Vezhnevets et al.,
2017). It would be interesting to see how effec-
tive they are for dialogue management.

Segmental structures are common in human
languages. In the NLP community, some related
research on segmentation includes word segmen-
tation (Gao et al., 2005; Zhang et al., 2016) to
divide the words into meaningful units. Alterna-
tively, topic detection and tracking (Allan et al.,
1998; Sun et al., 2007) segment a stream of data
and identify stories or events in news or social
text. In this work, we formulate subgoal discovery
as a trajectory segmentation problem. Section 3.1
presents our approach to subgoal discovery which
is inspired by a probabilistic sequence segmenta-
tion model (Wang et al., 2017).



2306

6 Discussion and Conclusion

We have proposed the Subgoal Discovery Net-
work to learn subgoals automatically in an unsu-
pervised fashion without human domain knowl-
edge. Based on the discovered subgoals, we learn
the dialogue policy for complex task-completion
dialogue agents using HRL. Our experiments with
both simulated and real users on a composite task
of travel planning, show that an agent trained with
automatically discovered subgoals performs com-
petitively against an agent with human-defined
subgoals, and significantly outperforms an agent
without subgoals. Through visualization, we find
that SDN discovers reasonable, comprehensible
subgoals given only a small amount of suboptimal
but successful dialogue state trajectories.

These promising results suggest several direc-
tions for future research. First, we want to inte-
grate subgoal discovery into dialogue policy learn-
ing rather than treat them as two separate pro-
cesses. Second, we would like to extend SDN to
identify multi-level hierarchical structures among
subgoals so that we can handle more complex
tasks than those studied in this paper. Third, we
would like to generalize SDN to a wide range
of complex goal-oriented tasks beyond dialogue,
such as the particularly challenging Atari game of
Montezuma‚Äôs Revenge (Kulkarni et al., 2016).

Acknowledgments

We would like to thank the anonymous reviewers,
members of the xlab at the University of Washing-
ton, and Chris Brockett, Michel Galley for their
insightful comments on the work. Most of this
work was done while DT, CW & LL were with
Microsoft.

References

James Allan, Jaime G Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
detection and tracking pilot study final report.

Jens Allwood. 2000. An activity based approach to
pragmatics. Abduction, belief and context in dia-
logue: Studies in computational pragmatics, pages
47‚Äì80.

Pierre-Luc Bacon. 2013. On the bottleneck concept for
options discovery: Theoretical underpinnings and
extension in continuous state spaces. Master‚Äôs the-
sis, McGill University.

Pierre-Luc Bacon, Jean Harb, and Doina Precup. 2017.
The option-critic architecture. In Proceedings of
the 31st AAAI Conference on Artificial Intelligence
(AAAI), pages 1726‚Äì1734.

Emma Brunskill and Lihong Li. 2014. PAC-inspired
option discovery in lifelong reinforcement learning.
In Proceedings of the 31st International Conference
on Machine Learning (ICML), pages 316‚Äì324.

Pawel Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola
Mrksic, Tsung-Hsien Wen, Inigo Casanueva, Lina
Rojas-Barahona, and Milica Gasic. 2017. Sub-
domain modelling for dialogue management with
hierarchical reinforcement learning. arXiv preprint
arXiv:1706.06210.

Heriberto CuayaÃÅhuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue sys-
tem. Computer Speech and Language, 24(2):395‚Äì
429.

Heriberto CuayaÃÅhuitl, Seunghak Yu, Ashley
Williamson, and Jacob Carse. 2016. Deep re-
inforcement learning for multi-domain dialogue
systems. arXiv preprint arXiv:1611.08675.

Layla El Asri, Hannes Schulz, Shikhar Sharma,
Jeremie Zumer, Justin Harris, Emery Fine,
Rahul Mehrotra, and Kaheer Suleman. 2017.
Frames: A corpus for adding memory to goal-
oriented dialogue systems. arXiv:1704.00057.
URL: https://datasets.maluuba.com/Frames.

Negin Entezari, Mohammad Ebrahim Shiri, and
Parham Moradi. 2011. Subgoal discovery in rein-
forcement learning using local graph clustering.

Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Computational Linguistics, 31(4):531‚Äì574.

Milica GasÃåicÃÅ, Dongho Kim, Pirros Tsiakoulis, and
Steve Young. 2015. Distributed dialogue policies
for multi-domain statistical dialogue management.
In ICASSP 2015, pages 5371‚Äì5375.

Milica GasÃåicÃÅ, Nikola MrksÃåicÃÅ, Pei hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve J. Young.
2015. Policy committee for adaptation in multi-
domain spoken dialogue systems. In ASRU, pages
806‚Äì812. IEEE.

Matthew Henderson. 2015. Machine learning for dia-
log state tracking: A review. In Proceedings of the
1st International Workshop on Machine Learning in
Spoken Language Processing.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.



2307

Tejas D Kulkarni, Karthik Narasimhan, Ardavan
Saeedi, and Josh Tenenbaum. 2016. Hierarchical
deep reinforcement learning: Integrating temporal
abstraction and intrinsic motivation. In Advances
in Neural Information Processing Systems, pages
3675‚Äì3683.

Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialog strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11‚Äì23.

Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016. A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688.

Xuijun Li, Yun-Nung Chen, Lihong Li, and Jianfeng
Gao. 2017. End-to-end task-completion neural dia-
logue systems. arXiv preprint arXiv:1703.01008.

Per Linell. 2009. Rethinking Language, Mind, and
World Dialogically. Information Age Publishing.

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. arXiv preprint arXiv:1709.06136.

Bing Liu, Gokhan Tur, Dilek Hakkani-Tur, Pararth
Shah, and Larry Heck. 2017. End-to-end op-
timization of task-oriented dialogue model with
deep reinforcement learning. arXiv preprint
arXiv:1711.10712.

Marlos C. Machado, Marc G. Bellemare, and
Michael H. Bowling. 2017. A Laplacian framework
for option discovery in reinforcement learning. In
Proceedings of the 34th International Conference on
Machine Learning (ICML), pages 2295‚Äì2304.

Shie Mannor, Ishai Menache, Amit Hoze, and Uri
Klein. 2004. Dynamic abstraction in reinforce-
ment learning via clustering. In Proceedings of the
21st International Conference on Machine learning
(ICML), pages 560‚Äì567. ACM.

Amy McGovern and Andrew G Barto. 2001. Auto-
matic discovery of subgoals in reinforcement learn-
ing using diverse density. In Proceedings of the
18th International Conference on Machine Learn-
ing, pages 361‚Äì368.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529‚Äì533.

Nikola MrksÃåicÃÅ, Diarmuid OÃÅ SeÃÅaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve J. Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1777‚Äì1788.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
Yun-Nung Chen, and Kam-Fai Wong. 2017a. Ad-
versarial advantage actor-critic model for task-
completion dialogue policy learning. arXiv preprint
arXiv:1710.11277.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Kam-Fai Wong. 2018. Integrating planning
for task-completion dialogue policy learning. arXiv
preprint arXiv:1801.06176.

Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017b. Composite task-completion dialogue policy
learning via hierarchical deep reinforcement learn-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 2221‚Äì2230.

Marc Pickett and Andrew G. Barto. 2002. Policy-
Blocks: An algorithm for creating useful macro-
actions in reinforcement learning. In Proceedings
of the 19th International Conference on Machine
Learning (ICML), pages 506‚Äì513.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In NAACL 2007; Companion Volume, Short
Papers, pages 149‚Äì152. Association for Computa-
tional Linguistics.

OÃàzguÃàr SÃßimsÃßek, Alicia P. Wolfe, and Andrew G. Barto.
2005. Identifying useful subgoals in reinforcement
learning by local graph partitioning. In Proceedings
of the 22nd International Conference on Machine
Learning (ICML), pages 816‚Äì823.

Martin Stolle and Doina Precup. 2002. Learning op-
tions in reinforcement learning. In International
Symposium on Abstraction, Reformulation, and Ap-
proximation, pages 212‚Äì223. Springer.

Bingjun Sun, Prasenjit Mitra, C Lee Giles, John Yen,
and Hongyuan Zha. 2007. Topic segmentation with
shared topic detection and alignment of multiple
documents. In SIGIR 2007, pages 199‚Äì206. ACM.

Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press,
Cambridge, MA.

Richard S Sutton, Doina Precup, and Satinder Singh.
1999. Between MDPs and semi-MDPs: A frame-
work for temporal abstraction in reinforcement
learning. Artificial intelligence, 112(1-2):181‚Äì211.

Sebastian Thrun and Anton Schwartz. 1994. Finding
structure in reinforcement learning. In Advances
in Neural Information Processing Systems 7 (NIPS),
pages 385‚Äì392.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2):26‚Äì31.



2308

Alexander Sasha Vezhnevets, Simon Osindero, Tom
Schaul, Nicolas Heess, Max Jaderberg, David Sil-
ver, and Koray Kavukcuoglu. 2017. FeUdal Net-
works for hierarchical reinforcement learning. In
Proceedings of the 34th International Conference on
Machine Learning (ICML), pages 3540‚Äì3549.

Chong Wang, Yining Wang, Po-Sen Huang, Abdel-
rahman Mohamed, Dengyong Zhou, and Li Deng.
2017. Sequence modeling via segmentations. arXiv
preprint arXiv:1702.07463.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: Practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL).

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016.
Transition-based neural word segmentation. In ACL
(1).

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560.

A Hierarchical Dialogue Policy Learning

This section provides more algorithmic details for
Section 3.2. Again, assume a conversation of
length T :

œÑ = (s0, a0, r0, . . . , sT‚àí1, aT‚àí1, rT‚àí1, sT ) .

Suppose an HRL agent segments the trajectory
into a sequence of subgoals as g0, g1, . . . ‚àà G,
and the corresponding subgoal termination time
steps as tg0 , tg1 , . . . ‚àà N‚àó. Furthermore, denote
the intrinsic reward at time step t by rit. The top-
level and low-level Q-functions satisfy the follow-
ing Bellman equations:

Q‚àó(s, g) = E
[ tgj+1‚àí1‚àë

i=tgj

Œ≥i‚àítgj ret+1

+ Œ≥tgj+1‚àítgj ¬∑max
g‚Ä≤‚ààG

Q‚àó(stgj+1 , g
‚Ä≤)

|stgj = s, gj = g
]

and

Q‚àó(s, a, g) = E
[
rit + Œ≥ ¬∑max

a‚Ä≤‚ààA
Q‚àói (st+1, gj , a

‚Ä≤)

|st = s, gj = g, at = a,

t ‚àà [tgj , tgj+1)
]
.

Here Œ≥ ‚àà [0, 1] is a discount factor, and the ex-
pectations are taken over the randomness of the
reward and the state transition,

We use deep neural networks to approximate
the two Q-value functions as Q‚àó(s, a, g) ‚âà
Q(s, a, g; Œ∏i) and Q‚àó(s, g) ‚âà Q(s, g; Œ∏e). The pa-
rameters Œ∏i and Œ∏e are optimized to minimize the
following quadratic loss functions:

Li(Œ∏i) =
1

2|Di|
‚àë

(s,a,g,s‚Ä≤,ri)‚ààDi
[(yi ‚àíQ(s, a, g; Œ∏i))2]

yi =ri + Œ≥ ¬∑max
a‚Ä≤‚ààA

Qi(s
‚Ä≤, a‚Ä≤, g; Œ∏i)

(3)
and

Le(Œ∏e) =
1

2|De|
‚àë

(s,g,s‚Ä≤,re)‚ààDe
[(ye ‚àíQ(s, g; Œ∏e))2]

ye =re + Œ≥ ¬∑max
g‚Ä≤‚ààG

Q(s‚Ä≤, g‚Ä≤; Œ∏e) .

(4)
Here, De, Di are the replay buffers storing dia-
logue experience for training top-level and low-
level policies.

Optimization of parameters Œ∏i and Œ∏e can be
done by stochastic gradient descent on the two loss
functions in Equations (3) and (4). The gradients
of the two loss functions w.r.t their parameters are

‚àáŒ∏iLi =
1

|Di|
‚àë

(s,a,g,s‚Ä≤,ri)‚ààDi

[
‚àáŒ∏iQ(s, a, g; Œ∏i)¬∑

(yi ‚àíQi(s, a, g; Œ∏i))
]

and

‚àáŒ∏eLe =
1

|De|
‚àë

(s,g,s‚Ä≤,re)‚ààDe

[
‚àáŒ∏eQ(s, g; Œ∏e)¬∑

(ye ‚àíQe(s, g; Œ∏e))
]
.

To avoid overfitting, we also addL2-regularization
to the objective functions above.



2309

B Sample Dialogue

Table 3: Sample dialogue by the m-HRL agent interact-
ing with real user: bolded slots are the joint constraints
between two subtasks. (agt: Agent, usr: User)

User Goal
reserve-hotel subtask:
{
‚Äúrequest slots‚Äù: { ‚Äúinform slots‚Äù: {
‚Äúhotel price‚Äù: ‚Äú?‚Äù ‚Äúhotel date checkin‚Äù:‚Äú2016-09-22‚Äù
‚Äúhotel date checkout‚Äù: ‚Äú?‚Äù ‚Äúhotel city‚Äù: ‚ÄúCuritiba‚Äù
‚Äúhotel name‚Äù: ‚Äú?‚Äù ‚Äúhotel numberofpeople‚Äù: ‚Äú4‚Äù
‚Äúhotel amenity wifi‚Äù: ‚Äú?‚Äù }
}
}

book-flight-ticket subtask:
{
‚Äúrequest slots‚Äù: { ‚Äúinform slots‚Äù: {
‚Äúprice‚Äù: ‚Äú?‚Äù ‚Äúor city‚Äù: ‚ÄúLima‚Äù,
‚Äúreturn time dep‚Äù: ‚Äú?‚Äù ‚Äúdst city‚Äù: ‚ÄúCuritiba‚Äù,
‚Äúreturn date dep‚Äù: ‚Äú?‚Äù ‚Äúnumberofpeople‚Äù: ‚Äú4‚Äù,
‚Äúdepart time dep‚Äù: ‚Äú?‚Äù ‚Äúdepart date dep‚Äù:‚Äú2016-09-22‚Äù
‚Äúseat‚Äù: ‚Äú?‚Äù }
}
}

m-HRL agent
agt: Hi, may I know your departure city and destination city?
usr: I depart from Lima to Curitiba.
agt: What is your departure date?
usr: I am leaving on 2016-09-22.
agt: There is one flight available at 8:00:00AM.
usr: What date is available for return flight?
agt: There is one return flight on 2016-09-27.
usr: What is the time for the return flight?
agt: It is 00:00:00AM.
usr: What seat is available?
agt: Only Economy seat now.
usr: We need 4 tickets for the flight.
agt: Total is 1201 dollars for flight.

inform(subtask complete): Subgoal reached!
usr: What hotel is available there?
agt: There is one hotel called Starlight.
usr: Is the wifi available for the hotel?
agt: Yes, it is free.
usr: What‚Äôs the price for the hotel?
agt: It is 1541 dollars for hotel.
usr: Could you please book the trip for me?
agt: Okay. Trip tickets are issued.
usr: Thanks!
agt: Thanks!


