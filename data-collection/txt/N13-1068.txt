










































Estimating effect size across datasets


Proceedings of NAACL-HLT 2013, pages 607–611,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Estimating effect size across datasets

Anders Søgaard
Center for Language Technology

University of Copenhagen
soegaard@hum.ku.dk

Abstract

Most NLP tools are applied to text that is dif-
ferent from the kind of text they were eval-
uated on. Common evaluation practice pre-
scribes significance testing across data points
in available test data, but typically we only
have a single test sample. This short paper
argues that in order to assess the robustness
of NLP tools we need to evaluate them on
diverse samples, and we consider the prob-
lem of finding the most appropriate way to es-
timate the true effect size across datasets of
our systems over their baselines. We apply
meta-analysis and show experimentally – by
comparing estimated error reduction over ob-
served error reduction on held-out datasets –
that this method is significantly more predic-
tive of success than the usual practice of using
macro- or micro-averages. Finally, we present
a new parametric meta-analysis based on non-
standard assumptions that seems superior to
standard parametric meta-analysis.

1 Introduction

NLP tools and online services such as the Stanford
Parser or Google Translate are used for a wide va-
riety of purposes and therefore also on very differ-
ent kinds of data. Some use the Stanford Parser
to parse literature (van Cranenburgh, 2012), while
others use it for processing social media content
(Brown, 2011). The parser, however, was not neces-
sarily evaluated on literature or social media content
during development. Still, users typically expect
reasonable performance on any natural language in-
put. This paper asks what we as developers can do

to estimate the effect of a change to our system – not
on the labeled test data that happens to be available
to us, but on future, still unseen datasets provided by
our end users.

The usual practice in NLP is to evaluate a sys-
tem on a small sample of held-out labeled data.
The observed effect size on this sample is then val-
idated by significance testing across data points,
testing whether the observed difference in perfor-
mance means is likely to be due to mere chance.
The preferred significance test is probably the non-
parametric paired bootstrap (Efron and Tibshirani,
1993; Berg-Kirkpatrick et al., 2012), but many re-
searchers also resort to Student’s t-test for depen-
dent means relying on the assumption that their met-
ric scores are normally distributed.

Such significance tests tell us nothing about how
likely our change to our system is to lead to improve-
ments on new datasets. The significance tests all rely
on the assumption that our datapoints are sampled
i.i.d. at random. The significance tests only tell us
how likely it is that the observed difference in per-
formance means would change if we sampled a big-
ger test sample the same way we sampled the one
we have available to us right now.

In standard machine learning papers a similar sit-
uation arises. If we are developing a new percep-
tron learning algorithm, for example, we are inter-
ested in how likely the new learning algorithm is to
perform better than other perceptron learning algo-
rithms across datasets, and we may for that reason
evaluate it on a large set of repository datasets.

Demsar (2006) presents motivation for using non-
parametric methods such as the Wilcoxon signed

607



rank test to estimate significance across datasets.
The t-test is based on means, and typically results
across datasets are not commensurable. The t-
test is also extremely sentitive to outliers. Notice
also that typically we do not have enough datasets
to do paired bootstrapping (van den Noortgate and
Onghena, 2005).

In this paper we will assume that the Wilcoxon
signed rank test provides a reasonable estimate of
the significance of an observed difference in perfor-
mance means across datasets, or of the significance
of observed error reductions, but note that this still
depends on the assumption that datasets are sampled
i.i.d. at random. More importantly, a non-parametric
test across data sets does not provide an actual esti-
mate of the effect size. Estimating effect size is im-
portant, e.g. when there is a trade-off between per-
formance gains and computational efficiency.

In evaluations across datasets in NLP we typically
use the macro-average as an estimate of effect size,
but in other fields such as psychology or medicine it
is more common to use a weighted mean obtained
using what is known as the fixed effects model or
the random effects model for meta-analysis.

The experiments reported on in this paper fo-
cus on estimating error reduction and show that
meta-analysis is generally superior to macro- and
micro-average in terms of predicting future error re-
ductions. Parametric meta-analysis, however, over-
parameterizes the distribution of error reductions,
leading to some instability. While meta-analysis is
generally superior to macro-average, it is sometimes
off by a large margin. We therefore introduce a new
parametric meta-analysis that seems better suited to
predicting error reductions. In our experiments test
set sizes are balanced, so micro-averages will be
near-identical to macro-averages.

2 Meta-analysis

Meta-analysis is the statistical analysis of the ef-
fect sizes of several studies and is very popular
in fields such as psychology or medicine. Meta-
analysis has not been applied very often to NLP.
In NLP most people work on applying new meth-
ods to old datasets, and meta-analysis is designed
to analyze series of studies applying old methods to
new datasets, e.g. running the same experiments on

new subjects. However, meta-analysis is applicable
to experiments with multiple datasets.

In psychology or medicine you often see stud-
ies running similar experiments on different sam-
ples with very different results. Meta-analysis stems
from the observation that if we want to estimate an
effect from a large set of studies, the average ef-
fect across all the studies will put too much weight
on results obtained on small datasets in which you
typically see more variance. The most popular ap-
proaches to meta-analysis are the fixed effects and
the random effects model. The fixed effects model is
applicable when you assume a true effect size (esti-
mated by the individual studies). If you cannot make
that assumption because the studies may differ in
various aspects, leading the within-study estimates
to be estimates of slightly different effect sizes, you
need to use the random effects model. Both ap-
proaches to meta-analysis are parametric and rely on
the effect sizes to be normally distributed.

2.1 Fixed effects model

In the fixed effects model we weight the effect sizes
T1, . . . , TM – or error reductions, in our case – by
the inverse of the variance vi in the study, i.e. wi =
1
vi

. The combined effect size T is then:

T̂ =
ΣMi≥1wiTi
∑M

i≥1 wi

The variance of the combined effect is now:

v =
1

∑M
i≥1 wi

and the 95% confidence interval is then T̂ ±
1.96

√
v.

2.2 Random effects model

In the random effects model we replace the variance
vi with the variance plus between-studies variance
τ2:

τ2 =

∑k
i≥1 wiT

2
i −

(
∑

k

i≥1 wiTi)
2

∑
k

i≥1 wi
− df

∑k
i≥1 wi −

∑
k

i≥1 w
2

i
∑

k

i≥1 wi

(1)

with df = N − 1, except all negative values are
replaced by 0.

608



⊤

comp rec sci talk other

sys others sport vehicles politics religion

(a) (b) (c) (d) (e) (f) (g) (h)

Figure 1: Hierarchical structure of 20 Newsgroups. (a) IBM, MAC, (b) GRAPHICS, MS-WINDOWS, X-WINDOWS,
(c) BASEBALL, HOCKEY, (d) AUTOS, MOTORCYCLES, (e) CRYPTOGRAPHY, ELECTRONICS, MEDICINE, SPACE,
(f) GUNS, MIDEAST, MISCELLANEOUS, (g) ATHEISM, CHRISTIANITY, MISCELLANEOUS, (h) FORSALE

macro-av fixed random gumbel
k = 5
err. -0.1656 -0.0350 -0.0428 -0.0400
p-value - < 0.001 < 0.001 < 0.001
k = 10
err. -0.1402 -0.0329 -0.0413 -0.0359
p-value - < 0.001 < 0.001 < 0.001
k = 15
err. -0.0809 -0.0799 -0.0804 -0.0704
p-value - < 0.001 < 0.001 < 0.001

Figure 2: Using macro-average and meta-analysis to pre-
dict error reductions on document classification datasets
based on k observations. The scores are averages across
20 experiments. The p-values were computed using
Wilcoxon signed rank tests.

The random effects model is obviously more con-
servative in its confidence intervals, and often we
will not be able to obtain significance across datasets
using a random effects model. If, for example, we
apply a fixed effects model to test whether Bernoulli
naive Bayes (NB) fairs better than a perceptron (P)
model on 25 randomly extracted cross-domain doc-
ument classification problem instances from the 20
Newsgroups dataset (see Sect. 4), the 95% confi-
dence interval is [3.9%, 5.2%]. The weighted mean
is 4.6% (macro-average 3.9%). Using a random
effects model on the same 25 datasets, the 95%
confidence interval becomes [−6.5%, 6.6%]. The
weighted mean estimate is also slighly different
from that of a fixed effects model. The first question
we ask is which of these models provides the best es-
timate of effect size as observed on future datasets?

2.3 The error reductions distribution

Both the fixed effects and the random effects model
assume that effect sizes are normally distributed. We

can apply Darling-Anderson tests to test whether er-
ror reductions in 20 Newsgroups are in fact normally
distributed. Even a small sample of ten 20 News-
groups datasets provides enough evidence to reject
the hypothesis that error reductions (of NB over
P) are normally distributed. The Darling-Anderson
tests consistently tell us that the chance that our sam-
ple distribtutions of error reductions are normally
distributed is below 1%. The over-paramaterization
means that the estimates we get are unstable. While
both models are superior to macro-average esti-
mates, they may provide ’far-off’ estimates.

Using Darling-Anderson tests we could also re-
ject the hypothesis that error reductions were lo-
gistically distributed, but we did not find evidence
for rejecting the hypothesis that error reductions are
Gumbel-distributed.1 Gumbel distributions are used
to model error distributions in the latent variable for-
mulation of multinomial logit regression. A para-
metric meta-analysis model based on the assumption
that error reductions are Gumbel distributed is an in-
teresting alternative to non-parametric meta-analysis
(Hedges and Olkin, 1984; van den Noortgate and
Onghena, 2005), since there seems to be little con-
sensus in the literature about the best way to ap-
proach non-parametric meta-analysis.

Gumbel distributions take the following form:

1

β
ez−e

−z

where z = x−α
β

with α the location, and β the
scale. We fit a Gumbel distribution to our weighted
error reductions (wiTi) and compute the combined

1Abidin et al. (2012) has shown that Darling-Anderson is
superior to other goodness-of-fit tests for Gumbel distributions.

609



macro-av fixed random gumbel
k = 5
err. 0.0531 0.0525 0.0526 0.0489
p-value - ∼ 0.98 ∼ 0.98 ∼ 0.79
k = 7
err. 0.0928 0.0852 0.0852 0.0858
p-value - < 0.001 < 0.001 < 0.001
k = 9
err. 0.0587 0.05743 0.05743 0.0532
p-value - ∼ 0.68 ∼ 0.68 ∼ 0.13

Figure 3: Using macro-average and meta-analysis to pre-
dict error reductions in cross-lingual dependency parsing.
See text for details.

effect

T̂ =
α + 0.57721β

1
M

∑M
i≥1 wi

where 0.57721 is the Euler-Mascheroni constant,
and the variance of the combined effect v = π

2

6 β
2.

3 Experiments in document classification
and dependency parsing

Our first experiment makes use of the 20 News-
groups document classification dataset.2 The top-
ics in 20 Newsgroups are hierarchically structured,
which enables us to extract a large set of binary
classification problems with considerable bias be-
tween source and target data (Chen et al., 2009;
Sun et al., 2011). See the hierarchy in Figure 1.
We extract 20 high-level binary classification prob-
lems by considering all pairs of top-level cate-
gories, e.g. COMPUTERS-RECREATIVE (comp-rec).
For each of these 20 problems, we have differ-
ent possible datasets, e.g. IBM-BASEBALL, MAC-
MOTORCYCLES, etc. A problem instance takes
training and test data from two different datasets be-
long to the same high-level problem. For exam-
ple a problem instance could be learning to dis-
tinguish articles about Macintosh and motorcycles
MAC-MOTORCYCLES (evaluated on the 20 News-
groups test section) using labeled data from IBM-
BASEBALL (the training section). In total we have
288 available problem instances in the 20 News-
groups dataset.

In our first experiment we are interested in pre-
dicting the error reductions of a naive Bayes learner

2http://people.csail.mit.edu/jrennie/20Newsgroups/

over a perceptron model. We use publicly available
implementations with default parameters.3 In each
experiment we randomly select k datasets and es-
timate the true effect size using macro-average, a
fixed effects model, a random effects model, and
a corrected random effects model. In order to es-
timate the within-study variance we take 50 paired
bootstrap samples of the system outputs. We evalu-
ate our estimates against the observed average effect
across 5 new randomly extracted datasets. For each
k we repeat the experiment 20 times and report aver-
age error. We vary k to see how many observations
are needed for our estimates to be reliable.

The results are presented in Figure 2. We note that
meta-analysis provides much better estimates than
macro-averages across the board. Our parametric
meta-analysis based on the assumption that error re-
ductions are Gumbel distributed performs best with
more observations.

Our second experiment repeats the same proce-
dure using available data from cross-lingual depen-
dency parsing. We use the submitted results by par-
ticipants in the CoNLL-X shared task (Buchholz and
Marsi, 2006) and try to predict the error reduction of
one system over another given k many observations.
Given that we only have 12 submissions per system
we use k ∈ {5, 7, 9} randomly extracted datasets
for observations and test on another five randomly
extracted datasets. While results (Figure 3) are only
statistically significant with k = 7, we see that meta-
analysis estimates effect size across data sets better
than macro-average in all cases.

4 Conclusions

We have argued that evaluation across datasets is
important for developing robust NLP tools, and
that meta-analysis can provide better estimates
of effect size across datasets than macro-average.
We also noted that parametric meta-analysis over-
parameterizes error reduction distributions and sug-
gested a new parametric method for estimating ef-
fect size across datasets.

Acknowledgements

Anders Søgaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.

3http://scikit-learn.org/stable/

610



References

Nahdiya Abidin, Mohd Adam, and Habshah Midi. 2012.
The goodness-of-fit test for Gumbel distribution: a
comparative study. MATEMATIKA, 28(1):35–48.

Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In EMNLP.

Gregory Brown. 2011. An error analysis of relation ex-
traction in social media documents. In ACL.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
CoNLL.

Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009. Extracting discriminative concepts for domain
adaptation in text mining. In KDD.

Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1–30.

Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap. Chapman & Hall, Boca Raton,
FL.

Larry Hedges and Ingram Olkin. 1984. Nonparametric
estimators of effect size in meta-analysis. Psychologi-
cal Bulletin, 96:573–580.

Qian Sun, Rita Chattopadhyay, Sethuraman Pan-
chanathan, and Jieping Ye. 2011. Two-stage weight-
ing framework for multi-source domain adaptation. In
NIPS.

Andreas van Cranenburgh. 2012. Literary author-
ship attribution with phrase-structure fragments. In
Workshop on Computational Linguistics for Litera-
ture, NAACL.

Wim van den Noortgate and Patrick Onghena. 2005.
Parametric and nonparametric bootstrap methods for
meta-analysis. Behavior Research Methods, 37:11–
22.

611


