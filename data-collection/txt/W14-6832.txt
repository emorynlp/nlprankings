



















































Chinese Spell Checking Based on Noisy Channel Model


Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 202–209,
Wuhan, China, 20-21 October 2014

Chinese Spell Checking Based on Noisy Channel Model

Hsun-wen Chiu

chiuhsunwen@gmail.com

Jian-cheng Wu
Department of Institute of Information Systems and Applications

National Tsing Hua University
wujc86@gmail.com

Jason S. Chang

jason.jschang@gmail.com

Abstract

Chinese spell checking is an important
component of many NLP applications, in-
cluding word processors, search engines,
and automatic essay rating. Compared
to English, Chinese has no word bound-
aries and there are various Chinese in-
put methods that cause different kinds of
typos, so it is more difficult to develop
spell checkers for Chinese. In this paper,
we introduce a novel method for correct-
ing Chinese typographical errors based on
sound or shape similarity. In our approach,
similar characters are automatically gener-
ated using Web corpora, and potential ty-
pos in a given sentence are then corrected
using a channel model and a character-
based language model in the noisy channel
model. In the training phase, we estimate
the channel probabilities for each charac-
ter based on ngrams in Web corpus. At
run-time, the system generates correction
candidates for each character in the given
sentence and selects the appropriate cor-
rection using the channel model and the
language model.

1 Introduction

Spell checking is a necessary task for text process-
ing of every written language, which involves au-
tomatically detecting and correcting typographical
errors. However, compared to spell checkers for
alphabetical languages (e.g., English or French),
Chinese spell checkers are more difficult to de-
velop because there are no word boundaries in
Chinese writing system and errors may be caused
by various Chinese input methods. In this the-
sis, we define typos as Chinese characters that are
misused due to sound or shape similarity. Liu et
al. (2011) show that people tend to unintention-
ally generate typos due to sound similarity (e.g.,

*索定 (suo ding) instead of 鎖定 (suo ding)) or
shape similarity (e.g., *銷 定 (xiao ding) instead
of鎖定 (suo ding)). On the other hand, some ty-
pos found on the Web (e.g., forums or blogs) are
used deliberately for the purpose of speed typing
or just for fun. Therefore, spell checking is an im-
portant component for many applications, includ-
ing computer-aided writing, search engines, and
social media text normalization.

Very little work has been done on the task of
Chinese spell checking. The methods proposed
in the literature can be classified into two types:
rule-based methods and statistical methods. Rule-
based methods use knowledge resources, for ex-
ample, dictionaries, confusion sets, and segmenta-
tion systems. Simple rule-based methods, how-
ever, have their limitations. The following sen-
tence is a snippet collected from students’ written
essays which is correct .

為什麼你要如此地用功呢？如果我不
用功，那以後我將趕不上自己所定的
目標。(wei she me ni yao ru ci di yong
gong ne ？ru guo wo bu yong gong ，
na yi hou wo jiang gan bu shang zi ji suo
ding de mu biao。)

Unfortunately, based on simple rules the two char-
acters 所 (suo) and 定 (ding) are likely to be re-
garded as typos of the dictionary word鎖定 (suo
ding) with identical pronunciation.

The data-driven, statistical spell checking ap-
proach appears to be more robust and perform bet-
ter. Statistical methods typically use a large corpus
to create a language model to validate the correc-
tion hypotheses. Intuitively, by using自己所定的
目標 (zi ji suo ding de mu biao), the three charac-
ters所定的 (suo ding de) are a trigram with high
probability in a monolingual corpus, we may de-
termine the所定 (suo ding) is not a typo after all.
Table 1 shows the frequency and probability of所
定的 (suo ding de) and鎖定的 (suo ding de).

202



Trigrams Freq. LM prob.(log)
所定的 (suo ding de) 5 -0.70
鎖定的 (suo ding de) 2 -1.49

Table 1: Example trigrams with corresponding
frequency and probability.

In this thesis, we propose a model using sta-
tistical approaches and model generates the most
appropriate corrections in a given sentence. In
the training phase, we automatically generate the
channel model (confusion set). We use a Chinese
spell checker to correct instances in the training
data and estimate the channel probability of a typo
condition on a correct character , then re-estimate
the probability, and iterate until convergence.

At run-time, the checker corrects typos using a
noisy channel model. Consider the following sen-
tence.

為什麼你要如此地用功呢？如果我
不用功，那以後我將趕不上自己鎖鎖鎖
定的目標。(wei she me ni yao ru ci di
yong gong ne？ru guo wo bu yong gong
， na yi hou wo jiang gan bu shang zi ji
suo ding de mu biao。)

The checker generates correction candidates by
the replacements of each character and confusable
characters with channel probabilities in a beam
search algorithm, then calculates the probability of
correction hypotheses according to the language
model and the channel model. Three correction
candidates are shown in Table 2. Finally, the
checker returns the correction with the highest
score, e.g., the follow sentence:

為什麼你要如此地用功呢？如果我
不用功，那以後我將趕不上自己所所所
定的目標。(wei she me ni yao ru ci di
yong gong ne？ru guo wo bu yong gong
， na yi hou wo jiang gan bu shang zi ji
suo ding de mu biao。)

The rest of the paper is organized as follows.
We describe the proposed model for automatically
correcting the spelling typos in section 2. Sec-
tion 3 presents the experimental data. We con-
clude in Section 4.

2 Method

Using fixed rule to correct typos in a given Chinese
sentence (e.g.,自己鎖定的目標 (zi ji suo ding de

Hypotheses
為什麼你要如此地用功呢？如果我不用功，
那以後我將趕不上自己所所所定的目標。

為什麼你要如此地用功呢？如果我不用功，
那以後我將趕不上自己瑣瑣瑣定的目標。

為什麼你要如此地用功呢？如果我不用功，
那以後我將趕不上自己鎖鎖鎖定的目標。

Table 2: The three correction candidates of the
given sentence.

mu biao)) does not work very well. Previous work
typically corrects typos based on a set of detection
rules. Unfortunately, the detection rules depend on
a lot of resources, and can be at times unreliable.
Typo positions usually are detected using heuristic
rules based on Chinese dictionary, word segmen-
tation and the frequency of the ngram. However,
Chinese dictionary, and word segmentation have
their limitations. For example, the segmentation
result of the sentence ”自己鎖鎖鎖定的目標 ” is ”自
己/鎖鎖鎖定/的/目標 ”, the two characters 鎖 and 定
may or may not be considered as a word, depend
on the segmentation system. To avoid the limita-
tions of rule-based method, a promising approach
for Chinese spell checking is to train a noisy chan-
nel model based on unannotated data, which con-
taining many information.

In the rest of this section, we describe our
solution to the problem of Chinese spell check-
ing. We describe the process of training the
channel model in Section 2.1. More specifi-
cally, we describe the method for limiting con-
fusable characters in Section 2.1.1, and the use
of ngrams in Section 2.1.2. We will also de-
scribe an Expectation-Maximization (EM) algo-
rithms for estimating channel probabilities in Sec-
tion 2.1.3. This algorithm relies on a set of con-
fusable characters and ngrams. Finally in Sec-
tion 2.2, we describe how to correct typos using
the trained noisy channel model at run-time by
combining channel model and language model.

2.1 Training Channel Model

We attempt to learn to develop a channel model
from the ngrams of Web corpus for correcting Chi-
nese spell typos.

203



Type Sound Shape
Full 所瑣索梭娑嗦縮 瑣銷鋇鏜鐺鑽貝

唆蓑簑數碩勺鑠 賞員賄煩鈔貼敗
說朔爍帥率妁鎗 狽盼賸賤賊損貽
鎰鎳鎢鎬鎮鎊鎚 貞負頁賽贊圓貧
莎蟀鎔鎘 財則

Limited 索瑣鎖所 賸鏜鎖

Table 3: The full confusion set and the limited
confusion set of鎖.

2.1.1 Limiting Confusable Characters

In the first stage of training the channel model, we
limit the confusable characters based on the sound
and shape similar characters, which containing un-
likely confusable characters (as the full confusion
set). For example, the full confusion set of 鎖
(suo) is shown in Table 3. Liu et al. (2011) ana-
lyzed erroneous Chinese character and found that
more than 70% of typos were related to the phono-
logically similar characters, about 50% are visu-
ally similar, and almost 30% are both phonologi-
cally and visually similar. The goal of this method
is to reduce the sizes of the confusion sets and im-
prove the accuracy.

The input to this stage is a set of confusable
characters. These confusable characters consti-
tutes the full confusion set. We generate potential
confusable characters by reducing some unlikely
confusable relations and expanding the confusable
characters slightly.

The output of this stage is confusion sets that
can be used to correct ngrams for training channel
model. Limited confusion set of 鎖 (suo), auto-
matically generated from the full confusion set is
shown in Table 3. We can see that the limited con-
fusion set minimize the confusable characters and
select more likely characters. The limited confu-
sion set is used to accurately correct ngrams and
reduce the computational complexity.

Our method for limiting confusable characters
can generate many characters, potentially includ-
ing a significant number of characters that are not
useful in correcting typos. We also remove some
loosely similarly relations and expand the confus-
able characters slightly. For example, we remove
all relations based on non-identical phonologically
similarity. After that, we add the similarly sound-
ing characters based on nasal consonant in Chi-
nese phonetics (e.g., ”ㄣ , ㄥ” (en, eng) and ”ㄢ
, ㄤ” (an, ang)), and retroflex consonant (e.g.,

”ㄙ , ㄕ” (shi, si) and ”彳 , ㄘ” (chi, chi)). We
also modify the shape similarity by comparing the
characters in Cangjie codes (倉頡碼) to filter out
confusable characters with low similarity. We re-
tain character pairs differing from each other by
at most one symbol in Cangjie codes that tend to
be highly similar in shape. For example, the code
of 徵 (zheng) and 微 (wei) are highly similar in
shape, and their corresponding codes ”竹人山土
大” and ”竹人山山大”, differ only in one place.

Note that we do not attempt to estimate the
channel probabilities of typos of a character at this
point. In contrast, we only use sound or shape
similarity to limit confusable characters, leading
to more effective confusion set as the basis for sub-
sequent probability estimation.

2.1.2 Retrieving Ngrams
In the second stage of the training phase, we re-
trieve ngrams (e.g.,所定目標 (suo ding mu biao))
possibly containing a typo characters (e.g., 所
(suo)) that can be corrected using the confusable
characters (e.g., 所 (suo), 鎖 (suo), or 索 (suo)).
Because estimating channel probabilities need a
parallel corpus with typos annotated, we use an
existing Chinese spell checker CSC to correct ty-
pos in the ngrams. We use ngrams generated based
on collocates of high frequency words containing
the confusable character. The procedure for re-
trieving and correcting ngrams consist of a number
of steps, namely, generating collocates for words
containing a specific character, filtering these col-
locates by frequency, producing the ngrams for the
remaining collocates, and correcting these ngrams
using CSC. Each step is described below in detail.

For this stage of the learning process, we use a
collection of (Word, Collocate) pairs (e.g., (目標,
鎖定) ((mu biao, suo ding)), (版面, 鎖定) ((ban
mian, suo ding))). We generate the word from the
corpus using word frequency and find correspond-
ing collocates using Dice coefficient, which is a
statistic association value used for comparing the
relation of words and collocates. The collocates of
each word are sorted according to the Dice coeffi-
cient. We retain at most K collocates per word to
reduce the computational cost. We compute Dice
coefficient using the following equation:

Dice(w, c) =
2 · freq(w) · freq(c)
freq(w) + freq(c)

(1)

where freq(w) is the frequency of the word, and
freq(c) is the frequency of the collocate. Take 鎖

204



Words Collocates Dice
鎖定 版面 .025

單擊 .021
防偷 .004
目標 .004
移動 .004
已經 .002
敬請 .001
解除 .001

Words Collocates Dice
封鎖 衝出 .019

長城 .017
突破 .015
嚴密 .007
網絡 .002
大陸 .001

Table 4: Two sample collocates of鎖定 and封鎖.

Typos Texts Count
所 中所定目標 86

依所定目標 83
達到所定目標 44
我們所定的目標 42

索 索定海珠收 66
索定起息日 93
索定高清 40

Table 5: Sample texts of typo所 and索 of鎖 from
the corpus.

(suo) for instance, the words (e.g.,鎖定 (suo ding)
and封鎖 (feng suo)) and their corresponding col-
locates of words are shown in Table 4. The word
鎖定 (suo ding) has the highest Dice coefficient of
0.025 with the collocate 版面 (ban mian), while
封鎖 (feng suo)) has the highest Dice coefficient
of 0.019 with the collocate衝出 (chong chu).

For each (Word, Collocate) pair, we generate all
possible potential ngrams N containing Word and
Collocate. This stage of the learning process op-
erates over a corpus of ngram words. The sample
texts of the typos (所,索, and瑣) of鎖 found in a

Words Collocates Characters Instances
鎖定 目標 所 目標所定
封鎖 突破 索 突破封索
深鎖 眉頭 瑣 眉頭深瑣

Table 6: A sample of instances containing charac-
ter鎖 and potentially confusable characters.

corpus is shown in Table 5. We find the ngrams in
the corpus with identical collocates and Word con-
taining confusable characters (e.g., (所定,目標)).
Sample instances of character 鎖 is shown in Ta-
ble 6. In this sample, we can find that鎖 may be
misused as confusable characters (e.g.,所,索,瑣)
in the corpus with such information in the ngrams,
we can generate typo pairs (e.g., [所,鎖], [索,鎖],
[瑣, 鎖]). Finally, we correct the typos in these
ngrams by using existing Chinese spell checker (In
Section 2.1.3). With the typos and corrections, we
can estimate the channel probabilities.

2.1.3 Correcting Ngrams and Training
Channel Model

In the third and final stage of training, we correct
the ngrams and train the channel model for sup-
porting correction candidates. Figure ?? shows the
algorithm for correcting ngrams using a Chinese
spell checker and estimating the channel proba-
bilities related to typo pairs. The procedure is
repeated for all ngrams obtained in the previous
stage until the channel probabilities converge.

We are given a set of ngrams as training data
(described in Section 2.1.2). Recall that our goal
is to estimate the channel model for each charac-
ter, in the form of [original, correction, log chan-
nel probability] (e.g., [所,鎖, -4.284] and [索,鎖,
-5.264]). In order to generate a parallel corpus, we
need to provide representative ngrams to the train-
ing algorithm. The training set is created by re-
trieving the ngrams from Words of each character
and the corresponding Collocates in the corpus.

We apply a previously developed Chinese spell
checker(CSC) to correct ngrams. In this checker,
we adopt the confusion set limited in Stage (1) to
reduce the unlikely confusable characters and im-
prove the accuracy for generating typo pairs. We
combine the global error probability (GP) and lo-
cal error probability (LP) to reliably estimate the
channel probabilities (CP) using following equa-
tion:

CP (O,C) = W GL ·GP +(1−W GL) ·LP (O,C)
(2)

where O is original character, C is corrected char-
acter, and WGL is a weight for probability. The
global error probability is a prior probability cal-
culated from a development data set, which can
instead the detection and avoid data sparse. The
global error probability calculated by the follow-

205



Ngrams Corrections Typo Pairs
目標所定 目標所定 [目,目], [標,標]

[所,所], [定,定]
突破封索 突破封鎖 [突,突], [破,破]

[封,封], [索,鎖]
眉頭深瑣 眉頭深鎖 [眉,眉], [頭,頭]

[深,深], [瑣,鎖]

Table 7: A sample of the typo pairs for鎖.

ing equation.

GP (Devedata) =

{
count(nochange)

count(char)
count(typos)
count(char)

}
(3)

where count(nochange) is the count of corrected
characters, count(typos) is the count of typos, and
count(char) is the count of characters. The Deve-
data is the development data.

We use the Expectation-maximization algo-
rithm to estimate the local error probabilities re-
lated to the confusion set. We initialize the confu-
sion set with uniformed probability in the E-step
and re-estimate the probability of each character in
M-step until the local error probability converge.
For each of the potentially confused ngram (e.g.,
所定目標 (suo ding mu biao), we attempt to find
typos and corrections using CSC (Step (1)) and
produce the typo pairs (Step (2)). The typo pairs
are in the form of [Original, Correction]. The
frequency is the count of how many times of the
ngram occurs in the corpus. We estimate the local
error probability based on nochange pair (e.g., [所,
所] ([suo, suo])), and correction pair (e.g., [所,鎖]
([suo, suo])). In Table 7, we show a sample of the
typo pairs in the ngrams of the character鎖 (suo).

Then we calculate the global error probability
using the development data (Step (3)). In Step (4),
the typo pairs are sorted according to the Original.
For each [Original, Correction] pair, we calculate
the local error probability of the Original condi-
tioned on Correction (Step (5a)). The probability
is calculated as follows:

LP (CO, O) =
count(O,CO)

count(O)
(4)

As shown in Table 8, the total count of所 (suo) is
6799532 + 529 + 235 = 6800296, the count of (索,
所) is 235, and the LocalErrorProbability(索—所)
is calculated as follows:

Original Correction Frequency
所 所 6,799,532
所 匠 529
所 索 235
Total Frequency 6,800,296

Table 8: Sample of the typo pairs with frequency.

Original Correction Freq. LPlog
所 所 6799532 -0.0001
所 匠 529 -9.4614
所 索 235 -10.2728
所 瑣 1 -15.7324
所 鎖 1 -15.7324

Table 9: The result of the local error probability
with smoothing.

LocalErrorProbability(索—所) = Count(索,
所)) / Count(所) = 235/6800296 =0.0000346

However, we can not estimate that 所 (suo) as
a typo of 瑣 (suo), if CSC does not find [所, 瑣]
([suo, suo]). In that case, we use smoothing al-
gorithm to solve this problem. If a confusable
character does not has a certain typo pair, we use
add-one smoothing algorithm to deal with the un-
seen problem. For example, confusable characters
(e.g.,瑣,鎖) of所 (suo) are not found in the cor-
pus, so we add count one for them. Table 9 shows
a confusion set of所 (suo) and the corresponding
smoothed local error probability.

We combine the global error probability and
the local error probability to estimate the chan-
nel probabilities in Step (5b), and save the Orig-
inal, Correction, and their channel probability in
the channel model in Step (5c). Steps (1) through
(5) are repeated to re-estimate the local error prob-
ability until the probabilities converge. The output
of this stage of training is a channel model with
reliable probabilities, automatically estimated us-
ing the confusable characters and ngrams based on
collocates. A samples of the channel model for所
(suo) is shown in Table 10.

2.2 Run-time Typo Correction

Once the channel model is automatically trained
for each character, we store the model as a con-
fusion set. We then correct a given sentence us-
ing the procedure shown in Figure ?? with the
character-based language model and the channel
model.

For each character in the given sentence of n

206



Original Correction Freq. CPlog
所 所 6799532 -0.1416
所 匠 529 -2.2111
所 索 235 -4.4357
所 瑣 1 -10.4947
所 鎖 1 -10.4947

Table 10: A sample of the channel model for 所
(suo).

Originals Corrections Ngrams Score
自 自 () 0.0
己 己 (,自) -2.6049
鎖 所 (自,己) -2.6756
定 定 (己,所) -5.1145
的 的 (所,定) -6.3698
目 目 (定,的) -5.1627
標 標 (的,目) -5.7875

(目,標) -10.2282

Table 11: A sample of the hypotheses.

characters (e.g., 自己鎖鎖鎖定的目標 (zi ji suo ding
de mu biao)), we correct typos as follows. In Step
(1), the system initializes n stacks for the channel
model, [Character, Ngram, Score]. In Step (2), the
system replaces each character with the confus-
able characters (e.g., 所,索,瑣,鎖 (suo, suo, suo,
suo)) in the channel model as the correction can-
didates. For each confusable characters, we cre-
ate new hypotheses with a score, character ngram
state, character, and correction candidates. In or-
der to reduce computational complexity, we use
beam search algorithm to replace each and calcu-
late the score of sentences. The score in a hypoth-
esis is calculated based on the channel model and
the language model as follows.

S(hypothesis) = log(LPW LC · CP (1−W LC)) (5)
= W LC · log(LP ) + (1 − W LC) · log(CP ) (6)

where LP is language model probability, CP is
channel probability, and WLC is a weight param-
eter in channel model and language model. A
sample hypothesis is shown in Table 11. In Step
(3), the new hypothesis are stored in the stack and
combined with the existing hypothesis in Step (4).
If the stack has too many hypotheses, we prune the
stack down to a fixed size in Step (5).

Finally in Step (6), we compare the score of all
the hypotheses in the last stack, and output the cor-
rection candidate with the highest score as output.

Sentences
Given 遇到逆竟時，我們必須勇於面對。
Corrected 遇到逆境時，我們必須勇於面對。

Given 人生難免會碰到的一些錯折。
Corrected 人生難免會碰到的一些挫折。

Table 12: A sample of the given sentences and cor-
rections.

When there is no correction candidates with the
highest score (e.g., score(自己所定的目標) = -
10.2282), we output the given sentence. Table 12
shows three input sentences and the corresponding
corrected sentences output. For example,竟 (jing)
is corrected as 境 (jing), because 境 (jing) is the
most appropriate for the context of 遇到逆 * 時
(yu dao ni jing shi).

3 Experiment Setting

Our systems were designed to provide wide cov-
erage spell checking for Chinese texts. As such,
we trained our systems using the confusion set, a
compiled corpus, Web-scale ngrams, and an ex-
isting Chinese spell checker. These resources are
used for different purposes: the confusion sets
provide the correction candidates; the compiled
corpus provide the training data for the language
model; Web-scale ngrams and the existing Chi-
nese spell checker are used for training the chan-
nel model. We evaluate our systems on the sen-
tence level. In this section, we present the details
of data sources used in training(Section 3.1 to Sec-
tion 3.4).

3.1 Confusion Set
The confusion sets we used are the same as in Liu
et al. (2011) and provided for SIGHAN 7 Bake-
off 2013: Chinese Spelling Check Shared Task.
The confusion sets represent sound similarity and
shape similarity between a typo and potential cor-
rections.

There four categories of phonological similarity
between two characters: identical sound and tone
(II), identical sound but different tone (ID), sim-
ilar sound and identical tone (SI), similar sound
and different tone (SD), and identical radical and
number of strokes (RS). A sample of sound-related
confusion sets from SIGHAN 7 Bake-off 2013.
In this sample, the confusion sets of 已 (yi), 勇
(yong), and 胡 (hu) contain a lot of unlikely con-

207



N-gram Types Google Chinese 5-gram
Unigram 1,616,150
Bigram 281,107,315
Trigram 1,024,642,142
Fourgram 1,348,990,533
Fivegram 1,256,043,325

Table 13: The information of n-grams in Google
Chinese 5-gram.

fusable characters. Examples of unlikely pairs in-
clude 已 (yi) and 肄 (yi) in ID, 勇 (yong) and 穩
(wen) in SI,胡 (hu) and馥 (fu) in SD. The shape-
related confusion sets of已 (yi),勇 (yong), and胡
(hu). The confusion sets also contain loosely sim-
ilarly relations, for instance,已 (yi) and圈 (quan)
are not very similar visually.

In our work, we expand the sets slightly and also
remove some unlikely confusable characters in or-
der to improve the performance. We modify the
confusion set using the pronunciation and Cangjie
codes (倉頡碼). The process is described in detail
in Section 2.1.1.

3.2 Google Chinese Web 5-gram
In 2010, Google published a Chinese Web 5-gram
dataset based on public webpages through Lin-
guistics Data Consortium (LDC).1 Chinese Web
5-gram consists of Chinese word n-grams and
their observed frequency counts generated from
approximately 883 billion word tokens of text in
publicly accessible Web pages. The Google Chi-
nese Web 5-gram contains 30 GB (gzip com-
pressed) of text files with n-grams ranging from
unigrams (single words) to fivegrams. In this
work, we used only the traditional Chinese 5-
grams. Table 13 and Table 14 show the informa-
tion of 5-grams in Google Chinese Web 5-gram
and traditional Chinese Web 5-gram. We use the
traditional Chinese Web 5-gram to retrieve ngrams
(at most ten Words) in the training phase for esti-
mate channel model probabilities. The advantage
of using the Web ngram is that unlike a compiled
corpus, it contains many typos.

3.3 Existing Chinese Spell Checker
We use an existing Chinese spell checker (CSC)
we previously developed in 2013 (Chiu et al.,
2013) with the training data described in (Wu et

1https://catalog.ldc.upenn.edu/
LDC2010T06

N-gram Types Traditional Chinese 5-gram
Unigram 527,694
Bigram 102,092,428
Trigram 237,599,483
Fourgram 201,500,549
Fivegram 126,959,922

Table 14: The information of n-grams in Tradi-
tional Chinese 5-gram.

al., 2013). This CSC is based on a novel method
for detecting and correcting Chinese typographical
typos. The approach involves word segmentation,
detection rules, and phrase-based machine transla-
tion. The error detection module detects typos by
segmenting words and checking word and phrase
frequency based on compiled and Web corpora.
The phonological or morphological typographical
typos found then are corrected by running a de-
coder based on the statistical machine translation
model. The language model is trained using the
word-based corpus using the SRILM (Stolcke et
al., 2011) toolkit. The translation model is trained
using the frequency of the word containing typos
and the corrected word. The results show that
the proposed system achieves high accuracy in er-
ror detecting and correcting. We use this Chinese
spell checker to train the channel model and as a
system to compared with the proposed method.

3.4 Sinica Corpus

”Academia Sinica Balanced Corpus of Modern
Chinese”, or ”Sinica Corpus”, is the first bal-
anced Chinese corpus with part-of-speech tags.
The size of the corpus we used is about 5 mil-
lion words. The corpus is segmented according
to the word segmentation standard proposed by
the ROC Computational Linguistic Society. Each
segmented word is manually tagged with a part
of speech. Texts in the corpus are collected from
different areas: Literature, Life, Society, Science,
Philosophy, and Art. Table 15 shows the informa-
tion about numbers of word, character, article, and
percentage by area. We use Sinica Corpus (ignor-
ing word segmentation) to train a character-based
n-gram language model running the SRILM (Stol-
cke et al., 2011) toolkit. The sizes of the ngrams
of the character-based language model is shown in
Table 16

208



Areas Word Token Character Article
Literature 777,050 1,169,801 1,385
Life 858,750 1,398,791 2,301
Society 1,610,997 2,711,720 3,246
Science 629,838 1,054,738 994
Philosophy 439,955 673,080 695
Art 474,340 781,415 518
Others 101,394 160,306 89

Total Count 4,892,324 7,949,851 9228

Table 15: The information of the word, character,
article, and percentage in the area of sinica corpus.

Ngram Types Ngram Count
Unigram 17,201
Bigram 741,739
Trigram 859,442
Fourgram 791,846
Fivegram 588,200

Table 16: The information of n-grams in
character-based language model.

4 Conclusions and Future Work

Many avenues exist for future research and im-
provement of our system. For example, confusion
sets can be automatically generated using Web-
based character n-grams to improve correction
performance. Part of speech tagging can be per-
formed to provide more information for the noisy
channel model. Named entities can be recognized
in order to avoid false alarms. A supervised statis-
tical classifier can be used to model channel proba-
bility more accurately. Additionally, an interesting
direction to explore is using Web corpus in addi-
tion to a compiled corpus for correcting typos. Yet
another direction of research would be to consider
errors related to a missing or redundant character,
or collect data from user to update channel proba-
bilities dynamically.

In summary, we have introduced a novel
method for Chinese spell checking. In our ap-
proach, the channel model is trained based the
sound and shape similarity using Web corpus, and
the potential typos in a given sentence is cor-
rected using a noisy channel model. In the train-
ing phase, we limit the confusable characters, re-
trieve the ngrams from the Web corpus, and cor-
rect ngrams and estimate the channel probability.
At run-time, our system generate the correction

candidates and calculate their probabilities using
the language model and channel model from a
given sentence. The results prove that the chan-
nel probability we estimate for the noisy channel
model are useful in Chinese spell checking.

References
[Chiu et al.2013] Hsun-wen Chiu, Jian-cheng Wu, and

Jason S. Chang. 2013. Chinese spelling checker
based on statistical machine translation. In Sixth In-
ternational Joint Conference on Natural Language
Processing, page 49.

[Liu et al.2011] C-L Liu, M-H Lai, K-W Tien, Y-H
Chuang, S-H Wu, and C-Y Lee. 2011. Visually
and phonologically similar characters in incorrect
chinese words: Analyses, identification, and appli-
cations. ACM Transactions on Asian Language In-
formation Processing (TALIP), 10(2):10.

[Stolcke et al.2011] Andreas Stolcke, Jing Zheng, Wen
Wang, and Victor Abrash. 2011. Srilm at sixteen:
Update and outlook. In Proceedings of IEEE Auto-
matic Speech Recognition and Understanding Work-
shop, page 5.

[Wu et al.2013] Shih-Hung Wu, Chao-Lin Wu, and
Lung-Hao Lee. 2013. Chinese spelling check evalu-
ation at sighan bake-off 2013. In Proceedings of the
7th SIGHAN Workshop on Chinese Language Pro-
cessing, pages 35–42.

209


