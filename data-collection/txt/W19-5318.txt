




































The AFRL WMT19 Systems: Old Favorites and New Tricks


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 203–208
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

203

The AFRLWMT19 Systems: Old Favorites and New Tricks

Jeremy Gwinnup, Grant Erdmann, Timothy Anderson
Air Force Research Laboratory

{jeremy.gwinnup.1, grant.erdmann, timothy.anderson.20}@us.af.mil

Abstract

This paper describes the Air Force Research
Laboratory (AFRL) machine translation sys-
tems and the improvements that were devel-
oped during theWMT19 evaluation campaign.
This year, we refine our approach to train-
ing popular neural machine translation toolk-
its, experiment with a new domain adaptation
technique and again measure improvements in
performance on the Russian–English language
pair.

1 Introduction

As part of the 2019 Conference onMachine Trans-
lation (Bojar et al., 2019) news-translation shared
task, the AFRL Human Language Technology
team participated in the Russian–English portion
of the competition. We build on our strategies from
last year (Gwinnup et al., 2018), adding additional
language ID based data processing and optimizing
subword segmentation strategies. For Russian–
English we again submitted an entry comprising
our best systems trained with Marian (Junczys-
Dowmunt et al., 2018), Sockeye (Hieber et al.,
2017) with Elastic Weight Consolidation (EWC)
(Thompson et al., 2019), OpenNMT (Klein et al.,
2018), and Moses (Koehn et al., 2007) combined
using the Jane system combination method (Fre-
itag et al., 2014).

2 Data and Preprocessing

2.1 Data Preparation
We used and preprocess data as outlined in Gwin-
nup et al. (2018). For all systems trained, we
applied either byte-pair encoding (BPE) (Sen-
nrich et al., 2016) or SentencePiece (Kudo and
Richardson, 2018) subword strategies to address
the vocabulary-size problem.
For this year, we also employed a language ID

filtering step for the BPE-based systems. Using

the pre-built language ID model developed by the
authors of fastText (Joulin et al., 2016a,b), we de-
veloped a utility that examined the source and tar-
get sentence pairs and discarded that pair if either
side fell below 0.81 probability of the desired lan-
guage. We applied this filtering to all provided par-
allel corpora, removing 33.7% of lines. This pro-
cess was particularly effective when used to filter
the Paracrawl corpus where 57.1% of lines were
removed. Pre and post-filtering line counts for var-
ious corpora are shown in Table 1.

Corpus Total Retained

CommonCrawl 723,256 655,069
newscommentary 290,866 264,089
Yandex 1,000,000 901,307
ParaCrawl 12,061,155 5,173,675
UN2016 11,365,709 9,871,406

Total Lines 25,440,968 16,865,546

Table 1: Training corpus total and retained lines after
fastText filtering

testset wmt18preproc wmt19filt

newstest2014 33.0 34.1
newstest2015 28.6 29.6
newstest2016 28.4 29.4
newstest2017 30.8 31.8
newstest2018 26.9 27.9

Table 2: Test set comparison for non-filtered WMT18
training corpus and filtered WMT19 training corpus
measured by SacreBLEU.

A comparison with the organizer-provided par-
allel training data used in our WMT18 system

1We chose this value arbitrarily; future work will explore
varying this threshold.



204

(which is largely the same as the provided paral-
lel data for WMT19 in the Russian–English lan-
guage pair) on baseline Marian transformer sys-
tems with identical training conditions show that
aggressive language ID based filtering yields an
approximate +1 BLEU point improvement as mea-
sured by SacreBLEU (Post, 2018). These results
are shown in Table 2.

2.2 Exploration of Byte-Pair Encoding
Merge Sizes

One of the problems faced when addressing the
closed-vocabulary problem is the granularity of the
subword units either produced by SentencePiece or
BPE. To that end, we examined varying the num-
ber of BPE merge operations in order to determine
an optimal setting tomaximize performance for the
Russian–English language pair.
For the OpenNMT-based systems, a vocabulary

size of 32k entries was employed during training
of a SentencePiece segmentation model2. This vo-
cabulary size was determined empirically from the
training data.
Alternatively, for the BPE-based systems, we

systematically examined varying sizes of BPE
merge operations and vocabulary sizes in 10k in-
crements from 30k to 80k. Results in Table 3 show
that 40kBPEmerge operations perform best across
all test sets decoded for this language pair. All
subsequent Marian experiments in this work uti-
lize this 40k BPE training corpus.

3 MT Systems

This year, we focused system-building efforts
on the Marian, Sockeye, OpenNMT, and Moses
toolkits, having explored a variety of parameters,
data, and conditions. While most of our exper-
imentation builds off of previous years’ efforts,
we did examine domain adaptation via continued
training, including Elastic Weight Consolidation
(EWC) (Thompson et al., 2019).

3.1 Marian

As with last year’s efforts, we train multiple
Marian (Junczys-Dowmunt et al., 2018) models
with both University of Edinburgh’s “bi-deep”
(Miceli Barone et al., 2017; Sennrich et al., 2017)
and Google’s transformer (Vaswani et al., 2017)

2SentencePiece was used in part to provide diversity be-
tween our OpenNMT and other systems trained with BPE
data.

architectures. Network hyperparameters are the
same as detailed in Gwinnup et al. (2018). We
again use newstest2014 as the validation set dur-
ing training.
Utilizing the best-performing BPE parameters

from Section 2.2, we first trained a baseline system
in each of the two network architectures, noting the
Transformer system’s better performance of +0.82
BLEU on average across decoded test sets. An ad-
ditional six distinct transformer models were then
independently3 trained for use in ensemble decod-
ing. We then ensemble decoded test sets with all
eight models.
Marian typically assigns each model used in

ensemble decoding a feature weight of 1.0; thus
each model contributes equally to the decoding
process. Borrowing from our Moses training ap-
proach, we utilize a multi-iteration decode and op-
timize feature weights using the “Expected Cor-
pus BLEU” (ECB)metric with the Drem optimizer
(Erdmann and Gwinnup, 2015). We experimented
using newstest2014 and newstest2017 as tun-
ing sets – 2017 did not help performance, but us-
ing 2014 did improve performance by up to +0.9
BLEU4 over the non-tuned ensemble.
Scores for all the above-mentioned systems are

shown in Table 4. The best-performing ensemble
(ensemble tune14) was used in system combina-
tion.

3.2 Sockeye
For our Sockeye (Hieber et al., 2017) systems, we
experimented with continued training (Luong and
Manning, 2015; Sennrich et al., 2015) – a means to
specialize a model in a new domain after a period
of training on a general domain. One downside
of utilizing continued training is the model adapts
“too-well” to the new domain at the expense of
performance in the original domain (Freitag and
Al-Onaizan, 2016). One method to mitigate this
performance drop is to prevent certain parameters
of the network from changing with Elastic Weight
Consolidation (EWC) (Kirkpatrick et al., 2017).
Thompson et al. (2019) conveniently provides an
implementation of this technique in Sockeye.
That work illustrated a use case where the orig-

inal domain is news articles, while the new do-
main is text of patent applications – a marked dif-

3Identical training data and starting parameters except for
random seed.

4This may be due to the choice of newstest2014 for vali-
dation during training.



205

System newstest2014 newstest2015 newstest2016 newstest2017 newstest2018

bpe30k 33.7 28.9 28.7 31.4 27.6
bpe40k 34.1 29.6 29.4 31.8 27.9
bpe50k 33.9 29.2 29.1 31.6 27.8
bpe60k 33.4 29.1 28.7 31.3 27.6
bpe70k 33.0 28.8 28.8 31.2 26.9
bpe80k 32.6 28.7 28.2 31.1 26.9

Table 3: Cased, detokenized BLEU for various test sets and BPE merge-value treatments. Best scores for each test
set are denoted with bold text.

System newstest2014 newstest2015 newstest2016 newstest2017 newstest2018

single bi-deep 32.7 29.0 28.7 31.3 27.0
single transformer 34.1 29.6 29.4 31.8 27.9
untuned ensemble 36.2 31.6 30.5 34.2 29.7
ensemble tune17 35.3 31.1 30.2 34.2 29.7
ensemble tune14 37.1 31.3 31.2 34.5 30.5

Table 4: Test set comparison for baseline bi-deep, transformer, untuned and tuned ensembles for various test sets
measured in cased, detokenized BLEU. Best scores for each test set are denoted with bold text.

ference in style and content. Here, we created a
news subdomain corpus from the newstest2014
through newstest2017 test sets. The intuition
is that more current events will be discussed in
these test sets than the remainder of the provided
training corpora, allowing better adaptation of new
events in the newest test sets (newstest2018 and
newstest2019.)
We first trained a baseline transformer system

using the best-performing BPE parameters from
Section 2.2, 512-dimension word embeddings, 6
layer encoder and decoder, 8 attention heads, la-
bel smoothing and transformer attention dropout
of 0.1. We then continue-train a model on the
adaptation set described above. We also followed
the Sockeye EWC training procedure, producing a
model more resilient to overfitting due to contin-
ued training. Results for these systems are shown
in Table 5.
We see that the baseline Sockeye transformer

model performs similarly to the baseline single-
model Marian transformer system shown in Table
4. The continued-training system (con’t train) sys-
tem predictably overfit on newstest2014 as ex-
pected, since that test set is a part of the adap-
tation set. Likewise, performance on the out-of-
domain newstest2018 also dropped as a result
of overfitting. The best-performing EWC system5

5EWC applied with weight-decay of 0.001 and learning-

actually improved performance on 2018 with less-
pronounced overfitting on 2014.

System newstest2014 newstest2018

baseline 33.4 27.6
con’t train 89.3 24.3
best EWC 48.5 29.5

Table 5: Sockeye system scores for newstest2014
(in-domain) and newstest2018 (out-of-domain) test
sets for various training conditions measured in Sacre-
BLEU.

For system combination outlined later in Section
4, we decoded test sets with an ensemble of the
four highest-scoring model checkpoints from the
best EWC training run.

3.3 OpenNMT-T

Our first Open-NMT system was trained using the
Transformer architecture with the default “Trans-
formerBig” settings as described in Vaswani et al.
(2017): 6 layers of 1024 units, 16 attention heads.
Dropout rates of 0.3 for layers and 0.1 for atten-
tion heads and relu’s. Training data for this sys-
tem utilized the training corpus from our WMT17
Russian–English system (Gwinnup et al., 2017)
consisting of provided parallel and backtranslated

rate of 0.00001



206

data. This data was then processed with a joint 32k
word vocabulary SentencePiece model.

3.4 OpenNMT-G

For our second OpenNMT system, we first trained
language-specific, 32k word vocabularies using
SentencePiece. WMTnews test data from all years
except 2014 and 2017 were used to train Senten-
cePiece. These data, with the addition of the lan-
guage ID filtered ParaCrawl corpus outlined in
Section 2.1, were used for training the system.
WMT news test data from 2014 was used for val-
idation. OpenNMT-tf was used to create the sys-
tem, using the stock “Transformer” model.

3.5 Moses

As in previous years, we trained a phrase-based
Moses (Koehn et al., 2007) system with the same
data as the Marian system outlined in Section 3.1
in order to provide diversity for system combina-
tion. This system employed a hierarchical reorder-
ingmodel (Galley andManning, 2008) and 5-gram
operation sequence model (Durrani et al., 2011).
The 5-gram English language model was trained
with KenLM on all permissable monolingual En-
glish news-crawl data. The BPE model used
was applied to both the parallel training data and
the language modeling corpus. System weights
were tuned with the Drem (Erdmann and Gwin-
nup, 2015) optimizer using the “Expected Corpus
BLEU” (ECB) metric.

4 System Combination

Jane system combination (Freitag et al., 2014) was
employed to combine outputs from the best sys-
tems from each approach outlined above. Indi-
vidual component system and final combination
scores are shown in Table 6 for cased, detokenized
BLEU and BEER 2.0 (Stanojević and Sima’an,
2014) .

5 Submission Systems

We submitted the final 5-system combination out-
lined in Section 4 and the four-checkpoint EWC
ensemble detailed in Section 3.2 to the Russian–
English portion of the WMT19 news task evalu-
ation. Selected newstest2019 automatic scores
from the WMT Evaluation Matrix6 are shown in
Table 7.

6http://matrix.statmt.org

System BLEU BEER

1. Marian 30.47 0.5995
2. Sockeye EWC 29.43 0.5968
3. OpenNMT-T 26.22 0.5737
4. OpenNMT-G 30.05 0.6017
5. Moses 27.33 0.5836

Syscomb-5 32.12 0.6072

Table 6: System combination and input system scores
measured in cased, detokenized BLEU and BEER on
the newstest2018 test set.

System BLEU BEER

afrl-syscomb19 37.2 0.627
afrl-ewc 34.3 0.613

Table 7: Final submission system scores measured in
cased BLEU and BEER on the newstest2019 test set.

6 Conclusion

We presented a series of improvements to our
Russian–English systems, including improved
preprocessing and domain adaptation. Clever
remixing of older techniques from the phrase-
based MT era enabled improvements in ensem-
bled neural decoding. Lastly, we performed sys-
tem combination to leverage benefits from these
new techniques and favorite approaches from pre-
vious years.

References
Ondřej Bojar, Christian Federmann, Mark Fishel,

Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, Christof Monz, Mathias Müller, and
Matt Post. 2019. Findings of the 2019 conference on
machine translation (wmt19). In Proceedings of the
Fourth Conference on Machine Translation, Volume
2: Shared Task Papers, Florence, Italy. Association
for Computational Linguistics.

Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proc. of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL ’11), pages 1045–1054, Portland, Oregon.

Grant Erdmann and Jeremy Gwinnup. 2015. Drem:
The AFRL submission to the WMT15 tuning task.

Opinions, interpretations, conclusions and recommen-
dations are those of the authors and are not necessarily en-
dorsed by the United States Government. Cleared for public
release on 12 June 2019. Originator reference number RH-
19-119921. Case number 88ABW-2019-2969.

http://matrix.statmt.org


207

In Proc. of the Tenth Workshop on Statistical Ma-
chine Translation, pages 422–427, Lisbon, Portugal.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast Do-
main Adaptation for Neural Machine Translation.
CoRR, abs/1612.06897.

Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open sourcemachine translation system
combination. In Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 29–32, Gothenburg, Sweden.

Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’08, pages 848–856.

Jeremy Gwinnup, Tim Anderson, Grant Erdmann, and
Katherine Young. 2018. The AFRL WMT18 sys-
tems: Ensembling, continuation and combination.
In Proceedings of the Third Conference on Machine
Translation: Shared Task Papers, pages 394–398.
Association for Computational Linguistics.

Jeremy Gwinnup, Timothy Anderson, Grant Erdmann,
Katherine Young, Michaeel Kazi, Elizabeth Salesky,
Brian Thompson, and Jonathan Taylor. 2017. The
AFRL-MITLL WMT17 systems: Old, new, bor-
rowed, BLEU. In Proceedings of the Second Con-
ference on Machine Translation, Volume 2: Shared
Task Papers, pages 303–309, Copenhagen, Den-
mark. Association for Computational Linguistics.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A toolkit for neural machine
translation. arXiv preprint arXiv:1712.05690.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
Matthijs Douze, Hérve Jégou, and Tomas Mikolov.
2016a. Fasttext.zip: Compressing text classification
models. arXiv preprint arXiv:1612.03651.

Armand Joulin, Edouard Grave, Piotr Bojanowski,
and Tomas Mikolov. 2016b. Bag of tricks
for efficient text classification. arXiv preprint
arXiv:1607.01759.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. In Proceedings
of ACL 2018, System Demonstrations, pages 116–
121, Melbourne, Australia. Association for Compu-
tational Linguistics.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2017. Overcoming catastrophic forgetting in neural

networks. Proceedings of the National Academy of
Sciences, 114(13):3521–3526.

Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent
Nguyen, Jean Senellart, and Alexander Rush. 2018.
OpenNMT: Neural machine translation toolkit. In
Proceedings of the 13th Conference of the Associa-
tion for Machine Translation in the Americas (Vol-
ume 1: Research Papers), pages 177–184. Associa-
tion for Machine Translation in the Americas.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions, ACL
’07, pages 177–180.

TakuKudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium.
Association for Computational Linguistics.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford Neural Machine Translation Sys-
tems for Spoken Language Domain. In Interna-
tional Workshop on Spoken Language Translation,
Da Nang, Vietnam.

Antonio Valerio Miceli Barone, Jindřich Helcl, Rico
Sennrich, Barry Haddow, and Alexandra Birch.
2017. Deep architectures for neural machine trans-
lation. In Proceedings of the Second Conference on
Machine Translation, pages 99–107. Association for
Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191. Association for Computational Linguistics.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The University of Edinburgh’s neural MT
systems for WMT17. In Proceedings of the Sec-
ond Conference on Machine Translation, Volume 2:
Shared Task Papers, pages 389–399, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Improving neural machine translation
models with monolingual data. arXiv preprint
arXiv:1511.06709.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual

http://aclweb.org/anthology/W18-6411
http://aclweb.org/anthology/W18-6411
http://www.aclweb.org/anthology/W17-4728
http://www.aclweb.org/anthology/W17-4728
http://www.aclweb.org/anthology/W17-4728
http://www.aclweb.org/anthology/P18-4020
http://www.aclweb.org/anthology/P18-4020
https://doi.org/10.1073/pnas.1611835114
https://doi.org/10.1073/pnas.1611835114
http://aclweb.org/anthology/W18-1817
https://www.aclweb.org/anthology/D18-2012
https://www.aclweb.org/anthology/D18-2012
https://www.aclweb.org/anthology/D18-2012
http://aclweb.org/anthology/W17-4710
http://aclweb.org/anthology/W17-4710
http://aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/W18-6319
http://www.aclweb.org/anthology/W17-4739
http://www.aclweb.org/anthology/W17-4739
https://doi.org/10.18653/v1/P16-1162
https://doi.org/10.18653/v1/P16-1162


208

Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1715–1725.
Association for Computational Linguistics.

Miloš Stanojević and Khalil Sima’an. 2014. Fit-
ting sentence level translation evaluation with many
dense features. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 202–206, Doha, Qatar.
Association for Computational Linguistics.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah,
Kevin Duh, and Philipp Koehn. 2019. Overcoming
catastrophic forgetting during domain adaptation of
neural machine translation. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), volume 2, page to appear.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

http://www.aclweb.org/anthology/D14-1025
http://www.aclweb.org/anthology/D14-1025
http://www.aclweb.org/anthology/D14-1025

