










































uOttawa: System description for SemEval 2013 Task 2 Sentiment Analysis in Twitter


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 380–383, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

uOttawa: System description for SemEval 2013 Task 2 Sentiment    

Analysis in Twitter 

 

 

Hamid Poursepanj, Josh Weissbock, and Diana Inkpen 

School of Electrical Engineering and Computer Science  

University of Ottawa 

Ottawa, K1N6N5, Canada 

{hpour099, jweis035, Diana.Inkpen}@uottawa.ca 

 

  

 

Abstract 

We present two systems developed at the Uni-

versity of Ottawa for the SemEval 2013 Task 2. 

The first system (for Task A) classifies the po-

larity / sentiment orientation of one target word 

in a Twitter message. The second system (for 

Task B) classifies the polarity of whole Twitter 

messages. Our two systems are very simple, 

based on supervised classifiers with bag-of-

words feature representation, enriched with in-

formation from several sources. We present a 

few additional results, besides results of the 

submitted runs. 

1 Introduction 

The Semeval 2013 Task 2 focused on classifying 

Twitter messages (“tweets”) as expressing a posi-

tive opinion, a negative opinion, a neutral opinion, 

or no opinion (objective). In fact, the neutral and 

objective were joined in one class for the require-

ments of the shared task. Task A contained target 

words whose sense had to be classified in the con-

text, while Task B was to classify each text into 

one of the three classes: positive, negative, and 

neutral/objective. The training data that was made 

available for each task consisted in annotated 

Twitter message. There were two test sets for each 

task, one composed of Twitter messages and one 

of SMS message (even if there was no specific 

training data for SMS messages). See more details 

about the datasets in (Wilson et al., 2013). 

 

2 System Description 

We used supervised learning classifiers from We-

ka (Witten and Frank, 2005). Initially we extracted 

simple bag-of-word features (BOW). For the sub-

mitted systems, we also used features calculated 

based on SentiWordNet information (Baccianella 

et al., 2010). SentiWordNet contains positivity, 

negativity, and objectivity scores for each sense of 

a word. We explain below how this information 

was used for each task. 

    As classifiers, we used Support Vector Ma-

chines (SVM) (SMO and libSVM from Weka with 

default values for parameters), because SVM is 

known to perform well on many tasks, and Multi-

nomial Naive Bayes (MNB), because MNB is 

known to perform well on text data and it is faster 

than SVM. 

2.1 Task A 

Our system for Task A involved two parts: the 

expansion of our training data and the classifica-

tion. The expansion was done with information 

from SentiWordNet. Stop words and words that 

appeared only once in the training data were fil-

tered out. Then the classification was completed 

with algorithms from Weka.  

As mentioned, the first task was to expand all of 

the tweets that were provided as training data. This 

was doing using Python and the Python NLTK 

library, as well as SentiWordNet. SentiWordNet 

provides a score of the sentient state for each word 

(for each sense, in case the word has more than 

380



one sense). As an example, the word “want” can 

mean “a state of extreme poverty” with the Senti-

WordNet score of (Positive: 0 Objective: 

0.75 Negative: 0.25). The same word could also 

mean “a specific feeling of desire” with a score of 

(Positive: 0.5 Objective: 0.5 Negative: 0). We also 

used for expansion the definitions and synonyms 

of each word sense, from WordNet. 

The tweets in the training data are labeled with 

their sentiment type (Positive, Negative, Objective 

and Neutral). Neutral and Objective are treated the 

same. The provided training data has the target 

word marked, and also the sentiment orientation of 

the word in the context of the tweeter message. 

These target words were the ones expanded by our 

method. When the target was a multi-word expres-

sion, if the expression was found in WordNet, then 

the expansion was done directly; if not, each word 

was expanded in a similar fashion and concatenat-

ed to the original tweet. These target words were 

looked up in SentiWordNet and matched with the 

definition that had the highest score that also 

matched their sentiment label in the training data.  

 

 
Original Tweet The great Noel Gallagher is about to 

hit the stage in St. Paul. Plenty of 

room here so we're 4th row center. 

Plenty of room. Pretty fired up 
Key Words Great 
Sentiment Positive 
Definition very good; "he did a bully job"; "a 

neat sports car"; "had a great time at 

the party"; "you look simply smash-

ing" 
Synonyms Swell, smashing, slap-up, peachy, 

not_bad, nifty, neat, keen, groovy, 

dandy, cracking, corking, bully, 

bang-up 
Expanded 

Tweet 
The great Noel Gallagher is about to 

hit the stage in St. Paul. Plenty of 

room here so were 4th row center. 

Plenty of room. Pretty fired up  swell 

smashing slap-up peachy not_bad 

nifty neat keen groovy dandy crack-

ing corking bully bang-up very good 

he did a bully job a neat sports car 

had a great time at the party you look 

simply smashing  
Table 1: Example of tweet expansion for Task A 

 

 

The target word’s definition and synonyms were then 

concatenated to the original tweet. No additional 

changes were made to either the original tweet or the 

features that were added from SentiWordNet.  An ex-

ample follows in Table 1. The test data (Twitter and 

SMS) was not expanded, because there are no labels in 

the test data to be able to choose the sense with corre-

sponding sentiment. 

2.2 Task B 

For this task, we used the following resources: 

SentiwordNet (Baccianella et al, 2010), the Polari-

ty Lexicon (Wilson et al., 2005), the General In-

quirer (Stone et al., 1966), and the Stanford NLP 

tools (Toutanova et al., 2003) for preprocessing 

and feature selection. The preprocessing of Twitter 

messages is implemented in three steps namely, 

stop-word removal, stemming, and removal of 

words with occurrence frequency of one. Several 

extra features will be used: the number of positive 

words and negative words identified by three lexi-

cal resources mentioned above, the number of 

emoticons, the number of elongated words, and the 

number of punctuation tokens (single or repeated 

exclamation marks, etc.). As for SentiWordNet, 

for each word a score is calculated that shows the 

positive or negative weight of that word. No sense 

disambiguation is done (the first sense is used), but 

the scores are used for the right part-of-speech (in 

case a word has more than one possible part-of-

speech). Part-of-Speech tagging was done with the 

Stanford NLP Tools. As for General Inquirer and 

Polarity Lexicon, we simply used the list positive 

and negative words from these resources in order 

to count how many positive and how many nega-

tive terms appear in a message.  

 

3 Results 

3.1 Task A 

For classification, we first trained on our expanded 

training data using 10-fold cross-validation and 

using the SVM (libSVM) and Multinomial Na-

iveBayes classifiers from Weka, using their default 

settings. The training data was represented as a 

bag of words (BOW). These classifiers were cho-

sen as they have given us good results in the past 

for text classification. The classifiers were run 

with 10-fold cross-validation. See Table 2 for the 

381



results. Without expanding the tweets, the accura-

cy of the SVM classifier was equal to the baseline 

of classifying everything into the most frequent 

class, which was “positive“ in the training data. 

For MNB, the results were lower than the baseline. 

After expanding the tweets, the accuracy increased 

to 73% for SVM and to 80.36% for MNB. We 

concluded that MNB works better for Task A. This 

is why the submitted runs used the MNB model 

that was created from the expanded training data. 

Then we used this to classify the Twitter and SMS 

test data. The average F-score for the positive and 

the negative class for our submitted runs can be 

seen in Table 3, compared to the other systems 

that participated in the task. We report this meas-

ure because it was the official evaluation measure 

used in the task. 

 

 

System SVM MNB 

Baseline 66.32% 66.32% 

BOW features 66.32% 33.23% 

BOW+ text expansion 73.00% 80.36% 
Table 2: Accuracy results for task A by 10-fold cross-

validation on the training data 

 

 

System Tweets SMS 

uOttawa system 0.6020 0.5589 

Median system 0.7489 0.7283 

Best system 0.8893 0.8837 
Table 3:  Results for Task A for the submitted runs 

(Average F-score for positive/negative class) 

    

   The precision, recall and F-score on the Twitter 

and SMS test data for our submitted runs can be 

seen in Tables 4 and 5, respectively. All our sub-

mitted runs were for the “constrained” task; no 

additional training data was used. 

 

 

Class Precision Recall F-Score 

Positive 0.6934 0.7659 0.7278 

Negative 0.5371 0.4276 0.4762 

Neutral 0.0585 0.0688 0.0632 
Table 4: Results for Tweet test data for Task A, for 

each class. 

 

 

Class Precision Recall F-Score 

Positive 0.5606 0.5705 0.5655 

Negative 0.5998 0.5118 0.5523 

Neutral 0.1159 0.2201 0.1518 

Table 5: Results for SMS test data for Task A, for each 

class. 

3.2 Task B 

First we present results on the training data (10-

fold cross-validation), then we present the results 

for the submitted runs (also without any additional 

training data).  

    Table 6 shows the overall accuracy for BOW 

features for two classifiers, evaluated based on 10-

fold cross validation on the training data, for two 

classifiers: SVM (SMO in Weka) and Multidimen-

sional Naïve Bays (MNB in Weka). The BOW 

plus SentiWordNet features also include the num-

ber of positive and negative words identified from 

SentiWordNet. The BOW plus extra features rep-

resentation includes the number of positive and 

negative words identified from SentiWordNet, 

General Inquirer, and Polarity Lexicon (six extra 

features). The last row of the table shows the over-

all accuracy for BOW features plus all the extra 

features mentioned in Section 2.2, including in-

formation extracted from SentiWordNet, Polarity 

Lexicon, and General Inquirer. We can see that the 

SentiWordNet features help, and that when includ-

ing all the extra features, the results improve even 

more. We noticed that the features from the Polari-

ty Lexicon contributed the most. When we re-

moved GI, the accuracy did not change much; we 

believe this is because GI has too small coverage. 

 

 

System SVM MNB 

Baseline 48.50% 48.50% 

BOW features 58.75% 59.56% 

BOW+ SentiWordNet 69.43% 63.30% 

BOW+ extra features 82.42% 73.09% 
Table 6: Accuracy results for task B by 10-fold cross-

validation on the training data 

 

    The baseline in Table 6 is the accuracy of a triv-

ial classifier that puts everything in the most fre-

quent class, which is neutral/objective for the 

training data (ZeroR classifier in Weka). 

382



    The results of the submitted runs are in Table 7 

for the two data sets. The features representation 

was BOW plus SentiWordNet information. The 

official evaluation measure is reported (average F-

score for the positive and negative class). The de-

tailed results for each class are presented in Tables 

8 and 9.  

     In Table 7, we added an extra row for a new 

uOttawa system (SVM with BOW plus extra fea-

tures) that uses the best classifier that we designed 

(as chosen based on the experiments on the train-

ing data, see Table 6). This classifier uses SVM 

with BOW and all the extra features. 

 

 

System Tweets SMS 

uOttawa submitted  

system 

0.4251 0.4051 

uOttawa new system 0.8684 0.9140 

Median system 0.5150 0.4523 

Best system 0.6902 0.6846  
Table 7:  Results for Task B for the submitted runs 

(Average F-score for positive/negative). 

 

Class Precision Recall F-score 

Positive 0.6206 0.5089 0.5592 

Negative 0.4845 0.2080 0.2910 

Neutral 0.5357 0.7402 0.6216 
Table 8: Results for each class for task B, for the sub-

mitted system (SVM with BOW plus SentiWordNet 

features) for the Twitter test data. 

 

Class Precision Recall F-score 

Positive 0.4822 0.5508 0.5142 

Negative 0.5643 0.2005 0.2959 

Neutral 0.6932 0.7988 0.7423 
Table 9: Results for each class for task B, for the sub-

mitted system (SVM with BOW plus SentiWordNet 

features) for the SMS test data. 

 

4 Conclusions and Future Work  

In Task A, we expanded upon the Twitter messag-

es from the training data using their keyword’s 

definition and synonyms from SentiWordNet. We 

showed that the expansion helped improve the 

classification performance. In future work, we 

would like to try an SVM using asymmetric soft-

boundaries to try and penalize the classifier for 

missing items in the neutral class, the class with 

the least items in the Task A training data.   

 The overall accuracy of the classifiers for Task 

B increased a lot when we introduced the extra 

features discussed in section 2.2. The overall accu-

racy of SVM increased from 58.75% to 82.42% 

(as measures by cross-validation on the training 

data). When applying this classifier on the two test 

data sets, the results were very surprisingly good 

(even higher that the best system submitted by the 

SemEval participants for Task B
1
). 

 

References  

Stefano Baccianella, Andrea Esuli and Fabrizio Sebas-

tiani. SentiWordNet 3.0: An Enhanced Lexical Re-

source for Sentiment Analysis and Opinion Mining. 

In Proceedings of the Seventh International Confer-

ence on Language Resources and Evaluation 

(LREC'10), Valletta, Malta, May 2010. 

Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, 

and Daniel M. Ogilvie. The General Inquirer: A 

computer approach to content analysis. MIT Press, 

1966. 

Kristina Toutanova, Dan Klein, Christopher Manning, 

and Yoram Singer. Feature-Rich Part-of-Speech 

Tagging with a Cyclic Dependency Network. In Pro-

ceedings of HLT-NAACL 2003, pp. 252-259, 2003. 

Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, 

Sara Rosenthal, Veselin Stoyanov and Alan Ritter. 

SemEval-2013 Task 2: Sentiment Analysis in Twit-

ter. In Proceedings of the International Workshop on 

Semantic Evaluation SemEval '13, Atlanta, Georgia, 

June 2013. 

Theresa Wilson, Janyce Wiebe and Paul Hoffmann. 

Recognizing contextual polarity in phrase- level sen-

timent analysis. In Proceedings of HLT/ EMNLP 
2005.  

Ian H. Witten and Eibe Frank. Data Mining: Practical 

Machine Learning Tools and Techniques, 2nd edi-

tion, Morgan Kaufmann, San Francisco, 2005. 

 

 

                                                 
1
 Computed with the provided scoring script. 

383


