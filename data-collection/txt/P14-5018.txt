



















































FAdR: A System for Recognizing False Online Advertisements


Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 103–108,
Baltimore, Maryland USA, June 23-24, 2014. c©2014 Association for Computational Linguistics

FAdR: A System for Recognizing False Online Advertisements 

 
 

Yi-jie Tang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 

National Taiwan University, Taipei, Taiwan 
tangyj@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 

 
 

 

 

Abstract 

More and more product information, in-
cluding advertisements and user reviews, 
are presented to Internet users nowadays. 
Some of the information is false, mislead-
ing or overstated, which can cause seri-
ousness and needs to be identified. Au-
thorities, advertisers, website owners and 
consumers all have the needs to detect 
such statements. In this paper, we propose 
a False Advertisements Recognition sys-
tem called FAdR by using one-class and 
binary classification models. Illegal adver-
tising lists made public by a government 
and product descriptions from a shopping 
website are obtained for training and test-
ing. The results show that the binary SVM 
models can achieve the highest perfor-
mance when unigrams with the weighting 
of log relative frequency ratios are used as 
features. Comparatively, the benefit of the 
one-class classification models is the ad-
justable rejection rate parameter, which 
can be changed to suit different applica-
tions. Verb phrases more likely to intro-
duce overstated information are obtained 
by mining the datasets. These phrases help 
find problematic wordings in the advertis-
ing texts. 

1 Introduction 
As online commerce and advertising keep grow-
ing, more and more consumers depend on infor-
mation on the Internet to make purchasing deci-
sions. This kind of information includes online 
advertisements posted by businesses, and discus-
sions or reviews generated by users. However, 
false statements can also be presented to con-
sumers. For example, some companies hire peo-
ple to post fake product reviews in an attempt to 

promote their own products or reduce competi-
tors’ reputations (Ott et al., 2011). It is referred 
to as deceptive opinion spamming and explored 
in recent researches (Ott et al., 2011; Mukherjee 
et al., 2012; Mukherjee et al., 2013; Fei et al., 
2013). 

False statements and exaggerated content can 
also be seen in online advertisements. These 
statements can also be regarded as opinion 
spams, while the authors, that is, the advertisers, 
can be more easily identified. Yeh (2014) report-
ed the top two types of illegal advertisements on 
the web, TV and broadcast are food (62.61%) 
and cosmetic (24.26%). Of the dissemination 
media, the web is the major source of false ad-
vertisements. Most inappropriate food-related 
advertisements contain overstated health claims. 
The medical effects and cure claims may also 
appear in cosmetic advertising. As a result, ad-
vertising regulations are enforced in many coun-
tries to protect consumers from fraudulent and 
misleading information. False, overstated or mis-
leading information and mentions of curative 
effects can be prohibited by the authorities (FTC, 
2000; DOH, 2009; CFIA, 2010). 

To regulate online advertising, the authorities 
need to review a large number of advertisements 
and determine their legality, which is cost- and 
time-consuming. Advertisers also need to know 
the legality of their advertisements to avoid vio-
lating advertising laws. This becomes especially 
important when every Internet user can be an 
advertiser if s/he posts messages related to any 
product announcement, promotion, or sales. 
Website owners that accept advertisements have 
to present appropriate advertisement contents to 
users and avoid legal issues. Even Internet users 
should also identify false advertisements in order 
not to be misled. Thus, the recognition of false, 
misleading or overstated information is an 
emerging task.  

This paper presents a False Advertisements 
Recognition system called FAdR, and take two 

103



major sources of illegal advertisements on the 
web, i.e., food and cosmetic advertising, as ex-
amples. Section 2 surveys the related work. Sec-
tion 3 introduces the datasets used in the experi-
ments. Section 4 presents classification models 
and shows their performance. Section 5 mines 
the overstated phrases. Section 6 demonstrates 
the uses of FAdR system with screenshot. Both 
sentence and document levels are considered. 

2 Related Work 
Gokhman et al. (2012) collected data from the 
Internet and explored methods to construct a 
gold standard corpus for “deception” studies. Ott 
et al. (2011) studied methods to detect “disrup-
tive opinion spams.” Unlike conventional adver-
tising spams, these fake opinions look authentic 
and are used to mislead users. Mukherjee et al. 
(2013) used reviewer’s behavioral footprints to 
detect spammer. As they pointed out, one of the 
largest problems to solve this issue is that there is 
no appropriate datasets for fake and non-fake 
reviews. 

Previous online advertising research mostly 
focuses on bidding, matching or recommendation 
of advertisements on websites. Ghosh et al. 
(2009) studied bidding strategies for advertise-
ment allocations. Huang et al. (2008) proposed 
an advertisement recommendation method by 
classifying instant messages into the Yahoo cate-
gories. Scaiano and Inkpen (2011) used Wikipe-
dia for negative keyphrase generation to hide 
advertisements that users are not interested in. 
This paper, in contrast, focuses on identifying 
false statements in online advertisements with 
classification models. 

3 Datasets 
We use the illegal advertising lists and state-

ments made public by the Taipei City Govern-
ment1 as the illegal advertising datasets. The con-
tents of the government data are split into sen-
tences by colon, period, question mark and ex-
clamation mark. Two types of datasets are built 
for illegal food and cosmetic advertising, named 
FOOD_ILLEGAL and COS_ILLEGAL, respec-
tively. Some illegal sentences in the illegal food 
advertising dataset are shown below: 

(1)  減少代謝廢物的堆積〈 
Reduces waste produced by metabolism 
process. 

(2)  減少失眠及疼痛〈 
                                                
1 http://www.health.gov.tw/Default.aspx?tabid=295 

Stops insomnia and pain. 
(3)  治療高血壓〈 

Cures hypertension. 
In the government website, the authority does 

not regularly announce legal advertising data. 
We adopt one-class classifiers with only illegal 
data for this scenario, as shown in Section 4.1. 
To experiment on binary classifiers, we collect 
product descriptions from a shopping website2 
and verify their legality manually to construct the 
legal advertising datasets. The legal food and 
cosmetic adverting datasets are named 
FOOD_LEGAL and COS_LEGAL, respectively. 
The numbers of the sentences in 
FOOD_LEGAL, FOOD_ILLEGAL, 
COS_LEGAL, and COS_ILLEGAL are 5,059, 
7,033, 10,520, and 11,381, respectively. 

4 Classification Models 
One-class Naïve Bayes and Bagging classifiers, 
and binary classifiers based on Naïve Bayes and 
SVM models are implemented.  

4.1 One-Class Classifiers  
We adopt the OneClassClassifier module 
(Hempstalk et al., 2008) in the WEKA machine 
learning tool to train one-class classifiers with 
illegal statements only. The OneClassClassifier 
module provides a rejection rate parameter for 
adjusting the threshold between target and non-
target instances.  The target class, which corre-
sponds to the illegal class in this study, is the 
single class used to train the classifier.  Higher 
rejection rate means that more legal statements 
will be preferred, but illegal statements may be 
still incorrectly classified into legal ones. Naïve 
Bayes and Bagging classifiers are chosen be-
cause they achieve best performance among the 
algorithms we have explored in this experiment. 

Each instance in the dataset, i.e., a sentence, is 
represented by a word vector (w1, w2, …, w1000), 
where wi is a binary value indicating whether a 
word occurs in the sentence or not. The vocabu-
lary is selected from the illegal advertising da-
tasets. To properly filter out common words, we 
count top 1,000 frequent words in the Sinica 
Balanced Corpus of Modern Chinese3 and re-
move them from the vocabulary. The remaining 
top 1,000 words are used for vector representation. 

Total 532 illegal statements provided by the 
Department of Health form the training set. An 

                                                
2 http://www.7net.com.tw 
3 http://app.sinica.edu.tw/kiwi/mkiwi/ 

104



illegal and a legal advertising dataset make up 
the test set. The former consists of 317 illegal 
sentences from Taipei City Government’s lists, 
and the latter contains 203 legal statement exam-
ples from the Department of Health. 

Table 1 shows the accuracies of Naïve Bayes 
and Bagging classifiers in the food dataset. The 
rejection rates from 0.7 to 0.8 are preferable for 
most applications, because they result in higher 
accuracy for legal statement classification while 
not significantly reducing the performance of 
illegal statement detection.  Using the 0.7 rejec-
tion rate produces high performance for the ille-
gal class while 0.8 rejection rate does better for 
the legal class.  The actual choice of rejection 
rate depends on the demands of users.  For an 
advertiser, it is important to avoid all possible 
problematic statements.  Thus, a lower rejection 
rate will be more suitable.  If the system is used 
by the authorities, a rejection rate higher than 0.7 
may be preferable because they don’t misjudge 
too many legal advertisements. 
 

Rejection rate 0.4 0.5 0.6 0.7 0.8 0.9 

Naïve 
Bayes 

Illegal 85.33% 82.39% 79.01% 74.49% 68.17% 59.14% 

Legal 31.07% 39.81% 53.40% 63.11% 72.82% 86.41% 

Bagging 
Illegal 92.78% 88.49% 84.65% 74.94% 69.07% 0.23% 

Legal 3.88% 17.48% 27.18% 65.72% 82.52% 99.77% 

Table 1: Accuracies of Classifiers in Different Rejec-
tion Rates. 

4.2 Binary Classifiers  
We use FOOD_LEGAL and FOOD_ILLEGAL 
datasets, and COS_LEGAL and COS_ILLEGAL 
datasets to build binary classifiers for food and 
cosmetic advertising classification, respectively. 
Naïve Bayes classifiers and SVM classifiers im-
plemented with libSVM (Chang & Lin, 2011) are 
adopted. Ten-fold cross validation is used for the 
training and testing tasks. Total 1,000 highly fre-
quent words are selected in the same way as in 
Section 4.1 to form a word-based unigram fea-
ture set.  

Two weighting schemes are considered. In the 
binary weighting, each sentence is represented 
by a word vector (w1, w2, …, w1000), where wi is a 
binary value indicating whether a word occurs in 
the sentence or not.  In the weighting of log rela-
tive frequency ratio, we follow the idea of collo-
cation mining (Damerau, 1993). Relative fre-
quency ratio between two datasets has been 
shown to be useful to discover collocations that 
are characteristic of a dataset when compared to 
the other dataset. It has been successfully applied 
to mine sentiment words from microblog and to 

model reader/writer emotion transition (Tang and 
Chen, 2011, 2012). 

The log relative frequency ratio (logRF) is 
defined formally as follows. Given two datasets 
A and B, the log relative frequency ratio for each 
wi∈A∪B is computed with the following formula. 

logRFAB (w
i ) = log

fA (w
i )

| A |
fB (w

i )
| B |

 

logRFAB(wi) is a log ratio of relative frequen-
cies of word wi in A and B, fA(wi) and fB(wi) are 
frequencies of wi in A and in B, respectively, and 
|A| and |B| are total words in A and in B, respec-
tively. logRF values are used to estimate the dis-
tribution of the words in datasets A and B. If wi 
has higher relative frequency in A than in B, then 
logRFAB(wi)>0, and vice versa. In our experi-
ments, logRF is used to present each unigram’s 
distribution in the legal and illegal datasets, re-
placing the binary value for a unigram feature. 

Tables 2 and 3 show the results of the classifi-
cation models with different combinations of 
feature sets. When logRF is combined with Uni-
gram, the accuracy is significantly improved in 
both the food and cosmetic datasets. We can also 
see that the performance of all FOOD models are 
higher than equivalent COS models. Possible 
reasons may be that the effects of cosmetics are 
related to body appearance, and inappropriate 
cure claims are also related to body improvement 
and appearance changes. There can be some 
overlaps between the words used in legal and 
illegal cosmetic advertising.  
 
Classification Mod-

els → Naïve Bayes SVM 

Illegal vs. Legal → 
Features ↓ Illegal Legal Illegal Legal 

Unigram 92.59% 85.06% 89.46% 88.00% 
Unigram + logRF 94.32% 86.37% 94.70% 91.68% 

Table 2: Classification Accuracies for FOOD Datasets. 
 
Classification Mod-

els → Naïve Bayes SVM 

Illegal vs. Legal → 
Features ↓ Illegal Legal Illegal Legal 

Unigram 86.48% 77.63% 82.47% 82.36% 
Unigram + logRF 88.20% 83.06% 88.46% 83.41% 

Table 3: Classification Accuracies for COS Datasets. 

5 Overstated Phrase Mining 

Since the authority focuses on health claims in 
advertising, almost all illegal statements an-
nounced by the government include an action 
related to health improvement and a name that 
refers to diseases or body conditions. Thus, we 
can observe that most of the illegal statements 

105



recognized and forbidden by the authority con-
tain a health-related verb phrase consisting of a 
transitive verb and an object. These illegal adver-
tising verb phrases can be mined from the da-
tasets for the government’s and advertisers’ ref-
erence. We can also use these verb phrases to 
help the users of our system understand possible 
reasons why the sentences in advertisements are 
labeled as illegal. 

We propose a mining method based on log 
relative frequency ratio, which is described in 
Section 4.2. We compute logRFAB(wi) to obtain 
the words that are most likely to be used in ille-
gal advertising. We identify transitive verbs and 
nouns in the word list based on POS tagging re-
sults generated by the CKIP parser4, and then use 
them to examine if a verb phrase is presented in a 
sentence. Total 979 verb phrases are mined from 
the FOOD datasets, and 2,302 from the COS da-
taset. Table 4 shows some examples. 

 

Dataset Illegal advertising verb phrases Transitive verb Object noun 

FOOD 

增強 
(improve) 

體質 
(physical condition) 

抑制 
(inactivate) 

細菌 
(bacteria) 

分解 
(decompose) 

膽固醇 
(cholesterol) 

COS 

淨化 
(purify) 

體質 
(body) 

舒緩 
(ease) 

疼痛 
(pain) 

治療 
(cure) 

面皰 
(acne vulgaris) 

Table 4: Example illegal verb phrases 
mined from the FOOD and COS datasets. 

6 System Architecture 

The FAdR system is composed of pre-
processing (Pre-Processor), recognition (Recog-
nizer), and explanation (Explainer) modules. 
Figure 1 shows the overall system architecture. 

6.1 Pre-processing Module 
Our classification models are sentence-based, so 
the main purpose of the Pre-processor in the sys-
tem is detecting sentence boundaries. Four types 
of punctuations, including period, colon, excla-
mation, and question mark, are used to segment a 
document into sentences. Line breaks are also 
regarded as a sentence boundary marker because 

                                                
4 http://ckipsvr.iis.sinica.edu.tw 

many advertisements in Chinese put sentences in 
separate lines and do not include any punctua-
tion. Sentences with less than three characters or 
more than 80 characters are ignored. 

Word segmentation is performed by using the 
CKIP segmenter, which is an online service and 
can be accessed through the TCP socket. Seg-
mented data will be represented by the corre-
sponding feature sets based on classification 
model and converted to a format that the Recog-
nizer can read as input.  

 

Recognizer Classification Models

Advertising Document

Sentence 
Segmenter

Word
Segmenter

Format 
Converter Feature Sets

Explainer

Advertising document
with sentence-based

legality labels and
explanations.

Pre-Processor

 
 

Figure 1. System architecture of FAdR 

6.2 Recognition Module 
All processed sentences are sent from the Pre-
Processor to the Recognizer for legality identifi-
cation.  

Since our training tasks are done in WEKA, 
we can use the model files generated by WEKA 
for implementing the Recognizer. The Recogniz-
er loads the pre-trained SVM models for food 
and cosmetic advertising classification, and then 
uses them for labeling the incoming sentences. 

For the One-Class models, the model files are 
pre-generated by training with different rejection 
rates from 0.4 to 0.9. When the user adjusts the 
threshold, the Recognizer chooses the corre-
sponding model to perform illegal sentences 
identification. 

106



6.3 Explanation Module 
To give users more information on the possible 
reasons why the advertising contents are consid-
ered illegal, the Explainer uses the illegal verb 
phrase list, which is discussed in Section 5, to 
extract the problematic words from the input sen-
tences. If the verb and the object noun in a verb 
phrase from the list both occur in an illegal sen-
tence, then the verb phrase will be shown besides 
the recognition results in the user interface. 

6.4 User Interface 
Users can copy and paste the advertising con-
tents to be recognized to the text field, or upload 
a document to the system. It usually takes less 
than 10 seconds on our server to process a doc-
ument with 200 characters, so the system is suit-
able to quickly process a large amount of data. 

If the users choose to use the one-class mod-
els, they can adjust the threshold value to fit dif-
ferent needs and receive useful results. Lowering 
the value can find as many problematic sentences 
as possible, but more legal sentences can also be 
misjudged.  Increasing the value can avoid 
wrongly labeling legal sentences as illegal, but 
more illegal sentences can be missed.  

Figure 2 shows a system screenshot. The 
recognition results of a food advertisement with 
11 sentences are demonstrated. Sentences la-
belled as illegal are highlighted in red. Verb 
phrases possibly causing illegality are listed in 
grey colour for illegal sentences. The number of 
all sentences, the number of illegal sentences, 
and the final score are shown at the bottom. The 
correct score of an advertisement is defined as 
the number of correct sentences divided by total 
sentences in this advertisement. The sample ad-
vertisement used in Figure 2 and its English 
translation are shown as follows. 

 
<A food advertisement> 

日本茶第一品牌︽全����������������������������������台首支融合三大天然色
素的茶飲︽可提升免疫力︽消除壓力︽增強
體內抵抗力︽增加體內抗體的形成〈溫和不
刺激︽適合天天飲用〈可降低自由基對細胞
的過氧化傷害︽強化人體免疫功能︽健康好
喝零負擔﹁  
  
(The leading brand for Japanese tea. The first tea 
product combining three kinds of natural colour-
ings in Taiwan. Can improve immunity. Can re-
lieve stress. Can strengthen resistance to disease.  
Can increase antibodies in your body. It is mild 
and not irritative. Good for daily use. Can pre-
vent body cells from being harmed by free radi-

cals. Can strengthen immunity. It is healthy and 
tasty, and brings no body burden.) 
 

 
 

Figure 2: Screenshot for Illegal Sentence Recognition 

7 Conclusion 
Detecting false information on the Internet has 
become an important issue for users and organi-
zations. In this paper, we present two types of 
classification methods to identify overstated sen-
tences in online advertisements and build a false 
online advertisements recognition system FAdR. 
The recognition on both document and sentence 
levels is addressed in the demonstration. 

In the binary models, using combinations of 
unigrams and the log relative frequency ratio as 
features can achieve highest performance. On the 
other hand, the one-class models can be used to 
build a system that is adjustable by users for dif-
ferent application domains. 

The authorities or website owners can use a 
rejection rate of 0.7 or 0.8 to highlight most seri-
ous illegal advertisements. An advertisement 

107



with a score lower than 0.5 means it may critical-
ly violate the regulations, and need to be regard-
ed as illegal advertising. Since not all advertise-
ment posters are professional advertisers, they 
may need detailed information on the legality of 
every sentence. The illegal verb phrases found in 
a sentence provide clues to the advertiser. The 
system is also useful for consumers, as they can 
check if the advertisement contents can be trust-
ed before making a purchase decision. 

As future work, we will extend the methodol-
ogy presented in this study to handle other types 
of advertisements and the materials in other lan-
guages.  We will also investigate what linguistic 
patterns can be used to mine the overstated 
phrases in different languages. 

Acknowledgments 
This research was partially supported by Nation-
al Taiwan University and Ministry of Science 
and Technology, Taiwan under 103R890858 and 
102-2221-E-002-103-MY3. 

References  
Chih-Chung Chang and Chih-Jen Lin. 2001. 

LIBSVM: a Library for Support Vector Machines. 
Available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 

CFIA. 2010. Advertising Requirements. Canadian 
Food Inspection Agency. Available at 
http://www.inspection.gc.ca/english/fssa/labeti/adv
pube.shtml. 

Fred J. Damerau. 1993. Generating and Evaluating 
Domain-Oriented Multi-Word Terms from Text. 
Information Processing and Management, 29:433-
477. 

DOH. 2009. Legal and Illegal Advertising Statements 
for Cosmetic Regulations. Department of Health of 
Taiwan. Available at 
http://www.doh.gov.tw/ufile/doc/0980305527.pdf. 

Geli Fei, Arjun Mukherjee, Bing Liu, Meichun Hsu, 
Malu Castellanos, and Riddhiman Ghosh. 2013. 
Exploiting Burstiness in Reviews for Review 
Spammer Detection. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social 
Media (ICWSM-2013), 175-184. 

FTC. 2000. Advertising and Marketing on the Inter-
net: Rules of the Road, Bureau of Consumer Pro-
tection. Federal Trade Commission, September 
2000. Available at 
http://business.ftc.gov/sites/default/files/pdf/bus28-
advertising-and-marketing-internet-rules-road.pdf. 

Stephanie Gokhman, Jeff Hancock, Poornima Prabhu, 
Myle Ott, and Claire Cardie. 2012. In Search of a 

Gold Standard in Studies of Deception. In Pro-
ceedings of the EACL 2012 Workshop on Compu-
tational Approaches to Deception Detection, 23–
30. 

Arpita Ghosh, Preston McAfee, Kishore Papineni, and 
Sergei Vassilvitskii. 2009. Bidding for Representa-
tive Allocations for Display Advertising. CoRR, 
abs/0910-0880, 2009. 

Hung-Chi Huang, Ming-Shun Lin and Hsin-Hsi Chen. 
2008. Analysis of Intention in Dialogues Using 
Category Trees and Its Application to Advertise-
ment Recommendation. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing (IJCNLP 2008), 625-630. 

Kathryn Hempstalk, Eibe Frank, and Ian H. Witten. 
2008. One-Class Classification by Combining 
Density and Class Probability Estimation. In Pro-
ceedings of the 12th European Conference on 
Principles and Practice of Knowledge Discovery in 
Databases and 19th European Conference on Ma-
chine Learning, 505-519. 

Arjun Mukherjee, Bing Liu, and Natalie Glance. 
2012. Spotting Fake Reviewer Groups in Consum-
er Reviews. In Proceedings of the International 
World Wide Web Conference (WWW 2012), 191-
200. 

Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui 
Wang, Meichun Hsu, Malu Castellanos, and Rid-
dhiman Ghosh. 2013. Spotting Opinion Spammers 
using Behavioral Footprints. In Proceedings of 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD 2013), 632-640. 

Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. 
Hancock. 2011. Finding Deceptive Opinion Spam 
by Any Stretch of the Imagination. In Proceedings 
of the 49th Annual Meeting of the Association for 
Computational Linguistics, 309–319.  

M. Scaiano and D. Inkpen. 2011. Finding Negative 
Key Phrases for Internet Advertising Campaigns 
Using Wikipedia. In Recent Advances in Natural 
Language Processing (RANLP 2011), 648–653. 

Yi-jie Tang and Hsin-Hsi Chen. 2011. Emotion Mod-
eling from Writer/Reader Perspectives Using a Mi-
croblog Dataset. In Proceedings of IJCNLP Work-
shop on Sentiment Analysis where AI Meets Psy-
chology, 11-19.  

Yi-jie Tang and Hsin-Hsi Chen. 2012. Mining Senti-
ment Words from Microblogs for Predicting Writ-
er-Reader Emotion Transition. In Proceedings of 
the 8th International Conference on Language Re-
sources and Evaluation (LREC 2012), 1226-1229. 

Ming-kung Yeh. 2014. Weekly Food and Drug Safety. 
No. 440, February, Food and Drug Administration, 
Taiwan. Available at 
http://www.fda.gov.tw/TC/PublishOther.aspx. 

108


