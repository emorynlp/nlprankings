Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984–992,

Beijing, August 2010

984

Multi-Document Summarization via

the Minimum Dominating Set

Chao Shen and Tao Li

School of Computing and Information Sciences

Florida Internation University
{cshen001|taoli}@cs.fiu.edu

Abstract

Multi-document
summarization has
been an important problem in infor-
mation retrieval.
It aims to dis-
till the most important information
from a set of documents to gener-
ate a compressed summary. Given
a sentence graph generated from a
set of documents where vertices repre-
sent sentences and edges indicate that
the corresponding vertices are simi-
lar, the extracted summary can be de-
scribed using the idea of graph dom-
ination.
In this paper, we propose
a new principled and versatile frame-
work for multi-document summariza-
tion using the minimum dominating
set. We show that four well-known
summarization tasks including generic,
query-focused, update, and compara-
tive summarization can be modeled as
diﬀerent variations derived from the
proposed framework. Approximation
algorithms for performing summariza-
tion are also proposed and empirical
experiments are conducted to demon-
strate the eﬀectiveness of our proposed
framework.

1

Introduction

As a fundamental and eﬀective tool for docu-
ment understanding and organization, multi-
document summarization enables better infor-
mation services by creating concise and infor-
mative reports for a large collection of doc-
uments. Speciﬁcally, in multi-document sum-
marization, given a set of documents as input,
the goal is to produce a condensation (i.e.,
a generated summary) of the content of the

entire input set (Jurafsky and Martin, 2008).
The generated summary can be generic where
it simply gives the important information con-
tained in the input documents without any
particular information needs or query/topic-
focused where it is produced in response to a
user query or related to a topic or concern the
development of an event (Jurafsky and Mar-
tin, 2008; Mani, 2001).

Recently, new summarization tasks such as
update summarization (Dang and Owczarzak,
2008) and comparative summarization (Wang
et al., 2009a) have also been proposed. Up-
date summarization aims to generate short
summaries of recent documents to capture
new information diﬀerent from earlier docu-
ments and comparative summarization aims
to summarize the diﬀerences between compa-
rable document groups.

In this paper, we propose a new principled
and versatile framework for multi-document
summarization using the minimum dominat-
ing set. Many known summarization tasks in-
cluding generic, query-focused, update, and
comparative summarization can be modeled
as diﬀerent variations derived from the pro-
posed framework. The framework provides an
elegant basis to establish the connections be-
tween various summarization tasks while high-
lighting their diﬀerences.

In our framework, a sentence graph is ﬁrst
generated from the input documents where
vertices represent sentences and edges indicate
that the corresponding vertices are similar. A
natural method for describing the extracted
summary is based on the idea of graph dom-
ination (Wu and Li, 2001). A dominating set
of a graph is a subset of vertices such that
every vertex in the graph is either in the sub-
set or adjacent to a vertex in the subset; and

985

a minimum dominating set is a dominating
set with the minimum size. The minimum
dominating set of the sentence graph can be
naturally used to describe the summary:
it
is representative since each sentence is either
in the minimum dominating set or connected
to one sentence in the set; and it is with
minimal redundancy since the set is of mini-
mum size. Approximation algorithms are pro-
posed for performing summarization and em-
pirical experiments are conducted to demon-
strate the eﬀectiveness of our proposed frame-
work. Though the dominating set problem has
been widely used in wireless networks, this pa-
per is the ﬁrst work on using it for modeling
sentence extraction in document summariza-
tion.

The rest of the paper is organized as fol-
lows. In Section 2, we review the related work
about multi-document summarization and the
dominating set. After introducing the min-
imum dominating set problem in graph the-
ory in Section 3, we propose the minimum
dominating set based framework for multi-
document summarization and model the four
summarization tasks including generic, query-
focused, update, and comparative summariza-
tion in Section 4. Section 5 presents the exper-
imental results and analysis, and ﬁnally Sec-
tion 6 concludes the paper.

2 Related Work

Generic Summarization For generic sum-
marization, a saliency score is usually as-
signed to each sentence and then the sen-
tences are ranked according to the saliency
score. The scores are usually computed based
on a combination of statistical and linguistic
features. MEAD (Radev et al., 2004) is an
implementation of the centroid-based method
where the sentence scores are computed based
on sentence-level and inter-sentence features.
SumBasic (Nenkova and Vanderwende, 2005)
shows that the frequency of content words
alone can also lead good summarization re-
sults.
Graph-based methods (Erkan and
Radev, 2004; Wan et al., 2007b) have also
been proposed to rank sentences or passages

based on the PageRank algorithm or its vari-
ants.

Query-Focused
Summarization In
query-focused summarization, the informa-
tion of the given topic or query should be
incorporated into summarizers, and sentences
suiting the user’s declared information need
should be extracted. Many methods for
generic summarization can be extended to
incorporate the query information (Saggion
et al., 2003; Wei et al., 2008). Wan et al.
(Wan et al., 2007a) make full use of both
the relationships among all the sentences in
the documents and relationship between the
given query and the sentences by manifold
ranking. Probability models have also been
proposed with diﬀerent assumptions on the
generation process of the documents and
the queries (Daum´e III and Marcu, 2006;
Haghighi and Vanderwende, 2009; Tang et
al., 2009).

Update Summarization and Compara-
tive Summarization Update summariza-
tion was introduced in Document Understand-
ing Conference (DUC) 2007 (Dang, 2007) and
was a main task of the summarization track in
Text Analysis Conference (TAC) 2008 (Dang
and Owczarzak, 2008). It is required to sum-
marize a set of documents under the assump-
tion that the reader has already read and
summarized the ﬁrst set of documents as the
main summary. To produce the update sum-
mary, some strategies are required to avoid re-
dundant information which has already been
covered by the main summary. One of the
most frequently used methods for remov-
ing redundancy is Maximal Marginal Rele-
vance(MMR) (Goldstein et al., 2000). Com-
parative document summarization is proposed
by Wang et.
(Wang et al., 2009a) to
summarize the diﬀerences between compara-
ble document groups. A sentence selection
approach is proposed in (Wang et al., 2009a)
to accurately discriminate the documents in
diﬀerent groups modeled by the conditional
entropy.

al.

986

The Dominating Set Many approxima-
tion algorithms have been developed for ﬁnd-
ing minimum dominating set for a given
graph (Guha and Khuller, 1998; Thai et al.,
2007). Kann (Kann, 1992) shows that the
minimum dominating set problem is equiv-
alent to set cover problem, which is a well-
known NP-hard problem. Dominating set has
been widely used for clustering in wireless net-
works (Chen and Liestman, 2002; Han and
Jia, 2007).
It has been used to ﬁnd topic
words for hierarchical summarization (Lawrie
et al., 2001), where a set of topic words is ex-
tracted as a dominating set of word graph. In
our work, we use the minimum dominating set
to formalize the sentence extraction for docu-
ment summarization.

3 The Minimum Dominating Set

Problem

Given a graph G =< V, E >, a dominating
set of G is a subset S of vertices with the
following property: each vertex of G is either
in the dominating set S, or is adjacent to some
vertices in S.
Problem 3.1. Given a graph G, the mini-
mum dominating set problem (MDS) is to ﬁnd
a minimum size subset S of vertices, such that
S forms a dominating set.

MDS is closely related to the set cover prob-

lem (SC), a well-known NP-hard problem.
Problem 3.2. Given F , a ﬁnite collection
{S1, S2, . . . , Sn} of ﬁnite sets, the set cover
problem (SC) is to ﬁnd the optimal solution

F ∗ = arg min

F (cid:2)⊆F |F (cid:4)| s.t. (cid:2)S(cid:2)∈F (cid:2)

S(cid:4) = (cid:2)S∈F

S.

Theorem 3.3. There exists a pair of polyno-
mial time reduction between MDS and SC.

So, MDS is also NP-hard and it has been
shown that there are no approximate solutions
within c log |V |, for some c > 0 (Feige, 1998;
Raz and Safra, 1997).

3.1 An Approximation Algorithm

A greedy approximation algorithm for the SC
problem is described in (Johnson, 1973). Ba-
sically, at each stage, the greedy algorithm

chooses the set which contains the largest
number of uncovered elements.

Based on Theorem 3.3, we can obtain a
greedy approximation algorithm for MDS.
Starting from an empty set, if the current sub-
set of vertices is not the dominating set, a new
vertex which has the most number of the ad-
jacent vertices that are not adjacent to any
vertex in the current set will be added.

Proposition 3.4. The greedy algorithm ap-
proximates SC within 1 + ln s where s is the
size of the largest set.

It was shown in (Johnson, 1973) that the
approximation factor for the greedy algorithm
is no more than H(s) , the s-th harmonic num-
ber:

H(s) =

1
k ≤ ln s + 1

s(cid:3)k=1

Corollary 3.5. MDS has a approximation al-
gorithm within 1 + ln Δ where Δ is the maxi-
mum degree of the graph.

Corollary 3.5 follows directly from Theo-

rem 3.3 and Proposition 3.4.

4 The Summarization Framework

4.1 Sentence Graph Generation

To perform multi-document summarization
via minimum dominating set, we need to ﬁrst
construct a sentence graph in which each node
is a sentence in the document collection.
In
our work, we represent the sentences as vec-
tors based on tf-isf, and then obtain the cosine
similarity for each pair of sentences.
If the
similarity between a pair of sentences si and
sj is above a given threshold λ, then there is
an edge between si and sj.

For generic summarization, we use all sen-
tences for building the sentence graph. For
query-focused summarization, we only use the
sentences containing at least one term in the
query. In addition, when a query q is involved,
we assign each node si a weight, w(si) =
d(si, q) = 1 − cos(si, q), to indicate the dis-
tance between the sentence and the query q.
After building the sentence graph, we can
formulate the summarization problem using

987

Generic Summary

Query-focused Summary

Updated Summary

Comparative Summary

query

(a)

(b)

C

2

C

1

(c)

Comparative Summary

C

2

C

1

C

3

Comparative Summary

(d)

Figure 1: Graphical illustrations of multi-document summarization via the minimum domi-
nating set. (a): The minimum dominating set is extracted as the generic summary. (b):The
minimum weighted dominating set is extracted as the query-based summary. (c):Vertices in
the right rectangle represent the ﬁrst document set C1, and ones in the left represent the sec-
ond document set where update summary is generated. (d):Each rectangle represents a group
of documents. The vertices with rings are the dominating set for each group, while the solid
vertices are the complementary dominating set, which is extracted as comparative summaries.

the minimum dominating set. A graphical il-
lustration of the proposed framework is shown
in Figure 1.

4.2 Generic Summarization

Generic summarization is to extract the most
representative sentences to capture the impor-
tant content of the input documents. Without
taking into account the length limitation of
the summary, we can assume that the sum-
mary should represent all the sentences in the
document set (i.e., every sentence in the docu-
ment set should either be extracted or be sim-
ilar with one extracted sentence). Meanwhile,
a summary should also be as short as possi-
ble. Such summary of the input documents
under the assumption is exactly the minimum
dominating set of the sentence graph we con-
structed from the input documents in Section
4.1. Therefore the summarization problem
can be formulated as the minimum dominat-
ing set problem.

However, usually there is a length restric-
tion for generating the summary. Moreover,
the MDS is NP-hard as shown in Section 3.
Therefore, it is straightforward to use a greedy
approximation algorithm to construct a subset
of the dominating set as the ﬁnal summary. In
the greedy approach, at each stage, a sentence
which is optimal according to the local crite-
ria will be extracted. Algorithm 1 describes

Algorithm 1 Algorithm for Generic Summariza-
tion
INPUT: G, W
OUTPUT: S
1: S = ∅
2: T = ∅
3: while L(S) < W and V (G)! = S do
4:
5:
6:
7:
8:

for v ∈ V (G) − S do
v∗ = arg maxv s(v)
S = S ∪ {v∗}
T = T ∪ ADJ(v∗)

s(v) = |{ADJ(v) − T}|

an approximation algorithm for generic sum-
marization.
In Algorithm 1, G is the sen-
tence graph, L(S) is the length of the sum-
mary, W is the maximal length of the sum-
mary, and ADJ(v) = {v(cid:4)|(v(cid:4), v) ∈ E(G)} is
the set of vertices which are adjacent to the
vertex v. A graphical illustration of generic
summarization using the minimum dominat-
ing set is shown in Figure 1(a).

4.3 Query-Focused Summarization

Letting G be the sentence graph constructed
in Section 4.1 and q be the query, the query-
focused summarization can be modeled as

D∗ = arg minD⊆G(cid:4)s∈D d(s, q)

s.t. D is a dominating set of G.

(1)

Note that d(s, q) can be viewed as the weight
of vertex in G. Here the summary length is
minimized implicitly, since if D(cid:4) ⊆ D, then

988

(cid:4)s∈D(cid:2) d(s, q) ≤ (cid:4)s∈D d(s, q). The problem

in Eq.(1) is exactly a variant of the minimum
dominating set problem,
i.e., the minimum
weighted dominating set problem (MWDS).

graph presentation of the document set in-
cluding C1 and C2. We can model the update
summary of C2 as

Similar to MDS, MWDS can be reduced
from the weighted version of the SC problem.
In the weighted version of SC, each set has a
weight and the sum of weights of selected sets
needs to be minimized. To generate an ap-
proximate solution for the weighted SC prob-
lem, instead of choosing a set i maximizing
|SET (i)|, a set i minimizing
is cho-
sen, where SET (i) is composed of uncovered
elements in set i, and w(i) is the weight of set
i. The approximate solution has the same ap-
proximation ratio as that for MDS, as stated
by the following theorem (Chvatal, 1979).

|SET (i)|

w(i)

Theorem 4.1. An approximate weighted
dominating set can be generated with a size at
most 1+log Δ·|OP T|, where Δ is the maximal
degree of the graph and OP T is the optimal
weighted dominating set.

Accordingly, from generic summarization to
query-focused summarization, we just need to
modify line 6 in Algorithm 1 to

v∗ = arg min

v

w(v)
s(v)

,

(2)

where w(v) is the weight of vertex v. A graph-
ical illustration of query-focused summariza-
tion using the minimum dominating set is
shown in Figure 1(b).

4.4 Update Summarization

Give a query q and two sets of documents C1
and C2, update summarization is to generate
a summary of C2 based on q, given C1. Firstly,
summary of C1, referred as D1 can be gener-
ated. Then, to generate the update summary
of C2, referred as D2, we assume D1 and D2
should represent all query related sentences in
C2, and length of D2 should be minimized.

Let G1 be the sentence graph for C1. First
we use the method described in Section 4.3 to
extract sentences from G1 to form D1. Then
we expand G1 to the whole graph G using the
second set of documents C2. G is then the

D∗ = arg minD2(cid:4)s∈D2

s.t. D2 ∪ D1 is a dominating set of G.

w(s)

(3)

Intuitively, we extract the smallest set of sen-
tences that are closely related to the query
from C2 to complete the partial dominating
set of G generated from D1. A graphical il-
lustration of update summarization using the
minimum dominating set is shown in Fig-
ure 1(c).

4.5 Comparative Summarization

Comparative document summarization aims
to summarize the diﬀerences among compara-
ble document groups. The summary produced
for each group should emphasize its diﬀerence
from other groups (Wang et al., 2009a).

We extend our method for update sum-
marization to generate the discriminant sum-
mary for each group of documents. Given N
groups of documents C1, C2, . . . , CN , we ﬁrst
generate the sentence graphs G1, G2, . . . , GN ,
respectively. To generate the summary for
Ci, 1 ≤ i ≤ N , we view Ci as the update
of all other groups. To extract a new sen-
tence, only the one connected with the largest
number of sentences which have no represen-
tatives in any groups will be extracted. We
denote the extracted set as the complemen-
tary dominating set, since for each group we
obtain a subset of vertices dominating those
are not dominated by the dominating sets of
other groups. To perform comparative sum-
marization, we ﬁrst extract the standard dom-
inating sets for G1, . . . , GN , respectively, de-
noted as D1, . . . , DN . Then we extract the
so-called complementary dominating set CDi
for Gi by continuing adding vertices in Gi to
ﬁnd the dominating set of ∪1≤j≤N Gj given
D1, . . . , Di−1, Di+1, . . . , DN . A graphical il-
lustration of comparative summarization is
shown in Figure 1(d).

989

Type of Summarization

#topics

#documents per topic

DUC04
Generic

NA
10

DUC05

DUC06

TAC08 A

Topic-focused

Topic-focused

Topic-focused

TAC08 B
Update

50

25-50

50
25

48
10

48
10

Summary length

665 bytes

250 words

250 words

100 words

100 words

Table 1: Brief description of the data set

5 Experiments

We have conducted experiments on all four
summarization tasks and our proposed meth-
ods based on the minimum dominating set
have outperformed many existing methods.
For the generic, topic-focused and update
summarization tasks, the experiments are per-
form the DUC data sets using ROUGE-2 and
ROUGE-SU (Lin and Hovy, 2003) as evalua-
tion measures. For comparative summariza-
tion, a case study as in (Wang et al., 2009a) is
performed. Table 1 shows the characteristics
of the data sets. We use DUC04 data set to
evaluate our method for generic summariza-
tion task and DUC05 and DUC06 data sets
for query-focused summarization task. The
data set for update summarization, (i.e. the
main task of TAC 2008 summarization track)
consists of 48 topics and 20 newswire articles
for each topic. The 20 articles are grouped
into two clusters. The task requires to pro-
duce 2 summaries, including the initial sum-
mary (TAC08 A) which is standard query-
focused summarization and the update sum-
mary (TAC08 B) under the assumption that
the reader has already read the ﬁrst 10 docu-
ments.

We apply a 5-fold cross-validation proce-
dure to choose the threshold λ used for gener-
ating the sentence graph in our method.

5.1 Generic Summarization

We implement the following widely used or
recent published methods for generic summa-
rization as the baseline systems to compare
with our proposed method (denoted as MDS).
(1) Centroid: The method applies MEAD al-
gorithm (Radev et al., 2004) to extract sen-
tences according to the following three pa-
centroid value, positional value,
rameters:
and ﬁrst-sentence overlap.
(2) LexPageR-

ank: The method ﬁrst constructs a sentence
connectivity graph based on cosine similarity
and then selects important sentences based on
the concept of eigenvector centrality (Erkan
and Radev, 2004).
(3) BSTM: A Bayesian
sentence-based topic model making use of
both the term-document and term-sentence
associations (Wang et al., 2009b).

Our method outperforms the simple Cen-
troid method and another graph-based Lex-
PageRank, and its performance is close to the
results of the Bayesian sentence-based topic
model and those of the best team in the DUC
competition. Note however that,
like clus-
tering or topic based methods, BSTM needs
the topic number as the input, which usually
varies by diﬀerent summarization tasks and is
hard to estimate.

5.2 Query-Focused Summarization

We compare our method (denoted as MWDS)
described in Section 4.3 with some recently
published systems.
(1) TMR (Tang et al.,
incorporates the query information
2009):
into the topic model, and uses topic based
score and term frequency to estimate the im-
portance of the sentences. (2) SNMF (Wang
et al., 2008):
calculates sentence-sentence
similarities by sentence-level semantic analy-
sis, clusters the sentences via symmetric non-
negative matrix factorization, and extracts
the sentences based on the clustering result.
(3) Wiki (Nastase, 2008): uses Wikipedia
as external knowledge to expand query and
builds the connection between the query and
the sentences in documents.

Table 3 presents the experimental compar-
ison of query-focused summarization on the
two datasets. From Table 3, we observe that
our method is comparable with these systems.
This is due to the good interpretation of the
summary extracted by our method, an ap-

990

DUC04

ROUGE-2

ROUGE-SU

DUC Best
Centroid

LexPageRank

BSTM
MDS

0.09216
0.07379
0.08572
0.09010
0.08934

0.13233
0.12511
0.13097
0.13218
0.13137

DUC05

DUC06

ROUGE-2

ROUGE-SU

ROUGE-2

ROUGE-SU

DUC Best

SNMF
TMR
Wiki

MWDS

0.0725
0.06043
0.07147
0.07074
0.07311

0.1316
0.12298
0.13038
0.13002
0.13061

0.09510
0.08549
0.09132
0.08091
0.09296

0.15470
0.13981
0.15037
0.14022
0.14797

Table 2: Results on generic summariza-
tion.

Table 3: Results on query-focused summariza-
tion.

proximate minimal dominating set of the sen-
tence graph. On DUC05, our method achieves
the best result; and on DUC06, our method
outperforms all other systems except the best
team in DUC. Note that our method based
on the minimum dominating set is much sim-
pler than other systems. Our method only
depends on the distance to the query and has
only one parameter (i.e., the threshold λ in
generating the sentence graph).

2
-
E
G
U
O
R

 0.095

 0.09

 0.085

 0.08

 0.075

 0.07

 0.065

DUC 06
DUC 05

 0.05

 0.1

 0.15

Similarity threshold λ

 0.2

 0.25

Figure 2: ROUGE-2 vs. threshold λ

We also conduct experiments to empirically
evaluate the sensitivity of the threshold λ.
Figure 2 shows the ROUGE-2 curve of our
MWDS method on the two datasets when λ
varies from 0.04 to 0.26. When λ is small,
edges fail to represent the similarity of the sen-
tences, while if λ is too large, the graph will
be sparse. As λ is approximately in the range
of 0.1 − 0.17, ROUGE-2 value becomes stable
and relatively high.

5.3 Update Summarization

Table 5 presents the experimental results on
update summarization.
‘TAC
Best” and “TAC Median” represent the best

In Table 5,

and median results from the participants of
TAC 2008 summarization track in the two
tasks respectively according to the TAC 2008
report (Dang and Owczarzak, 2008). As seen
from the results, the ROUGE scores of our
methods are higher than the median results.
The good results of the best team typically
come from the fact that they utilize advanced
natural language processing (NLP) techniques
to resolve pronouns and other anaphoric ex-
pressions. Although we can spend more eﬀorts
on the preprocessing or language processing
step, our goal here is to demonstrate the ef-
fectiveness of formalizing the update summa-
rization problem using the minimum dominat-
ing set and hence we do not utilize advanced
NLP techniques for preprocessing. The exper-
imental results demonstrate that our simple
update summarization method based on the
minimum dominating set can lead to compet-
itive performance for update summarization.

TAC08 A

TAC08 B

ROUGE-2

TAC Best

TAC Median

MWDS

0.1114
0.08123
0.09012

ROUGE-
SU
0.14298
0.11975
0.12094

ROUGE-2

0.10108
0.06927
0.08117

ROUGE-
SU
0.13669
0.11046
0.11728

Table 5: Results on update summarization.

5.4 Comparative Summarization

We use the top six largest clusters of doc-
uments from TDT2 corpora to compare the
summary generated by diﬀerent comparative
summarization methods. The topics of the six
document clusters are as follows: topic 1: Iraq
Issues; topic 2: Asia’s economic crisis; topic 3:
Lewinsky scandal; topic 4: Nagano Olympic
Games; topic 5: Nuclear Issues in Indian and
Pakistan; and topic 6: Jakarta Riot. From
each of the topics, 30 documents are extracted

991

Topic Complementary Dominating Set Discriminative Sentence Selection Dominating Set

1

2

3

4

5

6

··· U.S. Secretary of State
Madeleine Albright arrives to
consult on the stand-oﬀ between
the United Nations and Iraq.

currency,

Thailand’s
the
baht, dropped through a
key psychological level of ···
amid a regional sell-oﬀ sparked
by escalating social unrest in
Indonesia.
··· attorneys representing Pres-
ident Clinton and Monica
Lewinsky.

Eight women and six men were
named Saturday night as the
ﬁrst U.S. Olympic Snow-
board Team as their sport
gets set to make its debut in
Nagano, Japan.
U.S. oﬃcials have announced
sanctions Washington will im-
pose on India and Pakistan
for conducting nuclear tests.

···
remain in force around
Jakarta, and at the Parliament
building where thousands of
students staged a sit-in Tues-
day ··· .

the U.S. envoy to the United
·· ·
Nations, Bill Richardson,
play down China’s refusal to sup-
port threats of military force
against Iraq
Earlier, driven largely by the de-
clining yen, South Korea’s
stock market fell by ··· , while
the Nikkei 225 benchmark in-
dex dipped below 15,000 in the
morning ···
The following night Isikoﬀ ··· ,
where he directly followed the
recitation of the top-10 list: “Top
10 White House Jobs That
Sound Dirty.”
this tunnel is ﬁnland’s cross coun-
try version of tokyo’s alpine ski
dome, and olympic skiers ﬂock
from russia, ··· , france and aus-
tria this past summer to work out
the kinks ···
The sanctions would stop all for-
eign aid except for humanitarian
purposes, ban military sales to
India ···
“President Suharto has given
much to his country over the
past 30 years, raising Indone-
sia’s standing in the world ···

The United States and Britain
do not trust President Sad-
dam and wants cdotswarning
of serious consequences if Iraq
violates the accord.
In the fourth quarter,
IBM
Corp. earned $2.1 billion, up
3.4 percent from $2 billion a
year earlier.

In Washington, Ken Starr’s
grand jury continued its inves-
tigation of the Monica Lewin-
sky matter.

If the skiers the men’s super-
G and the women’s downhill
on Saturday, they will be back
on schedule.

And Pakistan’s prime min-
ister says his country will sign
the U.N.’s comprehensive
ban on nuclear tests if In-
dia does, too.
What were the students doing
at the time you were there, and
what was the reaction of the
students to the troops?

Table 4: A case study on comparative document summarization. Some unimportant words are skipped due to
the space limit. The bold font is used to annotate the phrases that are highly related with the topics, and italic
font is used to highlight the sentences that are not proper to be used in the summary.

randomly to produce a one-sentence summary.
For comparison purpose, we extract the sen-
tence with the maximal degree as the base-
line. Note that the baseline can be thought
as an approximation of the dominating set
using only one sentence. Table 4 shows the
summaries generated by our method (comple-
mentary dominating set (CDS)), discrimina-
tive sentence selection (DSS) (Wang et al.,
2009a) and the baseline method. Our CDS
method can extract discriminative sentences
for all the topics. DSS can extract discrimina-
tive sentences for all the topics except topic 4.
Note that the sentence extracted by DSS for
topic 4 may be discriminative from other top-
ics, but it is deviated from the topic Nagano
Olympic Games.
In addition, DSS tends to
select long sentences which should not be pre-
ferred for summarization purpose. The base-

line method may extract some general sen-
tences, such as the sentence for topic 2 and
topic 6 in Table 4.

6 Conclusion

In this paper, we propose a framework to
model the multi-document summarization us-
ing the minimum dominating set and show
that many well-known summarization tasks
can be formulated using the proposed frame-
work. The proposed framework leads to sim-
ple yet eﬀective summarization methods. Ex-
perimental results show that our proposed
methods achieve good performance on several
multi-document document tasks.

7 Acknowledgements

This work is supported by NSF grants IIS-
0549280 and HRD-0833093.

992

References

Chen, Y.P. and A.L. Liestman. 2002. Approximating
minimum size weakly-connected dominating sets for
clustering mobile ad hoc networks. In Proceedings
of International Symposium on Mobile Ad hoc Net-
working & Computing. ACM.

Chvatal, V. 1979. A greedy heuristic for the set-
covering problem. Mathematics of operations re-
search, 4(3):233–235.

Dang, H.T. and K Owczarzak. 2008. Overview of the
In Pro-

TAC 2008 Update Summarization Task.
ceedings of the Text Analysis Conference (TAC).

Dang, H.T. 2007. Overview of DUC 2007. In Docu-

ment Understanding Conference.

Daum´e III, H. and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the ACL-
COLING.

Erkan, G. and D.R. Radev. 2004. Lexpagerank: Pres-
tige in multi-document text summarization. In Pro-
ceedings of EMNLP.

Feige, U. 1998. A threshold of ln n for approximating
set cover. Journal of the ACM (JACM), 45(4):634–
652.

Goldstein,

J., V. Mittal,

and
M. Kantrowitz. 2000. Multi-document summariza-
tion by sentence extraction. In NAACL-ANLP 2000
Workshop on Automatic summarization.

J. Carbonell,

Guha, S. and S. Khuller. 1998. Approximation algo-
rithms for connected dominating sets. Algorithmica,
20(4):374–387.

Haghighi, A. and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of HLT-NAACL.

Mani, I. 2001. Automatic summarization. Computa-

tional Linguistics, 28(2).

Nastase, V.

2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of EMNLP.

Nenkova, A. and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.

Radev, D.R., H. Jing, M. Sty´s, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments.
Information Processing and Management,
40(6):919–938.

Raz, R. and S. Safra. 1997. A sub-constant error-
probability low-degree test, and a sub-constant
error-probability PCP characterization of NP.
In
Proceedings of STOC.

Saggion, H., K. Bontcheva, and H. Cunningham. 2003.
Robust generic and query-based summarisation. In
Proceedings of EACL.

Tang, J., L. Yao, and D. Chen. 2009. Multi-topic
based Query-oriented Summarization. In Proceed-
ings of SDM.

Thai, M.T., N. Zhang, R. Tiwari, and X. Xu. 2007.
On approximation algorithms of k-connected m-
dominating sets in disk graphs. Theoretical Com-
puter Science, 385(1-3):49–59.

Wan, X., J. Yang, and J. Xiao. 2007a. Manifold-
ranking based topic-focused multi-document sum-
marization. In Proceedings of IJCAI.

Wan, X., J. Yang, and J. Xiao. 2007b. Towards an
iterative reinforcement approach for simultaneous
document summarization and keyword extraction.
In Proceedings of ACL.

Han, B. and W. Jia. 2007. Clustering wireless ad
hoc networks with weakly connected dominating
set. Journal of Parallel and Distributed Computing,
67(6):727–737.

Wang, D., T. Li, S. Zhu, and C. Ding. 2008. Multi-
document summarization via sentence-level seman-
tic analysis and symmetric matrix factorization. In
Proceedings of SIGIR.

Johnson, D.S. 1973. Approximation algorithms for

combinatorial problems. In Proceedings of STOC.

Jurafsky, D. and J.H. Martin. 2008. Speech and lan-

guage processing. Prentice Hall New York.

Kann, V.

1992. On the approximability of NP-
complete optimization problems. PhD thesis, De-
partment of Numerical Analysis and Computing
Science, Royal Institute of Technology, Stockholm.

Lawrie, D., W.B. Croft, and A. Rosenberg.

2001.
Finding topic words for hierarchical summarization.
In Proceedings of SIGIR.

Lin, C.Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL.

Wang, D., S. Zhu, T. Li, and Y. Gong. 2009a. Com-
parative document summarization via discrimina-
tive sentence selection. In Proceeding of CIKM.

Wang, D., S. Zhu, T. Li, and Y. Gong.

2009b.
summarization using sentence-
In Proceedings of the ACL-

Multi-document
based topic models.
IJCNLP.

Wei, F., W. Li, Q. Lu, and Y. He. 2008. Query-
sensitive mutual reinforcement chain and its ap-
plication in query-oriented multi-document summa-
rization. In Proceedings of SIGIR.

Wu, J. and H. Li. 2001. A dominating-set-based rout-
ing scheme in ad hoc wireless networks. Telecom-
munication Systems, 18(1):13–36.

