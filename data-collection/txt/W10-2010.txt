










































Uncertainty Reduction as a Measure of Cognitive Processing Effort


Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 81–89,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

Uncertainty reduction as a measure of cognitive processing effort

Stefan L. Frank
University of Amsterdam

Amsterdam, The Netherlands
s.l.frank@uva.nl

Abstract

The amount of cognitive effort required to
process a word has been argued to depend
on the word’s effect on the uncertainty
about the incoming sentence, as quanti-
fied by the entropy over sentence probabil-
ities. The current paper tests this hypoth-
esis more thoroughly than has been done
before by using recurrent neural networks
for entropy-reduction estimation. A com-
parison between these estimates and word-
reading times shows that entropy reduc-
tion is positively related to processing ef-
fort, confirming the entropy-reduction hy-
pothesis. This effect is independent from
the effect of surprisal.

1 Introduction

In the field of computational psycholinguistics, a
currently popular approach is to account for read-
ing times on a sentence’s words by estimates of the
amount of information conveyed by these words.
Processing a word that conveys more information
is assumed to involve more cognitive effort, which
is reflected in the time required to read the word.

In this context, the most common formaliza-
tion of a word’s information content is its sur-
prisal (Hale, 2001; Levy, 2008). If word string
wt1 (short for w1, w2, . . . wt) is the sentence so
far and P (wt+1|wt1) the occurrence probability of
the next word wt+1, then that word’s surprisal is
defined as − log P (wt+1|wt1). It is well estab-
lished by now that word-reading times indeed cor-
relate positively with surprisal values as estimated
by any sufficiently accurate generative language
model (Boston et al., 2008; Demberg and Keller,
2008; Frank, 2009; Roark et al., 2009; Smith and
Levy, 2008).

A lesser known alternative operationalization of
a word’s information content is based on the un-
certainty about the rest of the sentence, quantified

by Hale (2003, 2006) as the entropy of the prob-
ability distribution over possible sentence struc-
tures. The reduction in entropy that results from
processing a word is taken to be the amount of
information conveyed by that word, and was ar-
gued by Hale to be predictive of word-reading
time. However, this entropy-reduction hypothesis
has not yet been comprehensively tested, possibly
because of the difficulty of computing the required
entropies. Although Hale (2006) shows how sen-
tence entropy can be computed given a PCFG, this
computation is not feasible when the grammar is
of realistic size.

Here, we empirically investigate the entropy-
reduction hypothesis more thoroughly than has
been done before, by using recurrent neural net-
works as language models. Since these networks
do not derive any structure, they provide estimates
of sentence entropy rather than sentence-structure
entropy. In practice, these two entropies will gen-
erally be similar: If the rest of the sentence is
highly uncertain, so is its structure. Sentence en-
tropy can therefore be viewed as a simplification
of structure entropy; one that is less theory depen-
dent since it does not rely on any particular gram-
mar. The distinction between entropy over sen-
tences and entropy over structures will simply be
ignored in the remainder of this paper.

Results show that, indeed, a significant fraction
of variance in reading-time data is accounted for
by entropy reduction, over and above surprisal.

2 Entropy and sentence processing

2.1 Sentence entropy

Let W be the set of words in the language and W i

the set of all word strings of length i. The set of
complete sentences, denoted S, contains all word
strings of any length (i.e.,

⋃

∞

i=0 W
i), except that a

special end-of-sentence marker </s> is attached
to the end of each string.

81



A generative language model defines a proba-
bility distribution over S. The entropy of this dis-
tribution is

H = −
∑

w
j
1
∈S

P (wj
1
) log P (wj

1
).

As words are processed one by one, the sen-
tence probabilities change. When the first t words
(i.e., the string wt1 ∈ W

t) of a sentence have been
processed, the entropy of the probability distribu-
tion over sentences is

H(t) = −
∑

w
j
1
∈S

P (wj
1
|wt1) log P (w

j
1
|wt1). (1)

In order to simplify later equations, we define
the function h(y|x) = −P (y|x) log P (y|x), such
that Eq. 1 becomes

H(t) =
∑

w
j
1
∈S

h(wj
1
|wt1).

If the first t words of wj
1

do not equal wt1 (or w
j
1

has fewer than t + 1 words),1 then P (wj
1
|wt1) = 0

so h(wj
1
|wt1) = 0. This means that, for computing

H(t), only the words from t + 1 onwards need to
be taken into account:

H(t) =
∑

w
j
t+1∈S

h(wjt+1|w
t
1).

The reduction in entropy due to processing the
next word, wt+1, is

∆H(t + 1) = H(t)−H(t + 1). (2)

Note that positive ∆H corresponds to a
decrease in entropy. According to Hale
(2006), the nonnegative reduction in entropy (i.e.,
max{0, ∆H}) reflects the cognitive effort in-
volved in processing wt+1 and should therefore be
predictive of reading time on that word.

2.2 Suffix entropy

Computing H(t) is computationally feasible only
when there are very few sentences in S, or when
the language can be described by a small grammar.
To estimate entropy in more realistic situations, an

1Since wj1 ends with < /s > and w
t
1 does not, the two

strings must be different. Consequently, if wj1 is t words long,
then P (wj1|w

t
1) = 0.

obvious solution is to look only at the next few
words instead of all complete continuations of wt1.

Let Sm be the subset of S containing all (and
only) sentences of length m or less, counting also
the </s> at the end of each sentence. Note that
this set includes the ‘empty sentence’ consisting
of only </s>. The set of length-m word strings
that do not end in </s> is Wm. Together, these
sets form Wm = Wm ∪ Sm, which contains all
the relevant strings for defining the entropy over
strings up to length m.2 After processing wt1, the
entropy over strings up to length t + n is:

Hn(t) =
∑

w
j
1
∈Wt+n

h(wj
1
|wt1) =

∑

w
j
t+1∈W

n

h(wjt+1|w
t
1).

It now seems straightforward to define suffix-
entropy reduction by analogy with sentence-
entropy reduction as expressed in Eq. 2: Simply
replace H by Hn to obtain

∆Hsufn (t + 1) = Hn(t)−Hn(t + 1). (3)

As indicated by its superscript label, ∆Hsufn
quantifies the reduction in uncertainty about the
upcoming n-word suffix. However, this is concep-
tually different from the original ∆H of Eq. 2,
which is the reduction in uncertainty about the
identity of the current sentence. The difference
becomes clear when we view the sentence proces-
sor’s task as that of selecting the correct element
from S. If this set of complete sentences is ap-
proximated by Wt+n, and the task is to select one
element from that set, an alternative definition of
suffix-entropy reduction arises:

∆Hsentn (t + 1)

=
∑

w
j
1
∈Wt+n

h(wj
1
|wt1) −

∑

w
j
1
∈Wt+n

h(wj
1
|wt+1

1
)

=
∑

w
j
t+1∈W

n

h(wjt+1|w
t
1) −

∑

w
j
t+2∈W

n−1

h(wjt+2|w
t+1
1

)

= Hn(t)−Hn−1(t + 1). (4)

The label ‘sent’ indicates that ∆Hsentn quantifies
the reduction in uncertainty about which sentence
forms the current input. This uncertainty is ap-
proximated by marginalizing over all word strings
longer than t + n.

It is easy to see that

lim
n→∞

∆Hsufn = lim
n→∞

∆Hsentn = ∆H,

2The probability of a string wm1 ∈ W
m is the summed

probability of all sentences with prefix wm1 .

82



so both approximations of entropy reduction ap-
propriately converge to ∆H in the limit. Nev-
ertheless, they formalize different quantities and
may well correspond to different cognitive factors.
If it is true that cognitive effort is predicted by
the reduction in uncertainty about the identity of
the incoming sentence, we should find that word-
reading times are predicted more accurately by
∆Hsentn than by ∆H

suf
n .

2.3 Relation to next-word entropy

In the extreme case of n = 1, Eq. 4 reduces to

∆Hsent1 (t + 1) = H1(t)−H0(t + 1) = H1(t),

so the reduction of entropy over the single next
word wt+1 equals the next-word entropy just be-
fore processing that word. Note that ∆Hsent1 (t+1)
is independent of the word at t + 1, making it a
severely impoverished measure of the uncertainty
reduction caused by that word. We would there-
fore expect reading times to be predicted more ac-
curately by ∆Hsentn with n > 1, and possibly even
by ∆Hsuf1 .

Roark et al. (2009) investigated the relation be-
tween H1(t + 1) and reading time on wt+1, and
found a significant positive effect: Larger next-
word entropy directly after processing wt+1 cor-
responded to longer reading time on that word.
This is of particular interest because H1(t + 1)
necessarily correlates negatively with entropy re-
duction ∆Hsentn (t + 1): If entropy is large after
wt+1, chances are that it did not reduce much
through processing of wt+1. Indeed, in our data
set, H1(t + 1) and ∆Hsentn (t + 1) correlate be-
tween r = −.29 and r = −.26 (for n = 2 to
n = 4) which is highly significantly (p ≈ 0) dif-
ferent from 0. Roark et al.’s finding of a positive
relation between H1(t + 1) and reading time on
wt+1 therefore seems to disconfirm the entropy-
reduction hypothesis.

3 Method

A set of language models was trained on a corpus
of POS tags of sentences. The advantage of using
POS tags rather than words is that their probabil-
ities can be estimated much more accurately and,
consequently, more accurate prediction of word-
reading time is possible (Demberg and Keller,
2008; Roark et al., 2009). Subsequent to training,
the models were made to generate estimates of sur-
prisal and entropy reductions ∆Hsufn and ∆H

sent
n

over a test corpus. These estimates were then com-
pared to reading times measured over the words
of the same test corpus. This section presents the
data sets that were used, language-model details,
and the evaluation metric.

3.1 Data

The models were trained on the POS tag se-
quences of the full WSJ corpus (Marcus et al.,
1993). They were evaluated on the POS-tagged
Dundee corpus (Kennedy and Pynte, 2005), which
has been used in several studies that investigate the
relation between word surprisal and reading time
(Demberg and Keller, 2008; Frank, 2009; Smith
and Levy, 2008). This 2 368-sentence (51 501
words) collection of British newspaper editorials
comes with eye-tracking data of 10 participants.
POS tags for the Dundee corpus were taken from
Frank (2009).

For each word and each participant, reading
time was defined as the total fixation time on that
word before any fixation on a later word of the
same sentence. Following Demberg and Keller
(2008), data points (i.e., word/participant pairs)
were removed if the word was not fixated, was
presented as the first or last on a line, contained
more than one capital letter or a non-letter (e.g.,
the apostrophe in a clitic), or was attached to punc-
tuation. Mainly due to the large number (over
46%) of nonfixations, 62.8% of data points were
removed, leaving 191 380 data points (between
16 469 and 21 770 per participant).

3.2 Language model

Entropy is more time consuming to compute than
surprisal, even for n = 1, because it requires es-
timates of the occurrence probabilities at t + 1 of
all word types, rather than just of the actual next
word. Moreover, the number of suffixes rises ex-
ponentially as suffix length n grows, and, conse-
quently, so does computation time.

Roark et al. (2009) used an incremental PCFG
parser to obtain H1 but this method rapidly be-
comes infeasible as n grows. Low-order Markov
models (e.g., a bigram model) are more efficient
and can be used for larger n but they do not form
particularly accurate language models. Moreover,
Markov models lack cognitive plausibility.

Here, Simple Recurrent Networks (SRNs) (El-
man, 1990) are used as language models. When
trained to predict the upcoming input in a word se-
quence, these networks can generate estimates of

83



P (wt+1|w
t
1) efficiently and relatively accurately.

They thereby allow to approximate sentence en-
tropy more closely than the incremental parsers
used in previous studies. Unlike Markov models,
SRNs have been claimed to form cognitively re-
alistic sentence-processing models (Christiansen
and MacDonald, 2009). Moreover, it has been
shown that SRN-based surprisal estimates can cor-
relate more strongly to reading times than surprisal
values estimated by a phrase-structure grammar
(Frank, 2009).

3.2.1 Network architecture and processing

The SRNs comprised three layers of units: the in-
put layer, the recurrent (hidden) layer, and the out-
put layer. Each input unit corresponds to one POS
tag, making 45 input units since there are 45 dif-
ferent POS tags in the WSJ corpus. The network’s
output units represent predictions of subsequent
inputs. The output layer also has one unit for each
POS tag, plus an extra unit that represents </s>,
that is, the absence of any further input. Hence,
there were 46 output units. The number of recur-
rent units was fairly arbitrarily set to 100.

As is common in these networks, the input layer
was fully connected to the recurrent layer, which
in turn was fully connected to the output layer.
Also, there were time-delayed connections from
the recurrent layer to itself. In addition, each re-
current and output unit received a bias input.

The vectors of recurrent- and output-layer ac-
tivations after processing wt1 are denoted arec(t)
and aout(t), respectively. At the beginning of each
sentence, arec(0) = 0.5.

The input vector aiin, representing POS tag i,
consists of zeros except for a single element (cor-
responding to i) that equals one. When input i is
processed, the recurrent layer’s state is updated ac-
cording to:

arec(t) = frec(Wrecarec(t− 1) + Wina
i
in + brec),

where matrices Win and Wrec contain the net-
work’s input and recurrent connection weights, re-
spectively; brec is the vector of recurrent-layer bi-
ases; and activation function frec(x) is the logistic
function f(x) = (1+e−x)−1 applied elementwise
to x. The new output vector is now given by

aout(t) = fout(Woutarec(t) + bout),

where Wout is the matrix of output connection
weights; bout the vector of output-layer biases; and
fout(x) the softmax function

fi,out(x1, . . . , x46) =
exi

∑

j e
xj

.

This function makes sure that aout sums to one
and can therefore be viewed as a probability dis-
tribution: The i-th element of aout(t) is the SRN’s
estimate of the probability that the i-th POS tag
will be the input at t + 1, or, in case i corresponds
to < /s >, the probability that the sentence ends
after t POS tags.

3.2.2 Network training

Ten SRNs, differing only in their random initial
connection weights and biases, were trained us-
ing the standard backpropagation algorithm. Each
string of WSJ POS tags was presented once, with
the sentences in random order. After each POS in-
put, connection weights were updated to minimize
the cross-entropy between the network outputs and
a 46-element vector that encoded the next input (or
marked the end of the sentence) by the correspond-
ing element having a value of one and all others
being zero.

3.3 Evaluation

3.3.1 Obtaining surprisal and entropy

Since aout(t) is basically the probability distribu-
tion P (wt+1|wt1), surprisal and H1 can be read off
directly. To obtain H2, H3, and H4, we use the
fact that

P (wt+nt+1 |w
t
1) =

n
∏

i=1

P (wt+i|w
t+i−1
1

). (5)

Surprisal and entropy estimates were averaged
over the ten SRNs. So, for each POS tag of the
Dundee corpus, there was one estimate of surprisal
and four of entropy (for n = 1 to n = 4).

Since Hn(t) approximates H(t) more closely
as n grows, it would be natural to expect a better
fit to reading times for larger n. On the other hand,
it goes without saying that Hn is only a very rough
measure of a reader’s actual uncertainty about the
upcoming n inputs, no matter how accurate the
language model that was used to compute these
entropies. Crucially, the correspondence between
Hn and the uncertainty experienced by a reader
will grow even weaker with larger n. This is ap-
parent from the fact that, as proven in the Ap-
pendix, Hn can be expressed in terms of H1 and
Hn−1:

Hn(t) = H1(t) + E(Hn−1(t + 1)),

84



1 2 3 4
0

0.25

0.5

suffix length n

co
rr

el
at

io
n 

w
ith

 s
ur

pr
is

al

∆H
n
suf

∆H
n
sent

Figure 1: Coefficient of correlation between es-
timates of surprisal and entropy reduction, as a
function of suffix length n.

where E(x) is the expected value of x. Obviously,
the expected value of Hn−1 is less appropriate as
an uncertainty measure than is Hn−1 itself. Hence,
Hn can be less accurate than Hn−1 as a quantifi-
cation of the actual cognitive uncertainty. For this
reason, we may expect larger n to result in worse
fit to reading-time data.3

3.3.2 Negative entropy reduction

Hale (2006) argued for nonnegative entropy re-
duction max{0, ∆H}, rather than ∆H itself, as
a measure of processing effort. For ∆Hsent, the
difference between the two is negligible because
only about 0.03% of entropy reductions are neg-
ative. As for ∆Hsuf, approximately 42% of val-
ues are negative so whether these are left out
makes quite a difference. Since preliminary ex-
periments showed that word-reading times are pre-
dicted much more accurately by ∆Hsuf than by
max{0, ∆Hsuf}, only ∆Hsuf and ∆Hsent were
used here, that is, negative values were included.

3.3.3 Relation between information measures

Both surprisal and entropy reduction can be taken
as measures for the amount of information con-
veyed by a word, so it is to be expected that they
are positively correlated. However, as shown in
Figure 1, this correlation is in fact quite weak,
ranging from .14 for ∆Hsuf4 to .38 for ∆H

sent
1 .

In contrast, ∆Hsufn and ∆H
sent
n correlate very

strongly to each other: The coefficients of correla-
tion range from .73 when n = 1 to .97 for n = 4.

3Not to mention the realistic possibility that the cognitive
sentence-processing system does not abide by the normative
chain rule expressed in Eq. 5.

0 4 8 12
10

−4

10
−3

10
−2

10
−1

10
0

Effect size

S
ig

ni
fic

an
ce

 (
p−

va
lu

e)

p = .05

3.84

Figure 2: Cumulative χ2 distribution with 1 de-
gree of freedom, plotting statistical significance
(p-value) as a function of effect size.

3.3.4 Fit to reading times

A generalized linear regression model for gamma-
distributed data was fitted to the reading times.4

This model contained several well-known predic-
tors of word-reading time: the number of letters
in the word, the word’s position in the sentence,
whether the next word was fixated, whether the
previous word was fixated, log of the word’s rel-
ative frequency, log of the word’s forward and
backward transitional probabilities,5 and surprisal
of the part-of-speech. Next, one set of entropy-
reduction estimates was added to the regression.
The effect size is the resulting decrease in the re-
gression model’s deviance, which is indicative of
the amount of variance in reading time accounted
for by those estimates of entropy reduction. Fig-
ure 2 shows how effect size is related to statis-
tical significance: A factor forms a significant
(p < .05) predictor of reading time if its effect
size is greater than 3.84.

4 Results and Discussion

4.1 Effect of entropy reduction

Figure 3 shows the effect sizes for both measures
of entropy reduction, and their relation to suffix
length n. All effects are in the correct direction,
that is, larger entropy reduction corresponds to
longer reading time. These results clearly support
the entropy-reduction hypothesis: A significant

4The reading times, which are approximately gamma dis-
tributed, were first normalized to make the scale parameters
of the gamma distributions the same across participants.

5These are, respectively, the relative frequency of the
word given the previous word, and its relative frequency
given the next word.

85



1 2 3 4
0

5

10

suffix length n

∆H
n 

ef
fe

ct
 s

iz
e

∆H
n
suf

∆H
n
sent

Figure 3: Size of the effect of ∆Hsufn and ∆H
sent
n

as a function of suffix length n.

fraction of variance in reading time is accounted
for by the entropy-reduction estimates ∆Hsentn ,
over and above what is explained by the other fac-
tors in the regression analysis, including surprisal.
Moreover, the effect of ∆Hsentn is larger than that
of ∆Hsufn , indicating that it is indeed uncertainty
about the identity of the current sentence, rather
than uncertainty about the upcoming input(s), that
matters for cognitive processing effort. Only at
n = 1 was the effect size of ∆Hsentn smaller than
that of ∆Hsufn , but it should be kept in mind that
∆Hsent1 is independent of the incoming word and
is therefore quite impoverished as a measure of the
effort involved in processing the word. Moreover,
the difference between ∆Hsent1 and ∆H

suf
1 is not

significant (p > .4), as determined by the boot-
strap method (Efron and Tibshirani, 1986). In con-
trast, the differences are significant when n > 1
(all p < .01), in spite of the high correlation be-
tween ∆Hsentn and ∆H

suf
n .

Another indication that cognitive processing ef-
fort is modeled more accurately by ∆Hsentn than by
∆Hsufn is that the effect size of ∆H

sent
n seems less

affected by n. Even though ∆H , the reduction in
entropy over complete sentences, is approximated
more closely as suffix length grows, increasing n
is strongly detrimental to the effect of ∆Hsufn : It
is no longer significant for n > 2. Presumably,
this can be (partly) attributed to the impoverished
relation between formal entropy and psychologi-
cal uncertainty, as explained in Section 3.3.1. In
any case, the effect of ∆Hsentn is more stable. Al-
though ∆Hsufn and ∆H

sent
n necessarily converge as

n →∞, the two effect sizes seem to diverge up to

1 2 3 4
5

10

15

20

ef
fe

ct
 s

iz
e

suffix length n

H
1

surprisal

∆H
n
sent

Figure 4: Effect size of entropy reduction
(∆Hsentn ), next-word entropy (H1), or surprisal,
over and above the other two predictors.

n = 3: The difference between the effect sizes
of ∆Hsentn and ∆H

suf
n is marginally significantly

(p < .07) larger for n = 3 than for n = 2.

4.2 Effects of other factors

It is also of interest that surprisal has a significant
effect over and above entropy reduction, in the cor-
rect (i.e., positive) direction. When surprisal esti-
mates are added to a regression model that already
contains ∆Hsentn , the effect size ranges from 8.7
for n = 1 to 13.9 for n = 4. This show that there
exist independent effects of surprisal and entropy
reduction on processing effort.

Be reminded from Section 2.3 that Roark et al.
(2009) found a positive relation between reading
time on wt+1 and H1(t + 1), the next-word en-
tropy after processing wt+1. When that value is
added as a predictor in the regression model that
already contains surprisal and entropy reduction
∆Hsentn , model fit greatly improves. In fact, as can
be seen from comparing Figures 3 and 4, the ef-
fect of ∆Hsentn is strengthened by including next-
word entropy in the regression model. Moreover,
each of the factors surprisal, entropy reduction,
and next-word entropy has a significant effect over
and above the other two. In all cases, these ef-
fects were in the positive direction. This confirms
Roark et al.’s finding and shows that it is in fact
compatible with the entropy-reduction hypothesis,
in contrast to what was suggested in Section 2.3.

86



5 Discussion and conclusion

The current results contribute to a growing body of
evidence that the amount of information conveyed
by a word in sentence context is indicative of the
amount of cognitive effort required for processing,
as can be observed from reading time on the word.
Several previous studies have shown that surprisal
can serve as a cognitively relevant measure for a
word’s information content. In contrast, the rele-
vance of entropy reduction as a cognitive measure
has not been investigated this thoroughly before.
Hale (2003; 2006) presents entropy-reduction ac-
counts of particular psycholinguistic phenomena,
but does not show that entropy reduction gener-
ally correlates with word-reading times. Roark et
al. (2009) presented data that could be taken as ev-
idence against the entropy-reduction hypothesis,
but the current paper showed that the next-word
entropy effect, found by Roark et al., is indepen-
dent of the entropy-reduction effect.

It is tempting to take the independent effects
of surprisal and entropy reduction as evidence
for two distinct cognitive representations or pro-
cesses, one related to surprisal, the other to en-
tropy reduction. However, it is very well possible
that these two information measures are merely
complementary formalizations of a single, cogni-
tively relevant notion of word information. Since
the quantitative results presented here provide no
evidence for either view, a more detailed qualita-
tive analysis is needed.

In addition, the relation between reading time
and the two measures of word information may
be further clarified by the development of mech-
anistic sentence-processing models. Both the sur-
prisal and entropy-reduction theories provide only
functional-level descriptions (Marr, 1982) of the
relation between information content and process-
ing effort, so the question remains which under-
lying mechanism is responsible for longer read-
ing times on words that convey more information.
That is, we are still without a model that pro-
poses, at Marr’s computational level, some spe-
cific sentence-processing mechanism that takes
longer to process a word that has higher surprisal
or leads to greater reduction in sentence entropy.
For surprisal, Levy (2008) makes a first step in
that direction by presenting a mechanistic account
of why surprisal would predict word-reading time:
If the state of the sentence-processing system is
viewed as a probability distribution over all possi-

ble interpretations of complete sentences, and pro-
cessing a word comes down to updating this distri-
bution to incorporate the new information, then the
word’s surprisal equals the Kullback-Leibler di-
vergence from the old distribution to the new. This
divergence is presumed to quantify the amount of
work (and, therefore, time) needed to update the
distribution. Likewise, Smith and Levy (2008) ex-
plain the surprisal effect in terms of a reader’s opti-
mal preparation to incoming input. When it comes
to entropy reduction, however, no reading-time
predicting mechanism has been proposed. Ideally,
of course, there should be a single computational-
level model that predicts the effects of both sur-
prisal and entropy reduction.

One recent model (Frank, 2010) shows that the
reading-time effects of both surprisal and entropy
reduction can indeed result from a single pro-
cessing mechanism. The model simulates sen-
tence comprehension as the incremental and dy-
namical update of a non-linguistic representation
of the state-of-affairs described by the sentence.
In this framework, surprisal and entropy reduc-
tion are defined with respect to a probabilistic
model of the world, rather than a model of the
language: The amount of information conveyed
by a word depends on what is asserted by the
sentence-so-far, and not on how the sentence’s
form matches the statistical patterns of the lan-
guage. As it turns out, word-processing times in
the sentence-comprehension model correlate pos-
itively with both surprisal and entropy reduction.
The model thereby forms a computational-level
account of the relation between reading time and
both measures of word information. According
to this account, the two information measures do
not correspond to two distinct cognitive processes.
Rather, there is one comprehension mechanism
that is responsible for the incremental revision of
a mental representation. Surprisal and entropy re-
duction form two complementary quantifications
of the extent of this revision.

Acknowledgments

The research presented here was supported by
grant 277-70-006 of the Netherlands Organization
for Scientific Research (NWO). I would like to
thank Rens Bod, Reut Tsarfaty, and two anony-
mous reviewers for their helpful comments.

87



References

M. F. Boston, J. Hale, U. Patil, R. Kliegl, and S. Va-
sishth. 2008. Parsing costs as predictors of read-
ing difficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Research,
2:1–12.

M. H. Christiansen and M. C. MacDonald. 2009. A
usage-based approach to recursion in sentence pro-
cessing. Language Learning, 59:129–164.

V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193–210.

B. Efron and R. Tibshirani. 1986. Bootstrap methods
for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1:54–75.

J. L. Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:179–211.

S. L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In N. A. Taatgen and H. van Rijn,
editors, Proceedings of the 31st Annual Conference
of the Cognitive Science Society, pages 1139–1144.
Austin, TX: Cognitive Science Society.

S. L. Frank. 2010. The role of world knowledge in
sentence comprehension: an information-theoretic
analysis and a connectionist simulation. Manuscript
in preparation.

J. Hale. 2001. A probabilistic Early parser as a psy-
cholinguistic model. In Proceedings of the sec-
ond conference of the North American chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159–166. Pittsburgh, PA: Association
for Computational Linguistics.

J. Hale. 2003. The information conveyed by words.
Journal of Psycholinguistic Research, 32:101–123.

J. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30:643–672.

A. Kennedy and J. Pynte. 2005. Parafoveal-on-foveal
effects in normal reading. Vision Research, 45:153–
168.

R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.

M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the Penn Treebank. Computational Linguis-
tics, 19:313–330.

D. Marr. 1982. Vision. San Francisco: W.H. Freeman
and Company.

B. Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top-down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 324–333. Associ-
ation for Computational Linguistics.

N. J. Smith and R. Levy. 2008. Optimal processing
times in reading: a formal model and empirical in-
vestigation. In B. C. Love, K. McRae, and V. M.
Sloutsky, editors, Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
595–600. Austin, TX: Cognitive Science Society.

88



Appendix

It is of some interest that Hn can be expressed in
terms of H1 and the expected value of Hn−1. First,
note that

h(wjt+1|w
t
1) = −P (w

j
t+1|w

t
1) log P (w

j
t+1|w

t
1)

= −P (wt+1|w
t
1)P (w

j
t+2|w

t+1
1

) log
(

P (wt+1|w
t
1)P (w

j
t+2|w

t+1
1

)
)

= P (wjt+2|w
t+1
1

)h(wt+1|w
t
1) + P (wt+1|w

t
1)h(w

j
t+2|w

t+1
1

).

For entropy Hn(t), this makes

Hn(t) =
∑

w
j
t+1∈W

n

h(wjt+1|w
t
1)

=
∑

w
j
t+1∈W

n

P (wjt+2|w
t+1
1

)h(wt+1|w
t
1) +

∑

w
j
t+1∈W

n

P (wt+1|w
t
1)h(w

j
t+2|w

t+1
1

)

=
∑

wt+1∈W1






h(wt+1|w

t
1)

∑

w
j
t+2∈W

n−1

P (wjt+2|w
t+1
1

)






+

∑

wt+1∈W1






P (wt+1|w

t
1)

∑

w
j
t+2∈W

n−1

h(wjt+2|w
t+1
1

)







=
∑

wt+1∈W1

h(wt+1|w
t
1) +

∑

wt+1∈W1

P (wt+1|w
t
1)Hn−1(t + 1)

= H1(t) + E(Hn−1(t + 1)).

89


