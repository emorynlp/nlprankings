










































Discriminative Training of 150 Million Translation Parameters and Its Application to Pruning


Proceedings of NAACL-HLT 2013, pages 335–341,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Discriminative Training of 150 Million Translation Parameters
and Its Application to Pruning

Hendra Setiawan and Bowen Zhou
IBM T.J. Watson Research Center

Yorktown Heights, NY 10598, USA
{hendras, zhou}@us.ibm.com

Abstract

Until recently, the application of discrimina-
tive training to log linear-based statistical ma-
chine translation has been limited to tuning
the weights of a limited number of features or
training features with a limited number of pa-
rameters. In this paper, we propose to scale
up discriminative training of (He and Deng,
2012) to train features with 150 million pa-
rameters, which is one order of magnitude
higher than previously published effort, and
to apply discriminative training to redistribute
probability mass that is lost due to model
pruning. The experimental results confirm the
effectiveness of our proposals on NIST MT06
set over a strong baseline.

1 Introduction

State-of-the-art statistical machine translation sys-
tems based on a log-linear framework are parame-
terized by {λ,Φ}, where the feature weights λ are
discriminatively trained (Och and Ney, 2002; Chi-
ang et al., 2008b; Simianer et al., 2012) by directly
optimizing them against a translation-oriented met-
ric such as BLEU. The feature parameters Φ can
be roughly divided into two categories: dense fea-
ture that measures the plausibility of each translation
rule from a particular aspect, e.g., the rule transla-
tion probabilities p(f |e) and p(e|f); and sparse fea-
ture that fires when certain phenomena is observed,
e.g., when a frequent word pair co-occured in a rule.
In contrast to λ, feature parameters in Φ are usually
modeled by generative models for dense features, or
by indicator functions for sparse ones. It is therefore
desirable to train the dense features for each rule in a
discriminative fashion to maximize some translation
criterion. The maximum expected BLEU training of
(He and Deng, 2012) is a recent effort towards this

direction, and in this paper, we extend their work
to a scaled-up task of discriminative training of the
features of a strong hierarchical phrase-based model
and confirm its effectiveness empirically.

In this work, we further consider the application
of discriminative training to pruned model. Various
pruning techniques (Johnson et al., 2007; Zens et al.,
2012; Eck et al., 2007; Lee et al., 2012; Tomeh et al.,
2011) have been proposed recently to filter transla-
tion rules. One common consequence of pruning is
that the probability distribution of many surviving
rules become deficient, i.e.

∑
f p(f |e) < 1. In prac-

tice, others have chosen either to leave the pruned
rules as it-is, or simply to re-normalize the proba-
bility mass by distributing the pruned mass to sur-
viving rules proportionally. We argue that both ap-
proaches are suboptimal, and propose a more prin-
cipled method to re-distribute the probability mass,
i.e. using discriminative training with some trans-
lation criterion. Our experimental results demon-
strate that at various pruning levels, our approach
improves performance consistently. Particularly at
the level of 50% of rules being pruned, the discrimi-
natively trained models performs better than the un-
pruned baseline grammar. This shows that discrim-
inative training makes it possible to achieve smaller
models that perform comparably or even better than
the baseline model.

Our contributions in this paper are two-folded:
First of all, we scale up the maximum expected
BLEU training proposed in (He and Deng, 2012) in
a number of ways including using 1) a hierarchical
phrase-based model, 2) a richer feature set, and 3) a
larger training set with a much larger parameter set,
resulting in more than 150 million parameters in the
model being updated, which is one order magnitude
higher than the phrase-based model reported in (He
and Deng, 2012). We are able to show a reasonable

335



improvement over this strong baseline. Secondly,
we combine discriminative training with pruning
techniques to reestimate parameters of pruned gram-
mar. Our approach is shown to alleviate the loss due
to pruning, and sometimes can even outperform the
baseline unpruned grammar.

2 Discriminative Training of Φ

Given the entire training data {Fn, En}Nn=1, and cur-
rent parameterization {λ,Φ}, we decode the source
side of training data Fn to produce hypothesis
{Ên}Nn=1. Our goal is to update Φ towards Φ′ that
maximizes the expected BLEU scores of the entire
training data given the current λ:

U(Φ)=
∑

∀Ê1...ÊN

P̃Φ(Ê1..ÊN |F1..FN )B(Ê1..ÊN ) (1)

where B(Ê1...ÊN ) is the BLEU score of the con-
catenated hypothesis of the entire training data, fol-
lowing (He and Deng, 2012).

Eq. 1 summarizes over all possible combina-
tions of Ê1...ÊN , which is intractable. Hence we
make two simplifying approximations as follows.
First, let the k-best hypotheses of the n-th sen-
tence, Ên =

{
Ê1n, ..., Ê

K
n

}
, approximate all its

possible translation. In other words, we assume
that

∑K
k=1 P̃ (Ê

k
n|Fn) = 1, ∀n. Second, let the

sum of sentence-level BLEU approximate the cor-
pus BLEU. We note that corpus BLEU is not strictly
decomposable (Chiang et al., 2008a), however, as
the training data’s size N gets big as in our case, we
expect them to become more positively correlated.

Under these assumptions and the fact that each
sentence is decoded independently, Eq. 1 can be al-
gebraically simplified into:

U(Φ) =
N∑
n=1

K∑
k=1

PΦ(Ê
k
n|Fn)B(Êkn) (2)

where PΦ(Êkn|Fn)=P̃Φ(Êkn|Fn)/
∑
∀k P̃Φ(Ê

k
n|Fn).

We detail the process in the Appendix.
To further simplify the problem and relate it with

model pruning, we consider to update a subset of
θ ⊂ Φ while keeping other parameterization of Φ
unchanged, where θ = {θij = p(ej |fi)} denotes our
parameter set that satisfies

∑
j θij = 1 and θij ≥ 0.

In experiments, we also consider {θji = p(fi|ej)}.

To alleviate overfitting, we introduce KL-distance
based reguralization as in (He and Deng, 2012). We
thus arrive at the following objective function:

O(θ) = log(U(θ))− τ ·KL(θ||θ0)/N (3)

where τ controls the regularization term’s contribu-
tion, and θ0 represents a prior parameter set, e.g.,
from the conventional maximum likelihood training.

The optimization algorithm is based on the Ex-
tended Baum Welch (EBW) (Gopalakrishnan et al.,
1991) as derived by (He and Deng, 2012). The final
update rule is as follow:

θ′ij =

∑
n

∑
k γ(n, k, i, j) + U(θ)τθ

0
ij/λ+Diθij∑

n

∑
k

∑
j γ(n, k, i, j) + U(θ)τ/λ+Di

(4)

where θ′ij is the updated parameter, γ(n, k, i, j) =
Pθ(Ê

k
n|Fn){B(Êkn) − Un(θ)}

∑
l 1(fn,k,l =

fi, en,k,l = ej); Un(θ) =
∑K

k=1 Pθ(Ê
k
n|Fn)B(Êkn);

Di =
∑

n,k,j max(0,−γ(n, k, i, j)) and λ is the
current feature’s weight.

3 DT is Beneficial for Pruning

Pruning is often a key part in deploying large-scale
SMT systems for many reasons, such as for reduc-
ing runtime memory footprint and for efficiency.
Many pruning techniques have been proposed to as-
sess translation rules and filter rules out if they are
less plausible than others. While different pruning
techniques may use different criterion, they all as-
sume that pruning does not affect the feature func-
tion values of the surviving rules. This assumption
may be suboptimal for some feature functions that
have probabilistic sense since pruning will remove
a portion of the probability mass that is previously
assigned to the pruned rules. To be concrete, for the
rule translation probabilities θij under consideration,
the constraint

∑
j θij = 1 will not hold for all source

rules i after pruning. Previous works typically left
the probability mass as it-is, or simply renormalize
the pruned mass, i.e. θ̄ij = θij/

∑
j θij .

We argue that applying the DT techniques to a
pruned grammar, as described in Sec. 2, provides
a more principled method to redistribute the mass,
i.e. by quantizing how each rule contributes to the
expected BLEU score in comparison to other com-
peting rules. To empirically verify this, we consider

336



the significance test based pruning (Johnson et al.,
2007), though our general idea can be appllied to
any pruning techniques. For our experiments, we
use the significance pruning tool that is available as
part of Moses decoder package (Koehn et al., 2007).

4 Experiments

Our experiments are designed to serve two goals:
1) to show the performance of discriminative train-
ing of feature parameters θ in a large-scale task;
and 2) to show the effectiveness of DT when ap-
plied to pruned grammar. Our baseline system is a
state-of-the-art hierarchical phrase-based system as
described in (Zhou et al., 2008), trained on six mil-
lion parallel sentences corpora that are available to
the DARPA BOLT Chinese-English task. The train-
ing corpora includes a mixed genre of news wire,
broadcast news, web-blog and comes from various
sources such as LDC, HK Hansard and UN data.

In total, there are 50 dense features in our trans-
lation system. In addition to the standard features
which include the rule translation probabilities, we
incorporate features that are found useful for devel-
oping a state-of-the-art baseline, e.g. provenance-
based lexical features (Chiang et al., 2011). We use
a large 6-gram language model, which we train on a
10 billion words monolingual corpus, including the
English side of our parallel corpora plus other cor-
pora such as Gigaword (LDC2011T07) and Google
News. To prevent possible over-fitting, we only kept
the rules that have at most three terminal words (plus
up to two nonterminals) on the source side, resulting
in a grammar with 167 million rules.

Our discriminative training procedure includes
updating both λ and θ, and we follow (He and Deng,
2012) to optimize them in an alternate manner. That
is, when we optimize θ via EBW, we keep λ fixed
and when we optimize λ, we keep λ fixed. We use
PRO (Hopkins and May, 2011) to tune λ.

For discriminative training of θ, we use a subset
of 550 thousands of parallel sentences selected from
the entire training data, mainly to allow for faster ex-
perimental cycle; they mainly come from news and
web-blog domains. For each sentence of this subset,
we generate 500-best of unique hypotheses using the
baseline model. The 1-best and the oracle BLEU
scores for this subset are 40.19 and 47.06 respec-

tively. Following (He and Deng, 2012), we focus on
discriminative training of p(f |e) and p(e|f), which
in practice affects around 150 million of parameters;
hence the title.

For the tuning and development sets, we set
aside 1275 and 1239 sentences respectively from
LDC2010E30 corpus. The tune set is used by PRO
for tuning λ while the dev set is used to decide the
best DT model. As for the blind test set, we re-
port the performance on the NIST MT06 evaluation
set, which consists of 1644 sentences from news and
web-blog domains. Our baseline system’s perfor-
mance on MT06 is 39.91 which is among the best
number ever published so far in the community.

Table 1 compares the key components of our
baseline system with that of (He and Deng, 2012).
As shown, we are working with a stronger system
than (He and Deng, 2012), especially in terms of the
number of parameters under consideration |θ|.

He&Deng(2012) This paper
Model phrase-based hierarchical
n-gram lm 3-gram 6-gram
# features 10 50
Max terminal 4 3
|θ| 9.2 M 150M
# training data 750K 6M
N for DT 750K 550K
max K-best 100 500

Table 1: Our system compares to He&Deng’s (2012).

4.1 DT of 150 Million Parameters
To ensure the correctness of our implementation,
we show in Fig 2, the first five EBW updates with
τ = 0.10. As shown, the utility function log(U(θ))
increases monotonically but is countered by the KL
term, resulting in a smaller but consistent increase
of the objective function O(θ). This monotonically-
increasing trend of the objective function confirms
the correctness of our implementation since EBW
algorithm is a bound-based technique that ensures
growth transformations between updates.

We then explore the optimal setting for τ which
controls the contribution of the regularization term.
Specifically, we perform grid search, exploring val-
ues of τ from 0.1 to 0.75. For each τ , we run several
iterations of discriminative training where each it-
eration involves one simultaneous update of p(f |e)

337



and p(e|f) according to Eq. 4, followed by one up-
date of λ via PRO (as in (He and Deng, 2012)). In
total, we run 10 such iterations for each τ .

tau=0.01N tau=0.05N tau=0.10N tau=0.25N tau=0.50N tau=0.75N tau=0.10
0 32.22 32.22 32.22 32.22 32.22 32.22 0 32.22
1 32.33 32.24 32.39 32.42 32.5 32.34 1 32.39
2 32.39 32.34 32.63 32.45 32.41 32.33 2 32.63
3 32.37 32.29 32.54 32.32 32.24 32.45 3 32.54
4 32.35 32.18 32.41 32.45 32.41 32.38 4 32.41
5 32.38 32.21 32.45 32.62 32.31 32.08 5 32.45
6 32.26 32.27 32.68 32.45 32.26 32.28 6 32.68
7 32.17 32.15 32.45 32.54 32.37 32.15 7 32.45
8 31.93 32.26 32.29 32.56 32.25 32.31 8 32.29
9 32.1 32.36 32.25 32.33 32.23 32.54 9 32.25

10 32.1 32.29 32.2 32.42 32.29 32.31 10 32.2
32.39 32.36 32.68 32.62 32.5 32.54

33.09
0.01 0.05 0.1 0.25 0.5 0.75

0.01 (2) 0.05 (9) 0.10 (6) 0.25 (5) 0.50 (1) 0.75 (9)
32.39 32.36 32.68 32.62 32.5 32.54
32.22 32.22 32.22 32.22 32.22 32.22

32.2$

32.3$

32.4$

32.5$

32.6$

32.7$

32.8$

0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$

tau=0.10$

32.39$
32.36$

32.68$

32.62$

32.5$
32.54$

32.22$

32.2$

32.25$

32.3$

32.35$

32.4$

32.45$

32.5$

32.55$

32.6$

32.65$

32.7$

0.01$(2)$ 0.05$(9)$ 0.10$(6)$ 0.25$(5)$ 0.50$(1)$ 0.75$(9)$

DT$
baseline$

τ = 0.01N

Figure 1: The dev set’s BLEU score (y-axis) on different
setting of τ (x-axis). The grey line indicates the baseline
performance on dev set. The number in bracket on the x-
axis indicates the iteration at which the score is obtained.

Across different τ , we find that the first iteration
provides most of the gain while the subsequent iter-
ations provide additional, smaller gain with occas-
sional performance degradation; thus the translation
performance is not always monotonically increasing
over iteration. We report the best score of each τ in
Fig. 1 and at which iteration that score is produced.
As shown in Fig. 1, all settings of τ improve over the
baseline and τ = 0.10 gives the highest gain of 0.45
BLEU score. This improvement is in the same ball-
park as in (He and Deng, 2012) though on a scaled-
up task. We next decode the MT06 using the best
model (i.e. τ = 0.10 at 6-th iteration) observed on
the dev set, and obtained 40.33 BLEU with an im-
provement of around 0.4 BLEU point. We see this
result as confirming the effectiveness of discrimi-
native training but on a larger-scale task, adding to
what was reported by (He and Deng, 2012).

4.2 DT for Significance Pruning

Next, we show the contribution of discriminative
training for model pruning. To do so, we prune the
translation grammar so that its size becomes 50%,
25%, 10% of the original grammar. Respectively,
we delete rules whose significance value below 15,
50 and 500. Table 2 compares the statistics of the
pruned grammars and the unpruned one. In particu-
lar, columns 4 and 5 show the total averaged prob-
ability mass of the remaining rules. This statistics
provides some indication of how deficient the fea-

Figure 2: Objective function (O(θ′)), the regularization
term (KL(θ′)) and the unregularized objective function
(log(U(θ′))) for five EBW updates of updating p(ej |fi)

tures are after pruning. As shown, the total averaged
probability mass after pruning is below 100% and
even lower for the more aggressive pruning.

To show that the deficiency is suboptimal, we con-
siders two baseline systems: models with/without
mass renormalization. We tune a new λ for each
model and use the new λ to decode the dev and test
sets. The results are shown in columns 6 and 9 of
Table 2 where we show the results for the unnor-
malized model in the brackets following the results
for the re-normalized model. The results show that
pruning degrades the performances and that naively
re-normalizing the model provides no significant
changes in performance. Subsequently, we will fo-
cus on the normalized models as the baseline as they
represents the starting points of our EBW iteration.

Next, we run discriminative training that would
reassign the probability mass to the surviving rules.
First, we normalize p(f |e) and p(e|f), so that they
satisfy the sum to one constraint required by the al-
gorithm. Then, we run discriminative training on
these pruned grammars using τ = 0.10 (i.e. the
setting that gives the best performance for the un-
pruned grammar as discussed in Section 4.1). We
report the results in columns 7 and 9 for the dev and
test sets respectively, as well as the gain over the
baseline system in columns 8 and 10.

As shown in Table 2, DT provides a nice im-
provement over the baseline model of no mass re-
assignment. For all pruning levels, DT can compen-
sate the loss associated with pruning. In particular,
at 50% level of pruning, there is a loss about 0.4

338



size |f | |e| p(∗|e) p(∗|f) dev-set test-set (MT06)
(%) (M) (M) baseline (un) DT (iter) gain baseline (un) DT gain
100 59 50 1.00 1.00 32.22(32.08) 32.68 (6) +0.44 39.91 (39.71) 40.33 +0.42
50 38 35 0.92 0.94 31.84 (32.02) 32.31 (6) +0.57 39.61(39.72) 40.08 +0.47
25 14 14 0.87 0.91 31.39 (31.43) 31.68 (2) +0.29 39.23 (39.17) 39.43 +0.20
10 4 3 0.77 0.84 27.27 (27.10) 27.82 (2) +0.55 36.01 (36.04) 36.43 +0.42

Table 2: The statistics of grammars pruned at various level (column 1), including the number of unique source and
target phrases (columns 2 & 3), total probability mass of the remaining rules for p(f |e) and p(e|f) (columns 4 & 5),
the performance of the pruned model before and after discriminative training as well as the gain on the dev and the
test sets (columns 6 to 11). The iteration at which DT gives the best dev set is indicated by the number enclosed by
bracket in column 7. The baseline performance is in italics, followed by a number in the bracket which refers to the
performance of using unnormalized model. The above-the-baseline performances are in bold.

BLEU point after pruning. With the DT on pruned
model, all pruning losses are reclaimed and the new
pruned model is even better than the unpruned orig-
inal model. This empirical result shows that leaving
probability mass unassigned after pruning is sub-
optimal and that discriminative training provides a
principled way to redistribute the mass.

5 Conclusion

In this paper, we first extend the maximum expected
BLEU training of (He and Deng, 2012) to train
two features of a state-of-the-art hierarchical phrase-
based system, namely: p(f |e) and p(e|f). Com-
pared to (He and Deng, 2012), we apply the algo-
rithm to a strong baseline that is trained on a big-
ger parallel corpora and comes with a richer feature
set. The number of parameters under consideration
amounts to 150 million. Our experiments show that
discriminative training these two features (out of 50)
gives around 0.40 BLEU point improvement, which
is consistent with the conclusion of (He and Deng,
2012) but in a much larger-scale system.

Furthermore, we apply the algorithm to redis-
tribute the probability mass of p(f |e) and p(e|f) that
is commonly lost due to conventional model prun-
ing. Previous techniques either leave the probability
mass as it is or distribute it proportionally among the
surviving rules. We show that our proposal of us-
ing discriminative training to redistribute the mass
empirically performs better, demonstrating the ef-
fectiveness of our proposal.

Appendix

We describe the process to simplify Eq. 1 to Eq. 2,
which is omitted in (He and Deng, 2012). For con-
ciseness, we drop the conditions and write P (Êi|Fi)
as P (Êi). We write Eq. 1 again below as Eq. 5 .∑

∀Ê1...ÊN

N∏
i=1

P (Êi|Fi) ·
N∑
i=1

B(Êi) (5)

We first focus on the first sentence E1/F1 and ex-
pand the related terms from the equation as follow:∑
∀Ê1

∑
∀Ê2...ÊN

P (Ê1)
N∏
i=2

P (Êi).

[
B(Ê1)+

N∑
i=2

B(Êi)

]

Expanding the inner summation, we arrive at:∑
∀Ê1

P (Ê1)B(Ê1)
∑

∀Ê2...ÊN

N∏
i=2

P (Êi) +

∑
∀Ê1

P (Ê1)
∑

∀Ê2...ÊN

N∏
i=2

P (Êi)

N∑
i=2

B(Êi)

Due to the that
∑K

k=1 P̃ (Ê
K
n |Fn) = 1, we can

equate
∑
∀Ê2...ÊN

∏N
i=2 P (Êi) and

∑
∀Ê1 P (Ê1) to

1. Thus, we arrive at:

∑
∀Ê1

P (Ê1)B(Ê1) +
∑

∀Ê2...ÊN

N∏
i=2

P (Êi)
N∑
i=2

B(Êi)

Notice that the second term has the same form
as Eq. 5 except that the starting index starts from
the second sentence. The same process can be per-
formed and at the end, thus we can arrive at Eq. 2.

339



Acknowledgments

We thank Xiadong He for helpful discussions. We
would like to acknowledge the support of DARPA
under Grant HR0011-12-C-0015 for funding part of
this work. The views, opinions, and/or findings con-
tained in this article/presentation are those of the au-
thor/presenter and should not be interpreted as rep-
resenting the official views or policies, either ex-
pressed or implied, of the DARPA.

References
David Chiang, Steve DeNeefe, Yee Seng Chan, and

Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient al-
gorithms. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 610–619, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.

David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224–233, Honolulu, Hawaii,
October. Association for Computational Linguistics.

David Chiang, Steve DeNeefe, and Michael Pust. 2011.
Two easy improvements to lexical weighting. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 455–460, Portland, Oregon, USA,
June. Association for Computational Linguistics.

Matthias Eck, Stephan Vogel, and Alex Waibel. 2007.
Translation model pruning via usage statistics for sta-
tistical machine translation. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Companion Volume, Short Papers,
pages 21–24, Rochester, New York, April. Association
for Computational Linguistics.

P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur Nádas,
and David Nahamoo. 1991. An inequality for ratio-
nal functions with applications to some statistical esti-
mation problems. IEEE Transactions on Information
Theory, 37(1):107–113.

Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 292–301, Jeju Island, Korea, July.
Association for Computational Linguistics.

Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-

ical Methods in Natural Language Processing, pages
1352–1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.

Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967–
975, Prague, Czech Republic, June. Association for
Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.

Seung-Wook Lee, Dongdong Zhang, Mu Li, Ming Zhou,
and Hae-Chang Rim. 2012. Translation model size
reduction for hierarchical phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 291–295, Jeju Is-
land, Korea, July. Association for Computational Lin-
guistics.

Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295–302, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.

Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11–21, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.

N. Tomeh, M. Turchi, G. Wisniewski, A. Allauzen, and
F. Yvon. 2011. How good are your phrases? assess-
ing phrase quality with single class classification. In
Proceedings of the International Workshop on Spoken
Language Translation, pages 261–268.

Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,

340



pages 972–983, Jeju Island, Korea, July. Association
for Computational Linguistics.

Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In Proceedings of the ACL-
08: HLT Second Workshop on Syntax and Structure in
Statistical Translation (SSST-2), pages 19–27, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

341


