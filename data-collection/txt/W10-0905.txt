










































Mining Script-Like Structures from the Web


Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34–42,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Mining Script-Like Structures from the Web

Niels Kasch

Department of Computer Science

and Electrical Engineering

University of Maryland,

Baltimore County

Baltimore, MD 21250, USA

nkasch1@umbc.edu

Tim Oates

Department of Computer Science

and Electrical Engineering

University of Maryland,

Baltimore County

Baltimore, MD 21250, USA

oates@cs.umbc.edu

Abstract

This paper presents preliminary work to ex-

tract script-like structures, called events and

event sets, from collections of web docu-

ments. Our approach, contrary to existing

methods, is topic-driven in the sense that event

sets are extracted for a specified topic. We

introduce an iterative system architecture and

present methods to reduce noise problems

with web corpora. Preliminary results show

that LSA-based event relatedness yields bet-

ter event sets from web corpora than previous

methods.

1 Introduction

In this paper, we present a preliminary system to ex-

tract script-like structures in a goal-directed fashion

from the web. For language processing purposes,

humans appear to have knowledge of many stylized

situations, such as what typically happens when go-

ing to a restaurant or riding a bus. This knowledge

is shared among a large part of the population and

lets us predict the next step in a sequence in a fa-

miliar situation, allows us to act appropriately, and

enables us to omit details when communicating with

others while ensuring common ground is maintained

between communication partners. It seems we have

such knowledge for a vast variety of situations and

scenarios, and thus natural language processing sys-

tems need access to equivalent information if they

are to understand, converse, or reason about these

situations.

These knowledge structures, comparable to

scripts (Schank and Abelson, 1977) or narrative

chains (Chambers and Jurafsky, 2008), describe typ-

ical sequences of events in a particular context.

Given the number of potential scripts, their develop-

ment by hand becomes a resource intensive process.

In the past, some work has been devoted to automat-

ically construct script-like structures from compiled

corpora (Fujiki et al., 2003) (Chambers and Juraf-

sky, 2008). Such approaches, however, only produce

scripts that are directly related to the topics repre-

sented in such corpora. Therefore, newspaper cor-

pora (e.g., the Reuters Corpus) are likely to contain

scripts relating to government, crime and financials,

but neglect other subject areas. We present a system

that extracts scripts from the web and removes the

constraints of specialized corpora and domain lim-

itations. We hope our iterative technique will pro-

duce scripts for a vast variety of topics, and has the

potential to produce more complete scripts.

Another drawback of existing approaches lies

with their passive extraction mechanisms. A

user/system does not have the ability to obtain

scripts for a specific topic, but rather is bound to

obtain the most prevalent scripts for the underlying

corpus. Furthermore, scripts derived in this fashion

lack an absolute labeling or description of their top-

ics. This can be problematic when a user/system is

looking for specific scripts to apply to a given sce-

nario. In contrast, our system facilitates the search

for scripts given a topic. This goal oriented approach

is superior in that (1) scripts are labeled by a de-

scriptive topic and can be organized, accessed and

searched by topic, (2) scripts can be constructed by

topic and are not reliant on existing and potentially

limiting corpora and (3) script coarseness and de-

34



tail can be be controlled through iterative script im-

provement and augmentation based on additional in-

formation retrieved from the web.

2 Related Work

Lin and Pantel describe an unsupervised algorithm

for discovering inference rules from text (DIRT)

(Lin and Pantel, 2001a) (Lin and Pantel, 2001b). In-

ference rules are derived from paths in dependency

trees. If two paths occur in similar contexts (i.e., the

words/fillers of their slots are distributionally simi-

lar) then the meaning of the paths is similar. Ver-

bOcean (Chklovski and Pantel, 2004) is a resource

of strongly associated verb pairs and their semantic

relationship. Verbs are considered strongly associ-

ated if DIRT deems dependency paths, which con-

tain the verbs, as being similar. A form of mutual

information between verb pairs and lexico-syntactic

patterns indicative of semantic relationship types is

used to categorize the verb pairs according to sim-

ilarity, strength, antonymy, happens-before and en-

ablement.

(Fujiki et al., 2003) describe a method to ex-

tract script knowledge from the first paragraph of

Japanese newspaper articles. The first paragraph of

such articles is assumed to narrate its contents in

temporal order. This circumvents the need to order

events as they can be extracted in presumed order.

Events are defined in terms of actions, where an ac-

tion consists of a tuple composed of a transitive verb

and its subject and object. The method’s goal is to

find sequences of pairs of actions by (1) using co-

occurrence of subjects and objects in neighboring

sentences, (2) locating sentences where two verbs

share the same subject and (3) identifying sentences

where two verbs share the same object. Once pairs

of events are extracted, their subject and objects are

generalized into semantic entities similar to seman-

tic roles.

(Chambers and Jurafsky, 2008) attempt to identify

narrative chains in newspaper corpora. They utilize

the notion of protagonist overlap or verbs sharing

co-referring arguments to establish semantic coher-

ence in a story. Co-referring arguments are taken

as indicators of a common discourse structure. This

assumption is used to find pairwise events in an un-

supervised fashion. Point wise mutual information

(PMI) is used to indicate the relatedness between

event pairs. A global narrative score, aiming to max-

imize the PMI of a set of events is utilized to gener-

ate a ranked list of events most likely to participate

in the narrative chain. Temporal order is established

by labeling events with temporal attributes and us-

ing those labels, along with other linguistic features,

to classify the relationship (before or other) between

two events.

For the purposes of our work, finding documents

related to a term and identifying similar terms is an

important step in the script creation process. (Deer-

wester et al., 1990) describe Latent Semantic Anal-

ysis/Indexing (LSA) as a technique superior to term

matching document retrieval. LSA aims to facili-

tate document retrieval based on the conceptual con-

tent of documents, thereby avoiding problems with

synonomy and polysemy of individual search terms

(or in documents). LSA employs singular-value-

decomposition (SVD) of a term-by-document ma-

trix to construct a “semantic space” in which related

documents and terms are clustered together.

3 Approach

In this work we aim to extract scripts from the

web. We define a script as a collection of typi-

cally related events that participate in temporal re-

lationships amongst each other. For example, e1
happens-before e2 denotes a relationship such

that event e1 occurs prior to event e2. An event is de-

fined as a tuple consisting of a verb, a grammatical

function and a set of arguments (i.e., words) which

act out the grammatical function in relation to the

verb. Figure 1 shows the structure of events.

e [verb, grammatical function, {set of arguments}]

Figure 1: The structure of an event. An event is a tuple

consisting of a verb, a grammatical function and a set of

arguments (i.e., instances of words filling the grammati-

cal function).

The set of arguments represents actual instances

found during the script extraction process. Figure 2

illustrates an incomplete script for the topic eating

at a restaurant.

3.1 The Task

We define the task of goal driven script extraction as:

(1) given a topic, compile a “relevant” corpus of doc-

35



e1 [ enter, nsubj, {customer, John}) ]
e2 [ enter, dobj, {restaurant} ]
e3 [ order, nsubj, {customer} ]
e4 [ order, dobj, {food} ]
e5 [ eat, nsubj, {customer} ]
e6 [ eat, dobj, {food} ]
e7 [ pay, nsubj, {customer} ]
e8 [ pay, dobj, {bill} ]
e9 [ leave, nsubj, {customer} ]
e10 [ leave, dobj, {restaurant} ]

Temporal Ordering = e1 < e2 < e3 < e4
e4 < e5 < e6 < e7 < e8 < e9 < e10

Figure 2: An excerpt of a script for the topic eating at

a restaurant. The script denotes the stylized actions of a

customer dining at a restaurant. The < relation denotes

event ei happens before event ej .

uments from a subset of documents on the web, (2)

extract events relevant for the topic, (3) (optional)

refine the topic and restart at 1, and (4) establish a

temporal ordering for the events.

We currently impose restrictions on the form of

acceptable topics. For our purposes, a topic is a short

description of a script, and contains at least a verb

and a noun from the script’s intended domain. For

example, the topic for a passenger’s typical actions

while using public transportation (i.e. a bus) can be

described by the topic riding on a bus.

3.2 System Architecture

The script extraction system consists of a variety of

modules where each module is responsible for a cer-

tain task. Modules are combined in a mixed fash-

ion such that sequential processing is combined with

an iterative improvement procedure. Figure 3 illus-

trates the system architecture and flow of informa-

tion between modules. The following sections de-

scribe each module in detail.

3.2.1 Document Retrieval

Our system utilizes the web as its underlying in-

formation source to circumvent domain limitations

of fixed corpora. However, using the entire web to

extract a script for a specific topic is, on one hand,

infeasible due to the size of the web and, on the

other hand, impractical in term of document rele-

vancy. Since only a subset of pages is potentially

Figure 3: System Architecture and flow of information.

relevant to a given topic, the web needs to be filtered

such that mostly relevant web pages are retrieved.

The document retrieval module makes use of exist-

ing search engines for this purpose.1

The document retrieval module is presented with

the topic for a script and issues this topic as a query

to the search engines. The search engines produce a

relevancy ranked list of documents/URLs (Brin and

Page, 1998) which, in turn, are downloaded. The

number of downloaded pages depends on the cur-

rent iteration number of the system (i.e., how often

the retrieval-analysis cycle has been executed for a

given topic2).

The document retrieval module is also responsi-

ble for cleaning the documents. The cleaning pro-

cess aims to remove “boilerplate” elements such as

navigational menus and advertising from web pages

while preserving content elements.3 The collection

of cleaned documents for a given topic is considered

to be a topic-specific corpus.

3.2.2 Latent Semantic Analysis (LSA)

The aim of the LSA module is to identify words

(verbs, nouns and adjectives) that are closely related

1The Google and Yahoo API’s are used to establish commu-

nication with these search engines.
2At the first iteration, we have arbitrarily choosesn to re-

trieve the first 1000 unduplicated documents.
3The Special Interest Group of the ACL on Web as Cor-

pus (SIGWAC) is interested in web cleaning methods for corpus

construction. Our web page cleaner uses a support vector ma-

chine to classify blocks of a web page as content or non-content.

The cleaner achieves ≈ 85% F1 on a random set of web pages.

36



to the topic presented to the document retrieval mod-

ule. To find such words, the topic-specific corpus is

(1) part-of-speech tagged and (2) transformed into

a term-document matrix. Each cell represents the

log-entropy for its respective term in a document.

Note that we consider a term to be a tuple consist-

ing of a word and its POS. The advantage of us-

ing word-POS tag combinations over words only is

the ability to query LSA’s concept space for words

by their word class. A concept space is created by

applying SVD to the term-document matrix, reduc-

ing the dimensionality of the scaling matrix and re-

constructing the term-document matrix using the re-

duced scaling matrix.

Once the concept space is constructed, the space

is searched for all terms having a high correlation

with the original topic. Terms from the original topic

are located in the concept space (i.e., term vectors

are located) and other term vectors with high co-

sine similarity are retrieved from the space. A list of

n = 50 terms4 for each word class ∈ {verb, noun,
adjective} is obtained and filtered using a stop list.
The stop list currently contains the 100 most com-

mon words in the English language. The idea behind

the stop list is to remove low content words from the

list. The resulting set of words is deemed to have

high information content with respect to the topic.

This set is used for two purposes: (1) to augment

the original topic and to restart the document col-

lection process using the augmented topic and (2)

identify event pairs constructed by the event finding

module which contain these highly correlated terms

(either as events or event arguments). The first pur-

pose aims to use an iterative process to construct a

higher quality topic-specific corpus. A new corpus

created in this fashion presumably represents docu-

ments that are richer and more relevant to the aug-

mented topic. The second purpose steers the extrac-

tion of events towards events containing those con-

stituents judged most relevant. This fact can be in-

corporated into a maximization calculation based on

pointwise mutual information to find highly corre-

lated events.

4The number was chosen experimentally and is based on

the correlation score (cosine similarity) between word vectors.

After about 50 words, the correlation score begins to drop sig-

nificantly indicating weaker relatedness.

3.2.3 Event Finding

The collection of documents (or topic-specific

corpus) is then processed to facilitate finding event

pairs. Finding event pairs involves the notion of

verb argument overlap using the assumption that two

events in a story are related if they share at least

one semantic argument across grammatical func-

tions. This virtue of discourse structure of coherent

stories has been described in (Trabasso et al., 1984)

and applied by (Fujiki et al., 2003) as subject and ob-

ject overlap and by (Chambers and Jurafsky, 2008)

as following a common protagonist in a story. For

example, in the sentences “John ordered a drink. He

enjoyed it very much.” we can establish that events

order and enjoy are part of a common theme because

the arguments (or loosely semantic roles) of order

and enjoy refer to the same entities, that is John =

He and a drink = it.

Figure 4: Example processing of the sentences “Yester-

day, Joe ordered coffee. It was so hot, he couldn’t drink

it right away”. The output after dependency parsing, ref-

erence resolution and event finding is a set of event pairs.

37



To identify such pairs, the topic specific corpus

is (1) co-reference resolved5 and (2) dependency

parsed6. Sentences containing elements referring

to the same mention of an element are inspected

for verb argument overlap. Figure 4 illustrates this

procedure for the sentences “Yesterday, Joe ordered

coffee. It was so hot, he couldn’t drink it right

away”.

Co-reference resolution tells us that mention he

refers to Joe and mention(s) it refer to coffee. By

our previous assumption of discourse coherence, it

is possible to deduce that events was and drink are

associated with event order. In a similar fashion,

event drink is associated with event was. This is

due to the fact that all events share at least one ar-

gument (in the case of events order and drink two

arguments are shared). For each pair of events shar-

ing arguments in a particular grammatic function, an

event pair is generated indicating where the overlap

occurred.

3.2.4 Constructing Event Sets

Sets of events representing script-like structures

are constructed through the use of pointwise mutual

information in combination with the lists of related

words found by Latent Semantic Analysis. We uti-

lize the definition of PMI described in (Chambers

and Jurafsky, 2008). For two events e1 and e2

pmi(e1, e2) = log
P (e1, e2)

P (e1)P (e2)
(1)

where

P (e1, e2) =
C(e1, e2)

∑

i,j C(ei, ej)
(2)

and C(e1, e2) is the number of times events e1 and
e2 had coreferring arguments.

We extend the definition of PMI between events to

assign more weight to events whose constituents are

contained in the list of words (verbs, nouns and ad-

jectives) judged by Latent Semantic Analysis to be

most relevant to the topic. For notational purposes,

these lists are denoted L. Thus, we can calculate the

weighted LSA PMI LP (e1, e2) as follows:

LP (e1, e2) = P (e1, e2) + LSA(e1, e2) (3)

5OpenNLP’s Co-Reference Engine is utilized http://

opennlp.sourceforge.net/.
6The Stanford Parser is utilized http://nlp.

stanford.edu/software/lex-parser.shtml

where

LSA(e1, e2) = α ∗ (E(e1) + E(e2)) (4)

α =

{

2 if e1verb ∈ L and e2verb ∈ L

1 otherwise
(5)

E(e) = (||everb ∩ L||+ 1)

∗ (
||eArgs∩L||
||eArgs||

+ 1)
(6)

To construct the set of n events related to the

topic, the LP scores are first calculated for each

event pair in the corpus. The set can then be con-

structed by maximizing:

max
i<k≤n

k−1
∑

i=0

LP (ei, ek) (7)

Therefore, events that share a larger number of

constituents with the LSA relevancy list are pre-

ferred for inclusion in the event set. This prac-

tice distributes the relatedness weight among the fre-

quency of events and LSA. The noisy nature of our

proposed corpus generation method makes such a

technique essential as we will see in section 4.3.

3.2.5 Ordering Events

At this time, we only establish a naive temporal

ordering on the events. The ordering process simply

assumes that an event appearing in the corpus prior

to another event also occurs earlier in time. We re-

alize that this assumption does not always hold, but

delay a more sophisticated ordering process as fu-

ture work.

4 Experiments

This section describes experimental results, obsta-

cles we have encountered, various approaches to

overcome these obstacles and lessons learned from

our work. Unless mentioned otherwise, the results

pertain to the topic eating at a restaurant. This topic

has been chosen for our investigation since previ-

ous work (Schank and Abelson, 1977) establishes a

comprehensive reference as to what a script for this

domain may entail.

4.1 Domain Richness

The first step in our work was to confirm the no-

tion that the web can be used as the underlying in-

formation source for topic-specific script extraction.

38



The overall goal was to investigate whether a topic-

specific corpus contains sufficiently useful informa-

tion which is conducive to the script extraction task.

Latent Semantic Analysis was performed on the

Part-of-Speech tagged topic specific corpus. The

semantic space was queried using the main con-

stituents of the original topic. Hence, this resulted in

two queries, namely eating and restaurant. For each

query, we identified the most related verbs, nouns

and adjectives/adverbs and placed them in respec-

tive lists. Lists are then combined according to word

class, lemmatized, and pruned. Auxiliary verbs such

as be, do and have consistently rank in the top 10

most similar words in the un-pruned lists. This re-

sult is expected due to the frequency distribution

of auxiliaries in the English language. It is a nat-

ural conclusion to exclude auxiliaries from further

consideration since their information content is rela-

tively low. Furthermore, we extend this notion to ex-

clude the 100 most frequent words in English from

these lists using the same justification. By the in-

verse reasoning, it is desirable to include words in

further processing that occur infrequently in natural

language. We can hypothesize that such words are

significant to a given script because their frequency

appears to be elevated in the corpus. Table 1 (left)

shows the resulting word class lists for both queries.

Duplicates (i.e., words with identical lemma) have

been removed.

The table reveals that some words also appear

in the restaurant script as suggested by (Schank

and Abelson, 1977). In particular, bold verbs re-

semble Schank’s scenes and bold nouns resemble

his props. We can also see that the list of ad-

verbs/adjectives appear to not contribute any signif-

icant information. Note that any bold words have

been hand selected using a human selector’s subjec-

tive experience about the eating at a restaurant do-

main. Furthermore, while some script information

appears to be encoded in these lists, there is a signif-

icant amount of noise, i.e., normal font words that

are seemingly unimportant to the script at hand.

For our purposes, we aim to model this noise so

that it can be reduced or removed to some degree.

Such a model is based on the notion of overlap of

noisy terms in the LSA lists derived from indepen-

dent topic related corpora for the main constituents

of the original topic. For example, for the topic eat-

eating at a restaurant Overlap removed

Verbs Nouns A&A Verbs Nouns A&A

keep home own order home fry

need place still set hand amaze

help table last expect bowl green

dine lot open share plate grill

love part full drink cook chain

order hand off try fish diet

feel reason long cut soup clean

avoid course fat decide service smart

add side right watch break total

let number down process drink relate

stay experience busy save cheese worst

include water fast offer rice black

tend point single provide serve fit

set dish low hear chance light

tell bowl free fill portion exist

found plate white forget body empty

bring bite wrong write party live

locate cook ready follow rest

eat fish true travel cream

leave soup close taste

Table 1: (Left) 20 most relevant terms (after pruning)

for LSA queries eating and restaurant on the eating at

a restaurant corpus. (Right) Terms remaining after noise

modeling and overlap removal. Bold terms in the table

were manually judged by a human to be relevant.

ing at a restaurant, we obtain two additional corpora

using the method described in Section 3.2.1, i.e., one

corpus for constituent eating and another for the sec-

ond main constituent of the original topic, restau-

rant. Both corpora are subjected to LSA analysis

from which two (one for each corpus) LSA word

lists are obtained. Each list was created using the

respective corpus query as the LSA query. The as-

sumption is made that words which are shared (pair-

wise) between all three lists (i.e., the two new LSA

lists and the LSA list for topic eating at a restaurant)

are noisy due to the fact that they occur independent

of the original topic.

Table 1 (right) illustrates the LSA list for topic

eating at a restaurant after removing overlapping

terms with the two other LSA lists. Bold words were

judged by a human selector to be relevant to the in-

tended script. From the table we can observe that:

1. A significant amount of words have been re-

moved. The original table contains 50 words

for each word class. The overlap reduced table

contains only 19 verbs, 29 nouns, and 17 adjec-

tives, a reduction of what we consider noise by

39



≈ 57%

2. More words (bold) were judged to be related to

the script (e.g., 6 vs. 5 relevant verbs, 12 vs. 9

nouns, and 3 vs. 0 adjectives/adverbs)

3. More relevant words appear in the top part of

the list (words in the list are ordered by rele-

vancy)

4. Some words judged to be relevant were re-

moved (e.g., dine, bring, eat).

Using the information from the table (left and

right) and personal knowledge about eating at a

restaurant, a human could re-arrange the verbs and

nouns into a partial script-like format of the form7:

e1 [ offer, nsubj, {waiter}) ]
e2 [ offer, dobj, {drink}) ]
Example: waiter offers a drink

e2 [ order, nsubj, {customer} ]
e3 [ order, dobj, {fish, soup, rice} ]
Example: customer orders fish

e4 [ serve/bring, nsubj, {waiter} ]
e5 [ serve/bring, nsubj, {bowl, plate} ]
Example: waiter serves/bings the bowl, plate

e6 [ eat, nsubj, {customer} ]
e7 [ eat, dobj, {portion, cheese} ]
Example: customer eats the portion, cheese

e8 [ leave, nsubj, {customer} ]
e9 [ leave, dobj, {table} ]
Example: customer leaves the table

Note that this script-like information was not ob-

tained by direct derivation from the information in

the table. It is merely an illustration that some script

information is revealed by LSA. Table 1 neither im-

plies any ordering nor suggest semantic arguments

for a verb. However, the analysis confirms that the

web contains information that can be used in the

script extraction process.

4.2 Processing Errors

As mentioned in section 3.2.3, events with co-

referring arguments are extracted in a pairwise fash-

ion. In the following section we describe observa-

tions about the characteristics of events extracted
7Bold terms do not appear in the LSA lists, but were added

for readability.

this way. However, we note that each step in our

system architecture is imperfect, meaning that er-

rors are introduced in each module as the result of

processing. We have already seen such errors in

the form of words with incorrect word class in the

LSA lists as the result of incorrect POS tagging.

Such errors are amplified through imprecise pars-

ing (syntactic and dependency parsing). Other er-

rors, such as omissions, false positives and incor-

rect class detection, are introduced by the named en-

tity recognizer and the co-reference module. With

this in mind, it comes as no surprise that some ex-

tracted events, as seen later, are malformed. For

example, human analysis reveals that the verb slot

of these events are sometimes “littered” with words

from other word classes, or that the arguments of a

verb were incorrectly detected. A majority of these

errors can be attributed to ungrammatical sentences

and phrases in the topic-specific corpus, the remain-

der is due to the current state of the art of the parsers

and reference resolution engine.

4.3 Observations about events

To compare relations between events, we looked at

three different metrics. The first metric M1 simply
observes the frequency counts of pairwise events in

the corpus. The second metric M2 utilizes point
wise mutual information as defined in (Chambers

and Jurafsky, 2008). The third metric M3 is our
LSA based PMI calculation as defined in section

3.2.4.

M1 reveals that uninformative event pairs tend
to have a high number of occurrences. These pairs

are composed of low content, frequently occurring

events. For example, event pair [e [ have, nsubj,{} ],
e [ say, nsubj, {} ]] occurred 123 times in our topic-
specific corpus. More sophisticated metrics, such as

M2, consider the frequency distributions of individ-
ual events and allocate more weight to co-occurring

events with lower frequency counts of their individ-

ual events.

In this fashion, M2 is capable of identifying
strongly related events. For example, Table 2 lists

the five pairwise events with highest PMI for our

topic-specific corpus.

From Table 2, it is apparent that these pairs partic-

ipate in mostly meaningful (in terms of human com-

prehensibility) relationships. For example, it does

40



Event Pairs

e[sack, dobj, {the, employees}] e[reassign, dobj, {them}]

e[identify, nsubj, {we, Willett}] e[assert, nsubj, {Willett}]

e[pour, dobj, {a sweet sauce}] e[slide, dobj, {the eggs}]

e[walk, nsubj, {you, his sister}] e[fell, nsubj, {Daniel}]

e[use, nsubj, {the menu}] e[access, dobj, {you}]

Table 2: Pairwise events with highest PMI according to

Equation 1.

not require a leap of faith to connect that sacking

employees is related to reassigning them in the con-

text of a corporate environment.

e(eat, nsubj), e(gobble, nsubj), e(give, nsubj),

e(live, nsubj), e(know, nsubj), e(go, nsubj),

e(need, nsubj), e(buy, nsubj), e(have, nsubj),

e(make, nsubj), e(say, nsubj), e(work, nsubj),

e(try, nsubj), e(like, nsubj), e(tell, dobj),

e(begin, nsubj), e(think, nsubj), e(tailor, nsubj),

e(take, nsubj), e(open, nsubj),e(be, nsubj)

Figure 5: An event set obtained through metric M2 (us-
ing the PMI between events). Temporal ordering is not

implied. Event arguments are omitted. Bold events indi-

cate subjectively judged strong relatedness to the eating

at a restaurant topic.

Figure 5 shows a set of events for our topic. The

set was created by greedily adding event en such

that for all events e1, e2, ...en−1 already in the set

∀n−1i pmi(ei, en) is largest (see (Chambers and Ju-
rafsky, 2008)). The topic constituent eating (i.e.,

eat) was used as the initial event in the set. If this

set is intended to approximate a script for the eating

at a restaurant domain, then it is easy to see that vir-

tually no information from Schank’s restaurant ref-

erence script is represented. Furthermore, by human

standards, the presented set appears to be incoherent.

From this observation, we can conclude that M2 is
unsuitable for topic-specific script extraction.

Figure 6 illustrates an event set constructed using

metric M3. Note that the sets in Figures 5 and 6
do not imply any ordering on the events. The bold

events indicate events that appear in our reference

script or were judged by a human evaluator to be

logically coherent with the eating at a restaurant do-

main. The evaluation was conducted using the eval-

uators personal experience of the domain. In the fu-

ture, we intend to formalize this evaluation process.

Figure 6 signifies an improvement of the results of

e(eat, nsubj), e(wait, dobj), e(total, nsubj)

e(write, dobj), e(place, dobj), e(complete, dobj)

e(exist, nsubj), e(include, dobj), e(top, nsubj)

e(found, dobj), e(keep, dobj), e(open, dobj)

e(offer, dobj), e(average, nsubj), e(fill, dobj)

e(taste, nsubj), e(drink, dobj), e(cook, dobj)

e(read, dobj), e(enjoy, dobj),e(buy, dobj)

Figure 6: An event set obtained through metric M3
(weighing PMI and LSA between events). Temporal or-

dering is not implied. Event arguments are omitted. Bold

events indicate subjectively judged strong relatedness to

the eating at a restaurant topic.

Figure 5 in terms of the number of events judged to

belong to the restaurant script. This leads us to the

conclusion that a metric based on scaled PMI and

LSA appears more suitable for the web based topic

driven script extraction task. Once a temporal order

is imposed on the events in Figure 6, these events

could, by themselves, serve as a partial event set for

their domain.

5 Discussion and Future Work

We have presented preliminary work on extracting

script-like information in a topic driven fashion from

the web. Our work shows promise to identify script

knowledge in a topic-specific corpus derived from

an unordered collection of web pages. We have

shown that while web documents contain signifi-

cant amounts of noise (both boilerplate elements and

topic unrelated content), a subset of content can be

identified as script-like knowledge.

Latent Semantic Analysis allows for the filter-

ing and pruning of lists of related words by word

classes. LSA furthermore facilitates noise removal

through overlap detection of word class list elements

between independent corpora of topic constituents.

Our method of weighted LSA and PMI for event re-

latedness produces more promising partial event sets

than existing metrics.

For future work, we leave the automated evalua-

tion of partial sets and the establishing of temporal

relations between events in a set. Our system archi-

tecture features an iterative model to event set im-

provement. We hope that this approach will allow

us to improve upon the quality of event sets by us-

ing extracted sets from one iteration to bootstrap a

new iteration of event extraction.

41



References

Sergey Brin and Lawrence Page. 1998. The anatomy of a

large-scale hypertextual web search engine. Comput.

Netw. ISDN Syst., 30(1-7):107–117.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-

pervised learning of narrative event chains. In Pro-

ceedings of ACL-08: HLT, pages 789–797, Columbus,

Ohio, June. Association for Computational Linguis-

tics.

Timothy Chklovski and Patrick Pantel. 2004. VerbO-

cean: Mining the Web for Fine-Grained Semantic Verb

Relations. In Proceedings of Conference on Empirical

Methods in Natural Language Processing, pages 33–

40, Barcelona, Spain, July. Association for Computa-

tional Linguistics.

Scott Deerwester, T. Susan Dumais, W. George Furnas,

K. Thomas Landauer, and Richard Harshman. 1990.

Indexing by Latent Semantic Analysis. Journal of the

American Society for Information Science, 41:391–

407.

Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-

mura. 2003. Automatic acquisition of script knowl-

edge from a text collection. In EACL ’03: Proceedings

of the tenth conference on European chapter of the As-

sociation for Computational Linguistics, pages 91–94,

Morristown, NJ, USA. Association for Computational

Linguistics.

Dekang Lin and Patrick Pantel. 2001a. DIRT - Discov-

ery of Inference Rules from Text. In KDD ’01: Pro-

ceedings of the seventh ACM SIGKDD international

conference on Knowledge discovery and data mining,

pages 323–328, New York, NY, USA. ACM.

Dekang Lin and Patrick Pantel. 2001b. Discovery of

Inference Rules for Question-Answering. Nat. Lang.

Eng., 7(4):343–360.

Roger C. Schank and Robert P. Abelson. 1977. Scripts,

Plans, Goals and Understanding. Lawrence Erlbaum

Associates, Hillsdale, NJ.

Tom Trabasso, T. Secco, and Paul van den Broek. 1984.

Causal Cohesion and Story Coherence. In Heinz

Mandl, N.L. Stein and Tom Trabasso (eds). Learning

and Comprehension of Text., pages 83–111, Hillsdale,

NJ, USA. Lawrence Erlbaum Associates.

42


