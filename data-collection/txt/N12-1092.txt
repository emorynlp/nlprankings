










































Mind the Gap: Learning to Choose Gaps for Question Generation


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 742–751,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Mind the Gap: Learning to Choose Gaps for Question Generation 

Lee Becker 

Department of Computer Science 

University of Colorado Boulder 

Boulder, CO 80309 

lee.becker@colorado.edu 

 

 

Sumit Basu and Lucy Vanderwende 

Microsoft Research 

One Microsoft Way 

Redmond, WA 98052 

{sumitb,lucyv}@microsoft.com 

 

 

Abstract 

Not all learning takes place in an educational 

setting: more and more self-motivated learners 

are turning to on-line text to learn about new 
topics. Our goal is to provide such learners 

with the well-known benefits of testing by au-

tomatically generating quiz questions for on-

line text. Prior work on question generation 

has focused on the grammaticality of generat-

ed questions and generating effective multi-

ple-choice distractors for individual question 

targets, both key parts of this problem. Our 

work focuses on the complementary aspect of 

determining what part of a sentence we should 

be asking about in the first place; we call this 

“gap selection.” We address this problem by 
asking human judges about the quality of 

questions generated from a Wikipedia-based 

corpus, and then training a model to effective-

ly replicate these judgments. Our data shows 

that good gaps are of variable length and span 

all semantic roles, i.e., nouns as well as verbs, 

and that a majority of good questions do not 

focus on named entities. Our resulting system 

can generate fill-in-the-blank (cloze) ques-

tions from generic source materials. 

1 Introduction 

Assessment is a fundamental part of teaching, both 
to measure a student’s mastery of the material and 

to identify areas where she may need reinforce-

ment or additional instruction. Assessment has also 
been shown an important part of learning, as test-

ing assists retention and can be used to guide learn-

ing (Anderson & Biddle, 1975). Thus, as learners 
move on from an educational setting to unstruc-

tured self-learning settings, they would still benefit 

from having the means for assessment available. 

Even in traditional educational settings, there is a 

need for automated test generation, as teachers 
want multiple tests for topics to give to different 

students, and students want different tests with 

which to study and practice the material. 
One possible solution to providing quizzes for 

new source material is the automatic generation of 

questions. This is a task the NLP community has 

already embraced, and significant progress has 
been made in recent years with the introduction of 

a shared task (Rus et al., 2010). However, thus far 

the research community has focused on the prob-
lem of generating grammatical questions (as in 

Heilman and Smith (2010a)) or generating effec-

tive distractors for multiple-choice questions 

(Agarwal and Mannem, 2011). 
While both of these research threads are of crit-

ical importance, there is another key issue that 

must be addressed – which questions should we be 
asking in the first place? We have highlighted this 

aspect of the problem in the past (see 

Vanderwende (2008)) and begin to address it in 
this work, postulating that we can both collect hu-

man judgments on what makes a good question 

and train a machine learning model that can repli-

cate these judgments. The resulting learned model 
can then be applied to new material for automated 

question generation. We see this effort as comple-

mentary to the earlier progress.    
In our approach, we factor the problem of gen-

erating good questions into two parts: first, the se-

lection of sentences to ask about, and second, the 
identification of which part of the resulting sen-

tences the question should address. Because we 

want to focus on these aspects of the problem and 

not the surface form of the questions, we have cho-
sen to generate simple gap-fill (cloze) questions, 

742



though our results can also be used to trigger Wh-

questions or multiple-choice questions by leverag-
ing prior work. For sentence selection, we turn to 

methods in summarization and use the simple but 

effective SumBasic (Nenkova et al., 2006) algo-

rithm to prioritize and choose important sentences 
from the article. We cast the second part, gap se-

lection, as a learning problem. To do this, we first 

select a corpus of sentences from a very general 
body of instructional material (a range of popular 

topics from Wikipedia). We then generate a con-

strained subset of all possible gaps via NLP heuris-
tics, and pair each gap with a broad variety of 

features pertaining to how it was generated. We 

then solicit a large number of human judgments via 

crowdsourcing to help us rate the quality of various 
gaps. With that data in hand, we train a machine 

learning model to replicate these judgments. The 

results are promising, with one possible operating 
point producing a true positive rate of 83% with a 

corresponding false positive rate of 19% (83% of 

the possible Good gaps are kept, and 19% of the 
not-Good gaps are incorrectly marked); see Figure 

6 for the full ROC curve and Section 4 for an ex-

planation of the labels. As the final model has only 

minimal dependence on Wikipedia-specific fea-
tures, we expect that it can be applied to an even 

wider variety of material (blogs, news articles, 

health sites, etc.).   

2 Background and Related Work 

There already exists a large body of work in auto-

matic question generation (QG) for educational 

purposes dating back to the Autoquest system 

(Wolfe, 1976), which used an entirely syntactic 
approach to generate Wh-Questions from individu-

al sentences. In addition to Autoquest, several oth-

ers have created systems for Wh-question 
generation using approaches including transfor-

mation rules (Mitkov and Ha, 2003), template-

based generation (Chen et al., 2009; Curto et al., 
2011), and overgenerate-and-rank (Heilman and 

Smith, 2010a). The work in this area has largely 

focused on the surface form of the questions, with 

an emphasis on grammaticality.   
Alternatively, generation of gap-fill style ques-

tions (a.k.a. cloze questions) avoids these issues of 

grammaticality by blanking out words or spans in a 
known good sentence. There is a large body of ex-

isting work that has focused on generation of this 

type of question, most of which has focused on 

vocabulary and language learning. The recent work 
of Agarwal and Mannem (2011) is closer to our 

purposes; they generated fill-in-the-blank questions 

and distractor answers for reading comprehension 

tests using heuristic scoring measures and a small 
evaluation set. Our work has similar aims but em-

ploys a data-driven approach. 

The Question-Generation Shared Task and 
Evaluation Challenge (QG-STEC) (Rus et al., 

2010) marks a first attempt at creating a common 

task and corpus for empirical evaluation of ques-
tion generation components. However, evaluation 

in this task was manual and the number of instanc-

es in both the development and training set were 

small. As there exists no other dataset for question 
generation, we created a new corpus using Amazon 

Mechanical Turk by soliciting judgments from 

non-experts. Snow et al. (2008) have validated 
AMT as a valid data source by comparing non-

expert with gold-standard expert judgments. Cor-

pus creation using AMT has numerous precedents 
now; see i.e. Callison-Burch and Dredze (2010) 

and Heilman and Smith (2010b). We have made 

our corpus (see Section 4) available online to ena-

ble others to continue research on the gap-selection 
problem we address here.  

3 Question Generation  

To achieve our goal of selecting better gap-fill 

questions, we have broken down the task into stag-
es similar to those proposed by Nielsen (2008): 1) 

sentence selection, 2) question construction, and 3) 

classification/scoring. Specifically, we utilize sum-

marization to identify key sentences from a pas-
sage. We then apply semantic and syntactic 

constraints to construct multiple candidate ques-

tion/answer pairs from a given source sentence.  
Lastly we extract features from these hypotheses 

for use with a question quality classification mod-

el. To train this final question-scoring component, 
we made use of crowdsourcing to collect ratings 

for a corpus of candidate questions. While this 

pipeline currently produces gap-fill questions, we 

envision these components can be used as input for 
more complex surface generation such as Wh- 

forms or distractor selection. 

 
 

743



3.1 Sentence Selection 

When learning about a new subject, a student will 

most likely want to learn about key concepts be-

fore moving onto more obscure details. As such, it 
is necessary to order target sentences in terms of 

their importance. This is fortunately very similar to 

the goals of automatic summarization, in which the 

selected sentences should be ordered by how cen-
tral they are to the article. 

As a result, we make use of our own implemen-

tation of SumBasic (Nenkova et al., 2006), a sim-
ple but competitive document summarization al-

gorithm motivated by the assumption that 

sentences containing the article’s most frequently 
occurring words are the most important. We thus 

use the SumBasic score for each sentence to order 

them as candidates for question construction.    

3.2 Question Construction 

We seek to empirically determine how to choose 
questions instead of relying on heuristics and rules 

for evaluating candidate surface forms. To do this, 

we cast question construction as a generate-and-

filter problem: we overgenerate potential ques-
tion/answer pairs from each sentence and train a 

discriminative classifier on human judgments of 

quality for those pairs. With the trained classifier, 
we can then apply this approach on unseen sen-

tences to return the highest-scoring ques-

tion/answer, all question/answer pairs scoring 
above a threshold, and so on. 

Generation 

Although it would be possible to select every word 

or phrase as a candidate gap, this tactic would pro-

duce a skewed dataset composed mostly of unusa-
ble questions, which would subsequently require 

much more annotation to discriminate good ques-

tions from bad ones. Instead we rely on syntactic 

and semantic constraints to reduce the number of 

questions that need annotation. 

To generate questions we first run the source 
sentence through a constituency parser and a se-

mantic role labeler (components of a state-of-the-

art natural language toolkit from (Quirk et al., 
2012)), with the rationale that important parts of 

the sentence will occur within a semantic role. 

Each verb predicate found within the roles then 

automatically becomes a candidate gap. From eve-
ry argument to the predicate, we extract all child 

noun phrases (NP) and adjectival phrases (ADJP) 

as candidate gaps as well. Figure 1 illustrates this 
generation process.  

Classification 

To train the classifier for question quality, we ag-

gregated per-question ratings into a single label 

(see Section 4 for details). Questions with an aver-
age rating of 0.67 or greater were considered as 

positive examples. This outcome was then paired 

with a vector of features (see Section 5) extracted 

from the source sentence and the generated ques-
tion. 

Because our goal is to score each candidate 

question in a meaningful way, we use a calibrated 
learner, namely L2-regularized logistic regression 

(Bishop 2006). This model’s output is 

�(�����|�	�
��	�); in our case this is the posteri-
or probability of a candidate receiving a positive 

label based on its features. 

4 Corpus Construction 

We downloaded 105 articles from Wikipedia’s 

listing of vital articles/popular pages.
1
 These arti-

cles represent a cross section of historical, social, 

                                                        
1http://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Popular
_pages 

In 1874 Röntgen              a lecturer at the University of Strassburg. 

In          Röntgen became a lecturer at the University of Strassburg. 

In 1874               became a lecturer at the University of Strassburg. 

In 1874 Röntgen became a              at the University of Strassburg. 

In 1874 Röntgen became a lecturer at                         of Strassburg. 

In 1874 Röntgen became a lecturer at the University of                  . 

In 1874 Röntgen became a lecturer at                                              . 

Figure 1 An example of the question generation process. 

744



and scientific topics. From each article we sampled 

10 sentences using the sentence selection algorithm 

described in Section 3.1 for 50% of the sentences; 

the other 50% were chosen at random to prevent 
any possible overfitting to the selection method. 

These sentences were then processed using the 

candidate generation method from Section 3.2. 
To collect training data outcomes for our ques-

tion classifier, we used Amazon’s Mechanical 

Turk (AMT) service to obtain human judgments of 
quality for each question. We initially considered 

asking about the quality of individual ques-

tion/answer pairs in isolation, but in pilot studies 

we found poor agreement in this case; we noticed 
that the inability to compare with other possible 

questions actually made the task seem difficult and 

arbitrary. We thus instead presented raters with a 
source sentence and a list of up to ten candidate 

questions along with their corresponding answers 

(see Figure 2). Raters were asked to rate questions’ 

quality as Good, Okay, or Bad. The task instruc-
tions defined a Good question as “one that tests 

key concepts from the sentence and would be rea-

sonable to answer.” An Okay question was defined 
as “one that tests key concepts but might be diffi-

cult to answer (the answer is too lengthy, the an-

swer is ambiguous, etc.).” A Bad question was 
“one that asks about an unimportant aspect of the 

sentence or has an uninteresting answer that can be 

figured out from the context of the sentence.” The-

se ratings were binarized into a score of one for 
Good and zero for not-Good (Okay or Bad), as our 

goal was to find the probability of a question being 

truly Good (and not just Okay).
2
  

                                                        
2 Heilman and Smith (2010a and b) asked raters to identify 
question deficiencies, including vague or obvious, but raters 
were not asked to differentiate between Good and Okay. Thus 
questions considered Good in their study would include Okay. 

Thus far we have run 300 HITs with 4 judges 

per HIT. Each HIT consisted of up to 10 candidate 

questions generated from a single sentence. In total 

this yielded 2252 candidate questions with 4 rat-
ings per question from 85 unique judges. We then 

wished to eliminate judges who were gaming the 

system or otherwise performing poorly on the task. 
It is common to do such filtering when using 

crowdsourced data by using the majority or median 

vote as the final judgment or to calibrate judges 
using expert judgments (Snow et al. 2008). Other 

approaches to annotator quality control include 

using EM-based algorithms for estimating annota-

tor bias (Wiebe et al. 1999, Ipeirotis et al. 2010).  
In our case, we computed the distance for each 

judge from the median judgment (from all judges) 

on each question, then took the mean of this dis-
tance over all questions they rated. We removed 

judges with a mean distance two standard devia-

tions above the mean distance, which eliminated 

the five judges who disagreed most with others. 
In addition to filtering judges, we wanted to fur-

ther constrain the data to those questions on which 

the human annotators had reasonable agreement, as 
it would not make sense to attempt to train a model 

to replicate judgments on which the annotators 

themselves could not agree. To do this, we com-
puted the variance of the judgments for each ques-

tion. By limiting the variance to 0.3, we kept ques-

tions on which up to 1 judge (out of 4) disagreed; 

this eliminated 431 questions and retained the 1821 
with the highest agreement. Of these filtered ques-

tions, 700 were judged to be Good (38%). 

To formally assess inter-rater reliability we 
computed Krippendorff’s alpha (Krippendorff, 

2004), a statistical measure of agreement applica-

ble for situations with multiple raters and incom-

plete data (in our case not all raters provided 
ratings for all items). An alpha value of 1.0 indi-

cates perfect agreement, and an alpha value of 0.0 

Source Sentence:  
    The large scale production of chemicals was an important development during the Industrial Revolution. 
 

Question Answer Ratings 

The _ _ _ _ _ _ _ _ _ _ _ _  of chemicals was an important 
development during the Industrial Revolution. 

large scale production ◯ Good   ◉  Okay   ◯ Bad 

The large scale production of _ _ _ _ _ _was an important 
development during the Industrial Revolution. 

chemicals ◯ Good   ◉  Okay   ◯ Bad 

The large scale production of chemicals was an important  
development during the _ _ _ _ _ _ _ _ _ _ _ . 

Industrial Revolution ◉  Good   ◯ Okay   ◯ Bad 

 
Figure 2: Example question rating HIT 

745



indicates no agreement. Our original data yielded 

an alpha of 0.34, whereas after filtering judges and 
questions the alpha was 0.51. It should be noted 

that because Krippendorff’s Alpha accounts for 

variability due to multiple raters and sample size, 

its values tend to be more pessimistic than many 
Kappa values commonly used to measure inter-

rater reliability. 

For others interested in working with this data, 
we have made our corpus of questions and ratings 

available for download at the following location: 

http://research.microsoft.com/~sumitb/questiongen
eration.   

5 Model Features 

While intuition would suggest that selecting high-

quality gaps for cloze questions should be a 

straightforward task, analysis of our features im-
plies that identifying important knowledge depends 

on more complex interactions between syntax, se-

mantics, and other constraints. In designing fea-
tures, we focused on using commonly extracted 

NLP information to profile the answer (gap), the 

source sentence, and the relation between the two. 
To enable extraction of these features, we used 

the MSR Statistical Parsing and Linguistic Analy-

sis Toolkit (MSR SPLAT)
3
, a state-of-the-art, web-

based service for natural language processing 
(Quirk et al., 2012). Table 1 shows a breakdown of 

our feature categories and their relative proportion 

of the feature space. In the subsections below, we 
describe the intuitions behind our choice of fea-

tures and highlight example features from each of 

these categories. An exhaustive list of the features 

can be found at the corpus URL listed in Section 4. 

5.1 Token Count Features 

A good question gives the user sufficient context to 

answer correctly without making the answer obvi-

ous. At the same time, gaps with too many words 
may be impossible to answer. Figure 3 shows the 

distributions of number of tokens in the answer 

(i.e., the gap) for Good and not-Good questions. As 

intuition would predict, the not-Good class has 
higher likelihoods for the longer answer lengths. In 

addition to the number and percentage of tokens in 

the answer features, we also included other token 

                                                        
3 http://research.microsoft.com/projects/msrsplat  

count features such as the number of tokens in the 

sentence, and the number of overlapping tokens 
between the answer and the remainder of the sen-

tence. 

 
Feature Category Number of Features 

Token Count 5 

Lexical 11 

Syntactic 112 

Semantic 40 

Named Entity 11 

Wikipedia link 3 

Total 182 

Table 1: Breakdown of features by category 

5.2 Lexical features 

Although lexical features play an important role in 

system performance for several NLP tasks like 

parsing, and semantic role labeling, they require a 

large number of examples to provide practical ben-
efit. Furthermore, because most sentences in Wik-

ipedia articles feature numerous domain-specific 

terms and names, we cannot rely on lexical fea-
tures to generalize across the variety of possible 

articles in our corpus. Instead we approximate lex-

icalization by computing densities of word catego-
ries found within the answer. The intuition behind 

these features is that an answer composed primari-

ly of pronouns and stopwords will make for a bad 

question while an answer consisting of specific 
entities may make for a better question. Examples 

of our semi-lexical features include answer pro-

noun density, answer abbreviation density, answer 
capitalized word density, and answer stopword 

density. 

5.3 Syntactic Features 

The answer’s structure relative to the sentence’s 

structure provides information as to where better 
spans for the gap may exist. Similarly, part-of-

speech (POS) tags give a topic-independent repre-

sentation of the composition and makeup of the 
questions and answers. The collection of syntactic 

features includes the answer’s depth with the sen-

tence’s constituent parse, the answer’s location 

relative to head verb (before/after), the POS tag 
before the answer, the POS tag after the answer, 

and the answer bag-of-POS tags. 

 

746



 
Figure 3: Distribution of number of tokens in the answer 

for Good and not-Good questions. 

5.4 Semantic Role Label Features 

Beyond syntactic constraints, semantics can yield 
additional cues in identifying the important spans 

for questioning. Shallow-semantic parses like those 

found in Propbank (Palmer et al., 2005) provide a 
concise representation for linking predicates 

(verbs) to their arguments. Because these semantic 

role labels (SRLs) often correspond to the “who, 

what, where, and when” of a sentence, they natu-
rally lend themselves for use as features for rating 

question quality. To compute SRL features, we 

used the MSR SPLAT’s semantic role labeler to 
find the SRLs whose spans cover the question’s 

answer, the SRLs whose spans are contained with-

in the answer, and the answer’s constituent parse 

depth within the closest covering SRL node. 
To investigate whether judges keyed in on spe-

cific roles or modifiers when rating questions, we 

plotted the distribution of the answer-covering 
SRLs (Figure 4). This graph indicates that good 

answers are not associated with only a single label 

but are actually spread across all SRL classes. 
While the bulk of questions came from the argu-

ments often corresponding to subjects and objects 

(ARG0-2, shown as A0-A2), we see that good and 

bad questions have mostly similar distributions 
over SRL classes. However, a notable exception 

are answers covered by verb predicates (shown as 

“predicate”), which were highly skewed with 190 
of the 216 (88.0%) question/answer pairs exhibit-

ing this feature labeled as Bad. Together these dis-

tributions may suggest that judges are more likely 
to rate gap-fill questions as Good if they corre-

spond   to  questions  of   “who,  what,  where,  and  

 
Figure 4: Distribution of semantic role labels for 

Good and not-Good questions.  

 
when” over questions pertaining to “why and 

how.” 

5.5 Named Entity Features 

For many topics, especially in the social sciences, 

knowing the relevant people and places marks the 
first step toward comprehending new material. To 

reflect these concerns we use the named-entity 

tagger in the toolkit to identify the spans of text 
that refer to persons, locations, or organizations, 

which are then used to derive additional features 

for distinguishing between candidate questions. 

Example named-entity features include: answer 
named entity density, answer named entity type 

frequency (LOC, ORG, PER), and sentence named 

entity frequency. 
Figure 5 shows the distribution of named entity 

types found within the answers for Good and not-

Good questions. From this graph, we see that Good 

questions have a higher class-conditional probabil-
ity of containing a named entity. Furthermore, we 

see that Good questions are not confined to a sin-

gle named entity type, but are spread across all 
types. Together, these distributions indicate that 

while named entities can help to identify important 

gaps, the majority of questions labeled Good do 
not contain any named entity (515/700, i.e. 74%). 

This provides substantial evidence for generating 

questions for more than only named entities. 

 

747



 
Figure 5: Distribution of answer named entity type for 

Good and not-Good questions. 

5.6 Wikipedia Link Features 

Wikipedia’s markup language allows spans of text 
to link to other articles. This annotation inherently 

indicates a span of text as noteworthy, and can 

serve as evidence of an answer’s importance. We 
use the presence of this markup to compute fea-

tures such as answer link density, sentence link 

density, and the ratio of the number of linked 

words in the answer to the ratio of linked words in 
the sentence. 

6 Model and Training 

We chose logistic regression as our classifier be-

cause of its calibrated output of the class posterior; 
we combined it with an L2 regularizer to prevent 

overfitting. As the data likelihood is convex in the 

model parameters, we trained the model to maxim-

ize this quantity along with the regularization term 
using the L-BFGS algorithm for Quasi-Newton 

optimization (Nocedal, 1980). Evaluation was 

conducted with 10-fold cross validation, taking 
care to stratify folds so that all questions generated 

from the same source sentence are placed in the 

same fold. Results are shown in Section 7 below. 
To ensure that we were not overly narrow in 

this model choice, we tested two other more pow-

erful classifiers that do not have calibrated outputs, 

a linear SVM and a boosted mixture of decision 
trees (Caruana and Niculescu-Mizil, 2006); both 

produced accuracies within a percentage point of 

our model at the equal error rate. 

7 Results and Discussion 

Figure 6 shows ROC curves for our question quali-

ty classifier produced by sweeping the threshold on 
the output probability, using the raw collected data, 

our filtered version as described above, and a fur-

ther filtered version keeping only those questions 

where judges agreed perfectly; the benefits of fil-
tering can be seen in the improved performance. In 

this context, the true positive rate refers to the frac-

tion of Good questions that were correctly identi-
fied, and the false positive rate refers to the 

fraction of not-Good questions that were incorrect-

ly marked. At the equal error rate, the true positive 
rate was 0.83 and the false positive rate was 0.19.  

Figure 6: ROC for our model using unfiltered data 

(green dots), our filtered version (red dashes), and fil-

tered for perfect agreement (blue line). 

 

Choosing the appropriate operating point depends 

on the final application. By tuning the classifier’s 

true positive and false positive rates, we can cus-
tomize the system for several uses. For example, in 

a relatively structured scenario like compliance 

training, it may be better to reduce any possibility 
of confusion by eliminating false positives. On the 

other hand, a self-motivated learner attempting to 

explore a new topic may tolerate a higher false 
positive rate in exchange for a broader diversity of 

questions. The balance is subtle, though, as ill-

formed and irrelevant questions could leave the 

learner bored or frustrated, but alternatively, overly 
conservative question classification could poten-

tially eliminate all but the most trivial questions. 
 

T
ru
e
 P
o
s
it
iv
e
 R
a
te

748



 
Figure 7: ROC for our model with (red dash) and with-

out (blue line) Wikipedia-specific features. 
 

 
Figure 8: Classifier learning curve; each point repre-

sents mean accuracy over 40 folds. 

 
We next wanted to get a sense of how well the 

model would generalize to other text, and as such 

ran an analysis of training the classifier without the 

benefit of the Wikipedia-specific features (Figure 
7). The resulting model performs about the same as 

the original on average over the ROC, slightly bet-

ter in some places and slightly worse in others. We 
hypothesize the effect is small because these fea-

tures relate only to Wikipedia entities, and the oth-

er named entity features likely make them 
redundant. 

Finally, to understand the sensitivity of our 

model to the amount of training data, we plot a 

learning curve of the question classifier’s accuracy 
by training it against fractions of the available data 

(Figure 8). While the curve starts to level out 

around 1200 data points, the accuracy is still rising 

slightly, which suggests the system could achieve 

some small benefits in accuracy from more data. 

7.1 Error Analysis 

To explore the nature of our system’s misclassifi-
cations we examine the errors that occur at the 

equal error rate operating point. For our system, 

false positive errors occur when the system labels a 
question as Good when the raters considered it not-

Good. Table 2 lists three examples of this type of 

error. The incorrect high score in example 1 
(“Greeks declared ___”) suggests that system per-

formance can be improved via language modeling, 

as such features would penalize questions with an-

swers that could be predicted mostly by word tran-
sition probabilities. Similarly, when classifying 

questions like example 2 (“such as ____ for a 

mathematical function”), the system could benefit 
from some measure of word frequency or answer 

novelty. While our model included a feature for the 

number of overlapping words between the question 
and the answer, the high classifier score for exam-

ple 3 (“atop ______, the volcano”), suggests that 

this can be solved by explicitly filtering out such 

questions at generation time.  
With false negative errors the judges rated the 

question as Good, whereas the system classified it 

as Bad. The question and answer pairs listed in 
Table 3 demonstrate some of these errors. In ex-

ample 1 (“where Pompey was soon ___”), the sys-

tem was likely incorrect because a majority of 

questions with verb-predicate answers had Bad 
ratings (only 12% are Good). Conversely, classifi-

cation of example 2 (“Over the course of dec-

ades…”) could be improved with a feature 
indicating the novelty of the words in the answers. 

Example 3 (“About 7.5% of the...”) appears to 

come from rater error or rater confusion as the 
question does little to test the understanding of the 

material. 

While the raters considered the answer to ex-

ample 4 as Good, the low classifier score argues 
for different handling of answers derived from 

long coordinated phrases. One alternative approach 

would be to generate questions that use multiple 
gaps. Conversely, one may argue that a learner 

may be better off answering any one of the noun 

phrases like palm oil or cocoa in isolation.  
 

 

a
c
c
u
ra
c
y

749



 Question Answer Confidence 

1 In 1821 the Greeks 
declared ___ on the 

sultan. 

war 0.732 

2 He also introduced 
much of the modern 

mathematical terminol-

ogy and notation, par-

ticularly for 

mathematical analysis, 

such as _________ of a 

mathematical function. 

the notion 0.527 

3 Not only is there much 
ice atop ________, the 

volcano is also being 

weakened by hydro-

thermal activity. 

the volcano 0.790 

Table 2: Example false positives (human judges rated 

these as not-Good) 

 
 Question Answer Confidence 

1 Caesar then pursued 
Pompey to Egypt, 

where Pompey was 

soon  ____.  

murdered 0.471 

2 Over the course of dec-
ades, individual wells 

draw down local tem-

peratures and water 

levels until _______ is 

reached with natural 

flows. 

a new  

equilibrium 

0.306 

3 About 7.5% of world 
sea trade is carried via 

the canal ____.  

today 0.119 

4 Asante and Dahomey 
concentrated on the 

development of “legiti-

mate commerce” in 

__________, forming 

the bedrock of West 

Africa’s modern export 

trade,  

the form of 

palm oil, 

cocoa, tim-

ber, and 

gold. 

0.029 

Table 3: Example false negatives (human judges rated 

these Good) 

 

7.2 Feature Analysis 

To ensure that all of the gain of the classifier was 

not coming from only a handful of isolated fea-
tures, we examined the mean values for each fea-

ture’s learned weight in the model over the course 

of 10 cross-validation folds, and then sorted the 
means for greater clarity (Figure 8). The weights 

indeed seem to be well distributed across many 

features. 

 
Figure 8: Feature weight means and standard deviations. 

8 Discussion and Future Work 

We have presented a method that determines 

which gaps in a sentence to ask questions about by 

training a classifier that largely agrees with human 
judgments on question quality. We feel this effort 

is complementary to the past work on question 

generation, and represents another step towards 

helping self-motivated learners with automatically 
generated tests. 

In our future work, we hope to expand the set of 

features as described in Section 7. We additionally 
intend to cast the sentence selection problem as a 

separate learning problem that can also be trained 

from human judgments. 

References 
Manish Agarwal and Prashanth Mannem. 2011. Auto-

matic Gap-fill Question Generation from Text 

Books. In Proceedings of the 6th Workshop on In-
novative Use of NLP for Building Educational Ap-

plications. Portland, OR, USA. pages 56-64. 

Richard C. Anderson and W. Barry Biddle. 1975. On 

asking people questions about what they are read-

ing. In G. Bower (Ed.) Psychology of Learning and 

Motivation, 9:90-132. 

Christopher M. Bishop. 2006. Pattern Recognition and 

Machine Learning. New York: Springer, 2006. 

Jonathan C. Brown, Gwen A. Frishkoff, and Maxine 

Eskenazi. 2005. Automatic Question Generation for 

Vocabulary Assessment. In Proceedings of 
HLT/EMNLP 2005. Vancouver, Canada: Associa-

tion for Computational Linguistics. pages 819-826. 

Rich Caruana and Alexandru Niculescu-Mizil. 2006. An 

Empirical Comparison of Supervised Learning Al-

gorithms. In Proceedings of ICML 2006. 

m
e
a
n
 a
n
d
 s
td
. 
d
e
v.
 o
f 
fe
a
tu
re
 w
e
ig
h
t

750



Chris Callison-Burch and Mark Dredze. 2010. Creating 

Speech and Language Data with Amazon's Me-

chanical Turk. In Proceedings of NAACL 2010 

Workshop on Creating Speech and Language Data 

with Amazon's Mechanical Turk. Los Angeles, CA. 

pages 1-12. 

Wei Chen, Gregory Aist, and Jack Mostow. 2009. Gen-

erating Questions Automatically from Information-

al Text. In S. Craig & S. Dicheva (Ed.), 

Proceedings of the 2nd Workshop on Question 

Generation. 

Sérgio Curto, Ana Cristina Mendes, and Luísa Coheur. 

2011. Exploring linguistically-rich patterns for 

question generation. In Proceedings of the UCNLG 

+ eval: Language Generation and Evaluation 

Workshop. Edinburgh, Scotland: Association for 

Computational Linguistics. Pages 33-38. 

Michael Heilman and Noah A. Smith. 2010a. Good 
Question! Statistical Ranking for Question Genera-

tion. In Proceedings of NAACL/HLT 2010. pages 

609-617. 

Michael Heilman and Noah A. Smith. 2010b. Rating 

Computer-Generated Questions with Mechanical 

Turk. In Proceedings of NAACL 2010 Workshop on 

Creating Speech and Language Data with Ama-

zon's Mechanical Turk. Los Angeles, CA. pages 35-

40. 

Ayako Hoshino and Hiroshi Nakagawa. 2005. A real-

time multiple-choice question generation for lan-
guage testing - a preliminary study -. In Proceed-

ings of the 2nd Workshop on Building Educational 

Applications Using NLP. Ann Arbor, MI, USA: 

Association for Computational Linguistics. pages 

17-20. 

Panagiotis G. Ipeirotis, Foster Provost, Jing Wang . 

2010. In Proceedings of the ACM SIGKDD Work-

shop on Human Computation (HCOMP’10). 

Klaus Krippendorff. 2004. Content Analysis: An Intro-

duction to Its Methodology. Thousand Oaks, CA: 

Sage. 

Ruslan Mitkov and Le An Ha. 2003. Computer-Aided 
Generation of Multiple-Choice Tests. Proceedings 

of the HLT-NAACL 2003 Workshop on Building 

Educational Applications Using Natural Language 

Processing, pages 17-22. 

Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 

2006. A computer-aided environment for generat-

ing multiple choice test items. Natural Language 

Engineering, 12(2): 177-194. 

Ani Nenkova, Lucy Vanderwende, and Kathleen 

McKeown. 2006. A Compositional Context Sensi-

tive Multidocument Summarizer. In Proceedings of 

SIGIR 2006. pages 573-580. 

Rodney D. Nielsen. 2008. Question Generation: Pro-

posed Challenge Tasks and Their Evaluation. In V. 

Rus, & A. Graesser (Ed.), In Proceedings of the 

Workshop on the Question Generation Shared Task 

and Evaluation Challenge. Arlington, VA. 

Jorge Nocedal. 1980. Updating Quasi-Newton Matrices 

with Limited Storage. Mathematics of Computa-

tion, 35:773-782. 

Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. 

The Proposition Bank: An Annotated Corpus of 

Semantic Roles. Computational Linguistics, vol. 

31, no. 1, pp. 71--106. 

Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami 

Suzuki, Kristina Toutanova, Michael Gamon, Wen-

tau Yih, and Lucy Vanderwende. 2012. MSR 

SPLAT, a language analysis toolkit. In Proceedings 
of NAACL HLT 2012 Demonstration Session. 

http://research.microsoft.com/projects/msrsplat . 

Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean, 

Svetlana Stoyanchev and Cristian Moldovan. 2010. 

Overview of The First Question Generation Shared 

Task Evaluation Challenge. In Proceedings of the 

Third Workshop on Question Generation. Pitts-

burgh, PA, USA. pages 45-57. 

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and 

Andrew Y. Ng. 2008. Cheap and Fast—but is it 

Good? Evaluating non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of 

EMNLP’08. pages 254-263. 

Lucy Vanderwende. 2008. The Importance of Being 

Important: Question Generation. In Proceedings of 

the 1
st
 Workshop on the Question Generation 

Shared Task Evaluation Challenge, Arlington, VA. 

Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 

O’Hara. 1999. Development and use of a gold-

standard data set for subjectivity classifications. In 

Proceedings of ACL 1999. 

John H. Wolfe. 1976. Automatic question generation 

from text - an aid to independent study. In Proceed-
ings of the ACM SIGCSE-SIGCUE technical sym-

posium on Computer science and education. New 

York, NY, USA: ACM. pages 104-112. 

 

 

751


