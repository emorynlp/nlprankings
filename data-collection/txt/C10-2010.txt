81

Coling 2010: Poster Volume, pages 81–89,

Beijing, August 2010

Improved Unsupervised Sentence Alignment for Symmetrical and

Asymmetrical Parallel Corpora

Fabienne Braune

Alexander Fraser

Institute for Natural Language Processing

Universit¨at Stuttgart

{braunefe,fraser}@ims.uni-stuttgart.de

Abstract

We address the problem of unsupervised
and language-pair independent alignment
of symmetrical and asymmetrical parallel
corpora. Asymmetrical parallel corpora
contain a large proportion of 1-to-0/0-to-1
and 1-to-many/many-to-1 sentence corre-
spondences. We have developed a novel
approach which is fast and allows us to
achieve high accuracy in terms of F1 for
the alignment of both asymmetrical and
symmetrical parallel corpora. The source
code of our aligner and the test sets are
freely available.

1

Introduction

Sentence alignment is the problem of, given a par-
allel text, ﬁnding a bipartite graph matching min-
imal groups of sentences in one language to their
translated counterparts. Because sentences do not
always align 1-to-1, the sentence alignment task is
non-trivial.

The achievement of high accuracy with mini-
mal consumption of computational resources is a
common requirement for sentence alignment ap-
proaches. However, in order to be applicable to
parallel corpora in any language without requir-
ing a separate training set, a method for sentence-
alignment should also work in an unsupervised
fashion and be language pair independent. By
“unsupervised”, we denote methods that infer the
alignment model directly from the data set to be
aligned. Language pair independence refers to ap-
proaches that require no speciﬁc knowledge about
the languages of the parallel texts to align.

We have developed an approach to unsuper-
vised and language-pair independent sentence
alignment which allows us to achieve high accu-
racy in terms of F1 for the alignment of both sym-
metrical and asymmetrical parallel corpora. Due
to the incorporation of a novel two-pass search
procedure with pruning, our approach is accept-
ably fast. Compared with Moore’s bilingual sen-
tence aligner (Moore, 2002), we obtain an average
F1 of 98.38 on symmetrical parallel documents,
while Moore’s aligner achieves 94.06. On asym-
metrical documents, our approach achieves 97.67
F1 while Moore’s aligner obtains 88.70. On av-
erage, our sentence aligner is only about 4 times
slower than Moore’s aligner.

This paper is organized as follows: previous
work is described in section 2. In section 3, we
present our approach. Finally, in section 4, we
conduct an extensive evaluation, including a brief
insight into the impact of our aligner on the over-
all performance of an MT system.

2 Related Work

Among approaches that are unsupervised and lan-
guage independent, (Brown et al., 1991) and (Gale
and Church, 1993) use sentence-length statistics
in order to model the relationship between groups
of sentences that are translations of each other. As
shown in (Chen, 1993) the accuracy of sentence-
length based methods decreases drastically when
aligning texts containing small deletions or free
translations. In contrast, our approach augments a
sentence-length based model with lexical statistics
and hence constantly provides high quality align-
ments.

(Moore, 2002) proposes a multi-pass search

82

procedure where sentence-length based statistics
are used in order to extract the training data for
the IBM Model-1 translation tables. The ac-
quired lexical statistics are then combined with
the sentence-length based model in order to ex-
tract 1-to-1 correspondences with high accuracy1.
Moore’s approach constantly achieves high preci-
sion, is robust to sequences of inserted and deleted
text, and is fast. However, the obtained recall is
at most equal to the proportion of 1-to-1 corre-
spondences contained in the parallel text to align.
This point is especially problematic when align-
ing asymmetrical parallel corpora.
In contrast,
our approach allows to extract 1-to-many/many-
to-1 correspondences. Hence, we achieve high
accuracy in terms of precision and recall on both
symmetrical and asymmetrical documents. More-
over, because we use, in the last pass of our multi-
pass method, a novel two-stage search procedure,
our aligner also requires acceptably low computa-
tional resources.

(Deng et al., 2006) have developed a multi-
pass method similar to (Moore, 2002) but where
the last pass is composed of two alignment pro-
cedures: a standard dynamic programming (DP)
search that allows one to ﬁnd many-to-many
alignments containing a large amount of sentences
in each language and a divisive clustering al-
gorithm that optimally reﬁnes those alignments
through iterative binary splitting. This alignment
method allows one to ﬁnd, in addition to 1-to-
1 correspondences, high quality 1-to-many/many-
to-1 alignments. However, 1-to-0 and 0-to-1 cor-
respondences are not modeled in this approach2.
This leads to poor performance on parallel texts
containing that type of correspondence. Further-
more performing an exhaustive DP search in or-
der to ﬁnd large size many-to-many alignments
involves high computational costs. In comparison
to (Deng et al., 2006), our approach works in the
opposite way. Our two-step search procedure ﬁrst
1The used search heuristic is a forward-backward compu-
tation with a pruned dynamic programming procedure as the
forward pass.

2In (Deng et al., 2006), p. 5, the p(ak) = p(x, y) which
determines the prior probability of having an alignment con-
taining x source and y target sentences, is equal to 0 if x < 1
or y < 1. As p(ak) is a multiplicative factor of the model,
the probability of having an insertion or a deletion is always
equal to 0.

ﬁnds a model-optimal alignment composed of the
smallest possible correspondences, namely 1-to-
0/0-to-1 and 1-to-1, and then merges those cor-
respondences into larger alignments. This allows
the ﬁnding of 1-to-0/0-to-1 alignments as well
as high quality 1-to-many/many-to-1 alignments,
leading to high accuracy on parallel texts but also
on corpora containing large blocs of inserted or
deleted text. Furthermore, our approach keeps the
computational costs of the alignment procedure
low: our aligner is, on average, about 550 times
faster than our implementation3 of (Deng et al.,
2006).

Many other approaches to sentence-alignment
are either supervised or language dependent. The
approaches by (Chen, 1993), (Ceausu et al., 2006)
or (Fattah et al., 2007) need manually aligned
pairs of sentences in order to train the used align-
ment models. The approaches by (Wu, 1994),
(Haruno and Yamazaki, 1996), (Ma, 2006) and
(Gautam and Sinha, 2007) require an externally
supplied bilingual lexicon.
the ap-
proaches by (Simard and Plamondon, 1998) or
(Melamed, 2000) are language pair dependent in-
sofar as they are based on cognates.

Similarly,

3 Two-Step Clustering Approach
We present here our two-step clustering approach
to sentence alignment4 which is the main contri-
bution of this paper. We begin by giving the main
ideas of our approach using an introductory exam-
ple (section 3.1). Then we show to which extent
computational costs are reduced in comparison to
a standard DP search (section 3.2) before present-
ing the theoretical background of our approach
(section 3.3). We further discuss a novel prun-
ing strategy used within our approach (section
3.4). This pruning technique is another important
contribution of this paper. Next, we present the
alignment model (section 3.5) which is a slightly
modiﬁed version of the alignment model used in
(Moore, 2002). Finally, we describe the overall

3In order to provide a precise comparison between our
aligner and (Deng et al., 2006), we have implemented their
model into our optimized framework.

4Note that our approach does not aim to ﬁnd many-to-
many alignments. None of the unsupervised sentence align-
ment approaches discussed in section 2 are able to correctly
ﬁnd that type of correspondence.

83

procedure required to align a parallel text with our
method (section 3.6).

sion:

D(l, m)∗ =

3.1 Sketch of Approach
Consider a parallel text composed of six source
language sentences Fi and four target language
sentences Ej. Further assume that the correct
alignment between the given texts is composed of
four correspondences: three 1-to-1 alignments be-
tween F1, E1; F2, E2 and F6, E4 as well as a 3-to-
1 alignment between F3, F4, F5 and E3. Figure 1
illustrates this alignment.

Figure 1: Correct Alignment between Fi and Ej

In the perspective of a statistical approach to
sentence alignment, the alignment in ﬁgure 1 is
found by computing the model-optimal alignment
A∗ for the bitext considered:

A∗ = argmax

A Yak∈A

SCORE(ak)

(1)

where SCORE(ak) denotes the score attributed
by the alignment model5 to a minimal alignment
ak composing A∗. The optimization given in
equation 1 relies on two commonly made assump-
(c1) a model-optimal alignment A∗ can
tions:
be decomposed into k minimal and independent
alignments ak; (c2) each alignment ak depends
only on local portions of text in both languages.

The search for A∗ is generally performed us-
ing a dynamic programming (DP) procedure over
the space formed by the l source and m target
sentences. The computation of A∗ using a DP
search relies on the assumption (c3) that sentence
alignment is a monotonic and continuous process.
The DP procedure recursively computes the opti-
mal score D(l, m)∗ for a sequence of alignments
covering the whole parallel corpus. The optimal
score D(l, m)∗ is given by the following recur-

5The alignment model will be presented in section 3.5.

min

0≤x,y≤R , x=1∨y=1

D(l − x, m − y)∗
− log SCORE(ak)
(2)

where x denotes the number of sentences on the
source language side of ak and y the number of
sentences on the target language side of ak.

The constant R constitutes an upper bound to
the number of sentences that are allowed on each
side of a minimal alignment ak. This constant has
an important impact on the computational costs
of the DP procedure insofar as it determines the
number of minimal alignments that have to be
compared and scored at each step of the recursion
given in equation 2. As will be shown in section
3.2, the number of comparisons increases depend-
ing on R.

The solution we propose to the combinatorial
growth of the number of performed operations
consists of dividing the search for A∗ into two
steps. First, a model-optimal alignment A∗1, in
which the value of R is ﬁxed to 1, is found. Sec-
ond, the alignments a0k composing A∗1 are merged
into clusters mr containing up to R sentences on
either the source or target language side. The
alignment composed of these clusters is A∗R.

The search for the ﬁrst alignment A∗1 is per-
formed using a standard DP procedure as given in
equation 2 but with R = 1. This ﬁrst alignment is,
hence, only composed of 0-to-1, 1-to-0 and 1-to-1
correspondences. Using our example, we show, in
ﬁgure 2, the alignment A∗1 found in the ﬁrst step
of our approach. The neighbors of F4, that is F3
and F5, are aligned as 1-to-0 correspondences.

Figure 2: A∗1 in our Approach (ﬁrst step)

The search for A∗R is performed using a DP
search over the alignments a0k composing A∗1. The
score D(AR)∗ obtained when all alignments a0k ∈
A∗1 have been optimally clustered can be written

F1

F2

F3

F4

F5

F6

E1

E2

E3

E4

F1

F2

F3

F4

F5

F6

E1

E2

E3

E4

84

recursively as:

D(AR − r)∗
D(AR)∗ = min
0≤r≤R
− log SCORE(mr)

(3)

where D(AR−r)∗ denotes the best score obtained
for the preﬁx covering all minimal alignments in
A∗1 except the last r minimal alignments consid-
ered for composing the last cluster mr.

The application of the second step of our ap-
proach is illustrated in ﬁgure 3. The ﬁrst align-
ment, between F1 and E1, cannot be merged to be
part of a 1-to-many or many-to-1 cluster because
the following alignment in A∗1 is also 1-to-1. So
it must be retained as given in A∗1. The ﬁve last
alignments are, however, candidates for compos-
ing clusters. For instance, the alignment F2-E2
and F3-, where  denotes the empty string, could
be merged in order to compose the 2-to-1 cluster
F2,F3-E2. However, in our example, the align-
ment model chooses to merge the alignments F3-
, F4-E3 and F5- in order to compose the 3-to-1
cluster F3,F4,F5-E3.

Figure 3: A∗R in our Approach (second step)

3.2 Computational Gains
The aim of this section is to give an idea about
why our method is faster than the standard DP
approach. Let C denote the number of compar-
isons performed at each step of the recursion of
the standard DP procedure, as given in equation
2. This amount is equivalent to the number of
possible combinations of x source sentences with
y target sentences. Hence, for an approach ﬁnd-
ing all types of correspondences except many-to-
many, we have:

C = 2R + 1

(4)

In terms of lookups in the word-correspondence
tables of a model including lexical statistics, the

number of operations Cl performed at each step
of the recursion is given by:

Cl = R0 ∗ w2

(5)
where R0 denotes the number of scored sen-
tences6. w denotes the average length of each
sentence in terms of words. The total number of
lookups performed in order to align a parallel text
containing l source and m target sentences using
a standard DP procedure is hence given by:

L = R0 ∗ w2 ∗ l ∗ m

(6)
In the perspective of our two-step search proce-
dure, the computational costs of the search for the
initial alignment A∗1 is given by:
L01 = w2 ∗ l ∗ m

(7)
For the second step of our approach, because A∗R
is a cluster of A∗1, the dynamic programming pro-
cedure used to ﬁnd this alignment is no longer
over the l ∗ m space formed by the source and
target sentences but instead over the space formed
by the minimal alignments a0k in A∗1. The aver-
age number of those alignments is approximately
2 .7 The number of lookups performed at each
l+m
step of our DP procedure is given by:
l + m

L02 = R0 ∗ w2 ∗

(8)
where R0 and w are deﬁned as in equation 6.
The total number of lookups for our clustering ap-
proach is hence given by:
L01+2 = (w2 ∗ l ∗ m) + (R0 ∗ w2 ∗
) (9)
In order to compare the costs of our approach and
a standard DP search over the l ∗ m space formed
by the source and target sentences, we re-write
equation 6 as:
L = (w2 ∗ l ∗ m) + ((R0 − 1) ∗ w2 ∗ l ∗ m) (10)
The comparison of equation 9 with equation 10
shows that the computational gains obtained using
our two-step approach reside in the reduction of
2 .8
the search space from l ∗ m to l+m
6In a framework where no caching of scores is performed,
we have R0 = R2 + R + 1 compared sentences while score-
caching allows one to reduce R0 to R.

l + m

7Note that this amount tends to l + m when A∗1 contains

2

2

a large number of 0-to-1/1-to-0 correspondences.

8It should be noted that through efﬁcient pruning, the
search space of the standard (DP) procedure can be further
reduced, see section 3.4.

F1

F2

F3

F4

F5

F6

E1

E2

E3

E4

85

3.3 Theoretical Background
We now present the theoretical foundation of our
approach. First, we rewrite equation 1 in a more
detailed fashion as:

A∗R = argmax

A

Yak(xk,yk)∈AR

P (ak(xk, yk), sq

i , tr
j )

(11)
with 0 ≤ xk, yk ≤ R, where R denotes the max-
imal amounts x and y of source and target lan-
guage sentences composing a minimal alignment
ak(xk, yk). The distribution P (ak(xk, yk), sq
i , tr
j )
speciﬁes the alignment model presented in section
3.5.

i and tr

As seen in section 3.1, the formulation of the
alignment problem as given in equation 11 and the
use of a DP search in order to solve this equation
rely on the assumptions (c1) to (c3). Following
these assumptions, a model-optimal alignment A∗1
can be deﬁned as an ordered set of minimal align-
ments a0k(xk, yk), with 0 ≤ xk, yk ≤ 1, where the
In other
aligned portions of text are sequential.
words, if the k − th alignment a0k(xk, yk) con-
tains the sequences sq
j of source and tar-
get language sentences, then the next alignment
a0k+1(xk+1, yk+1) is composed of the sequences
q+1 and tv
r+1. Hence, each alignment composing
su
AR, with R > 1, can be obtained through sequen-
tial merging of a series of alignments a0k(xk, yk) ∈
A∗1.9 Accordingly, the sequences of sentences su
1
1 with
and tv
r+1. It can then be assumed that (c4) the
q+1 and tv
su
ordered set of minimal alignments composing A∗R
under equation 11 is equivalent to the set of clus-
ters obtained by sequentially merging the minimal
alignments composing A∗1. Following assump-
tion (c4), the optimization over ak(xk, yk) ∈ AR
is equivalent to an optimization over the merged
alignments mr(xr, yr) ∈ AR. Hence, equation 11
is equivalent to:

1 are obtained by merging sq

1 and tr

P (mr(xr, yr), su

i , tv
j )

A∗R = argmax

AR

Ymr(xr,yr)∈AR

(12)
where each mr(xr, yr) is obtained by merging r
minimal alignments a0k(xk, yk) ∈ A∗1.

j ).

i , tv

l−x+1, tm

The computation of A∗R is done in two
steps. First, a model-optimal alignment A∗1 is
found using a standard DP procedure as de-
ﬁned in equation 2 but with R = 1 and where
SCORE(ak) is given by the alignment model
m−y+1). In the second step,
− log P (ak, sl
the search procedure used to ﬁnd the optimal
clusters is deﬁned as in equation 3 but where
SCORE(mr) is given by the alignment model
− log P (mr, su
3.4 Search Space Pruning
In order to further reduce the costs of ﬁnding A∗1,
we initially pruned the search space in the same
fashion as (Moore, 2002). We explored a nar-
row band around the main diagonal of the bi-
text to align. Each time the approximated align-
ment came close to the boundaries of the band,
the search was reiterated with a larger band size.
However, the computational costs for alignments
that were not along the diagonal quickly increased
with this pruning strategy. A high loss of efﬁ-
ciency was hence observed when aligning asym-
metrical documents with this technique. Inciden-
tally, Moore reports, in his experiments, that for
the alignment of a parallel text containing 300
deleted sentences, the computational costs of his
pruned DP procedure is 40 times higher than for a
corpus containing no deletions.

In order to overcome this problem, we devel-
oped a pruning strategy that allows us to avoid the
loss of efﬁciency occurring when aligning asym-
metrical documents. Instead of exploring a nar-
row band around the main diagonal of the text to
align, we use sentence-length statistics in order to
compute an approximate path through the consid-
ered bitext. Our search procedure then explores
the groups of sentences that are around this path.
If the approximated alignment comes close to the
boundaries of the band, the search is re-iterated.

The path initially provided using a sentence-
length model10 and then iteratively reﬁned is
closer to the correct alignment than the main di-
agonal of the bitext to align. Hence, the approxi-
mated alignment does not come close to the band

9Alignments of type 1-to-0/0-to-1 and 1-to-1 are assumed
to be clusters where a minimal alignment a0k(xk, yk) ∈ A∗1
has been merged with the empty alignment e0(0, 0)(, ).

10The used model is the sentence-length based component
of (Moore, 2002), which is able to ﬁnd 1-to-0/0-to-1 corre-
spondences.

86

as often as when searching around the main di-
agonal. This results in relatively high computa-
tional gains, especially for asymmetrical parallel
texts (see section 4).

3.5 Moore’s Alignment Model
The model we use is basically the same as in
(Moore, 2002) but minor modiﬁcations have been
made in order to integrate this model in our two-
step clustering approach. The three component
distributions of the model are given by11:

i , tr

P (ak, sq

i|ak)P (tr

j ) = P (ak)P (sq

j|ak, sq
i )
(13)
The ﬁrst component, P (ak), speciﬁes the gen-
eration of a minimal alignment ak. The second
component, P (sq
i|ak), speciﬁes the generation of
a sequence sq
i of source language sentences in
a minimal alignment ak. The last component,
i.e. P (tr
i ), speciﬁes the generation of a se-
quence of target language sentences depending on
a sequence of generated source sentences.

j|ak, sq

Our ﬁrst modiﬁcation to Moore’s model con-
cerns the component distribution P (ak).
In the
second pass of our two-step approach, which is
the computation of the model-optimal clustered
alignment A∗R, we estimate P (ak) by computing
the relative frequency of sequences of alignments
a0k in the initial alignment A∗1 that are candidates
for composing a cluster mr of speciﬁc size.12 A
second minimal modiﬁcation to Moore’s model
j|ak, sq
concerns the lexical constituent of P (tr
i ),
which we denote here by P (fb|en, ak). In contrast
with Moore, we use the best alignment (Viterbi
alignment) of each target word fb with all source
words en, according to IBM Model-1:

P (fb|en, ak) =

arg maxle

n=1 Pt(fb|en)
le + 1

(14)

where le denotes the number of words in the
source sentence(s) of ak. Our experimental results
have shown that this variant performed slightly
better than Moore’s summing over all alignments.

11In order to simplify the presentation of the model, we

use the short notation ak for denoting ak(xk, yk)

12For the computation of A∗1, the distribution P (ak) is de-

ﬁned as in Moore’s work.

1, tm

3.6 Alignment Procedure
In order to align a parallel text (sl
1 ) we use
a multi-pass procedure similar to (Moore, 2002)
but where the last pass is replaced by our two-
step clustering approach. In the ﬁrst pass, an ap-
proximate alignment is computed using sentence-
length based statistics and the one-to-one corre-
spondences with likelihood higher than a given
threshold are selected for the training of the IBM
Model-1 translation tables13. Furthermore, each
found alignment is cached in order to be used as
the initial diagonal determining the search space
for the next pass. In the second pass, the corpus is
re-aligned according to our two-step approach: (i)
a model-optimal14 alignment containing at most
one sentence on each side of the minimal align-
ments ak(xk, yk) is found; (ii) those alignments
are model-optimally merged in order to obtain an
alignment containing up to R sentences on each
side of the clusters mr(xr, yr).
In our experi-
ments, a maximum number of 4 sentences is al-
lowed on each side of a cluster.

4 Experiments
We evaluate our approach (CA) using three base-
lines against which we compare alignment qual-
ity and computational costs.15 The ﬁrst (Mo) is
the method by (Moore, 2002). As a second base-
line (Std), we have implemented an aligner that
ﬁnds the same type of correspondences as our ap-
proach but performs a standard DP search instead
of our two-pass clustering procedure and imple-
ments Moore’s pruning strategy. Our third base-
line (Std P.) is similar to (Std) but integrates our
pruning technique.16 We also evaluate the impact
13Words with frequency < 3 in the corpus have been

dropped.

14This is optimal according to the alignment model which

will be presented in section 3.5.

15We do not evaluate sentence-length based methods in
our experiments because these methods obtain an F1 which
is generally about 10% lower than for our approach on
symmetrical documents. For asymmetrical documents the
performance is even worse.
For example, when using
Gale&Church F1 sinks to 13.8 on documents which are not
aligned at paragraph level and contain small deletions.

16We do not include (Deng et al., 2006) in our exper-
iments because our implementation of this aligner is 550
times slower than our proposed method and the inability to
ﬁnd 1-to-0/0-to-1 correspondences makes it inappropriate for
asymmetrical documents.

87

0-1/1-0 Oth.
0.005% 0.85%
0.007% 0.53%
1.4%
4.3%
49%
0.01%

1-N/N-1
1-1
88.2% 10.9 %
91.9% 7.5%
91.6% 2.7%
44.8% 6.2%

S
Tot.
1
3,877
2
2,646
3
23,715
4
2,606
Table 1: Test Set for Evaluation with 2 ≤ N ≤ 4
of our aligner on the overall performance of an
MT system.
Evaluation. We evaluate the alignment accu-
racy of our approach using four test sets annotated
at sentence-level. The two ﬁrst are composed
of hand aligned documents from the Europarl
corpus for the language-pairs German-to-English
and French-to-English. The third is composed
of an asymmetric document from the German-to-
English part of the Europarl corpus. Our fourth
test set is a version of the BAF corpus (Simard,
1998), where we corrected the tokenization. BAF
is an interesting heterogeneous French-to-English
test set composed of 11 texts belonging to four
different genres. The types of correspondences
composing our test sets are given in table 1. The
17. Only
metrics used are precision, recall and F1
alignments that correspond exactly to reference
alignments count as correct. The computational
costs required for each approach are measured in
seconds. The time required to train IBM Model-1
is not included in our calculations18.
Summary of Results. Regarding alignment ac-
curacy, the results in table 2 show that (CA) ob-
tains, on average, an F1 that is 4.30 better than
for (Mo) on symmetrical documents. The results
in table 3 show that, on asymmetrical texts, (CA)
achieves an F1 which is 8.97 better than (Mo).
The accuracy obtained using (CA), (Std) and (Std
P.) is approximately the same. We have further
compared the accuracy of (CA) with (Std) for
ﬁnding 1-to-many/many-to-1 alignments. The ob-
tained results show that (CA) achieves an F1 that
is 5.0 better than (Std).

Regarding computational costs,

the time re-
quired by (CA) is on average 4 times larger than

17We measure precision, recall and F1 on the 1-to-N/N-to-
1 alignments, N >= 1, which means that we view insertions
and deletions as “negative” decisions, like Moore.

18The reason for this decision is that our optimized frame-
work trains the Model-1 translation tables far faster than
Moore’s bilingual sentence aligner.

for (Mo) when aligning symmetrical documents.
On asymmetrical documents, (Mo) is, however,
only 1.5 times faster than (CA). Compared to
(Std), (CA) is approximately 6 times faster on
symmetrical and 80 times faster on asymmetrical
documents. The time of (Std P.) is 3 times higher
than for (CA) on symmetrical documents and 22
times higher on asymmetrical documents. This
shows that, ﬁrst, our pruning technique is more
efﬁcient than Moore’s and, second, that the main
increase in speed is due to the two step clustering
approach.
Discussion. On the two ﬁrst
test sets, (Mo)
achieves high precision while the obtained recall
is limited by the number of correspondences that
are not 1-to-1 (see table 1). Regarding (Std), (Std
P.) and (CA), all aligners achieve high precision
as well as high recall, leading to an F1 which is
over 98% for both documents. The computational
costs of (CA) for the alignment of symmetrical
documents are, on average, 4 times higher than
(Mo), 6 times lower than (Std) and 3.5 times
lower than (Std P.). On our third test set (Mo)
achieves, with an F1 of 88.70, relatively poor
recall while the other aligners reach precision
and recall values that are over 98%. Regarding
the computational costs, (CA) is only 1.5 times
slower than (Mo) on asymmetrical documents
while it is 80 times faster than (Std) and about 22
times faster than (Std P.). On our fourth test set
all evaluated aligners perform approximately the
same than on Europarl. While (Mo) obtains, with
94.46, an F1 which is the same as for Europarl,
(CA) performs, with an F1 of 97.67, about
1% worse than on Europarl. A slightly larger
decrease of 1.6% is observed for (Std) which
obtains 96.81 F1. Note, however, that (CA), (Std)
and (Std P.) still perform about 3% better than
(Mo). Regarding computational costs, (CA) is
4 times slower than (Mo) and 40 times faster
than (Std). The high difference in speed between
our approach and (Std) is due to the fact that the
BAF corpus contains texts of variable symmetry
while (Std) shows a great speed decrease when
aligning asymmetrical documents. Finally, we
have compared the accuracy of (Std) and (CA) for
the ﬁnding of 1-to-many/many-to-1 alignments
containing at least 3 sentences on the “many”

88

Appr. Lang.
Mo
Mo
Std
Std
Std P.
Std P.
CA
CA

D-E
F-E
D-E
F-E
D-E
F-E
D-E
F-E

Prec. Rec.
87.88
98.75
91.56
98.97
98.57
98.42
98.83
98.45
98.37
98.49
98.78
98.41
98.70
98.25
98.00
98.60

F1
92.99
95.12
98.49
98.64
98.43
98.60
98.47
98.30

Speed
935s
1,661s
24,152s
35,041s
13,387s
21,848s
3,461s
6,978s

Table 2: Performance on Europarl

Appr.
Mo
Std
Std P.
CA

Prec. Rec.
81.08
97.90
97.66
97.74
97.81
97.74
97.38
97.97

F1
88.70
97.70
97.77
97.67

Speed
552s
71,475s
17,502s
800s

Table 3: Performance on asym. documents

Appr.
Mo
Std
CA

Prec. Rec.
92.43
96.58
96.80
96.82
97.05
97.63

F1
94.46
96.81
97.34

Speed
563s
84,988s
2,137s

Table 4: Performance on BAF

This experiment has shown that (Std)
side.
ﬁnds a larger amount of those alignments while
making numerous wrong conjectures. On the
other hand, (CA) ﬁnds less 1-to-many/many-to-1
correspondences but makes only few incorrect
hypotheses. Hence, F1 is about 5% better for
(CA).

MT evaluation We also measured the impact
of 1-to-N/N-to-1 alignments (which are not ex-
tracted by Moore) on MT. We used standard set-
tings of the Moses toolkit, and the Europarl de-
vtest2006 set as our test set. We ran MERT sep-
arately for each system. System (s1) was trained
just on the 1-to-1 alignments extracted from the
Europarl v3 corpus by our system while system
(s2) was trained with all correspondences found.
(s1) obtains a BLEU score of 0.2670 while (s2)
obtains a BLEU score of 0.2703. Application of
the pairwise bootstrap test (Koehn, 2004) shows
that (s2) is signiﬁcantly better than (s1).

5 Conclusion

We have addressed the problem of unsupervised
and language-pair independent alignment of sym-

metrical and asymmetrical parallel corpora. We
have developed a novel approach which is fast
and allows us to achieve high accuracy in terms
of F1 for the alignment of bilingual corpora.
Our method achieved high accuracy on symmet-
rical and asymmetrical parallel corpora, and we
have shown that the 1-to-N/N-to-1 alignments ex-
tracted by our approach are useful. The source
code of the aligner and the test sets are available
at http://sourceforge.net/projects/gargantua .

6 Acknowledgements
The ﬁrst author was partially supported by the
Hasler Stiftung19. Support for both authors was
provided by Deutsche Forschungsgemeinschaft
grants Models of Morphosyntax for Statistical
Machine Translation and SFB 732.

References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In In
Proceedings of 29th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 169–176.

Ceausu, Alexandru, Dan Stefanescu, and Dan Tuﬁs.
2006. Acquis communautaire sentence alignment
using support vector machines.
In LREC 2006:
Fifth International Conference on Language Re-
sources and Evaluation.

Chen, Stanley F. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics, pages 9–16.

Deng, Yoggang, Shankar Kumar, and William Byrne.
2006. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 12:1–26.

Fattah, Mohamed Abdel, David B. Bracewell, Fuji
Ren, and Shingo Kuroiwa. 2007. Sentence align-
ment using p-nnt and gmm. Computer Speech and
Language, (21):594–608.

Gale, William A. and Kenneth Ward Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75–102.

Gautam, Mrityunjay and R. M. K. Sinha. 2007. A
program for aligning sentences in bilingual cor-
pora. Proceedings of the International Conference

19http://www.haslerstiftung.ch/.

89

on Computing: Theory and Applications, ICCTA
’07, (1):480–484.

Haruno, M. and T. Yamazaki.

High-
performance bilingual text alignment using statisti-
cal and dictionary information.
In Proceedings of
ACL ’96, pages 131–138.

1996.

Koehn, Philipp. 2004. Statistical signiﬁcance tests for
machine translation evaluation. In Lin, Dekang and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In LREC 2006: Fifth In-
ternational Conference on Language Resources and
Evaluation.

Melamed, I. Dan.

2000. Models of translational
equivalence among words. Computational Linguis-
tics, 26:221–249.

Moore, Robert. 2002. Fast and accurate sentence
alignment of bilingual corpora. In In Proceedings
of 5th Conference of the Association for Machine
Translation in the Americas, pages 135–244.

Simard, Michel and Pierre Plamondon. 1998. Bilin-
gual sentence alignment: Balancing robustness and
accuracy. Machine Translation, 13(1):59–80.

Simard, Michel. 1998. The baf: A corpus of english-
french bitext. In Proceedings of LREC 98, Granada,
Spain.

Wu, Dekai. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In In Pro-
ceedings of the 32nd Annual Conference of the
Association for Computational Linguistics, 80–87,
Las, pages 80–87.

