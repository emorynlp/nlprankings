








































Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation

Ahmed El Kholy, Nizar Habash
Center for Computational Learning Systems, Columbia University

{akholy,habash}@ccls.columbia.edu

Gregor Leusch, Evgeny Matusov
Science Applications International Corporation

{gregor.leusch,evgeny.matusov}@saic.com

Hassan Sawaf
eBay Inc.

hsawaf@ebay.com

Abstract

An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.

1 Introduction

One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of

this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.

Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.

2 Related Work

Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al., 2008; Bertoldi et al.,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajič et al., 2000) as well as un-
related languages (Koehn et al., 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.

The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language



(Khalilov et al., 2008).
The second strategy is based on phrase pivot-

ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.

The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al., 2008).

In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.

Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.

To our knowledge, there hasn’t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and Köprü, 2010).

3 Pivoting Strategies

In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.

3.1 Sentence Pivoting

In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.

3.2 Phrase Pivoting

In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-

to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.

Since we build our models on top of Moses
phrase-based SMT (Koehn et al., 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (φ) and the
phrase probabilities (pw)

φ(f |a) =
∑
e
φ(f |e)φ(e|a)

φ(a|f) =
∑
e
φ(a|e)φ(e|f)

pw(f |a) =
∑
e
pw(f |e)pw(e|a)

pw(a|f) =
∑
e
pw(a|e)pw(e|f)

where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.

We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al. (2010).

As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2

We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.

3.3 Filtering for Phrase Pivoting

The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted

1Four different phrase translation scores are computed in
Moses’ phrase tables: two lexical weighting scores and two
phrase translation probabilities.

2The size of the induced phrase table size is computed but
not created.



Training Corpora Phrase Table
Translation Model Size # Phrase Pairs Size
Persian-English ≈4M words 96,04,103 1.1GB
English-Arabic ≈60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ≈2.5TB

Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.

Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.

We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.

4 Approach

One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.

To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.

Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ≤ i ≤ S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ≤ j ≤ T}
is the set of the equivalent target words. The
word alignment between S and T is defined as

A = {(i, j) : i ∈ S and j ∈ T }.

SCS =
|A|
|S|

(1)

TCS =
|A|
|T |

(2)

We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.

In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.

Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.

5 Experiments

In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.

3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al., 2007) in the figures with extensions for
Persian as suggested by Habash (2010).



Persian: "AςtmAd"myAn"dw"kšwr "" " ""‘ .-,ر"دو"()"ن"ا%$#"د ’"
" " " " " " " " " " " " "‘trust"between"the"two"countries’"

English: "trust"between"the"two"countries"

Arabic:" "Alθqħ"byn"Aldwltyn " "" " ""‘ ا52و2$3"34"ا012/ ’"
" " " " " " " " " " " " "‘the"trust"between"the"two"countries’"

Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.

Persian: "AyjAd"cnd"šrkt"mštrk " "" " "‘ 0/.+ک",+*(")'&"ا$#"د ’"
" " " " " " " " " " " " "‘Establish"few"joint"companies’"

English: "joint"ventures"

Arabic:" "bςD"šrkAt"AlmqAwlAt"fy"Albld" "‘ ا<=>&";:"ا89"و6ت",+5"ت"123 ’"
" " " " " " " " " " " " "‘Some"construcBon"companies"in"the"country’"

Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.

5.1 Experimental Setup

In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (≈60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.

Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).

All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al., 2007).
We use MERT (Och, 2003) for decoding weight

4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.

5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.

optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).

We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al., 2002) and METEOR
(Lavie and Agarwal, 2007).

5.2 Linguistic Preprocessing

In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.

One aspect of Arabic’s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). which include con-
junction proclitics, e.g., +ð w+ ‘and’, particle pro-
clitics, e.g., +È l+ ‘to/for’, the definite article +È@



Al+ ‘the’, and the class of pronominal enclitics,
e.g., Ñë+ +hm ‘their/them’. Beyond these cli-
tics, Arabic words inflect for person (PER), gen-
der (GEN), number (NUM), aspect (ASP), mood
(MOD), voice (VOX), state (STT) and case (CAS).
This morphological richness leads to thousands
of inflected forms per lemma and a high degree
of ambiguity: about 12 analyses per word, typ-
ically corresponding to two lemmas on average
(Habash, 2010) We follow El Kholy and Habash
(2010a) and use the PATB tokenization scheme
(Maamouri et al., 2004) in our experiments. which
separates all clitics except for the determiner clitic
Al+(DET) We use MADA v3.1 (Habash and Ram-
bow, 2005; Habash et al., 2009) to tokenize the
Arabic text. We only evaluate on detokenized and
orthographically correct (enriched) output follow-
ing the work of El Kholy and Habash (2010b).

Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix Aë+ +hA and sometimes 	à@+ +An,
or one of the Arabic plural markers. Persian also
possess a closed set of few broken plurals loaned
from Arabic. Moreover, unlike Arabic which ex-
presses definiteness, Persian expresses indefinite-
ness with an enclitic article ø



+ +y ‘a/an’ which

doesn’t have separate forms for singular and plu-
ral. When a noun is modified by one or more
adjective, the indefinite article is attached to the
last adjective. Persian adjectives are similar to
English in expressing comparative and superla-
tive constructions just by adding suffixes Q�K+ +tar
‘+er’ and 	áK
Q

�
K+ +taryn ‘+est’ respectively. Verbal

morphology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al., 2013). We use Perstem
(Jadidinejad et al., 2010) for segmenting Persian
text.

English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-

rating punctuation and splitting off “’s”.

5.3 Baseline Evaluation

We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.

Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6

Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).

We use the best performing setup across the rest
of the experiments.

5.4 Connectivity Strength Features
Evaluation

In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
(Phrase Pivot F1K).

Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9

Table 3: Connectivity strength features experi-
ment result.

The results in Table 3 show that we get a
nice improvement of ≈0.6/0.3 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).

6 Conclusion and Future Work

We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,



to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.

Acknowledgments

The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.

References
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-

erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143–149.

Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.

Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montréal, Canada.

Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.

Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69–77, Athens, Greece, March.

David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.

Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-

glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173–181, Athens, Greece, March.

Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05), pages 573–
580, Ann Arbor, Michigan.

Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49–52, New York City,
USA.

Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.

Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.

Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.

Jan Hajič, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP’2000), pages 7–12, Seat-
tle.

Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, UK.

Carlos Henriquez, Rafael E. Banchs, and José B.
Mariño. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.

Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98–101.

Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731–2734,
Brisbane, Australia.

M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp & i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116–123.



Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177–180, Prague, Czech Re-
public.

Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65–72.

Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP’04), Barcelona, Spain.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231, Prague, Czech Repub-
lic.

Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102–109, Cairo, Egypt.

Evgeny Matusov and Selçuk Köprü. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.

Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–52.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, PA.

Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.

Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901–904, Denver,
CO.

Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484–491, Rochester, New York, April. Association
for Computational Linguistics.

Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154–162, Suntec, Singapore, August.
Association for Computational Linguistics.

Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454–
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.


