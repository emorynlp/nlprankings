



















































Neural Models for Key Phrase Extraction and Question Generation


Proceedings of the Workshop on Machine Reading for Question Answering, pages 78–88
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

78

Neural Models for Key Phrase Extraction and Question Generation

Sandeep Subramanian♠♣ Tong Wang♣ Xingdi Yuan♣ Saizheng Zhang♠
Yoshua Bengio ♠† Adam Trischler♣

♣Microsoft Research, Montréal ♠MILA, Université de Montréal †CIFAR Senior Fellow
sandeep.subramanian.1@umontreal.ca

Abstract

We propose a two-stage neural model
to tackle question generation from doc-
uments. First, our model estimates the
probability that word sequences in a doc-
ument are ones that a human would
pick when selecting candidate answers by
training a neural key-phrase extractor on
the answers in a question-answering cor-
pus. Predicted key phrases then act as tar-
get answers and condition a sequence-to-
sequence question-generation model with
a copy mechanism. Empirically, our key-
phrase extraction model significantly out-
performs an entity-tagging baseline and
existing rule-based approaches. We fur-
ther demonstrate that our question genera-
tion system formulates fluent, answerable
questions from key phrases. This two-
stage system could be used to augment or
generate reading comprehension datasets,
which may be leveraged to improve ma-
chine reading systems or in educational
settings.

1 Introduction

Question answering and machine comprehension
has gained increased interest in the past few
years. An important contributing factor is the
emergence of several large-scale QA datasets (Ra-
jpurkar et al., 2016; Trischler et al., 2016; Nguyen
et al., 2016; Joshi et al., 2017). However, the cre-
ation of these datasets is a labour-intensive and ex-
pensive process that usually comes at significant
financial cost. Meanwhile, given the complexity
of the problem space, even the largest QA dataset
can still exhibit strong biases in many aspects in-
cluding question and answer types, domain cover-
age, linguistic style, etc.

To address this limitation, we propose and eval-
uate neural models for automatic question-answer
pair generation that involves two inter-related
components: first, a system to identify candidate
answer entities or events (key phrases) within a
passage or document (Becker et al., 2012); second,
a question generation module to construct ques-
tions about a given key phrases. As a financially
more efficient and scalable alternative to the hu-
man curation of QA datasets, the resulting system
can potentially accelerate further progress in the
field.

Specifically, We formulate the key phrase ex-
traction component as modeling the probability of
potential answers conditioned on a given docu-
ment, i.e., P (a|d). Inspired by successful work
in question answering, we propose a sequence-to-
sequence model that generates a set of key-phrase
boundaries. This model can flexibly select an ar-
bitrary number of key phrases from a document.
To teach it to assign high probability to human-
selected answers, we train the model on large-
scale, crowd-sourced question-answering datasets.

We thus take a purely data-driven approach
to understand the priors that humans have when
selecting answer candidates, working from the
premise that crowdworkers tend to select enti-
ties or events that interest them when formulat-
ing their own comprehension questions. If this
premise is correct, then the growing collection of
crowd-sourced question-answering datasets (Ra-
jpurkar et al., 2016; Trischler et al., 2016) can be
harnessed to learn models for key phrases of inter-
est to human readers.

Given a set of extracted key phrases, we then
approach the question generation component by
modeling the conditional probability of a ques-
tion given a document-answer pair, i.e., P (q|a, d).
To this end, we use a sequence-to-sequence
model with attention (Bahdanau et al., 2014) and



79

the pointer-softmax mechanism (Gulcehre et al.,
2016). This component is also trained to max-
imize the likelihood of questions estimated on a
QA dataset. When training this component, the
model sees the ground truth answers from the
dataset.

Empirically, our proposed model for key phrase
extraction outperforms two baseline systems by
a significant margin. We support these quantita-
tive findings with qualitative examples of gener-
ated question-answer pairs given documents.

2 Related Work

2.1 Key Phrase Extraction

An important aspect of question generation is
identifying which elements of a given document
are important or interesting to inquire about. Ex-
isting studies formulate key-phrase extraction in
two steps. In the first, lexical features (e.g., part-
of-speech tags) are used to extract a key-phrase
candidate list exhibiting certain types (Liu et al.,
2011; Wang et al., 2016; Le et al., 2016; Yang
et al., 2017). In the second, ranking models are of-
ten used to select a phrase from among the candi-
dates. Medelyan et al. (2009); Lopez and Romary
(2010) used bagged decision trees, while Lopez
and Romary (2010) used a Multi-Layer Perceptron
(MLP) and Support Vector Machine to perform
binary classification on the candidates. Mihalcea
and Tarau (2004); Wan and Xiao (2008); Le et al.
(2016) scored key phrases using PageRank. Heil-
man and Smith (2010b) asked crowdworkers to
rate the acceptability of computer-generated nat-
ural language questions as quiz questions, and
Becker et al. (2012) solicited quality ratings of text
chunks as potential gaps for Cloze-style questions.

These studies are closely related to our pro-
posed work by the common goal of modeling
the distribution of key phrases given a document.
The major difference is that previous studies begin
with a prescribed list of candidates, which might
significantly bias the distribution estimate. In con-
trast, we adopt a dataset that was originally de-
signed for question answering, where crowdwork-
ers presumably tend to pick entities or events that
interest them most. We postulate that the resulting
distribution, learned directly from data, is more
likely to reflect the true relevance of potential an-
swer phrases.

Recently, Meng et al. (2017) proposed a gen-
erative model for key phrase prediction with an

encoder-decoder framework that is able both to
generate words from a vocabulary and point to
words from the document. Their model achieved
state-of-the-art results on multiple keyword-
extraction datasets. This model shares certain sim-
ilarities with our key phrase extractor, i.e., using a
single neural model to learn the probabilities that
words are key phrases. Since their focus was on a
hybrid abstractive-extractive task in contrast to the
purely extractive task in this work, a direct com-
parison between works is difficult.

Yang et al. (2017) used rule-based methods to
extract potential answers from unlabeled text, and
then generated questions given documents and ex-
tracted answers using a pre-trained question gener-
ation model. The model-generated questions were
then combined with human-generated questions
for training question answering models. Experi-
ments showed that question answering models can
benefit from the augmented data provided by their
approach.

2.2 Question Generation

Automatic question generation systems are of-
ten used to alleviate (or eliminate) the burden of
human generation of questions to assess reading
comprehension (Mitkov and Ha, 2003; Kunichika
et al., 2004). Various NLP techniques have been
adopted in these systems to improve generation
quality, including parsing (Heilman and Smith,
2010a; Mitkov and Ha, 2003), semantic role la-
beling (Lindberg et al., 2013), and the use of lex-
icographic resources like WordNet (Miller, 1995;
Mitkov and Ha, 2003). However, the majority
of the proposed methods resort to simple, rule-
based techniques such as template-based slot fill-
ing (Lindberg et al., 2013; Chali and Golestanirad,
2016; Labutov et al., 2015) or syntactic transfor-
mation heuristics (Agarwal and Mannem, 2011;
Ali et al., 2010) (e.g., subject-auxiliary inversion,
(Heilman and Smith, 2010a)). These techniques
generally do not capture the diversity of human
generated questions.

To address this limitation, end-to-end-trainable
neural models have recently been proposed for
question generation in both vision (Mostafazadeh
et al., 2016) and language. For the latter, Du
et al. (2017) used a sequence-to-sequence model
with an attention mechanism derived from the en-
coder states. Yuan et al. (2017) proposed a sim-
ilar architecture but further improved model per-



80

formance with policy gradient techniques. Wang
et al. (2017) proposed a generative model that
learns jointly to generate questions or answers
from documents.

3 Model Description

3.1 Notations

Several components introduced in the following
sections share the same model architecture for en-
coding text sequences. The common notations are
explained in this section.

Unless otherwise specified, w refers to word to-
kens, e to word embeddings and h to the annota-
tion vectors (also commonly referred to as hidden
states) produced by an RNN. Superscripts specify
the source of a word, e.g., d for documents, p for
key phrases, a for (gold) answers, and q for ques-
tions. Subscripts index the position inside a se-
quence. For example, edi is the embedding vector
for the i-th token in the document.

A sequence of words are often encoded into an-
notation vectors (denoted h) by applying a bidi-
rectional LSTM encoder to the corresponding se-
quence of word embeddings. For example, hqj =
LSTM(eqj ,h

q
j−1) is the annotation vector for the

j-th word in a question.

3.2 Key Phrase Extraction

In this section, we describe a simple baseline as
well as two proposed neural models for extracting
key phrases (answers) from documents.

3.2.1 Entity Tagging Baseline
As our first baseline, we use spaCy1 to predict
all entities in a document as relevant key phrases
(call this model ENT). This is motivated by the
fact that entities constitute the largest proportion
(over 50%) of answers in the SQuAD dataset
(Rajpurkar et al., 2016). Entities includes dates
(September 1967), numeric entities (3, five), peo-
ple (William Smith), locations (the British Isles),
and other named concepts (Buddhism).

3.2.2 Neural Entity Selection
The baseline model above naı̈vely selects all en-
tities as candidate answers. One pitfall is that
it exhibits high recall at the expense of precision
(Table 1), since not all entities lead to interesting
questions. We first attempt to address this with a
neural entity selection model (NES) that selects a

1https://spacy.io/docs/usage/entity-recognition

subset of entities from a list of candidates provided
by our ENT baseline. Our neural model takes
as input a document (i.e., a sequence of words),
D = (wd1 , . . . , w

d
nd

), and a list of ne entities as a
sequence of (start, end) locations within the doc-
ument, E = ((estart1 , e

end
1 ), . . . , (e

start
ne , e

end
ne )).

The model is then trained on the binary classifica-
tion task of predicting whether an entity overlaps
with any of the human-provided answers.

Specifically, we maximize
∑ne

i log(P (ei|D)).
We parameterize P (ei|D) using a three-layer mul-
tilayer perceptron (MLP) that takes as input the
concatenation of three vectors 〈hdnd ;h

d
avg;hei〉.

Here, hdavg and h
d
nd

are the average and the fi-
nal state of the document annotation vectors, re-
spectively, and hei is the average of the annota-
tion vectors corresponding to the i-th entity (i.e.,
hdestarti

, . . . ,hd
eendi

).

During inference, we select the top k entities
with highest likelihood under our model. We use
k = 6 in our experiments as determined by hyper-
parameter search.

3.2.3 Pointer Networks

While a significant fraction of answers in QA
datasets like SQuAD are entities, entities alone
may be insufficient for detecting different aspects
of a document. Many documents are entity-less,
and entity taggers may fail to recognize some
entities. To this end, we build a neural model
that is trained from scratch to extract all human-
selected answer phrases in a particular document.
We parameterize this model as a pointer network
(Vinyals et al., 2015) trained to point sequentially
to the start and end locations of all labeled an-
swers in a document. An autoregressive decoder
LSTM augmented with an attention mechanism is
then trained to point (attend) to all of the start and
end locations of answers from left to right, condi-
tioned on the annotation vectors (extracted in the
same fashion as in the NES model), via an atten-
tion mechanism. We add a special termination to-
ken to the document and train the decoder to attend
to it once it has generated all key phrases. This en-
ables the model to extract variable numbers of key
phrases depending on the input document. This
is in contrast to the work of Meng et al. (2017),
where a fixed number of key phrases is generated
per document.

A pointer network is an extension of sequence-
to-sequence models (Sutskever et al., 2014),



81

where the target sequence consists of positions
in the source sequence. An autoregressive de-
coder RNN is trained to attend to these po-
sitions in the input conditioned on an encod-
ing of the input produced by an encoder RNN.
We denote the decoder’s annotation vectors as
(hp1,h

p
2, . . . ,h

p
2na−1,h

p
2na

), where na is the num-
ber of answer key phrases, hp1 and h

p
2 corre-

spond to the start and end annotation vectors for
the first answer key phrase, and so on. We
parameterize P (wdi = start|h

p
1 . . .h

p
j ,h

d) and
P (wdi = end|h

p
1 . . .h

p
j ,h

d) using the general at-
tention mechanism (Luong et al., 2015) between
the decoder and encoder annotation vectors,

P (wdi |h
p
1 . . .h

p
j ,h

d
· ) = softmax(W1h

p
j · h

d
· ),

where W1 is a learned parameter matrix. The in-
puts at each step of the decoder are words from
the document that correspond to the start and end
locations pointed to by the decoder.

During inference, we employ a decoding strat-
egy that greedily picks the best location from the
softmax vector at every step, then post process re-
sults to remove duplicate key phrases. Since the
output sequence is relatively short, we observed
similar performances when using greedy decoding
and beam search.

We also experimented with a BIO tagging
model using an LSTM-CRF (Lample et al., 2016)
but were unable to make the model predict any-
thing except “O” for every token.

3.3 Question Generation
The question generation model adopts a sequence-
to-sequence framework (Sutskever et al., 2014)
with an attention mechanism (Bahdanau et al.,
2014) and a pointer-softmax decoder (Gulcehre
et al., 2016). We make use of the pointer-softmax
mechanism since it lets us take advantage of the
inherent nature of RC datasets re-using words in
the document when framing questions. Our setup
for this module is identical to (Yuan et al., 2017).
It takes a document wd1:nd and an answer w

a
1:na as

input, and outputs a question ŵq1:nq .

An input wordw{d,a}i is represented by concate-
nating its word embedding ei with character-level
embedding echi . Each character in the alphabet
receives an embedding vector, and echi is the final
state of a bi-LSTM running over the embedding
vectors corresponding to the character sequence of
the word.

To leverage the extractive nature of answers in
SQuAD, we encode an answer using the document
annotation vectors at the answer-word positions.
Specifically, if an answer phrase wa1:n occupies the
document span wda1:an , we first encode the corre-
sponding document annotation vectors with a con-
dition aggregation BiLSTM into h′1:n. The con-
catenation of the final state h′n with the answer
annotation vector han as the answer representation.

The RNN decoder employs a pointer-softmax
module (Gulcehre et al., 2016). At each step of
the generation process, the decoder decides adap-
tively whether to (a) generate from the decoder
vocabulary or (b) point to a word in the source
sequence (the document) and copy over. The
pointer-softmax decoder thus has two components
— a pointer attention mechanism and a generative
decoder.

The subsequent mathematical notation deviates
from the previous notation slightly, we use (t) as
the superscript. In the pointing decoder, recur-
rence is implemented with two cascading LSTM
cells c1 and c2:

s
(t)
1 = c1(y

(t−1), s
(t−1)
2 ) (1)

s
(t)
2 = c2(v

(t), s
(t)
1 ), (2)

where s(t)1 and s
(t)
2 are the recurrent states, y

(t−1)

is the embedding of decoder output from the pre-
vious time step, and v(t) is the context vector,
which is the sum of the document annotations hdi
weighted by the document attention α(t)i (Equation
(3)):

v(t) =

n∑
i=1

α
(t)
i h

d
i .

At each time step t, the pointing decoder com-
putes a distribution α(t) over the document word
positions (i.e., a document attention, Bahdanau
et al. 2014). Each element is defined as:

α
(t)
i = f(h

d
i ,h

a, s1
(t−1)), (3)

where f is a two-layer MLP with tanh and softmax
activation, respectively.

The generative decoder, on the other hand, de-
fines a distribution over a prescribed decoder vo-
cabulary with a two-layer MLP g:

o(t) = g(y(t−1), s
(t)
2 ,v

(t),ha).



82

Table 1: Model evaluation on key phrase extrac-
tion

Validation Test
Models F1MS Prec. Rec. F1MS Prec. Rec.

SQuAD

H&S - - - 0.292 0.252 0.403
ENT 0.308 0.249 0.523 0.347 0.295 0.547
NES 0.334 0.335 0.354 0.362 0.375 0.380

PtrNet 0.352 0.387 0.337 0.404 0.448 0.387

NewsQA

ENT 0.187 0.127 0.491 0.183 0.125 0.479
PtrNet 0.452 0.480 0.444 0.435 0.467 0.427

Pointer-softmax is implemented by interpolating
the generative and the pointing distributions:

P (ŵt) ∼ s(t)α(t) + (1− s(t))o(t),

where s(t) is a switch scalar computed at each time
step by a three-layer MLP h:

s(t) = h(s
(t)
2 ,v

(t),α(t),o(t)).

The first two layers of h use tanh activation with
highway connections, and the final layer uses sig-
moid activation.2

4 Experiments and Results

4.1 Datasets
We conduct our experiments on the SQuAD (Ra-
jpurkar et al., 2016) and NewsQA (Trischler et al.,
2016) datasets. These are machine comprehension
corpora consisting of over 100k crowd-sourced
question-answer pairs. SQuAD contains 536 para-
graphs from Wikipedia while NewsQA was cre-
ated on 12,744 news articles. Simple prepro-
cessing is performed, including lower-casing and
word tokenization using NLTK. Since the test split
of SQuAD is hidden from the public, we use
5,158 question-answer pairs (self-contained in 23
Wikipedia articles) from the training set for devel-
opment, and use the official development data to
report test results.

4.2 Implementation Details
We train all models by stochastic gradient descent,
with a minibatch size of 32, using the ADAM op-
timizer.

2We also attach the entropy of the softmax distributions
to the input of the final layer, postulating that this guides the
switching mechanism by indicating the confidence of point-
ing vs generating. We observed an improvement in question
quality with this modification.

4.2.1 Key Phrase Extraction
Key phrase extraction models use pretrained, 300-
dimensional word embeddings generated using a
word2vec extension (Ling et al., 2015) and the En-
glish Gigaword 5 corpus. We used bidirectional
LSTMs of 256 dimensions (128 forward and back-
ward) to encode the document and an LSTM of
256 dimensions as our decoder in the pointer net-
work. A dropout rate of 0.5 was used at the output
of every layer in the network.

4.2.2 Question Generation
The question decoder uses a vocabulary of the
2000 most frequent words in the training data
(questions only). This limited vocabulary is possi-
ble because the question generator may copy over
out-of-vocabulary words from the document with
its Pointer-Softmax mechanism. The decoder em-
bedding matrix is initialized with 300-dimensional
GloVe vectors (Pennington et al., 2014), and di-
mensionality of the character representations is 32.
The number of hidden units is 384 for both the en-
coder and decoder RNN cells. Dropout is applied
at a rate of 0.3 to all embedding layers as well as
between the hidden states in the encoder/decoder
RNNs across time steps.

4.3 Quantitative Evaluation of Key Phrase
Extraction

Since each key phrase is itself a multi-word unit,
we believe that a naı̈ve, word-level F1 that con-
siders an entire key phrase as a single unit is not
well suited to this evaluation. We therefore pro-
pose an extension of the SQuAD F1 metric (for
a single answer span) to multiple spans within a
document, which we call the multi-span F1 score.

This metric is calculated as follows. Given the
predicted phrase êi and a gold phrase ej , we first
construct a pairwise, token-level F1 score matrix
of elements fi,j between the two phrases êi and
ej . Max-pooling along the gold-label axis essen-
tially assesses the precision of each prediction,
with partial matches accounted for by the pair-
wise F1 (identical to evaluation of a single an-
swer in SQuAD) in the cells: pi = maxj(fi,j).
Analogously, the recall for label ej can be com-
puted by max-pooling along the prediction axis:
rj = maxi(fi,j). We define the multi-span F1
score using the mean precision p̄ = avg(pi) and
recall r̄ = avg(rj):

F1MS =
2p̄ · r̄

(p̄+ r̄)
.



83

Table 2: Qualitative examples of detected key phrases and generated questions.
Doc. inflammation is one of the first responses of the immune system to infection . the symptoms of inflammation are redness ,

swelling , heat , and pain , which are caused by increased blood flow into tissue . inflammation is produced by eicosanoids
and cytokines , which are released by injured or infected cells . eicosanoids include prostaglandins that produce fever and the
dilation of blood vessels associated with inflammation , and leukotrienes that attract certain white blood cells ( leukocytes )
. . .

Q-A

H&S
by eicosanoids and cytokines — who is inflammation produced
by ?

of the first responses of the immune system to infection —
what is inflammation one of ?

Q-A

PtrNet
leukotrienes — what can attract certain white blood cells ? eicosanoids and cytokines — what are bacteria produced by ?

Q-A

Gold SQuAD
inflamation — what is one of the first responses the immune
system has to infection ?

eicosanoids and cytokines — what compounds are released by
injured or infected cells , triggering inflammation ?

Doc. research shows that student motivation and attitudes towards school are closely linked to student-teacher relationships .
enthusiastic teachers are particularly good at creating beneficial relations with their students . their ability to create effective
learning environments that foster student achievement depends on the kind of relationship they build with their students .
useful teacher-to-student interactions are crucial in linking academic success with personal achievement . here , personal
success is a student ’s internal goal of improving himself , whereas academic success includes the goals he receives from his
superior . a teacher must guide his student in aligning his personal goals with his academic goals . students who receive this
positive influence show stronger self-confidenche and greater personal and academic success than those without these teacher
interactions .

Q-A

H&S
research — what shows that student motivation and attitudes
towards school are closely linked to student-teacher relation-
ships ?

useful teacher-to-student interactions — what are crucial in
linking academic success with personal achievement ?

to student-teacher relationships — what does research show
that student motivation and attitudes towards school are closely
linked to ?

that student motivation and attitudes towards school are
closely linked to student-teacher relationships — what does re-
search show to ?

Q-A

PtrNet
student-teacher relationships — what are the student motiva-
tion and attitudes towards school closely linked to ?

enthusiastic teachers — who are particularly good at creating
beneficial relations with their students ?

teacher-to-student interactions — what is crucial in linking
academic success with personal achievement ?

a teacher — who must guide his student in aligning his personal
goals ?

Q-A

Gold SQuAD
student-teacher relationships — ’what is student motivation
about school linked to ?

beneficial — what type of relationships do enthusiastic teachers
cause ?

aligning his personal goals with his academic goals . — what
should a teacher guide a student in ?

student motivation and attitudes towards school — what is
strongly linked to good student-teacher relationships ?

Doc. the yuan dynasty was the first time that non-native chinese people ruled all of china . in the historiography of mongolia , it is
generally considered to be the continuation of the mongol empire . mongols are widely known to worship the eternal heaven
. . .

Q-A

H&S
the first time — what was the yuan dynasty that non-native chi-
nese people ruled all of china ?

the yuan dynasty — what was the first time that non-native chi-
nese people ruled all of china ?

Q-A

PtrNet
the mongol empire — the yuan dynasty is considered to be the
continuation of what ?

worship the eternal heaven — what are mongols widely known
to do in historiography of mongolia ?

Q-A

Gold SQuAD
non-native chinese people — the yuan was the first time all of
china was ruled by whom ?

the eternal heaven — what did mongols worship ?

Doc. on july 31 , 1995 , the walt disney company announced an agreement to merge with capital cities/abc for $ 19 billion . . . . . in
1998 , abc premiered the aaron sorkin-created sitcom sports night , centering on the travails of the staff of a sportscenter-style
sports news program ; despite earning critical praise and multiple emmy awards , the series was cancelled in 2000 after two
seasons .

Q-A

H&S
an agreement to merge with capital cities/abc for $19 billion
— what did the walt disney company announce on july 31 , 1995
?

the walt disney company — what announced an agreement to
merge with capital cities/abc for $19 billion on july 31 , 1995 ?

Q-A

PtrNet
2000 — in what year was the aaron sorkin-created sitcom sports
night cancelled ?

walt disney company — who announced an agreement to merge
with capital cities/abc for $ 19 billion ?

Q-A

Gold SQuAD
july 31 , 1995 — when was the disney and abc merger first an-
nounced ?

sports night — what aaron sorkin created show did abc debut
in 1998 ?

Note that existing evaluations (e.g., that of
Meng et al. (2017)) can be seen as the above com-
putation performed on the matrix of exact match
scores between predicted and gold key phrases.
By using token-level F1 scores between phrase
pairs, we allow fuzzy matches.

Our evaluation of key phrase extraction systems
by this metric is presented in Table 1. We com-
pare answer phrases extracted by the method of
Heilman and Smith (2010a) (henceforth refered to
as H&S),3 our baseline entity tagger, the neural

3http://www.cs.cmu.edu/ ark/mheilman/questions/



84

entity selection module, and the pointer network.
As expected, the entity tagging baseline achieves
the best recall, likely by over-generating candidate
answers. The NES model, on the other hand, ex-
hibits much better precision and consequently out-
performs the entity tagging baseline significantly
in F1. This trend persists when comparing the
NES model and the pointer network. The H&S
model exhibits high recall but lacks precision, sim-
ilar to the baseline entity tagger. This is not sur-
prising since that model is not trained on SQuAD’s
answer-phrase distribution.

4.4 Qualitative Evaluation of Key Phrase
Extraction

Qualitatively, we observe that the entity-based
models have a strong bias for numeric types,
which often fail to capture interesting informa-
tion in a document. We also notice that entity-
based systems tend to select the central topical en-
tity as answer, which does not match the distri-
bution of answers typically selected by humans.
For example, given a Wikipedia article on Kenya
stating that agriculture is the second largest con-
tributor to kenya ’s gross domestic product (gdp),
entity-based systems propose kenya as an answer
phrase. This leads to the (low-quality) question
what country is nigeria’s second largest contribu-
tor to? 4 Given the same document, the pointer
model picked agriculture as the answer and asked
what is the second largest contributor to kenya ’s
gross domestic product ?

4.5 Quantitative Evaluation of QA pairs
We can quantitatively evaluate our question gen-
eration module by conditioning it on gold answers
from the SQuAD development set. We can then
use standard automatic evaluation metrics for gen-
erative models of text such as BLEU. Our ques-
tion generation model evaluated in such a manner
yields 10.4 BLEU4.

However, there can exist a many possible ways
to formulate a question given the same answer.
BLEU thus becomes a less desirable metric by
penalizing any generation that does not closely
match (lexically) the reference question. To ad-
dress this issue, we propose to evaluate a generated
question by employing a pre-trained QA model.
Specifically, suppose question q̂ is generated from

4Since the answer word kenya cannot appear in the gen-
erated question, the decoder produced a similar word nigeria
instead.

document d and answer a, and the pre-trained QA
model outputs answer â given the input d and q̂. If
the QA model is assumed to be able to answer the
gold question q with the gold answer a, then the
F1 score between a and â may serve as a proxy
to the semantic equivalence between q and q̂ —
regardless of the amount of word/n-gram overlap
between q and q̂.

Quantitatively, a match-LSTM model (Wang
and Jiang, 2016) pre-trained on gold squad ques-
tion/answer pairs achieves an F1 score of 72.4%
on our generated questions in comparison to
73.8% on the SQuAD dev set.

In addition to the automatic evaluation metrics,
we also undertook a human evaluation of gener-
ated questions and answers.

4.6 Qualitative Evaluation of QA pairs

We present several answer-extraction and
question-generation examples in Table 2. Each
example contains a document and three corre-
sponding QA pairs, generated respectively by
H&S, by our two-stage framework, and by the
original SQuAD crowdworkers.

We now discuss the relative qualities of QA
pairs from each synthetic method.

H&S Key phrases selected by the H&S model
are structurally distinct from the PtrNet and
human-generated answers. For example, they may
start with prepositions, such as of, by, and to, or
consist of very long phrases like that student mo-
tivation and attitudes towards school are closely
linked to student-teacher relationships. As seen
in Figure 1, these key phrases may also contain
vague phrases such as “this theory”, “some stud-
ies”, “a person”, etc., which renders them less nat-
ural for question generation. The H&S question
generator appears to produce a few ungrammati-
cal sentences, e.g., the first time – what was the
yuan dynasty that non-native chinese people ruled
all of china ?

Our system Since our key phrase extractor was
trained on SQuAD, the selected key phrases more
closely resemble gold SQuAD answers. How-
ever, sometimes the generated questions do not
target the extracted answers, eg, eicosanoids and
cytokines — what are bacteria produced by ? (first
document in Table 2). Interestingly, our model is
sometimes able to resolve coreferent entities. For
instance, to generate the mongol empire -— the



85

Figure 1: A comparison of key phrase extraction methods. Red phrases are extracted by the pointer
network, violet by H&S, green by the baseline, brown correspond to squad gold answers and cyan indi-
cates an overlap between the pointer model and squad gold questions. The last paragraph is an exception
where lyndon b. johnson and april 20 are extracted by H&S as well as the baseline model.

yuan dynasty is considered to be the continuation
of what ? the model must resolve the pronoun it
to yuan dynasty in it is generally considered to be
the continuation of the mongol empire (third doc-
ument in Table 2).

4.7 Human Evaluation Studies

We carried out human evaluations on the question
generation module in isolation as well as in con-
junction with the key phrase extraction module.

Evaluating the ability of the Question Gener-
ation Module to transfer to new settings We
asked crowdworkers part of an internal evaluation
system to evaluate two different aspects of ques-
tions generated by our module - fluency and cor-
rectness. Our system was provided Internet arti-
cles and candidate answers selected from an inter-
nal search engine thereby evaluating the model’s
ability to generalize from simple RC datasets to
the real world. For fluency evaluations, they were
asked whether the generated questions sounded
natural (ignoring semantics) with scores of 0/1/2
corresponding to ”No”, ”Somewhat” and ”Yes”.
17.5% were labeled 0, 22.7% were labeled 1 and
59.8% were labeled 2. For correctness evalua-
tions, annotators were asked if the given answer
was the correct answer for the given question.
64.4% of questions were labeled incorrect, leav-

ing 35.6% labeled as correct. This particular eval-
uation differs slightly from others with regard to
the module used (it was trained a combination
of SQuAD + NewsQA + TriviaQA (Joshi et al.,
2017)). Also the documents and answers used pro-
vided via an internal tool. 1,302 annotations were
collected.

Comparison to human generated questions
We present annotators with documents from
SQuAD’s official development set and two sets of
question-answer pairs, one from our model (ma-
chine generated) and the other from SQuAD (hu-
man generated). Annotators are then asked to
identify which question-answer pair is machine
generated. The order in which the pairs appear is
randomized across examples. Annotators are free
to use any criterion to make a distinction, such as
poor grammar, the answer phrase not correctly an-
swering the generated question, unnatural answer
phrases, etc.

We presented 14 annotators with a total of 740
documents, each with 2 corresponding QA pairs.
Annotators identified the machine generated pairs
77.8% of the time with a standard deviation of
8.34%.

Implict comparison to H&S To compare our
system to existing methods (H&S), we orchestrate



86

an implict comparison grounded in human gener-
ated QA pairs from SQuAD. We present human
annotators with a document and two QA pairs –
one that comes from the true development set and
the other from either our system or H&S, at ran-
dom. Annotators are not told that there are two
different models generating QA pairs. As above,
annotators are asked to identify which QA pair is
human generated and which is synthetic.

We presented a single annotator with 100 doc-
uments, each with two QA pairs. For 45 docu-
ments, the synthetic QA pair came from from our
model; for the remaining 55, the synthetic pair
was from H&S. The annotator distinguished cor-
rectly between our system’s output and the human-
generated pair in 30 cases (66.7%), and did so in
45 cases (81.8%) for H&S. This experiment sug-
gests that our system’s generated QA pairs are less
distinguishable from human QA pairs.

Comparison to H&S In a more direct evalua-
tion, we present annotators with documents from
the SQuAD development set along with one QA
pair generated by the H&S model and one gener-
ated by ours. We then ask annotators which QA
pair they prefer.

We presented the same single annotator with
200 such examples. In 107 cases (53.5%), the an-
notator preferred the pair generated by our model.
This suggests that, without human generated QA
pairs for comparison, the annotator considers the
two models’ outputs to be roughly equal in qual-
ity.

5 Conclusion

We propose a two-stage framework to tackle the
problem of question generation from documents.
First, we use a question answering corpus to train
a neural model to estimate the distribution of key
phrases that are likely to be picked by humans to
ask questions about. We present two neural mod-
els, one that ranks entities proposed by an entity
tagging system, and another that points to key-
phrase start and end boundaries with a pointer net-
work. When compared to an entity tagging base-
line, the proposed models exhibit significantly bet-
ter results.

We adopt a sequence-to-sequence model to gen-
erate questions conditioned on the key phrases se-
lected in the framework’s first stage. Our question
generator is inspired by an attention-based trans-
lation model, and uses the pointer-softmax mech-

anism to dynamically switch between copying a
word from the document and generating a word
from a vocabulary. Qualitative examples show
that the generated questions exhibit both syntac-
tic fluency and semantic relevance to the condi-
tioning documents and answers, and appear use-
ful for assessing reading comprehension in educa-
tional settings. In future work we will investigate
fine-tuning the two-stage framework end to end.
Another interesting direction is to explore abstrac-
tive key-phrase extraction.

6 Acknowledgements

The authors would like to thank the reviewers for
their valuable feedback.

References
Manish Agarwal and Prashanth Mannem. 2011. Auto-

matic gap-fill question generation from text books.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 56–64. Association for Computational Lin-
guistics.

Husam Ali, Yllias Chali, and Sadid A Hasan. 2010.
Automation of question generation from sentences.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 58–67.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Lee Becker, Sumit Basu, and Lucy Vanderwende.
2012. Mind the gap: learning to choose gaps for
question generation. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 742–751. Associa-
tion for Computational Linguistics.

Yllias Chali and Sina Golestanirad. 2016. Ranking au-
tomatically generated questions using common hu-
man queries. In The 9th International Natural Lan-
guage Generation conference, page 217.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. arXiv preprint arXiv:1705.00106.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Michael Heilman and Noah A Smith. 2010a. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473


87

Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617. Association for Computational Lin-
guistics.

Michael Heilman and Noah A Smith. 2010b. Rating
computer-generated questions with mechanical turk.
In Proceedings of the NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon’s Mechanical Turk, pages 35–40. Association
for Computational Linguistics.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Hidenobu Kunichika, Tomoki Katayama, Tsukasa Hi-
rashima, and Akira Takeuchi. 2004. Automated
question generation methods for intelligent english
learning systems and its evaluation. In Proc. of
ICCE.

Igor Labutov, Sumit Basu, and Lucy Vanderwende.
2015. Deep questions without deep understanding.
In ACL (1), pages 889–898.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Tho Thi Ngoc Le, Minh Le Nguyen, and Akira Shi-
mazu. 2016. Unsupervised keyphrase extraction:
Introducing new kinds of words to keyphrases. 29th
Australasian Joint Conference, Hobart, TAS, Aus-
tralia, December 5-8, 2016.

David Lindberg, Fred Popowich, John Nesbit, and Phil
Winne. 2013. Generating natural language ques-
tions to support learning on-line. ENLG 2013, page
105.

Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2015. Two/too simple adaptations of
word2vec for syntax problems. In HLT-NAACL,
pages 1299–1304.

Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011. Automatic keyphrase extrac-
tion by bridging vocabulary gap. the Fifteenth Con-
ference on Computational Natural Language Learn-
ing.

Patrice Lopez and Laurent Romary. 2010. Humb: Au-
tomatic key term extraction from scientific articles
in grobidp. the 5th International Workshop on Se-
mantic Evaluation.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Olena Medelyan, Eibe Frank, and Ian H Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. Empirical Methods in Natural
Language.

Rui Meng, Sanqiang Zhao, shuguang Han, Daqing
He, Peter Brusilovsky, and Yu Chi. 2017. Deep
keyphrase generation. In the 55th Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. Empirical Methods in Natural
Language.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.

Ruslan Mitkov and Le An Ha. 2003. Computer-aided
generation of multiple-choice tests. In Proceed-
ings of the HLT-NAACL 03 workshop on Build-
ing educational applications using natural language
processing-Volume 2, pages 17–22. Association for
Computational Linguistics.

Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar-
garet Mitchell, Xiaodong He, and Lucy Vander-
wende. 2016. Generating natural questions about an
image. arXiv preprint arXiv:1603.06059.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Empirical Methods in Natural
Language Processing (EMNLP), pages 1532–1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. 2nd Workshop on Representation
Learning for NLP.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Xiaojun Wan and Jianguo Xiao. 2008. Single doc-
ument keyphrase extraction using neighborhood
knowledge. AAAI.

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162


88

Minmei Wang, Bo Zhao, and Yihua Huang. 2016.
Ptr: Phrase-based topical ranking for automatic
keyphrase extraction in scientific publications. 23rd
International Conference, ICONIP 2016.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Tong Wang, Xingdi Yuan, and Adam Trischler. 2017.
A joint model for question answering and question
generation. 1st Workshop on Learning to Generate
Natural Language.

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W. Cohen. 2017. Semi-supervised qa with
generative domain-adaptive nets. In the 55th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.

Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-
dro Sordoni, Philip Bachman, Sandeep Subrama-
nian, Saizheng Zhang, and Adam Trischler. 2017.
Machine comprehension by text-to-text neural ques-
tion generation. 2nd Workshop on Representation
Learning for NLP.


