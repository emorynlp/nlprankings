










































EMNLP@CPH: Is frequency all there is to simplicity?


First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408–412,
Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

EMNLP@CPH: Is frequency all there is to simplicity?

Anders Johannsen, Héctor Martínez, Sigrid Klerke†, Anders Søgaard
Centre for Language Technology

University of Copenhagen
{ajohannsen|alonso|soegaard}@hum.ku.dk

sigridklerke@gmail.com†

Abstract

Our system breaks down the problem of rank-
ing a list of lexical substitutions according to
how simple they are in a given context into a
series of pairwise comparisons between can-
didates. For this we learn a binary classifier.
As only very little training data is provided,
we describe a procedure for generating artifi-
cial unlabeled data from Wordnet and a corpus
and approach the classification task as a semi-
supervised machine learning problem. We use
a co-training procedure that lets each classi-
fier increase the other classifier’s training set
with selected instances from an unlabeled data
set. Our features include n-gram probabilities
of candidate and context in a web corpus, dis-
tributional differences of candidate in a cor-
pus of “easy” sentences and a corpus of normal
sentences, syntactic complexity of documents
that are similar to the given context, candidate
length, and letter-wise recognizability of can-
didate as measured by a trigram character lan-
guage model.

1 Introduction

This paper describes a system for the SemEval 2012
English Lexical Simplification shared task. The
task description uses a loose definition of simplic-
ity, defining “simple words” as “words that can be
understood by a wide variety of people, including for
example people with low literacy levels or some cog-
nitive disability, children, and non-native speakers of
English” (Specia et al., 2012).

Feature r
Nsf 0.33
Nsf+1 0.27
Nsf−1 0.27
Lsf -0.26
Lmax -0.26
RIproto(l) -0.18
Scn -0.17
Sw -0.17
Scp -0.17

Feature r
RIproto(f) -0.15
Cmax -0.14
RIorig(l) -0.11
Ltokens -0.10
Cmin 0.10
SWfreq 0.08
SWLLR 0.07
Cavg -0.04

Table 1: Pearson’s r correlations. The table shows
the three highest correlated features per group, all of
which are significant at the p < 0.01 level

2 Features

We model simplicity with a range of features divided
into six groups. Five of these groups make use of
the distributional hypothesis and rely on external cor-
pora. We measure a candidate’s distribution in terms
of its lexical associations (RI), participation in syn-
tactic structures (S), or corpus presence in order to
assess its simplicity (N, SW, C). A single
group, L, measures intrinsic aspects of the substi-
tution candidate, such as its length.

The substitution candidate is either an adjective,
an adverb, a noun, or a verb, and all candidates within
a list share the same part of speech. Because word
class might influence simplicity, we allow our model
to fit parameters specific to the candidate’s part of
speech by making a copy of the features for each part
of speech which is active only when the candidate is
in the given part of speech.

408



Simple Wikipedia (SW) These two features con-
tain relative frequency counts of the substitution
form in Simple English Wikipedia (SWfreq), and the
log likelihood ratio of finding the word in the simple
corpus to finding it in regular Wikipedia (SWLLR)1.

Word length (L) This set of three features de-
scribes the length of the substitution form in char-
acters (Lsf ), the length of the longest token
(Lmax), and the length of the substitution form in
tokens (Ltokens). Word length is an integral part
of common measures of text complexity, e.g in the
English Flesch–Kincaid (Kincaid et al., 1975) in the
form of syllable count, and in the Scandinavian LIX
(Bjornsson, 1983).

Character trigram model (C) These three
features approximate the reading difficulty of a word
in terms of the probabilities of its forming character
trigrams, with special characters to mark word be-
ginning and end. A word with an unusual combi-
nation of characters takes longer to read and is per-
ceived as less simple (Ehri, 2005).

We calculate the minimum, average, and maxi-
mum trigram probability (Cmin, Cavg, and
Cmax).2

Web corpus N-gram (N) These 12 features
were obtained from a pre-built web-scale language
model3. Features of the form Nsf±i, where
0 < i < 4, express the probability of seeing the
substitution form together with the following (or pre-
vious) unigram, bigram, or trigram. Nsf is
the probability of substitution form itself, a feature
which also is the backbone of our frequency base-
line.

Random Indexing (RI) These four features are
obtained from measures taken from a word-to-word
distributional semantic model. Random Indexing
(RI) was chosen for efficiency reasons (Sahlgren,
2005). We include features describing the seman-
tic distances between the candidate and the original

1Wikipedia dump obtained March 27, 2012. Date on the
Simple Wikipedia dump is March 22, 2012.

2Trigram probabilities derived from Google T1 unigram
counts.

3The “jun09/body” trigram model from Microsoft Web N-
gram Services.

form (RIorig), and between the candidate and a proto-
type vector (RIproto). For the distance between can-
didate and original, we hypothesize that annotators
would prefer a synonym closer to the original form.
A prototype distributional vector of a set of words is
built by summing the individual word vectors, thus
obtaining a representation that approximates the be-
havior of that class overall (Turney and Pantel, 2010).
Longer distances indicate that the currently exam-
ined substitution is far from the shared meaning of
all the synonyms, making it a less likely candidate.
The features are included for both lemma and surface
forms of the words.

Syntactic complexity (S) These 23 features
measure the syntactic complexity of documents
where the substitution candidate occurs. We used
measures from (Lu, 2010) in which they describe 14
automatic measures of syntactic complexity calcu-
lated from frequency counts of 9 types of syntactic
structures. This group of syntax-metric scores builds
on two ideas.

First, syntactic complexity and word difficulty go
together. A sentence with a complicated syntax is
more likely to be made up of difficult words, and
conversely, the probability that a word in a sentence
is simple goes up when we know that the syntax of
the sentence is uncomplicated. To model this we
search for instances of the substitution candidates in
the UKWAC corpus4 and measure the syntactic com-
plexity of the documents where they occur.

Second, the perceived simplicity of a word may
change depending on the context. Consider the ad-
jective “frigid”, which may be judged to be sim-
pler than “gelid” if referring to temperature, but per-
haps less simple than “ice-cold” when characterizing
someone’s personality. These differences in word
sense are taken into account by measuring the sim-
ilarity between corpus documents and substitution
contexts and use these values to provide a weighted
average of the syntactic complexity measures.

3 Unlabeled data
The unlabeled data set was generated by a three-
step procedure involving synonyms extracted from
Wordnet5 and sentences from the UKWAC corpus.

4http://wacky.sslmit.unibo.it/
5http://wordnet.princeton.edu/

409



1) Collection: Find synsets for unambigious lem-
mas in Wordnet. The synsets must have more than
three synonyms. Search for the lemmas in the cor-
pus. Generate unlabeled instances by replacing the
lemma with each of its synonyms. 2) Sampling: In
the unlabeled corpus, reduce the number of ranking
problems per lemma to a maximum of 10. Sample
from this pool while maintaining a distribution of
part of speech similar to that of the trial and test set.
3) Filtering: Remove instances for which there are
missing values in our features.

The unlabeled part of our final data set contains
n = 1783 problems.

4 Ranking

We are given a number of ranking problems (n =
300 in the trial set and n = 1710 for the test data).
Each of these consists of a text extract with a posi-
tion marked for substitution, and a set of candidate
substitutions.

4.1 Linear order
Let X (i) be the substitution set for the i-th problem.
We can then formalize the ranking problem by as-
suming that we have access to a set of (weighted)
preference judgments, w(a ≺ b) for all a, b ∈ X (i)
such that w(a ≺ b) is the value of ranking item a
ahead of b. The values are the confidence-weighted
pair-wise decisions from our binary classifier. Our
goal is then to establish a total order on X (i) that
maximizes the value of the non-violated judgments.
This is an instance of the Linear Ordering Problem
(Martí and Reinelt, 2011), which is known to be NP-
hard. However, with problems of our size (maximum
ten items in each ranking), we escape these complex-
ity issues by a very narrow margin—10! ≈ 3.6 mil-
lion means that the number of possible orderings is
small enough to make it feasible to find the optimal
one by exhaustive enumeration of all possibilities.

4.2 Binary classication
In order to turn our ranking problem into binary clas-
sification, we generate a new data set by enumerat-
ing all point-wise comparisons within a problem and
for each apply a transformation function Φ(a, b) =
a − b. Thus each data point in the new set is the
difference between the feature values of two candi-

dates. This enables us to learn a binary classifier for
the relation “ranks ahead of”.

We use the trial set for labeled training data L and,
in a transductive manner, treat the test set as unla-
beled data Utest. Further, we supplement the pool of
unlabeled data with artificially generated instances
Ugen, such that U = Utest ∪ Ugen.

Using a co-training setup (Blum and Mitchell,
1998), we divide our features in two independent sets
and train a large margin classifier6 on each split. The
classifiers then provide labels for data in the unla-
beled set, adding the k most confidently labeled in-
stances to the training data for the other classifier, an
iterative process which continues until there is no un-
labeled data left. At the end of the training we have
two classifiers. The classification result is a mixture-
of-experts: the most confident prediction of the two
classifiers. Furthermore, as an upper-bound of the
co-training procedure, we define an oracle that re-
turns the correct answer whenever it is given by at
least one classifier.

4.3 Ties
In many cases we have items a and b that tie—in
which case both a ≺ b and b ≺ a are violated. We
deal with these instances by omitting them from the
training set and setting w(a ≺ b) = 0. For the fi-
nal ranking, our system makes no attempt to produce
ties.

5 Experiments

In our experiments we vary feature-split, size of un-
labeled data, and number of iterations. The first fea-
ture split, S–SW, pooled all syntactic complexity
features and Wikipedia-based features in one view,
with the remaining feature groups in another view.
Our second feature split, S–C–L, combined
the syntactic complexity features with the character
trigram language model features and the basic word
length features. Both splits produced a pair of classi-
fiers with similar performance—each had an F-score
of around .73 and an oracle score of .87 on the trial
set on the binary decision problem, and both splits
performed equally on the ranking task.

6Liblinear with L1 penalty and L2 loss. Parameter settings
were default. http://www.csie.ntu.edu.tw/∼cjlin/liblinear/

410



System All N V R A
MF 0.449 0.367 0.456 0.487 0.493
S–SWf 0.377 0.283 0.269 0.271 0.421
S–SWl 0.425 0.355 0.497 0.408 0.425
S–C–Lf 0.377 0.284 0.469 0.270 0.421
S–C–Ll 0.435 0.362 0.481 0.465 0.439

Table 2: Performance on part of speech. Unlabeled
set was Utest. Subscripts tell whether the scores are
from the first or last iteration

With a large unlabeled data set available, the clas-
sifiers can avoid picking and labeling data points
with a low certainty, at least initially. The assump-
tion is that this will give us a higher quality training
set. However, as can be seen in Figure 1, none of our
systems are benefitting from the additional data. In
fact, the systems learn more when the pool of unla-
beled data is restricted to the test set.

Our submitted systems, O1 and O2 scored
0.405 and 0.393 on the test set, and 0.494 and 0.500
on the trial set. Following submission we adjusted
a parameter7 and re-ran each split with both U and
Utest.

We analyzed the performance by part of speech
and compared them to the frequency baseline as
shown in Table 2. For the frequency baseline, per-
formance is better on adverbs and adjectives alone,
and somewhat worse on nouns. Both our sys-
tems benefit from co-training on all word classes.
S–C–L, our best performing system, no-
tably has a score reduction (compared to the base-
line) of only 5% on adverbs, eliminates the score re-
duction on nouns, and effectively beats the baseline
score on verbs with a 6% increase.

6 Discussion
The frequency baseline has proven very strong, and,
as witnessed by the correlations in Table 1, frequency
is by far the most powerful signal for “simplicity”.
But is that all there is to simplicity? Perhaps it is.
For a person with normal reading ability, a sim-
ple word may be just a word with which the per-
son is well-acquainted—one that he has seen be-
fore enough times to have a good idea about what
it means and in which contexts it is typically used.

7In particular, we selected a larger value for the C parameter
in the liblinear classifier.

0 5000 10000 15000 20000 25000
Unlabeled datapoints

0.38

0.40

0.42

0.44

0.46

0.48

S
co

re

SYN-SW(Utest)

SYN-CHAR-LEN(Utest)

SYN-CHAR-LEN(U)

Figure 1: Test set kappa score vs. number of data
points labeled during co-training

And so an n-gram model might be a fair approxi-
mation. However, lexical simplicity in English may
still be something very different to readers with low
literacy. For instance, the highly complex letter-to-
sound mapping rules are likely to prevent such read-
ers from arriving at the correct pronunciation of un-
seen words and thus frequent words with exceptional
spelling patterns may not seem simple at all.

A source of misclassifications discovered in our
error analysis is the fact that substituting candidates
into the given contexts in a straight-forward manner
can introduce syntactic errors. Fixing these can re-
quire significant revisions of the sentence, and yet
the substitutions resulting in an ungrammatical sen-
tence are sometimes still preferred to grammatical al-
ternatives.8 Here, scoring the substitution and the
immediate context in a language model is of little
use. Moreover, while these odd grammatical errors
may be preferable to many non-native English speak-
ers with adequate reading skills, such errors can be
more obstructing to reading impaired users and be-
ginning language learners.

Acknowledgments
This research is partially funded by the European Commission’s
7th Framework Program under grant agreement n° 238405
(CLARA).

8For example sentence 1528: ”However, it appears they in-
tend to pull out all stops to get what they want.” Gold: {try ev-
erything} {do everything it takes} {pull} {stop at nothing} {go
to any length} {yank}.

411



References
C. H. Bjornsson. 1983. Readability of Newspa-

pers in 11 Languages. Reading Research Quarterly,
18(4):480–497.

A Blum and T Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92–100. ACM.

Linnea C. Ehri. 2005. Learning to read words: The-
ory, findings, and issues. Scientific Studies of Reading,
9(2):167–188.

J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.
1975. Derivation of New Readability Formulas (Auto-
mated Readability Index, Fog Count and Flesch Read-
ing Ease Formula) for Navy Enlisted Personnel.

Xiaofei Lu. 2010. Automatic analysis of syntactic com-
plexity in second language writing. International Jour-
nal of Corpus Linguistics, 15(4):474–496.

Rafael Martí and Gerhard Reinelt. 2011. The Lin-
ear Ordering Problem: Exact and Heuristic Methods
in Combinatorial Optimization (Applied Mathematical
Sciences). Springer.

Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.

Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.

P. D Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.

412


