18

Coling 2010: Poster Volume, pages 18–26,

Beijing, August 2010

Cross-Market Model Adaptation with Pairwise Preference Data for

Web Search Ranking

Jing Bai

Microsoft Bing
1065 La Avenida

Mountain View, CA 94043

jbai@microsoft.com

Fernando Diaz, Yi Chang, Zhaohui Zheng

Yahoo! Labs

701 First Avenue

Sunnyvale, CA 94089

{diazf,yichang,zhaohui}@yahoo-inc.com

Keke Chen

Computer Science

Wright State

Dayton, Ohio 45435
keke.chen@wright.edu

Abstract

Machine-learned ranking techniques au-
tomatically learn a complex document
ranking function given training data.
These techniques have demonstrated the
effectiveness and ﬂexibility required of a
commercial web search. However, man-
ually labeled training data (with multiple
absolute grades) has become the bottle-
neck for training a quality ranking func-
tion, particularly for a new domain. In
this paper, we explore the adaptation of
machine-learned ranking models across
a set of geographically diverse markets
with the market-speciﬁc pairwise prefer-
ence data, which can be easily obtained
from clickthrough logs. We propose
a novel adaptation algorithm, Pairwise-
Trada, which is able to adapt ranking
models that are trained with multi-grade
labeled training data to the target mar-
ket using the target-market-speciﬁc pair-
wise preference data. We present results
demonstrating the efﬁcacy of our tech-
nique on a set of commercial search en-
gine data.

1 Introduction
Web search algorithms provide methods for
ranking web scale collection of documents given
a short query. The success of these algorithms
often relies on the rich set of document prop-
erties or features and the complex relationships

between them.
Increasingly, machine learn-
ing techniques are being used to learn these
relationships for an effective ranking function
(Liu, 2009). These techniques use a set of la-
beled training data labeled with multiple rele-
vance grades to automatically estimate parame-
ters of a model which directly optimizes a per-
formance metric. Although training data often
is derived from editorial labels of document rel-
evance, it can also be inferred from a careful
analysis of users’ interactions with a working
system (Joachims, 2002). For example, in web
search, given a query, document preference in-
formation can be derived from user clicks. This
data can then be used with an algorithm which
learns from pairwise preference data (Joachims,
2002; Zheng et al., 2007). However, automati-
cally extracted pairwise preference data is sub-
ject to noise due to the speciﬁc sampling meth-
ods used (Joachims et al., 2005; Radlinski and
Joachim, 2006; Radlinski and Joachim, 2007).

One of the fundamental problems for a web
search engine with global reach is the develop-
ment of ranking models for different regional
markets. While the approach of training a single
model for all markets is attractive, it fails to fully
exploit of speciﬁc properties of the markets. On
the other hand, the approach of training market-
speciﬁc models requires the huge overhead of
acquiring a large training set for each market.
As a result, techniques have been developed to
create a model for a small market, say a South-
east Asian country, by combining a strong model
in another market, say the United States, with a

19

small amount of manually labeled training data
in the small market (Chen et al., 2008b). How-
ever, the existing Trada method takes only multi-
grade labeled training data for adaptation, mak-
ing it impossible to take advantage of the easily
harvested pairwise preference data.
In fact, to
our knowledge, there is no adaptation algorithm
that is speciﬁcally developed for pairwise data.
In this paper, we address the development
market-speciﬁc ranking models by leveraging
pairwise preference data. The pairwise prefer-
ence data contains most market-speciﬁc train-
ing examples, while a model from a large mar-
ket may capture the common characteristics of
a ranking function. By combining them algo-
rithmically, our approach has two unique advan-
tages. (1) The biases and noises of the pairwise
preference data can be depressed by using the
base model from the large market. (2) The base
model can be tailored to the characteristics of the
new market by incorporating the market speciﬁc
pairwise training data. As the pairwise data has
the particular form, the challenge is how to ef-
fectively use pairwise data in adaptation. This
appeals to the following objective of many web
search engines: design algorithms which mini-
mize manually labeled data requirements while
maintaining strong performance.

2 Related Work

In recent years,
the ranking problem is fre-
quently formulated as a supervised machine
learning problem, which combines different
kinds of features to train a ranking function.
The ranking problem can be formulated as learn-
ing a function with pair-wise preference data,
which is to minimize the number of contra-
dicting pairs in training data.
For example,
RankSVM (Joachims, 2002) uses support vector
machines to learn a ranking function from pref-
erence data; RankNet (Burges et al., 2005a) ap-
plies neural network and gradient descent to ob-
tain a ranking function; RankBoost (Freund et
al., 1998) applies the idea of boosting to con-
struct an efﬁcient ranking function from a set of
weak ranking functions; GBRank (Zheng et al.,
2007; Xia et al., 2008) using gradient descent in

function spaces, which is able to learn relative
ranking information in the context of web search.
In addition, Several studies have been focused
on learning ranking functions in semi-supervised
learning framework (Amini et al., 2008; Duh and
Kirchhoff, 2008), where unlabeled data are ex-
ploited to enhance ranking function. Another ap-
proach to learning a ranking function addresses
the problem of optimizing the list-wise perfor-
mance measures of information retrieval, such
as mean average precision or Discount Cumu-
lative Gain (Cao et al., 2007; Xu et al., 2008;
Wu et al., 2009; Chen et al., 2008c). The idea
of these methods is to obtain a ranking function
that is optimal with respect to some information
retrieval performance measure.

Model adaptation has previously been applied
in the area of natural language processing and
speech recognition. This approach has been suc-
cessfully applied to parsing (Hwa, 1999), tag-
ging (Blitzer et al., 2006), and language model-
ing for speech recognition (Bacchiani and Roark,
2003). Until very recently, several works have
been presented on the topic of model adaptation
for ranking (Gao et al., 2009; Chen et al., 2008b;
Chen et al., 2009), however, none of them target
the model adaptation with the pair-wise learn-
ing framework. Finally, multitask learning for
ranking has also been proposed as a means of
addressing problems similar to those we have
encountered in model adaptation (Chen et al.,
2008a; Bai et al., 2009; Geng et al., 2009).

3 Background
3.1 Gradient Boosted Decision Trees for

Ranking

Assume we have a training data set, D =
{h(q, d), yi1, . . . ,h(q, d), yin}, where h(q, d), tii
encodes the labeled relevance, y, of a docu-
ment, d, given query, q. Each query-document
pair, (q, d), is represented by a set of features,
(q, d) = {xi1, xi2, xi3, ..., xim}. These features
include, for example, query-document match
features, query-speciﬁc features, and document-
speciﬁc features. Each relevance judgment, y,
is a relevance grade mapped (e.g. “relevant”,
“somewhat relevant”, “non-relevant”) to a real

20

where h(q, d), (q, d0), ρii encodes the prefer-
ence, of a document, d, to a second document,
d0, given query, q. Again, each query-document
pair is represented by a set of features. Each
preference judgment, ρ ∈ {(cid:31),≺},
indicates
whether document d is preferred to document d0
(d (cid:31) d0) or not (d ≺ d0).
Preference data is attractive for several rea-
sons. First, editors can often more easily deter-
mine preference between documents than the ab-
solute grade of single documents. Second, rel-
evance grades can often vary between editors.
Some editors may tend to overestimate relevance
compared to another editor. As a result, judg-
ments need to be rescaled for editor biases. Al-
though preference data is not immune to inter-
editor inconsistency, absolute judgments intro-
duce two potential sources of noise: determin-
ing a relevance ordering and determining a rele-
vance grade. Third, even if grades can be accu-
rately labeled, mapping those grades to real val-
ues is often done in a heuristic or ad hoc manner.
Fourth, GBDTreg potentially wastes modeling
effort on predicting the grade of a document as
opposed to focusing on optimizing the rank order
of documents, the real goal a search engine. Fi-
nally, preference data can often be mined from a
production system using assumptions about user
clicks.

In

order

support

to
(Zheng et al.,

preference-based
training data,
2007) pro-
posed GBRANK based on GBDTreg. The
GBRANK training algorithm begins by con-
structing an initial tree which predicts a constant
score, c, for all instances. A pair is contra-
dicting if the h(q, d), (q, d0),(cid:31)i and prediction
f (q, d) < f (q, d0). At each boosting stage,
the algorithm constructs a set of contradicting
pairs, D(cid:31)contra. The GBRANK algorithm then
adjusts the response variables, f (q, d) and
f (q, d0), so that f (q, d) > f (q, d0). Assume
that (q, d) (cid:31) (q, d0) and f (q, d) < f (q, d0). To
correct the order, we modify the target values,

˜f (q, d) = f (q, d) + τ
˜f (q, d0) = f (q, d0) − τ

(3)
(4)

where τ > 0 is a margin parameter that we

Figure 1: An example of base tree, where x1, x2
and x3 are features and a1, a2 and a3 are their
splitting values.

number. Given this representation, we can learn
a gradient boosted decision tree (GBDT) which
models the relationship between document fea-
tures, (q, d), and the relevance score, y, as a de-
cision tree (Friedman, 2001). Figure 1 shows a
portion of such a tree. Given a new query docu-
ment pair, the GBDT can be used to predict the
relevance grade of the document. A ranking is
then inferred from these predictions. We refer to
this method as GBDTreg.

In the training phase, GBDTreg

iteratively
constructs regression trees. The initial regres-
sion tree minimizes the L2 loss with respect to
the targets, y,

L2(f, y) = Xh(q,d),yi

(f (q, d) − y)2

(1)

As with other boosting algorithms, the subse-
quent trees minimize the L2 loss with respect to
the residuals of the predicted values and the tar-
gets. The ﬁnal prediction, then, is the sum of the
predictions of the trees estimated at each step,

f (x) = f 1(x) + . . . + f k(x)

(2)

where f i(x) is the prediction of the ith tree.

3.2 Pairwise Training
As alternative to the absolute grades in D,
we can also imagine assembling a data set
of relative judgments.
In this case, as-
sume we have a training data set D(cid:31) =
{h(q, d), (q, d0), ρi1, . . . ,h(q, d), (q, d0), ρin},

x1 > a1?

YES

NO

x2 > a2?

x3 > a3?

21

need to assign. In our experiments, we set τ to
1. Note that if preferences are inferred from ab-
solute grades, D, minimizing the L2 to 0 also
minimizes the contradictions.

3.3 Tree Adaptation
Recall that we are also interested in using the
information learned from one market, which we
will call the source market, on a second market,
which we will call the target market. To this end,
the Trada algorithm adapts a GBDTreg model
from the source market for the target market by
using a small amount of target market absolute
relevance judgments (Chen et al., 2008b). Let
the Ds be the data in the source domain and
Dt be the data in target domain. Assume we
have trained a model using GBDTreg. Our ap-
proach will be to use the decision tree structure
learned from Ds but to adapt the thresholds in
each node’s feature. We will use Figure 1 to il-
lustrate Trada. The splitting thresholds are a1, a2
and a3 for rank features x1, x2 and x3. Assume
that the data set Dt is being evaluated at the root
node v in Figure 1. We will split the using the
feature vx = x1 but will compute a new thresh-
old v0a using Dt and the GBDTreg algorithm.
Because we are discussing the root node, when
we select a threshold b, Dt will be partitioned
into two sets, D>b
representing those
instances whose feature x1 has a value greater
and lower than b. The response value for each
partition will be the uniform average of instances
in that partition,

and D<b

t

t

f =

1
|D>b
t
1
|D<b

t

t

|Pdi∈D>b
|Pdi∈D<b

t

yi

yi

t

if di ∈ D>b
if di ∈ D<b

t

(5)

We would like to select a value for b which min-
imizes the L2 loss between y and f in Equation
5; equivalently, b can be selected to minimize the
variance of y in each partition.
In our imple-
mentation, we compute the L2 loss for all pos-
sible values of the feature v0x and select the value
which minimizes the loss.

Once b is determined, the adaptation consists
of performing a linear interpolation between the
original splitting threshold va and the new split-

ting threshold b as follows:

v0a = pva + (1 − p)b

(6)

where p is an adaptation parameter which deter-
mines the scale of how we want to adapt the tree
to the new task. If there is no additional informa-
tion, we can select p according to the size of the
data set,

p =

s

|

|D<a
| + |D<b

t

|D<a

s

(7)

|

In practice, we often want to enhance the adapta-
tion scale since the training data of the extended
task is small. Therefore, we add a parameter β
to boost the extended task as follows:

p =

s

|

|D<a
| + β|D<b

t

|D<a

s

(8)

|

The value of β can be determined by cross-
validation. In our experiments, we set β to 1.

The above process can also be applied to ad-

just the response value of nodes as follows:

v0f = pvf + (1 − p)f

(9)

where v0f is the adapted response at a node, vf is
its original response value of source model, and
f is the response value (Equation 5).

The complete Trada algorithm used in our ex-

periments is presented in Algorithm 1.

Algorithm 1 Tree Adaptation Algorithm
TRADA(v,Dt, p)
1 b ← COMPUTE-THRESHOLD(vx,Dt)
2 v0a ← pva + (1 − p)b
3 v0f ← pvf + (1 − p)MEAN-RESPONSE(Dt)
4 D0t ← {x ∈ Dt : xi < v0a}
5 v0< ← TRADA(v<,D0t, p)
6 D00t ← {x ∈ Dt : xi > v0a}
7 v0> ← TRADA(v>,D00t , p)
8 return v0

22

Ms

The Trada algorithm can be augmented with a
second phase which directly incorporates the tar-
get training data. Assume that our source model,
Ms, was trained using source data, Ds. Re-
call that Ms can be decomposed as a sum of
regression tree output, fMs(x) = f 1
(x) +
Ms
(x). Additive tree adaptation refers
. . . + f k
augmenting this summation with a set of regres-
sion trees trained on the residuals between the
model, Ms, and the target training data, Dt.
That is, fMt(x) = f 1
(x) +
Ms
fMt(x)k+1+. . .+fMt(x)k+k0. In order for us to
perform additive tree adaptation, the source and
target data must use the same absolute relevance
grades.

(x) + . . . + f k

Ms

4 Pairwise Adaptation

and Trada can be used
Both GBRANK
to reduce the requirement on editorial data.
GBRANK achieves the goal by leveraging pref-
erence data, while Trada does so by leveraging
data from a different search market. A natural
extension to these methods is to leverage both
sources of data simultaneously. However, no al-
gorithm has been proposed to do this so far in
the literature. We propose an adaptation method
using pairwise preference data.

Our approach shares the same intuition as
Trada: maintain the tree structure but adjust
decision threshold values against some target
value. However, an important difference is
that our adjustment of threshold values does not
regress against some target grade values; rather
its objective is to improve the ordering of doc-
uments. To make use of preference data in
the tree adaptation, we follow the method used
in GBRANK to adjust the target values when-
ever necessary to preserve correct document or-
der. Given a base model, Ms, and preference
data, D(cid:31)t , we can use Equations 3 and 4 to in-
fer target values. Speciﬁcally, we construct a set
D(cid:31)contra from D(cid:31)t and Ms. For each item (q, d)
in D(cid:31)contra, we use the value of ˜f (q, d) as the tar-
get. These tuples, h(q, d), ˜f (q, d)i along with
Ms are then are provided as input to Trada. Our
approach is described in Algorithm 2.
Compared to Trada, Pairwise-Trada has two

Algorithm 2 Pairwise Tree Adaptation Algo-
rithm
PAIRWISE-TRADA(Ms,D(cid:31)t , p)
1 Dcontra ← FIND-CONTRADICTIONS(Ms,D(cid:31)t )
2
3 return TRADA(ROOT(Ms), ˜Dt, p)

˜Dt ← {h(q, d), ˜f (q, d)i : (q, d) ∈ Dcontra}

important differences. First, Pairwise-Trada can
use a source GBDT model trained either against
absolute or pairwise judgments. When an orga-
nization maintains a set of ranking models for
different markets, although the underlying mod-
eling method may be shared (e.g. GBDT), the
learning algorithm used may be market-speciﬁc
(e.g. GBRANK or GBDTreg). Unfortunately,
classic Trada relies on the source model being
trained using GBDTreg. Second, Pairwise-Trada
can be adapted using pairwise judgments. This
means that we can expand our adaptation data to
include click feedback, which is easily obtain-
able in practice.

5 Methods and Materials

The proposed algorithm is a straightforward
modiﬁcation of previous ones. The question we
want to examine in this section is whether this
simple modiﬁcation is effective in practice. In
particular, we want to examine whether pairwise
adaptation is better than the original adaptation
Trada using grade data, and whether the pairwise
data from a market can help improve the ranking
function on a different market.

Our experiments evaluate the performance of
Pairwise-Trada for web ranking in ten target
markets. These markets, listed in Table 1, cover
a variety of languages and cultures. Further-
more, resources, in terms of documents, judg-
ments, and click-through data, also varies across
markets. In particular, editorial query-document
judgments range from hundreds of thousands
(e.g. SEA1) to tens of thousands (e.g. SEA5).
Editors graded query-document pairs on a ﬁve-
point relevance scale, resulting in our data set D.
Preference labels, D(cid:31), are inferred from these
judgments.

23

We also include a second set of experiments
which incorporate click data.1 In these experi-
ments, we infer a preference from click data by
assuming the following model. The user is pre-
sented with ten results. An item i (cid:31) j if i the fol-
lowing conditions hold: i is positioned below j,
i receives a click, and j does not receive a click.
In our experiments, we tested the following

runs,

• GBDTreg trained using only Ds or Dt
• GBRANK trained using only D(cid:31)s or D(cid:31)t
• GBRANK trained using only D(cid:31)s , D(cid:31)t , and
Ct

• Trada with both GBDTs and GBRANKs,

adapted with Dt.

• Pairwise-Trada with both GBDTs and
GBRANKs, adapted with D(cid:31)t and Ct at dif-
ferent ratios.

In the all experiments, we use 400 additive trees
when additive adaptation is used.

All models are evaluated using discounted cu-
mulative gain (DCG) at rank cutoff 5 (J¨arvelin
and Kek¨al¨ainen, 2002).

6 Results
6.1 Adaptation with Manually Labeled

Data

In Table 1, we show the results for all of our ex-
perimental conditions.

We can make a few observations about the
non-adaptation baselines. First, models trained
on the (limited) target editorial data, GBDTt
and GBRANKt, tend to outperform those trained
only on the source editorial data, GBDTs and
GBRANKs. The critical exception is SEA5, the
market with the fewest judgments. We believe
that this behavior is a result of similarity between
the United States source data and the SEA5 tar-
get market; both the source and target query pop-
ulations share the same language, a property not
1For technical reasons, this data set is slightly differ-
ent from the results we show with the purely editorial data.
Therefore the size of the training and testing sets are differ-
ent, but not to a signiﬁcant degree.

exhibited in other markets. Notice that other
small markets such as LA2 and LA3 see modest
improvements when using target-only runs com-
pared to source-only runs. Second, GBRANK
tends to outperform GBDT when only trained on
the source data. This implies that we should pre-
fer a base model which is based on GBRANK,
something that is difﬁcult to combine with clas-
sic Trada. Third, by comparing GBRANK and
GBDT when only trained on the target data, we
notice that the effectiveness of GBRANK de-
pends on the amount of training data. For mar-
kets where there training data is plentiful (e.g.
SEA1), GBRANK outperforms GBDT. On the
other hand, for smaller markets (e.g. LA3),
GBDT outperforms GBRANK.

In general, the results conﬁrm the hypothe-
sis that adaptation runs outperform all of non-
adaptation baselines. This is the case for both
Trada and Pairwise-Trada. As with the baseline
runs, the Australian market sees different perfor-
mance as a result of the combination of a small
target editorial set and a representative source
domain. This effect has been observed in pre-
vious results (Chen et al., 2009).

We can also make a few observations by com-
paring the adaptation runs. Trada works better
with a GBDT base model than with a GBRANK
base model. We We believe this is the case be-
cause the absolute regression targets are difﬁ-
cult to compare with the unbounded output of
GBRANK. Pairwise-Trada on the other hand
tends to perform better with a GBRANK base
model than with a GBDT base model. There
are a few exceptions, SEA3
and LA2, where
Pairwise-Trada works better with a GBDT base
model. Comparing Trada to Pairwise-Trada, we
ﬁnd that using preference targets tends to im-
prove performance for some markets but not all.
The underperformance of Pairwise-Trada tends
to occur in smaller markets such as LA1, LA2,
and LA3. This is similar to the behavior we ob-
served in the non-adaptation runs and suggests
that, in operation, a modeler may have to decide
on the training algorithm based on the amount of
data available.

24

training size
testing size

GBDTs
GBDTt
GBRANKs
GBRANKt

Trada
GBDTs, Dt
GBRANKs, Dt
Pairwise-Trada
GBDTs, Dt
GBRANKs, Dt

SEA1
243,790
18,652

9.4483
9.6011
9.6059
9.6952

SEA2
174,435
26,752

8.1271
8.6225
8.1784
8.6225

EU1

137,540
11,431

9.0018
9.3310
9.0775
9.3575

SEA3
135,066
13,839

10.0630
10.7591
10.2486
10.8595

EU2

101,076
12,118

8.5339
9.0323
8.6248
9.0384

SEA4
100,846
12,214

5.9176
6.4185
6.1298
6.4620

LA1
91,638
11,038

6.1699
6.8441
6.2614
6.8543

LA2
75,989
16,339

11.4167
11.8553
11.5186
11.7086

LA3
66,151
10,379

8.1416
8.5702
8.2851
8.4825

SEA5
37,445
21,034

10.5356
10.4561
10.5915
10.3469

9.6718
9.6116

8.6120
8.5681

9.3086
9.2125

10.8001
10.7597

9.1024
8.9675

6.3440
6.4110

6.9444
6.8286

11.9513
11.7326

8.6519
8.5498

10.6279
10.6508

9.7364
9.7539

8.6261
8.6538

9.3824
9.4269

10.8549
10.8362

9.0842
9.1044

6.4705
6.4716

6.9438
6.9438

11.8255
11.8034

8.5323
8.6187

10.4655
10.6564

Table 1: Adaptation using manually labeled training data Southeast Asia (SEA), Europe (EU), and
Latin America (LA) markets. Markets are sorted by target training set size. Signiﬁcance tests use
a t-test. Bolded numbers indicate statistically signiﬁcant improvements over the respective source
model.

training size
testing size

GBRANKs
Pairwise-Trada
GBRANKs, Dt, Ct
editorial
click
editorial+click

SEA1
194,114
15,655

SEA2
166,396
11,844

EU1

136,829
11,028

SEA3
161,663
11,839

EU2
94,875
11,118

SEA4
96,642
5,092

LA1
73,977
10,038

LA2

108,350
12,246

LA3
64,481
10,201

SEA5
71,549
7,477

9.0159

8.5763

8.7119

11.4512

9.7641

6.5941

6.894

7.9366

8.058

10.7935

9.3577
9.1149
9.4898

8.9205
8.7622
9.0177

8.901
8.8187
8.945

12.2247
11.9361
12.3172

9.9531
9.8818
10.1156

6.7421
6.7703
6.8459

7.1455
7.1812
7.2414

8.2811
8.264
8.4111

8.2503
8.2485
8.292

10.7973
10.9042
11.1407

Table 2: Adaptation incorporating click data. Bolded numbers indicate statistically signiﬁcant im-
provements over the baseline. Markets ordered as in Table 1.

Incorporating Click Data

6.2
One of the advantages of Pairwise-Trada is the
ability to incorporate multiple sources of pair-
wise preference data. In this paper, we use the
heuristic rule approach which is introduced by
(Dong et al., 2009) to extract pairwise preference
data from the click log of the search engine. This
approach yields both skip-next and skip-above
pairs (Joachims et al., 2005), which are sorted
by conﬁdence descending order respectively. In
these experiments, we combine manually gener-
ated preferences with those gathered from click
data. We present these results in Table 2.

We notice that no matter the source of prefer-
ence data, Pairwise-Trada outperforms the base-
line GBRANK model. The magnitude of the
improvement depends on the source data used.
Comparing the editorial-only to the click-only
models, we notice that click-only models outper-
form editorial-only models for smaller markets
(SEA4, LA1, and SEA5). This is likely the case
because the relative quantity of click data with

respect to editorial data is higher in these mar-
kets. This is despite the fact that the click data
may be noisier than the editorial data. The best
performance, though, comes when we combine
both editorial and click data.

6.3 Additive tree adaptation
Recall that Pairwise-Trada consists of two parts:
parameter adaptation and additive tree adapta-
tion.
In this section, we examine the contri-
bution to performance each part is responsible
for. Figure 2 illustrates the adaptation results for
the LA1 market. In this experiment, we use a
United States base model and 100K LA1 edito-
rial judgments for adaptation. Pairwise-Trada is
performed on top of differently sized base mod-
els with 600, 900 and 1200 trees. The original
base model has 1200 trees; we selected the ﬁrst
600, 900 or full 1200 trees for experiments. The
number of trees used in the additive tree adap-
tation step ranges up to 600 trees. From Fig-
ure 2 we can see that the additive adaptation can

25

works (Burges et al., 2005b). Therefore, the pro-
posed method also points to a new direction for
future improvements of search engines.

There are several areas of future work. First,
we believe that detecting other sources of pref-
erence data from user behavior can further im-
prove the performance of our model. Second,
we only used a single source model in our ex-
periments. We would also like to explore the
effect of learning from an ensemble of source
models. The importance of each may depend on
the similarity to the target domain. Finally, we
would also like to more accurately understand
the queries where click data improves adaptation
and those where editorial judgments is required.
This sort of knowledge will allow us to train sys-
tems which maximally exploit our editorial re-
sources.

References
Amini, M.-R., T.-V. Truong, and C. Goutte. 2008.
A boosting algorithm for learning bipartite rank-
ing functions with partially labeled data. In SIGIR
’08: Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval.

Bacchiani, M. and B. Roark.

2003. Unsuper-
vised language model adaptation. In ICASSP ’03:
Proceedings of the International Conference on
Acoustics, Speech and Signal Processing.

Bai, J., K. Zhou, H. Zha, B. Tseng, Z. Zheng, and
Y. Chang. 2009. Multi-task learning for learning
to rank in web search. In CIKM ’09: Proceeding
of the 18th ACM conference on Information and
knowledge management.

Blitzer, J., R. McDonald, and F. Pereira.

2006.
Domain adaptation with structural correspondence
learning.
In EMNLP ’06: Proceedings of the
2006 Conference on Empirical Methods on Nat-
ural Language Processing.

Burges, C., T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005a.
Learning to rank using gradient descent. In ICML
’05: Proceedings of the 22nd International Con-
ference on Machine learning.

Burges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hul-
2005b. Learning to rank using gradi-
lender.
ent descent.
In ICML ’05: Proceedings of the

Figure 2: Illustration of additive tree adaptation
for LA1. The curves are average performance
over a range of parameter settings.

signiﬁcantly increase DCG over simple parame-
ter adaptation and is therefore a critical step of
Pairwise-Trada. When the number of trees in
the additive tree adaptation step reaches roughly
400, the DCG plateaus.

7 Conclusion
We have proposed a model for adapting retrieval
models using preference data instead of abso-
lute relevance grades. Our experiments demon-
strate that, when much editorial data is present,
our method, Pairwise-Trada, may be preferable
to competing methods based on absolute rele-
vance grades. However, in real world systems,
we often have access to sources of preference
data beyond those resulting from editorial judg-
ments. We demonstrated that Pairwise-Trada can
exploit such data and boost performance signif-
icantly.
In fact, if we omit editorial data alto-
gether we see performance improvements over
the baseline model. This suggests that, in prin-
ciple, we can train a single, strong source model
and improve it using target click data alone. De-
spite the fact that the modiﬁcation we made is
quite simple, we showed that modiﬁcation is ef-
fective in practice. This tends to validate the
general principle of using pairwise data from a
different market. This principle can be easily
used in other frameworks such as neural net-

5
G
C
D

7.0

6.8

6.6

6.4

6.2

6.0

adaptation
additive (600)
additive (900)
additive (1200)
source model

0

500

1000

1500

2000

number of trees

26

22nd international conference on Machine learn-
ing, pages 89–96. ACM.

Cao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li.
from pairwise approach to listwise ap-
In ICML ’07: Proceedings of the 24th

2007.
proach.
international conference on Machine learning.

Chen, D., J. Yan, G. Wang, Y. Xiong, W. Fan, and
Z. Chen. 2008a. Transrank: A novel algorithm for
transfer of rank learning. In ICDM workshop ’08:
Proceeding of IEEE Conference on Data Mining.

Chen, K., R. Lu, C. K. Wong, G. Sun, L. Heck, and
B. Tseng. 2008b. Trada: tree based ranking func-
tion adaptation. In CIKM ’08: Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 1143–1152, New York,
NY, USA. ACM.

Chen, W., T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. 2008c.
Measures and loss functions in learning to rank. In
NIPS ’08: Proceedings of the Twenty-Second An-
nual Conference on Neural Information Process-
ing Systems.

Chen, K., J. Bai, S. Reddy, and B. Tseng. 2009. On
domain similarity and effectiveness of adapting-
to-rank.
In CIKM ’09: Proceeding of the 18th
ACM conference on Information and knowledge
management, pages 1601–1604, New York, NY,
USA. ACM.

Dong, A., Y. Chang, S. Ji, C. Liao, X. Li, and
Z. Zheng. 2009. Empirical exploitation of click
data for query-type-based ranking.
In EMNLP
’09: Proceedings of the 2009 Conference on Em-
pirical Methods on Natural Language Processing.

Duh, K. and K. Kirchhoff. 2008. Learning to rank
with partially-labeled data.
In SIGIR ’08: Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in
information retrieval.

Freund, Y., R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efﬁcient boosting algorithm for com-
bining preferences. In ICML ’98: Proceedings of
the Fifteenth International Conference on Machine
Learning.

Friedman, J. H. 2001. Greedy function approxima-
tion: A gradient boosting machine. The Annals of
Statistics, 29(5):1189–1232.

Gao, J., Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan,
Shah S., and H. Zhou.
2009. Model adapta-
tion via model interpolation and boosting for web
search ranking.
In EMNLP ’09: Proceedings of
the 2009 Conference on Empirical Methods on
Natural Language Processing.

Geng, B., L. Yang, C. Xu, and X.-S. Hua. 2009.
Ranking model adaptation for domain-speciﬁc
search. In CIKM ’09: Proceeding of the 18th ACM
conference on Information and knowledge man-
agement, pages 197–206, New York, NY, USA.
ACM.

Hwa, R. 1999. Supervised grammar induction using
training data with limited constituent information.
In ACL ’99: Proceedings of the Conference of the
Association for Computational Linguistics.

J¨arvelin, Kalervo and Jaana Kek¨al¨ainen. 2002. Cu-
mulated gain-based evaluation of ir techniques.
TOIS, 20(4):422–446.

Joachims, T., L. Granka, B. Pan, and G. Gay. 2005.
Accurately interpreting clickthrough data as im-
plicit feedback.

Joachims, T. 2002. Optimizing search engines using
clickthrough data.
In KDD ’02: Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
133–142. ACM Press.

Liu, T.-Y. 2009. Learning to Rank for Information

Retrieval. Now Publishers.

Radlinski, F. and T. Joachim. 2006. Minimally inva-
sive randomization for collecting unbiased prefer-
ences from clickthrough logs.

Radlinski, F. and T. Joachim.

2007. Active ex-
ploration for learning rankings from clickthrough
data.

Wu, M., Y. Chang, Z. Zheng, and H. Zha. 2009.
Smoothing dcg for learning to rank: A novel ap-
proach using smoothed hinge functions. In CIKM
’09: Proceeding of the 18th ACM conference on
Information and knowledge management.

Xia, F., T.-Y. Liu, J. Wang, W. Zhang, and H. Li.
2008. Listwise approach to learning to rank: The-
orem and algorithm.
In ICML ’08: Proceedings
of the 25th international conference on Machine
learning.

Xu, J., T.Y. Liu, M. Lu, H. Li, and W.Y. Ma. 2008.
Directly optimizing evaluation measures in learn-
ing to rank.
In SIGIR ’08: Proceedings of the
31st annual international ACM SIGIR conference
on Research and development in information re-
trieval.

Zheng, Z., K. Chen, G. Sun, and H. Zha. 2007. A re-
gression framework for learning ranking functions
using relative relevance judgments. In SIGIR ’07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 287–294. ACM.

