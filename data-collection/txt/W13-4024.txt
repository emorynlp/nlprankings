



















































Demonstration of an Always-On Companion for Isolated Older Adults


Proceedings of the SIGDIAL 2013 Conference, pages 148–150,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

Demonstration of an Always-On Companion for
Isolated Older Adults

Candace Sidner
Worcester Polytechnic Institute

Worcester, MA, USA
sidner@wpi.edu

Timothy Bickmore
Northeastern University

Boston, MA, USA
bickmore@ccs.neu.edu

Charles Rich
Worcester Polytechnic Institute

Worcester, MA, USA

Barbara Barry, Lazlo Ring
Northeastern University

Boston, MA, USA

Morteza Behrooz, Mohammad Shayganfar
Worcester Polytechnic Institute

Worcester, MA, USA

Abstract

We summarize the status of an ongoing
project to develop and evaluate a compan-
ion for isolated older adults. Four key
scientific issues in the project are: em-
bodiment, interaction paradigm, engage-
ment and relationship. The system ar-
chitecture is extensible and handles real-
time behaviors. The system supports mul-
tiple activities, including discussing the
weather, playing cards, telling stories, ex-
ercise coaching and video conferencing. A
live, working demo system will be pre-
sented at the meeting.

1 Introduction

The Always-On project1 is a four-year effort, cur-
rently in its third year, supported by the U.S. Na-
tional Science Foundation at Worcester Polytech-
nic Institute and Northeastern University. The goal
of the project is to create a relational agent that
will provide social support to reduce the isolation
of healthy, but isolated older adults. The agent is
“always on,” which is to say that it is continuously
available and aware (using a camera and infrared
motion sensor) when the user is in its presence and
can initiate interaction with the user, rather than,
for example requiring the user login to begin in-
teraction. Our goal is for the agent to be a natural,
human-like presence that “resides” in the user’s
dwelling for an extended period of time. Begin-
ning in the fall of 2013, we will be placing our
agents with about a number of users for a month-
long, 4 arm, evaluation/comparison study.

1http://www.cs.wpi.edu/˜rich/always

Some%reply!

Another%reply%

Something%else%

Me%too!!

I’ve%got%great%%cards%

Some%reply!

Another%reply%

Something%else%

Some%reply!

Another%reply%

Something%else%

Just%%play!%

I’ve%got%terrible%cards!%

Figure 1: Virtual agent interface — “Karen”

Our project focuses on four key scientific is-
sues:

• the embodiment of the agent,

• the interaction paradigm,

• the engagement between the user and the
agent, and

• the nature of the social relationship between
the user and the agent.

1.1 Embodiment

We are experimenting with two forms of agent em-
bodiment. Our main study will employ the vir-
tual agent Karen, shown in Figure 1, that comes
from the work of Bickmore et al. (Bickmore et
al., 2005). Karen is a human-like agent animated
from a cartoon-shaded 3D model. She is shown
in Figure 1 playing a social game of cards with
user. Notice that user input is via a touch-screen
menu. Also, the speech bubble does not appear

148



in the actual interface, which uses text-to-speech
generation.

We are also planning an exploratory study sub-
stituting the Reeti2 robot, shown in Figure 2,
for Karen, but otherwise keeping the rest of the
system (i.e., the menus, text-to-speech and other
screen graphics) as much the same as possible.
One big difference we expect is that the effect of
face tracking with the robotic agent will be much
stronger than with Karen. On the other hand, be-
cause Reeti is not as human-like as Karen, it is
possible that it will not be as well accepted overall
as Karen.

1.2 Interaction Paradigm

The main interaction paradigm in our system is
conversation, and in particular, dialog. The agent
makes its contributions to the dialog using speech,
and the user chooses his/her contribution from a
menu of utterances provided on the touch screen.
Dialogs evolve around various activities and can
extend for quite a long time (up to five or ten min-
utes) if the user chooses to continue the conversa-
tion. Dialog models can be created using whatever
system that the system designer chooses. In our
work, we use models that are scripting formats,
a Java state machine model based on adjacency
pairs or created with the dialog tool Disco (Rich
and Sidner, 2012). This variety of models makes
our system more flexible for system designers.

The agent is not designed to accept speech input
for several reasons:

• lack of voice models for older adults;

• no reliable means to circumscribe the collec-
tion of utterances that the system could un-
derstand;

• the wide range of activities to talk about with
the agent results in a huge number of utter-
ance structures, semantic structures and pos-
sible intentions. We doubt there are existing
speech-to-utterance semantics systems avail-
able to support such a plethora of choices
with high reliability. As our project is not
about spoken language understanding, we
opted not to take on this burden.

Some of the activities between user and agent
involve additional on-screen graphics, such as the

2http://www.reeti.fr

card game shown in Figure 1, or a Week-At-A-
Glance

TM
style planning calendar. When playing

cards together, the user is allowed to directly ma-
nipulate the cards on-screen. For the calendar,
the user may only do deictic gestures. All other
information is handled through dialog. We have
thus eschewed other traditional GUI methods us-
ing icons, pull-down lists, etc., in favor of using
speech and menu dialog interaction whenever pos-
sible. The other exception, like direct manipula-
tion of cards on-screen, is a virtual keyboard to
allow typing in of proper names of people and
places. Our motivation for this design choice is
to reinforce the relationship between the user and
the agent, and to simplify the interaction in com-
parison to standard GUIs.

1.3 Engagement

Our system continu-

Figure 2: Robotic
interface — “Reeti”

ously maintains a model
of the state of engage-
ment (Sidner et al., 2005)
between the user and the
agent. For example, when
the agent senses nearby
motion (via infrared) fol-
lowed by the appearance
of a face in its vision
system, it decides that the
user is initiating engagement. Disengagement
can come about at the natural conclusion of
the conversation or when the user leaves for an
unexpected reason, e.g., to answer a ringing door
bell. Because our agent cannot understand sounds
in the environment, it may not know why the user
has disengaged, but it does have simple strategies
for dealing with unexpected interruptions. Gen-
erally, the agent does not initiate disengagement,
although it may attempt to hurry the conclusion
of a session if some event in the user’s calendar is
about to start.

Since the user and agent have conversations
over an extended period of time, it is natural to
consider that they have some kind of social re-
lationship (Bickmore and Schulman, 2012; Kidd
and Breazeal, 2007). To reason about this rela-
tionship, we have implemented a planning system
(Coon et al., 2013) that decides which activities
are appropriate to suggest to the user each time
they interact (in what we call a session). This plan-
ning system uses a relationship model based on

149



the closeness between the agent and user. Their
closeness increases as they do activities together.
Closeness decreases when the user and agent do
not interact for a period of time, such as a few
days.

Each available activity has a required closeness
in order to be undertaken. Only those activities
whose required closeness is less than or equal to
the current closeness between the user and agent
will be suggested for the current session. Activi-
ties that, although suggested, do not actually occur
(due to user choice or other reasons) are reported
to the relationship planning system for planning
the next session.

2 Activities for User and Agent

We will demonstrate our operational system with a
several of the activities that the user and agent can
do together. In total, we will have more than ten
different activities including: discuss the weather,
learn about the activities to do with the agent, play
a social game of cards, talk about family/friends,
tell a life story to the agent, promote exercise,
promote nutrition, hear a humorous tale from the
agent, get health tips from the agent, speak with
a friend/family member via Skype

TM
(with all the

details of SKYPE managed by the agent), and
manage a personal calendar for the user.

A typical interaction with the agent might start
with some greetings (specific to the time of day)
and then some discussion of the weather. The
weather discussion can be as short as today’s
weather forecast or extend to the next day, weather
in other cities, and weather where friends or fam-
ily live. At the user’s choice, weather might be fol-
lowed by a social game of cards where the agent’s
and user’s hands in the game and the way the game
is played out are commented upon. If the user
and agent are somewhat well acquainted, there-
after might follow discussion of the user’s family
and friends. For each person that agent has learned
about, the agent might ask:

Figure 3 shows an excerpt from a menu-based
interaction in our currently running system. Fol-
lowing ths discussion of family and friends the
user might opt to end the conversation or continue
with other activities such as the calendaring sys-
tem, hearing some health tips from the agent, and
so on.

Agent: So how is Mary doing?
• I don’t want to talk about her today.
• I’m done talking about my family and friends.
• Overall pretty good.
• Not so great. ⇐User

Agent: Oh, what’s wrong?
• She is ill.
• She had an accident.
• She lost a family member. ⇐User
• She is pretty lonely.
• She is postponing a visit to me.

Agent: That’s very sad.

Figure 3: Example menu-based interaction.

Acknowledgments

This work is supported in part by the National Sci-
ence Foundation under award IIS-1012083. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.

References
T. Bickmore and D. Schulman. 2012. Empirical val-

idation of an accomodation theory-based model of
user-agent relationship. In Proc. Int. Conf. on Intel-
ligent Virtual Agents, Santa Cruz, CA.

T. Bickmore, L. Caruso, K. Clough-Gorr, and
T. Heeren. 2005. “It’s just like you talk to a friend”–
Relational agents for older adults. Interacting with
Computers, 17(6):711–735.

W. Coon, C. Rich, and C. Sidner. 2013. Activity plan-
ning for long-term relationships. In Proc. Int. Conf.
on Intelligent Virtual Agents, Edinburgh, UK.

C.D. Kidd and C. Breazeal. 2007. A robotic weight
loss coach. In Proc. 22nd National Conference on
Artificial Intelligence, Vancouver, Canada.

C. Rich and C. L. Sidner. 2012. Using collaborative
discourse theory to partially automate dialogue tree
authoring. In Proc. Int. Conf. on Intelligent Virtual
Agents, Santa Cruz, CA, September.

C. L. Sidner, C. Lee, C. Kidd, N. Lesh, and C. Rich.
2005. Explorations in engagement for humans and
robots. Artificial Intelligence, 166(1-2):104–164.

150


