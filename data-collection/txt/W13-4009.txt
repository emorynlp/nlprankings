



















































Generating More Specific Questions for Acquiring Attributes of Unknown Concepts from Users


Proceedings of the SIGDIAL 2013 Conference, pages 70–77,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

Generating More Specific Questions for Acquiring Attributes of
Unknown Concepts from Users

Tsugumi Otsuka†, Kazunori Komatani†, Satoshi Sato†, Mikio Nakano‡

† Graduate School of Engineering, Nagoya University, Nagoya, Aichi 464–8603, Japan
‡ Honda Research Institute Japan Co., Ltd., Wako, Saitama 351–0114, Japan
{t ootuka,komatani,ssato}@nuee.nagoya-u.ac.jp, nakano@jp.honda-ri.com

Abstract

Our aim is to acquire the attributes of con-
cepts denoted by unknown words from
users during dialogues. A word unknown
to spoken dialogue systems can appear in
user utterances, and systems should be ca-
pable of acquiring information on it from
the conversation partner as a kind of self-
learning process. As a first step, we pro-
pose a method for generating more spe-
cific questions than simple wh-questions
to acquire the attributes, as such ques-
tions can narrow down the variation of
the following user response and accord-
ingly avoid possible speech recognition er-
rors. Specifically, we obtain an appropri-
ately distributed confidence measure (CM)
on the attributes to generate more specific
questions. Two basic CMs are defined us-
ing (1) character and word distributions in
the target database and (2) frequency of
occurrence of restaurant attributes on Web
pages. These are integrated to comple-
ment each other and used as the final CM.
We evaluated distributions of the CMs by
average errors from the reference. Re-
sults showed that the integrated CM out-
performed the two basic CMs.

1 Introduction

In most spoken dialogue systems, knowledge
bases for the systems are constructed off-line. In
other words, they are not updated during dia-
logues. On the other hand, humans update their
knowledge not only by reading books but also
through interaction with other people. When they
encounter an unknown word during conversations,
humans notice that it is new to them and acquire
knowledge about it by asking their conversational
partner. This self-learning process is one of the

Tell me about 

“Osteria Liu”.
I don’t know that restaurant.

Is it “Italian”? 

Tell me about

“Toyo”.
I don’t know that restaurant.

What type of cuisine is it?

SystemUser

Figure 1: Example of simple and specific ques-
tions.

most intelligent features of humans. We think that
applying this intelligent feature to spoken dialogue
systems will make them more usable.

We present a method that generates appropri-
ate questions in order to acquire the attributes of
a concept that an unknown word denotes when it
appears in a user utterance. Here, we define un-
known words as those whose attributes necessary
for generating responses were not defined by the
system developer; that is, unknown to the response
generation module in the spoken dialogue system.
The system cannot reply to user utterances includ-
ing such words even if they are correctly recog-
nized by its automatic speech recognition (ASR)
module.

Questions to the user to acquire the attribute
should be specific. In spoken dialogue sys-
tems, specific questions are far preferable to wh-
questions because they can narrow down varia-
tions of the following user response. Such ques-
tions lead to a better ASR performance of the re-
sponse and reduce the risk that it includes new
other unknown words.

Two example dialogues are shown in Figure 1.
Since our target task is restaurant database re-
trieval, we set the unknown words as restaurant
names and the attribute as their cuisine in our
restaurant database. In the examples shown, the
system uses a simple wh-question (the upper part)
and a specific Yes-No question (the lower part) to
obtain cuisine types. Here, “Toyo” and “Osteria
Liu” are restaurant names. We assume that the

70



Table 1: Question types according to the number of cuisines (num).
num Question form Example

1 Yes-No question Is it cuisine c1?
2 Alternative question Which cuisine is it, c1 or c2?
3 3-choice question Which cuisine is it, c1, c2, or c3?

≥4 Wh-question What cuisine is it?

system already knows these are restaurant names
but does not know its attributes such as its cuisine
type. The system uses a wh-question for “Toyo”
since no clue is obtained for it. In contrast, since
“Osteria Liu” contains information on cuisines in
the name itself, a concrete Yes-No question is used
to ask whether the cuisine is “Italian”.

We propose a method for providing a well-
distributed confidence measure (CM) to generate
more specific questions. For this purpose, we esti-
mate the cuisine type of a restaurant from its name,
which is assumed to be unknown to the system.
There have been many previous studies that esti-
mate word and character attributes using Web in-
formation (Pasca et al., 2006; Yoshinaga and Tori-
sawa, 2007). Our two estimation methods are rel-
atively simpler than these studies, since our main
focus is to generate more concrete questions on the
basis of appropriate CMs. That is, the CMs should
be high when the system seems to correctly esti-
mate a cuisine type and low when the estimation
seems difficult.

We assume a restaurant name as the input; that
is, we suppose that the system can recognize the
restaurant name in the user’s utterance correctly
by its ASR module and understand it is a restau-
rant name by its LU module. Nevertheless, it
still remains unknown to its response generation
module. This is a feasible problem when using
a large vocabulary continuous speech recognition
(LVCSR) engine containing over several million
words (Jyothi et al., 2012) and a statistical named
entity (NE) tagger (Tjong Kim Sang and Meul-
der, 2003; Zhou and Su, 2002; Ratinov and Roth,
2009).

The problem we tackle in this paper is differ-
ent from trying to estimate the NE class of an un-
known word (Takahashi et al., 2002; Meng et al.,
2004). We assume the system already knows that
it is a restaurant name. Rather, we try to acquire
the attribute (e.g., cuisine type) of the concept of
the unknown word, which is required for generat-
ing responses about the restaurant in subsequent

dialogues.

2 Generating Questions Based on CM

The system determines a question type on the ba-
sis of CM. The CM is estimated for each cuisine
type cj in the target database. In this paper, the
number of cuisine types is 16, all of which are
in our restaurant database; that is, cj ∈ C and
|C| = 16.

Table 1 shows the four question types and their
examples. These are determined by parameter
num, which is the number of cuisine types that
should be included in the question. If the sys-
tem obtains one cuisine type that it is very con-
fident about and thus has a high CM, it should
generate the most specific question, i.e., a Yes-
No question; in this case, the number should be
1. In contrast, if unreliable cuisine types are ob-
tained, which means lower CMs, the system gen-
erates questions including several cuisine types.

The num can be determined by Equation (1):

num = min(n) s.t.
n∑

j=1

CM(cj) > θ, (1)

where CM(cj) is a confidence measure for cui-
sine type cj in its descending order. θ is a constant
and can be manually decided considering the dis-
tribution of CM(cj). This equation means that if
only the CM(c1) is greater than θ (i.e., n = 1),
the system generates a specific question includ-
ing only cuisine type c1, while if the total from
CM(c1) to CM(c4) is smaller than θ (i.e., n = 4),
the system does not use estimated cuisine types
and instead generates a wh-question.

If the CM on the cuisine type is well-distributed,
the system can generate appropriate questions. In
the following section, methods to obtain such CMs
are explained.

3 Estimating Cuisine Types and
Calculating CM

The final CM is obtained by integrating two ba-
sic CMs. The system then uses this final CM to

71



Feature	  
selec+on	  by	  
mutual	  

informa+on	  

Training	
data	

　	
		

	  
	  
	  
	  
	  
	  
	

DB	

Japanese pub	

Sushi Goichi (寿司 五一)	

Koikoi (こいこい)	

Japanese restaurant	Tanaka Sushi (田中寿司)　	

Maru Sushi (まる寿司) 	 Japanese restaurant	

Japanese restaurant	

Restaurant name	 Cuisine	

. . . .	 . . . .	

Quinci CENTRARE	 Italian	

Hyakuraku (百楽)	 Chinese restaurant　	

C’s ave cafe	 Cafe 	

Classifier:	  
Maximum	  entropy	  (ME)	  model	

	  Azuma	  Sushi	  
	  (東寿司)	

Input:	  	  
Restaurant	  name	

Output:	  CMD	

Japanese restaurant 	
Japanese pub	
Cafe	
. . . . .  	

: 0.9	
: 0.05	
: 0.0006	
	

Figure 3: Overview of CMD calculation.

Web	

Es&ma&on	  from	  DB	  

Es&ma&on	  from	  Web	  
CM	  

Integra&on	  

DB	

CMW	  

Restaurant	  
name	  

CMI	  

CMD	  

Question generation	
based on CM	

Figure 2: Process overview.

generate questions. The two basic CM estimation
methods are:

1. Using word and character distribution in the
target database

2. Using frequency of the restaurant attributes
on the Web

A process overview of the proposed method is
shown in Figure 2. Its input to the system is an
unknown restaurant name and its output is the es-
timated CMs. The system generates questions on
the basis of the estimated CMs, which are calcu-
lated for each cuisine type.

3.1 Attribute Estimation Using Word and
Character Distribution in Database

We estimate the cuisine types of an unknown
restaurant by using the word and character distri-
bution in the target database. The target database
contains many pairs of restaurant names and cui-
sine types. The estimation is performed by us-
ing supervised machine learning trained with the
pairs. The overview of calculating CMD is shown
in Figure 3. This approach is based on our in-
tuition that some cuisine types can be estimated
from restaurant names on the basis of their char-
acter types or typical character sequences they

contain. For example, a restaurant name com-
posed of only katakana1 is probably a French or
Italian restaurant because words imported from
other countries to Japan are called “katakana loan-
words” and are written in katakana characters
(Kay, 1995).

We use the maximum entropy (ME) model
(Berger et al., 1996) as a classifier. Its posterior
probability p(cj |si) is used as a CMD denoting
the CM estimated using a database. CMD is cal-
culated as

CMD(si, cj) = p(cj |si)

=
1

Z
exp

[
~λ(cj) · ~φ(si)

]
, (2)

where si is a restaurant name, cj (∈ C) is a
cuisine type, ~φ(si) is a feature vector obtained
from a restaurant name, ~λ(cj) is a weight vector,
and Z is a normalization coefficient that ensures∑

cj
CMD(si, cj) = 1.

We use three types of feature vectors obtained
from each restaurant name:

• Character n-grams (n = 1, 2, 3)
• Words
• Character types

The feature values of the character n-gram and the
word are scored as 1 if such features are contained
in the restaurant name. The Japanese morpholog-
ical analyzer Mecab (Kudo et al., 2004) with the
IPADIC dictionary is used to segment restaurant
names into word sequences. The character type

1Katakana is a Japanese syllabary. There are three kinds
of characters in Japanese. Kanji (Chinese character) are lo-
gograms and hiragana and katakana are syllabaries. Katakana
is mainly used for writing imported words and hiragana is
used for writing original Japanese words.

72



Web	  page	 Cuisine	  frequency	  
Japanese restaurant	
Italian restaurant	
Western-style restaurant	

74  times	
  8   times	
  1    time	

Output:	  CMW	
Japanese restaurant	
Italian restaurant	
Western-style restaurant	
. . . .  	

：　0.8	
：　0.11	
：　0.0009	
	

Azuma	  Sushi	  (東寿司)	
Input:	  	  Restaurant	  name	

	  Obtaining	  related	  pages	  	  
about	  target	  restaurant	

1. 

	  	  	  CalculaBng	  Pfreq(cj)	2. 

	  	  Scaling	  Pfreq(cj)	3. 

Yahoo!	  
Web	  search	  API 

“Azuma	  Sushi”	  	  	  Aichi	  	  	  restaurant	  
(東寿司　愛知　レストラン)	  

Ranking:  2nd	

Search	  query	  	  

Number of 	
cuisine types:  	3 

Figure 4: Overview of CMW calculation.

is represented by the four character types used in
the Japanese writing system: hiragana, katakana,
kanji (Chinese characters), and romaji (Roman let-
ters). For example, the restaurant name “Maru
Sushi (まる寿司)” includes two character types:
“Maru (まる)” is written in hiragana and “Sushi
(寿司)” is written in kanji. Therefore, the fea-
ture values for hiragana and kanji are both 1, while
those for katakana and romaji are 0. Another ex-
ample is shown using the restaurant “IB cafe (IB
カフェ)”, in which the “IB” part is romaji and the
“cafe (カフェ)” part is katakana. Therefore, in this
case, the feature values of katakana and romaji are
1 and those of hiragana and kanji are 0.

We perform feature selection for the obtained
features set (Guyon and Elisseeff, 2003). The clas-
sifier needs to be built without overfitting because
we assume that a restaurant name as the input to
this module is unknown and does not exist in the
database. We use the mutual information (Peng
et al., 2005; Yang and Pedersen, 1997) between
each feature and the set of cuisine types as its cri-
terion. This represents how effective each feature
is for the classification. For example, in the fea-
tures obtained from the restaurant name “まる寿
司”, which is a Japanese restaurant, the 2-gram
feature “寿司” frequently co-occurs with the cui-
sine type “Japanese restaurant”. This is an effec-
tive feature for the cuisine type estimation. In con-
trast, the 2-gram feature “まる” is not effective be-
cause its co-occurrence with cuisine types is infre-
quent. Mutual information is calculated as

I(fk;C) =
∑

cj∈C
p(fk, cj) log

p(fk, cj)

p(fk)p(cj)
, (3)

where p(fk) is an occurrence probability of feature
fk in the database, p(cj) is an occurrence probabil-
ity of cuisine type cj (∈ C), and p(fk, cj) is a joint
probability of the feature and the cuisine type.

Features having lower mutual information val-
ues are removed until we deem that overfitting has
been avoided, specifically, when the estimation
accuracies become almost the same between the
closed and open tests. We confirm this by cross-
validations (CV) instead of open tests.

3.2 Estimation Using the Web

We estimate a restaurant’s cuisine type and calcu-
late CMs by using its frequency on the Web as
CMW . This is based on an assumption that a
restaurant’s name appears with its cuisine type on
Web pages. CMW is calculated in the following
steps, as shown in Figure 4.

1. Obtaining related Web pages:
Twenty pages per search query were ob-
tained, as this was the limit of the number of
pages when this experiment was performed.
We used the Yahoo! Web search API2. The
query is formed with the target restaurant
name and the following two words: “Aichi
(愛知)” and “restaurant (レストラン)”. The
two are added to narrow down the search re-
sult since our domain is a restaurant search
in Aichi prefecture. For example, the query
is “<rest>愛知レストラン” for the target
restaurant name <rest>.

2http://developer.yahoo.co.jp/webapi/search/websearch
/v2/web search.html

73



2. Calculating Pfreq(cj):
We count the frequency of each cuisine type
cj in the i-th Web pages, which are ranked
by the Web search API. We then sum up the
frequency through all the obtained pages and
calculate its posterior probability.

Pfreq(cj) =

∑
i wi · freqi(cj)∑

cj

∑
i wi · freqi(cj)

(4)

Here, freqi(cj) is the frequency of cj in the
i-th page. Weight wi is calculated using two
factors, rank(i) and cuisine(i):

wi =
1

rank(i) · cuisine(i) (5)

(a) rank(i): The ranking of pages in the
Web search API
We assume that a Web page is more re-
lated to the target restaurant if the Web
search API ranks it higher.

(b) cuisine(i): The number of cuisine
types in the i-th Web page
We assume that a Web page contain-
ing many different cuisine types does
not indicate one particular cuisine. For
example, a page on which only “Chi-
nese restaurant” appears is more reliable
than that on which more cuisine types
(“Chinese restaurant”, “Japanese restau-
rant”, “Japanese pub”, and “Western-
style restaurant”, for example) appear,
as a page indicating a “Chinese restau-
rant”.

3. Scaling Pfreq(cj):
CMW is calculated by scaling each
Pfreq(cj) with the corresponding αj . αj
is a scaling coefficient that emphasizes the
differences among CMW : αj is equal to
or smaller than 1 and becomes smaller as j
increases.

CMW (cj) =
αjPfreq(cj)∑
cj

αjPfreq(cj)
(6)

αj = Pfreq(cj)/Pfreq(c1) (7)

3.3 Integration of CMs

We define CMI by integrating the two basic CMs:
CMD and CMW . Specifically, we integrate them
by the logistic regression (Hosmer Jr. et al., 2013)

shown in Equation (8). The optimal parameters,
i.e., weights for the CMs, are determined using a
data set with reference labels. The teacher signal
is 1 if the estimated cuisine type is correct and 0
otherwise.

CMI(cj) =
1

1 + exp(−f(cj))
(8)

f(cj) = wDCMD(cj) + wW CMW (cj) + w0

Here, wD and wW are the weights for CMD and
CMW , and w0 is a constant.

4 Experiment

We evaluate our method to obtain the CMs from
three aspects. First, we evaluate the effect of fea-
ture selection based on mutual information. Sec-
ond, we evaluate how the CMs were distributed
and whether they were appropriate measures for
question generation. Third, we determine the ef-
fectiveness of integrating the two basic CMs. In
this paper, we used a restaurant database in Aichi
prefecture containing 2,398 restaurants with 16
cuisine types.

4.1 Effect of Feature Selection Based on
Mutual Information

We determined whether overfitting could be
avoided by feature selection based on mutual in-
formation in the estimation using a database. We
regard overfitting to be avoided when estimation
accuracies become almost the same between the
closed and open tests. For the closed test, estima-
tion accuracy was calculated for all 2,398 restau-
rants in the database by using a classifier that was
trained with the same 2,398 restaurants. For the
open test, it was calculated by 10-fold CV for the
2,398 restaurants. This experiment is not for de-
termining a feature set but rather for determining
a feature selection ratio. That is, the feature se-
lection result is kept not as a feature set but as a
ratio. The resulting ratio is applied to the num-
ber of features appearing in another training data
(e.g., that in Section 4.2) and then the feature set
is determined.

Figure 5 shows the estimation accuracy of the
closed test and the 10-fold CV when the feature
selection was applied. The horizontal axis denotes
ratios of features used to train the classifier out of
20,679 features in total. They were selected in de-
scending order of mutual information. The ver-
tical axis denotes the estimation accuracy of the

74



 0.6

 0.7

 0.8

 0.9

 1

 1 10 100

E
s
ti
m

a
ti
o
n
 a

c
c
u
ra

c
y
 (

%
)

Feature selection ratio (%)

Closed
10-fold CV

Figure 5: Estimation accuracies of closed test and
10-fold CV.

cuisine types. Figure 5 shows that, at first, over-
fitting occurs if all features were used for training;
that is, the feature selection ratio = 100%. This
can be seen by the difference in estimation accu-
racies, which was 28.1% between the closed test
and the 10-fold CV. The difference decreased as
the number of used features decreased, and almost
disappeared at feature selection ratio = 0.8%. In
these selected features, as an example, the 2-gram
“gyoza (餃子)”, which seems intuitively effective
for cuisine type estimation is, included3.

4.2 Evaluation for Distribution of CMs

We evaluate the distribution of CMs obtained with
the estimation results. Specifically, we evaluated
three types of distributions: CMD, CMW , and
CMI . We extracted 400 restaurants from the
database and used them as evaluation data. The
remaining 1,998 restaurants were used as training
data for the classifier to calculate CMD. In all
features obtained from these 1,998 restaurants, the
ME classifier uses 0.8% of them, which is the fea-
ture selection ratio based on the mutual informa-
tion determined in Section 4.1. That is, the feature
set itself obtained in the feature selection is not de-
livered into the evaluation in this section.

We used average distances between each CM
score and its reference as the criterion to evalu-
ate the distribution of the CMs. Generally, CMs
should be as highly scored as possible when the
estimation is correct and as lowly scored as possi-
ble otherwise. We calculate the distances over the

3“Gyoza (餃子)” is a kind of dumplings and one of the
most popular Chinese foods. It often appears in Chinese
restaurant names in Japan.

Table 3: Evaluation against each CM.
eval(CMx) MB(CMx)

CMD 0.31 0.37
CMW 0.28 0.32
CMI 0.25 0.28

400 estimation results.

eval(CMx) =

∑N
i |CM ix − φix|

N
(9)

Here, N is the total number of the estimation re-
sult, so N = 400 in this paper. φix for CM

i
x is

defined as

φix =

{
1, If estimation result i is correct
0, Otherwise

(10)

Note that φx depends on CMx because estimation
results differ depending on the CMx used.

We also set the majority baseline as Equation
(11). Here, all CMs are regarded as 0 or 1 in Equa-
tion (9). Because there were more correct estima-
tion results than incorrect ones, as shown in Table
2, we used 1 for the majority baseline, as

MB(CMx) =

∑N
i |1 − φix|

N
. (11)

The results are shown in Table 3. A compar-
ison of the three eval(CMx) demonstrates that
the integrated CMI is the most appropriate in our
evaluation criterion because it is the lowest of the
three. The relative error reduction rates from CMI
against CMD and CMW were 16% and 37%, re-
spectively. Each eval(CMx) outperformed the
corresponding majority baseline.

4.3 Effectiveness of Integrated CM
We verify the effectiveness of the CM integration
from another viewpoint. Specifically, we confirm
whether the number of correct estimation results
increases by integration.

First, we show the distribution of the three CMs
and whether they were correct or not in Table 2.
The bottom row of the table shows that CMI ob-
tained correct estimation results for 297 restau-
rants, which is the highest of the three CMs.

More specifically, we investigated how many
estimation results changed by using the three
CMs. Here, an estimation result means the cui-
sine type that is given the highest confidence. This
result is shown in Table 4, where C denotes a case

75



Table 2: Distribution of estimation results by CM values.
CMD CMW CMI

CM range Correct Incorrect Correct Incorrect Correct Incorrect
0.0 – 0.1 0 0 0 32 2 10
0.1 – 0.2 0 0 0 11 9 15
0.2 – 0.3 1 16 14 22 15 18
0.3 – 0.4 6 19 28 19 10 8
0.4 – 0.5 11 25 29 21 13 12
0.5 – 0.6 21 29 56 9 13 12
0.6 – 0.7 22 28 85 7 15 7
0.7 – 0.8 41 16 42 3 17 6
0.8 – 0.9 21 9 19 1 19 9
0.9 – 1.0 131 4 1 1 184 10

Total 254 146 274 124 297 103

Table 4: Estimation results by three CMs.

CMD / CMW
I / I I / C C / I C / C

C 0 51 33 213
CMI I 85 10 8 0

C: correct, I: incorrect

when a cuisine type was correctly estimated and I
denotes that it was not. The four columns with ’/’
denote the numbers of estimation results for CMD
and CMW . For example, the C/I column denotes
that estimation results based on the database were
correct and those using the Web were incorrect,
that is, the I/C and C/I columns mean that the
two estimation results differed. The table shows
that 102 of 400 restaurants corresponded to these
cases, that is, either of the two estimation results
was incorrect. It also shows that estimation results
for 84 of the 102 (82%) restaurants became correct
by the integration.

Two examples are shown for which the esti-
mation results became correct by the integration.
First, “Kaya (加屋)” is a restaurant name whose
cuisine type is “Japanese-style pancake”. Its cui-
sine type was correctly estimated by CMW while
it was incorrectly estimated as “Japanese pub” by
CMD. This was because, in Japanese, “Kaya (加
屋)” has no special meaning associated with spe-
cific cuisine types. Thus, it is natural that its cui-
sine type was incorrectly estimated from the word
and character distribution of the name. On the
other hand, when Web pages about it were found,
“Japanese-style pancake” co-occurs frequently in
the obtained pages, and thus it was correctly es-
timated by CMW . Second, “Tama-Sushi Imaike
(玉寿司 今池)” is a restaurant name whose cui-
sine type is “Japanese restaurant”. Its cuisine type
was estimated correctly by CMD while it was in-

correctly estimated as “Japanese pub” by CMW .
CMD was effective in this case because the part
of “Sushi (寿司)” indicates a Japanese cuisine. No
Web pages for it were found indicating its cuisine
type correctly, and thus CMW failed to estimate
it.

5 Conclusion

Our aim is to acquire the attributes of an unknown
word’s concept from the user through dialogue.
Specifically, we set restaurant cuisine type as the
attribute to obtain and showed how to generate
specific questions based on the estimated CM. We
use two estimation methods: one based on the tar-
get database and the other on the Web. A more
appropriate CM was generated in terms of its dis-
tribution and estimation accuracy by integrating
these two CMs.

There is little prior research on obtaining and
updating system knowledge through dialogues,
with the notable exception of the knowledge au-
thoring system of (Knott and Wright, 2003). Their
system also uses the user’s text input for construct-
ing the system knowledge from scratch, which is
used to generate simple stories. Our study is dif-
ferent in two points: (1) we focus on generating
several kinds of questions because we use ASR,
and (2) we try to handle unknown words, which
will be stored in the target database to be used in
future dialogues.

We should point out that these kinds of ques-
tions can be generated only when the types of un-
known concepts are given. We assume the type
of unknown concepts is already known and thus
the attributes to be asked are also known. More
specifically, we assume that the concept denoted
by an unknown word is a restaurant name and its
attributes are also known. The cuisine type has
been estimated as one of the attributes. However,

76



when the type is unknown, the system first needs
to identify its attributes to ask. That is, the sys-
tem first needs to ask about its supertype and then
to ask about attributes that are typical for objects
of this type. This issue needs to be addressed in
order for the system to acquire arbitrary new con-
cepts. This paper has shown the first step for ob-
taining concepts through dialogues by generating
questions. Many issues remain in this field for fu-
ture work.

References
Adam L. Berger, Vincent J. Della Pietra, and Stephen

A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39–71.

Isabelle Guyon and André Elisseeff. 2003. An intro-
duction to variable and feature selection. The Jour-
nal of Machine Learning Research, 3:1157–1182.

David W. Hosmer Jr., Stanley Lemeshow, and Rod-
ney X. Sturdivant. 2013. Applied logistic regres-
sion. Wiley. com.

Preethi Jyothi, Leif Johnson, Ciprian Chelba, and Brian
Strope. 2012. Large-scale discriminative language
model reranking for voice search. In Proceedings
of the NAACL-HLT 2012 Workshop: Will We Ever
Really Replace the N-gram Model? On the Future
of Language Modeling for HLT, pages 41–49.

Gillian Kay. 1995. English loanwords in Japanese.
World Englishes, 14(1):67–76.

Alistair Knott and Nick Wright. 2003. A dialogue-
based knowledge authoring system for text genera-
tion. In AAAI Spring Symposium on Natural Lan-
guage Generation in Spoken and Written Dialogue,
Stanford, CA, pages 71–78.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), pages 230–237.

Helen Meng, P. C. Ching, Shuk Fong Chan, Yee Fong
Wong, and Cheong Chat Chan. 2004. ISIS:
An adaptive, trilingual conversational system with
interleaving interaction and delegation dialogs.
ACM Transactions on Computer-Human Interac-
tion, 11(3):268–299.

Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the World Wide Web of facts - step one:
the one-million fact extraction challenge. In Pro-
ceedings of the 21st National Conference on Artifi-
cial intelligence - Volume 2, AAAI ’06, pages 1400–
1405. AAAI Press.

Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(8):1226–1238.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155.

Yasuhiro Takahashi, Kohji Dohsaka, and Kiyoaki
Aikawa. 2002. An efficient dialogue control
method using decision tree-based estimation of out-
of-vocabulary word attributes. In Proc. Int’l Conf.
Spoken Language Processing (ICSLP), pages 813–
816.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147.

Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412–420.

Naoki Yoshinaga and Kentaro Torisawa. 2007.
Open-domain attribute-value acquisition from semi-
structured texts. In Proceedings of the Workshop
of OntoLex07 - From Text to Knowledge: The Lexi-
con/Ontology Interface, pages 55–66.

Guo Dong Zhou and Jian Su. 2002. Named entity
recognition using an HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 473–
480.

77


