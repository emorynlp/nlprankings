



















































Critical Reflections on Evaluation Practices in Coreference Resolution


Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 1–7,
Atlanta, Georgia, 13 June 2013. c©2013 Association for Computational Linguistics

Critical Reflections on Evaluation Practices in Coreference Resolution

Gordana Ilić Holen
Department of Informatics

University of Oslo
Norway

gordanil@ifi.uio.no

Abstract

In this paper we revisit the task of quantitative
evaluation of coreference resolution systems.
We review the most commonly used metrics
(MUC, B3, CEAF and BLANC) on the basis of
their evaluation of coreference resolution in
five texts from the OntoNotes corpus. We ex-
amine both the correlation between the met-
rics and the degree to which our human judge-
ment of coreference resolution agrees with the
metrics. In conclusion we claim that loss of
information value is an essential factor, insuf-
ficiently adressed in current metrics, in human
perception of the degree of success or failure
of coreference resolution. We thus conjec-
ture that including a layer of mention infor-
mation weight could improve both the coref-
erence resolution and its evaluation.

1 Introduction and motivation

Coreference resolution (CR) is the task of link-
ing together multiple expressions of a given entity
(Yang et al., 2003). The field has experienced a
surge of interest with several shared tasks in re-
cent years: SemEval 2010 (Recasens et al., 2010),
CoNLL 2011 (Pradhan et al., 2011) and CoNLL
2012 (Pradhan et al., 2012). However the field has
from the very start been riddled with problems re-
lated to the scoring and comparison of CR systems.
Currently there are five metrics in wider use: MUC
(Vilain et al., 1995), B3 (Bagga and Baldwin, 1998),
the two CEAF metrics (Luo, 2005) and BLANC (Re-
casens and Hovy, 2011). As there is no global agree-
ment on which metrics are the most appropriate, the

above-mentioned shared tasks have used a combi-
nation of several metrics to evaluate the contenders.
Although coreference resolution is a subproblem of
natural language understanding, coreference resolu-
tion evaluation metrics have predominately been dis-
cussed in terms of abstract entities and hypothetical
system errors. In our view, it is of utmost importance
to observe actual texts and actual system errors.

2 Background: The metrics

In this section, we will present the five metrics in the
usual terms of precision, recall and F-score. We fol-
low the predominant practice and use the term men-
tion for individual referring expressions, and entity
for sets of mentions that refer to the same object
(Luo et al., 2004). We use the term key entity (K)
for gold entities, and response entity (R) for entities
which were produced by the CR system.

2.1 Link-based: MUC and BLANC
The MUC metric (Vilain et al., 1995) is based on
comparing the number of links in the key entity
(|K| − 1) to the number of links missing from the
response entity, routinely calculated as the number
of partitions of the key entity |p(K)| minus one, so
Recall = (|K|−1)−(|p(K)|−1)|K|−1 =

|K|−|p(K)|
|K|−1 . For

the whole document, recalls for entities are simply
added: Recall =

∑ |Ki|−|p(Ki)|∑
(|Ki|−1) In calculating pre-

cision, the case is inverted: The base entity is now
the response, and the question posed is how many
missing links have to be added to the key partitions
to form the response entity.

BLANC (Recasens and Hovy, 2011) is a variant of
the Rand index (Rand, 1971) adapted for the task

1



of coreference resolution. The BLANC metric makes
use of both coreferent and non-coreferent links, cor-
rect and incorrect. The final precision, recall and
F-score are the average of the P, R and F-score of
corresponding coreferential and non-referential val-
ues. However, since this is an analysis of isolated
entities, there are no non-coreferential links. For
that reason, in this paper we only present corefer-
ential precision, recall and F-score for this metric:
Pc =

rc
rc+wc , Rc =

rc
rc+wn and Fc =

2PcRc
Pc+Rc

, where
rc is the number of correct coreferential links, wc
the number of incorrect coreferential links, and wn
is the number of non-coreferential links incorrectly
marked as coreferent by the system.

2.2 Entity and mention-based: B3 and CEAF

B3 (Bagga and Baldwin, 1998) calculates precision
and recall for every mention in the document, and
then combines them to an overall precision and re-
call. Precision of a single mention mi is the number
of correct mentions in the response entity Ri that
containsmi divided by the total number of mentions
in Ri. Recall of mi is again the number of correct
mentions in Ri, this time divided by the number of
mentions in the key entity Ki that contains mention
mi. The precision and recall for the entire docu-
ment can be calculated as weighted sums of preci-
sion and recall of the individual mentions. The de-
fault weight, also used in this experiment is 1n , where
n is the number of mentions in the document.

CEAF (Luo, 2005) is based on the best alignment
of subsets of key and response entities. For any
mapping g ∈ Gm the total similarity Φ(g) is the
sum of all similarities. The best alignment g∗ is
found by maximizing the sum of similarities Φ(g)
between the key and response entities, while the
maximum total similarity is the sum of the best sim-
ilarities. Precision and recall are defined in terms
of the similarity measure Φ(g∗): P = Φ(g

∗)∑
i φ(Ri,Ri)

R = Φ(g
∗)∑

i φ(Ki,Ki)
.

There are two versions of CEAF with different
similarity measures, φm(K,R) = |K ∩ R| and
φe(K,R) =

2|K∩R|
|K|+|R| . φe is the basis for CEAFe

which shows a measure of correct entities while
CEAFm, based on φm, shows the percentage of cor-
rectly resolved mentions.

MUC B3 CEAFe CEAFm BLANC MELA

MUC – 0.46 0.22 0.47 0.35 0.63
B3 0.59 – 0.47 0.56 0.42 0.61

CEAFe 0.46 0.59 – 0.51 0.26 0.38
CEAFm 0.57 0.70 0.62 – 0.46 0.60
BLANC 0.57 0.70 0.57 0.68 – 0.35
MELA 0.59 0.73 0.59 0.70 0.70 –

Table 1: Kendall τ rank correlation coefficient for teams
participating in CoNLL shared tasks, with CoNLL 2011
in the upper right, CoNLL 2012 in the lower left corner.

3 Correlating CoNLL shared tasks results

To illustrate the complexity of the present evaluation
best practices, we have applied the Kendall τ rank
correlation coefficient to the ratings the metrics gave
coreference resolution systems that competed in the
two recent CoNLL shared tasks. The official metrics
of the CoNLL shared tasks was MELA (Denis and
Baldridge, 2009), a weighted average of MUC, B3

and CEAFe.
The results for CoNLL 2011 (Table 1) show

a rather weak correlation among the metrics go-
ing down to as low as 0.22 between CEAFe and
MUC. Somewhat surprisingly, the two link-based
metrics, MUC and BLANC, also show a low degree of
agreement (0.35), while the mention-based metrics,
CEAFm and B3, show the highest agreement of all
non-composite metrics. However, this agreement is
not particularly high either as the two metrics agree
on just above the half of all the cases (0.56).

The results for CoNLL 2012 show much higher
correlation among the metrics ranging from 0.46 to
0.70. Again CEAFm and B3 show the highest corre-
lation, but unlike in 2011 BLANC “joins” this clus-
ter. CEAFe and MUC are again least correlated, while
CEAFe and BLANC, in 2011 almost independent,
show average correlation (0.57) in 2012.

In our view, comparatively low correlations as
well as surprising variation from year to year sug-
gests a certain degree of ’fuzziness’ in quantitative
coreference resolution evaluation. We leave the in-
vestigation of variation between the two years for
future work.

4 Error analysis

To better understand the functioning of the met-
rics we have conducted an error analysis on the
key/response entity pairs from five short texts from

2



the development corpus of the CoNLL 2011 Shared
Task (Pradhan et al., 2011), one text from each of
the five represented genres: Broadcast Conversa-
tions (BC), Broadcast News (BN), Magazine (MZ),
News Wire (NW) and Web Blogs and News Groups
(WB). The texts were chosen as randomly as pos-
sible, the only constraint being length1. The gold
standard texts are originally from OntoNotes 4.0,
and contain 64 mentions distributed among 21 key
entities. The response texts are the output of Stan-
ford’s Multi-Pass Sieve Coreference Resolution Sys-
tem (Lee et al., 2011).

4.1 Categorization

Instead of classifying entities according to their
score by some of the metrics, or a combination of
several of them, as done by the CoNLL shared tasks,
we have based the classification on a notion of lin-
guistic common sense – our subjective idea of how
humans evaluate the success or failure of CR. We
divide key/response entity pairs into four categories:

• Category 1: Perfect match

• Category 2: Partial match

• Category 3: Merged entities

• Category 4: Failed coreference resolution

We will concentrate on the amount of informational
value from the key entity that has been preserved in
the response entity. In the course of these experi-
ments, our aim is to see if that rather informal idea
can be operationalized in a way amenable to future
use in automated CR and/or quantitative evaluation.

4.1.1 Category 1: Perfect match

This class consists of four key/response entity
pairs with complete string match. The key and
response entities being identical, all metrics show
unanimously precision and recall of 100%. The
informational value is, of course, completely pre-
served. Unfortunately, those examples are few and
simple: They constitute only 19% of the entities and
14% of mentions in this sample, and all seem to be
achieved by the simplest form of string matching.

Key Response MUC B3 CEAFe CEAFm BLANC
entities entities

BC45
• The KMT P 100.00 100.00 90.90 100.00 100.00
vice chairman R 80.00 83.33 90.90 83.33 66.67
• Wang • Wang F 88.89 90.91 90.90 90.91 80.00
Jin-pyng Jin-pyng
• his • his
• his • his
• He • He
• he • he

BC22
• KMT • KMT P 100.00 100.00 80.00 100.00 100.00
Chairman Chairman R 50.00 66.67 80.00 66.67 33.33
Lien Chan Lien Chan F 66.67 80.00 80.00 80.00 50.00
• Chairman
Lien Chan
• Lien Chan • Lien Chan

BN1
• Bill • Bill P 100.00 100.00 76.92 100.00 68.42
Clinton Clinton R 85.71 62.50 76.92 62.50 46.43
• The President F 92.31 76.92 76.92 76.92 55.32
• he
• his
• Mr.Clinton • Mr.Clinton
• his • his
• He • He
• he • he

NW2
• New • New P 100.00 100.00 88.89 100.00 100.00
Zealand Zealand R 75.00 80.00 88.89 80.00 60.00
• New • New F 85.71 88.89 88.89 88.89 75.00
Zealand Zealand
• New • New
Zealand Zealand
• New • New
Zealand’s Zealand’s
• New
• Zealand

Table 2: Category 2a: Partial match (partial entities)

4.1.2 Category 2: Partial match

The partial response entities can be divided in two
subcategories: 2a) The cases where the response en-
tities are partial, i.e.they form a proper subset of the
key entity mentions (Table 2) and 2b) The cases
where the response mentions are partial, i.e. sub-
strings of the corresponding key mentions (Table 3).

The scoring of the examples has followed CoNLL
shared tasks’ strict mention detection requirements2

with the consequence that Category 2b entities have
received considerably lower scores than the Cate-
gory 2a entities even in cases where the loss of
informational value has been comparable. For in-
stance, the response entity NW1 (Table 3) has re-
ceived an average F-score of 56.67%, but its loss
of informational value is comparable to that in enti-
ties BC45 and BN1 (Table 2). The BC45’s response
entity has lost the information that Jiyun Tian
is a vice-chief, while entities BC45 and BN1
have lost the information that the person referred to

1The texts longer than five sentences were discarded, to
make the analysis tractable.

2Only response mentions with boundaries identical to the
gold mentions are recognized as correct (Pradhan et al., 2011)

3



is The KMT vice chairman (BC45) and The
President (BN1). However, the latter mentions
have received a considerably higher average F-score
of 88.32% and 75.68% respectively. This indicates
that stricter mention detection requirements do not
necessarily improve the quality of CR evaluation.

Key Response MUC B3 CEAFe CEAFm BLANC
entities entities

MZ22
• a school in • a school P 0.00 50.00 50.00 50.00 0.00
Shenzhen for in Shenzhen R 0.00 50.00 50.00 50.00 0.00
the children of F 0.00 50.00 50.00 50.00 0.00
Hong Kong
expats
• the school • the school
in Shenzhen in Shenzhen

NW0
• China’s • People’s P 0.00 0.00 0.00 0.00 0.00
People’s Congress R 0.00 0.00 0.00 0.00 0.00
Congress F 0.00 0.00 0.00 0.00 0.00
• China’s • People’s
People’s Congress
Congress

NW1
• vice-chief • committee P 50.00 66.67 66.67 66.67 33.33
committee member R 50.00 66.67 66.67 66.67 33.33
member Jiyun Tian F 50.00 66.67 66.67 66.67 33.33
Jiyun Tian
• Jiyun Tian • Jiyun Tian
• He • He

NW5
• China’s • China’s P 0.00 50.00 50.00 50.00 0.00
People’s People’s R 0.00 50.00 50.00 50.00 0.00
Congress Congress F 0.00 50.00 50.00 50.00 0.00
delegation delegation
led by
vice-chief
committee
member Jiyun
Tian
• the • the
delegation delegation
from China’s from China’s
People’s People’s
Congress Congress

Table 3: Category 2b: Partial match (partial mentions).

4.1.3 Category 3: Merged entities
This category consists of response entities that

contain mentions from two or more key entities (Ta-
ble 4). Our sample contains only four examples in
this category, but it is still possible to discern two
subcategories:

1. The new information is incorrect
In the key entity MZ40, the sex of the gender-neutral
her ten-year-old child has been given by
the mention him. Replacing it with the mention
she in the response entity gives the wrong informa-
tion about the child’s sex. Entities BN2 and MZ17
also belong to this subcategory, but here the men-
tions in the response entity are morphologically in-
consistent, thus making the mistake easier to detect.

2. The new information is correct or neutral
In entity pair MZ19 the key mention the latter
group was replaced with response mention them,

Key Response MUC B3 CEAFe CEAFm blanc
entities entities

BN2
• The P 66.67 25.00 33.33 25.00 0.00
President R 0.00 50.00 33.33 50.00 0.00

• he and his • he and his F 0.00 33.33 33.33 33.33 0.00
wife, now a wife, now a
New York New York
senator senator
• their • he

• his

MZ19
• the more • the more P 50.00 66.67 66.67 66.67 33.33
affluent affluent R 50.00 66.67 66.67 66.67 33.33
Taiwanese Taiwanese F 50.00 66.67 66.67 66.67 33.33
• their • their
• the latter • them
group

MZ17
• her elder • her elder P 0.00 33.33 40.00 33.33 0.00
son and son and R 0.00 50.00 40.00 50.00 0.00
daughter daughter F 0.00 40.00 40.00 40.00 0.00
• them • him

• him
MZ40

• Her • Her P 50.00 66.67 57.14 66.67 33.33
ten-year-old ten-year-old R 33.33 50.00 57.14 50.00 20.00
child child F 40.00 57.14 57.14 57.14 25.00
• him • she
• The child • The child
• him

Table 4: Category 3: Merged entities

the omitted and replacement mentions having very
similar informational content.

As expected, the scores in Category 3 are lower
then those in Category 2 (as a whole), but they are
still consistently better than the scores of the Cate-
gory 2b.

4.1.4 Category 4: Unsuccessful coreference
resolution

The entities in this category (Table 5) are divided
into two subcategories:

No response entity has been given Two of the
key entities (MZ38 and NW4) were not aligned with
any response entities, and not surprisingly all met-
rics agree that the CR precision, recall and F-score
equal zero.

The response entities do not contain a single
“heavy” mention that is correct Although the re-
sponse entities in the remaining entity pairs are non-
empty, an intuitive CR evaluation says there is not
much sense in aligning near-vacuous mentions if
the entity is otherwise wrong or empty. Already
in the two rather simple cases of WB0 and WB1
the metrics show large discrepancies: While link-
based MUC and BLANC correctly give an F-score of
0.00 as there are no correct links in the entity, the
mention-based B3 and CEAF measures award them

4



Key Response MUC B3 CEAFe CEAFm BLANC
entities entities

WB0
• the beauty • the one P 0.00 50.00 50.00 50.00 0.00
industry hand R 0.00 50.00 50.00 50.00 0.00
• it • it F 0.00 50.00 50.00 50.00 0.00

WB1
• the consumer • clinical P 0.00 50.00 50.00 50.00 0.00

dermatologists R 0.00 50.00 50.00 50.00 0.00
• they • they F 0.00 50.00 50.00 50.00 0.00

MZ33
• Chang, P 100.00 100.00 75.00 100.00 100.00
Mei-liang, R 50.00 60.00 75.00 60.00 30.00
chairperson F 66.67 75.00 75.00 75.00 46.15
of the TBAD
Women’s
Division,
• her • her
• she • she
• Her • Her
• she

Table 5: Category 4: Unsuccessful coreference resolution

with a rather high F-score of 50.00.
Entity MZ33 has been awarded high F-scores by

all metrics, averaging 67.56%. However, almost all
information from the key entity in MZ33 has been
lost in the response entity: The key entity contains
information on a person, a female, a Taiwanese na-
tional, her name (Chang Mei-lian) and the ad-
ditional information that she is a chairperson
of the TBAD, Women’s Division. The
response entity contains the information that its
mentions refer to a female, which is most probably
a person, but might be a ship, or a well loved pet.
None of the metrics indicate that such a substantial
loss of information renders the coreference resolu-
tion of MZ33 practically useless for a human user.

5 Entity ranking

As some of the metrics yield consistently lower
F-score levels, it is more appropriate to compare
rankings of entities than the actual F-scores (Table
6). We have also – to infuse an iota of old-school
armchair linguistics – added a sixth rating column,
showing intuitive rankings, based on informational
value retained. The lowest rankings for any metric
are marked in bold.

The entities showing broad agreement among the
metrics are only the best (Category 1) and the worst
ones (MZ38 and NW4, Category 4).

The metrics disagreement surfaces with entities
WB0 and WB1 of Category 4. The link-based met-
rics, MUC and BLANC, rank them last (13th), while
they are ranked much higher (13th out of 19) by
the mention-based and entity-based metrics (B3 and

Entity MUC B3 CEAFe CEAFm BLANC Human
BC45 6 5 5 5 5 7
BC22 8 7 7 7 8 8
BC51 1 1 1 1 1 1
BN0 1 1 1 1 1 1
BN1 5 8 8 8 7 13
BN2 13 18 18 18 13 15
MZ19 10 10 10 10 10 14
MZ33 8 9 9 9 9 17
MZ17 13 17 17 17 13 15
MZ40 12 12 12 12 12 17
MZ22 13 13 13 13 13 10
MZ24 1 1 1 1 1 1
MZ38 – 19 19 19 13 19
NW0 13 19 19 19 13 10
NW1 10 10 10 10 10 8
NW2 7 6 6 6 6 5
NW3 1 1 1 1 1 1
NW4 – 19 19 19 13 19
NW5 13 13 13 13 13 10
WB0 13 13 13 13 13 19
WB1 13 13 13 13 13 19

Table 6: Ranking of our example entities.

CEAF). In this case the human evaluator agrees with
the link-based metrics: If there is not a single cor-
rect link within an entity, our intuition says that no
useful CR has taken place.

However, the presence of a single correct coref-
erent link is not sufficient for our intuition of suc-
cessful resolution. Consider entities MZ22 and
NW5 (Table 3): They also consist of two en-
tities where only one is correct, and have re-
ceived the same ratings as WB0 and WB1, but
in this case we judge CR as much more success-
ful. There are two main differences between this
and the previous case. Firstly, the correct men-
tion is in the previous case a meaning-lean pro-
noun (it and they) while the correct mention
in this case is a ’full-bodied’ NP (the school
in Shenzhen and the delegation from
China’s People’s Congress). In addition,
in both of the Category 2 entity pairs, the incorrect
mention holds an informational value very close or
identical to that of the correct mention. This exam-
ple illustrates the importance of informational value
content of the mentions for the human evaluation of
the resolution.

6 Formalizing the intuition

We have earlier (§4.1) introduced a classificaion
based on an informal notion of (human) intuitive
coreference resolution evaluation. In this section we
will try to formalize the classification.

Category 1 The key entities and response entities
are identical:

5



∀x(x ∈ K ↔ x ∈ R) (1)

Category 2 The response entity is a proper subset
of the key entity:

∀x(x ∈ R→ x ∈ K)∧
∃y(y ∈ K ∧ y /∈ R) (2a)

This is the only condition for Category 2a. Cat-
egory 2b shares the condition (2a), but to formalize
it, we have to add overlap(x,y) relation. We can de-
fine it as a common substring for x and y of a certain
length, possibly including at least one major syntac-
tic category, or even the lexical ’head’ if some way
of operationalizing that notion is available.

∃x(x ∈ K ∧ x ∈ R)∧
∃y∃z(y ∈ K ∧ z ∈ R ∧ overlap(y, z)) (2b)

We need at least two correct mentions in the re-
sponse entity, and at least one that overlaps, as re-
sponse entities containing only one correct mention
do not have any correct links.

Category 3 Response entity contains a subset of
the key entity mentions as well as additional men-
tion(s) belonging to some other entity (E):

∃x(x ∈ K ∧ x ∈ R)∧
∃y(y /∈ K ∧ y ∈ R ∧ y ∈ E) (3)

Category 4 The entities belonging to this category
have a twofold definition: The response entity is ei-
ther empty or if it contains one correct mention, it
cannot contain an overlapping mention.
∀x(x ∈ K → x /∈ R)∨
∃x(x ∈ K ∧ x ∈ R)→
∀y∀z((y ∈ K ∧ z ∈ R)→ ¬overlap(y, z))

(4)

The classification that has been introduced as a in-
formal one in §4.1 is thus computable given an op-
erational definition of overlap. In future work we
will investigate the distribution of the four error cat-
egories on a larger sample.

7 Conclusion and outlook

In this paper we have compared metrics on the ba-
sis of their evaluation of coreference resolution per-
formed on real-life texts, and contrasted their eval-
uation to an intuitive human evaluation of corefer-
ence resolution. We conjecture that humans require

both correct coreferent links and correct (whole or
partial) mentions of a certain information weight to
consider a resolution successful.

This approach has some shortcomings. Firstly,
the manual nature of the analysis has imposed a
limit on the number of the examples, so our data
may not be representative. Secondly, there is un-
certainity connected to how well the coreference
resolution evaluation metrics are suited to be used
in this way. The latter drawback is the more seri-
ous one: the metrics were not designed to evaluate
single key/response pairs, but whole texts. How-
ever, we would argue that if we want to discover
new insights into the evaluation process, some level
of approximation is necessary. There are at least
two arguments in favor of this particular approxi-
mation: Firstly, all metrics are based on evaluating
key/response pairs. Analyzing their performance at
this level can be a reasonable indicator of their per-
formance on the text level. Secondly, even if metrics
are treated “unfairly”, they are all treated equally.

We thus believe that this work can be seen as an
illustration of remaining evaluation challenges in the
field of coreference resolution.

A natural extension of this work would be in-
cluding more humans in evaluating coreference res-
olution systems, to provide a more representative
human judgement. This evaluation should then be
extended from evaluating coreference resolution of
single key/response entity pairs, to assessing the
quality of coreference resolution on a text as a
whole.

And, finally: Every mention carries an in-
formation value, and this weight varies from
quite heavy (as in vice-chief committee
member Jiyun Tian), to somewhat lighter
(Jiyun Tian) to virtually weightless (He). In-
formation weights are not distributed randomly, but
conform to discourse structure. It would be inter-
esting to map the pattern of their distribution, and
see if incorporating this information could improve
both coreference resolution and its quantitative eval-
uation.

8 Acknowledgements

We would like to thank the anonymous reviewers for
their useful comments.

6



References

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Evalu-
ation Workshop on Linguistics Coreference, pages 563
– 566, Granada, Spain.

Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classication. Procesamiento del Lenguaje Natural,
42:87–96.

Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 28–34, Portland,
Oregon.

Xiaoqiang Luo, Abe Ittycheriah Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics, pages 136–143, Barcelona, Spain.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 25–32, Vancouver, Canada.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27, Portland,
Oregon.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the Joint
Conference on EMNLP and CoNLL: Shared Task,
pages 1–40, Jeju Island, Korea.

W. M. Rand. 1971. Objective criteria for evaluation of
clustering methods. Journal of American Statistical
Association, 66(336):846–850.

Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485–510.

Marta Recasens, Lluis Màrquez, Emili Sapena,
M. Antònia Marti, Mariona Taulé, Véronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International

Workshop on Semantic Evaluation, ACL 2010, pages
1–8, Uppsala, Sweden.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message understanding Conference
(MUC-6), pages 45–52, San Francisco, CA.

Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competition
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 176–183, Sapporo, Japan.

7


