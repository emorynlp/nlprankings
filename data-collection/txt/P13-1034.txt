



















































Scaling Semi-supervised Naive Bayes with Feature Marginals


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 343–351,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Scaling Semi-supervised Naive Bayes with Feature Marginals

Michael R. Lucas and Doug Downey
Northwestern University

2133 Sheridan Road
Evanston, IL 60208

mlucas@u.northwestern.edu
ddowney@eecs.northwestern.edu

Abstract

Semi-supervised learning (SSL) methods
augment standard machine learning (ML)
techniques to leverage unlabeled data.
SSL techniques are often effective in text
classification, where labeled data is scarce
but large unlabeled corpora are readily
available. However, existing SSL tech-
niques typically require multiple passes
over the entirety of the unlabeled data,
meaning the techniques are not applicable
to large corpora being produced today.

In this paper, we show that improving
marginal word frequency estimates using
unlabeled data can enable semi-supervised
text classification that scales to massive
unlabeled data sets. We present a novel
learning algorithm, which optimizes a
Naive Bayes model to accord with statis-
tics calculated from the unlabeled corpus.
In experiments with text topic classifica-
tion and sentiment analysis, we show that
our method is both more scalable and more
accurate than SSL techniques from previ-
ous work.

1 Introduction

Semi-supervised Learning (SSL) is a Machine
Learning (ML) approach that utilizes large
amounts of unlabeled data, combined with a
smaller amount of labeled data, to learn a tar-
get function (Zhu, 2006; Chapelle et al., 2006).
SSL is motivated by a simple reality: the amount
of available machine-readable data is exploding,
while human capacity for hand-labeling data for
any given ML task remains relatively constant.
Experiments in text classification and other do-
mains have demonstrated that by leveraging un-
labeled data, SSL techniques improve machine
learning performance when human input is limited

(e.g., (Nigam et al., 2000; Mann and McCallum,
2010)).

However, current SSL techniques have scal-
ability limitations. Typically, for each target
concept to be learned, a semi-supervised classi-
fier is trained using iterative techniques that exe-
cute multiple passes over the unlabeled data (e.g.,
Expectation-Maximization (Nigam et al., 2000) or
Label Propagation (Zhu and Ghahramani, 2002)).
This is problematic for text classification over
large unlabeled corpora like the Web: new tar-
get concepts (new tasks and new topics of interest)
arise frequently, and performing even a single pass
over a large corpus for each new target concept is
intractable.

In this paper, we present a new SSL text classi-
fication approach that scales to large corpora. In-
stead of utilizing unlabeled examples directly for
each given target concept, our approach is to pre-
compute a small set of statistics over the unlabeled
data in advance. Then, for a given target class and
labeled data set, we utilize the statistics to improve
a classifier.

Specifically, we introduce a method that ex-
tends Multinomial Naive Bayes (MNB) to lever-
age marginal probability statistics P (w) of each
word w, computed over the unlabeled data. The
marginal statistics are used as a constraint to im-
prove the class-conditional probability estimates
P (w|+) and P (w|−) for the positive and negative
classes, which are often noisy when estimated over
sparse labeled data sets. We refer to the technique
as MNB with Frequency Marginals (MNB-FM).

In experiments with large unlabeled data sets
and sparse labeled data, we find that MNB-
FM is both faster and more accurate on aver-
age than standard SSL methods from previous
work, including Label Propagation, MNB with
Expectation-Maximization,, and the recent Semi-
supervised Frequency Estimate (SFE) algorithm
(Su et al., 2011). We also analyze how MNB-

343



FM improves accuracy, and find that surprisingly
MNB-FM is especially useful for improving class-
conditional probability estimates for words that
never occur in the training set.

The paper proceeds as follows. We formally de-
fine the task in Section 2. Our algorithm is defined
in Section 3. We present experimental results in
Section 4, and analysis in Section 5. We discuss
related work in Section 6 and conclude in Section
7 with a discussion of future work.

2 Problem Definition

We consider a semi-supervised classification task,
in which the goal is to produce a mapping
from an instance space X consisting of T -tuples
of non-negative integer-valued features w =
(w1, . . . , wT ), to a binary output space Y =
{−,+}. In particular, our experiments will fo-
cus on the case in which the wi’s represent word
counts in a given document, in a corpus of vocab-
ulary size T .

We assume the following inputs:

• A set of zero or more labeled documents
DL = {(wd, yd)|d = 1, . . . , n}, drawn i.i.d.
from a distribution P (w, y) for w ∈ X and
y ∈ Y .

• A large set of unlabeled documents DU =
{(wd)|d = n+1, . . . , n+u} drawn from the
marginal distribution P (w) =

∑

y

P (w, y).

The goal of the task is to output a classifer
f : X → Y that performs well in predicting the
classes of given unlabeled documents. The met-
rics of evaluation we focus on in our experiments
are detailed in Section 4.

Our semi-supervised technique utilizes statis-
tics computed over the labeled corpus, denoted as
follows. We use N+w to denote the sum of the
occurrences of word w over all documents in the
positive class in the labeled data DL. Also, let
N+ =

∑n
w∈DL N

+
w be the sum value of all word

counts in the labeled positive documents. The
count of the remaining words in the positive doc-
uments is represented as N+¬w = N

+ −N+w . The
quantitiesN−,N−w , andN

−
¬w are defined similarly

for the negative class.

3 MNB with Feature Marginals

We now introduce our algorithm, which scalably
utilizes large unlabeled data stores for classifica-

tion tasks. The technique builds upon the multino-
mial Naive Bayes model, and is denoted as MNB
with Feature Marginals (MNB-FM).

3.1 MNB-FM Method
In the text classification setting , each feature value
wd represents count of observations of word w in
document d. MNB makes the simplifying assump-
tion that word occurrences are conditionally inde-
pendent of each other given the class (+ or −) of
the example. Formally, let the probability P (w|+)
of the w in the positive class be denoted as θ+w . Let
P (+) denote the prior probability that a document
is of the positive class, and P (−) = 1−P (+) the
prior for the negative class. Then MNB represents
the class probability of an example as:

P (+|d) =

∏

w∈d
(θ+w )

wdP (+)

∏

w∈d
(θ−w )

wdP (−) +
∏

w∈d
(θ+w )

wdP (+)

(1)
MNB estimates the parameters θ+w from the

corresponding counts in the training set. The
maximum-likelihood estimate of θ+w is N

+
w /N

+,
and to prevent zero-probability estimates we em-
ploy “add-1” smoothing (typical in MNB) to ob-
tain the estimate:

θ+w =
N+w + 1

N+ + |T | .

After MNB calculates θ+w and θ
−
w from the train-

ing set for each feature in the feature space, it can
then classify test examples using Equation 1.

MNB-FM attempts to improve MNB’s esti-
mates of θ+w and θ

−
w , using statistics computed over

the unlabeled data. Formally, MNB-FM leverages
the equality:

P (w) = θ+wPt(+) + θ
−
wPt(−) (2)

The left-hand-side of Equation 2, P (w), repre-
sents the probability that a given randomly drawn
token from the unlabeled data happens to be the
word w. We write Pt(+) to denote the probabil-
ity that a randomly drawn token (i.e. a word oc-
currence) from the corpus comes from the posi-
tive class. Note that Pt(+) can differ from P (+),
the prior probability that a document is positive,
due to variations in document length. Pt(−) is de-
fined similarly for the negative class. MNB-FM is
motivated by the insight that the left-hand-side of

344



Equation 2 can be estimated in advance, without
knowledge of the target class, simply by counting
the number of tokens of each word in the unla-
beled data.

MNB-FM then uses this improved estimate of
P (w) as a constraint to improve the MNB param-
eters on the right-hand-side of Equation 2. We
note that Pt(+) and Pt(−), even for a small train-
ing set, can typically be estimated reliably— ev-
ery token in the training data serves as an obser-
vation of these quantities. However, for large and
sparse feature spaces common in settings like text
classification, many features occur in only a small
fraction of examples—meaning θ+w and θ

−
w must

be estimated from only a handful of observations.
MNB-FM attempts to improve the noisy estimates
θ+w and θ

−
w utilizing the robust estimate for P (w)

computed over unlabeled data.
Specifically, MNB-FM proceeds by assuming

the MLEs for P (w) (computed over unlabeled
data), Pt(+), and Pt(−) are correct, and re-
estimates θ+w and θ

−
w under the constraint in Equa-

tion 2.
First, the maximum likelihood estimates of θ+w

and θ−w given the training data DL are:

argmax
θ+w ,θ

−
w

P (DL|θ+w , θ−w )

= argmax
θ+w ,θ

−
w

θ+(N
+
w )

w (1− θ+w )(N
+
¬w)

θ−(N
−
w )

w (1− θ−w )(N
−
¬w)

= argmax
θ+w ,θ

−
w

N+w ln(θ
+
w ) +N

+
¬w ln(1− θ+w )+

N−w ln(θ
−
w ) +N

−
¬w ln(1− θ−w )

(3)

We can rewrite the constraint in Equation 2 as:

θ−w = K − θ+wL
where for compactness we represent:

K =
P (w)

Pt(−)
;L =

Pt(+)

Pt(−)
.

Substituting the constraint into Equation 3
shows that we wish to choose θ+w as:

argmax
θ+w

N+w ln(θ
+
w ) +N

+
¬w ln(1− θ+w )+

N−w ln(K − Lθ+w ) +N−¬w ln(1−K + Lθ+w )
The optimal values for θ+w are thus located at the

solutions of:

0 =
N+w
θ+w

+
N+¬w
θ+w − 1

+
LN−w

Lθ+w −K
+

LN−¬w
Lθ+w −K + 1

Both θ+w and θ
−
w are constrained to valid prob-

abilities in [0,1] when θ+w ∈ [0, KL ]. If N+¬w and
N−w have non-zero counts, vertical asymptotes ex-
ist at 0 and KL and guarantee a solution in this
range. Otherwise, a valid solution may not ex-
ist. In that case, we default to the add-1 Smooth-
ing estimates used by MNB. Finally, after optimiz-
ing the values θ+w and θ

−
w for each word w as de-

scribed above, we normalize the estimates to ob-
tain valid conditional probability distributions, i.e.
with

∑
w θ

+
w =

∑
w θ
−
w = 1

3.2 MNB-FM Example

The following concrete example illustrates how
MNB-FM can improve MNB parameters using the
statistic P (w) computed over unlabeled data. The
example comes from the Reuters Aptemod text
classification task addressed in Section 4, using
bag-of-words features for the Earnings class. In
one experiment with 10 labeled training examples,
we observed 5 positive and 5 negative examples,
with the word “resources” occurring three times
in the set (once in the positive class, twice in the
negative class).

MNB uses add-1 smoothing to estimate the con-
ditional probability of the word “resources” in
each class as θ+w =

1+1
216+33504 = 5.93e-5, and

θ−w =
2+1

547+33504 = 8.81e-5. Thus,
θ+w
θ−w

= 0.673

implying that “resources” is a negative indicator
of the Earnings class. However, this estimate is
inaccurate. In fact, over the full dataset, the pa-
rameter values we observe are θ+w =

93
168549 =

5.70e-4 and θ−w =
263

564717 = 4.65e-4, with a ratio

of θ
+
w

θ−w
= 1.223. Thus, in actuality, the word “re-

sources” is a mild positive indicator of the Earn-
ings class. Yet because MNB estimates its param-
eters from only the sparse training data, it can be
inaccurate.

The optimization in MNB-FM seeks to accord
its parameter estimates with the feature frequency,
computed from unlabeled data, of P (w) = 4.89e-
4. We see that compared with P (w), the θ+w and
θ−w that MNB estimates from the training data are
both too low by almost an order of magnitude.
Further, the maximum likelihood estimate for θ−w
(based on an occurrence count of 2 out of 547 ob-
servations) is somewhat more reliable than that for
θ+w (1 of 216 observations). As a result, θ

+
w is ad-

justed upward relatively more than θ−w via MNB-
FM’s constrained ML estimation. MNB-FM re-
turns θ+w = 6.52e-5 and θ

−
w = 6.04e-5. The ratio345



θ+w
θ−w

is 1.079, meaning MNB-FM correctly identi-
fies the word “resources” as an indicator of the
positive class.

The above example illustrates how MNB-FM
can leverage frequency marginal statistics com-
puted over unlabeled data to improve MNB’s
conditional probability estimates. We analyze
how frequently MNB-FM succeeds in improving
MNB’s estimates in practice, and the resulting im-
pact on classification accuracy, below.

4 Experiments

In this section, we describe our experiments quan-
tifying the accuracy and scalability of our pro-
posed technique. Across multiple domains, we
find that MNB-FM outperforms a variety of ap-
proaches from previous work.

4.1 Data Sets
We evaluate on two text classification tasks: topic
classification, and sentiment detection. In topic
classification, the task is to determine whether a
test document belongs to a specified topic. We
train a classifier separately (i.e., in a binary clas-
sification setting) for each topic and measure clas-
sification performance for each class individually.

The sentiment detection task is to determine
whether a document is written with a positive or
negative sentiment. In our case, the goal is to de-
termine if the given text belongs to a positive re-
view of a product.

4.1.1 RCV1
The Reuters RCV1 corpus is a standard large cor-
pus used for topic classification evaluations (Lewis
et al., 2004). It includes 804,414 documents with
several nested target classes. We consider the 5
largest base classes after punctuation and stop-
words were removed. The vocabulary consisted
of 288,062 unique words, and the total number of
tokens in the data set was 99,702,278. Details of
the classes can be found in Table 1.

4.1.2 Reuters Aptemod
While MNB-FM is designed to improve the scala-
bility of SSL to large corpora, some of the com-
parison methods from previous work were not
tractable on the large topic classification data set
RCV1. To evaluate these methods, we also exper-
imented with the Reuters ApteMod dataset (Yang
and Liu, 1999), consisting of 10,788 documents
belonging to 90 classes. We consider the 10 most

Class # Positive
CCAT 381327 (47.40%)
GCAT 239267 (29.74%)
MCAT 204820 (25.46%)
ECAT 119920 (14.91%)
GPOL 56878 (7.07%)

Table 1: RCV1 dataset details

Class # Positive
Earnings 3964 (36.7%)
Acquisitions 2369 (22.0%)
Foreign 717 (6.6%)
Grain 582 (5.4%)
Crude 578 (5.4%)
Trade 485 (4.5%)
Interest 478 (4.4%)
Shipping 286 (2.7%)
Wheat 283 (2.6%)
Corn 237 (2.2%)

Table 2: Aptemod dataset details

frequent classes, with varying degrees of posi-
tive/negative skew. Punctuation and stopwords
were removed during preprocessing. The Apte-
mod data set contained 33,504 unique words and
a total of 733,266 word tokens. Details of the
classes can be found in Table 2.

4.1.3 Sentiment Classification Data

In the domain of Sentiment Classification, we
tested on the Amazon dataset from (Blitzer et al.,
2007). Stopwords listed in an included file were
ignored for our experiments and we only the con-
sidered unigram features. Unlike the two Reuters
data sets, each category had a unique set of doc-
uments of varying size. For our experiments, we
only used the 10 largest categories. Details of the
categories can be found in Table 3.

In the Amazon Sentiment Classification data
set, the task is to determine whether a review is
positive or negative based solely on the reviewer’s
submitted text. As such, the positive and negative

Class # Instances # Positive Vocabulary
Music 124362 113997 (91.67%) 419936
Books 54337 47767 (87.91%) 220275
Dvd 46088 39563 (85.84%) 217744
Electronics 20393 15918 (78.06%) 65535
Kitchen 18466 14595 (79.04%) 47180
Video 17389 15017 (86.36%) 106467
Toys 12636 10151 (80.33%) 37939
Apparel 8940 7642 (85.48%) 22326
Health 6507 5124 (78.75%) 24380
Sports 5358 4352 (81.22%) 24237

Table 3: Amazon dataset details
346



labels are equally relevant. For our metrics, we
calculate the scores for both the positive and neg-
ative class and report the average of the two (in
contrast to the Reuters data sets, in which we only
report the scores for the positive class).

4.2 Comparison Methods

In addition to Multinomial Naive Bayes (discussed
in Section 3), we evaluate against a variety of
supervised and semi-supervised techniques from
previous work, which provide a representation of
the state of the art. Below, we detail the compar-
ison methods that we re-implemented for our ex-
periments.

4.2.1 NB + EM
We implemented a semi-supervised version of
Naive Bayes with Expectation Maximization,
based on (Nigam et al., 2000). We found that 15
iterations of EM was sufficient to ensure approxi-
mate convergence of the parameters.

We also experimented with different weighting
factors to assign to the unlabeled data. While per-
forming per-data-split cross-validation was com-
putationally prohibitive for NB+EM, we per-
formed experiments on one class from each data
set that revealed weighting unlabeled examples at
1/5 the weight of a labeled example performed
best. We found that our re-implementation of
NB+EM slightly outperformed published results
on a separate data set (Mann and McCallum,
2010), validating our design choices.

4.2.2 Logistic Regression
We implemented Logistic Regression using L2-
Normalization, finding this to outperform L1-
Normalized and non-normalized versions. The
strength of the normalization was selected for each
training data set of each size utilized in our exper-
iments.

The strength of the normalization in the logis-
tic regression required cross-validation, which we
limited to 20 values logarithmically spaced be-
tween 10−4 and 104. The optimal value was se-
lected based upon the best average F1 score over
the 10 folds. We selected a normalization param-
eter separately for each subset of the training data
during experimentation.

4.2.3 Label Propagation
For our large unlabeled data set sizes, we found
that a standard Label Propogation (LP) approach,

which considers propagating information between
all pairs of unlabeled examples, was not tractable.
We instead implemented a constrained version of
LP for comparison.

In our implementation, we limit the number of
edges in the propagation graph. Each node prop-
agates to only to its 10 nearest neighbors, where
distance is calculated as the cosine distance be-
tween the tf-idf representation of two documents.
We found the tf-idf weighting to improve perfor-
mance over that of simple cosine distance. Propa-
gation was run for 100 iterations or until the en-
tropy dropped below a predetermined threshold,
whichever occurred first. Even with these aggres-
sive constraints, Label Propagation was intractable
to execute on some of the larger data sets, so we
do not report LP results for the RCV1 dataset or
for the 5 largest Amazon categories.

4.2.4 SFE
We also re-implemented a version of the recent
Semi-supervised Frequency Estimate approach
(Su et al., 2011). SFE was found to outperform
MNB and NB+EM in previous work. Consis-
tent with our MNB implementation, we use Add-
1 Smoothing in our SFE calculations although its
use is not specifically mentioned in (Su et al.,
2011).

SFE also augments multinomial Naive Bayes
with the frequency information P (w), although in
a manner distinct from MNB-FM. In particular,
SFE uses the equality P (+|w) = P (+, w)/P (w)
and estimates the rhs using P (w) computed over
all the unlabeled data, rather than using only la-
beled data as in standard MNB. The primary dis-
tinction between MNB-FM and SFE is that SFE
adjusts sparse estimates P (+, w) in the same way
as non-sparse estimates, whereas MNB-FM is de-
signed to adjust sparse estimates more than non-
sparse ones. Further, it can be shown that as P (w)
of a word w in the unlabeled data becomes larger
than that in the labeled data, SFE’s estimate of the
ratio P (w|+)/P (w|−) approaches one. Depend-
ing on the labeled data, such an estimate can be ar-
bitrarily inaccurate. MNB-FM does not have this
limitation.

4.3 Results

For each data set, we evaluate on 50 randomly
drawn training splits, each comprised of 1,000 ran-
domly selected documents. Each set included at
least one positive and one negative document. We

347



Data Set MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.306 0.271 0.336 0.306 0.245 0.208
Apte (100) 0.554 0.389 0.222 0.203 0.263 0.330
Apte (1k) 0.729 0.614 0.452 0.321 0.267 0.702
Amzn (10) 0.542 0.524 0.508 0.475 0.470* 0.499
Amzn (100) 0.587 0.559 0.456 0.456 0.498* 0.542
Amzn (1k) 0.687 0.611 0.465 0.455 0.539* 0.713
RCV1 (10) 0.494 0.477 0.387 0.485 - 0.272
RCV1 (100) 0.677 0.613 0.337 0.470 - 0.518
RCV1 (1k) 0.772 0.735 0.408 0.491 - 0.774

* Limited to 5 of 10 Amazon categories

Table 4: F1, training size in parentheses

respected the order of the training splits such that
each sample was a strict subset of any larger train-
ing sample of the same split.

We evaluate on the standard metric of F1 with
respect to the target class. For Amazon, in which
both the “positive” and “negative” classes are po-
tential target classes, we evaluate using macro-
averaged scores.

The primary results of our experiments are
shown in Table 4. The results show that MNB-FM
improves upon the MNB classifier substantially,
and also tends to outperform the other SSL and
supervised learning methods we evaluated. MNB-
FM is the best performing method over all data
sets when the labeled data is limited to 10 and 100
documents, except for training sets of size 10 in
Aptemod, where MNB has a slight edge.

Tables 5 and 6 present detailed results of the
experiments on the RCV1 data set. These exper-
iments are limited to the 5 largest base classes
and show the F1 performance of MNB-FM and
the various comparison methods, excluding Label
Propagation which was intractable on this data set.

Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.641 0.643 0.580 0.639 0.532
GCAT 0.639 0.686 0.531 0.732 0.466
MCAT 0.572 0.505 0.393 0.504 0.225
ECAT 0.306 0.267 0.198 0.224 0.096
GPOL 0.313 0.283 0.233 0.326 0.043
Average 0.494 0.477 0.387 0.485 0.272

Table 5: RCV1: F1, |DL|= 10

Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.797 0.793 0.624 0.713 0.754
GCAT 0.849 0.848 0.731 0.837 0.831
MCAT 0.776 0.737 0.313 0.516 0.689
ECAT 0.463 0.317 0.017 0.193 0.203
GPOL 0.499 0.370 0.002 0.089 0.114
Average 0.677 0.613 0.337 0.470 0.518

Table 6: RCV1: F1, |DL|= 100

Method 1000 5000 10k 50k 100k
MNB-FM 1.44 1.61 1.69 2.47 5.50
NB+EM 2.95 3.43 4.93 10.07 16.90
MNB 1.15 1.260 1.40 2.20 3.61
Labelprop 0.26 4.17 10.62 67.58 -

Table 7: Runtimes of SSL methods (sec.)

The runtimes of our methods can be seen in Ta-
ble 7. The results show the runtimes of the SSL
methods discussed in this paper as the size of the
unlabeled dataset grows. As expected, we find that
MNB-FM has runtime similar to MNB, and scales
much better than methods that take multiple passes
over the unlabeled data.

5 Analysis

From our experiments, it is clear that the perfor-
mance of MNB-FM improves on MNB, and in
many cases outperforms all existing SSL algo-
rithms we evaluated. MNB-FM improves the con-
ditional probability estimates in MNB and, sur-
prisingly, we found that it can often improve these
estimates for words that do not even occur in the
training set.

Tables 8 and 9 show the details of the improve-
ments MNB-FM makes on the feature marginal
estimates. We ran MNB-FM and MNB on the
RCV1 class MCAT and stored the computed fea-
ture marginals for direct comparison. For each
word in the vocabulary, we compared each clas-
sifier’s conditional probability ratios, i.e. θ+/θ−,
to the true value over the entire data set. We com-
puted which classifier was closer to the correct ra-
tio for each word. These results were averaged
over 5 iterations. From the data, we can see that
MNB-FM improves the estimates for many words
not seen in the training set as well as the most com-
mon words, even with small training sets.

5.1 Ranking Performance

We also analyzed how well the different meth-
ods rank, rather than classify, the test documents.
We evaluated ranking using the R-precision met-
ric, equal to the precision (i.e. fraction of positive
documents classified correctly) of the R highest-
ranked test documents, where R is the total num-
ber of positive test documents.

Logistic Regression performed particularly well
on the R-Precision Metric, as can be seen in Tables
10, 11, and 12. Logistic Regression performed
less well in the F1 metric. We find that NB+EM

348



Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10−6 - 0.165 0.847 - -0.805 0.349 - 0.02% 7.69%
10−6-10−5 0.200 0.303 0.674 0.229 -0.539 0.131 0.00% 0.54% 14.77%
10−5-10−4 0.322 0.348 0.592 -0.597 -0.424 0.025 0.74% 10.57% 32.42%
10−4-10−3 0.533 0.564 0.433 0.014 0.083 -0.155 7.94% 17.93% 7.39%
> 10−3 - - - - - - - - -

Table 8: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 10). “Known”
indicates words occurring in both positive and negative training examples, “Half Known” indicates words
occurring in only positive or negative training examples, while “Unknown” indicates words that never
occur in labelled examples. Data is for the RCV1 MCAT category. MNB-FM improves estimates by a
substantial amount for unknown words and also the most common known and half-known words.

Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10−6 0.567 0.243 0.853 0.085 -0.347 0.143 0.00% 0.22% 7.49%
10−6-10−5 0.375 0.310 0.719 -0.213 -0.260 0.087 0.38% 4.43% 10.50%
10−5-10−4 0.493 0.426 0.672 -0.071 -0.139 0.067 18.68% 20.37% 4.67%
10−4-10−3 0.728 0.669 - 0.233 0.018 - 31.70% 1.56% -
> 10−3 - - - - - - - - -

Table 9: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 100). Data is
for the RCV1 MCAT category (see Table 8). MNB-FM improves estimates by a substantial amount for
unknown words and also the most common known and half-known words.

performs particularly well on the R-precision met-
ric on ApteMod, suggesting that its modelling as-
sumptions are more accurate for that particular
data set (NB+EM performs significantly worse on
the other data sets, however). MNB-FM performs
essentially equivalently well, on average, to the
best competing method (Logistic Regression) on
the large RCV1 data set. However, these experi-
ments show that MNB-FM offers more advantages
in document classification than in document rank-
ing.

The ranking results show that LR may be pre-
ferred when ranking is important. However, LR
underperforms in classification tasks (in terms of
F1, Tables 4-6). The reason for this is that LR’s
learned classification threshold becomes less accu-
rate when datasets are small and classes are highly

Class MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.353 0.304 0.359 0.631 0.490 0.416
Apte (100) 0.555 0.421 0.343 0.881 0.630 0.609
Apte (1k) 0.723 0.652 0.532 0.829 0.754 0.795
Amzn (10) 0.536 0.527 0.516 0.481 0.535* 0.544
Amzn (100) 0.614 0.562 0.517 0.480 0.573* 0.639
Amzn (1k) 0.717 0.650 0.562 0.483 0.639* 0.757
RCV1 (10) 0.505 0.480 0.421 0.450 - 0.512
RCV1 (100) 0.683 0.614 0.474 0.422 - 0.689
RCV1 (1k) 0.781 0.748 0.535 0.454 - 0.802

* Limited to 5 of 10 Amazon categories

Table 10: R-Precision, training size in parentheses

skewed. In these cases, LR classifies too fre-
quently in favor of the larger class which is detri-
mental to its performance. This effect is visible
in Tables 5 and 6, where LR’s performance sig-
nificantly drops for the ECAT and GPOL classes.
ECAT and GPOL represent only 14.91% and
7.07% of the RCV1 dataset, respectively.

6 Related Work

To our knowledge, MNB-FM is the first approach
that utilizes a small set of statistics computed over

Data SetMNB-FM SFE MNB NBEM Logist.
CCAT 0.637 0.631 0.620 0.498 0.653
GCAT 0.663 0.711 0.600 0.792 0.671
MCAT 0.580 0.492 0.477 0.510 0.596
ECAT 0.291 0.217 0.214 0.111 0.297
GPOL 0.354 0.352 0.193 0.341 0.341
Average 0.505 0.480 0.421 0.450 0.512

Table 11: RCV1: R-Precision, DL= 10

Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.805 0.797 0.765 0.533 0.809
GCAT 0.849 0.858 0.780 0.869 0.843
MCAT 0.782 0.753 0.579 0.533 0.774
ECAT 0.471 0.293 0.203 0.119 0.498
GPOL 0.509 0.370 0.042 0.056 0.520
Average 0.683 0.614 0.474 0.422 0.689

Table 12: RCV1: R-Precision, DL= 100
349



a large unlabeled data set as constraints to im-
prove a semi-supervised classifier. Our exper-
iments demonstrate that MNB-FM outperforms
previous approaches across multiple text classi-
fication techniques including topic classification
and sentiment analysis. Further, the MNB-FM ap-
proach offers scalability advantages over most ex-
isting semi-supervised approaches.

Current popular Semi-Supervised Learning ap-
proaches include using Expectation-Maximization
on probabilistic models (e.g. (Nigam et al.,
2000)); Transductive Support Vector Machines
(Joachims, 1999); and graph-based methods such
as Label Propagation (LP) (Zhu and Ghahramani,
2002) and their more recent, more scalable vari-
ants (e.g. identifying a small number of represen-
tative unlabeled examples (Liu et al., 2010)). In
general, these techniques require passes over the
entirety of the unlabeled data for each new learn-
ing task, intractable for massive unlabeled data
sets. Naive implementations of LP cannot scale
to large unlabeled data sets, as they have time
complexity that increases quadratically with the
number of unlabeled examples. Recent LP tech-
niques have achieved greater scalability through
the use of parallel processing and heuristics such
as Approximate-Nearest Neighbor (Subramanya
and Bilmes, 2009), or by decomposing the sim-
ilarity matrix (Lin and Cohen, 2011). Our ap-
proach, by contrast, is to pre-compute a small
set of marginal statistics over the unlabeled data,
which eliminates the need to scan unlabeled data
for each new task. Instead, the complexity of
MNB-FM is proportional only to the number of
unique words in the labeled data set.

In recent work, Su et al. propose the Semi-
supervised Frequency Estimate (SFE), which like
MNB-FM utilizes the marginal probabilities of
features computed from unlabeled data to im-
prove the Multinomial Naive Bayes (MNB) clas-
sifier (Su et al., 2011). SFE has the same scal-
ability advantages as MNB-FM. However, unlike
our approach, SFE does not compute maximum-
likelihood estimates using the marginal statistics
as a constraint. Our experiments show that MNB-
FM substantially outperforms SFE.

A distinct method for pre-processing unlabeled
data in order to help scale semi-supervised learn-
ing techniques involves dimensionality reduction
or manifold learning (Belkin and Niyogi, 2004),
and for NLP tasks, identifying word representa-

tions from unlabeled data (Turian et al., 2010). In
contrast to these approaches, MNB-FM preserves
the original feature set and is more scalable (the
marginal statistics can be computed in a single
pass over the unlabeled data set).

7 Conclusion

We presented a novel algorithm for efficiently
leveraging large unlabeled data sets for semi-
supervised learning. Our MNB-FM technique op-
timizes a Multinomial Naive Bayes model to ac-
cord with statistics of the unlabeled corpus. In ex-
periments across topic classification and sentiment
analysis, MNB-FM was found to be more accu-
rate and more scalable than several supervised and
semi-supervised baselines from previous work.

In future work, we plan to explore utilizing
richer statistics from the unlabeled data, beyond
word marginals. Further, we plan to experiment
with techniques for unlabeled data sets that also
include continuous-valued features. Lastly, we
also wish to explore ensemble approaches that
combine the best supervised classifiers with the
improved class-conditional estimates provided by
MNB-FM.

8 Acknowledgements

This work was supported in part by DARPA con-
tract D11AP00268.

References
Mikhail Belkin and Partha Niyogi. 2004. Semi-

supervised learning on riemannian manifolds. Ma-
chine Learning, 56(1):209–239.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.

O. Chapelle, B. Schölkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.

Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, ICML ’99, pages 200–
209, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361–397.

350



Frank Lin and William W Cohen. 2011. Adaptation
of graph-based semi-supervised methods to large-
scale text data. In The 9th Workshop on Mining and
Learning with Graphs.

Wei Liu, Junfeng He, and Shih-Fu Chang. 2010. Large
graph construction for scalable semi-supervised
learning. In ICML, pages 679–686.

Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. J. Mach. Learn.
Res., 11:955–984, March.

Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classifica-
tion from labeled and unlabeled documents using
em. Mach. Learn., 39(2-3):103–134, May.

Jiang Su, Jelber Sayyad Shirab, and Stan Matwin.
2011. Large scale text classification using semisu-
pervised multinomial naive bayes. In Lise Getoor
and Tobias Scheffer, editors, ICML, pages 97–104.
Omnipress.

Amar Subramanya and Jeff A. Bilmes. 2009. En-
tropic graph regularization in non-parametric semi-
supervised classification. In Neural Information
Processing Society (NIPS), Vancouver, Canada, De-
cember.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Urbana, 51:61801.

Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 42–49. ACM.

X. Zhu and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical report, Technical Report CMU-CALD-
02-107, Carnegie Mellon University.

Xiaojin Zhu. 2006. Semi-supervised learning litera-
ture survey.

351


