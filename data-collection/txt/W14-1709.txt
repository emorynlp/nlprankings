



















































POSTECH Grammatical Error Correction System in the CoNLL-2014 Shared Task


Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 65â€“73,
Baltimore, Maryland, 26-27 July 2014. cÂ©2014 Association for Computational Linguistics

POSTECH Grammatical Error Correction System in the CoNLL-

2014 Shared Task 

 

 

Kyusong Lee, Gary Geunbae Lee 

Department of Computer Science and Engineering 

Pohang University of Science and Technology 

Pohang, Korea 

{Kyusonglee,gblee}@postech.ac.kr 

 

  

 

Abstract 

This paper describes the POSTECH gram-

matical error correction system. Various 

methods are proposed to correct errors 

such as rule-based, probability n-gram 

vector approaches and router-based ap-

proach. Google N-gram count corpus is 

used mainly as the correction resource. 

Correction candidates are extracted from 

NUCLE training data and each candidate 

is evaluated with development data to ex-

tract high precision rules and n-gram 

frames. Out of 13 participating teams, our 

system is ranked 4th on both the original 

and revised annotation.    

1 Introduction 

Automatic grammar error correction (GEC) is 

widely used by learners of English as a second 

language (ESL) in written tasks. Many methods 

have been proposed to correct grammatical errors; 

these include methods based on rules (Naber, 

2003), on statistical machine translation (Brockett 

et al., 2006), on machine learning, and on n-grams 

(Alam et al., 2006). Early research (Han et al., 

2006; De Felice, 2008; Knight & Chander, 1994; 

Nagata et al., 2006) on error correction for non-

native text was based on well-formed corpora.  

Most recent work (Cahill et al., 2013; 

Rozovskaya & Roth, 2011; Wu & Ng, 2013) has 

used machine learning methods that rely on a GE- 

tagged corpus such as NUCLE, Japanese English 

Learner corpus, and Cambridge Learner Corpus 

(Dahlmeier et al., 2013; Izumi et al., 2005; 

Nicholls, 2003), because well-formed and GE-

tagged approaches are closely related to each 

other, can be synergistically combined. Therefore, 

research using both types of data has also been 

conducted (Dahlmeier & Ng, 2011). Moreover, a 

meta-classification method using several GE-

tagged corpora and a native corpus has been pro-

posed to correct the grammatical errors (Seo et al., 

2012). A meta-classifier approach has been pro-

posed to combine a language model and error-spe-

cific classification for correction of article and 

preposition errors (Gamon, 2010). Web-scale 

well-formed corpora have been successfully ap-

plied to grammar error correction tasks instead of 

using error-tagged data (Bergsma et al., 2009; 

Gamon et al., 2009; Hermet et al., 2008). Espe-

cially in the CoNLL-2013 grammar error correc-

tion shared task, many of the high-ranked teams 

(Kao et al., 2013; Mark & Roth, 2013; Xing et al., 

2013) exploited the Google Web-1T n-gram cor-

pus. The major advantage of using these web-

scale corpora is that extremely large quantities of 

data are publicly available at no additional costs; 

thus fewer data sparseness problems arise com-

pared to previous approaches based on error-

tagged corpora. 

We also use the Google Web-1T n-gram corpus. 

We extract the candidate pairs (original erroneous 

text and its correction) from NUCLE training data. 

We use a router to choose the best frame to com-

pare the n-gram score difference between the orig-

inal and replacement in a given candidate pair.  

The intuition of our grammar error correction 

method is the following: First, if the uni-gram 

count is less than some threshold, we assume that 

the word is erroneous. Second, if the replacement 

word n-gram has more frequent than the original 

word n-gram, it presents strong evidence for cor-

rection. Third, depending on the candidate pair, 

tailored n-gram frames help to correct errors ac-

curately. Fourth, only high precision method and 

rules are applied. If correction precision on a can-

didate pair is less than 30% in development data, 

65



we do not make a correction for the candidate pair 

at runtime. 

In the CoNLL-Shared Task, objectives were 

presented yearly. In 2012, the objective was to 

correct article and preposition errors; in 2013, it 

was to correct article, preposition, noun number, 

verb form, and subject-verb agreement errors. 

This year, the objective is to correct all errors. 

Thus, our method should also correct prepro-

cessing and spelling errors.  Detailed description 

of the shared task set up, data set, and evaluation 

about the CoNLL-2014 Shared Task is explained 

in (Ng et al., 2014) 

2 Data and Recourse  

The Google Web-1T corpus contains 1012 words 

of running text and the counts for all 109 five-word 

sequences that appear > 40 times (Brants & Franz, 

2006). We used the NUS Corpus of Learner Eng-

lish (NUCLE) training data to extract the candi-

date pairs and CoNLL-2013 Official Shard Task 

test data as development data.  We used the Stan-

ford parser (De Marneffe & Manning, 2008) to 

extract part-of-speech, dependency, and constitu-

ency trees.  

3 Method 

3.1 Overall Process 

We correct the errors in the following order: 

Tokenizing â†’ spelling error correction â†’ punc-
tuation error correction â†’ N-gram Vector Ap-

proach for Noun number (Nn) â†’ Router-based 

                                                 
1 http://abisource.com/projects/enchant/ 

Correction (Deletion Correction â†’ Insertion Cor-

rection â†’ Replacement) for various error types â†’ 

Rule-based method for verb errors. Between each 

pair of step, we parse, tag, and tokenize again us-

ing the Stanford parser because the previous cor-

rection affects parsing, tagging, and tokenizing re-

sults.  

3.2 Preprocessing 

Because the correction task is no longer restricted 

to five error types, tokenizing and spelling error 

correction have become critical for error correc-

tion. To detect tokenizing error such as â€œciviliza-

tions.Itâ€, a re-tokenzing process is necessary. If a 

word contains a comma, punctuation (e.g., â€˜,â€™ or 

â€˜.â€™) and the word count in Google n-gram is less 

than some threshold (here, 1000), we tokenize the 

word, e.g., as â€œcivilizations . Itâ€. We also correct 

spelling errors by referring to the Google n-gram 

word count. If the word uni-gram count is less 

than a threshold (here, 60000) and the part-of-

speech (POS) tag is not NNP or NNPS, we assume 

that the word has o  ne or more errors. The thresh-

old is set based on the development set. We use 

the Enchant Python Library to correct the spelling 

errors1. However, using only one best result is not 

very accurate. Thus, among the best results in the 

Enchant Python Library, we select the one best 

word, i.e. that word with the highest frequency in 

the Google n-gram corpus. Using NUCLE train-

ing data, rules are constructed for comma, punc-

tuation, and other errors (Table 3).  

 

 

 
Figure 1. Overall Process of Router-based Correction 

 

66



3.3 Candidate Generation 

Selecting appropriate correction candidates is crit-

ical for the precision of the method. In article and 

noun number correction, the number of candidates 

is small: â€˜aâ€™,â€™anâ€™,â€™theâ€™ in article correction, â€˜pluralâ€™ 

or â€˜singularâ€™ in noun number correction. However, 

the number of correction candidates can be unlim-

ited in wrong collocation/idiom errors. Reducing 

the number of candidates is important in the gram-

mar error correction task.  

 

Nn Correction Candidate: noun number correc-

tion has just one replacement candidate. If the 

word is plural, its correction candidate is singular, 

and vice versa. The language tool2 can perform 

these changes. 

 

Other Correction Candidate: for corrections 

other than noun number, candidates are selected 

from the GE-tagged corpus. A total of 4206 pairs 

were extracted. We use the notation of candidate 

pair (oâ†’r), which links the original word (o) and 

its correction candidate (r). In the deletion correc-

tion step, we determine whether or not the word 

should be deleted. In the insertion correction step, 

we select the insertion position in a sentence as a 

space between two words. If o is âˆ…, insertion cor-
rection is required; if r is âˆ…, the pair deletion cor-
rection is required. We use the Stanford constitu-

ency parser (De Marneffe & Manning, 2008) to 

extract a noun phrase; if it does not contain a de-

terminer or article, we insert one in front of the 

noun phrase; if the noun in the noun phrase is sin-

gular, â€˜theâ€™, â€˜aâ€™, and, â€˜anâ€™ are selected an insertion 

candidates; if the noun is plural, only â€˜theâ€™ is se-

lected as an insertion candidate. We only apply in-

sertion correction at ArtOrDet, comma errors, and 

preposition; we skip insertion correction for other 

error types because selecting an insertion position 

is difficult and if every position is selected as in-

sertion position, precision decrease. 

 

4 N-gram Approach 

We used the following notation. 

N(o) n-gram vector in original sentence 

N(r) n-gram vector in replacement sen-

tence 

n(o)i i th element in N(o) 
ğ‘›(ğ‘Ÿ)ğ‘– i th element in N(r) 
N[i:j] n-gram vector from i th element to 

j th element 

                                                 
2http://www.languagetool.org 

 

Web-scale data have also been used successfully 

in many other research areas, such as lexical dis-

ambiguation (Bergsma et al., 2009). Most NLP 

systems resolve ambiguities with the help of a 

large corpus of text, e.g.: 

â€¢ The system tried to decide {among, between} 

the two confusable words.  

Disambiguation accuracy increases with the size 

of the corpus. Many systems incorporate the web 

count into their selection process. For the above 

example, a typical web-based system would query 

a search engine with the sequences â€œdecide among 

theâ€ and â€œdecide between theâ€ and select the can-

didate that returns the most hits. Unfortunately, 

this approach would fail when disambiguation re-

quires additional context. Bergsma (2009) sug-

gested using the context of samples of various 

lengths and positions. For example, from the 

above the example sentence, the following 5-gram 

patterns can be extracted: 

 

â€¢  system tried to decide {among, between}  

â€¢  tried to decide {among, between} the 

â€¢  to decide {among, between} the two 

â€¢ decide {among, between} the two confusable 

â€¢ {among, between} the two confusable words 
 

Similarly, four 4-gram patterns, three 3-gram pat-

terns and two 2-gram patterns are extracted by 

spanning the target. A score for each pattern is cal-

culated by summing the log-counts. This method 

was successfully applied in lexical disambigua-

tion. Web-scale data were used with the count in-

formation specified as features. Kao et al. (2013) 

used a â€œmoving window (MW)â€ : 

 
ğ‘€ğ‘Šğ‘–,ğ‘˜(w) = {ğ‘¤ğ‘–âˆ’ğ‘— , â€¦ , ğ‘¤ğ‘–âˆ’ğ‘—+(ğ‘˜âˆ’1), ğ‘— = 0, ğ‘˜ âˆ’ 1}  (1) 

 

where ğ‘–  denotes the position of the word, k the 

window size and w the original or replacement 

word at position ğ‘–. The window size is set to 2 to 

5 words. MW is the same concept as the SUMLM: 

 

ğ‘†ğ‘–,ğ‘˜(ğ‘¤) = âˆ‘ ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š)

ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šâˆˆğ‘€ğ‘Šğ‘˜(ğ‘¤)

(2) 

Both approaches apply the sum of all MWs in (1).  

Our approach is based on the MW method. The 

difference is that instead of summing all the MWs, 

we consider only one best MW which is referred 

to here as a frame. The following sentences 

 

67



demonstrate the case when the following words 

are the crucial features to correct errors: 

â€¢  I will do it (inâ†’at) home. 

â€¢  We need (anâ†’âˆ…) equipment to solve problems. 

However, following sentences demonstrate the 

case when preceding words is the crucial feature 

to correct errors: 

â€¢  One (areâ†’is) deemed to death at a later stage . 

â€¢  But data that (showsâ†’show) the rising of life 

expectancies 

We investigated which frame is the best based on 

the development set, then router is trained to de-

cide on the frame depending on the candidate pair.  

 

4.1 Router-based N-gram Correction 

A frame is a sequence of words around the target 

position. A frame is divided into a preceding 

frame and a following frame. The target position 

can be either a position of a target word (Figure 

2a) or a position in which a candidate word is 

judged to be necessary (Figure 2b). Once the size 

(i.e., number of words) of frames is chosen, sev-

eral forms of frames (n; m) with different sizes of 

preceding (n) and following (m) words are possi-

ble. 

 

 
Figure 2.  Frame for n-gram 

 

The router is designed to take care of two stages 

(training, run-time) error correction. During train-

ing, the router selects the best frame for each can-

didate pair. By testing each candidate pair with 

each frame in the development data; the frame 

with the best precision is selected as the best 

frame among (1;1), (1;2), (1;3), (2;1),(2:2), etc.  

 At the end of the training stage, the router has 

a list of pairs (x) which matches the best frame (y) 

associated with it (Table 1) as a result of compar-

ing each candidate pair with one in the develop-

ment corpus. 

During runtime, the router assigns each candi-

date pair to the best frame to produce the output 

sentence (Figure 1). For example, for a sentence 

â€œThis ability is not seen 40 years back where the 

technology advances were not as good as now .â€ 

the candidate pair for correction (backâ†’ ago) is 

suggested. The best frame assigned by the router 

for this pair (1;1), which is â€œyears back whereâ€. 

The best candidate frame for this is â€œyear ago 

whereâ€. At this point, we query the count of 

â€œyears back whereâ€ and â€œyears ago whereâ€ from 

the Google N-gram Count Corpus; these counts 

are 46 and 1815 respectively. Because the count 

of â€œyears ago whereâ€ is greater than that of â€œyears 

back whereâ€, the former is selected as the correct 

form. As a result, the sentence â€œThis ability is not 

seen 40 years back where the technology ad-

vances were not as good as now.â€ is corrected to 

â€œThis ability is not seen 40 years ago where the 

technology advances were not as good as now.â€ 

Some words are allowed to have multiple best 

frames; in all the best frames, if a candidate word 

sequence is more frequent than an original word 

sequence in the Google count, then correction is 

made. The multiple frames are also trained from 

the development data set.  

4.2 Probability n-gram Vector 

We use the probability n-gram Vector approach to 

correct Nn. Most errors are corrected using the 

router-based method; however, training the router 

for every noun is difficult because the number of 

nouns is extremely large. Moreover, for noun 

number, we found that rather than considering one 

direction or one frame of n-gram, every direction 

of n-gram should be considered for better perfor-

mance such as forward, backward, and two-way. 

Thus, the probability n-gram vector algorithm is 

applied only in the noun number error correction.  

We propose the probability n-gram vector method 

to correct grammatical errors to consider both di-

rections, forward and backward. In a forward n-

gram, the probability of each word is estimated 

Table 1. Example of Trained Router 

x (oâ†’r) y 

(anotherâ†’other) (1;3) 

(lessâ†’fewer) (1;3) 

(riseâ†’raise) (1;2) 

(backâ†’ago) (1;1) 

(couldâ†’can) (2;1) 

(wellâ†’good) (2;1) 

(nearâ†’âˆ…) No correction 
  
 

68



depending on the preceding word. On the other 

hand, in a backward n-gram the probability of 

each word is estimated depending on the follow-

ing words. When the probability of a candidate 

word is higher than original word, we replace the 

original with the candidate word in the correction 

step. 

Probability n-gram vectors are generated from the 

original word and a candidate word (Figure 3). 

Rather than using a single sequence of n-gram 

probability, we apply contexts of various lengths 

and positions. We applied the probability infor-

mation using the Google n-gram count infor-

mation as in the following equation: 

 P(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’2, ğ‘¤ğ‘–âˆ’1) =
ğ¶(ğ‘¤ğ‘–âˆ’2,ğ‘¤ğ‘–âˆ’1ğ‘¤ğ‘–)

ğ¶(ğ‘¤ğ‘–âˆ’2,ğ‘¤ğ‘–âˆ’1)
 

 

Moreover, rather than calculating one wordâ€™s 

probability given n words such as 

P(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’1, ğ‘¤ğ‘–âˆ’2, ğ‘¤ğ‘–âˆ’3), our model calculates the 
probability of m words given an n word sequence. 

The following is an example 4-gram with forward 

probability: 

â€¢ m = 3, n = 1 P(ğ‘¤ğ‘–âˆ’2, ğ‘¤ğ‘–âˆ’1ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’3) 

â€¢ m = 2, n = 2 P(ğ‘¤ğ‘–âˆ’1, ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’3, ğ‘¤ğ‘–âˆ’2) 

â€¢ m = 1, n = 3 P(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’3, ğ‘¤ğ‘–âˆ’2, ğ‘¤ğ‘–âˆ’1). 

We construct a 40-dimensional probability vector 

with forward and backward probabilities consid-

ering of twenty 5-grams, twelve 4-grams, six 3-

gram, and two 2-gram. Additionally, the elements 

of the n-gram vector are detailed in Table 2. 

 

Back-Off Model: A high-order n-gram is more 

effective than a low-order n-gram. Thus, we ap-

plied back-off methods (Katz, 1987) to assign 

higher priority to higher order probabilities. If all 

elements in 5-gram vectors are 0 for both the orig-

inal and candidate sentence, which means 

âˆ‘ {ğ‘›(ğ‘œ)ğ‘– + ğ‘›(ğ‘Ÿ)ğ‘–} = 0
19
ğ‘–=0 , we consider 4-gram 

vectors (N[20:31]). If 4-gram vectors are 0, we con-

sider 3-gram vectors. Moreover, when the pro-

posed method calculates each of the forward, 

backward and two-way probabilities, the back-off 

method is used to get each score.  

 

Correction: Here, we explain the process of error 

correction using n-gram vectors. First, we gener-

ate Nn error candidates. Second, we construct the 

n-gram probability vector for each candidate. The  

back-off method is applied in N(o)+N(r), The vec-

tor contains various directions and ranges of prob-

abilities of words given a sample sentence. We 

then calculate forward n-gram score by summing 

even elements in the vector. We calculate the 

backward n-gram by summing odd elements in 

Table 2. Next, the two-way n-gram is calculated 

by summing all elements for both directions n-

gram. If forward, backward, and two-way n-

grams have higher probabilities for the candidate 

word, we select the candidate as corrected word 

(Figure 3). 

Table 2: The elements of n-gram vector  

5-GRAM 

ğ‘›0 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–+1ğ‘¤ğ‘–+2ğ‘¤ğ‘–+3ğ‘¤ğ‘–+4) backward 
ğ‘›1 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’4ğ‘¤ğ‘–âˆ’3ğ‘¤ğ‘–âˆ’2ğ‘¤ğ‘–âˆ’1) forward 
ğ‘›2 = ğ‘ƒ(ğ‘¤ğ‘–ğ‘¤ğ‘–+1|ğ‘¤ğ‘–+2ğ‘¤ğ‘–+3ğ‘¤ğ‘–+4) backward 
â€¦â€¦.. 

4-GRAM 

ğ‘›20 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–+1ğ‘¤ğ‘–+2ğ‘¤ğ‘–+3) backward 
ğ‘›21 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’3ğ‘¤ğ‘–âˆ’2ğ‘¤ğ‘–âˆ’1) forward 
â€¦â€¦.. 

 

3-GRAM 

ğ‘›32 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–+1ğ‘¤ğ‘–+2) backward 
ğ‘›33 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’2ğ‘¤ğ‘–âˆ’1) forward 
ğ‘›34 = ğ‘ƒ(ğ‘¤ğ‘–ğ‘¤ğ‘–+1|ğ‘¤ğ‘–+2) backward 
ğ‘›35 = ğ‘ƒ(ğ‘¤ğ‘–âˆ’1ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’2) forward 
ğ‘›36 = ğ‘ƒ(ğ‘¤ğ‘–âˆ’1ğ‘¤ğ‘–|ğ‘¤ğ‘–+1) backward 
ğ‘›37 = ğ‘ƒ(ğ‘¤ğ‘–ğ‘¤ğ‘–+1|ğ‘¤ğ‘–âˆ’1) forward 
2-GRAM 

ğ‘›38 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–+1) backward 
ğ‘›39 = ğ‘ƒ(ğ‘¤ğ‘–|ğ‘¤ğ‘–âˆ’1) forward 

 

 
Figure 3. Overall process of Nn Correction 

69



 

5 Verb Correction (Rule-based)  

There are several types of verb errors in non-na-

tive text such as verb tense, verb modal, missing  

verb, verb form, and subject-verb-agreement 

(SVA). Among these errors, we attempt to correct 

SVA errors using rule-based methods (Table 3). 

In non-native text, parsing and tagging errors are 

inevitable, and it may cause false alarm. Thus, in-

stead of dependency parsing to find subject and 

verb, we consider the preceding five words be-

cause erroneous sentences often contain depend-

ency errors. Moreover, in erroneous sentences, 

POS tagging accuracy is lower than native text. 

Thus, NN and VB are misclassified, as are VBZ 

and NNS. A rule is used that encodes the relevant 

linguistic knowledge that these words or POSs 

should not occur in the five positions preceding 

the VBZ: â€˜NNâ€™, â€™thisâ€™, â€™itâ€™ ,â€™oneâ€™, â€™VBGâ€™. Moreover, 
words that preceded and follow â€˜whichâ€™ should 
agree in verb form, as indicated in Rule3 and 
Rule4. 
 

6 Experiment 

The CoNLL-2014 training data consist of 1,397 

articles together with gold-standard annotation. 

Algorithm Rule1-Comma 
1: function rule1( toksent,  tokpos) 

2: for i â† 0 â€¦ len(toksent) do 

3: if  toksent[i] in [ Howeverâ€™, â€˜Thereforeâ€™, â€˜Thusâ€™] and not  toksent[i + 1] == â€˜,â€™  then 
4: toksent[i]= toksent[i] + â€˜ ,â€™ 

 

Algorithm Rule2-preposition 
1: function rule2( toksent,  tokpos) 

2: for i â† 0 â€¦ len(toksent) do 

3: if  toksent[i] = â€˜accordingâ€™ and not  toksent [i+1] = â€˜toâ€™ 
4:  toksent [i+1] = â€˜to â€˜+  toksent [i+1] 

 

Algorithm Rule3-Subject Verb Agreement 

1: function rule3( toksent,  tokpos) 

2: for i â† 0 â€¦ len(toksent) do 

3: if  toksent[i] is â€˜whichâ€™ 
4: if  tokpos[i âˆ’ 1] == â€˜NNSâ€™ and  tokpos[i + 1] == â€˜VBZâ€™  then 

5: toksent[i + 1]= changeWordForm (toksent[i + 1], â€˜VBPâ€™) 
6: else if  tokpos[i âˆ’ 1]  == â€˜NNSâ€™ and  tokpos[i + 1] == â€˜NNSâ€™ then 

7: toksent[i + 1]=  =changeWordForm(toksent[i + 1], â€˜VBPâ€™) 
8: else if  tokpos[i âˆ’ 1] == â€˜NNâ€™ and  tokpos[i + 1]== â€˜areâ€™ then 

9: toksent[i + 1]=  = is 
10: else if  tokpos[i âˆ’ 1] == â€˜NNâ€™ and  tokpos[i + 1] in [â€˜VBPâ€™,â€™VBâ€™,â€™NNâ€™] then 

11: toksent[i + 1]=  = makePlural(toksent[i + 1]) 
 

Algorithm Rule4-Subject Verb Agreement 

1: function rule4( toksent,  tokpos) 

2: for i â† 0 â€¦ len(toksent) do 

3: if not ( tokpos[i]is â€²VBZâ€² and [â€˜NNâ€™,â€™thisâ€™,â€™itâ€™,â€™oneâ€™,â€™VBGâ€™] in  tokpos[i âˆ’ 5: i]) then 

4:  tokcandâ†changeWordForm( tokword[i], â€˜VBPâ€™) 

5: else if not ( tokpos[i]is â€²VBPâ€² and [â€˜Iâ€™,â€™weâ€™,â€™theyâ€™,â€™andâ€™] in  toksent[i âˆ’ 5: i]) then 

6:  tokcand â†changeWordForm( tokword[i], â€˜VBZâ€™) 

7: else if not ( tokpos[i]is â€²NNâ€² and [â€˜beâ€™,â€™ingâ€™] in  toksent[i âˆ’ 5: i]) then 

8:  tokcandâ†changeWordForm( tokword[i], â€˜VBNâ€™) 

9:         original = ngramCount( toksent), candidate =ngramCount(tokcand) 
10: If original < candidate then 
11: Return tokcand 

Table 3. Examples of Rules  

 

 

70



The documents are a subset of the NUS Corpus of 

Learner English (NUCLE). We use the Max-

Match (M2) scorer provided by the CoNLL-2014 

Shared Task. The M2 scorer works by using the 

set that maximally matches the set of gold-stand-

ard edits specified by the annotator as being equal 

to the set of system edits that are automatically 

computed and used in scoring (Dahlmeier & Ng, 

2012). The official evaluation metric is F0.5, 

weighting precision twice as much as recall. We 

achieve F0.5 of 30.88; precision of 34.51; recall 

of 21.73 in the original annotation (Table 4). After 

original official annotations announced by organ-

izers (i.e., only based on the annotations of the two 

annotators), another set of annotations is offered 

based on including the additional answers pro-

posed by the 3 teams (CAMB, CUUI, UMC). The 

improvement gap between the original annotation 

and the revised annotation of our team (POST) is 

5.89%.  We obtain the highest improvement rate 

except for the 3 proposed teams (Figure 4), F0.5 

of 36.77; precision of 41.28; recall of 25.59 in the 

revised annotation. Our system achieves the 4th 

highest scores of 13 participating teams based on 

both the original and revised annotations. To ana-

lyze the scores of each of the error types and mod-

ules, we apply the method of n-gram vector (Nn), 

rule-based (Verb, Mec), and router-based (others) 

separately in both the original and the revised an-

notation of all error types. We achieve high preci-

sion by rules at the Mec which indicates punctua-

tion, capitalization, spelling, and typos errors. Ad-

ditionally, the Nn type has the highest improve-

ment gap between the original and revised anno-

tation (17% â†’ 24.31 of F0.5).  In order for our 

team to improve the high precision in the rule-

based approach, we tested potential rules on the 

development data and kept a rule only if its preci-

sion on that data set was 30% or greater. When we 

trained router, the same strategy was conducted. 

If a frame could not achieve 30% precision, we 

assigned the candidate pair as â€œno correctionâ€ in 

the router. These constraints achieve precision of 

30 % in most error types.  

7 Discussion  

Although preposition errors are frequently com-

mitted in non-native text, we mostly skip the cor-

rection of preposition error. This is because as-

signing prepositions correctly is extremely diffi-

cult, because (1) the preposition used can vary 

(e.g., Canada: â€˜on the weekendâ€™ vs. Britain â€˜at the 

weekendâ€™); (2) in a given location, more than one 

preposition may be possible, and the choice af-

fects the meaning (e.g., â€˜on the wallâ€™, vs. â€˜at the 

wallâ€™). Verb errors can consist of many multi-

 
Figure 4. Improvement gap between the original annotation and revised annotation of each team 

 

0

2

4

6

8

10

 
Table 4. Performance on each error type 

 Original annotation  Revised annotation 

 Precision Recall F0.5  Precision Recall F0.5 

N-gram (Nn) 31.0 6.55 17.75  42.28 9.0 24.31 

Rule (Verb) 28.95 1.12 4.86  31.17 1.29 5.52 

Rule (Mec) 49.34 5.47 18.94  52.16 6.17 20.93 

Router (Others) 28.11 12.49 22.49  35.29 15.45 28.08 

All 34.51 21.73 30.88  41.28 25.59 36.77 
 

71



word errors due to errors of usages of passive and 

active voice. (e.g. releaseâ†’be released). Our cur-

rent system cannot correct these multi-words er-

rors, for three reasons. First, if the original exam-

ple consists of one word and the optimal replace-

ment consists of two words, n-gram scores cannot 

be applied easily to compare probabilities be-

tween them. Second, the n-gram approach also 

fails if the distance between subject and verb is 

more than 5. Third, multiply dependent errors are 

critical for verb error correction. For example, 

noun number, determiner, and subject verb agree-

ment are often dependent upon each other: e.g. 

â€œAnd once this happens, privacy does not exist 

any more and people's (lifeâ†’lives) (isâ†’are) un-

der great threaten.â€ The correction order will be 

important when all error type must be corrected 

simultaneously.  

Grammar error correction is a challenging 

problem. In CoNNL-2013, more than half of the 

related teams obtained F-score < 10.0. This low 

performance in the grammar error correction can 

be explained by several reasons, which indicate 

the present limitations of grammar correction sys-

tems. 

Among a total of 4206 pairs, we only use small 

amount of candidate pairs, 215 pairs are used for 

candidate pairs. The other 3991 pairs are dis-

carded in the router training step because these 

pairs cannot be corrected by the n-gram approach. 

Various classification methods and statistical ma-

chine translation based methods will be investi-

gated in the router-based approach to find the tai-

lored methods for the given word. A demonstra-

tion and progress of our grammar error correction 

system is available to the public3.  

8 Conclusion 

We have described the POSTECH grammatical 

error correction system. We use the Google N-

gram count corpus to detect spelling errors, punc-

tuation, and comma errors. A rule-based method 

is used to correct verb, punctuation, comma errors 

and preposition errors. The Google corpus is also 

used for an n-gram vector approach and a router-

based approaches. Currently we use the router to 

select the best frame. In the future, we will train a 

router to select the best method among classifica-

tion, n-gram approach, statistical machine transla-

                                                 
3 http://isoft.postech.ac.kr/grammar 

tion-based method and pattern matching ap-

proaches. A machine learning method will be used 

to train the router with various features.   

Acknowledgements 

This research was supported by the MSIP(The Ministry 

of Science, ICT and Future Planning), Korea and Mi-

crosoft Research, under IT/SW Creative research pro-

gram supervised by the NIPA(National IT Industry 

Promotion Agency) (NIPA-2013- H0503-13-1006) 

and this research was supported by the Basic Science 

Research Program through the National Research 

Foundation of Korea(NRF) funded by the Ministry of 

Education, Science and Technology(2010-0019523). 

 

References 

Han, Na-Rae, Chodorow, Martin, & Leacock, Claudia. 

(2006). Detecting errors in English article 

usage by non-native speakers.  

Alam, Md Jahangir, UzZaman, Naushad, & Khan, 

Mumit. (2006). N-gram based statistical 

grammar checker for Bangla and English.  

Bergsma, Shane, Lin, Dekang, & Goebel, Randy. 

(2009). Web-Scale N-gram Models for 

Lexical Disambiguation. Paper presented at 

the IJCAI. 

Brants, Thorsten, & Franz, Alex. (2006). The Google 

Web 1T 5-gram corpus version 1.1. 

LDC2006T13.  

Brockett, Chris, Dolan, William B, & Gamon, Michael. 

(2006). Correcting ESL errors using phrasal 

SMT techniques. Paper presented at the 

Proceedings of the 21st International 

Conference on Computational Linguistics and 

the 44th annual meeting of the Association for 

Computational Linguistics. 

Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, & 

Napolitano, Diane. (2013). Robust Systems 

for Preposition Error Correction Using 

Wikipedia Revisions. Paper presented at the 

Proceedings of NAACL-HLT. 

Dahlmeier, Daniel, & Ng, Hwee Tou. (2011). 

Grammatical error correction with 

alternating structure optimization. Paper 

presented at the Proceedings of the 49th 

Annual Meeting of the Association for 

Computational Linguistics: Human Language 

Technologies-Volume 1. 

Dahlmeier, Daniel, & Ng, Hwee Tou. (2012). Better 

evaluation for grammatical error correction. 

Paper presented at the Proceedings of the 

2012 Conference of the North American 

Chapter of the Association for Computational 

Linguistics: Human Language Technologies. 

 

72



Dahlmeier, Daniel, Ng, Hwee Tou, & Wu, Siew Mei. 

(2013). Building a large annotated corpus of 

learner English: The NUS corpus of learner 

English. Paper presented at the Proceedings of 

the Eighth Workshop on Innovative Use of 

NLP for Building Educational Applications. 

De Felice, Rachele. (2008). Automatic error detection 

in non-native English. University of Oxford.    

De Marneffe, Marie-Catherine, & Manning, 

Christopher D. (2008). The Stanford typed 

dependencies representation. Paper presented 

at the Coling 2008: Proceedings of the 

workshop on Cross-Framework and Cross-

Domain Parser Evaluation. 

Gamon, Michael. (2010). Using mostly native data to 

correct errors in learners' writing: a meta-

classifier approach. Paper presented at the 

Human Language Technologies: The 2010 

Annual Conference of the North American 

Chapter of the Association for Computational 

Linguistics. 

Gamon, Michael, Leacock, Claudia, Brockett, Chris, 

Dolan, William B, Gao, Jianfeng, Belenko, 

Dmitriy, & Klementiev, Alexandre. (2009). 

Using statistical techniques and web search to 

correct ESL errors. Calico Journal, 26(3), 

491-511.  

Hermet, Matthieu, DÃ©silets, Alain, & Szpakowicz, Stan. 

(2008). Using the web as a linguistic resource 

to automatically correct lexico-syntactic 

errors.  

Izumi, Emi, Uchimoto, Kiyotaka, & Isahara, Hitoshi. 

(2005). Error annotation for corpus of 

Japanese learner English. Paper presented at 

the Proceedings of the Sixth International 

Workshop on Linguistically Interpreted 

Corpora. 

Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, & 

Yen, Tzu-Hsi. (2013). CoNLL-2013 Shared 

Task: Grammatical Error Correction NTHU 

System Description. CoNLL-2013, 20.  

Katz, Slava. (1987). Estimation of probabilities from 

sparse data for the language model 

component of a speech recognizer. Acoustics, 

Speech and Signal Processing, IEEE 

Transactions on, 35(3), 400-401.  

Knight, Kevin, & Chander, Ishwar. (1994). Automated 

postediting of documents. Paper presented at 

the AAAI. 

Mark, Alla Rozovskaya Kai-Wei Chang, & Roth, 

Sammons Dan. (2013). The University of 

Illinois System in the CoNLL-2013 Shared 

Task. CoNLL-2013, 51, 13.  

Naber, Daniel. (2003). A rule-based style and grammar 

checker. Diploma Thesis 

Nagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, & Isu, 

Naoki. (2006). A feedback-augmented method 

for detecting errors in the writing of learners 

of English. Paper presented at the Proceedings 

of the 21st International Conference on 

Computational Linguistics and the 44th 

annual meeting of the Association for 

Computational Linguistics. 

Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted , 

Hadiwinoto, Christian , Susanto, Raymond 

Hendy, & Bryant, Christopher (2014). The 

CoNLL-2014 Shared Task on Grammatical 

Error Correction. Paper presented at the the 

Eighteenth Conference on Computational 

Natural Language Learning: Shared Task 

(CoNLL-2014 Shared Task), Baltimore, 

Maryland, USA. 

Nicholls, Diane. (2003). The Cambridge Learner 

Corpus: Error coding and analysis for 

lexicography and ELT. Paper presented at the 

Proceedings of the Corpus Linguistics 2003 

conference. 

Rozovskaya, Alla, & Roth, Dan. (2011). Algorithm 

selection and model adaptation for ESL 

correction tasks. Urbana, 51, 61801.  

Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee, 

Kyusong, Kang, Sechun, & Lee, Gary 

Geunbae. (2012). A meta learning approach 

to grammatical error correction. Paper 

presented at the Proceedings of the 50th 

Annual Meeting of the Association for 

Computational Linguistics: Short Papers-

Volume 2. 

Wu, Yuanbin, & Ng, Hwee Tou. (2013). Grammatical 

error correction using integer linear 

programming. Paper presented at the 

Proceedings of the 51st Annual Meeting of 

the Association for Computational 

Linguistics. 

Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao, 

Lidia S, & Zeng, Xiaodong. (2013). UM-

Checker: A Hybrid System for English 

Grammatical Error Cor-rection. CoNLL-2013, 

34.  

Yannakoudakis, Helen, Briscoe, Ted, & Medlock, Ben. 

(2011). A New Dataset and Method for 

Automatically Grading ESOL Texts. Paper 

presented at the ACL. 

 

 

73


