



















































Effective Use of Word Order for Text Categorization with Convolutional Neural Networks


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 103–112,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Effective Use of Word Order for Text Categorization
with Convolutional Neural Networks

Rie Johnson
RJ Research Consulting

Tarrytown, NY, USA
riejohnson@gmail.com

Tong Zhang
Baidu Inc., Beijing, China

Rutgers University, Piscataway, NJ, USA
tzhang@stat.rutgers.edu

Abstract

Convolutional neural network (CNN) is a neu-
ral network that can make use of the inter-
nal structure of data such as the 2D structure
of image data. This paper studies CNN on
text categorization to exploit the 1D structure
(namely, word order) of text data for accurate
prediction. Instead of using low-dimensional
word vectors as input as is often done, we
directly apply CNN to high-dimensional text
data, which leads to directly learning embed-
ding of small text regions for use in classifi-
cation. In addition to a straightforward adap-
tation of CNN from image to text, a sim-
ple but new variation which employs bag-of-
word conversion in the convolution layer is
proposed. An extension to combine multiple
convolution layers is also explored for higher
accuracy. The experiments demonstrate the
effectiveness of our approach in comparison
with state-of-the-art methods.

1 Introduction

Text categorization is the task of automatically as-
signing pre-defined categories to documents writ-
ten in natural languages. Several types of text cat-
egorization have been studied, each of which deals
with different types of documents and categories,
such as topic categorization to detect discussed top-
ics (e.g., sports, politics), spam detection (Sahami et
al., 1998), and sentiment classification (Pang et al.,
2002; Pang and Lee, 2008; Maas et al., 2011) to de-
termine the sentiment typically in product or movie
reviews. A standard approach to text categorization
is to represent documents by bag-of-word vectors,
namely, vectors that indicate which words appear in

the documents but do not preserve word order, and
use classification models such as SVM.

It has been noted that loss of word order caused
by bag-of-word vectors (bow vectors) is particularly
problematic on sentiment classification. A simple
remedy is to use word bi-grams in addition to uni-
grams (Blitzer et al., 2007; Glorot et al., 2011; Wang
and Manning, 2012). However, use of word n-grams
with n > 1 on text categorization in general is not
always effective; e.g., on topic categorization, sim-
ply adding phrases or n-grams is not effective (see,
e.g., references in (Tan et al., 2002)).

To benefit from word order on text categoriza-
tion, we take a different approach, which employs
convolutional neural networks (CNN) (LeCun et al.,
1986). CNN is a neural network that can make use
of the internal structure of data such as the 2D struc-
ture of image data through convolution layers, where
each computation unit responds to a small region of
input data (e.g., a small square of a large image).
We apply CNN to text categorization to make use of
the 1D structure (word order) of document data so
that each unit in the convolution layer responds to a
small region of a document (a sequence of words).

CNN has been very successful on image clas-
sification; see e.g., the winning solutions of Im-
ageNet Large Scale Visual Recognition Challenge
(Krizhevsky et al., 2012; Szegedy et al., 2014; Rus-
sakovsky et al., 2014).

On text, since the work on token-level applica-
tions (e.g., POS tagging) by Collobert et al. (2011),
CNN has been used in systems for entity search, sen-
tence modeling, word embedding learning, product
feature mining, and so on (Xu and Sarikaya, 2013;
Gao et al., 2014; Shen et al., 2014; Kalchbrenner et
al., 2014; Xu et al., 2014; Tang et al., 2014; Weston

103



et al., 2014; Kim, 2014). Notably, in many of these
CNN studies on text, the first layer of the network
converts words in sentences to word vectors by ta-
ble lookup. The word vectors are either trained as
part of CNN training, or fixed to those learned by
some other method (e.g., word2vec (Mikolov et al.,
2013)) from an additional large corpus. The latter is
a form of semi-supervised learning, which we study
elsewhere. We are interested in the effectiveness
of CNN itself without aid of additional resources;
therefore, word vectors should be trained as part of
network training if word vector lookup is to be done.

A question arises, however, whether word vector
lookup in a purely supervised setting is really useful
for text categorization. The essence of convolution
layers is to convert text regions of a fixed size (e.g.,
“am so happy” with size 3) to feature vectors, as de-
scribed later. In that sense, a word vector learning
layer is a special (and unusual) case of convolution
layer with region size one. Why is size one appro-
priate if bi-grams are more discriminating than uni-
grams? Hence, we take a different approach. We di-
rectly apply CNN to high-dimensional one-hot vec-
tors; i.e., we directly learn embedding1 of text re-
gions without going through word embedding learn-
ing. This approach is made possible by solving the
computational issue2 through efficient handling of
high-dimensional sparse data on GPU, and it turned
out to have the merits of improving accuracy with
fast training/prediction and simplifying the system
(fewer hyper-parameters to tune). Our CNN code
for text is publicly available on the internet3.

We study the effectiveness of CNN on text cate-
gorization and explain why CNN is suitable for the
task. Two types of CNN are tested: seq-CNN is a
straightforward adaptation of CNN from image to
text, and bow-CNN is a simple but new variation of
CNN that employs bag-of-word conversion in the
convolution layer. The experiments show that seq-
CNN outperforms bow-CNN on sentiment classi-

1We use the term ‘embedding’ loosely to mean a structure-
preserving function, in particular, a function that generates low-
dimensional features that preserve the predictive structure.

2CNN implemented for image would not handle sparse data
efficiently, and without efficient handling of sparse data, convo-
lution over high-dimensional one-hot vectors would be compu-
tationally infeasible.

3riejohnson.com/cnn_download.html

Output

Input

[ 0 0 1 0 0 0 0 0 0 0 ]

Convolution 
layer

Pooling layer

Convolution 
layer

Pooling layer

Output layer
(Linear classifier)

[ 0 0 1 0 0 0 0 0 0 0 ]

Figure 1: Convolutional neural network.

Figure 2: Convolution layer for image. Each computation
unit (oval) computes a non-linear function σ(W ·r`(x)+b) of
a small region r`(x) of input image x, where weight matrix W
and bias vector b are shared by all the units in the same layer.

fication, vice versa on topic classification, and the
winner generally outperforms the conventional bag-
of-n-gram vector-based methods, as well as previ-
ous CNN models for text which are more complex.
In particular, to our knowledge, this is the first work
that has successfully used word order to improve
topic classification performance. A simple exten-
sion that combines multiple convolution layers (thus
combining multiple types of text region embedding)
leads to further improvement. Through empirical
analysis, we will show that CNN can make effec-
tive use of high-order n-grams when conventional
methods fail.

2 CNN for document classification

We first review CNN applied to image data and then
discuss the application of CNN to document classi-
fication tasks to introduce seq-CNN and bow-CNN.

2.1 Preliminary: CNN for image

CNN is a feed-forward neural network with convo-
lution layers interleaved with pooling layers, as il-
lustrated in Figure 1, where the top layer performs
classification using the features generated by the lay-
ers below. A convolution layer consists of several
computation units, each of which takes as input a
region vector that represents a small region of the
input image and applies a non-linear function to it.
Typically, the region vector is a concatenation of
pixels in the region, which would be, for example,

104



75-dimensional if the region is 5×5 and the number
of channels is three (red, green, and blue). Concep-
tually, computation units are placed over the input
image so that the entire image is collectively cov-
ered, as illustrated in Figure 2. The region stride
(distance between the region centers) is often set to
a small value such as 1 so that regions overlap with
each other, though the stride in Figure 2 is set larger
than the region size for illustration.

A distinguishing feature of convolution layers
is weight sharing. Given input x, a unit associ-
ated with the `-th region computes σ(W · r`(x) +
b), where r`(x) is a region vector representing
the region of x at location `, and σ is a pre-
defined component-wise non-linear activation func-
tion, (e.g., applying σ(x) = max(x, 0) to each vec-
tor component). The matrix of weights W and the
vector of biases b are learned through training, and
they are shared by the computation units in the same
layer. This weight sharing enables learning useful
features irrespective of their location, while preserv-
ing the location where the useful features appeared.

We regard the output of a convolution layer as an
‘image’ so that the output of each computation unit
is considered to be a ‘pixel’ of m channels where
m is the number of weight vectors (i.e., the number
of rows of W) or the number of neurons. In other
words, a convolution layer converts image regions
to m-dim vectors, and the locations of the regions
are inherited through this conversion.

The output image of the convolution layer is
passed to a pooling layer, which essentially shrinks
the image by merging neighboring pixels, so that
higher layers can deal with more abstract/global in-
formation. A pooling layer consists of pooling units,
each of which is associated with a small region
of the image. Commonly-used merging methods
are average-pooling and max-pooling, which respec-
tively compute the channel-wise average/maximum
of each region.

2.2 CNN for text
Now we consider application of CNN to text data.
Suppose that we are given a document D =
(w1, w2, . . .) with vocabulary V . CNN requires vec-
tor representation of data that preserves internal lo-
cations (word order in this case) as input. A straight-
forward representation would be to treat each word

as a pixel, treat D as if it were an image of |D| × 1
pixels with |V | channels, and to represent each pixel
(i.e., each word) as a |V |-dimensional one-hot vec-
tor4. As a running toy example, suppose that vocab-
ulary V = { “don’t”, “hate”, “I”, “it”, “love” } and
we associate the words with dimensions of vector
in alphabetical order (as shown), and that document
D=“I love it”. Then, we have a document vector:

x = [ 0 0 1 0 0 | 0 0 0 0 1 | 0 0 0 1 0 ]> .

2.2.1 seq-CNN for text
As in the convolution layer for image, we repre-

sent each region (which each computation unit re-
sponds to) by a concatenation of the pixels, which
makes p|V |-dimensional region vectors where p is
the region size fixed in advance. For example, on
the example document vector x above, with p = 2
and stride 1, we would have two regions “I love” and
“love it” represented by the following vectors:

r0(x) =



0
0
1
0
0

—
0
0
0
0
1



don′t
hate
I
it

love

don′t
hate

I
it

love

r1(x) =



0
0
0
0
1
—
0
0
0
1
0



don′t
hate

I
it

love

don′t
hate

I
it

love

The rest is the same as image; the text region vec-
tors are converted to feature vectors, i.e., the con-
volution layer learns to embed text regions into low-
dimensional vector space. We call a neural net with
a convolution layer with this region representation
seq-CNN (‘seq’ for keeping sequences of words) to
distinguish it from bow-CNN, described next.

2.2.2 bow-CNN for text
A potential problem of seq-CNN however, is that

unlike image data with 3 RGB channels, the number
of ‘channels’ |V | (size of vocabulary) may be very
large (e.g., 100K), which could make each region
vector r`(x) very high-dimensional if the region size

4Alternatively, one could use bag-of-letter-n-gram vectors
as in (Shen et al., 2014; Gao et al., 2014) to cope with out-of-
vocabulary words and typos.

105



p is large. Since the dimensionality of region vec-
tors determines the dimensionality of weight vec-
tors, having high-dimensional region vectors means
more parameters to learn. If p|V | is too large, the
model becomes too complex (w.r.t. the amount of
training data available) and/or training becomes un-
affordably expensive even with efficient handling of
sparse data; therefore, one has to lower the dimen-
sionality by lowering the vocabulary size |V | and/or
the region size p, which may or may not be desir-
able, depending on the nature of the task.

An alternative we provide is to perform bag-
of-word conversion to make region vectors |V |-
dimensional instead of p|V |-dimensional; e.g., the
example region vectors above would be converted
to:

r0(x) =


0
0
1
0
1


don′t
hate
I
it

love

r1(x) =


0
0
0
1
1


don′t
hate

I
it

love

With this representation, we have fewer param-
eters to learn. Essentially, the expressiveness
of bow-convolution (which loses word order only
within small regions) is somewhere between seq-
convolution and bow vectors.

2.2.3 Pooling for text
Whereas the size of images is fixed in image ap-

plications, documents are naturally variable-sized,
and therefore, with a fixed stride, the output of a con-
volution layer is also variable-sized as shown in Fig-
ure 3. Given the variable-sized output of the convo-
lution layer, standard pooling for image (which uses
a fixed pooling region size and a fixed stride) would
produce variable-sized output, which can be passed
to another convolution layer. To produce fixed-sized
output, which is required by the fully-connected top
layer5, we fix the number of pooling units and dy-
namically determine the pooling region size on each
data point so that the entire data is covered without
overlapping.

In the previous CNN work on text, pooling is
typically max-pooling over the entire data (i.e., one

5In this work, the top layer is fully-connected (i.e., each neu-
ron responds to the entire data) as in CNN for image. Alterna-
tively, the top layer could be convolutional so that it can receive
variable-sized input, but such CNN would be more complex.

I  love  it This  isn’t   what   I  expected  ! 

(a)                                                       (b)

This  isn’t   what   I  expected  ! 

(a)                                                       (b)

Figure 3: Convolution layer for variable-sized text.

pooling unit associated with the whole text). The dy-
namic k-max pooling of (Kalchbrenner et al., 2014)
for sentence modeling extends it to take the k largest
values where k is a function of the sentence length,
but it is again over the entire data, and the operation
is limited to max-pooling. Our pooling differs in that
it is a natural extension of standard pooling for im-
age, in which not only max-pooling but other types
can be applied. With multiple pooling units associ-
ated with different regions, the top layer can receive
locational information (e.g., if there are two pooling
units, the features from the first half and last half of
a document are distinguished). This turned out to be
useful (along with average-pooling) on topic classi-
fication, as shown later.

2.3 CNN vs. bag-of-n-grams
Traditional methods represent each document en-
tirely with one bag-of-n-gram vector and then ap-
ply a classifier model such as SVM. However, since
high-order n-grams are susceptible to data sparsity,
use of a large n such as 20 is not only infeasible
but also ineffective. Also note that a bag-of-n-gram
represents each n-gram by a one-hot vector and ig-
nores the fact that some n-grams share constituent
words. By contrast, CNN internally learns embed-
ding of text regions (given the consituent words as
input) useful for the intended task. Consequently,
a large n such as 20 can be used especially with the
bow-convolution layer, which turned out to be useful
on topic classification. A neuron trained to assign a
large value to, e.g., “I love” (and a small value to “I
hate”) is likely to assign a large value to “we love”
(and a small value to “we hate”) as well, even though
“we love” was never seen during training. We will
confirm these points empirically later.

2.4 Extension: parallel CNN
We have described CNN with the simplest network
architecture that has one pair of convolution and
pooling layers. While this can be extended in sev-
eral ways (e.g., with deeper layers), in our experi-
ments, we explored parallel CNN, which has two or

106



Output 1 (positive)

Input: “I really love it !”                One

region size s1 region size 
Convolution 

layers

Pooling layers

Output layer
1 (positive)

Input: “I really love it !”                One-hot vectors

region size s2

Figure 4: CNN with two convolution layers in parallel.

more convolution layers in parallel6, as illustrated in
Figure 4. The idea is to learn multiple types of em-
bedding of small text regions so that they can com-
plement each other to improve model accuracy. In
this architecture, multiple convolution-pooling pairs
with different region sizes (and possibly different re-
gion vector representations) are given one-hot vec-
tors as input and produce feature vectors for each
region; the top layer takes the concatenation of the
produced feature vectors as input.

3 Experiments

We experimented with CNN on two tasks, topic clas-
sification and sentiment classification. Detailed in-
formation for reproducing the results is available on
the internet along with our code.

3.1 CNN

We fixed the activation function to rectifier σ(x) =
max(x, 0) and minimized square loss with L2 reg-
ularization by stochastic gradient descent (SGD).
We only used the 30K words that appeared most
frequently in the training set; thus, for example, in
seq-CNN with region size 3, a region vector is 90K
dimensional. Out-of-vocabulary words were repre-
sented by a zero vector. On bow-CNN, to speed up
computation, we used variable region stride so that a
larger stride was taken where repetition7 of the same
region vectors can be avoided by doing so. Padding8

size was fixed to p− 1 where p is the region size.
6Similar architectures have been used for image. Kim

(2014) used it for text, but it was on top of a word vector con-
version layer.

7For example, if we slide a window of size 3 over “* * foo
* *” where “*” is out of vocabulary, a bag of “foo” will be
repeated three times with stride fixed to 1.

8As is commonly done, to the beginning and the end of each
document, special words that are treated as unknown words
(and converted to zero vectors instead of one-hot vectors) were
added as ‘padding’. The purpose is to equally treat the words at
the edge and words in the middle.

We used two techniques commonly used with
CNN on image, which typically led to small per-
formance improvements. One is dropout (Hinton
et al., 2012) optionally applied to the input to the
top layer. The other is response normalization as in
(Krizhevsky et al., 2012), which in our case scales
the output of the pooling layer z at each location by
multiplying (1 + |z|2)−1/2.
3.2 Baseline methods
For comparison, we tested SVM with the linear ker-
nel and fully-connected neural networks (see e.g.,
Bishop (1995)) with bag-of-n-gram vectors as in-
put. To experiment with fully-connected neural nets,
as in CNN, we minimized square loss with L2 reg-
ularization and optional dropout by SGD, and ac-
tivation was fixed to rectifier. To generate bag-of-
n-gram vectors, on topic classification, we first set
each component to log(x + 1) where x is the word
frequency in the document and then scaled them to
unit vectors, which we found always improved per-
formance over raw frequency. On sentiment classi-
fication, as is often done, we generated binary vec-
tors and scaled them to unit vectors. We tested three
types of bag-of-n-gram: bow1 with n ∈ {1}, bow2
with n ∈ {1, 2}, and bow3 with n ∈ {1, 2, 3};
that is, bow1 is the traditional bow vectors, and with
bow3, each component of the vectors corresponds to
either uni-gram, bi-gram, or tri-gram of words.

We used SVMlight9 for the SVM experiments.

NB-LM We also tested NB-LM, which first ap-
peared (but without performance report10 ) as NB-
SVM in WM12 (Wang and Manning, 2012) and
later with a small modification produced perfor-
mance that exceeds state-of-the-art supervised meth-
ods on IMDB (which we experimented with) in
MMRB14 (Mesnil et al., 2014). We experimented
with the MMRB14 version, which generates bi-
nary bag-of-n-gram vectors, multiplies the com-
ponent for each n-gram fi with log(P (fi|Y =
1)/P (fi|Y = −1)) (NB-weight) where the prob-
abilities are estimated using the training data,
and does logistic regression training. We used
MMRB14’s software11 with a modification so that

9http://svmlight.joachims.org/
10WM12 instead reported the performance of an ensemble of

NB and SVM as it performed better.
11https://github.com/mesnilgr/nbsvm

107



the regularization parameter can be tuned on devel-
opment data.

3.3 Model selection
For all the methods, the hyper-parameters such
as net configurations and regularization parameters
were chosen based on the performance on the devel-
opment data (held-out portion of the training data),
and using the chosen hyper-parameters, the models
were re-trained using all the training data.

3.4 Data, tasks, and data preprocessing
IMDB: movie reviews The IMDB dataset (Maas
et al., 2011) is a benchmark dataset for sentiment
classification. The task is to determine if the movie
reviews are positive or negative. Both the training
and test sets consist of 25K reviews. For preprocess-
ing, we tokenized the text so that emoticons such as
“:-)” are treated as tokens and converted all the char-
acters to lower case.

Elec: electronics product reviews Elec consists
of electronic product reviews. It is part of a large
Amazon review dataset (McAuley and Leskovec,
2013). We chose electronics as it seemed to be very
different from movies. Following the generation of
IMDB (Maas et al., 2011), we chose the training set
and the test set so that one half of each set consists
of positive reviews and the other half is negative, re-
garding rating 1 and 2 as negative and 4 and 5 as
positive, and that the reviewed products are disjoint
between the training set and test set. Note that to
extract text from the original data, we only used the
text section, and we did not use the summary sec-
tion. This way, we obtained a test set of 25K reviews
(same as IMDB) and training sets of various sizes.
The training and test sets are available on the inter-
net12. Data preprocessing was the same as IMDB.

RCV1: topic categorization RCV1 is a corpus
of Reuters news articles as described in LYRL04
(Lewis et al., 2004). RCV1 has 103 topic categories
in a hierarchy, and one document may be associated
with more than one topic. Performance on this task
(multi-label categorization) is known to be sensitive
to thresholding strategies, which are algorithms ad-
ditional to the models we would like to test. There-
fore, we also experimented with single-label cate-

12riejohnson.com/cnn_data.html

label #train #test #class
Table 2 single 15,564 49,838 55
Fig. 6 single varies 49,838 55

Table 4 multi 23,149 781,265 103

Table 1: RCV1 data summary.

gorization to assign one of 55 second-level topics
to each document to directly evaluate models. For
this task, we used the documents from a one-month
period as the test set and generated various sizes of
training sets from the documents with earlier dates.
Data sizes are shown in Table 1. As in LYRL04, we
used the concatenation of the headline and text ele-
ments. Data preprocessing was the same as IMDB
except that we used the stopword list provided by
LYRL04 and regarded numbers as stopwords.

3.5 Performance results

Table 2 shows the error rates of CNN in comparison
with the baseline methods. The first thing to note
is that on all the datasets, the best-performing CNN
outperforms the baseline methods, which demon-
strates the effectiveness of our approach.

To look into the details, let us first focus on CNN
with one convolution layer (seq- and bow-CNN in
the table). On sentiment classification (IMDB and
Elec), the configuration chosen by model selection
was: region size 3, stride 1, 1000 weight vectors, and
max-pooling with one pooling unit, for both types
of CNN; seq-CNN outperforms bow-CNN, as well
as all the baseline methods except for one. Note
that with a small region size and max-pooling, if a
review contains a short phrase that conveys strong
sentiment (e.g., “A great movie!”), the review could
receive a high score irrespective of the rest of the re-
view. It is sensible that this type of configuration is
effective on sentiment classification.

By contrast, on topic categorization (RCV1), the
configuration chosen for bow-CNN by model selec-
tion was: region size 20, variable-stride≥2, average-
pooling with 10 pooling units, and 1000 weight vec-
tors, which is very different from sentiment classifi-
cation. This is presumably because on topic clas-
sification, a larger context would be more predic-
tive than short fragments (→ larger region size),
the entire document matters (→ the effectiveness of
average-pooling), and the location of predictive text
also matters (→ multiple pooling units). The last

108



point may be because news documents tend to have
crucial sentences (as well as the headline) at the be-
ginning. On this task, while both seq and bow-CNN
outperform the baseline methods, bow-CNN outper-
forms seq-CNN, which indicates that in this setting
the merit of having fewer parameters is larger than
the benefit of keeping word order in each region.

Now we turn to parallel CNN. On IMDB, seq2-
CNN, which has two seq-convolution layers (region
size 2 and 3; 1000 neurons each; followed by one
unit of max-pooling each), outperforms seq-CNN.
With more neurons (3000 neurons each; Table 3) it
further exceeds the best-performing baseline, which
is also the best previous supervised result. We pre-
sume the effectiveness of seq2-CNN indicates that
the length of predictive text regions is variable.

The best performance 7.67 on IMDB was ob-
tained by ‘seq2-bown-CNN’, equipped with three
layers in parallel: two seq-convolution layers (1000
neurons each) as in seq2-CNN above and one layer
(20 neurons) that regards the entire document as one
region and represents the region (document) by a
bag-of-n-gram vector (bow3) as input to the compu-
tation unit; in particular, we generated bow3 vectors
by multiplying the NB-weights with binary vectors,
motivated by the good performance of NB-LM. This
third layer is a bow-convolution layer13 with one re-
gion of variable size that takes one-hot vectors with
n-gram vocabulary as input to learn document em-
bedding. The seq2-bown-CNN for Elec in the ta-
ble is the same except that the regions sizes of seq-
convolution layers are 3 and 4. On both datasets,
performance is improved over seq2-CNN. The re-
sults suggest that what can be learned through these
three layers are distinct enough to complement each
other. The effectiveness of the third layer indicates
that not only short word sequences but also global
context in a large window may be useful on this task;
thus, inclusion of a bow-convolution layer with n-
gram vocabulary with a large fixed region size might
be even more effective, providing more focused con-
text, but we did not pursue it in this work.

Baseline methods Comparing the baseline meth-
ods with each other, on sentiment classification, re-
ducing the vocabulary to the most frequent n-grams

13It can also be regarded as a fully-connected layer that takes
bow3 vectors as input.

methods IMDB Elec RCV1
SVM bow3 (30K) 10.14 9.16 10.68
SVM bow1 (all) 11.36 11.71 10.76
SVM bow2 (all) 9.74 9.05 10.59
SVM bow3 (all) 9.42 8.71 10.69
NN bow3 (all) 9.17 8.48 10.67
NB-LM bow3 (all) 8.13 8.11 13.97
bow-CNN 8.66 8.39 9.33
seq-CNN 8.39 7.64 9.96
seq2-CNN 8.04 7.48 –
seq2-bown-CNN 7.67 7.14 –

Table 2: Error rate (%) comparison with bag-of-n-gram-
based methods. Sentiment classification on IMDB and
Elec (25K training documents) and 55-way topic cate-
gorization on RCV1 (16K training documents). ‘(30K)’
indicates that the 30K most frequent n-grams were used,
and ‘(all)’ indicates that all the n-grams (up to 5M) were
used. CNN used the 30K most frequent words.

SVM bow2 [WM12] 10.84 –
WRRBM+bow [DAL12] 10.77 –
NB+SVM bow2 [WM12] 8.78 ensemble
NB-LM bow3 [MMRB14] 8.13 –
Paragraph vectors [LM14] 7.46 unlabeled data
seq2-CNN (3K×2) [Ours] 7.94 –
seq2-bown-CNN [Ours] 7.67 –

Table 3: Error rate (%) comparison with previous best
methods on IMDB.

notably hurt performance (also observed on NB-LM
and NN) even though some reduction is a common
practice. Error rates were clearly improved by ad-
dition of bi- and tri-grams. By contrast, on topic
categorization, bi-grams only slightly improved ac-
curacy, and reduction of vocabulary did not hurt per-
formance. NB-LM is very strong on IMDB and
poor on RCV1; its effectiveness appears to be data-
dependent, as also observed by WM12.

Comparison with state-of-the-art results As
shown in Table 3, the previous best supervised result
on IMDB is 8.13 by NB-LM with bow3 (MMRB14),
and our best error rate 7.67 is better by nearly 0.5%.
(Le and Mikolov, 2014) reports 7.46 with the semi-
supervised method that learns low-dimensional vec-
tor representations of documents from unlabeled
data. Their result is not directly comparable with our
supervised results due to use of additional resource.
Nevertheless, our best result rivals their result.

We tested bow-CNN on the multi-label topic
categorization task on RCV1 to compare with

109



models micro-F macro-F
LYRL04’s best SVM 81.6 60.7

bow-CNN 84.0 64.8

Table 4: RCV1 micro-averaged and macro-averaged F-
measure results on multi-label task with LYRL04 split.

LYRL04. We used the same thresholding strategy as
LYRL04. As shown in Table 4, bow-CNN outper-
forms LYRL04’s best results even though our data
preprocessing is much simpler (no stemming and no
tf-idf weighting).

Previous CNN We focus on the sentence classifi-
cation studies due to its relation to text categoriza-
tion. Kim (2014) studied fine-tuning of pre-trained
word vectors to produce input to parallel CNN. He
reported that performance was poor when word vec-
tors were trained as part of CNN training (i.e., no ad-
ditional method/corpus). On our tasks, we were also
unable to outperform the baselines with this type of
model. Also, with our approach, a system is sim-
pler with one fewer layer – no need to tune the di-
mensionality of word vectors or meta-parameters for
word vector learning.

Kalchbrenner et al. (2014) proposed complex
modifications of CNN for sentence modeling. No-
tably, given word vectors ∈ Rd, their convolution
with m feature maps produces for each region a ma-
trix ∈ Rd×m (instead of a vector ∈ Rm as in stan-
dard CNN). Using the provided code, we found that
their model is too resource-demanding for our tasks.
On IMDB and Elec14 the best error rates we ob-
tained by training with various configurations that
fit in memory for 24 hours each on GPU (cf. Fig 5)
were 10.13 and 9.37, respectively, which is no bet-
ter than SVM bow2. Since excellent performances
were reported on short sentence classification, we
presume that their model is optimized for short sen-
tences, but not for text categorization in general.

Performance dependency CNN training is
known to be expensive, compared with, e.g., linear
models – linear SVM with bow3 on IMDB only
takes 9 minutes using SVMlight (single-core) on a
high-end Intel CPU. Nevertheless, with our code on
GPU, CNN training only takes minutes (to a few
hours) on these datasets shown in Figure 5.

14We could not train adequate models on RCV1 on either
Tesla K20 or M2070 due to memory shortage.

7

7.5

8

8.5

0 20 40 60

E
r
r
o

r
 r

a
te

 (
%

)

minutes

Elec (25K)

seq2-CNN

seq2-bown

7.5

8

8.5

9

9.5

0 50 100

E
r
r
o

r
 r

a
te

 (
%

)

minutes

IMDB (25K)
seq-CNN

seq2-bown

9

9.5

10

10.5

11

0 10 20 30 40

E
r
r
o

r
 r

a
te

 (
%

)

minutes

RCV1 (16K)

bow-CNN

80

CNN

bown

Figure 5: Training time (minutes) on Tesla K20. The
horizontal lines are the best-performing baselines.

5

6

7

8

9

10

11

12

5000 50000

E
rr

o
r 

ra
te

 (
%

)

Training data size (log-scale)

Elec
SVM bow2 (all)
SVM bow3 (all)
NB-LM bow3 (all)
seq2-CNN

E
rr

o
r 

ra
te

 (
%

)

6
7
8
9

10
11
12
13
14
15
16
17

2000 20000

E
rr

o
r 

ra
te

 (
%

)

Training data size (log-scale)

RCV1

SVM bow1 (all)
SVM bow2 (all)
SVM bow3 (all)
bow-CNN

Figure 6: Error rate in relation to training data size. For
readability, only representative methods are shown.

Finally, the results with training sets of various
sizes on Elec and RCV1 are shown in Figure 6.

3.6 Why is CNN effective?

In this section we explain the effectiveness of CNN
through looking into what it learns from training.

First, for comparison, we show the n-grams that
SVM with bow3 found to be the most predictive;
i.e., the following n-grams were assigned the 10
largest weights by SVM with binary features on Elec
for the negative and positive class, respectively:

• poor, useless, returned, not worth, return, worse,
disappointed, terrible, worst, horrible
• great, excellent, perfect, love, easy, amazing, awe-

some, no problems, perfectly, beat

Note that, even though SVM was also given bi- and
tri-grams, the top 10 features chosen by SVM with
binary features are mostly uni-grams; furthermore,
the top 100 features (50 for each class) include 28
bi-grams but only four tri-grams. This means that,
with the given size of training data, SVM still heav-
ily counts on uni-grams, which could be ambiguous,
and cannot fully take advantage of higher-order n-
grams. By contrast, NB-weights tend to promote n-
grams with a larger n; the 100 features that were as-
signed the largest NB-weights are 7 uni-, 33 bi-, and
60 tri-grams. However, as seen above, NB-weights
do not always lead to the best performance.

110



N1 completely useless ., return policy .
N2 it won’t even, but doesn’t work
N3 product is defective, very disappointing !
N4 is totally unacceptable, is so bad
N5 was very poor, it has failed
P1 works perfectly !, love this product
P2 very pleased !, super easy to, i am pleased
P3 ’m so happy, it works perfect, is awesome !
P4 highly recommend it, highly recommended !
P5 am extremely satisfied, is super fast

Table 5: Examples of predictive text regions in the train-
ing set.

In Table 5, we show some of text regions learned
by seq-CNN to be predictive on Elec. This net has
one convolution layer with region size 3 and 1000
neurons; thus, embedding by the convolution layer
produces a 1000-dim vector for each region, which
(after pooling) serves as features in the top layer
where weights are assigned to the 1000 vector com-
ponents. In the table, Ni/Pi indicates the component
that received the i-th highest weight in the top layer
for the negative/positive class, respectively. The ta-
ble shows the text regions (in the training set) whose
embedded vectors have a large value in the corre-
sponding component, i.e., predictive text regions.

Note that the embedded vectors for the text re-
gions listed in the same row are close to each other
as they have a large value in the same component.
That is, Table 5 also shows that the proximity of
the embedded vectors tends to reflect the proximity
in terms of the relations to the target classes (pos-
itive/negative sentiment). This is the effect of em-
bedding, which helps classification by the top layer.

With the bag-of-n-gram representation, only the
n-grams that appear in the training data can partici-
pate in prediction. By contrast, one strength of CNN
is that n-grams (or text regions of size n) can con-
tribute to accurate prediction even if they did not
appear in the training data, as long as (some of)
their constituent words did, because input of embed-
ding is the constituent words of the region. To see
this point, in Table 6 we show the text regions from
the test set, which did not appear in the training
data, either entirely or partially as bi-grams, and yet
whose embedded features have large values in the
heavily-weighted (predictive) component thus con-
tributing to the prediction. There are many more of
these, and we only show a small part of them that

were unacceptably bad, is abysmally bad, were uni-
versally poor, was hugely disappointed, was enor-
mously disappointed, is monumentally frustrating,
are endlessly frustrating
best concept ever, best ideas ever, best hub ever,
am wholly satisfied, am entirely satisfied, am in-
credicbly satisfied, ’m overall impressed, am aw-
fully pleased, am exceptionally pleased, ’m entirely
happy, are acoustically good, is blindingly fast,

Table 6: Examples of text regions that contribute to
prediction. They are from the test set, and they did not
appear in the training set, either entirely or partially as
bi-grams.

fit certain patterns. One noticeable pattern is (be-
verb, adverb, sentiment adjective) such as “am en-
tirely satisfied” and “’m overall impressed”. These
adjectives alone could be ambiguous as they may be
negated. To know that the writer is indeed “satis-
fied”, we need to see the sequence “am satisfied”,
but the insertion of adverb such as “entirely” is very
common. “best X ever’ is another pattern that a dis-
criminating pair of words are not adjacent to each
other. These patterns require tri-grams for disam-
biguation, and seq-CNN successfully makes use of
them even though the exact tri-grams were not seen
during training, as a result of learning, e.g., “am X
satisfied” with non-negative X (e.g., “am very satis-
fied”, “am so satisfied”) to be predictive of the pos-
itive class through training. That is, CNN can ef-
fectively use word order when bag-of-n-gram-based
approaches fail.

4 Conclusion

This paper showed that CNN provides an alternative
mechanism for effective use of word order for text
categorization through direct embedding of small
text regions, different from the traditional bag-of-n-
gram approach or word-vector CNN. With the paral-
lel CNN framework, several types of embedding can
be learned and combined so that they can comple-
ment each other for higher accuracy. State-of-the-art
performances on sentiment classification and topic
classification were achieved using this approach.

Acknowledgements

We thank the anonymous reviewers for useful sug-
gestions. The second author was supported by NSF
IIS-1250985 and NSF IIS-1407939.

111



References
Christopher Bishop. 1995. Neural networks for pattern

recognition. Oxford University Press.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.

Biographies, bollywood, boom-boxes, and blenders:
Domain adaptation for sentiment classification. In
Proceedings of ACL.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.

Jianfeng Gao, Patric Pantel, Michael Gamon, Xiaodong
He, and Li dent. 2014. Modeling interestingness with
deep neural networks. In Proceedings of EMNLP.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of ICML.

Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R. Salakhutdinov.
2012. Improving neural networks by preventing
co-adaptation of feature detectors. arXiv:1207.0580.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
eling sentences. In Proceedings of ACL.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of EMNLP, pages
1746–1751.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
2012. ImageNet classification with deep convolutional
neural networks. In Proceedings of NIPS.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of ICML.

Yann LeCun, León Bottou, Yoshua Bengio, and Patrick
Haffner. 1986. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE,
pages 2278–2324.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.
2004. RCV1: A new benchmark collection for text
categorization research. Journal of Marchine Learn-
ing Research, 5:361–397.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of ACL.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In RecSys.

Grégoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-
zato, and Yoshua Bengio. 2014. Ensemble of genera-
tive and discriminative techniques for sentiment analy-

sis of movie reviews. arXiv:1412.5335v5 (4 Feb 2015
version).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Proceedings of NIPS.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1–2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 79–86.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2014. Im-
ageNet Large Scale Visual Recognition Challenge.
arXiv:1409.0575.

Mehran Sahami, Susan Dumais, David Heckerman, and
Eric Horvitz. 1998. A bayesian approach to filtering
junk e-mail. In Proceedings of AAAI’98 Workshop on
Learning for Text Categorization.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and
Grégoire Mensnil. 2014. A latent semantic model
with convolutional-pooling structure for information
retrieval. In Proceedings of CIKM.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2014. Going deeper with convolutions.
arXiv:1409.4842.

Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee.
2002. The use of bigrams to enhance text catego-
rization. Information Processing and Management,
38:529–546.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proceedings of ACL, pages 1555–1565.

Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of ACL (short paper).

Jason Weston, Sumit Chopra, and Keith Adams. 2014.
#tagspace: Semantic embeddings from hashtags. In
Proceedings of EMNLP, pages 1822–1827.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neu-
ral network based triangular crf for joint intent detec-
tion and slot filling. In ASRU.

Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. 2014.
Product feature mining: Semantic clues versus syntac-
tic constituents. In Proceedings of ACL.

112


