



















































Data-Anonymous Encoding for Text-to-SQL Generation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5405â€“5414,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

5405

Data-Anonymous Encoding for Text-to-SQL Generation

Zhen Dong1âˆ—, Shizhao Sun2, Hongzhi Liu1, Jian-Guang Lou2 and Dongmei Zhang2
1Peking University 2Microsoft Research
{zhendong,liuhz}@pku.edu.cn

{shizsu,jlou,dongmeiz}@microsoft.com

Abstract

On text-to-SQL generation, the input utterance
usually contains lots of tokens that are related
to column names or cells in the table, called
table-related tokens. These table-related to-
kens are troublesome for the downstream neu-
ral semantic parser because it brings com-
plex semantics and hinders the sharing across
the training examples. However, existing ap-
proaches either ignore handling these tokens
before the semantic parser or simply use de-
terministic approaches based on string-match
or word embedding similarity. In this work,
we propose a more efficient approach to han-
dle table-related tokens before the semantic
parser. First, we formulate it as a sequen-
tial tagging problem and propose a two-stage
anonymization model to learn the semantic re-
lationship between tables and input utterances.
Then, we leverage the implicit supervision
from SQL queries by policy gradient to guide
the training. Experiments demonstrate that our
approach consistently improves performances
of different neural semantic parsers and signif-
icantly outperforms deterministic approaches.

1 Introduction

Interacting with relational databases or tables
through natural language is an important and chal-
lenging problem (Androutsopoulos et al., 1995;
Li and Jagadish, 2014; Pasupat and Liang, 2015;
Zhong et al., 2017; Yu et al., 2018c). To tackle
this problem, the neural semantic parser has been
widely studied (Dong and Lapata, 2016; Jia and
Liang, 2016; Herzig and Berant, 2017; Dong
and Lapata, 2018), which automatically maps
the input natural language into the logical form
(e.g., SQL query) following the typical encoder-
decoder structure (Hochreiter and Schmidhuber,
1997; Bahdanau et al., 2015; Vinyals et al., 2015).

âˆ—This work was done when the author was visiting Mi-
crosoft Research Asia.

Semantic parsing usually tackles two kinds of
problems (Goldman et al., 2018; Herzig and Be-
rant, 2018), i.e., the lexical problem and the struc-
tural problem. On text-to-SQL generation, the lex-
ical problem refers to mapping tokens in input ut-
terances to constants in SQL queries, e.g., the col-
umn names or cells in SQL queries. The structural
problem refers to mapping intentions conveyed by
input utterances to operators in SQL queries, e.g.,
the aggregators or the existence of WHERE clause
in SQL queries. Intuitively, the lexical problem
can be formulated as a sequential tagging problem,
called anonymization, where each token in the in-
put utterance will be tagged as being related to a
column name, a cell or nothing. For ease of refer-
ence, we call the tokens in input utterances related
to column names or cells table-related tokens, and
the tagged utterance anonymous utterance.

If the lexical problem can be reduced before the
neural semantic parsing, the training difficulties
will be greatly alleviated. The reason is two-fold.
First, by anonymizing table-related tokens in the
input utterance before the neural semantic parser,
we can conceal the complex semantics of table-
related tokens. Second, different input utterances,
which have different table-related tokens but the
same structure, can be reduced to the same anony-
mous utterance before they are fed into the parser.
This will result in sharing across training data and
thus alleviate training difficulties (Goldman et al.,
2018; Herzig and Berant, 2018). For example, in
Figure 1a and 1b, input utterances (denoted as x)
seemingly ask different questions about country
and college team, but they can be reduced to a sim-
ilar anonymous utterance (denoted as xÌƒ) by replac-
ing the token related to the column name as c and
the token related to the cell as v.

However, reducing the lexical problem before
the neural semantic parser is far from being well-
studied on text-to-SQL generation although it



5406

ğ‘¥ What country does the goaltender
come from?

à·¤ğ‘¥ What ğ’„ does the ğ’— come from?

ğ‘¦ SELECT Nationality WHERE 
Position=Goaltender

Pick # Player Position Nationa
lity

â€¦

83 Wayne 
Wood

Goaltender Canada â€¦

â€¦ â€¦ â€¦ â€¦ â€¦

(a) Example A

ğ‘¥ What college team did Ryan 
Golden come from?

à·¤ğ‘¥ What ğ’„ did ğ’— come from?

ğ‘¦ SELECT College/junior/club team 
WHERE Player=Ryan Golden

Pick Player Position â€¦ College/junior
/club team

181 Ryan 
Golden

Left 
Wing

â€¦ HS-
Massachusetts

â€¦ â€¦ â€¦ â€¦ â€¦

(b) Example B

ğ‘¥ What school did player number 6
come from?

à·¤ğ‘¥ What ğ’„ did player ğ’„ ğ’— come from?

ğ‘¦ SELECT School/Club Team WHERE 
No.=6

Player No. Nationality â€¦ School/C
lub Team

Alan 
Anderson

6 United 
States

â€¦ Michigan 
State

â€¦ â€¦ â€¦ â€¦ â€¦

(c) Example C

Figure 1: Examples for anonymizing table-related tokens on WikiSQL. For each subfigure, the upper box shows
the utterance and the SQL query, while the lower box shows the table. In the upper box, x denotes the input
utterance, xÌƒ denotes the anonymous utterance and y denotes the SQL query. In anonymous utterance xÌƒ, we use c
and v to replace the tokens that are related to column names and cells respectively. In addition, we use the color to
indicate the correspondence of column names or cells among x, xÌƒ and the table.

has been demonstrated to be helpful on other
tasks (Goldman et al., 2018; Herzig and Berant,
2018). First, most approaches ignores reducing
the lexical problem (Zhong et al., 2017; Xu et al.,
2017; Dong and Lapata, 2018; Shi et al., 2018;
Wang et al., 2018a; Hwang et al., 2019). Without
conducting anonymization before the neural se-
mantic parser, these approaches cannot receive the
aforementioned benefit, although they may have
modules to predict the presence of column names
and cells inside the parser. Second, a few studies
have tried the anonymization by deterministic ap-
proaches, which compare the similarity based on
string-match or word embedding between the in-
put utterance and the table (Wang et al., 2018b; Yu
et al., 2018a). These approaches cannot fully un-
derstand the semantic relationship between input
utterances and tables, and ignore the relationship
with components of SQL queries. For example, in
Figure 1c, although the token â€˜playerâ€™ is similar to
the column name â€˜Playerâ€™, it should not be tagged
as a table-related token because the token â€playerâ€
is just a non-sense demonstrative pronoun and the
column â€™Playerâ€™ is not related to the SQL query.

To this end, we propose to use the learning-
based approach to reduce the lexical problem, i.e.,
anonymize the table-related tokens. First, we pro-
pose a two-stage anonymization model to learn
the semantic relationship between input utterances
and tables. Then, considering that there is no la-
beled data for the anonymization, we propose to
extract the set of column names and cells appear-
ing in the SQL query and use this set as the super-

vision. Another benefit of leveraging such implicit
supervision is that we can make the anonymization
model consider the relationship with the compo-
nents of the SQL query when it learns the semantic
relationship, and thus avoid suffering the same dis-
advantage as deterministic approaches. Further-
more, to bridge the gap that our model is a sequen-
tial tagging model while the supervision extracted
from the SQL query is an unordered set, we lever-
age the policy gradient (Williams, 1992) to train
the model. Moreover, we train the anonymization
model and the neural semantic parser as whole
by a varational inference-like framework with the
anonymous utterance as the hidden variable.

Experimental results demonstrate that our ap-
proach can consistently improve performances of
different neural semantic parsers and significantly
outperform typical deterministic approach on the
anonymization problem.

2 Related Work

Semantic Parsing. Semantic parsing aims to
map natural languages into executable programs.
In the area of semantic parsing, the programs
could be in various types, e.g., Î»-calculus (Zettle-
moyer and Collins, 2005), Python (Oda et al.,
2015), SQL (Zhong et al., 2017), etc; the source
of the knowledge can also be different, e.g., the
knowledge base, the table, the image (Suhr et al.,
2017), etc; and the supervision can take different
forms as well, e.g., question-denotation pairs (Pa-
supat and Liang, 2015), question-program pairs,



5407

etc. In this work, we focus on text-to-SQL genera-
tion with the table as the source of the knowledge
and with question-SQL query pairs as supervision.

Reducing Lexical Problem. On tasks other
than text-to-SQL generation, some studies have
tried to reduce lexical problem by anonymizing
tokens that are related to the program constants.
Goldman et al. (2018) lifts tokens in utterances to
an abstract form by referring to fixed mappings on
visual reasoning task. Herzig and Berant (2018)
employs a rule-based method to transform content
words in utterances to an abstract representation.
Although these studies inspire us to consider re-
ducing lexical problem on text-to-SQL generation,
the rules they employed cannot be directly applied
to text-to-SQL generation.

On text-to-SQL generation, a few studies tried
to anonymize table-related tokens although they
do not aim at reducing lexical problem. To bet-
ter understand rare entities, Yu et al. (2018a) used
string match to recognize column names. To learn
a domain-transferable semantic parser, Wang et al.
(2018b) detected column names by measuring the
closeness of edit distance and word embedding.
As discussed in Section 1, these deterministic ap-
proaches can not fully understand the semantic
relationship between input utterances and tables,
and ignore the relationship with components of
SQL queries.

Entity Linking. Our approach can be regarded
as an implementation of entity linking on the con-
crete task, because general entity linking (Shen
et al., 2015; Kolitsas et al., 2018) approaches fail
to handle particular challenges in our scenario. On
the one hand, there are lots of cases cannot be
handled by the deterministic entity linking method
which only relies on measuring the similarity; on
the other hand, there is no labeled data for the
learning-based entity linking method.

3 Problem Formulation

Denote the input utterance as x = x1 . . . x|x| and
the corresponding SQL query as y = y1 . . . y|y|.
We formulate the lexical problem as a sequen-
tial tagging problem. Formally, for each token
xt in the input utterance, we give it a tag xÌƒt âˆˆ
{COL1, . . . ,COLK ,CELL,UNK}, where COLk
represents that xt is related to the k-th column
name in the table, CELL represents that xt is re-
lated to a cell in the table and UNK represents that

ğ’™ğŸ ğ’™ğ’• ğ’™ğ‘»

à·¥ğ’™ğŸ à·¥ğ’™ğ’• à·¥ğ’™ğ‘»

ğ’ˆğŸ ğ’ˆğ’• ğ’ˆğ‘»

Neural Semantic Parsing

ğ’šğŸ ğ’šğ’• ğ’šğ‘»

â€¦ â€¦

â€¦ â€¦

â€¦ â€¦

â€¦ â€¦

Anonymization Model

ğ’‘(ğ’š|à·¥ğ’™, ğ’™)

ğ’‘(à·¥ğ’™|ğ’™)

Figure 2: Framework.

xt is not related to the table. Here K is the num-
ber of column names in the table. Note that in-
dexes of column names cannot be ignored in the
algorithm to better leverage the implicit supervi-
sion from SQL queries (see Section 4.2) although
it is ignored when giving examples in Figure 1 and
Table 4 for ease of read. For ease of reference, we
call this sequential tagging problem anonymiza-
tion and the tagged sequence xÌƒ = xÌƒ1 . . . xÌƒ|x|
anonymous utterance.

The typical neural semantic parser aims to esti-
mate p (y|x). In our work, we decompose p (y|x)
into two processes (as shown in Figure 2), i.e.,

p (y|x) =
âˆ‘
xÌƒ

p (y|x, xÌƒ) p (xÌƒ|x) . (1)

Specifically, one process is to learn the anonymous
utterance xÌƒ given the input utterance x for the pur-
pose of reducing lexical problem, i.e.,

p (xÌƒ|x) =
âˆ
t

p (xÌƒt|x) , (2)

while the other process is to learn the neural se-
mantic parser with anonymous utterance xÌƒ as the
additional input, i.e.,

p (y|x, xÌƒ) =
âˆ
t

p (yt|y<t, x, xÌƒ) . (3)

In the following, we will discuss how to learn
the anonymous utterance (Eqn (2)) and how to
learn the neural semantic parser with anonymous
utterance as additional input (Eqn (1)) in detail.

4 Approach

4.1 Anonymization Model
The anonymization is a structural problem by na-
ture. First, we need to differentiate whether the to-
ken is related column names, cells or not related to



5408

Input Encoder

ğ‘¸ğŸ ğ‘¸ğ’• ğ‘¸ğ‘» ğ‘ªğŸ ğ‘ªğ’Œ ğ‘ªğ‘²â€¦ â€¦ â€¦ â€¦

Column Aggregation

ğ‘ªğ’•
ğ‘¸

Channel Selection Column Binding

what goaltender ?â€¦ â€¦ [SEP] Pick# Player Nationalityâ€¦ â€¦

Input utterance Column names

ğ‘¬ ğ‘ºğ‘¬ğ‘·

Figure 3: Illustration of the anonymization model when
the table content is not available. We take the t-th token
in the input utterance as the example.

the table. Second, we need to further recognize the
concrete column/cell that the token is related to.
Therefore, we design a model that uses two stages,
i.e., channel selection stage and the column/cell
binding stage, to tackle these two subproblems re-
spectively. The probability distributions produced
by these two stages together determine the result,

p (xÌƒt|x) = (4)âˆ‘
at

pchannel (at|x) pbinding (xÌƒt|at, x) ,

where at âˆˆ {COL,CELL,UNK} is the se-
lected channel indicating that the token is related
to column names, cells or nothing respectively,
pchannel (Â·) is the probability produced by the chan-
nel selection stage, and pbinding (Â·) is the probabil-
ity produced by the column/cell binding stage.

For ease of reference, we call the two-stage
model anonymization model, and illustrate it in
Figure 3. In the following, we introduce details
of its different components.

Input Encoder. We leverage BiLSTM or
BERT (Devlin et al., 2018) to encode both the
input utterance and the table. Due to the privacy
problem, the table content is not allowed to be
used on most text-to-SQL tasks (Zhong et al.,
2017; Yu et al., 2018c). Therefore, we use the
concatenation of the embedding of the tokens
in the input utterance, the embedding of the
separator and the embedding of the column
names in the table as the input for BiLSTM or
BERT. We denote the output of input encoder
as {Q1, . . . , QT , E[SEP], C1, . . . , CK}, where

Qt, t âˆˆ {1, . . . , T} is the vector for the encoding
of the t-th token in the input utterance, E[SEP] is
the vector for the encoding of the separator, and
Ck, k âˆˆ {1, . . . ,K} is the vector for the encoding
of the k-th column name in the table. Note that we
run a BiLSTM or BERT between column names
instead of over each column name. The reason is
that Yu et al. (2018a) have shown that although the
order of column names does not matter, running
a BiLSTM or BERT between colomn names can
capture relationships between them, which will
benefit the accuracy and the training time.

Channel Selection. We implement a linear gate
with encodings of the input utterance and aggre-
gated encodings of column names as input:

pchannel (at|x) = softmax
(
Wgate

[
Qt;C

Q
t

])
,

(5)

where Wgate stands for learnable parameters, C
Q
t

is the aggregated encoding of column names, and
[Qt;C

Q
t ] is the concatenation of Qt and C

Q
t .

Specifically, to obtain the most relevant col-
umn names for the t-th token in the input utter-
ance, we leverage attention mechanism (Bahdanau
et al., 2015) towards column names {C1, . . . , CK}
to compute the aggregated encoding of the column
names CQt , i.e., C

Q
t =

âˆ‘K
k=1 Î±

k
tCk, where Î±

k
t is

obtained through ukt = v1 tanh (W1Qt +W2Ck)
and Î±kt = softmax

(
ukt
)
, k âˆˆ {1, . . . ,K}. Here,

v1, W1 and W2 are learnable parameters.

Column/Cell Binding1. The probability distri-
bution generated by this stage, i.e., pbinding(Â·), can
be categorized into three types, i.e., pCOLbinding(Â·),
pCELLbinding(Â·) and pUNKbinding(Â·), corresponding to the
three channels in the first stage respectively. Ob-
viously, pUNKbinding (xÌƒt|at, x) = 1.

For column names, we produce a probabil-
ity distribution over the K columns in the table
through measuring the relevance between the t-th
token and the k-th column name:

pCOLkbinding (xÌƒt|at, x) = softmax
(
zkt

)
, (6)

zkt = v2 tanh (W3Qt +W4Ck) , k âˆˆ {1, . . . ,K},

where v2, W3 and W4 are learnable parameters.

1For the situation that the input utterance contains table
names or the table content is allowed to be used, we can han-
dle table names or cells by the similar way of column names.



5409

For cells, considering that table content is not
available due to the privacy problem, we sim-
ply predict a substring from the input utterance.
Specifically, we follow the longest match principle
to merge consecutive tokens in the input utterance,
which are labeled as CELL by the channel selec-
tion stage, to one cell. Therefore, the generated by
column/cell binding stage is simply set as 1, i.e.,
pCELLbinding (xÌƒt|at, x) = 1.

4.2 Implicit Supervision from SQL Queries
To train the anonymization model, the ground
truth for the anonymous utterance is indispens-
able. However, there is no such labeled data, and
manually labeling the whole training data for each
text-to-SQL task is unrealistic, especially when
the amount of training data is tremendous.

To tackle this problem, we propose to extract
the set of column names and cells appearing in
the SQL query and use this set as the supervi-
sion to guide the training of the anonymization
model. Another benefit of such approach is that
we can make our model consider the relationship
with components of the SQL query, and thus avoid
suffering the same trouble as deterministic ap-
proaches. Concretely, for each SQL query y, we
denote the set of column names and cells appear-
ing in it as SSQL, which consists of three parts, i.e.,
SSQL = Ssel col âˆª Sother col âˆª Scell:
1. Ssel col is the set of column names appearing in
the SELECT clause;
2. Sother col is the set of column names appearing
in other clauses;
3. Scell is the set of cells appearing in the whole
SQL query.

4.3 Policy Gradient for Sequential Tagging
Maximizing Expected Reward. Ideally, if we
have the ground truth, we can train the anonymiza-
tion model by minimizing the gap (e.g., KL di-
vergence) between the predicted probabilities and
the ground truth. Unfortunately, it is infeasible be-
cause the implicit supervision from SQL queries
(i.e., SSQL) takes a form of the unordered set while
the anonymization model faces a sequential tag-
ging task. To address this problem, we propose to
encourage the set of column names and cells ap-
pearing in the predicted anonymous utterance, de-
noted as Spred, to be similar to that extracted from
the SQL query, i.e., SSQL. To this end, we define a
reward of the predicted anonymous utterance r(xÌƒ)
as the similarity between Spred and SSQL, and then

train the anonymization model by maximizing ex-
pected reward,

JER(Î¸) =
âˆ‘

(x,xÌƒ)âˆˆD

ExÌƒâˆ¼pÎ¸(Â·|x)r(xÌƒ), (7)

where D represents the set of training pairs.
However, directly computing expected reward

requires traversing all the possible anonymous ut-
terance xÌƒ, which is unrealistic. Therefore, we
leverage Monte Carlo estimate as the approxima-
tion of the expected reward. Concretely, we sam-
ple N anonymous utterances following probabil-
ity distribution generated by the anonymization
model, and then average the reward of the samples
to estimate the expected reward,

JER(Î¸) â‰ˆ
âˆ‘

(x,xÌƒ)âˆˆD

Nâˆ‘
j=1

1

N
r(xÌƒj), (8)

where xÌƒj denotes the j-th sample and xÌƒj âˆ¼
pÎ¸ (Â·|x).

To maximize the above objective func-
tion by gradient descent, we employ REIN-
FORCE (Williams, 1992) method, i.e,

âˆ‡Î¸JER(Î¸) â‰ˆ
âˆ‘

(x,xÌƒ)âˆˆD

Nâˆ‘
j=1

1

N
r(xÌƒj)âˆ‡Î¸ log pÎ¸

(
xÌƒj |x

)
.

(9)

Measurement of Reward. For ease of refer-
ence, we decompose Spred into two parts, i.e.,
Spred = Spred col âˆª Spred cell, where Spred col and
Spred cell represent the set of column names and the
set of cells appearing in the predicted anonymous
utterance respectively.

The measurement of the reward r(xÌƒ) is de-
signed based on the similarity between Spred and
SSQL by referring to the following principles:

1. The predicted anonymous utterance should
contain the column names in the SELECT clause
of the SQL query, i.e., Ssel col âŠ‚ Spred col. The
anonymization model will be punished when it
misses the column names in Ssel col. This is mo-
tivated by our preliminary analysis that almost ev-
ery column name in the SELECT clause has the
corresponding token in the input utterance.

2. The column names in the predicted anony-
mous utterance should be at least a subset of the
column names appearing in the SQL query, i.e.,
Spred col âŠ† Ssel col âˆª Sother col. Unlike Ssel col, the
column names appearing in other clauses of the



5410

SQL query may possible not have corresponding
tokens in the input utterance. For example, in
Figure 1a, the column name â€˜Positionâ€™ does not
have corresponding tokens in the input utterance.
Therefore, it is unreasonable to strictly force col-
umn names in the predicted anonymous utterance
to be the same as that appearing in the SQL query.
Instead, it is better to punish the anonymization
model when it predicts column names outside the
set of column names appearing in the SQL query.

3. The cell names in the predicted anony-
mous utterance should be the same as that appear-
ing in the SQL query, i.e., Spred cell = Scell. If
there is any missing or extra cell in Spred cell, the
anonymization model will be punished.

According to above principles, we design the re-
ward r(xÌƒ) as,
1. if Ssel col âŠ† Spred col, Spred cell = Scell and
Spred col âŠ† Ssel colâˆªSother col are all true, r(xÌƒ) = 1;
2. if one of Ssel col * Spred col, Spred cell 6= Scell and
Spred col * Ssel col âˆª Sother col is true, r(xÌƒ) = âˆ’1.

4.4 Training and Inference

To train the anonymization model and the parser as
a whole, we regard the anonymous utterance xÌƒ as
the hidden variable for the neural semantic parser
p(y|x). Then, maximizing the log likelihood of
p(y|x) is equivalent to maximizing its Evidence
Lower BOund (ELBO) (Kim et al., 2018), i.e.,

ELBO (Ï•, Î¸) = EqÎ¸(xÌƒ|x)
[
log

pÏ•(y, xÌƒ|x)
qÎ¸(xÌƒ|x)

]
(10)

One popular strategy to maximizing ELBO
is the coordinate ascent. Specifically, it itera-
tively executes following two steps (Neal and
Hinton, 1998): 1) variational E-step, which
maximizes ELBO w.r.t. Î¸ keeping Ï• fixed,
i.e., Î¸? = argminÎ¸ KL (qÎ¸(xÌƒ|x)â€–pÏ•(xÌƒ|y, x));
and 2) variational M-step, which maximizes
ELBO w.r.t. Ï• keeping Î¸ fixed, i.e., Ï•? =
argmaxÏ• EqÎ¸(xÌƒ|x) [log pÏ•(y, xÌƒ|x)].

In our scenario, Ï• and Î¸ refer to the learnable
parameters in the neural semantic parser and the
anonymization model respectively. For variational
E-step, it usually finds the best variational ap-
proximation to the true posterior (Neal and Hin-
ton, 1998). As discussed in Section 4.2, the true
posterior we can obtain is in the form of the un-
ordered set. Thus, we actually minimize the the
expected reward, i.e., JER(Î¸), instead of the KL di-
vergence (see Section 4.3). For variational M-step,

to save training time, we simply sample one exam-
ple greedily, which approximates to maximizing
log likelihood of pÏ•(y, xÌƒ|x), i.e., JMLE(Ï•). More-
over, since performing coordinate ascent on the
entire dataset is too expensive, the variational E-
step and M-step are usually performed over mini-
batches (Hoffman et al., 2013). To further improve
time efficiency, we optimize objectives of the vari-
ational E-step and M-step simultaneously instead
of alternatively. Thus, the actual objective is,

J(Ï•, Î¸) = (11)âˆ‘
(x,y)âˆˆD

log pÏ•(y|xÌƒ, x)ï¸¸ ï¸·ï¸· ï¸¸
JMLE(Ï•)

+
âˆ‘

(x,xÌƒ)âˆˆD

ExÌƒâˆ¼qÎ¸(Â·|x)r(xÌƒ)ï¸¸ ï¸·ï¸· ï¸¸
JER(Î¸)

,

where JMLE(Ï•) is the log likelihood of the gener-
ated SQL query given the input utterance and the
anonymous utterance, and JER(Î¸) is the expected
reward of the anonymous utterance given the input
utterance (see Eqn (8) for details.)

Specifically, when optimizing JMLE(Ï•), we use
the concatenation of the encoding of the input ut-
terance (denoted as e(xt)) and the encoding of
the anonymous utterance (denoted as h(xÌƒt)) as the
input for the neural semantic parser, i.e., gt =
[e(xt);h(xÌƒt)], t âˆˆ {1, . . . , T} (see Figure 2). For
e(xt), it is determined by the parser itself. For
h(xÌƒt), it is concatenated by two parts: 1) embed-
ding of the channel name, i.e., COL, CELL and
UNK, 2) embedding of index k when the channel
is COL, indicating that the t-th token is related to
the k-th column name in the table.

At test time, the prediction for input utterance
x is obtained by xÌ‚ = argmaxxÌƒâ€² p (xÌƒâ€²|x) and yÌ‚ =
argmaxyâ€² p (y

â€²|xÌ‚, x).

5 Experiments

5.1 Experimental Setup
We conduct experiments on the WikiSQL (Zhong
et al., 2017) and Spider (Yu et al., 2018c). Each
example consists of a natural language question, a
SQL query and a table. On WikiSQL, one ques-
tion is only related to one table; while on Spider,
one question is usually related to multiple tables.

Our model is implemented in PyTorch (Paszke
et al., 2017). The type of the input encoder in the
anonymization model (i.e., BiLSTM or BERT) is
set the same as that in the concrete parser. Em-
bedding vectors of the anonymous utterance are
initiated by GloVe (Pennington et al., 2014). We



5411

Method Dev (%) Test (%)

ACCQM ACCEX ACCQM ACCEX

Seq2SQL (Zhong et al., 2017)1 53.5 62.1 51.6 60.4
Seq2SQL + DAE 60.0(+6.5) 67.5(+5.4) 58.0(+6.4) 66.1(+5.7)

SQLNet (Xu et al., 2017) 63.2 69.8 61.3 68.0
SQLNet + DAE 64.6(+1.4) 71.1(+1.3) 63.4(+2.1) 70.0(+2.0)

TypeSQL (Yu et al., 2018a) 68.0 74.5 66.7 73.5
TypeSQL + DAE 68.6(+0.6) 74.5 67.8(+1.1) 74.1(+0.6)

SQLova (Hwang et al., 2019)2 80.3 85.8 79.4 85.2
SQLova + DAE 81.3(+1.0) 86.4(+0.6) 80.5(+1.1) 86.1(+0.9)

âˆ’ Supervision 81.0 86.1 80.0 85.4
âˆ’ Co-training 80.6 86.0 79.8 85.3

1 For Seq2SQL, we use Xu et al. (2017)â€™s version because it achieves better performance.
2 For SQLova, we use BERT-Base as the input encoder due to limited computation resources.

Table 1: Performance improvement of the neural semantic parser on WikiSQL.

Method Easy (%) Medium (%) Hard (%) Extra Hard (%) All (%)

SyntaxSQLNet (Yu et al., 2018b) 38.4 15.0 16.1 3.5 18.9
SyntaxSQLNet + DAE 39.6(+1.2) 18.2(+3.2) 20.7(+4.6) 7.6(+4.1) 22.1(+3.2)

SyntaxSQLNetAug (Yu et al., 2018b) 44.4 23.0 23.0 2.9 24.9
SyntaxSQLNetAug + DAE 44.8(+0.4) 27.0(+4.0) 24.1(+1.1) 5.9(+3.0) 27.4(+2.5)

Table 2: Performance improvement of the neural semantic parser on Spider with different hardness levels.

#Total #Distinct x #Distinct xÌƒ

Dev 8421 8387 5488
Test 15878 15828 9680

Table 3: The number of distinct input utterances and
distinct anonymous utterances on WikiSQL.

use the manually labeled training data (which is
10% of the whole training data) to initiate our
anonymization model and then optimize the en-
tire framework on the whole training data. All the
other hyperparameters, e.g., the learning rate, hy-
perparameters in ADAM optimizer, the number of
training epochs, etc., are tuned on the dev set2.

5.2 Performance of Neural Semantic Parsing
First, we show that reducing the lexical problem
through the anonymization model can improve the
performance of neural semantic parser.

To this end, we add the anonymization model
to typical neural semantic parsers as presented in
Section 4. We use â€˜[A]+DAEâ€™ to denote the neu-
ral semantic parser with the anonymization model,
where A stands for the original name of the con-
crete parser and DAE is the abbreviated name of
our approach, i.e., data-anonymous encoding.

2For Spider, we tune hyperparameters on training set and
test the model on dev set.

On WikiSQL, the performance is evaluated
by query-match accuracy (ACCQM) and execu-
tion accuracy (ACCEX), which measure accura-
cies of canonical representation and execution re-
sult matches between the predicted SQL and the
ground truth respectively (Yu et al., 2018a). Ta-
ble 1 shows the results. First, we can observe
that query match accuracy on test data can be im-
proved by 6.4% at most and 1.1% at least. Fur-
thermore, for TypeSQL, query match accuracy can
be further improved by 1.1% although it has used
string-match based approach to anonymize table-
related tokens. Moreover, we perform ablation
studies by 1) removing the supervision for the
anonymization model (denoted as â€˜-Supervisionâ€™
in Table 1), and 2) simply using the output of the
trained anonymization model as the input for the
parser without training them as a whole (denoted
as â€˜-Co-trainingâ€™ in Table 1). We can observe that
the performance improvement is limited without
supervision and co-training, indicating that both
of them are indispensable.

On Spider, the performance is evaluated by
exact-match accuracy on different difficulty levels
of SQL queries, i.e., easy, medium, hard and ex-
tra hard. (Yu et al., 2018c). Table 2 shows the re-
sults. First, the overall accuracy can be improved
by 3.2% and 2.5% respectively. Furthermore, per-



5412

xÌƒ which column have column of cell?

x which place [column] has a rank [column] of 71 [cell]?
which county [column] has a median household income [column] of $98,090 [cell] ?

xÌƒ what be column when column be cell?

x what is the inclination [column] when the alt name [column] is ops-1584 [cell]?
what was the district [column] when the reason for change [column] was died january 1, 1974 [cell]?

xÌƒ name column for cell

x name the candidates [column] for georgia 8 [cell]
name the party [column] for jack thomas brinkley [cell]

Table 4: Top frequent anonymous utterances on dev set of WikiSQL. The tokens with underlines are table-related
tokens. We use â€˜[column]â€™ and â€˜[cell]â€™ to differentiate whether the token is related to a column name or a cell.

Method Dev (%) Test (%)

ACCSC ACCOC ACCCE ACCSC ACCOC ACCCE

TypeSQL (Yu et al., 2018a) 75.9 92.9 âˆ’ 76.0 92.9 âˆ’
AnnotatedSeq2Seq (Wang et al., 2018b) 88.8 64.6 âˆ’ 88.8 63.6 âˆ’
DAE 92.6 93.6 86.7 92.0 93.7 86.2

Table 5: Performances of different anonymization models on WikiSQL.

formances on medium, hard and extra hard SQL
queries achieve more improvement than that on
easy SQL queries, indicating that our approach is
more helpful for solving complicated cases.

5.3 Performance of Reducing Input
Utterances

To further demonstrate the effectiveness of reduc-
ing the lexical problem, we show that different in-
put utterance can be reduced to the same anony-
mous utterance. To this end, we process input ut-
terances and anonymous utterances by 1) convert-
ing the characters to the lowercase, 2) lemmatizing
the tokens and 3) removing the articles.

Table 3 shows that by anonymizing table-
related tokens, the number of distinct utterances
is reduced from 8387 to 5488 on dev set and from
15828 to 9680 on test set. Furthermore, Table 4
shows three anonymous utterances that are most
frequent on dev set and examples of correspond-
ing input utterances. All of these indicate that
although input utterances can hardly be identical,
they often share the same anonymous utterance.

5.4 Performance of Anonymization Methods

In addition, we compare performances of differ-
ent anonymization methods, including 1) Type-
SQL, which uses exact string match to detect col-
umn names, 2) AnnotatedSeq2Seq, which detects
column names by setting a threshold for the edit

distance and the cosine similarity of word embed-
ding, and 3) DAE, our learning-based approach3.

Anonymization methods are evaluated by fol-
lowing metrics: 1) whether the column names
in SELECT clause is included in the pre-

diction (ACCSC =
âˆ‘Z
i=1 I(Sisel colâŠ†S

i
pred col)

Z ); 2)
whether there is wrongly predicted column names

(ACCOC =
âˆ‘Z
i=1 I(Sipred colâŠ†S

i
sel colâˆªS

i
other col)

Z ); and 3)
whether all the cells in the SQL query are correctly

predicted (ACCCE =
âˆ‘Z
i=1 I(Sipred cell=S

i
cell)

Z ). Here,
Z is the amount of test data, I(Â·) is the indicator
function, and superscript i is the index of data.

Table 5 shows that DAE significantly outper-
forms TypeSQL and AnnotatedSeq2Seq on all the
evaluation metrics. First, for ACCSC, DAE out-
performs TypeSQL and AnotatedSeq2Seq by 16%
and 3.5% on test data; for ACCOC, DAE outper-
forms TypeSQL and AnnotatedSeq2Seq by 0.8%
and 28% on test data. Moreover, DAE can achieve
around 86% for ACCCE, while other methods fail
to recognize cells when the table content is not
available due to the privacy problem.

6 Conclusion

In this work, we propose a learning-based ap-
proach to reduce the lexical problem before the

3For experiments in this subsection, to make our
anonymization model in the same settings as the others,
we use BiLSTM as the input encoder and do not train the
anonymization model with neural semantic parser as a whole.



5413

neural semantic parser on text-to-SQL generation.
Specifically, we propose a two-stage anonymiza-
tion model and leverage implicit supervision from
SQL queries by policy gradient to guide its train-
ing. In the future, we plan to improve the perfor-
mance of the anonymization model by exploring
more efficient expected reward. In addition, we
also plan to extend our approach to the tasks with
question-denotation pairs as supervision.

References
Ion Androutsopoulos, Graeme D Ritchie, and Peter

Thanisch. 1995. Natural language interfaces to
databasesâ€“an introduction. Natural language engi-
neering, 1(1):29â€“81.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 33â€“43.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, pages 731â€“742.

Omer Goldman, Veronica Latcinnik, Ehud Nave, Amir
Globerson, and Jonathan Berant. 2018. Weakly su-
pervised semantic parsing with abstract examples.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics, pages
1809â€“1819.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, pages 623â€“
628.

Jonathan Herzig and Jonathan Berant. 2018. Decou-
pling structure and lexicon for zero-shot semantic
parsing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1619â€“1629.

Sepp Hochreiter and JuÌˆrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735â€“1780.

Matthew D Hoffman, David M Blei, Chong Wang,
and John Paisley. 2013. Stochastic variational infer-
ence. The Journal of Machine Learning Research,
14(1):1303â€“1347.

Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and
Minjoon Seo. 2019. A comprehensive exploration
on wikisql with table-aware word contextualization.
arXiv preprint arXiv:1902.01069.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, pages 12â€“22.

Yoon Kim, Sam Wiseman, and Alexander M Rush.
2018. A tutorial on deep latent variable models of
natural language. arXiv preprint arXiv:1812.06834.

Nikolaos Kolitsas, Octavian-Eugen Ganea, and
Thomas Hofmann. 2018. End-to-end neural entity
linking. In Proceedings of the 22nd Conference on
Computational Natural Language Learning, pages
519â€“529.

Fei Li and HV Jagadish. 2014. Constructing an in-
teractive natural language interface for relational
databases. Proceedings of the VLDB Endowment,
8(1):73â€“84.

Radford M Neal and Geoffrey E Hinton. 1998. A
view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in graphical
models, pages 355â€“368. Springer.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical ma-
chine translation. In 30th IEEE/ACM International
Conference on Automated Software Engineering,
pages 574â€“584.

Panupong Pasupat and Percy Liang. 2015. Composi-
tional semantic parsing on semi-structured tables. In
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1470â€“
1480.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
NIPS Workshop.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing, pages 1532â€“1543.

Wei Shen, Jianyong Wang, and Jiawei Han. 2015. En-
tity linking with a knowledge base: Issues, tech-
niques, and solutions. IEEE Transactions on Knowl-
edge and Data Engineering, 27(2):443â€“460.



5414

Tianze Shi, Kedar Tatwawadi, Kaushik Chakrabarti,
Yi Mao, Oleksandr Polozov, and Weizhu Chen.
2018. Incsql: Training incremental text-to-sql
parsers with non-deterministic oracles. arXiv
preprint arXiv:1809.05054.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
2017. A corpus of natural language for visual rea-
soning. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 217â€“223.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692â€“2700.

Chenglong Wang, Kedar Tatwawadi, Marc
Brockschmidt, Po-Sen Huang, Yi Mao, Olek-
sandr Polozov, and Rishabh Singh. 2018a. Robust
text-to-sql generation with execution-guided
decoding. arXiv preprint arXiv:1807.03100.

Wenlu Wang, Yingtao Tian, Hongyu Xiong, Haixun
Wang, and Wei-Shinn Ku. 2018b. A transfer-
learnable natural language interface for databases.
arXiv preprint arXiv:1809.02649.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229â€“256.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sqlnet:
Generating structured queries from natural language
without reinforcement learning. arXiv preprint
arXiv:1711.04436.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir Radev. 2018a. Typesql: Knowledge-
based type-aware neural text-to-sql generation. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, volume 2, pages 588â€“594.

Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,
Dongxu Wang, Zifan Li, and Dragomir Radev.
2018b. Syntaxsqlnet: Syntax tree networks for com-
plex and cross-domain text-to-sql task. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 1653â€“1663.

Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018c. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-sql task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3911â€“3921.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Twenty-First Con-
ference on Uncertainty in Artificial Intelligence,
pages 658â€“666.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.


