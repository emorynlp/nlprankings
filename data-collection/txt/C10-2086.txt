748

Coling 2010: Poster Volume, pages 748–756,

Beijing, August 2010

Head-modifier Relation based Non-lexical Reordering Model 

for Phrase-Based Translation  

Shui Liu1, Sheng Li1, Tiejun Zhao1, Min Zhang2, Pengyuan Liu3 

1School of Computer Science and Technology, Habin Institute of Technology 

{liushui,lisheng,tjzhao}@mtlab.hit.edu.cn 

2Institute for Infocomm Research 

mzhang@i2r.a-star.edu.sg 

3Institute of Computational Linguistics, Peking Unive rsity  

liupengyuan@pku.edu.cn 

Abstract 

Phrase-based  statistical  MT  (SMT)  is  a 
milestone  in  MT.  However,  the  transla-
tion  model  in  the  phrase  based  SMT  is 
structure  free  which  greatly  limits  its 
reordering  capacity.  To  address  this  is-
sue,  we  propose  a  non-lexical  head-
modifier  based  reordering  model  on 
word level by utilizing constituent based 
parse  tree  in  source  side.  Our  experi-
mental  results  on  the  NIST  Chinese-
English  benchmarking  data  show  that, 
with  a  very  small  size  model,  our  me-
thod significantly  outperforms the base-
line by 1.48% bleu score. 

1 

Introduction 

Syntax has been successfully applied to  SMT to 
improve  translation  performance.  Research  in 
applying  syntax  information  to  SMT  has  been 
carried out in two aspects. On  the one hand, the 
syntax knowledge is employed by directly  inte-
grating  the  syntactic  structure  into  the  transla-
tion rules i.e. syntactic translation rules. On this 
perspective, the word order of the target transla-
tion is modeled by the syntax structure explicit-
ly.  Chiang (2005), Wu (1997) and Xiong (2006) 
learn  the  syntax  rules  using  the  formal  gram-
mars. While more research is conducted to learn 
syntax rules with the help  of  linguistic analysis 
(Yamada and Knight, 2001;  Graehl and Knight, 
2004).  However,  there  are  some  challenges  to 
these  models.  Firstly,  the  linguistic  analysis  is 
far from perfect. Most of these methods require 
an  off-the-shelf  parser  to  generate  syntactic 
structure,  which  makes  the  translation  results 
sensitive  to  the  parsing  errors  to  some  extent. 

 
To  tackle  this  problem,  n-best  parse  trees  and 
parsing  forest  (Mi  and  Huang,  2008;  Zhang, 
2009) are proposed to relieve the error propaga-
tion  brought  by  linguistic  analysis.  Secondly, 
some  phrases  which  violate  the  boundary  of 
linguistic  analysis are also useful in these mod-
els ( DeNeefe et al., 2007; Cowan et al. 2006). 
Thus, a tradeoff needs to be found between lin-
guistic sense and formal sense. 

On the other hand,  instead of using syntactic 
translation  rules,  some  previous  work  attempts 
to  learn  the  syntax  knowledge  separately  and 
then  integrated  those  knowledge  to  the  original 
constraint. Marton and Resnik (2008) utilize the 
language linguistic analysis that is derived from 
parse  tree  to  constrain  the  translation  in  a  soft 
way.  By  doing  so,  this  approach  addresses  the 
challenges  brought  by 
linguistic  analysis 
through the log-linear model in a soft way.  

in 

the  phrase-based 

Starting from the state-of-the-art phrase based 
model Moses ( Koehn e.t. al, 2007), we propose 
a head-modifier relation based reordering model 
and  use  the  proposed  model  as    a  soft  syntax 
constraint 
translation 
framework.  Compared  with  most  of  previous 
soft constraint models, we study the way to util-
ize the constituent based parse tree structure by 
mapping the parse tree to sets of head-modifier 
for  phrase  reordering.  In  this  way,  we  build  a 
word  level  reordering  model  instead  of  phras-
al/constituent  level  model.    In  our  model,  with 
the help of the alignment  and the head-modifier 
dependency  based  relationship  in  the  source 
side,  the  reordering  type  of  each  target  word 
with  alignment  in  source  side  is  identified  as 
one of pre-defined reordering types. With these 
reordering  types,  the  reordering  of  phrase  in 
translation is estimated on word level.   

 

749

Fig 1. An Constituent based Parse Tree 

 

 
 

2  Baseline  

Moses, a state-of-the-art phrase based SMT sys-
tem  is  used  as  our  baseline  system.  In  Moses, 
given the source language f and target language 
e, the decoder is to find: 
ebest = argmaxe p ( e | f ) pLM ( e ) ωlength(e)        (1)    

where  p(e|f)  can  be  computed  using  phrase 
translation  model,  distortion  model  and  lexical 
reordering model. pLM(e) can be computed us-
ing the language model. ωlength(e) is word penalty 
model.  

Among  the  above  models,  there  are  three 
reordering-related components: language model, 
lexical  reordering  model  and  distortion  model. 
The language model can reorder the local target 
words within a fixed window in an implied way. 
The  lexical  reordering  model  and  distortion 
reordering model tackle the reordering problem 
between  adjacent  phrase  on  lexical  level  and 
alignment level. Besides these reordering model, 
the  decoder  induces  distortion  pruning  con-
straints  to  encourage  the  decoder  translate  the 
leftmost  uncovered  word  in  the  source  side 
firstly  and  to  limit  the  reordering  within  a  cer-
tain range. 

3  Model  

In  this  paper,  we  utilize  the  constituent  parse 
tree of source language to enhance the  reorder- 

 
 
ing capacity of the translation model.  Instead of 
directly  employing  the  parse  tree  fragments 
(Bod,  1992;  Johnson,  1998)  in  reordering  rules 
(Huang and Knight, 2006; Liu 2006; Zhang and 
Jiang  2008),  we  make a  mapping  from  trees  to 
sets  of  head-modifier  dependency  relations 
(Collins 1996 ) which  can be obtained  from the 
constituent  based  parse  tree  with  the  help  of 
head rules ( Bikel, 2004 ). 

3.1  Head-modifier Relation  

According  to  Klein  and  Manning  (2003)  and 
Collins (1999), there are two shortcomings in n-
ary Treebank grammar.   Firstly, the grammar  is 
too  coarse  for  parsing.  The  rules  in  different 
context  always  have  different  distributions.  Se-
condly,  the  rules  learned  from  training  corpus 
cannot cover the rules in testing set. 

Currently,  the  state-of-the-art  parsing  algo-
rithms  (Klein  and  Manning,  2003;  Collins  1999) 
decompose  the  n-ary  Treebank  grammar  into 
sets of head-modifier relationships. The parsing 
rules  in  these  algorithms  are constructed  in  the 
form  of  finer-grained  binary  head-modifier  de-
pendency relationships. Fig.2 presents an exam-
ple  of  head-modifier  based  dependency  tree 
mapped from the constituent parse tree in Fig.1.  

 

750

Fig. 2. Head-modifier Relationships with Aligned Translation 

 

 

Moreover, there are several reasons for which 
we  adopt  the  head-modifier  structured  tree  as 
the main frame of our reordering model. Firstly, 
the  dependency  relationships  can  reflect  some 
underlying  binary  long  distance  dependency 
relations in the source side. Thus,  binary depen-
dency  structure  will  suffer  less  from  the  long 
distance  reordering  constraint.  Secondly, 
in 
head-modifier  relation,  we  not  only  can  utilize 
the context of dependency relation in reordering 
model,  but  also  can  utilize  some  well-known 
and  proved  helpful  context  (Johnson,  1998)  of 
constituent  base  parse tree in reordering model. 
Finally,  head-modifier  relationship  is  mature 
and widely adopted method in full parsing.   

3.2  Head-modifier  Relation  Based  Reor-

dering Model  

Before  elaborating  the  model,  we  define  some 
notions  further  easy  understanding.  S=<f1,  f 
2…fn> is the source sentence; T=<e1,e2,…,em> is 
the  target  sentence;  AS={as(i)  |  1≤  as(i)  ≤  n  } 
where as(i) represents that the ith word in source 
sentence    aligned  to  the  as(i)th  word  in  target 
sentence; AT={aT(i) | 1≤ aT (i) ≤ n } where aT(i) 
represents  that  the  ith  word  in  target  sentence  
aligned  to  the  aT(i)th word  in  source  sentence; 
D=  {(  d(i),  r(i)  )|  0≤  d(i)  ≤n}  is  the  head-
modifier  relation  set  of    the  words  in  S  where 
d(i) represents  that  the  ith  word  in  source  sen-
tence  is  the  modifier  of  d(i)th    word  in  source 
sentence under relationship r(i); O= < o1, o2,…, 
om  >  is  the  sequence  of  the  reordering  type  of 
every  word  in  target  language.  The  reordering 
model probability is P(O| S, T, D, A).  

Relationship: in this paper,  we not only use the 
label  of  the  constituent  label  as  Collins  (1996), 
but  also  use  some well-known  context  in  pars-
ing to define the head-modifier relationship  r(.), 
including  the POS of the modifier  m,   the POS 
of  the  head  h,  the  dependency  direction  d,  the 
parent  label  of  the  dependency  label  l,  the 
grandfather  label  of  the  dependency  relation  p, 
the  POS  of  adjacent siblings  of  the  modif ier  s. 
Thus,  the  head-modifier  relationship  can  be 
represented as a 6-tuple <m, h, d, l, p, s>. 
 

r(.) 
r(1) 
r(2) 
r(3) 
r(4) 
r(5) 
r(6) 
r(7) 

relationship 
<VV, - , -, -, -, - > 
<NN, NN, right, NP, IP, - > 
<NN,VV, right, IP, CP, - > 
<VV, DEC, right, CP, NP, - > 
<NN,VV, left, VP, CP, - > 
<DEC, NP, right, NP, VP, - > 
<NN, VV, left, VP,  TOP, - > 

Table 1. Relations Extracted from Fig 2.  

 

In Table 1, there are 7 relationships extracted 
from the source head-modifier based dependen-
cy tree as shown in Fig.2. Please notice that, in 
this  paper,  each  source  word  has a  correspond-
ing relation.  
Reordering  type :  there  are  4  reordering  types 
for target words with  linked word  in the source 
side in our model: R= {rm1, rm2, rm3 , rm4}. The 
reordering type of target word as(i) is defined  as 
follows: 

 

rm1:  if  the  position  number  of  the  ith 
word’s  head  is  less  than  i  (  d(i)  <  i  )  in 
source language, while the position num-
ber  of  the  word  aligned  to  i  is  less  than 

751

 

 

 

as(d(i))  (as(i)    <  as(d(i))  )  in  target  lan-
guage;  

rm2:  if  the  position  number  of  the  ith 
word’s  head  is  less  than  i  (  d(i)  <  i  )  in 
source language, while the position num-
ber of the word aligned to  i  is larger than 
as(d(i))  (as(i)  >  as(d(i))  )  in  target  lan-
guage. 

rm3:  if  the  position  number  of  the  ith 
word’s head is larger than i ( d(i) > i ) in 
source language, while the position num-
ber of the word aligned to  i  is larger than 
as(d(i)) (as(i) > as(d(i))) in target language. 

rm4:  if  the  position  number  of  the  ith 
word’s head is  larger than  i ( d(i) >  i)  in 
source language, while the position num-
ber  of  the  word  aligned  to  i  is  less  than 
as(d(i))  (as(i)  <  as(d(i))  )  in  target  lan-
guage. 

ing  type  into  rm2;  otherwise,  we  classify  the 
reordering type into rm4. 
Probability  estimation:  we  adopt  maximum 
likelihood (ML) based estimation  in this paper. 
In  ML  estimation,  in  order  to  avoid  the  data 
sparse  problem  brought  by  lexicalization,  we 
discard  the  lexical  information  in  source  and 
target language: 

                                  (2) 

where oi∈{rm1,rm2,rm3,rm4} is the reorder-
ing type of ith word in  target language. 

To get a non-zero probability, additive smooth 

ing( Chen and Goodman, 1998) is used: 

                                                                                 
(3) 
where F(. ) is the frequency of the statistic event 
in training corpus. For a given set of dependen-
cy  relationships  mapping  from  constituent  tree, 
the  reordering  type  of  ith  word  is  confined  to 
two types:  it  is whether one of rm1 and rm2 or 
rm3 and rm4. Therefore, |O|=2 instead of |O|=4 
in  (2).  The  parameter α  is an additive  factor  to 
prevent zero probability. It is computed as:    
                                        

       

(4) 
where c is a constant parameter(c=5 in this  pa-
per). 
   In above, the additive parameter α is an adap-
tive  parameter  decreasing  with  the  size  of  the 
statistic  space.  By  doing  this,  the  data  sparse 
problem can be relieved. 

4  Apply the Model to Decoder 

Our  decoding  algorithm  is  exactly  the  same  as 
(Kohn,  2004).  In  the  translation  procedure,  the 
decoder  keeps  on  extending  new  phrases  with-
out overlapping, until all source words are trans-
lated.  In  the  procedure,  the  order  of  the  target 

 

 

Fig. 3.  An example of the reordering types in 

Fig. 2. 

Fig.  3  shows  examples  of  all  the  reordering 
types. In Fig. 3, the reordering type is labeled at 
the target word aligned to the  modifier: for ex-
ample, the reordering type of rm1 belongs to the 
target word “scale”. Please note that, in general, 
these  four  types  of  reordering  can  be  divided 
into 2 categories: the target words order of rm2 
and  rm4  is  identical  with  source  word  order, 
while  rm1  and  rm3  is  the  swapped  order  of 
source. In practice, there are some special cases 
that  can’t  be classified  into  any  of  the  defined 
reordering  types:  the  head  and  modifier  in 
source  link  to  the  same word  in  target.  In  such 
cases,  rather  than  define  new  reordering  types, 
we  classify  these  special  cases  into  these  four 
defined  reordering  types:  if  the  head  is  right  to 
the  modifier  in  source,  we classify  the  reorder-

P(O

 S, |

 T,

 D,

A)



m




1i

P(o

i

-,-,
 |

r(a

(i)))

T

P(o

i

|

-,-,

iarF
(

((

t

) )))





(

iaroF
(

(,

T
iarF
(
)))

i
((

t

)))







|

O

|






Ro



i

moF

(

,

i

,

h

,

d

,

l

a

T

i
)(

,

p

a

T

i
)(

,

s

a

T

i
)(

)





a

T

i
)(

a

T

i
)(

a

T

i
)(

mF

(

,

h

,

d

,

l

a

T

i
)(

,

p

a

T

i
)(

,

s

a

T

i
)(

a

T

i
)(

a

T

i
)(

a

T

i
)(

)



|

O

|






Ro



i



C 



Ro



i

1

(

mF
a
T

,

h
a
T

i
)(

,

d

i
)(

a
T

i
)(

,

l
a
T

,

p
a
T

,

s
a
T

)

i
)(

i
)(

i
)(

752

words  in  decoding  procedure  is  fixed.    That  is, 
once a hypothesis  is generated, the order of tar-
get words cannot be changed in the future. Tak-
ing advantage of this feature, instead of compu-
ting  a  totally  new  reordering  score  for a  newly 
generated  hypothesis,  we  merely  calculate  the 
reordering  score  of  newly  extended  part  of  the 
hypothesis  in  decoding.  Thus,  in  decoding,  to 
compute  the  reordering  score,  the  reordering 
types of each target word in the newly extended 
phrase need to be identified.  

The  method  to  identify  the  reordering  types 
in  decoding  is  proposed  in  Fig.4.  According  to 
the definition of reordering,  the reordering type 
of the target word is  identified  by the direction 
of head-modifier dependency on the source side, 
the  alignment  between  the  source  side  and  tar-
get  side,  and  the  relative  translated  order  of 
word pair under the head-modifier relationship. 
The direction of  dependency and the alignment 
can  be  obtained  in  input  sentence  and  phrase 
table. While the relative translation order needs 
to record during decoding.  A word index is em-
ployed  to  record  the  order.  The  index  is  con-
structed in the form of true/false array: the index 
of  the  source  word  is  set  with  true  when  the 
word has been translated. With the help of this 
index,  reordering  type  of  every  word  in  the 
phrase can be identified. 
 
1:  Input:  alignment  array  AT;  the  Start  is  the 
start position  of the phrase in the source side; 
head-modifier relation d(.); source word in-
dex  C,  where  C[i]=true    indicates  that  the 
ith word in source has been translated.   

2:  Output:  reordering  type  array  O  which  re-
serves the reordering types of each word in 
the target phrase 
3: for i = 1, |AT| do 
4:    P  ← aT(i) + Start 
5:    if (d (P)<P) then 
6:      if C [d(p)] = false then 
7:         O[i] ← rm1 
8:      else 
9:         O[i] ← rm2 
10:        end if 
11:  else   
12:     if  C[d(p)] = true then 
13:        O[i] ← rm3 
14:       else 
15:          O[i] ← rm4 

16:       end if 
17:    end if 
18: C[p] ←true //update word index 
19: end for 
Fig. 4.  Identify the Reordering Types of  Newly 

Extended Phrase 

After all the reordering types in the newly ex-
tended  phrase  are  identified,  the  reordering 
scores  of  the  phrase can  be computed  by  using 
equation (3). 

5  Preprocess the Alignme nt 

In Fig. 4, the word index is to identify the  reor-
dering  type  of  the  target  translated  words.  Ac-
tually,  in  order  to  use  the  word  index  without 
ambiguity,  the  alignment  in  the  proposed  algo-
rithm needs to satisfy some constraints.  

Firstly,  every  word  in  the  source  must  have 
alignment  word  in  the  target  side.  Because,  in 
the decoding  procedure, if the head word is not 
covered by the word index, the algorithm cannot 
distinguish  between  the  head  word  will  not  be 
translated in the future and the head word is not 
translated  yet.  Furthermore,  in  decoding,  as 
shown in Fig.4, the index of source would be set 
with  true  only  when  there  is  word  in  target 
linked to  it. Thus, the index of the source word 
without alignment in target is never set with true.  

Fig. 5.  A complicated Example of Alignment in 

 

Head-modifier based Reordering Model 

Secondly, if the head word has more than one 
alignment  words  in  target,  different  alignment 
possibly  result  in  different  reordering  type.  For 
example,  in  Fig.  5,  the  reordering  type  of  e2  is 
different  when  f2  select  to  link  word  e1 and  e3   
in the source side.  

To  solve  this  problem,  we  modify  the  align-
ment  to  satisfy  following  conditions:  a)  each 
word  in  source  just  has  only  one  alignment 
word in target, and b) each word in target has at 
most  one  word  aligned  in  source  as  its  anchor 
word  which  decides  the  reordering  type  of  the 
target word.  

To  make  the  alignment  satisfy  above  con-
straints,  we  modify  the  alignment  in  corpus.  In 

753

order  to  explain  the  alignment  preprocessing, 
the  following  notions  are  defined:  if  there  is  a 
link between the source word f j  and target word 
ei,  let    l(ei  ,fj)  =  1  ,  otherwise  l(ei  ,fj)  =  0;  the 
source word  fj∈F1-to-N  ,  iff   ∑i l(ei,fj)  >1,  such 
as the source word f2 in Fig. 5; the source word 
fj∈FNULL,  iff  ∑i l(ei,fj)  =  0,  such  as  the  source 
word f4 in Fig. 5; the target word ei∈E1-to-N  , iff 
∑j l(ei,fj) > 1, such as the target word e1 in Fig. 
5.  

In  preprocessing,  there  are  3  types  of  opera-
tion, including DiscardLink(fj) , BorrowLink( f j )  
and FindAnchor(ei ) : 
DiscardLink( fj ) :  if the word fj in source with 
more than one words aligned  in target, i.e.  fj∈
F1-to-N ; We set the target word en with l(en, fj) = 
1,  where  en=  argmaxi  p(ei | fj)  and      p(ei | fj)  is 
estimated  by  (  Koehn  e.t.  al,  2003),  while  set  
rest of words linked to fj with l (en, fj) = 0.    
BorrowLink( fj ):  if the word  fj in source with-
out a alignment word  in target,  i.e.  fj∈FNULL ; 
let  l(ei,fj)=1  where  ei    aligned  to  the  word  fj  , 
which  is  the  nearest  word  to    fj    in  the  source 
side; when there are two words nearest to fj with 
alignment  words  in  the  target  side  at  the  same 
time,  we select  the alignment  of    the  left  word 
firstly .  
):  for  the  word  ei  in  target  with 
FindAnchor(
more than one words aligned in source ,  i.e.   ei
∈E1-to-N ; we select the word  fm  aligned to ei as 
its anchor word to decide the reordering type of 
ei  ,  where fm= argmaxj p(ei | fj) and  p(fj | ei) is 
estimated  by  ( Koehn  et  al,  2003);  For  the  rest 
of words aligned to  ei , we would set their word 
indexes  with  true  in  the  update  procedure  of 
decoding  in the 18th line of Fig.4.    

With these operations, the required alignment 
can  be  obtained  by  preprocessing  the  origin 
alignment as shown in Fig. 6. 
1: Input: set of alignment A between target lan-

guage e and source language f  

2: Output: the 1-to-1 alignment required by the 

model 

3:  foreach fi∈F1-to-N do 
4:    DiscardLink( fi ) 
5:  end for 
6:  foreach fi  ∈FNULL  do 
7:    BorrowLink( fi ) 
8:  end for 
9:  foreach  ei∈E1-to-N do  

10:   FindAnchor(ei ) 
11:endfor           

Fig. 6. Alignment Pre-Processing algorithm 

 

 

 
Fig. 7. An Example of Alignment Preprocessing. 

   An  example  of   the  preprocess  the alignment 
in  Fig.  5  is  shown  in  Fig.  7  :  firstly,  Discar-
dLink(f 2) operation discards the link between f2 
and e1  in (a); then the link between f4 and e3 is 
established by operation BorrowLink(f 4 )  in (b); 
at  last,  FindAnchor(e3)  select  f2  as  the  anchor 
word  of  e3    in  source  in  (c).  After  the  prepro-
cessing, the reordering type of e3   can be identi-
fied.  Furthermore,  in  decoding,  when  the  de-
coder  scans  over  e2,  the  word  index  sets  the 
word  index  of  f3  and  f4 with  true.  In  this  way, 
the  never-true  word  indexes  in  decoding  are 
avoided.  

6  Training the Reordering Model 

Before  training,  we  get  the  required  alignment 
by alignment preprocessing as  indicated above. 
Then  we  train  the  reordering  model  with  this 
alignment:  from  the  first  word  to  the  last  word 
in  the  target  side,  the  reordering  type  of  each 
word is identified. In this procedure, we skip the 
words  without  alignment  in  source.  Finally,  all 
the  statistic  events  required  in  equation  (3)  are 
added to the model.   

In our model, there are 20,338 kinds of rela-
tions  with  reordering  probabilities  which  are 
much smaller than most phrase level reordering 
models on the training corpus FBIS.   

Table  1  is  the  distribution  of  different  reor-

dering types in training model.  

754

Type of Reordering 

  Percentage   %    
           

rm1 
rm2 
rm3 
rm4 
Table 1: Percentage of different reordering 

3.69 
27.61 
20.94 
47.75 

types in model 

In  these  feature  groups,  the  top  5  groups  of 
features  are  the  baseline  model,  the  left  two 
group scores are related with our model.  

In decoding, we drop  all the OOV words and 
use  default  setting  in  Moses:  set  the  distortion 
limitation  with  6,  beam-width  with  1/100000, 
stack size with  200 and max number of phrases 
for each span with 50.  

From Table 1, we can conclude that the  reor-
dering  type  rm2 and  rm4  are  preferable  in  reor-
dering which take over nearly 3/4 of total num-
ber  of  reordering  type  and  are  identical  with 
word order of the source. The statistic data indi-
cate that most of the words order doesn’t change 
in  our  head-modifier  reordering  view.    This 
maybe can explain why the models (Wu, 1997; 
Xiong,  2006;  Koehn,  et.,  2003)  with  limited 
capacity of reordering can  reach certain perfor-
mance. 

7  Experime nt and Discussion  

7.1  Experiment Settings 

We perform Chinese-to-English translation task 
on NIST MT-05 test set, and  use NIST MT-02 
as our tuning set. FBIS corpus is selected as our 
training  corpus,  which  contains  7.06M  Chinese 
words  and  9.15M  English  words.  We  use  GI-
ZA++(Och and Ney, 2000) to  make the corpus 
aligned.  A  4-gram  language  model  is  trained 
using  Xinhua  portion  of  the  English  Gigaword 
corpus (181M words). All models are tuned on 
BLEU, and evaluated on both  BLEU and NIST 
score. 

To  map  from  the  constituent  trees  to  sets  of 
head-modifier  relationships,  firstly  we  use  the 
Stanford  parser  (Klein,  2003)  to  parse  the 
source  of  corpus  FBIS,  then  we  use  the  head-
finding  rules  in  (Bikel,  2004)  to  get  the  head-
modifier dependency sets. 

In our system, there are  7 groups of features. 

They are: 

1.  Language model score (1 feature) 
2.  word penalty score (1 feature) 
3.  phrase model scores (5 features) 
4.  distortion score (1 feature) 
5. 
6.  Number  of  each  reordering  type  (4  fea-

lexical RM scores (6 features) 

tures) 

7.  Scores  of  each  reordering  type  (4  fea-

tures, computed by equation (3)) 

7.2  Results and Discussion 

We  take  the  replicated  Moses  system  as  our 
baseline. Table 2 shows the results of our model.  
In the table, Baseline model is the model includ-
ing feature group 1, 2, 3 and 4. Baseliner m mod-
el is the Baseline model with feature group 5. H-
M  model  is  the  Baseline  model  with  feature 
group  6  and  7.  H-Mrm  model  is  the  Baseliner m 
model with feature group 6 and 7.  

Model 
Baseline  
Baseliner m  
H-M  
H-Mrm 

BLEU%  NIST 
27.06  7.7898 
27.58     7.8477 
28.47     8.1491 
29.06 
8.0875 

Table 2: Performance of  the Systems on NIST-

05(bleu4 case-insensitive). 

From table 2, we can conclude that our reor-
dering  model  is  very  effective.  After  adding 
feature  group  6  and  7,  the  performance  is  im-
proved by 1.41% and 1.48% in bleu score  sepa-
rately.  Our  reordering  model  is  more  effective 
than  the  lexical  reordering  model  in  Moses:  
1.41% in  bleu score is improved  by adding our 
reordering model to Baseline model, while 0.48 
is  improved by adding the  lexical reordering to 
Baseline model.   

threshold 
≥1 
≥2 
≥3 
≥4 
≥5 

KOR 
20,338  
     13,447 
     10,885 
       9,518 
       8,577 

NIST 
 8.0875 
  8.3658 
8.0350 
8.1002 
  8.1213 
Table 3: Performance on NIST-05 with Differ-
ent Relation Frequency Threshold (bleu4 case-

BLEU 
29.06 
28.83 
28.64 
28.94 
      29.18 

insensitive). 

Although  our  model  is  lexical  free,  the  data 
sparse  problem  affects  the  performance  of  the 
model.  In  the  reordering  model,  nearly  half 
numbers of the relations in our model occur less 
than three times. To investigate this, we statistic 

755

the frequency of the relationships  in our model, 
and  expertise  our  H-M full  model  with  different 
frequency threshold.  

In Table 3, when the frequency of relation  is 
not less than the threshold, the relation is added 
into  the  reordering  model;  KOR  is  the  number 
of relation type in the reordering model. 

Table 3 shows that, in our model, many rela-
tions  occur  only  once.  However,  these  low-
frequency  relations  can  improve  the  perfor-
mance of the model according to the experimen-
tal  results.  Although  low  frequency  statistic 
events always do harm to the parameter estima-
tion in ML, the model can estimate more events 
in the test corpus with the help of low frequency 
event.  These  two  factors  affect  the  experiment 
results  on  opposite  directions:  we  consider  that 
is  the  reason  the  result  don’t  increase  or  de-
crease  with  the  increasing  of  frequency  thre-
shold in the model. According to the results, the 
model without frequency threshold achieves the 
highest bleu score. Then, the performance drops 
quickly,  when  the  frequency  threshold  is  set 
with 2. It is because there are many events can’t 
be estimated by the smaller model. Although, in 
the  model  without  frequency  threshold,  there 
are  some  probabilities  overestimated  by  these 
events  which  occur  only  once,  the  size  of  the 
model affects the performance to a larger extent. 
When the frequency threshold increases above 3, 
the  size  of  model  reduces  slowly  which  makes 
the  overestimating  problem  become  the  impor-
tant  factor  affecting  performance.  From  these 
results,  we  can  see  the  potential  ability  of  our 
model:  if our  model suffer less from data spars 
problem, the performance should be further im-
proved, which is to be verified in the future.   

8  Related Work and Motivation 

There  are  several  researches  on  adding  linguis-
tic  analysis  to  MT  in  a  “soft  constraint”  way. 
Most of them are based on  constituents in parse 
tree.  Chiang(2005),  Marton  and  Resnik(2008) 
explored the constituent match/violation  in hie-
ro; Xiong (2009 a) added constituent parse tree  
based  linguistic  analysis 
into  BTG  model; 
Xiong (2009 b) added source dependency struc-
ture  to BTG;  Zhang(2009)  added  tree-kernel  to 
BTG model.  All these studies show promising 
results.  Making  soft  constrain  is  an  easy  and 

efficient  way  in  adding  linguistic  analysis  into 
formal sense SMT model.   

In modeling the reordering, most of previous 
studies are on phrase level. In Moses, the lexical 
reordering  is  modeled  on  adjacent  phrases.  In 
(Wu, 1996; Xiong, 2006), the reordering is also 
modeled on adjacent translated phrases. In hiero, 
the  reordering  is  modeled  on  the  segments  of 
the  unmotivated  translation  rules.  The  tree-to-
string  mode ls  (Yamada  et  al.  2001;  Liu  et 
al.2006)  are  model  on  phrases  with  syntax  re-
presentations.  All  these  studies  show  excellent 
performance,  while  there  are  few  studies  on 
word level model  in recent years. It is because, 
we consider, the alignment in  word  level model 
is complex  which limits the reordering capacity 
of word level models.  

However,  our  work  exploits  a  new  direction 
in  reordering  that,  by  utilizing  the  decomposed 
dependency relations mapped from parse tree as 
a  soft  constraint,  we  proposed  a  novel  head-
modifier  relation  based  word  level  reordering 
model.  The  word  level  reordering  model  is 
based on a phrase based SMT framework. Thus, 
the task to find the proper position of translated 
words  converts  to  score  the  reordering  of  the 
translated  words,  which  relax  the  tension  be-
tween  complex  alignment  and  word  level  reor-
dering in MT.  

9  Conclusion and Future Work 

Experimental  results  show  our  head-modifier 
relationship base model  is effective to the base-
line  (enhance  by  1.48%  bleu  score), even  with 
limited size of model and simple parameter es-
timation. In the future, we will try more compli-
cated smooth methods or use  maximum entropy 
based reordering model. We will study the per-
formance with  larger distortion constraint, such 
as the performances of   the distortion constraint 
over 15, or even the performance without distor-
tion model.  

10  Acknowledge ment 

The  work  of  this  paper  is  funded  by  National 
Natural Science Foundation of China (grant no. 
60736014),  National  High  Technology  Re-
search and Development Program of China (863 
Program)  (grant  no.  2006AA010108),  and  Mi-
crosoft  Research  Asia  IFP  (grant  no.  FY09-
RES-THEME-158). 

756

References  

Deka i  Wu.  1997.  Stochastic  inversion  transduction 
grammars and bilingual parsing of parallel corpo-
ra. Computational Lingustics,23(3):377-403. 

David  Ch iang.  2005.  A  hie rarchica l  phrase-based 

model for SMT. ACL-05.263-270. 

Mark Johnson. 1998. PCFG models of linguistic tree  
Linguistics, 

Computational 

representations. 
24:613-632. 

Liang Huang, Kevin Knight, and Aravind Joshi. Sta-
tistical  Syntax-Directed  Translation  with  Ex-
tended Domain of  Locality.  2006.  In Proceedings 
of the 7th AMTA. 

David  Chiang.  2007.  Hiera rchical  phrase-based 
translation. Computational Linguistics, 33(2):201-
228. 

Yang Liu, Qun Liu, and Shou xun Lin. Tree-to-String 
for  Statistica l  Machine 

Align ment  Temp late 
Translation. 2006.In Proceedings of the ACL 2006.  

Kenji  Ya mada and K.  Knight. 2001. A syntax-based 

statistical translation model. ACL-01.523-530. 

Yuva l Marton and Philip  Resnik. 2008.  Soft syntac-
tic  Constraints  for  Hie rarchica l  Phrased-based 
Translation. ACL-08. 1003-1011. 

Libin  shen, Jin xi Xu and Ra lph Weischedel. 2008. A 
New  String-to-Dependency  Machine  Translation 
Algorith m  with  a  Target  Dependency  Language 
Model. ACL-08. 577-585. 

J.  Graehl  and  K.  Kn ight.2004.Train ing  Tree  trans-
ducers.  In  proceedings  of  the  2004  Human  Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computatio n-
al Linguistics. 

Deka i Wu.  1996. A  Polyno mia l-Time Algorithm for 
Statistical Machine Translation. In proceedings of 
ACL-1996 

Deyi Xiong, Qun Liu, and Shou xun Lin. 2006. Ma x-
imu m  Entropy  Based  Phrase  Reordering  Model 
for  Statistical  Machine  Translation.  In   proceed-
ings of COLIN G-ACL 2006 

Deyi  Xiong,  Min  Zhang,  Aiti  AW  and  Haizhou  Li.  
2009a.  A  Syntax-Driven  Bracket  Model  for 
Phrase-Based Translation. ACL-09.315-323. 

Deyi  Xiong,  Min  Zhang,  Aiti  AW  and  Haizhou   Li. 
2009b. A  Source Dependency Model for  Statistic 
Machine translation. MT-Summit 2009. 

Och,  F.J.  and  Ney,  H.  2000.  Imp roved  statistical 

align ment models. In Proceedings of ACL 38. 

Philipp  Koehn,  et  al.  Moses:  Open  Source  Toolkit 

for Statistical Machine Translation, A CL 2007. 

Philipp  Koehn,  Fran z  Joseph  Och,  and  Daniel  Mar-
cu.2003.  Statistical  Phrase-based  Translation.  In 
Proceedings of HLT-NAACL. 

Philipp  Koehn.  2004.  A  Bea m  Sea rch  Decoder  for 
Phrase-Based  Translation  model.  In :  Proceeding 
of AMTA-2004,Washington 

Rens  Bod.  1992.  Data   oriented  Pa rsing(  DOP  ).  In  

Proceedings of COLIN G-92. 

Min  Zhang,  Hongfei  Jiang,  Ai  Ti  Aw,  Ha izhou  Li, 
Chew  Lim  Tan  and  Sheng  Li.  2008.  A  Tree  Se-
quence Alignment-based Tree-to-Tree Translation 
Model. ACL-HLT-08. 559-567. 

Dan  Kle in,  Christopher  D.  Manning.  Accurate  Un-

le xica lized  Parsing.  2003.  In  Proceedings  of  
ACL-03. 423-430. 

M.  Collins.  1996.  A  new  statistical  parser  based  on 
bigra m  le xical  dependencies.  In  Proceedings  of  
ACL-96. 184-191. 

M. Co llins. 1999. Head-Driven Statistical Models for 
Natural  Language  Parsing.  Ph.D.  thesis,  Univ.  of 
Pennsylvania. 

Andreas Zollmann. 2005. A Consistent and Effic ient 
Estimator  for  the  Data-Oriented  Parsing  Model. 
Journal of Automata, Languages  and Co mbinator-
ics. 2005(10):367-388 

Mark Johnson. 2002. The DOP estimation  method is 
biased  and  inconsistent.  Computational  Linguis-
tics 28, 71-76. 

Danie l  M.  Bikel.  2004.  On  the  Para meter  Space  of  
Generative Le xica lized Statistical Parsing Models. 
Ph.D. thesis. Univ. of Pennsylvania. 

S.  F.  Chen,  J.  Good man.  An  Empirica l  Study  of 
Smoothing  Techniques  for  Language  Modeling. 
In Proceedings of the 34th annual meeting on As-
sociation 
Linguistics, 
1996.310-318. 

Computational 

for 

Haitao  M i  and  Liang  Huang.    2008.  Forest-based 
translation Rule Extract ion. ENMLP -08. 2006-214. 

Hui  Zhang,  Min  Zhang  ,  Haizhou  Li,  A iti  A w  and 
Chew  Lim  Tan.  Forest-based  Tree  Sequence  to 
String Translation Model. ACL-09: 172-180 

S  DeNeefe ,  K.  Kn ight,  W.  Wang,  and  D.  Marcu. 
2007.  What  can  syntax-based  MT  learn  fro m 
phrase-based MT ? In Proc. EMN LP-CoNULL. 

Brooke   Co wan,  Ivona  Kucerova,  and  Michael 
Collins.2006.  A  discriminative  model  for  tree-to-
tree translation. In Proc. EMNLP. 

