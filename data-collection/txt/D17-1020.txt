



















































Affinity-Preserving Random Walk for Multi-Document Summarization


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 210â€“220
Copenhagen, Denmark, September 7â€“11, 2017. cÂ©2017 Association for Computational Linguistics

Affinity-Preserving Random Walk for Multi-Document Summarization

Kexiang Wang, Tianyu Liu, Zhifang Sui and Baobao Chang
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China

{wkx,tianyu0421,szf,chbb}@pku.edu.cn

Abstract

Multi-document summarization provides
users with a short text that summarizes
the information in a set of related doc-
uments. This paper introduces affinity-
preserving random walk to the summa-
rization task, which preserves the affin-
ity relations of sentences by an absorb-
ing random walk model. Meanwhile, we
put forward adjustable affinity-preserving
random walk to enforce the diversity con-
straint of summarization in the random
walk process. The ROUGE evaluations on
DUC 2003 topic-focused summarization
task and DUC 2004 generic summariza-
tion task show the good performance of
our method, which has the best ROUGE-
2 recall among the graph-based ranking
methods.

1 Introduction

Multi-document summarization provides users
with summary that reflects the main information
in a set of given documents. The documents are
often related and talk about more than one top-
ics. Generic multi-document summarization and
topic-focused multi-document summarization are
two typical kinds of summarization. The former is
a summarization delivering the main information
of the documents with no bias while the latter is
a one delivering the main information biased to a
given topic description (a few sentences or even
phrases). Most existing summarization systems
are designed for these two kinds of summariza-
tion.

There are two goals for generic multi-document
summarization. The first one is saliency. Sum-
mary sentences should be central sentences that
capture the majority of information in a docu-

ment cluster. The sentences with little informa-
tion about the document cluster should not be in-
cluded in the summary. The second one is di-
versity. The information overlap between sum-
mary sentences should be as minimal as possi-
ble due to the length limit of summary. In other
words, the information coverage of summary is
a determinant, which requires that the summary
sentences should cover diverse aspects of infor-
mation. Besides the two goals, there is another
goal for the topic-focused summarization and that
is relevancy. It requires that the summary sen-
tences be relevant to the topic description. A se-
ries of conferences and workshops on automatic
text summarization (e.g. NTCIR, DUC), special
topic sessions in ACL, EMNLP and SIGIR have
advanced the techniques to achieve these goals and
many approaches have been proposed so far.

In this paper, we focus on the extractive summa-
rization methods, which extract the summary sen-
tences from the input document cluster. We pro-
pose affinity-preserving random walk for multi-
document summarization. The method is a graph-
based ranking method, which takes into account
the global information collectively computed from
the entire sentence affinity graph. Different from
the previous graph-based ranking methods, our
method adopts â€œglobal normalizationâ€ to trans-
form sentence affinity matrix into sentence tran-
sition matrix and formulates the sentence rank-
ing process in an absorbing random walk model.
Meanwhile, the adjustable affinity-preserving ran-
dom walk is proposed to facilitate the diversity of
summary by adjusting the sentence transition ma-
trix after each iteration of random walk. Experi-
mental results on DUC generic and topic-focused
multi-document summarization tasks show the
competitive performance of our method. To our
best knowledge, our system has the best ROUGE-
2 recall on both tasks among all existing graph-

210



based ranking methods, which defeats most other
methods.

We summarize our contributions as follows.
(1) We preserve the original affinity relations be-
tween sentences in a novel affinity-preserving ran-
dom walk view for multi-document summariza-
tion. The preservation of affinity leads to a more
salient summary. And it is applicable to both
generic and topic-focused summarization. (2)
We propose adjustable affinity-preserving random
walk to enforce the diversity constraint of summa-
rization in the random walk process. (3) Experi-
ments on DUC 2003 and DUC 2004 tasks demon-
strate the competitive performance of our method.

The rest of the paper is organized as follows.
Section 2 discusses the related work. Section 3
describes traditional random walk model for sum-
marization. Section 4 proposes affinity-preserving
random walk for the saliency goal of summa-
rization and this section also proposes adjustable
affinity-preserving random walk to produce both
salient and diverse summary. Section 5 gives our
evaluation results and the conclusion is made in
Section 6.

2 Related Work

Our method belongs to the graph-based ranking
methods to select sentences in the documents to
produce the summary. Erkan and Radev (2004)
proposed LexPageRank to compute the sentence
saliency based on the concept of eigenvector cen-
trality. It constructs the sentence affinity graph and
computes the sentence saliency based on an al-
gorithm similar to PageRank (Page et al., 1999).
Like PageRank, the affinity matrix is converted
into the row-stochastic matrix, which is used as the
transition matrix of random walk on the weighted
graph. Wan (2007) proposed manifold ranking for
topic-focused multi-document summarization. It
makes full use of both the relationships among all
sentences in the documents and the relationships
between the given topic description and the sen-
tences. Manifold ranking conducts a different nor-
malization on the sentence affinity matrix to guar-
antee the algorithmâ€™s convergence. GRASSHOP-
PER (Zhu et al., 2007) ranks sentences with an
emphasis on the diversity constraint of summa-
rization. It turns already ranked sentences into
absorbing states, which effectively prevents re-
dundant sentences from receiving a high rank.
The algorithm is based on an absorbing random

walk and produces only one summary sentence af-
ter one particular random walk becomes station-
ary. And the normalization from sentence affinity
matrix to sentence transition matrix is the same
as PageRank. DivRank (Mei et al., 2010) is a
method to balance the saliency and diversity of the
top ranked sentences in a reinforced random walk
model. Also, the normalization in DivRank from
affinity matrix to transition matrix is the same
as PageRank. Another notable diversified graph-
based ranking method GCD (Dubey et al., 2011)
relies on large amounts of training data to learn
edge conductances.

Our method formulates the multi-document
summarization as an affinity-preserving random
walk and uses the â€œglobal normalizationâ€ to trans-
form sentence affinity matrix into sentence transi-
tion matrix, which is different from all those pro-
posed methods. And the adjustable transition ma-
trix in our method balances the saliency and diver-
sity goals of summarization. Like GRASSHOP-
PER, our method relies on the absorbing random
walk model. The difference is that our method
does not turn the sentence vertex into absorbing
state but add an absorbing vertex to the original
sentence affinity graph. And all summary sen-
tences are extracted after the random walk reaches
a stationary state in our method. Like DivRank,
the sentence transition matrix is adjustable in our
method to enforce the diversity constraint of sum-
marization. However, our method differs from Di-
vRank in the mechanism to adjust the transition
matrix.

3 Traditional Random Walk for
Summarization

Suppose G = (S,E) is a graph with vertex set S
and edge set E âŠ‚ S2. Suppose there is a con-
ductance c(si, sj) > 0 associated with each edge
(si, sj) âˆˆ E and c(si, sj) = 0 associated with the
set S2 âˆ’ E (the conductance of nonexistent edge
is zero). Let

C(si) =
âˆ‘
sjâˆˆS

c(si, sj), si âˆˆ S (1)

so that C(si) is the total conductance of the edges
coming from si. And the traditional random walk
on graph is defined as

Definition 3.1. The discrete-time Markov chain
X = (X0, X1, X2, ...) with state space and tran-

211



sition probability matrix P given by

P (si, sj) =
c(si, sj)
C(si)

, (si, sj) âˆˆ S2 (2)

is called a random walk on the graph G.

This chain governs a particle moving along the
vertices of G. If the particle in the state Xm is
at vertex si âˆˆ S, it will be at a neighbor of si in
the next state Xm+1, which is chosen randomly in
proportion to the conductance. We can prove thatâˆ‘

sjâˆˆS P (si, sj) = 1 for any si âˆˆ S (C(si) 6= 0)
so P is a row-stochastic matrix by P = Dâˆ’1W,
where D is a diagonal matrix with entries Dii =
C(si) and W is the adjacency matrix of G where
Wij = c(si, sj).

For the summarization task, G is the sentence
affinity graph. The vertex set S = {s1, s2, ..., sn}
contains every sentence in the document cluster
and the edge set E contains the pairwise affinity
between sentences. We use the tf*isf formula
to calculate the weight associated with each term
occurring in the sentence, where tf is the term fre-
quency in the sentence and isf is the inverse sen-
tence frequency of the term among all sentences.
isf is often calculated as 1+log(n/nt), where n is
the total number of sentences and nt is the number
of sentences containing the term t. Wij is com-
puted using the standard cosine measure (Baeza-
Yates et al., 1999).

Wij = simcosine(vi, vj) =
vi Â· vj

â€–viâ€–2 Ã— â€–vjâ€–2 (3)

where vi and vj are the corresponding term vec-
tors of si and sj . Two vertices are connected if
their affinity is larger than 0 and Wii is set as 0 to
avoid self transition. We get an undirected graph
G as well as a symmetric sentence affinity matrix
W in this way. Then we transform W into P by
P = Dâˆ’1W and use the stationary distribution of
random walk as sentence ranking scores. The tra-
ditional random walk model is a simple practice
of PageRank algorithm for multi-document sum-
marization.

4 Affinity-Preserving Random Walk for
Summarization

4.1 Prior of Multi-Document Summarization
In the above traditional random walk on graph,
the normalization from affinity matrix W to tran-
sition matrix P is to make P a row-stochastic ma-

trix. This can be interpreted as a democratic nor-
malization because the surfer of a traditional ran-
dom walk visits neighbors of a vertex with prob-
ability 1. The surfer has to choose a neighbor
to visit next although it is a random choice w.r.t.
the conductance distribution of the vertex. How-
ever this democratic normalization is not suitable
for multi-document summarization due to the fact
that most sentences are not salient and should not
be normalized democratically as the few salient
ones. The prior here is that the number ratio of
good candidate sentences over bad candidate sen-
tences is very low due to the summary length limit.
Good candidate sentences are the sentences highly
overlapping with sentences in the reference sum-
mary written by humans. And the remaining sen-
tences are bad candidate sentences. The demo-
cratic normalization of P = Dâˆ’1W will extend
the adverse effect of bad candidate sentences and
suppress the effect of good candidate sentences,
because the total conductance of the bad candi-
date sentence is usually smaller than that of the
good candidate sentence. In this case, the random
surfer has to choose a neighbor to visit even when
she is currently at a bad candidate sentence, which
will direct her to visit other bad candidate sen-
tences neighboring to the current sentence. The
invariant behavior of the surfer at all vertices in the
graph is not consistent with the prior which makes
a distinction between good and bad candidate sen-
tences. It may pervert the random walk process
to get an ideal distribution in which only few sen-
tences are assigned with a high ranking score.

It is worth noting here that manifold ranking
(Wan et al., 2007) for summarization uses a dif-
ferent normalization: P = Dâˆ’

1
2 WDâˆ’

1
2 . It is

a symmetric normalization on both endpoints of
an edge, which makes P a suitable choice in the
manifold ranking process to smooth the scores of
neighboring vertices. The symmetric normaliza-
tion can be deduced from the objective function
of manifold ranking (Zhou et al., 2003) and does
not make a distinction between the good and bad
candidate sentences. It is also not consistent with
the prior. We can conclude that existing graph-
based ranking methods can not well characterize
the prior of multi-document summarization.

4.2 Affinity-Preserving Random Walk

We need a new normalization method that dis-
tinguishes between good and bad candidate sen-

212



ğ‘® ğ‘® ğ‘¨

Figure 4.1: Sentence graphs for summarization.
G: sentence affinity graph constructed from the
document cluster. GA: sentence augmented graph
with an absorbing vertex s0. Cmax equals toC(s1)
indicating that sentence s1 has the maximum con-
ductance, so there is no edge (s1, s0).

tences to satisfy the prior of multi-document sum-
marization. Affinity-preserving random walk has
an intrinsic mechanism that preserves the original
affinity relations between sentences in the docu-
ments, which is proposed and defined as follows.

Definition 4.1. For the graph G, the vertex set S
has (n + 1) vertices: s0, s1, s2, ..., sn. The max-
imum conductance Cmax = maxi=1,2,...nC(si).
Of the (n + 1) vertices, s0 is an absorbing vertex
with c(si, s0) > 0, c(s0, si) = 0, and c(s0, s0) =
Cmax for i = 1, 2, ..., n. The remaining vertices
are the normal vertices with c(si, sj) > 0 for
i, j = 1, 2, ..., n. The discrete-time Markov chain
X = (X0, X1, X2, ...) with state space and tran-
sition probability matrix P given by

P (si, sj) =
c(si, sj)
Cmax

P (si, s0) = 1âˆ’ C(si)
Cmax

, P (s0, si) = 0

P (s0, s0) = 1
for i, j = 1, 2, ..., n

(4)

is called an affinity-preserving random walk on the
graph G.

For our summarization task, we construct a sen-
tence augmented graph GA (as shown in Fig-
ure 4.1) by adding an absorbing vertex s0 to the
sentence affinity graph G. The unabsorbed ver-
tices si (i = 1, 2, ..., n) represent sentences in
the documents. The affinity-preserving random
walk process as defined above is implemented on
GA to rank sentences in the documents. In the
affinity-preserving random walk, once the surfer

reaches the absorbing vertex, she cannot walk out
of there. Because P (si, s0) is small for the ver-
tex si with a large conductance, it is less likely for
the surfer at si to walk into s0. As for the ver-
tex with a small conductance, the surfer has a ten-
dency to be absorbed by s0. The absorbing ver-
tex here plays a role of soaking unreliable rank-
ing scores from large numbers of bad candidate
sentences and highlighting the few good candidate
sentences. The affinity matrix W is normalized by
its first norm (equivalent to Cmax) in the affinity-
preserving random walk, which results in a kind of
â€œsoftâ€ stochastic matrix for n unabsorbed vertices.
â€œsoftâ€ here means that the sum of row elements in
the matrix can be less than 1. By contrast, P in the
traditional random walk is a â€œhardâ€ stochastic ma-
trix in which every sum of row elements has to be
1. Meanwhile, P in this absorbing Markov chain
(Seneta, 2006) preserves the original affinity rela-
tions in W as all sentences are globally normal-
ized by the same factor (i.e. Cmax). We call this
approach an â€œaffinity-preserving random walkâ€ as
it is used in (Cho et al., 2010), which deals with
a graph matching problem that aims at assigning
1-vs-1 correspondences between two graphs. The
similar idea is also applied in the case of ontology
matching (Xiang et al., 2015). Transition matrix
P including the absorbing vertex is formulated in
(Cho et al., 2010) as follows

P =
(

1 0 T

eâˆ’ c/â€–Wâ€–1 W/â€–Wâ€–1

)
(5)

where 0 T is a 1 Ã— n vector with all elements 0,
e is an n Ã— 1 vector with all elements 1, c =
[C(s1), C(s2), ..., C(sn)]

T is a vector containing
the conductances of n sentences and W/â€–Wâ€–1
is the n Ã— n soft substochastic matrix. How-
ever, the stationary distribution of such a random
walk on graph with one absorbing vertex is always(
1 0T

)
, which is not a good characterization of

the sentence ranking distribution. We turn to the
quasi-stationary distribution xÌ„ (Cho et al., 2010;
Darroch and Seneta, 1965) of absorbing random
walk for ranking sentences. xÌ„ (K) is defined as

xÌ„ (K)i = Prob(X
(K) = si|X(K) 6= s0)

=
x (K)iâˆ‘
j x

(K)
j

(6)

where X(K) denotes the position of random surfer
at time K. It can be proven that xÌ„ (K) has its

213



stationary distribution xÌ„ if W is irreducible (Dar-
roch and Seneta, 1965). We remove the sentences
that have the total conductance 0 (i.e. the isolated
sentences) when constructing the sentence affinity
graphG. In this way,Gwill be strongly connected
and has an irreducible adjacency matrix W.

We introduce the teleport vector y as used in
personalized PageRank (Page et al., 1999; Haveli-
wala, 2002; Jeh and Widom, 2003) for the summa-
rization task. In the generic summarization case,
we define the vector y in a way that reflects the
position of each sentence in a document. If the
sentence si+1 is right after the sentence si in the
same document, then

yi+1
yi

= Î»âˆ’1, Î» > 1 (7)

where Î» is the decay factor. In the topic-focused
summarization case, we incorporate the topic de-
scription as a vertex in the random walk process,
which is a standard way of achieving the rele-
vancy goal of this kind of summarization. Vector
y is defined to be [y1, y2, ..., yn, yn+1]T in which
yi = 0(1 6 i 6 n) and yn+1 = 1 when the first
n elements represent sentences in the documents
and the last one represents the topic description.
We normalize y by its first norm to get a prior
sentence ranking for multi-document summariza-
tion. Based on W and y, sentence ranking scores
in affinity-preserving random walk can be formu-
lated in a recursive form as follows

x =
ÂµWT /â€–Wâ€–1x + (1âˆ’ Âµ)y
â€–ÂµWT /â€–Wâ€–1x + (1âˆ’ Âµ)yâ€–1

(8)

where x = [Score(si)]nÃ—1 is the vector of sen-
tence ranking scores. Âµ is the damping factor that
trades off between two actions: the transition ac-
cording to WT /â€–Wâ€–1 and the teleport specified by
y. Transpose operation in Eq.(8) can be removed
because of symmetry of W. The final transition
matrix of affinity-preserving random walk is given
by A = ÂµW/â€–Wâ€–1 + (1âˆ’ Âµ)y Â· e T and x should
be normalized by its first norm after each itera-
tion of random walk. Like PageRank, the quasi-
stationary distribution is obtained by the normal-
ized principal eigenvector of A.

For implementation, the initial ranking scores of
all sentences are set to 1/n and the iterative pro-
cess in Eq.(8) is adopted to compute new ranking
scores of sentences. Usually convergence of the it-
erative algorithm is achieved when the difference
between scores computed at two successive itera-
tions falls below a given threshold.

4.3 Adjustable Affinity-Preserving Random
Walk for Summarization

Affinity-preserving random walk preserves the
affinity relations between sentences and gives high
ranking scores to the salient sentences. However,
the diversity constraint of summarization has not
been taken into account. The surfer of affinity-
preserving random walk has no knowledge about
what a diverse summary should be. If we just
take redundancy removing as the post-processing
separate step to improve diversity, sentences that
highly overlap with other summary sentences may
be chosen and sentences that include information
about different topics may be submerged. This
phenomenon can be explained by the theorem as
follows.

Theorem 4.1. Suppose xÌ„ is the quasi-stationary
distribution of affinity-preserving random walk
as defined in Sec.4.2 and x is the solution
of a continuous quadratic optimization problem
argmax(x T A x) s.t. x âˆˆ [0, 1]n, â€–xâ€–2 = 1 and
A has definition in Sec.4.2. The following equa-
tion holds

xÌ„ = x/â€–xâ€–1 (9)
when Âµ = 1.

Proof. In mathematics, for a given symmetric real
matrix A (when Âµ = 1) and nonzero real vector x,
the Rayleigh quotient R(A, x) is defined as

R(A, x) =
x T Ax
x T x

and it reaches its maximum value when x is the
principal eigenvector of A. If â€–xâ€–2 = 1, R(A, x)
is equivalent to x T A x. So the solution x is the
principal eigenvector of A. From Section 4.2, xÌ„ is
the normalized principal eigenvector of A. xÌ„ and
x have the relation in Eq.(9). The conclusion is
made.

From Theorem 4.1, affinity-preserving random
walk tends to produce a stationary distribution in
which the total sum of affinity between sentences
(i.e. x T Ax) is large if there is a subtle teleport-
ing effect. It will lead to a summary consisting
of many sentences overlapping with each other,
which clearly violates the diversity constraint of
summarization. Good candidate sentences may
not have high affinity with other sentences and are
likely to be submerged by affinity-preserving ran-
dom walk. Conversely, some bad candidate sen-
tences could have high affinity with others and will

214



be highlighted by the random walk process. An
extreme example is that a cluster of sentences will
all get high ranking scores if they are very sim-
ilar to each other. However, only one sentence
in this cluster should be included in the summary
and the others should be suppressed. The random
surfer is caught in a trap of larger sentence clus-
ter, which operates against the exploration of good
candidates in smaller cluster.

We introduce adjustable affinity-preserving ran-
dom walk to enforce the diversity constraint of
summarization. In the original affinity-preserving
random walk, sentence transition matrix A is fixed
and set as ÂµW/â€–Wâ€–1+(1âˆ’Âµ)yÂ·e T . Edge (si, s0)
(if C(si) 6= Cmax) always exists and has an in-
variant conductance c(si, s0), which means that
the surfer at si walks into s0 in the same manner
for the entire random walk process. The random
surfer makes her decision only based on invariant
A to select salient sentences and form the sum-
mary. To equip the surfer with knowledge about
what a diverse summary should be, we propose to
adjust sentence transition matrix A in each itera-
tion of random walk. The key point is that good
candidate sentences should be normalized locally
while bad ones should be normalized globally in
the transformation from affinity matrix W to tran-
sition matrix A.

A â€œvirtualâ€ summary V is produced based on
x in each iteration of affinity-preserving random
walk. â€œVirtualâ€ here means that V is a summary
based on transient distribution x, which differs
from the final summary based on quasi-stationary
distribution xÌ„. The method of diversity penalty im-
position (Wan et al., 2007) is used to produce V ,
which is denoted by the producingSummary func-
tion in Algorithm 4.1. It is a simple greedy al-
gorithm to select sentences that are both salient
and diverse, which often plays a role of greedy
post-processing step to produce the final summary.
Rather, we use it to produce virtual summary V
that satisfies both the saliency and diversity con-
straints based on a specific iteration. V is an indi-
cator for the diversity constraint of summarization.

The sentence transition matrix A in the iteration
(K+1) is then constructed with help of the virtual
summary V in the iterationK. Here, different nor-
malization methods are used to transform W into
A. If V includes the sentence si, elements in the
corresponding i-th row of W will be normalized
by the sum of the row (i.e. C(si)). Otherwise, el-

ements will be normalized by the maximum sum
of row elements in W (i.e. Cmax). In this way,
â€œlocal normalizationâ€ is adopted for the sentences
in V while â€œglobal normalizationâ€ is adopted for
the sentences not in V . We differentiate the nor-
malization methods to lead the surfer of affinity-
preserving random walk to explore more in the
neighborhood of the sentences in V rather than
end in the absorbing vertex s0. As a result, the sen-
tences that satisfy the saliency and diversity con-
straints will be highlighted even though they are
in a small sentence cluster. We characterize differ-

Algorithm 4.1: Adjustable Affinity-
Preserving Random Walk for Multi-Document
Summarization
Input: The sentence affinity matrix, W; The

starting and maximum number of
iteration, B and M ; The teleport
vector, y; The damping factor, Âµ;

Output: The multi-document summary, V ;
1 Aâ† ÂµW/Cmax + (1âˆ’ Âµ)y Â· eT
2 Initialize the starting distribution x as uniform
3 for iâ† 1, 2, ...,M do
4 if i > B then
5 V â† producingSummary(x)
6 Dâ† adjustingNormalization(V )
7 Aâ† Âµ(Dâˆ’1W)T + (1âˆ’ Âµ)y Â· eT
8 xÌ„â† Ax
9 xÌ„â† xÌ„/â€–xÌ„â€–1

10 if â€–xÌ„âˆ’ xâ€–1 < Îµ then
11 break

12 xâ† xÌ„
13 V â† producingSummary(x)
14 Return V

ent normalizations in the diagonal matrix D. Dii is
C(si) if si âˆˆ V and Dii is Cmax if si /âˆˆ V ,which
is denoted by the adjustingNormalization function
in Algorithm 4.1. D in the current iteration is here
dependent on x in the previous iteration. We will
have different sentence augmented graphs GA in
each iteration. Figure 4.2 shows an example of
GA(K) and GA(K + 1) in the respective itera-
tions K and (K + 1). The probability distribution
in the adjustable affinity-preserving random walk
is updated as follows.

x =
Âµ(Dâˆ’1W)T x + (1âˆ’ Âµ)y
â€–Âµ(Dâˆ’1W)T x + (1âˆ’ Âµ)yâ€–1

(10)

215



ğ‘® ğ‘¨(ğ‘²+ ğŸ)ğ‘® ğ‘¨(ğ‘²)

Figure 4.2: Sentence augmented graphs for sum-
marization in two successive iterations. GA(K):
augmented graph in the iteration K. Virtual sum-
mary V = {s1, s3, s5}, which is constructed from
x in the iteration (Kâˆ’1) by producingSummary.
D = diag([C(s1), Cmax, C(s3), Cmax, C(s5)]).
GA(K + 1): augmented graph in the iteration
(K+1). V = {s1, s4, s5}, which is constructed
from x in the iteration K by producingSummary.
D = diag([C(s1), Cmax, Cmax, C(s4), C(s5)]).
In both cases, Cmax equals to C(s1) indicating
that sentence s1 has the maximum conductance.

This is an adjustable Markov chain for which the
transition matrix A is Âµ(Dâˆ’1W)T + (1âˆ’Âµ)y Â· e T .
In this setting, A is dependent on the transient
distribution x in the previous iteration, which dif-
fers from the invariant transition matrix in Eq.(8).
As the diversity constraint is embedded in A,
subsequent random walks move to the solution
that induces a better summary. The algorithm of
the adjustable affinity-preserving random walk for
multi-document summarization is demonstrated in
Algorithm 4.1.

The parameter B in Algorithm 4.1 is used to
produce a transient distribution which is reliable
enough to adjust the transition matrix. To get the
final multi-document summary, we use the same
producingSummary function.

5 Experiments

5.1 Data Sets
Generic and topic-focused multi-document sum-
marization have been the main tasks in DUC1.
Task 2 of DUC 2004 is a generic summarization
task and task 3 of DUC 2003 is a topic-focused
summarization task. Both tasks are used for per-
formance evaluation of our method. In the exper-
iments, task 2 of DUC 2003 is used for the pa-

1http://www-nlpir.nist.gov/projects/duc/intro.html

rameter tuning of our method. We preprocess the
document data sets by removing stopwords from
each sentence and stemming the remaining words
using the Porterâ€™s stemmer2. Also, the sentences
containing the said clause (if a said, says, told, tells
word and quotation marks appear simultaneously)
are filtered out.

For evaluation, four reference summaries gen-
erated by human judges for each document cluster
are provided by DUC as the ground truth. A brief
summary over the evaluation datasets is shown in
Table 5.1. According to (Hong et al., 2014), we
adjust the length limit of summary in DUC 2004
from 665 bytes to 100 words as it provides the
same setting for system evaluations.

DUC
2003

DUC
2003

DUC
2004

Task Task 2 Task 3 Task 2

Type Generic
Topic-

focused
Generic

Cluster numbers 30 30 50
Data source TDT TREC TDT

Summary length 100 words 100 words 100 words

Table 5.1: Summary of data sets used in our ex-
periments.

5.2 Evaluation Metric
We use the ROUGE-1.5.5 (Lin and Hovy, 2003)
toolkit for evaluation, which has been officially
adopted by DUC for automatic summarization
evaluation. The toolkit measures summary qual-
ity by counting overlapping units such as the n-
gram, word sequences and word pairs between the
candidate summary and the reference summary.
ROUGE-N is an n-gram based measure and the
ROUGE-N recall is computed as follows

ROUGE-NR =

âˆ‘
Sâˆˆ{RefSum}

âˆ‘
n-gramâˆˆS

Countmatch(n-gram)âˆ‘
Sâˆˆ{RefSum}

âˆ‘
n-gramâˆˆS

Count(n-gram)
(11)

where n stands for the length of the n-gram, and
Countmatch(n-gram) is the maximum number of n-
grams co-occurring in the candidate summary and
the set of reference summaries. Count(n-gram)
is the number of n-grams in the reference sum-
maries.

We conduct our ROUGE experiments following
the recommended standard in (Owczarzak et al.,

2https://tartarus.org/martin/PorterStemmer/

216



2012; Hong et al., 2014)3. We compute ROUGE-2
recall with stemming and stopwords not removed,
which provides the best agreement with manual
evaluations. We also compute ROUGE-1 recall
which has the highest recall of ability to identify
the better summary in a pair, and ROUGE-4 recall
which has the highest precision of ability to iden-
tify the better summary in a pair (Owczarzak et al.,
2012).

5.3 Experimental Results

In the experiments, the parameters of our method
are set as follows: the decay factor Î» is 2, the max-
imum number of iterationM is 100, the number of
starting iteration B is 30, the damping factor Âµ is
0.85 and the minimum error Îµ is 1E-30.

System R-1 R-2 R-4
Cont. LexPageRank* 35.95 7.47 0.82

FreqSum 35.30 8.11 1.00
CLASSY 04 37.62 8.96 1.51
CLASSY 11 37.22 9.20 1.48

GRASSHOPPER* 37.20 9.26 1.50
DivRank* 37.60 9.30 1.52

GCD* 38.68 9.31 1.45
Submodular 39.18 9.35 1.39

APRW* 38.10 9.39 1.35
DPP 39.79 9.62 1.57

ICSISumm 38.41 9.78 1.73
AAPRW* 38.92 10.06 1.61
WFS-NMF 39.24 10.94 1.65

Table 5.2: System comparisons on task 2 of DUC
2004 (%). *: Graph-based ranking methods.

Table 5.2 shows the performance of our method
and other eleven well-known systems on task 2
of DUC 2004 according to ROUGE-1,2,4 recall,
sorted by ROUGE-2 recall in the ascending order.
Some of the results are from (Hong et al., 2014).
Cont. LexPageRank (Erkan and Radev, 2004) is a
graph-based ranking method and a representative
of traditional random walk approach. Here we em-
ploy the continuous version of LexPageRank. Fre-
qSum (Nenkova et al., 2006) is a simple approach
to approximate the importance of words with their
probability in the input and then select sentences
with high average word probability. CLASSY 04

3ROUGE-1.5.5 with the parameters: -n 4 -m -a -l 100 -x
-c 95 -r 1000 -f A -p 0.5 -t 0

(Conroy et al., 2004) was the participant of the of-
ficial DUC 2004 evaluation with the best evalua-
tion score. It employs a Hidden Markov Model us-
ing topic signature feature and requires a linguis-
tic preprocessing component. CLASSY 11 (Con-
roy et al., 2011) is the successor of CLASSY 04
and selects the non-redundant sentences using the
non-negative matrix factorization algorithm. In
the Submodular system (Lin and Bilmes, 2011),
multi-document summarization is formulated as
a submodular set function maximization prob-
lem. DPP (Lin and Bilmes, 2011) combines a
sentence saliency model with a global diversity
model encouraging non-overlapping information.
ICSISumm (Gillick and Favre, 2009) aims at find-
ing the globally optimal summary by formulating
the summarization task in Integer Linear Program-
ming. WFS-NMF (Wang et al., 2010) extends the
non-negative matrix factorization algorithm and
provides a good framework for weighting different
terms and documents. GRASSHOPPER, DivRank
and GCD are the three graph-based ranking mod-
els mentioned in Section 2. APRW and AAPRW
are our methods. APRW is the method of affinity-
preserving random walk described in Section 4.2
and AAPRW is the method of adjustable affinity-
preserving random walk described in Section 4.3.

System R-1 R-2 R-4
S17 31.81 4.98 0.47
S13 31.99 5.83 0.73
S16 35.00 7.31 1.04

Manifold Ranking 37.33 7.68 1.26
APRW 35.72 7.72 1.34

AAPRW 36.36 8.21 1.40

Table 5.3: System comparisons on task 3 of DUC
2003 (%).

Table 5.3 shows the evaluation results on task 3
of DUC 2003 according to ROUGE-1,2,4 recall,
sorted also by ROUGE-2 recall in the ascending
order. S13, S16 and S17 are the system IDs of the
top performing systems in the official DUC 2003
evaluation, whose details are described in DUC
publications (Zhou and Hovy, 2003; Chali et al.,
2003). Manifold Ranking is the method proposed
in (Wan et al., 2007) to make use of both the re-
lationships among all sentences in the documents
and the relationships between the given topic de-

217



scription and the sentences. APRW and AAPRW
are our methods.

From Tables 5.2 and 5.3, our method has the
best ROUGE-2 score among all graph-based rank-
ing methods for generic multi-document summa-
rization, and it also has the best ROUGE-2 score
for topic-focused multi-document summarization.
AAPRW has the ROUGE-2 score 10.06% on DUC
2004 task 2, which is 0.28% higher than the
best system ICSISumm reported by (Hong et al.,
2014) and 1.1% higher than the official best sys-
tem CLASSY 04. WFS-NMF has the overall best
score on DUC 2004 task 2 due to the sentence
feature selection and the weights on the document
side, which is reported by (Wang et al., 2010; Al-
guliev et al., 2013). AAPRW has the ROUGE-2
score 8.21% on DUC 2003 task 3, which is 0.53%
higher than Manifold Ranking and 0.9% higher
than the official best system S16. In DUC 2004
AAPRW has 0.67% more ROUGE-2 score than
APRW and the gap is 0.49% in DUC 2003, which
proves the effectiveness of the adjustable transi-
tion matrix in the random walk process. It is worth
mentioning that our method has the best ROUGE-
4 score on the DUC 2003 topic-focused summa-
rization task.

We conducted the two-sided Wilcoxon signed-
rank tests between each pair of AAPRW and other
methods. For the generic summarization in DUC
2004, our method provides a significant improve-
ment over the official best system CLASSY 04 on
ROUGE-2 (with p-value lower than 0.05). For the
query-focused summarization in DUC 2003, our
method also provides a significant improvement
over S17, S13 and S16 on ROUGE-2.

In order to further investigate the influences of
the parameter in our proposed method, the damp-
ing factor Âµ is varied from 0 to 1. Figures 5.1
and 5.2 show the ROUGE-1 and ROUGE-2 re-
call curves of our method on the two data sets, re-
spectively. We can see from the figures that the
damping factor has an effect on the performance
of multi-document summarization.

6 Conclusion and Future Work

In this paper we propose the adjustable affinity-
preserving random walk for generic and topic-
focused multi-document summarization, which
deals with the saliency and diversity goals in a uni-
fied framework. Experiments demonstrate the ef-
fectiveness of our method.

0.0 0.2 0.4 0.6 0.8 1.0

Âµ

0.34

0.35

0.36

0.37

0.38

0.39

R
O

U
G

E
-1

DUC 2003
DUC 2004

Figure 5.1: ROUGE-1 recall scores vs. Âµ of our
method.

0.0 0.2 0.4 0.6 0.8 1.0

Âµ

0.06

0.07

0.08

0.09

0.10

R
O

U
G

E
-2

DUC 2003
DUC 2004

Figure 5.2: ROUGE-2 recall scores vs. Âµ of our
method.

In the future work, we will focus on the self
transition of adjustable affinity preserving random
walk, which could be used to remove the redun-
dancy between summary sentences.

Acknowledgements

We would like to thank our three anonymous
reviewers for their helpful advice on various
aspects of this work. This research is sup-
ported by National Key Basic Research Pro-
gram of China (No.2014CB340504) and Na-
tional Natural Science Foundation of China
(No.61375074,61273318). The contact authors
for this paper are Zhifang Sui and Baobao Chang.

References

Rasim M Alguliev, Ramiz M Aliguliyev, and Nijat R
Isazade. 2013. Multiple documents summarization

218



based on evolutionary optimization algorithm. Ex-
pert Systems with Applications, 40(5):1675â€“1689.

Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.
1999. Modern information retrieval, volume 463.
ACM press New York.

Yllias Chali, Maheedhar Kolla, Nanak Singh, and
Zhenshuan Zhang. 2003. The university of leth-
bridge text summarizer at duc 2003. In the Pro-
ceedings of the HLT/NAACL workshop on Automatic
Summarization/Document Understanding Confer-
ence (DUC 2003).

Minsu Cho, Jungmin Lee, and Kyoung Lee. 2010.
Reweighted random walks for graph matching.
Computer Visionâ€“ECCV 2010, pages 492â€“505.

John M Conroy, Judith D Schlesinger, Jade Gold-
stein, and Dianne P Oleary. 2004. Left-brain/right-
brain multi-document summarization. In Proceed-
ings of the Document Understanding Conference
(DUC 2004).

John M Conroy, Judith D Schlesinger, Jeff Kubina, Pe-
ter A Rankel, and Dianne P Oâ€™Leary. 2011. Classy
2011 at tac: Guided and multi-lingual summaries
and evaluation metrics. TAC, 11:1â€“8.

John N Darroch and Eugene Seneta. 1965. On quasi-
stationary distributions in absorbing discrete-time fi-
nite markov chains. Journal of Applied Probability,
2(1):88â€“100.

Avinava Dubey, Soumen Chakrabarti, and Chiranjib
Bhattacharyya. 2011. Diversity in ranking via resis-
tive graph centers. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 78â€“86. ACM.

GuÌˆnes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22:457â€“479.

Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Langauge Processing, pages 10â€“18. Association
for Computational Linguistics.

Taher H Haveliwala. 2002. Topic-sensitive pagerank.
In Proceedings of the 11th international conference
on World Wide Web, pages 517â€“526. ACM.

Kai Hong, John M Conroy, Benoit Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A repos-
itory of state of the art and competitive baseline sum-
maries for generic news summarization. In LREC,
pages 1608â€“1616.

Glen Jeh and Jennifer Widom. 2003. Scaling person-
alized web search. In Proceedings of the 12th in-
ternational conference on World Wide Web, pages
271â€“279. ACM.

Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 71â€“78.
Association for Computational Linguistics.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 510â€“520. As-
sociation for Computational Linguistics.

Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010.
Divrank: the interplay of prestige and diversity in
information networks. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1009â€“1018.
Acm.

Ani Nenkova, Lucy Vanderwende, and Kathleen McK-
eown. 2006. A compositional context sensitive
multi-document summarizer: exploring the factors
that influence summarization. In Proceedings of
the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 573â€“580. ACM.

Karolina Owczarzak, John M Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1â€“9. Association for Computa-
tional Linguistics.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Eugene Seneta. 2006. Non-negative matrices and
Markov chains. Springer Science & Business Me-
dia.

Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In IJCAI, volume 7,
pages 2903â€“2908.

Dingding Wang, Tao Li, and Chris Ding. 2010.
Weighted feature subset non-negative matrix factor-
ization and its applications to document understand-
ing. In Data Mining (ICDM), 2010 IEEE 10th Inter-
national Conference on, pages 541â€“550. IEEE.

Chuncheng Xiang, Baobao Chang, and Zhifang Sui.
2015. An ontology matching approach based on
affinity-preserving random walks. In IJCAI, pages
1471â€“1478.

Dengyong Zhou, Jason Weston, Arthur Gretton,
Olivier Bousquet, and Bernhard SchoÌˆlkopf. 2003.
Ranking on data manifolds. In NIPS, volume 3.

219



Liang Zhou and Eduard Hovy. 2003. Headline summa-
rization at isi. In Document Understanding Confer-
ence (DUC-2003), Edmonton, Alberta, Canada.

Xiaojin Zhu, Andrew B Goldberg, Jurgen Van Gael,
and David Andrzejewski. 2007. Improving diversity
in ranking using absorbing random walks. In HLT-
NAACL, pages 97â€“104.

220


