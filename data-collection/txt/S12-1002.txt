










































Adaptive Clustering for Coreference Resolution with Deterministic Rules and Web-Based Language Models


First Joint Conference on Lexical and Computational Semantics (*SEM), pages 11–19,
Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

Adaptive Clustering for Coreference Resolution with Deterministic Rules
and Web-Based Language Models

Razvan C. Bunescu
School of EECS
Ohio University

Athens, OH 45701, USA
bunescu@ohio.edu

Abstract

We present a novel adaptive clustering model
for coreference resolution in which the expert
rules of a state of the art deterministic sys-
tem are used as features over pairs of clus-
ters. A significant advantage of the new ap-
proach is that the expert rules can be eas-
ily augmented with new semantic features.
We demonstrate this advantage by incorporat-
ing semantic compatibility features for neutral
pronouns computed from web n-gram statis-
tics. Experimental results show that the com-
bination of the new features with the expert
rules in the adaptive clustering approach re-
sults in an overall performance improvement,
and over 5% improvement in F1 measure for
the target pronouns when evaluated on the
ACE 2004 newswire corpus.

1 Introduction

Coreference resolution is the task of clustering a
sequence of textual entity mentions into a set of
maximal non-overlapping clusters, such that men-
tions in a cluster refer to the same discourse entity.
Coreference resolution is an important subtask in
a wide array of natural language processing prob-
lems, among them information extraction, question
answering, and machine translation. The availabil-
ity of corpora annotated with coreference relations
has led to the development of a diverse set of super-
vised learning approaches for coreference. While
learning models enjoy a largely undisputed role in
many NLP applications, deterministic models based
on rich sets of expert rules for coreference have been

shown recently to achieve performance rivaling, if
not exceeding, the performance of state of the art
machine learning approaches (Haghighi and Klein,
2009; Raghunathan et al., 2010). In particular, the
top performing system in the CoNLL 2011 shared
task (Pradhan et al., 2011) is a multi-pass system that
applies tiers of deterministic coreference sieves from
highest to lowest precision (Lee et al., 2011). The
PRECISECONSTRUCTS sieve, for example, creates
coreference links between mentions that are found
to match patterns of apposition, predicate nomina-
tives, acronyms, demonyms, or relative pronouns.
This is a high precision sieve, correspondingly it is
among the first sieves to be applied. The PRONOUN-
MATCH sieve links an anaphoric pronoun with the
first antecedent mention that agrees in number and
gender with the pronoun, based on an ordering of the
antecedents that uses syntactic rules to model dis-
course salience. This is the last sieve to be applied,
due to its lower overall precision, as estimated on
development data. While very successful, this de-
terministic multi-pass sieve approach to coreference
can nevertheless be quite unwieldy when one seeks
to integrate new sources of knowledge in order to
improve the resolution performance. Pronoun reso-
lution, for example, was shown by Yang et al. (2005)
to benefit from semantic compatibility information
extracted from search engine statistics. The seman-
tic compatibility between candidate antecedents and
the pronoun context induces a new ordering between
the antecedents. One possibility for using compat-
ibility scores in the deterministic system is to ig-
nore the salience-based ordering and replace it with
the new compatibility-based ordering. The draw-

11



back of this simple approach is that now discourse
salience, an important signal in pronoun resolution,
is completely ignored. Ideally, we would want to
use both discourse salience and semantic compat-
ibility when ranking the candidate antecedents of
the pronoun, something that can be achieved natu-
rally in a discriminative learning approach that uses
the two rankings as different, but overlapping, fea-
tures. Consequently, we propose an adaptive cluster-
ing model for coreference in which the expert rules
are successfully supplemented by semantic compat-
ibility features obtained from limited history web n-
gram statistics.

2 A Coreference Resolution Algorithm

From a machine learning perspective, the determin-
istic system of Lee et al. (2011) represents a trove
of coreference resolution features. Since the de-
terministic sieves use not only information about a
pair of mentions, but also the clusters to which they
have been assigned so far, a learning model that uti-
lized the sieves as features would need to be able
to work with features defined on pairs of clusters.
We therefore chose to model coreference resolu-
tion as the greedy clustering process shown in Algo-
rithm 1. The algorithm starts by initializing the clus-
tering C with a set of singleton clusters. Then, as
long as the clustering contains more than one clus-
ter, it repeatedly finds the highest scoring pair of
clusters 〈Ci, Cj〉. If the score passes the threshold
τ = f(∅, ∅), the clusters Ci, Cj are joined into one
cluster and the process continues with another high-
est scoring pair of clusters.

Algorithm 1 CLUSTER(X ,f )
Input: A set of mentions X = {x1, x2, ..., xn};

A measure f(Ci, Cj) = wT Φ(Ci, Cj).
Output: A greedy agglomerative clustering of X .

1: for i = 1 to n do
2: Ci ← {xi}
3: C ← {Ci}1≤i≤n
4: 〈Ci, Cj〉 ← argmax

p∈P(C)
f(p)

5: while |C| > 1 and f(Ci, Cj) > τ do
6: replace Ci, Cj in C with Ci ∪ Cj
7: 〈Ci, Cj〉 ← argmax

p∈P(C)
f(p)

8: return C

The scoring function f(Ci, Cj) is a linearly
weighted combination of features Φ(Ci, Cj) ex-
tracted from the cluster pair, parametrized by a
weight vector w. The function P takes a cluster-
ing C as argument and returns a set of cluster pairs
〈Ci, Cj〉 as follows:

P(C)={〈Ci, Cj〉 | Ci, Cj∈C, Ci 6=Cj}∪{〈∅, ∅〉}

P(C) contains a special cluster pair 〈∅, ∅〉, where
Φ(∅, ∅) is defined to contain a binary feature
uniquely associated with this empty pair. Its cor-
responding weight is learned together with all other
weights and will effectively function as a clustering
threshold τ = f(∅, ∅).

Algorithm 2 TRAIN(C,T )
Input: A dataset of training clusterings C;

The number of training epochs T .
Output: The averaged parameters w.

1: w← 0
2: for t = 1 to T do
3: for all C ∈ C do
4: w← UPDATE(C,w)
5: return w

Algorithm 3 UPDATE(C,w)
Input: A gold clustering C = {C1, C2, ..., Cm};

The current parameters w.
Output: The updated parameters w.

1: X ← C1 ∪ C2 ∪ ... ∪ Cm = {x1, x2, ..., xn}
2: for i = 1 to n do
3: Ĉi ← {xi}
4: Ĉ ← {Ĉi}1≤i≤n
5: while |Ĉ| > 1 do
6: 〈Ĉi, Ĉj〉 = argmax

p∈P (Ĉ)
wT Φ(p)

7: B ← {〈Ĉk, Ĉl〉 ∈ P(Ĉ) | g(Ĉk, Ĉl|C) >
g(Ĉi, Ĉj |C)}

8: if B 6= ∅ then
9: 〈Ĉk, Ĉl〉 = argmax

p∈B
wT Φ(p)

10: w← w + Φ(Ĉk, Ĉl)− Φ(Ci, Cj)
11: if 〈Ĉi, Ĉj〉 = 〈∅, ∅〉 then
12: return w
13: replace Ĉi, Ĉj in Ĉ with Ĉi ∪ Ĉj
14: return w

12



Algorithms 2 and 3 show an incremental learning
model for the weight vector w that is parametrized
with the number of training epochs T and a set of
training clusterings C in which each clustering con-
tains the true coreference clusters from one docu-
ment. Algorithm 2 repeatedly uses all true cluster-
ings to update the current weight vector and instead
of the last computed weights it returns an averaged
weight vector to control for overfitting, as originally
proposed by Freund and Schapire (1999). The core
of the learning model is in the update procedure
shown in Algorithm 3. Like the greedy clustering of
Algorithm 1, it starts with an initial system cluster-
ing Ĉ that contains all singleton clusters. At every
step in the iteration (lines 5–13), it joins the high-
est scoring pair of clusters 〈Ĉi, Ĉj〉, computed ac-
cording to the current parameters. The iteration ends
when either the empty pair obtains the highest score
or everything has been joined into only one cluster.
The weight update logic is implemented in lines 7–
10: if a more accurate pair 〈Ĉk, Ĉl〉 can be found,
the highest scoring such pair is used in the percep-
tron update in line 10. If multiple cluster pairs obtain
the maximum score in lines 6 and 9, the algorithm
selects one of them at random. This is useful es-
pecially in the beginning, when the weight vector is
zero and consequently all cluster pairs have the same
score of 0. We define the goodness g(Ĉk, Ĉl|C) of a
proposed pair 〈Ĉk, Ĉl〉 with respect to the true clus-
teringC as the accuracy of the coreference pairs that
would be created if Ĉk and Ĉl were joined:

g(·) =

∣∣∣{(x, y)∈ Ĉk×Ĉl | ∃Ci∈C : x, y∈Ci}∣∣∣
|Ĉk| · |Ĉl|

(1)
It can be shown that this definition of the goodness
function selects a cluster pair (lines 7–9) that, when
joined, results in a clustering with a better pairwise
accuracy. Therefore, the algorithm can be seen as
trying to fit the training data by searching for param-
eters that greedily maximize the clustering accuracy,
while overfitting is kept under control by comput-
ing an averaged version of the parameters. We have
chosen to use a perceptron update for simplicity, but
the algorithm can be easily instantiated to accommo-
date other types of incremental updates, e.g. MIRA
(Crammer and Singer, 2003).

3 Expert Rules as Features

With the exception of mention detection which is
run separately, all the remaining 12 sieves men-
tioned in (Lee et al., 2011) are used as Boolean fea-
tures defined on cluster pairs, i.e. if any of the men-
tion pairs in the cluster pair 〈Ĉi, Ĉj〉 were linked
by sieve k, then the corresponding sieve feature
Φk(Ĉi, Ĉj) = 1. We used the implementation from
the Stanford CoreNLP package1 for all sieves, with a
modification for the PRONOUNMATCH sieve which
was split into 3 different sieves as follows:

• ITPRONOUNMATCH: this sieve finds an-
tecedents only for neutral pronouns it.

• ITSPRONOUNMATCH: this sieve finds an-
tecedents only for neutral possessive pronouns
its.

• OTHERPRONOUNMATCH: this is a catch-all
sieve for the remaining pronouns.

This 3-way split was performed in order to enable
the combination of the discourse salience features
captured by the pronoun sieves with the semantic
compatibility features for neutral pronouns that will
be introduced in the next section. The OTHER-
PRONOUNMATCH sieve works exactly as the orig-
inal PRONOUNMATCH: for a given non-neutral pro-
noun, it searches in the current sentence and the pre-
vious 3 sentences for the first mention that agrees in
gender and number with the pronoun. The candi-
date antecedents for the pronoun are ordered based
on a notion of discourse salience that favors syntac-
tic salience and document proximity (Raghunathan
et al., 2010).

4 Discourse Salience Features

The IT/SPRONOUNMATCH sieves use the same im-
plementation for finding the first matching candi-
date antecedent as the original PRONOUNMATCH.
However, unlike OTHERPRONOUNMATCH and the
other sieves that generate Boolean features, the neu-
tral pronoun sieves are used to generate real valued
features. If the neutral pronoun is the leftmost men-
tion in the cluster Ĉj from a cluster pair 〈Ĉi, Ĉj〉,
the corresponding normalized feature is computed
as follows:

1http://nlp.stanford.edu/software/corenlp.shtml

13



1. Let Sj = 〈S1j , S2j , ..., Snj 〉 be the sequence
of candidate mentions that precede the neutral
pronoun and agree in gender and number with
it, ordered from most salient to least salient.

2. Let Ai ⊆ Ĉi be the set of mentions in the clus-
ter Ĉi that appear before the pronoun and agree
with it.

3. For each mention m ∈ Ai, find its rank in the
sequence Sj :

rank(m,Sj) = k ⇔ m = Skj (2)

4. Find the minimum rank across all the mentions
in Ai and compute the feature as follows:

Φit/s(Ĉi, Ĉj) =

(
min
m∈Ai

rank(m,Sj)

)−1
(3)

If Ai is empty, set Φit/s(Ĉi, Ĉj) = 0.

The discourse salience feature described above is by
definition normalized in the interval [0, 1]. It takes
the maximum value of 1 when the most salient men-
tion in the discourse at the current position agrees
with the pronoun and also belongs to the candidate
cluster. The feature is 0 when the candidate cluster
does not contain any mention that agrees in gender
and number with the pronoun.

5 Semantic Compatibility Features

Each of the two types of neutral pronouns is associ-
ated with a new feature that computes the semantic
compatibility between the syntactic head of a candi-
date antecedent and the context of the neutral pro-
noun. If the neutral pronoun is the leftmost mention
in the cluster Ĉj from a cluster pair 〈Ĉi, Ĉj〉 and cj
is the pronoun context, then the new normalized fea-
tures Ψit/s(Ĉi, Ĉj) are computed as follows:

1. Compute the maximum semantic similarity be-
tween the pronoun context and any mention in
Ĉi that precedes the pronoun and is in agree-
ment with it:

Mj = max
m∈Ai

comp(m, cj)

2. Compute the maximum and minimum seman-
tic similarity between the pronoun context and
any mention that precedes the pronoun and is
in agreement with it:

Mall = max
m∈Sj

comp(m, cj)

mall = min
m∈Sj

comp(m, cj)

3. Compute the semantic compatibility feature as
follows:

Ψit/s(Ĉi, Ĉj) =
Mj −mall
Mall −mall

(4)

To avoid numerical instability, if the over-
all maximum and minimum similarities are
very close (Mall − mall < 1e−4) we set
Ψit/s(Ĉi, Ĉj) = 1.

Like the salience feature Φit/s, the semantic com-
patibility feature Ψit/s is normalized in the interval
[0, 1]. Its definition assumes that we can compute
comp(m, cj), the semantic compatibility between a
candidate antecedent mention m and the pronoun
context cj . For the possessive pronoun its, we ex-
tract the syntactic head h of the mention m and re-
place the pronoun with the mention head h in the
possessive context. We use the resulting possessive
pronoun context pcj(h) to define the semantic com-
patibility as the following conditional probability:

comp(m, cj) = logP (pcj(h)|h) (5)
= logP (pcj(h))− logP (h)

To compute the n-gram probabilities P (pcj(h)) and
P (h) in Equation 6, we use the language mod-
els provided by the Microsoft Web N-Gram Cor-
pus (Wang et al., 2010), as described in the next sec-
tion.

Figure 1 shows an example of a possessive neu-
tral pronoun context, together with the set of can-
didate antecedents that agree in number and gender
with the pronoun, from the current and previous 3
sentences. Each candidate antecedent is given an in-
dex that reflects its ranking in the discourse salience
based ordering. We see that discourse salience does
not help here, as the most salient mention is not
the correct antecedent. The figure also shows the

14



In 1946, the nine justices dismissed a case[7] involving
the apportionment[8] of congressional districts. That
view[6] would slowly change. In 1962, the court[3]
abandoned its[5] caution[4]. Finding remedies to the
unequal distribution[1] of political power[2] was indeed
within its constitutional authority.

[3] P (court’s constitutional authority | court)
≈ exp(−5.91)

[5] P (court’s constitutional authority | court) (*)
≈ exp(−5.91)

[7] P (case’s constitutional authority | case)
≈ exp(−8.32)

[2] P (power’s constitutional authority | power)
≈ exp(−9.30)

[8] P (app-nt’s constitutional authority | app-nt)
≈ exp(−9.32)

[4] P (caution’s constitutional authority | caution)
≈ exp(−9.39)

[1] P (dist-ion’s constitutional authority | dist-ion)
≈ exp(−9.40)

[6] P (view’s constitutional authority | view)
≈ exp(−9.69)

Figure 1: Possessive neutral pronoun example.

compatibility score computed for each candidate an-
tecedent, using the formula described above. In this
example, when ranking the candidate antecedents
based on their compatibility scores, the top ranked
mention is the correct antecedent, whereas the most
salient mention is down in the list.

When the set of candidate mentions contains pro-
nouns, we require that they are resolved to a nominal
or named mention, and use the head of this mention
to instantiate the possessive context. This is the case
of the pronominal mention [5] in Figure 1, which
we assumed was already resolved to the noun court
(even if the pronoun [5] were resolved to an incor-
rect mention, the noun court would still be ranked
first due to mention [3]). This partial ordering be-
tween coreference decisions is satisfied automati-
cally by setting the semantic compatibility feature
Ψit/s(Ĉi, Ĉj) = 0 whenever the antecedent cluster
Ĉi contains only pronouns.

A similar feature is introduced for all neutral
pronouns it appearing in subject-verb-object triples.

The letter[5] appears to be an attempt[6] to calm the
concerns of the current American administration[7]. “I
confirm my commitment[1] to the points made therein,”
Aristide said in the letter[2], “confident that they will
help strengthen the ties between our two nations where
democracy[3] and peace[4] will flourish.” Since 1994,
when it sent 20,000 troops to restore Aristide to power,
the administration ...

[7] P (administration sent troops | administration)
≈ exp(−6.00)

[2] P (letter sent troops | letter)
≈ exp(−6.57)

[5] P (letter sent troops | letter)
≈ exp(−6.57)

[4] P (peace sent troops | peace)
≈ exp(−7.92)

[6] P (attempt sent troops | attempt)
≈ exp(−8.26)

[3] P (democracy sent troops | democracy)
≈ exp(−8.30)

[1] P (commitment sent troops | commitment)
≈ exp(−8.62)

Figure 2: Neutral pronoun example.

The new pronoun context pcj(h) is obtained by
replacing the pronoun it in the subject-verb-object
context cj with the head h of the candidate an-
tecedent mention. Figure 2 shows a neutral pro-
noun context, together with the set of candidate an-
tecedents that agree in number and gender with the
pronoun, from an abridged version of the original
current and previous 3 sentences. Each candidate
antecedent is given an index that reflects its ranking
in the discourse salience based ordering. Discourse
salience does not help here, as the most salient men-
tion is not the correct antecedent. The figure shows
the compatibility score computed for each candidate
antecedent, using Equation 6. In this example, the
top ranked mention in the compatibility based order-
ing is the correct antecedent, whereas the most most
salient mention is at the bottom of the list.

To summarize, in the last two sections we de-
scribed two special features for neutral pronouns:
the discourse salience feature Φit/s and the seman-
tic compatibility feature Ψit/s. The two real-valued

15



Candidate mentions Original context N-gram context
capital, store, GE, side, offer with its corporate tentacles reaching GE’s corporate tentacles
AOL, Microsoft, Yahoo, product its substantial customer base AOL’s customer base
regime, Serbia, state, EU, embargo meets its international obligations Serbia’s international obligations
company, secret, internet, FBI it was investigating the incident FBI was investigating the incident
goal, team, realm, NHL, victory something it has not experienced since NHL has experienced
Onvia, line, Nasdaq, rating said Tuesday it will cut jobs Onvia will cut jobs
coalition, government, Italy but it has had more direct exposure Italy has had direct exposure
Pinochet, arrest, Chile, court while it studied a judge ’s explanation court studied the explanation

Table 1: N-gram generation examples.

features are computed at the level of cluster pairs as
described in Equations 3 and 4. Their computation
relies on the mention level rank (Equation 2) and se-
mantic compatibility (Equation 6) respectively.

6 Web-based Language Models

We used the Microsoft Web N-Gram Corpus2 to
compute the pronoun context probability P (pcj(h))
and the candidate head probability P (h). This
corpus provides smoothed back-off language mod-
els that are computed dynamically from N-gram
statistics using the CALM algorithm (Wang and Li,
2009). The N-grams are collected from the tok-
enized versions of the billions of web pages indexed
by the Bing search engine. Separate models have
been created for the document body, the document
title and the anchor text. In our experiments, we
used the April 2010 version of the document body
language models. The number of words in the pro-
noun context and the antecedent head determine the
order of the language models used for estimating the
conditional probabilities. For example, to estimate
P (administration sent troops | administration), we
used a trigram model for the context probability
P (administration sent troops) and a unigram model
for the head probability P (administration). Since
the maximum order of the N-grams available in the
Microsoft corpus is 5, we designed the context and
head extraction rules to return N-grams with size
at most 5. Table 1 shows a number of examples
of N-grams generated from the original contexts, in
which the pronoun was replaced with the correct an-
tecedent. To get a sense of the utility of each con-
text in matching the right antecedent, the table also

2http://web-ngram.research.microsoft.com

shows a sample of candidate antecedents.
For possessive contexts, the N-gram extraction

rules use the head of the NP context and its clos-
est premodifier whenever available. Using the pre-
modifier was meant to increase the discriminative
power of the context. For the subject-verb-object
N-grams, we used the verb at the same tense as in
the original context, which made it necessary to also
include the auxiliary verbs, as shown in lines 4–7 in
the table. Furthermore, in order to keep the gener-
ated N-grams within the maximum size of 5, we did
not include modifiers for the subject or object nouns,
as illustrated in the last line of the table. Some of
the examples in the table also illustrate the limits of
the context-based semantic compatibility feature. In
the second example, all three company names are
equally good matches for the possessive context. In
these situations, we expect the discourse salience
feature to provide the additional information neces-
sary for extracting the correct antecedent. This com-
bination of discourse salience with semantic com-
patibility features is done in the adaptive clustering
algorithm introduced in Section 2.

7 Experimental Results

We compare our adaptive clustering (AC) approach
with the state of the art deterministic sieves (DT)
system of Lee et al. (2011) on the newswire portion
of the ACE-2004 dataset. The newswire section of
the corpus contains 128 documents annotated with
gold mentions and coreference information, where
coreference is marked only between mentions that
belong to one of seven semantic classes: person, or-
ganization, location, geo-political entity, facility, ve-
hicle, and weapon. This set of documents has been
used before to evaluate coreference resolution sys-

16



System Mentions P R F1
DT Gold, all 88.1 73.3 80.0
AC Gold, all 88.7 73.5 80.4
DT Gold, neutral 82.5 51.5 63.4
AC Gold, neutral 83.0 52.1 64.0
DT Auto, neutral 84.4 34.9 49.3
AC Auto, neutral 86.1 40.0 54.6

Table 2: B3 comparative results on ACE 2004.

tems in (Poon and Domingos, 2008; Haghighi and
Klein, 2009; Raghunathan et al., 2010), with the best
results so far obtained by the deterministic sieve sys-
tem of Lee at al. (2011). There are 11,398 annotated
gold mentions, out of which 135 are possessive neu-
tral pronouns its and 88 are neutral pronouns it in
a subject-verb-object triple. Given the very small
number of neutral pronouns, in order to obtain re-
liable estimates for the model parameters we tested
the adaptive clustering algorithm in a 16 fold cross-
validation scenario. Thus, the set of 128 documents
was split into 16 folds, where each fold contains 120
documents for training and 8 documents for testing.
The final results were pooled together from the 16
disjoint test sets. During training, the AC’s update
procedure was run for 10 epochs. Since the AC al-
gorithm does not need to tune any hyper parameters,
there was no need for development data.

Table 2 shows the results obtained by the two sys-
tems on the newswire corpus under three evaluation
scenarios. We use the B3 version of the precision
(P), recall (R), and F1 measure, computed either on
all mention pairs (all) or only on links that contain at
least one neutral pronoun (neutral) marked as a men-
tion in ACE. Furthermore, we report results on gold
mentions (Gold) as well as on mentions extracted
automatically (Auto). Since the number of neutral
pronouns marked as gold mentions is small com-
pared to the total number of mentions, the impact
on the overall performance shown in the first two
rows is small. However, when looking at corefer-
ence links that contain at least one neutral pronoun,
the improvement becomes substantial. AC increases
F1 with 5.3% when the mentions are extracted auto-
matically during testing, a setting that reflects a more
realistic use of the system. We have also evaluated
the AC approach in the Gold setting using only the

original DT sieves as features, obtaining an F1 of
80.3% for all mentions and 63.4% – same as DT –
for neutral pronouns.

By matching the performance of the DT system in
the first two rows of the table, the AC system proves
that it can successfully learn the relative importance
of the deterministic sieves, which in (Raghunathan
et al., 2010) and (Lee et al., 2011) have been manu-
ally ordered using a separate development dataset.
Furthermore, in the DT system the sieves are ap-
plied on mentions in their textual order, whereas the
adaptive clustering algorithm AC does not assume
a predefined ordering among coreference resolution
decisions. Thus, the algorithm has the capability to
make the first clustering decisions in any section of
the document in which the coreference decisions are
potentially easier to make. We have run experiments
in which the AC system was augmented with a fea-
ture that computed the normalized distance between
a cluster and the beginning of the document, but this
did not lead to an improvement in the results, lend-
ing further credence to the hypothesis that a strictly
left to right ordering of the coreference decisions is
not necessary, at least with the current features.

The same behavior, albeit with smaller increases
in performance, was observed when the DT and AC
approaches were compared on the newswire section
of the development dataset used in the CoNLL 2011
shared task (Pradhan et al., 2011). For these exper-
iments, the AC system was trained on all 128 docu-
ments from the newswire portion of ACE 2004. On
gold mentions, the DT and AC systems obtained a
very similar performance. When evaluated only on
links that contain at least one neutral pronoun, in a
setting where the mentions were automatically de-
tected, the AC approach improved the F1 measure
over the DT system from 58.6% to 59.1%. One rea-
son for the smaller increase in performance in the
CoNLL experiments could be given by the different
annotation schemes used in the two datasets. Com-
pared to ACE, the CoNLL dataset does not include
coreference links for appositives, predicate nomi-
nals or relative pronouns. The different annotation
schemes may have led to mismatches in the training
and test data for the AC system, which was trained
on ACE and tested on CoNLL. While we tried to
control for these conditions during the evaluation
of the AC system, it is conceivable that the differ-

17



System Mentions P R F1
DT Auto, its 86.0 46.9 60.7
AC Auto, its 91.7 47.5 62.6

Table 3: B3 comparative results on CoNLL 2011.

ences in annotation still had some effect on the per-
formance of the AC approach. Another cause for
the smaller increase in performance was that the
pronominal contexts were less discriminative in the
CoNLL data, especially for the neutral pronoun it.
When evaluated only on links that contained at least
one possessive neutral pronoun its, the improvement
in F1 increased at 1.9%, as shown in Table 3.

8 Related Work

Closest to our clustering approach from Section 2
is the error-driven first-order probabilistic model of
Culotta et al. (2007). Among significant differences
we mention that our model is non-probabilistic, sim-
pler and easier to understand and implement. Fur-
thermore, the update step does not stop after the
first clustering error, instead the algorithm learns and
uses a clustering threshold τ to determine when to
stop during training and testing. This required the
design of a method to order cluster pairs in which the
clusters may not be consistent with the true coref-
erence chains, which led to the introduction of the
goodness function in Equation 1 as a new scoring
measure for cluster pairs. The strategy of contin-
uing the clustering during training as long as a an
adaptive threshold is met better matches the training
with the testing, and was observed to lead to better
performance. The cluster ranking model of Rahman
and Ng (2009) proceeds in a left-to-right fashion and
adds the current discourse old mention to the highest
scoring preceding cluster. Compared to it, our adap-
tive clustering approach is less constrained: it uses
only a weak, partial ordering between coreference
decisions, and does not require a singleton cluster at
every clustering step. This allows clustering to start
in any section of the document where coreference
decisions are easier to make, and thus create accu-
rate clusters earlier in the process.

The use of semantic knowledge for coreference
resolution has been studied before in a number of
works, among them (Ponzetto and Strube, 2006),

(Bengtson and Roth, 2008), (Lee et al., 2011), and
(Rahman and Ng, 2011). The focus in these studies
has been on the semantic similarity between a men-
tion and a candidate antecedent, or the parallelism
between the semantic role structures in which the
two appear. One of the earliest methods for using
predicate-argument frequencies in pronoun resolu-
tion is that of Dagan and Itai (1990). Closer to our
use of semantic compatibility features for pronouns
are the approaches of Kehler et al. (2004) and Yang
et al. (2005). The last work showed that pronoun
resolution can be improved by incorporating seman-
tic compatibility features derived from search engine
statistics in the twin-candidate model. In our ap-
proach, we use web-based language models to com-
pute semantic compatibility features for neutral pro-
nouns and show that they can improve performance
over a state-of-the-art coreference resolution system.
The use of language models instead of search engine
statistics is more practical, as they eliminate the la-
tency involved in using search engine queries. Web-
based language models can be built on readily avail-
able web N-gram corpora, such as Google’s Web 1T
5-gram Corpus (Brants and Franz, 2006).

9 Conclusion

We described a novel adaptive clustering method
for coreference resolution and showed that it can
not only learn the relative importance of the origi-
nal expert rules of Lee et al. (2011), but also ex-
tend them effectively with new semantic compati-
bility features. Experimental results show that the
new method improves the performance of the state
of the art deterministic system and obtains a sub-
stantial improvement for neutral pronouns when the
mentions are extracted automatically.

Acknowledgments

We would like to thank the anonymous reviewers for
their helpful suggestions. This work was supported
by grant IIS-1018590 from the NSF. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author and
do not necessarily reflect the views of the NSF.

18



References
Eric Bengtson and Dan Roth. 2008. Understanding the

value of features for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 294–303, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.

Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1.

Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951–991.

Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81–88, Rochester,
New York, April. Association for Computational Lin-
guistics.

Ido Dagan and Alon Itai. 1990. Automatic processing
of large corpora for the resolution of anaphora refer-
ences. In Proceedings of the 13th conference on Com-
putational linguistics - Volume 3, COLING’90, pages
330–332.

Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277–296.

Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152–1161, Singapore, August.

Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
HLT-NAACL 2004: Main Proceedings, pages 289–
296, Boston, Massachusetts, USA. Association for
Computational Linguistics.

Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28–34.

Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 192–199.

Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.

In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Honolulu,
Hawaii, October.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–27.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher D. Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP’10), pages 492–501.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968–977, Singapore, Au-
gust. Association for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814–824, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Kuansan Wang and Xiaolong Li. 2009. Efficacy of a con-
stantly adaptive language modeling technique for web-
scale applications. In Proceedings of the 2009 IEEE
International Conference on Acoustics, Speech and
Signal Processing, ICASSP ’09, pages 4733–4736,
Washington, DC, USA. IEEE Computer Society.

Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, and Bo-june (Paul) Hsu. 2010. An
overview of microsoft web n-gram corpus and appli-
cations. In Proceedings of the NAACL HLT 2010
Demonstration Session, HLT-DEMO ’10, pages 45–
48, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 165–172.

19


