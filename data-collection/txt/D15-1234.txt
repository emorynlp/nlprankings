



















































A quantitative analysis of gender differences in movies using psycholinguistic normatives


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1996–2001,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

A quantitative analysis of gender differences in movies using
psycholinguistic normatives

Anil Ramakrishna1, Nikolaos Malandrakis1, Elizabeth Staruk1, Shrikanth Narayanan1, 2
1Department of Computer Science,

2Department of Electrical Engineering,
University of Southern California, Los Angeles, USA

{akramakr,malandra,staruk}@usc.edu, shri@sipi.usc.edu

Abstract

Direct content analysis reveals important
details about movies including those of
gender representations and potential bi-
ases. We investigate the differences be-
tween male and female character depic-
tions in movies, based on patterns of lan-
guage used. Specifically, we use an au-
tomatically generated lexicon of linguis-
tic norms characterizing gender ladenness.
We use multivariate analysis to investigate
gender depictions and correlate them with
elements of movie production. The pro-
posed metric differentiates between male
and female utterances and exhibits some
interesting interactions with movie genres
and the screenplay writer gender.

1 Introduction

Gender has been an important research topic in the
social sciences, with studies conducted on the ef-
fect of gender on various aspects of human percep-
tion and expression (Benshoff and Griffin, 2011)
as well as investigations of the societal (Behm-
Morawitz and Mastro, 2008) and career implica-
tions of gender and possible underlying biases.
Previous studies report significant implications of
gender on career progress in medicine (Sidhu et
al., 2009), information technology (Cohoon and
Aspray, 2006), politics (Niven, 2006) and show-
business (Smith, 2010).

In this paper we investigate the depictions of
the genders in feature films, through the analysis
of their respective dialogues. The differences in
depiction are a contentious subject, since aspects
of these can be viewed as the result of stereotyp-
ing or gender bias, with the relative presence of
women being a well investigated subject (Bielby
and Bielby, 1996; Lincoln and Allen, 2004). We
are interested in the existing gender depictions, re-

gardless of relative frequencies, as well as any fac-
tors that may affect them. While popular tools
such as the Bechdel test provide a test for detecting
female presence in the movies, we hope to iden-
tify more subtle forms of gender differences across
character gender from the dialogues. Our aim is
to devise a non-binary metric that can be used to
compare or rank movies, characters and perhaps
individual utterances.

To analyze the dialogues we propose using
a metric of language gender ladenness, a num-
ber representing a normative rating of the “per-
ceived feminine or masculine association” (Paivio
et al., 1968) of language. The metric, as origi-
nally defined, is meant to provide an indication
of gender-specificity of individual words, with ex-
treme values assigned to highly stereotypical con-
cepts. Generating this rating for male and fe-
male character dialogues and comparing the char-
acter gender with this rating of “language gender”
should allow us to observe stereotypical behavior.

Word based ratings such as the gender laden-
ness are referred to as linguistic norms (or psy-
cholinguistic norms when corresponding to psy-
chological constructs) and are popular in cognitive
psychology (Clark and Paivio, 2004) and some
computational disciplines, such as sentiment anal-
ysis (Nielsen, 2011) and opinion mining. To uti-
lize gender ladenness, we follow an approach sim-
ilar to simple sentiment analysis, with word-level
norms automatically generated based on a small
starting set of manually annotated norms and sen-
tence (and higher) level norms estimated through
word-level norm statistics. The resulting algo-
rithm allows us to estimate gender ladenness at
any arbitrary granularity.

We use these ratings of dialogue language to
quantify the depictions of male and female char-
acters and attempt to relate the observed gender
ladenness with objective factors.

In section 2 and 3 we describe the data corpus

1996



Word Norm
Infantry -0.97
Truck -0.73
Dictator -0.56
Strider -0.36
United -0.18
Volunteerism 0.04
Hygiene 0.22
Candle 0.45
Radiant 0.66
Bride 0.84
Gorgeous 0.96

Table 1: Sample word norms(∈ [−1, 1]); −1:
Most masculine and +1: Most feminine.

and the feature extraction process respectively. We
detail the experimental procedure in section 4 and
analysis in section 5 and conclude with future ex-
tensions in section 6.

2 Estimating Gender Ladenness

Gender Ladenness, as defined in (Clark and
Paivio, 2004) represents the degree of perceived
“feminine or masculine association” on a numeri-
cal scale ranging from very masculine to very fem-
inine. It is important to note that there was no re-
striction to what “association” may mean: while
it is reasonable to assume that associations of the
form “A is B” or “A has B” would dominate an-
notator perception, that does not preclude other
forms of association. Because of that, referring to
the norms as indicators of how masculine or femi-
nine the words are is not entirely accurate, though
it is a reasonable approximation. The original rat-
ings were re-scaled to [−1, 1] for our purposes,
with lower values indicating a masculine associ-
ation and high values indicating a feminine asso-
ciation. Some sample words, utterances and their
corresponding ratings are presented in Table 1 and
Table 3. Figure 1 shows the average gender laden-
ness across all utterances for the major characters
of a few movies. The annotations as a whole are
reflective of stereotypical views of gender roles,
e.g., words related to war and violence have a
strong masculine association, whereas words re-
lated to family or positive emotions carry strong
feminine associations.

The manual annotations from (Clark and Paivio,
2004) contain ratings for only 925 words, which
are not enough to provide sufficient coverage.

-0.05 -0.04 -0.03 -0.02 -0.01 0.0 0.01 0.02
Gender Ladenness

AH

BC

KB

PF

Alvy Annie

Andrew Brian

Bender

Claire

Bride Bill

Jules Vincent Butch

Avg Gender Ladenness for Major Characters

Figure 1: Average Gender Ladenness for a few
sample movies, marker size proportional to num-
ber of utterances. Filled markers: Female charac-
ters, Hollow markers: Male characters; PF: Pulp
Fiction, KB: Kill Bill, BC: Breakfast Club, AH:
Annie Hall.

Therefore we use a lexicon expansion method, in-
spired by the work of (Malandrakis et al., 2013)
to estimate the gender ladenness ĝ(wi) of word wi
using the semantic similarities s() between wi and
reference words or concepts cj , as

ĝ(wi) = θ0 +
N∑

j=1

θjs(wi, cj), (1)

where the terms θi are trained model parameters.
Given a manually annotated lexicon and a set of

reference words, this equation can be used to cre-
ate a linear system. Solving the system via Least
Squares Estimation (LSE) gives us the parameters
θ and an equation that can be used to generate gen-
der ladenness for any new set of words.

Gender ladenness for larger lexical units is gen-
erated via simple statistics, as the average of word
gender ladenness over all content words (adjec-
tives, nouns, verbs and adverbs).

3 Data

Our primary data source is the Movie DiC corpus
(Banchs, 2012) which includes 619 movie scripts
parsed from The Internet Movie Script Database
(IMSDb, 2015). The xml formatted scripts con-
tain transcripts with speaker information as well as
some structural information. Additional metadata
for each movie were collected from the Internet
Movie Database (IMDb, 2015).

1997



Since our goal was to analyze gender depic-
tions, we had to annotate each script utterance with
a gender label. The process was complicated by
inconsistencies between the information contained
in the IMDb and Movie DiC corpora, like mis-
matched names, particularly for minor characters.
Initially the script character names were cleaned
using simple heuristics, such as the removal of
all instances of the possessive “’s”. The IMDb
api (IMDbPY, 2015) was used to recover candi-
date movies matching the script movie name and,
in the case of multiple candidates, the best candi-
date was selected based on the number of character
names matching the script. Character names were
compared using the Jaro-Winkler distance (Win-
kler, 1990). Having achieved a one to one map-
ping between IMDb and Movie DiC, we assigned
a gender label to each matched character, using
the gender predictor (NamSor Applied Onomas-
tics, 2015). To make these predictions, we first
use the name of the corresponding actor portraying
that role; if there was no character match, we use
the name of the character. Finally, we calculate a
confidence score of our gender assignment per ut-
terance for each movie, equal to the percentage of
utterances with perfectly matched character name
and a high confidence by the gender predictor. For
the movies for which the confidence scores are not
satisfactory, we manually match the script charac-
ters with IMDb’s characters and annotate genders.
In our experiments, we did this manual annotation
with roughly 75 movies.

Having a mapping of scripts to IMDb en-
tries, we collected more information about the
movie such as the list of genres it belongs to and
the members of the production team (producers,
scriptwriters, directors), and followed a similar
process as described above to assign genders to all
persons. While movies may be created by multi-
ple scriptwriters and directors, we retain only the
first name, the primary credit, in each category.
We removed infrequent genres and movies which
belonged only to the removed genres. We also fil-
tered out utterances with missing or incorrect char-
acter information and the utterances correspond-
ing to characters for which the gender predictor
fails to make confident predictions.

Movies with missing fields were also removed,
leaving us with a total of 568 movies after the
aforementioned pre-processing steps. Table 2 lists
some descriptive statistics of the processed movie

Property Name Female Male Total
Movie Utterances 107372 256274 363646

Producers 746 2974 3720
Directors 33 572 605

Assistant Directors 846 2739 3585
Screenplay Writers 130 1217 1347
Casting Directors 548 195 743

Table 2: Movie Dataset statistics

Utterance Avg. GL
Flowers for the Diva. 0.77
Yeah, what a woman. 0.47

Got the house to yourselves? -0.01
What about the crew? -0.51

Yeah? You and what army? -0.85

Table 3: Average gender ladenness for sample ut-
terances from the dataset

corpus. At least in terms of raw frequencies, the
gender ratio is clearly skewed towards male, par-
ticularly in the case of directors and with the ex-
ception of casting directors.

The norm generating equation (1) requires a se-
mantic similarity estimate s(), which for the pur-
poses of this paper is the cosine of context vectors
generated over a large corpus of raw web text. The
corpus was created by posing a query to the Ya-
hoo! search engine for every word in the English
version of the aspell spell-checker and collecting
the top 500 result previews. Each preview is com-
posed of a title and a sample of the content, each
being a single sentence. Overall the collected cor-
pus contains approximately 117 million sentences.

4 Experimental Procedure

The descriptive feature in this method is gender
ladenness, so we extracted an estimate for each
utterance of every movie. Initially, all utterances
were part-of-speech tagged and non-content words
were removed. Then, word-level gender landen-
ness norms were generated for every remaining
word.

To generate word-level norms, we used equa-
tion (1) with the intermediate seed words wi be-
ing the top 10000 most frequent words in our cor-
pus of web text with length longer that 3 char-
acters. For each word in our corpus, we gener-
ated a binary weighted context vector (of window
size 1) of size ∼ 125000. Then, for each word

1998



of interest we calculated a 10000 place similar-
ity vector, containing the cosine similarity scores
between the context vector of said word and the
context vectors of the 10000 intermediate seeds.
Using the training set we generated a K × 10000
matrix of similarities to the seed words and ap-
plied dimensionality reduction via Principal Com-
ponent Analysis (PCA), keeping the firstN = 300
components. These transformed similarities be-
came the similarity terms s() of equation (1) and
were used to train the model. For any word in
the scripts, a 10000 place similarity vector is gen-
erated and transformed using the pre-calculated
PCA matrix, then equation (1) is used to create
the gender ladenness estimate.

Ratings were generated at the utterance level,
and collective ratings (per character, gender or
movie) were calculated as utterance rating aver-
ages.

5 Results

To evaluate the word norm generation algorithm,
we performed a 10-fold cross-validation experi-
ment on the 925 manually annotated norms in
(Paivio et al., 1968). The generated norms were
evaluated against the ground truth and the method
achieved a 0.801 Pearson correlation to the ground
truth. While there is no comparable result in lit-
erature, the resulting performance appears suffi-
ciently high.

We first investigated the overall gender laden-
ness of movies, represented as the average of all
utterance level scores, with respect to the genre(s)
the movie belongs to. The independent variables
for this analysis were nine binary indicator vari-
ables, one for each of the most frequent genre la-
bels in our movie corpus, with values of zero if
the movie does not belong to the specific genre
and one if it does. The particular representa-
tion of genres as separate variables was chosen
because each movie can belong to multiple gen-
res. Interaction terms were included. Running n-
way ANOVA with the aggregate gender ladenness
across both character genders as the dependent
variable revealed significant differences between
genres, with Action movies leaning towards the
masculine (p = 0.013) compared to Non-Action
movies, a not surprising result.

A few significant interactions between genres
are shown in figure 2. Fig. 2a indicates that among
non-drama movies, romantic movies tend to in-

Drama Others
−12

−10

−8

−6

−4

−2
x 10

−3

 

 

Rom

Oth

Crime Others
−0.014

−0.012

−0.01

−0.008

 

 

Thr

Oth

(a) Drama v/s Romance (b) Crime v/s Thriller

Action Others
−0.018

−0.016

−0.014

−0.012

−0.01

−0.008

 

 

Dra

Oth

Adventure Others
−0.02

−0.01

0

0.01

 

 

Mys

Oth

(c) Action v/s Drama (d) Adventure v/s Mystery

Figure 2: Interactions between genres

clude more feminine language compared to non-
romantic movies. However, if a movie belongs
to the genre drama, its mean gender ladenness
scores remain fairly constant, irrespective of its
other genres. Similar interpretations can be drawn
from the other plots in figure 2.

To analyze the effect of character gender on
the gender ladenness scores, we next ran ANOVA
with the character gender and the movie writer’s
gender as additional independent variables. The
dependent variable in this case was the aggregate
gender ladenness score across all utterances for
male and female characters, so two scores per
movie. The interaction of character gender and
movie genre is shown in figure 3. The scores of
male and female characters are correlated, which
can be attributed to the underlying concepts in
the utterances from these movies. The differ-
ence between genders is significant (p = 0.034),
with male characters consistently using signifi-
cantly more masculine language than their female
counterparts, a finding that lends some credence
to the metric used. Looking at the binary genre
variables revealed that

Action movies contained significantly more
masculine language than Non-Action movies (p
< 10−5) and the same holds for Crime movies (p
< 10−5). Conversely, Romantic movies leaned
towards the more feminine language than non-
Romantic movies (p < 10−5) and similarly for
Comedy movies compared to non-Comedy movies
(p = 0.02). The male - female character gender

1999



−0.05 −0.04 −0.03 −0.02 −0.01 0

Mystery

Romance

Comedy

Crime

Drama

Sci−Fi

Thriller

Action

Adventure

Feature mean

G
e
n
re

Mean feature value across character gender

 

 

Female

Male

Figure 3: Interaction of movie genre with charac-
ter gender

ladenness distance however is not affected in any
significant way by the genre.

We include only the screenplay writer’s gen-
der in our analysis, though both the directors and
screenplay writers influence the dialog lines (utter-
ances), since the writers are more likely to directly
influence the actual language used. In addition, the
very small number of female directors in the data,
as seen in table 2, leads to a violation of ANOVA’s
homoscedasticity assumption. Though the writer
gender itself was not a significant factor, the inter-
action of writer’s gender with the Action genre was
significant (p = 0.005). The plot illustrating this
interaction is shown in figure 4. It appears that
female script writers write more masculine utter-
ances compared to their male colleagues, at least
for Action movies. We also investigated interac-
tions between the writer and character gender, but
none proved significant.

6 Conclusions and Future Work

We used regression to extrapolate manually an-
notated psycholinguistic normatives to movie ut-
terances and investigated the use of these met-
rics to describe gender depictions. The metric
proved successful, showing significant differences
between the genders and predictable patterns with
respect to movie genres.

Future work will include the use of further met-
rics, with those describing emotions being the first
candidates. We also intend to collect more movie
and character level metadata to be used in anal-
ysis. Finally, it is worth remembering that lan-
guage provides only a partial description of de-

−0.04 −0.02 0 0.02

Action

Crime

Thriller

Romance

Comedy

Drama

Mystery

Adventure

Feature mean

G
e
n
re

Mean feature value across writer gender

 

 

Female

Male

Figure 4: Interaction of screenplay writer’s gender
with genre

picted characters, so we should aim to augment
with aural/visual information.

7 Acknowledgements

The authors gratefully acknowledge support from
NSF, Geena Davis Institute on Gender in Media
and Google.org.

References
Rafael E Banchs. 2012. Movie-Dic: a movie dia-

logue corpus for research and development. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers-
Volume 2, pages 203–207. Association for Compu-
tational Linguistics.

Elizabeth Behm-Morawitz and Dana E Mastro. 2008.
Mean girls? the influence of gender portrayals in
teen movies on emerging adults’ gender-based atti-
tudes and beliefs. Journalism & Mass Communica-
tion Quarterly, 85(1):131–146.

Harry M Benshoff and Sean Griffin. 2011. America on
film: Representing race, class, gender, and sexuality
at the movies. John Wiley & Sons.

Denise D Bielby and William T Bielby. 1996. Women
and men in film gender inequality among writers in a
culture industry. Gender & Society, 10(3):248–270.

James M Clark and Allan Paivio. 2004. Extensions of
the Paivio, Yuille, and Madigan (1968) norms. Be-
havior Research Methods, Instruments, & Comput-
ers, 36(3):371–383.

J McGrath Cohoon and William Aspray. 2006. Women
and information technology: Research on underrep-
resentation, volume 1. The MIT Press.

2000



IMDb. 2015. Internet movie database. [Online; ac-
cessed 10-June-2015].

IMDbPY. 2015. [Online; accessed 10-June-2015].

IMSDb. 2015. Internet movie script database. [On-
line; accessed 10-June-2015].

Anne E Lincoln and Michael Patrick Allen. 2004.
Double jeopardy in hollywood: Age and gender in
the careers of film actors, 1926–1999. Sociological
Forum, 19(4):611–631.

Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013. Distri-
butional semantic models for affective text analy-
sis. Audio, Speech, and Language Processing, IEEE
Transactions on, 21(11):2379–2392, Nov.

NamSor Applied Onomastics. 2015. NamSor gender
API. [Online; accessed 10-June-2015].

Finn Årup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
arXiv preprint arXiv:1103.2903.

David Niven. 2006. Throwing your hat out of the
ring: Negative recruitment and the gender imbalance
in state legislative candidacy. Politics & Gender,
2(04):473–489.

Allan Paivio, John C Yuille, and Stephen A Madigan.
1968. Concreteness, imagery, and meaningfulness
values for 925 nouns. Journal of experimental psy-
chology, 76(1p2):1.

Reena Sidhu, Praveen Rajashekhar, Victoria L Lavin,
Joanne Parry, James Attwood, Anita Holdcroft, and
David S Sanders. 2009. The gender imbalance in
academic medicine: a study of female authorship in
the united kingdom. Journal of the Royal Society of
Medicine, 102(8):337–342.

Stacy L Smith. 2010. Gender oppression in cinematic
content? a look at females on-screen & behind-
the-camera in top-grossing 2007 films. Retrieved
September, 2:2010.

William E Winkler. 1990. String comparator met-
rics and enhanced decision rules in the fellegi-sunter
model of record linkage.

2001


