



















































Heuristic Search for Non-Bottom-Up Tree Structure Prediction


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 899–908,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Heuristic Search for Non-Bottom-Up Tree Structure Prediction

Andrea Gesmundo
Department of Computer Science

University of Geneva
andrea.gesmundo@unige.ch

James Henderson
Department of Computer Science

University of Geneva
james.henderson@unige.ch

Abstract

State of the art Tree Structures Prediction
techniques rely on bottom-up decoding. These
approaches allow the use of context-free fea-
tures and bottom-up features. We discuss
the limitations of mainstream techniques in
solving common Natural Language Process-
ing tasks. Then we devise a new framework
that goes beyond Bottom-up Decoding, and
that allows a better integration of contextual
features. Furthermore we design a system that
addresses these issues and we test it on Hierar-
chical Machine Translation, a well known tree
structure prediction problem. The structure
of the proposed system allows the incorpora-
tion of non-bottom-up features and relies on
a more sophisticated decoding approach. We
show that the proposed approach can find bet-
ter translations using a smaller portion of the
search space.

1 Introduction

Tree Structure Prediction (TSP) techniques have
become relevant in many Natural Language Pro-
cessing (NLP) applications, such as Syntactic Pars-
ing, Semantic Role Labeling and Hierarchical Ma-
chine Translation (HMT) (Chiang, 2007). HMT
approaches have a higher complexity than Phrase-
Based Machine Translation techniques, but exploit
a more sophisticated reordering model, and can
produce translations with higher Syntactic-Semantic
quality.

TSP requires as inputs: a weighted grammar, G,
and a sequence of symbols or a set of sequences en-
coded as a Lattice (Chappelier et al., 1999). The

input sequence is often a sentence for NLP applica-
tions. Tree structures generating the input sequence
can be composed using rules, r, from the weighted
grammar, G. TSP techniques return as output a tree
structure or a set of trees (forest) that generate the
input string or lattice. The output forest can be rep-
resented compactly as a weighted hypergraph (Klein
and Manning, 2001). TSP tasks require finding the
tree, t, with the highest score, or the best-k such
trees. Mainstream TSP relies on Bottom-up Decod-
ing (BD) techniques.

With this paper we propose a new framework
as a generalization of the CKY-like Bottom-up ap-
proach. We also design and test an instantiation of
this framework, empirically showing that wider con-
textual information leads to higher accuracy for TSP
tasks that rely on non-local features, like HMT.

2 Beyond Bottom-up Decoding

TSP decoding requires scoring candidate trees,
cost(t). Some TSP tasks require only local features.
For these cases cost(t) depends only on the local
score of the rules that compose t :

cost(t) =
∑

ri∈t
cost(ri ) (1)

This is the case for Context Free Grammars. More
complex tasks need non-local features. Those fea-
tures can be represented by a non-local factor,
nonLocal(t), into the overall t score:

cost(t) =
∑

ri∈t
cost(ri ) + nonLocal(t) (2)

899



For example, in HMT the Language Model (LM) is
a non-local fundamental feature that approximates
the adequacy of the translation with the sum of log-
probabilities of composing n-grams.

CKY-like BD approaches build candidate trees in
a bottom-up fashion, allowing the use of Dynamic
Programming techniques to simplify the search
space by mering sub-trees with the same state, and
also easing application of pruning techniques (such
as Cube Pruning, e.g. Chiang (2007), Gesmundo
(2010)). For clarity of presentation and follow-
ing HMT practice, we will henceforth restrict our
focus to binary grammars. Standard CKY works
by building objects known as items (Hopkins and
Langmead, 2009). Each item, ι, corresponds to a
candidate sub-tree. Items are built linking a rule
instantiation, r, to two sub-items that represents
left context, ι1, and right context, ι2; formally:
ι ≡ 〈 ι1 ⋗ r ⋖ ι2 〉. An item is a triple that
contains a span, a postcondition and a carry. The
span contains the indexes of the starting and end-
ing input words delimiting the continuous sequence
covered by the sub-tree represented by the item. The
postcondition is a string that represents r’s head non-
terminal label, telling us which rules may be applied.
The carry, κ, stores extra information required to
correctly score the non-local interactions of the item
when it will be linked in a broader context (for HMT
with LM the carry consists of boundary words that
will form new n-grams).

Items, ι ≡ 〈ι1 ⋗ r ⋖ ι2〉, are scored according to
the following formula:

cost(ι) = cost(r) + cost(ι1) + cost(ι2) (3)

+ interaction(r, κ1, κ2)

Where: cost(r) is the cost associated to the weighted
rule r; cost(ι1) and cost(ι2) are the costs of the two
sub-items computed recursively using formula (3);
interaction(r, κ1, κ2) is the interaction cost between
the rule instantiation and the two sub-items. In HMT
the interaction cost includes the LM score of new n-
grams generated by connecting the childrens’ sub-
spans with terminals of r. Notice that formula (3) is
equal to formula (2) for items that cover the whole
input sequence.

In many TSP applications, the search space is
too large to allow an exhaustive search and there-

fore pruning techniques must be used. Pruning deci-
sions are based on the score of partial derivations.
It is not always possible to compute exactly non-
local features while computing the score of partial
derivations, since partial derivations miss part of the
context. Formula (3) accounts for the interaction be-
tween r and sub-items ι1 and ι2, but it does not in-
tegrate the cost relative to the interaction between
the item and the surrounding context. Therefore the
item score computed in a bottom-up fashion is an
approximation of the score the item has in a broader
context. For example, in HMT the LM score for n-
grams that partially overlap the item’s span cannot
be computed exactly since the surrounding words
are not known.

Basing pruning decisions on approximated scores
can introduce search errors. It is possible to reduce
search errors using heuristics based on future cost
estimation. In general the estimation of the interac-
tion between ι and the surrounding context is func-
tion of the carry, κ. In HMT it is possible to estimate
the cost of n-grams that partially overlap ι’s span
considering the boundary words. We can obtain the
heuristic cost for an item, ι, adding to formula (3)
the factor, est(κ), for the estimation of interaction
with missing context:

heuristicCost(ι) = cost(ι) + est(κ) (4)

And use heuristicCost(ι) to guide BD pruning de-
cisions. Anyway, even if a good interaction estima-
tion is available, in practice it is not possible to avoid
search errors while pruning.

More sophisticated parsing models allow the use
of non-bottom-up features within a BD framework.
Caraballo and Charniak (1998) present best-first
parsing with Figures of Merit that allows condition-
ing of the heuristic function on statistics of the input
string. Corazza et al. (1994), and Klein and Man-
ning (2003) propose an A* parsing algorithm that
estimates the upper bound of the parse completion
scores using contextual summary features. These
models achieve time efficiency and state-of-the-art
accuracy for PCFG parsing, but still use a BD frame-
work that doesn’t allow the application of a broader
class of non-bottom-up contextual features.

In HMT, knowing the sentence-wide context in
which a sub-phrase is translated is extremely impor-
tant. It is obviously important for word choice: as

900



a simple example consider the translation of the fre-
quent English word “get” into Chinese. The choice
of the correct set of ideograms to translate “get” of-
ten requires being aware of the presence of particles
that can be at any distance within the sentence. In a
common English to Chinese dictionary we found 93
different sets of ideograms that could be translations
of “get”. Sentence-wide context is also important
in the choice of word re-ordering: as an example
consider the following translations from English to
German:

1. EN : I go home.
DE : Ich gehe nach Hause.

2. EN : I say, that I go home.
DE : Ich sage, dass ich nach Hause gehe.

3. EN : On Sunday I go home.
DE : Am Sonntag gehe ich nach Hause.

The English phrase “I go home” is translated in Ger-
man using the same set of words but with different
orderings. It is not possible to choose the correct
ordering of the phrase without being aware of the
context. Thus a bottom-up decoder without context
needs to build all translations for “I go home”, intro-
ducing the possibility of pruning errors.

Having shown the importance of contextual fea-
tures, we define a framework that overcomes the
limitations of bottom-up feature approximation.

3 Undirected-CKY Framework

Our aim is to propose a new Framework that over-
comes BD limitations allowing a better integration
of contextual features. The presented framework can
be regarded as a generalization of CKY.

To introduce the new framework let us focus on a
detail of CKY BD. The items are created and scored
in topological order. The ordering constraint can be
formally stated as: an item covering the span [i, j]
must be processed after items covering sub spans
[h, k]|h ≥ i, k ≤ j. This ordering constraint im-
plies that full yield information is available when
an item is processed, but information about ances-
tors and siblings is missing. Therefore non-bottom-
up context cannot be used because of the ordering
constraint. Now let us investigate how the decoding

algorithm could change if we remove the ordering
constraint.

Removing the ordering constraint would lead to
the occurrence of cases in which an item is pro-
cessed before all child items have been processed.
For example, we could imagine to create and score
an item, ι, with postcondition X and span [i, j], link-
ing the rule instantiation r : X→AB with only
the left sub-item, ιA, while information for the right
sub-item, ιB is still missing. In this case, we can
rely on local and partial contextual features to score
ι. Afterwards, it is possible to process ιB using the
parent item, ι, as a source of additional informa-
tion about the parent context and sibling ιA yield.
This approach can avoid search errors in cases where
pruning at the parent level can be correctly done us-
ing only local and partial yield context, while prun-
ing at the child level needs extra non-bottom-up con-
text to make a better pruning decision. For exam-
ple, consider the translation of the English sentence
“I run” into French using the following synchronous
grammar:

r1 : S → X 1 X 2 | X 1 X 2
r2 : X → I | Je
r3 : X → run | course
r4 : X → run | courir
r5 : X → run | cours
r6 : X → run | courons
...

Where: r1 is a Glue rule and boxed indexes de-
scribe the alignment; r2 translates “I” in the cor-
responding French pronoun; r3 translates “run” as
a noun; remaining rules translate “run” as one of
the possible conjugations of the verb “courir”. Us-
ing only bottom-up features it is not possible to re-
solve the ambiguity of the word “run”. If the beam
is not big enough the correct translation could be
pruned. Anyway a CKY decoder would give the
highest score to the most frequent translation. In-
stead, if we follow a non bottom-up approach, as
described in Figure 1, we can: 1) first translate “I”;
2) Then create an item using r1 with missing right
child; 3) finally choose the correct translation for
“run” using r1 to access a wider context. Notice
that with this undirected approach it is possible to
reach the correct translation using only beam size of

901



Figure 1: Example of undirected decoding for HMT. The arrows point to the direction in which information is propa-
gated. Notice that the parent link at step 3 is fundamental to correctly disambiguate the translation for “run”.

1 and the LM feature.
To formalize Undirected-CKY, we define a gen-

eralized item called undirected-item. Undirected-
items, ι̊, are built linking rule instantiations with
elements in L ≡ {left child, right child, parent};
for example: ι̊ ≡ 〈̊ι1 ⋗ r ∨̇ ι̊p〉, is built linking
r with left child, ι̊1, and parent, ι̊p. We denote
with L+ι̊ the set of links for which the undirected-
item, ι̊, has a connection, and with L−ι̊ the set
of missing links. An undirected-item is a triple
that contains a span, a carry and an undirected-
postcondition. The undirected-postcondition is a
set of strings, one string for each of ι̊’s missing links,
l ∈ L−ι̊ . Each string represents the non-terminal re-
lated to one of the missing links available for expan-
sion. Bottom-up items can be considered specific
cases of undirected-items having L+ = { left child,
right child} and L− = {parent}. We can formally
describe the steps of the example depicted in Figure
1 with:

1)
r2 : X → I|Je , terminal : [0, 1]

ι̊1 : [0, 1, {X p }, κ1]

2)
r1 : S → X 1X 2 |X 1X 2 , ι̊1 : [0, 1, {X p }, κ1]

ι̊2 : [0, 1, {X 2 }, κ2)]

3)
r5 :X → run|cours,̊ ι2 : [· · · ] , terminal : [1, 2]

ι̊3 : [0, 2, {}, κ3 ]

The scoring function for undirected-items can be ob-
tained generalizing formula (3):

cost(̊ι) = cost(r)

+
∑

l∈L+
cost(̊ιl ) (5)

+ interaction(r ,L+)

In CKY, each span is processed separately in
topological order, and the best-k items for each span
are selected in sequence according to scoring func-
tion (4). In the proposed framework, the selec-
tion of undirected-items can be done in any order,
for example: in a first step selecting an undirected-
item for span s1, then selecting an undirected-item
for span s2, and in a third step selecting a second
undirected-item for s1, and so on. As in agenda
based parsing (Klein and Manning, 2001), all candi-
date undirected-items can be handled with an unique
queue. Allowing the system to decide decoding or-
der based on the candidates’ scores, so that candi-
dates with higher confidence can be selected earlier
and used as context for candidates with lower confi-
dence.

Having all candidates in the same queue intro-
duces comparability issues. In CKY, candidates are
comparable since each span is processed separately
and each candidate is scored with the estimation of
the yield score. Instead, in the proposed framework,

902



the unique queue contains candidates relative to dif-
ferent nodes and with different context scope. To
ensure comparability, we can associate to candidate
undirected-items a heuristic score of the full deriva-
tion:

heuristicCost(̊ι) = cost(̊ι) + est(̊ι) (6)

Where est(̊ι) estimates the cost of the missing
branches of the derivation as a function of ι̊′s par-
tial structure and carry.

In this framework, the queue can be initialized
with a candidate for each rule instantiation. These
initializing candidates have no context information
and can be scored using only local features. A
generic decoding algorithm can loop selecting the
candidate undirected-item with the highest score, ι̊,
and then propagating its information to neighboring
candidates, which can update using ι̊ as context. In
this general framework the link to the parent node is
not treated differently from links to children. While
in CKY the information is always passed from chil-
dren to parent, in Undirected-CKY the information
can be propagated in any direction, and any decod-
ing order is allowed.

We can summarize the steps done to generalize
CKY into the proposed framework: 1) remove the
node ordering constraint; 2) define the scoring of
candidates with missing children or parent; 3) use
a single candidate queue; 4) handle comparability of
candidates from different nodes and/or with differ-
ent context scope; 5) allow information propagation
in any direction.

4 Undirected Decoding

In this section we propose Undirected Decod-
ing (UD), an instantiation of the Undirected-CKY
framework presented above. The generic framework
introduces many new degrees of freedom that could
lead to a higher complexity of the decoder. In our
actual instantiation we apply constraints on the ini-
tialization step, on the propagation policy, and fix a
search beam of k. These constraints allow the sys-
tem to converge to a solution in practical time, al-
low the use of dynamic programming techniques to
merge items with equivalent states, and gives us the
possibility of using non-bottom-up features and test-
ing their relevance.

Algorithm 1 Undirected Decoding
1: function decoder (k) : out-forest
2: Q← LeafRules();
3: while |Q| > 0 do
4: ι̊← PopBest (Q);
5: if CanPop(̊ι) then
6: out-forest.Add(̊ι);
7: if ι̊.HasChildrenLinks() then
8: for all r ∈ HeadRules(̊ι) do
9: Ĉ← NewUndirectedItems(r ,̊ ι);

10: for all ĉ ∈ Ĉ do
11: if CanPop(ĉ) then
12: Q.Insert(ĉ);
13: end if
14: end for
15: end for
16: end if
17: end if
18: end while

Algorithm 1 summarizes the UD approach. The
beam size, k, is given as input. At line 2 the queue
of undirected-item candidates, Q, is initialized with
only leafs rules. At line 3 the loop starts, it will
terminate when Q is empty. At line 4 the candi-
date with highest score, ι̊, is popped from Q. line 5
checks if ι̊ is within the beam width: if ι̊ has a span
for which k candidates were already popped, then ι̊
is dropped and a new iteration is begun. Otherwise
ι̊ is added to the out-forest at line 6. From line 7
to line 10 the algorithm deals with the generation of
new candidate undirected-items. line 7 checks if ι̊
has both children links, if not a new decoding iter-
ation is begun. line 8 loops over the rule instantia-
tions, r, that can use ι̊ as child. At line 9, the set of
new candidates, Ĉ, is built linking r with ι̊ and any
context already available in the out-forest. Finally,
between line 10 and line 12, each element ĉ in Ĉ
is inserted in Q after checking that ĉ is within the
beam width: if ĉ has a span for which k candidates
were already popped it doesn’t make sense to insert
it in Q since it will be surely discarded at line 5.

In more detail, the function
NewUndirectedItems(r,̊ ι) at line 9 creates new
undirected-items linking r using: 1) ι̊ as child; 2)
(optionally) as other child any other undirected-item
that has already been inserted in the out-forest and

903



doesn’t have a missing child and matches missing
span coverage; 3) and using as parent context the
best undirected-item with missing child link that
has been incorporated in the out-forest and can
expand the missing child link using r. In our current
method, only the best possible parent context is
used because it only provides context for ranking
candidates, as discussed at the end of this section.
In contrast, a different candidate is generated for
each possible other child in 2), as well as for
the case where no other child is included in the
undirected-item.

We can make some general observations on the
Undirected Decoding Algorithm. Notice that, the
if statement at line 7 and the way new undirected-
items are created at line 9, enforce that each
undirected-item covers a contiguous span. An
undirected-item that is missing a child link cannot
be used as child context but can be used as parent
context since it is added to the out-forest at line 6
before the if statement at line 7. Furthermore, the
if statements at line 5 and line 11 check that no
more than k candidates are selected for each span,
but the algorithm does not require the the selection
of exactly k candidates per span as in CKY.

The queue of candidates, Q, is ordered according
to the heuristic cost of formula (6). The score of the
candidate partial structure is accounted for with fac-
tor cost(̊ι) computed according to formula (5). The
factor est(̊ι) accounts for the estimation of the miss-
ing part of the derivation. We compute this factor
with the following formula:

est(̊ι) =
∑

l∈L−
ι̊

(
localCost(̊ι, l) + contextEst(̊ι, l)

)

(7)
For each missing link, l ∈ L−ι̊ , we estimate the cost
of the corresponding derivation branch with two fac-
tors: localCost(̊ι, l) that computes the context-free
score of the branch with highest score that could
be attached to l; and contextEst(̊ι, l) that estimates
the contextual score of the branch and its interac-
tion with ι̊. Because our model is implemented in
the Forest Rescoring framework (e.g. Huang and
Chiang (2007), Dyer et al. (2010), Li et al. (2009)),
localCost(̊ι, l) can be efficiently computed exactly.
In HMT it is possible to exhaustively represent and
search the context-free-forest (ignoring the LM),

which is done in the Forest Rescoring framework be-
fore our task of decoding with the LM. We exploit
this context-free-forest to compute localCost(̊ι, l):
for missing child links the localCost(·) is the In-
side score computed using the (max, +) semiring
(also known as the Viterbi score), and for missing
parent links the localCost(·) is the corresponding
Outside score. The factor contextEst(·) estimates
the LM score of the words generated by the missing
branch and their interaction with the span covered
by ι̊. To compute the expected interaction cost we
use the boundary words information contained in ι̊’s
carry as done in BD. To estimate the LM cost of the
missing branch we use an estimation function, con-
ditioned on the missing span length, whose parame-
ters are tuned on held-out data with gradient descent,
using the search score as objective function.

To show that UD leads to better results than BD,
the two algorithms are compared in the same search
space. Therefore we ensure that candidates em-
bedded in the UD out-forest would have the same
score if they were scored from BD. We don’t need
to worry about differences derived from the missing
context estimation factor, est(·), since this factor is
only considered while sorting the queue, Q, accord-
ing to the heuristicCost(·). Also, we don’t have to
worry about candidates that are scored with no miss-
ing child and no parent link, because in that case
scoring function (3) for BD is equivalent to scoring
function (5) for UD. Instead, for candidates that are
scored with parent link, we remove the parent link
factor from the cost(·) function when inserting the
candidate into the out forest. And for the candi-
dates that are scored with a missing child, we ad-
just the score once the link to the missing child is
created in the out-forest. In this way UD and BD
score the same derivation with the same score and
can be regarded as two ways to explore the same
search space.

5 Experiments

In this section we test the algorithm presented, and
empirically show that it produces better translations
searching a smaller portion of the search space.

We implemented UD on top of a widely-used
HMT open-source system, cdec (Dyer et al., 2010).
We compare with cdec Cube Pruning BD. The ex-

904



 0

 100

 200

 300

 400

 500

 600

 700

 800

 900

 2  4  6  8  10  12  14  16

T
es

t S
et

Beam Size

UD best score
BD best score

Figure 2: Comparison of the quality of the translations.

periments are executed on the NIST MT03 Chinese-
English parallel corpus. The training corpus con-
tains 239k sentence pairs with 6.9M Chinese words
and 8.9M English words. We use a hierarchical
phrase-based translation grammar extracted using a
suffix array rule extractor (Lopez, 2007). The NIST-
03 test set is used for decoding, it has 919 sentence
pairs. The experiments can be reproduced on an
average desktop computer. Since we compare two
different decoding strategies that rely on the same
training technique, the evaluation is primarily based
on search errors rather than on BLEU. We compare
the two systems on a variety of beam sizes between
1 and 16.

Figure 2 reports a comparison of the translation
quality for the two systems in relation to the beam
size. The blue area represents the portion of sen-
tences for which UD found a better translation. The
white area represents the portion of sentences for
which the two systems found a translation with the
same search score. With beam 1 the two systems ob-
viously have a similar behavior, since both the sys-
tems stop investigating the candidates for a node af-
ter having selected the best candidate immediately
available. For beams 2-4, UD has a clear advan-
tage. In this range UD finds a better translation for
two thirds of the sentences. With beam 4, we ob-
serve that UD is able to find a better translation for
63.76% of the sentences, instead BD is able to find a
better translation for only 21.54% of the sentences.
For searches that employ a beam bigger than 8, we
notice that the UD advantage slightly decreases, and

-126.5

-126

-125.5

-125

-124.5

-124

-123.5

-123

 2  4  6  8  10  12  14  16

S
ea

rc
h 

S
co

re

Beam Size

Bottom-up Decoding
Guided Decoding

Figure 3: Search score evolution for BD and UD.

the number of sentences with equivalent translation
slowly increases. We can understand this behavior
considering that as the beam increases the two sys-
tems get closer to exhaustive search. Anyway with
this experiment UD shows a consistent accuracy ad-
vantage over BD.

Figure 3 plots the search score variation for dif-
ferent beam sizes. We can see that UD search leads
to an average search score that is consistently bet-
ter than the one computed for BD. Undirected De-
coding improves the average search score by 0.411
for beam 16. The search score is the logarithm of
a probability. This variation corresponds to a rel-
ative gain of 50.83% in terms of probability. For
beams greater than 8 we see that the two curves keep
a monotonic ascendant behavior while converging to
exhaustive search.

Figure 4 shows the BLEU score variation. Again
we can see the consistent improvement of UD over
BD. In the graph we report also the performance ob-
tained using BD with beam 32. BD reaches BLEU
score of 32.07 with beam 32 while UD reaches
32.38 with beam 16: UD reaches a clearly higher
BLEU score using half the beam size. The differ-
ence is even more impressive if we consider that UD
reaches a BLEU of 32.19 with beam 4.

In Figure 5 we plot the percentage reduction of the
size of the hypergraphs generated by UD compared
to those generated by BD. The size reduction grows
quickly for both nodes and edges. This is due to the
fact that BD, using Cube Pruning, must select k can-
didates for each node. Instead, UD is not obliged to

905



 31.2

 31.4

 31.6

 31.8

 32

 32.2

 32.4

 2  4  6  8  10  12  14  16

B
LE

U
 S

co
re

Beam Size

Bottom-up Decoding
Undirected Decoding

BD beam 30

Figure 4: BLEU score evolution for BD and UD.

 5

 10

 15

 20

 25

 30

 35

 40

 2  4  6  8  10  12  14  16

R
ed

uc
tio

n 
(%

)

Beam Size

Nodes Reduction
Edges Reduction

Figure 5: Percentage of reduction of the size of the hy-
pergraph produced by UD.

select k candidates per f -node. As we can see from
Algorithm 1, the decoding loop terminates when the
queue of candidates is empty, and the statements at
line 5 and line 11 ensure that no more than k can-
didates are selected per f -node, but nothing requires
the selection of k elements, and some bad candidates
may not be generated due to the sophisticated prop-
agation strategy. The number of derivations that a
hypergraph represents is exponential in the number
of nodes and edges composing the structure. With
beam 16, the hypergraphs produced by UD contain
on average 4.6k fewer translations. Therefore UD
is able to find better translations even if exploring a
smaller portion of the search space.

Figure 6 reports the time comparison between
BD and UD with respect to sentence length. The

 0

 200

 400

 600

 800

 1000

 1200

 10  15  20  25  30  35  40  45  50

T
im

e 
(m

s)

Input Sentence Size

Bottom-up Decoding, beam = 16
Undirected Decoding, beam = 8

Figure 6: Time comparison between BD and UD.

sentence length is measured with the number of
ideogram groups appearing in the source Chinese
sentences. We compare BD with beam of 16 and
UD with beam of 8, so that we compare two sys-
tems with comparable search score. We can notice
that for short sentences UD is faster, while for longer
sentences UD becomes slower. To understand this
result consider that for simple sentences UD can
rely on the advantage of exploring a smaller search
space. While, for longer sentences, the amount of
candidates considered during decoding grows ex-
ponentially with the size of the sentence, and UD
needs to maintain an unique queue whose size is not
bounded by the beam size k, as for the queues used
in BD’s Cube Pruning. It may be possible to address
this issue with more efficient handling of the queue.

In conclusion we can assert that, even if explor-
ing a smaller portion of the search space, UD finds
often a translation that is better than the one found
with standard BD. UD’s higher accuracy is due to
its sophisticated search strategy that allows a more
efficient integration of contextual features. This set
of experiments show the validity of the UD approach
and empirically confirm our intuition about the BD’s
inadequacy in solving tasks that rely on fundamental
contextual features.

6 Future Work

In the proposed framework the link to the parent
node is not treated differently from links to child
nodes, the information in the hypergraph can be
propagated in any direction. Then the Derivation

906



Hypergraph can be regarded as a non-directed graph.
In this setting we could imagine applying mes-
sage passing algorithms from graphical model the-
ory (Koller and Friedman, 2010).

Furthermore, considering that the proposed
framework lets the system decide the decoding or-
der, we could design a system that explicitly learns
to infer the decoding order at training time. Sim-
ilar ideas have been successfully tried: Shen et al.
(2010) and Gesmundo (2011) investigate the Guided
Learning framework, that dynamically incorporates
the tasks of learning the order of inference and train-
ing the local classifier.

7 Conclusion

With this paper we investigate the limitations of
Bottom-up parsing techniques, widely used in Tree
Structures Prediction, focusing on Hierarchical Ma-
chine Translation. We devise a framework that al-
lows a better integration of non-bottom-up features.
Compared to a state of the art HMT decoder the pre-
sented system produces higher quality translations
searching a smaller portion of the search space, em-
pirically showing that the bottom-up approximation
of contextual features is a limitation for NLP tasks
like HMT.

Acknowledgments

This work was partly funded by Swiss NSF grant
CRSI22 127510 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).

References

Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing, Computational Linguistics, 24:275-298.

J. C. Chappelier and M. Rajman and R. Arages and A.
Rozenknop. 1999. Lattice Parsing for Speech Recog-
nition. In Proceedings of TALN 1999, Cargse, France.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228,
2007.

Anna Corazza, Renato De Mori, Roberto Gretter and
Giorgio Satta. 1994. Optimal Probabilistic Evalu-
ation Functions for Search Controlled by Stochastic
Context-Free Grammars. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 16(10):1018-
1027.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2010, Uppsala, Sweden.

Andrea Gesmundo and James Henderson 2010. Faster
Cube Pruning. Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), Paris, France.

Andrea Gesmundo 2011. Bidirectional Sequence Classi-
fication for Tagging Tasks with Guided Learning. Pro-
ceedings of TALN 2011, Montpellier, France.

Mark Hopkins and Greg Langmead 2009. Cube prun-
ing as heuristic search. Proceedings of the Conference
on Empirical Methods in Natural Language Processing
2009, Singapore.

Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2007, Prague, Czech Re-
public.

Dan Klein and Christopher D. Manning. 2001 Pars-
ing and Hypergraphs, In Proceedings of the Interna-
tional Workshop on Parsing Technologies 2001, Bei-
jing, China.

Dan Klein and Christopher D. Manning. 2003 A* Pars-
ing: Fast Exact Viterbi Parse Selection, In Proceed-
ings of the Conference of the North American Associ-
ation for Computational Linguistics 2003, Edmonton,
Canada.

Daphne Koller and Nir Friedman. 2010. Probabilistic
Graphical Models: Principles and Techniques. The
MIT Press, Cambridge, Massachusetts.

Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient Minimum Error Rate
Training and Minimum Bayes-Risk decoding for
translation hypergraphs and lattices, In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, Suntec,
Singapore.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An Open Source Toolkit for
Parsing-based Machine Translation. In Proceedings of
the Workshop on Statistical Machine Translation 2009,
Athens, Greece.

Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing 2007, Prague, Czech Republic.

907



Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of the Conference
of the Association of Computational Linguistics 2008,
Columbus, OH.

Libin Shen, Giorgio Satta and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the Conference of the As-
sociation of Computational Linguistics 2007, Prague,
Czech Republic.

Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
2002, Denver, CO.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing,
Proceedings of the Workshop on Statistical Machine
Translation, New York City, New York.

908


