



















































Modeling of term-distance and term-occurrence information for improving n-gram language model performance


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233–237,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Modeling of term-distance and term-occurrence information for im-

proving n-gram language model performance 

 

Tze Yuang Chong
1,2

, Rafael E. Banchs
3
, Eng Siong Chng

1,2
, Haizhou Li

1,2,3
 

1
Temasek Laboratory, Nanyang Technological University, Singapore 639798 

2
School of Computer Engineering, Nanyang Technological University, Singapore 639798 

3
Institute for Infocomm Research, Singapore 138632 

tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg, 

aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg 

  

 

Abstract 

In this paper, we explore the use of distance 

and co-occurrence information of word-pairs 

for language modeling. We attempt to extract 

this information from history-contexts of up to 

ten words in size, and found it complements 

well the n-gram model, which inherently suf-

fers from data scarcity in learning long histo-

ry-contexts. Evaluated on the WSJ corpus, bi-

gram and trigram model perplexity were re-

duced up to 23.5% and 14.0%, respectively. 

Compared to the distant bigram, we show that 

word-pairs can be more effectively modeled in 

terms of both distance and occurrence. 

1 Introduction 

Language models have been extensively studied 

in natural language processing. The role of a lan-

guage model is to measure how probably a (tar-

get) word would occur based on some given evi-

dence extracted from the history-context. The 

commonly used n-gram model (Bahl et al. 1983) 

takes the immediately preceding history-word 

sequence, of length � � 1 , as the evidence for 
prediction. Although n-gram models are simple 

and effective, modeling long history-contexts 

lead to severe data scarcity problems. Hence, the 

context length is commonly limited to as short as 

three, i.e. the trigram model, and any useful in-

formation beyond this window is neglected. 

In this work, we explore the possibility of 

modeling the presence of a history-word in terms 

of: (1) the distance and (2) the co-occurrence, 

with a target-word. These two attributes will be 

exploited and modeled independently from each 

other, i.e. the distance is described regardless the 

actual frequency of the history-word, while the 

co-occurrence is described regardless the actual 

position of the history-word. We refer to these 

two attributes as the term-distance (TD) and the 

term-occurrence (TO) components, respectively. 

The rest of this paper is structured as follows. 

The following section presents the most relevant 

related works. Section 3 introduces and moti-

vates our proposed approach. Section 4 presents 

in detail the derivation of both TD and TO model 

components. Section 5 presents some perplexity 

evaluation results. Finally, section 6 presents our 

conclusions and proposed future work. 

2 Related Work 

The distant bigram model (Huang et.al 1993, 

Simon et al. 1997, Brun et al. 2007) disassembles 

the n-gram into (n−1) word-pairs, such that each 

pair is modeled by a distance-k bigram model, 

where 1 � � � � � 1 . Each distance-k bigram 
model predicts the target-word based on the oc-

currence of a history-word located k positions 

behind.  

Zhou & Lua (1998) enhanced the effective-

ness of the model by filtering out those word-

pairs exhibiting low correlation, so that only the 

well associated distant bigrams are retained. This 

approach is referred to as the distance-dependent 

trigger model, and is similar to the earlier pro-

posed trigger model (Lau et al. 1993, Rosenfeld 

1996) that relies on the bigrams of arbitrary dis-

tance, i.e. distance-independent. 

Latent-semantic language model approaches 

(Bellegarda 1998, Coccaro 2005) weight word 

counts with TFIDF to highlight their semantic 

importance towards the prediction. In this type of 

approach, count statistics are accumulated from 

long contexts, typically beyond ten to twenty 

words. In order to confine the complexity intro-

duced by such long contexts, word ordering is 

ignored (i.e. bag-of-words paradigm). 

Other approaches such as the class-based lan-

guage model (Brown 1992, Kneser & Ney 1993) 

233



use POS or POS-like classes of the history-words 

for prediction. The structured language model 

(Chelba & Jelinek 2000) determines the “heads” 

in the history-context by using a parsing tree. 

There are also works on skipping irrelevant his-

tory-words in order to reveal more informative n-

grams (Siu & Ostendorf 2000, Guthrie et al. 

2006). Cache language models exploit temporal 

word frequencies in the history (Kuhn & Mori 

1990, Clarkson & Robinson 1997). 

3 Motivation of the Proposed Approach 

The attributes of distance and co-occurrence are 

exploited and modeled differently in each lan-

guage modeling approach. In the n-gram model, 

for example, these two attributes are jointly taken 

into account in the ordered word-sequence. Con-

sequently, the n-gram model can only be effec-

tively implemented within a short history-context 

(e.g. of size of three or four). 

Both, the conventional trigger model and the 

latent-semantic model capture the co-occurrence 

information while ignoring the distance informa-

tion. It is reasonable to assume that distance in-

formation at far contexts is less likely to be in-

formative and, hence, can be discarded. Howev-

er, intermediate distances beyond the n-gram 

model limits can be very useful and should not 

be discarded. 

On the other hand, distant-bigram models and 

distance-dependent trigger models make use of 

both, distance and co-occurrence, information up 

to window sizes of ten to twenty. They achieve 

this by compromising inter-dependencies among 

history-words (i.e. the context is represented as 

separated word-pairs). However, similarly to n-

gram models, distance and co-occurrence infor-

mation are implicitly tied within the word-pairs. 

In our proposed approach, we attempt to ex-

ploit the TD and TO attributes, separately, to in-

corporate distant context information into the n-

gram, as a remedy to the data scarcity problem 

when learning the far context. 

4 Language Modeling with TD and TO 

A language model estimates word probabilities 

given their history, i.e. ��� 	 
�| 	 
��������� , 
where � denotes the target word and  denotes its 
corresponding history. Let the word located at i

th
 

position, 
� , be the target-word and its preceding 
word-sequence 
�������� 	 �
�����…
���
����  of 
length � � 1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-

sume the occurrences of the history-words to be 

independent from each other, conditioned to the 

occurrence of the target-word 
� , i.e.  
��� �
���|
� , where 
��� , 
��� �  , and � � � . The 
probability can then be approximated as: 

��� 	 
�| 	 
���������� ��� 	 
��∏ ��� 	 
���|� 	 
������!� "��  (1)
where "�� is a normalizing term, and � 	 
��� 
indicates that 
��� is the word at position  kth. 
4.1 Derivation of the TD-TO Model 

In order to define the TD and TO components for 

language modeling, we express the observation 

of an arbitrary history-word, 
��� at the kth posi-
tion behind the target-word, as the joint of two 

events: i) the word 
��� occurs within the histo-
ry-context: 
��� � , and ii) it occurs at distance �  from the target-word: ∆�
���� 	 � , (∆	 �  for 
brevity); i.e. �� 	 
���� $ �
��� � � % �∆	 ��. 

Thus, the probability in Eq.1 can be written as: 

��� 	 
�| 	 
���������� ��� 	 
��∏ ��
��� � , ∆	 �|� 	 
������!� "��  (2)
where the likelihood ��
��� � , ∆	 �|� 	 
�� 
measures how likely the joint event �
��� � ,∆	 �� would be observed given the target-word 
�. This can be rewritten in terms of the likelih-
ood function of the distance event (i.e.  ∆	 � ) 
and the occurrence event (i.e.  
��� � ), where 
both of them can be modeled and exploited sepa-

rately, as follows: 

��� 	 
�| 	 
���������

�
& ��� 	 
��∏ ��∆	 �|
��� � , � 	 
������!�∏ ��
��� � |� 	 
������!� '"��  

(3)

The formulation above yields three terms, re-

ferred to as the prior, the TD likelihood, and the 

TO likelihood, respectively. 

In Eq.3, we have decoupled the observation of 

a word-pair into the events of distance and co-

occurrence. This allows for independently mod-

eling and exploiting them. In order to control 

their contributions towards the final prediction of 

the target-word, we weight these components: 

��� 	 
�| 	 
���������

�
& ��� 	 
��()�∏ ��∆	 �|
��� � , � 	 
������!� �(*�∏ ��
��� � |� 	 
������!� �(+ '"��  

(4)

234



where ,� , ,- , and ,.  are the weights for the 
prior, TD and TO models, respectively. 

Notice that the model depicted in Eq.4 is the 

log-linear interpolation (Klakow 1998) of these 

models. The prior, which is usually implemented 

as a unigram model, can be also replaced with a 

higher order n-gram model as, for instance, the 

bigram model: 

��� 	 
�| 	 
���������

�
& ��� 	 
�| 	 
����()�∏ ��∆	 �|
��� � , � 	 
������!� �(*�∏ ��
��� � |� 	 
������!� �(+ '"��  

(5)

Replacing the unigram model with a higher 

order n-gram model is important to compensate 

the damage incurred by the conditional indepen-

dence assumption made earlier. 

4.2 Term-Distance Model Component 

Basically, the TD likelihood measures how likely 

a given word-pair would be separated by a given 

distance. So, word-pairs possessing consistent 

separation distances will favor this likelihood. 

The TD likelihood for a distance � given the co-
occurrence of the word-pair �
���, 
��  can be 
estimated from counts as follows: 

��∆	 �|
��� � , � 	 
��	 C�
��� � , � 	 
� , ∆	 ��C�
��� � , � 	 
��  (6)
The above formulation of the TD likelihood 

requires smoothing for resolving two problems: 

i) a word-pair at a particular distance has a zero 

count, i.e. C�
��� � , � 	 
� , ∆	 �� 	 0 , which 
results in a zero probability, and ii) a word-pair is 

not seen at any distance within the observation 

window, i.e. zero co-occurrence C�
��� � , � 	
�� 	 0, which results in a division by zero. 
For the first problem, we have attempted to 

redistribute the counts among the word-pairs at 

different distances (as observed within the win-

dow). We assumed that the counts of word-pairs 

are smooth in the distance domain and that the 

influence of a word decays as the distance in-

creases. Accordingly, we used a weighted mov-

ing-average filter for performing the smoothing. 

Similar approaches have also been used in other 

works (Coccaro 2005, Lv & Zhai 2009). Notice, 

however, that this strategy is different from other 

conventional smoothing techniques (Chen & 

Goodman 1996), which rely mainly on the count-

of-count statistics for re-estimating and smooth-

ing the original counts. 

For the second problem, when a word-pair 

was not seen at any distance (within the win-

dow), we arbitrarily assigned a small probability 

value, ��∆	 �|
��� � , � 	 
�� 	 0.01 , to pro-
vide a slight chance for such a word-pair �
��� , 
�� to occur at close distances. 
4.3 Term-Occurrence Model Component 

During the decoupling operation (from Eq.2 to 

Eq.3), the TD model held only the distance in-

formation while the count information has been 

ignored. Notice the normalization of word-pair 

counts in Eq.6.  

As a complement to the TD model, the TO 

model focuses on co-occurrence, and holds only 

count information. As the distance information is 

captured by the TD model, the co-occurrence 

count captured by the TO model is independent 

from the given word-pair distance. 

In fact, the TO model is closely related to the 

trigger language model (Rosenfeld 1996), as the 

prediction of the target-word (the triggered word) 

is based on the presence of a history-word (the 

trigger). However, differently from the trigger 

model, the TO model considers all the word-

pairs without filtering out the weak associated 

ones. Additionally, the TO model takes into ac-

count multiple co-occurrences of the same histo-

ry-word within the window, while the trigger 

model would count them only once (i.e. consid-

ers binary counts).  

The word-pairs that frequently co-occur at ar-

bitrary distances (within an observation window) 

would favor the TO likelihood. It can be esti-

mated from counts as: 

��
��� � |� 	 
�� 	 C�
��� � , � 	 
��C�� 	 
��  (7)
When a word-pair did not co-occur (within the 

observation window), we assigned a small prob-

ability value, ��
��� � |� 	 
�� 	 0.01, to pro-
vide a slight chance for the history word to occur 

within the history-context of the target word. 

5 Perplexity Evaluation 

A perplexity test was run on the BLLIP WSJ 

corpus (Charniak 2000) with the standard 5K 

vocabulary. The entire WSJ ’87 data (740K sen-

tences 18M words) was used as train-set to train 

the n-gram, TD, and TO models. The dev-set and 

the test-set, each comprising 500 sentences and 

about 12K terms, were selected randomly from 

WSJ ’88 data. We used them for parameter fine-

tuning and performance evaluation. 

235



5.1 Capturing Distant Information 

In this experiment, we assessed the effectiveness 

of the TD and TO components in reducing the n-

gram’s perplexity. Following Eq.5, we interpo-

lated n-gram models (of orders from two to six) 

with the TD, TO, and the both of them (referred 

to as TD-TO model).  

By using the dev-set, optimal interpolation 

weights (i.e. ,�, ,-, and ,.) for the three combi-
nations (n-gram with TD, TO, and TD-TO) were 

computed. The resulting interpolation weights 

were as follows: n-gram with TD = (0.85, 0.15), 

n-gram with TO = (0.85, 0.15), and n-gram with 

TD-TO = (0.80, 0.07, 0.13). 

The history-context window sizes were opti-

mized too. Optimal sizes resulted to be 7, 5 and 8 

for TD, TO, and TD-TO models, respectively. In 

fact, we observed that the performance is quite 

robust with respect to the window’s length. De-

viating about two words from the optimum 

length only worsens the perplexity less than 1%.  

Baseline models, in each case, are standard n-

gram models with modified Kneser-Ney interpo-

lation (Chen 1996). The test-set results are de-

picted in Table 1. 

 

N NG NG-

TD 

Red. 

(%) 

NG-

TO 

Red. 

(%) 

NG-

TDTO 

Red. 

(%) 

2 151.7 134.5 11.3 119.9 21.0 116.0 23.5 

3 99.2 92.9 6.3 86.7 12.6 85.3 14.0 

4 91.8 86.1 6.2 81.4 11.3 80.1 12.7 

5 90.1 84.7 6.0 80.2 11.0 79.0 12.3 

6 89.7 84.4 5.9 79.9 10.9 78.7 12.2 

Table 1. Perplexities of the n-gram model (NG) 

of order (N) two to six and their combinations 

with the TD, TO, and TD-TO models. 

 

As seen from the table, for lower order n-gram 

models, the complementary information captured 

by the TD and TO components reduced the per-

plexity up to 23.5% and 14.0%, for bigram and 

trigram models, respectively. Higher order n-

gram models, e.g. hexagram, observe history-

contexts of similar lengths as the ones observed 

by the TD, TO, and TD-TO models. Due to the 

incapability of n-grams to model long history-

contexts, the TD and TO components are still 

effective in helping to enhance the prediction. 

Similar results were obtained by using the stan-

dard back-off model (Katz 1987) as baseline. 

5.2 Benefit of Decoupling Distant-Bigram 

In this second experiment, we examined whether 

the proposed decoupling procedure leads to bet-

ter modeling of word-pairs compared to the dis-

tant bigram model. Here we compare the per-

plexity of both, the distance-k bigram model and 

distance-k TD model (for values of k ranging 

from two to ten), when combined with a standard 

bigram model. 

In order to make a fair comparison, without 

taking into account smoothing effects, we trained 

both models with raw counts and evaluated their 

perplexities over the train-set (so that no zero-

probability will be encountered). The results are 

depicted in Table 2. 

 

k 2 4 6 8 10 

DBG 105.7 112.5 114.4 115.9 116.8 

TD 98.5 106.6 109.1 111.0 112.2 

Table 2. Perplexities of the distant bigram (DBG) 

and TD models when interpolated with a stan-

dard bigram model. 

 

The results from Table 2 show that the TD 

component complements the bigram model bet-

ter than the distant bigram itself. Firstly, these 

results suggest that the distance information (as 

modeled by the TD) offers better cue than the 

count information (as modeled by the distant bi-

gram) to complement the n-gram model.  

The normalization of distant bigram counts, as 

indicated in Eq.6, aims at highlighting the infor-

mation provided by the relative positions of 

words in the history-context. This has been 

shown to be an effective manner to exploit the 

far context. By also considering the results in 

Table 1, we can deduce that better performance 

can be obtained when the TO attribute is also 

involved. Overall, decoupling the word history-

context into the TD and TO components offers a 

good approach to enhance language modeling. 

6 Conclusions 

We have proposed a new approach to compute 

the n-gram probabilities, based on the TD and 

TO model components. Evaluated on the WSJ 

corpus, the proposed TD and TO models reduced 

the bigram’s and trigram’s perplexities up to 

23.5% and 14.0%, respectively. We have shown 

the advantages of modeling word-pairs with TD 

and TO, as compared to the distant bigram. 

As future work, we plan to explore the useful-

ness of the proposed model components in actual 

natural language processing applications such as 

machine translation and speech recognition. Ad-

ditionally, we also plan to develop a more prin-

cipled framework for dealing with TD smoothing. 

236



References  

Bahl, L., Jelinek, F. & Mercer, R. 1983. A statistical 

approach to continuous speech recognition. IEEE 

Trans. Pattern Analysis and Machine Intelligence, 

5:179-190. 

Bellegarda, J. R. 1998. A multispan language model-

ing framework for larfge vocabulary speech recog-

nition. IEEE Trans. on Speech and Audio 

Processing, 6(5): 456-467. 

Brown, P.F. 1992 Class-based n-gram models of natu-

ral language. Computational Linguistics, 18: 467-

479. 

Brun, A., Langlois, D. & Smaili, K. 2007. Improving 

language models by using distant information. In 

Proc. ISSPA 2007, pp.1-4. 

Cavnar, W.B. & Trenkle, J.M. 1994. N-gram-based 

text categorization. Proc. SDAIR-94, pp.161-175. 

Charniak, E., et al. 2000. BLLIP 1987-89 WSJ Cor-

pus Release 1. Linguistic Data Consortium, Phila-

delphia. 

Chen, S.F. & Goodman, J. 1996. An empirical study 

of smoothing techniques for language modeling. 

In. Proc. ACL ’96, pp. 310-318. 

Chelba, C. & Jelinek, F. 2000. Structured language 

modeling. Computer Speech & Language, 14: 283-

332. 

Clarkson, P.R. & Robinson, A.J. 1997. Language 

model adaptation using mixtures and an exponen-

tially decaying cache. In Proc. ICASSP-97, pp.799-

802. 

Coccaro, N. 2005. Latent semantic analysis as a tool 

to improve automatic speech recognition perfor-

mance. Doctoral Dissertation, University of Colo-

rado, Boulder, CO, USA. 

Guthrie, D., Allison, B., Liu, W., Guthrie, L., & 

Wilks, Y. 2006. A closer look at skip-gram model-

ling. In Proc. LREC-2006, pp.1222-1225. 

Huang, X. et al. 1993. The SPHINX-II speech recog-

nition system: an overview. Computer Speech and 

Language, 2: 137-148. 

Katz, S.M. 1987. Estimation of probabilities from 

sparse data for the language model component of a 

speech recognizer. IEEE Trans. on Acoustics, 

Speech, & Signal Processing, 35:400-401. 

Klakow, D. 1998. Log-linear interpolation of lan-

guage model. In Proc. ICSLP 1998, pp.1-4. 

Kneser, R. & Ney, H. 1993. Improving clustering 

techniques for class-based statistical language 

modeling. In Proc. EUROSPEECH ’93, pp.973-

976. 

Kuhn, R. & Mori, R.D. 1990. A cache-based natural 

language model for speech recognition. IEEE 

Trans. Pattern Analysis and Machine Intelligence, 

12(6): 570-583. 

Lau, R. et al. 1993. Trigger-based language models: a 

maximum-entropy approach. In Proc. ICASSP-94, 

pp.45-48. 

Lv Y. & Zhai C. 2009. Positional language models for 

information retrieval. In Proc. SIGIR’09, pp.299-

306. 

Rosenfeld, R. 1996. A maximum entropy approach to 

adaptive statistical language modelling. Computer 

Speech and Language, 10: 187-228. 

Simons, M., Ney, H. & Martin S.C. 1997. Distant 

bigram language modelling using maximum entro-

py. In Proc. ICASSP-97, pp.787-790. 

Siu, M. & Ostendorf, M. 2000. Variable n-grams and 

extensions for conversational speech language 

modeling. IEEE Trans. on Speech and Audio 

Processing, 8(1): 63-75. 

Zhou G. & Lua K.T. 1998. Word association and MI-

trigger-based language modeling. In Proc. COL-

ING-ACL, 1465-1471. 

 

237


