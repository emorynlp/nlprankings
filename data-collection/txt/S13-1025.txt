










































DLS@CU-CORE: A Simple Machine Learning Model of Semantic Textual Similarity


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 176â€“180, Atlanta, Georgia, June 13-14, 2013. cÂ©2013 Association for Computational Linguistics

DLS@CU-CORE: A Simple Machine Learning Model of Semantic    

Textual Similarity 

Md. Arafat Sultan, Steven Bethard, Tamara Sumner 

Institute of Cognitive Science and Department of Computer Science  

University of Colorado, Boulder, CO 80309 
{arafat.sultan, steven.bethard, sumner}@colorado.edu 

   

Abstract 

We present a system submitted in the Semantic 

Textual Similarity (STS) task at the Second 

Joint Conference on Lexical and Computa-

tional Semantics (*SEM 2013). Given two 

short text fragments, the goal of the system is 

to determine their semantic similarity. Our sys-

tem makes use of three different measures of 

text similarity: word n-gram overlap, character 

n-gram overlap and semantic overlap. Using 

these measures as features, it trains a support 

vector regression model on SemEval STS 2012 

data. This model is then applied on the STS 

2013 data to compute textual similarities. Two 

different selections of training data result in 

very different performance levels: while a cor-

relation of 0.4135 with gold standards was ob-

served in the official evaluation (ranked 63rd 

among all systems) for one selection, the other 

resulted in a correlation of 0.5352 (that would 

rank 21st).  

1 Introduction 

Automatically identifying the semantic similarity 

between two short text fragments (e.g. sentences) is 

an important research problem having many im-

portant applications in natural language processing, 

information retrieval, and digital education. Exam-

ples include automatic text summarization, question 

answering, essay grading, among others.  

   However, despite having important applications, 

semantic similarity identification at the level of 

short text fragments is a relatively recent area of in-

vestigation. The problem was formally brought to 

attention and the first solutions were proposed in 

2006 with the works reported in (Mihalcea et al., 

2006) and (Li et al., 2006). Work prior to these fo-

cused primarily on large documents (or individual 

words) (Mihalcea et al., 2006). But the sentence-

level granularity of the problem is characterized by 

factors like high specificity and low topicality of the 

expressed information, and potentially small lexical 

overlap even between very similar texts, asking for 

an approach different from those that were designed 

for larger texts. 

Since its inception, the problem has seen a large 

number of solutions in a relatively small amount of 

time. The central idea behind most solutions is the 

identification and alignment of semantically similar 

or related words across the two sentences, and the 

aggregation of these similarities to generate an over-

all similarity score (Mihalcea et al., 2006; Islam and 

Inkpen, 2008; Å ariÄ‡ et al., 2012). 

The Semantic Textual Similarity task (STS) or-

ganized as part of the Semantic Evaluation Exer-

cises (see (Agirre et al., 2012) for a description of 

STS 2012) provides a common platform for evalua-

tion of such systems via comparison with human-

annotated similarity scores over a large dataset.  

In this paper, we present a system which was 

submitted in STS 2013. Our system is based on very 

simple measures of lexical and character-level over-

lap, semantic overlap between the two sentences 

based on word relatedness measures, and surface 

features like the sentencesâ€™ lengths. These measures 

are used as features for a support vector regression 

model that we train with annotated data from 

SemEval STS 2012. Finally, the trained model is ap-

plied on the STS 2013 test pairs. 

Our approach is inspired by the success of simi-

lar systems in STS 2012: systems that combine mul-

tiple measures of similarity using a machine learn-

ing model to generate an overall score (BÃ¤r et al., 

2012; Å ariÄ‡ et al., 2012). We wanted to investigate 

how a minimal system of this kind, making use of 

very few external resources, performs on a large da-

taset. Our experiments reveal that the performance 

of such a system depends highly on the training 

data. While training on one dataset yielded a best 

176



correlation (among our three runs, described later in 

this document) of only 0.4135 with the gold scores, 

training on another dataset showed a considerably 

higher correlation of 0.5352.  

2 Computation of Text Similarity: System 
Overview 

In this section, we present a high-level description 

of our system. More details on extraction of some of 

the measures of similarity are provided in Section 3. 

Given two input sentences ğ‘†1 and ğ‘†2, our algo-
rithm can be described as follows: 

1. Compute semantic overlap (8 features): 
a. Lemmatize ğ‘†1 and ğ‘†2 using a memory-

based lemmatizer1 and remove all stop 

words. 

b. Compute the degree to which the concepts 

in ğ‘†1 are covered by semantically similar 
concepts in ğ‘†2 and vice versa (see Section 3 
for details). The result of this step is two dif-

ferent â€˜degree of containmentâ€™ values (ğ‘†1 in 
ğ‘†2 and vice versa). 

c. Compute the minimum, maximum, arith-
metic mean and harmonic mean of the two 

values to use as features in the machine 

learning model. 

d. Repeat steps 1a through 1c for a weighted 
version of semantic overlap where each 

word in the first sentence is assigned a 

weight which is proportional to its specific-

ity in a selected corpus (see Section 3). 

2. Compute word ğ‘›-gram overlap (16 features): 

a. Extract ğ‘›-grams (for ğ‘› = 1, 2, 3, 4) of all 
words in ğ‘†1 and ğ‘†2 for four different setups 
characterized by the four different value 

combinations of the two following varia-

bles: lemmatization (on and off), stop-

WordsRemoved (on and off). 

b. Compute the four measures (min, max, 
arithmetic and harmonic mean) for each 

value of n. 

3. Compute character ğ‘›-gram overlap (16 fea-
tures):  

a. Repeat   all steps in 2 above for character ğ‘›-
grams (ğ‘› = 2, 3, 4, 5). 

                                                           
1 http://www.clips.ua.ac.be/pages/MBSP#lemmatizer 
2 http://conceptnet5.media.mit.edu/data/5.1/as-

soc/c/en/cat? filter=/c/en/dog&limit=1 

4. Compute sentence length features (2 features): 

a. Compute the lengths of ğ‘†1 and ğ‘†2; and the 
minimum and maximum of the two values. 

b. Include the ratio of the maximum to the min-
imum and the difference between the maxi-

mum and minimum in the feature set. 

5. Train a support vector regression model on the 
features extracted in steps 1 through 4 above us-

ing data from SemEval 2012 STS (see Section 

4 for specifics on the dataset). We used the 

LibSVM implementation of SVR in WEKA. 

6. Apply the model on STS 2013 test data. 

3 Semantic Overlap Measures 

In this section, we describe the computation of the 

two sets of semantic overlap measures mentioned in 

step 1 of the algorithm in Section 2. 

We compute semantic overlap between two sen-

tences by first computing the semantic relatedness 

among their constituent words. Automatically com-

puting the semantic relatedness between words is a 

well-studied problem and many solutions to the 

problem have been proposed. We compute word re-

latedness in two forms: semantic relatedness and 

string similarity. For semantic relatedness, we uti-

lize two web services. The first one concerns a re-

source named ConceptNet (Liu and Singh, 2004), 

which holds a large amount of common sense 

knowledge concerning relationships between real-

world entities. It provides a web service2 that gener-

ates word relatedness scores based on these relation-

ships. We will use the term ğ¶ğ‘ğ‘Ÿğ‘’ğ‘™(ğ‘¤1, ğ‘¤2) to de-
note the relatedness of the two words ğ‘¤1 and ğ‘¤2 as 
generated by ConceptNet. 

We also used the web service3 provided by an-

other resource named Wikipedia Miner (Milne and 

Witten, 2013). While ConceptNet successfully cap-

tures common sense knowledge about words and 

concepts, Wikipedia Miner specializes in identify-

ing relationships between scientific concepts pow-

ered by Wikipedia's vast repository of scientific in-

formation (for example, Einstein and relativity). We 

will use the term ğ‘Šğ‘€ğ‘Ÿğ‘’ğ‘™(ğ‘¤1, ğ‘¤2) to denote the re-
latedness of the two words ğ‘¤1 and ğ‘¤2 as generated 
by Wikipedia Miner. Using two systems enabled us 

3 http://wikipedia-miner.cms.waikato.ac.nz/ser-

vices/compare?  term1=cat&term2=dog 

177



to increase the coverage of our word similarity com-

putation algorithm.  

Each of these web services return a score in the 

range [0, 1] where 0 represents no relatedness and 1 

represents complete similarity. A manual inspection 

of both services indicates that in almost all cases 

where the servicesâ€™ word similarity scores deviate 

from what would be the human-perceived similar-

ity, they generate lower scores (i.e. lower than the 

human-perceived score). This is why we take the 

maximum of the two servicesâ€™ similarity scores for 

any given word pair as their semantic relatedness: 

ğ‘ ğ‘’ğ‘šğ‘…ğ‘’ğ‘™(ğ‘¤1, ğ‘¤2) 

= maxâ¡{ğ¶ğ‘ğ‘Ÿğ‘’ğ‘™(ğ‘¤1, ğ‘¤2),ğ‘Šğ‘€ğ‘Ÿğ‘’ğ‘™(ğ‘¤1, ğ‘¤2)} 

We also compute the string similarity between 

the two words by taking a weighted combination of 

the normalized lengths of their longest common 

substring, subsequence and prefix (normalization is 

done for each of the three by dividing its length with 

the length of the smaller word). We will refer to the 

string similarity between words ğ‘¤1 and ğ‘¤2 as 
ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘†ğ‘–ğ‘š(â¡ğ‘¤1, ğ‘¤2). This idea is taken from (Islam 
and Inkpen, 2008); the rationale is to be able to find 

the similarity between (1) words that have the same 

lemma but the lemmatizer failed to lemmatize at 

least one of the two surface forms successfully, and 

(2) words at least one of which has been misspelled. 

We take the maximum of the string similarity and 

the semantic relatedness between two words as the 

final measure of their similarity: 

ğ‘ ğ‘–ğ‘š(ğ‘¤1, ğ‘¤2) 

= maxâ¡{ğ‘ ğ‘’ğ‘šğ‘…ğ‘’ğ‘™(ğ‘¤1, ğ‘¤2), ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘›ğ‘”ğ‘†ğ‘–ğ‘š(ğ‘¤1, ğ‘¤2)} 

At the sentence level, our first set of semantic 

overlap measures (step 1b) is an unweighted meas-

ure that treats all content words equally. More spe-

cifically, after the preprocessing in step 1a of the al-

gorithm, we compute the degree of semantic cover-

age of concepts expressed by individual content 

words in ğ‘†1 by ğ‘†2 using the following equation: 

ğ‘ğ‘œğ‘£ğ‘¢ğ‘¤(ğ‘†1, ğ‘†2) =
âˆ‘ [max

ğ‘¡âˆˆğ‘†2
{ğ‘ ğ‘–ğ‘š(ğ‘ , ğ‘¡)}]ğ‘ âˆˆğ‘†1

|ğ‘†1|
 

                                                           
4 http://googleresearch.blogspot.com/2006/08/all-our-n-

gram-are-belong-to-you.html 

where ğ‘ ğ‘–ğ‘š(ğ‘ , ğ‘¡) is the similarity between the two 
lemmas ğ‘  and ğ‘¡. 

We also compute a weighted version of semantic 

coverage (step 1d in the algorithm) by incorporating 

the specificity of each word (measured by its infor-

mation content) as shown in the equation below:  

ğ‘ğ‘œğ‘£ğ‘¤(ğ‘†1, ğ‘†2) =
âˆ‘ [max

ğ‘¡âˆˆğ‘†2
{ğ‘–ğ‘(ğ‘ ). ğ‘ ğ‘–ğ‘š(ğ‘ , ğ‘¡)}]ğ‘ âˆˆğ‘†1

|ğ‘†1|
 

where ğ‘–ğ‘(ğ‘¤) stands for the information content of 
the word ğ‘¤. Less common words (across a selected 
corpus) have high information content: 

ğ‘–ğ‘(ğ‘¤) = ln
âˆ‘ ğ‘“(ğ‘¤â€²)ğ‘¤â€²âˆˆğ¶

ğ‘“(ğ‘¤)
 

where C is the set of all words in the chosen corpus 

and f(w) is the frequency of the word w in the cor-

pus. We have used the Google Unigram Corpus4 to 

assign the required frequencies to these words. 

4 Evaluation 

The STS 2013 test data consists of four datasets: 

two datasets consisting of gloss pairs (OnWN: 561 

pairs and FNWN: 189 pairs), a dataset of machine 

translation evaluation pairs (SMT: 750 pairs) and a 

dataset consisting of news headlines (headlines: 750 

pairs). For each dataset, the output of a system is 

evaluated via comparison with human-annotated 

similarity scores and measured using the Pearson 

Correlation Coefficient. Then a weighted sum of the 

correlations for all datasets are taken to be the final 

score, where each datasetâ€™s weight is the proportion 

of sentence pairs in that dataset. 

We computed the similarity scores using three 

different feature sets (for our three runs) for the sup-

port vector regression model: 

1. All features mentioned in Section 2. This set of 
features were used in our run 1. 

2. All features except word ğ‘›-gram overlap (ex-
periments on STS 2012 test data revealed that 

using word n-grams actually lowers the perfor-

mance of our model, hence this decision). These 

are the features that were used in our run 2. 

3. Only character ğ‘›-gram and length features (just 
to test the performance of the model without 

178



any semantic features). Our run 3 was based on 

these features. 

We trained the support vector regression model 

on two different training datasets, both drawn from 

STS 2012 data: 

1. In the first setup, we chose the training datasets 
from STS 2012 that we considered the most 

similar to the test dataset. The only exception 

was the FNWN dataset, for which we selected 

the all the datasets from 2012 because no single 

dataset from STS 2012 seemed to have similar-

ity with this dataset. For the OnWN test dataset, 

we selected the OnWN dataset from STS 2012. 

For both headlines and SMT, we selected SMT-

news and SMTeuroparl from STS 2012. The ra-

tionale behind this selection was to train the ma-

chine learning model on a distribution similar to 

the test data. 

2. In the second setup, we aggregated all datasets 
(train and test) from STS 2012 and used this 

combined dataset to train the three models that 

were later applied on each STS 2013 test data. 

Here the rationale is to train on as much data as 

possible. 

Table 1 shows the results for the first setup. This 

is the performance of the set of scores which we ac-

tually submitted in STS 2013. The first four col-

umns show the correlations of our system with the 

gold standard for all runs. The rightmost column 

shows the overall weighted correlations. As we can 

see, run 1 with all the features demonstrated the best 

performance among the three runs. There was a con-

siderable drop in performance in run 3 which did not 

utilize any semantic similarity measure. 

Table 1. Results for manually selected training data 

Run headlines OnWN FNWN SMT Total 

1 .4921 .3769 .4647 .3492 .4135 

2 .4669 .4165 .3859 .3411 .4056 

3 .3867 .2386 .3726 .3337 .3309 

As evident from the table, evaluation results did 

not indicate a particularly promising system. Our 

best system ranked 63rd among the 90 systems eval-

uated in STS 2013. We further investigated to find 

out the reason: is the set of our features insufficient 

to capture text semantic similarity, or were the train-

ing data inappropriate for their corresponding test 

data? This is why we experimented with the second 

setup discussed above. Following are the results:  

Table 2. Results for combined training data 

Run headlines OnWN FNWN SMT Total 

1 .6854 .5981 .4647 .3518 .5339 

2 .7141 .5953 .3859 .349 .5352 

3 .6998 .4826 .3726 .3365 .4971 

As we can see in Table 2, the correlations for all 

feature sets improved by more than 10% for each 

run. In this case, the best system with correlation 

0.5352 would rank 21st among all systems in STS 

2013. These results indicate that the primary reason 

behind the systemâ€™s previous bad performance (Ta-

ble 1) was the selection of an inappropriate dataset. 

Although it was not clear in the beginning which of 

the two options would be the better, this second ex-

periment reveals that selecting the largest possible 

dataset to train is the better choice for this dataset. 

5 Conclusions 

In this paper, we have shown how simple measures 

of text similarity using minimal external resources 

can be used in a machine learning setup to compute 

semantic similarity between short text fragments. 

One important finding is that more training data, 

even when drawn from annotations on different 

sources of text and thus potentially having different 

feature value distributions, improve the accuracy of 

the model in the task. Possible future expansion in-

cludes use of more robust concept alignment strate-

gies using semantic role labeling, inclusion of struc-

tural similarities of the sentences (e.g. word order, 

syntax) in the feature set, incorporating word sense 

disambiguation and more robust strategies of con-

cept weighting into the process, among others. 

References 
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-

lez-Agirre. 2012. SemEval-2012 Task 6: a pilot on se-

mantic textual similarity. In Proceedings of the First 

Joint Conference on Lexical and Computational Se-

mantics. ACL, Stroudsburg, PA, USA, 385-393. 
Daniel BÃ¤r, Chris Biemann, Iryna Gurevych, and Torsten 

Zesch. 2012. UKP: computing semantic textual simi-

larity by combining multiple content similarity 

measures. In Proceedings of the First Joint Confer-

ence on Lexical and Computational Semantics. ACL, 

Stroudsburg, PA, USA, 435-440. 
Aminul Islam and Diana Inkpen. 2008. Semantic text 

similarity using corpus-based word similarity and 

string similarity.  ACM Trans. Knowl. Discov. Data 2, 

2, Article 10 (July 2008), 25 pages. 

179



Yuhua Li, David Mclean, Zuhair A. Bandar, James D. 

Oâ€™Shea, and Keeley Crockett. 2006. Sentence similar-

ity based on semantic nets and corpus statistics. IEEE 

Transactions on Knowledge and Data Engineering, 

vol.18, no.8, 1138-1150. 

Hugo Liu and Push Singh. 2004. ConceptNet â€” a prac-

tical commonsense reasoning tool-kit. BT Technology 

Journal 22, 4 (October 2004), 211-226. 

Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 

2006. Corpus-based and knowledge-based measures 

of text semantic similarity. In Proceedings of the 21st 

national conference on Artificial intelligence - Volume 

1 (AAAI'06), Anthony Cohn (Ed.), Vol. 1. AAAI 

Press 775-780. 

David Milne and Ian H. Witten. 2013. An open-source 

toolkit for mining Wikipedia. Artif. Intell. 194 (Janu-

ary 2013), 222-239. 

Frane Å ariÄ‡, Goran GlavaÅ¡, Mladen Karan, Jan Å najder, 

and Bojana Dalbelo BaÅ¡iÄ‡.Å ariÄ‡. 2012. TakeLab: sys-

tems for measuring semantic text similarity. In Pro-

ceedings of the First Joint Conference on Lexical and 

Computational Semantics. ACL, Stroudsburg, PA, 
USA, 441-448.  

180


