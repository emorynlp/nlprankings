



















































Diamonds in the Rough: Event Extraction from Imperfect Microblog Data


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 641–650,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Diamonds in the Rough: Event Extraction from Imperfect Microblog Data

Ander Intxaurrondo†, Eneko Agirre†, Oier Lopez de Lacalle†, Mihai Surdeanu‡
†IXA NLP Group, University of the Basque Country

‡University of Arizona
{ander.intxaurrondo, e.agirre, oier.lopezdelacalle}@ehu.eus

msurdeanu@email.arizona.edu

Abstract

We introduce a distantly supervised event ex-
traction approach that extracts complex event
templates from microblogs. We show that this
near real-time data source is more challeng-
ing than news because it contains information
that is both approximate (e.g., with values that
are close but different from the gold truth) and
ambiguous (due to the brevity of the texts),
impacting both the evaluation and extraction
methods. For the former, we propose a novel,
“soft”, F1 metric that incorporates similarity
between extracted fillers and the gold truth,
giving partial credit to different but similar
values. With respect to extraction method-
ology, we propose two extensions to the dis-
tant supervision paradigm: to address approx-
imate information, we allow positive training
examples to be generated from information
that is similar but not identical to gold values;
to address ambiguity, we aggregate contexts
across tweets discussing the same event. We
evaluate our contributions on the complex do-
main of earthquakes, with events with up to
20 arguments. Our results indicate that, de-
spite their simplicity, our contributions yield
a statistically-significant improvement of 33%
(relative) over a strong distantly-supervised
system. The dataset containing the knowledge
base, relevant tweets and manual annotations
is publicly available.

1 Introduction

Twitter is an excellent source of near real-time data
on recent events, motivating the need for informa-
tion extraction (IE) systems that operate on tweets

rather than traditional news articles. However, us-
ing this data comes with its own challenges: tweets
tend to use colloquial speech, noisy syntax and dis-
course, and, more importantly, the information re-
ported is often inaccurate (e.g., reporting a differ-
ent but similar magnitude for an earthquake) and
ambiguous (e.g., reporting multiple potential earth-
quake locations, with insufficient context to guess
which is the correct one).1 The top rows in Ta-
ble 1 show examples of these problems for an ac-
tual event in our dataset on earthquakes. This comes
in contrast with “traditional” IE work on newswire
documents, where information is considerably more
accurate than microblog material, and none of the
above observations hold (Grishman and Sundheim,
1996; Doddington et al., 2004).

As an example of the benefits of event extraction
from a near real-time social-media resource, the last
row in Table 1 lists a motivating example, where our
system extracts the correct depth of an earthquake
from the text tweeted by the U.S. Geological Sur-
vey, which is novel information that is missing in
our manually-curated knowledge base.

In this work we take a classic event extraction
(EE) task, where events are defined by templates
containing a predefined set of arguments, and imple-
ment it using data from Twitter. We avoid the pro-
hibitive cost of manual annotation through distant
supervision (DS): we automatically generate train-

1We focus on microblogs here because they commonly con-
tain inaccurate and/or ambiguous information. However, we be-
lieve that our contributions extend beyond microblogs because
these innacuracies, especially inaccurate information, may ap-
pear in news article as well.

641



Earthquake in Honduras. So strong it
Approximate strong it was felt in Guatemala

information as well. 7.1 offshore atlantic.
DTN Indonesia: Peru Earthquake

Ambiguous Destroys Homes, Injures 100...
information 6.9 magnitude earthquake rocks Peru.

U.S.G.S. reports 6.9 Earthquake in
Peru. NO TSUNAMI threat to Hawaii.

Information #Earthquake M 7.0 – Ryukyu Islands,
not in the Japan T20:31:27 UTC , 25.95 128.40

knowledge depth: 22 km <USGS URL>
base Local tsunami alert issued

Table 1: Challenges and opportunities for event extrac-
tion from Twitter. The first row shows a tweet with ap-
proximate information (in bold); the correct magnitude is
7.3 (cf. Table 2). The second row shows a first tweet with
ambiguous information, which leads our baseline model
to extract the incorrect country (in bold; correct country
is Peru). The following two tweets help disambiguate the
context. The last row shows a tweet containing informa-
tion (in bold) that is missing in the knowledge base.

ing data by aligning a knowledge base of known
event instances with tweets (Mintz et al., 2009;
Hoffmann et al., 2011), which is then used to train a
supervised extraction model (sequence tagger in our
case). In seminal work on event extraction, (Benson
et al., 2011) applied DS to both detect tweets about
local events and then extracted values about two ar-
guments (artist and venue). In our work, we work on
automatically selected tweets, and scale the task to
complex events with a large number of arguments.
We focus on the domain of earthquakes, where each
event has up to 20 arguments. Table 2 summarizes
this task.

The contributions of this work are the following:
1. To our knowledge, this is one of the first works
that analyzes the problem of distantly supervised
extraction of complex events with many arguments
from microblogs.
2. Our analysis shows (Section 3) that the biggest
barrier is that information on Twitter can be inaccu-
rate (containing approximately correct event argu-
ment values) and ambiguous (with insufficient con-
text for accurate extraction). The top two blocks in
Table 1 show an example of each. These challenges
impact both evaluation and system development.
3. The analysis also highlights the need to adapt
evaluation metrics to approximately correct infor-

mation, which may appear both in text and in the
knowledge base itself. For example, for a partic-
ular earthquake, the USGS reports a depth of 22
km., while NOAA reports 25 km2. We propose a
new evaluation metric that gives partial credit to ex-
tracted argument values based on their similarity to
existing values in the knowledge base.
4. We introduce two simple strategies that address
the above barriers for system development: approx-
imate matching, which addresses inaccurate values
by allowing the distant supervision process to map
values from the knowledge base to text even when
they do not match exactly; and feature aggrega-
tion, which responds to small, ambiguous contexts
by aggregating information across multiple tweets
for the same event. For example, the first strategy
considers the 7.1 magnitude in the first tweet in Ta-
ble 1 as a training example because it is close to the
value in the knowledge base (7.3). The second strat-
egy classifies all instances of Peru jointly using a
single set of features, extracted from all available
tweets for the corresponding earthquake. For ex-
ample, this feature set contains three values for the
feature previous-word (:, rocks, and in). Each
approach yields 19% relative improvement, 33% in
combination.
5. We release a public dataset containing a knowl-
edge base of earthquake instances and correspond-
ing tweets for each earthquake3.

2 Experimental framework

In this section we detail the creation of the knowl-
edge base of earthquake events, the collection pro-
cess for potentially-relevant tweets, and, lastly, our
distant supervision framework, which serves as a
platform for our contributions (Sections 5 and 6).

2.1 Knowledge base and tweet dataset creation

The knowledge base (KB) was created from the
list of globally significant earthquakes during the
21st century, as reported by Wikipedia.4 We se-

2http://bit.ly/aq9Vxa and http://1.usa.
gov/1p1gELB

3http://ixa.eus/Ixa/Argitalpenak/
Artikuluak/1425465524/publikoak/
earthquake-kb-dataset.zip

4https://en.wikipedia.org/wiki/List_of_
21st-century_earthquakes. Accessed on July 9th,

642



Argument Arg. # KB Example # DS # MA
Name Type Values Values Values Values
Date D 108 2009-5-28 291 706
Time T 108 T08:24:00 378 589
Country L 108 Honduras 6294 6327
Region L 77 2598 2663
City L 77 1426 1723
Latitude N 108 16.733 2 28
Longitude N 108 -86.22 4 28
Dead N 71 7 143 984
Injured N 39 22 192
Missing N 8 - 18
Magnitude N 108 7.3 933 3403
Depth (km) N 99 10 27 313
Countries Guatemala,

affected(*) L 37 Belize 436 357
Regions

affected(*) L 4 - 36
Landslides B 8 7 9
Tsunami B 10 408 273
Aftershocks N 20 5 22
Foreshocks N 3 6 -
Duration T 7 - 1
Peak

accel. N 8 - -
TOTAL 1,116 13,562 17,672

Table 2: Event arguments and types in the earthquake do-
main (first and second column), summary statistics for
the knowledge base, i.e., the gold truth (third column),
and values for one example earthquake (4th column). (*)
indicates multi-valued arguments (all other are single-
valued). The two rightmost columns give statistics for
the number of mentions in the tweets per argument, as
obtained through manual annotation (MA) or distant su-
pervision (DS) (cf. Section 2.4). The argument types are
the following: D date, T time, L location, N numeric, and
B boolean.

lected earthquakes from the beginning of 2009,
with the last reported earthquake happening on July
7th, 2013, and constructed the KB from the above
Wikipedia list page and the individual infoboxes.
Where necessary, argument values were normal-
ized.5 See Table 2 for a summary and an example.

We used the Topsy API6 to search for tweets that
are potentially relevant for each earthquake. We
formed a query using the word “earthquake” plus
the location, encoded as a disjunction of city, region,
and country arguments. We retrieved tweets from
the day before the date and time of the earthquake,
up to seven days after. This procedure might also re-
trieve tweets about aftershocks, which we consider
to be different events. We applied an aggressive
method to discard aftershock tweets: we only kept

2013, at 2PM CET.
5Time and date expressions were converted to TimeML. Nu-

merical values in English were converted to numbers, latitude
and longitudes were converted to decimal format.

6http://api.topsy.com/doc/

tweets up to the first tweet that mentions a time ex-
pression more than a minute different from that of
the main earthquake (after adjusting for time zone).
For example, this heuristic removes all tweets start-
ing with “A 4.9 earthquake occurred in Ryukyu Is-
lands, Japan on 2010-2-27 T10:33:21 at epicenter.”
because the main earthquake occurred on February
26th at 8:31PM UTC. It is important to note that
identifying event-relevant tweets is not the focus of
this work (hence the simple heuristics used for tweet
extraction). We focus instead on the extraction of
information from such tweets. In a complete sys-
tem, our approach would follow a component that
detects event tweets automatically (Benson et al.,
2011). The final dataset contains 108 earthquakes
and 7,841 tweets, 72 tweets per earthquake on av-
erage, a maximum of 654 and a minimum of 2. 19
earthquakes had less than 10 tweets.

2.2 Manual annotation of tweets
In order to analyze the challenges faced by our EE
system based on distant supervision, we also man-
ually annotated all tweets.7 The manual annotation
included any mention of an event argument in the
tweets. This included information already in the KB,
but also information that is missing, caused by: vari-
ations of dates and times, similar but not identical
latitude/longitude values, different reported num-
bers for dead/injured/missing etc. The first tweet in
Table 1 is an example of this situation: even though
the reported magnitude is different from the value
in the KB (cf. example in Table 2), it was anno-
tated during this process. In total, we annotated
17,672 mentions (at an average of two event argu-
ments per tweet). Table 2 shows the breakdown per
argument (the MA column), compared to the auto-
matic annotations generated through distant super-
vision (the DS column). Note that some of the ar-
guments have a very different coverage in the tweets
compared with the KB. For example, latitude and
longitude are rarely present in tweets, but affected
countries are commonly mentioned. The quality of
the manual annotation was assessed on a 5% sample
of the dataset, which was annotated by an additional
expert. The agreement was very high: 90% ITA and
85% Fleiss Kappa. Disagreements were generally

7These manual annotations are used solely for post-hoc
analysis, not to train our system.

643



due to missed argument mentions. Note that the cost
of annotation was around 75 hours, confirming the
cost-saving properties of distant supervision.

2.3 Dataset and experiment organization

We sorted the list of earthquakes in the KB chrono-
logically, and chose the earliest 75% of the earth-
quakes as the training dataset, and the most recent
(25%) for testing. The training set contained 81
earthquakes and their corresponding 6078 tweets,
while the testing set contained 27 earthquakes and
1763 tweets. All development experiments were
performed using 5-fold cross-validation over the
training partition, where the folds were organized
randomly by earthquake. Each fold contained tweets
for around 15 earthquakes, but the number of tweets
varied widely, with one fold having 585 tweets and
another 2229.

The evaluation compares the argument values in-
duced by our system with those in the gold KB,
and computes precision, recall and F1 using the
official scorer from the Knowledge Base Popula-
tion (KBP) Slot Filling (SF) shared task (Surdeanu,
2013). We also incorporated the notion of equiva-
lence classes proposed in the SF task. For instance,
if the system predicted Guerrero State for the ar-
gument region, when the KB contains just Guer-
rero, we consider this result correct because the two
strings are equivalent in this context. Our equiv-
alence classes also include countries, regions, and
cities with hashtags, unnormalized temporal expres-
sions, etc. Where applicable, we checked statisti-
cal significance of performance differences using the
bootstrap resampling technique proposed in (Berg-
Kirkpatrick et al., 2012), in which we draw many
simulated test sets by sampling with replacement
from the set of earthquakes in the test partition.

2.4 Distant supervision for event extraction

For the initial extraction experiment, we followed
a traditional distant supervision approach (Mintz et
al., 2009), which has four steps: the KB of past
events is aligned to the text; a supervised system
is trained on the resulting annotated text; the sys-
tem is run on test data; and the output slot values
are inferred from the annotations produced by the
system. We thus started by aligning the information
in the KB to the training tweets using strict match-

ing8. Table 2 compares the number of mentions au-
tomatically generated through DS against the num-
ber of manually annotated mentions. As expected,
the strict matching criterion yields fewer mentions
than the manual annotation.

As an example of this process, given the Honduras
earthquake in Table 2, this procedure will annotate
two argument mentions in the first tweet from Ta-
ble 1, country and affected-country, as fol-
lows:

Earthquake in <country>Honduras</country>.
So strong it was felt in <affected-
country>Guatemala</affected-country> as
well. 7.1 offshore atlantic.

Note that the magnitude in the tweet is different
from the one reported in the KB and it will thus be
left unmarked (we revisit this issue in Section 5).

Using this automatically-generated data, we
trained a sequential tagger based on Conditional
Random Fields (CRF)9. Based on the output of the
CRF, we inferred the arguments values using noisy-
or (Surdeanu et al., 2012), which selects the value
with the largest probability for each single-valued
argument by aggregating the individual mention
probabilities produced by the CRF.10 In the case of
multi-valued arguments (affected-country and
affected-region) we choose all values that had
been annotated by the sequential tagger.

3 Initial results and analysis

The left block in Table 3 reports the results on devel-
opment (5-fold cross-validation) of the initial event

8We identified two types of arguments: those that have bi-
nary (yes/no) values (tsunami and landslides) and
those having other values. For the first type, we search the
tweets corresponding to the target earthquake for a small num-
ber of strings (e.g., tsunami and tsunamis), and annotate all
matches (e.g., <tsunami> tsunami </tsunami>). For non-
binary valued arguments, we searched the tweets for exact oc-
currences of the corresponding values, and annotated all match-
ing strings. When the same value appears in more than one
argument for the same earthquake (e.g., 7 as both magnitude
and number of dead people), we choose the most common label
(e.g., magnitude cf. Table 2).

9We used the linear CRF in Stanford’s CoreNLP package,
with the default features (word form, PoS, lemma, NERC) for
the macro configuration: http://nlp.stanford.edu/
software/corenlp.shtml.

10For multi-token mentions (e.g. New Zealand) we use the
average of the token probabilities.

644



Strict Evaluation
System Prec. Rec. F1

DS-CRF 53.1 22.0 31.1
MA-CRF 44.1 26.1 32.8

Lenient Evaluation
DS-CRF 67.4 27.9 39.4

MA-CRF 62.1 36.8 46.2

Table 3: Development: Results for the distant supervision
system (DS-CRF). We also include results for the same
CRF trained on manual annotations (MA-CRF). The reg-
ular evaluation is shown in the left columns and lenient
evaluation (cf. Section 4) in the right.

extraction system based on a distantly-supervised
CRF (DS-CRF), which notably attains higher pre-
cision than recall. These results are fair, e.g., they
are comparable to those of (Benson et al., 2011),
even though their events had much fewer argument
types than ours (two vs. twenty). More importantly,
we use this system’s output to analyze where the ap-
proach could be improved. For the sake of compari-
son, we trained the same CRF with the manually an-
notated tweets, cf. Section 2 (MA-CRF). The MA-
CRF results in Table 3 indicate that the main loss
when doing distant supervision is in recall, but the
overall F1 is close. This is remarkable, as the much
more expensive MA-CRF (75 hours of human anno-
tation) is taken to be an upperbound for DS-CRF.

Manual inspection showed that that DS-CRF re-
turns fewer argument values than MA-CRF (328
vs. 469), from “easier” (more common) arguments
which have a higher chance of appearing both in the
text and the KB. Importantly, MA-CRF has lower
precision than its distant supervision counterpart be-
cause it is trained on manual annotations, which in-
cluded many mentions not in the KB. The conse-
quence of this strategy is that MA-CRF tends to
produce spurious mentions (i.e., mentions not in the
KB) at evaluation time, which lowers precision.

In addition, we analyzed the annotations cre-
ated through distant supervision11, which produced
13,562 argument mentions in the training tweets (cf.
Table 2, which also includes a breakdown by ar-

11Note that these are the argument mention annotations used
to train DS-CRF, not the arguments inferred by the DS-CRF
system.

gument). This data contains incorrectly annotated
strings (false positives) and also misses relevant ar-
gument values (false negatives). A comparison of
these DS annotations against the manual annotations
on all training tweets (17,672 mentions) yielded that
97.4% were correct, but that 27.4% of the gold man-
ual annotations were missed. This is an important
result: it demonstrates that, unlike in the problem
of relation extraction (RE) where the major issue is
the large percentage (higher than 30%) of false pos-
itives in automatically-created annotations (Riedel
et al., 2010), here the fundamental roadblock is
missing annotations (i.e., false negatives). We ex-
plain this difference by the fact that for this event
extraction domain, it is trivial to identify domain-
relevant tweets, which reduces the number of false
positives for event arguments. We believe this gen-
eralizes to many other EE domains, e.g., airplane
crashes (Reschke et al., 2014) or terrorist attacks,
where the event context can be summarized accu-
rately with a small number of keywords (e.g., flight
number and date for the airplane crashes domain).

We also did a post-hoc analysis of the quality of
the arguments induced by DS-CRF. One of the most
significant outcomes of the analysis is that a large
portion of numeric values (31.3%) were partially
correct, in that the returned values were very simi-
lar to those in the KB (see for instance the 7.1 vs.
7.3 example in Section 1). This strongly suggests
that the evaluation metric should be more lenient,
and give credit to argument values that are similar to
the gold ones.

4 Lenient evaluation

The previous analysis suggests that traditional eval-
uation measures unnecessarily penalize arguments
containing values that do not match the gold truth
exactly. Rather than giving no credit when predicted
values are different from gold ones, we devised a
simple extension to the KBP evaluation measures
that take into account the similarity between the val-
ues of system and gold arguments, where the simi-
larity depends on the type of each slot (cf. Table 2).
For numeric values, we use the following formula,
where x is the predicted value, and g the gold value:

sim(x, g) = max
(

1− |x− g|
g

, 0
)

(1)

645



For example, given a gold value of 7.3, a system
value of 7.2 would have a similarity of 0.98, and a
system value of 14.6 or larger would have a similar-
ity 0. If both values are equal, similarity is 1.

For the other slot types, the similarity function is
discrete, with values set to 1 (proposed slot is cor-
rect) or 0 (incorrect) as follows. We consider a pro-
posed temporal argument as correct if it is within a
span of 5 minutes of the corresponding gold tem-
poral value. Durations are judged as correct if they
are within 10 seconds of the gold values. We con-
sidered proposed dates as correct if they differ by at
most one day from the gold date.12

For location arguments, we use GeoNames13 to
obtain the coordinates of the locations produced by
the system that do not match the information in the
KB. Based on the average size of countries, regions,
and cities, we consider these additional locations
as correct if they are at the following distance (or
closer) from the gold locations: 500 kms for coun-
tries, 50 kms for regions, and 10 kms for cities.

The original KBP scorer increases the value of
True Positives (TP) by 1 every time a predicted argu-
ment matches its gold value. In the proposed lenient
scorer, TP is increased by the similarity between the
predicted and gold values. The precision and recall
will be thus calculated as follows (SYS for number
of predicted argument values, GOLD for number of
gold argument values):

prec =
∑

sim(x, g)
SYS

(2)

rec =
∑

sim(x, g)
GOLD

(3)

The right block in Table 3 lists the results under
this lenient evaluation for the experiment initially
reported in the left block in the same table. As ex-
pected, these results are higher than the ones using
the strict measure, but maintain the relative order of
the systems in each of the evaluation measures. The
difference in precision between DS-CRF and MA-
CRF decreases, indicating that the new measure as-
signs partial credit to the larger amount of argument
values extracted by MA-CRF. The difference in re-

12These thresholds might change in other domains, but ad-
justing these values is trivial.

13http://www.geonames.org/

 0.45

 0.5

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0.85

 0.9

 0.95

 1

 0  0.05  0.1  0.15  0.2  0.25  0.3

P
re

c
is

io
n

Recall

DS-CRF
DS

appr
-CRF

Figure 1: Test: Precision/Recall curves for regular DS
and approximate DS on test (lenient evaluation).

System Prec. Rec. F1
DS-CRF 68.4 21.3 32.5

DSappr-CRF 70.6 27.8 39.9 †

Table 4: Test: Regular (DS-CRF) and approximate DS
(DSappr-CRF) results, with lenient evaluation. † indicates
statistically significant improvement over DS-CRF (p <
0.05).

call values remains large. We address this in the next
section.

5 Approximate distant supervision

The previous section demonstrated that many tweets
contain argument values which are similar but not
identical to the data in the knowledge base. These
values would not be annotated during alignment by
traditional distant supervision, which expects an ex-
act match between knowledge base values and tweet
texts. This means that DS-CRF will be trained with
less data than what is available (e.g., without the
7.1 magnitude example in the tweet in Section 2.4).
Here we demonstrate that a simple extension to dis-
tant supervision that annotates values close to the
values in the knowledge base, results in improved
performance.

The proposed alignment algorithm scans the
training tweets, and labels named and numeric en-
tities as positive argument examples (with the cor-
responding label from the KB), if they are deemed
similar to the gold values according to the similar-
ity formulas introduced in the previous section. This

646



 0.45

 0.5

 0.55

 0.6

 0.65

 0.7

 0.75

 0.8

 0.85

 0.9

 0.95

 1

 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35

P
re

c
is

io
n

Recall

DS-CRF
DS

aggr
-CRF

DS
comb

-CRF

Figure 2: Test: P/R curves for DS-CRF, feature aggrega-
tion and combination with approximate DS (lenient eval-
uation).

is a trivial process for discrete similarities, but re-
quires some care for continuous similarity functions,
which are triggered for numeric arguments. In this
situation, numeric entities are considered as positive
examples only if their similarity function returns a
value over a certain threshold with a known argu-
ment in the KB. If a numeric mention has more than
one matching argument in the KB, the algorithm
chooses the argument label with the highest simi-
larity value; if all have the same similarity, the algo-
rithm chooses the most frequent label in training.

We tuned the threshold hyper parameter for nu-
meric values over the training dataset using 5-fold
cross validation, which yielded 0.95 as the optimal
value. Table 4 shows the results for the test parti-
tion using this threshold, and Figure 1 shows the
corresponding P/R curves. Both results are gen-
erated using the proposed lenient evaluation. The
results in the table show that, despite its simplic-
ity, the proposed alignment algorithm yields consid-
erable, statistically-significant improvements. The
P/R curves show that the improvement holds for all
recall points14.

6 Feature aggregation

The second block in Table 1 illustrates a common
scenario on Twitter, where a short, ambiguous tweet
derails the extraction. We address this problem of

14The curves for the strict evaluation are similar, and were
omitted for brevity.

System Prec. Rec. F1
DS-CRF 68.4 21.3 32.5

DSaggr-CRF 70.1 26.6 38.6 †
DScomb-CRF 69.2 31.2 43.1 †

MA-CRF 69.1 37.9 48.9

Table 5: Test: Results for regular DS (DS-CRF), DS with
feature aggregation (DSaggr-CRF), and the DS model that
combines feature aggregation and approximate matching
(DScomb-CRF), with lenient evaluation. † indicates statis-
tically significant improvement over DS-CRF (p < 0.05).
We include the results of the CRF trained on manual an-
notations (MA-CRF) as a performance ceiling for this
task.

insufficient local context with a method inspired by
work in relation extraction, where relation instances
between identical entities are classified jointly using
the conjunction of features from all instances (Mintz
et al., 2009). We adapt this idea to our sequence
tagging EE model as follows:

1: We focus on location, date and temporal enti-
ties (both earthquake time and duration) which are
argument candidates that are often ambiguous, i.e.,
they may be classified as more than one argument
type. For example, a location entity may be labeled
as country, region, country-affected, etc.
We exclude numeric entities due to potential feature
collisions between different argument types: we ob-
served that, in training, several earthquakes had dif-
ferent numeric arguments with the same value. For
example, the magnitude and depth of the 2012 Zo-
han earthquake were 5.6. Applying feature aggrega-
tion to examples of these arguments would lead to
collisions between features from different classes.15

2: For each token that appears in one of these named
entities, we identify all its instances across the rele-
vant tweets, and share features across all these token
instances. For example, for the tweets in the sec-
ond block in Table 1, our approach identifies Peru
as an argument mention candidate. All three in-
stances of Peru are then classified using the same
shared features, e.g., using three values for the fea-

15Initial experiments confirmed this hypothesis: feature ag-
gregation did not improve results for numeric arguments in
development. In future work, we will explore multi-instance
multi-label algorithms to handle this situation (Surdeanu et al.,
2012).

647



ture previous-word (:, rocks, and in). This pro-
cess is repeated for each earthquake individually,
because tokens may be labeled differently in differ-
ent earthquakes. This approach produced 37% more
features than the DS-CRF baseline.16

The positive effect of feature aggregation is con-
firmed by the formal evaluation on the test dataset.
Table 5 shows a statistically significant improvement
in overall F1, for the lenient evaluation. The P/R
curves (Fig. 2) indicate that DSaggr-CRF’s improve-
ment comes from both better recall and better preci-
sion that the DS-CRF baseline.

Table 5 and Fig. 2 also show that the combina-
tion of approximate matching and aggregation out-
performs the individual models, demonstrating that
feature aggregation is complementary to approxi-
mate matching. The combined model attains a rela-
tive improvement of 33% over the DS-CRF baseline,
reaching approximately 88% of the ceiling perfor-
mance for this task (MA-CRF row, the CRF trained
on manual annotations).

7 Related work

There has been considerable recent interest in IE
from Twitter. However, in general, these works
use supervised learning frameworks (Popescu et
al., 2011; Ritter et al., 2012), and/or they use ei-
ther a coarse representation of events, which re-
duces to topic modeling or classification of entire
tweets (Popescu et al., 2011; Becker et al., 2011;
Ritter et al., 2012), or a simplified representation
of events with few arguments (Sakaki et al., 2010;
Popescu et al., 2011; Benson et al., 2011; Ritter
et al., 2012). In contrast, our work uses a com-
plex event representation with 20 arguments, and
does not require any manual annotation of tweets.
Our work is closest, but complementary to the work
of (Benson et al., 2011), which also uses distant su-
pervision for event extraction: We provide solutions
for two problems they do not address (inaccurate and
ambiguous information) and we focus on more com-
plex events (20 arguments vs. two).

This paper is also complementary to systems
which detect event-relevant tweets (Sakaki et al.,

16We also tried skip-chain CRFs (Getoor and Taskar, 2007),
but found that our simpler approach converges considerably
faster and produces slightly better results. We do not show those
results for brevity.

System Prec. Rec. F1
DS-CRF 66.21 20.66 31.49

DSaggr-CRF 68.27 25.92 37.58 †
DScomb-CRF 61.53 27.61 38.25 †

MA-CRF 68.76 27.61 39.40

Table 6: Test: Replica of the experiments in Table 5 using
a threshold of 0.95 for the lenient evaluation measure. All
other settings are identical to the experiments in Table 5.
† indicates statistically significant improvement over DS-
CRF (p < 0.05).

2010; Petrović et al., 2010). In future work, we plan
to replace our simple method of extracting relevant
tweets by one of these approaches, producing a sys-
tem that monitors microblogs in realtime to automat-
ically construct event-specific knowledge bases.

Our work uses the framework of distant supervi-
sion, which has also received considerable attention
recently. Nevertheless, most of these works focus on
the extraction of binary relations from well-formed
documents (Mintz et al., 2009; Riedel et al., 2010;
Hoffmann et al., 2011; Surdeanu et al., 2012). We
use the much noisier Twitter as the underlying text,
and extract complex events instead of binary rela-
tions. We note, however, that the idea of feature ag-
gregation is inspired by these works (Mintz et al.,
2009; Riedel et al., 2010), but, to our knowledge,
we are the first to apply it to event extraction and
sequence tagging. In the DS space, our work is clos-
est to (Reschke et al., 2014), which use it to extract
complex events (airplane crashes) from newswire
text. Because they focus on newswire, they do not
need to address the potential for inaccurate or am-
biguous information, which is the main focus of our
work.

8 Discussion: An alternate evaluation
measure

Designing relevant measures for lenient evaluations,
such as the one discussed here, is an open research
issue. For example, the method proposed in Sec-
tion 4 gives partial credit to all reported (positive)
numeric values in the interval [0, 2g], where g is
the correct value for the corresponding slot (see the
equation in Section 4). But other, stricter, measures

648



are certainly possible.17 For example, one stricter
variant of our proposed measure would assign par-
tial credit only for predicted values that have a sim-
ilarity of 0.95 or higher with the gold truth (inline
with our approximate DS training process). For ex-
ample, for the same gold numeric value g, the mea-
sure assigns partial credit only for predicted values
in the interval [0.95g, 1.05g].

We repeated the experiments in Table 5 using this
alternate evaluation measure. The result are summa-
rized in Table 6. The results reported in Table 5 do
not alter the findings of the paper. In fact, under this
stricter evaluation measure, our results are stronger:
DScomb-CRF, which combines both our ideas, ap-
proaches with nearly 1 F1 point MA-CRF, which
trains on manually annotated data.

9 Conclusions

To our knowledge, this is one of the first works that
analyzes the problem of distantly supervised com-
plex event extraction on microblogs. This near real-
time data source is challenging, with inaccurate in-
formation and short, ambiguous texts, as shown by
our empirical analysis of the dataset. We proposed
two simple techniques to address these problems:
(a) a novel distant supervision paradigm, which im-
plements an alignment algorithm that allows text
snippets that are similar but not identical to argu-
ment values in the knowledge base to be annotated
(thus producing better training data); and (b) a fea-
ture aggregation strategy that provides richer infor-
mation across tweets to cope with ambiguity. Our
results on earthquake-related tweets show that each
improvement yields 19% significant improvement
when applied on top of a strong system based on se-
quence tagging (CRFs). We show that these contri-
butions are complementary: a model that combines
both performs better than each of the above individ-
ual models, with an improvement of 33% over the
baseline. All in all, our approach attains approxi-
mately 88% of the ceiling performance for this task,
which is obtained by a system trained on manually-
annotated tweets, validating the hypothesis that dis-
tant supervision is useful for a complex event extrac-
tion task.

17We thank the anonymous reviewer for the suggestion.

In addition, we devised a lenient evaluation mea-
sure which incorporates the similarity between the
extracted argument values and the gold truth, rather
than considering as correct only the extractions that
exactly match the gold values. We show that this
evaluation models the event extraction task better,
and, furthermore, is more realistic, especially in
view of imperfect knowledge bases.

Lastly, we release a dataset containing an event
knowledge base constructed from Wikipedia infor-
mation on earthquakes, which contains 108 earth-
quakes, 20 different argument types, and 1,116 argu-
ment values. The dataset also includes a collection
of relevant tweets about these earthquakes, totaling
7,841 tweets. The dataset is publicly available.18

Acknowledgements

This work was partially funded by MINECO
(CHIST-ERA READERS project – PCIN-2013-
002-C02-01, EXTRECM project – TIN2013-46616-
C2-1-R, SKaTeR project – TIN2012-38584-C06-
02), and the European Commission (QTLEAP –
FP7-ICT-2013.4.1-610516). Ander Intxaurrondo is
supported by a PhD grant from the Basque Coun-
try Government. The IXA group is funded by the
Basque Government (A type Research Group).

18http://ixa.eus/Ixa/Argitalpenak/
Artikuluak/1425465524/publikoak/
earthquake-kb-dataset.zip

649



References
Hila Becker, Mor Naaman, and Luis Gravano. 2011. Be-

yond trending topics: Real-world event identication on
twitter. In Proceedings of the Conference of the Asso-
ciation for the Advancement of Articial Intelligence.

Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the Conference of the Association for Com-
putational Linguistics (ACL).

Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ’12, pages 995–1005, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program – tasks, data, and
evaluation. In Proceedings of LREC.

L. Getoor and B. Taskar, 2007. Introduction to statistical
relational learning. MIT Press.

R. Grishman and B. Sundheim. 1996. Message under-
standing conference - 6: A brief history. In Proceed-
ings of the International Conference on Computational
Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).

Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.

Saša Petrović, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 181–189, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.

Ana-Maria Popescu, Marco Pennacchiotti, and Deepa
Paranjpe. 2011. Extracting events and event descrip-
tions from twitter. In Proceedings of the 20th Interna-
tional Conference on World Wide Web.

Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,
Christopher D. Manning, and Daniel Jurafsky. 2014.
Event extraction using distant supervision. In Pro-
ceedings of LREC.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).

Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of KDD.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: Real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on World Wide Web,
WWW ’10, pages 851–860, New York, NY, USA.
ACM.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning (EMNLP-CoNLL).

Mihai Surdeanu. 2013. Overview of the tac2013 knowl-
edge base population evaluation: English slot filling
and temporal slot filling. In Proceedings of the TAC-
KBP 2013 Workshop.

650


