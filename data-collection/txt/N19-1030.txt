




































Learning to Attend On Essential Terms: An Enhanced Retriever-Reader Model for Open-domain Question Answering


Proceedings of NAACL-HLT 2019, pages 335‚Äì344
Minneapolis, Minnesota, June 2 - June 7, 2019. c¬©2019 Association for Computational Linguistics

335

Learning to Attend On Essential Terms: An Enhanced Retriever-Reader
Model for Open-domain Question Answering

Jianmo Ni1‚àó, Chenguang Zhu2, Weizhu Chen3, Julian McAuley1
1 University of California, San Diego

2 Microsoft Speech and Dialogue Research Group
3 Microsoft Dynamics 365 AI

{jin018,jmcauley}@ucsd.edu, {chezhu,wzchen}@microsoft.com

Abstract
Open-domain question answering remains a
challenging task as it requires models that are
capable of understanding questions and an-
swers, collecting useful information, and rea-
soning over evidence. Previous work typi-
cally formulates this task as a reading com-
prehension or entailment problem given evi-
dence retrieved from search engines. How-
ever, existing techniques struggle to retrieve
indirectly related evidence when no directly
related evidence is provided, especially for
complex questions where it is hard to parse
precisely what the question asks. In this pa-
per we propose a retriever-reader model that
learns to attend on essential terms during the
question answering process. We build (1) an
essential term selector which first identifies the
most important words in a question, then refor-
mulates the query and searches for related ev-
idence; and (2) an enhanced reader that distin-
guishes between essential terms and distract-
ing words to predict the answer. We evaluate
our model on multiple open-domain multiple-
choice QA datasets, notably performing at the
level of the state-of-the-art on the AI2 Reason-
ing Challenge (ARC) dataset.

1 Introduction

Open-domain question answering (QA) has been
extensively studied in recent years. Many exist-
ing works have followed the ‚Äòsearch-and-answer‚Äô
strategy and achieved strong performance (Chen
et al., 2017; Kwon et al., 2018; Wang et al., 2018b)
spanning multiple QA datasets such as TriviaQA
(Joshi et al., 2017), SQuAD (Rajpurkar et al.,
2016), MS-Macro (Nguyen et al., 2016), ARC
(Clark et al., 2018) among others.

However, open-domain QA tasks become inher-
ently more difficult when (1) dealing with ques-
tions with little available evidence; (2) solving

‚àóMost of the work was done during internship at Mi-
crosoft, Redmond.

questions where the answer type is free-form text
(e.g. multiple-choice) rather than a span among
existing passages (i.e., ‚Äòanswer span‚Äô); or when
(3) the need arises to understand long and com-
plex questions and reason over multiple passages,
rather than simple text matching. As a result, it is
essential to incorporate commonsense knowledge
or to improve retrieval capability to better capture
partially related evidence (Chen et al., 2017).

As shown in Table 1, the TriviaQA, SQuAD,
and MS-Macro datasets all provide passages
within which the correct answer is guaranteed to
exist. However, this assumption ignores the diffi-
culty of retrieving question-related evidence from
a large volume of open-domain resources, espe-
cially when considering complex questions which
require reasoning or commonsense knowledge.
On the other hand, ARC does not provide passages
known to contain the correct answer. Instead, the
task of identifying relevant passages is left to the
solver. However, questions in ARC have multi-
ple answer choices that provide indirect informa-
tion that can help solve the question. As such
an effective model needs to account for relations
among passages, questions, and answer choices.
Real-world datasets such as Amazon-QA (a cor-
pus of user queries from Amazon) (McAuley and
Yang, 2016) also exhibit the same challenge, i.e.,
the need to surface related evidence from which to
extract or summarize an answer.

Figure 1 shows an example of a question in the
ARC dataset and demonstrates the difficulties in
retrieval and reading comprehension. As shown
for Choice 1 (C1), a simple concatenation of the

1For SQuAD and TriviaQA, since the questions are paired
with span-type answers, it is convenient to obtain ranking su-
pervision where retrieved passages are relevant via distant su-
pervision; however free-form questions in ARC and Amazon-
QA result in a lack of supervision which makes the problem
more difficult. For MS-Macro, the dataset is designed to an-
notate relevant passages though it has free-form answers.



336

Dataset # of questions Open-
domain

Multiple
choice

Passage
retrieval

No ranking
supervision1

ARC (Clark et al., 2018) ‚âà 7K 3 3 3 3
Amazon-QA (McAuley and Yang, 2016) ‚âà 1.48M 3 3

SQuAD (Rajpurkar et al., 2016) ‚âà 100K 3
TriviaQA (Joshi et al., 2017) ‚âà 650K 3
MS-Macro (Nguyen et al., 2016) ‚âà 1M 3

Table 1: Differences among popular QA datasets.

Query1: Mercury , the planet nearest to the Sun , has extreme surface 
temperatures , ranging from 465 C in sunlight to -180 C in darkness . 
Why is there such a large range of temperatures on Mercury?  The 
planet is too small to hold heat.

Q: Mercury , the planet nearest to the Sun , has extreme surface temperatures , ranging from 465 
C in sunlight to -180 C in darkness . Why is there such a large range of temperatures on Mercury?

C1: The planet is too small to hold heat. C4: The planet lacks an atmosphere to hold heat . 

Query1 = Q+C1

S1: The lack of atmosphere also contributes to the 
planet 's wild temperature extremes . 
S2: Mercury is the closest planet to the sun and has a 
thin atmosphere, no air pressure and an extremely
high temperature. 
‚Ä¶

S1: Other planets such as Mercury has extreme
hot and cold temperatures . 
S2: The planet Mercury is too small and has too 
little gravity to hold onto an atmosphere. 
‚Ä¶

MRC

‚Ä¶

‚Ä¶

Retrieving evidence

Sending evidence to reader

Query4: Mercury extreme surface temperatures. 
The planet lacks an atmosphere to hold heat .

Retrieving evidence

Query4 = Essential-term(Q)+C4

Figure 1: Example of the retrieve-and-read process to solve open-domain questions. Words related with the ques-
tion are in bold; and words related with C1 and C4 are in italics.

question and the answer choice is not a reliable
query and is of little help when trying to find sup-
porting evidence to answer the question (e.g. we
might retrieve sentences similar to the question or
the answer choice, but would struggle to find ev-
idence explaining why the answer choice is cor-
rect). On the other hand, a reformulated query
consisting of essential terms in the question and
Choice 4 can help retrieve evidence explaining
why Choice 4 is a correct answer. To achieve this,
the model needs to (1) ensure that the retrieved
evidence supports the fact mentioned in both the
question and the answer choices and (2) capture
this information and predict the correct answer.

To address these difficulties, we propose an
essential-term-aware Retriever-Reader (ET-RR)
model that learns to attend on essential terms dur-
ing retrieval and reading. Specifically, we develop
a two-stage method with an essential term selector
followed by an attention-enhanced reader.

Essential term selector. ET-Net is a recurrent
neural network that seeks to understand the ques-
tion and select essential terms, i.e., key words,
from the question. We frame this problem as a
classification task for each word in the question.

These essential terms are then concatenated with
each answer choice and fed into a retrieval engine
to obtain related evidence.

Attention-Enhanced Reader. Our neural
reader takes the triples (question, answer choice,
retrieved passage) as input. The reader consists
of a sequence of language understanding layers:
an input layer, attention layer, sequence model-
ing layer, fusion layer, and an output layer. The
attention and fusion layers help the model to ob-
tain a refined representation of one text sequence
based on the understanding of another, e.g. a pas-
sage representation based on an understanding of
the question. We further add a choice-interaction
module to handle the semantic relations and differ-
ences between answer choices. Experiments show
that this can further improve the model‚Äôs accuracy.

We evaluate our model on the ARC Chal-
lenge dataset, where our model achieves an ac-
curacy of 36.61% on the test set, and outper-
formed all leaderboard solutions at the time of
writing (Sep. 2018). To compare with other
benchmark datasets, we adapt RACE (Lai et al.,
2017) and MCScript (Ostermann et al., 2018) to
the open domain setting by removing their super-



337

vision in the form of relevant passages. We also
consider a large-scale real-world open-domain
dataset, Amazon-QA, to evaluate our model‚Äôs
scalability and to compare against standard bench-
marks designed for the open-domain setting. Ex-
periments on these three datasets show that ET-
RR outperforms baseline models by a large mar-
gin. We conduct multiple ablation studies to show
the effectiveness of each component of our model.
Finally, we perform in-depth error analysis to ex-
plore the model‚Äôs limitations.

2 Related Work

There has recently been growing interest in build-
ing better retrievers for open-domain QA. Wang
et al. (2018b) proposed a Reinforced Ranker-
Reader model that ranks retrieved evidence and
assigns different weights to evidence prior to pro-
cessing by the reader. Min et al. (2018) demon-
strated that for several popular MRC datasets
(e.g. SQuAD, TriviaQA) most questions can be
answered using only a few sentences rather than
the entire document. Motivated by this observa-
tion, they built a sentence selector to gather this
potential evidence for use by the reader model.
Nishida et al. (2018) developed a multi-task learn-
ing (MTL) method for a retriever and reader in or-
der to obtain a strong retriever that considers cer-
tain passages including the answer text as posi-
tive samples during training. The proposed MTL
framework is still limited to scenarios where it
is feasible to discover whether the passages con-
tain the answer span. Although these works have
achieved progress on open-domain QA by improv-
ing the ranking or selection of given evidence,
few have focused on the scenario where the model
needs to start by searching for the evidence itself.

Scientific Question Answering (SQA) is a rep-
resentative open-domain task that requires capa-
bility in both retrieval and reading comprehen-
sion. In this paper, we study question answering
on the AI2 Reasoning Challenge (ARC) scientific
QA dataset (Clark et al., 2018). This dataset con-
tains multiple-choice scientific questions from 3rd
to 9th grade standardized tests and a large cor-
pus of relevant information gathered from search
engines. The dataset is partitioned into ‚ÄúChal-
lenge‚Äù and ‚ÄúEasy‚Äù sets. The challenge set consists
of questions that cannot be answered correctly by
either of the solvers based on Pointwise Mutual
Information (PMI) or Information Retrieval (IR).

Existing models tend to achieve only slightly bet-
ter and sometimes even worse performance than
random guessing, which shows that existing mod-
els are not well suited to this kind of QA task.

Jansen et al. (2017) first developed a rule-based
focus word extractor to identify essential terms in
the question and answer candidates. The extracted
terms are used to aggregate a list of potential an-
swer justifications for each answer candidate. Ex-
periments shown that focus words are beneficial
for SQA on a subset of the ARC dataset. Khashabi
et al. (2017) also worked on the problem of find-
ing essential terms in a question for solving SQA
problems. They published a dataset containing
over 2,200 science questions annotated with es-
sential terms and train multiple classifiers on it.
Similarly, we leverage this dataset to build an es-
sential term selector using a neural network-based
algorithm. More recently, Boratko et al. (2018)
developed a labeling interface to obtain high qual-
ity labels for the ARC dataset. One finding is that
human annotators tend to retrieve better evidence
after they reformulate the search queries which are
originally constructed by a simple concatenation
of question and answer choice. By feeding the
evidence obtained by human-reformulated queries
into a pre-trained MRC model (i.e., DrQA (Chen
et al., 2017)) they achieved an accuracy increase
of 42% on a subset of 47 questions. This shows
the potential for a ‚Äúhuman-like‚Äù retriever to boost
performance on this task.

Query reformulation has been shown to be
effective in information retrieval (Lavrenko and
Croft, 2001). Nogueira and Cho (2017) modeled
the query reformulation task as a binary term se-
lection problem (i.e., whether to choose the term
in the original query and the documents retrieved
using the original query). The selected terms are
then concatenated to form the new query. Instead
of choosing relevant words, Buck et al. (2018) pro-
posed a sequence-to-sequence model to generate
new queries. Das et al. (2019) proposed Multi-
step Retriever-Reader which explores an iterative
retrieve-and-read strategy for open-domain ques-
tion answering. It formulates the query reformu-
lation problem in the embedding space where the
vector representation of the question is changed to
improve the performance. Since there is no super-
vision for training the query reformulator, all these
methods using reinforcement learning to maxi-
mize the task-specific metrics (e.g. Recall for para-



338

graph ranking, F1 and Exact Matching for span-
based MRC). Different from these works, we train
the query reformulator using an annotated dataset
as supervision and then apply the output to a sep-
arate reader model. We leave the exploration of
training our model end-to-end using reinforcement
learning as future work.

3 Approach

In this section, we introduce the essential-term-
aware retriever-reader model (ET-RR). As shown
in Figure 2, we build a term selector to discover
which terms are essential in a question. The se-
lected terms are then used to formulate a more effi-
cient query enabling the retriever to obtain related
evidence. The retrieved evidence is then fed to the
reader to predict the final answer.

For a question with q words Q = {wQt }
q
t=1

along with its N answer choices C = {Cn}Nn=1
where Cn = {wCt }ct=1, the essential-term selector
chooses a subset of essential terms E ‚äÇ Q, which
are then concatenated with each Cn to formulate a
query. The query for each answer choice, E+Cn,
is sent to the retriever (e.g. Elastic Search2), and
the top K retrieved sentences based on the scores
returned by the retriever are then concatenated into
the evidence passage Pn = {wPt }Kt=1.

Next, given these text sequences Q, C, and
P = {Pn}Nn=1, the reader will determine a match-
ing score for each triple {Q,Cn,Pn}. The answer
choice Cn‚àó with the highest score is selected.

We first introduce the reader model in Sec-
tion 3.1 and then the essential term selector in Sec-
tion 3.2.

3.1 Reader Model
3.1.1 Input Layer
To simplify notation, we ignore the subscript n
denoting the answer choice until the final output
layer. In the input layer, all text inputs‚Äîthe ques-
tion, answer choices, and passages, i.e., retrieved
evidence‚Äîare converted into embedded represen-
tations. Similar to Wang (2018), we consider the
following components for each word:
Word Embedding. Pre-trained GloVe word em-
bedding with dimensionality dw = 300.
Part-of-Speech Embedding and Named-Entity
Embedding. The part-of-speech tags and named
entities for each word are mapped to embeddings
with dimension 16.

2https://www.elastic.co/products/elasticsearch

Relation Embedding. A relation between each
word in P and any word in Q or C is mapped to
an embedding with dimension 10. In the case that
multiple relations exist, we select one uniformly
at random. The relation is obtained by querying
ConceptNet (Speer et al., 2017).
Feature Embeddings. Three handcrafted features
are used to enhance the word representations: (1)
Word Match; if a word or its lemma of P exists in
Q or C, then this feature is 1 (0 otherwise). (2)
Word Frequency; a logarithmic term frequency is
calculated for each word. (3) Essential Term; for
the i-th word in Q, this feature, denoted as wei ,
is 1 if the word is an essential term (0 otherwise).
Let we = [we1 , we2 , . . . , weq ] denote the essential
term vector.

For Q,C,P, all of these components are con-
catenated to obtain the final word representations
WQ ‚àà Rq√ódQ ,WC ‚àà Rc√ódC ,WP ‚àà Rp√ódP ,
where dQ, dC , dP are the final word dimensions
of Q,C, and P.

3.1.2 Attention Layer
As shown in Figure 2, after obtaining word-level
embeddings, attention is added to enhance word
representations. Given two word embedding se-
quences WU ,WV , word-level attention is calcu-
lated as:

M
‚Ä≤
UV = WUU ¬∑ (WV V)>

MUV = softmax(M
‚Ä≤
UV )

WVU = MUV ¬∑ (WV V),

(1)

where U ‚àà RdU√ódw and V ‚àà RdV √ódw are two
matrices that convert word embedding sequences
to dimension dw, and M

‚Ä≤
UV contains dot products

between each word in WU and WV , and soft-
max is applied on M

‚Ä≤
UV row-wise. Three types

of attention are calculated using Equation (1):
(1) question-aware passage representation WQP ‚àà
Rp√ódw ; (2) question-aware choice representation
WQC ‚àà Rc√ódw ; and (3) passage-aware choice rep-
resentation WPC ‚àà Rc√ódw .

3.1.3 Sequence Modeling Layer
To model the contextual dependency of each text
sequence, we use BiLSTMs to process the word
representations obtained from the input layer and
attention layer:

Hq = BiLSTM[WQ]

Hc = BiLSTM[WC ;W
P
C ;W

Q
C ]

Hp = BiLSTM[WP ;W
Q
P ],

(2)



339

Question Essential 
terms

Choice 1

Choice N

‚Ä¶
Query 1

Query N

Passage 1

IR retriever

Attention enhanced reader

‚Ä¶

‚Ä¶

Prediction

Essential term 
selector

Choice 1 Passage 1

+

+

=

= Passage N

Softmax

Hc

Self fusionQuestion-aware 
matching

Self fusion

Hp Hq

Choice 
interaction

ùëùùëù ùëûùëû ùëêùëê

Question Choice 1 Choice N

Sequence Modeling layer

Prediction

Input layer

Attention layer

(a) Retriever-reader
model

(c) Fusion layer

(b) Essential term
selector

‚Ä¶

Projection layer

+ +

+

Sequence Modeling layer

Fusion layer

Score 1

Input layer

Attention layer

Output layer

Question Choice 1 Passage 1

Sequence Modeling layer

Fusion layer

Input layer

Attention layer

Output layer

Question Choice N Passage N

Sequence Modeling layer

Fusion layer

Score N

Input layer

Attention layer

Output layer‚Ä¶

Figure 2: Model structure for our essential-term-aware retriever-reader model.

where Hq ‚àà Rq√ól,Hc ‚àà Rc√ól, and Hp ‚àà
Rp√ól are the hidden states of the BiLSTMs, ‚Äò;‚Äô is
feature-wise concatenation, and l is the size of the
hidden states.

3.1.4 Fusion Layer
We further convert each question and answer
choice into a single vector: q ‚àà Rl and c ‚àà Rl:

Œ±q = softmax([H
q;we] ¬∑w>sq), q = Hq

>Œ±q

Œ±c = softmax(H
c ¬∑w>sc), c = Hc

>Œ±c,
(3)

where the essential-term feature we from Sec-
tion 3.1.1 is concatenated with Hq, and wsq and
wsc are learned parameters.

Finally, a bilinear sequence matching is calcu-
lated between Hp and q to obtain a question-aware
passage representation, which is used as the final
passage representation:

Œ±p = softmax(H
p ¬∑ q); p = Hp>Œ±p. (4)

3.1.5 Choice Interaction
When a QA task provides multiple choices for se-
lection, the relationship between the choices can
provide useful information to answer the question.
Therefore, we integrate a choice interaction layer
to handle the semantic correlation between multi-
ple answer choices. Given the hidden state Hcn of
choice cn and Hci of other choices ci, ‚àÄi 6= n, we
calculate the differences between the hidden states
and apply max-pooling over the differences:

cinter = Maxpool(H
cn ‚àí 1

N ‚àí 1
‚àë
i 6=n

Hci), (5)

where N is the total number of answer choices.
Here, cinter characterizes the differences between

an answer choice cn and other answer choices.
The final representation of an answer choice is up-
dated by concatenating the self-attentive answer
choice vector and inter-choice representation as
cfinal = [c; cinter ].

3.1.6 Output Layer
For each tuple {q,pn, cn}Nn=1, two scores are cal-
culated by matching (1) the passage and answer
choice and (2) question and answer choice. We
use a bilinear form for both matchings. Finally, a
softmax function is applied over N answer choices
to determine the best answer choice:

spcn = pnW
pccfinaln ; s

qc
n = qW

qccfinaln

s = softmax(spc) + softmax(sqc),
(6)

where spcn , s
qc
n are the scores for answer choice

1 ‚â§ n ‚â§ N ; spc, sqc are score vectors for all N
choices; and s contains the final scores for each
answer choice. During training, we use a cross-
entropy loss.

3.2 Essential Term Selector
Essential terms are key words in a question that
are crucial in helping a retriever obtain related ev-
idence. Given a question Q and N answer choices
C1, . . . ,CN , the goal is to predict a binary vari-
able yi for each word Qi in the question Q, where
yi = 1 if Qi is an essential term and 0 otherwise.
To address this problem, we build a neural model,
ET-Net, which has the same design as the reader
model for the input layer, attention layer, and se-
quence modeling layer to obtain the hidden state
Hq for question Q.

In detail, we take the question Q and the con-
catenation C of all N answer choices as input to



340

Question If an object is attracted to a magnet, the
object is most likely made of (A) wood (B)
plastic (C) cardboard (D) metal

# annotators 5

Annotation If,0; an,0; object,3; is,0; attracted,5;
to,0; a,0; magnet,,5; the,0; object,1; is,0;
most,0; likely,0; made,2; of,0

Table 2: Example of essential term data.

Model Precision Recall F1

MaxPMI 0.88 0.65 0.75
SumPMI 0.88 0.65 0.75
PropSurf 0.68 0.64 0.66
PropLem 0.76 0.64 0.69
ET Classifier 0.91 0.71 0.80
ET-Net 0.74 0.90 0.81

Table 3: Performance of different selectors.

ET-Net. Q and C first go through an input layer
to convert to the embedded word representation,
and then word-level attention is calculated to ob-
tain a choice-aware question representation WCQ
as in Equation (1). We concatenate the word repre-
sentation and word-level attention representation
of the question and feed it into the sequence mod-
eling layer:

Hq = BiLSTM[WQ;W
C
Q]. (7)

As shown in Figure 2, the hidden states obtained
from the attention layer are then concatenated with
the embedded representations of Q and fed into
a projection layer to obtain the prediction vector
y ‚àà Rq for all words in the question:

y = [Hq;WfQ] ¬∑w
s, (8)

where ws contains the learned parameters, and
WfQ is the concatenation of the POS embedding,
NER embedding, relation embedding, and feature
embedding from Section 3.1.1.

After obtaining the prediction for each word, we
use a binary cross-entropy loss to train the model.
During evaluation, we take words with yi greater
than 0.5 as essential terms.

4 Experiments

In this section, we first discuss the performance
of the essential term selector, ET-Net, on a public
dataset. We then discuss the performance of the
whole retriever-reader pipeline, ET-RR, on mul-
tiple open-domain datasets.For both the ET-Net
and ET-RR models, we use 96-dimensional hid-
den states and 1-layer BiLSTMs in the sequence
modeling layer. A dropout rate of 0.4 is applied
for the embedding layer and the BiLSTMs‚Äô out-
put layer. We use adamax (Kingma and Ba, 2014)
with a learning rate of 0.02 and batch size of
32. The model is trained for 100 epochs. Our
code is released at https://github.com/
nijianmo/arc-etrr-code.

4.1 Performance on Essential Term Selection
We use the public dataset from Khashabi et al.
(2017) which contains 2,223 annotated questions,
each accompanied by four answer choices. Ta-
ble 2 gives an example of an annotated question.
As shown, the dataset is annotated for binary clas-
sification. For each word in the question, the
data measures whether the word is an ‚Äúessential‚Äù
term according to 5 annotators. We then split the
dataset into training, development, and test sets us-
ing an 8:1:1 ratio and select the model that per-
forms best on the development set.

Table 3 shows the performance of our essential
term selector and baseline models from Khashabi
et al. (2017). The second best model (ET Classi-
fier) is an SVM-based model from Khashabi et al.
(2017) requiring over 100 handcrafted features.
As shown, our ET-Net achieves a comparable re-
sult with ET Classifier in terms of the F1 Score.

Table 4 shows example predictions made by ET-
Net. As shown, ET-Net is capable of selecting
most ground-truth essential terms. It rejects cer-
tain words such as ‚Äúorganisms‚Äù which have a high
TF-IDF in the corpus but are not relevant to an-
swering a particular question. This shows its abil-
ity to discover essential terms according to the
context of the question.

4.2 Performance on Open-domain
Multiple-choice QA

We train and evaluate our proposed pipeline
method ET-RR on four open-domain multiple-
choice QA datasets. All datasets are associated
with a sentence-level corpus. Detailed statistics
are shown in Table 5.

‚Ä¢ ARC (Clark et al., 2018): We consider the
‚ÄòChallenge‚Äô set in the ARC dataset and use
the provided corpus during retrieval.

‚Ä¢ RACE-Open: We adapted the RACE dataset
(Lai et al., 2017) to the open-domain setting.
Originally, each question in RACE comes



341

Example questions

Which unit of measurement can be used to describe the
length of a desk ?
One way animal usually respond to a sudden drop in
temperature is by
Organisms require energy to survive. Which of the follow-
ing processes provides energy to the body ?

Table 4: Examples of essential term prediction (in ques-
tions) by ET-Net. True positives are marked bold and
underlined while false positives are only underlined.

Dataset Train Dev Test Corpus

ARC 1,119 299 1,172 1.46M
RACE-Open 9,531 473 528 0.52M
MCScript-Open 1,036 156 319 24.2K
Amazon-Patio 36,587 4,531 4,515 2.55M
Amazon-Auto 49,643 6,205 6,206 7.32M
Amazon-Cell 40,842 5,105 5,106 1.86M

Table 5: Statistics on ARC, RACE-Open,
MCScript-Open and Amazon-QA. Corpus size is
the number of sentences.

Dataset Example questions

ARC
The best way to separate salt from water is with the use of
Which geologic process most likely caused the formation of the Mount St. Helens Volcano?

RACE-Open
According to the article, what does the band Four Square hope to do in the future?
According to the article we know it is to prevent the forests from slowly disappearing.

Amazon-QA
For anyone with small ears, do these fit comfortably or do they feel like they are always going to fall out,
not in correctly, etc.
Does it remove easily and does it leave any sticky residue behind? thanks in advance.

Table 6: Example of predictions on ARC, RACE-Open and Amazon-QA. Predicted terms are underlined.

with a specific passage. To enable passage
retrieval, we concatenate all passages into a
corpus with sentence deduplication.3

‚Ä¢ MCScript-Open: The MCScript (Ostermann
et al., 2018) dataset is also adapted to the
open-domain setting. Again we concatenate
all passages to build the corpus.4

‚Ä¢ Amazon-QA: The Amazon-QA dataset
(McAuley and Yang, 2016) is an open-
domain QA dataset covering over one
million questions across multiple product
categories. Each question is associated
with a free-form answer. We adapt it into a
2-way multiple-choice setting by randomly
sampling an answer from other questions as
an answer distractor. We split all product
reviews at the sentence-level to build the
corpus. We consider three categories from
the complete dataset in our experiments.

In the experiments, ET-RR uses ET-Net to
choose essential terms in the question. Ta-
ble 6 shows example predictions on these target
datasets. Then it generates a query for each of
the N answer choices by concatenating essential

3 As short questions might not contain any words which
can relate the question to any specific passage or sentence,
we only keep questions with more than 15 words.

4We keep questions with more than 10 words rather than
15 words to ensure that there is sufficient data.

terms and the answer choice. For each query, ET-
RR obtains the top K sentences returned by the re-
triever and considers these sentences as a passage
for the reader. We set K = 10 for all experiments.

We compare ET-RR with existing retrieve-and-
read methods on both datasets. As shown in Ta-
ble 7, on the ARC dataset, ET-RR outperforms all
previous models without using pre-trained mod-
els and achieves a relative 8.1% improvement over
the second best BiLSTM Max-out method (Mi-
haylov et al., 2018). Recently, finetuning on pre-
trained models has shown great improvement over
a wide range of NLP tasks. Sun et al. (2019) pro-
posed a ‚ÄòReading Strategies‚Äô method to finetune
the pre-trained model OpenAI GPT, a language
model trained on the BookCorpus dataset (Rad-
ford, 2018). They trained Reading Strategies on
the RACE dataset to obtain more auxiliary knowl-
edge and then finetune that model on the ARC cor-
pus. Table 8 demonstrates the performance com-
parison of ET-RR and Reading Strategies on ARC.
As shown, though Reading Strategies trained on
both ARC and RACE dataset outperforms ET-RR,
ET-RR outperforms Reading Strategies using only
the ARC dataset at training time.

On the RACE-Open and MCScript-Open
datasets, ET-RR achieves a relative improvement
of 24.6% and 10.5% on the test set compared
with the second best method IR solver respec-
tively. We also evaluate on multiple categories of



342

Model ARC RACE-Open MCScript-Open
Test Test Test

IR solver 20.26 30.70 60.46
Random 25.02 25.01 50.02
BiDAF 26.54 26.89 50.81
BiLSTM Max-out 33.87 / /
ET-RR (Concat) 35.33 36.87 66.46
ET-RR 36.61 38.61 67.71

Table 7: Accuracy on multiple-choice selection on ARC,
RACE-Open and MCScript-Open.

Training Corpus model ARC

ARC
Reading Strategies 35.0
ET-RR 36.6

ARC+RACE Reading Strategies 40.7

Table 8: Comparisons of ET-RR and Reading
Strategies on ARC.

Model Amazon Amazon Aamzon
-Patio -Auto -Cell

IR solver 72.80 73.60 70.50
Moqa 84.80 86.30 88.60
ET-RR (Concat) 96.19 95.21 93.26
ET-RR 96.61 95.96 93.81

Table 9: Accuracy on multiple-choice selection on
three product categoris of Amazon-QA.

Pre-trained model RACE

3 Reading Strategies 63.8
3 OpenAI GPT 59.0

ET-RR (reader) 52.3
Bi-attn (MRU) 50.4

Hier. Co-Matching 50.4

Table 10: Experimental results for reader on RACE.

the Amazon-QA dataset. As shown in Table 9,
ET-RR increases the accuracy by 10.33% on aver-
age compared to the state-of-the-art model Moqa
(McAuley and Yang, 2016). We also compare ET-
RR with ET-RR (Concat), which is a variant of
our proposed model that concatenates the question
and choice as a query for each choice. Among
all datasets, ET-RR outperforms ET-RR (concat)
consistently which shows the effectiveness of our
essential-term-aware retriever.

4.3 Ablation study

We investigate how each component contributes to
model performance.

Performance of reader. Our reader alone can
be applied on MRC tasks using the given pas-
sages. Here, we evaluate our reader on the orig-
inal RACE dataset to compare with other MRC
models as shown in Table 10. As shown, the re-
cently proposed Reading Strategies and OpenAI
GPT models, that finetune generative pre-trained
models achieve the highest scores. Among non-
pre-trained models, our reader outperforms other

Model Test

ET-RR 36.61
‚Äì inter-choice 36.36
‚Äì passage-choice 35.41
‚Äì question-choice 34.47
‚Äì passage-question 34.05

Table 11: Ablation test on attention components of ET-
RR on ARC. ‚Äò‚Äì‚Äô denotes the ablated feature.

baselines: Bi-attn (MRU) (Tay et al., 2018) and
Hierarchical Co-Matching (Wang et al., 2018a) by
a relative improvement of 3.8%.

Attention components. Table 11 demonstrates
how the attention components contribute to the
performance of ET-RR. As shown, ET-RR with
all attention components performs the best on the
ARC test set. The performance of ET-RR without
passage-question attention drops the most signifi-
cantly out of all the components. It is worth not-
ing that the choice interaction layer gives a further
0.24% boost on test accuracy.

Essential term selection. To understand the
contribution of our essential-term selector, we
compare ET-RR with two variants: (1) ET-RR
(Concat) and (2) ET-RR (TF-IDF). For ET-RR
(TF-IDF), we calculate the TF-IDF scores and
take words with the top 30% of TF-IDF scores
in the question to concatenate with each answer
choice as a query.5

Table 12 shows an ablation study comparing
different query formulation methods and amounts
of retrieved evidence K. As shown, with the es-
sential term selector ET-Net, the model consis-
tently outperforms other baselines, given different
numbers of retrievals K. Performance for all mod-
els is best when K = 10. Furthermore, only using
TF-IDF to select essential terms in a question is
not effective. When K = 10, the ET-RR (TF-IDF)

5According to the annotated dataset, around 30% of the
terms in each question are labelled as essential.



343

Model
ET-RR ET-RR

ET-RR
(Concat) (TF-IDF)

Top K Dev Test Dev Test Dev Test

5 39.26 33.36 39.93 34.73 39.93 35.59
10 38.93 35.33 39.43 35.24 43.96 36.61
20 41.28 34.56 38.59 33.88 42.28 35.67

Table 12: Comparison of query formulation methods
and amounts of retrieved evidence (i.e., top K) on the
ARC dataset, in terms of percentage accuracy.

method performs even worse than ET-RR (Con-
cat). This illustrates the challenges in understand-
ing what is essential in a question.

Though ET-RR consistently outperforms ET-
RR (TF-IDF), the improvement is relatively mod-
est on the Test set (around 1.4%). A similar out-
come has been reported in Jansen et al. (2017);
Khashabi et al. (2017) where essential term ex-
traction methods have shown around 2%-4% gain
compared with TF-IDF models and struggle to ob-
tain further improvement on SQA tasks. This con-
sensus might show the discrepancy of essential
terms between human and machine (i.e., the es-
sential terms obtained using a human annotated
dataset might not be helpful in a machine infer-
ence model). Another reason might be the current
retrieval method does not effectively use these es-
sential terms and the performance highly depends
on the dataset. Note that the ET-RR outperforms
ET-RR (TF-IDF) by around 4% on the Dev set.
Therefore, how to develop well-formed single or
even multi-hop queries using these terms are worth
studying in the future.

4.4 Error Analysis

Table 13 shows two major types of error, where the
correct answer choice is in bold and the predicted
answer choice is in italics.
Retrieved supporting evidence but failed to rea-
son over it. For the first question, there exists ev-
idence that can justify the answer candidate (C).
However, the model chooses (D) which has more
words overlapping with its evidence. This shows
that the model still lacks the reasoning capability
to solve complex questions.
Failed to retrieve supporting evidence. For the
second question, the retrieved evidence of both
the correct answer (D) and the prediction (B) is
not helpful to solve the question. Queries such as
‚Äòwhat determines the year of a planet‚Äô are needed
to acquire the knowledge for solving this question.

The elements carbon, hydrogen, and oxygen are parts of
many different compounds. Which explains why these
three elements can make so many different compounds?
(A) They can be solids, liquids, or gases.
(B) They come in different sizes and shapes.
(C) They combine in different numbers and ratios.
* There are many different types of compounds because
atoms of elements combine in many different ways (and
in different whole number ratios) to form different com-
pounds.
(D) They can be a proton, a neutron, or an electron.
* Atoms of different elements have a different number of
protons, neutrons, and electrons.

Which planet in the solar system has the longest year?
(A) The planet closest to the Sun.
(B) The planet with the longest day.
* The planet with the longest day is Venus; a day on Venus
takes 243 Earth days.
(C) The planet with the most moons.
(D) The planet farthest from the Sun.
* The last planet discovered in our solar system is farthest
away from the sun.

Table 13: Examples where ET-RR fails on ARC. The
retrieved evidence for each answer candidate is marked
by *.

This poses further challenges to design a retriever
that can rewrite such queries.

5 Conclusion

We present a new retriever-reader model (ET-RR)
for open-domain QA. Our pipeline has the follow-
ing contributions: (1) we built an essential term se-
lector (ET-Net) which helps the model understand
which words are essential in a question leading
to more effective search queries when retrieving
related evidence; (2) we developed an attention-
enhanced reader with attention and fusion among
passages, questions, and candidate answers. Ex-
perimental results show that ET-RR outperforms
existing QA models on open-domain multiple-
choice datasets as ARC Challenge, RACE-Open,
MCScript-Open and Amazon-QA. We also per-
form in-depth error analysis to show the limita-
tions of current work. For future work, we plan
to explore the directions of (1) constructing multi-
hop query and (2) developing end-to-end retriever-
reader model via reinforcement learning.

Acknowledgments

We thank Jade Huang for proofreading the paper,
Liang Wang and Daniel Khashabi for sharing code
and the annotated dataset with us. We thank all the
reviewers for their constructive suggestions.



344

References
Michael Boratko, Harshit Padigela, Divyendra Mikki-

lineni, Pritish Yuvraj, Rajarshi Das, Andrew Mc-
Callum, Maria Chang, Achille Fokoue, Pavan Ka-
panipathi, Nicholas Mattei, Ryan Musa, Kartik Ta-
lamadupula, and Michael Witbrock. 2018. A sys-
tematic classification of knowledge, reasoning, and
context within the arc dataset. In 1st Workshop on
Machine Reading for Question Answering.

Christian Buck, Jannis Bulian, Massimiliano Cia-
ramita, Andrea Gesmundo, Neil Houlsby, Wojciech
Gajewski, and Wei Wang. 2018. Ask the right ques-
tions: Active question reformulation with reinforce-
ment learning. In ICLR.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In ACL.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question
answering? try arc, the ai2 reasoning challenge.
CoRR, abs/1803.05457.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
and Andrew McCallum. 2019. Multi-step retriever-
reader interaction for scalable open-domain question
answering. In ICLR.

Peter Jansen, Rebecca Sharp, Mihai Surdeanu, and Pe-
ter Clark. 2017. Framing qa as building and ranking
intersentence answer justifications. Computational
Linguistics, 43:407‚Äì449.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and
Luke S. Zettlemoyer. 2017. Triviaqa: A large scale
distantly supervised challenge dataset for reading
comprehension. In ACL.

Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and
Dan Roth. 2017. Learning what is essential in ques-
tions. In CoNLL.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Heeyoung Kwon, Harsh Trivedi, Peter Jansen, Mi-
hai Surdeanu, and Niranjan Balasubramanian. 2018.
Controlling information aggregation for complex
question answering. In ECIR.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard H. Hovy. 2017. Race: Large-scale read-
ing comprehension dataset from examinations. In
EMNLP.

Victor Lavrenko and W. Bruce Croft. 2001. Relevance-
based language models. SIGIR Forum, 51:260‚Äì267.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
customer reviews. In WWW.

Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In EMNLP.

Sewon Min, Victor Zhong, Richard Socher, and Caim-
ing Xiong. 2018. Efficient and robust question an-
swering from minimal context over documents. In
ACL.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. CoRR, abs/1611.09268.

Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako
Asano, and Junji Tomita. 2018. Retrieve-and-
read: Multi-task learning of information retrieval
and reading comprehension. In CIKM.

Rodrigo Nogueira and Kyunghyun Cho. 2017. Task-
oriented query reformulation with reinforcement
learning. In EMNLP.

Simon Ostermann, Michael Roth, Ashutosh Modi, Ste-
fan Thater, and Manfred Pinkal. 2018. Semeval-
2018 task 11: Machine comprehension using com-
monsense knowledge. In SemEval@NAACL-HLT.

Alec Radford. 2018. Improving language understand-
ing by generative pre-training. In Technical report.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In EMNLP.

Robert Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In AAAI.

Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2019.
Improving machine reading comprehension with
general reading strategies. In NAACL.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Multi-range reasoning for machine comprehension.
CoRR, abs/1803.09074.

Liang Wang. 2018. Yuanfudao at semeval-2018 task
11: Three-way attention and relational knowledge
for commonsense machine comprehension. In
SemEval@NAACL-HLT.

Shuohang Wang, Mo Yu, Shiyu Chang, and Jing Jiang.
2018a. A co-matching model for multi-choice read-
ing comprehension. In ACL.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerald
Tesauro, Bowen Zhou, and Jing Jiang. 2018b. R3:
Reinforced ranker-reader for open-domain question
answering. In AAAI.


