










































Predicting Change in Student Motivation by Measuring Cohesion between Tutor and Student


Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 136–141,
Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics

Predicting Change in Student Motivation by Measuring Cohesion between
Tutor and Student

Arthur Ward
Department of Biomedical

Informatics
University of Pittsburgh
Pittsburgh, Pa., 15232
akw13@pitt.edu

Diane Litman
Department of Computer

Science, LRDC
University of Pittsburgh
Pittsburgh, Pa., 15260

litman@cs.pitt.edu

Maxine Eskenazi
Language Technologies

Institute
Carnegie Mellon University

Pittsburgh, Pa., 15213
max@cmu.edu

Abstract

We apply a previously reported measure of di-
alog cohesion to a corpus of spoken tutoring
dialogs in which motivation was measured.
We find that cohesion significantly predicts
changes in student motivation, as measured
with a modified MSLQ instrument. This sug-
gests that non-intrusive dialog measures can
be used to measure motivation during tutoring.

1 Introduction

Motivation is widely believed to be an important fac-
tor in learning, and many studies have found rela-
tionships between motivation and educational out-
comes. For example Pintrich and DeGroot (1990)
found that students’ motivational state was a signif-
icant predictor of classroom performance. In ad-
dition, pedagogically significant behaviors such as
dictionary lookup in the REAP (Brown and Eske-
nazi, 2004) vocabulary tutor have been shown to
be positively correlated with motivation assessments
(DelaRosa and Eskenazi, 2011). Also, in a separate
study with the REAP tutor, attempts to manipulate
reading motivation by presenting more interesting
stories were shown to improve vocabulary learning
(Heilman et al., 2010).

In addition to influencing learning outcomes, mo-
tivational state may also affect which interventions
will be effective during tutoring. For example, Ward
and Litman (2011) have shown that motivation can
significantly affect which students benefit from a re-
flective reading following interactive tutoring with a
the Itspoke (Litman and Silliman, 2004) tutor.

An accurate way to measure student motivation
during tutoring could therefore be valuable to Intel-
ligent Tutoring System (ITS) researchers. Several
self-report instruments have been developed which
measure various aspects of motivation (e.g. (Pintrich
and DeGroot, 1990; McKenna and Kear, 1990)).
However, these instruments are too intrusive to be
administered during tutoring, for fear of fatally dis-
rupting learning. We would prefer a non-intrusive
measure which would allow an ITS to detect when
student motivation is decreasing so as to launch a
motivational intervention. Similarly, the ITS should
be able to detect when motivation is increasing
again, to determine if the intervention worked. As
mentioned above, such a measure might also allow
an ITS to determine when it would be effective to
use certain instructional tactics.

In this work we investigate cohesion as a non-
intrusive measure of motivation for natural language
dialog based ITS. As defined more precisely below,
our measure of cohesion quantifies lexical and se-
mantic similarity between tutor and student dialog
utterances. We hypothesize that this measure of lexi-
cal similarity may be related to motivation in part be-
cause other measures of dialog similarity have been
shown to be related to task success. For example,
there is evidence that perceived similarity between a
student’s own speech rate and that of a recorded task
request increases the student’s feelings of immedi-
acy, which are in turn linked to greater compliance
with the request to perform a task (Buller and Aune,
1992). 1 In addition, Ward and Litman (2006; 2008)
investigated a measure of lexical similarity between

1In this experiment, the task was to watch a series of videos.

136



the tutor and student partners in a tutoring dialog
which was shown to be correlated with task success
in several corpora of tutorial dialogs.

Measures of cohesion have also been used in a va-
riety of NLP tasks such as measuring text readability
(e.g. (Pitler and Nenkova, 2008)), measuring stylis-
tic differences in text (Mccarthy et al., 2006), and
for topic segmentation in tutorial dialog (Olney and
Cai, 2005).

Given the previously mentioned results relating
motivation to educational task success, these links
between task success and cohesion lead us to hy-
pothesize a direct correlation between motivation
and cohesion when using the Itspoke tutor.

We will first briefly describe the Itspoke tutor, and
the corpus of tutoring dialogs used in this study. We
will then describe the instrument we used to mea-
sure motivation both before and immediately after
tutoring, then we will describe the algorithm used
to measure cohesion in the tutoring dialogs. Finally,
we show results of correlations between the measure
of motivation and the measure of cohesion. We will
find that the change in motivation is significantly
correlated with dialog cohesion.

2 Itspoke System and Corpus

Itspoke (Intelligent Tutoring SPOKEn dialog sys-
tem) is a spoken dialog tutoring system which
teaches qualitative physics. It provides a spoken di-
alog interface to the Why2-Atlas (VanLehn et al.,
2002) tutor, and has recently been re-implemented
using the TuTalk (Jordan et al., 2007) dialog plat-
form. The Itspoke tutor presents a problem in qual-
itative physics, and asks the student an initial ques-
tion. The student answers the question, and the di-
alog continues until all points have been covered to
the tutor’s satisfaction.

The corpus used in the current work was collected
in a previous study (Ward and Litman, 2011), us-
ing novice subjects who had never taken a college
physics course. Before tutoring, students were given
a motivation survey which will be described in Sec-
tion 3. They then engaged Itspoke in five tutoring
dialogs as described above. Immediately after tu-
toring they were given the motivation questionnaire
again, with tenses changed as appropriate.

166 subjects were recruited by flyer, by advertise-

Speaker Utterance
Tutor To see which vehicle’s change in motion

is greater, we use the definition of accel-
eration. What is the definition of accel-
eration?

Student Change in velocity.
Tutor Excellent. Acceleration is defined as the

amount velocity changes per unit time.

Table 1: Dialog turns, with Token, Stem, and Semantic
Similarity Matches in bold (as discussed in Section 4).

ment during an undergraduate psychology course, or
from the University of Pittsburgh’s psychology sub-
ject pool. Of these, 40 were dismissed after pretest
as “middle third” knowledge students, following ex-
treme groups design (Feldt, 1961). Extreme groups
design was adopted to increase the power of a “high”
vs “low” knowledge comparison, which is reported
elsewhere (Ward, 2010). Another 27 students were
not used for various reasons including incomplete
data. This left a corpus of 99 subjects who each par-
ticipated in 5 tutorial dialogs.

Table 1 shows an exchange from one of these di-
alogs. The tutor asks a question about the current
problem, which the student then answers. The tutor
restates the answer, and (later in the dialog) proceeds
on to the next point of discussion.

3 Motivation Measure

In this study we measure motivation using a reduced
version of the “Motivated Strategies for Learning
Questionnaire (MSLQ)” developed by Pintrich and
DeGroot (1990). This version of the MSLQ is also
based on work by Roll (2009), who adapted it for
use in an IPL (Invention as Preparation for Learning
(Schwartz and Martin, 2004)) tutoring environment.
Our motivational survey is shown in Figure 1.

Questions one and two address “self-regulation,”
particularly the students’ tendency to manage and
control their own effort. Question one is on a re-
versed scale relative to the other questions, so re-
sponses to it were inverted. Question three addresses
“self-efficacy,” the students’ expectation of success
on the task. Questions four and five address “intrin-
sic value,” the students’ beliefs about the importance
and interest of the task.

These dimensions of motivation are theoretically

137



Please read the following statements and then
click a number on the scale that best matches
how true it is of you. 1 means “not at all true
of me” whereas 7 means “very true of me”.

1. I think that when the tutor is talking
I will be thinking of other things and
won’t really listen to what is being said.

2. If I could take as much time as I want,
I would spend a lot of time on physics
tutoring sessions.

3. I think I am going to find the physics tu-
tor activities difficult.

4. I think I will be able to use what I learn
in the physics tutor sessions in my other
classes.

5. I think that what I will learn in the
physics tutor sessions is useful for me to
know.

Figure 1: Pre-tutoring Motivational Survey

distinct. However, except for question three (the
self-efficacy question), responses to these questions
were all very significantly correlated with each other
in our survey (p < .01).

Table 2 shows values of Cronbach’s Alpha (Cron-
bach, 1951) for various subsets of the motivation
questions. Alpha measures the internal consistency
of responses to a multi-point questionnaire, and is a
function of the number of test items and the corre-
lation between them. Higher values are thought to
indicate that the various test items are measuring the
same underlying latent construct. For this study we
omit Question 3, maximizing Alpha at .716. This
is just above the commonly accepted (Gliem and
Gliem, 2003) threshold for reliability in such an in-
strument.

Questions Alpha
1, 2, 3, 4, 5 0.531

1, 2, 4, 5 0.716
2, 4, 5 0.703

4, 5 0.683

Table 2: Alpha reliability
scores for various subsets
of questions.

As mentioned above,
this instrument was ad-
ministered both before
and (with suitable tense
changes) immediately
after tutoring. We will
report correlations using
mean scores on the
pre- and post-tutoring
measures, as well as
for the change-in-motivation score, calculated as
post-minus-pre.

4 Semantic Cohesion Measure

In this work we measure cohesion between tutor and
student using the “semantic cohesion” measure first
reported by Ward and Litman (2008). This measure
counts the number of “cohesive ties” (Halliday and
Hasan, 1976) between adjacent tutor and student di-
alog turns. A cohesive tie can be the repetition of an
exact word or word stem, or the use of two words
with similar meanings in adjacent turns. Stop words
are excluded, and cohesive ties are counted in both
the student-to-tutor and the tutor-to-student direc-
tions. For example, in the dialog shown in Table 1,
the final tutor turn repeats the word “velocity” from
the previous student turn. This repetition would be
counted as an exact cohesive tie. Similarly, the tu-
tor uses the word “changes” following the student’s
use of “change.” This would be counted as a stem
repetition cohesive tie.

Finally, the student’s use of “velocity” will be
counted as a cohesive tie because of its semantic
similarity to “acceleration,” from the preceding turn.
The algorithm therefore counts four ties in Table 1.
As described more completely in (Ward and Litman,
2008), semantic similarity cohesive ties are counted
by measuring two words’ proximity in the Word-
Net (Miller et al., 1990) hierarchy. We use a simple
path distance similarity measure, as implemented in
NLTK (Loper and Bird, 2002). This measure counts
the number of edges N in the shortest path between
two words in WordNet, and calculates similarity as 1
/ (1 + N). Our implementation of this semantic simi-
larity measure allows setting a threshold θ, such that
only word pairs with stronger-than-threshold simi-
larity are counted. Table 3 shows some semantic
similarity pairs counted with a threshold of 0.3.

motion-contact
man-person

decrease-acceleration
acceleration-change

travel-flying

Table 3: Example Se-
mantic ties: θ = 0.3

We obtain a normal-
ized cohesion score for
each dialog by dividing
the tie count by the num-
ber of turns in the dialog.
We then sum the line nor-
malized counts over all
the dialogs for each stu-
dent, resulting in a per-
student cohesion measure.

138



5 Results

We ran correlations between the change-in-
motivation score described in Section 3 and the
semantic similarity measure of cohesion described
in Section 4. We report results for a semantic
similarity threshold of .3 for consistency with
(Ward and Litman, 2008), however the pattern of
results is not sensitive to this threshold. Significant
results were obtained for all thresholds between .2
and .5, in .1 increments. 2 In addition, we report
results for the motivation measure with the third
question removed for consistency with (Ward and
Litman, 2011). However the pattern of results is not
sensitive to this exclusion, either. Significant results
were also obtained using the entire questionnaire.

Motivation
Measure Cor. pValue

pre-Tutoring 0.02 0.86
Change 0.21 0.03

post-Tutoring 0.19 0.055

Table 4: Cohesion - Motivation
Correlations. N = 99. θ = 0.3

In all cases,
the change in
motivation was
found to be
significantly
and positively
correlated with
the cohesive-
ness of the
tutoring dialog. More lexical similarity between
tutor and student was predictive of increased student
motivation. As shown in the middle row of Table
4, the correlation with motivational change, using a
threshold of .3 and the reduced motivation measure
was r(97)= .21, p = 0.03.

Interestingly, as shown in the top and bottom rows
of Table 4, neither motivation before tutoring r(97)
= .02, p=.86, nor after tutoring r(97) = .19, p = .055,
was significantly correlated with cohesion, although
the post-tutoring measure achieves a strong trend.

Pre- and post-tutoring mean motivation levels
were, however, significantly correlated with each
other (R(97) = .69, p < .0001). Mean motivation
levels also showed a non-significant improvement
from 4.31 before tutoring to 4.44 after tutoring.

6 Discussion and Future Work

We have brought forward evidence that cohesion in
tutorial dialog, as measured in this paper, is corre-
lated with changes in student motivation. This sug-

2Note from the path distance formula that thresholds be-
tween .5 and 1 are impossible

gests that dialog cohesion may be useful as a non-
intrusive measure of motivational fluctuations.

As discussed in Section 1, other researchers have
investigated various types of cohesion, and their re-
lationship to things such as task success and learn-
ing. In addition, work has been done investigating
the role of motivation in learning. However, we be-
lieve ours is the first work relating dialog cohesion
directly to user motivation.

The presence of a correlation between cohesion
and motivation leaves open the possibility that more
motivated students are experiencing greater task
success in the tutor, and so generating more cohe-
sive dialogs. 3 Note, however, that the very non-
significant correlation between pre-dialog motiva-
tion and dialog cohesion argues against this pos-
sibility. Instead, it seems that some process is
both creating dialog cohesion and improving student
motivation. The lack of significance in the post-
dialog/motivation correlation may be due to data
sparsity.

In future work, we hope to investigate other dia-
log features which may be even better predictors of
student motivation. As mentioned in Section 1, we
became interested in dialog similarity metrics partly
because of their association with task success. These
kinds of associations between task success and dia-
log have also been shown for dialog entrainment.

In this discussion we will use the term “entrain-
ment” for the phenomenon in which conversational
partners’ speech features become more similar to
each other at many levels, including word choice,
over the course of a dialog. 4 As mentioned above,
we use the term “cohesion” for overall similarity of
word choice between speakers in a dialog, perhaps
resulting from entrainment.

Users appear to entrain strongly with dialog sys-
tems. For example, Brennan (1996) has found that
users are likely to adopt the terms used by a WOZ
dialog system, and that this tendency is at least as
strong as with human dialog partners. Similarly, Par-
ent and Eskenazi (2010) showed that users of the
Let’s Go (Raux et al., 2005) spoken dialog system
quickly entrain to its lexical choices.

3We thank an anonymous reviewer for prompting this dis-
cussion.

4This definition conflates studies of priming, alignment,
convergence and accommodation.

139



As with measures of dialog similarity, dialog en-
trainment has been found to be related to satisfac-
tion and success in task oriented dialogs. For ex-
ample, Reitter and Moore (2007) found that lexi-
cal and syntactic repetition predicted task success
in the MapTask corpus. Similarly, Ward and Lit-
man (2007) found that lexical and acoustic-prosodic
entrainment are correlated with task success in the
Itspoke dialog system. Interestingly, in that work
entrainment was more strongly correlated with task
success than a measure of dialog cohesion similar
to the one used in the current paper. This raises the
question of whether such a measure of dialog en-
trainment might also be a better predictor of motiva-
tion than the current measure of cohesion. We hope
in future work to further investigate this possibility.

Finally, because we are interested in predicting
motivation during tutoring, our dialog metrics may
be improved by making them sensitive to the educa-
tional domain. For example, exploratory work with
our tutor has suggested that a measure of cohesion
which only counts cohesive ties between physics
terms is better correlated with certain measures of
learning than a measure which counts non-physics
terms. This suggests that measures of cohesion
or entrainment which recognize educational domain
words may also improve correlations with motiva-
tion.

Acknowledgments

This work was supported by the ONR (N00014-07-
1-0039), by the NSF (0631930 and 0914615), and
by the LRDC and ISP at the University of Pittsburgh.

References
Susan E. Brennan. 1996. Lexical entrainment in sponta-

neous dialog. In International Symposium on Spoken
Dialog, pages 41–44.

Jonathan Brown and Maxine Eskenazi. 2004. Re-
trieval of authentic documents for reader-specific lexi-
cal practice. In In Proceedings of InSTIL/ICALL Sym-
posium.

David Buller and R.Kelly Aune. 1992. The effects of
speech rate similarity on compliance: Application of
communication accommodation theory. Western Jour-
nal of Communication, 56:37–53.

Lee Cronbach. 1951. Coefficient alpha and the internal
structure of tests. Psychometrika, 16(3):297–334.

Kevin DelaRosa and Maxine Eskenazi. 2011. Self-
assessment of motivation: Explicit and implicit indi-
cators of l2 vocabulary learning. Proceedings 15th In-
ternational Conference on Artificial Intelligence Edu-
cation (AIED).

Leonard Feldt. 1961. The use of extreme groups to
test for the presence of a relationship. Psychometrika,
26(3):307–316.

Joesph Gliem and Rosemary Gliem. 2003. Calculating,
interpreting, and reporting cronbach’s alpha reliability
coefficient for likert-type scales. Midwest Research to
Practice in Adult, Continuing and Community Educa-
tion.

M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. English Language Series. Pearson Educa-
tion Limited.

Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, Maxine Eskenazi, Alan Juffs, and Lois Wil-
son. 2010. Personalization of reading passages im-
proves vocabulary acquisition. International Journal
of Artificial Intelligence in Education, 20:73–98, Jan-
uary.

Pamela Jordan, Brian Hall, Michael Ringenberg, Yui Cui,
and Carolyn Rosé. 2007. Tools for authoring a di-
alogue agent that participates in learning studies. In
Proc. of Artificial Intelligence in Ed., AIED, pages 43–
50.

D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-
gent tutoring spoken dialogue system. In Companion
Proc. of the Human Language Technology Conf: 4th
Meeting of the North American Chap. of the Assoc. for
Computational Linguistics.

Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia: Association for Computa-
tional Linguistics.

Philip M. Mccarthy, Gwyneth A. Lewis, David F. Dufty,
and Danielle S. Mcnamara. 2006. Analyzing writ-
ing styles with coh-metrix. In In Proceedings of the
Florida Artificial Intelligence Research Society Inter-
national Conference (FLAIRS.

M.C. McKenna and D.J. Kear. 1990. Measuring attitude
toward reading: A new tool for teachers. The Reading
Teacher, 43(8):626–639.

George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: An on-line lexical database.
International Journal of Lexicography (special issue),
3 (4):235–312.

Andrew Olney and Zhiqiang Cai. 2005. An orthonormal
basis for topic segmentation in tutorial dialog. In Pro-

140



ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 971–978.
Vancouver, October.

Gabriel Parent and Maxine Eskenazi. 2010. Lexical en-
trainment of real users in the let’s go spoken dialog
system. In Proceedings Interspeech-2010, pages 3018
– 3021, Makuhari, Chiba, Japan.

Paul Pintrich and Elisabeth DeGroot. 1990. Motivational
and self-regulated learning components of classroom
academic performance. Journal of Educational Psy-
chology, 82(1):33–40.

Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP), pages 186 – 195.

Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Lets go public!
taking a spoken dialog system to the real world. In
Proceedings Interspeech-2005, pages 885 – 888, Lis-
bon, Portugal.

David Reitter and Johanna D. Moore. 2007. Predict-
ing success in dialogue. In In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 808 – 815, Prague, Czech Re-
public.

Ido Roll. 2009. Structured Invention Tasks to Pre-
pare Students for Future Learning: Means, Mecha-
nisms, and Cognitive Processes. Doctor of philoso-
phy, Carnegie Mellon University, 5000 Forbes Ave.
Pittsburgh, Pa.

Daniel Schwartz and Taylor Martin. 2004. Inventing
to prepare for future learning: The hidden efficiency
of encouraging original student production in statistics
instruction. Cognition and Instruction, 22:129 – 184.

K. VanLehn, P. Jordan, C. Rose, D. Bhembe, M. Boettner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, and Srivastava R. 2002.
The architecture of why2-atlas: A coach for qualita-
tive physics essay writing. In Proc. 6th Int. Conf. on
Intelligent Tutoring Systems, pages 158–167.

Arthur Ward and Diane Litman. 2006. Cohesion and
learning in a tutorial spoken dialog system. In Pro-
ceedings of the 19th International FLAIRS Conference
(FLAIRS-19), pages 533–538, May.

Arthur Ward and Diane Litman. 2007. Dialog con-
vergence and learning. In Proceedings 13th Interna-
tional Conference on Artificial Intelligence Education
(AIED), Los Angeles, Ca.

Arthur Ward and Diane Litman. 2008. Semantic
cohesion and learning. In Proceedings 9th Inter-
national Conference on Intelligent Tutoring Systems
(ITS), pages 459–469, Ann Arbor, June.

Arthur Ward and Diane Litman. 2011. Adding abstrac-
tive reflection to a tutorial dialog system. Proceedings
24th International FLAIRS (Florida Artificial Intelli-
gence Research Society) Conference.

Arthur Ward. 2010. Reflection and Learning Robust-
ness in a Natural Language Conceptual Physics Tutor-
ing System. Doctor of philosophy, University of Pitts-
burgh, Pittsburgh, PA. 15260.

141


