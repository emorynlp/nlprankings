



















































A Comparison of Neural Models for Word Ordering


Proceedings of The 10th International Natural Language Generation conference, pages 208–212,
Santiago de Compostela, Spain, September 4-7 2017. c©2017 Association for Computational Linguistics

A Comparison of Neural Models for Word Ordering

Eva Hasler1,2, Felix Stahlberg1, Marcus Tomalin1, Adrià de Gispert1,2, Bill Byrne1,2

1Department of Engineering, University of Cambridge, UK
2SDL Research, Cambridge, UK

{ech57,fs439,mt126,ad465,wjb31}@cam.ac.uk
{ehasler,agispert,bbyrne}@sdl.com

Abstract

We compare several language models for the
word-ordering task and propose a new bag-
to-sequence neural model based on attention-
based sequence-to-sequence models. We eval-
uate the model on a large German WMT data
set where it significantly outperforms existing
models. We also describe a novel search strat-
egy for LM-based word ordering and report
results on the English Penn Treebank. Our
best model setup outperforms prior work both
in terms of speed and quality.

1 Introduction

Finding the best permutation of a multi-set of words
is a non-trivial task due to linguistic aspects such
as “syntactic structure, selective restrictions, subcat-
egorization, and discourse considerations” (Elman,
1990). This makes the word-ordering task useful
for studying and comparing different kinds of mod-
els that produce text in tasks such as general natu-
ral language generation (Reiter and Dale, 1997), im-
age caption generation (Xu et al., 2015), or machine
translation (Bahdanau et al., 2015). Since plausi-
ble word order is an essential criterion of output flu-
ency for all of these tasks, progress on the word-
ordering problem is likely to have a positive impact
on these tasks as well. Word ordering has often been
addressed as syntactic linearization which is a strat-
egy that involves using syntactic structures or part-
of-speech and dependency labels (Zhang and Clark,
2011; Zhang et al., 2012; Zhang and Clark, 2015;
Liu et al., 2015; Puduppully et al., 2016). It has also
been addressed as LM-based linearization which re-
lies solely on language models and obtains better

Work partially supported by U.K. EPSRC grant EP/L027623/1.

scores (de Gispert et al., 2014; Schmaltz et al.,
2016). Recently, Schmaltz et al. (2016) showed that
recurrent neural network language models (Mikolov
et al., 2010, RNNLMs) with long short-term mem-
ory (Hochreiter and Schmidhuber, 1997, LSTM)
cells are very effective for word ordering even with-
out any explicit syntactic information.

We continue this line of work and make the fol-
lowing contributions. We compare several language
models on the word-ordering task and propose a
bag-to-sequence neural architecture that equips an
LSTM decoder with explicit context of the bag-of-
words (BOW) to be ordered. This model performs
particularly strongly on WMT data and is comple-
mentary to an RNNLM: combining both yields large
BLEU gains even for small beam sizes. We also
propose a novel search strategy which outperforms a
previous heuristic. Both techniques together surpass
prior work on the Penn Treebank at ∼4x the speed.

2 Bag-to-Sequence Modeling with
Attentional Neural Networks

Given the BOW {at, bottom, heap, now, of, the, the,
we, ’re, .}, a word-ordering model may generate an
output string w = “now we ’re at the bottom of the
heap .“. We can use an RNNLM (Mikolov et al.,
2010) to assign it a probability P (w) by decompos-
ing into conditionals:

P (wT1 ) =
T∏

t=1

P (wt|wt−11 ) (1)

Since we have access to the input BOWs, we extend
the model representation by providing the network
additionally with the BOW to be ordered, thereby al-
lowing it to focus explicitly on all tokens it generates

208



(a) (b)

Figure 1: (a) Attention-based seq2seq model and (b)
bag2seq model used in this work.

in the output during decoding. Thus, instead of mod-
eling the a priori distribution of sentences P (w) as
in Eq. 1, we condition the distribution on BOW(w):

P (wT1 |BOW(w)) =
T∏

t=1

P (wt|wt−11 , BOW(w))
(2)

This dependency is realized by the neural atten-
tion mechanism recently proposed by Bahdanau et
al. (2015). The resulting bag-to-sequence model
(bag2seq) is inspired by the attentional sequence-to-
sequence model RNNSEARCH (seq2seq) proposed
by Bahdanau et al. (2015) for neural machine trans-
lation between a source sentence x = xI1 and a tar-
get sentence y = yJ1 . Fig. 1a illustrates how seq2seq
generates the j-th target token yj using the decoder
state sj and the context vector cj . The context vec-
tor is the weighted sum of source side annotations
hi which encode sequence information.

To modify seq2seq for problems with unordered
input, we make the encoder architecture order-
invariant by replacing the recurrent layer with non-
recurrent transformations of the word embeddings,
as indicated by the missing arrows between source
positions in Fig. 1b. For convenience, we formalize
BOW(w) as sequence 〈w̃1, . . . , w̃T 〉 in which words
are sorted, e.g. alphabetically, so that we can refer
to the t-th word in the BOW. The model can be
trained to recover word order in a sentence by using
BOW(w) = 〈w̃1, . . . , w̃T 〉 as input and the original
sequence 〈w1, . . . , wT 〉 as target. This network ar-
chitecture does not prevent words outside the BOW
to appear in the output. Therefore, we explicitly
constrain our beam decoder by limiting its available
output vocabulary to the remaining tokens in the in-
put bag at each time step, thereby ensuring that all
model outputs are valid permutations of the input.

3 Search

Beam search is a popular decoding algorithm for
neural sequence models (Sutskever et al., 2014;
Bahdanau et al., 2015). However, standard beam
search suffers from search errors when applied to
word ordering and Schmaltz et al. (2016) reported
that gains often do not saturate even with a large
beam of 512. They suggested adding external un-
igram probabilities of the remaining words in the
BOW as future cost estimates to the beam-search
scoring function and reported large gains for an n-
gram LM and RNNLM. We re-implement this fu-
ture cost heuristic, f(·), and further propose a new
search heuristic, g(·), which collects internal uni-
gram statistics during decoding. We keep hypothe-
ses in the beam if their score is close to a theoretical
upper bound, the product of the best word proba-
bilities given any history within the explored search
space. For each word w̃ ∈ BOW(w) we maintain a
heuristic score estimate P̂ (w̃) which we initialize to
0. Each time the search algorithm visits a new con-
text, we update the estimates such that P̂ (w̃) is the
current best score for w̃:

P̂ (w̃) = max
c∈Ct

P (w̃|c, BOW(w)) (3)

where Ct is the set of contexts (i.e. ordered prefixes
in the form of wt1) explored by beam search so far.
Thus, instead of computing a future cost, we com-
pare the actual score of a partial hypothesis with the
product of heuristic estimates of its words. This is
especially useful for model combinations since all
models are taken into account. We also implement
hypothesis recombination to further reduce the num-
ber of search errors. More formally, at each time
step t our beam search keeps the n best hypothe-
ses according to scoring function S(·) using partial
model score s(·) and estimates g(·):

S(wt1) = s(w
t
1)− g(wt1)

s(wt1) = log P (w
t
1|BOW(w))

g(wt1) =
∑

w′∈wt1
log P̂ (w′)

(4)

4 Experimental Setup

We evaluate using data from the English-German
news translation task (Bojar et al., 2015, WMT) and
using the English Penn Treebank data (Marcus et
al., 1993, PTB). Since additional knowledge sources

209



are often available in practice, such as access to the
source sentence in a translation scenario, we also re-
port on bilingual experiments for the WMT task.

4.1 Data and evaluation
The WMT parallel training data includes Europarl
v7, Common Crawl, and News Commentary v10.
We use news-test2013 for tuning model combina-
tions and news-test2015 for testing. All monolin-
gual models for the WMT task were trained on
the German news2015 corpus (∼51.3M sentences).
For PTB, we use preprocessed data by Schmaltz et
al. (2016) for a fair comparison (∼40k sentences
for training).1 We evaluate using the multi-bleu.perl
script for WMT and mteval-v13.pl for PTB.

4.2 Model settings
For WMT, the bag2seq parameter settings follow
the recent NMT systems trained on WMT data. We
use a 50k vocabulary, 620 dimensional word embed-
dings and 1000 hidden units in the decoder LSTM
cells. On the encoder side, the input tokens are em-
bedded to form annotations of the same size as the
hidden units in the decoder. The RNNLM is based
on the “large” setup of Zaremba et al. (2014) which
uses an LSTM. NPLM, a 5-gram neural feedfor-
ward language model, was trained for 10 epochs
with a vocabulary size of 100k, 150 input and out-
put units, 750 hidden units and 100 noise samples
(Vaswani et al., 2013). The n-gram language model
is a 5-gram model estimated with SRILM (Kneser
and Ney, 1995). For the bilingual setting, we im-
plemented a seq2seq NMT system following Bah-
danau et al. (2015) using a beam size of 12 in line
with recent NMT systems for WMT (Sennrich et al.,
2016). RNNLM, bag2seq and seq2seq were imple-
mented using TensorFlow (Abadi et al., 2015)2 and
we used sgnmt for beam decoding3.

Following Schmaltz et al. (2016), our neural mod-
els for PTB have a vocabulary of 16,161 incl. two
different unk tokens and the RNNLM is based on the
“medium” setup of Zaremba et al. (2014). bag2seq
uses 300 dimensional word embeddings and 500
hidden units in the decoder LSTM. We also com-
pare to GYRO (de Gispert et al., 2014) which explic-
itly targets the word-ordering problem. We extracted
1-gram to 5-gram phrase rules from the PTB train-
1We thank the authors for help to reproduce their results.
2https://github.com/ehasler/tensorflow
3https://github.com/ucam-smt/sgnmt

RNNLM NPLM n-gram bag2seq seq2seq BLEU
X 29.4

X 30.3
X 32.5

X 33.6
X X X 34.9
X X X X 39.4

X 49.7
X X 52.6

X X X X 51.3
X X X X X 53.1

Table 1: German word ordering on news-test2015
with beam=12, single models/combinations. Mono-
lingual models use heuristic f(·), bag2seq as a sin-
gle model and bilingual models use no heuristic.

ing data and used an n-gram LM for decoding.For
model combinations, we combine the predictive dis-
tributions in a log-linear model and tune the weights
by optimizing BLEU on the validation set with the
BOBYQA algorithm (Powell, 2009).

5 Results
5.1 Word Ordering on WMT data
The top of Tab. 1 shows that bag2seq outperforms
all other language models by up to 4.2 BLEU on or-
dering German (bold numbers highlight its improve-
ments). This suggests that explicitly presenting all
available tokens to the decoder during search en-
ables it to make better word order choices. A com-
bination of RNNLM, NPLM and n-gram LM yields
a higher score than the individual models, but fur-
ther adding bag2seq yields a large gain of 4.5 BLEU
confirming its suitability for the word-ordering task.

In the bilingual setting in the bottom of Tab. 1,
the seq2seq model is given English input text and
the beam decoder is constrained to generate per-
mutations of German BOWs. This is effectively a
translation task with knowledge of the target BOWs
and seq2seq provides a strong baseline since it uses
source sequence information. Still, adding bag2seq
yields a 2.9 BLEU gain and adding it to the com-
bination of all other models still improves by 1.8
BLEU. This suggests that it could also help for ma-
chine translation rescoring by selecting hypotheses
that constitute good word orderings.

5.2 Word Ordering on the Penn Treebank
Tab. 2 shows the performance of different models
and search heuristics on the Penn Treebank: using

210



Model none f(·) g(·)
Previous work beam=512
GYRO5 42.2 – –
NGRAM-512 – 38.6 –
LSTM-512 – 42.7 –
This work beam=512
n-gram 35.7 38.6 38.9
RNNLM 38.6 43.2 44.2
bag2seq 37.1 33.6 37.1

Table 2: BLEU scores for PTB word-ordering task
(test). NGRAM-512 and LSTM-512 are quoted from
Schmaltz et al. (2016).

no heuristic (none) vs. f(·) and g(·) described in
Section 3. Numbers in bold mark the best result
for a given model. We compare against the LM-
based method of de Gispert et al. (2014) and the
n-gram and RNNLM (LSTM) models of Schmaltz
et al. (2016), of which the latter achieves the best
BLEU score of 42.7. We can reproduce or sur-
pass prior work for n-gram and RNNLM and show
that g(·) outperforms f(·) for these models. This
also holds when adding a 900k sample from the
English Gigaword corpus as proposed by Schmaltz
et al. (2016).4 However, bag2seq underperforms
RNNLM at this large beam size.

Since decoding is slow for large beam sizes, we
compare bag2seq to the n-gram and RNNLM using
a small beam of size 5 in Tab. 3. The first three rows
show that decoding without heuristics is much easier
with bag2seq and outperforms n-gram and RNNLM
by a large margin with 33.4 BLEU. The RNNLM
needs heuristic f(·) to match this performance. For
bag2seq, using heuristic estimates is worse than just
using its partial scores for search. We suspect that its
partial model scores are obfuscated by the heuris-
tic estimates and the amount of their contribution
should probably be tuned on a heldout set. Using the
same beam size, ensembles yield better results but
the best results are achieved by combining RNNLM
and bag2seq (37.9 BLEU). This confirms our find-
ings on WMT data that these models are highly
complementary for word ordering. The results for
beam=64 follow this pattern and identify an inter-
action between heuristics and beam size. While we
get the best results for beam=5 using f(·), heuris-
tic g(·) seems to perform better for larger beams,
4Results omitted from Tab. 2 to save space.
5Note that this model has an advantage because longer sen-
tences are processed in chunks of maximum length 20.

Model none f(·) g(·)
beam=5

n-gram 23.3 30.1 26.5
RNNLM 24.5 33.6 29.7
bag2seq 33.4 27.0 31.7
RNNLM-ensemble 25.5 34.2 30.6
bag2seq-ensemble 34.8 35.1 32.8
RNNLM+bag2seq 35.7 37.9 34.4

beam=64
RNNLM 34.6 40.9 42.5
bag2seq 36.2 31.4 36.5
RNNLM-ensemble 35.4 42.4 43.2
RNNLM+bag2seq 40.5 43.1 43.5

Table 3: BLEU scores for PTB word-ordering task
for different search heuristics and beam sizes (test).

1 5 10 64 128 256 512
0

100

200

300

400

500

600

700

800

Beam Size

T
im

e 
(s

ec
s/

se
nt

en
ce

)

 

 

Rnnlm
Bag2seq
Rnnlm−ensemble
Rnnlm+bag2seq

Figure 2: Decoding time in relation to beam size for
PTB word ordering task (test).

perhaps because the internal unigram statistics be-
come more reliable. Finally, RNNLM+bag2seq with
g(·) and beam=64 outperforms LSTM-512 by 0.8
BLEU. This is significant because decoding in this
configuration is also ∼4x faster than decoding with
a single RNNLM and beam=512 as shown in Fig. 2.

6 Conclusion

We have compared various models for the word-
ordering task and proposed a new model architecture
inspired by attention-based sequence-to-sequence
models that helps performance for both German and
English tasks. We have also proposed a novel search
heuristic and found that using a model combination
together with this heuristic and a modest beam size
provides a good trade-off between speed and quality
and outperforms prior work on the PTB task.

211



References

Martın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2015. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. 1. Software available
from http://www.tensorflow.org/.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp, Philipp
Koehn, Varvara Logacheva, Christof Monz, Matteo
Negri, Matt Post, Carolina Scarton, Lucia Specia, and
Marco Turchi. 2015. Findings of the 2015 workshop
on statistical machine translation. In Proceedings of
the Tenth Workshop on Statistical Machine Transla-
tion, pages 1–46. Association for Computational Lin-
guistics.

Adrià de Gispert, Marcus Tomalin, and W Byrne. 2014.
Word ordering with phrase-based grammars. In EACL,
pages 259–268.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling. In
ICASSP, volume 1, pages 181–184.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
NAACL.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313–330.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nockỳ, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Interspeech,
volume 2, page 3.

Michael JD Powell. 2009. The BOBYQA algorithm for
bound constrained optimization without derivatives.
Cambridge NA Report NA2009/06, University of Cam-
bridge, Cambridge.

Ratish Puduppully, Yue Zhang, and Manish Shrivastava.
2016. Transition-based syntactic linearization with
lookahead features. In Proceedings of NAACL-HLT,
pages 488–493.

Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(01):57–87.

Allen Schmaltz, Alexander M Rush, and Stuart M
Shieber. 2016. Word ordering without syntax. In
EMNLP.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Edinburgh neural machine translation systems
for wmt 16. In Proceedings of the First Conference on
Machine Translation, pages 371–376.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Proceedings of NIPS.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale neu-
ral language models improves translation. In EMNLP.

Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. arXiv preprint arXiv:1502.03044.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.

Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using CCG and guided
search. In EMNLP, pages 1147–1157.

Yue Zhang and Stephen Clark. 2015. Discriminative
syntax-based word ordering for text generation. Com-
putational Linguistics, 41(3):503–538.

Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In EACL, pages 736–746.

212


