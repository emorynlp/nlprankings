



















































Scalable Decipherment for Machine Translation via Hash Sampling


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 362–371,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Scalable Decipherment for Machine Translation via Hash Sampling

Sujith Ravi
Google

Mountain View, CA 94043
sravi@gooogle.com

Abstract

In this paper, we propose a new Bayesian
inference method to train statistical ma-
chine translation systems using only non-
parallel corpora. Following a probabilis-
tic decipherment approach, we first intro-
duce a new framework for decipherment
training that is flexible enough to incorpo-
rate any number/type of features (besides
simple bag-of-words) as side-information
used for estimating translation models. In
order to perform fast, efficient Bayesian
inference in this framework, we then de-
rive a hash sampling strategy that is in-
spired by the work of Ahmed et al. (2012).
The new translation hash sampler enables
us to scale elegantly to complex mod-
els (for the first time) and large vocab-
ulary/corpora sizes. We show empirical
results on the OPUS data—our method
yields the best BLEU scores compared to
existing approaches, while achieving sig-
nificant computational speedups (several
orders faster). We also report for the
first time—BLEU score results for a large-
scale MT task using only non-parallel data
(EMEA corpus).

1 Introduction

Statistical machine translation (SMT) systems
these days are built using large amounts of bilin-
gual parallel corpora. The parallel corpora are
used to estimate translation model parameters in-
volving word-to-word translation tables, fertilities,
distortion, phrase translations, syntactic transfor-
mations, etc. But obtaining parallel data is an ex-
pensive process and not available for all language

pairs or domains. On the other hand, monolin-
gual data (in written form) exists and is easier to
obtain for many languages. Learning translation
models from monolingual corpora could help ad-
dress the challenges faced by modern-day MT sys-
tems, especially for low resource language pairs.
Recently, this topic has been receiving increasing
attention from researchers and new methods have
been proposed to train statistical machine trans-
lation models using only monolingual data in the
source and target language. The underlying moti-
vation behind most of these methods is that statis-
tical properties for linguistic elements are shared
across different languages and some of these sim-
ilarities (mappings) could be automatically identi-
fied from large amounts of monolingual data.

The MT literature does cover some prior work
on extracting or augmenting partial lexicons using
non-parallel corpora (Rapp, 1995; Fung and McK-
eown, 1997; Koehn and Knight, 2000; Haghighi
et al., 2008). However, none of these meth-
ods attempt to train end-to-end MT models, in-
stead they focus on mining bilingual lexicons from
monolingual corpora and often they require par-
allel seed lexicons as a starting point. Some of
them (Haghighi et al., 2008) also rely on addi-
tional linguistic knowledge such as orthography,
etc. to mine word translation pairs across related
languages (e.g., Spanish/English). Unsupervised
training methods have also been proposed in the
past for related problems in decipherment (Knight
and Yamada, 1999; Snyder et al., 2010; Ravi and
Knight, 2011a) where the goal is to decode un-
known scripts or ciphers.

The body of work that is more closely related to
ours include that of Ravi and Knight (2011b) who
introduced a decipherment approach for training
translation models using only monolingual cor-

362



pora. Their best performing method uses an EM
algorithm to train a word translation model and
they show results on a Spanish/English task. Nuhn
et al. (2012) extend the former approach and im-
prove training efficiency by pruning translation
candidates prior to EM training with the help of
context similarities computed from monolingual
corpora.

In this work we propose a new Bayesian in-
ference method for estimating translation mod-
els from scratch using only monolingual corpora.
Secondly, we introduce a new feature-based repre-
sentation for sampling translation candidates that
allows one to incorporate any amount of additional
features (beyond simple bag-of-words) as side-
information during decipherment training. Fi-
nally, we also derive a new accelerated sampling
mechanism using locality sensitive hashing in-
spired by recent work on fast, probabilistic infer-
ence for unsupervised clustering (Ahmed et al.,
2012). The new sampler allows us to perform fast,
efficient inference with more complex translation
models (than previously used) and scale better to
large vocabulary and corpora sizes compared to
existing methods as evidenced by our experimen-
tal results on two different corpora.

2 Decipherment Model for Machine
Translation

We now describe the decipherment problem for-
mulation for machine translation.
Problem Formulation: Given a source text f
(i.e., source word sequences f1...fm) and a mono-
lingual target language corpus, our goal is to deci-
pher the source text and produce a target transla-
tion.

Contrary to standard machine translation train-
ing scenarios, here we have to estimate the transla-
tion model Pθ(f |e) parameters using only mono-
lingual data. During decipherment training, our
objective is to estimate the model parameters in or-
der to maximize the probability of the source text
f as suggested by Ravi and Knight (2011b).

argmax
θ

∏

f

∑

e

P (e) · Pθ(f |e) (1)

For P (e), we use a word n-gram language
model (LM) trained on monolingual target text.
We then estimate the parameters of the translation
model Pθ(f |e) during training.

Translation Model: Machine translation is a
much more complex task than solving other de-
cipherment tasks such as word substitution ci-
phers (Ravi and Knight, 2011b; Dou and Knight,
2012). The mappings between languages involve
non-determinism (i.e., words can have multiple
translations), re-ordering of words can occur as
grammar and syntax varies with language, and
in addition word insertion and deletion operations
are also involved.

Ideally, for the translation model P (f |e) we
would like to use well-known statistical models
such as IBM Model 3 and estimate its parame-
ters θ using the EM algorithm (Dempster et al.,
1977). But training becomes intractable with com-
plex translation models and scalability is also an
issue when large corpora sizes are involved and the
translation tables become huge to fit in memory.
So, instead we use a simplified generative process
for the translation model as proposed by Ravi and
Knight (2011b) and used by others (Nuhn et al.,
2012) for this task:

1. Generate a target (e.g., English) string e =
e1...el, with probability P (e) according to an
n-gram language model.

2. Insert a NULL word at any position in the
English string, with uniform probability.

3. For each target word token ei (including
NULLs), choose a source word translation fi,
with probability Pθ(fi|ei). The source word
may be NULL.

4. Swap any pair of adjacent source words
fi−1, fi, with probability P (swap); set to
0.1.

5. Output the foreign string f = f1...fm, skip-
ping over NULLs.

Previous approaches (Ravi and Knight, 2011b;
Nuhn et al., 2012) use the EM algorithm to es-
timate all the parameters θ in order to maximize
likelihood of the foreign corpus. Instead, we pro-
pose a new Bayesian inference framework to esti-
mate the translation model parameters. In spite of
using Bayesian inference which is typically slow
in practice (with standard Gibbs sampling), we
show later that our method is scalable and permits
decipherment training using more complex trans-
lation models (with several additional parameters).

363



2.1 Adding Phrases, Flexible Reordering and
Fertility to Translation Model

We now extend the generative process (described
earlier) to more complex translation models.
Non-local Re-ordering: The generative process
described earlier limits re-ordering to local or ad-
jacent word pairs in a source sentence. We ex-
tend this to allow re-ordering between any pair of
words in the sentence.
Fertility: We also add a fertility model Pθfert to
the translation model using the formula:

Pθfert =
∏

i

nθ(φi|ei) · pφ01 (2)

nθ(φi|ei) =
αfert · P0(φi|ei) + C−i(ei, φi)

αfert + C−i(ei)
(3)

where, P0 represents the base distribution
(which is set to uniform) in a Chinese Restau-
rant Process (CRP)1 for the fertility model and
C−i represents the count of events occurring in
the history excluding the observation at position i.
φi is the number of source words aligned to (i.e.,
generated by) the target word ei. We use sparse
Dirichlet priors for all the translation model com-
ponents.2 φ0 represents the target NULL word fer-
tility and p1 is the insertion probability which is
fixed to 0.1. In addition, we set a maximum thresh-
old for fertility values φi ≤ γ ·m, where m is the
length of the source sentence. This discourages
a particular target word (e.g., NULL word) from
generating too many source words in the same sen-
tence. In our experiments, we set γ = 0.3. We en-
force this constraint in the training process during
sampling.3

Modeling Phrases: Finally, we extend the trans-
lation candidate set in Pθ(fi|ei) to model phrases
in addition to words for the target side (i.e., ei can
now be a word or a phrase4 previously seen in the
monolingual target corpus). This greatly increases
the training time since in each sampling step, we
now have many more ei candidates to choose
from. In Section 4, we describe how we deal

1Each component in the translation model (word/phrase
translations Pθ(fi|ei), fertility Pθfert , etc.) is modeled using
a CRP formulation.

2i.e., All the concentration parameters are set to low val-
ues; αf |e = αfert = 0.01.

3We only apply this constraint when training on source
text/corpora made of long sentences (>10 words) where the
sampler might converge very slowly. For short sentences, a
sparse prior on fertility αfert typically discourages a target
word from being aligned to too many different source words.

4Phrase size is limited to two words in our experiments.

with this problem by using a fast, efficient sam-
pler based on hashing that allows us to speed up
the Bayesian inference significantly whereas stan-
dard Gibbs sampling would be extremely slow.

3 Feature-based representation for
Source and Target

The model described in the previous section while
being flexible in describing the translation pro-
cess, poses several challenges for training. As
the source and target vocabulary sizes increase the
size of the translation table (|Vf | · |Ve|) increases
significantly and often becomes too huge to fit in
memory. Additionally, performing Bayesian in-
ference with such a complex model using stan-
dard Gibbs sampling can be very slow in prac-
tice. Here, we describe a new method for doing
Bayesian inference by first introducing a feature-
based representation for the source and target
words (or phrases) from which we then derive a
novel proposal distribution for sampling transla-
tion candidates.

We represent both source and target words in
a vector space similar to how documents are rep-
resented in typical information retrieval settings.
But unlike documents, here each word w is as-
sociated with a feature vector w1...wd (where wi

represents the weight for the feature indexed by i)
which is constructed from monolingual corpora.
For instance, context features for word w may in-
clude other words (or phrases) that appear in the
immediate context (n-gram window) surrounding
w in the monolingual corpus. Similarly, we can
add other features based on topic models, orthog-
raphy (Haghighi et al., 2008), temporal (Klemen-
tiev et al., 2012), etc. to our representation all of
which can be extracted from monolingual corpora.

Next, given two high dimensional vectors u and
v it is possible to calculate the similarity between
the two words denoted by s(u,v). The feature
construction process is described in more detail
below:
Target Language: We represent each word (or
phrase) ei with the following contextual features
along with their counts: (a) f−context: every (word
n-gram, position) pair immediately preceding ei
in the monolingual corpus (n=1, position=−1), (b)
similar features f+context to model the context fol-
lowing ei, and (c) we also throw in generic context
features fscontext without position information—
every word that co-occurs with ei in the same sen-

364



tence. While the two position-features provide
specific context information (may be sparse for
large monolingual corpora), this feature is more
generic and captures long-distance co-occurrence
statistics.
Source Language: Words appearing in a source
sentence f are represented using the correspond-
ing target translation e = e1...em generated for
f in the current sample during training. For each
source word fj ∈ f , we look at the corresponding
word ej in the target translation. We then extract
all the context features of ej in the target trans-
lation sample sentence e and add these features
(f−context, f+context, fscontext) with weights to the
feature representation for fj .

Unlike the target word feature vectors (which
can be pre-computed from the monolingual tar-
get corpus), the feature vector for every source
word fj is dynamically constructed from the tar-
get translation sampled in each training iteration.
This is a key distinction of our framework com-
pared to previous approaches that use contextual
similarity (or any other) features constructed from
static monolingual corpora (Rapp, 1995; Koehn
and Knight, 2000; Nuhn et al., 2012).

Note that as we add more and more features for
a particular word (by training on larger monolin-
gual corpora or adding new types of features, etc.),
it results in the feature representation becoming
more sparse (especially for source feature vectors)
which can cause problems in efficiency as well
as robustness when computing similarity against
other vectors. In the next section, we will describe
how we mitigate this problem by projecting into a
low-dimensional space by computing hash signa-
tures.

In all our experiments, we only use the features
described above for representing source and tar-
get words. We note that the new sampling frame-
work is easily extensible to many additional fea-
ture types (for example, monolingual topic model
features, etc.) which can be efficiently handled by
our inference algorithm and could further improve
translation performance but we leave this for fu-
ture work.

4 Bayesian MT Decipherment via Hash
Sampling

The next step is to use the feature representations
described earlier and iteratively sample a target
word (or phrase) translation candidate ei for every

word fi in the source text f . This involves choos-
ing from |Ve| possible target candidates in every
step which can be highly inefficient (and infeasi-
ble for large vocabulary sizes). One possible strat-
egy is to compute similarity scores s(wfi ,we′) be-
tween the current source word feature vector wfi
and feature vectors we′∈Ve for all possible candi-
dates in the target vocabulary. Following this, we
can prune the translation candidate set by keeping
only the top candidates e∗ according to the sim-
ilarity scores. Nuhn et al. (2012) use a similar
strategy to obtain a more compact translation table
that improves runtime efficiency for EM training.
Their approach requires calculating and sorting all
|Ve| · |Vf | distances in timeO(V 2 · log(V )), where
V = max(|Ve|, |Vf |).
Challenges: Unfortunately, there are several ad-
ditional challenges which makes inference very
hard in our case. Firstly, we would like to in-
clude as many features as possible to represent
the source/target words in our framework besides
simple bag-of-words context similarity (for exam-
ple, left-context, right-context, and other general-
purpose features based on topic models, etc.). This
makes the complexity far worse (in practice) since
the dimensionality of the feature vectors d is a
much higher value than |Ve|. Computing similar-
ity scores alone (naı̈vely) would incur O(|Ve| · d)
time which is prohibitively huge since we have to
do this for every token in the source language cor-
pus. Secondly, for Bayesian inference we need to
sample from a distribution that involves comput-
ing probabilities for all the components (language
model, translation model, fertility, etc.) described
in Equation 1. This distribution needs to be com-
puted for every source word token fi in the corpus,
for all possible candidates ei ∈ Ve and the process
has to be repeated for multiple sampling iterations
(typically more than 1000). Doing standard col-
lapsed Gibbs sampling in this scenario would be
very slow and intractable.

We now present an alternative fast, efficient
inference strategy that overcomes many of the
challenges described above and helps acceler-
ate the sampling process significantly. First,
we set our translation models within the con-
text of a more generic and widely known fam-
ily of distributions—mixtures of exponential fam-
ilies. Then we derive a novel proposal distribu-
tion for sampling translation candidates and intro-
duce a new sampler for decipherment training that

365



is based on locality sensitive hashing (LSH).
Hashing methods such as LSH have been

widely used in the past in several scenarios in-
cluding NLP applications (Ravichandran et al.,
2005). Most of these approaches employ LSH
within heuristic methods for speeding up nearest-
neighbor look up and similarity computation tech-
niques. However, we use LSH hashing within
a probabilistic framework which is very different
from the typical use of LSH.

Our work is inspired by some recent work by
Ahmed et al. (2012) on speeding up Bayesian in-
ference for unsupervised clustering. We use a sim-
ilar technique as theirs but a different approximate
distribution for the proposal, one that is better-
suited for machine translation models and without
some of the additional overhead required for com-
puting certain terms in the original formulation.
Mixtures of Exponential Families: The transla-
tion models described earlier (Section 2) can be
represented as mixtures of exponential families,
specifically mixtures of multinomials. In exponen-
tial families, distributions over random variables
are given by:

p(x; θ) = exp(〈φ(x), θ〉)− g(θ) (4)

where, φ : X → F is a map from x to the space
of sufficient statistics and θ ∈ F . The term g(θ)
ensures that p(x; θ) is properly normalized. X is
the domain of observations X = x1, ..., xm drawn
from some distribution p. Our goal is to estimate
p. In our case, this refers to the translation model
from Equation 1.

We also choose corresponding conjugate
Dirichlet distributions for priors which have the
property that the posterior distribution p(θ|X)
over θ remains in the same family as p(θ).

Note that the (translation) model in our
case consists of multiple exponential families
components—a multinomial pertaining to the lan-
guage model (which remains fixed5), and other
components pertaining to translation probabilities
Pθ(fi|ei), fertility Pθfert , etc. To do collapsed
Gibbs sampling under this model, we would per-
form the following steps during sampling:
1. For a given source word token fi draw target

5A high value for the LM concentration parameter α en-
sures that the LM probabilities do not deviate too far from the
original fixed base distribution during sampling.

translation

ei ∼ p(ei|F,E−i)
∝ p(e) · p(fi|ei, F−i, E−i)
· pfert(·|ei, F−i, E−i) · ... (5)

where, F is the full source text and E the full
target translation generated during sampling.
2. Update the sufficient statistics for the changed
target translation assignments.

For large target vocabularies, computing
p(fi|ei, F−i, E−i) dominates the inference pro-
cedure. We can accelerate this step significantly
using a good proposal distribution via hashing.
Locality Sensitive Hash Sampling: For general
exponential families, here is a Taylor approxima-
tion for the data likelihood term (Ahmed et al.,
2012):

p(x|·) ≈ exp(〈φ(x), θ∗〉)− g(θ∗) (6)

where, θ∗ is the expected parameter (sufficient
statistics).
For sampling the translation model, this involves
computing an expensive inner product 〈φ(fi), θ∗e′〉
for each source word fi which has to be repeated
for every translation candidate e′, including candi-
dates that have very low probabilities and are un-
likely to be chosen as the translation for fj .

So, during decipherment training a standard
collapsed Gibbs sampler will waste most of its
time on expensive computations that will be dis-
carded in the end anyways. Also, unlike some
standard generative models used in other unsu-
pervised learning scenarios (e.g., clustering) that
model only observed features (namely words ap-
pearing in the document), here we would like to
enrich the translation model with a lot more fea-
tures (side-information).

Instead, we can accelerate the computation of
the inner product 〈φ(fi), θ∗e′〉 using a hash sam-
pling strategy similar to (Ahmed et al., 2012).
The underlying idea here is to use binary hash-
ing (Charikar, 2002) to explore only those can-
didates e′ that are sufficiently close to the best
matching translation via a proposal distribution.
Next, we briefly introduce some notations and ex-
isting theoretical results related to binary hashing
before describing the hash sampling procedure.

For any two vectors u, v ∈ Rn,

〈u, v〉 = ‖u‖ · ‖v‖ · cos](u, v) (7)

366



](u, v) = πPr{sgn[〈u,w〉] 6= sgn[〈v, w〉]}
(8)

where, w is a random vector drawn from a sym-
metric spherical distribution and the term inside
Pr{·} represents the relation between the signs of
the two inner products.

Let hl(v) ∈ {0, 1}l be an l-bit binary hash of v
where: [hl(v)]i := sgn[〈v, wi〉]; wi ∼ Um. Then
the probability of matching signs is given by:

zl(u, v) :=
1

l
‖h(u)− h(v)‖1 (9)

So, zl(u, v) measures how many bits differ be-
tween the hash vectors h(u) and h(v) associated
with u, v. Combining this with Equations 6 and 7
we can estimate the unnormalized log-likelihood
of a source word fi being translated as target e′

via:

sl(fi, e
′) ∝ ‖θe′‖ · ‖φ(fi)‖ · cosπzl(φ(fi), θe′)

(10)
For each source word fi, we now sample from

this new distribution (after normalization) instead
of the original one. The binary hash representa-
tion for the two vectors yield significant speedups
during sampling since Hamming distance compu-
tation between h(u) and h(v) is highly optimized
on modern CPUs. Hence, we can compute an es-
timate for the inner product quite efficiently.6

Updating the hash signatures: During training,
we compute the target candidate projection h(θe′)
and corresponding norm only once7 which is dif-
ferent from the setup of Ahmed et al. (2012). The
source word projection φ(fi) is dynamically up-
dated in every sampling step. Note that doing this
naı̈vely would scale slowly as O(Dl) where D is
the total number of features but instead we can up-
date the hash signatures in a more efficient manner
that scales as O(Di>0 l) where Di>0 is the number
of non-zero entries in the feature representation for
the source word φ(fi). Also, we do not need to
store the random vectors w in practice since these
can be computed on the fly using hash functions.
The inner product approximation also yields some
theoretical guarantees for the hash sampler.8

6We set l = 32 bits in our experiments.
7In practice, we can ignore the norm terms to further

speed up sampling since this is only an estimate for the pro-
posal distribution and we follow this with the Metropolis
Hastings step.

8For further details, please refer to (Ahmed et al., 2012).

4.1 Metropolis Hastings
In each sampling step, we use the distribution
from Equation 10 as a proposal distribution in
a Metropolis Hastings scheme to sample target
translations for each source word.

Once a new target translation e′ is sampled
for source word fi from the proposal distribution
q(·) ∝ expsl(fi,e′), we accept the proposal (and
update the corresponding hash signatures) accord-
ing to the probability r

r =
q(eoldi ) · pnew(·)
q(enewi ) · pold(·)

(11)

where, pold(·), pnew(·) are the true conditional
likelihood probabilities according to our model
(including the language model component) for the
old, new sample respectively.

5 Training Algorithm

Putting together all the pieces described in the pre-
vious section, we perform the following steps:
1. Initialization: We initialize the starting sample
as follows: for each source word token, randomly
sample a target word. If the source word also ex-
ists in the target vocabulary, then choose identity
translation instead of the random one.9

2. Hash Sampling Steps: For each source word
token fi, run the hash sampler:

(a) Generate a proposal distribution by comput-
ing the hamming distance between the feature vec-
tors for the source word and each target translation
candidate. Sample a new target translation ei for
fi from this distribution.

(b) Compute the acceptance probability for the
chosen translation using a Metropolis Hastings
scheme and accept (or reject) the sample. In prac-
tice, computation of the acceptance probability
only needs to be done every r iterations (where
r can be anywhere from 5 or 100).
Iterate through steps (2a) and (2b) for every word
in the source text and then repeat this process for
multiple iterations (usually 1000).
3. Other Sampling Operators: After every k it-
erations,10 perform the following sampling opera-
tions:

(a) Re-ordering: For each source word token fi
at position i, randomly choose another position j

9Initializing with identity translation rather than random
choice helps in some cases, especially for unknown words
that involve named entities, etc.

10We set k = 3 in our experiments.

367



Corpus Language Sent. Words Vocab.
OPUS Spanish 13,181 39,185 562

English 19,770 61,835 411
EMEA French 550,000 8,566,321 41,733

Spanish 550,000 7,245,672 67,446

Table 1: Statistics of non-parallel corpora used
here.

in the source sentence and swap the translations ei
with ej . During the sampling process, we compute
the probabilities for the two samples—the origi-
nal and the swapped versions, and then sample an
alignment from this distribution.

(b) Deletion: For each source word token,
delete the current target translation (i.e., align it
with the target NULL token). As with the re-
ordering operation, we sample from a distribution
consisting of the original and the deleted versions.
4. Decoding the foreign sentence: Finally, once
the training is done (i.e., after all sampling iter-
ations) we choose the final sample as our target
translation output for the source text.

6 Experiments and Results

We test our method on two different corpora.
To evaluate translation quality, we use BLEU
score (Papineni et al., 2002), a standard evaluation
measure used in machine translation.

First, we present MT results on non-parallel
Spanish/English data from the OPUS cor-
pus (Tiedemann, 2009) which was used by Ravi
and Knight (2011b) and Nuhn et al. (2012).
We show that our method achieves the best
performance (BLEU scores) on this task while
being significantly faster than both the previous
approaches. We then apply our method to a
much larger non-parallel French/Spanish corpus
constructed from the EMEA corpus (Tiedemann,
2009). Here the vocabulary sizes are much larger
and we show how our new Bayesian decipherment
method scales well to this task inspite of using
complex translation models. We also report the
first BLEU results on such a large-scale MT task
under truly non-parallel settings (without using
any parallel data or seed lexicon).

For both the MT tasks, we also report BLEU
scores for a baseline system using identity trans-
lations for common words (words appearing in
both source/target vocabularies) and random trans-
lations for other words.

6.1 MT Task and Data

OPUS movie subtitle corpus (Tiedemann, 2009):
This is a large open source collection of parallel
corpora available for multiple language pairs. We
use the same non-parallel Spanish/English corpus
used in previous works (Ravi and Knight, 2011b;
Nuhn et al., 2012). The details of the corpus are
listed in Table 1. We use the entire Spanish source
text for decipherment training and evaluate the fi-
nal English output to report BLEU scores.
EMEA corpus (Tiedemann, 2009): This is a par-
allel corpus made out of PDF documents (arti-
cles from the medical domain) from the Euro-
pean Medicines Agency. We reserve the first 1k
sentences in French as our source text (also used
in decipherment training). To construct a non-
parallel corpus, we split the remaining 1.1M lines
as follows: first 550k sentences in French, last
550k sentences in Spanish. The latter is used to
construct a target language model used for deci-
pherment training. The corpus statistics are shown
in Table 1.

6.2 Results

OPUS: We compare the MT results (BLEU
scores) from different systems on the OPUS cor-
pus in Table 2. The first row displays baseline
performance. The next three rows 1a–1c display
performance achieved by two methods from Ravi
and Knight (2011b). Rows 2a, 2b show results
from the of Nuhn et al. (2012). The last two rows
display results for the new method using Bayesian
hash sampling. Overall, using a 3-gram language
model (instead of 2-gram) for decipherment train-
ing improves the performance for all methods. We
observe that our method produces much better re-
sults than the others even with a 2-gram LM. With
a 3-gram LM, the new method achieves the best
performance; the highest BLEU score reported on
this task. It is also interesting to note that the hash
sampling method yields much better results than
the Bayesian inference method presented in (Ravi
and Knight, 2011b). This is due to the accelerated
sampling scheme introduced earlier which helps it
converge to better solutions faster.

Table 2 (last column) also compares the effi-
ciency of different methods in terms of CPU time
required for training. Both our 2-gram and 3-gram
based methods are significantly faster than those
previously reported for EM based training meth-
ods presented in (Ravi and Knight, 2011b; Nuhn

368



Method BLEU Time (hours)
Baseline system (identity translations) 6.9

1a. EM with 2-gram LM (Ravi and Knight, 2011b) 15.3 ∼850h
1b. EM with whole-segment LM (Ravi and Knight, 2011b) 19.3
1c. Bayesian IBM Model 3 with 2-gram LM (Ravi and Knight, 2011b) 15.1
2a. EM+Context with 2-gram LM (Nuhn et al., 2012) 15.2 50h
2b. EM+Context with 3-gram LM (Nuhn et al., 2012) 20.9 200h
3. Bayesian (standard) Gibbs sampling with 2-gram LM 222h
4a. Bayesian Hash Sampling∗ with 2-gram LM (this work) 20.3 2.6h
4b. Bayesian Hash Sampling∗ with 3-gram LM (this work) 21.2 2.7h

(∗sampler was run for 1000 iterations)

Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours)
on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian
methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel
processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al. (2012) based on their EM
implementation for different settings.

Method BLEU
Baseline system (identity translations) 3.0
Bayesian Hash Sampling with 2-gram LM

vocab=full (Ve), add fertility=no 4.2
vocab=pruned∗, add fertility=yes 5.3

Table 3: MT results on the French/Spanish EMEA
corpus using the new hash sampling method. ∗The
last row displays results when we sample target
translations from a pruned candidate set (most fre-
quent 1k Spanish words + identity translation can-
didates) which enables the sampler to run much
faster when using more complex models.

et al., 2012). This is very encouraging since Nuhn
et al. (2012) reported obtaining a speedup by prun-
ing translation candidates (to ∼1/8th the original
size) prior to EM training. On the other hand, we
sample from the full set of translation candidates
including additional target phrase (of size 2) can-
didates which results in a much larger vocabulary
consisting of 1600 candidates (∼4 times the orig-
inal size), yet our method runs much faster and
yields better results. The table also demonstrates
the siginificant speedup achieved by the hash sam-
pler over a standard Gibbs sampler for the same
model (∼85 times faster when using a 2-gram
LM).

We also compare the results against MT per-
formance from parallel training—MOSES sys-
tem (Koehn et al., 2007) trained on 20k sentence
pairs. The comparable number for Table 2 is 63.6
BLEU.

Spanish (e) French (f)
el → les
la → la

por → des
sección → rubrique

administración → administration

Table 4: Sample (1-best) Spanish/French transla-
tions produced by the new method on the EMEA
corpus using word translation models trained with
non-parallel corpora.

EMEA Results Table 3 shows the results achieved
by our method on the larger task involving EMEA
corpus. Here, the target vocabulary Ve is much
higher (67k). In spite of this challenge and the
model complexity, we can still perform decipher-
ment training using Bayesian inference. We report
the first BLEU score results on such a large-scale
task using a 2-gram LM. This is achieved without
using any seed lexicon or parallel corpora. The re-
sults are encouraging and demonstrates the ability
of the method to scale to large-scale settings while
performing efficient inference with complex mod-
els, which we believe will be especially useful for
future MT application in scenarios where parallel
data is hard to obtain. Table 4 displays some sam-
ple 1-best translations learned using this method.

For comparison purposes, we also evaluate MT
performance on this task using parallel training
(MOSES trained with hundred sentence pairs) and
observe a BLEU score of 11.7.

369



7 Discussion and Future Work

There exists some work (Dou and Knight, 2012;
Klementiev et al., 2012) that uses monolingual
corpora to induce phrase tables, etc. These when
combined with standard MT systems such as
Moses (Koehn et al., 2007) trained on parallel cor-
pora, have been shown to yield some BLEU score
improvements. Nuhn et al. (2012) show some
sample English/French lexicon entries learnt us-
ing EM algorithm with a pruned translation can-
didate set on a portion of the Gigaword corpus11

but do not report any actual MT results. In ad-
dition, as we showed earlier our method can use
Bayesian inference (which has a lot of nice proper-
ties compared to EM for unsupervised natural lan-
guage tasks (Johnson, 2007; Goldwater and Grif-
fiths, 2007)) and still scale easily to large vocabu-
lary, data sizes while allowing the models to grow
in complexity. Most importantly, our method pro-
duces better translation results (as demonstrated
on the OPUS MT task). And to our knowledge,
this is the first time that anyone has reported MT
results under truly non-parallel settings on such a
large-scale task (EMEA).

Our method is also easily extensible to out-
of-domain translation scenarios similar to (Dou
and Knight, 2012). While their work also uses
Bayesian inference with a slice sampling scheme,
our new approach uses a novel hash sampling
scheme for decipherment that can easily scale
to more complex models. The new decipher-
ment framework also allows one to easily incorpo-
rate additional information (besides standard word
translations) as features (e.g., context features,
topic features, etc.) for unsupervised machine
translation which can help further improve the per-
formance in addition to accelerating the sampling
process. We already demonstrated the utility of
this system by going beyond words and incorpo-
rating phrase translations in a decipherment model
for the first time.

In the future, we can obtain further speedups
(especially for large-scale tasks) by parallelizing
the sampling scheme seamlessly across multiple
machines and CPU cores. The new framework can
also be stacked with complementary techniques
such as slice sampling, blocked (and type) sam-
pling to further improve inference efficiency.

11http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2003T05

8 Conclusion

To summarize, our method is significantly faster
than previous methods based on EM or Bayesian
with standard Gibbs sampling and obtains better
results than any previously published methods for
the same task. The new framework also allows
performing Bayesian inference for decipherment
applications with more complex models than pre-
viously shown. We believe this framework will
be useful for further extending MT models in the
future to improve translation performance and for
many other unsupervised decipherment applica-
tion scenarios.

References
Amr Ahmed, Sujith Ravi, Shravan Narayanamurthy,

and Alex Smola. 2012. Fastex: Hash clustering
with exponential families. In Proceedings of the
26th Conference on Neural Information Processing
Systems (NIPS).

Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the thiry-fourth annual ACM Symposium on Theory
of Computing, pages 380–388.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):1–38.

Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266–275.

Pascale Fung and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192–202.

Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744–751.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL:
HLT, pages 771–779.

Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296–305.

370



Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics.

Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
Proceedings of the ACL Workshop on Unsupervised
Learning in Natural Language Processing, pages
37–44.

Philipp Koehn and Kevin Knight. 2000. Estimating
word translation probabilities from unrelated mono-
lingual corpora using the em algorithm. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence and Twelfth Conference on Inno-
vative Applications of Artificial Intelligence, pages
711–715.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180.

Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 156–164.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311–318.

Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 320–322.

Sujith Ravi and Kevin Knight. 2011a. Bayesian in-
ference for zodiac and other homophonic ciphers.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, pages 239–247.

Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
12–21.

Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 622–629.

Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1048–1057.

Jörg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237–248.

371


