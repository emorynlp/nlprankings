



















































Language and Domain Independent Entity Linking with Quantified Collective Validation


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 695–704,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Language and Domain Independent Entity Linking
with Quantified Collective Validation

Han Wang∗,1, Jin Guang Zheng∗,2, Xiaogang Ma1, Peter Fox1, and Heng Ji2
{Tetherless World Constellation1, Computer Science Department2}

Rensselaer Polytechnic Institute
Troy, NY, USA

{wangh17, zhengj6, max7, pfox, jih}@rpi.edu

Abstract

Linking named mentions detected in a
source document to an existing knowl-
edge base provides disambiguated entity
referents for the mentions. This al-
lows better document analysis, knowl-
edge extraction and knowledge base pop-
ulation. Most of the previous research
extensively exploited the linguistic fea-
tures of the source documents in a su-
pervised or semi-supervised way. These
systems therefore cannot be easily ap-
plied to a new language or domain. In
this paper, we present a novel unsuper-
vised algorithm named Quantified Col-
lective Validation that avoids excessive
linguistic analysis on the source docu-
ments and fully leverages the knowledge
base structure for the entity linking task.
We show our approach achieves state-
of-the-art English entity linking perfor-
mance and demonstrate successful de-
ployment in a new language (Chinese)
and two new domains (Biomedical and
Earth Science). Experiment datasets
and system demonstration are available
at http://tw.rpi.edu/web/doc/
hanwang_emnlp_2015 for research
purpose.

1 Introduction and Motivation

The entity linking (EL) task aims at analyzing
each named entity mention in a source document
and linking it to its referent in a knowledge base
(KB). Consider the following example: “One day
after released by the Patriots, Florida born Cald-
well visited the Jets. ...... The New York Jets
have six receivers on the roster: Cotchery, Coles,
...”. Here “Caldwell” is an ambiguous mention

∗These authors contributed equally to this work.

because not only are there thousands of people
with different professions named “Caldwell”, but
even if as an American football player, as most
people would recognize it from the context, there
are several “Caldwell”s who are/were associated
with either “the Patriots” or “the Jets”. An EL
system should be able to disambiguate the men-
tion by carefully examining the context and then
identify the correct KB referent, which is Reche
Caldwell in this case.

Although EL has attracted a lot of community
attention in the recent years, most research efforts
have been focused on developing systems only ef-
fective for generic English corpora. When these
systems are migrated to a new language or do-
main, their performance will usually suffer from
a noticeable decline due to the following reasons:

1) State-of-the-art EL systems have developed
comprehensive linguistic features from the source
documents to generate advanced representations
of the mentions and their context. While this
methodology has been proved rewarding for a
resource-rich language such as English, it prevents
the systems from being adopted to a new language,
especially to one with limited linguistic resources.
One can imagine that it would be very difficult,
if not impossible, for an English EL system that
benefits from the part-of-speech tagging, depen-
dency parsing, and named entity recognition to be
deployed to a new language such as Chinese that
has quite different linguistic characteristics.

2) The current EL approaches mostly target at
people, organizations, and geo-political entities
which are widely present in a general KB such
as Wikipedia. However, domain-specific EL tends
to pay more attention to entities beyond the above
three types. For instance, in the biomedical sci-
ence domain, protein is a major class of entities
that greatly interest scientists. Conventional EL
systems are very likely to fail in linking protein
mentions in the text due to the lack of labeled

695



training data. Moreover, their reliance on general
reference KBs seems insufficient for a specific do-
main. Take “A20”, a type of protein as an example.
Wikipedia has more than a few items listed un-
der the name of “A20” and their types range from
aircrafts to roads. This diversified information in-
evitably introduces noise for a biomedical EL ap-
plication.

One potential solution to tackle these limita-
tions is, instead of concentrating on the source
documents, to conduct more deliberate study on
the KB. Structured KBs such as DBpedia1 typi-
cally offer detailed descriptions about entities, a
large collection of named relations between enti-
ties, and a growing number of multi-lingual en-
tity surface forms. By embracing these ready-
for-use information and linked structures, we will
be able to obtain sufficient contextual information
for disambiguation without generating a full list
of linguistic features from the source documents,
and therefore eliminate the language dependency.
Moreover, currently there exist numerous pub-
licly available domain ontology repositories such
as BioPortal2 and OBO Foundry3 which provide
significantly more domain knowledge than general
KBs for EL to leverage. By incorporating these
domain ontologies, we can easily increase the en-
tity coverage and reduce noise for deploying EL in
various new domains.

In order to make the most of the KB structure,
the mention context should be matched against
the KB such that the relevant KB information
can be extracted. A collective way of aligning
co-occurred mentions to the KB graph has been
proved to be a successful strategy to better rep-
resent the source context (Pennacchiotti and Pan-
tel, 2009; Fernández et al., 2010; Cucerzan, 2011;
Han et al., 2011; Ratinov et al., 2011; Dalton and
Dietz, 2013; Zheng et al., 2014; Pan et al., 2015).
We take a further step to consider quantitatively
differentiating entity relations in the KB in or-
der to evaluate entity candidates more precisely.
Meanwhile, we jointly validate these candidates
by aligning them back to the source context and in-
tegrating multiple ranking results. This novel EL
framework deeply exploits the KB structure with a
light weight representation of the source context,
and thus enables a smooth migration to new lan-

1http://wiki.dbpedia.org
2http://bioportal.bioontology.org
3http://www.obofoundry.org

guages and domains.
The main novel contributions of this paper are

summarized as follows: 1) We design an unsuper-
vised EL algorithm, namely, Quantified Collec-
tive Validation (QCV) that builds KB entity can-
didate graphs with quantified relations for the pur-
pose of collective disambiguation and inference.
2) We develop a procedure of building language
and domain independent EL systems by incorpo-
rating various ontologies into the QCV compo-
nent. 3) We demonstrate that our system is able
to achieve state-of-the-art performance in English
EL, and it can also produce promising results for
Chinese EL as well as EL in Biomedical Science
and Earth Science.

2 Baseline Collective EL

As a baseline, we adopt a competitive unsuper-
vised collective EL system (Zheng et al., 2014)
utilizing structured KBs. It defines entropy based
weights for the KB relations, and embeds them in
a two-step candidate ranking process to produce
the EL results.

Structured KB Terminologies: In a structured
KB, a fact is usually expressed in the form of a
triple: (eh, r, et) where eh, et are called the head
entity and the tail entity, respectively, and r is the
relation between eh and et.

Entropy Based KB Relation Weights: The goal
is to leverage various levels of granularity of KB
relations. The calculation of the relation weight
H(r) is given in Equation (1):

H(r) = −
∑

et∈Et(r)
P (et) log(P (et)) (1)

where Et(r) is the tail entity set for r in the KB,
and P (et) is the probability of et appearing as the
tail entity for r in the KB.

Salience Ranking: As the first ranking step, we
examine the candidates without the context and
prefers those with higher importance in the KB.
Equation (2) computes the salience score Sa(c) for
a candidate c:

Sa(c) =
∑

r∈R(c),et∈Et(r)
H(r)

Sa(et)
L(et)

(2)

whereR(c) is the relation set for c in the KB;H(r)
is given by Equation (1); Et(r) is the tail entity
set with c being the head entity and r being the
connecting relation in the KB; L(et) denotes the

696



cardinality of the tail entity set with et being the
head entity in the KB. Sa(c) is recursively com-
puted until convergence.

Collective Ranking: The similarity SimF (m, c)
between a candidate c and its mentionm is defined
using Equation (3) as the final ranking score:

SimF (m, c) = α · JS(m, c) · Sa(c)
+ β ·

∑
r∈R(c)

H(r) ·
∑

n∈Et(r)∩C(m)
Sa(n) (3)

where JS(m, c) is the Jaccard similarity between
the string surface forms of m and c; Sa(c) and
Sa(n) are both evaluated by Equation (2); C(m)
denotes the candidate set for mention m; α and β
are hyperparameters.

3 Quantified Collective Validation

Incorporating the KB relation weighing mecha-
nism of the baseline system, our QCV algorithm
constructs a number of candidate graphs for a
given set of collaborative mentions, and then per-
forms a two-level ranking followed by a collective
validation on those candidate graphs to acquire the
linking results. Because this procedure minimally
relies on linguistic analysis of the source docu-
ments while mainly uses the KB structure which
by nature keeps detached from any specific lan-
guage or domain, we claim that QCV comes with
language and domain independence.

3.1 Candidate Graph Construction
The KB entity candidate graphs are constructed
based on a mention context graph and a KB graph.
We will introduce them in order as follows.

Mention Context Graph: To avoid abusing lin-
guistic knowledge from the source documents, we
construct a mention context graph Gm simply in-
volving mention co-occurrence. Figure 1 depicts
a constructed Gm for the Caldwell example at
the beginning of Section 1. In this figure, men-
tions “New York Jets”, “Cotchery” and “Coles”
are brought into Gm through the coreference be-
tween “Jets” and “New York Jets” since the three
of them are outside the context window of “Cald-
well”, “Florida”, “Patriots”, and “Jets”. Gm con-
tains a set of vertices representing the mentions
extracted from the source document and a set of
undirected edges. There will be an edge between
two mention vertices if both of them fall into a
context window with width wm in the source doc-
ument. Ideally, wm should cover a single dis-

course according to the one sense per discourse
assumption (Gale et al., 1992), but for simplic-
ity we heuristically set wm to be 7-sentence wide
as a hyperparameter. Two mention vertices will
be connected via a dashed edge if they are coref-
erential but are not located in the same context
window. Here we determine the coreference by
performing substring matching and abbreviation
expansion. The dashed edge indicates the out-
of-context coreferential mention together with its
neighbors will be indirectly included in Gm as
extended context to later facilitate the candidate
graph collective validation. Note that all of these
loose settings comply with our intention of gener-
ating a light-weight source context representation
born with domain and language independence.

Caldwell

Florida

Patriots Jets New York Jets

Cotchery

Coles

Figure 1: Mention context graph for the Caldwell
example.

KB Graph: A structured KB such as DBpedia
can be represented as a weighted graph Gk that
consists of a set of vertices representing the en-
tities and a set of directed edges labeled with re-
lations between entities. The weights of relations
are computed using Equation (1). In order to fur-
ther enrich the KB relations, we add a type of re-
lation named “wiki link” between two entities if
one of them appears in the Wikipedia article of the
other. Figure 2 presents a subgraph of the DBpe-
dia KB graph containing the relevant entities in the
Caldwell example.

Candidate Graph: The candidate graph is a
set of graphs Gic (i = 1, 2, ...) used for com-
puting ranking scores for the KB entity candi-
dates. For each of the mentions extracted from
the source context, we first select a list of entity
candidates from Gk with heuristic rules such as
fuzzy string matching, synonyms, Wikipedia redi-
rect, etc. Then we pick one candidate for each of
the mentions to constitute the vertices of a Gic. In
each Gic, we add an edge between two vertices if
they are connected in Gk by some relation r and
their mentions are connected in Gm. The edge la-
bel r from Gk is transferred to Gic. Upon comple-

697



New York Jets

Reche Caldwell

Florida

New England Patriots

Andre Caldwell Jim Caldwell (American Football)
James Caldwell 
(clergyman)

Patriots     
(American Revolution)

birth place
0.93

birth place
0.93

wiki link
0.21

former team
0.61

wiki link
0.21

wiki link
0.21

Newcastle Jets FC

Jet aircraftwiki link
0.21

Florida, Ohio

Florida City, Florida

… …

…

Patriot Act …

Jerricho Cotchery
wiki link

0.21

former team
0.61

Laveranues Coles

former team
0.61

birth place
0.93

wiki link
0.21

Danny Coles

…

Figure 2: KB graph for the Caldwell example.

tion, every Gic represents a collective linking solu-
tion to the given mention set. Figure 3 shows three
of the constructed candidate graphs for the Cald-
well example. One can see that the first two graphs
are very likely to be good solutions since they in-
herit many of the relation edges from GK , while
the third one is probably a poor collection as the
candidates barely connect to one another. In the
next section, we will more formally reveal how to
rank these candidate graphs to obtain the optimal
linking results.

New York Jets

Reche Caldwell

Florida New England Patriots

A

wiki link

birth place former team

Jerricho Cotchery Laveranues Coles

former team former team

New York Jets

Andre Caldwell

Florida New England Patriots

B

wiki link

birth place wiki link

Jerricho Cotchery Laveranues Coles

former team former team

Newcastle Jets FC

James Caldwell 
(clergyman)

Florida City, Florida
Patriots         

(American Revolution)

C

wiki link

Jerricho Cotchery Danny Coles

Figure 3: Candidate graphs for the Caldwell ex-
ample.

3.2 Candidate Ranking

With the constructed candidate graphs, QCV per-
forms two levels of ranking. First, it uses Equa-
tion (2) to compute the candidates’ salience scores
as a priori ranking. Then it compares each can-
didate graph with the mention context graph, and
evaluates their vertex set similarity for context
similarity ranking. Finally, by considering the re-
lation weights in the candidate graphs as well as
previous ranking scores, QCV collectively vali-
dates all the candidates and assembles the linking
results. Below we will focus on introducing the
context similarity ranking and the collective vali-
dation since the salience ranking resembles that of
our baseline system.

Context Similarity Ranking: As shown in Fig-
ure 3, among the constructed candidate graphs,
some of them contain many connected vertices
while some are otherwise quite disconnected. In-
tuitively we would like to measure this structure
difference by comparing each candidate graph Gic
with its mention context graph Gm. Granted, we
can only assert co-occurrence between two con-
nected mentions in Gm, but it should be of great
probability that two co-occurring mentions have
their entity referents connected by some relation
in the KB. In other words, the more a Gic is struc-
turally similar to its Gm, the better the candidates
in this Gic represent their mentions in Gm. There-
fore, we define the context similarity Sm(mc, c)
between a candidate c and its mention mc using
Jaccard similarity in Equation (4):

Sm(mc, c) =
|ΘGm(mc) ∩ΘGic(c)|
|ΘGm(mc) ∪ΘGic(c)|

(4)

698



where ΘGm(mc) and ΘG
i
c(c) denote mc’s neigh-

bor set in Gm and c’s neighbor set in Gic, re-
spectively. The intersection takes the candidates
of those mentions in ΘGm(mc) that appear in
ΘG

i
c(c), and the union is equivalent to ΘGm(mc)

due to the way we constructGic. We rankG
i
c using

the summation of the context similarity of every c
in Gic. Note that our baseline system uses Jaccard
similarity to achieve approximate string match be-
tween the surface forms of a mention and a can-
didate, while we alternatively use it to capture the
graph’s structural similarity. After ranking with
the context similarity, those Gic with more con-
nected vertices such as Figure 3A and Figure 3B
will get closer to the top of the ranked candidate
graph list.

Candidate Graph Collective Validation: Be-
sides the salience, the context similarity provides
another ranking score for each candidate c in Gic,
and it promotes those candidates remaining con-
nected in Gic. However, it fails to differenti-
ate how two candidates are connected. In Fig-
ure 3A, Reche Caldwell is a former player
of New England Patriots, and in Fig-
ure 3B, Andre Caldwell’s Wikipedia article
includes a hyperlink pointing to New England
Patriots. The former seems a “tighter” rela-
tion than the latter. Although these two distinct
relations imply that these two candidate pairs are
related with different relation types, the context
similarity rankings for these two candidate graphs
are identical. Based on this observation, assuming
that a “tighter” relation between two candidates is
more likely to be an appropriate representation of
the relation between their co-occurring mentions
in the source context, we propose a novel valida-
tion step that not only considers the two previous
ranking scores of each candidate but also quantita-
tively examines the relations between candidates.
We transfer the calculated relation weights from
Gk to Gic as positive indicators of how tightly two
candidates are related, and then define the com-
posite graph weight W (Gic) for each G

i
c in Equa-

tion (5) as the final ranking metric:

W (Gic) =
∑

c∈V (Gic)
Sa(c)Sm(mc, c) +

∑
r∈E(Gic)

H(r) (5)

where V (Gic) andE(G
i
c) are the vertex set and the

edge set of Gic; Sa(c), Sm(mc, c), and H(r) are
given by Equation (2), Equation (4), and Equa-
tion (1), respectively. With this composite graph

weight, since the relation “former team” has a
greater weight than “wiki link”, the candidate
graph in Figure 3A outweighs that in Figure 3B,
and therefore is ranked to the top.

4 Experiments

In this section, we first show QCV’s performance
on generic English corpora and compare it with
our baseline together with other state-of-the-art
EL systems. Then we move to a new language
(Chinese) and two new domains (Biomedical Sci-
ence and Earth Science) to demonstrate the lan-
guage and domain independent nature of our algo-
rithm.

4.1 EL on Generic English Corpora

For this evaluation, we used the TAC-KBP2013
EL dataset1, which contains 2,190 mentions ex-
tracted from English newswire, web blogs, and
discussion forums. We selected a subset of 1,090
linkable mentions that have entity referents in the
KB for our experiment. DBpedia 3.9, which was
generated from the Wikipedia dump in early 2013
and includes more than 4 million entities and more
than 470 million facts2, was used as our KB. We
followed the KBP EL track using B-Cubed+ (Ji
et al., 2011) as the evaluation metric. Table 1
presents the results of QCV, our baseline system,
as well as the top 3 supervised participant sys-
tems3and the top 3 unsupervised participant sys-
tems3 of the TAC-KBP2013 EL track.

System B3+ F1
Supervised 1st 0.7244

Supervised 2nd 0.7214

Supervised 3rd 0.7184

Unsupervised 1st 0.6324

Unsupervised 2nd 0.5764

Unsupervised 3rd 0.5734

Baseline (unsupervised) 0.697
QCV (unsupervised) 0.749

Table 1: Performance on the TAC-KBP2013 EL
Dataset (1,090 linkable mentions).

1http://www.nist.gov/tac/2013/KBP/data.html
2http://wiki.dbpedia.org/services-resources/datasets/data-

set-39
3Due to NIST policy, the names of the TAC-KBP2013

participant systems are not revealed.
4http://www.nist.gov/tac/publications/2013/papers.html

699



As shown in Table 1, QCV not only substan-
tially outperforms the best unsupervised systems
but also beats the best supervised systems from
the KBP participants. In order to understand this
notable advancement, we broke down our system
into components and evaluated them accumula-
tively using the same dataset as above. The ex-
periment results are summarized in Table 2.

Components B3+ P B3+R B3+ F1
SR 0.680 0.598 0.636
SR + CS 0.699 0.624 0.659
SR + CS + CV 0.789 0.712 0.749

Table 2: QCV Performance by Component.

In Table 2, SR, CS, and CV correspond to the
Salience Ranking, the Context Similarity Rank-
ing, and the Collective Validation in our QCV
algorithm, respectively. It can be seen that SR
already outperforms the best KBP unsupervised
systems from Table 1. This is mainly attributed
to the engagement of the entropy based relation
weights which injects the impact of different re-
lations into the entity salience. Notwithstanding
being somewhat effective, SR solely depends on
the KB and plays its role without the source con-
text. It should be straightforward that the sys-
tem performance gets improved after enabling CS
since the source context has been incorporated.
However, it was a little puzzling that the perfor-
mance boost by enabling CS turned out to be rel-
atively small. We took a careful look at the in-
termediate experiment results and discovered that
although CS did not produce a lot more correct
linking results than SR did, it did promote a great
number of good candidates to the top of the rank-
ing list. For example, in the Caldwell case, CS
successfully raised the rankings of the context-
related candidates such as Reche Caldwell,
Andre Caldwell, and Jim Caldwell, de-
spite the fact that it delivered Andre Caldwell
instead of Reche Caldwell as the final linking
result. This convincingly implies that CS is able to
well capture the context of the target mentions, but
meanwhile it is deficient in recognizing the subtle
contextual difference among similar candidates.
In Table 2 there is a significant performance gain
after enabling CV. As described in Section 3.2, CV
collectively validates the candidates of the target
mention “Caldwell” and the mentions in its con-
text such as “Florida”, “Patriots”, and “Jets” by

integrating their SR and CS scores as well as the
weights of the KB relations between them. There-
fore this improvement is reasonably substantial.

By investigating the remaining errors, we iden-
tified several potential causes: 1) Our system occa-
sionally could not capture enough context for the
target mention. This happened more frequently
for web blogs and discussion forums, where the
language was informal and casual. Without any
linguistic analysis on the source documents, it
was difficult for us to extract additional context
words. 2) Our simple coreference rules some-
times failed to work correctly and introduced false
candidates, which, without clear context to dis-
ambiguate, could lead to linking errors. 3) Our
KB had limited knowledge about some entities in
a way that certain relations were missing. This
kept us from creating necessary links in the can-
didate graphs and further effectively validating the
graphs.

4.2 EL on Generic Chinese Corpora

Using Chinese as a case study, we evaluate the lan-
guage portability of our approach. We used the
TAC-KBP2012 Chinese EL dataset1, and selected
a subset of 1,240 linkable mentions out of the total
2,122 mentions extracted from Chinese newswire,
web blogs, and discussion forums. For KB, we
still used DBpedia because it contains multilingual
surface forms for its entities. For instance, the en-
tity Barack Obama has surface forms in over 30
languages including the Chinese one: “贝拉克·奥
巴马”. This cross-lingual surface form mapping
naturally provides us with a convenient translation
tool. Table 3 shows the linking performance com-
parison among QCV, our baseline system, and the
top 3 participant systems of the KBP Chinese EL
track. Again, we employed the B-Cubed+ met-
ric.

System B3+ F1
Clarke et al. (2012) (supervised) 0.493
Monahan and Carpenter (2012) (supervised) 0.660
Fahrni et al. (2012) (supervised) 0.736
Baseline (unsupervised) 0.648
QCV (unsupervised) 0.671

Table 3: Performance on the TAC-KBP2012 Chi-
nese EL Dataset (1240 linkable mentions).

As shown in Table 3, the best performance is
1http://www.nist.gov/tac/2012/KBP/data.html

700



achieved by Fahrni et al. (2012), a supervised sys-
tem using over 20 fine-tuned features and many
linguistic resources. In contrast, our QCV is an
unsupervised approach without using any labeled
data or linguistic resources. During the error anal-
ysis, we found that in this dataset multiple men-
tions are often the variants of the surface form of
a single KB entity. For example, “奥巴马” and
“欧巴马”, being just different Chinese transliter-
ations, both refer to “Obama”. This fact tends to
result in a low recall for our system because one
or more of the mention variants may not exist in
the KB. We decided to heuristically apply a sub-
string matching in addition to the Wikipedia redi-
rection mapping to boost the recall. However, as
one can imagine, this simple strategy will impair
the system precision due to the introduced noise.
Take “奥巴马” again for example. If we only
match its second and third characters, “欧巴马”
will be correctly picked, but “巴马镇” (a small
town in China) will also be falsely included. For-
tunately, our QCV algorithm was able to select and
rank candidates complying with the source con-
text. Consequently most of this kind of noise got
filtered out, and we thus could produce balanced
precision and recall.

We acknowledge that, without performing
deeper linguistic analysis on the source docu-
ments, the cross-language surface form mapping
of the KB plays a crucial role in our approach. One
can replace it with any machine translation prod-
uct which, however, is not always available espe-
cially for a low-resource language. We should take
advantage of the existing KBs where such cross-
lingual mapping has already been widely created.
The latest DBpedia provides localized versions in
125 languages1, for instance.

4.3 EL in Biomedical Science

To demonstrate the domain portability of our ap-
proach, we first take the biomedical science do-
main as a case study. We conducted our ex-
periment using the evaluation dataset created by
Zheng et al. (2014) which contains 208 linkable
mentions extracted from several biomedical pub-
lications. We built our KB with over 300 domain
ontologies downloaded from BioPortal. Table 4
compares the linking accuracy of QCV and our
baseline system.

As shown in Table 4, our approach achieves

1http://wiki.dbpedia.org/about

System Correct Total Accuracy
Baseline 173 208 83.17%

QCV 177 208 85.10%

Table 4: Biomedical Science EL Performance.

similar performance to our baseline system which
is the state-of-the-art to our knowledge. However,
we were curious why QCV did not improve the
baseline system in the biomedical domain as much
as it did in the general domain. After some in-
depth analysis of the experiment results, we dis-
covered that in this dataset the candidates of the
related mentions (i.e. those mentions within the
same context window) mostly have similar rela-
tions in the KB. In other words, for each men-
tion, the candidate entity types are not as diverse
as those in the general domain. As a consequence,
the collective validation step in QCV does not take
much effect since the weights of the involved re-
lations are quite close to one another. On such a
dataset, the context similarity ranking will play a
major part for the disambiguation, and QCV will
not be able to function at its full power. Nonethe-
less, from the results we can see that our approach
can be efficiently and effectively adapted to this
new domain.

4.4 EL in Earth Science

Now we move to another new domain, Earth Sci-
ence. As far as we know, we are the first to study
EL in this domain. In order to create an evaluation
dataset, our domain expert selected three scientific
papers about Early Triassic discovery, Global Stra-
totype Section, and Triassic crisis, which are three
different aspects of Earth Science related discov-
ery, and then identified 296 mentions that can be
linked to DBpedia entities. Table 5 presents the
linking accuracy comparison between QCV and
our baseline system. We can see that QCV pro-
vided significant gains.

System Correct Total Accuracy
Baseline 221 296 74.66%

QCV 236 296 79.73%

Table 5: Earth Science EL Performance.

The linking errors were mainly caused by the
following reasons: 1) As a general KB, DBpe-
dia has introduced certain noise for our domain-

701



specific EL. For example, in Geology, the term
“Beds” mostly refers to “Geology Bed”, which is
a division of a geologic formation. But in general,
“Beds” usually means the beds people sleep on.
Much more common in the KB, the latter had such
a significantly higher salience score than the for-
mer that the final ranking score of our system got
biased. 2) Some relations between Earth Science
related entities are not clearly defined in DBpe-
dia. For instance, in geology time scale, the period
“Chattian” is immediately preceded by the period
“Rupelian”. An explicit relation such as “preceded
by” should be inserted between these two period
entities. Instead, only a vague “wiki link” relation
is present in our KB. This directly diminishes the
differentiating power of our system on the KB re-
lations.

It is worth mentioning that there exists a large
number of well established ontologies for different
sub-domains of Earth Science. SWEET ontolo-
gies1, for example, widely capture Earth and En-
vironmental terminologies. By adopting these on-
tologies, we will be able to considerably improve
our domain EL performance, and the benefits of
EL in the domain will further get revealed.

4.5 System Complexity

We indexed our KB and ontologies in the format of
triples using Apache Lucene2 such that retrieving
entity candidates of a mention is O(1). We pre-
computed all the entropy-based relation weights
and entity salience scores with complexities of
O(nr · ne) and O(ne · k), respectively, where nr
is the number of KB relations, ne is the number
of KB entities, and k is the number of iterations it
took for the salience score to get converged. For
the final QCV score computation, the upper bound
of the computing time to link all the mentions in a
document is O(nm · nc · nnc · nnm), where nm is
the number of linkable mentions in the document,
nc is the number of candidates for each mention,
and nnc is the number of neighbor nodes of a can-
didate, and nnm is the number of neighbors of a
mention.

5 Related Work

In recent years, collective inference methods for
EL have become increasingly popular. Many ef-
forts have been devoted to encoding linguistic fea-

1http://sweet.jpl.nasa.gov
2https://lucene.apache.org/

tures from the source documents in order to pre-
cisely select collaborator mentions for collective
inference. These features include topic model-
ing (Xu et al., 2012; Cassidy et al., 2012), re-
lation constraint (Cheng and Roth, 2013), coref-
erential chaining (Nguyen et al., 2012; Huang et
al., 2014), and dependency restriction (Ling et al.,
2014). Some recent work utilized multi-layer lin-
guistic analysis integration to capture contextual
properties for better mention collection (Pan et al.,
2015). While many of these approaches have been
proved to be effective, the dependency on deep
linguistic knowledge makes it difficult to migrate
them to a new language or domain. In contrast to
these methods, we establish a very loose setting
for the mention selection, and rely on the quanti-
fied information computed from the structured KB
to collectively evaluate and validate the entity can-
didates. Since the KB is relatively universal to
languages and domains, our approach inherently
is language and domain independent.

Recent cross-lingual EL approaches can be di-
vided into two types. The first type (McNamee et
al., 2011; Cassidy et al., 2011; McNamee et al.,
2012; Guo et al., 2012; Miao et al., 2013) trans-
lated entity mentions and source documents from
the new language into English and then ran En-
glish mono-lingual EL to link to English KB. The
second type (Monahan et al., 2011; Fahrni et al.,
2011; Fahrni et al., 2012; Monahan and Carpenter,
2012; Clarke et al., 2012; Fahrni et al., 2013) de-
veloped EL systems on the new language and used
cross-lingual KB links to map the link results back
to English KB. While the bottleneck of the former
method usually is on translation errors, the latter
approach heavily relies on the linguistic resources
and the KB of the new language. In comparison,
our system mainly uses the English KB and a men-
tion surface form mapping that can either come
from translation or cross-lingual KB links, and re-
quires minimal linguistic resources from the new
language.

There is a limited amount of research work
in the literature that focused solely on domain-
specific EL (Zheng et al., 2014). In the biomed-
ical domain, a few studies have been found on
EL-related tasks such as scientific name discov-
ery (Akella et al., 2012), gene name normaliza-
tion (Hirschman et al., 2005; Fang et al., 2006;
Dai et al., 2010), biomedical named entity recog-
nition (Usami et al., 2011; Van Landeghem et al.,

702



2012) and concept mention extraction (Tsai et al.,
2013). The baseline system (Zheng et al., 2014)
in this paper is the work most similar to ours in
a sense of collectively aligning mentions to struc-
tured KBs. However, our system differs by inte-
grating a context similarity ranking and a candi-
date validation to conduct a two-way collective in-
ference with better performance.

6 Conclusions and Future Work

Language and domain independence is a new re-
quirement to EL systems and this capability is par-
ticularly welcome by low-resource language re-
lated applications and domain scientists. In this
paper we demonstrated a high-performance EL
approach that can be easily migrated to new lan-
guages and domains due to the minimal reliance
on linguistic analysis and the deep utilization of
structured KBs. In the future, we plan to improve
the source document processing such that the sys-
tem can better extract the mention context without
involving extensive linguistic knowledge. We are
also experimenting with our collective validation
algorithm to incorporate the impact of more dis-
tant KB entities other than just the neighbors.

7 Acknowledgement

This work was supported by the U.S. DARPA
DEFT Program No. FA8750-13-2-0041, ARL
NS-CTA No. W911NF-09-2-0053, NSF CA-
REER Award IIS-1523198, DARPA LORELEI,
AFRL DREAM project, gift awards from IBM,
Google, Disney and Bosch. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.

References
Lakshmi Manohar Akella, Catherine N. Norton, and

Holly Miller. 2012. NetiNeti: Discovery of Sci-
entific Names from Text Using Machine Learning
Methods. BMC Bioinformatics, 13:211.

Taylor Cassidy, Zheng Chen, Javier Artiles, Heng Ji,
Hongbo Deng, Lev-Arie Ratinov, Jing Zheng, Ji-
awei Han, and Dan Roth. 2011. CUNY-UIUC-SRI
TAC-KBP2011 Entity Linking System Description.
In Proceedings of Text Analysis Conference 2011.

Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
Enhancement of Wikification for Microblogs with
Context Expansion. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics.

Xiao Cheng and Dan Roth. 2013. Relational Inference
for Wikification. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.

James Clarke, Yuval Merhav, Ghalib Suleiman, Shuai
Zheng, and David Murgatroyd. 2012. Basis Tech-
nology at TAC 2012 Entity Linking. In Proceedings
of Text Analysis Conference 2012.

Silviu Cucerzan. 2011. TAC Entity Linking by Per-
forming Full-Document Entity Extraction and Dis-
ambiguation. In Proceedings of Text Analysis Con-
ference 2011.

Hong-Jie Dai, Po-Ting Lai, and Richard Tzong-Han
Tsai. 2010. Multistage Gene Normalization and
SVM-Based Ranking for Protein Interactor Extrac-
tion in Full-Text Articles. IEEE/ACM Transac-
tions on Computational Biology and Bioinformatics,
7(3):412–420.

Jeffrey Dalton and Laura Dietz. 2013. A Neighbor-
hood Relevance Model for Entity Linking. In Pro-
ceedings of the 10th Conference on Open Research
Areas in Information Retrieval.

Angela Fahrni, Vivi Nastase, and Michael Strube.
2011. HITS’ Cross-lingual Entity Linking System
at TAC2011: One Model for All Languages. In Pro-
ceedings of Text Analysis Conference 2011.

Angela Fahrni, Thierry Göckel, and Michael Strube.
2012. HITS’ Monolingual and Cross-lingual Entity
Linking System at TAC 2012: A Joint Approach. In
Proceedings of the Text Analysis Conference 2012.

Angela Fahrni, Benjamin Heinzerling, Thierry Göckel,
and Michael Strube. 2013. HITS’ Monolingual and
Cross-lingual Entity Linking System at TAC 2013.
In Proceedings of Text Analysis Conference 2013.

Haw-ren Fang, Kevin Murphy, Yang Jin, Jessica S.
Kim, and Peter S. White. 2006. Human Gene Name
Normalization Using Text Matching with Automati-
cally Extracted Synonym Dictionaries. In Proceed-
ings of the Workshop on Linking Natural Language
Processing and Biology: Towards Deeper Biologi-
cal Literature Analysis, pages 41–48.

Norberto Fernández, Jesus A. Fisteus, Luis Sánchez,
and Eduardo Martı́n. 2010. WebTLab: A
Cooccurence-Based Approach to KBP 2010 Entity-
Linking Task. In Proceedings of Text Analysis Con-
ference 2010.

William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In
Proceedings of the Fifth DARPA Speech and Natu-
ral Language Workshop.

703



Filipe Guo, Ying Xu, Filipe Mesquita, Denilson Bar-
bosa, and Grzegorz Kondrak. 2012. ualberta at
TAC-KBP 2012: English and Cross-Lingual Entity
Linking. In Proceedings of Text Analysis Confer-
ence 2012.

Xianpei Han, Le Sun, and Jun Zhao. 2011. Collec-
tive Entity Linking in Web Text: A Graph-Based
Method. In Proceedings of the 34th Annual ACM
SIGIR Conference.

Lynette Hirschman, Marc Colosimo, Alexander Mor-
gan, and Alexander Yeh. 2005. Overview of
BioCreAtIvE Task 1B: Normalized Gene Lists.
BMC Bioinformatics, 6.

Hongzhao Huang, Yunbo Cao, Xiaojiang Huang, Heng
Ji, and Chin-Yew Lin. 2014. Collective Tweet Wik-
ification based on Semi-supervised Graph Regular-
ization. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics.

Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popu-
lation Track. In Proceedings of Text Analysis Con-
ference 2011.

Xiao Ling, Sameer Singh, and Daniel S. Weld. 2014.
Context Representation for Named Entity Linking.
In Proceedings of the 3rd Pacific Northwest Re-
gional NLP Workshop.

Paul McNamee, James Mayfield, Dawn Lawrie, Dou-
glas W. Oard, and David Doermann. 2011. Cross-
Language Entity Linking. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing.

Paul McNamee, Veselin Stoyanov, James Mayfield,
Tim Finin, Tim Oates, Tan Xu, Douglas W. Oard,
and Dawn Lawrie. 2012. HLTCOE Participation at
TAC 2012: Entity Linking and Cold Start Knowl-
edge Base Construction. In Proceedings of Text
Analysis Conference 2012.

Qingliang Miao, Ruiyu Fang, Yao Meng, and Shu
Zhang. 2013. FRDC’s Cross-lingual Entity Link-
ing System at TAC 2013. In Proceedings of Text
Analysis Conference 2013.

Sean Monahan and Dean Carpenter. 2012. Lorify: A
Knowledge Base from Scratch. In Proceedings of
Text Analysis Conference 2012.

Sean Monahan, John Lehmann, Timothy Nyberg, Jesse
Plymale, and Arnold Jung. 2011. Cross-Lingual
Cross-Document Coreference with Entity Linking.
In Proceedings of Text Analysis Conference 2011.

Hien T. Nguyen, Huy H. Minha, Tru H. Cao, and
Trong T. Nguyen. 2012. JVN-TDT Entity Linking
Systems at TAC-KBP2012. In Proceedings of Text
Analysis Conference 2012.

Xiaoman Pan, Taylor Cassidy, Ulf Hermjakob, Heng Ji,
and Kevin Knight. 2015. Unsupervised Entity Link-
ing with Abstract Meaning Representation. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics – Human Language Technologies.

Marco Pennacchiotti and Patrick Pantel. 2009. En-
tity Extraction via Ensemble Semantics. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing EMNLP2009.

Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and Global Algorithms for
Disambiguation to Wikipedia. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.

Chen-Tse Tsai, Gourab Kundu, and Dan Roth. 2013.
Concept-Based Analysis of Scientific Literature. In
Proceedings of 22nd ACM International Conference
on Information and Knowledge Management (CIKM
2013).

Yu Usami, Han-Cheol Cho, Naoaki Okazaki, and
Jun’ichi Tsujii. 2011. Automatic Acquisition of
Huge Training Data for Bio-medical Named Entity
Recognition. In Proceedings of BioNLP 2011 Work-
shop.

Sofie Van Landeghem, Jari Björne, Thomas Abeel,
Bernard De Baets, Tapio Salakoski, and Yves Van de
Peer. 2012. Semantically Linking Molecular Enti-
ties in Literature through Entity Relationships. BMC
Bioinformatics, 13.

Jian Xu, Qin Lu, Jie Liu, and Ruifeng Xu. 2012. NLP-
Comp in TAC 2012 Entity Linking and Slot-Filling.
In Proceedings of Text Analysis Conference 2012.

Jin Guang Zheng, Daniel Howsmon, Boliang Zhang,
Juergen Hahn, Deborah McGuinness, James
Hendler, and Heng Ji. 2014. Entity Linking for
Biomedical Literature. In Proceedings of the ACM
8th International Workshop on Data and Text
Mining in Bioinformatics.

704


